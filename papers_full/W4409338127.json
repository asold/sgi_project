{
  "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure",
  "url": "https://openalex.org/W4409338127",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4226764491",
      "name": "Budding, Céline",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3122548859",
    "https://openalex.org/W2921802966",
    "https://openalex.org/W3132758650",
    "https://openalex.org/W2995739402",
    "https://openalex.org/W2160156652",
    "https://openalex.org/W2330954386",
    "https://openalex.org/W2991187744",
    "https://openalex.org/W2122988375",
    "https://openalex.org/W2118373646",
    "https://openalex.org/W1521895194",
    "https://openalex.org/W2255466643",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W4392935324",
    "https://openalex.org/W2007368676",
    "https://openalex.org/W2521237138",
    "https://openalex.org/W4223947928",
    "https://openalex.org/W2040288146",
    "https://openalex.org/W4205842577",
    "https://openalex.org/W3018827121",
    "https://openalex.org/W4385573436",
    "https://openalex.org/W4392384010",
    "https://openalex.org/W4379347837",
    "https://openalex.org/W3010694149",
    "https://openalex.org/W2169818249",
    "https://openalex.org/W4243775346",
    "https://openalex.org/W4389765664"
  ],
  "abstract": "Abstract It is sometimes assumed that large language models (LLMs) know language, or for example that they know that Paris is the capital of France. But what—if anything—do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.",
  "full_text": "ARTICLE\nWhat Do Large Language Models Know? Tacit\nKnowledge as a Potential Causal-Explanatory\nStructure\nCéline Budding\nPhilosophy & Ethics Group and Eindhoven Artificial Intelligence Systems Institute, Eindhoven University\nof Technology, the Netherlands.\nEmail: c.e.budding@tue.nl\n(Received 26 January 2024; revised 28 January 2025; accepted 27 March 2025)\nAbstract\nIt is sometimes assumed that large language models (LLMs) know language, or for example\nthat they know that Paris is the capital of France. But what—if anything—do LLMs actually\nknow? In this paper, I argue that LLMs can acquiretacit knowledgeas defined by Martin Davies\n(1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I\ndemonstrate that certain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as\na conceptual framework for describing, explaining, and intervening on LLMs and their\nbehavior.\n1. Introduction\nAlthough large language models (LLMs) are generally trained on next-word\nprediction tasks, these systems have exhibited impressive performance in the\ngeneration of seemingly human-like, coherent texts (e.g. ChatGPT, OpenAI2022). Yet,\nwe do not know how these models work and what they learn from their data. Like\nother types of deep neural networks, LLMs are opaque and suffer from the black-box\nproblem: during training, LLMs learn a highly complex function with distributed\nrepresentations and we cannot simply“look inside” to determine how they work\n(Burrell 2016; Creel 2020). Because of this, it is difficult to evaluate their potential\nlinguistic and cognitive capacities (Shanahan2023).\nThe field ofexplainable AI(XAI) aims to solve the problem of opacity by developing\nexplanations through mathematical techniques that show how or why a network\nmakes a certain decision (Zednik2021). One specific question that has been asked is\nwhether LLMs learn any kind of rules or algorithms, akin to symbolic rules in classical\nsystems (Olah et al.2020; Pavlick 2023). Uncovering such rules would help explain\nhow AI systems work, facilitate the evaluation of potential cognitive and linguistic\n© The Author(s), 2025. Published by Cambridge University Press on behalf of the Philosophy of Science Association. This\nis an Open Access article, distributed under the terms of the Creative Commons Attribution licence ( https://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted re-use, distribution and reproduction, provided\nthe original article is properly cited.\nPhilosophy of Science (2025), 00,1 – 22\ndoi:10.1017/psa.2025.19\n\ncapacities, and potentially allow for interventions in the model internals to update\nthe system’s predictions.\nIn the particular case of LLMs, it has been suggested that they do not just perform\nnext-word prediction based on surface statistics, but that they learn to follow\nsymbolic rules (Pavlick2023), develop linguistic competence (Mahowald et al.2024),\nor even acquire a kind of knowledge (Meng et al.2022; Yildirim and Paul2024). Indeed,\nwith respect to syntax, LLMs seem to successfully represent grammatical rules, and\napply formal linguistic structure (Linzen and Baroni2021; Mahowald et al.2024). Less\nattention has been paid to semantics, but some very recent studies claim to have\nidentified representations of world models (Li et al.2023), implicit meaning (Hase\net al.2021), and facts (De Cao, Aziz, and Titov2021; Meng et al.2022). Although it is\nclear that many of these previous discussions are concerned with what LLMs“know,”\nit remains difficult to conceptualize exactly what this“knowledge” actually consists\nin. In this paper, I will focus on semantics, and in particular, LLMs’ knowledge of\nsemantic rules considered as representations of facts or other propositions that play a\ncausal role in the system’s behavior.\nSpecifically, taking inspiration from debates between symbolic and connectionist\nAI in the 1980s and’90s (e.g. Clark1991; Fodor and Pylyshyn1988), I propose thattacit\nknowledge, as defined by Martin Davies ( 1990), provides a suitable way to\nconceptualize semantic knowledge in LLMs. Tacit knowledge, in this context, refers\nto implicitly represented rules that causally affect the system ’s behavior. As\nconnectionist systems are known not to have explicit knowledge, tacit knowledge\nprovides a promising way to characterize and identify meaningful representations in\nthe model internals. Moreover, if representations of knowledge can be successfully\nidentified, these could be used both to explain how the model works and to\npotentially change the behavior of the system by overwriting or otherwise changing\nthis knowledge.\nThis paper is not the first to propose tacit knowledge for connectionist neural\nnetworks. As mentioned previously, Davies (1990) originally developed the account of\ntacit knowledge as a way to attribute knowledge to connectionist systems in the\nabsence of explicit rules. Nevertheless, Davies argues that connectionist neural\nnetworks do not actually meet the constraints needed for attribution of tacit\nknowledge. As such, my contribution goes beyond Davies’by applying tacit knowledge\nto contemporary transformer-based LLMs and showing that these can in fact meet the\nconstraints. More recently, Lam ( 2022) has argued that the identification and\ndescription of tacit knowledge should be a target for explainable AI, but does not\ndemonstrate that neural networks can actually have such tacit knowledge.\nOverall, I claim that if LLMs meet Davies’ constraints, they are capable of acquiring\na form of tacit knowledge, and that there is preliminary evidence that some LLMs\nhave in fact done so. To support this argument, I will (i) discuss why LLMs, while they\nare trained to perform next-word prediction, might actually learn semantic rules;\n(ii) introduce Davies’ account of tacit knowledge and the constraints that should be\nmet to attribute such knowledge; (iii) address Davies’ objection to applying tacit\nknowledge to more complex connectionist networks and show this does not hold for\nLLMs; and (iv) analyze an example from the recent technical literature to\ndemonstrate that some current LLMs might actually meet these constraints. Taken\ntogether, the contributions of this paper are twofold. First, I suggest that tacit\n2 Céline Budding\nknowledge, as defined by Davies (1990), is a promising way to conceptualize semantic\nknowledge in LLMs and show how the constraints can be modified to apply to current\nsystems. Second, I show that there is convincing—albeit preliminary—evidence that\nat least some LLMs meet these constraints.\n2. A short introduction to large language models\n2.1 The transformer architecture\nAlthough LLMs have recently gained immense popularity, creating a computational\n(rule-based or probabilistic) model o f language has been the goal of natural\nlanguage processing (NLP) since the 1950s, with machine learning– based methods\nbeing available since the 1980s (Nadkarni et al., 2011). Like other machine\nlearning – based neural networks, LLMs are trained on a particular task, generally a\nnext-word prediction task.1 In the simplest version of this task, a network receives a\nparticular input sequence and needs to predict the most likely next word based on\na learned probability distribution over t he vocabulary. User interfaces like\nChatGPT perform this task repeatedly to produce individual sentences or even\nessay-length texts.\nThe difficulty of this task lies in the fact that the next word(s) in a sequence often\ndepend on the context given by all previous text, and not just the words directly\npreceding it. While earlier NLP systems were fairly successful overall, they\ngenerally struggled with long-range dependencies (Graves 2012), leading to\nincoherent or repeating output text. Current LLMs, based on the transformer\narchitecture (Vaswani et al. 2017), do better at integrating context, even when\ngenerating long pieces of text. What makes the transformer architecture\nparticularly suitable for language is the addition of an attention mechanism\n(Bahdanau et al., 2015), which allows the network to keep track of long-range\ndependencies and integrate information about previous words (i.e. context) in its\npredictions. That is, the attention mechanism allows the network to learn which\nprevious words or parts of the input sequence it should focus on when processing\na particular word.\nMore specifically, transformer models consist of an input layer, an embedding\nlayer (see section4), an output layer that returns the final prediction, and stacked\ntransformer blocks in between (figure 1). These transformer blocks themselves\nconsist of two layers: the attention l ayer and a fully connected multilayer\nperceptron (MLP) layer. While MLP layers have not been discussed as much as the\nattention layer, they have recently been suggested to play a role in enriching the\ninternal representations with relevant context (Geva et al.2021;S u k h b a a t a re ta l .\n2019; also see section5). After the model has been trained by tuning its weights for\na particular task, the network can predict the most probable next word given the\nprobability distribution over the vocabu lary and the context given by the input\nsequence itself.\n1 Strictly speaking, LLMs predicttokens rather than words. Although tokens can and do sometimes\ncorrespond to words, they can also refer to other subsets of the input sequence, such as parts of words,\nsingle letters, or punctuation. For the sake of simplicity, however, I will talk about words, even when I\nshould technically speak about tokens.\nPhilosophy of Science 3\n2.2 What do LLMs learn?\nAlthough LLMs are optimized for next-word prediction, their internal processing\nremains opaque. During the training process, LLMs learn a highly nonlinear function\nthat allows them to predict the most likely next word. In an ideal case, this function\ntracks relevant features of the data, such as correlations between words in language\ntasks. Nevertheless, the system might also learn to track spurious correlations—\ncorrelations in the dataset that may be considered irrelevant for a particular\nprediction. An example of this was identified by Lapuschkin et al. (2019) in a well-\nperforming vision model. Despite its performance, they found that this model learned\nso-called “Clever Hans” solutions for some input– output pairs. For example, although\nimages containing a horse were correctly classified as“horse,” this classification was\nbased on the presence of a watermark in the input, rather than on the depiction of a\nhorse. Upon closer inspection of the dataset, it turned out that all images of horses in\nthe training set contained such a watermark, leading the model to track this spurious\ncorrelation. As the data on which the model was tested were drawn from the same\ndataset, this problem had not been identified before, but could have led to poor\nperformance in other contexts.\nWhile the example above might seem innocent, the fact that it is unclear which\nrules or features neural networks learn from the data is problematic for multiple\nreasons. First, it might be that the system learns incorrect or biased rules or features.\nFor instance, it might be that while learning the typical characteristics of medical\ndoctors—e.g., that they wear white coats and carry stethoscopes—the system learns\nthat being male is one such characteristic, possibly on the basis of oft-repeated\nFigure 1. Simplified overview of\nthe transformer architecture. At\nthe bottom, the input is“The Eiffel\nTower is in.” The embedding layer\ncomputes word embeddings for\neach word in the input (section4).\nAfterwards, the embedding is\nprocessed by a number of stacked\ntransformer blocks, each consist-\ning of an attention and MLP layer\n(section 5). Finally, at the top, the\nmost likely word, in this case\n“Paris,” is predicted as the output.\n4 Céline Budding\nstereotypes. Knowing which information is used by LLMs for their predictions—and\nideally, where this information is stored—might facilitate interventions that can help\nfix the predictions of these systems.\nSecond, and more important for the purposes of this paper, the impressive\nperformance of LLMs has led to the sometimes reflex-like attribution of linguistic and\ncognitive capacities like understanding and knowledge (e.g. Piantadosi and Hill2022;\nMeng et al.2022). It is unclear and even contested, however, whether such attributions\nare warranted and what it means to attribute something like knowledge to an LLM.\nInvestigating the underlying processingof these systems might provide clarity on\nwhether the attribution of such capacities to LLMs is actually warranted (Buckner2023;\nZednik 2018).\n2.3 Attributing knowledge to LLMs\nStudying the internal processing of LLMs might help determine whether LLMs merely\nuse statistical correlations in the text to predict the next word, or if they acquire\nsomething like knowledge, which might then explain their performance. There is fairly\nrobust evidence that LLMs learn and represent grammatical rules, i.e. syntax (Linzen and\nBaroni2021;M a h o w a l de ta l .2024). In contrast, technical research on semantic aspects of\nknowledge, such as representations of world models (Li et al.2023; Yildirim and Paul\n2024) or facts (Geva et al.2021;M e n ge ta l .2022), is still in early stages.\nRecent work on identifying representations of semantic facts in LLMs reports\npromising results, however. For example, Geva et al. (2021) show that LLMs acquire\nso-called key– value memories, which seem to correlate with human-interpretable\npatterns such as TV series, time ranges, and specific words like“press.” Similarly, Meng\net al. (2022) identify representations of facts in the model internals, which they\nsuccessfully edit to change the output ofthe network, both for the target input– output\npair and semantically similar input– output pairs. For example, assuming that a network\nrepresents“The Eiffel Tower is located in Paris,” they update this factual association to\n“The Eiffel Tower is located in Berlin.” Afterwards, the network responds to subsequent\nprompts as if the Eiffel Tower is now located in Berlin, for example stating that one passes\nthe Brandenburg Gate when walking to the Eiffel Tower (see section5). These results are\nin line with other recent studies (De Cao, Aziz, and Titov2021;G e v ae ta l .2021)t h a t\nidentify and edit representations of semantic facts in the model internals.\nNevertheless, this literature is largely motivated by practical concerns such as\nfacilitating edits, also referred to as interventions, to improve the performance of AI\nsystems, or explaining how the system works to identify when a system might make\nmistakes. As such, there is limited consideration of what it means to attribute\nknowledge in the first place. Indeed, there is no clear definition of what kind of\nknowledge LLMs might acquire, nor of the conditions that should be met in order to\nattribute such knowledge. In this paper, I propose one way to conceptualize this kind\nof knowledge in LLMs: tacit knowledge as defined by Davies (1990).\n3. Davies\n’ account of tacit knowledge\n3.1 Knowledge in neural networks\nFollowing Davies, I will motivate the focus on tacit knowledge specifically by first\nintroducing two alternative notions of knowledge of rules: explicit representation as a\nPhilosophy of Science 5\nstrong notion of knowledge, and mere conformity as a weak notion of knowledge.\nSymbolic AI systems such as expert systems like Cyc (Lenat and Guha 1989)a r e\noften said to have explicit knowledge of rules, in the sense that they have a pre-\nprogrammed knowledge base containing representations of rules that are invoked\nduring processing. These rules might be explicitly represented in the system ’s\nsource code, in which case an external observer might be able to“ read off” the\nrules given sufficient background knowle dge, access, and technical expertise.\nAlternatively, a system might make use of a database containing rules or logical\nstatements. While this database might not be accessible to an external observer,\nany rules or statements inthe database can be considered explicit knowledge to\nthe system.\nExplicit knowledge plays a causal role in mediating transitions from inputs to\noutputs. When processing a particular input, the system needs to identify and retrieve\nthe appropriate piece of knowledge, which then plays a causal role in the transition to\na particular output. For example, in a question-answering system, this knowledge\ncould be a statement of the form“Paris is the capital of France.” When the model is\nqueried with the input“What is the capital of France?” it returns“Paris” because of\nthis explicitly represented knowledge. Thus, given that a system is known to have\nexplicit knowledge, it is possible to explain its behavior in virtue of the system having\nthis knowledge.\nConnectionist systems like neural networks, on the other hand, cannot be said to\nhave explicit knowledge of rules (see also Fodor and Pylyshyn1988). In contrast to\nsymbolic systems, neural networks do not invoke rules represented in a\nknowledge base or database while processing inputs. Instead, neural networks\nare optimized using machine learning techniques to perform a particular function.\nThis means that the way an input is processed is determined by the weights of the\nnetwork, rather than by explicit rules stored elsewhere in the network. As the\nweights of individual nodes generally do not correspond one-to-one to rules due to\ndistributed representation (Hinton, McClelland, and Rumelhart 1986;V a nG e l d e r\n1992), neural networks cannot be said to have knowledge of rules in the\nstrong sense.\nInstead, neural networks can be said to conform to rules, or have weak knowledge\nof rules (Davies1990). Knowledge of rules in this sense does not require the system\nto represent rules in any way, but only requires that the behavior of the system—\nin terms of input– output transitions —matches what is described by the rule in\nquestion. Imagine we have a network thatcan correctly classify images of cats and\ndogs. Given that it successfully performs this task, the network could be said to\nconform to a rule“ pointy ears → cat” and “ floppy ears → dog,” b u ti tc o u l da l s o\nand alternatively be said to conform to the rule“ slitted pupils\n→ cat” and “ round\npupils → dog.” This kind of indeterminacy shows the limitation of only attributing\nknowledge of rules in the weak sense: one network might conform to multiple sets\nof rules (Kripke 1982), or multiple networks might conform to the same sets of\nrules while relying on wildly different underlying processing mechanisms (see e.g.\nBlock 1981). To distinguish between these k inds of systems, an intermediate\nnotion of knowledge is needed that chara cterizes knowledge of rules without\nrequiring explicit representation.\n6 Céline Budding\n3.2 Tacit knowledge as an intermediate notion of knowledge\nDavies’ account of tacit knowledge is meant to provide an intermediate account of\nknowledge, one that is more than mere conformity but less than explicit knowledge.\nTacit knowledge, according to this account, refers to rules that are not represented\nexplicitly, but that nevertheless describe causally relevant structures that guide\nbehavior (Davies 1990). Tacit knowledge is different from explicit knowledge in the\nsense that an observer (e.g. a computer programmer) cannot straightforwardly“read\noff” the rules by inspecting the system, but rather requires significant interpretative\nlabor. While it might be possible for such an observer to determine whether or not a\nclassical system has knowledge of a particular rule by inspecting its source code or\nknowledge base, identifying tacit knowledge of a rule—for instance in an LLM or\nanother artificial neural network—requires interpretation. As will be discussed in\nsection 5, this kind of interpretation might be facilitated through the use of XAI\nmethods that determine what is represented by particular structures within the\nsystem and how these structures contribute to the system’s behavior. As such, tacit\nknowledge differs from explicit knowledge in the amount of epistemic or\ninterpretative labor that is needed to identify it.\nIn addition to this epistemic way of distinguishing between tacit and explicit\nknowledge, it might also be possible to draw a metaphysical distinction. In this case,\nthe relevant distinguishing factor might be the way in which a rule can —in\nprinciple—be encoded in different architectures. To illustrate this difference,\nconsider the case of a network implementing a syntactic rule, for example regarding\nsubject– verb agreement. If a neural network learns this rule, it might be possible to\nidentify where it is implemented through the use of XAI. In this case, the system\nwould have tacit knowledge in an epistemic sense, as additional tools would be\nnecessary for identifying the encoded rules. Yet, this system would have explicit\nknowledge in a metaphysical sense, insofar as it is in principle possible to describe\nand interpret the encoded rule. In contrast, a system would have tacit knowledge in\nthe metaphysical sense when the causally relevant structures in the internal\nprocessing cannot be given a semantic description even in principle, despite the fact\nthat these structures are actually present and guide the system’s behavior.\n2 That said,\neven on this metaphysical conception of tacit knowledge, it remains an empirical\nquestion whether, and how, such causally relevant structures can actually be\nidentified, which is the question I will focus on in the remainder of the paper.\nTacit knowledge is also different from mere conformity to rules, insofar as it offers\na potential causal explanation of the model’s observed behavior. An attribution of\ntacit knowledge is grounded in a particular internal causal-explanatory structure (see\nsection 3.3.2), which can for example be used to predict the effects of targeted\n2 It is worth noting that even this metaphysical notion of tacit knowledge has a subject-relative\ndimension to it. Whether or not a causally relevant structure in the internal processing of an LLM can be\ngiven a particular semantic description (e.g. as an instance of a particular rule) depends on the concepts\nused in that description: while some intricate causal structures might not be describable in our day-to-\nday conceptual repertoire, they might nevertheless afford a semantic description in a more specialized\n(e.g. scientific or technical) vocabulary. Ultimately, determining which—if any—semantic description\ncan be given to a causally relevant structure will therefore depend in some way on an external observer\nwho aims to explain the behavior of the system.\nPhilosophy of Science 7\ninterventions on the internal processing of the system. Conformity, on the other\nhand, does not provide such causal explanations or constrain the internal causal\nprocessing of a system. To illustrate, recall the example described above about\nclassifying images of cats and dogs. It would be possible to distinguish between the\ntwo sets of rules described there by introducing new data points—such as a dog with\nfloppy ears and slitted pupils—and studying the system’s behavior. Yet, these rules\nstill describe only the behavior of the system, and do not provide a causal explanation\nof why those rules are an apt description. In particular, such a description does not\nidentify or localize particular internal causally relevant structures, nor facilitate\ninterventions to change the observed behavior.\nIt should be noted that the notion of “tacit knowledge ” is historically\ncontroversial. First, while a causal-explanatory structure as specified by Davies ’\naccount of tacit knowledge might help explain the behavior of a speaker or LLM, it is\ncontroversial whether referring to this as“knowledge” is appropriate at all (see, e.g.,\nGascoigne and Thornton 2014; Davies 2015). Having knowledge is often closely\nassociated with having justified true beliefs with a particular conceptual or\npropositional content. For instance, knowing the capital of France requires some\nconcept of France and what it means to be the capital of a country. Yet, it might be\nargued that states of tacit knowledge typically do not have such conceptual content: a\nspeaker might have tacit knowledge of a syntactic rule, but not possess the concepts\nto actually specify this rule. Moreover, states of tacit knowledge might not be\nconsidered beliefs insofar as a subject is often unaware of their tacit knowledge and\nstates of tacit knowledge are mostly employed in specific circumstances —for\ninstance, forming sentences—whereas beliefs can be used in a wider range of\ncontexts (Evans1985; Davies2015). While I am aware of these debates, it is beyond the\nscope of this paper to address these concerns in detail. Nevertheless, I will adopt the\nterm “tacit knowledge” to stay in line with the precedent set by Davies as well as the\nrelevant technical and theoretical literature in the AI community (e.g. Lam2022;\nMeng et al. 2022). In doing so, I will focus on the causal-explanatory role tacit\nknowledge might play in the explanation of the behavior of LLMs, and bracket\ndiscussions about whether attribution of tacit knowledge warrants the attribution of\nstronger notions of knowledge.\nSecond, Quine raised an objection to Chomsky’s( 1965) account of tacit knowledge\nthat is relevant to Davies’ account as well. Chomsky suggested that human linguistic\nbehavior could be explained through tacit knowledge, insofar as speakers of a\nlanguage seem to have knowledge that guides their linguistic behavior, even if they\nare unable to verbalize the grammatical rules they use to structure a sentence. In\nresponse to this, Quine (1970) asked how we can distinguish between the many\ndifferent sets of rules that adequately describe behavior, if no particular set of rules is\nexplicitly represented by the language user. In other words, how can we be sure that a\ngiven rule actually guides behavior if it is merely represented tacitly?\nTaking into account these concerns, there are two reasons that Davies’ account of\ntacit knowledge is particularly promising for explaining the (linguistic) behavior of\nLLMs. First, given that LLMs do not explicitly represent knowledge in the sense of\nallowing an observer to “read off” such knowledge, it seems reasonable to ask\nwhether there are any implicit representations that guide their behavior. While\nChomsky’s account focuses on human linguistic behavior and is not directly\n8 Céline Budding\napplicable to LLMs, Davies ’ account targets neural networks and can thus be\ntranslated to LLMs with relatively minor modifications. Second, Davies’ account\nallows for an answer to Quine’s concern, insofar as it spells out clear constraints that\nshould be met by a network in order to be attributed tacit knowledge of rules.\n3.3. Davies’ account of tacit knowledge\nTo specify what is required for attributing tacit knowledge, Davies introduces three\nconstraints: semantic description, causal systematicity, andsyntactic structure. To further\ncharacterize these constraints and thereby explain Davies ’ account of tacit\nknowledge, I will use a toy network with two outputs—“Paris” and “Berlin”—as\nan example. The inputs to this network are text prompts referring to particular\ntourist attractions in these two cities. For example, inputs to the system could be“The\nBrandenburg Gate is located in::: ,”“ The Sacré Coeur is located in the city of::: ,”\n“To see the Eiffel Tower, I need to go to::: .” We assume that the network has been\ntrained to successfully predict the next word for these prompts. Based on the\nbehavior, it seems that the network has weak knowledge of the location of tourist\nattractions: it is able to correctly predict the output based on a particular input\nsentence. So what does it take for this network to be attributed tacit knowledge?\n3.3.1 Semantic description\nThe first constraint for attributing tacit knowledge is semantic description. Davies\nargues that tacit knowledge can only be attributed to systems whose input and output\nstates can be described semantically in the sense that they register or represent\nsomething. Davies does not spell out in detail what it means for an input or output\nstate to have a semantic description, and I will not do so either.3 As the inputs and\noutputs of LLMs are word sequences that can be considered to refer to something in a\nconventional sense, it seems reasonable to conclude that they meet this constraint.\nBeyond ensuring that the input and output states of the system have semantic\nproperties, semantic description might also reveal certain patterns of semantic\nsimilarity in the data. Recall the toy network described above. The inputs and outputs\nof this network have a particular semantic structure: there are two types of outputs,\nnamely “Paris” and “Berlin.” The inputs are statements about tourist attractions in\nthese respective cities, and the network predicts in which of these cities the tourist\nattractions are. Then, there is a semantic pattern in the data insofar as inputs like\n“The Eiffel Tower is the main tourist attraction in::: ,”“ To see the Eiffel Tower, you\nhave to travel to the city of::: ,” etc. all refer to the same fact, namely that the Eiffel\nTower is in Paris. Similarly, input statements referring to the Brandenburg Gate being\nin Berlin can be considered semantically similar as well.\nThis is of course a very simple example and semantic similarity is expected to be\nmore complex in the actual inputs to real-world LLMs. Nevertheless, this example\nprovides a simple way to describe a particular semantic pattern in the input– output\ndata of the network. While semantic patterns by themselves do not indicate whether a\nnetwork has tacit knowledge or not, Davies’second constraint is defined in relation to\nsuch semantic patterns in the data.\n3 For further discussion on this topic, see, e.g., Dretske (1981), Millikan (1984).\nPhilosophy of Science 9\n3.3.2 Causal systematicity\nThe second and main constraint for attributing tacit knowledge iscausal systematicity.\nCausal systematicity puts a constraint on the internal processing of the system: the\ninternal processing in the network should mirror the semantic structure in the input–\noutput pairs. For instance, there should be one causal process that mediates all input–\noutput pairs referring to the Eiffel Tower, another one for all input– output pairs\nreferring to the Brandenburg Gate, and so forth (see figure2a). As Davies describes\nthis constraint (1990, 88/89):“If we now consider several input– output pairs which\nexhibit a common pattern, we can ask whether those input– output transitions have a\ncommon causal explanation corresponding to the common pattern that they\ninstantiate. Is there, within the physical system, a component mechanism or\nprocessor that operates as a causal common factor to mediate those several\ntransitions? If there is a common causal explanation, then we can say that the process\nleading from those inputs to outputs is causally systematic, relative to that pattern.”\nIn other words, the network should exhibit a pattern of causal common factors,\nwhich are structures in the network that mediate transitions from inputs to outputs\nfor input– output pairs of a particular type, and as such serve as a common causal\nexplanation for these transitions. This pattern of causal common factors should not\njust mirror any pattern in the data, but should correspond to the semantic patterns in\nthe input– output data, for example the fact that the Eiffel Tower is in Paris. If such a\npattern of causal common factors exists in the network, the network can be said to be\ncausally systematic with regards to the semantic description of the input and output\nstates.\nContrast this with a different underlying causal-explanatory structure, namely a\nnetwork that has memorized all individual input– output pairs—a structure that is\ncommonly called overfitting. In this case, there is an independent causal process that\nmediates each individual input– output pair (see figure 2b). As such, the network\nprocesses each input as being independent from all other inputs, and does not learn\nthat there are semantic similarities within the data. Given the semantic structure of\nlanguage, this pattern of memorization does not reflect the correct semantic patterns\nand therefore the network cannot be said to have tacit knowledge of semantic rules.\nAs such, the main requirement for attributing tacit knowledge is that the internal\nstructure of the network is causally systematic with regards to semantic patterns in\nthe input– output data: there should be a pattern of causal common factors that\nmediates transitions from inputs to outputs in semantically similar cases. Going back\nto the toy example, this would mean that all transitions from inputs referring to the\nEiffel Tower to outputs referring to Paris could be explained by one causal common\nfactor, with another causal common factor mediating transitions for input– output\npairs referring to the Brandenburg Gate.\nBriefly returning to the constraint of semantic description, note that causal\ncommon factors might, but need not, themselves have semantic descriptions. Insofar\nas the pattern of causal common factors mirrors the semantic structure of the data,\nDavies might have assumed that these internal factors are interpretable in the sense\nof being meaningful to human observers. Yet, given the black-box nature of\nconnectionist networks, it is currently unknown whether these systems exhibit any\ncausal common factors at all, and if so, whether these causal common factors are\ninterpretable in the sense of allowing for semantic description. At the same time, it\n10 Céline Budding\nseems likely that the causal structure that ensures that an LLM performs well at\npredicting words of a particular natural language, acquired through extensive\ntraining, does so because that structure reflects something about the semantic\nstructure of that language—see, e.g., recent mechanistic interpretability work by\nTempleton et al. (2024). Although I will follow Davies in not requiring semantic\ndescriptions of the internal structures for the attribution of tacit knowledge,\ndetermining if potential causal common factors can be given a semantic description\nseems like a particularly promising direction of future research.\nThe requirement of causal systematicity also allows for a response to Quine’s\nconcern about tacit knowledge. Consider the examples in figure2, one adhering to the\nrequirement of causal systematicity and one consisting of mere memorization.\nAlthough these systems might not be distinguishable based on behavior,\nexperiments—in particular using interventions —can be used to identify the\nunderlying causal structure (Davies1987; Evans 1985). That is, changing the value\nof a causal common factor in a causally systematic system should lead to different\nFigure 2. Examples of internal processing. (a) Causally systematic processing, with three semantically\nsimilar inputs, and a causal common factor that is involved in the processing for all three input–output\ntransitions. (b) An example of a network that memorizes input–output pairs: there is a one-to-one mapping\nbetween individual inputs and outputs.\nPhilosophy of Science 11\noutputs for all inputs of a particular type (figure3a). Interventions on the model\ninternals of a network with one-to-one mappings, on the other hand, would have\nlocalized effects (figure 3b). Therefore, interventions are one way to distinguish\nbetween different causal-explanatory structures and to verify whether a network\nmeets the constraint of causal systematicity (see also section5).\n3.3.3 Syntactic structure\nThe last constraint—syntactic structure4—serves as a bridge connecting semantic\ndescription with causal systematicity. Causal systematicity requires that all inputs\nthat have a similar meaning —i.e. the same semantic description —should be\nprocessed by the same causal common factor (figure 2). To ensure that all\nFigure 3. These structures are the same as in figure2, but illustrate the effect of interventions. (a) The effect\nof intervening on a causal common factor. Since a causal common factor is implicated in all transitions from\ninputs to outputs of a particular type, intervention on this causal common factor will affect all corresponding\noutputs. (b) In the case of memorization, intervening on a single input–output pair will only affect the output\nfor that particular input, and not affect any other input–output pairs.\n4 Note that Davies’ use of“syntax” is different than in linguistics and NLP, where“syntax” generally\nrefers to grammatical rules. In Davies’ work, “syntax” specifically refers to the physical properties of the\ninput that determine the causal consequences.\n12 Céline Budding\nsemantically similar inputs are recognized and processed in a similar way, Davies\nargues that there should be a shared property within input representations that\ndetermines their causal processing. More precisely, semantically similar inputs\nshould have some shared property that (i) allows the network to recognize all inputs\nof a particular (semantic) type and (ii) determines the causal processing for these\ninputs. For example, there should be one unit or pattern of activation at the input\nlayer that is active whenever there is an input about the Eiffel Tower, and another\nunit or pattern of activation that is active for inputs about the Brandenburg Gate.\nWhenever a new input contains one of the respective representations, the\ncorresponding causal common factor is engaged. This ensures that all semantically\nsimilar inputs are processed by the same causal common factor.\n3.4. Davies’ objection\nAccording to Davies, all three constraints—semantic description, causal system-\naticity, and syntactic structure—are necessary for attributing tacit knowledge to a\nconnectionist neural network. Yet, Davies argues that connectionist neural networks\nwith distributed representation do not meet the constraint of syntactic structure, as\nthey do not have shared properties across representations of semantically similar\ninputs, due to the so-called dimension shift (Smolensky 1988). Whereas symbols in\nsymbolic systems are the same regardless of context, neural networks represent their\ninputs as patterns of distributed activation. These patterns might be slightly different\ndepending on the context of the input. For example, while both“drinking coffee” and\n“a cup of coffee” contain the word“coffee,” their representations in the network are\nlikely to differ given the different context. As such, there is no single pattern of\nactivation—i.e. no syntactic property—that is shared by all inputs that contain the\nword “coffee.” Because syntactic structure determines the causal processing of the\ninputs, the lack of a shared property means that different causal processes may be\nengaged and there is no single causal common factor engaged in all these input–\noutput transitions. In other words, without syntactic structure, there cannot be causal\nsystematicity and the network cannot be attributed tacit knowledge.\n4. A reply: Architectural innovations in LLMs\nIn the remainder of the paper, I endorse the constraints of semantic description and\ncausal systematicity as defined by Davies. However, Davies’ constraint on syntactic\nstructure might be too stringent for LLMs. In light of architectural differences\nbetween LLMs and earlier connectionist systems, I argue that LLMs meet a weaker\nversion of the syntactic structure constraint that does not require identical properties\nacross input representations, but that nevertheless suffices to attribute tacit\nknowledge. This weakened criterion for syntactic structure requires that the model\nmaps semantically similar inputs to proximate regions in the input vector space\nthrough the embedding layer. This mapping allows the network to recognize and\nprocess inputs via a shared causal mechanism. Unlike Davies’ original constraint,\nwhich requires an identical shared property in the input representation, this weaker\nconstraint acknowledges the role of the embedding layers in recognizing semantically\nsimilar inputs and ensuring similar processing.\nPhilosophy of Science 13\nLet me elaborate on the weaker constraint and why it can be considered to have\nthe same function, for the purpose of attributing tacit knowledge, as the more\nstringent constraint proposed by Davies. Presumably, being able to recognize\nsemantically similar inputs is one of the reasons that transformer-based LLMs have\nbeen so successful. Whereas connectionist networks generally consist of architectur-\nally (almost) identical layers—except for the trained weights and number of nodes—\ntransformer-based models consist of specialized layers that take different roles in the\nprocessing of input sequences, such as the attention and feedforward layer (section2).\nFor the current discussion, I focus on theembedding layer, which computes aword\nembedding for each word in the input.\nA word embedding is a representation of a word in the input that not only contains\ninformation about the word itself, but also about the context and the relationship\nto other words in the vocabulary. An important characteristic of a word\nembedding is that similar words are represented by similar embedding vectors,\nand dissimilar words are represented by more dissimilar embedding vectors, i.e.\nwhose values are less alike (Mikolov et al. 2013; Grand et al. 2022). As such,\nembeddings of words with a similar meaningare not necessarily identical, but will\nbe closer together in vector space than embeddings of words with more dissimilar\nmeanings. This way of looking at the meaning of words, as given by their context\nand co-occurrence with other words, originates in the distributional hypothesis ,\nwhich states that words that occur in a similar context will tend to have similar\nmeanings (Firth 1957;H a r r i s1954).\nAs such, the embedding layer in LLMs acts as a kind ofsemantic categorizer which\ngroups similar inputs together in a high-dimensional vector space. For example, it has\nbeen shown that the embedding vectors at various stages of processing reflect\nsemantic and topical properties of the input, such as color similarity (Abdou et al.\n2021), linguistic structure (Chang, Tu, and Bergen2022), and word meaning (Grand\net al. 2022). This means that the embedding layer allows the network to recognize\nsemantically similar inputs and to make use of this information to predict the most\nlikely next word.\nMoreover, the fact that semantically similar inputs are close together in the high-\ndimensional vector space also guides the network towards similar processing\npathways. While it is difficult to confirm empirically that these pathways are identical\nand research on how LLMs categorize and process their inputs is ongoing (e.g.\nTempleton et al.2024), recent indirect evidence from intervention studies supports\nthe conclusion that similar inputs are processed in similar ways (De Cao, Aziz, and\nTitov 2021; Meng et al. 2022). In particular, as will be illustrated in section 5,\nintervening on the network to change the prediction for“Paris is the capital of\nFrance” to “Paris is the capital of Germany” was shown to generalize to semantically\nsimilar inputs, e.g. paraphrases and other variations of the inputs. This suggests that\nthe same internal processing is involved for these predictions, even in the absence of\nidentical input vectors. By capturing the essence of syntactic structure through\nproximity in the embedding space, LLMs can thus achieve a form of causal\nsystematicity without requiring identical activation patterns for semantically similar\ninputs.\nIn summary, Davies’ requirement of identical syntactic properties for semantically\nsimilar inputs seems to be too strong in the context of LLMs. While transformer-based\n14 Céline Budding\nLLMs do not have identical syntactic representations at the input layers, the weaker\ncriterion acknowledges the role of the embedding layer in achieving semantic\nclustering and consistent causal processing. This adjustment enables the attribution\nof tacit knowledge to LLMs by satisfying a more flexible yet functionally equivalent\nversion of the syntactic structure constraint.\n5. Do LLMs have tacit knowledge?\nIn the previous section, I have shown that transformer-based LLMs meet a weaker\nversion of the syntactic structure constraint, which still allows for the attribution of\ntacit knowledge. The next step is to verify whether LLMs meet the constraint of causal\nsystematicity. In this section, I analyze a recent example from the technical literature\n(Meng et al.2022) and show that the representations of factual associations identified\nin this work could be considered causal common factors in Davies’ sense, providing\ncompelling—albeit preliminary —evidence that at least some LLMs meet the\nconstraints for tacit knowledge.\nFigure 4. An example of an intervention that is optimized to associate“The Eiffel Tower is in” with “Rome.”\nThe target is an MLP module in an intermediate layer of the network. In this layer, the key–value pair (k, v)a t\nthe last token of the subject (in this case,“Tower”) is replaced with an updated key–value pair (k*, v*) that is\noptimized for the new association. Redrawn and adapted from Meng et al. (2022).\nPhilosophy of Science 15\n5.1 An example\nIn their paper, Meng and colleagues investigate factual associations in LLMs,\nspecifically where and how facts are stored in the model internals. Take the example\nof an LLM that correctly predicts“Paris” in response to an input like“The Eiffel Tower\nis located in,”“ Berlin” to “The Brandenburg Gate is located in the city of,” etc. While it\nis often suggested that such a network“knows” the location of particular tourist\nattractions purely based on its behavior, Meng and colleagues investigate where the\nnetwork stores such facts and how potential representations are used in the\nprocessing of the network.\nMeng and colleagues approach this problem in two steps: first, localizing potential\nrepresentations of facts, and then, verifying that these play a causal role in the\nobserved behavior. To localize representations of facts in the network, Meng and\ncolleagues use a method calledcausal tracing, which determines which activations, for\nexample at which layer, are most important for a particular prediction. To test this,\nthe activations of the network are corrupted such that the network no longer predicts\nthe correct outcome. Then, activations are iteratively restored to the original value,\nuntil the network returns the correct prediction. Using this method, activations in the\nMLP layers are found to play an important role in representing factual associations.\nBased on these results, Meng et al. suggest that facts are stored in certain MLP\nmodules in the middle layers of the network. Moreover, in line with Geva et al. (2021),\nthey suggest that MLP modules might function as so-calledkey– value pairs, which take\ninputs that represent a particular input and return outputs that reflect memorized\nproperties of that input. For example, given an input “the Space Needle is in\ndowntown,”“ Space Needle” might be viewed as a key, which prompts the MLP\nmodule to recall a particular value which represents properties of the Space Needle,\nsuch as its location. This information is accumulated over multiple layers and finally\nleads to a particular output at the last layer.\nIn the second part of their study, Meng and colleagues verify whether MLP\nmodules function as key– value pairs and, if so, whether they can be updated to change\nthe model’s behavior. To do this, they introduce a method calledrank-one model editing\n(ROME) which identifies and applies edits to an LLM ’s weights, based on two\nassumptions. First, if the MLP module functions as a key– value pair, this key– value\npair can presumably be updated to include new information. Second, if the key– value\npair plays a causal role in the model’s behavior, updating the stored information\nshould be reflected in the model’s output. As such, the goal is to calculate a new key–\nvalue pair that encodes a particular updated factual association, e.g.“The Eiffel Tower\nis in Rome.” The existing key– value pair is then replaced with this new key– value pair\n(see figure4), after which the output of the network is evaluated for various prompts.\nThe edit is successful if the output of the network changes to the newly inserted fact.\nAs mentioned, Meng et al. ’s goal is to update what they call the “\nfactual\nknowledge” of the system, in order to change its behavior. More precisely, what Meng\net al. consider to be knowledge is the ability to generalize across contexts. This is\ncontrasted with the network merely memorizing sequences of words from the input\ndata and regurgitating these in response to a prompt. If an edit successfully targets\nthis kind of knowledge, the updated fact should generalize: the network not only\nreturns the updated fact for the exact input prompt, but also other semantically\n16 Céline Budding\nsimilar paraphrases. Yet, while Meng et al. operationalize knowledge as generaliza-\ntion, my goal is to determine if this can be considered a more robust kind of\nknowledge, namely tacit knowledge in Davies’ sense. In the following sections, I show\nthat MLP key– value pairs could be considered causal common factors, providing\npreliminary evidence that some LLMs can meet the constraint of causal systematicity\nand might thus have tacit knowledge.\n5.2 MLP key– value pairs as causal common factors?\nImagine a causally systematic network with a pattern of causal common factors that\nreflects semantic similarity in the data. What would be the expected pattern of\nbehavior in response to an intervention? In this network, an intervention successfully\ntargeting a causal common factor would have a generalized effect. That is, intervening\non a causal common factor that is implicated in a causal processX would be expected\nto change all outputs that depend onX (see figure3). For example, changing a causal\ncommon factor that represents information about the capital of France to state“The\ncapital of France is Berlin” would be expected to have an effect on all outputs\nreferring to the capital of France. Moreover, this intervention would not affect\ntransitions for other input– output pairs, i.e. the model does not suddenly predict that\nthe capital of all countries is Berlin. In other words, a common causal factor should be\ninvolved in all transitions of input– output pairs of a given type, and not be involved in\nany other input– output transitions.\nBased on two metrics—generalization to measure whether the update generalizes to\nsemantically equivalent inputs, andspecificity to measure whether the update does\nnot affect unrelated input– output pairs—Meng and colleagues report that their edits\nsuccessfully update predictions both for the target input – output pair and for\nsemantically similar input– output pairs. Although their results do not suggest that\nthe network is causally systematic for all inputs—in which case generalization and\nspecificity would both be 100%—the network largely seems to follow the expected\npattern for causal systematicity. Even though Meng and colleagues only applied edits\nto single input– output pairs, generalization is high. This means that interventions on\na specific fact update the predictions for semantically similar input– output pairs as well,\nfor example paraphrases of the original input. Moreover, they report relatively high\nspecificity suggesting that representations that concern different concepts are relatively\ndisentangled. Taken together, this suggests that the target of intervention—the MLP\nmodule—does in fact function as a kind of causal common factor.\n5.3 Implications and reasons for caution\nMeng et al.’s results provide compelling preliminary evidence that at least some LLMs\nmeet Davies’ constraints, and can therefore be attributed tacit knowledge. LLMs meet\nthe first constraint, semantic description, as their inputs and outputs consist of word\nsequences. The second constraint, causal systematicity, is also met, insofar as the LLM\ninvestigated by Meng and colleagues exhibits behavior characteristic of causally\nsystematic systems in response to interventions. Finally, while Davies argues that\nconnectionist neural networks do not meet the third constraint of syntactic structure,\nI have shown that LLMs meet a weaker version of this constraint, as word embeddings\nallow them to recognize semantically similar inputs. Moreover, the fact that\nPhilosophy of Science 17\ninterventions on MLP key– value pairs update both the target input– output pair and\nsemantically similar paraphrases provides further empirical support for the claim\nthat LLMs not only recognize semantically similar inputs, but also process these in a\nsimilar way.\nDespite the promising results, there are some reasons for caution. First, although\nthe reported specificity and generalization are high, they are not perfect. As such,\nupdates might affect unrelated input– output pairs or fail to generalize. Possible\nexplanations are that some updates do not target a causal common factor, or that\ncausal common factors are not perfectly delineated in neural networks with distributed\nrepresentation. The latter is of particular concern, as networks with distributed\nrepresentation often exhibitpolysemanticityor superposition, meaning that the same nodes\nin a network are involved in different decision processes (Van Gelder1992; Elhage et al.\n2022). Interventions might then affect multiple predictions, or even lead to catastrophic\nforgetting of previously learned associations. Further empirical research should\ndetermine to what extent this is a problem for interventions in practice.\nSecond, the work by Meng et al. (2022) relies heavily on interventions to locate and\nidentify representations of tacit knowledge. Such interventions have recently been\ncriticized, however. Specifically, while replicating the results reported by Meng and\ncolleagues, Hase et al. (2023) found that interventions in different locations have a\nsimilar efficacy to the ones applied in the original study. From this, Hase and\ncolleagues conclude that while causal tracing might localize a fact to a particular MLP\nkey– value pair, this might not always be the best target for editing this fact. In the\ncontext of tacit knowledge, this means that causal tracing might not reliably localize\nthe relevant causal common factors in the network.\nMore research is thus needed to investigate how causal common factors are\nrepresented in LLMs. Indeed, while Meng et al. seem to assume that facts are stored in\na single layer—by focusing on layers during causal tracing—causal common factors,\nand thus tacit knowledge, might be represented over multiple layers. Neural networks\noften exhibit redundancy, meaning that various layers perform a similar task. As\nsuch, causal tracing might only partially characterize the relevant causal common\nfactors (if there are any). While the work by Meng and colleagues provides\npreliminary evidence for the claim that neural networks can and do at least\nsometimes acquire tacit knowledge, the notion of“causal common factors” and how\nthese factors are represented in the relevant networks deserves further investigation.\n6. Conclusion\nThe contributions of this paper are twofold. As a methodological contribution, I\nargued that we can take inspiration from Davies’ account of tacit knowledge to\nconceptualize semantic knowledge in LLMs. More precisely, Davies’ account provides\nclear criteria that should be met in order to attribute tacit knowledge to a particular\nsystem. While Davies himself argued that connectionist systems cannot meet the\nconstraint of syntactic structure, I argued that this constraint can be appropriately\nweakened to acknowledge the role of the embedding layer in current LLMs. Armed\nwith this weakened constraint, LLMs can in fact be attributed tacit knowledge,\nprovided that they meet the other constraints. As an empirical contribution, I\nevaluated the recent work by Meng et al. (2022) to argue that there is compelling\n18 Céline Budding\npreliminary evidence to suggest that some LLMs actually acquire tacit knowledge in\nthis sense. While this evidence is currently preliminary, tacit knowledge could thus be\na promising tool to guide further research into the internal causal processing of LLMs.\nNotably, this paper fits into a broader literature suggesting that LLMs represent\nsomething akin to knowledge in the model internals (Hase et al.2021; Li et al.2023;\nPavlick 2023). Provided this is supported by future work, this would have promising\nimplications for explainable AI. In particular, the identification of such knowledge\nstructures would not only provide a novel way to explain how these systems work, by\nidentifying internal common causal factors, but also to improve the performance of\nthese systems, by applying interventions to change the internal knowledge\nrepresentation so as to, e.g., counteract misinformation, hallucination, and bias.\nIn this context, a promising direction of future research is the notion of\ninterventions. Interventions are often used to investigate the causal processing\nwithin neural networks and to update the behavior of the network for particular\ninputs. In both cases, interventions are given a causal interpretation. It is not clear,\nhowever, to what extent such a causal interpretation is warranted for interventions\nas currently applied in machine learning. In particular, a concern is that interventions\nmight have unintended results, for example affecting more predictions than intended.\nOne way of approaching this problem could be to use existing frameworks like\ninterventionism that lay out criteria for causal interventions (Woodward2003).\nIn general, showing that LLMs have tacit knowledge would provide further insight\ninto what LLMs can learn from pure-text data. Whereas it has been argued that\ncurrent training methods for LLMs are insufficient for acquiring meaning (Bender and\nKoller 2020) or social aspects of language (Mahowald et al.2024), tacit knowledge\nprovides one way to conceptualize knowledge of semantic rules in LLMs. Since Meng\nand colleagues only analyze an early type of LLM, GPT-J, the question remains\nwhether more recent LLMs also acquire tacit knowledge. Given the improvements in\nperformance, however, this does not seem like an unreasonable assumption.\nAcknowledgments. I am grateful to Carlos Zednik, Vincent Müller, Lambèr Royakkers, and Charles\nRathkopf, as well as my colleagues at the Philosophy & Ethics group at Eindhoven University of\nTechnology, for helpful discussions and comments. Versions of this paper were presented at the Early\nCareer Researchers Workshop hosted by the Ruhr-Universität Bochum, the 49th meeting of the Society of\nPhilosophy and Psychology at the University of Pittsburgh, and the 5th Conference on Philosophy of\nArtificial Intelligence hosted by the Universität Erlangen-Nuremberg. I would like to thank the\nparticipants for their helpful questions and comments.\nFunding information. This research was funded by an EAISI startup grant “Cognitive models as\nsurrogate models for explainable AI.”\nDeclarations. None to declare.\nReferences\nAbdou, Mostafa, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders Søgaard.\n2021. “Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color.”\narXiv preprint. https://doi.org/10.48550/arXiv.2109.06129.\nBahdanau, Dzmitry, Kyung Hyun Cho, and Yoshua Bengio. 2015.“Neural Machine Translation by Jointly\nLearning to Align and Translate. ” In Proceedings of the 3rd International Conference on Learning\nRepresentations (ICLR 2015). San Diego, United States.\nPhilosophy of Science 19\nBender, Emily M., and Alexander Koller. 2020.“Climbing towards NLU: On Meaning, Form, and Understanding\nin the Age of Data.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\n5185– 98. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-\nmain.463.\nBlock, Ned. 1981.“Psychologism and Behaviorism.” The Philosophical Review90 (1):5– 43. https://doi.org/\n10.2307/2184371.\nBuckner, Cameron J. 2023.From Deep Learning to Rational Machines: What the History of Philosophy Can Teach\nUs about the Future of Artificial Intelligence. Oxford: Oxford University Press.\nBurrell, Jenna. 2016. “How the Machine ‘Thinks’: Understanding Opacity in Machine Learning\nAlgorithms.” Big Data & Society3 (1):205395171562251. https://doi.org/10.1177/2053951715622512.\nChang, Tyler, Zhuowen Tu, and Benjamin Bergen. 2022.“The Geometry of Multilingual Language Model\nRepresentations.” In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\n119– 36. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics.https://doi.org/\n10.18653/v1/2022.emnlp-main.9.\nChomsky, Noam. 1965.Aspects of the Theory of Syntax. Cambridge, MA: MIT Press.\nClark, Andy. 1991. “Systematicity, Structured Representations and Cognitive Architecture: A Reply to\nFodor and Pylyshyn.” In Connectionism and the Philosophy of Mind, edited by Terence Horgan and John\nTienson, 198– 218. Dordrecht: Springer Netherlands.https://doi.org/10.1007/978-94-011-3524-5_9.\nCreel, Kathleen A. 2020. “Transparency in Complex Computational Systems. ” Philosophy of Science\n87 (4):568– 89. https://doi.org/10.1086/709729.\nDavies, Martin. 1987.“Tacit Knowledge and Semantic Theory: Can a Five Per Cent Difference Matter?”\nMind 96 (384):441– 62.\nDavies, Martin. 1990.“Knowledge of Rules in Connectionist Networks.” Intellectica. Revue de l’Association\nPour La Recherche Cognitive9 (1):81– 126. https://doi.org/10.3406/intel.1990.881.\nDavies, Martin. 2015.“Knowledge (Explicit, Implicit and Tacit): Philosophical Aspects.” In International\nEncyclopedia of the Social & Behavioral Sciences,7 4– 90. Amsterdam: Elsevier. https://doi.org/10.1016/\nB978-0-08-097086-8.63043-X.\nDe Cao, Nicola, Wilker Aziz, and Ivan Titov. 2021.“Editing Factual Knowledge in Language Models.” arXiv\npreprint. https://doi.org/10.48550/arXiv.2104.08164.\nDretske, Fred I. 1981.Knowledge and the Flow of Information. Cambridge, MA: MIT Press.\nElhage, Nelson, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac\nHatfield-Dodds, et al. 2022.“Toy Models of Superposition.” arXiv preprint. https://doi.org/10.48550/\narXiv.2209.10652.\nEvans, Gareth. 1985.Semantic Theory and Tacit Knowledge, in Collected Papers. Oxford: Clarendon Press.\nFirth, John Rupert. 1957.“A Synopsis of Linguistic Theory, 1930– 1955.” In Studies in Linguistic Analysis,\nedited by John Rupert Firth,1 – 32. Oxford: Blackwell.\nFodor, Jerry A., and Zenon W. Pylyshyn. 1988.“Connectionism and Cognitive Architecture: A Critical\nAnalysis.” Cognition 28 (1):3– 71. https://doi.org/10.1016/0010-0277(88)90031-5.\nGascoigne, Neil, and Tim Thornton. 2014.Tacit Knowledge. London: Routledge.https://doi.org/10.4324/\n9781315729886.\nGeva, Mor, Roei Schuster, Jonathan Berant, and Omer Levy. 2021.“Transformer Feed-Forward Layers Are\nKey– Value Memories.” arXiv preprint. https://doi.org/10.48550/arXiv.2012.14913.\nGrand, Gabriel, Idan Asher Blank, Francisco Pereira, and Evelina Fedorenko. 2022.“Semantic Projection\nRecovers Rich Human Knowledge of Multiple Object Features from Word Embeddings.” Nature Human\nBehaviour 6 (7):975– 87. https://doi.org/10.1038/s41562-022-01316-8.\nGraves, Alex. 2012. “Long Short-Term Memory.” In Supervised Sequence Labelling with Recurrent Neural\nNetworks, edited by Alex Graves, 37– 45. Studies in Computational Intelligence. Berlin: Springer.\nhttps://doi.org/10.1007/978-3-642-24797-2_4.\nHarris, Zellig S. 1954. “Distributional Structure.” WORD 10 (2– 3):146– 62. https://doi.org/10.1080/\n00437956.1954.11659520.\nHase, Peter, Mohit Bansal, Been Kim, and Asma Ghandeharioun. 2023.“Does Localization Inform Editing?\nSurprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models.”\nAdvances in Neural Information Processing Systems36:17643– 68.\n20 Céline Budding\nHase, Peter, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2021. “Do Language Models Have Beliefs? Methods for Detecting, Updating, and\nVisualizing Model Beliefs.” arXiv preprint. https://doi.org/10.48550/arXiv.2111.13654.\nHinton, Geoffrey E., James L. McClelland, and David E. Rumelhart. 1986.“Distributed Representations.” In\nParallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1,Foundations, edited by\nDavid E. Rumelhart and James L. McClelland, 77– 109. Cambridge, MA: MIT Press.\nKripke, Saul A. 1982.Wittgenstein on Rules and Private Language: An Elementary Exposition. Cambridge, MA:\nHarvard University Press.\nLam, Nardi. 2022. “Explanations in AI as Claims of Tacit Knowledge.” Minds and Machines 32:135– 58.\nhttps://doi.org/10.1007/s11023-021-09588-1.\nLapuschkin, Sebastian, Stephan Wäldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek, and\nKlaus-Robert Müller. 2019.“Unmasking Clever Hans Predictors and Assessing What Machines Really\nLearn.” Nature Communications 10 (1):1096. https://doi.org/10.1038/s41467-019-08987-4.\nLenat, Douglas B., and R. V. Guha. 1989.Building Large Knowledge-Based Systems; Representation and Inference\nin the Cyc Project. 1st ed. Boston, MA: Addison-Wesley.\nLi, Kenneth, Aspen K. Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.\n2023. “Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task.”\nIn Proceedings of the 11th International Conference on Learning Representations (ICLR 2023). ICLR.\nLinzen, Tal, and Marco Baroni. 2021.“Syntactic Structure from Deep Learning.” Annual Review of Linguistics\n7 (1):195– 212. https://doi.org/10.1146/annurev-linguistics-032020-051035.\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina\nFedorenko. 2024.“Dissociating Language and Thought in Large Language Models.” Trends in Cognitive\nSciences 28 (6):517– 40. https://doi.org/10.1016/j.tics.2024.01.011.\nMeng, Kevin, David Bau, Alex Andonian, and Yonatan Belinkov. 2022.“Locating and Editing Factual\nAssociations in GPT.” Advances in Neural Information Processing Systems35:17359– 72.\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. “Distributed\nRepresentations of Words and Phrases and Their Compositionality.” Advances in Neural Information\nProcessing Systems 26:3111– 19.\nMillikan, Ruth Garrett. 1984.Language, Thought, and Other Biological Categories: New Foundations for Realism.\nCambridge, MA: MIT Press.\nNadkarni, Prakash M, Lucila Ohno-Machado, and Wendy W Chapman. 2011. “Natural Language\nProcessing: An Introduction.” Journal of the American Medical Informatics Association 18 (5):544– 51.\nhttps://doi.org/10.1136/amiajnl-2011-000464.\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\n2020. “Zoom In: An Introduction to Circuits.” Distill 5 (3):e00024.001.https://doi.org/10.23915/distill.\n00024.001.\nOpenAI. 2022.“ChatGPT: Optimizing Language Models for Dialogue.” https://openai.com/blog/chatgpt/.\nPavlick, Ellie. 2023.“Symbols and Grounding in Large Language Models.” Philosophical Transactions of the\nRoyal Society A: Mathematical, Physical and Engineering Sciences381 (2251):20220041.https://doi.org/10.\n1098/rsta.2022.0041.\nPiantadosi, Steven T., and Felix Hill. 2022.“Meaning without Reference in Large Language Models.” arXiv\npreprint. https://doi.org/10.48550/arXiv.2208.02957.\nQuine, W.V. 1970.“Methodological Reflections on Current Linguistic Theory.” Synthese 21:13. https://doi.\norg/10.1007/978-94-010-2557-7_14.\nShanahan, Murray. 2023. “Talking About Large Language Models.” arXiv preprint. https://doi.org/10.\n48550/arXiv.2212.03551.\nSmolensky, Paul. 1988. “On the Proper Treatment of Connectionism.” Behavioral and Brain Sciences\n11 (1):1– 23. https://doi.org/10.1017/S0140525X00052432.\nSukhbaatar, Sainbayar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. 2019.\n“Augmenting Self-Attention with Persistent Memory.” arXiv preprint.https://doi.org/10.48550/arXiv.\n1907.01470.\nTempleton, Adly, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce,\net al. 2024. “Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.”\nTransformer Circuits Thread.https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html.\nPhilosophy of Science 21\nVan Gelder, Tim. 1992.“Defining ‘Distributed Representation.’” Connection Science4( 3– 4):175– 91. https://\ndoi.org/10.1080/09540099208946614.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser,\nand Illia Polosukhin. 2017.“Attention Is All You Need.” Advances in Neural Information Processing Systems\n30:6000– 10.\nWoodward, James. 2003.Making Things Happen: A Theory of Causal Explanation. New York: Oxford University\nPress.\nYildirim, Ilker, and L. A. Paul. 2024.“From Task Structures to World Models: What Do LLMs Know?” Trends\nin Cognitive Sciences 28 (5):404– 15. https://doi.org/10.1016/j.tics.2024.02.008.\nZednik, Carlos. 2018.“Will Machine Learning Yield Machine Intelligence?” In Philosophy and Theory of\nArtificial Intelligence 2017, edited by Vincent Müller, 225– 27. Studies in Applied Philosophy, Epistemology\nand Rational Ethics 44.New York: Springer.https://doi.org/10.1007/978-3-319-96448-5_23.\nZednik, Carlos. 2021.“Solving the Black Box Problem: A Normative Framework for Explainable Artificial\nIntelligence.” Philosophy & Technology 34 (2):265– 88. https://doi.org/10.1007/s13347-019-00382-7.\nCite this article:Budding, Céline. 2025.“What Do Large Language Models Know? Tacit Knowledge as a\nPotential Causal-Explanatory Structure.” Philosophy of Science. https://doi.org/10.1017/psa.2025.19\n22 Céline Budding",
  "topic": "Tacit knowledge",
  "concepts": [
    {
      "name": "Tacit knowledge",
      "score": 0.7186393141746521
    },
    {
      "name": "Epistemology",
      "score": 0.4733037054538727
    },
    {
      "name": "Explanatory model",
      "score": 0.4724336564540863
    },
    {
      "name": "Causal model",
      "score": 0.418667197227478
    },
    {
      "name": "Computer science",
      "score": 0.3902702331542969
    },
    {
      "name": "Philosophy",
      "score": 0.2347501516342163
    },
    {
      "name": "Mathematics",
      "score": 0.177219957113266
    },
    {
      "name": "Statistics",
      "score": 0.06622645258903503
    }
  ],
  "institutions": [],
  "cited_by": 2
}