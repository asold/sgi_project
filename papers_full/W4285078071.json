{
    "title": "Sparse Pairwise Re-ranking with Pre-trained Transformers",
    "url": "https://openalex.org/W4285078071",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5008775203",
            "name": "Lukas Gienapp",
            "affiliations": [
                "Leipzig University"
            ]
        },
        {
            "id": "https://openalex.org/A5068616222",
            "name": "Maik FrÃ¶be",
            "affiliations": [
                "Martin Luther University Halle-Wittenberg"
            ]
        },
        {
            "id": "https://openalex.org/A5014322854",
            "name": "Matthias Hagen",
            "affiliations": [
                "Martin Luther University Halle-Wittenberg"
            ]
        },
        {
            "id": "https://openalex.org/A5083712311",
            "name": "Martin Potthast",
            "affiliations": [
                "Leipzig University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3155713313",
        "https://openalex.org/W2013784666",
        "https://openalex.org/W1978810924",
        "https://openalex.org/W3023238803",
        "https://openalex.org/W2502655619",
        "https://openalex.org/W2012318340",
        "https://openalex.org/W3045033475",
        "https://openalex.org/W2069870183",
        "https://openalex.org/W2203835818",
        "https://openalex.org/W3136109765",
        "https://openalex.org/W3208821253",
        "https://openalex.org/W3045745713",
        "https://openalex.org/W3100107515",
        "https://openalex.org/W2117273438",
        "https://openalex.org/W2068098598",
        "https://openalex.org/W2091413566",
        "https://openalex.org/W3173588320",
        "https://openalex.org/W2075806862",
        "https://openalex.org/W3172072815",
        "https://openalex.org/W2995261664",
        "https://openalex.org/W3104738015",
        "https://openalex.org/W3213719910",
        "https://openalex.org/W1965296145",
        "https://openalex.org/W1581231885",
        "https://openalex.org/W3104914963",
        "https://openalex.org/W3197223198",
        "https://openalex.org/W1973435495",
        "https://openalex.org/W3105107530"
    ],
    "abstract": "Pairwise re-ranking models predict which of two documents is more relevant to\\na query and then aggregate a final ranking from such preferences. This is often\\nmore effective than pointwise re-ranking models that directly predict a\\nrelevance value for each document. However, the high inference overhead of\\npairwise models limits their practical application: usually, for a set of $k$\\ndocuments to be re-ranked, preferences for all $k^2-k$ comparison pairs\\nexcluding self-comparisons are aggregated. We investigate whether the\\nefficiency of pairwise re-ranking can be improved by sampling from all pairs.\\nIn an exploratory study, we evaluate three sampling methods and five preference\\naggregation methods. The best combination allows for an order of magnitude\\nfewer comparisons at an acceptable loss of retrieval effectiveness, while\\ncompetitive effectiveness is already achieved with about one third of the\\ncomparisons.\\n",
    "full_text": "Sparse Pairwise Re-ranking with Pre-trained Transformers\nLukas Gienapp\nLeipzig University\nMaik FrÃ¶be\nMartin-Luther-UniversitÃ¤t Halle-Wittenberg\nMatthias Hagen\nMartin-Luther-UniversitÃ¤t Halle-Wittenberg\nMartin Potthast\nLeipzig University\nABSTRACT\nPairwise re-ranking models predict which of two documents is more\nrelevant to a query and then aggregate a final ranking from such\npreferences. This is often more effective than pointwise re-ranking\nmodels that directly predict a relevance value for each document.\nHowever, the high inference overhead of pairwise models limits\ntheir practical application: usually, for a set of ğ‘˜ documents to be\nre-ranked, preferences for allğ‘˜2 âˆ’ğ‘˜comparison pairs excluding self-\ncomparisons are aggregated. We investigate whether the efficiency\nof pairwise re-ranking can be improved by sampling from all pairs.\nIn an exploratory study, we evaluate three sampling methods and\nfive preference aggregation methods. The best combination allows\nfor an order of magnitude fewer comparisons at an acceptable loss\nof retrieval effectiveness, while competitive effectiveness is already\nachieved with about one third of the comparisons.\nCCS CONCEPTS\nâ€¢ Information systems â†’Learning to rank ; Rank aggrega-\ntion; Retrieval effectiveness; Retrieval efficiency.\nKEYWORDS\nPairwise re-ranking; Sampling; Efficiency; Pre-trained transformers\nACM Reference Format:\nLukas Gienapp, Maik FrÃ¶be, Matthias Hagen, and Martin Potthast. 2022.\nSparse Pairwise Re-ranking with Pre-trained Transformers. In Proceedings\nof the 2022 ACM SIGIR International Conference on the Theory of Information\nRetrieval (ICTIR â€™22), July 11â€“12, 2022, Madrid, Spain. ACM, New York, NY,\nUSA, 9 pages. https://doi.org/10.1145/3539813.3545140\n1 INTRODUCTION\nPre-trained transformers have ushered in a new era in information\nretrieval: with a sufficient amount of training data, transformer-\nbased re-ranking models can significantly outperform traditional\nretrieval models [24]. Two classes of re-rankers are implemented\nusing pre-trained transformers [25]: (1) pointwise re-rankers that\npredict the relevance of a documentğ‘‘to a query ğ‘, and (2) pairwise\nre-rankers that predict which of two documents (ğ‘‘ğ‘–,ğ‘‘ğ‘—)is more rel-\nevant to ğ‘. To further maximize re-ranking effectiveness, the mono-\nduo design pattern [34] shown in Figure 1 applies both sequentially.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICTIR â€™22, July 11â€“12, 2022, Madrid, Spain\nÂ© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9412-3/22/07. . . $15.00\nhttps://doi.org/10.1145/3539813.3545140\nRanking\nby s1, ..., sk\n...\nd3 s3\nd2 s2\nd5 s5\ndk sk\nPairwise re-ranking\nSampling\nPairwise inference\nAggregation\np1,2\nd2\nd1\nq\np1,3\nd3\nd1\nq\npi,j\ndj\ndi\nq\ns1\nd1\ns2\nd2\ns3\nd3\nsk\ndk\nd2\nd1\ndj\ndi\nd1\nd2\nd3\nd1\nd1\nd3 ...\n...\n...\nPoinwise re-ranking d1, ..., dk'\nQuery qRanking of D with respect to q\nFigure 1: The mono-duo design pattern for re-ranking. Parts\ninvestigated in this paper are highlighted in orange. Com-\nparisons omitted by sampling are striped.\nGiven a query ğ‘, a document set ğ·, and a ranking of ğ·produced by\na traditional retrieval model like BM25, the top-ğ‘˜â€²documents ğ·ğ‘˜â€² =\n{ğ‘‘1,...,ğ‘‘ ğ‘˜â€²}âŠ‚ ğ· are re-ranked according to their pointwise rel-\nevance to ğ‘. Then, the top- ğ‘˜ documents ğ·ğ‘˜ âŠ‚ğ·ğ‘˜â€², ğ‘˜ â‰ªğ‘˜â€², are\nre-ranked based on pairwise comparisons in three steps. First, pairs\nof documents (ğ‘‘ğ‘–,ğ‘‘ğ‘—)are sampled, whereğ‘–,ğ‘— âˆˆ[1,ğ‘˜]and ğ‘– â‰  ğ‘—. Sec-\nond, each pair (ğ‘‘ğ‘–,ğ‘‘ğ‘—)is passed to a transformer model to predict a\nprobability ğ‘ğ‘–ğ‘— indicating whether the document ğ‘‘ğ‘– (ğ‘ğ‘–ğ‘— â‰¥0.5) or\nthe document ğ‘‘ğ‘— (ğ‘ğ‘–ğ‘— < 0.5) is more relevant to ğ‘. Third, for each\ndocument ğ‘‘ğ‘–, a relevance score ğ‘ ğ‘– is aggregated from all probabili-\nties of comparisons including ğ‘‘ğ‘–, and these scores are then used to\nderive the final pairwise re-ranking.\nEmpirical evidence suggests that pairwise re-rankers are more\neffective than pointwise re-rankers since their relevance scores\ntake the relative relevance differences between documents into ac-\ncount, rather than making independent relevance predictions [34].\nTo maximize the potential effectiveness gains, previous work has\nrelied on exhaustive comparisons of all ğ‘˜2 âˆ’ğ‘˜ pairs of the top-ğ‘˜\ndocuments ğ·ğ‘˜ to be re-ranked. Given the high run time overhead\nof transformer inferences, this quadratic step led to the recommen-\ndation that the re-ranking depth should be limited to ğ‘˜ â‰¤50.\nHowever, many of the estimated comparison probabilities may\nbe redundant in that they can be predicted from those of other\ncomparisons. A theoretical lower bound on the run time complexity\nis ğ‘‚(ğ‘˜log ğ‘˜)using a suitable sorting algorithm if the estimated\narXiv:2207.04470v1  [cs.IR]  10 Jul 2022\nICTIR â€™22, July 11â€“12, 2022, Madrid, Spain Lukas Gienapp, Maik FrÃ¶be, Matthias Hagen, and Martin Potthast\ncomparisons were â€œconsistentâ€ (i.e., ğ‘ğ‘–ğ‘— = 1 âˆ’ğ‘ğ‘—ğ‘–) and transitive.\nWe investigate for the first time if the efficiency of pairwise re-\nrankers can be increased without a significant loss of effectiveness\nby sampling from and thus sparsifying the comparison set.\nThe two components of the mono-duo re-ranking pipeline that\nwe study in this paper are highlighted in Figure 1: We introduce\na sampling step before the pairwise inference to draw a subset of\nthe ğ‘˜2 âˆ’ğ‘˜ possible comparisons, and we revisit the aggregation\nstep since its effectiveness directly depends on the kind of sample it\nreceives (Section 3). To investigate the effect of sparsification on the\nretrieval effectiveness, we study three sampling methods (global\nrandom, exhaustive window, skip-window) and five aggregation\nmethods (sorting, summation, regression, greedy, and graph-based)\non three datasets (ClueWeb09, ClueWeb12, MS MARCO) using the\npointwise monoT5 and the pairwise duoT5 models [34]. Our results\nshow that skip-window sampling with greedy aggregation allows\nfor an order of magnitude fewer comparisons at an acceptable\nloss of effectiveness, while competitive effectiveness to the all-\npairs approach can already be achieved with only one third of\nthe comparisons (Section 5). All code and data underlying our\nexperiments are publicly available.1\n2 RELATED WORK\nWe briefly give some background on the history of learning-to-rank\nretrieval models before detailing the nature of pairwise learning-\nto-rank models and reviewing rank aggregation approaches, which\nwe employ to aggregate pairwise preferences into a final ranking.\nFinally, we describe some related efforts at making transformer-\nbased learning to rank more efficient.\nLearning to Rank. Since decades, machine learning has been ap-\nplied to improve retrieval effectiveness [17, 25]. Traditional feature-\nbased learning-to-rank models evolved from pointwise over pair-\nwise to listwise approaches [26]. While feature-based models are\nstill successful [35], the recent promising retrieval effectiveness\nresults of pre-trained transformer models [25] has shifted the com-\nmunityâ€™s focus away from feature-based learning to rank. But his-\ntory appears to repeat itself as the aforementioned evolution from\npointwise approaches like monoBERT [ 32] and monoT5 [ 31] to\npairwise approaches [25] can been observed as well.\nPairwise Learning to Rank. Pairwise learning-to-rank approaches\npredict which document in a pair is probably more relevant to a\nquery and should be ranked higher [26]. In feature-based as well as\ntransformer-based learning to rank, pairwise approaches usually\noutperform pointwise ones that score documents independently of\neach other [26]. Yet, when the inference step of a pairwise model\ncompares all pairs of documents, the run time requirement is qua-\ndratic in the number of documents to be ranked. In an effort to re-\nduce the comparison count, extensive studies of theoretical proper-\nties of feature-based pairwise approaches [9, 23] have led to sugges-\ntions for better run time characteristics. For example, SortNet [36]\nuses a learned preference function that is guaranteed to output\nsymmetric preferences, allowing to skip half of the comparisons.\nYet, recent pairwise transformer-based models like duoBERT [34]\nand the more effective duoT5 [34] lack the desirable symmetry prop-\nerty of models like SortNet. Additionally, the theoretical analysis of\n1https://github.com/webis-de/ICTIR-22\nduoBERT and duoT5 is still in its infancy; previous work even found\nsuch models difficult to be interpreted [27, 41]. The effectiveness\nof duoBERT or duoT5 relies on computing preferences for all pairs\nof documents, at the expense of their efficiency [45] limiting their\napplicability in search scenarios with run time constraints.\nRank Aggregation. Rank aggregation [ 26] uses pairwise rele-\nvance preference probabilities to derive a rankingâ€”often by com-\nputing a score for each individual document. Finding an optimal ag-\ngregation with arbitrarily-sized inputs is an NP-hard problem [10],\nbut many approaches are known to work well in practice. While\ndynamic aggregation methods decide which documents to compare\nnext based on all previous comparisons, static aggregation meth-\nods assume that the required pairwise comparisons are conducted\nbefore the actual aggregation starts [42].\nIn our study, we employ the following five aggregation methods\n(details in Section 3.2): (1) Sorting via the KwikSort method [ 2],\nwhich assumes that the comparisons are consistent and form a total\norder, (2) additive aggregation [34], where the rank of a document\nis indicated by the sum of the documentâ€™s comparison probabili-\nties (potentially transforming the probabilities before summation),\n(3) regression-based aggregation [4, 38, 40, 46], where latent scores\nfor documents are learned so that they optimally correspond to a\ngiven set of pairwise comparisons, (4) greedy aggregation [3, 10], in\nwhich a heuristic iteratively selects and removes the best document\nfrom a given set and then proceeds with the rest, and (5) graph-\nbased aggregation [43], where comparisons are interpreted as di-\nrected edges between document nodes and a measure of graph\ncentrality is used to derive a ranking score.\nEfficiency Improvements for Transformer-based Re-Rankers. The\nhigh computational cost of re-ranking documents with pre-trained\ntransformers has recently received attention [20]. Even for point-\nwise approaches, the inference overhead can be prohibitive for\npractical applications [45]. There are two ideas to improve the ef-\nficiency of neural re-rankers: (1) improving the efficiency of the\nranking model, and (2) reducing the required number of inferences.\nApproaches to the former include early-exiting from inference by\nintermediate between-layer classification in BERT-like models [44],\nmodel distillation [18, 19], or improved dense representations [39].\nFor the latter, Zhang et al. [45] propose to introduce filtering steps\nin multi-stage re-ranking pipelines. They utilize feature-based learn-\ning to rank to compute a set of candidate documents that is then\nre-ranked using a BERT-like neural model, increasing efficiency by\na factor of up to 18 compared to an unfiltered baseline at the same\neffectiveness. But while document filtering has been studied for\npointwise re-ranking, to our knowledge, filtering approaches for\npairwise re-ranking have not been addressed to date.\n3 SPARSIFIED PAIRWISE RE-RANKING\nIn this section, we describe the steps we adapted in the mono-\nduo re-ranking pipeline (Figure 1): sampling methods to select\nthe to-be-compared document pairs (three methods, Section 3.1),\nand aggregation methods that derive a ranking from the ranking\npreferences (five methods, Section 3.2). For completeness, we also\nbriefly detail the steps adopted from the literature: initial retrieval,\nas well as pointwise and pairwise re-ranking (Section 3.3).\nSparse Pairwise Re-ranking with Pre-trained Transformers ICTIR â€™22, July 11â€“12, 2022, Madrid, Spain\n3.1 Sampling\nBased on the top-ğ‘˜ results ğ·ğ‘˜ of the pointwise re-ranking step of\nthe mono-duo paradigm, we propose to sparsify the set ğ¶all of all\nğ‘˜2 âˆ’ğ‘˜ comparisons (no self-comparisons) and to use a sampled\ncomparison set ğ¶ âŠ‚ğ¶all as input for the pairwise re-ranking step.\nThe goal is to minimize the size ofğ¶and thus the effort of pairwise\nre-ranking without compromising the quality of the final ranking.\nWe distinguish random from structured sampling, with the main\ndifference being their (non-)determinism. Independent random sam-\nples from a given ğ¶all very likely contain different comparisons,\nbut structured samples always choose the same comparisons. To\nbe compatible with a variety of aggregation methods, a sampling\nmust meet two requirements: (1) each document is part of at least\none comparison, (2) each comparison is sampled at most once. Fig-\nure 2 illustrates the three sampling methods introduced below for a\ndocument set ğ·ğ‘˜ of size ğ‘˜ = 20 at two sampling rates, one per line.\nGlobal random sampling (G-Random). For each of the top-ğ‘˜ doc-\numents ğ‘‘ âˆˆğ·ğ‘˜ from the pointwise ranking, a fraction ğ‘Ÿ âˆˆ(0...1]\nof the remaining ğ‘˜ âˆ’1 documents is randomly selected for the\ncomparison set ğ¶ that then has the size |ğ¶|= âŒŠğ‘Ÿ Â·(ğ‘˜2 âˆ’ğ‘˜)âŒ‹.\nNeighborhood window sampling (N-Window). A sliding window\nof size ğ‘š â‰¤ğ‘˜ âˆ’1 is moved over the top- ğ‘˜ documents ğ·ğ‘˜ from\nthe pointwise ranking. For document ğ‘‘ğ‘– âˆˆğ·ğ‘˜, its ğ‘šdirect succes-\nsors in the ranking are sampled for comparison. But since the last\nğ‘šdocuments in the ranking of ğ·ğ‘˜ have less than ğ‘šsuccessors, we\nlet the window â€œwrap aroundâ€ to the top-ranked documents. For\ndocument ğ‘‘ğ‘–, the ğ‘šsampled comparisons (ğ‘‘ğ‘–,ğ‘‘ğ‘—)âˆˆ ğ¶ thus fulfill\nğ‘— = 1 +(ğ‘mod ğ‘˜)for ğ‘ âˆˆ{ğ‘–,...,ğ‘– +ğ‘šâˆ’1}. The size of the sampled\ncomparison set ğ¶ is ğ‘˜Â·ğ‘šand each document is the first entry of\nğ‘šcomparisons and the second entry of another ğ‘šcomparisons.\nAn assumption underlying the neighborhood sampling is that\nthe â€œglobalâ€ pointwise ranking is sensible but that â€œlocalâ€ re-ranking\nleads to an improved effectiveness. If true, however, it would be\nplausible to stop the window for ğ‘– > ğ‘˜ âˆ’ğ‘š rather than to wrap\nit around. However, pilot experiments have shown that this leads\nto poorer effectiveness, possibly because fewer comparisons are\nsampled at both the beginning and the end of a top-ğ‘˜ ranking.\nSkip-window sampling (S-Window). N-Window samples from\nthe â€œlocalâ€ neighborhood in a ranking. To enable more â€œglobalâ€\ncomparisons, we introduce a skip sizeğœ† âˆˆN+. For ğ‘‘ğ‘– âˆˆğ·ğ‘˜, compar-\nisons (ğ‘‘ğ‘–,ğ‘‘ğ‘—)to ğ‘šsuccessors are sampled so that ğ‘— = 1 +(ğ‘mod ğ‘˜)\nfor ğ‘ âˆˆ{ğ‘–+ğœ†âˆ’1,ğ‘–+2ğœ†âˆ’1,...,ğ‘– +ğ‘šğœ†âˆ’1}; when ğ‘— = ğ‘–for someğ‘, that\ncomparison is not included in the sample. Forğœ†= 1, S-Window cor-\nresponds to N-Window, and for ğœ†= 3, for example, each document\nis compared to every third of its successors. The ğœ†-skip determinis-\ntically controls the â€œglobalityâ€ of a sample without increasing the\namount |ğ¶|of sampled comparisons compared to N-Window.\n3.2 Aggregation\nFor each comparison (ğ‘‘ğ‘–,ğ‘‘ğ‘—)âˆˆ ğ¶, a pairwise model computes a\npreference probability ğ‘ğ‘–ğ‘—, which indicates how likely ğ‘‘ğ‘– should\nbe ranked above ( ğ‘ğ‘–ğ‘— â‰¥ 0.5) or below ğ‘‘ğ‘— (ğ‘ğ‘–ğ‘— < 0.5). From the\nprobabilities computed for ğ¶, an aggregation method derives a rele-\nvance valueğ‘ ğ‘– for each documentğ‘‘ğ‘–. We study five paradigmatically\ndifferent aggregation methods.\ni\nG-Random (f=0.2) N-Window (m=4) S-Window (m=4, Î»=4)\nj\ni\nG-Random (f=0.5)\nj\nN-Window (m=10)\nj\nS-Window (m=10, Î»=2)\nFigure 2: Example comparison sets of different sampling\nprocedures for 20 documents at different sampling rates. If\ncomparison (ğ‘‘ğ‘–,ğ‘‘ğ‘—)is sampled, cell ğ‘–,ğ‘— is colored blue; the\ngrey cell for S-Window illustrates the omitted case ğ‘— = ğ‘–.\nKwikSort. As a baseline, we use the KwikSort method [2]. It is\nan extension of the Quicksort algorithm for data with preferences\nand, in our case of top-ğ‘˜ ranking, has an expected number of com-\nparisons in ğ‘‚(ğ‘˜log ğ‘˜). First, a random document ğ‘‘ğ‘– is chosen to\nbe the pivot. Then, all other documents are compared to the pivot\nplacing the ones to be lower-ranked than ğ‘‘ğ‘– and the ones to be\nhigher-ranked in separate subsets. These subsets are recursively\nranked until a final ranking is obtained. KwikSort does not rely on a\npreceding sampling step; that the expected number of comparisons\nis in ğ‘‚(ğ‘˜log ğ‘˜)is a feature of the dynamic aggregation itself.\nAdditive Aggregation. Pradeep et al. [34] propose four different\naggregation techniques based on preference probability summation.\nThey find the symmetric sum of preference probabilities to yield\nthe best effectiveness:\nğ‘ ğ‘– =\nâˆ‘ï¸\nğ‘—âˆˆ1...ğ‘˜\n(ğ‘ğ‘–ğ‘— +(1 âˆ’ğ‘ğ‘—ğ‘–)).\nHowever, in our samples, not all comparisons are present so that\nwe replace missing summands ğ‘ğ‘–ğ‘— or (1 âˆ’ğ‘ğ‘—ğ‘–)by 0.\nBradley-Terry Aggregation. The Bradley-Terry model [4] infers a\nlatent score ğ‘ ğ‘– âˆˆğ‘† for each document ğ‘‘ğ‘– âˆˆğ·ğ‘˜ based on the prefer-\nences expressed in the sampled comparison set ğ¶ using maximum-\nlikelihood estimation. In its original form, exponential score func-\ntions were used, which corresponds to a logistic regression on\npairwise data [1] and can be expressed as:\nL(ğ‘†,ğ¶)=\nâˆ‘ï¸\nğ‘‘ğ‘– â‰»ğ‘‘ğ‘—\nlog ğ‘’ğ‘ ğ‘–\nğ‘’ğ‘ ğ‘– +ğ‘’ğ‘ ğ‘— +\nâˆ‘ï¸\nğ‘‘ğ‘– â‰ºğ‘‘ğ‘—\nlog ğ‘’ğ‘ ğ‘—\nğ‘’ğ‘ ğ‘– +ğ‘’ğ‘ ğ‘— .\nHere, ğ‘‘ğ‘– â‰»ğ‘‘ğ‘— denotes all comparisons (ğ‘‘ğ‘–,ğ‘‘ğ‘—)âˆˆ ğ¶ with ğ‘ğ‘–ğ‘— â‰¥0.5\nand ğ‘‘ğ‘– â‰ºğ‘‘ğ‘— denotes all comparisons (ğ‘‘ğ‘–,ğ‘‘ğ‘—)âˆˆ ğ¶ with ğ‘ğ‘–ğ‘— < 0.5.\nThe unknown latent score setğ‘†is usually found via BFGS optimiza-\ntion [16] so that a ranking according to the ğ‘ ğ‘– violates as few of the\npreferences from the comparison sample ğ¶ as possible.\nICTIR â€™22, July 11â€“12, 2022, Madrid, Spain Lukas Gienapp, Maik FrÃ¶be, Matthias Hagen, and Martin Potthast\nInput: Document set ğ·ğ‘˜, preference probabilities ğ‘ğ‘–ğ‘—\nOutput: Score ğ‘  for each ğ‘‘ âˆˆğ·ğ‘˜\nforeach ğ‘‘ğ‘– âˆˆğ·ğ‘˜ do ğ‘¡ğ‘– â†Ã\nğ‘‘ğ‘— âˆˆğ·ğ‘˜ ğ‘ğ‘–ğ‘— âˆ’Ã\nğ‘‘ğ‘— âˆˆğ·ğ‘˜ ğ‘ğ‘—ğ‘–;\nwhile ğ·ğ‘˜ â‰  âˆ…do\nğ‘‘ğ‘— â†arg maxğ‘‘ğ‘– âˆˆğ·ğ‘˜ ğ‘¡ğ‘–;\nğ‘ ğ‘— â†|ğ·ğ‘˜|;\nğ·ğ‘˜ â†ğ·ğ‘˜ \\{ğ‘‘ğ‘—};\nforeach ğ‘‘ğ‘– âˆˆğ·ğ‘˜ do ğ‘¡ğ‘– â†ğ‘¡ğ‘– âˆ’ğ‘ğ‘–ğ‘— +ğ‘ğ‘—ğ‘–;\nend\nAlgorithm 1: Greedy aggregation of preferences [10].\nGreedy Aggregation. Cohen et al. [10] propose a greedy ordering\nalgorithm that is proven to closely approximate the best total order\nin terms of the number of violated preferences. Algorithm 1 shows\nits pseudocode. In every iteration, the documentğ‘‘ğ‘— with the highest\nâ€œpotentialâ€ ğ‘¡ğ‘—2 is appended to the re-ranking on the highest still\nunoccupied rank by setting score ğ‘ ğ‘— accordingly. The potentials of\nthe remaining documents are updated by canceling out the respec-\ntive terms that include ğ‘‘ğ‘—. With sampling, the comparison set is\nincomplete; missing probabilities ğ‘ğ‘–ğ‘— are set to zero.\nPageRank Aggregation. A comparison set ğ¶ induces a directed\ngraph withğ·ğ‘˜ as nodes and comparisons as directed edges weighted\nwith preference probabilities. We introduce a new aggregation\nmethod that computes the graph centrality measure PageRank [33],\nextended for weighted graphs [ 29], to rank the documents. The\nfundamental principle of PageRank is that nodes with incoming\nedges from nodes with high PageRank scores should also receive\nhigh PageRank scores. The respective PageRank-style aggregation\nof a score ğ‘ ğ‘– for a document ğ‘‘ğ‘– then is\nğ‘ ğ‘– = ğ›¾ Â· 1\n|ğ·ğ‘˜|+(1 âˆ’ğ›¾)Â·\nâˆ‘ï¸\n(ğ‘‘ğ‘— ,ğ‘‘ğ‘– )âˆˆğ¶\nğ‘ğ‘—ğ‘–Ã\nğ‘™âˆˆ[1,ğ‘˜]ğ‘ğ‘—ğ‘™\nÂ·ğ‘ ğ‘—,\nwhere using the components with the damping factorğ›¼ âˆˆ[0,1]en-\nsure convergence when computing the PageRank scores iteratively.\n3.3 Initial Retrieval and Re-ranking\nFollowing the experimental setup of Pradeep et al . [34] closely\n(see Figure 1), for each query, we first obtain an initial ranking\nusing BM25 (PyTerrier implementation [28], default configuration).\nThe top-1000 BM25 results are then re-ranked using monoT5 [31]\nin the pointwise re-ranking step. For the top-50 monoT5 results,\nduoT5 [34] infers preference probabilities in the second step of pair-\nwise re-ranking, after sampling. For both, monoT5 and duoT5, we\napply the largest available pre-trained version.3 We use T5 instead\nof BERT variants, as T5 has been shown to be more effective [25].\nTo avoid repeated inferences in our experiments, allğ‘˜2 âˆ’ğ‘˜pairwise\npreference probabilities are cached once for each query.\nThe maximum input length of transformer models is limited, so\nthat a representative passage has to be chosen from each document\nfor inference. Following the method of Dai and Callan [15], we\nsplit each document into fixed-length non-overlapping passages of\n2The â€œpotentialâ€ basically tallies ğ‘‘ğ‘– â€™s â€œwinsâ€ against other documents compared to its\nâ€œlossesâ€ in terms of preference probabilities.\n3monoT5: https://huggingface.co/castorini/monot5-3b-msmarco\nduoT5: https://huggingface.co/castorini/duot5-3b-msmarco\nabout 250 words (using the TREC CAsT Y4 tools;4 splits at sentence\nboundaries). Fixed-length passages have been shown to be more ef-\nfective than variable-length passages [22]. In our pilot experiments,\nusing the first passage was the most effective heuristic, so that we\nuse them for preference probability inference.\n4 EXPERIMENTAL SETUP\nIn this section, we introduce our evaluation measures, detailing in\nparticular measures for consistency, complementarity, and transitiv-\nity of the aggregated relevance scores, and recap the used datasets.\n4.1 Evaluation Measures\nWe follow similar studies of the mono/duoT5 models [34] and use\nnDCG@10 [21] to evaluate the retrieval effectiveness. When re-\nranking the top-ğ‘˜ results of a pointwise model, the ğ‘˜2 âˆ’ğ‘˜ com-\nparisons ğ¶all usually performed by a pairwise model may result in\ninconsistent preference probabilities (1) at the level of a document\npair and (2) at the level of document triples. We further examine\nthese potential inconsistencies, as they can â€œcomplicateâ€ the aggre-\ngation step and affect the retrieval effectiveness. At the document\npair level, one would expect ğ‘ğ‘–ğ‘— â‰ˆ1 âˆ’ğ‘ğ‘—ğ‘– but pairwise models do\nnot guarantees this and may predict both ğ‘‘ğ‘– â‰»ğ‘‘ğ‘— for the input pair\n(ğ‘‘ğ‘–,ğ‘‘ğ‘—)and ğ‘‘ğ‘— â‰»ğ‘‘ğ‘– for the input pair (ğ‘‘ğ‘—,ğ‘‘ğ‘–), or vice versa, where\nğ‘‘ğ‘– â‰»ğ‘‘ğ‘— denotes a ranking preference of the left document ğ‘‘ğ‘– over\nthe right one ğ‘‘ğ‘—. At the document triple level, transitivity may be\nviolated as a model may predict ğ‘‘ğ‘– â‰»ğ‘‘ğ‘— and ğ‘‘ğ‘— â‰»ğ‘‘ğ‘™ but ğ‘‘ğ‘™ â‰»ğ‘‘ğ‘–.\nThe consistency of an all-(ğ‘˜2 âˆ’ğ‘˜)-pairs comparison set ğ¶all at\nthe level of document pairs is the fraction of pairs (ğ‘‘ğ‘–,ğ‘‘ğ‘—)âˆˆ ğ¶all\nfor which ğ‘ğ‘–ğ‘— â‰¥0.5 and ğ‘ğ‘—ğ‘– < 0.5:\nconsistency(ğ¶all ) = |{(ğ‘‘ğ‘–,ğ‘‘ğ‘—)âˆˆ ğ¶all : ğ‘ğ‘–ğ‘— â‰¥0.5 and ğ‘ğ‘—ğ‘– < 0.5}|\n|ğ¶all | .\nWhile consistency captures the comparison direction, also the nu-\nmerical complementarity of how close ğ‘ğ‘–ğ‘— +ğ‘ğ‘—ğ‘– is to the â€œidealâ€ 1\ncan be interesting. Thus, we also measure the ğœ€-complementarity\nwith respect to a margin of error ğœ€as:\nğœ€-complementarity(ğ¶all )= |{(ğ‘‘ğ‘–,ğ‘‘ğ‘—)âˆˆ ğ¶all : |ğ‘ğ‘–ğ‘— +ğ‘ğ‘—ğ‘– âˆ’1|< ğœ€}|\n|ğ¶all | .\nFinally, the transitivity ofğ¶all measures the fraction of document\ntriples for which the pairwise comparisons are transitive:\ntransitivity(ğ¶all ) = |ğ‘‡|\n|ğ‘‡|+| ğ¼|, where\nğ‘‡ = {(ğ‘‘ğ‘–,ğ‘‘ğ‘—,ğ‘‘ğ‘™): ğ‘ğ‘–ğ‘— â‰¥0.5, ğ‘ğ‘—ğ‘™ â‰¥0.5, and ğ‘ğ‘–ğ‘™ â‰¥0.5}âˆª\n{(ğ‘‘ğ‘–,ğ‘‘ğ‘—,ğ‘‘ğ‘™): ğ‘ğ‘–ğ‘— < 0.5, ğ‘ğ‘—ğ‘™ < 0.5, and ğ‘ğ‘–ğ‘™ < 0.5} and\nğ¼ = {(ğ‘‘ğ‘–,ğ‘‘ğ‘—,ğ‘‘ğ‘™): ğ‘ğ‘–ğ‘— â‰¥0.5, ğ‘ğ‘—ğ‘™ â‰¥0.5, but ğ‘ğ‘–ğ‘™ < 0.5}âˆª\n{(ğ‘‘ğ‘–,ğ‘‘ğ‘—,ğ‘‘ğ‘™): ğ‘ğ‘–ğ‘— < 0.5, ğ‘ğ‘—ğ‘™ < 0.5, but ğ‘ğ‘–ğ‘™ â‰¥0.5}.\nThe more the ğœ€-complementarity for some small ğœ€and the more\nthe transitivity of some ğ¶all approach 1, the more a total order\nbetween the documents is implied that probably can also be derived\nfrom some smaller comparison sample ğ¶ âŠ‚ğ¶all .\n4https://github.com/grill-lab/trec-cast-tools\nSparse Pairwise Re-ranking with Pre-trained Transformers ICTIR â€™22, July 11â€“12, 2022, Madrid, Spain\n(a)\nCount\n104\n103\n0 10.2 0.4 0.6 0.8\nPreference probability (b) Consistency\nCorpus Mean Std. Min Max\nClueWeb09 0.312 0.136 0.054 0.637\nClueWeb12 0.383 0.115 0.120 0.643\nMS MARCO 0.498 0.126 0.136 0.731\n(c) Transitivity\nCorpus Mean Std. Min Max\nClueWeb09 0.783 0.079 0.616 0.954\nClueWeb12 0.742 0.064 0.611 0.898\nMS MARCO 0.693 0.073 0.580 0.901\n(d)\nCumulatie Îµ-complementarity\nÎµ\n0\n1\n0.2\n0.4\n0.6\n0.8\n0 0.1 0.2 0.3 0.4\nFigure 3: (a) Distribution of the pairwise duoT5 preference probabilities for the comparison set ğ¶all of the top-50 point-\nwise results per corpus (log-scaled y-axis). (b) Preference probability consistency, (c) transitivity, and (d) cumulative ğœ€-\ncomplementarity over ğœ€. In the plots, the corpora are color-coded as ClueWeb09 , ClueWeb12 , and MS MARCO (Passage)\n.\n4.2 Evaluation Data\nWe employ three standard retrieval corpora in our experiments: the\nClueWeb09, the ClueWeb12, and the MS MARCO passage corpus.\nClueWeb09. The ClueWeb09 corpus5 consists of 1 billion docu-\nments crawled between January and February 2009. It was used for\nthe ad-hoc search tasks of the TREC Web tracks 2009â€“2012 [5â€“8],\nwhere 70,575 graded relevance judgments were collected on a\n4-point scale for 200 topics (avg. 356 judgments per topic).\nClueWeb12. The ClueWeb12 corpus6 consists of 733 million doc-\numents crawled between April and May 2012. It was used for the\nad-hoc search tasks of the TREC Web tracks 2013/14[11, 12], where\n28,116 graded relevance judgments were collected on a4-point scale\nfor 100 topics (avg. 281 judgments per topic).\nMS MARCO (Passage). The MS MARCO passage corpus [ 30]\nconsists of 8.8 million passages extracted from Bing search en-\ngine results. It was used for the passage ranking task of the TREC\nDeep Learning tracks 2019/20 [13, 14], where 20,646 graded rele-\nvance judgments were collected on a 4-point scale for 97 topics\n(avg. 213 judgments per topic). With this corpus, we replicate the\nexperimental setup of Pradeep et al. [34].\nRemark. In our evaluation of re-ranking results, we only consider\njudged documents. Evaluation scores calculated excluding unjudged\ndocuments correlate well with evaluations including them [37].\n5 EVALUATION RESULTS\nWe conduct two experiments to evaluate the suitability of sampling\nand aggregation methods for efficient pairwise re-ranking. The first\nexperiment (Section 5.1) explores the properties of the comparison\nsets inferred for each of the three corpora. This supplies context to\nthe ranking effectiveness evaluation in the second experiment (Sec-\ntion 5.2), in which we analyze rankings for different combinations\nof samplers and aggregators.\n5http://lemurproject.org/clueweb09.php/\n6http://lemurproject.org/clueweb12.php/\n5.1 Evaluation of Pairwise Prediction\nProperties\nFor each topic from each corpus, we derive the duoT5 preference\nprobabilities for the set ğ¶all of all ğ‘˜2 âˆ’ğ‘˜ pairwise comparisons for\nthe top-50 results of the pointwise re-ranking and compute the\nstatistics and measures per corpus shown in Figure 3.\nThe preference probabilities are highly skewed towards the ex-\ntremes of the scale (cf. Figure 3a): for the majority of document\npairs, the preference probability is approximately zero or one. This\neffect is stronger for the MS MARCO passage corpus (on which the\nmodel was trained) than for the ClueWeb corpora. Interestingly,\nthe score distributions are not symmetric, but are slightly skewed\ntowards 1.0 for all three corpora. Since the comparison setğ¶all con-\ntains both comparison directions for every pair, the observed skew\ndirectly suggests to further inspect how consistent, transitive, and\ncomplementary the preferences are for document pairs or triples.\nIndeed, on average, only between half (MS MARCO) and a third\nof the comparisons (ClueWeb09) are consistent in their direction\n(cf. Figure 3b). Some variation across topics exists, yet the consis-\ntency is rather low in the majority of the topic-wise comparison\nsets. Further, also the cumulative ğœ–-complementarity (cf. Figure 3d)\nconfirms that the preferences of the pairwise duoT5 model are\nnot that complementary (i.e., ğ‘ğ‘–ğ‘— +ğ‘ğ‘—ğ‘– â‰  1) for a document pairâ€™s\ntwo possible input orders. Only for a rather large value of ğœ€ = 0.4\nall probabilities for pairs in all corpora are ğœ€-complementary (i.e.,\n|ğ‘ğ‘–ğ‘— +ğ‘ğ‘—ğ‘– âˆ’1| â‰  0.4), and more than half of the pairs require\nan ğœ–-value between 0.3 (MS MARCO) and 0.2 (ClueWeb09). For\nall corpora, almost no comparison pairs reach a complementarity\nof ğœ– < 0.1. Also the transitivity rates are consistently between 0.7\nand 0.8 across all corpora with very little variation per topic. These\nobservations (consistency and transitivity not that high) suggest\nthat the comparison-based KwikSort aggregation will not output the\nmost effective re-ranking and that very likely more thanğ‘‚(ğ‘˜log ğ‘˜)\ncomparison pairs are needed in a sample for the other aggrega-\ntion methods to â€œwork aroundâ€ the low consistency and lacking\ncomplementarity using more information.\nICTIR â€™22, July 11â€“12, 2022, Madrid, Spain Lukas Gienapp, Maik FrÃ¶be, Matthias Hagen, and Martin Potthast\nSample rate\n0 0.2 0.4 0.6 0.8 1\nSample rate\n0 0.2 0.4 0.6 0.8 1\nSample rate\n0 0.2 0.4 0.6 0.8 1\nSample rate\n0 0.2 0.4 0.6 0.8 1\n(a) (b) (c) (d)\n(e) (f) (g) (h)\n(i) (j) (k) (l)\n0.70\n0.65\n0.60\n0.55\n0.50\n0.45\n0.42\n0.40\n0.38\n0.36\n0.34\n0.50\n0.48\n0.46\n0.44\n0.42\n0.40 nDCG@10\nMS MARCOClueWeb09ClueWeb12\nnDCG@10nDCG@10\nAdditive Aggregation Bradley-Terry Aggregation Greedy Aggregation PageRank Aggregation\nFigure 4: Effectiveness measured as nDCG@10 on three corpora (ClueWeb09, ClueWeb12, MS MARCO) for each aggregator\nand the two samplers G-Random and N-Window with different sampling rates. Dotted: pointwise re-ranking, dashed:\nunsampled ğ¶all .\n5.2 Evaluation of Ranking Effectiveness\nTo evaluate the effectiveness of different combinations of sampling\nand aggregation methods on all three corpora, we simulate runs on\nsparsified comparison sets at sample rates ranging from 0.05 to 0.95\nin steps of 0.05. Each simulation is repeated ten times and the effec-\ntiveness is averaged to account for random variation in both the\nsampling and the aggregation step. In addition, baseline runs for\neach aggregator use the full comparison sets ğ¶all per topic without\nany sampling. In total, 4950 runs are simulated. The pointwise rank-\ning to be re-ranked by a pairwise model achieves nDCG@10 scores\nof 0.38 (ClueWeb09), 0.49 (ClueWeb12) and 0.50 (MS MARCO).\nEffectiveness of KwikSort. From the observations in Section 5.1\n(low consistency and low transitivity), it is clear that KwikSort\nwith its pivot-based dynamic sampling of a rather â€œfewâ€ ğ‘‚(ğ‘˜log ğ‘˜)\ncomparisons will not be able to achieve a really good effectiveness.\nIndeed, the nDCG@10 scores of KwikSort of 0.34 (ClueWeb09),\n0.39 (ClueWeb12), and 0.42 (MS MARCO) are even lower than the\npointwise effectiveness. We tested different pivot selection methods\nbut all resulted in a similarly bad effectiveness. Using KwikSort for\npairwise rank aggregation can thus not be recommended in settings\nwith inconsistent and intransitive comparisons.\nEffectiveness on the full comparison set ğ¶all . Figure 4 shows the\nnDCG@10 of the non-KwikSort aggregation methods on each cor-\npus. The dotted and dashed horizontal lines indicate the baseline\neffectiveness of the pointwise ranking and the pairwise re-ranking,\nrespectively, when using the respective aggregation method on the\ncomplete comparison set ğ¶all without sampling. Since the point-\nwise and the ğ¶all -aggregation effectiveness are thus independent\nof the sampling rate, they are depicted as horizontal lines.\nUsing the full comparison set ğ¶all without any sampling, greedy\naggregation yields the most effective re-rankings for the ClueWeb09\n(Subplot 4c) and the MS MARCO passage corpus (Subplot 4g) while\nadditive aggregation is the most effective on the ClueWeb12 (Sub-\nplot 4i). On MS MARCO (first row of Figure 4), all aggregation\nmethods are approximately equally effective when using the full\ncomparison set ğ¶all (dashed lines show about the same nDCG@10).\nSparse Pairwise Re-ranking with Pre-trained Transformers ICTIR â€™22, July 11â€“12, 2022, Madrid, Spain\n0.70\n0.65\n0.60\n0.55\n0.50\n0.45\nAdditive Aggregation Bradley-Terry Aggregation\n Greedy Aggregation PageRank Aggregation\nSample rate\n0 0.2 0.4 0.6 0.8 1\nSample rate\n0 0.2 0.4 0.6 0.8 1\nSample rate\n0 0.2 0.4 0.6 0.8 1\nSample rate\n0 0.2 0.4 0.6 0.8 1\nnDCG@10\nMS MARCO\nFigure 5: nDCG@10 on MS MARCO for each aggregator at different sampling rates for S-Window , G-Random , and\nN-Window . Dotted line: pointwise ranking, dashed line: effectiveness on unsampled ğ¶all .\nOn the ClueWeb corpora, though, PageRank aggregation is less\neffective than the pointwise ranking (Subplots 4h and l; dashed line\nbelow dotted line) and also Bradley-Terry aggregation struggles on\nthe ClueWeb12 (Subplot 4j). Only additive and greedy aggregation\nalways improve upon the pointwise ranking when using the full\ncomparison set ğ¶all (Subplots 4e, g, i, and k) but the improvement\nis smaller on the ClueWeb corpora. That the effectiveness and the\nimprovement over the pointwise ranking are the highest on the\nMS MARCO passage corpus is not surprising since the duoT5 re-\nranking model was trained on MS MARCO, and since the TREC\nWeb track relevance judgments used to evaluate the effectiveness\non the ClueWeb corpora are at the document level, while we only\nrank one passage per document due to input length limitations.\nEffectiveness with G-Random and N-Window sampling. The color-\ncoded curves in the plots of Figure 4 show the effectiveness of the\ndifferent aggregation methods with G-Random or N-Window sam-\npling at different sampling rates from the full comparison set ğ¶all .\nFour trends are apparent across all corpora.\nFirst, N-Window sampling results in less effective re-rankings\nthan G-Random sampling in nearly all cases, especially at smaller\nsampling rates. This effect is particularly noticeable for additive\naggregation (Subplots 4a, e, and i). One reason probably is that\nthe more â€œlocalâ€ comparisons of N-Window are likely to yield less\nextreme comparison probability differences that decrease the over-\nall separability of document pairs in sparse sampling setups. Also,\ninconsistencies in pairwise judgments are more likely for â€œlocalâ€\npairs. Overall, this indicates that the global context of documents\n(which is better represented by G-Random) is important to obtain\neffective re-rankings via aggregation.\nSecond, greedy aggregation is the most effective aggregation\nmethod for both, G-Random and N-Window sampling (comparing\nSubplots 4c, g, and k to the rest). It is also the only aggregation\nmethod for which the effectiveness on the full ğ¶all is reached by\nsome sparsified comparison sets.\nThird, the effectiveness degradation is not linear with respect\nto the sample rate (all subplots), but drops sharply below 15â€“20%.\nThis suggests a lower bound of comparisons needed to derive good\nrankings from the pairwise comparisons of duoT5 which lack in\nconsistency and transitivity.\nFourth, Bradley-Terry- and PageRank-aggregated re-rankings of-\nten are the least effective (Subplots 4b, f, and j, as well as d, h, and l).\nA possible reason for Bradley-Terry is similar to the bad effective-\nness of KwikSort-aggregated re-rankings: Bradley-Terry only takes\nthe direction of a comparison into account but not the magnitude\nof the respective probability. With the inconsistent probabilities\nof duoT5 that lead to inconsistent comparison directions, Bradley-\nTerry cannot derive good final re-rankingsâ€”just like KwikSort. In\ncase of PageRank aggregation, also the inconsistent probabilities\nthat are used as edge weights, might â€œconfuseâ€ the actual derivation\nof the PageRank scores.\nEffectiveness with S-Window sampling. To find a good value forğœ†\nin S-Window sampling, we run a grid search overğœ†= 2 ... 15, sepa-\nrately for all sample rates (0.05 to 0.95 in steps of 0.05). We use five-\nfold cross validation to determine the best choice on the MS MARCO\ncorpus, as the overall effectiveness gains on the ClueWeb corpora\nwere too small to meaningfully distinguish between setups. Fig-\nure 5 shows the nDCG@10 effectiveness on MS MARCO of the run\nwith the optimal ğœ†-value for each sample rate. The best runs for\nG-Random and N-Window are also shown for reference.\nRe-rankings aggregated from S-Window samples are more effec-\ntive by a margin for each of the aggregation methods at all sam-\npling rates. The combination of S-Window sampling with greedy\naggregation allows for a rather stable effectiveness down to sam-\npling only 30% of the comparisons. Even when using an order of\nmagnitude fewer comparisons (i.e., â‰ˆ10% of ğ¶all ), a competitive\neffectiveness is achieved (nDCG@10 only 0.04 less).\nThe best values forğœ†are between 7 and 10 in most cases; they are\nnot correlated with the window size (Pearsonâ€™s Â¯ğœŒ = 0.04). Already\nthe better effectiveness of aggregated rankings using G-Random\nsampling over N-Window sampling suggests that the global context\nis important when sampling comparisons. Also the rather large best-\nworking ğœ†-values for S-Window sampling corroborate this since\neven for small sample sizes ğ‘šthey ensure that the sampled com-\nparisons cover a pretty â€œglobalâ€ context. For large sample sizes ğ‘š,\nğœ† is not as important as the sample then already covers a larger\namount of the full comparison set ğ¶all .\nICTIR â€™22, July 11â€“12, 2022, Madrid, Spain Lukas Gienapp, Maik FrÃ¶be, Matthias Hagen, and Martin Potthast\nTable 1: Effectiveness on the MS MARCO corpus as\nnDCG@10 for the full comparison set ğ¶all and the\nlowest similarly effective sampling rate (non-significant\nnDCG@10 difference; delta in brackets) per sampling\nmethod and aggregator. Bonferroni-correction for all (incl.\nhidden) tests per row.\nAggregator nDCG@10 Lowest Similarly Effect. Sampl. Rate\nUnsampled ğ¶all S-Window G-Random N-Window\nAdditive 0.691 0.35 (-0.014) 0.85 (-0.019) 0.95 (-0.004)\nBradley-Terry 0.691 0.50 (-0.012) 1.00 (-0.000) 0.90 (-0.008)\nGreedy 0.707 0.30 (-0.013) 0.85 (-0.006) 0.50 (-0.010)\nPageRank 0.695 0.30 (-0.016) 0.65 (-0.012) 0.95 (-0.004)\nMinimal sampling rates. Table 1 shows the minimum attainable\nsampling rates on the MS MARCO corpus for which each combina-\ntion of sampling and aggregation method is not significantly less\neffective in terms of nDCG@10 than the respective aggregation\non the full comparison set ğ¶all . Per aggregator, the difference of\nthe runs for each of the 19 sampling rates is tested against the run\nthat aggregates a ranking from the full comparison set ğ¶all using\na paired Studentâ€™s t-test with an ğ›¼-level of 0.05 and Bonferroni\ncorrection for the multiple tests. For G-Random with potentially\ndifferent effectiveness scores for the 10 runs per sampling rate, we\nuse the least effective run per sampling rate in terms of nDCG@10\nto increase the overall confidence in case of observed differences.\nAmong the sampling strategies, S-Window achieves the by far\nlowest sampling rates per aggregator without hurting the retrieval\neffectiveness too much. About the same effectiveness is possible\nwith S-Window for additive, greedy, and PageRank aggregation\nwith just one third of the usually used unsampled comparisons.\nG-Random and N-Window need more comparisons with any ag-\ngregation method to achieve the same re-ranking effectiveness\nand, in fact, only lead to some substantial savings compared to the\nunsampled ğ¶all for PageRank aggregation (G-Random) or greedy\naggregation (N-Window).\nAmong the aggregation strategies, additive and Bradley-Terry\naggregation are the least effective and all sampling methods need\nlarger sample rates for Bradley-Terry than for the other aggregators.\nGreedy aggregation leads to the best effectiveness and G-Random\nand N-Window achieve their lowest sampling rates without effec-\ntiveness loss for greedy aggregation.\nOverall, the best combination in terms of effectiveness and sam-\npling rate is greedy aggregation with S-Window sampling: with\nabout one third of the usual ğ‘˜2 âˆ’ğ‘˜ comparisons, the best effective-\nness can be reached.\n6 CONCLUSION\nIn this paper, we analyze several methods to substantially reduce\nthe quadratic number of document comparisons usually conducted\nin pairwise re-ranking with transformers. To this end, we intro-\nduce a sampling step at the beginning of the pairwise re-ranking\nand adapt the aggregation step to derive relevance scores for a\nre-ranking from smaller samples of the pairwise comparisons. By\ncomparing combinations of three sampling methods and five aggre-\ngation methods, we show that only one third of the comparisons\nare needed to achieve competitive re-ranking effectiveness and that\nalso an order of magnitude less comparisons still can yield only a\nvery slightly decreased effectiveness.\nCompared to the usually applied additive rank aggregation with-\nout sampling, our new combination of skip-window sampling with\ngreedy aggregation achieves an even better effectiveness at only\nabout one third of the comparisons. When tolerating a very slight\nloss in effectiveness, even an order of magnitude fewer comparisons\nsuffice. The more local exhaustive window sampling method leads\nto less effective rankings for which larger samples are needed than\nfor skip-window or a global random sample. This suggests that a\ngood sample of pairwise comparisons should not just sample from\na very local environment per rank in the pointwise ranking.\nSparsification in pairwise re-ranking opens up new areas of re-\nsearch. Of the sampling paradigms evaluated (random vs. structured\nand local vs. global), the global structured sampling works better\nthan the random one. Still, the samples are static in the sense that\nthe sample is pre-computed before aggregation. Dynamic sampling\ntechniques, in which new comparisons could be selected even dur-\ning aggregation could merit further analyses. Sparsification could\nalso be used to increase the depth of the pairwise re-ranking rather\nthan its efficiency. Instead of minimizing the comparison budget\nfor a fixed depth ğ‘˜, a fixed comparison budget can be used to max-\nimize ğ‘˜. For example, the usually recommended depth ğ‘˜ = 50 re-\nquires 2,450 comparisons (ğ‘˜2 âˆ’ğ‘˜) for traditional pairwise re-ranking.\nA sampling rate of 30% (or 10%) now allows a re-ranking depth\nof ğ‘˜ = 90 (ğ‘˜ = 157) for a budget of about 2,450 comparisons. This\nmay have a strong effect for recall-intensive retrieval tasks.\nREFERENCES\n[1] Alan Agresti and Maria Kateri. 2011. Categorical Data Analysis. In International\nEncyclopedia of Statistical Science . Springer, 206â€“208.\n[2] Nir Ailon, Moses Charikar, and Alantha Newman. 2008. Aggregating\nOnconsistent Information: Ranking and Clustering. J. ACM 55, 5 (2008),\n23:1â€“23:27.\n[3] Juan A. Aledo, JosÃ© A. GÃ¡mez, and Alejandro Rosete. 2021. A Highly Scalable\nAlgorithm for Weak Rankings Aggregation. Inf. Sci. 570 (2021), 144â€“171.\n[4] Ralph Allan Bradley and Milton E Terry. 1952. Rank Analysis of Incomplete\nBlock Designs: The Method of Paired Comparisons. Biometrika 39, 3/4 (1952),\n324â€“345.\n[5] Charles L. A. Clarke, Nick Craswell, and Ian Soboroff. 2009. Overview of the\nTREC 2009 Web Track. In Proc. of TREC 2009 .\n[6] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Gordon V. Cormack. 2010.\nOverview of the TREC 2010 Web Track. In Proc. of TREC 2010 .\n[7] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Ellen M. Voorhees. 2011.\nOverview of the TREC 2011 Web Track. In TRECâ€™11.\n[8] Charles L. A. Clarke, Nick Craswell, and Ellen M. Voorhees. 2012. Overview of\nthe TREC 2012 Web Track. In Proc. of TREC 2012 .\n[9] StÃ©phan ClÃ©menÃ§on, GÃ¡bor Lugosi, and Nicolas Vayatis. 2008. Ranking and\nEmpirical Minimization of U-Statistics. The Annals of Statistics 36, 2 (2008),\n844â€“874.\n[10] William W. Cohen, Robert E. Schapire, and Yoram Singer. 1999. Learning to\nOrder Things. J. Artif. Intell. Res. 10 (1999), 243â€“270.\n[11] Kevyn Collins-Thompson, Paul Bennett, Fernando Diaz, Charles L. A. Clarke,\nand Ellen M. Voorhees. 2013. Overview of the TREC 2013 Web Track. In Proc. of\nTREC 2013 .\n[12] Kevyn Collins-Thompson, Craig Macdonald, Paul Bennett, Fernando Diaz, and\nEllen M. Voorhees. 2014. Overview of the TREC 2014 Web Track. In Proc. of\nTRECâ€™14.\n[13] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021.\nOverview of the TREC 2020 Deep Learning Track. CoRR abs/2102.07662 (2021).\n[14] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M.\nVoorhees. 2020. Overview of the TREC 2019 Deep Learning Track. CoRR\nabs/2003.07820 (2020).\nSparse Pairwise Re-ranking with Pre-trained Transformers ICTIR â€™22, July 11â€“12, 2022, Madrid, Spain\n[15] Zhuyun Dai and Jamie Callan. 2020. Context-Aware Document Term Weighting\nfor Ad-Hoc Search. In Proc. of WWW 2020 . ACM / IW3C2, 1897â€“1907.\n[16] Roger Fletcher. 1987. Practical Methods of Optimization (2 ed.). John Wiley &\nSons, New York.\n[17] Norbert Fuhr. 1989. Optimum Polynomial Retrieval Functions Based on the\nProbability Ranking Principle. ACM Trans. Inf. Syst. 7, 3 (1989), 183â€“204.\n[18] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Understanding BERT Rankers\nUnder Distillation. In Proc. of ICTIR 2020 . ACM, 149â€“152.\n[19] Sebastian HofstÃ¤tter, Sophia Althammer, Michael SchrÃ¶der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with\nCross-Architecture Knowledge Distillation. CoRR abs/2010.02666 (2020).\narXiv:2010.02666\n[20] Sebastian HofstÃ¤tter and Allan Hanbury. 2019. Letâ€™s Measure Run Time!\nExtending the IR Replicability Infrastructure to Include Performance Aspects. In\nProc. of OSIRRC@SIGIR 2019 (CEUR, Vol. 2409) . CEUR-WS.org, 12â€“16.\n[21] Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002. Cumulated Gain-Based Evaluation\nof IR Techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422â€“446.\n[22] Marcin Kaszkiel and Justin Zobel. 1997. Passage Retrieval Revisited. In Proc. of\nSIGIR 1997 . ACM, 178â€“185.\n[23] Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Tie-Yan Liu. 2012. Statistical\nConsistency of Ranking Methods in A Rank-Differentiable Probability Space. In\nProc. of NeurIPS 2012 . 1241â€“1249.\n[24] Jimmy Lin. 2019. The Neural Hype, Justified! A Recantation. SIGIR Forum 53, 2\n(2019), 88â€“93. https://doi.org/10.1145/3458553.3458563\n[25] Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021. Pretrained Transformers\nfor Text Ranking: BERT and Beyond. Synthesis Lectures on Human Language\nTechnologies 14, 4 (2021), 1â€“325.\n[26] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval . Springer, Berlin\nHeidelberg.\n[27] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman\nCohan. 2020. ABNIRML: Analyzing the Behavior of Neural IR Models. CoRR\nabs/2011.00696 (2020).\n[28] Craig Macdonald and Nicola Tonellotto. 2020. Declarative Experimentation in\nInformation Retrieval using PyTerrier. InProc. of ICTIR 2020 . ACM, 161â€“168.\n[29] Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Text. In\nProc. of EMNLP 2004 . ACL, 404â€“411.\n[30] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. CoRR abs/1611.09268 (2016).\n[31] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020.\nDocument Ranking with a Pretrained Sequence-to-Sequence Model. In Proc. of\nFindings EMNLP 2020 . ACL, New York, 708â€“718.\n[32] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\nDocument Ranking with BERT. CoRR abs/1910.14424 (2019), 1â€“13.\n[33] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The\nPageRank Citation Ranking: Bringing Order to the Web . Technical Report.\n[34] Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The\nExpando-Mono-Duo Design Pattern for Text Ranking with Pretrained\nSequence-to-Sequence Models. CoRR abs/2101.05667 (2021), 1â€“23.\n[35] Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui\nWang, Michael Bendersky, and Marc Najork. 2021. Are Neural Rankers still\nOutperformed by Gradient Boosted Decision Trees?. In Proc. ICLR 2021 .\n[36] Leonardo Rigutini, Tiziano Papini, Marco Maggini, and Franco Scarselli. 2011.\nSortNet: Learning to Rank by a Neural Preference Function. IEEE Trans. Neural\nNetworks 22, 9 (2011), 1368â€“1380.\n[37] Tetsuya Sakai. 2007. Alternatives to Bpref. In Proc. of SIGIR 2007 . ACM, 71â€“78.\n[38] Hal Stern. 1992. Are All Linear Paired Comparison Models Empirically\nEquivalent? Mathematical Social Sciences 23, 1 (1992), 103â€“117.\n[39] Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, and\nWei Wu. 2021. Improving Document Representations by Generating Pseudo\nQuery Embeddings for Dense Retrieval. In Proc. of ACL 2021 . ACL, 5054â€“5064.\n[40] Louis Thurstone. 1927. The Method of Paired Comparisons for Social Values.\nThe Journal of Abnormal and Social Psychology 21, 4 (1927), 384.\n[41] Michael VÃ¶lske, Alexander Bondarenko, Maik FrÃ¶be, Benno Stein, Jaspreet\nSingh, Matthias Hagen, and Avishek Anand. 2021. Towards Axiomatic\nExplanations for Neural Ranking Models. In Proc. of ICTIR 2021 . ACM, 13â€“22.\n[42] Yue Wu, Tao Jin, Hao Lou, Pan Xu, Farzad Farnoud, and Quanquan Gu. 2021.\nAdaptive Sampling for Heterogeneous Rank Aggregation from Noisy Pairwise\nComparisons. CoRR abs/2110.04136 (2021).\n[43] Yu Xiao, Hongzhong Deng, Xin Lu, and Jun Wu. 2021. Graph-Based Rank\nAggregation Method for High-Dimensional and Partial Rankings. J. Oper. Res.\nSoc. 72, 1 (2021), 227â€“236.\n[44] Ji Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin. 2020. Early Exiting BERT\nfor Efficient Document Ranking. In Proc. of SustaiNLP 2020 . ACL, 83â€“88.\n[45] Yue Zhang, ChengCheng Hu, Yuqi Liu, Hui Fang, and Jimmy Lin. 2021. Learning\nto Rank in the Age of Muppets: Effectivenessâ€“Efficiency Tradeoffs in\nMulti-Stage Ranking. In Proc. of SustaiNLP 2021 . ACL, 64â€“73.\n[46] Ke Zhou, Gui-Rong Xue, Hongyuan Zha, and Yong Yu. 2008. Learning To Rank\nWith Ties. In Proc. of SIGIR 2008 . ACM, New York, NY, USA, 275â€“282."
}