{
  "title": "Probing Task-Oriented Dialogue Representation from Language Models",
  "url": "https://openalex.org/W3100532709",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2532837580",
      "name": "Chien-Sheng Wu",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035305735",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2963844597",
    "https://openalex.org/W3104078590",
    "https://openalex.org/W2963797754",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2886305736",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3100128199",
    "https://openalex.org/W2150593711",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W4299585995",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2593864460",
    "https://openalex.org/W1546503963",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2162833336",
    "https://openalex.org/W4288624561",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2891732163",
    "https://openalex.org/W3100110884",
    "https://openalex.org/W2986193249",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3037026762",
    "https://openalex.org/W3016625483",
    "https://openalex.org/W4300672471",
    "https://openalex.org/W3035451444",
    "https://openalex.org/W2624448691",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2945475330"
  ],
  "abstract": "This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5036–5051,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n5036\nProbing Task-Oriented Dialogue Representation from Language Models\nChien-Sheng Wu and Caiming Xiong\nSalesforce Research\n[wu.jason, cxiong]@salesforce.com\nAbstract\nThis paper investigates pre-trained language\nmodels to ﬁnd out which model intrinsically\ncarries the most informative representation for\ntask-oriented dialogue tasks. We approach the\nproblem from two aspects: supervised classi-\nﬁer probe and unsupervised mutual informa-\ntion probe. We ﬁne-tune a feed-forward layer\nas the classiﬁer probe on top of a ﬁxed pre-\ntrained language model with annotated labels\nin a supervised way. Meanwhile, we propose\nan unsupervised mutual information probe to\nevaluate the mutual dependence between a real\nclustering and a representation clustering. The\ngoals of this empirical paper are to 1) in-\nvestigate probing techniques, especially from\nthe unsupervised mutual information aspect,\n2) provide guidelines of pre-trained language\nmodel selection for the dialogue research com-\nmunity, 3) ﬁnd insights of pre-training factors\nfor dialogue application that may be the key to\nsuccess.\n1 Introduction\nTask-oriented dialogue systems achieve speciﬁc\nuser goals within a limited number of dialogue\nturns via natural language. They have been used\nin a wide range of applications, such as booking\nrestaurants (Wen et al., 2017), providing tourist\ninformation (Budzianowski et al., 2018), ordering\ntickets (Schulz et al., 2017), and healthcare con-\nsultation (Wei et al., 2018). They are also crucial\ncomponents of intelligent virtual assistants like Siri,\nAlexa, and Google Assistant.\nMost of the task-oriented dialogue systems\nnowadays, are beneﬁted from transfer learning (Wu\net al., 2019; Lin et al., 2020), especially pre-trained\nlanguage models trained on general text, such as\nBERT (Devlin et al., 2018) and GPT2 (Radford\net al., 2019). However, previous work claims that\nlinguistic patterns could differ between writing text\nModel Dial. Data Parameters Output Dim.\nBERT-base X 109.5M 768\nAlBERT-base X 11.7M 768\nDistilBERT-base X 66.4M 768\nRoBERTa-based X 124.6M 768\nGPT2-small X 124.4M 768\nELECTRA-GEN X 33.5M 256\nELECTRA-DIS X 108.9M 768\nConveRT V 29M 1024\nDialoGPT-small V 124.4M 768\nTOD-BERT-mlm V 119.5M 768\nTOD-BERT-jnt V 119.5M 768\nTOD-GPT2 V 124.4M 768\nTable 1: An overview of selected pre-trained language\nmodels (Details in Section 2).\nand human conversation, resulting in a large gap\nof data distributions (Bao et al., 2019; Wolf et al.,\n2019b). Recently, several approaches are lever-\naging open-domain data (Henderson et al., 2019;\nZhang et al., 2019), or aggregating task-oriented\ndata (Wu et al., 2020) to pre-train language models.\nIn this paper, we are interested in answering\nthese questions: which language model has the\nmost informative representations that is better\nfor what task-oriented dialogue task? Does pre-\ntraining with dialogue-speciﬁc data or different ob-\njectives make any difference? We investigate how\ngood these pre-trained representations are for a\ntask-oriented dialogue system, ignoring the model\narchitectures and training strategies by only prob-\ning their ﬁnal representations with ﬁne-tuning mod-\nels. A good representation implies better knowl-\nedge transferring and domain generalization abil-\nity, making downstream applications easier and\ncheaper to be improved.\nWe tackle this problem with two probing solu-\ntions: supervised classiﬁer probe and unsupervised\nmutual information probe. Classiﬁer probe is com-\nmonly used in different NLP tasks such as morphol-\nogy (Belinkov et al., 2017), sentence length (Adi\net al., 2016), or linguistic structure (Hewitt and\n5037\nManning, 2019). In this setting, we ﬁne-tune a sim-\nple classiﬁer for a speciﬁc task (e.g., intent identiﬁ-\ncation) on a ﬁxed pre-trained language model. The\nprobe uses supervision to ﬁnd the best transforma-\ntion for each sub-task.\nIn addition, we present mutual information probe\nto investigate these language models by directly\nclustering their output representations, as recent\nstudy (Pimentel et al., 2020) suggests that a sim-\nple classiﬁer may not be able to achieve the best\nestimate of mutual information between features\nand the downstream task. We apply two cluster-\ning techniques, K-means (Lloyd, 1982) and Gaus-\nsian mixture model (Reynolds, 2009), to calcu-\nlate its adjusted normalized mutual information\n(ANMI) (Vinh et al., 2010) between the predicted\nclustering and the true task-speciﬁc clustering.\nWe investigate 12 language models, as shown in\nTable 1, where ﬁve of them have been pre-trained\nwith dialogue data. We evaluate four core task-\noriented dialogue tasks, domain identiﬁcation, in-\ntent detection, slot tagging, and dialogue act predic-\ntion. They correspond to the commonly deﬁned nat-\nural language understanding, dialogue state track-\ning, and dialogue management modules (Wen et al.,\n2017). We hope our probing analysis can provide\ninsights to facilitate future task-oriented dialogue\nresearch. Some of the key observations in this\nwork are summarized here (More discussion in\nSection 4.4):\n•No matter the open-domain or close-domain, pre-\ntraining with dialogue data helps learning better\nrepresentations for task-oriented dialogue.\n•Pre-trained language models intrinsically contain\nmore information about intents and dialogue acts\nbut less for slots.\n•ConveRT (Henderson et al., 2019) and TOD-\nBERT-jnt (Wu et al., 2020) have the highest clas-\nsiﬁcation accuracy and mutual information score,\nsuggesting that response selection is useful for\ndialogue pre-training, especially when we com-\npare TOD-BERT-jnt to TOD-BERT-mlm.\n•Top models also include TOD-GPT2 and Distil-\nBERT (Sanh et al., 2019). The distilled version\nof BERT surprisingly outperforms BERT and\nother strong baselines such as RoBERTa (Liu\net al., 2019).\n•DialoGPT and GPT2 do not perform well on mu-\ntual information evaluation but have a middle-\nranking classiﬁcation accuracy, implying that\ntheir representations are informative but not suit-\nable for unsupervised clustering.\n•Models such as AlBERT (Lan et al., 2019) and\nELECTRA (Clark et al., 2020) have low classiﬁ-\ncation accuracy and mutual information, show-\ning the least useful information on task-oriented\ndialogue tasks.\n2 Pre-Trained Language Models\nW can roughly divide pre-trained language mod-\nels into two categories: uni-directional and bi-\ndirectional. BERT-based systems are bi-directional\nlanguage models and usually trained with the\nmasked language modeling (MLM) objective, i.e.,\ngiven the left and right context to predict the cur-\nrent masked token. GPT-based models, on the\nother hand, are uni-directional language models\ntrained always to predict the next token in an auto-\nregressive way.\nFor a BERT-based model, we use the ﬁnal-layer\nhidden state of its ﬁrst token, [CLS], to represent\nan input sequence. This built-in token is originally\ndesigned to aggregate the information. Since GPT-\nbased models are uni-directional and do not have a\nsimilar design as the [CLS] token, we use the mean\npooling of its output hidden states to represent the\ninput sequence, which is better than only using the\nlast hidden state in our experiments.\nBERT-based BERT is a Transformer (Vaswani\net al., 2017) encoder with a self-attention mecha-\nnism, which is trained on Wikipedia and BookCor-\npus using the MLM and next sentence prediction\nobjectives. Liu et al. (2019) proposed a robustly op-\ntimized approach for BERT, call RoBERTa, where\nthey improved it by training the model longer\nwith bigger batches over more data and longer\nsequences, and removing the next sentence pre-\ndiction objective. Lan et al. (2019) proposed a\nlite BERT (AlBERT) that trained with MLM and\ninter-sentence coherence losses, and aimed to lower\nmemory consumption and increase the training\nspeed. With similar motivation, Sanh et al. (2019)\ntrained a DistilBERT that reduce 40% of param-\neters with a triple loss, including MLM, distilla-\ntion, and cosine-distance losses. Clark et al. (2020)\nproposed ELECTRA using a sample-efﬁcient pre-\ntraining task called replaced token detection. They\nused a generator network (ELECTRA-GEN) to re-\nplace tokens with plausible alternative tokens and\n5038\ntrained a discriminative model (ELECTRA-DIS) to\npredict whether the generator replaced each token\nin the input.\nMost of the pre-trained models above are trained\non general text corpora with language modeling ob-\njectives. Henderson et al. (2019), on the other hand,\nused social media conversational data to train the\nConveRT model. It is a Transformer-based dual-\nencoder model pre-trained on a dialogue response\nselection task using 727M Reddit (input, response)\npairs. Very recently, Wu et al. (2020) proposed task-\noriented dialogue BERT (TOD-BERT), which is\ninitialized by BERT and further pre-trained on nine\npublicly available task-oriented dialogue corpora.\nThey have one version with only MLM objective\n(TOD-BERT-mlm) and another with both MLM\nand contrastive learning objectives of response se-\nlection (TOD-BERT-jnt). TOD-BERT has shown\ngood performance on several task-oriented down-\nstream tasks, especially in the few-shot setting.\nGPT-based GPT2 (Radford et al., 2019) is the\nrepresentative of uni-directional language models\nusing a Transformer decoder, where the objective\nis to maximize left-to-right generation likelihood.\nTo ensure diverse and nearly unlimited text sources,\nthey use Common Crawl to obtain 8M documents\nas its training data. Budzianowski and Vuli´c (2019)\ntrained GPT2 on task-oriented response genera-\ntion task, taking system belief, database result,\nand last dialogue turn as inputs. It only uses\none dataset to train its model because few pub-\nlic datasets have database information available\nfor pre-training. Zhang et al. (2019) pre-trained\nGPT2 on 147M open-domain Reddit data for re-\nsponse generation and called it DialoGPT. It aims\nto generate more relevant, contentful, and consis-\ntent responses for chit-chat dialogue systems. In\nthis paper, following TOD-BERT’s idea, we train\na task-oriented GPT2 model (TOD-GPT2) built\non the GPT2 model and further pre-trained with\ntask-oriented datasets. We use the same dataset\ncollection, which contains nine datasets in total, as\nshown in Wu et al. (2020), to pre-train the model\nas a reference.\n3 Method\nWe deﬁne a dialogue corpus D= {D1,...,D M }\nhas M dialogue samples, and each dialogue sam-\nple Dm has T turns of conversational exchange\n{U1,S1 ...,U T ,ST }between a user and a sys-\ntem. For every utterance Ut or St, we have human-\nannotated domain, user intent, slot, and dialogue\nact labels. We ﬁrst feed all the utterances to a\npre-trained model and obtain user and system rep-\nresentations. In this section, we ﬁrst discuss how\nwe design our classiﬁer probe and then introduce\nour mutual information probe’s background and\nusage.\n3.1 Classiﬁer Probe\nWe use a simple classiﬁer to transform those repre-\nsentations for a speciﬁc task and optimize it with\nannotated data.\nVi = A(FFN (Ei)), (1)\nwhere Ei ∈RdB is the output representation with\ndimension dB from a pre-trained model, FFN ∈\nRN×dB is a feed-forward layer that maps from di-\nmension dB to a prediction with N classes, and\nAis an activation layer. For domain identiﬁcation\nand intent detection, we use a Softmax layer and\nbackpropagate with the cross-entropy loss. For di-\nalogue slot and act prediction, we use a Sigmoid\nlayer and the binary cross-entropy loss since they\nare multi-label classiﬁcation tasks.\n3.2 Mutual Information Probe\nWe ﬁrst cluster utterances in an unsupervised fash-\nion using either K-means (Lloyd, 1982) or Gaus-\nsian mixture model (GMM) (Reynolds, 2009) with\nKclusters. Then we compute the adjusted mutual\ninformation score (Vinh et al., 2010) between the\npredicted clustering and each of the true cluster-\nings (e.g., domain and intent) for different hyper-\nparameters K. Note that the predicted clustering is\nnot dependent on any particular labels.\n3.2.1 Utterance Clustering\nK-means is a common clustering algorithm that\naims to partition N samples into K clusters A=\n{A1,...,A K}in which each sample is assigned\nto a cluster centroid with the nearest mean.\narg max\nA\nK∑\ni=1\n∑\nx∈Ai\n∥x−µi∥2, (2)\nwhere µi is the centroid of the Ai cluster and the\nalgorithm is updated in an iterative manner.\nOn the other hand, GMM assumes a certain num-\nber of Gaussian distributions (K mixture compo-\nnents). It takes both mean and variance of the\ndata into account, while K-means only consider the\n5039\ndata’s mean. By the Expectation-Maximization al-\ngorithm, GMM ﬁrst calculates each sample’s prob-\nability belongs to a cluster Ai during the E-step,\nthen updates its density function to compute new\nmean and variance during the M-step.\nIn our experiments, we cluster separately for user\nutterances U and system response S. Note that\nK is a hyper-parameter since we may not know\nthe true distribution in a real scenario. To avoid\nthe local minimum issue, we run multiple times\n(typically ten runs) and use the best clustering result\nfor mutual information evaluation.\n3.2.2 ANMI\nTo evaluate two clusterings’ quality, we compute\nthe ANMI score between a clustering and its\nground-truth annotation. ANMI is adjusted for\nrandomness, which accounts for the bias in mutual\ninformation, giving high values to the clustering\nwith a larger number of clusters. ANMI has a\nvalue of 1 when two partitions are identical, and\nan expected value of 0 for random (independent)\npartitions.\nMore speciﬁcally, we assume two label cluster-\nings, Aand B, that have the same N objects. The\nmutual information (MI) between Aand Bis de-\nﬁned by\nMI(A,B) =\n|A|∑\ni=1\n|B|∑\nj=1\nP(i,j) log( P(i,j)\nP(i)P(j)), (3)\nwhere P(i,j) =|Ai∩Bj|/Nis the probability that\na randomly picked sample falls into both Ai and\nBj classes. Similarly, P(i) =|Ai|/Nand P(j) =\n|Bj|/N are the probabilities that the sample falls\ninto either the Ai or Bj class.\nThe normalized mutual information (NMI) nor-\nmalizes MI with the mean of entropy, which is\ndeﬁned as\nNMI(A,B) = MI(A,B)\nmean(H(A),H(B)), (4)\nwhere H(A) =−∑|A|\ni=1 P(i) log(P(i)) is the en-\ntropy of the A clustering, which measures the\namount of uncertainty for the partition set.\nMI and NMI are not adjusted for chance and will\ntend to increase as the number of cluster increases,\nregardless of the actual amount of “mutual infor-\nmaiton” between the label assignments. Therefore,\nadjusted normalized mutual information (ANMI) is\ndesigned to modify NMI score with its expectation,\nwhich is deﬁned by\nANMI = MI−E[MI]\nmean(H(A),H(B))−E[MI], (5)\nMWOZ\nDomain Dialogue Act Slot\nrestaurant\nhotel\nattraction\ntrain\ntaxi\nnobook\nbye\nrequest\nrecommend\nwelcome\nbook\ngreet\nnooffer\nreqmore\nofferbooked\nselect\ninform\nofferbook\ntype\nbook day\nbook people\nday\npricerange\nleaveat\narriveby\nparking\nbook time\nname\ndestination\ninternet\nstars\nbook stay\ndeparture\narea\nfood\ndepartment\nTable 2: Labels classes in the MWOZ Data.\nwhere the expectation E[MI] can be calculated us-\ning the equation in Vinh et al. (2010).\n4 Experiments\n4.1 Datasets\nThe multi-domain Wizard-of-Oz (MWOZ)\ndataset (Budzianowski et al., 2018) is one\nof the most common benchmark datasets for\ntask-oriented dialogue systems. We use MWOZ\nto evaluate domain identiﬁcation, dialogue slot\ntagging, and dialogue act prediction tasks. It\ncontains 8420/1000/1000 dialogues for training,\nvalidation, and testing sets, respectively. There are\nseven domains in the training set and ﬁve domains\nin the others. There are 13 unique system dialogue\nacts and 18 unique slots as shown in Table 2.\nBesides, we use the out-of-scope intent (OOS)\ndataset (Larson et al., 2019) for our intent de-\ntection experiment. The OOS dataset is one of\nthe largest annotated intent datasets, including\n15,100/3,100/5,500 samples for the train, valida-\ntion, and test sets, respectively. It has 150 intent\nclasses over ten domains and an additional out-of-\nscope intent class, a user utterance that does not\nfall into any of the predeﬁned intents. The whole\nintent list is shown in the Appendix.\n4.2 Training Details\nWe ﬁrst process user utterance and system re-\nsponse using the tokenizer corresponding to each\nper-trained model. To obtain each representation,\nwe run most of the pre-trained models using the\n5040\nHuggingFace (Wolf et al., 2019a) library, except\nthe ConveRT 1 and TOD-BERT 2. We ﬁne-tune\nGPT2 using its default hyper-parameters and the\nsame nine datasets as shown in Wu et al. (2020) to\ntrain for TOD-GPT2 model. For classiﬁer probing,\nwe ﬁne-tune the top layer with a consistent hyper-\nparameter setting. We apply AdamW (Loshchilov\nand Hutter, 2017) optimizer with a learning rate\n5e−5 and gradient clipping 1.0. We use K =\n4,8,16,32,64,128,256 with 50 iterations each,\nand report the moving trend for MI probing. We\nuse GMM clustering from the scikit-learn library,\nand we adopt the K-means implementation from\nthe faiss library (Johnson et al., 2017). Experiments\nwere conducted on a single NVIDIA Tesla V100\nGPU.\n4.3 Evaluation\nDomain identiﬁcation and intent detection tasks are\nmulti-class classiﬁcation problems. Therefore, we\ncan directly use their annotated domain and intent\nlabels to compute the ANMI scores. Slot tagging\nand dialogue act prediction tasks, meanwhile, are\nmulti-label classiﬁcation problems. For example,\neach utterance can include multiple slots mentioned\n(<food>and <price>slots) and various actions\ntriggered (<greeting>and <inform>acts). In our\nexperiment, we use a naive way that is viewing a\ndifferent set of slot or act combination as different\nlabels, e.g., three slot sets <food>, <food, price>,\nand <price, location > belong to three different\nclusters.\n4.4 Results\nClassiﬁer results are shown in Figure 1. We can\nobserve that ConveRT, TOD-BERT-jnt, and TOD-\nGPT2 achieve the best performance, implying that\npre-training with dialogue-related data captures\nbetter representations, at least in these sub-tasks.\nMoreover, the performance of ConveRT and TOD-\nBERT-jnt suggests that it is helpful to pre-train\nwith a response selection contrastive objective, es-\npecially when comparing TOD-BERT-jnt to TOD-\nBERT-mlm. Moreover, most of the pre-trained\nmodels have a similar and high micro-F1 score in\n(d) system dialogue act prediction, as most of them\nare above 75% over 13 classes. Dialogue slot (c)\ninformation, meanwhile, is not well captured by\n1https://github.com/PolyAI-LDN/\npolyai-models\n2https://github.com/jasonwu0731/\nToD-BERT\n0.0\n0.2\n0.4\n0.6\n0.8\nElectra-genRoBERTaElectra-dis\nAlBERT BERT\nDialoGPT\nGPT2\nToD-BERT-mlm\nDistilBERTToD-GPT2ConveRT\nToD-BERT-jnt\n(a) MWOZ Domain (System)\n0.00\n0.25\n0.50\n0.75\n1.00\nAlBERT\nElectra-genRoBERTaElectra-disDialoGPT\nToD-BERT-mlm\nBERT GPT2\nDistilBERT\nToD-BERT-jnt\nToD-GPT2ConveRT\n(b) OOS Intent (User)\n0.0\n0.1\n0.2\n0.3\n0.4\nElectra-genRoBERTaAlBERT\nElectra-dis\nBERT\nDialoGPT\nToD-BERT-mlm\nDistilBERT\nGPT2\nToD-BERT-jnt\nToD-GPT2ConveRT\n(c) MWOZ Slot (System)\n0.00\n0.25\n0.50\n0.75\n1.00\nElectra-gen\nAlBERTRoBERTa\nToD-BERT-mlm\nDialoGPTElectra-dis\nBERT GPT2\nToD-BERT-jnt\nDistilBERTToD-GPT2ConveRT\n(d) MWOZ Act (System)\nFigure 1: The results of supervised classiﬁer probe.\nThe y-axis in (a) and (b) represents the accuracy. The\ny-axis in (c) and (d) represents the micro-F1 score.\nthese representations, resulting in a micro-F1 lower\nthan 30%. On the other hand, ELECTRA-GEN,\nRoBERTa, and AlBERT show the worst classiﬁca-\ntion results. Especially in (b) intent classiﬁcation\n5041\n(a) MWOZ Domain (User)\n (b) MWOZ Domain (System)\n (c) OOS Intent (User)\n(d) MWOZ Slot - Set (User)\n (e) MWOZ Slot - Set (System)\n (f) MWOZ Act - Set (System)\nFigure 2: The ANMI evaluation of pre-trained models with the domain, intent, slot, and action labels. The X-axis\nis the number of clusters and the y-axis is the ANMI score (Best view in color)\nand (c) dialogue slot tagging, some of them seem to\nhave zero useful information to make a prediction.\nMutual information results using K-means clus-\ntering are shown in Figure 2. Due to the space limit,\nwe report the results using GMM in the Appendix,\nas the two of them have similar trends. The x-axis\nis the number of clusters in each subplot, rang-\ning from 4 to 256, and the y-axis is the ANMI\nscore between a predicted clustering and its cor-\nresponding true clustering. In general, the mutual\ninformation probe results are similar to what we\nobserve in the classiﬁer probe. We can ﬁnd that\nToD-BERT-jnt and ConveRT are those with the\nhighest mutual information, and they are usually\nfollowed by TOD-GPT2 and DistilBERT.\nAnother observation is that representations from\nthose pre-trained language models, especially the\ntop ones, seem to have more connection with user\nintent and system dialogue act labels than do-\nmain and slot labels. The average ANMI scores\nacross 12 models and 7 different number of clus-\nters for intent and dialogue act are 0.193 ±0.169\nand 0.226 ±0.107, respectively. But domain\nand dialogue slot only have 0.086 ±0.087 and\n0.077 ±0.057 AMNI scores in average. We dis-\ncuss each subplot in detail in the following:\nFigure 2 (a) and (b) show the mutual information\nbetween predicted clustering and the true domain\nlabels on the MWOZ dataset. A user utterance\nseems to have higher domain mutual information\nthan a system response. TOD-BERT-jnt, in this\ncase, outperforms others by a large margin, achiev-\ning around 0.4 ANMI with 8 clusters. Figure 2\n(c) is about user intent using user utterances. Con-\nveRT surpasses others by far in the mutual infor-\nmation of intent, achieving over 0.7 ANMI at 128\nclusters when the true number of classes equals to\n151. Other than the top three models (ConveRT,\nTOD-BERT-jnt, and DistilBERT), the remaining\npre-trained models have ANMI scores lower than\n0.2.\nFigure 2 (d) and (e) show the mutual information\nevaluation using the slot labels. When comparing\n(d) to (e), we can ﬁnd that user utterances contain\n5042\nFigure 3: The tSNE visualization of dialogue representations from ToD-BERT-jnt. (Best view in color)\nFigure 4: The tSNE visualization of dialogue representations from GPT2. (Best view in color)\nmore slot information than system responses (Max\naround 0.35 and 0.25). It is not surprising because\na user in task-oriented dialogue is usually the slot\ninformation provider, informing what location or\nwhich cuisine s/he prefers. ToD-BERT and Con-\nveRT perform similar in this case, still outperform\nothers by a big margin.\nFigure 2 (f) shows the mutual information for\nthe predicted clustering of system dialogue acts.\nWe can ﬁnd that most of the pre-trained language\nmodels have shown a relatively high ANMI score\n(average 0.226) and closed the gap between their\nperformance and the top model. ConveRT works\nthe best, in this case, followed by TOD-BERT-jnt\nand TOD-GPT2, in which two of them seem to\nhave similar ANMI scores.\n5 More Analysis\nDifference Between Probes Ideally, both probes\nshould distinguish the goodness of different pre-\ntrained language models, i.e., features that can be\neasily classiﬁed or features with high correlation\nwith true distributions are preferred. However, we\nfound that although, in general, the trends we ob-\nserve from two probing methods are similar, they\nare not the same in terms of the ranking. When\ncomparing the ranking of GPT2 and DialoGPT\nmodels in Figure 1 and Figure 2, we found that\nthey obtain almost the worse ANMI scores but\nwork quite good in classiﬁcation accuracy. This\nobservation means that their representations of dif-\nferent classes are “close” to each other as a low\nANMI score suggesting a more noisy clustering.\nStill, at the same time, it is not hard to ﬁnd a hyper-\nplane that can well discriminate those features.\nWe discuss some possible reasons for this inter-\nesting observation in the following. The ﬁrst guess\nis that these features may not follow a Gaussian\ndistribution, as we assume during clustering, sug-\ngesting that more advanced clustering techniques\ncan be investigated in future work. The second\nguess is that these features have an unavoidable\nclustering noise that can be denoised or debiased\neasily by a strong supervision signal. The third\nguess, which may be a possible reason, is that these\nfeatures are clustered by some other factors that are\n5043\nConveRT\nCluster 1(Failed Booking)\ni am sorry but dojo noodle bar is solidly booked at that time . i can try a different time or day for you .1 moment while i try to make the reservation of table for 8 , friday at 16:30 .booking was unsuccessful . can you try another time slot ?i am very sorry i was unable to book at acorn guest house for 5 nights , would you like to try for a shorter stay ?i am afraid that booking is unsuccessful . would you like a different day or amount of days ?\nCluster 2(Train Time)\nthere are 5 trains available , may i book 1 for you that leaves at 7:40 and arrives at 10:23 ?tr0330 departs at 14:09 and arrives by 15:54 . would you like a ticket ?the tr2141 arrives by 15:27 . would you like me to reserve some seats for you ?i have train tr4283 that leaves cambridge at 5:29 and arrives in bishops stortford at 6:07 . would you like to make reservations ?i have a train that leaves cambridge 14:01 arriving in birmingham new street at 16:44 . would that work ?\nCluster 3(Restaurant Request)\nthere are 21 restaurant -s available in the centre of town . how about a speciﬁc type of cuisine ?there are 9 indian restaurant -s in centre what price range do you want ?i am sorry , there are no catalan dining establishments in the city centre . would you like to look for a different cuisine or area ?i found 4 restaurant -s with the name tandoori that serve indian food on the south , west , and east . do you have a location preference ?there are no singaporean restaurant -s , but there are cheap ones offering several different cuisines .\nCluster 4(Conﬁrm Booking)\nall set . your reference number is k2bo09vq .i have got you booked for 16:30 . the reference number is eq0yaq1g .your reservation was a success and the reference number is jtwxfm7m .i have got your booking set , the reference number is 9rmfgjma .i booked tr3932 , reference number is ﬁw5abo2 .\nCluster 5(Hotel Request)\nwhat part of town there are none in the west .i can help you with that . do you have any special area you would like to stay ? or possibly a star request ?there are no colleges close to the area you are requesting , would you like to chose another destination ?sure , what area are you thinking of staying ?i would be happy to help . may i ask what price range and area of town you are looking for ?\nTable 3: Clustering results of the ConveRT model. The samples are picked from each randomly selected ﬁve\nclusters with K=32. We can roughly label a topic for each cluster.\nnot tested, and at the same time, the factors we are\ninterested in are scattered in groups for different\nclasses in a similar way. Intuitively, there are four\nclustering results shown in Figure 5, where GPT2\nand DialoGPT may fall into the (d) clustering type,\nwhich has a lower mutual information score but\nhigher classiﬁcation accuracy.\nAs a result, we suggest a simple rule of thumb re-\ngarding which probing results. In short, the results\nof the classiﬁer probe could be useful if a super-\nvised approach for a downstream task is designed,\ne.g., user dialogue act prediction and dialogue state\ntracking. On the other hand, the mutual informa-\ntion probe is more effective for an unsupervised\nproblem, e.g., utterance clustering and dialogue\nparsing tasks.\nVisualization In Figure 3 and Figure 4, we visu-\nalize the embeddings of TOD-BERT-jnt and GPT2\ngiven the same system responses from the MWOZ\ntest set. Each point is reduced from its high-\ndimension features to a two-dimension point us-\ning the t-distributed stochastic neighbor embedding\n(tSNE). We use different colors to represent differ-\nent domains (left), dialogue acts (middle), and turn\nslots (right). As one can observe, TOD-BERT-jnt\nhas more clear group boundaries and better cluster-\ning results than GPT2. Visualization plots for other\npre-trained models are shown in the Appendix.\nWhat utterances are clustered together? In\nTable 3, we show the clustering examples of system\nresponses from the top performance model Con-\nFigure 5: Illustration of four different type of cluster-\nings related to mutual information and accuracy.\nveRT. We useK = 32clustering and randomly se-\nlect ﬁve clusters and ﬁve samples each. We found\nthat most of the utterances are related to an un-\nsuccessful booking in the cluster 1, containing “I\nam sorry,” “solidly booked,” or “booking was un-\nsuccessful.” We also found other clusters showing\ngood clustering results, such as selecting departure\nor arrival time for a train ticket or requesting more\nuser preference for a restaurant reservation. More\nclustering results are shown in the Appendix.\n5044\n6 Conclusion\nWe investigate representations from pre-trained lan-\nguage models for task-oriented dialogue tasks, in-\ncluding domain identiﬁcation, intent detection, slot\ntagging, and dialogue act prediction. We use a\nsupervised classiﬁer probe and a proposed unsuper-\nvised mutual information probe. From the ranking\nresults of two different probings, we show a list\nof interesting observations to provide model selec-\ntion guidelines and shed light on future research\ntowards a more advanced language modeling learn-\ning for dialogue applications.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207.\nSiqi Bao, Huang He, Fan Wang, and Hua Wu.\n2019. Plato: Pre-trained dialogue generation\nmodel with discrete latent variable. arXiv preprint\narXiv:1910.07931.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do neu-\nral machine translation models learn about morphol-\nogy? In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 861–872, Vancouver,\nCanada. Association for Computational Linguistics.\nPaweł Budzianowski and Ivan Vuli´c. 2019. Hello, it’s\ngpt-2–how can i help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. arXiv preprint arXiv:1907.05774.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Inigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Ga ˇsi´c. 2018. Multiwoz-a\nlarge-scale multi-domain wizard-of-oz dataset for\ntask-oriented dialogue modelling. arXiv preprint\narXiv:1810.00278.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMatthew Henderson, I ˜nigo Casanueva, Nikola Mrkˇsi´c,\nPei-Hao Su, Ivan Vuli ´c, et al. 2019. Con-\nvert: Efﬁcient and accurate conversational rep-\nresentations from transformers. arXiv preprint\narXiv:1911.03688.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\nBillion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nStefan Larson, Anish Mahendran, Joseph J Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K Kummerfeld, Kevin Leach, Michael A\nLaurenzano, Lingjia Tang, et al. 2019. An evalua-\ntion dataset for intent classiﬁcation and out-of-scope\nprediction. arXiv preprint arXiv:1909.02027.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nand Pascale Fung. 2020. Mintl: Minimalist transfer\nlearning for task-oriented dialogue systems. arXiv\npreprint arXiv:2009.12005.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nStuart Lloyd. 1982. Least squares quantization in\npcm. IEEE transactions on information theory,\n28(2):129–137.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. arXiv preprint arXiv:2004.03061.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nDouglas A Reynolds. 2009. Gaussian mixture models.\nEncyclopedia of biometrics, 741.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nHannes Schulz, Jeremie Zumer, Layla El Asri, and\nShikhar Sharma. 2017. A frame tracking model for\nmemory-enhanced dialogue systems. arXiv preprint\narXiv:1706.01690.\n5045\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nNguyen Xuan Vinh, Julien Epps, and James Bailey.\n2010. Information theoretic measures for cluster-\nings comparison: Variants, properties, normaliza-\ntion and correction for chance. The Journal of Ma-\nchine Learning Research, 11:2837–2854.\nZhongyu Wei, Qianlong Liu, Baolin Peng, Huaixiao\nTou, Ting Chen, Xuan-Jing Huang, Kam-Fai Wong,\nand Xiang Dai. 2018. Task-oriented dialogue sys-\ntem for automatic diagnosis. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n201–207.\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic,\nLina Maria Rojas-Barahona, Pei hao Su, Stefan\nUltes, David Vandyke, and Steve J. Young. 2017. A\nnetwork-based end-to-end trainable task-oriented di-\nalogue system. In EACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019a. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. arXiv preprint arXiv:1910.03771.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019b. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149.\nChien-Sheng Wu, Steven Hoi, Richard Socher, and\nCaiming Xiong. 2020. Tod-bert: Pre-trained natural\nlanguage understanding for task-oriented dialogues.\narXiv preprint arXiv:2004.06871.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-\nAsl, Caiming Xiong, Richard Socher, and Pascale\nFung. 2019. Transferable multi-domain state gener-\nator for task-oriented dialogue systems. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers). Association for Computational Linguistics.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\n5046\nOOS Intent\n’translate’, ’transfer’, ’timer’, ’deﬁnition’, ’meaningof life’, ’insurancechange’, ’ﬁndphone’,\n’travel alert’, ’ptorequest’, ’improvecredit score’, ’funfact’, ’changelanguage’, ’payday’,\n’replacement card duration’, ’time’, ’applicationstatus’, ’ﬂightstatus’, ’ﬂipcoin’,\n’change user name’, ’whereare you from’, ’shoppinglist update’, ’whatcan i ask you’,\n’maybe’, ’oilchange how’, ’restaurantreservation’, ’balance’, ’conﬁrmreservation’,\n’freeze account’, ’rollover401k’, ’whomade you’, ’distance’, ’username’, ’timezone’,\n’next song’, ’transactions’, ’restaurantsuggestion’, ’rewardsbalance’, ’paybill’,\n’spending history’, ’ptorequest status’, ’creditscore’, ’newcard’, ’lostluggage’, ’repeat’,\n’mpg’, ’oilchange when’, ’yes’, ’travelsuggestion’, ’insurance’, ’todolist update’, ’reminder’,\n’change speed’, ’tirepressure’, ’no’, ’apr’, ’nutritioninfo’, ’calendar’, ’uber’, ’calculator’, ’date’,\n’carry on’, ’ptoused’, ’schedulemaintenance’, ’travelnotiﬁcation’, ’syncdevice’, ’thankyou’,\n’roll dice’, ’foodlast’, ’cooktime’, ’reminderupdate’, ’reportlost card’, ’ingredientsubstitution’,\n’make call’, ’alarm’, ’todolist’, ’changeaccent’, ’w2’, ’billdue’, ’calories’, ’damagedcard’,\n’restaurant reviews’, ’routing’, ’doyou have pets’, ’schedulemeeting’, ’gastype’, ’plugtype’,\n’tire change’, ’exchangerate’, ’nextholiday’, ’changevolume’, ’whodo you work for’,\n’credit limit’, ’howbusy’, ’acceptreservations’, ’orderstatus’, ’pinchange’, ’goodbye’,\n’account blocked’, ’whatsong’, ’internationalfees’, ’lastmaintenance’, ’meetingschedule’,\n’ingredients list’, ’reportfraud’, ’measurementconversion’, ’smarthome’, ’bookhotel’,\n’current location’, ’weather’, ’taxes’, ’minpayment’, ’whispermode’, ’cancel’, ’internationalvisa’,\n’vaccines’, ’ptobalance’, ’directions’, ’spelling’, ’greeting’, ’resetsettings’, ’whatis your name’,\n’direct deposit’, ’interestrate’, ’creditlimit change’, ’whatare your hobbies’, ’bookﬂight’,\n’shopping list’, ’text’, ’billbalance’, ’sharelocation’, ’redeemrewards’, ’playmusic’,\n’calendar update’, ’areyou a bot’, ’gas’, ’expirationdate’, ’updateplaylist’, ’cancelreservation’,\n’tell joke’, ’changeai name’, ’howold are you’, ’carrental’, ’jumpstart’, ’mealsuggestion’,\n’recipe’, ’income’, ’order’, ’trafﬁc’, ’orderchecks’, ’carddeclined’, ’oos’\nTable 4: OOS intent\nName # Dialogue # Utterance Avg. Turn\nMetaLWOZ 37,884 432,036 11.4\nSchema 22,825 463,284 20.3\nTaskmaster 13,215 303,066 22.9\nMWOZ 10,420 71,410 6.9\nMSR-E2E 10,087 74,686 7.4\nSMD 3,031 15,928 5.3\nFrames 1,369 19,986 14.6\nWOZ 1,200 5,012 4.2\nCamRest676 676 2,744 4.1\nTable 5: The data statistics is from Wu et al. (2020).\n5047\n(a) MWOZ Domain (User)\n (b) MWOZ Domain (Sys)\n (c) OOS Intent (User)\n(d) MWOZ Slot - Set (User)\n (e) MWOZ Slot - Set (Sys)\n (f) MWOZ Act - Set (Sys)\nFigure 6: The ANMI evaluation of pre-pretrained models with domain, intent, slot, and action labels using GMM.\n(Best view in color)\nFigure 7: The tSNE visualization of dialogue representations from the ToD-BERT-jnt. (Best view in color.)\n5048\nFigure 8: The tSNE visualization of dialogue representations from the ConveRT. (Best view in color)\nFigure 9: The tSNE visualization of dialogue representations from the DistilBERT. (Best view in color.)\nFigure 10: The tSNE visualization of dialogue representations from the ToD-GPT. (Best view in color.)\n5049\nFigure 11: The tSNE visualization of dialogue representations from the ELECTRA-Dis. (Best view in color.)\nFigure 12: The tSNE visualization of dialogue representations from the ELECTRA-Dis. (Best view in color.)\nFigure 13: The tSNE visualization of dialogue representations from the RoBERTa. (Best view in color.)\n5050\nFigure 14: The tSNE visualization of dialogue representations from the DialoGPT. (Best view in color.)\nFigure 15: The tSNE visualization of dialogue representations from the AlBERT. (Best view in color.)\nTOD-BERT-jnt\nCluster 1(Restaurant Request)\ni have many options available for you ! is there a certain area or cuisine that interests you ?there are 21 restaurant -s available in the centre of town . how about a speciﬁc type of cuisine ?do you have any speciﬁc type of food you would like ?there 33 place -s that ﬁt your criteria . do you have a particular cuisine type in mind so that i can narrow the results down ?is there a particular cuisine you are looking for ?\nCluster 2(Taxi/Train)\nwhat time do you want to leave and what time do you want to arrive by ?do you have a time preference ?when would you like to leave and arrive ?what time would you like to leave the junction ?wonderful , i can help you . what time on sunday would you like to depart ?\nCluster 3(Attraction Recommend)\ni can recommend the allenbell . it s in the east , is cheap yet has a 4 star rating and free wiﬁ and parking . can i help you book ?the university arms is an expensive , 4 star hotel with free wiﬁ . comparatively , the alexander bed and breakfast is a cheap -ly priced guesthouse , also 4 stars .i have found the guesthouse you were wanting . would you like me to book this for you ?how about the express by holiday inn cambridge , it s in the east .the expensive 1 is actually not much more than the other 2 . i would highly recommend it . that would be at the express by holiday inn cambridge . it s in the east .\nCluster 4(Hotel Inform)\nthe address is hills road city centretheir address is unit g6 , cambridge leisure park , clifton road . the postcode is cb17dy .the address is corn exchange street . is there anything else i can help you with ?yes , the phone number is 01223277977 . the address is hotel felix whitehouse lane huntingdon road , and the post code is cb30lx . want to book ?the bridge guest house is at 151 hills road and their number is 01223247942 .\nCluster 5(Welcome/End)\nyou are welcome . is there anything else i can help you with today ?great . is there anything else that you need help with ?is there anything else that you would like ?no problem . can i help you with anything else ?is there something else i can help you with then ?\nTable 6: Clustering results of the TOD-BERT-jnt model. The samples are randomly picked from each randomly\nselected ﬁve clusters (using K=32).\n5051\nGPT2\nCluster 1\nthere are 9 indian restaurant -s in centre what price range do you want ?do you have any speciﬁc type of food you would like ?105 minutes is the total travel time . can i help you with anything else ?there are lots to choose from under that criteria . what day would you like to travel on ?i have the cote in the centre . it is in the expensive range . would you like to make a booking ?\nCluster 2\nyour reference number is x5ny66zv .i booked tr3932 , reference number is ﬁw5abo2 .nusha is in the south , and the phone number is 01223902158 .they are located at 12 lensﬁeld road city centre , postcode cb21eg , and phone number 01842753771 .it would cost 16.50 pounds .\nCluster 3\ni hope i have been of helpthe entrance fee is free . anything else i can do for you today ?sure , lookout for a blue volvo the contact number is 07941424083 . can i help with anything else ?1 moment while i try to make the reservation of table for 8 , friday at 16:30 .i have 3 options for you 2 in the north in the moderate price range and 1 that s expensive in the east .\nCluster 4\nwhen would you like to leave and arrive ?booking was unsuccessful . can you try another time slot ?on what day will you be traveling ?tr3823 will arrive at 16:55 , would that work for you ?okay , what day did you have in mind ?\nCluster 5\nsaffron brasserie is an expensive restaurant that serves italian foodthere are 21 restaurant -s available in the centre of town . how about a speciﬁc type of cuisine ?i have 5 different restaurant -s to choose from . there are 4 in the centre of town , and 1 in the west . do you have a preference ?i have about 5 different entertainment venue -s if that is what you are looking for . do you have a preference on the area its located in ?there are no colleges close to the area you are requesting , would you like to chose another destination ?\nTable 7: Clustering results of the GPT2 model. The samples are randomly picked from each randomly selected\nﬁve clusters (using K=32).\nDialoGPT\nCluster 1\nit is located in jesus laneyour booking was successful , the reference number is waeyaq0m . may i assist you with anything else today ?your booking is successful ! your reference number is iigra0mi . do you need anything else ?1 moment while i try to make the reservation of table for 8 , friday at 16:30 .this booking is successful for 1 night . your reference number is 85bgkwo4 . is there anything else i can assist you with ?\nCluster 2\nsure , how many days and how many people ?i recommend castle galleries and it s free to get in !i have plenty of trains departing from leicester , what destination did you have in mind ?i have 5 colleges in the centre area . what speciﬁc college are you looking for ?oh yes quite a few . which part of town will you be dining in ?\nCluster 3\ni have many options available for you ! is there a certain area or cuisine that interests you ?there are lots to choose from under that criteria . what day would you like to travel on ?actually all 5 have free wiﬁ . what star rating would you like ?i have found the guesthouse you were wanting . would you like me to book this for you ?yes , the hamilton lodge has internet .\nCluster 4\nits entrance fee is free .sure , lookout for a blue volvo the contact number is 07941424083 . can i help with anything else ?how many people is the reservation for ?how about train tr3934 ? it leaves at 12:34 and arrives at 13:24 . travel time is 50 minutes .sure , the phone number is 01223902112 and they are in postcode cb58sx . can i help you with anything else today ?\nCluster 5\nyes i can . what restaurant are you looking for ?what time would you like to leave the junction ?no problem . can i help you with anything else ?you are welcome . is there anything else i can help you with today ?is there anything else i can help you with ?\nTable 8: Clustering results of the DialoGPT model. The samples are randomly picked from each randomly selected\nﬁve clusters (using K=32).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8233863711357117
    },
    {
      "name": "Mutual information",
      "score": 0.7275151610374451
    },
    {
      "name": "Artificial intelligence",
      "score": 0.696619987487793
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6942271590232849
    },
    {
      "name": "Cluster analysis",
      "score": 0.6916157603263855
    },
    {
      "name": "Language model",
      "score": 0.5510318279266357
    },
    {
      "name": "Representation (politics)",
      "score": 0.5030996203422546
    },
    {
      "name": "Natural language processing",
      "score": 0.47534796595573425
    },
    {
      "name": "Task (project management)",
      "score": 0.4688884913921356
    },
    {
      "name": "Machine learning",
      "score": 0.46676915884017944
    },
    {
      "name": "Unsupervised learning",
      "score": 0.41571491956710815
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}