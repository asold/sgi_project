{
  "title": "Transformers with Learnable Activation Functions",
  "url": "https://openalex.org/W4386566428",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4367621994",
      "name": "Haishuo Fang",
      "affiliations": [
        "Hess (United States)",
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2122494202",
      "name": "Ji Ung Lee",
      "affiliations": [
        "Hess (United States)",
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A1966671982",
      "name": "Nafise Sadat Moosavi",
      "affiliations": [
        "Technical University of Darmstadt",
        "University of Sheffield",
        "Hess (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A40109512",
      "name": "Iryna Gurevych",
      "affiliations": [
        "Technical University of Darmstadt",
        "Hess (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2963582035",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3132956762",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W3106331693",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4206136559",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4294808066",
    "https://openalex.org/W4301581299",
    "https://openalex.org/W4205694016",
    "https://openalex.org/W3042896572",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3213305054",
    "https://openalex.org/W3092448486",
    "https://openalex.org/W4226102207",
    "https://openalex.org/W3035035250",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4287815523",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3038944566",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4283791586",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4320930577",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2899675781",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4287855051",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W3128090102",
    "https://openalex.org/W2058568633"
  ],
  "abstract": "Activation functions can have a significant impact on reducing the topological complexity of input data and therefore, improving a model’s performance. However, the choice of activation functions is seldom discussed or explored in Transformer-based language models. As a common practice, commonly used activation functions like Gaussian Error Linear Unit (GELU) are chosen beforehand and then remain fixed from pre-training to fine-tuning. In this paper, we investigate the impact of activation functions on Transformer-based models by utilizing rational activation functions (RAFs). In contrast to fixed activation functions (FAF), RAFs are capable of learning the optimal activation functions from data. Our experiments show that the RAF-based Transformer model (RAFT) achieves a better performance than its FAF-based counterpart (). For instance, we find that RAFT outperforms on the GLUE benchmark by 5.71 points when using only 100 training examples and by 2.05 points on SQuAD with all available data. Analyzing the shapes of the learned RAFs further unveils that they vary across different layers and different tasks; opening a promising way to better analyze and understand large, pre-trained language models.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2382–2398\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nTransformers with Learnable Activation Functions\nHaishuo Fang1 Ji-Ung Lee1 Nafise Sadat Moosavi1,2 Iryna Gurevych1\n1Ubiquitous Knowledge Processing Lab (UKP Lab)\nDepartment of Computer Science and Hessian Center for AI (hessian.AI)\nTechnical University of Darmstadt\nwww.ukp.tu-darmstadt.de\n2Department of Computer Science, The University of Sheffield\nAbstract\nActivation functions can have a significant im-\npact on reducing the topological complexity of\ninput data and therefore, improving a model’s\nperformance. However, the choice of activa-\ntion functions is seldom discussed or explored\nin Transformer-based language models. As\na common practice, commonly used activa-\ntion functions like Gaussian Error Linear Unit\n(GELU) are chosen beforehand and then re-\nmain fixed from pre-training to fine-tuning. In\nthis paper, we investigate the impact of activa-\ntion functions on Transformer-based models by\nutilizing rational activation functions (RAFs).\nIn contrast to fixed activation functions (FAF),\nRAFs are capable of learning the optimal ac-\ntivation functions from data. Our experiments\nshow that the RAF-based Transformer model\n(RAFT) achieves a better performance than its\nFAF-based counterpart (FAFT). For instance,\nwe find that RAFT outperforms FAFT on the\nGLUE benchmark by 5.71 points when using\nonly 100 training examples and by 2.05 points\non SQuAD with all available data. Analyzing\nthe shapes of the learned RAFs further unveils\nthat they vary across different layers and dif-\nferent tasks; opening a promising way to better\nanalyze and understand large, pre-trained lan-\nguage models.1\n1 Introduction\nActivation functions introduce non-linearity and\nincrease neural networks’ representational capac-\nity, and therefore, play an essential role in design-\ning deep learning models (Nwankpa et al., 2018;\nSharma et al., 2020; Dubey et al., 2022). Naitzat\net al. (2020) explain the importance of activation\nfunctions by proposing to consider data as a topol-\nogy with its own shape. They empirically show that\nactivation functions accelerate the data topology\ntransformation through different layers of a neu-\nral network to simplify its complexity and make\n1Code, models, and datasplits are available on GitHub\nhttps://github.com/UKPLab/2022-RAFT.\nit linearly separable in the output space. Their ex-\nperiments show that choosing the right activation\nfunction can have a significant impact on the over-\nall performance.\nWhile any activation function can be used with\nTransformers (Vaswani et al., 2017), their choice\nis made before pre-training and remains fixed af-\nterwards. Hence, the inductive bias an activa-\ntion function imposes on the model cannot be ad-\njusted during pre-training or fine-tuning. As many\nTransformer-based models are pre-trained on a\nlarge amount of data, and changing the activation\nfunction for or during fine-tuning may negatively\nimpact the performance 2. Moreover, the simple\ncase of finding the optimal combination of kdiffer-\nent activation functions in ndifferent feedforward\nlayers results in kn possible combinations and be-\ncomes intractable; e.g., 531,441 experiments for\na 12-layer BERT model and three different acti-\nvation functions. As a result, most Transformer-\nbased pre-trained models adopt the GELU activa-\ntion function that has been initially used for the\nBERT model (Devlin et al., 2019).\nTo overcome the limitation of using a potentially\nsuboptimal activation function that remains fixed\nduring training, we propose to use a learnable ac-\ntivation function, namely, the rational activation\nfunction (RAF, Molina et al. 2020). The RAF is a\nuniversal function approximator that can approxi-\nmate any existing activation function. The advan-\ntage of using RAFs over fixed activation functions\n(FAF) such as ReLU or GELU, is that the model\ncan learn the optimal activation function from the\ndata during (pre)training without the need to con-\nsider the choice of activation function as an addi-\ntional dimension during hyperparameter tuning.3\n2In our preliminary experiments, the performance of BERT\nbecomes worse on downstream tasks when the activation func-\ntions are changed after pre-training.\n3Liu et al. (2019a) consider different activation functions\nduring Neural Architecture Search (Zoph and Le, 2017), but\nthis becomes quickly infeasible for compute-intensive experi-\n2382\nTo evaluate the effectiveness of RAFs, we pre-train\ntwo encoder-only Transformers using RAF and\nGELU respectively, within an academic budget.\nIn our experiments, we find that:\n• The RAF-based Transformer (RAFT) learns\ndifferent activation functions at different lay-\ners after pre-training with shapes that differ\nfrom frequently used activation functions.\n• During fine-tuning, RAFT outperforms its\nfixed activation function counterpart (FAFT)\non the general language understanding bench-\nmark (GLUE) and the SQuAD machine read-\ning comprehension dataset in various settings.\n• After fine-tuning, the learned RAFs of the top\nlayers are more task-specific and change the\nmost, which are corresponding to layer be-\nhaviors of Transformers according to prior\nwork (Mosbach et al., 2020; Merchant et al.,\n2020; Zhou and Srikumar, 2022). This pro-\nvides new opportunities to analyze language\nmodels with respect to their learned activation\nfunctions at different layers for different tasks.\n• RAFT boosts the performance when com-\nbined with a parameter-efficient fine-tuning\napproach, i.e., BitFit (Ben Zaken et al., 2022),\nwhich improves the model performance by\n3.08 points in full-data scenario.\n2 Related Work\nActivation functions. There exists various prede-\nfined activation functions such as Sigmoid, Hyper-\nbolic Tangent (Tanh), Rectified Linear Unit (ReLU,\nFukushima 1969), and Gaussian Error Linear Unit\n(GELU, Hendrycks and Gimpel 2016). There are\nalso approaches that leverage automatic search to\nobtain optimal combinations of several base acti-\nvation functions in a predefined search space (Ra-\nmachandran et al., 2018; Manessi and Rozza, 2018;\nSütfeld et al., 2020; Bingham and Miikkulainen,\n2022; Bingham et al., 2020). For instance, Ra-\nmachandran et al. (2018) discovered the Swish ac-\ntivation function by using this method. Bingham\net al. (2020) show that further extending the search\nspace using evolutionary algorithms can also lead\nto an improvement. Finally, several search-based\nworks investigate how to train a combination of a\nset of activation functions to better adapt to spe-\ncific tasks and architectures (Manessi and Rozza,\nments such as pre-training large language models.\n2018; Sütfeld et al., 2020; Bingham and Miikku-\nlainen, 2022). One substantial drawback of these\nsearch-based methods is that they are computation-\nally expensive. Especially for pre-trained language\nmodels where pre-training is costly, it is infeasible\nto perform a hyperparameter search for selecting\nthe best activation function (even more so their\ncombination). In contrast, the flexibility of ratio-\nnal activation functions (RAFs) allows them to be\ntrained along with the model parameters in an end-\nto-end fashion (Molina et al., 2020). Therefore,\nthey can learn the optimized activation function\nfrom data during training. RAFs have been suc-\ncessfully used in deep reinforcement learning for\nimproving plasticity (Delfosse et al., 2021), cell\ndetection models in biology (Prangemeier et al.,\n2020), and adapter architectures (Moosavi et al.,\n2022).\nModel Act. Funct.\nBERT (Devlin et al., 2019) GELU\nGPT-1 (Radford et al., 2018) GELU\nRoBERTa (Liu et al., 2019b) GELU\nXLNet (Yang et al., 2019) GELU\nALBERT (Lan et al., 2020) GELU\nGPT-2∗(Radford et al., 2019) GELU\nMegatron-LM (Shoeybi et al., 2019) GELU\nELECTRA+(Clark et al., 2020) GELU\nT5 (Raffel et al., 2020) ReLU\nT5v1.1 (Raffel et al., 2020) GeGLU\nDeBERTa+(He et al., 2021) GELU\nBART (Lewis et al., 2020) GELU\nGPT-3∗(Brown et al., 2020) GELU\nJurassic∗(Lieber et al., 2021) GELU\nGopher∗(Rae et al., 2021) GELU\nMegatron-Turing NLG∗(Smith et al., 2022) GELU\nChinchilla∗(Hoffmann et al., 2022) GELU\nCANINE+(Clark et al., 2022) GELU\nLaMBDA (Thoppilan et al., 2022) GeGLU\nOPT (Zhang et al., 2022) ReLU\nTable 1: Activation functions in different NLP Trans-\nformer models. Models marked by ∗ do not explic-\nitly state the activation function but refer to GPT-1 as\nthe base architecture (+ refers to BERT respectively).\nGeGLU is a variant that combines GELU and GLU.\nFrequently used activation functions in NLP.\nTable 1 shows a list of 20 different language models\nthat have been introduced after BERT. As we see,\nthe vast majority of the works (80%) use the GELU\nactivation function. Moreover, many works even\ndo not explicitly state the used activation function\n(45%). There are only a few works that investigate\nthe impact of activation functions on pre-trained\nTransformer models. So et al. (2021) leverage au-\ntomatic search methods to identify more efficient\nTransformer architectures. They find that a combi-\n2383\nnation of squared ReLU used in the feedforward\nnetwork (FFN) layer and a convolution layer added\nin self-attention can lead to a substantial boost in\nperformance. Shazeer (2020) replace the FFN in\nthe Transformer with a gated linear unit (GLU,\nDauphin et al. 2017) combined with different acti-\nvation functions and find a higher performance dur-\ning pre-training as well as on downstream tasks. In\nour work, we do not change the structure of FFNs\nand only replace activation functions in them.\nClosest to our work is the work by Moosavi\net al. (2022) who investigate the use of RAF in\nadapters (Houlsby et al., 2019); i.e., lightweight\nlayers that are added on top of pre-trained Trans-\nformer layers. They propose adaptable adapters\nthat consist of RAFs and learnable switches to se-\nlect a subset of adapter layers during training. They\nshow that using both RAFs and a fewer number of\nadapter layers results in considerable performance\ngains, especially in low-data settings. However,\nonly using RAF instead of ReLU does not result in\na considerable gain in their experiments. Further-\nmore, adapter layers are only added and updated\nduring fine-tuning, as a result using RAF in adapter\nlayers has a limited impact compared to already\napplying them for pre-training.\nIn this work, we show that using RAF in Trans-\nformer layers brings additional flexibility to the\nmodel to learn the optimized activation function\nfor each of its layers during training, and that this\nadditional flexibility benefits both pre-training and\nfine-tuning steps.\n3 RAFT: RAF-based Transformers\nWe adopt the BERT architecture (Devlin et al.,\n2019) where all activation functions in feed-\nforward layers Activation(W1X)W2 are replaced\nwith rational activation functions (illustrated in Ap-\npendix A). The equation of rational activation func-\ntion F(x) is as below:\nF(x) =P(x)\nQ(x) =\n∑m\nj=0 ajxj\n1 +|∑n\nk=0 bkxk| (1)\nWhere a and b are learnable parameters, and m\nand nare degrees of F(x), which decide the com-\nplexity and fitting ability of rational functions. Fol-\nlowing Molina et al. (2020), we use the safe PAU\nformulation that further stabilizes training.\nSelecting mand n. Similar to Taylor series, the\nhigher the degrees mand nare, the more precise\nis the approximation of rational functions. How-\never, indefinitely increasing the degrees also means\nadding more complexity and increasing training\ntime. The challenge is to find suitable degrees\nthat leads to rational functions with a strong fitting\nability while keeping their complexity as low as\npossible. As this is still an open question, we set\nthe search space of mand nto {4,5}, and evaluate\ntheir ability to approximate the GELU function in\nthe range of [-3,3]. Our results show that using\nm= 5and n= 4perfectly fits the GELU function\nwith a low complexity and thus, are adopted in this\nwork (cf. Figure 5, Appendix B). This matches\nthe findings in previous work (Telgarsky, 2017;\nMolina et al., 2020; Delfosse et al., 2021) as well.\nSo overall, each rational activation function adds\nnine parameters, resulting in a total of 108 addi-\ntional parameters in a 12-layer Transformer model\n(less than 0.000098% of its original parameters).\nThe weights of F(x) can further be initialized to\napproximate any existing activation functions. In\nour experiments, we initialize it with weights that\napproximate GELU.\n4 Pre-training\nTo evaluate the viability of RAFT, we pre-train\ntwo comparable Transformer models from scratch—\none using the common fixed GELU activation func-\ntion (FAFT), and another one using RAFs (RAFT).\nModel architecture. For our experiments, we\nuse a frequently considered model configuration\nand train 12 Transformer encoder layers with a\nhidden size of 768 and 12 attention heads (Devlin\net al., 2019; Liu et al., 2019b; Rae et al., 2021;\nZhang et al., 2022). The only difference between\nRAFT and FAFT is the use of RAFs instead of\nGELUs as activation functions.\nData. We use English Wikipedia as our pre-\ntraining data.4 The dataset consists of 3.8 ×109\ntokens from which we select 50k sentences con-\ntaining 6.4 ×106 tokens as the validation data.\nPre-training objective. Following RoBERTa\n(Liu et al., 2019b), we use dynamic masked lan-\nguage modeling (MLM) as our learning task and\nrandomly mask tokens in the input sentences at\neach step before feeding them into the model. We\nuse the same masking probabilities and mask 15%\nof the tokens with an 80% chance of replacing them\n4https://dumps.wikimedia.org\n2384\nModel Validation loss Validation PPL\nFAFT 1.645 5.18\nRAFT 1.611 5.00\nTable 2: Performance of the models on the validation\nset after pre-training.\nwith the [MASK] token, a 10% chance of replacing\nthem with a randomly selected different token, and\na 10% chance of not replacing them at all.\nTraining parameters. As our primary goal is to\nvalidate the effectiveness of RAFs in Transformers\nrather than releasing a RoBERTa-like model, we\nfocus on training two comparable models within a\nlimited training budget. Both models are optimized\nusing AdamW (Loshchilov and Hutter, 2019) with\nβ1 = 0.9, β2 = 0.98 and a weight decay of 0.01.\nThe learning rate lrθ is set to 7E-4 for both mod-\nels while the learning rate lrRAF for the RAF co-\nefficients is set to 5E-3. Both learning rates are\nwarmed up over the first 1% steps, then lrθ decays\nlinearly while lrRAF remains constant.5 The batch\nsize is set to 4096. Tuning hyperparameters during\npre-training is expensive, to conduct hyperparame-\nters tuning of both models with limited resources,\nwe follow up 24hour BERT (Izsak et al., 2021) to\npre-train the model for 23k steps equipped with\nvarious methods to accelerate training, including\nmixed-precision, sparse output prediction, fused\nlinear layer, and tied embeddings (Press and Wolf,\n2017). Detailed parameters and results of hyperpa-\nrameter tuning are provided in Appendix C. It takes\n∼16 hours for RAFT and ∼12 hours for FAFT us-\ning four A100 GPUs.\nResults. Table 2 shows the MLM validation\nlosses and validation perplexity of the best per-\nforming hyperparameter configuration for RAFT\nand FAFT. We observe that RAFT achieves a bit\nlower perplexity than FAFT during pre-training.\nThe learned RAFs vary across different layers af-\nter pre-training (cf. Figure 6, Appendix E). More\nanalysis is conducted in Section 6.\n5 Fine-tuning\nWe conduct experiments on the General Lan-\nguage Understanding Evaluation (GLUE) bench-\nmark (Wang et al., 2019) and SQuAD (Rajpurkar\n5We find in our preliminary experiments that a constant\nrational learning rate with warm up leads to better results.\net al., 2016) to see how well pre-trained RAFs can\nadapt to downstream tasks. Dataset descriptions\nare provided in Appendix D. We further investigate\nthe flexibility of the pre-trained RAFs by consid-\nering different training data sizes especially in a\nlow-data regime. We fine-tune RAFT in two differ-\nent settings:\n• RAFTfull: We fine-tune the whole model, i.e.,\nall model parameters including the RAFs.\n• RAFTfixed: We fix the pre-trained RAFs and\nonly tune the rest of the parameters.\n5.1 Evaluation on the GLUE Benchmark\nWe evaluate pre-trained models on GLUE bench-\nmark in different data settings: (a) the full-data\nscenario, and (b) two low-data scenarios when only\n100 or 300 labelled examples are available.\nExperimental Setup. We split 75% of the train-\ning dataset as the training set and use the remaining\n25% as the development set in the full-data sce-\nnario. Following previous works, we use the pro-\nvided development set as the test dataset. For our\nlow-data scenarios, we randomly sample 100 or\n300 examples with ten different random seeds and\nreport the average and standard deviation across\nall runs. For the full-data scenario, we report the\naverage and standard deviation of the results across\nsix runs with different random seeds. We use the\nsame evaluation metrics as proposed in the GLUE\nbenchmark; more specifically, for MRPC, QQP,\nand STSB, we use the average of the two corre-\nsponding metrics as the final score.6\nResults. Table 3 shows the performance of RAFT\nand FAFT on the GLUE benchmark. We observe\nthat on average, RAFT achieves consistent im-\nprovements in all data settings. We further find that\nespecially in the low-data scenarios, the flexible\nactivation functions of RAFT substantially outper-\nform their static GLUE counterparts of the FAFT\nmodel. For 100 examples, RAFT achieves better\nresults in seven out of eight tasks, outperforming\nFAFT by 5.31 points (RAFTfull) and 5.71 points\n(RAFTfixed) on average, respectively. While the\nperformance gap becomes smaller as the number\nof examples increases, the tendency remains the\nsame with an average performance gain of 0.98\n6Note that the full-data scenario is computationally more\nexpensive to run, but also more stable as the training instances\nexperience less variability.\n2385\nModel ColA SST2 MRPC QQP STSB MNLI-matched/mismatched QNLI RTE Avg.\nlow-data 100 examples1\nFAFT 1.88±2.27 71.02±5.61 74.88±0.23 55.19±5.96 57.57±8.32 32.86±1.50/32.92±1.46 53.34±3.2453.14±1.6748.07RAFTfull 4.38±3.273.28±3.95 75.89±1.39 62.65±2.8670.30±3.44 38.31±1.87/39.06±2.3563.58±3.7453.0±1.91 53.38RAFTfixed 7.25±4.7772.04±5.04 75.76±0.65 62.15±4.0971.39±3.56 39.3±1.60/40.4±1.7363.13±3.05 52.6±2.9953.78\nlow-data 300 examples1\nFAFT 13.12±5.29 77.67±3.0779.37±1.5666.63±1.35 76.70±1.89 43.74±2.20/45.33,2.29 69.17±2.25 55.45±2.66 58.58RAFTfull 12.36±5.07 78.22±2.10 77.84±1.0968.25±1.0179.77±2.3445.70±1.69/47.27±1.8671.92±1.10 54.70±2.26 59.56RAFTfixed 17.34±3.23 78.95±2.3376.97±0.96 68.20±0.7680.32±0.145.35±1.62/46.53±1.6372.07±1.56 55.78±2.72 60.17\nFull data2\nFAFT 43.18±1.52 89.2±0.63 86.42±1.37 88.08±0.0887.08±0.2180.92±0.21/81.78±0.2289.42±0.3862.22±1.35 78.70RAFTfull 45.84±1.4789.85±0.4587.21±0.54 88.27±0.1086.96±0.29 80.88±0.22/81.85±0.23 89.32±0.2064.44±2.49 79.40RAFTfixed 45.66±1.5590.06±0.7086.36±1.03 88.21±0.06 86.64±0.2481.10±0.22/82.06±0.2189.36±0.34 63.90±2.85 79.28\n1 Results are averaged over ten random seeds: 5309, 202206, 20220602, 2259, 49, 2022, 1046, 622, 320, 532 Results are averaged over six random seeds: 5309, 202206, 20220602, 2259, 49, 2022\nTable 3: The performance of RAFT and FAFT on the GLUE benchmark across different data sizes. RAFT full\nfine-tunes all model parameters including RAFs. RAFTfixed instead fixes the RAFs pre-training.\npoints (RAFTfull) and 1.59 points (RAFTfixed) for\n300 examples. In the full data scenario, RAFT\nstill outperforms FAFT by 0.7 (RAFTfull) and 0.58\n(RAFTfixed) points on average.\nOur experiments indicate that fixing the RAFs\nis a better choice for the GLUE benchmark in the\nlow-data scenarios. We conjecture that one reason\nfor this may be that the number of instances to tune\nall parameters of the model are insufficient. On\nthe contrary, we find that in the full-data scenario\ntuning RAFs can lead to better results. The increas-\ning number of instances especially benefit RAFs\nas they can better adapt to different downstream\ntasks and learn better features. We provide further\nanalysis in Section 6.\n5.2 Evaluation on SQuAD\nSimilar to GLUE, we evaluate models on SQuAD\nv1.1 in different data settings: (a) the full-data sce-\nnario, and (b) four low-data scenarios with 100,\n300, 500, and 1000 training examples.\nExperimental Setup. We split the official train-\ning data into separate training (75%) and develop-\nment sets (25%)7 and use the official development\nset as the test data. We evaluate the results by com-\nputing the F1 score over the word overlap of the\npredicted answer and the gold answer. The hyper-\nparameters search space is provided in Appendix C.\nResults. Table 4 shows our results of RAFT\nand FAFT. Compared to GLUE, that consists of\nsentence-level text matching tasks, SQuAD is a\nmore complex task in which the model needs to\ncomprehend a longer text sequence to predict an\nanswer span. The increased task difficulty is es-\npecially reflected in the low-data scenarios, as the\n7Again, we use the development set to identify the best\nperforming model across all epochs.\n100 examples1 300 examples1 500 examples1 1000 examples1 full data2\nFAFT 12.72±1.54 22.11±2.4626.46±1.42 34.58±1.68 72.33±0.23RAFTfull 11.81±0.95 19.49±2.0126.68±1.91 36.69±1.56 74.45±0.47RAFTfixed12.19±1.08 19.00±2.68 26.27±1.39 35.98±1.81 74.38±0.25\n1 Results are averaged over ten random seeds: 5309, 202206, 20220602, 2259, 49, 2022, 1046, 622, 320, 532 Results are averaged over six random seeds: 5309, 202206, 20220602, 2259, 49, 2022\nTable 4: Results of RAFTs and FAFT on SQuAD.\nValidation Loss Validation PPL.\nIdentity Divergent Divergent\nRELU 1.626 5.08\nGELU 1.611 5.00\nTable 5: Different initializations of RAF.\nperformances of both models are below 25 points\nwhen only 100 or 300 annotated examples are avail-\nable. As a result, when there are not enough an-\nnotated examples available to learn the task, the\nuse of RAFs instead of GELU is not beneficial for\nthe Transformer model. However, we again see\nthat RAFT outperforms the FAFT model as enough\ntraining examples become available.\nIn addition, we observe that tuning RAFs during\nfine-tuning (RAFTfull) is more beneficial compared\nto fixing RAFs (RAFTfixed) when the task is more\ncomplex. Considering our findings on the GLUE\nbenchmark, we conjecture that the task difficulty\nmay play an additional role besides the amount\nof available training data for the performance of\nRAFTfull vs. RAFTfixed; however, this remains to\nbe investigated in future work.\n6 Analysis\nImpact of RAF initialization. To investigate\nhow initialization affects the performance of RAFT,\nwe train RAFT models initialized with GELU,\nRELU, and the identity function. Other hyperpa-\nrameters are the same as those in section 4. Table 5\nshows the performance of different initialization\n2386\nSNLI Trivia QA\nverified-web verified-wiki\nFAFT 74.22±0.19 24.62±1.48 21.01±0.75\nRAFTfull 74.80±0.29 25.40±1.84 21.50±0.76\nRAFTfixed 74.76±0.25 25.40±1.25 21.78±0.87\nTable 6: Zero-shot performance of FAFT and RAFT.\nModels evaluated on SNLI are trained on MNLI. Results\non TriviaQA are based on models trained on SQuAD.\n(a) Pre-training\n (b) Fine-tuned on SQuAD\n(c) Fine-tuned on MNLI\n (d) Fine-tuned on SST2\nFigure 1: Rational activation functions of RAFT full\namong different layers after pre-training and fine-tuning.\nmethods during pre-training. As we can see, choos-\ning common activation functions such as ReLU\nor GELU leads to a similar performance while us-\ning the identity function for initialization leads to\ndivergence.\nZero-shot generalization. To investigate if the\nhigher performances of RAFT vs FAFT come from\noverfitting on the in-domain data, we conduct cross-\ndomain zero-shot experiments. We use the models\nthat have been fine-tuned on MNLI and SQuAD\nin the full-data scenario and evaluate them on the\nsame tasks but for different data, namely, SNLI\n(Bowman et al., 2015) and TriviaQA (Joshi et al.,\n2017), respectively. MNLI and SNLI are both\ndatasets that aim to evaluate natural language in-\nference while SQuAD and TriviaQA contain ex-\namples for evaluating reading comprehension in\ndifferent domains. Table 6 shows the results of our\nzero-shot evaluation. We observe that the increased\nflexibility and adaptivity of RAFT does not nega-\ntively impact its generalization capabilities. In fact,\nboth variants of RAFT consistently achieve better\nperformance than the corresponding FAFT model.\nVisualizing learned RAFs. Next, we analyze\nhow the shapes of RAFs change after pre-training\nand fine-tuning. First, we analyze the learned RAFs\nin different layers of RAFT after pre-training. As\nshown in Figure 1a, rational functions have differ-\nent shapes across different layers, none of which\nare similar to GELU, or other commonly used ac-\ntivation functions in Transformers (cf. Table 1).\nThis indicates that different layers may need dif-\nferent activation functions to achieve the optimal\nperformance. Moreover, we see that some features\nlike monotonicity that often are deemed to be good\nfor predefined activation functions are not neces-\nsary, which is in line with the findings of the Swish\nactivation function (Ramachandran et al., 2018).\nSecond, we analyze how the learned RAFs\nduring pre-training change after fine-tuning in\nRAFTfull. Figures 1b–1d show learned RAFs after\nfine-tuning RAFTfull on SQuAD, MNLI and SST2\ndatasets. We observe that some of the learned RAFs\ntrained on these three tasks differ from each other\nand the RAFs after pre-training. We further see\nthat several RAFs between both tasks have similar\nshapes but different slopes across many layers.\nTo better understand the behavior of learned\nRAFs after fine-tuning in different layers on various\ntasks, we plot RAFs from the same layer together\nacross all tasks. Figure 2 shows the learned RAFs\nin layer 1 (the bottom layer), layer 6, and layer 12\n(the top layer) after pre-training and fine-tuning on\ndifferent tasks. We observe that after fine-tuning,\nthe RAFs in the top layer are more task-specific\nand change the most, compared to those in bot-\ntom layers. This is in line with prior work that\nanalyzed the behavior of BERT layers during fine-\ntuning, which showed that higher layers exhibit\nmore changes compared to lower layers (Mosbach\net al., 2020; Merchant et al., 2020; Zhou and Sriku-\nmar, 2022). Our results confirm this finding from\nthe perspective of learned activation functions. It\nalso demonstrates that RAFs can self-adapt to dif-\nferent layers and tasks during fine-tuning. In addi-\ntion, an interesting observation is that the output\nranges of the RAFs of MNLI and QQP in the top\nlayer are very close to zero. The output of the FFN\nlayer Layernorm(FFN(x) +x) consists of two\nparts: the feedforward branch FFN(x) and the skip\nconnection branch x. The very small output of acti-\nvation functions may indicate that the FFN branch\nof the top layer does not contribute much to the\nfinal model performance on MNLI and QQP and\n2387\nFigure 2: Learned rational activation functions of RAFTfull in layers 1 (bottom), 6, and 12 (top) among different\ntasks.\nModel ColA SST2 MRPC QQP STSB MNLI-matched/mismatched QNLI RTE Avg.\nlow data 100 examples1\nBitFitFAFT 1.44±2.85 63.33±9.63 68.82±1.74 55.49±3.94 46.04±24.69 32.92±1.33/32.95±1.24 51.95±3.5052.20±2.8245.02BitFitfull 4.39±3.4176.49±1.9074.11±1.0461.53±3.09 50.41±20.20 33.75±1.38/33.81±1.3057.22±6.1550.83±2.7449.17BitFitfixed 6.25±3.6875.96±1.2474.71±0.3461.35±3.42 49.91±26.88 33.73±1.40/34.04±1.7153.19±4.02 51.63±2.26 48.97\nFull data1\nBitFitFAFT 37.75±1.26 87.80±0.67 82.94±1.2081.35±0.1359.29±33.0471.94±0.38/73.57±0.38 85.38±1.0755.89±1.70 70.66BitFitfull 38.46±1.37 88.19±0.1686.73±1.0081.03±0.12 85.28±0.33 70.23±0.41/72.53±0.33 80.51±10.7560.72±1.88 73.74BitFitfixed 39.96±1.95 88.46±0.2884.91±5.10 81.02±0.1485.55±0.4471.25±0.19/73.26±0.36 77.23±14.23 60.15±0.90 73.53\n1Results are averaged over five random seeds: 5309, 202206, 20220602, 2259, 49\nTable 7: Comparison between RAFT and FAFT combined with BitFit.\nthus could be pruned. We leave this as future work.\nRAFTfixed vs. RAFTfull. In our experiments on\nGLUE and SQuAD (Tables 3 and 4), we observe\nthat fixing the RAFs after fine-tuning (RAFTfixed)\noften achieves the best or second best performance\ncompared to the full-tuning model (RAFTfull) and\nFAFT. Fine-tuning RAFs results in higher perfor-\nmances when (a) more data is available, i.e., the\nfull-data scenario in GLUE, or (b) the input task is\nmore complex such as in SQuAD. We hypothesize\nthat training RAFs during fine-tuning will be more\neffective when evaluated on more complex tasks\nand datasets than the ones used this work.\nEfficiency comparison between RAFT and\nFAFT. In RAFT, RAFs are polynomial ratios and\ntheir coefficients are learned during training, which\nadds extra computation overhead. We use RAFs\nlibrary with CUDA extension to accelerate. As\nshown in Table 8, RAFT is slower than FAFT dur-\ning training since RAFs need to be updated (36.8%\nslower at pre-training, 14.8% slower at fine-tuning).\nHowever, RAFT is faster when doing inference due\nto the CUDA implementation (13.8% faster at pre-\ntraining, 3.9% faster at fine-tuning).\nParameter-efficient fine-tuning with RAFTs.\nIn contrast to fine-tuning all parameters in a\npre-trained language model, parameter-efficient\ntuning techniques that freeze the majority of\nsteps/second Pre-training Fine-tuning\nTrain Inference Train Inference\nRAFT 0.38 3.3 12.54 71.05\nFAFT 0.52 2.9 14.4 68.38\nTable 8: Number of steps per second for training and\ninference for RAFT and FAFT.\npre-trained parameters and only fine-tune a small\nset can be promising alternatives (Ding et al.,\n2022). One such method is BitFit (Ben Zaken\net al., 2022) which only updates the bias terms\nin the Transformer model. To investigate the\neffectiveness of RAFT in a parameter-efficient\nfine-tuning paradigm, we fine-tune the FAFT and\nRAFT models with BitFit on the GLUE bench-\nmark. We use the same settings as in our previous\nexperiments and test RAFT and FAFT in three\nconfigurations in the low-data 100 and full-data\nscenario: (a) BitFitFAFT uses BitFit with FAFT,\n(b) BitFitfull uses BitFit with RAFT full, and (c)\nBitFitfixed uses BitFit with RAFTfixed. As shown\nin Table 7, RAFT-based BitFit achieves higher\nperformance than the FAFT on average in both\ndata settings: BitFitfixed achieves 3.95 points\nimprovements and BitFitfull gets 4.15 points\nimprovements in the low-data scenario while\nBitFitfixed performs better with a 2.87 points\nboost and BitFitfull performs better with a 3.08\n2388\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000019/uni00000008/uni00000013/uni00000011/uni00000013/uni0000001c/uni00000008\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056\n/uni00000017/uni00000013\n/uni00000017/uni00000018\n/uni00000018/uni00000013\n/uni00000018/uni00000018\n/uni00000019/uni00000013\n/uni00000019/uni00000018\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018\n/uni0000001b/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000050/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000046/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002a/uni0000002f/uni00000038/uni00000028/uni00000003/uni00000045/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000050/uni00000044/uni00000055/uni0000004e\n/uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a\nRAFRAFT\nBitFitsub\nRAFT\nBitFitsub\nFAFT\nBitFitfull\nBitFitfixed\nBitFitFAFT\nRAFTfull\nRAFTfixed\nFAFT\n(a) Comparison performance in low-data 100 scenario\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000019/uni00000008/uni00000013/uni00000011/uni00000013/uni0000001c/uni00000008\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056\n/uni00000017/uni00000013\n/uni00000017/uni00000018\n/uni00000018/uni00000013\n/uni00000018/uni00000018\n/uni00000019/uni00000013\n/uni00000019/uni00000018\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018\n/uni0000001b/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000050/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000046/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002a/uni0000002f/uni00000038/uni00000028/uni00000003/uni00000045/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000050/uni00000044/uni00000055/uni0000004e\n/uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a\nRAFRAFT\nBitFitsub\nRAFT\nBitFitsub\nFAFT\nBitFitfull\nBitFitfixed\nBitFitFAFT\nRAFTfull\nRAFTfixed\nFAFT\n(b) Comparison performance in full-data scenario\nFigure 3: The number of parameters vs. the perfor-\nmance for fine-tuning of RAFT and FAFT.\npoints boost in the full-data scenario. It is worth\nnoting that in some tasks, the reported results have\na very large standard deviation (e.g., 33.04 for\nBitFitFAFT on STSB) due to several random seed\nruns not converging. In our experiments, BitFit is\nnot as stable as fine-tuning the whole model.\nHow much can we achieve by only fine-tuning\nRAFs? To see to what extent the model can\nlearn from different tasks by only updating RAFs,\nwe conduct experiments to only tune RAFs on\nthe GLUE benchmark in low- and full-data set-\ntings. We call this setup where only 117 8 param-\neters of the RAFs are updated during fine-tuning,\nRAFRAFT.\nFor comparison, we tune our models with the\nBitFit setting using the same amount of parameters,\n8Including RAF in the pooling layer for classification\ni.e., 117.9 BitFitsub\nFAFT represents tuning the sub-\nset of BitFit of FAFT, and BitFitsub\nRAFT represents\ntuning the subset of BitFit of RAFT. The result is\npresented in Appendix F (Table 13). To compare it\nfrom a broader view, we plot Figure 3 based on Ta-\nble 3, Table 7 and Table 13. We observe that if only\na few annotated examples are available (100 ex-\namples), BitFitfixed and BitFitfull can achieve\nbetter performance than full fine-tuning of FAFT.\nOnly fine-tuning 117 parameters ( BitFitsub\nFAFT,\nBitFitsub\nRAFT and RAFRAFT) —i.e., a negligible\nnumber of parameters compared to 110M parame-\nters in FAFT—results in a comparable performance\nas fine-tuning all the parameters with only a drop\nof 4.21–6.68 percentage points. In the full-data\nscenario, the performance of BitFit ( BitFitfull,\nBitFitfixed and BitFitFAFT ) lags behind full\nfine-tuning of both models. Only tuning RAFs\nor a subset of BitFit cannot achieve comparable re-\nsults as well. However, RAFRAFT outperforms\nBitFitsub\nFAFT by 7.8% and performs better than\nBitFitsub\nRAFT by 2.94% in this setting.\n7 Conclusion and Future Work\nIn this work, we propose to utilize rational activa-\ntion functions (RAF) in Transformers to directly\nlearn optimal activation functions from data dur-\ning pre-training and fine-tuning. To evaluate the\neffectiveness of rational activation functions, we\npre-trained a Transformer-based language model,\nnamely, RAFT. RAFT achieves a lower validation\nperplexity than FAFT during pre-training. Our ex-\nperimental results show that RAFT performs better\nthan FAFT in general language understanding tasks\nand reading comprehension tasks across different\ndata size scenarios. We further visualize and ana-\nlyze rational activation functions across different\nlayers and tasks after pre-training and fine-tuning\nand find that they can substantially vary across dif-\nferent layers and tasks. This provides us a new\nway to analyze and better understand Transformer-\nbased language models. For instance, we can inves-\ntigate whether layers with similar rational activa-\ntion functions encode similar linguistic properties.\nWe further find that some layers exhibit a close to\nzero throughput of the rational activation function\nwhich indicates that the corresponding feedforward\nlayer does not contribute too much to a model’s\nprediction. We consider these as our future work.\n9Note that we also update the classification head in all\nmodels and experiments.\n2389\nLimitations\nLimited training resources. This work evaluates\nthe effectiveness of rational activation Transform-\ners using limited GPU resources. To provide a\nfair comparison, we train and release RAF- and\nGELU-based models for a reduced GPU budget;\nhence, they are not comparable to publicly avail-\nable large pre-trained models such as RoBERTa-\nbase etc. Still, a fully pre-trained RAFT could be\nreleased once more GPU resources are available.\nWe furthermore note that we use GELU activation\nfunctions and the original FFN architecture as our\nbaseline as it is dominantly used in existing models.\nSocietal impact. The main focus of this work\nis the evaluation of trainable activation functions.\nWhile our visualization of the learned activation\nfunctions show that they exhibit substantial differ-\nences depending on the downstream task, further\nanalysis is necessary to better understand and in-\nterpret the shapes. Moreover, it is unclear if the\nadditional flexibility of the models may increase\ntheir susceptibility towards capturing biases in the\ndata. At the same time, we conjecture that espe-\ncially susceptible models could also be used as\ngood indicators to detect such biases.\nAcknowledgements\nWe thank Quentin Delfosse for his continued sup-\nport and valuable advice regarding the existing im-\nplementation of rational activation functions. We\nfurther thank our anonymous reviewers and Stella\nBiderman, Fengyu Cai, Nils Dycke, Haau-Sing Li,\nAndreas Rücklé, Martin Tutek, Kexin Wang, and\nNeha Warikoo for their fruitful discussions and\nhelpful feedback. This work has been funded by\nthe German Research Foundation (DFG) as part\nof the UKP-SQuARE project (grant GU 798/29-\n1), the German Federal Ministry of Education and\nResearch and the Hessian Ministry of Higher Ed-\nucation, Research, Science and the Arts within\ntheir joint support of the National Research Cen-\nter for Applied Cybersecurity ATHENE and the\nhessian.AI Service Center.\nReferences\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. BitFit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 1–9, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nGarrett Bingham, William Macke, and Risto Miikku-\nlainen. 2020. Evolutionary optimization of deep\nlearning activation functions. In Proceedings of the\n2020 Genetic and Evolutionary Computation Confer-\nence, GECCO ’20, page 289–296, New York, NY ,\nUSA. Association for Computing Machinery.\nGarrett Bingham and Risto Miikkulainen. 2022. Dis-\ncovering parametric activation functions. Neural Net-\nworks, 148:48–65.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2015, Lisbon, Portugal, September 17-21,\n2015, pages 632–642. The Association for Computa-\ntional Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2022. Canine: Pre-training an efficient\ntokenization-free encoder for language representa-\ntion. Transactions of the Association for Computa-\ntional Linguistics, 10:73–91.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges, Eval-\nuating Predictive Uncertainty, Visual Object Classi-\nfication and Recognizing Textual Entailment, First\n2390\nPASCAL Machine Learning Challenges Workshop,\nMLCW 2005, Southampton, UK, April 11-13, 2005,\nRevised Selected Papers, volume 3944 of Lecture\nNotes in Computer Science, pages 177–190. Springer.\nYann N. Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In Proceedings of the 34th In-\nternational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017 ,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 933–941. PMLR.\nQuentin Delfosse, Patrick Schramowski, Alejandro\nMolina, and Kristian Kersting. 2021. Recurrent ra-\ntional networks. CoRR, abs/2102.09407.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juanzi Li, and Maosong\nSun. 2022. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language\nmodels. CoRR, abs/2203.06904.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing, IWP@IJCNLP 2005, Jeju Island,\nKorea, October 2005, 2005. Asian Federation of Nat-\nural Language Processing.\nShiv Ram Dubey, Satish Kumar Singh, and\nBidyut Baran Chaudhuri. 2022. Activation\nfunctions in deep learning: A comprehensive survey\nand benchmark. Neurocomputing, 503:92–108.\nKunihiko Fukushima. 1969. Visual feature extraction by\na multilayered network of analog threshold elements.\nIEEE Trans. Syst. Sci. Cybern., 5(4):322–333.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging non-\nlinearities and stochastic regularizers with gaussian\nerror linear units. CoRR, abs/1606.08415.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. CoRR,\nabs/2203.15556.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nProceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nPeter Izsak, Moshe Berchansky, and Omer Levy. 2021.\nHow to train BERT with an academic budget. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 10644–\n10652, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nPrinciples of Knowledge Representation and Rea-\nsoning: Proceedings of the Thirteenth International\nConference, KR 2012, Rome, Italy, June 10-14, 2012.\nAAAI Press.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.\n2021. Jurassic-1: Technical details and evaluation.\nWhite Paper. AI21 Labs.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2019a. DARTS: differentiable architecture search.\nIn 7th International Conference on Learning Repre-\nsentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n2391\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nFranco Manessi and Alessandro Rozza. 2018. Learning\ncombinations of activation functions. In 24th Inter-\nnational Conference on Pattern Recognition, ICPR\n2018, Beijing, China, August 20-24, 2018, pages 61–\n66. IEEE Computer Society.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and\nIan Tenney. 2020. What happens to BERT embed-\ndings during fine-tuning? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 33–44,\nOnline. Association for Computational Linguistics.\nAlejandro Molina, Patrick Schramowski, and Kristian\nKersting. 2020. Padé activation units: End-to-end\nlearning of flexible activation functions in deep net-\nworks. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nNafise Moosavi, Quentin Delfosse, Kristian Kersting,\nand Iryna Gurevych. 2022. Adaptable adapters. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3742–3753, Seattle, United States. Association\nfor Computational Linguistics.\nMarius Mosbach, Anna Khokhlova, Michael A. Hed-\nderich, and Dietrich Klakow. 2020. On the interplay\nbetween fine-tuning and sentence-level probing for\nlinguistic knowledge in pre-trained transformers. In\nProceedings of the Third BlackboxNLP Workshop on\nAnalyzing and Interpreting Neural Networks for NLP,\npages 68–82, Online. Association for Computational\nLinguistics.\nGregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim.\n2020. Topology of deep neural networks. J. Mach.\nLearn. Res., 21:184:1–184:40.\nChigozie Nwankpa, Winifred Ijomah, Anthony Gacha-\ngan, and Stephen Marshall. 2018. Activation func-\ntions: Comparison of trends in practice and research\nfor deep learning. CoRR, abs/1811.03378.\nTim Prangemeier, Christoph Reich, and Heinz Koeppl.\n2020. Attention-based transformers for instance\nsegmentation of cells in microstructures. In IEEE\nInternational Conference on Bioinformatics and\nBiomedicine, BIBM 2020, Virtual Event, South Korea,\nDecember 16-19, 2020, pages 700–707. IEEE.\nOfir Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. InProceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 157–163, Valencia, Spain.\nAssociation for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\nCoRR, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nPrajit Ramachandran, Barret Zoph, and Quoc V . Le.\n2018. Searching for activation functions. In 6th\nInternational Conference on Learning Representa-\ntions, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Workshop Track Proceedings. OpenRe-\nview.net.\nSiddharth Sharma, Simone Sharma, and Anidhya\nAthaiya. 2020. Activation functions in neural net-\nworks. International Journal of Engineering Applied\nSciences and Technology, 04:310–316.\nNoam Shazeer. 2020. GLU variants improve trans-\nformer. CoRR, abs/2002.05202.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\nCoRR, abs/1909.08053.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zheng, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing NLG 530b, A large-scale genera-\ntive language model. CoRR, abs/2201.11990.\n2392\nDavid R. So, Wojciech Manke, Hanxiao Liu, Zihang\nDai, Noam Shazeer, and Quoc V . Le. 2021. Search-\ning for efficient transformers for language modeling.\nIn Advances in Neural Information Processing Sys-\ntems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 6010–6022.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2013, 18-21 October 2013, Grand Hyatt\nSeattle, Seattle, Washington, USA, A meeting of SIG-\nDAT, a Special Interest Group of the ACL , pages\n1631–1642. ACL.\nLeon René Sütfeld, Flemming Brieger, Holger Finger,\nSonja Füllhase, and Gordon Pipa. 2020. Adaptive\nblending units: Trainable activation functions for\ndeep neural networks. In Intelligent Computing - Pro-\nceedings of the 2020 Computing Conference, Volume\n3, volume 1230 of Advances in Intelligent Systems\nand Computing, pages 37–50. Springer.\nMatus Telgarsky. 2017. Neural networks and rational\nfunctions. In Proceedings of the 34th International\nConference on Machine Learning, ICML 2017, Syd-\nney, NSW, Australia, 6-11 August 2017, volume 70 of\nProceedings of Machine Learning Research, pages\n3387–3393. PMLR.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. CoRR, abs/2201.08239.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang,\nDongdong Zhang, and Furu Wei. 2022. Deep-\nnet: Scaling transformers to 1, 000 layers. CoRR,\nabs/2203.00555.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754–5764.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\nCoRR, abs/2205.01068.\nYichu Zhou and Vivek Srikumar. 2022. A closer look\nat how fine-tuning changes BERT. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1046–1061, Dublin, Ireland. Association for\nComputational Linguistics.\nBarret Zoph and Quoc V . Le. 2017. Neural architecture\nsearch with reinforcement learning. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net.\nA Model Architecture\nFigure 4 shows the difference part of RAFT and\nFAFT.\nFigure 4: Rational activation function in the feed-\nforward layer (left) and the vanilla GELU counterpart\n(right).\n2393\nB Fitting abilities of different degrees of\nRational Functions\nFigure 5 show the approximate functions of GELU\nusing rational functions with different degrees. As\nwe can see, when m = 5 and n = 4 or n = 5,\nrational function fit GELU very well in the same\nshape. Finally, it is important to note that rational\nfunctions are an universal approximator in a lim-\nited range, e.g., [-5,5]. Especially for out-of-bound\ninputs (i.e., values that are not guaranteed by ra-\ntional functions), the output of rational functions\nmay result in values very different from the approx-\nimated function (e.g., GELU). While pre-training a\nmodel from scratch with RAFs does not lead to any\nproblem, directly replacing activation functions in\npre-trained models with RAFs only for fine-tuning\nmay lead to divergence due to out-of-bound inputs.\nC Hyperparameters Tuning\nC.1 Pre-training\nIn our preliminary experiments that some hyperpa-\nrameter configurations can lead to instability dur-\ning training due to diverging model updates (e.g.,\nfor lrθ =7E-4 and batch size of 2048). To stabi-\nlize the training without having to rely on a larger\nwarmup phase (e.g., 6% of the training steps), we\ninstead adopt the DeepNorm (Wang et al., 2022)\nto initialize both models. DeepNorm stabilizes\ntraining by bounding the updates and further scal-\ning the residual branches in Transformers. Using\nDeepNorm makes both models, FAFT and RAFT,\nachieve lower validation loss and leads to a more\nstable training.\nWe tune the learning rate lrθ for model parame-\nters and lrRAF for RAFs, batch size, warmup steps,\nand learning rate scheduler as hyperparameters for\nboth models separately. The hyperparameter search\nspace for pre-training stage is as follows:\n• Learning rate lrθ for model parameters: 1E-4,\n4E-4, 7E-4, 1E-3\n• Learning rate lrRAF for RAFs: 1E-3, 5E-3,\n1E-2\n• Batch size: 2048, 4096\n• Warmup ratio: 0%, 1%, 6%\nSome results of hyperparameters tuning are pro-\nvided in Table 9.\nTable 10 shows final hyperparameters we used\nfor pre-training RAFT and FAFT.\nlrθ lrRAF Batch Size Validation Loss\nRAFT 1E-4 0.005 2048 2.217\nRAFT 4E-4 0.005 2048 1.808\nRAFT 7E-4 0.005 4096 1.732\nRAFT 7E-4 0.005 4096 1.611\nRAFT 1E-3 0.005 4096 1.638\nTable 9: Part of Hyperparameters Tuning Results of\nRAFT\nHyperparameters FAFT RAFT\nPeak lrθ 7E-4 7E-4\nPeak lrRAF n/a 5E-3\nLearning rate decay linear constant\nGradient clipping 0 0\nBatch size 4096 4096\nSequence length 128 128\nAdam_beta1 0.9 0.9\nAdam_beta2 0.98 0.98\nAttention dropout 0.1 0.1\nWarmup ratio 1% 1%\nTraining steps 23k 23k\nTable 10: Hyperparameters for pre-training RAFT and\nFAFT\nC.2 Fine-tuning\nThe hyperparameters search space for GLUE dur-\ning fine-tuning stage is as follows:\n• lrθ: 2E-5, 5E-5\n• lrRAF: 1E-4, 5E-4, 1E-3, 5E-3\n• Batch size: 32\n• Weight decay: 0.1\n• Number of epochs: 3, 10, 20\nWe further tune the learning rates and number of\ntraining epochs for RAFT and FAFT separately\non a single random seed. For our low-data experi-\nments we fix the number of training epochs to 20\nand use early stopping with a patience of 10 epochs.\nFor our full-data experiments, we train the large\ndatasets (QQP, MNLI, and QNLI) for 3 epochs and\nthe others for 10 epochs.\nThe hyperparameters search space for SQuAD\nduring fine-tuning is as below:\n• lrθ: 2E-5, 5E-5, 1E-4\n2394\n(a) Approximate function with degrees m = 4and n = 4\n (b) Approximate function with degrees m = 4and n = 5\n(c) Approximate function with degrees m = 5 and n = 4\nRational Function is overlapping with GELU\n(d) Approximate function with degrees m = 5 and n = 5\nRational Function is overlapping with GELU\nFigure 5: Approximate Functions of GELU using rational functions\n• lrRAF: 1E-4, 5E-4, 1E-3, 5E-3\n• Batch size: 32\n• Weight decay: 0.1\n• Number of epochs: 10, 20\nFor our experiments, we fine-tune both models with\ntheir best performing lrθ =1E-4 for 10 epochs in\nthe full-data scenario and 20 epochs in the low-data\nscenario.\nThe hyperparameters search space for BitFit is\nas below:\n• Learning rate lrθ for model parameters: 5E-5,\n1E-3, 5E-3, 1E-2\n• Learning rate lrRAF for RAFs: 1E-3, 5E-3,\n1E-2\n• Batch size: 32\n• Training epochs: 3, 10, 20 epochs\nWe use 3 training epochs for large dataset(QQP,\nMNLI, QNLI), 10 epochs for other datasets and 20\nepochs for low-resource scenarios. Both models\ncan converge in the above settings.\nD Data Statistics\nGLUE is a collection of nine different language\nunderstanding tasks: CoLA (Warstadt et al., 2019),\nSST2 (Socher et al., 2013), MRPC (Dolan and\nBrockett, 2005), QQP 10, STSB (Cer et al., 2017),\nMNLI (Williams et al., 2018), RTE (Dagan et al.,\n2005), and WNLI (Levesque et al., 2012). We\nexclude WNLI due to the adversarial nature of its\ndevelopment set and the still unbeaten majority\nvote upper bound.11\nTable 11 show data statistics of GLUE bench-\nmark.\n10https://quoradata.quora.com/First-Quo\nra-Dataset-Release-Question-Pairs\n11Cf. (12) in https://gluebenchmark.com/faq\n2395\nTask CoLA SST2 MRPC QQP STSB MNLI-matched/mismatched QNLI RTE\n|Train| 8,551 67,349 3,668 363,846 5,749 392,702 104,743 2,490\n|Dev| 1,043 872 408 40,430 1,500 9,815/9,832 5,463 277\nMetric Matthews corr. acc. acc./F1 acc./F1 Person/Spearman corr. acc. acc. acc.\nTable 11: Dataset statistics of the GLUE benchmark\nSQuAD is a reading comprehension task where\neach example consists of a question, a context,\nand the respective span from the context that an-\nswers the question. Table 12 show data statistics of\nSQuAD.\nE Learned RAFs during pre-training and\nafter fine-tuning\nFigure 6 and Figure 7 show learned RAFs in 12\nlayers after pre-training and fine-tuning on different\ntasks, respectively.\nF Results of only tuning RAFs\nTable 13 shows comparison results between only\ntuning RAFs and BitFit with the same parameters\nwith RAFT and FAFT.\n2396\n|Train| |Dev| | Test|\nSQuAD v1.1 66,236 21,530 10,789\nTable 12: Statistics of SQuAD: the official training dataset is split into training and development sets, and the official\ndevelopment dataset is used as the test data.\nFigure 6: Learned RAFs of different layers after pre-training\nModel ColA SST2 MRPC QQP STSB MNLI-matched/mismatched QNLI RTE Avg.\nlow data 100 examples1\nBitFitsubFAFT 1.49±1.87 62.82±7.5674.80±0.0052.57±3.83 14.71±7.21 32.73±1.41/32.76±1.30 49.77±0.4050.83±1.8641.39BitFitsubRAFT 2.45±3.58 72.34±3.41 74.67±0.6855.61±2.35 23.99±10.41 35.32±0.67/35.66±1.05 51.08±0.71 51.70±1.8544.75RAFRAFT 4.33±3.02 72.91±2.8274.47±0.88 51.92±5.03 17.27±10.60 35.24±0.61/35.69±0.9251.12±0.48 50.47±1.63 43.71\nFull data1\nBitFitsubFAFT 6.61±7.08 79.52±0.52 71.32±0.22 70.48±0.66 37.33±5.70 53.33±1.13/55.30±0.75 64.04±2.03 54.88±1.42 54.76BitFitsubRAFT 8.78±5.5482.02±0.5771.76±0.77 70.88±1.17 71.40±0.52 51.57±0.54/53.27±1.20 69.87±1.2057.04±1.1959.62RAFRAFT 9.71±12.0481.70±0.1274.81±3.09 73.57±0.48 80.79±0.60 57.34±0.19/60.69±0.5167.89±8.64 56.53±1.8362.56\n1Results are averaged over five random seeds: 5309, 202206, 20220602, 2259, 49\nTable 13: Comparison between fine-tuning RAFs and a subset of 117 BitFit parameters with RAFT and FAFT.\n2397\nFigure 7: Learned RAFs in 12 layers across different tasks after fine-tuning\n2398",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7256640195846558
    },
    {
      "name": "Transformer",
      "score": 0.7000536322593689
    },
    {
      "name": "Rational function",
      "score": 0.4493376612663269
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.43887773156166077
    },
    {
      "name": "Gaussian",
      "score": 0.4274599850177765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.341383695602417
    },
    {
      "name": "Mathematics",
      "score": 0.1600005328655243
    },
    {
      "name": "Voltage",
      "score": 0.10155287384986877
    },
    {
      "name": "Engineering",
      "score": 0.08959439396858215
    },
    {
      "name": "Physics",
      "score": 0.07816609740257263
    },
    {
      "name": "Electrical engineering",
      "score": 0.07409775257110596
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}