{
    "title": "Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT",
    "url": "https://openalex.org/W3197138502",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2798548111",
            "name": "Elena Voita",
            "affiliations": [
                "University of Edinburgh",
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A93200637",
            "name": "Rico Sennrich",
            "affiliations": [
                "University of Edinburgh",
                "University of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2127391507",
            "name": "Ivan Titov",
            "affiliations": [
                "University of Edinburgh",
                "University of Amsterdam"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3035463087",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2988975212",
        "https://openalex.org/W2963736842",
        "https://openalex.org/W2469296930",
        "https://openalex.org/W2597891111",
        "https://openalex.org/W3211259717",
        "https://openalex.org/W2888779557",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2148708890",
        "https://openalex.org/W2949193663",
        "https://openalex.org/W2962834107",
        "https://openalex.org/W2964222566",
        "https://openalex.org/W2952682849",
        "https://openalex.org/W2963260202",
        "https://openalex.org/W2971347700",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W3036528584",
        "https://openalex.org/W2979202502",
        "https://openalex.org/W3100924069",
        "https://openalex.org/W2970045405",
        "https://openalex.org/W3173506780",
        "https://openalex.org/W2413436069",
        "https://openalex.org/W2006832571",
        "https://openalex.org/W2962708992",
        "https://openalex.org/W2576713500",
        "https://openalex.org/W4298170715",
        "https://openalex.org/W3104881680",
        "https://openalex.org/W2962945603",
        "https://openalex.org/W2765271678",
        "https://openalex.org/W2998135987",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W2964174820",
        "https://openalex.org/W2995999067",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2963434219",
        "https://openalex.org/W2963333747",
        "https://openalex.org/W1915251500",
        "https://openalex.org/W2566564022",
        "https://openalex.org/W2566623769",
        "https://openalex.org/W3014432698",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4239181501",
        "https://openalex.org/W4297801368",
        "https://openalex.org/W2915573484",
        "https://openalex.org/W2963366552",
        "https://openalex.org/W2952446148",
        "https://openalex.org/W2995186722",
        "https://openalex.org/W3016970897",
        "https://openalex.org/W2963598809",
        "https://openalex.org/W2982713930",
        "https://openalex.org/W2950513705",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W1985514943",
        "https://openalex.org/W2516649778",
        "https://openalex.org/W2964334713",
        "https://openalex.org/W2902614977",
        "https://openalex.org/W2757154661",
        "https://openalex.org/W2955926951",
        "https://openalex.org/W2099224638",
        "https://openalex.org/W2963593215",
        "https://openalex.org/W2803023299",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W2963174344",
        "https://openalex.org/W3030128163",
        "https://openalex.org/W1787224781",
        "https://openalex.org/W2963699608",
        "https://openalex.org/W2748679025"
    ],
    "abstract": "Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8478–8491\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n8478\nLanguage Modeling, Lexical Translation, Reordering:\nThe Training Process of NMT through the Lens of Classical SMT\nElena Voita1,2 Rico Sennrich3,1 Ivan Titov1,2\n1University of Edinburgh, Scotland 2University of Amsterdam, Netherlands\n3University of Zurich, Switzerland\nlena-voita@hotmail.com sennrich@cl.uzh.ch ititov@inf.ed.ac.uk\nAbstract\nDifferently from the traditional statistical MT\nthat decomposes the translation task into dis-\ntinct separately learned components, neural\nmachine translation uses a single neural net-\nwork to model the entire translation process.\nDespite neural machine translation being de-\nfacto standard, it is still not clear how NMT\nmodels acquire different competences over the\ncourse of training, and how this mirrors the dif-\nferent models in traditional SMT. In this work,\nwe look at the competences related to three\ncore SMT components and ﬁnd that during\ntraining, NMT ﬁrst focuses on learning target-\nside language modeling, then improves transla-\ntion quality approaching word-by-word trans-\nlation, and ﬁnally learns more complicated\nreordering patterns. We show that this be-\nhavior holds for several models and language\npairs. Additionally, we explain how such an\nunderstanding of the training process can be\nuseful in practice and, as an example, show\nhow it can be used to improve vanilla non-\nautoregressive neural machine translation by\nguiding teacher model selection.\n1 Introduction\nIn the last couple of decades, the two main ma-\nchine translation paradigms have been statistical\nand neural MT. Statistical MT (SMT) decomposes\nthe translation task into several components (e.g.,\nlexical translation probabilities, alignment proba-\nbilities, target-side language model, etc.) which are\nlearned separately and then combined in a transla-\ntion model. Differently, neural MT (NMT) models\nthe entire translation process with a single neural\nnetwork that is trained end-to-end.\nAlthough joint training of all the components\nis one of the obvious NMT strengths, this is\nalso one of its challenging aspects. While SMT\nmodels different competences with distinct model\ncomponents and, therefore, can easily validate\nand/or improve each of them, NMT acquires these\ncompetences within the same network over the\ncourse of training. Even though previous work\nshows how to improve some of the competences\nin NMT, e.g., by using lexical translation proba-\nbilities, phrase memories, target-side LM, align-\nment information (Arthur et al., 2016; He et al.,\n2016; Tang et al., 2016; Wang et al., 2017; Zhang\net al., 2017a; Dahlmann et al., 2017; Gülçehre et al.,\n2015; Gülçehre et al., 2017; He et al., 2016; Sri-\nram et al., 2017; Dahlmann et al., 2017; Stahlberg\net al., 2018; Mi et al., 2016b; Liu et al., 2016;\nChen et al., 2016; Alkhouli et al., 2016; Alkhouli\nand Ney, 2017; Park and Tsvetkov, 2019; Song\net al., 2020a among others), it is still not clear how\nand when NMT acquires these competences during\ntraining. For example, are there any stages where\nNMT focuses on different aspects of translation,\ne.g., ﬂuency (agreement on the target side) or ad-\nequacy (i.e. connection to the source), or does it\nimprove everything at the same rate? Does it learn\nword-by-word translation ﬁrst and more compli-\ncated patterns later, or is there a different behavior?\nThis is especially interesting in light of a recent\nwork analyzing how NMT balances the two differ-\nent types of context: the source and preﬁx of the\ntarget sentence (V oita et al., 2021). As it turns out,\nchanges in NMT training are non-monotonic and\nform several distinct stages (e.g., stages changing\ndirection from decreasing inﬂuence of source to\nincreasing), which hints that the NMT training con-\nsists of stages with qualitatively different changes.\nIn this paper, we try to understand what hap-\npens in these stages by analyzing translations gen-\nerated at different training steps. Speciﬁcally, we\nfocus on the aspects related to the three core SMT\ncomponents: target-side language modeling, lex-\nical translation, and reordering. We ﬁnd that dur-\ning training, NMT focuses on these aspects in the\nspeciﬁed above order. Intuitively, it starts by hal-\nlucinating frequent n-grams and sentences in the\ntarget language, then comes close to word-by-word\n8479\ntranslation, and ﬁnally learns more complicated re-\nordering patterns. We conﬁrm these ﬁndings for\nseveral models, LSTM and Transformer, and dif-\nferent modeling paradigms, encoder-decoder and\ndecoder-only, i.e. LM-style machine translation\nwhere a left-to-right language model is trained on\nthe concatenation of source and target sentences.\nFinally, we show how such an understanding\nof the training process can be useful in practice.\nNamely, we note that during a large part of train-\ning, a model’s quality (e.g. BLEU and token-level\npredictive accuracy) changes little, but reordering\nbecomes more complicated. This means that by us-\ning different training checkpoints, we can get high-\nquality translations of varying complexity, which\nis useful in settings where data complexity matters.\nFor example, guiding teacher model selection for\ndistillation in non-autoregressive machine transla-\ntion (NAT) can improve the quality of a vanilla\nNAT model by more than 1 BLEU.\nOur contributions are as follows:\n• we show that during training, NMT undergoes\nthe following three stages:\n◦ target-side language modeling;\n◦ learning how to use source and approach-\ning word-by-word translation;\n◦ reﬁning translations, visible by increas-\ningly complex reorderings, but almost in-\nvisible to standard metrics (e.g. BLEU).\n• we conﬁrm our ﬁnding for different models\nand modeling paradigms;\n• we explain how our analysis can be useful in\npractice and, as an example, show how it can\nimprove a non-autoregressive NMT model.\n2 Training Stages: The Two Viewpoints\nIn this section, we introduce two points of view on\nthe NMT training process. The ﬁrst one comes\nfrom previous work showing distinct stages in\nNMT training. These stages are formed by looking\nat a model’s internal workings and changes in the\nway it balances source and target information when\nforming a prediction. The second point of view\nis from this work: we take model translations at\ndifferent training steps and look at some of their\naspects mirroring, in a way, core SMT components.\nWhile these two points of view are complete\nopposites (one sees only the model’s innermost\nworkings, the other – only its output), only taken\nFigure 1: Contribution of source and entropy of source\ncontributions. En-Ru. Vertical lines separate the stages.\ntogether they can fully describe the training pro-\ncess. We start from the ﬁrst, abstract, stages, then\nshow how these inner processes look on the outside\nand conclude with one of the immediate practical\napplications of our analysis (Section 6).\n2.1 The Abstract Viewpoint: Relative Token\nContributions to NMT Predictions\nThe ‘abstract’ stages come from our previous work\nmeasuring how NMT balances the two different\ntypes of context: the source and preﬁx of the target\nsentence (V oita et al., 2021). We adapt one of the\nattribution methods, Layerwise Relevance Propa-\ngation (Bach et al., 2015), to the Transformer, and\nshow how to evaluate the proportion of each token’s\ninﬂuence for a given prediction. Then these rela-\ntive token inﬂuences are used to evaluate the total\ncontribution of the source (by summing up contri-\nbutions of all source tokens) or to see whether the\ntoken contributions are more or less focused (by\nevaluating the entropy of these contributions).\nAmong other things, V oita et al. (2021) look at\nhow the total source contribution and the entropy\nof source contributions change during training. We\nrepeated these experiments for WMT14 En-Ru and\nEn-De.1 Figure 1 conﬁrms previous observations:\nthe training process is non-monotonic with several\ndistinct stages, e.g. stages changing direction from\ndecreasing inﬂuence of source to increasing.\nThese results suggest that during training, NMT\nundergoes stages of qualitatively different changes.\nFor example, a decreasing and then increasing in-\nﬂuence of the source likely indicates that the model\nﬁrst learns to rely on the target preﬁx more (i.e. to\nfocus on target-side language modeling) and only\nafter that focuses on the connection to the source\n(i.e. adequacy rather than ﬂuency). While these\nhypotheses are reasonable, to conﬁrm them we\nhave to look not only at how model predictions are\nformed but also at the predictions themselves.\n1Using the released code: https://github.com/\nlena-voita/the-story-of-heads .\n8480\n2.2 The Practical Viewpoint: Model\nTranslations\nIn this viewpoint, we are interested in changes in\nmodel output, i.e. translations. We measure:\n◦ target-side language modeling scores;\n◦ translation quality;\n◦ monotonicity of alignments.\nNote that these characteristics are related to three\ncore components of the traditional SMT models:\ntarget-side language model, translation model, and\nreordering model. Although we are mainly inter-\nested in NMT models and, except for the language\nmodeling scores, do not measure the quality of\nthe corresponding SMT components directly, this\nrelation to SMT is important. While machine trans-\nlation is now mostly neural, it is still not clear how\n(e.g., in which order) those competences which\nused to be modelled with distinct components are\nnow learned jointly within a single neural network.\n3 Experimental Setting\n3.1 Models, Data and Preprocessing\nModels. We consider three models:\n◦ Transformer encoder-decoder;\n◦ LSTM encoder-decoder;\n◦ Transformer decoder (LM-style NMT).\nFor the ﬁrst model, we follow the setup of the Trans-\nformer base (Vaswani et al., 2017). LSTM encoder-\ndecoder is a single-layer GNMT (Wu et al., 2016).\nThe last model is the Transformer decoder trained\nas a left-to-right language model. In training, the\nmodel receives concatenated source and target sen-\ntences separated by a token-delimiter; in inference,\nit receives only the source sentence and the delim-\niter and is asked to continue generation.\nDatasets. We use the WMT news translation\nshared task for English-German and English-\nRussian: for En-De, WMT 2014 with 5.8m sen-\ntence pairs, for En-Ru – 2.5m sentence pairs (par-\nallel training data excluding UN and Paracrawl).\nSince our observations are similar for both lan-\nguages, in the main text we show ﬁgures for one of\nthem and in the appendix – for the other.\nPreprocessing. The data is lowercased and en-\ncoded using BPE (Sennrich et al., 2016). We use\nseparate source and target vocabularies of about\n32k tokens for encoder-decoder models, and a joint\nvocabulary of about 50k tokens for LM-style mod-\nels. For each experiment, we randomly choose 2/3\nof the dataset for training and use the remaining\n1/3 as a held-out set for analysis (see Section 3.3).\nMore details on hyperparameters, preprocessing,\nand training can be found in the appendix.\n3.2 Target-Side LM Scores\nFor each of the models, we train 2-, 3-, 4- and 5-\ngram KenLM (Heaﬁeld, 2011)2 language models\non target sides of the corresponding training data\n(segmented with BPE). We report KenLM scores\nfor the translations of the development sets.\n3.3 Monotonicity of Alignments\nTo measure how the relative ordering of words in\nthe source and its translation changes during train-\ning, we use two different scores used in previous\nwork (Burlot and Yvon, 2018; Zhou et al., 2020).\nWe evaluate the scores for two permutations of the\nsource: the trivial monotonic alignment and the\nalignment inferred for the generated translation.\nFuzzy Reordering Score (Talbot et al., 2011)\ncounts the number of chunks of contiguously\naligned words and, intuitively, it is based on the\nnumber of times a reader would need to jump in\norder to read one reordering in the order proposed\nby the other. The score is between 0 and 1, where a\nlarger score indicates more monotonic alignments.\nKendall tau distance (Kendall, 1938) is also\ncalled bubble-sort distancesince it is equivalent to\nthe number of swaps that the bubble sort algorithm\nwould take to place one list in the same order as the\nother list. We evaluate the normalized distance: it is\nbetween 0 and 1, where 0 indicates the monotonic\nalignment.\nThe main difference between the scores is that\nthe ﬁrst one takes into account only the number\nof jumps, while the second also considers their\ndistance. For a formal description of the scores and\ntheir differences, see the appendix.\nOur setting. For each of the considered model\ncheckpoints, we obtain datasets where the sources\ncome from the held-out 1/3 of the original dataset,\nand targets are their translations. For these datasets,\nwe infer alignments using fast_align (Dyer\net al., 2013)3.\n2https://github.com/kpu/kenlm\n3https://github.com/clab/fast_align\n8481\n(a)\n (b)\nFigure 2: (a) KenLM scores (horizontal dashed lines are the scores for the references); (b) proportion of tokens of\ndifferent frequency ranks in model translations. En-Ru.\nFigure 3: Translations at different steps during training. En-De.\n4 Transformer Training Stages\nIn this section, we discuss the standard encoder-\ndecoder Transformer. In the next section, we men-\ntion differences with several other models.\nWe ﬁrst analyze the results for each of the three\ncompetences and then characterize the stages based\non these practical observations. In all ﬁgures, we\nshow the abstract stages with vertical lines to link\nthe results to the changes in token contributions.\n4.1 Target-Side Language Modeling\nFigure 2a shows changes in the language modeling\nscores. We see that most of the change happens\nin the very beginning: the scores go up and peak\nmuch higher than that of the references. This means\nthat the model generates sentences with very fre-\nquent n-grams rather than diverse texts similar to\nreferences. Indeed, Figure 2a (right) shows that\nfor a part of the training (from 1k to 2k iterations),\nthe scores for simpler models (e.g., 2-gram) are\nhigher than for the more complicated ones (e.g.,\n5-gram). This means that generated translations\ntend to consist of frequent words and bigrams, but\nlarger subsequences are not necessarily ﬂuent.\nTo illustrate this, we show how translations of\none of the sentences evolve at the beginning of\ntraining (Figure 3). As expected, ﬁrst the trans-\nlations evolve from repetitions of frequent tokens\nto frequent bigrams and trigrams, and ﬁnally to\nlonger frequent phrases. To make this more clear,\nwe also show the proportion of tokens of differ-\nent frequency ranks in generated translations (Fig-\nure 2b). First (iterations 0-500), all generated to-\nkens are from the top-10 most frequent tokens, then\nonly from the top-50, and only later less frequent\ntokens are starting to appear. From Figure 3 we\nsee that this happens when the source comes into\nplay: tokens related to the source become weaved\ninto translations. Overall, this evolution from using\nshort target-side contexts to longer ones and, subse-\nquently, to using the source relates to works in com-\nputer vision discussing ‘shortcut features’ (Geirhos\net al., 2020), as well as differences in the progres-\nsion of extracting ‘easy’ and ‘difﬁcult’ features\nduring training (Hermann and Lampinen, 2020).\nNote also that model translations converge to\nhigher LM scores than references (Figure 2a).\nThis is expected: compared to references, beam\n8482\n(a)\n (b)\nFigure 4: (a) BLEU score; (b) token-level accuracy (the\nproportion of cases where the correct next token is the\nmost probable choice). WMT En-Ru.\nsearch translations are simpler in various aspects,\ne.g. they are simpler syntactically, contain fewer\nrare tokens and less reordering (Burlot and Yvon,\n2018; Ott et al., 2018; Zhou et al., 2020), and lead\nto more conﬁdent token contributions inside the\nmodel (V oita et al., 2021). For language models\nmore generally, beam search texts are also less sur-\nprising than human ones (Holtzman et al., 2020).\nTo summarize, the beginning of training is\nmostly devoted to target-side language modeling:\nwe see huge changes in the LM scores (Figure 2a),\nand the model hallucinates frequent n-grams (Fig-\nure 3). This agrees with the abstract stages shown\nin Figure 1: in the ﬁrst stage, the total contribution\nof the source substantially decreases. This means\nthat in the trade-off between information coming\nfrom the source and the target preﬁx, the model\ngives more and more priority to the preﬁx.\n4.2 Translation Quality\nFigure 4a shows the BLEU score on the devel-\nopment set during training. For a more ﬁne-\ngrained analysis, we also plot token-level predic-\ntive accuracy separately for target token frequency\ngroups (Figure 4b). We see that both the BLEU\nscore and accuracy become large very fast, e.g.\nafter the ﬁrst 20k iterations (25 % of the training\nprocess), the scores are already good. What is in-\nteresting, is that the accuracy for frequent tokens\nreaches the maximum value (the score of the con-\nverged model) very quickly. This agrees with our\nprevious observations in Figures 3 and 2b: at the\nbeginning of training, the model generates frequent\ntokens more readily than the rare ones. Figure 4b\nfurther conﬁrms this: the accuracy for the rare to-\nkens improves slower than for the rest of them.\nWhat is not clear, is what happens during the\nlast half of the training (iterations from 40k to\n80k): BLEU score improves only by 0.4, accu-\nracy does not seem to change noticeably even for\nrare tokens, the proportion of generated tokens of\n(a)\n (b)\nFigure 5: (a) fuzzy reordering score (for references:\n0.6), (b) Kendall tau distance (for references: 0.06);\nWMT En-Ru. The arrows point in the direction of less\nmonotonic alignments (more complicated reorderings).\ndifferent frequency ranks converges even earlier\n(Figure 2b), and patterns in token contributions\nalso do not change much (Figure 1). This is what\nwe are about to ﬁnd out in the next section.\n4.3 Monotonicity of Alignments\nWhile it is known that, compared to references,\nbeam search translations have more monotonic\nalignments (Burlot and Yvon, 2018; Zhou et al.,\n2020), it is not clear how monotonicity of align-\nments changes during model training. We show\nchanges in the two reordering scores in Figure 5.4\nWe can say that during the second half of the\ntraining, the model is slowly reﬁning translations,\nand, among the three competences we look at, the\nmost visible changes are due to more complicated\n(i.e. less monotonic) reorderings. For example, as\nwe already mentioned above, during this part of\nthe training none of the scores we looked at so far\nchanges much, whereas changes in both reorder-\ning scores are very substantial. The change in the\nfuzzy reordering score is only twice smaller than\nduring the preceding stage. Moreover, the align-\nments keep changing and become less monotonic\neven after both BLEU and token-level accuracy (i.e.\nthe metric that matches the model’s training objec-\ntive) converged, i.e. iterations after 80k (Figure 5).\nOverall, we interpret this reﬁnement stage as\nthe model slowly learning to reduce interference\nfrom the source text (typical for human transla-\ntion (V olansky et al., 2015) and exacerbated even\nmore in NMT (Toral, 2019)): it learns to apply\ncomplex reorderings to more closely follow typical\nword order in the target language. This means that\nwhile language modeling improves more promi-\nnently during the ﬁrst training stage, there is a long\n4Note that we evaluate the scores starting not from the\nvery beginning of training but after at least 6k updates. This is\nbecause evaluating monotonicity of alignments makes sense\nonly when translations are reasonable.\n8483\n(a) En-De\n(b) En-Ru\nFigure 6: Translations at different training steps. Same-colored chunks are approximately aligned to each other.\ntail of less frequent and more nuanced patterns\nthat the model learns later. Another example of\nsuch nuanced changes in translation not detected\nwith standard metrics is context-aware NMT. Previ-\nous work has criticized using BLEU as a stopping\ncriterion, showing that even when a model has con-\nverged in terms of BLEU, it continues to improve\nin terms of agreement with context (V oita et al.,\n2019b).\nTo illustrate changes during this last stage, we\nshow two examples in Figure 6. On average, the\ntranslations at the beginning of the last stage tend\nto have the same word order as the corresponding\nsource sentences: the alignments are highly mono-\ntonic. Formally, the similarity to the word-by-word\ntranslation is seen from the very low Kendall tau\ndistance after 6k-14k training iterations (Figure 5b):\nthis means that a very small number of permuta-\ntions is needed to transform the trivial monotonic\ntranslation into the one produced by the model.\nInterestingly, at this point, some undertranslation\nerrors can be explained via failures to perform a\ncomplex reordering. In the example in Figure 6b,\nthe phrase ‘axis conﬁguration’ cannot be translated\ninto Russian preserving the same word order, which\nmakes the model to omit the translation of ‘conﬁg-\nuration’.\n4.4 Characterizing Training Stages\nTo summarize, the NMT training process can be\ndescribed as undergoing the following three stages:\n◦ target-side language modeling;\n◦ learning how to use source and coming close\nto a word-by-word translation;\n◦ reﬁning translations, visible by an increase\nin complexity of the reorderings and almost\ninvisible by standard evaluation (e.g. BLEU).\nWhile the borders of these practical stages are\nnot as strictly deﬁned as the abstract ones with\nthe changes of monotonicity in contribution\ngraphs (Figure 1), these two points of view on the\ntraining process mirror each other very well. From\nthe abstract point of view with token contributions,\nthe model ﬁrst starts to form its predictions based\nmore on the preﬁx and ignores the source, then\nsource inﬂuence increases quickly, then very little\nis going on (Figure 1). From the practical point of\nview with model translations, the model ﬁrst hal-\nlucinates frequent tokens, then phrases, then sen-\ntences (mirrors source contributions going down),\nthen quickly improves translation quality (mirrors\nsource contribution going up), then little is going\non according to the standard scores, but alignments\nbecome noticeably less monotonic. As we see, both\npoints of view show the same kinds of processes\nfrom different perspective: from the inside and the\noutside of the model.\n5 Other NMT Models\nIn this section, we compare different architec-\ntures within the same encoder-decoder framework\n(Transformer vs LSTM), and different frameworks\nwith the Transformer architecture (encoder-decoder\nvs decoder-only). Overall, we ﬁnd that all models\nfollow the behavior described in Section 4.4; here\nwe discuss some of their differences.\nTransformer vs LSTM. As might be expected\nfrom the low BLEU scores (Table 1), LSTM trans-\nlations are simpler than the Transformer ones. We\nsee that they are less surprising according to the\ntarget-side language modeling scores (Figure 7a5)\n5Note that in Figure 7a, only the scores of the encoder-\ndecoder models can be compared because of differences in\n8484\nmodel En-Ru En-De\nTransformer (enc-dec) 35.93 28.18\nLSTM (enc-dec) 30.14 24.03\nTransformer-LM (dec) 34.16 26.76\nTable 1: BLEU scores: newstest2014 for En-Ru\nand newstest2017 for En-De.\n(a)\n (b)\nFigure 7: (a) target-side LM scores (5-gram), (b) fuzzy\nreordering score (for references: 0.5); WMT En-De.\nand have more monotonic alignments (Figure 7b).\nRegarding the latter, it is not clear whether this is\nbecause of the lower model capacity or because\nLSTM has an inductive bias towards more mono-\ntonic alignments; we leave this to future work.\nEncoder-decoder vs decoder-only. Table 1\nshows that decoder-only (LM-style) NMT is not\nmuch worse than the standard encoder-decoder\nmodel, especially in the higher-resource setting\n(e.g., En-De). However, the decoder-only model\nhas much simpler reordering patterns compared\nto the standard Transformer: its reordering scores\nare very close to the much weaker LSTM model\n(Figure 7b). One possible explanation is that the\nbidirectional nature of Transformer’s encoder facil-\nitates learning more complicated reorderings.\n6 Practical Implications\nWe showed that during a large part of the training,\nthe translation quality (e.g., BLEU) changes little,\nbut the alignments become less monotonic. Intu-\nitively, the translations become more complicated\nwhile their quality remains roughly the same.\nOne way to directly apply our analysis is to\nconsider tasks and settings where data properties\nsuch as regularity and/or simplicity are important.\nFor example, in neural machine translation, higher\nmonotonicity of artiﬁcial sources was hypothesized\nto be a facilitating factor for back-translation (Bur-\nlot and Yvon, 2018); additionally, complexity of\nmodel vocabulary (see Section 3). In the appendix, we show\nscores for all three models.\nthe distilled data is crucial for sequence-level dis-\ntillation in non-autoregressive machine transla-\ntion (Zhou et al., 2020). Such examples are not\nlimited to machine translation: in emergent lan-\nguages, languages with higher ‘regularity’ bring\nlearning speed advantages for communicating neu-\nral agents (Ren et al., 2020).\nIn this section, we consider non-autoregressive\nNMT, and leave the rest to future work.\n6.1 Non-Autoregressive Machine Translation\nNon-autoregressive neural machine transla-\ntion (NAT) (Gu et al., 2018) is different from the\ntraditional NMT in the way it generates target\nsequences: instead of the standard approach\nwhere target tokens are predicted step-by-step by\nconditioning on the previous ones, NAT models\npredict the whole sequence simultaneously. This is\npossible only with an underlying assumption that\nthe output tokens are independent from each other,\nwhich is unrealistic for natural language.\nFortunately, while this independence assumption\nis unrealistic for real references, it might be more\nplausible for simpler sequences, e.g. artiﬁcially\ngenerated translations. That is why targets for NAT\nmodels are usually not references but beam search\ntranslations of the standard autoregressive NMT\n(which, as we already mentioned above, are sim-\npler than references in many aspects). This is called\nsequence-level knowledge distillation (Kim and\nRush, 2016), and it is currently one of the de-facto\nstandard parts of the NAT training pipelines (Gu\net al. (2018); Lee et al. (2018); Ghazvininejad et al.\n(2019) to name a few).\nRecently Zhou et al. (2020) showed that the\nquality of a NAT model strongly depends on the\ncomplexity of the distilled data, and changing this\ncomplexity can improve the model. Since distilled\ndata consists of translations from a standard au-\ntoregressive teacher, our analysis gives a very sim-\nple way of modifying the complexity of this data.\nWhile usually a teacher is a fully converged model,\nwe propose to use as teachers intermediate check-\npoints during training. Since during a large part\nof training, NMT quality (e.g., BLEU) changes\nlittle, but the alignments become less monotonic,\nearlier checkpoints can produce simpler and more\nmonotonic translations. We hypothesize that these\ntranslations are more suitable as targets for NAT\nmodels, and we conﬁrm this with the experiments.\n8485\n6.2 Setting\nFollowing previous work (Zhou et al., 2020), we\ntrain the same NAT model on their preprocessed\ndataset6 and vary only distilled targets.\nModel. The model is the re-implemented\nby Zhou et al. (2020) version of the vanilla NAT\nby Gu et al. (2018). For more details, see appendix.\nDataset. The dataset is WMT14 English-German\n(En-De) with newstest2013 as the validation set and\nnewstest2014 as the test set, and BPE vocabulary\nof 37,000. We use the preprocessed dataset and the\nvocabularies released by Zhou et al. (2020).\nDistilled targets. The teacher is the standard\nTransformer-base fromfairseq (Ott et al., 2019).\nFor the baseline distilled dataset, we use the fully\nconverged model (in this case, the model after 200k\nupdates). For other datasets, we use earlier check-\npoints.\nEvaluation. We average the last 10 checkpoints.\n6.3 Experiments\nFigure 8c shows the BLEU scores for NAT models\ntrained with distilled data obtained from different\nteacher’s checkpoints; the baseline is the fully con-\nverged model (200k iterations). We see that by\ntaking an earlier checkpoint, after 40k iterations,\nwe improve NAT quality by 1.1 BLEU. For this\ncheckpoint, the teacher’s BLEU score is not much\nlower than that of the ﬁnal model (Figure 8a), but\nthe reorderings are much simpler (a higher fuzzy\nreordering score in Figure 8b).\nTo vary the complexity of the distilled data,\nZhou et al. (2020) proposed to apply either Born-\nAgain networks (BANs) (Furlanello et al., 2018) or\nmixture-of-experts (MoE) (Shen et al., 2019). Un-\nfortunately, MoE is rather complicated and requires\ncareful hyperparameter tuning (Shen et al., 2019),\nand BANs are time- and resource-consuming. They\ninvolve training the AT model till convergence and\nthen translating the training data to get a distilled\ndataset; this happens in several iterations (e.g., 5-7)\nusing for training the latest generated dataset. Com-\npared to these methods, our approach is extremely\nsimple and does not require a lot of computational\nresources (e.g., instead of fully training the AT\n6We used the code and the data from https://\ngithub.com/pytorch/fairseq/tree/master/\nexamples/nonautoregressive_translation.\n(a)\n (b)\n (c)\nFigure 8: (a) BLEU score of the AT Transformer-base\n(teacher for distillation); (b) fuzzy reordering score for\nthe distilled training data obtained from checkpoints of\nthe AT teacher; (c) BLEU scores for the vanilla NAT\nmodel trained on different distilled data.\nteacher several times as in BANs, our approach\nrequires only to partially train one AT teacher).\nNote that in this work, we provide these experi-\nments mainly to illustrate how our analysis can be\nuseful in the settings where data complexity matters\nand, therefore, limit ourselves to only using differ-\nent teacher checkpoints. Future work, however, can\ninvestigate possible combinations with other ap-\nproaches. For example, to further improve quality,\nour method can be combined with the Born-Again\nnetworks while still requiring fewer resources due\nto only partial training of the teachers.\n7 Additional Related Work\nOther work connecting neural and traditional ap-\nproaches include modeling modiﬁcations, such as\nmodeling coverage and/or fertility (Tu et al., 2016;\nMi et al., 2016a; Cohn et al., 2016; Feng et al.,\n2016) and several other modiﬁcations (Zhang et al.,\n2017b; Stahlberg et al., 2017; Huang et al., 2018),\nanalysis of the relation between attention and word\nalignments (Ghader and Monz, 2017), and word\nalignment induction from NMT models (Li et al.,\n2019; Garg et al., 2019; Song et al., 2020b; Zenkel\net al., 2020; Chen et al., 2020).\nPrevious analysis of NMT learning dynamics in-\nclude analyzing how the trainable parameters affect\nan NMT model (Zhu et al., 2020) and looking at\nthe speed of learning speciﬁc discourse phenomena\nin context-aware NMT (V oita et al., 2019b,a).\n8 Conclusions\nWe analyze how NMT acquires different competen-\ncies during training and look at the competencies\nrelated to three core SMT components. We ﬁnd\nthat NMT ﬁrst focuses on learning target-side lan-\nguage modeling, then improves translation quality\n8486\napproaching word-by-word translation, and ﬁnally\nlearns more complicated reordering patterns. We\nshow that such an understanding of the training pro-\ncess can be useful in settings where data complexity\nmatters and illustrate this for non-autoregressive\nMT; other tasks can be considered in future work.\nAdditionally, our results can contribute to the dis-\ncussion of (i) ‘easy’ and ‘difﬁcult’ task-relevant\nfeatures, including ‘shortcut features’, and (ii) the\nlimitations of the BLEU score.\nAcknowledgments\nWe would like to thank the anonymous review-\ners for their comments. Lena is supported by the\nFacebook PhD Fellowship. Rico Sennrich acknowl-\nedges support of the Swiss National Science Foun-\ndation (MUTAMUR; no. 176727). Ivan Titov\nacknowledges support of the European Research\nCouncil (ERC StG BroadSem 678254), Dutch Na-\ntional Science Foundation (VIDI 639.022.518) and\nEU Horizon 2020 (GoURMET, no. 825299).\nReferences\nTamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Pe-\nter, Mohammed Hethnawi, Andreas Guta, and Her-\nmann Ney. 2016. Alignment-based neural machine\ntranslation. In Proceedings of the First Conference\non Machine Translation: Volume 1, Research Pa-\npers, pages 54–65, Berlin, Germany. Association for\nComputational Linguistics.\nTamer Alkhouli and Hermann Ney. 2017. Biasing\nattention-based recurrent neural networks using ex-\nternal alignment information. In Proceedings of the\nSecond Conference on Machine Translation, pages\n108–117, Copenhagen, Denmark. Association for\nComputational Linguistics.\nPhilip Arthur, Graham Neubig, and Satoshi Nakamura.\n2016. Incorporating discrete translation lexicons\ninto neural machine translation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1557–1567, Austin,\nTexas. Association for Computational Linguistics.\nSebastian Bach, Alexander Binder, Grégoire Mon-\ntavon, Frederick Klauschen, Klaus-Robert Müller,\nand Wojciech Samek. 2015. On pixel-wise explana-\ntions for non-linear classiﬁer decisions by layer-wise\nrelevance propagation. PloS one, 10(7):e0130140.\nFranck Burlot and François Yvon. 2018. Using mono-\nlingual data in neural machine translation: a system-\natic study. In Proceedings of the Third Conference\non Machine Translation: Research Papers, pages\n144–155, Brussels, Belgium. Association for Com-\nputational Linguistics.\nWenhu Chen, Evgeny Matusov, Shahram Khadivi, and\nJan-Thorsten Peter. 2016. Guided alignment train-\ning for topic-aware neural machine translation.\nYun Chen, Yang Liu, Guanhua Chen, Xin Jiang, and\nQun Liu. 2020. Accurate word alignment induction\nfrom neural machine translation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 566–576,\nOnline. Association for Computational Linguistics.\nTrevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-\nmolova, Kaisheng Yao, Chris Dyer, and Gholamreza\nHaffari. 2016. Incorporating structural alignment bi-\nases into an attentional neural translation model. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 876–885, San Diego, California. Association\nfor Computational Linguistics.\nLeonard Dahlmann, Evgeny Matusov, Pavel\nPetrushkov, and Shahram Khadivi. 2017. Neu-\nral machine translation leveraging phrase-based\nmodels in a hybrid search. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1411–1420, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameter-\nization of IBM model 2. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 644–648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nShi Feng, Shujie Liu, Nan Yang, Mu Li, Ming Zhou,\nand Kenny Q. Zhu. 2016. Improving attention mod-\neling with implicit distortion and fertility for ma-\nchine translation. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics: Technical Papers, pages 3082–3092,\nOsaka, Japan. The COLING 2016 Organizing Com-\nmittee.\nTommaso Furlanello, Zachary Lipton, Michael Tschan-\nnen, Laurent Itti, and Anima Anandkumar. 2018.\nBorn again neural networks. In Proceedings of the\n35th International Conference on Machine Learning,\nvolume 80 of Proceedings of Machine Learning Re-\nsearch, pages 1607–1616. PMLR.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4453–4462, Hong\nKong, China. Association for Computational Lin-\nguistics.\n8487\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio\nMichaelis, Richard S. Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A. Wichmann. 2020.\nShortcut learning in deep neural networks. CoRR,\nabs/2004.07780.\nHamidreza Ghader and Christof Monz. 2017. What\ndoes attention in neural machine translation pay at-\ntention to? In Proceedings of the Eighth Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 30–39,\nTaipei, Taiwan. Asian Federation of Natural Lan-\nguage Processing.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6112–\n6121, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In Inter-\nnational Conference on Learning Representations.\nÇaglar Gülçehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loïc Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion.\nÇaglar Gülçehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, and Yoshua Bengio. 2017. On integrating a lan-\nguage model into neural machine translation. Com-\nput. Speech Lang., 45:137–148.\nW. He, Zhongjun He, Hua Wu, and H. Wang. 2016. Im-\nproved neural machine translation with smt features.\nIn AAAI.\nKenneth Heaﬁeld. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the\nSixth Workshop on Statistical Machine Translation,\npages 187–197, Edinburgh, Scotland. Association\nfor Computational Linguistics.\nKatherine Hermann and Andrew Lampinen. 2020.\nWhat shapes feature representations? exploring\ndatasets, architectures, and training. In Advances in\nNeural Information Processing Systems, volume 33,\npages 9995–10006. Curran Associates, Inc.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations.\nPo-Sen Huang, Chong Wang, Sitao Huang, Dengyong\nZhou, and Li Deng. 2018. Towards neural phrase-\nbased machine translation. In International Confer-\nence on Learning Representations.\nM.G. Kendall. 1938. A new measure of rank correla-\ntion. Biometrika, 30(1/2):81–93.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1317–1327, Austin,\nTexas. Association for Computational Linguistics.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the International Conference on Learning Repre-\nsentation (ICLR 2015).\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1173–\n1182, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the word alignment from\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1293–1303, Florence,\nItaly. Association for Computational Linguistics.\nLemao Liu, Masao Utiyama, Andrew Finch, and Ei-\nichiro Sumita. 2016. Neural machine translation\nwith supervised attention. In Proceedings of COL-\nING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pages\n3093–3102, Osaka, Japan. The COLING 2016 Orga-\nnizing Committee.\nHaitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe\nIttycheriah. 2016a. Coverage embedding models\nfor neural machine translation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 955–960, Austin,\nTexas. Association for Computational Linguistics.\nHaitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016b.\nSupervised attentions for neural machine translation.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2283–2288, Austin, Texas. Association for Compu-\ntational Linguistics.\nMyle Ott, Michael Auli, David Grangier, and\nMarc’Aurelio Ranzato. 2018. Analyzing uncer-\ntainty in neural machine translation. In Proceed-\nings of the 35th International Conference on Ma-\nchine Learning, volume 80 of Proceedings of Ma-\nchine Learning Research, pages 3956–3965, Stock-\nholmsmässan, Stockholm Sweden. PMLR.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n8488\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nChan Young Park and Yulia Tsvetkov. 2019. Learn-\ning to generate word- and phrase-embeddings for ef-\nﬁcient phrase-based neural machine translation. In\nProceedings of the 3rd Workshop on Neural Gener-\nation and Translation, pages 241–248, Hong Kong.\nAssociation for Computational Linguistics.\nYi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Co-\nhen, and Simon Kirby. 2020. Compositional lan-\nguages emerge in a neural iterated learning model.\nIn International Conference on Learning Represen-\ntations.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nTianxiao Shen, Myle Ott, Michael Auli, and\nMarc’Aurelio Ranzato. 2019. Mixture models for\ndiverse machine translation: Tricks of the trade. In\nProceedings of the 36th International Conference on\nMachine Learning, volume 97 ofProceedings of Ma-\nchine Learning Research, pages 5719–5728. PMLR.\nKai Song, K. Wang, H. Yu, Y . Zhang, Zhongqiang\nHuang, Wei-Hua Luo, Xiangyu Duan, and M. Zhang.\n2020a. Alignment-enhanced transformer for con-\nstraining nmt with pre-speciﬁed translations. In\nAAAI.\nKai Song, Xiaoqing Zhou, Heng Yu, Zhongqiang\nHuang, Yue Zhang, Weihua Luo, Xiangyu Duan,\nand Min Zhang. 2020b. Towards better word align-\nment in transformer. volume 28, pages 1801–1812.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2017. Cold fusion: Training seq2seq\nmodels together with language models.\nFelix Stahlberg, James Cross, and Veselin Stoyanov.\n2018. Simple fusion: Return of the language model.\nIn Proceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 204–211, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nFelix Stahlberg, Adrià de Gispert, Eva Hasler, and\nBill Byrne. 2017. Neural machine translation by\nminimising the Bayes-risk with respect to syntactic\ntranslation lattices. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 362–368, Valencia, Spain. Association\nfor Computational Linguistics.\nDavid Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-\nson Katz-Brown, Masakazu Seno, and Franz Och.\n2011. A lightweight evaluation framework for ma-\nchine translation reordering. In Proceedings of the\nSixth Workshop on Statistical Machine Translation,\npages 12–21, Edinburgh, Scotland. Association for\nComputational Linguistics.\nYaohua Tang, Fandong Meng, Zhengdong Lu, Hang Li,\nand Philip L. H. Yu. 2016. Neural machine transla-\ntion with external phrase memory.\nAntonio Toral. 2019. Post-editese: an exacerbated\ntranslationese. In Proceedings of Machine Transla-\ntion Summit XVII Volume 1: Research Track, pages\n273–281, Dublin, Ireland. European Association for\nMachine Translation.\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,\nand Hang Li. 2016. Modeling coverage for neural\nmachine translation. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 76–\n85, Berlin, Germany. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008, Los Angeles.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a.\nContext-aware monolingual repair for neural ma-\nchine translation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 877–886, Hong Kong, China. As-\nsociation for Computational Linguistics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019b.\nWhen a good translation is wrong in context:\nContext-aware machine translation improves on\ndeixis, ellipsis, and lexical cohesion. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1198–1212, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2021. An-\nalyzing the source and target contributions to predic-\ntions in neural machine translation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics, Online. Association for\nComputational Linguistics.\nVered V olansky, Noam Ordan, and S. Wintner. 2015.\nOn the features of translationese. Digit. Scholarsh.\nHumanit., 30:98–118.\nXing Wang, Zhaopeng Tu, Deyi Xiong, and Min Zhang.\n2017. Translating phrases in neural machine trans-\nlation. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1421–1431, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\n8489\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2020. End-to-end neural word alignment outper-\nforms GIZA++. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1605–1617, Online. Association for\nComputational Linguistics.\nJiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu,\nand Maosong Sun. 2017a. Prior knowledge integra-\ntion for neural machine translation using posterior\nregularization. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1514–1523,\nVancouver, Canada. Association for Computational\nLinguistics.\nJingyi Zhang, Masao Utiyama, Eiichro Sumita, Gra-\nham Neubig, and Satoshi Nakamura. 2017b. Im-\nproving neural machine translation through phrase-\nbased forced decoding. In Proceedings of the\nEighth International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers) ,\npages 152–162, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nChunting Zhou, Jiatao Gu, and Graham Neubig.\n2020. Understanding knowledge distillation in non-\nautoregressive machine translation. In International\nConference on Learning Representations.\nConghui Zhu, Guanlin Li, Lemao Liu, Tiejun Zhao,\nand Shuming Shi. 2020. Understanding learning dy-\nnamics for neural machine translation.\n8490\nA Experimental Setting\nA.1 Data preprocessing\nTranslation pairs were batched together by approx-\nimate sequence length. Each training batch con-\ntained a set of translation pairs containing approxi-\nmately 32000 source tokens.7\nA.2 Model parameters\nTransformer (encoder-decoder). We follow\nthe setup of Transformer base model (Vaswani\net al., 2017). More precisely, the number of layers\nin the encoder and in the decoder isN = 6. We em-\nploy h= 8parallel attention layers, or heads. The\ndimensionality of input and output isdmodel = 512,\nand the inner-layer of a feed-forward networks has\ndimensionality dff = 2048. We use regularization\nas described in (Vaswani et al., 2017).\nTransformer (decoder). The difference from\nthe previous model is that the decoder has 12 lay-\ners.\nLSTM (encoder-decoder) is a single-layer\nGNMT (Wu et al., 2016) with the input and output\ndimensionality of 512 and hidden sizes of 1024.\nA.3 Optimizer\nThe optimizer we use is the same as in (Vaswani\net al., 2017). We use the Adam optimizer (Kingma\nand Ba, 2015) with β1 = 0.9, β2 = 0.98 and\nε = 10−9. We vary the learning rate over the\ncourse of training, according to the formula:\nlrate = scale· min(step_num−0.5,\nstep_num· warmup_steps−1.5)\nWe use warmup_steps= 16000, scale= 4.\nB Monotonicity of Alignments\nTo measure how the relative ordering of words in\nthe source and target sentences changes during\ntraining, we use two different scores: fuzzy re-\nordering score (Talbot et al., 2011) and Kendall tau\ndistance. We evaluate both scores for two permu-\ntations of the source sentence σ1 and σ2, where\nσ1is the trivial monotonic alignment and σ2 – the\nalignment inferred for the generated translation.\n7This can be reached by using several of GPUs or by accu-\nmulating the gradients for several batches and then making an\nupdate.\nFuzzy Reordering Score aligns each word in σ1\nto an instance of itself in σ2 taking the ﬁrst un-\nmatched instance of the word if there is more than\none. If Cis the number of chunks of contiguously\naligned words and M is the number of words in the\nsource sentence, then the fuzzy reordering score is\ncomputed as\nFRS(σ1,σ2) = 1− C− 1\nM − 1. (1)\nThis metric assigns a score between 0 and 1, where\n1 indicates that the two reorderings are identical.\nIntuitively,Cis the number of times a reader would\nneed to jump in order to read the reordering σ1 in\nthe order proposed byσ2. A larger fuzzy reordering\nscore indicates more monotonic alignments.\nKendall tau distance counts the number of pair-\nwise disagreements between two ranking lists. The\nlarger the distance, the more dissimilar the two lists\nare. Kendall tau distance is also called bubble-sort\ndistance since it is equivalent to the number of\nswaps that the bubble sort algorithm would take to\nplace one list in the same order as the other list. We\nevaluate the normalized distance, i.e. for a list of\nlength nit is normalized by n(n−1)\n2 . The normal-\nized score is between 0 and 1, where 0 indicates\nthat the two reorderings are identical.\nDifferences between the scores.While the ﬁrst\nscore counts only the number of chunks of contigu-\nously aligned words, the second one takes into ac-\ncount only how distant the changes are. For exam-\nple, let us consider two reorderings:(2,1,4,3,6,5)\nand (4,5,6,1,2,3). While for the fuzzy reorder-\ning score the least monotonic reordering is the\nﬁrst (more jumps for a reader), for the Kendall\ntau score – the second (requires more permutations\nto reorder). As we will see in Section 4.3, results\nfor the two scores are similar.\nOur setting. We take sentences of at least 2\nwords for the fuzzy reordering score and at least 10\ntokens for the Kendall tau distance.\nC Transformer Training Stages\nFigure 9 shows the abstract stages for En-De, Fig-\nures 10-13 provide the results from Section 4 for\nthe other language pair (En-De).\nD Other Models\nFigure 14 is a version of the Figure 7a from the\nmain text, but with the scores for all three mod-\n8491\nFigure 9: Left: contribution of source, right: entropy\nof source contributions. En-De. Vertical lines separate\nthe stages.\nFigure 10: KenLM scores. Left: 5-gram model, all\ntraining stages; right: different models, the ﬁrst stage.\nHorizontal lines show the scores for the references. En-\nDe.\nFigure 11: Proportion of tokens of different frequency\nranks in model translations. En-De.\n(a)\n (b)\nFigure 12: (a) BLEU score; (b) token-level accuracy\n(the proportion of cases where the correct next token is\nthe most probable choice). WMT En-De.\nels. Figure 15 provides corresponding results for\nthe other language pair (En-Ru). Note that in Fig-\nure 15b the reordering score for the LSTM model\nstops earlier: this is because the LSTM model con-\nverges earlier than other models.\nE Practical Applications\nE.1 Experimental Setting\nModel. The model is the re-implemented\nby Zhou et al. (2020) version of the vanilla NAT\n(a)\n (b)\nFigure 13: (a) fuzzy reordering score (for references:\n0.5), (b) Kendall tau distance (for references: 0.08);\nWMT En-De. The arrows point in the direction of less\nmonotonic alignments (more complicated reorderings).\nFigure 14: Target-side LM scores (5-gram); En-De.\n(a)\n (b)\nFigure 15: (a) target-side LM scores, (b) fuzzy reorder-\ning score (for references: 0.6); WMT En-Ru.\nby Gu et al. (2018). Namely, instead of modeling\nfertility as described in the original paper, Zhou\net al. (2020) monotonically copy the encoder em-\nbeddings to the input of the decoder. We used the\ncode released by Zhou et al. (2020).8\nTraining. For all experiments, we follow the set-\nting by Zhou et al. (2020). Note that in their work,\ntraining NAT models required 32 GPUs. In our\nsetting, we ensured the same batch size by accumu-\nlating gradients for several batches (in fairseq,\nthis is done using the -update-freq option).\nNAT Inference. Following previous work, for\nthis vanilla NAT model we use a straight-\nforward decoding algorithm which simply picks\nthe argmax at every position.\n8https://github.com/pytorch/fairseq/\ntree/master/examples/nonautoregressive_\ntranslation"
}