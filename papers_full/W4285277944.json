{
    "title": "Adaptive Differential Privacy for Language Model Training",
    "url": "https://openalex.org/W4285277944",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2128504963",
            "name": "Xinwei Wu",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A1880586979",
            "name": "Li Gong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2139156831",
            "name": "Deyi Xiong",
            "affiliations": [
                "Tianjin University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4287553002",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4287888099",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W3029016412",
        "https://openalex.org/W3158631574",
        "https://openalex.org/W4298125864",
        "https://openalex.org/W2109426455",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W2995417065",
        "https://openalex.org/W2027595342",
        "https://openalex.org/W3188505388",
        "https://openalex.org/W3193647133",
        "https://openalex.org/W4231844697",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2997181764",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W3103901889",
        "https://openalex.org/W3207429447",
        "https://openalex.org/W2784621220"
    ],
    "abstract": "Although differential privacy (DP) can protect language models from leaking privacy, its indiscriminative protection on all data points reduces its practical utility. Previous works improve DP training by discriminating privacy and non-privacy data. But these works rely on datasets with prior privacy information, which is not available in real-world scenarios. In this paper, we propose an Adaptive Differential Privacy (ADP) framework for language modeling without resorting to prior privacy information. We estimate the probability that a linguistic item contains privacy based on a language model. We further propose a new Adam algorithm that adjusts the degree of differential privacy noise injected to the language model according to the estimated privacy probabilities. Experiments demonstrate that our ADP improves differentially private language modeling to achieve good protection from canary attackers.",
    "full_text": "Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022), pages 21 - 26\nMay 27, 2022 ©2022 Association for Computational Linguistics\nAdaptive Differential Privacy for Language Modeling\nXinwei Wu1†, Li Gong2, Deyi Xiong1 ∗\n1College of Intelligence and Computing, Tianjin University, Tianjin, China\n2ByteDance Lark AI, Beijing, China\n{wuxw2021,dyxiong}@tju.edu.cn, franck.gong@bytedance.com\nAbstract\nAlthough differential privacy (DP) can protect\nlanguage models from leaking privacy, its in-\ndiscriminative protection on all data points re-\nduces its practical utility. Previous works im-\nprove DP training by discriminating private\nand non-private data. But these works rely on\ndatasets with prior privacy information, which\nis not available in real-world scenarios. In this\npaper, we propose an Adaptive Differential\nPrivacy (ADP) framework for language model-\ning without resorting to prior privacy informa-\ntion. We estimate the probability that a linguis-\ntic item contains privacy based on a language\nmodel. We further propose a new Adam al-\ngorithm that adjusts the degree of differential\nprivacy noise injected to the language model\naccording to the estimated privacy probabili-\nties. Experiments demonstrate that our ADP\nimproves differentially private language mod-\neling to achieve good protection from canary\nattackers.\n1 Introduction\nLanguage modeling is a foundation problem in nat-\nural language processing (Bommasani et al., 2021).\nRecent large language models (Brown et al., 2020;\nZeng et al., 2021) are usually trained at scale. Un-\nfortunately, large language models have a tendency\nto remember training data in the absence of ap-\npropriate privacy protection mechanisms (Carlini\net al., 2019, 2021). Since data, which are usually\ncollected from public sources, e.g., tweets, blogs,\nmay contain sensitive information (personal ad-\ndress, SSN numbers, and so on) learning a safe\nlarge language model has become increasingly im-\nportant.\nIn recent years, differential privacy (Dwork,\n2008; Dwork et al., 2014) has become a key pri-\nvacy preservation method, which attempts to ran-\n*Corresponding author.\n†Work done while this author was an intern at BtyeDance\nLark AI.\ndomize the training algorithm so that the model\ndoes not rely too much on any single training in-\nstances. Abadi et al. (2016) propose Differential\nPrivate Stochastic Gradient Descent (DP-SGD) to\nprotect deep learning models by adding random\nnoise to gradients. However, traditional differen-\ntial privacy ignores individual attributes of data\n(McMahan et al., 2018). This overly pessimistic\nprivacy protection results in poor performance or\neven mis-convergence of training for differentially\nprivate language models (Anil et al., 2021). There-\nfore, approaches are proposed to mitigate this prob-\nlem by treating private and non-private data sep-\narately during the DP training process, such as\nselective differential privacy (Shi et al., 2021) and\nsensory-based privacy-χ(Qu et al., 2021). These\nmethods require training data to provide privacy\ninformation as a hard label. Unfortunately, it is\nusually difficult and expensive to manually anno-\ntate privacy labels to data. Other studies (Xu et al.,\n2019; Tesfay et al., 2019) learn to detect privacy\ninformation in unstructured texts. However, the pre-\nrequisite is knowing keywords or reference texts\nof privacy information (Neerbek, 2020). Therefore,\nlearning differentially private language models on\ndata without prior privacy information is an open\nproblem yet to be investigated.\nIn this paper, we propose an Adaptive Differen-\ntial Privacy(ADP) framework without resorting\nto prior privacy information. The basic assump-\ntion behind ADP is that linguistic items containing\nprivate information do not occur frequently in real-\nworld texts. Hence, the probability that a linguistic\nitem contains privacy information (hereinafter pri-\nvacy probability) is inversely proportional to the\nfrequency of the linguistic item occurring in the\ndataset. With this assumption, we can estimate the\nprivacy probability of a linguistic item based on\na language model. After estimating these proba-\nbilities, we relax the constraint of differential pri-\nvacy, and propose an adaptive differential privacy\n21\nmethod, which adjusts the Guassian noise of dif-\nferential privacy based on privacy probabilities. To\nenable this adaptive differential privacy strategy,\nwe further present Adaptive-DP-Adam Algorithm\nto train differentially private language models.\nTo evaluate our approach, we train transformer-\nbased language models, and compare the perfor-\nmance of adaptive differential privacy against tra-\nditional differential privacy methods. Additionally,\nwe verify the protection effectiveness of ADP mod-\nels with canary attackers (Carlini et al., 2019). The\nresults suggest that our adaptive differential pri-\nvacy method can achieve good performance and\nprotection from canary attackers.\nThe main contributions of this paper are three-\nfold.\n• We propose a method to automatically esti-\nmate the probability that a linguistic item con-\ntains privacy information, relaxing the require-\nment of prior privacy information of previous\nmethods.\n• A new Adaptive-DP-Adam algorithm is pro-\nposed, which adaptively adjusts the magnitude\nof differential privacy noise to be injected into\nlanguage models according to privacy proba-\nbilities.1\n• We conduct experiments to validate the ef-\nfectiveness of the proposed adaptive differen-\ntial privacy in improving the performance of\ndifferentially private models and protecting\nsensitive information.\n2 Related Work\nLarge language models (Brown et al., 2020; Zhang\net al., 2020) have been attracting growing atten-\ntion. Powerful large language models can achieve\nsubstantial improvements on a wide range of down-\nstream NLP tasks. Unfortunately, large language\nmodels have a tendency to memorize training data\n(Carlini et al., 2019). Carlini et al. (2021) have\nsuccessfully induced GPT-2 (Radford et al., 2019)\nto output sensitive information in its training data.\nDifferential privacy (Dwork, 2008; Dwork et al.,\n2014) is widely used to protect private information\nof data. Abadi et al. (2016) propose the DP-SGD\nalgorithm to train deep learning models, and apply\nmoment accounting to calculate cumulative pri-\nvacy loss during training. Although DP-SGD can\n1Code is available at https://github.com/\nflamewei123/ADP.\nlimit the risk of leaking information from training\ndata, random noise on gradients usually degrades\ncorresponding models (Li et al., 2021), and even\ncause training to not converge when a large model\nis trained.\nTo improve DP-SGD, one way is to change train-\ning settings (Li et al., 2021; Hoory et al., 2021),\ne.g., increasing the batch size or decreasing clip-\nping norm. However, these methods are usually\nat a higher cost. Other attempts to improve the\nutilization of dataset information by relaxing the\nconstraints of differential privacy. For example,\nEbadi et al. (2015) propose personalized differenti-\nated privacy to provide different levels of privacy\nprotection for different users. Kotsogiannis et al.\n(2020) develop one-sided differential privacy that\nonly protects sensitive users. Shi et al. (2021) in-\ntroduce Selective Differential Privacy to add noise\nonly into private data. These methods all need\nto know which items in the dataset contain pri-\nvate information, which is prohibitively expensive\nfor large-scale datasets. There are some previous\nworks (Xu et al., 2019; Tesfay et al., 2019) detect-\ning sensitive information in unstructured texts, but\nrelying on labeled keywords or reference texts.\n3 Preliminary\nWe will introduce differential privacy (Dwork,\n2008; Dwork et al., 2014), and the DP-SGD al-\ngorithm (Abadi et al., 2016) as preliminaries in this\nsection.\n3.1 Differential Privacy\nIntuitively, an algorithm is (ϵ; δ)-DP if the output\nof the algorithm cannot be used to probabilistically\ndetermine the presence of a single record in the\ndataset by a factor of eϵ. Formally, an algorithm\nA satisfies (ϵ; δ)-DP if for all datasets ( D1;D2)\nthat differ from each other by at least one instance,\nand for any set S, we have P{A(D1) ∈S} ≤\neϵP{A(D2) ∈ S}+ δ, where smaller ϵ values\nindicate a stronger privacy protection.\n3.2 DP-SGD Optimization\nThe basic idea of DP-SGD is to clip each example\ngradients and add noise during model training.\nSpecifically, for a batch of size L, the loss func-\ntion is L(θ) = 1\nL\n∑\nxi L(xi; θ). For each sample xi\nin the batch, the gradient of g(xi) is first cut us-\ning the l2 norm according to the gradient clipping\nlevel C, so that the maximum value of loss does\n22\nnot exceed C:\ng(xi) = 1\nmax{1,∥∇θL(xi; θ)∥2 /C}∇θL(xi; θ).\n(1)\nFor a batch Lt, after the sum of clipping gradients\nof all samples in Lt is calculated, the Gaussian\nnoise z ∼N (0,σ2C2I) is added to the sum of\ngradients. Hence a new gradient ˜gLt required for\nback propagation is computed as follows:\n˜gLt = 1\nL(∑\nxi g(xi) + zt). (2)\nThe smaller C can lead to more stable training.\nAnd a smaller value of σindicates smaller noise z.\n4 Adaptive Differential Privacy\nIn this section, we will elaborate the proposed\nAdaptive Differential Privacy. First, we intro-\nduce a method to evaluate the privacy probability\nof a linguistic item. Second, we propose an adap-\ntive noise method, which adjusts the noise magni-\ntude according to the privacy probability of an item\nin DP-SGD process. Finally, an Adam gradient\noptimization algorithm based on adaptive noise is\nproposed.\n4.1 Privacy Probability Evaluation\nThe range of privacy is not fixed but relying on its\nowner, which makes it hard to judge the privacy.\nTo solve this problem, we introduce the following\nassumption.\nAssumption 1: Texts containing privacy infor-\nmation do not occur frequently in a large dataset.\nWe assume that the probability of texts contain-\ning private information is related to the frequency\nof texts appearing in dataset. Hence, the judgment\nof privacy can be transformed into the evaluation of\nthe text frequency, which means the privacy proba-\nbility of a token sequence is in direct proportion to\nthe frequency of this sequence.\nWe then introduce a simple yet effective method\nto measure the frequency of text based on large-\nscale pre-trained language models. Giving a token\nsequence s = x1,x2,...,x n, the perplexity of the\nsequence is computed as follows:\nP(s) = exp(−1\nn\nn∑\ni=1\nlog fθ(xi|x1,...,x i−1)).\n(3)\nWhen the perplexity is low, it indicates that the\naverage probability of text prediction is high. Large\nlanguage models like GPT use a huge amount of\ntext data for training. Hence, we consider such a\nlarge language model to be a trustworthy estimator.\nThe perplexity from a trustworthy language\nmodel is inversely proportional to the occurrence\nfrequence of the text o(s) ∝ 1\nP(s) , and the privacy\nprobability of s is proportional to the perplexity\nof s: ρ(s) ∝P(s). Based on this, we propose a\nformula for calculating the privacy probability:\nρ(s) = normalize(P(s)), (4)\nwhere s∈Dand normalize is a normalization op-\nerator that transforms values into probability values\n(i.e., falling between 0 and 1).\nThe above method that estimates the privacy\nprobability is not precise enough, which will in-\nevitably cause some non-private and long-tail in-\nstances to be identified as private samples. How-\never, from the perspective of privacy protection,\nsuch a cost is still acceptable.\n4.2 Adaptive Noise\nDuring differential privacy training, in the batch\nB = s1,s2,...,s Lof size L, the privacy probability\nof a token sequence si ∈Bis ρ(si), and the Gaus-\nsian noise of Bis zB = N(0,C2σ2I2), where σis\na noise multiplier, and Cis the clipping norm. To\nimprove the target model performance, we intro-\nduce the privacy weightto change the magnitude\nof Gaussian noise\nγB =\n∑L\ni ρ(si)\nL . (5)\nThe privacy weightdenotes a privacy probability\naveraged over batch B. We incorporate it to the\nGaussian noise:\nzBadp = γB ·N(0,C2σ2I2). (6)\nThrough this method, we adaptively change the\nnoise of every batch according to its privacy weight.\n4.3 Adaptive DP Optimization\nWith the adaptive noise, we further develop a pri-\nvacy mechanism to train models. Abadi et al.\n(2016) propose DP-SGD that adds Gaussian noise\nto gradients and applies stochastic gradient descent\n(SGD) to train private deep learning models. We\nincorporate our proposed adaptive noise into DP-\nSGD.\nSuch adapted framework is also suitable for\nother optimization algorithms such as Adam. The\nwhole procedure of Adaptive-DP-Adam is de-\nscribed in Algorithm 1.\n23\nAlgorithm 1:Adaptive-DP-Adam\n1 Input: dataset D= {xi}N\ni=1, a large\nlanguage model fLM, loss function L(θ)\n2 Parameters: learning rate η, noise level σ,\nbatch Bof size L, clipping norm C, step\nE, Adam parameters {θ0,m0,m1,δ1,δ2}\n1: Let G(φ) = 0\n2: for allt∈T do\n3: Sample a batch Bt, with sampling\nprobability L/N\n4: Calculate γBt based on Eq. (5)\n5: for allxi ∈Bt do\n6: Clip gradients\n˜gt(xi) ←gt(xi) ·min(1,C/ ∥gt(xi)∥2)\n7: end for\n8: Generate adaptive noise zt based on Eq. (6)\n9: Calculate average gradients\n¯gt(xi) = 1\nL(zt + ∑L\ni=1 ˜gt(xi))\n10: Update parameters θusing usual Adam\n11: end for\n12: return θT\n5 Experiments\n5.1 Settings\nDataset We used Wikitext-103 (Merity et al.,\n2016) to train our model, which is a widely used\ndataset for language modeling from a set of verified\nGood and Featured articles on Wikipedia.\nBaselines We have two baselines, one without\nDP (denoted by “No-DP”), and the other trained\nwith DP-SGD (denoted by “DP-SGD”). We refer\nto our models trained with ADP-SGD as “ADP”.\nHyper-parameters We used a 12-layer trans-\nformer decoder to train the language model with\nhidden size of 1024 and batch size of 4096, training\n20 epoches with inital learning rate of 5 ×10−5.\nThe clipping norm C was set to 0.001, and the\nnoise multiplier σwas 1 or 5.\n5.2 Canary Attacker\nCanary insertion is proposed by Carlini et al.\n(2019), which inserts random sequences called ca-\nnaries into the training dataset and calculates the\nexposure for the inserted canaries during testing\nto measure whether the model memorizes these\ncanaries. In our setting, we injected “My ID is\n955320” into the Wikitext-103 dataset for 10, 100,\nand 1000 times to make the differences between\nmodel test loss test PPL sigma epsilon\nNo-DP 7.08 256.66 - -\nDP-SGD 13.08 7582.65 1.0 4.22\nADP 12.65 4426.05 1.0 6.35\nNo-DP 7.08 256.66 - -\nDP-SGD 17.65 20815.23 5.0 0.1\nADP 14.85 8635.66 5.0 2.47\nTable 1: The performance of language models trained\nby our method and baselines. We compare results by\nvarying the noise level σ.\nmodels more salient. Given a canary s[r], and a\nmodel with parameters θ, the exposure of s[r] is\ncalculated as:\nexposure = log2 |R|−log2 rankθ(s[r]), (7)\nwhere Ris the set of all possible results, and\nrank(s[r]) is the position of s[r] in R. The lower\nthe exposure, the safer the model is.\n5.3 Results\nModel Performance We first evaluated models\ntrained by different privacy settings on language\nmodeling task. Both models were trained using\na transformer decoder architecture. As shown in\nTable 1, DP-SGD performs poorly, and larger noise\nσfurther worses the model. In contrast, our ADP\nhelps model to alleviate the decaying performance,\nand the utility grows when the noise multiplier σ\nis large. Although the privacy guarantee ϵof ADP\nincreases compared to DP-SGD when the noise\nmultiplier σ is 1 and 5, the privacy guarantee of\nADP is within the acceptable range. It suggests that\nour ADP can improve the performance of differen-\ntially private language models with tight privacy\nguarantee.\nProtection Against Attacker Our second group\nof experiments, described in section 5.2, is to test\nthe model memorization of private information.\nWe evaluated models trained on the Wikitext-103\ndataset injected canaries. We used text generation\nto evaluate the exposure of canaries from differ-\nent language models. As can be seen from Fig-\nure 1, even when private item appears as many as\n1000 times in the data, the ADP model performs\nsignificantly better than the non-DP model. How-\never, exposures of the ADP model are larger than\nthe DP-SGD model. It suggests that ADP method\ncan protect privacy information from leaking from\ntraining data, but the protection performance is\nslightly worse than DP-SGD.\n24\n101 102 103\nCanary Count\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25Exposure\nCanary Exposure of Language Models\nNo-DP\nDP-SGD\nADP\nFigure 1: The exposure of canaries from different lan-\nguage models. All models were trained for 20 epoches.\n6 Conclusion\nWe have presented a new method to estimate the\nprivacy probability of a linguistic item when the\nprivacy information of the dataset is not known.\nWith estimated privacy probabilities, we propose\nadaptive differential privacy (ADP), to improve the\nmodel utility. We also present a privacy optimiza-\ntion algorithm, Adaptive-DP-Adam, to train differ-\nentially private models. Our experiments show that\nmodels trained with ADP achieve better utilities\nthan traditional DP and are capable of protecting\nsensitive information from being leaked.\nAcknowledgements\nThe work was partially supported by a\nByteDance Research Collaboration Project\n(PJ20210625900030) and the Natural Sci-\nence Foundation of Tianjin (Grant No.\n19JCZDJC31400). We would like to thank\nthe anonymous reviewers for their insightful\ncomments.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security,\npages 308–318.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar,\nand Pasin Manurangsi. 2021. Large-scale differen-\ntially private bert. arXiv preprint arXiv:2108.01624.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al. 2021. On\nthe opportunities and risks of foundation models.\nhttps://openreview.net/forum?id=NTs-oIaO6O.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th USENIX Security Symposium\n(USENIX Security 19), pages 267–284.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\nCynthia Dwork. 2008. Differential privacy: A survey\nof results. In International conference on theory and\napplications of models of computation, pages 1–19.\nSpringer.\nCynthia Dwork, Aaron Roth, et al. 2014. The algo-\nrithmic foundations of differential privacy. Found.\nTrends Theor. Comput. Sci., 9(3-4):211–407.\nHamid Ebadi, David Sands, and Gerardo Schneider.\n2015. Differential privacy: Now it’s getting personal.\nAcm Sigplan Notices, 50(1):69–81.\nShlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell,\nAlon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri\nStemmer, Ayelet Benjamini, Avinatan Hassidim, et al.\n2021. Learning and evaluating a differentially pri-\nvate pre-trained language model. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1178–1189.\nIos Kotsogiannis, Stelios Doudalis, Sam Haney, Ash-\nwin Machanavajjhala, and Sharad Mehrotra. 2020.\nOne-sided differential privacy. In 2020 IEEE 36th In-\nternational Conference on Data Engineering (ICDE),\npages 493–504. IEEE.\nXuechen Li, Florian Tramer, Percy Liang, and Tat-\nsunori Hashimoto. 2021. Large language mod-\nels can be strong differentially private learners.\nhttps://openreview.net/forum?id=bVuP3ltATMz.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In International Confer-\nence on Learning Representations.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. https://openreview.net/forum?id=Byj72udxe.\n25\nJan Neerbek. 2020. Sensitive Information Detection:\nRecursive Neural Networks for Encoding Context.\nPh.D. thesis, Aarhus University.\nChen Qu, Weize Kong, Liu Yang, Mingyang Zhang,\nMichael Bendersky, and Marc Najork. 2021. Natural\nlanguage understanding with privacy-preserving bert.\nIn Proceedings of the 30th ACM International Con-\nference on Information & Knowledge Management,\npages 1488–1497.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nWeiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou\nYu. 2021. Selective differential privacy for language\nmodeling. arXiv preprint arXiv:2108.12944.\nWelderufael B Tesfay, Jetzabel Serna, and Kai Rannen-\nberg. 2019. Privacybot: Detecting privacy sensitive\ninformation in unstructured texts. In 2019 Sixth In-\nternational Conference on Social Networks Analysis,\nManagement and Security (SNAMS), pages 53–60.\nIEEE.\nGuosheng Xu, Chunhao Qi, Hai Yu, Shengwei Xu,\nChunlu Zhao, and Jing Yuan. 2019. Detecting sen-\nsitive information of unstructured text using convo-\nlutional neural network. In 2019 International Con-\nference on Cyber-Enabled Distributed Computing\nand Knowledge Discovery (CyberC), pages 474–479.\nIEEE.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang,\nYi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang\nYang, Kaisheng Wang, Xiaoda Zhang, et al. 2021.\nPangu: Large-scale autoregressive pretrained chi-\nnese language models with auto-parallel computation.\nhttps://openreview.net/forum?id=-AArJ2Qrh38.\nZachariah Zhang, Jingshu Liu, and Narges Razavian.\n2020. BERT-XML: Large scale automated ICD cod-\ning using BERT pretraining. In Proceedings of the\n3rd Clinical Natural Language Processing Workshop,\npages 24–34, Online. Association for Computational\nLinguistics.\n26"
}