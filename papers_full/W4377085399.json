{
  "title": "Inshore Dense Ship Detection in SAR Images Based on Edge Semantic Decoupling and Transformer",
  "url": "https://openalex.org/W4377085399",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5011454815",
      "name": "Yongsheng Zhou",
      "affiliations": [
        "Beijing University of Chemical Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5103279243",
      "name": "Feixiang Zhang",
      "affiliations": [
        "Beijing University of Chemical Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5082881761",
      "name": "Qiang Yin",
      "affiliations": [
        "Beijing University of Chemical Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5069637938",
      "name": "Fei Ma",
      "affiliations": [
        "Beijing University of Chemical Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100690860",
      "name": "Fan Zhang",
      "affiliations": [
        "Beijing University of Chemical Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2165616753",
    "https://openalex.org/W2922891743",
    "https://openalex.org/W3094484482",
    "https://openalex.org/W3215588016",
    "https://openalex.org/W4205652744",
    "https://openalex.org/W3200421546",
    "https://openalex.org/W3200733355",
    "https://openalex.org/W2928007866",
    "https://openalex.org/W2997464028",
    "https://openalex.org/W3089780760",
    "https://openalex.org/W4226456028",
    "https://openalex.org/W4226538900",
    "https://openalex.org/W4312669952",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W4246399668",
    "https://openalex.org/W6777046832",
    "https://openalex.org/W3001406127",
    "https://openalex.org/W4205338556",
    "https://openalex.org/W3011481750",
    "https://openalex.org/W3184737818",
    "https://openalex.org/W6798838024",
    "https://openalex.org/W3013734213",
    "https://openalex.org/W3013341635",
    "https://openalex.org/W4225268843",
    "https://openalex.org/W3210586215",
    "https://openalex.org/W4283809070",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3194790201",
    "https://openalex.org/W4297003442",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3184439416",
    "https://openalex.org/W3018757597"
  ],
  "abstract": "Synthetic aperture radar ship detection has recently received significant attention from scholars. However, accurately distinguishing between ships is challenging due to the significant overlap between inshore ship labels. In addition, some labeled boxes contain interference information, such as land areas, which can cause false alarms and confusion in ship feature learning. To address these challenges, this article creates an edge semantic decoupling (ESD) module, adds semantic segmentation branches, and introduces the edge semantic information of ships into the training process. As a result, the model can accurately distinguish between ship targets even when significant overlap exists between inshore labeled boxes. In addition, considering that transformer has the benefit of capturing global and contextual information, this article introduces it into the detection layer to construct a transformer detection layer (TDL) to limit the interference of land and other regions within the labeled box. Experimental results from the public SAR ship detection dataset show that the proposed ESD module and TDL detection layer effectively distinguish different ship targets in the inshore dense ship area, which is less affected by interference areas, such as land in the labeled box. The average precision improves to 96.72&#x0025;, and both false alarms and miss detections inshore are reduced.",
  "full_text": "4882 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nInshore Dense Ship Detection in SAR Images Based\non Edge Semantic Decoupling and Transformer\nYongsheng Zhou , Member , IEEE, Feixiang Zhang ,Q i a n gY i n, Senior Member , IEEE,\nFei Ma , Member , IEEE, and Fan Zhang , Senior Member , IEEE\nAbstract—Synthetic aperture radar ship detection has recently\nreceived signiﬁcant attention from scholars. However, accurately\ndistinguishing between ships is challenging due to the signiﬁcant\noverlap between inshore ship labels. In addition, some labeled\nboxes contain interference information, such as land areas, which\ncan cause false alarms and confusion in ship feature learning.\nTo address these challenges, this article creates an edge semantic\ndecoupling (ESD) module, adds semantic segmentation branches,\nand introduces the edge semantic information of ships into the\ntraining process. As a result, the model can accurately distinguish\nbetween ship targets even when signiﬁcant overlap exists between\ninshore labeled boxes. In addition, considering that transformer\nhas the beneﬁt of capturing global and contextual information,\nthis article introduces it into the detection layer to construct a\ntransformer detection layer (TDL) to limit the interference of land\nand other regions within the labeled box. Experimental results from\nthe public SAR ship detection dataset show that the proposed ESD\nmodule and TDL detection layer effectively distinguish different\nship targets in the inshore dense ship area, which is less affected by\ninterference areas, such as land in the labeled box. The average\nprecision improves to 96.72%, and both false alarms and miss\ndetections inshore are reduced.\nIndex Terms—Edge semantic decoupling (ESD), inshore ship\ndetection, transformer.\nI. INTRODUCTION\nS\nYNTHETIC aperture radar (SAR) has the capability to\nconduct all-day and all-weather observations, allowing for\nlong-term monitoring of the ocean without the interference of\nweather conditions like cloud cover, fog, etc.,[1], [2], [3], [4],\n[5], [6]. With the increase of public datasets and the develop-\nment of convolution neural networks (CNN), more and more\nresearchers are utilizing CNNs for SAR ship detection[7], [8],\n[9], [10], [11], [12], [13].\nManuscript received 1 March 2023; revised 3 April 2023 and 28 April 2023;\naccepted 9 May 2023. Date of publication 17 May 2023; date of current version\n5 June 2023. This work was supported in part by the National Natural Science\nFoundation of China under Grant 62171016 and Grant 62271034, and in part\nby the Fundamental Research Funds for the Central Universities under Grant\nbuctrc202001 and Grant XK2020-03.(Corresponding author: Qiang Yin.)\nYongsheng Zhou, Feixiang Zhang, Qiang Yin, and Fei Ma are with the College\nof Information Science and Technology, Beijing University of Chemical\nTechnology, Beijing 100029, China (e-mail: zhyosh@mail.buct.edu.cn;\n2019200710@mail.buct.edu.cn; yinq@mail.buct.edu.cn; mafei@mail.buct.\nedu.cn).\nFan Zhang is with the College of Information Science and Tech-\nnology, Interdisciplinary Research Center for Artiﬁcial Intelligence, Bei-\njing University of Chemical Technology, Beijing 100029, China (e-mail:\nzhangf@mail.buct.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3277013\nFig. 1. Issue of labeled boxes for inshore ships. (a) Contain interference\ninformation. (b) Labeled boxes overlapped.\nCurrent methods for SAR ship detection using CNNs can be\nclassiﬁed into two categories as follows: 1) single-stage and 2)\ntwo-stage detection. The two-stage detection method involves\na two-step process where the detection box is ﬁrst coarsely\nextracted using region proposal network (RPN)[14], followed\nby regression and classiﬁcation of the box. Some representa-\ntive methods using this approach are faster region-CNN (R-\nCNN) [14] and cascade R-CNN[15]. The single-stage detection\nmethod has the advantage of being faster and more efﬁcient, as\nit can perform regression and classiﬁcation without the need\nfor coarse extraction. Some representative methods using this\napproach are YOLOv3[16] and YOLOv4[17]. In consideration\nof practical application requirements, the current trend in SAR\nship detection primarily favors the single-stage detection method\ndue to its speed and simplicity.\nThe detection of ships in SAR images using CNN-based meth-\nods requires a large number of labels. However, dense inshore\nship labels often overlap, making it challenging to differentiate\nbetween different ship targets. As a result, the dense region is\nprone to miss detection, as illustrated in Fig.1.T i a ne ta l .[18]\nattempted to solve this issue by utilizing rotating boxes for\ndetection. Rotating box labeling has been applied to mitigate\na signiﬁcant portion of the inshore interference. However, this\napproach faces a limitation in its ability to include contextual\nsemantic information, resulting in a higher likelihood of false\nalarms in inshore scenarios. Conversely, the horizontal box\ncontains richer contextual information; but, its susceptibility to\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nZHOU et al.: INSHORE DENSE SHIP DETECTION IN SAR IMAGES BASED ON ESD AND TRANSFORMER 4883\nhigher levels of inshore interference creates a pressing issue for\ndetection of inshore ships. One critical challenge lies in how to\nsuppress the inshore interference when working with a horizon-\ntal box containing signiﬁcant contextual semantic information.\nAnother approach was used by Ma et al.[19] who employed\nkey point estimation to differentiate individual targets in dense\ninshore ships. However, this method could only highlight the\ncentral area of the ship, not the edges, resulting in a biased ﬁt\nof inshore ship labels in the dense region. Su et al.[20] and Wu\net al.[21] attempted to overcome the problem of dense overlap\nby merging the edge semantic information of ships and em-\nploying instance segmentation methods for detection. However,\ntheir algorithms were complex and inefﬁcient to implement.\nGe et al.[22] demonstrated that decoupling the detection layer\ncan improve the regression and classiﬁcation performance of\nthe model. Decoupling can be utilized to construct a simple\nand easy-to-implement module that introduces inshore edge\nsemantic information about ships, thereby addressing the miss\ndetection problem in dense inshore scenarios.\nThe inshore ship labels often contain interference informa-\ntion, such as land, which can lead to land false alarms and\nmislead ship feature learning, as illustrated in Fig.1. Wang\net al.[23] and Hou et al.[24] showed that introducing contextual\nsemantic information in combination with the scene effectively\nreduces land false alarms in the inshore region. Ke et al.[25]\nexpanded the encoding and increased contextual information\nto obtain feature maps of multiple sensory ﬁelds, which was\nmore effective but computationally complex and unsuitable for\npractical applications. Zhu et al.[26] demonstrated that intro-\nducing transformer can capture contextual information more\ncomprehensively, especially for high-density occlusion objects,\nwith minimal computational overhead. The structure of the\ntransformer is composed of an encoder and decoder, which can\nbetter obtain the contextual semantic information of the target,\nimprove the feature extraction ability, and better locate the edges\nof the target in the target detection[27]. Therefore, transformer\ncan be introduced to build a plug-and-play module for learning\ncontextual information to reduce the land false alarms caused\nby land area interference in ship labels.\nIn this article, an SAR ship detection method based on edge\nsemantic decoupling and transformer is proposed to address the\nissue of inshore dense ship detection. To tackle the challenge\nof miss detection caused by inshore label overlap, a semantic\nsegmentation layer is added by decoupling the detection layer,\nthereby enhancing the model’s ability to differentiate ship edges\nand reduce miss detection in dense scenes. Furthermore, to\nmitigate the interference from regions, such as land in the labeled\nboxes and facilitate feature learning, a transformer detection\nlayer is constructed that leverages the transformer’s capacity\nto capture global and contextual information. This enables the\nmodel to better distinguish between inshore false alarm targets\nand ship targets, leading to a reduction in land false alarms.\nIn summary, it is worthwhile to note the following contribu-\ntions of the proposed method.\n1) The edge semantic decoupling (ESD) module is intro-\nduced to address the challenge of distinguishing between\ndense inshore ship targets in SAR images. By adding\nsemantic segmentation branches and incorporating edge\nsemantic information, the model is able to accurately\ndiscriminate between ships even in regions with signiﬁcant\noverlap between inshore labeled boxes.\n2) The transformer detection layer (TDL) is introduced to\nlimit interference caused by land areas and other regions\nwithin the labeled box. By taking advantage of the trans-\nformer’s ability to capture global and contextual informa-\ntion, the TDL helps to reduce false alarms and improve\nthe accuracy of ship target detection.\nThe rest of this article is organized as follows. SectionII\npresents the proposed method. In Section III, the proposed\nmethod is validated by comparison to other methods. SectionIV\npresents discussions. Finally, SectionV concludes this article.\nII. METHODOLOGY\nFig. 2 illustrates the overall structure of the proposed method.\nIt consists of the feature extraction and detection layer parts, with\nthe solid orange line representing the improved ESD module\nand the transformer-based TDL module. In SectionII-A,t h e\nESD module designed for SAR images of dense inshore ships is\nintroduced ﬁrst. Then the design details of the TDL module are\npresented. Finally, the decoupling loss function is presented.\nA. Edge Semantic Decoupling Module\nInshore ships are known to have a dense appearance with\nsigniﬁcant labeled box overlap, which can negatively impact\nthe model’s ability to assess individual ship targets, resulting\nin the inclusion of several ship targets within a detection box\n(i.e., missed detection), as illustrated in Fig.1(b). Conventional\nsingle-stage detection algorithms utilize a single branch to si-\nmultaneously handle the tasks of classiﬁcation and detection box\ncoordinate regression. However, the goals of classiﬁcation and\nlocalization is different, as classiﬁcation is primarily concerned\nwith the texture information of the target, while localization is\nfocused on the edge information of the target. This difference in\nfocus can lead to conﬂicts between the two tasks as follows.\n1) Higher-level convolutional ﬁelds have a larger receptive\nﬁeld, allowing them to extract more global information,\nwhich is useful for classiﬁcation. However, the corre-\nsponding areas in the original image become larger, which\ncan be detrimental to localization. Therefore, while the\ninformation contained in higher level feature maps is\nadvantageous for classiﬁcation, it is not necessarily helpful\nfor localization.\n2) Lower-level convolution and other operations correspond\nto smaller areas of the original image, making them more\naccurate for localization. However, they may only contain\nlocal information about the object and therefore are not\nsuitable for classiﬁcation. Consequently, the information\ncontained in the lower-level feature map is suitable for\ntarget localization but not for classiﬁcation.\nIn order to overcome the limitations of performing the two\ntasks in one branch, the approach proposed in this article is\ninspired by[22] and [28], which separates the tasks of classiﬁ-\ncation and detection box coordinate regression into two distinct\n4884 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 2. Overall structure of the method proposed in this article.\nFig. 3. (a) Single-branch structure. (b) Edge semantic decoupling (ESD)\nstructure.\nbranches. Speciﬁcally, an additional ship semantic segmentation\nbranch is incorporated to capture edge information of ships,\nenhancing the model’s ability to differentiate between different\nship targets in dense scenes and reducing the occurrence of\nmissed detections.\nThe ESD module and the conventional single branch are\ncompared in the middle of Fig.3. It can be observed that the\ndecoupled detection head performs multiple tasks, obtaining\nthe results of both ship detection and semantic segmentation\nsimultaneously, compared to the conventional single branch. All\nthree branches, regression, classiﬁcation, and segmentation, are\nused separately after the backbone network to further specialize\nin learning features using more convolutional layers, decoupling\nwhile making the learned features richer and helping to further\ndeﬁne the ship’s position and edges. This not only introduces\nsemantic information about the ship’s edge but also signiﬁcantly\nimproves the model’s scalability and ability to carry out multiple\ntasks. An example of conventional single-branch decoding is\nprovided as follows for reference:\nInfersingle = Conv (ch,clsdet +numreg) (1)\nwhere Infersingle represents the prediction result of the con-\nventional single-branch structure, and Conv represents the 2-D\nconvolution. ch represents the number of the channels of the\nfeature map extracted by the detection layer, which is set to\n255 in this article. clsdet represents the number of detected target\ncategories, which is set to 1 as there is only one ship category in\nthe detection task of this article. numreg represents the coordinate\nvalues of the regression. It is set to 4, which corresponds to the\nupper left and lower right horizontal and vertical coordinates of\nthe detection box. The edge semantic decoupling is as follows:\nInferdecouple = Conv (ch,clsdet)+ Conv (ch,numreg)\n+Conv (ch,clsseg) (2)\nwhere Inferdecouple is the prediction result of the edge semantic\ndecoupling structure and clsseg is the number of split categories.\nSince there is only one ship category in the detection task of this\narticle, the value is set to 1.\nB. Transformer Detection Layer Module\nDue to the proximity of inshore ships to land, their labeled\nboxes often include land, as illustrated in Fig.1(a). This can lead\nto the model mistakenly identifying certain features of the land\nas features of the ship in the absence of contextual information,\nresulting in false alarms.\nCompared to CNN, the transformer architecture can efﬁ-\nciently extract contextual information of the target by uniformly\ncropping the input into multiple patches and utilizing a multi-\nheaded attention mechanism[29]. To mitigate the impact of land\nareas in the detection labels, this article introduces transformer\nto construct the TDL module, which incorporates contextual\ninformation and enhances the model’s ability to distinguish land\ntargets, thus reducing false alarms.\nThe input of transformer is a 1-D sequence of token embed-\ndings. To handle 2-D feature maps, feature mapsx ∈ Rh×w×c\nare reshaped into a sequence of ﬂattened 2-D patches xp ∈\nRn×(p2·c), where(h,w)is the resolution of the feature map,cis\nthe number of channels,(p,p) is the resolution of each feature\npatch, andn = hw/p2 is the resulting number of patches. The\nprocess of patch embedding can be described as\nOutputembedding = Flatten(Conv(Part(x))), (3)\nZHOU et al.: INSHORE DENSE SHIP DETECTION IN SAR IMAGES BASED ON ESD AND TRANSFORMER 4885\nFig. 4. TDL module structure.\nwhere Outputembedding means embedded patches, Part repre-\nsents the chunking operation, which divides the input feature\nmap into patches of a speciﬁc size. Conv is used to reduce\ndimensions, and Flatten is used to construct a 1-D vector by\npulling ﬂat.\nThe design of the transformer detection layer is illustrated in\nFig. 4, which mainly consists of a multihead attention module\nand a feedforward neural network multilayer perceptron (MLP)\nmodule.\nThe multihead attention module aliquots the input x ∈\nRN×din in the feature dimension to obtain several copies of\nxi ∈ RN×di ,i =1 ,2,...n , whereN is the sequence length,\nddenotes the feature dimension and∑ n\ni=1 di = din. Eachxi is\nprocessed with an attention to obtainn copies of the output,\nwhich are then stitched together in the feature dimension to\nobtain the ﬁnal result. The calculation of a single attention is\nas follows:\nAttention(Q,K,V )=S o f t m a x\n(QKT\n√\nd\n+B\n)\nV (4)\nwhere Q, K, andV are obtained from the inputxi through the\nfully connected layer andB is the position information. The\ncomponents of MLP are shown as follows:\nMLP = drop (fc(drop(act(fc(x))))) (5)\nwhere drop means dropout operation,fc means fully connected\nlayer, x is the input, act represents Gaussian error linear unit\n(GELU) activation function as follows:\nGELU(x)= xP(X ≤ x)= xΦ(x)\n≈ 0.5x\n(\n1+tanh\n[√\n2/π\n(\nx+0.044715x3)])\n.\n(6)\nThe input of transformer encoder are embedded patches, and\nthe added LayerNorm and Dropout layers are used to prevent\noverﬁtting. The TDL module mainly replaces one convolutional\nlayer of the detection layer, which enhances the ability to capture\ndiverse contextual information with only a minor increase in\ncomputational cost. It also leverages the self-attention mecha-\nnism to explore the potential of feature representation.\nC. Decoupling Loss Function\nThe decoupling loss function in this article is designed to\noptimize both the detection and segmentation tasks in a single\nnetwork structure. Instead of training and optimizing the two\ntasks individually, the decoupling loss allows for the inclusion\nof edge semantic information in the ship detection optimization\nprocess by back-propagating the loss after both ship segmenta-\ntion and detection have been performed.\nThe designed decoupling loss function has two components,\nnamely, 1) ship detection loss function and 2) ship semantic\nsegmentation loss function.\n1) Ship Detection Loss Function: The loss function of the\nship detection component is as follows:\nlossdet =l o s sCIoU +losscls . (7)\nlossCIoU is the detection box regression loss. In order to more\neffectively ﬁlter out the high quality detection results that are\ncloser to the labeled box, this article uses CIoU[30] as the\nregression loss for ship detection. The CIoU is calculated as\nlossCIoU =1 −IoU + ρ2 (Bp,Bg)\nc2 +α (8)\nwhere ρ2(Bp,Bg) represents the Euclidean distance between\nthe center point of the detection box and the labeled box,Bp is\nthe detection box,Bg is the labeled box,crepresents the length\nof the diagonal between the upper left and lower right corners of\nthe smallest outer rectangle of the detection box and the labeled\nbox. α is a parameter to measure the consistency of the aspect\nratio andv is a tradeoff parameter\nα = v\n1−IoU +v (9)\nv = 4\nπ2\n(\narctan wgt\nhgt −arctan w\nh\n)2\n(10)\nwhere w and h represent the width and height of the prediction\nbox, respectively;wgt and hgt represent the width and height of\nthe labeled box, respectively. The IoU is deﬁned as\nIoU(Bp,Bg)= |Bp ∩Bg|\n|Bp ∪Bg|. (11)\nlosscls is the category classiﬁcation loss for ship detection. In\nthe dataset used in this study, there is only one category, so the\nforeground and background of the ships need to be separated.\nThe binary cross-entropy function used is shown as follows:\nlosscls = −1\nn\nn∑\ni=1\n[Iobj logd+(1 −Iobj)log(1 −d)] (12)\n4886 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 5. SSDD dataset labels.\nwhere Iobj is the value of the detection label: 0 for the background\nand 1 for the ship.d is the output of the detection layer by the\nSigmoid function, andn is the number of detected samples.\n2) Ship Semantic Segmentation Loss Function: Since the\nship semantic segmentation also needs to distinguish between\njust two types of pixels, i.e., ship and background, the same\nbinary cross-entropy function is used as(12)\nlossseg = −1\nn\nn∑\ni=1\n[Iobj logs+(1 −Iobj)log(1 −s)] (13)\nwhere Iobj is the true value of the semantic segmentation of the\nship edges. The value 0 indicates that the pixel belongs to the\nbackground, whereas 1 indicates that it belongs to the ship.s\nis the output of the segmentation layer by the Sigmoid function\nand n is the number of segmented samples.\nThe joint loss for detection and segmentation in this article\nis shown in(14), and the values of the optimization process are\ninvolved in the ﬁnal parameter update.\nlosstotal =l o s sdet +lossseg . (14)\nIII. EXPERIMENTAL RESULTS ANDDISCUSSION\nThe effectiveness of the proposed method is evaluated using\nthe publicly accessible dataset SSDD labeled by Zhang et al.[7].\nThis section ﬁrst introduces the dataset and the hyperparameter\nsettings of the experiments, followed by presenting the ablation\nexperiment results of each module. Finally, the proposed method\nis compared with the current mainstream detection algorithms.\nA. Dataset and Experimental Parameter Setting\nThe SSDD dataset used in this study contains SAR im-\nages with resolutions ranging from 1–15 m, sourced from\nRADARSAT-2, TerraSAR-X, and Sentinel-1. The dataset in-\ncludes ship detection box labels as well as ship semantic seg-\nmentation labels. An example of a labeled dataset can be seen\nin Fig.5.\nThe SSDD dataset was labeled with 1160 images containing\n2456 ship targets. Among the 2456 ship targets in the dataset,\n928 were used for training and 232 were used for testing. In\naddition, 46 of the test images were taken from the coast and\n186 were taken from the ocean.\nAlgorithm 1: Update Parameters During Training.\nThe network is implemented using the Pytorch deep learning\nframework. The optimizer utilized is stochastic gradient descent\n(SGD) with momentum. A Geforce RTX 2070 GPU is used to\ntrain 100 epochs, starting with an initial learning rate of 1e-3,\nmomentum of 0.9, and weight decay of 5e-4. Joint training is\nnecessary to incorporate ship edge semantic information into the\noptimization of the ship detection model throughout the training\nphase. Given that semantic segmentation and target detection\nhave different labels, the losses are calculated separately during\ntraining. First, the segmentation and detection losses are added\ntogether for back-propagation. Then, the gradient is accumulated\nto a preset value and a parameter update is performed. The\ntraining process is shown in Algorithm1.\nGradient accumulation training offers the advantage of\nachieving large batches even on machines with limited video\nmemory, thereby mitigating the oscillations lost during training\nand allowing for faster acquisition of the best model.\nB. Evaluation Metric\nIn this article, the average precision (AP) metric is used to\nassess the performance of the ship detection model, which is\ncalculated as follows:\nAP =\n∫ 1\n0\nP(R)dR×100% (15)\nwhere\nP= TP\nTP +FP (16)\nR= TP\nTP +FN (17)\nwhere TP, FP, and FN refer to the number of correctly predicted\nship targets, the number of incorrectly predicted ship targets,\nand the number of ship targets judged to be nonship targets,\nrespectively. P represents the accuracy rate, which is the pro-\nportion of the number of correct predictions to the total number\nof predictions among all predictions.Rrepresents the recall rate,\nwhich is the proportion of the number of correctly predicted ship\ntargets to the total number of annotations among all annotated\nship targets. AP describes the area under the Precision–Recall\nZHOU et al.: INSHORE DENSE SHIP DETECTION IN SAR IMAGES BASED ON ESD AND TRANSFORMER 4887\nTABLE I\nCOMPARISON OF THEESD MODULE WITH CONVENTIONAL SINGLE-BRANCH\nSTRUCTURE\nFig. 6. Feature map visualization.\n(P-R) curve. It is a compromise between the two metrics and\nalso shows the overall performance of different methods.\nC. Effectiveness of ESD\nThe aim of the ESD module is to reduce the ship’s missed\ndetection in inshore dense scenarios. To assess the module’s\neffectiveness, comparative experiments were performed on the\npublicly available SSDD dataset, and the experimental results\nare shown in TableI.\nIn Table I, P, R and AP represent the precision, recall,\nand average precision in the inshore region, respectively. AP50\nrepresents the AP of inshore ship detection calculated with 0.5\nas the threshold value. AP50−95 means that the threshold value\nof IOUs is taken from 0.5 to 0.95 in steps of 0.05, and then the\naverage value of APs under these IOUs is calculated. Compared\nwith AP50, the calculation of AP50−95 is more rigorous and\nbetter reﬂects the advantages and disadvantages of the model.\nAs can be seen from TableI, compared to the conventional\nsingle-branch detection structure, theP, R,A P50, and AP50−95\nof the inshore ships are improved after adding the ESD module.\nThe Rhas increased by 2.31%, indicating that the miss detection\nof the inshore ships is alleviated. To further analyze the impact\nof the ESD module, feature visualization is performed in this\narticle. The PLT image processing package is used in the model\ninference process to save the feature matrices at different scales\nby channel and colorize them, which makes the visualized\nfeature maps visually better compared with grayscale maps. The\nfeature visualization results in Fig.6 also demonstrate that the\ninclusion of the ESD module results in sharper edges of ships\nTABLE II\nABLATION EXPERIMENTS OF ESD AND TDL\nFig. 7. P-R curve of the ablation experiment of ESD and TDL.\nin dense areas and clearer distinction between individual ships.\nThe interference in the land area is also effectively suppressed,\nwhich helps to decrease the rate of miss detection in the dense\ninshore scenario.\nD. Ablation Experiments of ESD and TDL Module\nThe proposed transformer-based TDL module is capable of\neffectively extracting contextual information about the target,\nwhich enables it to accurately differentiate between the ship\ntarget and land-based false alarms. Compared with the baseline,\nthe TDL module is added, and the number of model parame-\nters only increases by 0.0064%, which is a small increase in\ncomputational burden. Ablation experiments were conducted\nto conﬁrm the effectiveness of this module. According to the\nexperimental results in TableII, the addition of the TDL detec-\ntion layer improves theP, R,A P50, and AP50−95. Compared to\nthe baseline algorithm, theP improves by 5.96%, and adding\nTDL to ESD, theP improves by 0.73%, indicating that TDL\ncan effectively combine contextual information to reduce false\nalarms in the inshore region. The AP50−95 is improved by\n5.16%, indicating that the model’s overall performance has been\noptimized. The P-R curves in Fig.7 with the enclosed region of\nthe coordinate axes are the values of AP50. From which, the\nimprovement in accuracy of the proposed method in this article\ncan be seen more intuitively.\nTo demonstrate the superiority of the proposed method in\nthis article, comparative experiments were conducted with typ-\nical two-stage detection algorithms (Faster R-CNN[14],C a s -\ncade R-CNN [15]), rotating box algorithm (OSCD-Net[31])\nand typical single-stage detection algorithms (YOLOv3[16],\n4888 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 8. Comparison with some detection results of relevant benchmark methods.\nTABLE III\nCOMPARISON WITH OTHER WELL-KNOWN METHODS\nYOLOv4 [17]). Among the above methods, the backbone net-\nwork of faster R-CNN, cascade R-CNN, and OSCD-Net is\nResNet. YOLOv3, YOLOv4, and the backbone network of the\nmethod proposed in this article use DarkNet. The ship detection\nresults are shown in TableIII, where the experimental results of\nOSCD-Net is derived from[31].\nThe proposed method in this article has been shown to be more\neffective than several commonly used conventional single-stage,\ntwo-stage, and rotate box detection algorithms in detecting SAR\ninshore ships. The detection results of different comparison\nmethods are shown in Fig. 8. The ground truth images are\ncolorized for different ship targets to facilitate better differen-\ntiation of dense adjacent ship targets. The red ellipse in the\nresult comparison graph indicates the miss detection and the\nyellow ellipse indicates the false alarm. These visualizations\nprovide an intuitive demonstration of the effectiveness of the\nproposed method in suppressing false alarms and miss detection\nin the inshore scenario when compared to conventional detection\nalgorithms.\nIV . DISCUSSION\nThe detection of inshore ships presents a greater challenge\nthan that of ships located solely at sea due to the higher rates of\nfalse alarms and missed detections.\nThe higher rates of false alarms are due to that SAR images\nof inshore ship targets are prone to interference from non-\nship targets. Therefore, context information is needed to better\ndifferentiate between ships and false alarms. In this article, a\ntransformer-based TDL detection layer is introduced to capture\nglobal and context information, and comparative experiments\nhave shown that adding TDL can effectively reduce false alarms.\nThe higher rates of missed detections are due to that inshore\nships are densely arranged, making it difﬁcult to distinguish\nbetween adjacent targets. To alleviate this issue, ship edge se-\nmantic information needs to be introduced to better distinguish\nadjacent targets. In this article, by decoupling the detection layer\nand adding a semantic segmentation branch to introduce ship\nedge information, the ship recall rate was improved and missed\ndetections were reduced. Compared to methods that increase\ncomputational complexity, such as dilated convolution or fusion\nZHOU et al.: INSHORE DENSE SHIP DETECTION IN SAR IMAGES BASED ON ESD AND TRANSFORMER 4889\nof high-resolution feature layers to add context information, the\nproposed TDL layer only adds a small number of parameters\nwhile achieving high accuracy. Compared to using instance seg-\nmentation to introduce ship edge information, the proposed ESD\nmethod is simple to implement and does not require complex\ninstance labels, making it an effective and easily implementable\nmodule.\nHowever, training the proposed method requires ship se-\nmantic segmentation labels, which undoubtedly increases the\nannotation workload for large datasets. How to reduce the de-\npendence of the decoupled semantic segmentation layer on ship\nsemantic labels is a direction for future algorithm improvements.\nCompared to SAR images of purely sea scenes, inshore scenes\nare more complex and ship detection is more difﬁcult. Therefore,\nhow to improve the detection of inshore ships while increasing\na minimal or even no computational burden is an important and\nmeaningful research direction.\nV. CONCLUSION\nThis article presents a novel method for detecting dense\ninshore ships in SAR images using an ESD module and a\ntransformer-based TDL layer. The ESD module incorporates\nedge semantic information of inshore ship targets during train-\ning, improving the model’s ability to distinguish between neigh-\nboring ships and reducing miss detections. Meanwhile, the\nTDL layer utilizes transformer to extract contextual information\nand reduce false alarms caused by interference from land and\nother regions in the labeled boxes. The results of comparison\nexperiments with some two-stage and single-stage detection\nalgorithms on the SSDD dataset showed the proposed method\nachieved the highest AP50 of 96.72%, demonstrating its effec-\ntiveness in detecting inshore ships. The simple structure of the\nsingle-stage detector makes it easier to perform improvements\nand experiments, so this article performs experimental valida-\ntion on a single-stage detector. However, the TDL and ESD\nproposed in this article are both plug-and-play improvement\nmodules, which are less dependent on the overall structure of\nthe detection algorithm. Theoretically, they can be fully ported\nto two-stage detectors, and whether the porting is effective\nrequires extensive experimental veriﬁcation. The future work is\nto explore the potential of combining this method with other\ntwo-stage detection frameworks for further optimization and\nimprovement.\nACKNOWLEDGMENT\nThe authors would like to thank Zhang et al.[7] for providing\nSSDD dataset used in this article. The authors would also like ex-\npress their deepest gratitude to the anonymous reviewers, whose\ncareful work and thoughtful suggestions helped to improve this\narticle considerably.\nREFERENCES\n[1] K. El-Darymli, P. McGuire, D. Power, and C. R. Moloney, “Target de-\ntection in synthetic aperture radar imagery: A state-of-the-art survey,”J.\nAppl. Remote Sens., vol. 7, no. 1, Mar. 2013, Art. no. 071598.\n[2] Z. Yue et al., “A novel semi-supervised convolutional neural network\nmethod for synthetic aperture radar image recognition,”Cogn. Compu-\ntation, vol. 13, no. 4, pp. 795–806, 2021.\n[3] H.-C. Li, W.-S. Hu, W. Li, J. Li, Q. Du, and A. Plaza, “A3CLNN:\nSpatial, spectral and multiscale attention ConvLSTM neural net-\nwork for multisource remote sensing data classiﬁcation,” IEEE\nTrans. Neural Netw. Learn. Syst. , vol. 33, no. 2, pp. 747–761,\nFeb. 2022.\n[4] J.-Y . Yang, H.-C. Li, W.-S. Hu, L. Pan, and Q. Du, “Adaptive cross-\nattention-driven spatial–spectral graph convolutional network for hyper-\nspectral image classiﬁcation,”IEEE Geosci. Remote Sens. Lett., vol. 19,\n2022, Art. no. 6004705.\n[5] F. Ma, F. Zhang, D. Xiang, Q. Yin, and Y . Zhou, “Fast task-speciﬁc region\nmerging for SAR image segmentation,”IEEE Trans. Geosci. Remote Sens.,\nvol. 60, 2022, Art. no. 5222316.\n[6] F. Ma, F. Zhang, Q. Yin, D. Xiang, and Y . Zhou, “Fast SAR im-\nage segmentation with deep task-speciﬁc superpixel sampling and soft\ngraph convolution,” IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5214116.\n[7] T. Zhang et al., “SAR ship detection dataset (SSDD): Ofﬁcial re-\nlease and comprehensive data analysis,”Remote Sens., vol. 13, no. 18,\npp. 3690–3730, Sep. 2021.\n[8] Y . Wang, C. Wang, H. Zhang, Y . Dong, and S. Wei, “A SAR dataset of ship\ndetection for deep learning under complex backgrounds,”Remote Sens.,\nvol. 11, no. 7, pp. 765–778, Mar. 2019.\n[9] X. Sun, Z. Wang, Y . Sun, W. Diao, Y . Zhang, and K. Fu, “AIR-SARShip-\n1.0: High-resolution SAR ship detection dataset,”J. Radar, vol. 8, no. 6,\npp. 852–862, 2019.\n[10] T. Zhang et al., “LS-SSDD-v1.0: A deep learning dataset dedicated to\nsmall ship detection from large-scale Sentinel-1 SAR images,”Remote\nSens., vol. 12, no. 18, pp. 2997–3033, Sep. 2020.\n[11] X. Sun et al., “FAIR1M: A benchmark dataset for ﬁne-grained object\nrecognition in high-resolution remote sensing imagery,”ISPRS J. Pho-\ntogrammetry Remote Sens., vol. 184, pp. 116–130, 2022.\n[12] Y . Zhou, F. Zhang, F. Ma, D. Xiang, and F. Zhang, “Small vessel detection\nbased on adaptive dual-polarimetric feature fusion and sea-land segmen-\ntation in SAR images,”IEEE J. Sel. Topics Appl. Earth Observ. Remote\nSens., vol. 15, pp. 2519–2534, 2022.\n[13] F. Ma, X. Sun, F. Zhang, Y . Zhou, and H.-C. Li, “What catch your attention\nin SAR images: Saliency detection based on soft-superpixel lacunarity\ncue,” IEEE Trans. Geosci. Remote Sens., vol. 61, 2023, Art. no. 5200817.\n[14] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time\nobject detection with region proposal networks,”IEEE Trans. Pattern Anal.\nMach. Intell., vol. 39, no. 6, pp. 1137–1149, Jun. 2017.\n[15] Z. Cai and N. Vasconcelos, “Cascade R-CNN: Delving into high quality\nobject detection,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., Salt\nLake City, UT, USA, 2018, pp. 6154–6162.\n[16] J. Redmon and A. Farhadi, “YOLOv3: An incremental improvement,”\n2018, arXiv:1804.02767.\n[17] A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao, “YOLOv4: Optimal\nspeed and accuracy of object detection,” 2020,arXiv:2004.10934.\n[18] T. Tian, Z. Pan, X. Tan, and Z. Chu, “Arbitrary-oriented inshore ship\ndetection based on multi-scale feature fusion and contextual pooling on\nrotation region proposals,” Remote Sens., vol. 12, no. 2, pp. 339–357,\nJan. 2020.\n[19] X. Ma, S. Hou, Y . Wang, J. Wang, and H. Wang, “Multiscale and dense\nship detection in SAR images based on key-point estimation and at-\ntention mechanism,” IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5221111.\n[20] H. Su et al., “HQ-ISNet: High-quality instance segmentation for re-\nmote sensing imagery,” Remote Sens., vol. 12, no. 6, pp. 989–1012,\nMar. 2020.\n[21] Z. Wu, B. Hou, B. Ren, Z. Ren, S. Wang, and L. Jiao, “A deep detec-\ntion network based on interaction of instance segmentation and object\ndetection for SAR images,”Remote Sens., vol. 13, no. 13, pp. 2582–2607,\nJul. 2021.\n[22] Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “YOLOX: Exceeding yolo series\nin 2021,” 2021,arXiv:2107.08430.\n[23] R. Wang, Y . Huang, Y . Zhang, J. Pei, J. Wu, and J. Yang, “An inshore\nship detection method in SAR images based on contextual ﬂuctuation\ninformation,” inProc. 6th Asia-Paciﬁc Conf. Synthetic Aperture Radar,\nXiamen, China, 2019, pp. 1–5.\n[24] X. Hou and F. Xu, “Inshore ship detection based on multi-aspect infor-\nmation in high-resolution SAR images,” inProc. 6th Asia-Paciﬁc Conf.\nSynthetic Aperture Radar. Xiamen, China, 2019, pp. 1–4.\n4890 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\n[25] X. Ke, X. Zhang, and T. Zhang, “GCBANet: A global context boundary-\naware network for SAR ship instance segmentation,”Remote Sens., vol. 14,\nno. 9, pp. 2165–2185, Apr. 2022.\n[26] X. Zhu, S. Lyu, X. Wang, and Q. Zhao, “TPH-YOLOv5: Improved\nYOLOv5 based on transformer prediction head for object detection on\ndrone-captured scenarios,” inProc. IEEE/CVF Int. Conf. Comput. Vis.,\n2021, pp. 2778–2788.\n[27] K. Li, M. Zhang, M. Xu, R. Tang, L. Wang, and H. Wang, “Ship detection in\nSAR images based on feature enhancement swin transformer and adjacent\nfeature fusion,”Remote Sens., vol. 14, no. 13, pp. 3186–3208, Jul. 2022.\n[28] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for\ndense object detection,” in Proc. IEEE Int. Conf. Comput. Vis., 2017,\npp. 2980–2988.\n[29] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” 2020,arXiv:2010.11929.\n[30] Z. Zheng et al., “Enhancing geometric factors in model learning and\ninference for object detection and instance segmentation,”IEEE Trans.\nCybern., vol. 52, no. 8, pp. 8574–8586, Aug. 2022.\n[31] J. Zhang, R. Huang, Y . Li, and B. Pan, “Oriented ship detection based on\nintersecting circle and deformable ROI in remote sensing images,”Remote\nSens., vol. 14, no. 19, pp. 4749–4769, Sep. 2022.\nYongsheng Zhou (Member, IEEE) received the B.E.\ndegree in communication engineering from Beijing\nInformation Science and Technology University, Bei-\njing, China, in 2005, and the Ph.D. degree in sig-\nnal and information processing from the Institute of\nElectronics, Chinese Academy of Sciences, Beijing,\nChina, in 2010.\nHe was with the Academy of Opto-Electronics,\nChinese Academy of Sciences, during 2010 and 2019.\nHe is currently a Professor of Electronic and Infor-\nmation Engineering with the College of Information\nScience and Technology, Beijing University of Chemical Technology, Beijing,\nChina. His research interests include target detection and recognition from\nmicrowave remotely sensed image, digital signal, and image processing.\nFeixiang Zhang received the M.S. degree in informa-\ntion and communication engineering from the Col-\nlege of Information Science and Technology, Beijing\nUniversity of Chemical Technology, Beijing, China,\nin 2022.\nHis research interests include image processing\nand deep learning-based small target detection.\nQiang Yin (Senior Member, IEEE) received the\nB.S. degree in electronic and information engineering\nfrom Beijing University of Chemical Technology,\nBeijing, China, in 2004, and the M.S. and Ph.D.\ndegrees in signal and information processing from the\nInstitute of Electronics, Chinese Academy of Science,\nBeijing, in 2008 and 2016, respectively.\nFrom 2008 to 2013, she was a Research Assistant\nwith the Institute of Electronics, Chinese Academy\nof Sciences. From 2014 to 2015, she was a Research\nFellow with the European Space Agency, Rome, Italy.\nShe is currently an Associate Professor with the College of Information Science\nand Technology, Beijing University of Chemical Technology. Her research\ninterests include polarimetric/polarimetric interferometric synthetic aperture\nradar and deep learning.\nFei Ma (Member, IEEE) received the B.S., M.S., and\nPh.D. degrees in electronic and information engineer-\ning from Beijing University of Aeronautics and As-\ntronautics, Beijing, China, in 2013, 2016, and 2020,\nrespectively.\nHe is currently with the College of Information\nScience and Technology, Beijing University of Chem-\nical Technology, Beijing, China, as an Associate\nProfessor. His research interests include radar signal\nprocessing, image processing, machine learning, and\ntarget detection.\nFan Zhang (Senior Member, IEEE) received the B.E.\ndegree in communication engineering from the Civil\nAviation University of China, Tianjin, China, in 2002,\nthe M.S. degree in signal and information processing\nfrom Beihang University, Beijing, China, in 2005, and\nthe Ph.D. degree in signal and information processing\nfrom Institute of Electronics, Chinese Academy of\nSciences, Beijing, in 2008.\nHe is currently a Full Professor of Electronic and\nInformation Engineering with the Beijing University\nof Chemical Technology, Beijing, China. His research\ninterests incldue remote sensing image processing, high-performance comput-\ning, and artiﬁcial intelligence.\nDr. Zhang is also an Associate Editor of IEEE ACCESS and a Reviewer of\nthe IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, the IEEE\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS ANDRE-\nMOTE SENSING, the IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,a n d\nthe Journal of Radars.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7697292566299438
    },
    {
      "name": "Synthetic aperture radar",
      "score": 0.6588135361671448
    },
    {
      "name": "Segmentation",
      "score": 0.6187644600868225
    },
    {
      "name": "Transformer",
      "score": 0.5718352794647217
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47743257880210876
    },
    {
      "name": "Object detection",
      "score": 0.4437197744846344
    },
    {
      "name": "Pixel",
      "score": 0.4104444980621338
    },
    {
      "name": "Computer vision",
      "score": 0.376873254776001
    },
    {
      "name": "Remote sensing",
      "score": 0.3587421774864197
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3444865345954895
    },
    {
      "name": "Geology",
      "score": 0.16617965698242188
    },
    {
      "name": "Engineering",
      "score": 0.09584164619445801
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I75390827",
      "name": "Beijing University of Chemical Technology",
      "country": "CN"
    }
  ],
  "cited_by": 18
}