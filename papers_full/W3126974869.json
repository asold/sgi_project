{
  "title": "Combining pre-trained language models and structured knowledge",
  "url": "https://openalex.org/W3126974869",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287562951",
      "name": "Colon-Hernandez, Pedro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160993805",
      "name": "Havasi Catherine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287562953",
      "name": "Alonso, Jason",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287129463",
      "name": "Huggins, Matthew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3049364175",
      "name": "Breazeal, Cynthia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3026990524",
    "https://openalex.org/W2997545008",
    "https://openalex.org/W2968908603",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2950501607",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W3103296573",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W2952828476",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2998374885",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2971600926",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2970555085",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1978620866",
    "https://openalex.org/W2250861254",
    "https://openalex.org/W2978010506",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094815596",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2972119829",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2945392868",
    "https://openalex.org/W2985671014",
    "https://openalex.org/W2972969579",
    "https://openalex.org/W2107901333",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W3040558716",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3021578384",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2991612931",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W3017231514"
  ],
  "abstract": "In recent years, transformer-based language models have achieved state of the art performance in various NLP benchmarks. These models are able to extract mostly distributional information with some semantics from unstructured text, however it has proven challenging to integrate structured information, such as knowledge graphs into these models. We examine a variety of approaches to integrate structured knowledge into current language models and determine challenges, and possible opportunities to leverage both structured and unstructured information sources. From our survey, we find that there are still opportunities at exploiting adapter-based injections and that it may be possible to further combine various of the explored approaches into one system.",
  "full_text": "Combining pre-trained language models and\nstructured knowledge\nPedro Colon-Hernandez∗\nMIT Media Lab\nCatherine Havasi\nDalang Health\nJason Alonso\nDalang Health\nMatthew Huggins\nMIT Media Lab\nCynthia Breazeal\nMIT Media Lab\nIn recent years, transformer-based language models have achieved state of the art performance\nin various NLP benchmarks. These models are able to extract mostly distributional information\nwith some semantics from unstructured text, however it has proven challenging to integrate\nstructured information, such as knowledge graphs into these models. We examine a variety\nof approaches to integrate structured knowledge into current language models and determine\nchallenges, and possible opportunities to leverage both structured and unstructured information\nsources. From our survey, we ﬁnd that there are still opportunities at exploiting adapter-based\ninjections and that it may be possible to further combine various of the explored approaches into\none system.\n1. Introduction\nRecent developments in Language Modeling (LM) techniques have greatly improved\nthe performance of systems in a wide range of Natural Language Processing (NLP)\ntasks. Many of the current state of the art systems are based on variations to the\ntransformer (Vaswani et al. 2017) architecture. The transformer architecture, along with\nmodiﬁcations such as the Transformer XL (Dai et al. 2019) and various training regimes\nsuch as the Masked Language Modeling (MLM) used in BERT (Devlin et al. 2018) or\nthe Permutation Language Modeling (PLM) used in XLNet(Yang et al. 2019), uses an\nattention based mechanism to model long range dependencies between text. This mod-\neling encodes syntactic knowledge, and to a certain extent some semantic knowledge\ncontained in unstructured texts.\nThere has been interest in being able to understand what kinds of knowledge are\nencoded in these models’ weights. Hewitt et al. (Hewitt and Manning 2019) devise a\nsystem that generates a distance metric between embeddings for words in language\nmodels such as BERT. They show that there is some evidence that there are syntax trees\nembedded in the transformer language models and this could explain the performance\nof these models in some tasks that utilize syntactic elements of text.\nPetroni et al. (Petroni et al. 2019) build a system (LAMA) to gauge what kinds of\nknowledge are encoded in these weights. They discover that language models em-\n∗75 Amherst St, Cambridge, MA, 02139. E-mail: pe25171@mit.edu.\narXiv:2101.12294v2  [cs.CL]  5 Feb 2021\nbed some facts and relationships in their weights during pre-training. This in turn\ncan help explain the performance of these models in semantic tasks. However, these\ntransformer-based language models have some tendency to hallucinate knowledge\n(whether through bias or incorrect knowledge in the training data). This also means\nthat some of the semantic knowledge they incorporate is not rigidly enforced or utilized\neffectively.\nAvenues of research have begun to open up on how to prevent this hallucination\nand how to inject additional knowledge from external sources into the transformer-\nbased language models. One promising avenue is through the integration of knowl-\nedge graphs such as Freebase(Bollacker et al. 2008), WordNet(Miller 1998), Concept-\nNet(Speer, Chin, and Havasi 2017), and ATOMIC(Sap et al. 2019).\nA knowledge graph (used somewhat interchangeably with knowledge base al-\nthough they are different concepts) is deﬁned as “a graph of data intended to accu-\nmulate and convey knowledge of the real world, whose nodes represent entities of\ninterest and whose edges represent relations between these entities\" (Hogan et al. 2020).\nFormally, a knowledge graph is a set of triples that represents nodes and edges between\nthese nodes. Let us deﬁne a set of vertices (which we will refer to as concepts) asV , a set\nof edges as E (which we will refer to as assertions as per Speer and Havasi(Speer and\nHavasi 2012)), and a set of labels L (which we will refer to as relations). A knowledge\ngraph is a tuple G := (V, E, L)1. The set of edges (E) or assertions is composed of triples\nE ⊆V ×L ×V which are seen as a subject (a concept), a relation (a label), and object\n(another concept) respectively (e.g. (subject, relation, object)). These edges in some\ncases can have weights to represent the strength of the assertion. Broadly speaking,\nknowledge graphs (KGs) are a collection of tuples that represent things that should be\ntrue within the knowledge of the world that we are representing. An example assertion\nis “a dog is an animal\" and its representation as a tuple would be: (dog,isA,animal).\nIdeally, we would want to “inject\" this structured collection of conﬁdent information\n(i.e. knowledge graph) into that of the high-coverage, contextual information found in\nlanguage models. This injection would permit the model to incorporate some of the\ninformation found in the KG to improve its performance in inference tasks.\nThere are currently various approaches that try to achieve this injection. The ap-\nproaches in general take either one or combinations of three forms: input focused\ninjections, architecture focused injections, and output focused injections. We deﬁne an\ninput focused injection as any technique that modiﬁes the data pre-processing or the pre-\ntransformer layer inputs that the base model uses(i.e. injecting knowledge graph triples\ninto the training data to pre-train/ﬁne tune on them or combining entity embeddings\ninto the static word embeddings that the models have). We deﬁnearchitecture focused in-\njections as techniques that alter a base model’s transformer layers (i.e. adding additional\nlayers that inject in some representation). Lastly, we deﬁne an output focused injection\nas any techniques that either modify the output of the base models or that modify/add\ncustom loss functions. In addition to these three basic types, there are approaches that\nutilize combinations of these (i.e. a system that uses both input and output injections),\nwhich we call combination injections. Figure 1 gives an abstract visualization of the types\nof injections that we describe.\nTo be consistent throughout the type of injections, we will now give some deﬁ-\nnitions and nomenclature. Let us deﬁne a sequence of words (unstructured text) as\nS. Typically in a transformer-based model, this sequence of words is converted to a\n1 We use the formal deﬁnitions found in Appendix B of (Hogan et al. 2020)\n2\nsequence of tokens that is then converted into some initial context-independent embed-\ndings. To a word sequence we can apply a tokenization techniqueTto convert the word\nsequence into a token sequence T. This can be seen as T(S) =T. This sequence T is\nused as a lookup in an embedding layerEto produce context independent token vector\nembeddings: E(T) =E. These are then passed sequentially through various contextu-\nalization layers (i.e. transformers) which we deﬁne as the set H, H= (H1, ..., Hn). The\nsuccessive application of these ultimately produces a sequence of contextual embed-\ndings C: C = Hn(Hn−1(...H1(E))). We additionally deﬁne Gas graph embeddings of a\nknowledge graph G that are the result of some embedding function Eg: G= Eg(G). This\nﬁnal sequence is run through a ﬁnal layer LLM that is used to calculate the language\nmodeling loss function Lthat is optimized through back-propagation. The notation that\nwe utilize is intentionally vague on the deﬁnition of the functions, in order for us to ﬁt\nthe different works that we survey.\nIn the following sections we will look at attempts of injecting knowledge graph\ninformation that fall into the aforementioned categories. Additionally, we will highlight\nrelevant beneﬁts in these approaches. We conclude with possible opportunities and\nfuture directions for injecting structured knowledge into language models.\nFigure 1\nVisualization of boundaries of the different categories of knowledge injections. Combination\ninjections involve combinations of the three categories.\n2. Input Focused Injections\nIn this section we will describe knowledge injections whose techniques center around\nmodifying either the structure of the input or the data that is selected to be fed into the\nbase transformer models. A common approach to inject information from a knowledge\ngraph is by converting its assertions into a set of words (possibly including separator\ntokens) and pre-training or ﬁne-tuning a language model with these inputs. We discuss\ntwo particular papers that focus on structuring the input in different ways as to capture\nthe semantic information from triples found in a KG. These approaches start from a pre-\ntrained model, and ﬁne-tune on their knowledge infusing datasets. A summary of these\napproaches can be found in table 1.\nInput focused injections can be seen any technique whose output is a modiﬁed E,\nhereby known asE′. This modiﬁcation can be achieved either by modifyingS,T, T, E,or\ndirectly E. (i.e. the word sequence, the token sequence, the tokenization function, the\ncontext-less embedding function, or the actual context-less embeddings). The hope of\n3\ninput focused injections is that the knowledge in E′will be distributed and contextual-\nized through Has the language models are trained.\n2.1 Align, Mask, Select (AMS) (Ye et al. 2019)\nAMS is an approach in which a question answering dataset is created, whose questions\nand possible answers are generated by aligning a knowledge graph (in this particular\ncase ConceptNet) with plain text. A BERT model is trained on this dataset to inject it\nwith the knowledge.\nTaking an example from their work, the ConceptNet triple (population, AtLocation,\ncity) is aligned with a sentence from the English Wikipedia (i.e. “The largest city by\npopulation is Birmingham, which has long been the most industrialized city.\") that\ncontains both concepts in the triple. They then proceed to mask out one of the concepts\nwith a special token ([QS]) and produce 4 plausible concepts as answers to the masking\ntask by looking at the neighbors in ConceptNet that have the same masked token and\nrelationship. Lastly, they concatenate the generated question with the plausible answers\nand run it through a BERT model tailored for question answering (QA) (following the\nsame approach as the architecture and loss for the SWAG task in the original BERT).\nAt the output, they run the classiﬁcation token ([CLS]) through a softmax classiﬁer to\ndetermine if the selected concept is the correct one or not.\nThe authors note that the work is sensitive to what it has seen in the pre-training\nbecause when asked a question that needs to disambiguate a pronoun it tries to match\nwhat it has seen the most in the training data. This may mean that the generalization of\nthe structured knowledge (here commonsense information) or the understanding of the\nit is overshadowed by the distributional information that it is learning, however more\ntesting would need to be done to verify this. Overall some highlights of the work are:\n• Automated pre-training approach which constructs a QA dataset aligned\nto a KG\n• Utilization of graph-based confounders in generated dataset entries\n2.2 COMmonsEnse Transformers (COMET) (Bosselut et al. 2019)\nCOMET is a GPT(Radford et al. 2018) based system which is trained on triples from KGs\n(ConceptNet and Atomic) to learn to predict the object of the triple (the triples being\ndeﬁned as (subject, relation, object)). The triples are fed as a concatenated sequence of\nwords into the model (i.e. the words for the subject, the relationship, and the object)\nalong with some separators.\nThe authors initialize the GPT model to the ﬁnal weights in the training from\nRadford et al.(Radford et al. 2018) and proceed to train it to predict the words that\nbelong to the object in the triple. A very interesting part of this work is that it is capable\ndirectly of performing knowledge graph completion for nodes and relations that may\nnot have been seen during training, in the form of sentences.\nSome plausible shortcomings of this work is that you are still having the model\nextract the semantic information from the distributional one and possibly suffering\nfrom the same bias as AMS. In addition to this, by training on the text version of these\ntriples, it may be the case that we lose some of the syntax that the model learns due to\nawkwardly formatted inputs (i.e. “cat located at housing\" rather than “a cat is located\nat a house\"), however further testing of these two needs to be performed.\n4\nThere is some relevant derivative work for COMET by Bosselut et al.(Bosselut and\nChoi 2019) which looks into how effective COMET is at building KGs on the ﬂy given\na certain context, a question, and a proposed answer. They combine the context with a\nrelation from ATOMIC and feed it into COMET to represent reasoning hops. They do\nthis for multiple relations and keep redoing this with the generated outputs to represent\na reasoning chain for which they can derive a probability. They use this in a zero-shot\nevaluation of a question-answering system and ﬁnd that it is effective. Overall some\nhighlights of COMET are:\n• Generative language model that can provide natural language\nrepresentations of triples\n• Useful model for zero-shot KG completion\n• Simple pre-processing of triples for training\nModel Summary of Injection Example of Injection\nAlign,\nMask,\nSelect\nAligns a knowledge base with textual\nsentences, masks entities in the sen-\ntences, and selects alternatives with con-\nfounders to create a QA dataset\nKG Assertion: (population, AtLocation,\ncity)\nModel Input: The largest [QW] by pop-\nulation is Birming- ham, which has long\nbeen the most industrialized city? city,\nMichigan, Petrie dish, area with people\ninhabiting, country\nCOMET Ingests a formatted sentence version of\na triple from ConceptNet and Atomic\nKG Assertion: (PersonX goes to the\nmall, xIntent, to buy clothes)\nModel input: PersonX goes to the mall\n[MASK] ⟨xIntent ⟩to buy clothes\nTable 1\nInput Injection System Comparisons\n3. Architecture Injections\nIn this section we describe approaches that focus on architectural changes to language\nmodels. This involves either adding additional layers that integrate knowledge in some\nway with the contextual representations or modifying existing layers to manipulate\nthings such as attention mechanisms. We discuss two approaches within this category\nthat fall under layer modiﬁcations. These approaches utilize adapter-like mechanisms\nto be able to inject information into the models. A summary of these approaches can be\nfound in table 2.\n3.1 KnowBERT(Peters et al. 2019)\nKnowBERT modiﬁes BERT’s architecture by integrating some layers that they call the\nKnowledge Attention and Recontextualization (KAR). These layers take graph entity\nembeddings, that are based on Tucker Tensor Decompositions for KG completion (Bal-\naževi´ c, Allen, and Hospedales 2019), and run them through an attention mechanism to\ngenerate entity span embeddings. These span embeddings are then added to the regular\nBERT contextual representations. The summed representations are then uncompressed\n5\nand passed on to the next layer in a regular BERT. Once the KAR entity linker has been\ntrained, the rest of the BERT model is unfrozen and is trained in the pre-training. These\nKAR layers are trained for every KG that is to be injected, in this work they use data\nfrom Wikipedia and Wordnet.\nAn interesting observation is that the injection happens in the later layers, which\nmeans that the contextual representation up to that point may be unaltered by the\ninjected knowledge. This is done to stabilize the training, but could present an oppor-\ntunity to inject knowledge in earlier levels. Additionally, the way the system is trained,\nthe entity linking is ﬁrst trained, and then the whole system is unfrozen to incorporate\nthe additional knowledge into BERT. This strategy could lead to the catastrophic forget-\nting(Kirkpatrick et al. 2017) problem where the knowledge from the underlying BERT\nmodel or the additional structured injection may be forgotten or ignored.\nThis technique falls into a broader category of what is called Adapters(Houlsby\net al. 2019). Adapters are layers that are added into a language model and are subse-\nquently ﬁne tuned to a speciﬁc task. The interesting aspect of adapters is that they add\na minimal amount of additional parameters, and freeze the original model weights.\nThe added parameters are also initialized to produce a close to identity output. It is\nworth noting that the KnowBERT is not explicitly an Adapter technique as the model is\nunfrozen during training. Some highlights of KnowBERT are the following:\n• Fusion of contextual and graph representation of entities\n• Attention enhanced entity spanned knowledge infusion\n• Permits the injection of multiple KGs in varying levels of the model\n3.2 Common sense or world knowledge? investigating adapter-based knowledge in-\njection into pre-trained transformers(Lauscher et al. 2020)\nThis work explores what kinds of knowledge are infused by ﬁne tuning an adapter\nequipped version of BERT on ConceptNet. They generate and test models trained on\nsentences from the Open Mind Common Sense (OMCS)(Singh et al. 2002) corpus and\nfrom walks in the ConceptNet graph. They note that with simple adapters and as little\nas 25k/100k update steps on their training sentences, they are able greatly improve the\nencoded “World Knowledge\" (another name for the knowledge found in ConceptNet).\nHowever, it is worth noting that the information is presented as sentences to which\nthe adapters are ﬁne tuned. This may mean that the model may have similar possible\nshortcomings such as with the approaches that are input-focused (model may rely more\non the distributional rather than the semantic information), however testing needs to be\nperformed to conﬁrm this. Overall some highlights of this work are the following:\n• Adapter based approach which ﬁne-tunes a minimal amount of\nparameters\n• Shows that a relatively small amount of additional iterations can inject the\nknowledge in the adapters\n• Show that adapters, trained on KGs, do indeed boost the semantic\nperformance of transformer-based models\n6\nModel\nInjected\nin Pre-\nTraining\nInjected\nin Fine-\nTuning\nSummary of Injection\nKnowBERT Yes Yes Sandwich Adapter-like layers\nwhich sum contextual repre-\nsentation of layer with graph\nrepresentation of entities and\ndistributes it in an entity span\nCommon sense or world\nknowledge?[...]\nNo Yes Use sandwich adapters to\nﬁne tune on a KG\nTable 2\nArchitecture Injection System Comparisons\n4. Output Injections\nIn this section we describe approaches that focus on changing either the output structure\nor the losses that were used in the base model in some way to incorporate knowledge.\nOnly one model falls strictly under this category, the model injects entity embeddings\ninto the output of a BERT model.\n4.1 SemBERT(Zhang et al. 2019)\nSemBERT uses a subsystem that generates embedding representations of the output\nof a semantic role labeling(Màrquez et al. 2008) system. They then concatenate this\nrepresentation with the output of the contextualized representation from BERT to help\nincorporate relational knowledge. The approach, although clever, may fall short in that\nalthough it gives a representation for the roles, it leaves the model to ﬁgure out the exact\nrelationship that the roles are performing, however testing would need to be performed\nto check this. Some highlights of SemBERT are:\n• Encodes semantic role in an entity embedding that is combined at the\noutput\n5. Combination and Hybrid Injections\nHere we describe approaches that use combinations of injection types such as in-\nput/output injections or architecture/output injections, etc. We start by looking at\nmodels that perform input injections and reinforce these with output injections (LIBERT,\nKALM), We then look at models that manipulate the attention mechanisms to mimic\ngraph connections (BERT-MK,K-BERT). We follow this by looking into KG-BERT, a\nmodel that operates on KG triples, and K-Adapter, a modiﬁcation of RoBERTa that\nencodes KGs into Adapter Layers and fuses them. After this, we look into the approach\npresented as Cracking the Contextual Commonsense Code[...] which determines that\nthere are areas lacking in BERT that could be addressed by supplying appropriate data,\nand we look at ERNIE 2.0, a framework for multi-task training for semantically aware\nmodels. Lastly, we look at two hybrid approaches which extract LM knowledge and\nleverage it for different tasks. A summary of these injections can be found in table 3.\n7\n5.1 Knowledge-Aware Language Model (KALM) Pre-Training (Rosset et al. 2020)\nKALM is a system that does not modify the internal architecture of the model that it\nis looking to inject the knowledge into, rather it modiﬁes the input of the model by\nfusing entity embeddings with the normal word embeddings that the language model\n(in KALM’s case, GPT-2) uses. They then enforce the model in the output to uphold the\nentity information by adding an additional loss component in the pre-training that uses\na max margin between the cosine distance of the output contextual representation and\nthe input entity embedding and the cosine distance of the contextual representation and\na confounder entity. Altogether what this does is that it forces the model to notice when\nthere is an entity and tries to make the contextual representation have the semantics of\nthe correct input entity. Some highlights of KALM are:\n• Sends an entity signal in the beginning and and enforces it in the output of\na generative model to notice its semantics\n5.2 Exploiting structured knowledge in text via graph-guided representation learn-\ning(Shen et al. 2020)\nThis work masks informative entities that are drawn from a knowledge graph, in\nBERT’s MLM objective. In addition to this, they have an auxiliary objective which uses\na max-margin loss for a ranking task which is composed of using a bilinear model that\ncalculates a similarity score between a contextual representation of an entity mention\nand the representation of the [CLS] token for the text. The use for this is to determine if\nit is a relevant entity or a distractor. Both KALM and this work are very similar, but a key\ndifference is that KALM uses a generative model without any kind of MLM objective,\nand KALM does not do any kind of ﬁltering for the entities. Some highlights of this\nwork are:\n• Filters relevant entities to incorporate their information into the model\n• Enforces entity signal at beginning and end of the model through masking\nand max-margin losses\n5.3 Lexically Informed BERT (LIBERT)(Lauscher et al. 2019)\nLIBERT converts batches of lexical constraints and negative examples, into a BERT-\ncompatible format. The lexical constraints are synonyms and direct hyponyms-\nhypernyms (speciﬁc,broad) and take the form of a set of tuples of words: (C =\n{(w1, w2)i}N\ni=1). In addition to this set, the authors generate some negative examples\nby ﬁnding the words that are semantically close to (w1) and (w2) in a given batch. They\nthen format the examples into something BERT can use which is simply the wordpieces\nthat pertain to words in the batch separated by the separator token. They pass this input\nthrough BERT and use the [CLS] token as an input to a softmax classiﬁer to determine\nif the example is a valid lexical relation or not.\nDuring pre-training they alternate between a batch of sentences and a batch of\nconstraints. LIBERT outperforms BERT with lesser (1M) iterations of pre-training. It\nis worth noting that as the amount of iterations of training increase, the gap between\nthe two systems, although present, becomes smaller. This may indicate that, although\nthe additional training objective is effective, it may be getting overshadowed by the\n8\nregular MLM coupled with large amounts of data, however more testing needs to be\nperformed. It is also worth noting that the authors do not align the sentences with the\nconstraint batches, combine the training tuples which may hinder training as BERT has\nto alternate between different training input structures, and lastly, they do not incor-\nporate antonymy constraints in their confounder selection, so further experimentation\nwould be required to verify the effects of these. Some highlights of LIBERT are the\nfollowing:\n• Incorporate lexical constraints from entity embeddings\n• Good performance with constrained amounts of data\n5.4 BERT-MK(He et al. 2019)\nBERT-MK utilizes a combination of architecture injection and an output injection (addi-\ntional training loss). In BERT-MK, they utilize the KG-transformer modules which are\ntransformer layers that are combined with learned entity representations. These entity\nrepresentations are generated from another set of transformer layers that are trained\non a KG converted to natural language sentences. The interesting aspect is that these\nadditional layers incorporate an attention mask that mimics the connections in the KG,\nso to a certain extent, incorporating the structure of the graph and propagating it back\ninto the embeddings. These additional layers are trained to reconstruct the input set of\ntriples. The authors evaluate the system for medical knowledge (MK) however it may\nbe interesting to evaluate this on the GLUE benchmark along with utilizing other KGs\nsuch as ATOMIC or ConceptNet. Some highlights of BERT-MK are:\n• Utilization of a modiﬁed attention mechanism to mimic KG structure\nbetween terms\n• Incorporation of triple reconstruction loss to train the KG-transformer\nmodules\n• Merges KG-transformer with regular transformer for\ncontextual+knowledge-informed representation\n5.5 K-BERT (Liu et al. 2020)\nK-BERT uses a combination of input injection and architecture injections. For a given\nsentence, they inject relevant triples for the entities that are present in the sentence and\nin a KG. They inject these triples in between the actual text and utilize a soft-position\nembedding to determine the order in which the triples are evaluated. These soft position\nembeddings simply add positional embeddings to the injected triple tokens. This in turn\ncreates a problem that the tokens are injected as entities appear in a sentence, and hence\nthe ordering of the tokens is altered.\nTo remedy this the authors utilize a masked self attention similar to BERT-MK. What\nthis means is that the attention mechanism should only be able to see everything up to\nthe entity that matched in the injected triple. This attention mechanism helps the model\nfocus on what relevant knowledge it should incorporate. It would have been good to\nsee a comparison of just adding these as sentences in the input rather than having to\n9\nﬁx the attention mechanism to compensate for the erratic placement. Some highlights\nof K-BERT are:\n• Utilization of attention mechanism to mimic connected subgraphs of\ninjected triples\n• Injection of relevant triples as text inputs\n5.6 KG-BERT (Yao, Mao, and Luo 2019)\nThe authors present a combination approach which ﬁne-tunes a BERT model with the\ntext of triples from a KG similar to COMET. The authors also feed confounders in the\nform of random samples of entities into the training of the system. It utilizes a binary\nclassiﬁcation task to determine if the triple is valid and a relationship type prediction\ntask to determine which relations are present between pairs of entities. Although this\nsystem is useful for KG completion, there is no evidence of its performance on other\ntasks. Additionally they train one triple at a time which may limit the model’s ability to\nlearn the extended relationships for a given set of entities. Some highlights of KG-BERT\nare the following:\n• Fine tunes BERT into completing triples from a KG\n• Uses a binary classiﬁcation to predict if a triple is valid\n• Uses multi-class classiﬁcation to predict relation type\n5.7 K-Adapter (Wang et al. 2020)\nA work based on adapters, K-Adapter works by adding projection layers before and\nafter a subset of transformer layers. They do this only for some speciﬁc layers in a pre-\ntrained RoBERTa model (the ﬁrst layer, the middle layer, and the last layer). They then\nfreeze RoBERTa as per the Adapter work in (Houlsby et al. 2019) and train 2 adapters\nto learn factual knowledge from Wikipedia triples (Elsahar et al. 2019) and linguistic\nknowledge from outputs of the Stanford parser(Chen and Manning 2014). They then\ntrain the adapters with a triple classiﬁcation (whether the triple is true or not) task\nsimilar to KG-BERT.\nIt is worth noting that the authors compare RoBERTa and their K-Adapter approach\nagainst BERT, and BERT has considerably better performance on the LAMA probes. The\nauthors attribute RoBERTa’s byte pair encodings(Shibata et al. 1999) (BPE) as the major\nperformance delta between their approach and BERT. Another possible reason may be\nthat they only perform injection in a few layers rather than throughout the entire model,\nalthough testing needs to be done to conﬁrm this. Some highlights of K-Adapter are:\n• Approach provides a framework for continual learning\n• Use a fusion of trained adapter outputs for evaluation tasks\n5.8 Cracking the Contextual Commonsense Code: Understanding Commonsense\nReasoning Aptitude of Deep Contextual Representations(Da and Kusai 2019)\nThe authors analyze BERT to determine that it is deﬁcient in certain attribute repre-\nsentations of entities. The authors use the RACE (Lai et al. 2017) dataset, and based\n10\non ﬁve attribute categories (Visual, Encyclopedic, Functional Perceptual, Taxonomic),\nselect samples from the dataset that may be help a BERT model compensate deﬁciencies\nin the areas. They then ﬁne tune on this data. In addition to this, the authors concatenate\nthe ﬁne tuned BERT embeddings with some knowledge graph embeddings. These\ngraph embeddings are generated based on assertions that involve the entities that are\npresent in the questions and passages they train their ﬁnal joint model on (MCScript\n2.0(Ostermann, Roth, and Pinkal 2019)). Their selection of additional ﬁne-tuning data\nfor BERT improves their performance in MCScript 2.0, highlighting that their selection\naddressed missing knowledge.\nIt is worth noting that the graph embeddings that they concatenate boost the\nperformance of their system which shows that there is still some information in KGs\nthat is not in BERT. We classify this approach as a combination approach because they\nconcatenate the result of the BERT embeddings and KG embeddings and ﬁne tune both\nat the same time. The authors however gave no insight as to how the KG embeddings\ncould have been incorporated in the ﬁne-tuning/pre-training of BERT with the RACE\ndataset. Some highlights of this work are:\n• BERT has some commonsense information in some areas, but is lacking in\nothers\n• Fine-tuning on the deﬁcient areas increases performance accordingly\n• The combination of graph embeddings plus contextual representations are\nuseful\n5.9 ERNIE 2.0 (Sun et al. 2020)\nThe authors develop a framework that constructs pre-training tasks that center around\nWord-aware Pre-training, Structure-aware Pre-training, Semantic-aware Pre-training\nTasks, and proceeds to train a transformer based model on these tasks. An interesting as-\npect is that as they ﬁnish training on tasks, they keep training on older tasks in order for\nthe model to not forget what it has learned. In ERNIE 2.0 the authors do not incorporate\nKG information explicitly. They do have a sub-task within the Word-aware pre-training\nthat masks entities and phrases with the hope that it learns the dependencies of the\nmasked elements which may help to incorporate assertion information.\nA possible shortcoming of this model is that some tasks that are intended to infuse\nsemantic information into the model (i.e. the Semantic aware tasks which are a Dis-\ncourse Relation task and an information retrieval (IR) relevance) rely on the model to\npick it up from the distributional examples. This could have the same possible issue as\nwith the Input Injections and would need to be investigated further. Additionally, they\ndo not explicitly use KGs in the work. Some highlights of ERNIE 2.0 are:\n• Continual learning platform keeps training on older tasks to maintain\ntheir information\n• Framework permits ﬂexibility on the underlying model\n• Wide variety of semantic pre-training tasks\n11\n5.10 Graph-based reasoning over heterogeneous external knowledge for common-\nsense question answering (Lv et al. 2020)\nA hybrid approach in which the authors do not inject knowledge into a language model\n(namely XLNet(Yang et al. 2019), rather they utilize a language model as a way to unify\ngraph knowledge and contextual information. They combine XLNet embeddings as\nnodes in a Graph Convolutional Network (GCN) to answer questions.\nThey generate relevant subgraphs of ConceptNet and Wikipedia (from ConceptNet\nthe relations that include entities in a question/answer exercise and the top 10 most\nrelevant sentences from ElasticSearch on Wikipedia). They then perform a topological\nsorting on the combined graphs and pass them as input to XLNet. XLNet then generates\ncontextual representations that are then used as representations for nodes in a Graph\nConvolutional Network (GCN). They then utilize graph attention to generate a graph\nlevel representation and combine it with XLNet’s input ([CLS] token) representation to\ndetermine if an answer is valid for a question. In this model they do not ﬁne-tune XLNet,\nwhich could have been done on the dataset to give better contextual representations,\nand additionally they do not leverage the different levels of representation present in\nXLNet. Some highlights of this work are the following\n• Combination of GCN, Generative Language Model, and Search systems to\nanswer questions\n• Use XLNet as contextual embedding for GCN nodes\n• Perform QA reasoning with the GCN output\n5.11 Commonsense knowledge base completion with structural and semantic con-\ntext(Malaviya et al. 2020)\nAnother hybrid approach, the authors ﬁne tune a BERT model on a list of the unique\nphrases that are used to represent nodes in a KG. They then take the embeddings\nfrom BERT and from a sub-graph in the form of a GCN and run it through an en-\ncoder/decoder structure to determine the validity of an assertion.2\nThey then take this input and concatenate it with node representations for for\na subgraph (in this case a combination of ConceptNet and Atomic). They treat this\nconcatenation as an encoded representation, and run combinations of these through\na convolutional decoder that additionally takes an embedding of a relation type.\nThe result of the convolutional decoder is run through a bilinear model and a\nsigmoid function to determine the validity of the assertion. It seems interesting that the\nauthors only run the convolution through one side: convolution of (ei, erel) rather than\nboth the convolution of (ei, erel) and (erel, ej) (where (ei, ej) are the entity embeddings\nfor entity i and j respectively and(erel) is the embedding for a speciﬁc relationship) and\nthen a concatenation. They rely on the bilinear model joining the two representations.\nSome highlights of this work are the following:\n• Use a GCN and a LM to generate contextualized assertions representations\n2 It is worth noting that the two hybrid projects possibly beneﬁted from the ability for these language\nmodels to encode assertions as shown by Feldman et al. (Davison, Feldman, and Rush 2019) and Petroni\net al.(Petroni et al. 2019).\n12\n• Use BERT to generate contextual embeddings for nodes\n• Use an encoder-decoder structure to learn triples\n6. Future Directions\n6.1 Input Injections\nMost input injections are to format KG information into whatever format a transformer\nmodel can ingest. Although KALM has explored incorporating a signal to the input\nrepresentations, it would be interesting to add additional information such as the lexical\nconstraints mentioned in LIBERT, to the word embeddings that are trained with the\ntransformer based models like BERT. A possible approach could be to build a post-\nspecialization system that could generate retroﬁtted(Faruqui et al. 2014) representations\nthat can then be fed into language models.\n6.2 Architecture Injections\nAdapters seem to be a promising ﬁeld of research in language models overall. The\nidea that one can ﬁne tune a small amount of parameters may simplify the injection of\nknowledge and KnowBERT has explored some of these beneﬁts. It would be interesting\nto apply a similar approach to generative models and see the results.\nAnother possible avenue of research would be to incorporate neural memory mod-\nels/modules such as the ones by Munkhdalai(Munkhdalai et al. 2019) into adapter-\nbased injections. The reasoning would be that the model can simply look up relevant\ninformation encoded into a memory architecture and fuse it into a contextual represen-\ntation.\n6.3 Combined Approaches\nThere are a variety of combined approaches, but none of them tackle all three areas\n(input, architecture, and output) at the same time. It seems promising to test out a\nsignaling method such as KALM and see how this would work with an adapter based\nmethod similar to KnowBERT. The idea being that the input signal could help the\nentity embeddings contextualize better within the injected layers. Additionally, it would\nbe interesting to see how the aforementioned combination would look with a system\nsimilar to LIBERT; such that you could fuse entity embeddings with some semantic\ninformation.\n7. Conclusion\nInfusing structured information from Knowledge Graphs into pre-trained language\nmodels has had some success. Overall, the works reviewed here give evidence that the\nmodels beneﬁt from the incorporation of the structured information. By analyzing the\nexisting works, we give some research avenues that may help to develop more tightly\ncoupled language/KG models.\nReferences\nBalaževi´ c, Ivana, Carl Allen, and Timothy M Hospedales. 2019. Tucker: Tensor factorization for\nknowledge graph completion. arXiv preprint arXiv:1901.09590.\n13\nBodenreider, Olivier. 2004. The uniﬁed medical language system (umls): integrating biomedical\nterminology. Nucleic acids research, 32(suppl_1):D267–D270.\nBollacker, Kurt, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a\ncollaboratively created graph database for structuring human knowledge. In Proceedings of the\n2008 ACM SIGMOD international conference on Management of data, pages 1247–1250.\nBosselut, Antoine and Yejin Choi. 2019. Dynamic knowledge graph construction for zero-shot\ncommonsense question answering. arXiv preprint arXiv:1911.03876.\nBosselut, Antoine, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and\nYejin Choi. 2019. Comet: Commonsense transformers for automatic knowledge graph\nconstruction. arXiv preprint arXiv:1906.05317.\nChen, Danqi and Christopher D Manning. 2014. A fast and accurate dependency parser using\nneural networks. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), pages 740–750.\nDa, Jeff and Jungo Kusai. 2019. Cracking the contextual commonsense code: Understanding\ncommonsense reasoning aptitude of deep contextual representations. arXiv preprint\narXiv:1910.01157.\nDai, Zihang, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860.\nDavison, Joe, Joshua Feldman, and Alexander M Rush. 2019. Commonsense knowledge mining\nfrom pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1173–1178.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nElsahar, Hady, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Elena\nSimperl, and Frederique Laforest. 2019. T-rex: A large scale alignment of natural language\nwith knowledge base triples.\nFaruqui, Manaal, Jesse Dodge, Sujay K Jauhar, Chris Dyer, Eduard Hovy, and Noah A Smith.\n2014. Retroﬁtting word vectors to semantic lexicons. arXiv preprint arXiv:1411.4166.\nGabrilovich, Evgeniy, Michael Ringgaard, and Amarnag Subramanya. 2013. Facc1: Freebase\nannotation of clueweb corpora, version 1 (release date 2013-06-26, format version 1, correction\nlevel 0). Note: http://lemurproject. org/clueweb09/FACC1/Cited by, 5:140.\nHe, Bin, Di Zhou, Jinghui Xiao, Qun Liu, Nicholas Jing Yuan, Tong Xu, et al. 2019. Integrating\ngraph contextualized knowledge into pre-trained language models. arXiv preprint\narXiv:1912.00147.\nHewitt, John and Christopher D Manning. 2019. A structural probe for ﬁnding syntax in word\nrepresentations. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 4129–4138.\nHogan, Aidan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio\nGutierrez, José Emilio Labra Gayo, Sabrina Kirrane, Sebastian Neumaier, Axel Polleres, et al.\n2020. Knowledge graphs. arXiv preprint arXiv:2003.02320.\nHoulsby, Neil, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efﬁcient transfer\nlearning for nlp. arXiv preprint arXiv:1902.00751.\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,\net al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national\nacademy of sciences, 114(13):3521–3526.\nLai, Guokun, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale\nreading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683.\nLauscher, Anne, Olga Majewska, Leonardo FR Ribeiro, Iryna Gurevych, Nikolai Rozanov, and\nGoran Glavaš. 2020. Common sense or world knowledge? investigating adapter-based\nknowledge injection into pretrained transformers. arXiv preprint arXiv:2005.11787.\nLauscher, Anne, Ivan Vuli´ c, Edoardo Maria Ponti, Anna Korhonen, and Goran Glavaš. 2019.\nInforming unsupervised pretraining with external linguistic knowledge. arXiv preprint\narXiv:1909.02339.\n14\nLiu, Weijie, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020.\nK-bert: Enabling language representation with knowledge graph. In AAAI, pages 2901–2908.\nLv, Shangwen, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, and Songlin Hu. 2020. Graph-based reasoning over heterogeneous\nexternal knowledge for commonsense question answering. In AAAI, pages 8449–8456.\nMalaviya, Chaitanya, Chandra Bhagavatula, Antoine Bosselut, and Yejin Choi. 2020.\nCommonsense knowledge base completion with structural and semantic context. In AAAI,\npages 2925–2933.\nMàrquez, Lluís, Xavier Carreras, Kenneth C Litkowski, and Suzanne Stevenson. 2008. Semantic\nrole labeling: an introduction to the special issue.\nMiller, George A. 1998. WordNet: An electronic lexical database. MIT press.\nMunkhdalai, Tsendsuren, Alessandro Sordoni, Tong Wang, and Adam Trischler. 2019.\nMetalearned neural memory. In Advances in Neural Information Processing Systems, pages\n13331–13342.\nOstermann, Simon, Michael Roth, and Manfred Pinkal. 2019. Mcscript2. 0: A machine\ncomprehension corpus focused on script events and participants. arXiv preprint\narXiv:1905.09531.\nPeters, Matthew E, Mark Neumann, Robert L Logan IV , Roy Schwartz, Vidur Joshi, Sameer\nSingh, and Noah A Smith. 2019. Knowledge enhanced contextual word representations. arXiv\npreprint arXiv:1909.04164.\nPetroni, Fabio, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint\narXiv:1909.01066.\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training.\nRosset, Corby, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, and Saurabh Tiwary. 2020.\nKnowledge-aware language model pretraining. arXiv preprint arXiv:2007.00655.\nSap, Maarten, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah\nRashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine\ncommonsense for if-then reasoning. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pages 3027–3035.\nShen, Tao, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, and Weizhu Chen. 2020.\nExploiting structured knowledge in text via graph-guided representation learning. arXiv\npreprint arXiv:2004.14224.\nShibata, Yusuxke, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara,\nTakeshi Shinohara, and Setsuo Arikawa. 1999. Byte pair encoding: A text compression scheme\nthat accelerates pattern matching. Technical report, Technical Report DOI-TR-161,\nDepartment of Informatics, Kyushu University.\nSingh, Push, Thomas Lin, Erik T Mueller, Grace Lim, Travell Perkins, and Wan Li Zhu. 2002.\nOpen mind common sense: Knowledge acquisition from the general public. In OTM\nConfederated International Conferences\" On the Move to Meaningful Internet Systems\", pages\n1223–1237, Springer.\nSpeer, Robyn, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual\ngraph of general knowledge. In Thirty-First AAAI Conference on Artiﬁcial Intelligence.\nSpeer, Robyn and Catherine Havasi. 2012. Representing general relational knowledge in\nconceptnet 5. In LREC, pages 3679–3686.\nSun, Yu, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020.\nErnie 2.0: A continual pre-training framework for language understanding. In AAAI, pages\n8968–8975.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008.\nWang, Ruize, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin\nJiang, Ming Zhou, et al. 2020. K-adapter: Infusing knowledge into pre-trained models with\nadapters. arXiv preprint arXiv:2002.01808.\nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances\nin neural information processing systems, pages 5753–5763.\n15\nYao, Liang, Chengsheng Mao, and Yuan Luo. 2019. Kg-bert: Bert for knowledge graph\ncompletion. arXiv preprint arXiv:1909.03193.\nYe, Zhi-Xiu, Qian Chen, Wen Wang, and Zhen-Hua Ling. 2019. Align, mask and select: A simple\nmethod for incorporating commonsense knowledge into language representation models.\narXiv preprint arXiv:1908.06725.\nZhang, Zhuosheng, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, and Xiang\nZhou. 2019. Semantics-aware bert for language understanding. arXiv preprint\narXiv:1909.02209.\n16\nModel\nInjected\nin Pre-\nTraining\nInjected\nin\nFine-\nTuning\nSummary of Input Injec-\ntion\nSummary of Architecture\nInjection\nSummary of Output In-\njection\nKALM Yes No Combines an entity em-\nbedding with the model’s\nword embeddings\nN/a Incorporates a max-\nmargin loss with cosine\ndistances to enforce\nsemantic information\nExploiting\nstructured\nknowledge\nin text [...]\nYes No Uses a KG informed\nmasking scheme to exploit\nMLM learning\nN/A Incorporates a max-\nmargin loss with\ndistractors from a KG\nand a bilinear scoring\nmodel for the MM loss.\nLIBERT Yes No Alternates between\nbatches of sentences,\nand batches of tuples that\nare lexically related\nN/a Adds a binary classiﬁer as\na third training task to de-\ntermine if the tuples form\na valid lexical relation\nBERT-MK Yes No N/A Combines a base trans-\nformer with KG trans-\nformer modules which are\ntrained to learn contextual\nentity representations and\nhave an attention mecha-\nnism that mimics the con-\nnections between a graph\nUse a triple reconstruction\nloss, similar to MLM, but\nfor triples\nK-BERT Yes No Incorporate as part of\ntheir training batches,\nassertions from entities\npresent in a sample\nModify the attention\nmechanism and position\nembeddings to reorder\ninjected information, and\nmimic KG connections\nN/A\nKG-BERT No Yes Feeds triples from a\nknowledge graph as input\nexamples\nN/A Uses a binary classiﬁca-\ntion objective to deter-\nmine if a triple is correct\nand a multi-class classiﬁ-\ncation objective to deter-\nmine what kind of relation\nthe triple has.\nK-Adapter No Yes N/A Fine tune adapter layers to\nstore information from a\nKG\nCombine the model and\ndifferent adapter layers to\ngive a contextual repre-\nsentation with information\nfrom different sources, ad-\nditionally use a relation\nclassiﬁcation loss for each\ntrained adapter.\nCracking\nthe\ncontextual\ncommon-\nsense\ncode[...]\nYes No Pre-process the data to ad-\ndress commonsense rela-\ntion properties that are de-\nﬁcient in BERT\nN/A Concatenates a graph em-\nbedding to the output of\nthe BERT model\nERNIE 2.0 Yes No Construct data for pre-\ntraining tasks\nN/A Provides a battery of tasks\nthat are trained in parallel\nto enforce different seman-\ntic areas in a model\nTable 3\nCombination Injection Systems Comparisons\n17\nKnowledge Injection\nApproach\nUnderlying\nLanguage Model\nType of Injection Knowledge Sources Training Objective\nAlign, Mask, Select BERT Input ConceptNet Binary Cross Entropy\nCOMET GPT Input Atomic, ConceptNet Language Modeling\nKnowBERT BERT Architecture Wikipedia, WordNet Masked Language\nModeling\nCommon sense or\nworld knowledge?[...]\nBERT Architecture ConceptNet Masked Language\nModeling\nSemBERT BERT Output Semantic Role\nLabeling of Pre-\nTraining data\nMasked Language\nModeling\nKALM GPT-2 Combination\n(Input,Output)\nFACC1 and FAKBA\nentity annota-\ntion(Gabrilovich,\nRinggaard, and\nSubramanya 2013)\nLanguage Modeling +\nMax Margin\nExploiting Struc-\ntured[...]\nBERT Combination\n(Input+Output)\nConceptNet Masked Language\nModeling, Max\nMargin\nLiBERT BERT Combination (Input,\nOutput)\nWordnet, Roget’s\nThesaurus\nMasked Language\nModeling + Max\nMargin\nGraph-based\nreasoning over\nheterogeneous\nexternal knowledge\nXLNet + Graph Con-\nvolutional Network\nHybrid (Language\nModel + Graph\nReasoning)\nWikipedia, Concept-\nNet\nCross Entropy\nCommonsense\nknowledge base\ncompletion with\nstructural and\nsemantic context\nBERT+Graph Convo-\nlutional Net\nHybrid (Language\nModel + GCN\nEmbeddings)\nAtomic,ConceptNet Binary Cross Entropy\nERNIE 2.0 Transformer-Based\nModel\nCombination\n(Input+Output)\nWikipedia, Book-\nCorpus, Reddit,\nDiscovery Data\n(Various types\nof relationships\nextracted from these\ndatasets)\nVarious tasks, among\nthem Knowledge\nMasking, Token-\nDocument Relation\nPrediction, Sentence\nDistance Task, IR\nRelevance Task\nBERT-MK BERT Combination (Archi-\ntecture+Output)\nUniﬁed Medical\nLanguage Sys-\ntem(Bodenreider\n2004)(UMLS)\nMasked Language\nModeling, Max\nMargin\nK-Bert BERT Combination (Input +\nArchitecture)\nTBD Same As BERT\nKG-Bert BERT Combination (Input +\nOutput)\nFreebase, Wordnet,\nUMLS\nBinary + Categorical\nCross Entropy\nK-Adapter RoBERTa Combination (Archi-\ntecture+Output)\nWikipedia, Depen-\ndency Parsing from\nBook Corpus\nBinary Cross Entropy\nCracking the\nCommonsense Code\nBERT Combination\n(Input+Output)\nN/A:Fine Tuning on\nRACE dataset subset\nBinary Cross Entropy\nTable 4\nKnowledge Injection Models Overview\n18\nKnowledge\nInjection\nApproach\nBenchmark\nName\nBase Model Base Model\nBenchmark\nPerformance\nKnowledge\nInjected Model\nPerformance\nPercent\nDifference\nBERT-CSbase\n(Align, Mask, Se-\nlect)\nGLUE (Average) Bert-Base 78.975 79.612 0.81%\nBERT-CSlarge\n(Align, Mask, Se-\nlect)\nGLUE (Average) Bert-Base-large 81.5 81.45 -0.06%\nLIBERT (2M) GLUE (Average) BERT Baseline\nTrained with 2m\nexamples\n72.775 74.275 2.06%\nSemBERTbase GLUE(Average) BERT-Base 78.975 80.35 1.74%\nSemBERTlarge GLUE(Average) BERT-Base 81.5 84.262 3.39%\nK-Adapter F+L CosmosQA, TA-\nCRED\nRoBERTa + Mul-\ntitask training\n81.19,71.62 81.83,71.93 1.54%, 0.95%\nErnie 2.0 (large) GLUE (Average) BERT-Base-Large 81.5 84.65 3.87%\nBERT-MK Entity Typing,\nRel. Classiﬁcation\n(using UMLS)\nBERT-base 96.55 ,77.75 97.26,83.02 0.74%,6.78%\nK-BERT XNLI BERT-base 75.4 76.1 0.93%\nCracking the\ncontextual\ncommonsense\ncode (Bert large +\nKB+RACE)\nMCScript 2.0 BERT-Large 82.3 85.5 3.89\nKnowBERT TACRED BERT-Base 66 71.5 8%\nCommon Sense\nor World\nKnowledge?\n(OM-ADAPT\n100K)\nGLUE (Average) BERT-Base 78.975 79.225 0.40%\nCommon Sense\nor World\nKnowledge?\n(CN-ADAPT\n50K)\nGLUE (Average) BERT-Base 78.975 79.225 0.32%\nTable 5\nKnowledge Injection Models Performance Comparison\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7773053050041199
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.656143307685852
    },
    {
      "name": "Transformer",
      "score": 0.6248816251754761
    },
    {
      "name": "Language model",
      "score": 0.5846938490867615
    },
    {
      "name": "Knowledge graph",
      "score": 0.5360548496246338
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5332507491111755
    },
    {
      "name": "Unstructured data",
      "score": 0.5302139520645142
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5135143399238586
    },
    {
      "name": "Natural language processing",
      "score": 0.4535275101661682
    },
    {
      "name": "Language understanding",
      "score": 0.4498632252216339
    },
    {
      "name": "Data science",
      "score": 0.4444974362850189
    },
    {
      "name": "Data mining",
      "score": 0.1698962152004242
    },
    {
      "name": "Big data",
      "score": 0.12853646278381348
    },
    {
      "name": "Engineering",
      "score": 0.0847373902797699
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}