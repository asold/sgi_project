{
  "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
  "url": "https://openalex.org/W4389520114",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2800964500",
      "name": "Shushan Arakelyan",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": null,
      "name": "Rocktim Das",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1986153193",
      "name": "Mao Yi",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4290802886",
    "https://openalex.org/W2021688474",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4394638297",
    "https://openalex.org/W4285210829",
    "https://openalex.org/W4319451811",
    "https://openalex.org/W3091905774",
    "https://openalex.org/W1542791059",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W1964544799",
    "https://openalex.org/W4297733041",
    "https://openalex.org/W4300514939",
    "https://openalex.org/W3183882189",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4297926917",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4284664028",
    "https://openalex.org/W4376167326",
    "https://openalex.org/W2952505042",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W2012335756",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2046830558",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2125595978",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W4285282571",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W2964105864",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2060635016",
    "https://openalex.org/W2472819217",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3152740956",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W87135498",
    "https://openalex.org/W4389524484",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4288345647",
    "https://openalex.org/W2145680191",
    "https://openalex.org/W2963341924"
  ],
  "abstract": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 16298–16314\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nExploring Distributional Shifts in Large Language Models for Code\nAnalysis\nShushan Arakelyan\nUniversity of Southern California\nshushana@usc.edu\nRocktim Jyoti Das\nIIT, Delhi & MBZUAI\nrocktimjyotidas@gmail.com\nYi Mao\nMicrosoft Azure AI\nmaoyi@microsoft.com\nXiang Ren\nUniversity of Southern California\nxiangren@usc.edu\nAbstract\nWe systematically study how three large lan-\nguage models with code capabilities - CodeT5,\nCodex, and ChatGPT - generalize to out-of-\ndomain data. We consider two fundamental ap-\nplications - code summarization, and code gen-\neration. We split data into domains following\nits natural boundaries - by an organization, by\na project, and by a module within the software\nproject. We establish that samples from each\nnew domain present all the models with a signif-\nicant challenge of distribution shift. We study\nhow established methods adapt models to better\ngeneralize to new domains. Our experiments\nshow that while multitask learning alone is a\nreasonable baseline, combining it with few-shot\nfinetuning on examples retrieved from train-\ning data can achieve very strong performance.\nMoreover, this solution can outperform direct\nfinetuning for very low-data scenarios. Finally,\nwe consider variations of this approach to cre-\nate a more broadly applicable method to adapt\nto multiple domains at once. We find that for\ncode generation, a model adapted to multiple\ndomains simultaneously performs on par with\nthose adapted to a single domain1.\n1 Introduction\nSince the late 2000s, researchers have been re-\nporting poor generalization of statistical learning\nmodels to new software systems (Turhan, 2012;\nZimmermann et al., 2009), a phenomenon that has\nbecome important with the rise of large language\nmodels (LLMs) for code, such as GitHub Copilot,\nAmazon CodeWhisperer, Replit, etc. Thus, it is\ncrucial to understand when pretrained large lan-\nguage model performance on a private software\nsystem will differ from the performance obtained\non a benchmark. Prior work has studied some as-\npects of this problem, among others studying gener-\nalization from older to newer code, large software\n1Code and data for the paper are available at https://\ngithub.com/ShushanArakelyan/code_shift/\nprojects, and small competition problems, authors,\nand code representations (Nie et al., 2022; Li et al.,\n2021; Hu et al., 2022).\nHowever, the challenges of distribution shifts\nstemming from the hierarchical nature of software\ndata, as depicted in Figure 1, have not been sys-\ntematically studied with regard to large language\nmodels for code. Motivated by that, in this work,\nwe probe the generalization capacity of large lan-\nguage models with code capabilities, specifically\nCodex (Chen et al., 2021), CodeT5 (Wang et al.,\n2021) and ChatGPT, in code generation and sum-\nmarization tasks, examining three scenarios: gen-\neralization across companies, projects, and project\ncomponents. These scenarios are routinely con-\nsidered for analyzing software systems (Ma et al.,\n2012; Li et al., 2009; Mair et al., 2000) due to the\ncareful consideration that goes into combining or\nseparating such entities.\nFigure 1: Organization of a software system by the\ngranularity of its components\nFirst, we want to understand how models per-\nform on new domains- if models struggle with\nout-of-domain generalization, they should be used\nwith caution. At the same time, we empirically\nestablish the legitimacy of our definitions for out-\nof-domain scenarios by demonstrating that these\nexamples present a distributional shift. To answer\nthis question, we compare the performance of the\nmodels without any additional adaptation with that\nof the models that have been adapted on limited\ndata from a random domain or from the test domain.\nAdaptation with labeled examples from the test do-\nmain is the proxy for model performance if there\nwere no distributional shift. We find that all three\n16298\nmodels suffer from a drop in performance when\napplied out-of-domain. In this experiment, the dif-\nference is more pronounced for code summariza-\ntion, where adapting models with few in-domain\nexamples, on average, leads to an improvement of\nover 10 BLEU (Papineni et al., 2002) score points.\nNext, we explore ways to improve the out-of-\ndomain generalization of large language models\nwith code capabilities, recognizing that relying on\nlabeled in-domain data for every new domain is\nimpractical. Instead, we investigate the use of la-\nbeled out-of-domain data and small amounts of\nunlabelled in-domain data to enhance generaliza-\ntion. We test methods known to be successful\nin other transfer learning scenarios, such as meta-\nlearning (Thrun and Pratt, 1998; Vilalta and Drissi,\n2002) and multitask learning (Caruana, 1996; Sil-\nver, 1996). We also leverage unlabeled in-domain\ndata to retrieve similar labeled examples from an\nout-of-domain corpus for adapting to the new do-\nmain. We find that while meta-learning and multi-\ntask learning do not solve the out-of-domain gen-\neralization problem, domain adaptation with re-\ntrieved examples is a good technique for low-data\ndomains. In our evaluation on CodeSearchNet\ndataset we find that models supervised with re-\ntrieved examples perform on par, or better, than\nmodels that have been adapted using a few samples\n(e.g., 8 or 16) of in-domain labeled data. We are\nparticularly interested in scenarios with an extreme\nscarcity of labeled data - ranging from a few la-\nbeled instances to no labeled data at all. This is\ndue to how new data emerges in software engineer-\ning domains - it is not difficult to imagine a new\nrepository, or a new module, with fewer than 32\nfunctions, let alone - 32 labeled functions.\nLastly, we study if we can make the code models\nmore broadly applicable and retain their gener-\nalization capacities, rather than having to adapt\nthem to every new domain? Depending on the ap-\nproach to model adaptation (e.g. weight update vs\nin-context demonstrations) we vary the set of re-\ntrieved examples for each new domain, or for each\ntest input individually. We compare performance\nobtained this way with that of the models that are\nadapted simultaneously to multiple domains (or\ninstances, correspondingly). We find that Codex\nis very sensitive to these changes, so it is best to\nretrieve similar instances for each test data point.\nOn the other hand, CodeT5 has a minor drop in\ncode summarization and a negligible drop in code\ngeneration. This makes it feasible to adapt and\napply CodeT5 to multiple domains simultaneously\nwith minimal tradeoff, eliminating the need to store\nseparate copies of the model for each domain.\n2 Background\nThe shifts in underlying semantics between the\ntraining and evaluation data can be one of the\nmost impacting factors for deteriorating perfor-\nmance at test time. Prior work in code analysis has\nmainly focused on cross-project shifts, i.e. training\nand evaluating the model on disjunct sets of code\nprojects. Additionally, the studies were mainly\nconducted in the context of traditional machine\nlearning methods, such as linear classifiers, support\nvector machines, and later, LSTMs (Zimmermann\net al., 2009; Turhan, 2012; Angioni et al., 2022).\nMore recent works consider shifts caused by\ndifferent authors of the code, the timeline of the\nproject, distributions of code tokens, etc (Li et al.,\n2021; Hu et al., 2022; Nie et al., 2022). How-\never, the abilities of large language models under\ndistribution shift are still under-explored. We con-\nduct a comprehensive empirical analysis to probe\nthe large language models’ capabilities in handling\nthree different granularity of distribution shifts\n(company, domain, module) when different train-\ning and adaptation methods are used. In addition\nto directly fine-tuning vanilla LLMs, we experi-\nment with enhancing pretrained models using the\nmethods described below.\nMeta-Learning and Multi-task Learning. In\nour work, we experiment with both Meta-Learning\nand Multi-task learning to get better initialization\nfor few-shot performance on the downstream task.\nFor meta-learning, we employ Model-agnostic\nMeta-Learning (MaML) (Finn et al., 2017) which\nis a gradient-based method. It is a conceptually\nsimple and model-agnostic algorithm that has been\nshown to outperform existing approaches in several\ntasks. Multi-task Learning (MTL) aims to learn a\nshared and generalized representation by jointly\ntraining on several tasks. We adopt the simplest ap-\nproach to multi-task learning by jointly finetuning\na shared language model on multiple tasks.\nParameter Efficient Methods. Parameter-\nefficient methods have been shown to obtain\nperformance comparable to finetuning all model\nparameters with finetuning only a tiny fraction of\nmodel parameters. In our work, we experiment\nwith Low-Rank Adaptation (LoRA) (Hu et al.,\n2021), which is a low-rank update method.\n16299\nIn-Context Learning. GPT-3 (Brown et al.,\n2020) demonstrated the ability of large language\nmodels to perform few-shot predictions, where the\nmodel is given a description of the task in natural\nlanguage with few examples. In our work, we con-\nduct experiments on in-context learning on Codex.\nRetrieval Based Example Selection. It has been\nshown in Liu et al. (2021) that in-context exam-\nples selected following a strategy may serve as\nmore informative input to unleash GPT3’s exten-\nsive knowledge. Inspired by this, we leverage a\nsimilarity-based retrieval for domain adaptation.\n3 Problem setting\nFigure 2: We group the functions from CodeSearchNet\nby repos, orgs, and folders they belong to.\nWe study scenario where users seek to integrate\na large language model, such as Codex or CodeT5,\ninto their software project. The primary focus of\nthis study is to gain a deeper understanding of\nthe performance characteristics exhibited by these\nmodels, particularly when confronted with source\ncode originating from an unseen organization, an\nunseen project, or specific project components that\nhave not been previously encountered.\nFor every code data point in the dataset, we have\ninformation about the organization, project, and\nthe module within the project that the data point\ncomes from. Based on this information, we can\ngroup data points into sets, and end up with three\nsets of sets, as illustrated in Figure 2. For example,\nthe middle set in the figure contains multiple sets\nof data points. Each of those sets corresponds to a\nunique organization to which all data points within\nit belong. In other words, all data points within a set\nbelong to the same domain. Appendix, Section 7.3\ncontains additional analysis on splitting the data\npoints in this manner. For simplicity, we refer to a\nset of examples from the same domain as τi. We\nrefer to splits of such a set into train, development,\nτ⊂Xtrain(total)τ⊂Xtrain(|τ|≥96) τ⊂Xtest(|τ|≥96)\norg. 9737 195 8repos. 15858 147 15fold. 25268 100 10\nTable 1: Domains in CodeSearchNet dataset. Left col-\numn: training set. Middle column: number of domains\nof each kind in Xtrain with > 96 samples. Right col-\numn: number of domains in Xtest with > 96 samples.\nor test sections as τi\ntrain, τi\ndev, and τi\ntest.\n3.1 Data\nWe use CodeSearchNet (Husain et al., 2019)\ndataset2, in particular, the partition containing\nJavaScript language. We refer to the train section\nof the dataset as Xtrain, and the development and\ntest sections as Xtest.\nWe want to keep all of the domains in Xtest un-\nseen, and for that reason, we remove any domain\nfrom Xtest that also appears in Xtrain. This can\nhappen because CodeSearchNet dataset is split into\npartitions by projects, so the same organizations\ncan appear in different splits. This way, any do-\nmain coming from Xtest is, by our definition, out-\nof-domain for any model trained on Xtrain. We\nfurther split each domain τi ⊂Xtest into τi\ntrain,\nτi\ndev and τi\ntest. The evaluation is performed onτi\ntest.\nτi\ntrain and τi\ndev are used to obtain a proxy for the\nupper-bound performance of the model if the do-\nmain τi was seen during training, i.e. if there is no\ndistribution shift for τi\ntest.\nPreprocessing We use the “path” field of the data\npoint to determine each code snippet’s organization,\nrepository, and lowest-level folder. Using 5 differ-\nent random seeds, we divide a domain into τi\ntrain,\nτi\ndev, and τi\ntest. We aim to have at least 32 sam-\nples each in τi\ntest and τi\ndev, and up to 32 samples\nfor τi\ntrain. Thus, from Xtest we filter any domain\nthat has less than 96 samples in total. Final dataset\nstatistics are presented in Table 1.\n3.2 Applications and Metrics\nWe study two generation applications: code sum-\nmarization and code generation. Code summa-\nrization aims to summarize a code snippet into\na natural language description. The code snippet\nin CodeSearchNet dataset is a function, while the\nnatural language description is the docstring of\nthat function. This task is evaluated with BLEU-\n4 (Papineni et al., 2002) metric. Code genera-\n2Since the training data of Codex models is undisclosed,\nwe cannot be sure that it did not include CodeSearchNet. Nev-\nertheless, we see a performance difference for ID and OOD\nexperiments.\n16300\nCode summarization folder repo org\n8-shot 16-shot 32-shot 8-shot 16-shot 32-shot 8-shot 16-shot 32-shot\nCodeT5 FT ID 14.39 16.06 18.31 12.68 14.73 16.82 13.14 16.35 17.65\nCodeT5 LoRA ID 16.57 19.07 20.93 15.22 17.14 21.20 15.61 18.56 20.87\nCodeT5 FT random 3.58 4.30 5.02 4.35 4.70 5.79 4.53 5.47 6.27\nCodeT5 LoRA random 3.69 4.37 4.92 4.70 5.56 5.92 5.27 5.53 6.26\nTable 2: Model performance for code summarization on in-domain ( ID) vs out-of-domain ( random) test data.\nReported metric is BLEU (higher is better).\nCode generation folder repo org\n8-shot 16-shot 32-shot 8-shot 16-shot 32-shot 8-shot 16-shot 32-shot\nCodeT5 FT ID 14.67 15.22 16.13 16.15 17.42 18.62 14.54 15.34 16.43\nCodeT5 LoRA ID 14.14 15.06 16.36 16.23 17.45 18.96 14.17 15.30 16.62\nCodeT5 FT random 15.23 14.94 15.15 14.19 14.14 14.67 13.39 13.43 14.44\nCodeT5 LoRA random 14.45 14.29 15.37 14.29 13.74 15.04 13.76 13.85 14.81\nTable 3: Model performance for code generation on in-domain (ID) vs out-of-domain (random) test data. Reported\nmetric is CodeBLEU (higher is better).\nCode summarization folder repo org\nCodex\ninstr. only(0-shot) 1.55 1.52 1.61\nICL random(8-shot) 7.17 6.84 6.73\nICL ID(8-shot) 20.34 19.00 20.72\nChatGPT\ninstr. only(0-shot) 5.74 5.48 4.63\nICL random(8-shot) 5.47 6.58 6.48\nICL ID(8-shot) 7.47 9.15 7.54\nCode generation folder repo org\nCodex\ninstr. only(0-shot) 5.49 5.72 5.77\nICL random(8-shot)16.82 17.47 16.82\nICL ID(8-shot) 25.73 24.64 23.87\nChatGPT\ninstr. only(0-shot) 8.45 8.39 8.04\nICL random(8-shot)12.95 13.19 12.70\nICL ID(8-shot) 15.17 15.81 15.55\nTable 4: Codex and ChatGPT performance for code\nsummarization and generation tasks. Models are evalu-\nated 0-shot, as well as using ICL with in-domain (ID)\nand out-of-domain (random) data. Reported metric is\nBLEU for code summarization (higher is better), and\nCodeBLEU for code generation (higher is better)\ntion generates the function given a natural lan-\nguage description of the code. We follow prior\nwork and use CodeBLEU (Ren et al., 2020) for\nevaluating generated code. We added our own\nJavaScript keywords (the full list is in Appendix,\nSection 7.1) to an existing CodeBLEU imple-\nmentation. However, recently it has been shown\nthat CodeBLEU scores can disagree with human\njudgment scores (Evtikhiev et al., 2022). Moti-\nvated by these findings we additionally evaluate\ncode generation models with chrF (Popovic, 2015),\nRougeL (Lin, 2004) and CodeBERTScore (Zhou\net al., 2023) metrics. These metrics are in agree-\nment in our experiments, so we report the results\nfor them in Appendix, Section 7.7.\n3.3 Models\nWe experiment with three large language mod-\nels: (1) CodeT5 (Wang et al., 2021), which is an\nencoder-decoder model based on T5 (Raffel et al.,\n2019), (2) Codex (Chen et al., 2021), which is\na decoder only model based on GPT-3 (Brown\net al., 2020) and (3) ChatGPT ( gpt-3.5-turbo)\nwhich is the chat optimized version of Instruct-\nGPT (Ouyang et al., 2022) which is fine-tuned\nwith Reinforcement Learning with Human Feed-\nback(RLHF) (Christiano et al., 2017). The models\nvary in size: CodeT5 utilizes the T5-large architec-\nture with 700 million parameters, while the Codex\nmodel employs the GPT-3 architecture with over\n100 billion parameters. Although the architecture\nof ChatGPT has not been disclosed, it is presumed\nto have billions of parameters. A more detailed\ndiscussion of these models is provided in the Ap-\npendix, Section 7.4.\n4 Analysis\nIn this section, we formulate the research questions\nthat we aim to answer and give a more detailed\ndescription of the setups that we have used for\nanalyzing and answering each question.\nRQ 1 How do code models perform on new do-\nmains?\nWe test the models’ capacity for generalization\nto new domains by comparing the performance of\nthe models that have been adapted to the new do-\nmain using few-shot instances of in-domain data\n(ID) vs those that only encountered out-of-domain\n(OOD) data. For CodeT5, few-shot domain adap-\n16301\ntation data is used to update the model weights,\nwhereas for Codex, it is included as demonstra-\ntions in the prompt to the model.\nCodeT5\nFor adaptation techniques for the CodeT5 model,\nwe experiment with using a different number of\nsupervision examples - 8, 16, or 32.\nThe first adaptation method we use is full model\nfine-tuning (FT). Information on the hyperparam-\neters for this and all other methods is available in\nAppendix, Section 7.5. Besides FT, we also experi-\nment with a parameter-efficient fine-tuning method\n- Low-Rank Adaptation (LoRA) (Hu et al., 2021).\nThis method adds trainable pairs of rank decompo-\nsition matrices in parallel to existing weight matri-\nces, thus enabling parameter-efficient adaptation to\nnew domains without forgetting.\nCodex and ChatGPT\nFor GPT models, we do not perform weight up-\ndates. Very large models have been shown to be\ncapable to generalize to unseen tasks with just an\ninstruction. Thus, we evaluate these models with\njust the task instruction, for example, \"Summarize\nfollowing JavaScript code\", and input (i.e. instruc-\ntion only). Models can be sensitive to the wording\nof the instructions, so we use a number of differ-\nent instruction variations for each application and\naverage the results. The full list of instruction vari-\nations that we have used with Codex and ChatGPT\nmodels is presented in Appendix, Section 7.10.\nMoreover, larger models have been shown to\n“learn” from demonstration examples that are pro-\nvided as part of their input, even though this process\ndoes not involve any weight updates. This phe-\nnomenon is known as in-context learning ( ICL),\nwhich is what we use for domain adaptation for\nGPT models. Due to the limit on the size of the\ninput to the models (4096 tokens), we use as many\ndemonstrations as would fit, including up to 8\ndemonstrations with each test example. And since\nthe models can also be sensitive to the order of ex-\namples, we shuffle the order of the demonstrations\n5 times and average the results.\nFinding: Models struggle on new domains\nTables 2 and 3 demonstrate the performance ob-\ntained by CodeT5, and Table 4 shows perfor-\nmance for Codex and ChatGPT. Additional results\nfor other code generation metrics, such as chrF,\nRougeL, and CodeBERTScore are available in Ap-\npendix, Section 7.7. We see that the performance\ndegrades for models that have not encountered in-\ndomain examples vs those that have, i.e. models\nstruggle with out-of-domain generalization. For\nexample, CodeT5 model on code summarization\nin most scenarios gains about 200% relative im-\nprovement after updating the model with few-shot\nin-domain data.\nWhile there is a difference in performance for\nCodeT5 model on code generation ID and OOD,\nthe performance difference is next to negligible.\nWe hypothesize that this can be due to the fact that\ncode generation is a more challenging task for a\nlarge language model, and so the effect of distri-\nbution shift is less noticeable. This observation\nbecomes evident when examining Table 3, which\ndemonstrates that the smaller model, CodeT5, ex-\nhibits lower performance compared to larger mod-\nels such as Codex. Thus, for CodeT5 adding in-\ndomain data results in a smaller gain. On the other\nside, for Codex, the addition of the in-domain data\nresults in up to 50% relative improvement.\nFrom Table 4, it is evident that while ChatGPT\noutperforms Codex in 0-shot setting, we don’t see\nas large of an improvement with the addition of\nin-context examples, whether in-domain or out-of-\ndomain. Upon closer inspection of model outputs,\nwe notice that this is due to the specifics of the\nChatGPT model, which errs on the side of caution,\nrefusing to provide any answer when presented\nwith a vague or noisy input. This results in 0 scores\nfor those entries, lowering the overall model per-\nformance and smoothing the effect of in-domain\ndemonstrations. Due to this characteristic of Chat-\nGPT model, having established that it is affected\nby distributional shifts same as other models in this\nstudy, we do not perform further comparisons with\nit in the rest of the paper.\nRQ 2 How to get better out-of-domain general-\nization?\nWe have seen that models for code performed\nsignificantly better after being adapted for new\ndomains using in-domain data. However, there\nare many reasons why adapting to every new\ndomain with the help of labeled examples might\nbe impractical. Thus, we consider some alternative\napproaches, that would not require labeled data but\ncan hopefully close the performance gap partially\nor fully. Figure 3 shows an overview.\nCodeT5\nTo answer RQ1, we start from a pre-trained check-\npoint of the model and experiment with different\n16302\n(a) CodeT5\n(b) Codex\nFigure 3: For the CodeT5 model we use different meth-\nods for training and domain adaptation. We evaluate\nboth in scenarios with different data sources during the\ndomain adaptation stage.\napproaches for domain adaptation. To answer the\ncurrent question, we additionally consider different\nmethods to use before the domain adaptation stage,\nparticularly, multi-task learning and meta-learning.\nThe resulting setups are illustrated in Figure 3a.\nMultitask learning (MTL) MTL method trains\na single model on all the domains simultaneously.\nFor code summarization, we use the model check-\npoint that has been provided by the authors of\nCodeT5, which is fine-tuned on the training portion\nof CodeSearchNet. For code generation, we per-\nform our own training since there was no JavaScript\ncheckpoint shared by CodeT5 authors.\nDual-gen MTL In addition to MTL, we experi-\nment with a multitask model that has been trained\non both code generation and code summarization\nsimultaneously. We refer to this model as “dual-\ngen” MTL, following the authors of CodeT5. We\nprepend the inputs to the model with a generation\nor summarization instruction for each instance.\nModel-Agnostic Meta Learning For model-\nagnostic meta-learning or MaML (Finn et al.,\n2017), we filter the domains in Xtrain set, only\nleaving those that have at least 96 samples (see the\nmiddle column of Table 1). This is to ensure that\neach domain contains disjoint sets of adequate size\nfor both training and meta-training.\nStratified Example Retrieval for Supervision\nIn addition to the strategies above, we experiment\nwith a domain adaptation method that does not re-\nquire in-domain labeled data for supervision. We\nuse a similarity metric on embeddings obtained\nfrom the pre-trained CodeT5 model checkpoint to\nretrieve kmost similar examples for every exam-\nple in τtest from Xtrain. We set k to 4, 8, or 32,\nand since |τtest|= 32 the combined size of the set\nwould be 128, 256, or 1024. Finally, we remove\nany duplicates. We refer to this set as τret.\nFor similarity metric, we experiment with co-\nsine similarity, as well as a more recent approach -\nIsoScore (Rudman et al., 2022). In our experiments,\nwe find that cosine similarity performs better over-\nall, so the results reported in the paper are using\ncosine similarity. Additional results using IsoScore\nmetric are reported in Appendix Section 7.8.\nChallenge Scenario In addition to using test data\nfrom CodeSearchNet dataset, in an attempt to make\nthe evaluation more realistic, we experiment with a\nsetting where the out-of-domain data comes from\na different dataset. Here we use the test split of\nThe Vault dataset (Manh et al., 2023), which we\nhave processed in the same manner as described in\nSection 3.1. The details of the processing for the\nVault dataset are provided in Appendix Section 7.6.\nCodex\nStratified Example Retrieval for Demonstra-\ntions Similarly to the strategy for CodeT5, for\nCodex we employ in-context learning with re-\ntrieved demonstration examples. For each test\nquery, instead of using random sets of in-domain\nor out-of-domain demonstrations, we use 4 or 8 of\nthe query’s most similar samples from Xtrain as\ndemonstrations. This case is referred to as ICL ret.\nFinding: Strategic adaptation is advantageous\nin very low data scenarios\nFigure 4a and 4c demonstrate the performance of\nthe CodeT5 and Codex models. For CodeT5, it\ncontains the performance obtained without adapta-\ntion (0-shot), as well as after in-domain few-shot\nfine-tuning (additional results for LoRA are pre-\nsented in Appendix Section 7.7). None of the eval-\nuated methods perform comparably in zero-shot\nsetting to those with few-shot domain adaptation -\nwhether on examples retrieved from training data\nor obtained from test domains. So these training\nmethods do not result in a general-purpose model\nthat handles out-of-domain generalization well.\nThe same pattern is evident in the challenge eval-\nuation scenario, presented in Figure 4b. From this\nfigure, we also conclude that retrieved supervision\nis less effective when supervised and test exam-\nples are extracted from different datasets - even\nwhen both are collected from the same source, i.e.\n16303\n(a) CodeT5, trained and evaluated on CodeSearchNet\n (b) CodeT5, trained on CodeSearchNet, evaluated on The Vault\n(c) Codex\nFigure 4: Models with ID and retrieved downstream adaptations.\nFigure 5: CodeT5 model finetuned with retrieved supervision using different number of retrieved examples per test\nsample. Scores reported are BLEU for code summarization and CodeBLEU for code generation. CodeT5 MTL\nmodel performances in zero-shot, and 8-shot (ID) scenarios are shown with dotted lines for reference.\nGitHub. While we have done our best to process\nthe data in The Vault dataset as similar to the pro-\ncessing done in CodeSearchNet, there must still be\nsubtle differences remaining from data collection,\nonce again emphasizing how sensitive code models\nare even to minute changes.\nAdapting the model trained with MTL objective\nto test domains with the help of stratified supervi-\nsion provides a considerable boost to the perfor-\nmance of CodeT5 and Codex. Results for CodeT5\nare shown in Figure 5 with bars marked “ret k”,\nwhere krefers to the number of examples included\nin τret per test example. Figure 4c reports Codex\nperformance with 4 or 8 retrieved demonstrations\nas “ICL ret 4” and “ICL ret 8” respectively.\nFirst of all, we notice that there is a saturation in\nterms of gained performance vs the number of strat-\nified supervision or demonstration examples used.\nFor CodeT5 using 32 examples per test instance\nis almost always worse than using 4 or 8 exam-\nples. For Codex, using 4 or 8 examples results in\napproximately the same performance.\nNext, for code summarization, retrieving 4 or 8\nexamples from out-of-domain train data leads to\nperformance comparable, or even better, than that\nof the model adapted using 8 examples from the test\ndomain. This trend is observed for both Codex and\nCodeT5, particularly strongly when generalizing to\nnew repositories and new organizations. A similar\ntrend can be observed for code generation, and to a\nmuch stronger degree for CodeT5 - stratified super-\nvision models can even outperform models trained\nwith 32 examples from the test domain. While the\nperformance of the stratified supervision models\nplateau after a certain number of examples, super-\nvision on in-domain samples does not demonstrate\nsuch a trend.\nRQ 3 Can we have more generic solutions for\nout-of-domain generalization?\nFrom our analysis of RQ2, we see that models can\n16304\nCode Summarization\nBLEU /∆BLEU\nCode Generation\nCodeBLEU /∆CodeBLEU\norg repo folder org repo folder\nFT: combined 4 18.74 / -4.74 18.59 / -4.47 18.06 / -1.06 29.46 / -0.19 29.41 / -0.01 26.60 / -1.53\nFT: combined 8 18.46 / -5.07 18.58 / -3.03 17.57 / -3.48 29.13 / -0.73 28.83 / -0.22 27.23 / -0.92\nFT: combined 32 17.35 / -2.31 17.63 / -0.94 15.57 / -2.56 26.28 / -3.63 25.01 / -4.02 25.14 / -2.88\nICL: 4 fromτret 14.66 / -7.04 12.68 / -7.95 12.10 / -6.96 20.52 / -6.73 20.06 / -7.78 19.39 / -6.21\nICL: 8 fromτret 13.77 / -8.53 12.96 / -8.52 12.26 / -7.17 20.81 / -7.05 20.23 / -8.16 19.48 / -7.00\nTable 5: Using retrieved supervision examples for general domain adaptaion. The first number in each cell of the\ntable is the score obtained by the corresponding model, which is followed by the change in the performance w.r.t\ndomain-specific model or test sample-specific demonstrations.\ngeneralize better to new domains without relying\non labeled data from that domain. Unfortunately,\nthis still requires adapting to every test domain\nindividually for CodeT5, and even more strictly\n- to every test sample individually - for Codex.\nFor example, for CodeT5, this means maintaining\nmultiple copies of the model, performing the\ntraining for the adaptation stage multiple times,\nand storing a large amount of out-of-domain data to\nretrieve examples from. In this RQ, we experiment\nwith approaches that would eliminate the need\nto train CodeT5 on multiple domains separately.\nFor Codex, we experiment with sampling from\ndemonstrations collected for the entire domain. For\nCodeT5, we try two approaches. First, we finetune\nit on the combined set of τret for all domains.\nWe also try using fast vote-kalgorithm (Su et al.,\n2022), which selects representative examples from\nthe supervision dataset, while ensuring diversity\namong selected examples. For Codex, for a\nquery from τtest, we consider sampling 4 or 8\ndemonstration examples from τret.\nFinding: Multi-domain code generation models\ndo not require a large performance sacrifice.\nThe results for both models are presented in Ta-\nble 5. Results for CodeT5 for this experiment are\nreferred to as “FT: combined k”, where k is the\nnumber of retrieved examples per test example.\nFast vote-k is less effective as an adaptation tech-\nnique compared to fine-tuning on a combined set\nof retrieved examples, and the results for it are\npresented in the Appendix Section 7.9. As can be\nseen, training a single model on combined retrieved\nsamples results in a moderate drop in performance\nfor code summarization, and a negligible drop for\ncode generation. In other words, a model finetuned\non stratified supervision data for new domains can\nbe a viable solution for the out-of-domain general-\nization problem for code generation. Interestingly,\nthis also indicates that for code generation, good\nperformance on one domain does not hinder the\nperformance on another domain, i.e. there is little\nto no negative transfer between different domains.\nFor Codex, the results of the experiment are re-\nferred to as “ICL: kfrom τret” in Table 5, where\nkis the number of sampled demonstrations. It ap-\npears that for Codex replacing demonstrations se-\nlected for individual examples with those selected\nfor a domain introduce too much noise, and de-\ngrade the performance a lot because of the high\nsensitivity of ICL to demonstrations.\n5 Conclusion\nWe evaluate large language models for code -\nCodeT5, Codex (code-cushman-001), and Chat-\nGPT (gpt-3.5-turbo) - on two fundamental code\napplications - code generation and code summa-\nrization. We study how the models perform under\ndistribution shifts that can commonly occur due to\nthe nature of the software. We experiment with\nthree granularities for defining domains in applica-\ntions for code - organization, project, and module\nor folder. Our experiments show that all models\nevaluated are susceptible to reduced performance\ndue to domain shifts. We experiment with a num-\nber of training and domain adaptation techniques\nfor achieving better out-of-domain generalization.\nWe discover that retrieving similar out-of-domain\nexamples from training data is the most effective ap-\nproach for adapting to new domains in the absence\nof in-domain data. In addition, we experiment with\nadapting models to multiple new domains simul-\ntaneously and find that such models can perform\nvery well for code generation. However, we find\nthe generality of the model to be a tradeoff for its\nperformance for code summarization.\n16305\n6 Limitations and Threats to Validity\nAs can be seen from Table 1, as a result of the\nprocess of filtering, we skew the data towards\nlarger projects and eliminate from the dataset many\nsamples that could potentially come from smaller\nprojects. We believe that this step is necessary to\nmake the results more reliable, due to the high vari-\nance that can be observed in datasets with very\nsmall test sets. However, we want to draw attention\nto this circumstance once more, to make sure that\nour findings are interpreted correctly.\nReferences\nDaniele Angioni, Luca Demetrio, Maura Pintor, and\nBattista Biggio. 2022. Robust machine learning for\nmalware detection over time. In Proceedings of\nthe Italian Conference on Cybersecurity (ITASEC\n2022), Rome, Italy, June 20-23, 2022, volume 3260\nof CEUR Workshop Proceedings, pages 169–180.\nCEUR-WS.org.\nAntreas Antoniou, Harrison Edwards, and Amos J.\nStorkey. 2018. How to train your maml. ArXiv,\nabs/1810.09502.\nAnkur Bapna, N. Arivazhagan, and Orhan Firat. 2019.\nSimple, scalable adaptation for neural machine trans-\nlation. In Conference on Empirical Methods in Natu-\nral Language Processing.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. ArXiv,\nabs/2005.14165.\nRich Caruana. 1996. Algorithms and applications\nfor multitask learning. In Machine Learning, Pro-\nceedings of the Thirteenth International Conference\n(ICML ’96), Bari, Italy, July 3-6, 1996, pages 87–95.\nMorgan Kaufmann.\nRich Caruana. 1997. Multitask learning. Machine\nLearning, 28:41–75.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde, Jared Kaplan, Harrison Ed-\nwards, Yura Burda, Nicholas Joseph, Greg Brockman,\nAlex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Moham-\nmad Bavarian, Clemens Winter, Philippe Tillet, Fe-\nlipe Petroski Such, David W. Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-V oss, William H. Guss, Alex Nichol, Igor\nBabuschkin, S. Arun Balaji, Shantanu Jain, Andrew\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew M. Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv,\nabs/2107.03374.\nPaul Francis Christiano, Jan Leike, Tom B. Brown, Mil-\njan Martic, Shane Legg, and Dario Amodei. 2017.\nDeep reinforcement learning from human prefer-\nences. ArXiv, abs/1706.03741.\nRajarshi Das, Manzil Zaheer, Dung Ngoc Thai, Ameya\nGodbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan,\nLazaros Polymenakos, and Andrew McCallum. 2021.\nCase-based reasoning for natural language queries\nover knowledge bases. In Conference on Empirical\nMethods in Natural Language Processing.\nMikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov,\nand Timofey Bryksin. 2022. Out of the BLEU: how\nshould we assess quality of the code generation mod-\nels? CoRR, abs/2208.03133.\nChelsea Finn, P. Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In International Conference on Ma-\nchine Learning.\nChelsea Finn, Kelvin Xu, and Sergey Levine. 2018.\nProbabilistic model-agnostic meta-learning. In Neu-\nral Information Processing Systems.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In Inter-\nnational Conference on Machine Learning.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. ArXiv, abs/2106.09685.\nQiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy,\nLei Ma, Mike Papadakis, and Yves Le Traon.\n2022. Codes: A distribution shift benchmark\ndataset for source code learning. arXiv preprint\narXiv:2206.05480.\nHamel Husain, Hongqi Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of semantic\ncode search. ArXiv, abs/1909.09436.\nGregory R. Koch. 2015. Siamese neural networks for\none-shot image recognition.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven C. H. Hoi. 2022. Coderl: Mas-\ntering code generation through pretrained models and\ndeep reinforcement learning. CoRR, abs/2207.01780.\n16306\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. ArXiv, abs/2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), abs/2101.00190.\nYan-Fu Li, Min Xie, and T. N. Goh. 2009. A study of\nmutual information based feature selection for case\nbased reasoning in software cost estimation. Expert\nSyst. Appl., 36(3):5921–5931.\nYufei Li, Simin Chen, and Wei Yang. 2021. Estimating\npredictive uncertainty under program data distribu-\ntion shift. CoRR, abs/2107.10989.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt-3? In Work-\nshop on Knowledge Extraction and Integration for\nDeep Learning Architectures; Deep Learning Inside\nOut.\nYing Ma, Guangchun Luo, Xue Zeng, and Aiguo Chen.\n2012. Transfer learning for cross-company software\ndefect prediction. Inf. Softw. Technol., 54(3):248–\n256.\nCarolyn Mair, Gada F. Kadoda, Martin Lefley, Keith\nPhalp, Chris Schofield, Martin J. Shepperd, and Steve\nWebster. 2000. An investigation of machine learning\nbased prediction systems. J. Syst. Softw., 53(1):23–\n29.\nDung Nguyen Manh, Nam Le Hai, Anh T. V . Dau,\nAnh Minh Nguyen, Khanh Nghiem, Jin Guo, and\nNghi D. Q. Bui. 2023. The vault: A comprehensive\nmultilingual dataset for advancing code understand-\ning and generation. CoRR, abs/2305.06156.\nElliot Meyerson and Risto Miikkulainen. 2019. Mod-\nular universal reparameterization: Deep multi-\ntask learning across diverse domains. ArXiv,\nabs/1906.00097.\nPengyu Nie, Jiyang Zhang, Junyi Jessy Li, Raymond J.\nMooney, and Milos Gligoric. 2022. Impact of eval-\nuation methodologies on code summarization. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 4936–4960. Association for Computa-\ntional Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke E. Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Francis Christiano, Jan Leike, and\nRyan J. Lowe. 2022. Training language models to\nfollow instructions with human feedback. ArXiv,\nabs/2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2019. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. ArXiv, abs/1910.10683.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie\nLiu, Duyu Tang, M. Zhou, Ambrosio Blanco, and\nShuai Ma. 2020. Codebleu: a method for automatic\nevaluation of code synthesis. ArXiv, abs/2009.10297.\nWilliam Rudman, Nate Gillman, Taylor Rayne, and\nCarsten Eickhoff. 2022. Isoscore: Measuring the\nuniformity of embedding space utilization. In Find-\nings of the Association for Computational Linguistics:\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages\n3325–3339. Association for Computational Linguis-\ntics.\nAdam Santoro, Sergey Bartunov, Matthew M.\nBotvinick, Daan Wierstra, and Timothy P. Lillicrap.\n2016. Meta-learning with memory-augmented neural\nnetworks. In International Conference on Machine\nLearning.\nRichard Shin, C. H. Lin, Sam Thomson, Charles C.\nChen, Subhro Roy, Emmanouil Antonios Platan-\nios, Adam Pauls, Dan Klein, Jas’ Eisner, and Ben-\njamin Van Durme. 2021. Constrained language\nmodels yield few-shot semantic parsers. ArXiv,\nabs/2104.08768.\nDaniel L. Silver. 1996. The parallel transfer of task\nknowledge using dynamic learning rates based on a\nmeasure of relatedness. Connect. Sci., 8(2):277–294.\nJake Snell, Kevin Swersky, and Richard S. Zemel. 2017.\nPrototypical networks for few-shot learning. ArXiv,\nabs/1703.05175.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022.\nSelective annotation makes language models better\nfew-shot learners. CoRR, abs/2209.01975.\n16307\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip\nH. S. Torr, and Timothy M. Hospedales. 2017. Learn-\ning to compare: Relation network for few-shot learn-\ning. 2018 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1199–1208.\nSebastian Thrun and Lorien Y . Pratt. 1998. Learning to\nlearn: Introduction and overview. In Sebastian Thrun\nand Lorien Y . Pratt, editors,Learning to Learn, pages\n3–17. Springer.\nBurak Turhan. 2012. On the dataset shift problem\nin software engineering prediction models. Empir.\nSoftw. Eng., 17(1-2):62–74.\nRicardo Vilalta and Youssef Drissi. 2002. A perspective\nview and survey of meta-learning. Artif. Intell. Rev.,\n18(2):77–95.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Ko-\nray Kavukcuoglu, and Daan Wierstra. 2017. Match-\ning networks for one shot learning.\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven\nC. H. Hoi. 2021. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code under-\nstanding and generation. ArXiv, abs/2109.00859.\nYongxin Yang and Timothy M. Hospedales. 2016. Deep\nmulti-task representation learning: A tensor factori-\nsation approach. ArXiv, abs/1605.06391.\nShuyan Zhou, Uri Alon, Sumit Agarwal, and Gra-\nham Neubig. 2023. Codebertscore: Evaluating code\ngeneration with pretrained models of code. ArXiv,\nabs/2302.05527.\nThomas Zimmermann, Nachiappan Nagappan, Har-\nald C. Gall, Emanuel Giger, and Brendan Murphy.\n2009. Cross-project defect prediction: a large scale\nexperiment on data vs. domain vs. process. In Pro-\nceedings of the 7th joint meeting of the European\nSoftware Engineering Conference and the ACM SIG-\nSOFT International Symposium on Foundations of\nSoftware Engineering, 2009, Amsterdam, The Nether-\nlands, August 24-28, 2009, pages 91–100. ACM.\n16308\n7 Appendix\n7.1 Javascript Keywords\nThe Javascript keywords that we included in the\nCodeBleu implementation for evaluation is listed\nin table 7.1.\n7.2 Extended Background\n7.2.1 Meta-learning and Multi-task-learning\nMeta-learning focuses on adapting knowledge\ngained from previous tasks to be applied to\nnew tasks with limited training examples. Most\nmeta-learning algorithms can be categorized into\nthree groups: 1) Black-box meta-learning ap-\nproaches (Santoro et al., 2016) train a black-box\nmodel to take in training data of a target task to\noutput parameters for the neural network used for\nmaking prediction for that task; 2) Optimization-\nbased methods (Finn et al., 2017, 2018; Antoniou\net al., 2018) uses gradient descent to learn model\nparameters which can be adapted to a future target\ntask with few gradient steps on a few-shot training\ndataset; 3) Non-parametric methods (Vinyals et al.,\n2017; Snell et al., 2017; Sung et al., 2017; Koch,\n2015) learns a metric space in which predictions\ncan be performed by computing some similarity\nmetric, like distance and cosine similarity, to repre-\nsentations of each class. In our work, we are using\nthe MAML (Finn et al., 2017) approach, which is\na gradient-based method and learns model initial-\nization (i.e., initial parameters) that is amenable to\nfast fine-tuning with few instances. This method\nis a conceptually simple and model-agnostic algo-\nrithm that has been shown to outperform existing\napproaches in several tasks.\nMulti-task Learning aims to jointly learn sev-\neral related tasks providing a generalized represen-\ntation with the added benefit of compute and mem-\nory in terms of shared model parameters (Yang and\nHospedales, 2016; Caruana, 1997; Meyerson and\nMiikkulainen, 2019). MTL also has a regulariza-\ntion effect on the model parameters. By definition,\nMTL aims to solve a fixed number of known tasks,\nwhereas the point of meta-learning is often to solve\nunseen future tasks. But both methods capture a\ngood prior from the training tasks, which can be\nused for getting model parameters for future target\ntasks.\nIn our work, we have experimented with both\nMAML and multi-task learning to check which\nof the method gives us a better prior for few-shot\nperformance in our setting.\n7.2.2 Few-shot Methods\nParameter-efficient finetuning: Conventional\nfine-tuning methods retrains all the model parame-\nters for every new task, which becomes infeasible\nas the model size increases to the level of GPT-3.\nIn recent times, parameter-efficient methods have\nbeen studied and it has been demonstrated that\nstate-of-the-art PEFT methods can match the per-\nformance of finetuning all the model’s parameters\nwhile updating only a tiny fraction of the model\nparameters. Initially adapters (Raffel et al., 2019;\nHoulsby et al., 2019; Bapna et al., 2019) were intro-\nduced, which are new feed-forward modules added\nbetween the layers of the fixed pre-trained model.\nSince then, various sophisticated PEFT methods\nhave been proposed, including methods like LoRA\nthat produce low-rank updates (Hu et al., 2021)\nand prompt tuning (Lester et al., 2021) and prefix-\ntuning (Li and Liang, 2021) concatenate learned\ncontinuous embeddings to the model’s input or ac-\ntivations to induce it to perform a task.\nRetrieval-based Example selection: In a study\nconducted by Liu et al. (2021) , they explored how\ndifferent prompts can impact the performance of\nGPT-3 and found that the use of in-context exam-\nples has a significant influence on the downstream\nresults. To achieve this, they utilized an unsuper-\nvised sentence encoder to encode training examples\nand then retrieved the nearest neighbors for each\ntest instance. On a similar note, Das et al. (2021)\ndeveloped a supervised prompt retriever for an-\nswering knowledge-based questions. Their method\nused tailored supervision specifically designed for\nknowledge-based queries and relied on surface sim-\nilarity between formal queries. Furthermore, Shin\net al. (2021) employed GPT-3 to select examples\nfor the prompt in few-shot semantic parsing. They\ndemonstrated the effectiveness of this approach by\nusing GPT-3 to identify relevant examples for the\nprompt, which in turn improved the overall perfor-\nmance of the system.\n7.3 Domain split visualization\nTo better understand how different splits of do-\nmains are different from each other, we visualize\nour resulting test domains in Figure 6. We plot each\ndomain as a dot, where different colors correspond\nto different splits. X axis demonstrates average pair-\n16309\nLanguages Keywords\nJavaScript await, break, case, catch, class, const, continue, debugger, default, delete, do, else,\nenum, export, extends, false, finally, for, function, if, implements, import, in, in-\nstanceof, interface, let, new, null, package, private, protected, public, return, super,\nswitch, static, this, throw, try, true, typeof, var, void, while, with, yield\nTable 6: Keywords used for CodeBLEU evaluation\nCode generation folder repo org\n8-shot 16-shot 32-shot 8-shot 16-shot 32-shot 8-shot 16-shot 32-shot\nCodeT5 FT ID 19.36 20.92 21.95 20.42 22.44 24.47 19.29 20.73 22.6\nCodeT5 LoRA ID 20.05 21.66 22.56 20.81 23.12 24.52 20.08 21.28 22.99\nCodeT5 FT random 17.61 18.03 17.94 16.92 17.50 17.59 16.47 17.46 17.85\nCodeT5 LoRA random 17.87 18.02 17.81 17.45 17.15 17.63 17.24 17.13 17.29\nCodex ICL ID 28.78 - - 31.05 - - 29.19 - -\nCodex ICL random 20.62 - - 20.87 - - 21.10 - -\nCodex instr. only(0-shot) (10.24) - - (10.60) - - (10.25) - -\nTable 7: Comparison of model performance for code generation on in-domain (ID) vs out-of-domain (random) test\ndata. Reported metric is ChrF (higher is better).\nFigure 6: Each dot signifies a domain. Average pairwise\nsimilarities of examples within each domain (x axis)\nplotted against average similarities of that domain to all\nother domains (y axis).\nwise similarity of examples within a domain, i.e. x\ncoordinate of a domain corresponds to how uniform\nexamples within a domain are. Y axis demonstrates\npairwise similarities of examples within a domain\nto examples in all other domains, i.e. y coordinate\nof a domain demonstrates its similarity to other do-\nmains. From the figure we see that the vast majority\nof domains are clustered in the lower right corner,\nwhich corresponds to the domains that are uniform,\nand dissimilar to other domains. A small handful\nof domains are located in the upper left corner, that\ncorresponds to domains with dissimilar examples\nwithin itself, but higher similarity to other domains.\nIt is notable, that quantitatively, upper left corner\ncontains more folders than repos, and more repos\nthan orgs. We hypothesize, that such distribution\ncould be explained by functional, rather than hi-\nerarchicals similarities across domains. A clear\nexample of such instance can be a folder with util-\nity functions that can have high similarity to other\nfolders with utility functions, all the while individ-\nual functions within that folder are implementing\ndifferent utilities, and thus - are dissimilar.\n7.4 Models\nCodeT5: CodeT5 (Wang et al., 2021) is a pre-\ntrained encoder-decoder transformer model based\non T5 (Raffel et al., 2019) for programming lan-\nguages. It uses a unified framework to support\ncode understanding and generation tasks seam-\nlessly. To improve the model’s ability to handle the\nunique characteristics of programming languages,\nCodeT5 is trained on an identifier-aware pretrain-\ning task. Additionally, the model is trained to ex-\nploit user-written code comments with a bimodal\ndual-generation task for better alignment between\nnatural language and programming languages. This\nmakes this model suitable for the applications that\nwe consider. For both of our applications, we used\nthe CodeT5-large model (Le et al., 2022) without\nmaking any changes to the model architecture.\nCodex Codex (Chen et al., 2021) is the language\nmodel for code released by OpenAI. It is a GPT\nlanguage model finetuned on 54 million public soft-\nware repositories hosted on GitHub, containing 179\nGB of unique Python files under 1 MB. VLLMs\n16310\nCode generation folder repo org\n8-shot 16-shot 32-shot 8-shot 16-shot 32-shot 8-shot 16-shot 32-shot\nCodeT5 FT ID 14.15 15.84 16.73 14.93 16.98 19.19 13.75 14.93 16.94\nCodeT5 LoRA ID 14.49 16.58 17.87 15.47 17.69 19.60 14.10 15.48 17.61\nCodeT5 FT random 11.34 11.62 11.73 9.91 10.10 10.32 9.49 10.20 10.68\nCodeT5 LoRA random 11.45 12.05 12.58 10.09 10.04 11.08 10.15 10.30 11.15\nCodex ICL ID 23.70 - - 24.62 - - 22.58 - -\nCodex ICL random 15.76 - - 15.67 - - 15.81 - -\nCodex instr. only(0-shot) (6.44) - - (6.50) - - (6.18) - -\nTable 8: Comparison of model performance for code generation on in-domain (ID) vs out-of-domain (random) test\ndata. Reported metric is RougeL (higher is better).\nCode generation folder repo org\n8-shot 16-shot 32-shot 8-shot 16-shot 32-shot 8-shot 16-shot 32-shot\nCodeT5 FT ID 0.68 / 0.68 0.69 / 0.68 0.69 / 0.69 0.69 / 0.69 0.69 / 0.68 0.68 / 0.67 0.69 / 0.67 0.69 / 0.68 0.70 / 0.69\nCodeT5 LoRA ID 0.68 / 0.67 0.69 / 0.68 0.70 / 0.69 0.69 / 0.68 0.70 / 0.70 0.71 / 0.71 0.69 / 0.68 0.69 / 0.68 0.71 / 0.69\nCodeT5 FT random 0.65 / 0.66 0.66 / 0.66 0.66 / 0.66 0.66 / 0.65 0.66 / 0.66 0.66 / 0.66 0.65 / 0.65 0.65 / 0.65 0.65 / 0.65\nCodeT5 LoRA random 0.65 / 0.65 0.65 / 0.65 0.66 / 0.66 0.66 / 0.66 0.65 / 0.65 0.66 / 0.66 0.65 / 0.65 0.65 / 0.65 0.66 / 0.66\nCodex ICL ID 0.74 / 0.72 - - 0.75 / 0.73 - - 0.74 / 0.72 - -\nCodex ICL random 0.69 / 0.67 - - 0.70 / 0.68 - - 0.69 / 0.67 - -\nCodex instr. only(0-shot)0.62 / 0.61 - - 0.63 / 0.62 - - 0.63 / 0.62 - -\nTable 9: Comparison of model performance for code generation on in-domain (ID) vs out-of-domain (random) test\ndata. Reported metric in each cell is CodeBERTScore F1 on the left (higher is better), and CodeBERTScore F3 on\nthe right (higher is better).\nare capable of zero-shot generalization to unseen\ntasks, which is achieved by providing them with\nan instruction of what the model is expected to\ndo. This allowed us to successfully evaluate Codex\nfor both code generation and code summarization\nwithout any need for training.\nChatGPT ChatGPT is a conversational variant\nderived from InstructGPT/GPT 3.5 model (Ouyang\net al., 2022). It features a dialogue interface and\nis trained using a more refined objective function\ncalled Reinforcement Learning from Human Feed-\nback (RLHF) (Christiano et al., 2017). However,\nthere is currently limited information available re-\ngarding the specific architecture and training data\nemployed in the creation of ChatGPT. We utilize\nthe GPT-3.5 Turbo API, provided by OpenAI, to\naccess ChatGPT for conducting our experiments.\nThis API version allows a maximum token length\nrestriction of 4096 tokens.\n7.5 Hyperparameters and training details\nFor full finetuning of CodeT5, we updated the\nmodel for 500 steps using batch size of 8, the best\nmodel was identified by the performance on the\nτdev portion. For LoRA, we use a rank of 4 with\nan initialization scale of 0.01 and update all the at-\ntention and feedforward layers. We train for 1000\nsteps with a batch size of 8.\nFor multitask learning (MTL) of CodeT5, we\nupdate the model for 150K steps on 80% of the\nXtrain data, using a batch size of 4. The best check-\npoint is selected by evaluating the model on the\nremaining 20% of Xtrain which was held-out from\ntraining. For dual-gen MTL, we followed the same\ntrain/dev division strategy as for MTL for code gen-\neration, and updated the model for 150K steps with\nbatch size of 4. The best checkpoints were again\ndecided by evaluating the model on the created\ndevelopment set. In particular, we selected two\ncheckpoints - one according to CodeBLEU metric,\nand another according to BLEU metric for code\ngeneration and code summarization respectively.\nFor Model-agnostic meta-learning, we updated the\nmodel from the pretrained CodeT5 checkpoint for\n10K steps and used the last checkpoint in our ex-\nperiments.\n7.6 The Vault\nThe Vault is a multilingual dataset extracted from\nGitHub. Despite the fact that it comes pretokenized,\nwe noticed that some of the preprocessing for The\nVault is different from the preprocessing of Code-\nSearchNet. For example, while CodeSearchNet\n16311\nfunction body may have inlined comments, the\nVault functions are stripped of those. On the other\nside, the Vault docstring typically includes func-\ntion parameter documentation, whereas the Code-\nSearchNet omits those. On average, CodeSearch-\nNet function docstrings are also shorter than those\nof the Vault. In our work, we processed the Vault\ndataset, to fix these inconsistencies and make new\ndata points consistent with data from CodeSearch-\nNet.\n7.7 Additional experimental results\nBesides the experiments presented in the main pa-\nper, in this section, we report some additional ex-\nperiments. Tables 7, 8 and 9 report results for code\ngeneration as measured using chrF, RougeL and\nCodeBERTScore metrics correspondingly.\nAdditionally, Figure 7 illustrates how LoRA pa-\nrameter efficient finetuning method compares to\nthe full model finetuning for CodeT5.\nCode Summarization\nBLEU\nCode Generation\nCodeBLEU\norg repo folder org repo folder\nIsoScore (4) 16.71 16.57 15.47 15.05 16.01 14.93\nIsoScore (8) 17.27 16.72 15.71 15.32 16.55 15.28\nIsoScore (32) 17.46 16.90 14.34 16.13 17.89 16.26\nTable 10: Results for CodeT5 model using IsoScore for\nmeasuring embedding similarity and supervising with\nretrieved examples from train data.\n7.8 IsoScore\nIsoScore is a similarity metric of isotropy of an\nembedding space. The way we use it to measure\nsimilarity is by computing IsoScore value of a com-\nbined set of test example embeddings and every\nindividual training set embedding. The “closest”\nexamples selected for supervision are the ones that\nresulted in the largest IsoScore value for each set\nof test examples. We then use the same number of\nsupervision examples as we used with cosine sim-\nilarity - selecting 4*32, 8*32, or 32*32 “closest”\nexamples for supervision. The results for model\nadapted using IsoScore metric similarity are re-\nported in Table 10.\n7.9 Fast vote-k\nTo make the setup for fast vote-k similar to the ver-\nsion with the combination of nearest examples, we\nrun this algorithm to select 4*32 (128), 8*32 (256),\nand 32*32 (1024) supervision examples. Table 11\nshow results obtained for a CodeT5 MTL model\nthat has additionally been finetuned using a set of\nexamples obtained from fast vote-k algorithm.\nCode Summarization\nBLEU\nCode Generation\nCodeBLEU\norg repo folder org repo folder\nFast vote-k (4) 10.96 12.34 10.33 24.96 25.76 24.77\nFast vote-k (8) 11.40 12.74 10.60 25.10 26.21 25.14\nFast vote-k (32) 10.84 12.03 10.06 24.25 25.06 24.17\nTable 11: Results for CodeT5 model using Fast V ote-\nk for measuring embedding similarity and supervising\nwith retrieved examples from train data.\n7.10 Instructions for Codex and ChatGPT\nTable 12 contains list of instructions we used with\nCodex and ChatGPT models in instruction-only\nand in-context learning scenarios.\n7.11 Sample outputs\nTable 13 presents some examples and the outputs\nobtained by different models for those. Here we\ncan see that CodeT5 model finetuned on in-domain\nexamples sometimes has the advantage of having\nrelevant context and thus is using correct mem-\nber names as opposed to other models. On the\nother hand, we also see that similar out-of-domain\nexamples from the train split can in fact be near\nduplicates of the ones in the test split. As a result,\nthe model supervised with retrieved examples may\ngenerate output that is extremely close to that of\nthe gold test data.\n16312\nFigure 7: Performance for CodeT5 model finetuned with LoRA compared to regular finetuning.\nCopilot Task instruction\n\"Write in javascript:\",\n\"Write code:\",\n\"Summarize code:\",\n\"Summarize javascript snippet:\",\n\"Write code intent:\"\nDemonstration example\ntemplate\n\"Intent: {text} \\\\n Snippet: {code}\\\\n\\\\n\",\n\"Intent: {text} \\\\n Code: {code}\\\\n\\\\n\",\n\"Code: {code} \\\\n Intent: {text}\\\\n\\\\n\",\n\"Code: {code} \\\\n Summary: {text}\\\\n\\\\n\",\n\"Snippet: {code} \\\\n Intent: {text}\\\\n\\\\n\",\n\"Snippet: {code} \\\\n Summary: {text}\\\\n\\\\n\",\nChatGPT System messages\n’You are a helpful assistant that writes JavaScript code based on English description.\nYou only output code without any English text.’\n“You are a helpful assistant that writes single sentence summarizes for JavaScript code in English.\nYou only output code summary without any other English text.”\nTask instruction \"Write a single sentence summary for the following JavaScript code in English. \"\n\"Implement this functionality using JavaScript. \"\nDemonstration example\ntemplate\n[\"Below are some examples of JavaScript code implemented based on English summary. \\n\",\n\"Summary: {text}\\nCode: {code}\\n\\n\"]\n[\"Below are some examples of English summaries of JavaScript code. \\n\",\n\"Code: {code}\\nSummary: {text}\\n\\n\"]\nTable 12: Task instructions and demonstration templates used for generating results in the experiments with Codex\nand ChatGPT.\n16313\nInput Gold CodeT5MTL (0-shot) CodeT5MTL + ID (32-shot)CodeT5MTL + ret 4 ChatGPT\nDispatch stack informationto all handlers\nSetup captions\nToggle event listener\nReturns the absolute pathto the class file\nReturns the tag name of thegiven library in the given contribrepository if installed.Returns false if not installed.\nTable 13: Sample outputs from different models.\n16314",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9013810157775879
    },
    {
      "name": "Computer science",
      "score": 0.8566727638244629
    },
    {
      "name": "Code (set theory)",
      "score": 0.634918749332428
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5705916285514832
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5700328350067139
    },
    {
      "name": "Source code",
      "score": 0.4899381995201111
    },
    {
      "name": "Source lines of code",
      "score": 0.45756393671035767
    },
    {
      "name": "Code generation",
      "score": 0.4424409866333008
    },
    {
      "name": "Language model",
      "score": 0.42587733268737793
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40302133560180664
    },
    {
      "name": "Theoretical computer science",
      "score": 0.39988213777542114
    },
    {
      "name": "Software",
      "score": 0.375454843044281
    },
    {
      "name": "Machine learning",
      "score": 0.36856019496917725
    },
    {
      "name": "Data mining",
      "score": 0.3557049632072449
    },
    {
      "name": "Programming language",
      "score": 0.33591973781585693
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I68891433",
      "name": "Indian Institute of Technology Delhi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ],
  "cited_by": 13
}