{
  "title": "Learning Graph Structures with Transformer for Multivariate Time Series\\n Anomaly Detection in IoT",
  "url": "https://openalex.org/W3184127157",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3127091308",
      "name": "Chen Zekai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301426524",
      "name": "Chen, Dingshuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108511592",
      "name": "Zhang Xiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202042112",
      "name": "Yuan, Zixuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2377731517",
      "name": "Cheng, Xiuzhen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2891273344",
    "https://openalex.org/W4293718132",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2963460174",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2950361482",
    "https://openalex.org/W4310895557",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W4246193833",
    "https://openalex.org/W4245050711",
    "https://openalex.org/W3152893301",
    "https://openalex.org/W3106504817",
    "https://openalex.org/W2911200746",
    "https://openalex.org/W3139204882",
    "https://openalex.org/W3080253043",
    "https://openalex.org/W3177624059",
    "https://openalex.org/W2906498146",
    "https://openalex.org/W2963166639",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W3169450514",
    "https://openalex.org/W2270574292",
    "https://openalex.org/W2982407593",
    "https://openalex.org/W3012539654",
    "https://openalex.org/W4288419263",
    "https://openalex.org/W2127979711",
    "https://openalex.org/W2891536563",
    "https://openalex.org/W2581973374",
    "https://openalex.org/W2963197901",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W4248484754",
    "https://openalex.org/W2964248614",
    "https://openalex.org/W3004207920",
    "https://openalex.org/W1994373811",
    "https://openalex.org/W2743617586",
    "https://openalex.org/W2887985735",
    "https://openalex.org/W4297814361",
    "https://openalex.org/W3155567600",
    "https://openalex.org/W3128634608",
    "https://openalex.org/W1530232915",
    "https://openalex.org/W2800806089",
    "https://openalex.org/W2278868814",
    "https://openalex.org/W2407991977",
    "https://openalex.org/W1978832189",
    "https://openalex.org/W2056081083",
    "https://openalex.org/W3106543020",
    "https://openalex.org/W2604247107",
    "https://openalex.org/W3099185017"
  ],
  "abstract": "Many real-world IoT systems, which include a variety of internet-connected\\nsensory devices, produce substantial amounts of multivariate time series data.\\nMeanwhile, vital IoT infrastructures like smart power grids and water\\ndistribution networks are frequently targeted by cyber-attacks, making anomaly\\ndetection an important study topic. Modeling such relatedness is, nevertheless,\\nunavoidable for any efficient and effective anomaly detection system, given the\\nintricate topological and nonlinear connections that are originally unknown\\namong sensors. Furthermore, detecting anomalies in multivariate time series is\\ndifficult due to their temporal dependency and stochasticity. This paper\\npresented GTA, a new framework for multivariate time series anomaly detection\\nthat involves automatically learning a graph structure, graph convolution, and\\nmodeling temporal dependency using a Transformer-based architecture. The\\nconnection learning policy, which is based on the Gumbel-softmax sampling\\napproach to learn bi-directed links among sensors directly, is at the heart of\\nlearning graph structure. To describe the anomaly information flow between\\nnetwork nodes, we introduced a new graph convolution called Influence\\nPropagation convolution. In addition, to tackle the quadratic complexity\\nbarrier, we suggested a multi-branch attention mechanism to replace the\\noriginal multi-head self-attention method. Extensive experiments on four\\npublicly available anomaly detection benchmarks further demonstrate the\\nsuperiority of our approach over alternative state-of-the-arts. Codes are\\navailable at https://github.com/ZEKAICHEN/GTA.\\n",
  "full_text": "1\nLearning Graph Structures with Transformer for\nMultivariate Time Series Anomaly Detection in IoT\nZekai Chen, Student Member, IEEE, Dingshuo Chen, Xiao Zhang, Member, IEEE, Zixuan Yuan,\nand Xiuzhen Cheng, Fellow, IEEE\nAbstract—Many real-world IoT systems, which include a\nvariety of internet-connected sensory devices, produce substantial\namounts of multivariate time series data. Meanwhile, vital IoT\ninfrastructures like smart power grids and water distribution net-\nworks are frequently targeted by cyber-attacks, making anomaly\ndetection an important study topic. Modeling such relatedness is,\nnevertheless, unavoidable for any efﬁcient and effective anomaly\ndetection system, given the intricate topological and nonlinear\nconnections that are originally unknown among sensors. Further-\nmore, detecting anomalies in multivariate time series is difﬁcult\ndue to their temporal dependency and stochasticity. This paper\npresented GTA, a new framework for multivariate time series\nanomaly detection that involves automatically learning a graph\nstructure, graph convolution, and modeling temporal dependency\nusing a Transformer-based architecture. The connection learning\npolicy, which is based on the Gumbel-softmax sampling approach\nto learn bi-directed links among sensors directly, is at the heart\nof learning graph structure. To describe the anomaly information\nﬂow between network nodes, we introduced a new graph convo-\nlution called Inﬂuence Propagation convolution. In addition, to\ntackle the quadratic complexity barrier, we suggested a multi-\nbranch attention mechanism to replace the original multi-head\nself-attention method. Extensive experiments on four publicly\navailable anomaly detection benchmarks further demonstrate\nthe superiority of our approach over alternative state-of-the-arts.\nCodes are available at https://github.com/ZEKAICHEN/GTA.\nIndex Terms—Multivariate time series, anomaly detection,\ngraph learning, self-attention\nI. I NTRODUCTION\nDue to the fast rising number of Internet-connected sensory\ndevices, the Internet of Things (IoT) infrastructure has created\nvast sensory data. IoT data is often characterized by its\nspeed in terms of geographical and temporal dependency\n[1], [2], and it is frequently subjected to correspondingly\nrising abnormalities and cyberattacks [3], [4]. Many critical\ninfrastructures constructed on top of Cyber-Physical Systems\n(CPS) [5], such as smart power grids, water treatment and\ndistribution networks, transportation, and autonomous cars, are\nespecially in need of security monitoring [6], [7], [4]. As\na result, an efﬁcient and accurate anomaly detection system\nhas great research value because it can help with continuous\nmonitoring of fundamental controls or indicators and promptly\nprovide notiﬁcations for any probable anomalous occurrence.\nZ. Chen is with the Department of Computer Science, George Washington\nUniversity, Washington, DC, 20052 USA (email: zech chan@gwu.edu)\nZ. Yuan is with the School of Business, Rutgers University, New Jersey,\n08901 USA (email: zy101@rutgers.edu)\nX. Zhang (corresponding author), X. Cheng and D. Chen are with School\nof Computer Science and Technology, Shandong University, China (emails:\nxiaozhang@sdu.edu.cn, xzcheng@sdu.edu.cn)\nIn this work, we focus on anomaly detection for multivariate\ntime series [8] as a copious amount of IoT sensors in many\nreal-life scenarios consecutively generate substantial volumes\nof time series data. For instance, in a Secure Water Distribution\n(W ADI) system [9], multiple sensing measurements such as\nﬂowing meter, transmitting level, valve status, water pressure\nlevel, etc., are recorded simultaneously at each timestamp to\nform a multivariate time series. In this case, the central water\ntreatment testbed is also known as an entity. It is commonly\naccepted to detect anomalies from the entity-level instead of\nthe sensor-level since the overall status detection is generally\nworth more concern and less expensive. Predominantly, data\nfrom these sensors are highly correlated in a complex topo-\nlogical and nonlinear fashion: for example, opening a valve\nwould result in pressure and ﬂow rate changes, leading to\nfurther chain reactions of other sensors within the same entity\nfollowing an internal mechanism. Nevertheless, the dependen-\ncies among sensors are initially hidden and somehow costly\nto access in most real-life scenarios, leading to an intuitive\nquestion of how to model such complicated relationships\nbetween sensors without knowing prior information?\nRecently, deep learning-based techniques have demonstrated\nsome promising improvements in anomaly detection due to\nthe superiority in sequence modeling over high-dimensional\ndatasets. Generally, the existing approaches can roughly fall\ninto two lines: reconstruction-based models (R-model) [10],\n[11], [12], [6], [13] and forecasting-based models (F-model)\n[14], [15], [8], [16], [17], [4]. For example, Auto-Encoders\n(AE) [10] is a popular approach for anomaly detection, which\nuses reconstruction error as an outlier score. More recently,\nGenerative Adversarial Networks (GANs) [18], [19] based\non reconstruction [20], [6] and RNN-based forecasting ap-\nproaches [8], [17] have also reported promising performance\nfor multivariate anomaly detection. However, these methods\ndo not explicitly learn the topological structure among sen-\nsors, thus leaving room for improvements in modeling high-\ndimensional sensor data with considerable potential inter-\nrelationships appropriately.\nGraph Convolutional Networks (GCNs) [21], [22], [23],\n[24] have recently revealed discriminative power in learning\ngraph representations due to their permutation-invariance, lo-\ncal connectivity, and compositionality [21], [25]. Graph neural\nnetworks allow each graph node to acknowledge its neigh-\nborhood context by propagating information through struc-\ntures. Recent works [17], [26], [4] then combined temporal\nmodeling methods with GCNs to model the topological rela-\ntionships between sensors. Speciﬁcally, most existing graph-\narXiv:2104.03466v3  [cs.LG]  17 Jan 2022\n2\nbased approaches [25], [4] aim to learn the graph structure\nby measuring the cosine similarity (or other distance metrics)\nbetween sensor embeddings and deﬁning top-K closest nodes\nas the source node’sconnections, followed by a graph attention\nconvolution to capture the information propagation process.\nHowever, we argue that (1) dot products among sensor embed-\ndings lead inevitably to quadratic time and space complexity\nregarding the number of sensors ; (2) the tightness of spatial\ndistance can not entirely indicate that there exists a strong\nconnection in a topological structure .\nTo address the problems above, we propose an innova-\ntive framework named Graph Learning with Transformer for\nAnomaly detection (GTA) in this paper. We devise from the\nperspective of learning a global bi-directed graph structure\ninvolving all sensors within the entity through a connec-\ntion learning policy based on the Gumbel-Softmax Sampling\ntrick to overcome the quadratic complexity challenge and\nthe limitations of top-K nearest strategy. The policy logits\ncan automatically discover the hidden associations during the\ntraining process by determining whether any speciﬁc node’s\ninformation should ﬂow to the other targets to achieve the\nbest forecasting accuracy while restricting each node’s neigh-\nborhoods’ scope as much as possible. The discovered hidden\nassociations are then fed into the graph convolution layers\nfor information propagation modeling. We then integrate these\ngraph convolution layers with different level dilated convolu-\ntion layers to construct a hierarchical context encoding block\nspeciﬁcally for temporal data. While recurrent mechanisms can\nbe naturally applied to temporal dependency modeling, it is\nhard to parallelize in many mobile environments (e.g., IoT),\nwhich require high computation efﬁciency. Hence we adopt the\nTransformer [27] based architecture for the sequence modeling\nand forecasting due to the parallel efﬁciency and capability of\ncapturing long-distance context information. We also propose\na novel multi-branch attention strategy to reduce the quadratic\ncomplexity of original self multi-head attention.\nThe main contributions of our work are summarized as\nfollows:\n• We propose a novel and differentiable connection learn-\ning policy to automatically learn the graph structure of\ndependency relationships between sensors. Meanwhile,\neach node’s neighborhood ﬁeld is restricted by integrating\na new loss term for further inference efﬁciency.\n• We introduce a novel graph convolution named Informa-\ntion Propagation (IP) convolution to model the anomaly\ninﬂuence ﬂowing process. A multi-scale dilated convo-\nlution is then combined with the graph convolution to\nform an effective hierarchical temporal context encoding\nblock.\n• We propose a novel multi-branch attention mechanism\nto tackle the original multi-head attention mechanism’s\nquadratic complexity challenge.\n• We conduct extensive experiments on a wide range of\nmultivariate time series anomaly detection benchmarks\nto demonstrate the superiority of our proposed approach\nover state-of-the-arts.\nII. R ELATED WORK\nThe existing literature for addressing time series anomaly\ndetection usually can be divided into two major categories.\nThe ﬁrst category usually modeled each time series variable\nindependently, while the second category took into considera-\ntion the correlations among multivariate time series to improve\nthe performance.\nA. Anomaly Detection in Univariate Time Series\nThe anomaly detection in univariate time series has drawn\nmany researchers’ attentions in recent years. Traditionally,\nthe anomaly detection frameworks included two main phases:\nestimation phase and detection phase [28]. In estimation phase,\nthe variable values at one timestamp or time interval can\nbe predicted or estimated by speciﬁc algorithm. Then the\nestimated values were compared with real values based on\ndynamically adjusted thresholds to detect anomalies in detec-\ntion phase. For example, Zhang et. al [29] applied ARIMA\nto capture the linear dependencies between the future values\nand the past values, thus modeling the time series behavior\nfor anomaly detection. Lu et.al [30] utilized wavelet analysis\nto construct the estimation model. With the development of\ndeep learning, various neural network architectures have also\nbeen applied to anomaly detection. DeepAnt [31] was an\nunsupervised approach using convolutional neural network\n(CNN) to forecast future time series values and adopted\nEuclidean distance to measure the discrepancy for anomaly\ndetection. The LSTM neural network was also widely used\nin modeling time series behaviors [32], [33], [6]. The LSTM-\nbased encoder-decoder [32] reconstructed the variable values\nand measured the reconstruction errors for detection.\nB. Anomaly Detection in Multivariate Time Series\nIn real-world scenarios, the time series data acquisition\nsources could be multiple [34]. Therefore, many work began\nto pay attention to exploiting the correlations among multiple\nvariables to improve the accuracy of anomaly detection. Jones\net.al [35] extracted statistical and smoothed trajectory (SST)\nfeatures of time series and utilized a set of non-linear func-\ntions to model related variables to detect anomalies. Using\nthe LSTM network as the base models to to capture the\ntemporal correlations of time series data, MAD-GAN [6]\nproposed an unsupervised anomaly detection method com-\nbining generative adversarial networks (GAN) by considering\ncomplex dependencies amongst different time series variables.\nSakurada et al. [36] conducted dimentionality reduction based\non autoencoders for anomaly detection. The ODCA frame-\nwork [37] included three parts: data preprocessing, outlier\nanalysis, and outlier rank, which used cross correlation to\ntranslate high-dimentional data sets to one-dimentional cross-\ncorrelation function. OmniAnomaly [13] was a stochastic\nmodel to avoid potential misguiding by uncertain instances,\nwhich used stochastic variable connection and normalizing\nﬂow to get reconstruction probabilities and adopted stream-\ning POT with drift (DSPOT) algorithm [38] for automatic\nthreshold selection. Senin et al. [39] proposed two algorithms\n3\nFig. 1: The visualization of our proposed GTA’s architecture with llevels dilated convolution and graph convolution, 3 encoder\nlayers, and 1 decoder layer. Generally, the input multivariate time series inputs are split into train sequences and label sequences,\nof which train sequences are fed into encoder while label sequences are fed to the decoder.\nthat conducted symbolic time series discretization and used\ngrammar reduction to compress the input sequence and com-\npactly encode them with grammar rules. Those rarely used\nsubstrings in the grammar rules were regarded as anomalies.\nAutoregressive with exogenous inputs (ARX) and artiﬁcial\nneural network (ANN) [40] extracted time-series features and\ndetected anomalous data points by conducting hypothesis\ntesting on the extrema of residuals.\nTo cover the shortage that the convolution and pooling\noperators of CNNs are deﬁned for regular grids, recent GNN\n[41] generalizes CNNs to graphs that are able to encode\nirregular and non-Euclidean structures. GNN adopted the\nlocalized spectral ﬁlters and used a graph coarsening algorithm\nto cluster similar vertices for speeding up. In this way, GNN\nefﬁciently extracted the local stationary property and captured\nthe correlation between nodes. In real-world IoT environment,\nthe graph structure modeling the correlations between sensors\nis often not predeﬁned in advance. Graph deviation network\n(GDN) [4] learned the pairwise relationship by cosine simi-\nlarity to elaborate adjacent matrix which can be modeled as a\ngraph. Then it predicted the future values by graph attention-\nbased forecasting and computed the absolute error value to\nevaluate graph deviation score. MTAD-GAN [17] concate-\nnated feature-oriented and time-oriented graph attention layer\nto learn graph structure and used both forecasting-based model\nand reconstruction-based model to calculate integrated loss.\nThen automatic threshold algorithm was adopted to perform\nanomaly detection.\nIII. P ROBLEM STATEMENT\nIn this work, we focus on the task of multivariate time\nseries anomaly detection. Let X(t) ∈RM denote the original\nmultivariate time series data at any timestamp t, where M is\nthe total number of sensors or any data measuring node within\nthe same entity. M is also reported as the number of features\nor variables in some literature [6], [13], [4]. Considering\nthe high unbalance between normal data and anomalies, we\nonly construct the sequence modeling process on normal data\n(without anomalies) and make prediction on testing data (with\nanomalies) for anomaly detection. Speciﬁcally, we let X and\nˆX represent the entire normal data and data with anomalies,\nrespectively. For sequence modeling on normal data, we inherit\na forecasting-based strategy to predict the time series value\nx(t) ∈RM at next time step t(aka. single-step time series fore-\ncasting) based on the historical data x = {x(t−n),··· ,x(t−1)}\nwith a speciﬁc window size n. Therefore, given a sequence of\nhistorical ntime steps of multivariate contiguous observations\nˆx ∈RM×n, the goal of anomaly detection is to predict the\noutput vector ˆy ∈Rn, where ˆy(t) ∈{0,1}denotes binary\nlabels indicating whether there is an anomaly at time tick t.\nPrecisely, our proposed approach returns an anomaly score\nfor each testing timestamp, and then the anomaly result can\nbe obtained via selecting different thresholds.\nWe also provide some basic graph-related concepts for\nbetter understanding formulated as follows:\nDeﬁnition III.1 (Graph). A directed graph is formulated as\nG= (V,E) where V= {1,··· ,M}is the set of nodes, and\nE ⊆ V×Vis the set of edges, where ei,j represents the\nuni-directed edge ﬂowing from node i to node j.\nDeﬁnition III.2 (Node Neighborhood) . Let i ∈V denote a\nnode and ei,j ∈E denote the edge pointing from node i to\nnode j. The neighborhood of any node iis deﬁned as N(i) =\n{j ∈V|ei,j ∈E}.\n4\nFig. 2: Suppose we have 3 sensors ( N1,N2,N3) of which the\ndependencies are yet hidden. Our connection learning policy’s\nmain idea is to use the Gumbel-Softmax Sampling strategy to\nsample a random categorical vector for determining whether\nany directed connection between two nodes can be established.\nFor N1 and N2, if the value of P1,2 is relatively high, it\nrepresents N1 is highly possibly pointed to N2, vice versa.\nIV. M ETHODOLOGY\nIn most real-life scenarios of IoT, there are usually complex\ntopological relationships between sensors where the entire\nentity can be seen as a graph structure. Each sensor is also\nviewed as a speciﬁc node in the graph. Previous methods [25],\n[4] focused on applying various distance metrics to measure\nthe relations between nodes mostly by selecting the top-K\nclosest ones as their neighbor dependencies. Different from\nexisting approaches, we devise a directed graph structure\nlearning policy (see Fig. 1) to automatically learn the adja-\ncency matrix among nodes such that the network can achieve\nthe maximum beneﬁts. The core of the learning policy is\nnamed Gumbel-Softmax Sampling strategy [42], [43] inspired\nby the policy learning network in many reinforcement learning\nmethods [44], [45]. These discovered hidden associations are\nthen fed into the graph convolution layers for information\npropagation modeling. We then integrate these graph convo-\nlution layers with different level dilated convolution layers\ntogether to construct a hierarchical context encoding block\nspeciﬁcally for temporal data. The outputs of the context\nencoding block are then applied positional encoding as the\ninputs of Transformer [27] for single-step time series forecast-\ning. We also propose a global attention strategy to overcome\nthe quadratic computation complexity challenge of the multi-\nhead attention mechanism. Fig. 1 further illustrates the entire\narchitecture in detail.\nA. Gumbel-Softmax Sampling\nThe sampling process of discrete data from a categori-\ncal distribution is originally non-differentiable, where typical\nbackpropagation in deep neural networks cannot be conducted.\n[42], [43] proposed a differentiable substitution of discrete\nrandom variables in stochastic computations by introducing\nGumbel-Softmax distribution, a continuous distribution over\nthe simplex that can approximate samples from a categorical\ndistribution. In our graph learning policy with a total number\nof M candidate nodes, we let zi,j be a binary connection\ncontrol variable for any pair of nodes iand j with uni-directed\nprobabilities from node i to node j as {πi,j\n0 ,πi,j\n1 }, where\nπi,j\n0 + πi,j\n1 = 1 and πi,j\n1 represents the probability that there\nexists an information ﬂow from node ito node j in the graph\n(see Fig. 2). Similarly, by Gumbel-Max trick, we can sample\nany pair of nodes’ connection strategy zi,j ∈{0,1}2 with:\nzi,j = arg max\nc∈{0,1}\n(log πi,j\nc + gi,j\nc ) (1)\nwhere g0,g1 are i.i.d samples drawn from a standard Gumbel\ndistribution which can be easily sampled using inverse trans-\nform sampling by drawing u∼Uniform(0,1) and computing\ng = −log(−log u). We further substitute this arg maxoper-\nation, since it is not differentiable, with a Softmax reparam-\neterization trick, also known as Gumbel-Softmax trick, as:\nzi,j\nc = exp((log πi,j\nc + gi,j\nc )/τ)∑\nv∈{0,1}\nexp((log πi,j\nv + gi,j\nv )/τ)\n(2)\nwhere c ∈ {0,1} and τ is the temperature parameter to\ncontrol Gumbel-Softmax distribution’s smoothness, as the\ntemperature τ approaches 0, the Gumbel-Softmax distribution\nbecomes identical to the one-hot categorical distribution. As\nthe randomness of g is independent of π, we can now directly\noptimize our gating control policy using standard gradient\ndescent algorithms.\nCompared to the previous graph structure learning ap-\nproaches, our proposed method signiﬁcantly reduces the com-\nputation complexity from O(M2) to O(1) since it requires\nno dot products among high-dimensional node embeddings.\nAdditionally, the graph structure learning policy is able to\nautomatically learn the global topological connections among\nall nodes, thereby avoiding the limitation of selecting only the\ntop-K nearest nodes as neighbors.\nB. Inﬂuence Propagation via Graph Convolution\nOn top of the learned topological structure, the graph\nconvolution block aims to further model the inﬂuence propa-\ngation process and update each speciﬁc node’s representation\nby incorporating its neighbors’ information. Considering the\ncharacteristics of tasks such as anomaly detection, usually,\nthe occurrence of abnormalities is due to a series of chain\ninﬂuences caused by one or several nodes being attacked.\nTherefore, it is intuitive for us to model the relationships\nbetween upstream and downstream nodes by capturing both\ntemporal and spatial differences. Thus, we deﬁne our Inﬂu-\nence Propagation (IP) convolution process concerning each\nspeciﬁc node and its neighborhoods by applying a node-wise\nsymmetric aggregating operation □ (e.g., add, mean, or max)\non the differences between nodes associated with all the edges\nemanating from each node. The updated output of IPConv at\nthe i-th node is given by:\nx′\ni =\n∑\nj∈N(i)\nhΘ(xi||xj −xj||xj + xi) (3)\nwhere □ is chosen as summation in our method, hΘ denotes a\nneural network, i.e. MLPs (Multi-layer Perceptrons), xi ∈RT\nrepresents the time series embedding of node i and || de-\nnotes the concatenation operation. We denote xj −xi as the\ndifferences between nodes to explicitly model the inﬂuence\npropagation delay from node j to i, captured by the value\ndifference at each timestamp of the time series embedding. We\n5\nFig. 3: Visualization of hierarchical dilated convolution com-\nbined with graph convolution.\nalso incorporate the term xi +xj with the differences to work\nas a scale benchmark such that the model can learn the truly\ngeneralized impact to the other nodes brought by anomalies\ninstead of extreme values. Intuitively, for any speciﬁc node\ni, if one of its neighbor nodes j being attacked, node i\nshall be severely affected sooner or later due to the restricted\ntopological relationship.\nTraining Strategy and Regularization. Graph convolution\nbased on the learned dependencies among sensors only aggre-\ngates the information from nodes’ neighbors without taking\nefﬁciency into account. Under the extreme circumstance that\nall nodes are mutually connected, aggregating neighborhood\ninformation adds considerable noise to each node. However,\nit is preferred to form a compact sub-graph structure for\nevery single node, in which redundant connections are omitted\nas much as possible without deteriorating the forecasting\naccuracy. To this end, we propose a sparsity regularization\nLs to enhance the compactness of each node by minimizing\nthe log-likelihood of the probability of a connection being\nestablished as\nLs =\n∑\n1≤i,j≤M,i̸=j\nlog πi,j\n1 (4)\nFurthermore, to encourage better learning convergence, the\nconnection learning policy is initialized with all nodes con-\nnected. We warm up the network weights by training with\nthis complete graph structure for a few epochs to provide a\ngood starting point for the policy learning.\nC. Hierarchical Dilated Convolution\nThe dilated convolution [46] is widely used in sequence\nmodeling due to its powerful capability in extracting high-level\ntemporal context features by capturing sequential patterns of\ntime series data through standard 1D convolution ﬁlters. Set-\nting different dilation size levels can discover temporal patterns\nwith various ranges and handle very long sequences. However,\nchoosing the right kernel size is often a challenging prob-\nlem for convolutional operations. Some previous approaches\nadopted the widely employed inception learning strategy [47]\nin computer vision which concatenates the outputs of convolu-\ntional ﬁlters with different kernel sizes followed by a weighted\nmatrix. Unlike them, we propose a hierarchical dilated convo-\nlution learning strategy combined with the graph convolution\nabove to fully explore the temporal context modeling process\nwith different sequence lengths and receptive ﬁelds by setting\nmulti-scale dilation sizes. Speciﬁcally, as Fig. 3 illustrates, the\nbottom layer represents the multivariate time series input (for\nsome time t, onto which repeated dilated convolutions, with\nincreasing dilation rates, are applied; the ﬁlter width is again\nset to equal two in the observed model). The ﬁrst level block\napplies dilated convolutions with the dilation rate equal to one,\nmeaning that the layer applies the ﬁlter onto two adjacent\nelements, x(t) and x(t+1), of the input series. The outputs\nof the ﬁrst-level dilated convolutions are fed into the graph\nconvolution module proposed above. Then the second-level\nlayer applies dilated convolutions, with the rate now set to\nequal two, which means that the ﬁlter is applied onto elements\nx(t) and x(t+2) (notice here that the number of parameters\nremains the same, but the ﬁlter width has been “widened”). By\nsetting multi-scale dilation sizes with a hierarchical learning\nstyle, abundant temporal representations concerning different\ntemporal positions and sequence lengths can be effectively\nlearned.\nThe hierarchical dilated convolution and the graph convo-\nlution together form the temporal context embedding progres-\nsion where dilated convolution captures the long-term tempo-\nral dependencies while graph convolution describes the topo-\nlogical connection relationships between sensors (or nodes).\nAs a result, the ﬁnal outputs have been well represented to be\nthe inputs of the next forecasting procedure using Transformer\narchitecture.\nD. More Efﬁcient Multi-branch Transformer\nTransformer [27] has been widely used in sequence model-\ning due to the superior capability of multi-head attention mech-\nanism in long-distance dependencies capturing. However, one\nmain efﬁciency bottleneck in self-attention is that the pairwise\ntoken interaction dot-production incurs a complexity of O(n2)\nwith respect to sequence length. To tackle this challenge, in\nthis section, we ﬁrst brieﬂy review the background of some\nrecent development of multi-head attention mechanism and\nthen propose a more efﬁcient Transformer architecture based\non the innovative multi-branch attention mechanism which is\nmore computationally efﬁcient.\nSelf-attention in Transformers. The vanilla multi-head\nself-attention mechanism was originally proposed by [27].\nFor a sequence of token representations X ∈ Rn×d (with\nsequence length n and dimensionality d), the self-attention\nfunction ﬁrstly projects them into queries Q ∈Rn×dk , keys\nK ∈Rn×dk and values V ∈Rn×dv , h times with different,\nlearned linear projections to dk, dk and dv dimensions, re-\nspectively. Then a particular scaled dot-product attention was\ncomputed to obtain the weights on the values as:\nAttention(Q,K,V) = Softmax(QKT\n√dk\n)V (5)\nMulti-head attention allows the model to jointly attend to in-\nformation from different representation subspaces at different\n6\n(a) Vanilla multi-branch Trans-\nformer.\n(b) Multi-branch Transformer with\nglobal-ﬁxed attention.\n(c) Our proposed multi-branch at-\ntention mechanism.\nFig. 4: Different variants of efﬁcient multi-branch attention mechanism. Right: Replacing vanilla multi-head attention with\na combination of both global-ﬁxed attention and vanilla multi-head attention and neighborhood convolution by splitting\nembeddings into multiple channels.\npositions. With a concatenated computing way, the ﬁnal output\nof multi-head attention is as following:\nMultiHead(Q,K,V) = Concat(head1,··· ,headh)WO\n(6)\nin which, his the number of total heads. Each head is deﬁned\nas:\nheadi = Attention(QWQ\ni ,KWK\ni ,VWV\ni ) (7)\nwhere the projections are parameter matrices WQ\ni ∈Rd×dk ,\nWK\ni ∈Rd×dk , WV\ni ∈Rd×dv and WO ∈Rhdv×d.\nGlobal-learned Attention. Recent research [48], [49] claim\nthat the self-attention in Transformers can be substantially\nsimpliﬁed with trivial attentive patterns at training time: only\npreserving adjacent and previous tokens is necessary. The adja-\ncent positional information such as ”current token”, ”previous\ntoken” and ”next token” are the key features learned across all\nlayers by encoder self-attention. Instead of costly learning the\ntrivial pattern using massive corpus with considerable compu-\ntational resources, the conventional pairwise token interaction\nattention could be replaced by a more computation-efﬁcient\nglobal attention pattern. In practice, manually pre-deﬁne all\nglobal-ﬁxed patterns is easy to implement but can barely\ncover all possible situations. To generalize the global-ﬁxed\nattention pattern proposed in [50], we apply a parameter matrix\nS ∈ Rm×m (m > n) as a learnable global compatibility\nfunction across all training samples following the Synthesizer\n[51]. Hence, each head adds m2 parameters while reducing\ntwo projection matrices WQ and WK. The attention now has\nbeen as following:\nAttention(S,V) = Softmax(S)V (8)\nwhere S is a learnable matrix which can be randomly initial-\nized.\nMulti-branch Architecture for Transformers. [52] has\ndemonstrated the effectiveness of multi-branch attention in\ncapturing global and local context patterns, especially under\nmobile computational constraints. As Fig. 4 illustrates, this\ndouble-branch architecture splits the original input sequences\ninto two pieces along the embedding channel, followed by\ntwo attention branches: one convolution branch for extracting\ninformation in a restricted neighborhood and one multi-head\nattention branch for capturing long-distance dependencies. As\na substitute for vanilla self-attention, we apply a task-speciﬁc\nalignment matrix that learns globally across all training sam-\nples where attention weights are no longer conditioned on any\ninput token in our architecture. By simply replacing the dot-\nproduction with global-learned alignment as Fig. 4 shows, the\ninput sequences will only be projected into value matrices. A\nweighted sum up of values is then calculated using this global-\nlearned attention. In order to explore a better trade-off between\ncomputation efﬁciency and model performance, we propose\nto combine the pairwise token interactions and global-learned\nattention in terms of a branch-wise mixing strategy.\nBranch-wise Mixing. For branch-wise mixing, the input\nsequences are split into multiple branches along the embedding\ndimension as Fig. 4 clearly describes. Different from the\noriginal two-branch architecture, we build one more branch\nfor global-learned attention. Thus,\nAttention = Concat(A(1),A(2))\nA(1) = MultiHead(X(1))\nA(2) = Global(X(2))\n(9)\nwhere X(1) ∈Rn×d1 , X(2) ∈Rn×d2 and d= d1 + d2.\nIn our models, we only change the branch that captures\nthe global contexts while remaining the local pattern extractor\nusing either lightConv or dynamicConv [53].\nComputation Analysis. Table I lists the different model\nvariants explored within our proposed framework. The column\n|θ| refers to the total number of parameters in one self-\nattention module excluding the feed-forward layer. Obviously,\ncompared to the original scaled dot-production, the amount of\ncomputation of global-learned attention is directly reduced by\nhalf in terms of Mult-Adds. Our proposed multi-branch mixing\nstrategy increases the amount of calculation in varying degrees\ndue to the mix with scaled dot-production. However, this is\na trade-off between computation complexity and model size.\nMore precisely, when m≤\n√\n2/hd, the global attention mod-\nule is more computationally efﬁcient than the other variants.\nE. Anomaly Scoring\nInspired by [54], the original multivariate time series inputs\nare split into two parts: training sequences for the encoder and\nlabel sequences for the decoder. The decoder receives long\nsequence inputs, pads the target elements into zero, measures\nthe weighted attention composition of the feature map, and\n7\nTABLE I: Memory and computation analysis on different attention types.\nAttention Type |θ| # Mult-Adds Global/Inter\nScaled Dot-Product 4d2 O(4nd2 + 2n2d) Inter\nGlobal-Learned m2h+ 2d2 O(2nd2 + n2d) Global\nBranch-Wise Mixing 4d2\n1 + m2h+ 2d2\n2 O(4nd2\n1 + n2d1 + 2nd2\n2 + n2d) Both\ninstantly predicts output elements in a generative style. Let\nthe single-step prediction denote as ˆY∈ RM×n. We apply the\nMean Square Error (MSE) between the predicted outputs ˆY\nand the observation Y, as the loss function to minimize:\nLmse = 1\nM\nn∑\nt=1\n||Y(t) −ˆY(t)||2\n2 (10)\nSimilar to the loss objective, the anomalous score compares\nthe expected value at time tto the observed value, computing\nan anomaly score via the deviation level as:\nˆy(t) =\nM∑\ni=1\n||Y(t)\ni −ˆY(t)\ni ||2\n2 (11)\nFinally, we label a timestamp tas an anomaly if ˆy(t) exceeds a\nﬁxed threshold. Since different approaches could be employed\nto set the threshold such as extreme value theory [38], the same\nanomaly detection model could result in different prediction\nperformance with different anomaly thresholds. Thus, we\napply a grid search on all possible anomaly thresholds to\nsearch for the best F1-score (with notation ∗∗) and Recall\n(with notation ∗) in theory and report them.\nV. E XPERIMENTS\nA. Datasets\nWe evaluate our method over a wide range of real-world\nanomaly detection datasets. SWaT [55] The Secure Water\nTreatment dataset is collected from a water treatment testbed\nfor cyber-attack investigation initially launched in May 2015.\nThe SWaT dataset collection process lasted for 11 days, with\nthe system operated 24 hours per day such that the network\ntrafﬁc and all the values obtained from all 51 sensors and\nactuators are recorded. Due to the system working ﬂow char-\nacteristics, there is a natural topological structure relationship\nbetween all sensing nodes. After this, a total of 41 attacks\nderived through an attack model considering the intent space\nof a CPS were launched during the last 4 days of the 2016\nSWaT data collection process. As such, the overall sequential\ndata is labeled according to normal and abnormal behaviors\nat each timestamp. W ADI[9] Water Distribution dataset is\ncollected from a water distribution testbed as an extension\nof the SWaT testbed. It consists of a total of 16 days of\ncontinuous operations with 14 days under regular operation\nand 2 days with attack scenarios. The entire testbed contains\n123 sensors and actuators. Moreover, SMAP (Soil Moisture\nActive Passive satellite) and MSL (Mars Science Laboratory\nrover) are two public datasets published by NASA [56]. Each\ndataset has a training and a testing subset, and anomalies in\nboth testing subsets have been labeled [8].\nTable II and III summarises the statistics of the four datasets.\nIn order to fair comparison with other methods, the original\ndata samples for SWaT and W ADIare downsampled to one\nmeasurement every 10 seconds by taking the median values\nfollowing [4].\nTABLE II: Statistical summary of datasets SWaT and W ADI.\nDatasets SWaT W ADI\nFeature Desc. All sensors and actuators.\n# Features 51 112\n# Attacks 41 15\nAttack durations (mins) 2 ∼ 25 1.5 ∼ 30\nTraining size (normal data) 49619 120899\nTesting size (data with attacks) 44931 17219\nAnomaly rate (%) 12.14 5.75\nTABLE III: Statistical summary of datasets SMAP and MSL.\nDatasets SMAP MSL\nFeature Desc. Radiation, temperature,\npower, etc.\n# Features 25 25\nTraining size (normal data) 135183 58317\nTesting size (data with anomalies) 427617 73729\nAnomaly rate (%) 13.13 10.72\nB. Experimental Setup\n1) Data preprocessing: We perform a data standardization\nbefore training to improve the robustness of our model. Data\npreprocessing is applied on both training and testing set:\n˜x= x−min Xtrain\nmax Xtrain −min Xtrain\n(12)\nwhere max(Xtrain) and min(Xtrain) are the maximum value\nand the minimum value of the training set respectively.\n2) Evaluation metrics: We adopt the standard evaluation\nmetrics in anomaly detection tasks, namely Precision, Recall\nand F1 score, to evaluate the performance of our approach, in\nwhich:\nPrecision = TP\nTP + FP (13)\nRecall = TP\nTP + FN (14)\nF1 = 2×Precision ×Recall\nPrecision + Recall (15)\nwhere TP represents the truly detected anomalies (aka. true\npositives), FP stands for the falsely detected anomalies (aka.\nfalse positives), TN represents the correctly classiﬁed normal\nsamples (aka. true negatives), and FN is the misclassiﬁed\nnormal samples (aka. false negatives). Given the fact that\n8\nTABLE IV: Experimental results on SWaT and W ADI.\nDatasets Methods Precision(%) Recall(%) F1-score\nPCA 24.92 21.63 0.23\nKNN 7.83 7.83 0.08\nFB 10.17 10.17 0.10\nAE 72.63 52.63 0.61\nDAGMM 27.46 69.52 0.39\nLSTM-V AE 96.24 59.91 0.74\nMAD-GAN 98.97 63.74 0.77\nGDN 99.35 68.12 0.81\nGTA ∗ (ours) 74.91 96.41 0.84\nGTA ∗∗ 94.83 88.10 0.91\nSWaT\n∆↑ (best F1) -4.55% +29.33 % +12.35 %\nPCA 39.53 5.63 0.10\nKNN 7.76 7.75 0.08\nFB 8.60 8.60 0.09\nAE 34.35 34.35 0.34\nDAGMM 54.44 26.99 0.36\nLSTM-V AE 87.79 14.45 0.25\nMAD-GAN 41.44 33.92 0.37\nGDN 97.50 40.19 0.57\nGTA ∗ (ours) 74.56 90.50 0.82\nGTA ∗∗ 83.91 83.61 0.84\nW ADI\n∆↑ (best F1) -13.94% +108.04 % +47.37 %\nBest performance in bold. Second-best with underlines.\n∗ represents the results chosen by best Recall.\n∗∗ represents the results chosen by best F1-score.\n∆↑ represents the percentage increase between our best F1-score\nperformance and the second-best method (GDN).\nTABLE V: Experimental results on SMAP and MSL.\nSMAP MSL\nMethod Precision(%) Recall(%) F1-score Precision(%) Recall(%) F1-score\nKitNet 77.25 83.27 0.8014 63.12 79.36 0.7031\nGAN-Li 67.10 87.06 0.7579 71.02 87.06 0.7823\nLSTM-V AE 85.51 63.66 0.7298 52.57 95.46 0.6780\nMAD-GAN 80.49 82.14 0.8131 85.17 89.91 0.8747\nR-Models\nOmniAnomaly 74.16 97.76 0.8434 88.67 91.17 0.8989\nLSTM-NDT 89.65 88.46 0.8905 59.44 53.74 0.5640\nDAGMM 58.45 90.58 0.7105 54.12 99.34 0.7007\nMTAD-GAT 89.06 91.23 0.9013 87.54 94.40 0.9084\nF-Models\nGTA∗∗ (ours) 89.11 91.76 0.9041 91.04 91.17 0.9111\nBest performance in bold. Second-best with underlines.\n∗∗ represents the results chosen by best F1-score.\nin many real-world anomaly detection scenarios, it is more\nvital for the system to detect all the real attacks or anomalies\nby tolerating a few false alarms. As such, we generally give\nmore concern to Recall and the overall F1 score instead\nof Precision. Considering different anomaly score thresholds\nmay result in different metric scores, we hence report both\nour best Recall and F1 results (with notations ∗ and ∗∗\nrespectively) on all datasets for a thorough comparison.\nAlso, we adopt the point-adjust way to calculate the perfor-\nmance metrics following [13]. In practice, anomalous observa-\ntions usually occur consecutively to form contiguous anomaly\nsegments. An alert for anomalies can be triggered within any\nsubset of an actual anomaly window. Thus, for any observation\nin the ground truth anomaly segment, if it is detected as an\nanomaly or attack, we would consider this whole anomaly\nwindow is correctly detected and every observation point in\nthis segment has been classiﬁed as anomalies. The observa-\ntions outside the ground truth anomaly segment are treated as\nusual. In all, we ﬁrst train our model on the training set to\nlearn the general sequence pattern and make the forecasting\non the test set for anomaly detection.\n3) Baselines: We compare our GTA with a wide range of\nstate-of-the-arts in multivariate time series anomaly detection,\nincluding: (1) reconstruction-based models: PCA, AE [10],\nKitNet [57], DAGMM [12], GAN-Li [20], OmniAnomaly\n[13], LSTM-V AE [11], MAD-GAN [6], and (2) forecasting-\n9\nbased models: KNN [14], FB [15], MTAD-GAT [17] and GDN\n[4].\n4) Training Settings: We implement our method and all its\nvariants using Pytorch 1 version 1.7.0 with CUDA 10.1 and\nPytorch Geometric Library [58] version 1.6.3. We conduct\nall experiments on four NVIDIA Tesla P100 GPUs. For time\nseries forecasting, we set the historical window size to 60\nframes with a label series length as 30 to predict the value at\nnext timestamp. The number of dilated convolution levels for\ntemporal context modeling is set to 3. Also, the general model\ninput embedding dimension is set to 128. For the conventional\nmulti-head attention mechanism, the number of heads is set\nto 8. In total, we have 3 encoder layers and 2 decoder layers\nand the dimensional of fully connected network is set to 128\nwhich is equal to the model dimension. Additionally, we apply\nthe dropout strategy to prevent overﬁtting with dropout rate\nconsistently equals to 0.05. The models are trained using the\nAdam optimizer with learning rate initialized as 1e−4 and\nβ1,β2 as 0.9, 0.99, respectively. A learning rate adjusting\nstrategy is also applied. We train our models for up to 50\nepochs and early stopping strategy is applied with patience of\n10. We run each experiment for 5 trials and report the mean\nvalue.\nC. Experimental Results\nIn Table IV, we show the anomaly detection accuracy in\nterms of precision, recall, and F1-score, of our proposed GTA\nmethod and other state-of-the-arts on datasets SWaT and\nW ADI. Each of these baselines provides a speciﬁc threshold\nselection method, and the reported F1-score is calculated\ncorrespondingly. Our proposed GTA signiﬁcantly outperforms\nall the other approaches on both datasets by achieving the\nbest F1-score as 0.91 for SWaT and 0.84 for W ADI. As-\ntonishingly, compared to the second-best model GDN, GTA\ncan achieve an overall 12.35% increase and an impressive\n47.47% improvement in terms of the best F1-score on these\ntwo datasets, respectively. Moreover, we have the following\nobservations: (1) Compared to the conventional unsupervised\napproaches such as PCA, KNN, FB, deep learning-based tech-\nniques (AE, LSTM-V AE, MAD-GAN, etc.) generally have a\nbetter detection performance on both datasets. By adopting the\nrecurrent mechanism (RNN, GRU, LSTM) in modeling long\nsequences and capturing the temporal context dependencies,\nthe deep learning-based methods demonstrate superiority over\nthe conventional methods. (2) DAGMM [12] aims to handle\nmultivariate data without temporal information, indicating the\ninput data contains only one observation instead of a historical\ntime series window. Hence this approach is not suitable for\ntemporal dependency modeling, which is crucial for multivari-\nate time series anomaly detection. (3) Most existing methods\nare based on recurrent neural networks to capture tempo-\nral dependency, including both reconstruction-based models\n(LSTM-V AE, OmniAnomaly, MAD-GAN) and forecasting-\nbased models (LSTM-NDT, MTAD-GAT). Of which, LSTM-\nNDT [8] is a deterministic model without leveraging stochastic\n1https://pytorch.org/\nTABLE VI: Anomaly detection accuracy in terms of preci-\nsion(%), recall(%), and F1-score of GTA and its variants.\nMethod SWaT W ADI\nPrec(%) Rec(%) F1-score Prec(%) Rec(%) F1-score\nGTA 94.83 88.10 0.91 83.91 83.61 0.84\nw/o Graph 88.64 65.73 0.75 71.25 68.23 0.70\nw/o LP 89.36 72.12 0.80 79.56 77.10 0.78\nw/o Attn 78.75 65.34 0.71 74.75 70.90 0.73\ninformation for modeling the inherent stochasticity of time se-\nries. LSTM-V AE [11] combines LSTM with V AE for sequence\nmodeling; however, it ignores the temporal dependencies\namong latent variables. OmniAnomaly [13] was then proposed\nto solve this problem. Additionally, MAD-GAN [6] aims to\nadopt a general adversarial training fashion to reconstruct\nthe original time series, which also uses recurrent neural\nnetworks. Nevertheless, the recurrent learning mechanism’s\ncore properties restrict the modeling process to be sequential.\nPast information has to be retained through the past hidden\nstates, limiting the long-term sequence modeling capability\nof the model. Transformer adopts a non-sequential learning\nfashion, and the powerful self-attention mechanism makes the\ncontext distance between any token of a time series shrink\nto one, which is of high importance to sequence modeling\nas more historical data can provide more pattern information.\n(4) Though GDN [4] is also a graph learning-based anomaly\ndetection approach, it adopts the top-K nearest connection\nstrategy to model the topological graph structure among sen-\nsors, which have certain limitations as we discussed in Section\nI. MTAD-GAT [17] directly utilizes the initial graph structure\ninformation by assuming all sensors are mutually connected,\nmaking it a complete graph that is not suitable for many real-\nlife situations.\nFrom Table V, we can see that the overall improvements\nin terms of best F1-score on datasets SMAP and MSL are\nnot as impressive as Table IV shows. We argue the main\ndifference of results between the NASA anomaly datasets\nand the Cyber-attack datasets lies in the features’ dependen-\ncies. SMAP provides measurements of the land surface soil\nmoisture by measuring various separated attributes such as\nradiation, temperature, computational activities, etc. Though\nthese attributes are not entirely independent of each other,\nthe internal relationships between them are much weaker than\nthose within SWaT or W ADIwhere any slight change that\nappears on one sensor can propagate to the whole network.\nTherefore, our proposed graph structure learning strategy\nmight be more effective on datasets with a strong topological\nstructure.\nD. Ablation Studies\nTo study each component of our approach’s effectiveness,\nwe gradually exclude the elements to observe how the model\nperformance degrades on datasets SWaT and W ADI. First, we\nstudy the signiﬁcance of modeling the dependencies among\nsensors using graph learning. We directly apply the original\ntime series as the inputs for the Transformer and make the\n10\n(a) Partial graph structure learned by the learning policy.\n (b) The attacked sensor with three other malicious sensors.\nFig. 5: A case study of showing an attack in W ADI.\nforecasting without graph learned phase. Second, we study\nthe signiﬁcance of our proposed structure learning policy\n(LP) by substituting it with a static complete graph where\nevery node is bi-directionally linked to each other. Finally,\nto study the necessity of the Transformer-based architecture\nfor sequence modeling, we substitute the Transformer with\na GRU-based recurrent neural network for forecasting. The\nresults are summarized in Table VI and provide the following\nobservations: (1) Our proposed learning policy helps the graph\nconvolution operation by capturing only proper information\nﬂow with noises ﬁltered out. (2) There is a considerable gap\nbetween GTA and the variant without graph learning which\nagain demonstrates the importance of topological structure\nmodeling in handling multivariate time series anomaly detec-\ntion. (3) Transformer-based architecture exhibits superiority\nin sequence modeling, where the self-attention mechanism\nplays a critical role. Moreover, these results again conﬁrm that\nevery component of our method is indispensable and make\nthis framework powerful in multivariate time series anomaly\ndetection.\nE. Graph Learning and Case Study\nBy introducing a case study of an actual attack from\nthe Cyber-attack dataset W ADI, we evaluate what a graph\nstructure would the graph learning policy learn and how this\nhelps us localize and comprehend an anomaly in this section.\nAn assault with a period of 25.16 minutes was logged in the\nW ADI data collecting log, which fraudulently turned on the\nmotorized valve 1 MV 001 STATUS and caused an overﬂow\non the primary tank. It’s difﬁcult for the operation engineers\nto ﬁnd out the status of this valve manually because it’s still\nwithin normal range. As a result, it’s not easy to spot this\noddity.\nThe water distribution treatment, for example, consists\nof three-state processes from the water supply, distribution\nnetwork, and return water system, which are denoted as\nP1, P2, and P3, respectively. Every sensor and actuator, in\nevery condition, is inextricably linked. The raw water inlet\nvalve that regulates the SUTD entering, for example, is\nrepresented by 1 MV 001 STATUS. Because 1 FIT 001 PV\nis a downstream ﬂow indicator transmitter of the water dis-\ntribution, the value of 1 FIT 001 PV rises rapidly if the\n1 MV 001 valve is turned on abruptly. As its outcome of\nthe ﬁrst stage propagates the inﬂuence from the raw water\ntransfer pump to the second stage, 2 FIT 001 PV is also\nvulnerable to the same malicious attack. In addition, as\nLEAK DIFF PRESSURE becomes irregular during this pro-\ncedure, the leaking water pressure grows without a doubt. Our\ngraph learning policy learned a partial graph in Fig. 5a, which\nalmost properly depicts the topological interactions among\nsensors. The LEAD DIFF PRESSURE is almost related to\nevery other displayed node as malicious information passes\nfrom upstream sensors to downstream ones. More importantly,\nFig. 5b shows our model’s predicted sensor curves (blue lines)\nagainst the ground truth (red lines) of sensor 1 FIT 001 PV ,\n2 FIT 001 PV , and 2 PIT 001 PV within the attack dura-\ntion. The predictions of these sensors are consistently higher\nthan the ground truth, where the anomaly score increases\ncorrespondingly. It is mainly because the input time series has\nbeen embedded with the graph structure information through\nthe inﬂuence propagation convolution operation. Sensors that\nare not directly attacked will still be severely affected if\nsensors that are highly related to them are attacked. Therefore,\nour model can capture this dependency and result in an\nabnormal prediction, which is vital for the following anomaly\ndetection.\nVI. C ONCLUSION\nIn this work, we proposed GTA, a Transformer-based frame-\nwork for anomaly detection that uses the introduced connec-\ntion learning policy to automatically learn sensor dependen-\ncies. To simulate the information ﬂow among the sensors in the\ngraph, we devised an unique Inﬂuence Propagation (IP) graph\nconvolution. The inference speed of our proposed multi-branch\nattention technique is greatly improved without sacriﬁcing\nmodel performance. Extensive experiments on four real-world\ndatasets demonstrated that our strategy outperformed other\nstate-of-the-art approaches in terms of prediction accuracy. We\nalso provided a case study to demonstrate how our approach\nidentiﬁes the anomaly by utilizing our proposed techniques.\nWe aim to explore more about combining this approach with\nthe online learning strategy to land it on the mobile IoT\nscenarios for future work.\n11\nREFERENCES\n[1] M. S. Mahdavinejad, M. Rezvan, M. Barekatain, P. Adibi, P. M.\nBarnaghi, and A. P. Sheth, “Machine learning for internet of things\ndata analysis: A survey,” CoRR, vol. abs/1802.06305, 2018.\n[2] Z. Cai and Z. He, “Trading private range counting over big iot data,” in\n39th IEEE International Conference on Distributed Computing Systems,\nICDCS 2019, Dallas, TX, USA, July 7-10, 2019 . IEEE, 2019, pp. 144–\n153. [Online]. Available: https://doi.org/10.1109/ICDCS.2019.00023\n[3] M. Mohammadi, A. I. Al-Fuqaha, S. Sorour, and M. Guizani, “Deep\nlearning for iot big data and streaming analytics: A survey,” IEEE\nCommun. Surv. Tutorials, vol. 20, no. 4, pp. 2923–2960, 2018.\n[4] A. Deng and B. Hooi, “Graph neural network-based anomaly detection\nin multivariate time series,” in Proceedings of the 35th AAAI Conference\non Artiﬁcial Intelligence , 2021.\n[5] Z. Cai and X. Zheng, “A private and efﬁcient mechanism for\ndata uploading in smart cyber-physical systems,” IEEE Trans. Netw.\nSci. Eng. , vol. 7, no. 2, pp. 766–775, 2020. [Online]. Available:\nhttps://doi.org/10.1109/TNSE.2018.2830307\n[6] D. Li, D. Chen, B. Jin, L. Shi, J. Goh, and S. Ng, “MAD-GAN:\nmultivariate anomaly detection for time series data with generative\nadversarial networks,” in 28th International Conference on Artiﬁcial\nNeural Networks, ser. Lecture Notes in Computer Science, vol. 11730.\nSpringer, 2019, pp. 703–716.\n[7] X. Zheng and Z. Cai, “Privacy-preserved data sharing towards\nmultiple parties in industrial iots,” IEEE J. Sel. Areas Commun. ,\nvol. 38, no. 5, pp. 968–979, 2020. [Online]. Available: https:\n//doi.org/10.1109/JSAC.2020.2980802\n[8] K. Hundman, V . Constantinou, C. Laporte, I. Colwell, and\nT. S ¨oderstr¨om, “Detecting spacecraft anomalies using lstms and non-\nparametric dynamic thresholding,” in Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data\nMining. ACM, 2018, pp. 387–395.\n[9] C. M. Ahmed, V . R. Palleti, and A. P. Mathur, “W ADI: a water\ndistribution testbed for research in the design of secure cyber physical\nsystems,” in Proceedings of the 3rd International Workshop on Cyber-\nPhysical Systems for Smart Water Networks . ACM, 2017, pp. 25–28.\n[10] C. C. Aggarwal, Outlier Analysis. Springer, 2013.\n[11] D. Park, Y . Hoshi, and C. C. Kemp, “A multimodal anomaly detector\nfor robot-assisted feeding using an lstm-based variational autoencoder,”\nCoRR, vol. abs/1711.00614, 2017.\n[12] B. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu, D. Cho, and\nH. Chen, “Deep autoencoding gaussian mixture model for unsupervised\nanomaly detection,” in 6th International Conference on Learning Rep-\nresentations. OpenReview.net, 2018.\n[13] Y . Su, Y . Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, “Robust anomaly\ndetection for multivariate time series through stochastic recurrent neural\nnetwork,” in Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . ACM, 2019,\npp. 2828–2837.\n[14] F. Angiulli and C. Pizzuti, “Fast outlier detection in high dimensional\nspaces,” in Principles of Data Mining and Knowledge Discovery, 6th\nEuropean Conference , ser. Lecture Notes in Computer Science, vol.\n2431. Springer, 2002, pp. 15–26.\n[15] A. Lazarevic and V . Kumar, “Feature bagging for outlier detection,”\nin Proceedings of the 11th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining . ACM, 2005, pp. 157–166.\n[16] Y . Liang, Z. Cai, J. Yu, Q. Han, and Y . Li, “Deep learning based\ninference of private information using embedded sensors in smart\ndevices,” IEEE Netw. , vol. 32, no. 4, pp. 8–14, 2018. [Online].\nAvailable: https://doi.org/10.1109/MNET.2018.1700349\n[17] H. Zhao, Y . Wang, J. Duan, C. Huang, D. Cao, Y . Tong, B. Xu, J. Bai,\nJ. Tong, and Q. Zhang, “Multivariate time-series anomaly detection via\ngraph attention network,” in 20th IEEE International Conference on\nData Mining. IEEE, 2020, pp. 841–850.\n[18] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. C. Courville, and Y . Bengio, “Generative adversarial\nnetworks,” CoRR, vol. abs/1406.2661, 2014.\n[19] Z. Cai, Z. Xiong, H. Xu, P. Wang, W. Li, and Y . Pan,\n“Generative adversarial networks: A survey towards private and secure\napplications,” CoRR, vol. abs/2106.03785, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2106.03785\n[20] D. Li, D. Chen, J. Goh, and S. Ng, “Anomaly detection with gen-\nerative adversarial networks for multivariate time series,” CoRR, vol.\nabs/1809.04758, 2018.\n[21] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, and M. Sun, “Graph\nneural networks: A review of methods and applications,” CoRR, vol.\nabs/1812.08434, 2018.\n[22] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li `o, and\nY . Bengio, “Graph attention networks,” in 6th International Conference\non Learning Representations . OpenReview.net, 2018.\n[23] F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger,\n“Simplifying graph convolutional networks,” in Proceedings of the 36th\nInternational Conference on Machine Learning , ser. Proceedings of\nMachine Learning Research, vol. 97. PMLR, 2019, pp. 6861–6871.\n[24] Y . Wang, Y . Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, “Dynamic graph CNN for learning on point clouds,” ACM\nTrans. Graph., vol. 38, no. 5, pp. 146:1–146:12, 2019.\n[25] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang, “Con-\nnecting the dots: Multivariate time series forecasting with graph neural\nnetworks,” in Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . ACM, 2020,\npp. 753–763.\n[26] D. Cao, Y . Wang, J. Duan, C. Zhang, X. Zhu, C. Huang, Y . Tong, B. Xu,\nJ. Bai, J. Tong, and Q. Zhang, “Spectral temporal graph neural network\nfor multivariate time-series forecasting,” in NeurIPS, 2020.\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS,\n2017, pp. 5998–6008.\n[28] A. Bl ´azquez-Garc´ıa, A. Conde, U. Mori, and J. A. Lozano, “A re-\nview on outlier/anomaly detection in time series data,” CoRR, vol.\nabs/2002.04236, 2020.\n[29] Y . Zhang, Z. Ge, A. G. Greenberg, and M. Roughan, “Network anomog-\nraphy,” in Proceedings of the 5th Internet Measurement Conference,\nIMC 2005, Berkeley, California, USA, October 19-21, 2005 . USENIX\nAssociation, 2005, pp. 317–330.\n[30] W. Lu and A. A. Ghorbani, “Network anomaly detection based on\nwavelet analysis,” EURASIP J. Adv. Signal Process. , vol. 2009, 2009.\n[31] M. Munir, S. A. Siddiqui, A. Dengel, and S. Ahmed, “Deepant: A deep\nlearning approach for unsupervised anomaly detection in time series,”\nIEEE Access, vol. 7, pp. 1991–2005, 2019.\n[32] P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, and\nG. Shroff, “Lstm-based encoder-decoder for multi-sensor anomaly de-\ntection,” CoRR, vol. abs/1607.00148, 2016.\n[33] P. Filonov, A. Lavrentyev, and A. V orontsov, “Multivariate industrial\ntime series with cyber-attack simulation: Fault detection using an lstm-\nbased predictive data model,” CoRR, vol. abs/1612.06676, 2016.\n[34] A. A. Cook, G. Misirli, and Z. Fan, “Anomaly detection for iot time-\nseries data: A survey,” IEEE Internet Things J. , vol. 7, no. 7, pp. 6481–\n6494, 2020.\n[35] M. Jones, D. Nikovski, M. Imamura, and T. Hirata, “Exemplar learning\nfor extremely efﬁcient anomaly detection in real-valued time series,”\nData Min. Knowl. Discov. , vol. 30, no. 6, pp. 1427–1454, 2016.\n[36] M. Sakurada and T. Yairi, “Anomaly detection using autoencoders with\nnonlinear dimensionality reduction,” in Proceedings of the MLSDA 2014\n2nd Workshop on Machine Learning for Sensory Data Analysis, Gold\nCoast, Australia, QLD, Australia, December 2, 2014 , A. Rahman, J. D.\nDeng, and J. Li, Eds. ACM, 2014, p. 4.\n[37] H. Lu, Y . Liu, Z. Fei, and C. Guan, “An outlier detection algorithm\nbased on cross-correlation analysis for time series dataset,”IEEE Access,\nvol. 6, pp. 53 593–53 610, 2018.\n[38] A. Siffer, P. Fouque, A. Termier, and C. Largou ¨et, “Anomaly detection\nin streams with extreme value theory,” in Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge Discovery and Data\nMining. ACM, 2017, pp. 1067–1075.\n[39] P. Senin, J. Lin, X. Wang, T. Oates, S. Gandhi, A. P. Boedihardjo,\nC. Chen, and S. Frankenstein, “Time series anomaly discovery with\ngrammar-based compression,” in Proceedings of the 18th International\nConference on Extending Database Technology, EDBT 2015, Brussels,\nBelgium, March 23-27, 2015, G. Alonso, F. Geerts, L. Popa, P. Barcel ´o,\nJ. Teubner, M. Ugarte, J. V . den Bussche, and J. Paredaens, Eds.\nOpenProceedings.org, 2015, pp. 481–492.\n[40] H. N. Akouemo and R. J. Povinelli, “Data improving in time series using\narx and ann models,” IEEE Transactions on Power Systems , vol. 32,\nno. 5, pp. 3352–3359, 2017.\n[41] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural\nnetworks on graphs with fast localized spectral ﬁltering,” in Advances\nin Neural Information Processing Systems 29: Annual Conference on\nNeural Information Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon,\nand R. Garnett, Eds., 2016, pp. 3837–3845.\n12\n[42] C. J. Maddison, D. Tarlow, and T. Minka, “A* sampling,” in NeurIPS,\n2014, pp. 3086–3094.\n[43] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with\ngumbel-softmax,” in 5th International Conference on Learning Repre-\nsentations. OpenReview.net, 2017.\n[44] C. Rosenbaum, T. Klinger, and M. Riemer, “Routing networks: Adaptive\nselection of non-linear functions for multi-task learning,” in 6th Inter-\nnational Conference on Learning Representations . OpenReview.net,\n2018.\n[45] Y . Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. S. Feris,\n“Spottune: Transfer learning through adaptive ﬁne-tuning,” in IEEE\nConference on Computer Vision and Pattern Recognition . Computer\nVision Foundation / IEEE, 2019, pp. 4805–4814.\n[46] F. Yu and V . Koltun, “Multi-scale context aggregation by dilated convo-\nlutions,” in 4th International Conference on Learning Representations ,\nY . Bengio and Y . LeCun, Eds., 2016.\n[47] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. E. Reed, D. Anguelov,\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper with\nconvolutions,” in IEEE Conference on Computer Vision and Pattern\nRecognition. IEEE Computer Society, 2015, pp. 1–9.\n[48] A. Raganato and J. Tiedemann, “An analysis of encoder representations\nin transformer-based machine translation,” in EMNLP. Association for\nComputational Linguistics, 2018, pp. 287–297.\n[49] E. V oita, R. Sennrich, and I. Titov, “The bottom-up evolution of\nrepresentations in the transformer: A study with machine translation\nand language modeling objectives,” in EMNLP-IJCNLP. Association\nfor Computational Linguistics, 2019, pp. 4395–4405.\n[50] A. Raganato, Y . Scherrer, and J. Tiedemann, “Fixed encoder self-\nattention patterns in transformer-based machine translation,” in EMNLP,\nT. Cohn, Y . He, and Y . Liu, Eds. Association for Computational\nLinguistics, 2020, pp. 556–568.\n[51] Y . Tay, D. Bahri, D. Metzler, D. Juan, Z. Zhao, and C. Zheng,\n“Synthesizer: Rethinking self-attention in transformer models,” CoRR,\nvol. abs/2005.00743, 2020.\n[52] Z. Wu, Z. Liu, J. Lin, Y . Lin, and S. Han, “Lite transformer with\nlong-short range attention,” in 8th International Conference on Learning\nRepresentations. OpenReview.net, 2020.\n[53] F. Wu, A. Fan, A. Baevski, Y . N. Dauphin, and M. Auli, “Pay less atten-\ntion with lightweight and dynamic convolutions,” in 7th International\nConference on Learning Representations . OpenReview.net, 2019.\n[54] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,\n“Informer: Beyond efﬁcient transformer for long sequence time-series\nforecasting,” CoRR, vol. abs/2012.07436, 2020.\n[55] A. P. Mathur and N. O. Tippenhauer, “Swat: a water treatment testbed for\nresearch and training on ICS security,” in 2016 International Workshop\non Cyber-physical Systems for Smart Water Networks . IEEE Computer\nSociety, 2016, pp. 31–36.\n[56] P. O’Neill, D. Entekhabi, E. G. Njoku, and K. H. Kellogg, “The\nNASA soil moisture active passive (SMAP) mission: Overview,” inIEEE\nInternational Geoscience & Remote Sensing Symposium . IEEE, 2010,\npp. 3236–3239.\n[57] Y . Mirsky, T. Doitshman, Y . Elovici, and A. Shabtai, “Kitsune: An\nensemble of autoencoders for online network intrusion detection,” in\n25th Annual Network and Distributed System Security Symposium . The\nInternet Society, 2018.\n[58] M. Fey and J. E. Lenssen, “Fast graph representation learning with\npytorch geometric,” CoRR, vol. abs/1903.02428, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.756437361240387
    },
    {
      "name": "Anomaly detection",
      "score": 0.6806672215461731
    },
    {
      "name": "Time series",
      "score": 0.5691145062446594
    },
    {
      "name": "Multivariate statistics",
      "score": 0.5665478706359863
    },
    {
      "name": "Transformer",
      "score": 0.48422837257385254
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.46615538001060486
    },
    {
      "name": "Graph",
      "score": 0.45944392681121826
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3917085826396942
    },
    {
      "name": "Data mining",
      "score": 0.3628115653991699
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33646029233932495
    },
    {
      "name": "Real-time computing",
      "score": 0.3234817385673523
    },
    {
      "name": "Machine learning",
      "score": 0.3123224973678589
    },
    {
      "name": "Electrical engineering",
      "score": 0.11057832837104797
    },
    {
      "name": "Engineering",
      "score": 0.08626067638397217
    },
    {
      "name": "Voltage",
      "score": 0.08587071299552917
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}