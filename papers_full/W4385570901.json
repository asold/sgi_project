{
  "title": "Self-Evolution Learning for Discriminative Language Model Pretraining",
  "url": "https://openalex.org/W4385570901",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3030359797",
      "name": "Qihuang Zhong",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2072270871",
      "name": "Liang, Ding",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2155941866",
      "name": "Juhua Liu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A1911857409",
      "name": "Bo Du",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2104129307",
      "name": "Dacheng Tao",
      "affiliations": [
        "University of Sydney"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4321472103",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4385573374",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4292948773",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3113715281",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3173844397",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3034756453",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4285252640",
    "https://openalex.org/W4385567255",
    "https://openalex.org/W4385573128",
    "https://openalex.org/W4385573252",
    "https://openalex.org/W4308828465",
    "https://openalex.org/W3176693010",
    "https://openalex.org/W4287646293",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1593045043",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W3208933101",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W4296142184",
    "https://openalex.org/W3199761064",
    "https://openalex.org/W2777662428",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2927103915",
    "https://openalex.org/W4385572498",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2146950091",
    "https://openalex.org/W4285258432",
    "https://openalex.org/W4302567632",
    "https://openalex.org/W4310825537"
  ],
  "abstract": "Masked language modeling, widely used in discriminative language model (e.g., BERT) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted. Therefore, various masking strategies (e.g., entity-level masking) are proposed, but most of them require expensive prior knowledge and generally train from scratch without reusing existing model weights. In this paper, we present Self-Evolution learning (SE), a simple and effective token masking and learning method to fully and wisely exploit the knowledge from data. SE focuses on learning the informative yet under-explored tokens and adaptively regularizes the training by introducing a novel Token-specific Label Smoothing approach. Experiments on 10 tasks show that our SE brings consistent and significant improvements (+1.43 2.12 average scores) upon different PLMs. In-depth analyses demonstrate that SE improves linguistic knowledge learning and generalization.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 4130–4145\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSelf-Evolution Learning for Discriminative Language Model Pretraining\nQihuang Zhong1∗, Liang Ding2∗, Juhua Liu3†, Bo Du1†, Dacheng Tao4\n1 National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence, School of Computer Science\nand Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, China\n2 JD Explore Academy, China 3 Research Center for Graphic Communication, Printing and Packaging,\nand Institute of Artificial Intelligence, Wuhan University, China 4 University of Sydney, Australia\n{zhongqihuang, liujuhua, dubo}@whu.edu.cn, {liangding.liam, dacheng.tao}@gmail.com\nAbstract\nMasked language modeling, widely used in\ndiscriminative language model ( e.g., BERT)\npretraining, commonly adopts a random mask-\ning strategy. However, random masking does\nnot consider the importance of the different\nwords in the sentence meaning, where some of\nthem are more worthy to be predicted. There-\nfore, various masking strategies ( e.g., entity-\nlevel masking) are proposed, but most of them\nrequire expensive prior knowledge and gener-\nally train from scratch without reusing existing\nmodel weights. In this paper, we present Self-\nEvolution learning (SE), a simple and effective\ntoken masking and learning method to fully and\nwisely exploit the knowledge from data. SE\nfocuses on learning the informative yet under-\nexplored tokens and adaptively regularizes the\ntraining by introducing a novel Token-specific\nLabel Smoothing approach. Experiments on 10\ntasks show that our SE brings consistent and\nsignificant improvements (+1.43∼2.12 average\nscores) upon different PLMs. In-depth anal-\nyses demonstrate that SE improves linguistic\nknowledge learning and generalization.\n1 Introduction\nMasked language modeling (MLM), which com-\nmonly adopts a random masking strategy to select\nthe mask tokens, has become the de-facto stan-\ndard for discriminative pretrained language models\n(PLMs) (Devlin et al., 2019; Liu et al., 2019; He\net al., 2020; Joshi et al., 2020). However, such a\nrandom masking process is usually criticized as\nbeing sub-optimal, as it allocates an equal masking\nrate for all tokens. In particular, the masked tokens\nare sometimes too easy to guess with only local\ncues or shallow patterns (Joshi et al., 2020), while\nthe informative tokens that carry more critical lin-\nguistic knowledge may be neglected (Church and\n∗ Equal contribution.\n† Corresponding Authors: Juhua Liu (e-mail: liu-\njuhua@whu.edu.cn), Bo Du (e-mail: dubo@whu.edu.cn)\nHanks, 1990; Sadeq et al., 2022). For example,\n“Bush” and “Sharon” express more important mean-\ning than “a” in the sample sentence “Bush held a\ntalk with Sharon”. MLM with predicting the above\neasy-to-guess tokens, e.g., “a”, would lead to low\ndata efficiency and sub-optimal model capability.\nTo address this problem, various methods have\nbeen carefully designed to improve MLM via fully\nleveraging the training data (Sun et al., 2019; Joshi\net al., 2020; Levine et al., 2020). The common\ngoal is to inject language prior knowledge into the\npretraining process (Cui et al., 2022; Ding et al.,\n2021). Although empirically successful, there are\nstill some limitations. First, they usually require\nannotation derived from off-the-shelf tools to select\nmask tokens, which is not only expensive but also\ntoo deterministic1, and may cause error propaga-\ntion from the third-party tool. For instance, Sun\net al. (2019) employ external linguistic tools, e.g.,\nStanford CoreNLP (Manning et al., 2014), to an-\nnotate the entities. Second, to ensure the effective-\nness of the masking strategy, most previous works\ntrain PLM from scratch without reusing the exist-\ning models trained with vanilla MLM (Sun et al.,\n2019; Joshi et al., 2020; Levine et al., 2020; Sadeq\net al., 2022), which is wasteful and inefficient.\nThus, there raises a question: whether we can\nstrengthen the PLM capability and data efficiency\nthrough further learning from the informative yet\nunder-explored tokens, where such tokens are de-\ntermined by the existing PLM itself. In fact, an\noff-the-shelf PLM already has the ability to deter-\nmine the worthy and informative tokens that should\nbe further exploited, as the representation of PLM\ngenerally can reveal good enough linguistic prop-\nerties (Hewitt and Manning, 2019; Swayamdipta\net al., 2020). For example, tokens that PLMs pre-\ndict incorrect or low confidence are usually more\n1The once-for-all prior is not suitable for different PLMs,\ne.g., under-explored word for BERT may already well-\nmastered by RoBERTa.\n4130\nhard-to-learn and challenging, which are essen-\ntial for further training. Also, the conjecture to\nimprove the off-the-shelf PLM is model-agnostic,\ngreen, and efficient, thus having the great potential\nto evolve any existing discriminative PLMs.\nMotivated by this, we design a simple and ef-\nfective Self-Evolution learning (SE) mechanism to\nimprove the pretraining of discriminative PLMs.\nSpecifically, the SE contains two stages: ❶self-\nquestioning and ❷self-evolution training . In\nstage 1, the PLM is forced to locate the informa-\ntive but under-explored tokens2 from the pretrain-\ning data. After locating these hard-to-learn tokens,\nwe then encourage the PLM to learn from them\nin stage 2, where we basically follow the vanilla\nMLM to mask these tokens and then optimize the\nPLM by minimizing the loss between the predic-\ntions and one-hot labels. It should be noted that due\nto the hard-to-learn properties, directly enforcing\nthe PLM to fit the hard labels may lead to overfit-\nting or overconfidence problem (Miao et al., 2021).\nInspired by the label smoothing (LS) (Szegedy\net al., 2016) that regularizes the learning by smooth-\ning target labels with a pre-defined (static) prior\ndistribution, we propose a novel Token-specific La-\nbel Smoothing (TLS) approach. Our TLS consid-\ners both the precise hard label and, importantly,\nthe easily-digestible3 distribution that is adaptively\ngenerated by the PLM itself.\nWe validated our SE on several benchmarks\nincluding GLUE (Wang et al., 2018), Super-\nGLUE (Wang et al., 2019), SQuAD2.0 (Ra-\njpurkar et al., 2018), SWAG (Zellers et al., 2018)\nand LAMA (Petroni et al., 2019) over several\nPLMs: BRET (Devlin et al., 2019)-BASE , -LARGE ,\nRoBERTa (Liu et al., 2019)-BASE , and -LARGE .\nExperiments demonstrate the effectiveness and uni-\nversality of our approach. Extensive analyses con-\nfirm that SE effectively enhances the ability of\nPLMs on linguistic knowledge learning, model gen-\neralization and robustness.\nContributions Our main contributions are:\n• We propose SE to strengthen the MLM-based\nPLMs, where our mechanism does not require\nexternal tools and enjoys a simple recipe: con-\ntinue pretraining with SE.\n2We refer to those hard-to-learn tokens that are not learned\nwell by PLMs as the informative but under-explored tokens.\n3Analogous to human learning behavior, it is often easier\nfor humans to grasp new things described by their familiar\nknowledge (Reder et al., 2016).\n• We design a novel token-specific label smooth-\ning approach for regularization, which adopts\nthe token-specific knowledge-intensive distri-\nbutions to adaptively smooth the target labels.\n• Extensive experiments show that our SE\ncould significantly and robustly evolve a se-\nries of backbone PLMs, up to +2.36 aver-\nage score improvement on GLUE benchmark\nupon RoBERTa.\n2 Related Works\nIn recent years, we have witnessed numerous dis-\ncriminative PLMs (Devlin et al., 2019; Liu et al.,\n2019; He et al., 2020; Sun et al., 2019; Joshi et al.,\n2020) that achieved tremendous success in vari-\nous natural language understanding (NLU) tasks.\nAlthough the discriminative PLMs vary in terms\nof pretraining data or model architecture, they are\ncommonly based on MLM loss function. MLM\nmechanism is pioneered in BERT (Devlin et al.,\n2019) that uses a random masking strategy to mask\nsome tokens, and then enforces the PLM to learn\nto recover word information from the masked to-\nkens. Obviously, the vanilla MLM is a linguistic-\nagnostic task, as the random masking procedure\ndoes not integrate linguistic knowledge explicitly,\nwhich is sub-optimal. Thus, several previous stud-\nies attempt to improve MLM by exploring a diverse\nof linguistically-motivated masking strategies, such\nas entity-level masking (Sun et al., 2019), span-\nlevel masking (Joshi et al., 2020), N-grams mask-\ning (Levine et al., 2020), etc., to fully leverage the\npretraining data.\nAlthough achieving remarkable performance,\nthese strategies still have some limitations. First,\ntheir implementations are relatively complex, as\nthey usually require annotation derived from exter-\nnal models or tools to select tokens for masking.\nEven for the unsupervised PMI-masking (Sadeq\net al., 2022), it is still expensive to measure the\npointwise mutual information for pretrain-level\nlarge-scale data, and the annotated labels are static,\nwhile our SE could obtain dynamic annotations\nvia given existing PLMs. Second, in order to en-\nsure the effectiveness of masking strategy, most\nprevious works (Sun et al., 2019; Joshi et al., 2020;\nLevine et al., 2020; Sadeq et al., 2022) train the\nlanguage models from scratch without reusing the\nexisting PLMs trained with vanilla MLM, which is\nwasteful and inefficient.\n4131\nFigure 1: Overview of the proposed SE mechanism, which contains two stages: ❶ using an existing PLM to locate\nthe informative yet under-explored tokens and ❷ encouraging the PLM to robustly learn from these tokens via a\ntoken-specific label smoothing approach.\nAlong the same research line, in this paper, we\nimprove the MLM-based PLMs with a novel self-\nevolution learning mechanism. Instead of training\na PLM from scratch based on a carefully-designed\nand complex masking strategy, our mechanism\naims to strengthen the PLM’s capability and data\nefficiency by further learning from the informative\nyet under-explored tokens, which are determined\nby the existing PLM itself.\n3 Methodology\n3.1 Preliminary\nGiven a sentenceS = {t1,t2,...,t n}with ntokens,\nMLM first randomly selects some percentage of the\ninput tokens and replaces them with a special mask\nsymbol [MASK]. Suppose that there are mmasked\ntokens and {k1,k2,...,k m}is the set of masked\npositions, we can denote the masked tokens as\nM = {tk1 ,tk2 ,...,t km }. Let S′denote the masked\nsentence, we can feed S′into the model and obtain\nthe last hidden layer representations as H ∈Rn×d\n(d is the hidden size), and a subset of represen-\ntations w.r.t masked positions as Hm ∈ Rm×d.\nSubsequently, the input word embedding matrix\nE ∈RV ×d (V is the vocabulary size) is used to\nproject the hidden representations into vocabulary\nspace. Lastly, we can get the normalized prediction\nprobabilities for each masked token as:\npi = softmax(Hm\ni ET + b), (1)\nwhere pi ∈ RV and i ∈ {1,2,...,m }. Finally,\ngiven the one-hot labels yi, we use the cross-\nentropy loss to optimize the MLM task:\nLMLM = −1\nm\nm∑\ni=1\nyi log pi (2)\n3.2 Self-Evolution Learning for PLMs\nIn this part, we introduce our SE mechanism in de-\ntail. At its core, SE is to enforce the existing PLM\nto further learn from the informative yet under-\nexplored tokens, which are wisely determined by\nthe PLM itself. Figure 1 illustrates the process\nof SE mechanism, which contains two stages: (1)\nself-questioning and (2) self-evolution training.\n❶ Self-questioning Stage. The goal of this stage\nis to select the informative yet under-explored to-\nkens, i.e., these hard-to-learn tokens that the PLMs\ndo not learn well during the previous pretraining.\nHowever, how to select these target tokens? In-\nspired by the finding of the representations of the\noff-the-shelf PLM on individual tokens can reveal\ngood enough linguistic properties (Hewitt and Man-\nning, 2019; Swayamdipta et al., 2020), we hereby\npropose to straightforwardly leverage the behavior\nof PLMs to wisely select target tokens in this stage.\nSpecifically, we mainly focus on two important\nproperties, i.e., correctness (accuracy) and confi-\ndence (the probability output that the model assigns\nto the prediction), as the tokens that PLMs pre-\ndict incorrect or low confidence are usually more\nhard-to-learn and worthy for further exploring (Guo\net al., 2017; Park and Caragea, 2022). Based on\nthe above two properties, we introduce two simple\nmetrics to estimate the learning value of tokens:\n4132\nCorrectness-based metric. In practice, we first\nfeed the original sentence Sinto the existing frozen\nPLM and enforce it to output the prediction proba-\nbilities pi (i∈{1,2,...,n }) for each token. Given\nthe one-hot labels yi (i∈{1,2,...,n }), we calcu-\nlate the cross-entropy loss ( i.e., correctness) for\neach token position (denoted as {l1,l2,...,l n}).\nThen, we set a loss threshold Tl and select the\ntokens that exceed Tl as the target tokens, i.e.,\nM = {ti|li >Tl}where i∈{1,2,...,n }.\nConfidence-based metric. Similarly, we can\nmeasure the confidence of tokens and use it as the\nmetric. Different from the above process, in this\nmetric, we compute the entropy of pi as the confi-\ndence for each token (denoted as {e1,e2,...,e n}).\nIntuitively, the tokens with high entropy value are\nhard-to-learn, as the PLM predict them with low\nconfidence towards the gold labels. Also, an en-\ntropy threshold Te is used to select the target tokens,\ni.e., M = {ti|ei >Te}4.\n❷ Self-evolution Training Stage. After estimat-\ning these hard-to-learn tokens, we can then choose\nthem for masking and encourage the PLM to learn\nfrom them. Intuitively, we can follow the vanilla\nMLM process to optimize the PLM by minimizing\nthe loss between the predictions and one-hot la-\nbels, as implemented in Eq. 2. However, due to the\nhard-to-learn properties of these tokens, directly en-\nforcing the PLM to fit the hard labels may lead to\noverfitting or overconfidence problem (Miao et al.,\n2021; Li et al., 2022). To tackle this issue, in this\nstage, inspired by the label smoothing (LS) reg-\nularization approach (Szegedy et al., 2016), we\nfurther propose a noveltoken-specific label smooth-\ning (TLS) approach to adaptively regularize the\ntraining and improve the generalization of PLMs.\nMathematically, in LS approach, it minimizes\nthe cross-entropy between modified label distribu-\ntion y′\ni and the model output pi, where y′\ni is the\nsmoothed label distribution formulated as:\ny′\ni = (1 −λ) ∗yi + λ∗ui, (3)\nwhere ui is a fixed distribution that is usually a\nuniform distribution, and λis a weighting factor.\nFurthermore, following Yuan et al. (2020), we re-\nformulate the loss function of LS as:\nLLS = (1 −λ) ∗H(y,p) + λ∗Dkl(u,p), (4)\n4In practice, Tl and Te are empirically set as 0.1 and 1, re-\nspectively. The detailed analyses are shown in Appendix A.4.\nwhere H denotes the ordinary cross-entropy loss\nand Dkl denotes the KL divergence loss. We can re-\ngard Dkl(u,p) as a knowledge distillation process,\nwhere ucorresponds to a virtual teacher to guide\nthe student model (i.e., the PLM). Obviously, it is\nsub-optimal as uhardly provides enough linguistic\ninformation to guide the training of PLM.\nMotivated by this, in our TLS, we design a more\ninformative prior distribution to smooth the labels.\nSpecifically, inspired by human learning behavior\n(it is often easier for humans to grasp new things\ndescribed by their familiar knowledge (Reder et al.,\n2016)), we improve the Dkl supervision with a\nmore easily-digestible and informative distribution\nthat is adaptively generated by the PLM itself. In\nother words, Dkl can be recast as a self-distillation\nprocess, where the virtual teacher distribution is\nacquired from the student model itself. In practice,\nfor each masked position ki, in addition to the pre-\ndiction probabilities pi on the corrupted S′, we also\nfeed the original sentence Sinto the current PLM\nand regard the corresponding probabilities as the\nreference probabilities ri5. Then, similar to Eq. 3,\nwe can obtain the smoothed label ˜yi via:\n˜yi = (1 −λ) ∗yi + λ∗ri (5)\nLastly, we use the cross-entropy as the loss function\nin the SE training stage, as follows:\nLSE = −1\nm\nm∑\ni=1\n˜yi log pi (6)\n4 Experiments\n4.1 Tasks and Datasets\nWe follow many previous studies (Zhong et al.,\n2022a,c, 2023a,b) and conduct extensive experi-\nments on various NLU tasks, including a diversity\nof tasks from GLUE (Wang et al., 2018) and Su-\nperGLUE (Wang et al., 2019) benchmarks, i.e.,\nlinguistic acceptability (CoLA), natural language\ninference (RTE, CB), paraphrase (MRPC), ques-\ntion answering (BoolQ), word sense disambigua-\ntion (WiC) and causal reasoning (COPA). Addition-\nally, we also evaluate on three knowledge-intense\ntasks, which require the ability of commonsense\nknowledge reasoning, i.e., SQuAD2.0 (Rajpurkar\net al., 2018), SWAG (Zellers et al., 2018) and\n5It is noteworthy that, although the ri may not be much\nclose to the ground-truths, the linguistic information contained\nin ri is potentially beneficial for further learning.\n4133\nCoLA MRPC RTE BoolQ CB WiC COPA Score\nMethod Mcc. Acc. Acc. Acc. Acc. Acc. Acc. Avg. ∆ (↑)\nPerformance of Different Masking Strategies\nBERTbase 62.33 88.97 76.89 75.05 85.71 66.77 63.00 74.10 –\n-w/ Entity-level masking 60.06 88.73 76.53 74.77 87.50 66.61 65.00 74.17 +0.07\n-w/ Span-level masking 61.41 88.48 78.34 74.28 87.50 67.40 65.00 74.63 +0.53\n-w/ PMI-based masking 61.09 88.24 76.90 74.25 87.50 66.61 65.00 74.23 +0.13\n-w/ Self-questioning 63.78 87.99 78.34 74.13 85.71 67.87 66.00 74.83 +0.73\nBERT-SEbase 63.63 89.50 77.98 74.37 89.29 67.40 66.00 75.45 +1.35\nPerformance upon More Discriminative PLMs\nBERTlarge 63.00 87.25 83.80 78.40 91.07 67.24 72.00 77.54 –\nBERT-SElarge 65.66 88.23 85.20 80.18 92.86 68.34 78.00 79.78 +2.24\nRoBERTabase 62.00 90.20 83.12 78.72 83.93 69.12 70.00 76.72 –\nRoBERTa-SEbase 62.11 89.71 84.12 79.39 92.86 71.40 74.00 79.08 +2.36\nRoBERTalarge 64.73 90.69 88.44 84.37 91.07 69.90 78.00 81.03 –\nRoBERTa-SElarge 67.80 91.91 90.25 84.56 96.40 70.53 80.00 83.06 +2.03\nTable 1: Comparison between our SE and the vanilla method applied to all PLMs on the combination of GLUE\nand SuperGLUE benchmarks. Average scores on all tasks are underlined. The best results are given in bold. “∆”\ndenotes the improvement of SE methods compared to the baseline PLMs.\nLAMA (Petroni et al., 2019). In practice, we re-\nport the performance with Accuracy (“Acc.”) met-\nric for most tasks, except the Matthew correla-\ntion (“Mcc.”) for CoLA, the F1 and Exact Match\n(“EM”) scores for SQuAD2.0, and the Mean Recip-\nrocal Rank (“MRR”) scores for LAMA. We report\nthe averaged results over 10 random seeds to avoid\nstochasticity. The details of all tasks and datasets\nare provided in Appendix A.1.\n4.2 Implementation Details\nPre-training. We employ the representative\nBRET (Devlin et al., 2019)- BASE , - LARGE ,\nRoBERTa (Liu et al., 2019)-BASE , and -LARGE as\nthe backbone discriminative PLMs, and implement\nour methods in a continued pretraining manner.\nFor pretraining settings, we follow the original pa-\npers (Devlin et al., 2019; Liu et al., 2019) and use\nthe same pretraining corpus and (most of) hyper-\nparameters6 (e.g., batch size and the maximum\nlength of the input sentence), respectively.\nEspecially, as suggested by Liu et al. (2019), we\ndo not use the next sentence prediction (NSP) ob-\njective during BERT pretraining. For our methods,\nwe continue pretraining the backbone PLMs with\n2 epochs. Additionally, for reference, we train the\nPLMs with the vanilla MLM for the same steps and\nrefer to them as the baselines.\n6Notably, for the continued pretraining process, we use\n1/10 of the learning rate in the original paper as the initial one.\nFine-tuning. The learning rate is selected in\n{1e-5, 2e-5, 3e-5, 5e-5}, while the batch size is\nin {12, 16, 32} depending on tasks. The max-\nimum length of the input sentence is 384 for\nSQuAD2.0 and 256/512 for other tasks. We use\nAdamW (Loshchilov and Hutter, 2018) as the op-\ntimizer, and set the β2 and weight decay as 0.98\nand 0.01, respectively. All experiments are con-\nducted on NVIDIA A100 GPUs. The detailed\nhyper-parameters are provided in Appendix A.2.\nCompared Methods. For references, we com-\npare our SE method with other cutting-edge coun-\nterparts. Specifically, taking the BERTbase as the\nbaseline, we use the following masking strategies\nto further improve its performance:\n• Entity-level masking: following Sun et al.\n(2019), we mask the named entities in the sen-\ntence and enforce the model to predict them.\n• Span-level masking: as done in (Joshi et al.,\n2020), we randomly select spans from the sen-\ntence based on a geometric distribution and\nmask the selected span.\n• PMI-based masking: similar to (Sadeq et al.,\n2022), we use PMI to identify a set of contigu-\nous (informative) N-grams and mask them.\n• Self-questioning masking 7: We adopt our\n7The main difference from our full SE is that it does not\n4134\nMethod SQuAD2.0 SW AG Avg.\nEM F1 Acc.\nBERTbase 72.18 75.07 77.53 74.93\nBERT-SEbase 72.89 75.64 77.91 75.48\nBERTlarge 81.35 84.38 83.40 83.04\nBERT-SElarge 81.94 85.00 83.61 83.52\nRoBERTabase 78.79 81.92 79.69 80.13\nRoBERTa-SEbase 79.41 82.55 79.88 80.61\nRoBERTalarge 84.70 87.65 84.34 85.56\nRoBERTa-SElarge 85.03 87.93 84.54 85.83\nTable 2: Performance on SQuAD2.0 (Rajpurkar et al.,\n2018) and SW AG (Zellers et al., 2018) dev sets.\nMethod Google-RE (LAMA) Avg.\ndate-birth place-birth place-death\nRoBERTabase 5.51 11.52 2.68 6.57\nRoBERTa-SEbase 6.35 15.16 9.61 10.37\nTable 3: Performance of our SE on LAMA (Petroni\net al., 2019) to probe the factual knowledge.\nstage 1 to select the hard-to-learn tokens and\ndirectly follow the vanilla MLM to mask them\nand predict the one-hot labels.\nNotably, for a fair comparison, we implement all\nthese methods in a continual pretraining manner,\nsame to the settings of our SE.\n4.3 Main Results\nSE surpasses the previous carefully-designed\nmasking strategies. Results on GLUE and Su-\nperGLUE benchmarks are shown in Table 1. Com-\npared with the baseline BERT base, all masking\nstrategies bring the average performance gains,\nproving the necessity of improving MLM. Among\nall these methods, our proposed self-questioning\nmasking achieves the relatively better performance\non many tasks, confirming the effectiveness of\nusing the PLMs themselves to select the hard-to-\nlearn tokens. More encouragingly, with the help of\nself-evolution training, our final BERT-SEbase can\nachieve further performance improvements. These\nresults can prove the superiority of our SE.\nSE brings consistent and significant perfor-\nmance improvements among all PLMs. In ad-\ndition to the results upon BERTbase, we also apply\nour method on more discriminative PLMs and re-\nport the results in Table 1. Compared with the base-\nlines, SE brings consistent and significant perfor-\ninvolve the self-evolution training process of stage 2.\nFigure 2: Parameter analysis of λon BERT-SElarge.\nmance improvements across all BERT/RoBERTa\nmodel sizes. Specifically, for Base and Large\nRoBERTa models, SE brings 2.36% and 2.03%\nrelative gains in overall score respectively. Also,\nthe gain for BERT is up to 2.24%. These results\nprove the effectiveness and universality of ourSE.\nSE enhances the ability of knowledge learning.\nFor the knowledge-intense tasks, i.e., SQuAD2.0\nand SW AG, we report the results in Table 2. With\nthe help of SE, all PLMs consistently achieve bet-\nter performance. Specifically, the performance im-\nprovements on SQuAD2.0 in terms of EM and F1\nare up to 0.71% and 0.64%, respectively. Besides\nQA tasks that require to be fine-tuned, we conduct\nexperiments on a widely-used factual knowledge\nprobing task, i.e., LAMA (Petroni et al., 2019), to\nverify whether SE improves the ability of PLMs on\ncommonsense knowledge. We report the results in\nTable 3. Based on the powerful RoBERTa,SE still\nbrings significant improvements, i.e. +3.8 average\nscore, to the knowledge-learning ability of PLMs.\n4.4 Ablation Study\nWe evaluate the impact of each component of our\nSE, including i) token-selecting metrics, ii) token-\nspecific label smoothing approach, iii) coefficient\nλ, and iv) more SE iterations.\nImpact of Token Selecting Metrics. As men-\ntioned in §3.2, we introduce several metrics to se-\nlect the hard-to-learn tokens in the self-questioning\nstage. Here, we conduct experiments to analyze\nthe impact of different metrics. Specifically, for ref-\nerence, we compare the “Correctness-based” and\n“Confidence-based” metrics8 with a simple alter-\nnative, i.e., “randomly selecting”. Results in Ta-\n8Our preliminary study shows the non-complementarity be-\ntween two token-selecting metrics, we compute their vocabu-\nlary distribution difference and give evidence at Appendix A.5.\n4135\nMethod BERT base BERTlarge RoBERTabase RoBERTalarge Avg.\nBaseline 74.10 77.54 76.73 81.03 77.35\nSelecting metrics inself-questioningstage\n-w/ randomly selecting 73.85 (-0.25) 78.28 (+0.74) 77.09 (+0.36) 81.64 (+0.61) 77.72 (+0.37)\n-w/ Correctness-based 75.45 (+1.35) 79.78 (+2.24) 79.08 (+2.35) 83.06 (+2.03) 79.34 (+1.99)\n-w/ Confidence-based 75.77 (+1.67) 78.88 (+1.34) 77.86 (+1.13) 82.46 (+1.43) 78.74 (+1.39)\nTable 4: Ablation study of different metrics used to select the hard-to-learn tokens in SE, evaluated on the\ncombination of GLUE and SuperGLUE benchmarks. For simplicity, we show the overall score here. The full results\nand analyses about the superiority of the correctness-based metric can be found in Appendix (Table 11&10).\nMethod GLUE/SGLUE SQuAD/SW AG\nAvg. (∆↑) Avg. ( ∆↑)\nRoBERTabase 76.73 80.13\nRoBERTa-SEbase\n-w/ vanilla LS 78.37 (+1.64) 80.37 (+0.24)\n-w/ TLS (Ours)79.08 (+2.35) 80.61 (+0.48)\nTable 5: Ablation study of our TLS approach. “-w/\nvanilla LS” and “-w/ TLS (Ours)” refer to using the\nvanilla and our proposed token-specific label smoothing\napproaches in SE mechanism, respectively. Full results\nare shown in Appendix (Table 12).\nble 4 show that 1) although the “randomly se-\nlecting” performs worst, it still outperforms the\ncontinually trained baseline, showing the effec-\ntiveness of the self-evolution training. 2) both\nour proposed metrics “Correctness-based” and\n“Confidence-based” achieve significantly better per-\nformance, confirming our claim that learning on in-\nformative yet under-explored tokens can strengthen\nthe capability of PLMs and data efficiency. No-\ntably, the correctness-based metric outperforms the\nconfidence-based metric in most cases, thus leaving\nas our default setting in SE.\nImpact of Token-specific Label Smoothing. A\nkey technology in our SE is the TLS, which uses\nthe token-specific smoothed label to adaptively\nguide training. To verify its effectiveness, we con-\nduct experiments and present the results in Table 5.\nWe show that 1) the vanilla label smoothing ap-\nproach equipped SE could easily outperform the\ncontinuously trained backbone, showing the superi-\nority of our SE framework, and importantly, 2) our\nTLS could further improve the results by a large\nmargin against vanilla LS equipped SE, e.g. aver-\naging +0.71, indicating the effectiveness of TLS.\nImpact of Coefficient λ. The factor λin Eq. 5,\nwhich is used to control the ratio of label smooth-\ning, is an important hyper-parameters. In this study,\nGLUE N = 1 N = 2 N = 3\nCoLA 63.63 63.59 63.60\nMRPC 89.50 88.23 88.97\nRTE 77.98 79.42 78.70\nAvg. (∆ ↑) +0.97 +1.02 +1.03\nTable 6: Performance for different iterations N on\nBERT-SEbase. “ Avg. (∆ ↑)” indicates the relative im-\nprovement against the vanilla BERTbase.\nFigure 3: Analysis of task generalization. The model\nis fine-tuned on the QNLI task and transferred to four\ndifferent tasks. We can see that SE consistently brings\nbetter generalization compared with its counterparts.\nwe analyze its influence by evaluating the perfor-\nmance with different λspanning {0.1, 0.3, 0.5, 0.7,\n0.9} on several GLUE tasks. Figure 2 illustrates the\naverage results. Compared with the baseline, our\nSE consistently brings improvements across all ra-\ntios of λ, basically indicating that the performance\nof SE is not sensitive to λ. More specifically, the\ncase of λ= 0.1 performs best, and we thereby use\nthis setting in our experiments.\nImpact of MoreSE Iterations. Researchers may\ndoubt whether SE can be further augmented by\nperforming the self-questioning and token-specific\nlabel smoothing with already evolved PLMs that\nown better representations. That is, whether more\n4136\nFigure 4: The 3D loss surface comparison between baseline, SE (“-w/ vanilla LS”) and SE (“-w/ TLS”) methods\napplied to RoBERTabase. Note that the PLMs are fine-tuned on the CoLA task.\nFigure 5: 1D visualization of loss landscapes of\nRoBERTabase models fine-tuned on different tasks.\niterations (denoted as “ N”) further enhance SE?\nTo answer this question, we continuously train the\nPLMs with more SE iterations and report the per-\nformance of several GLUE tasks in Table 6. As\nseen, increasing the iterations improves the perfor-\nmance but the gain margin is insignificant. Given\nthat increasing N costs more, we suggest using SE\nfor only one iteration to achieve a better trade-off\nbetween costs and performance.\n5 Discussion\nTo better understandSE, we conduct extensive anal-\nyses to discuss whether it gains better generaliza-\ntion/ robustness and knowledge-learning ability.\n5.1 Does SE Bring Better Generalization?\nWe examine from two perspectives: i) measuring\nthe cross-task zero-shot performance, and ii) visu-\nalizing the loss landscapes of PLMs.\nTask Generalization. The performance of out-\nof-domain (OOD) data is widely used to verify\nthe model generalization (Wang et al., 2022; Ding\net al., 2022). Thus, we follow Xu et al. (2021);\nZhong et al. (2022b) and evaluate the performance\nof PLMs on several OOD data. In practice, we first\nfine-tune RoBERTabase models trained with differ-\nent methods (including “Baseline”, “SE (-w/ LS)”,\nand “SE (-w/ TLS)”) on the QNLI task, and then in-\nference on other tasks, i.e., CoLA, MRPC, STS-B,\nand RTE. The results are illustrated in Figure 3. We\nobserve that “SE (-w/ TLS)” consistently outper-\nforms the other counterparts. To be more specific,\ncompared with baseline, our SE brings a +2.90 av-\nerage improvement score on these tasks, indicating\nthat our SE boosts the performance of PLMs on\nOOD data.\nVisualization of Landscape. To have a close\nlook, we visualize the loss landscapes of differ-\nent RoBERTabase models fine-tuned on the CoLA\ntask. In practice, we first show the 3D loss surface\nresults in Figure 4 following the “filter normalized”\nsetting in (Li et al., 2018; Zan et al., 2022). As seen,\nSE-equipped PLMs show flatter smoother surfaces\ncompared with the vanilla. To closely compare the\ndifferences of “SE (-w/ LS)” and “SE (-w/ TLS)”\nin the loss landscape, we follow He et al. (2021) to\nplot the 1D loss curve on more tasks in Figure 5.\nWe find that through detailed 1D visualization, our\noptimal setting “SE (-w/ TLS)” shows a flatter and\noptimal property. These results prove that SE can\nsmooth the loss landscape and improve the gener-\nalization of PLMs effectively.\n5.2 Cloze Test\nTo verify whether SE enforces the PLMs to learn\nfrom the informative tokens, we follow Sun et al.\n(2019) and apply the Cloze test (Taylor, 1953) to\nevaluate the knowledge learning ability of PLMs.\nFor each test sample, we first remove the informa-\ntive token and then enforce the PLMs to infer what\nit is. Some cases are shown in Figure 6.\nIn case 1 and case 2, both BERTbase and BERT-\nSEbase can successfully predict the type of masked\ntokens according to the contexts. However, with\nthe help of the SE mechanism, BERT-SEbase per-\nforms more correctly on filling in the slot. Dra-\n4137\nFigure 6: Cloze test comparison between BERTbase and BERT-SEbase. The correct predictions are in bold.\nmatically, in case 3, the baseline BERTbase makes\nunreasonable predictions. One possible reason is\nthat the baseline PLM only learns the shallow pat-\ntern and fails to understand the meaning of the con-\ntext. Additionally, due to the unsatisfactory ability\nof the baseline PLM on commonsense reasoning,\nthe baseline PLM also predicts strangely in case\n4. Different from the baseline, while BERT-SEbase\ndoes not predict the completely correct tokens in\ncase 3 and case 4, it can capture deep patterns\nand make more reasonable predictions. In gen-\neral, these cases prove that SE indeed improves\nthe knowledge-learning ability of PLMs.\n☞ More analyses in Appendix In addition\nto the above discussions, we conduct more related\nanalyses and show them in Appendix, e.g., param-\neter analyses on Tl and Te (Appendix A.4), ro-\nbustness analysis based on the empirical results\non AdvGLUE (Wang et al., 2021) (Appendix A.3),\nand non-complementarity analysis between token-\nselecting metrics (Appendix A.5). Please refer to\nAppendix for more details.\n6 Conclusion\nIn this paper, we propose a simple and effective self-\nevolution (SE) learning mechanism to improve the\nexisting discriminative PLMs by fully exploiting\nthe knowledge from data. SE follows two stages,\ni.e., self-questioning and self-evolution training,\nand can be used to evolve any MLM-based PLMs\nwith a simple recipe: continue pretraining with\nSE. We empirically demonstrated the effectiveness\nand universality of the SE on a series of widely-\nused benchmarks. Further analyses show our ap-\nproach improves the generalization, robustness,\nand knowledge-learning ability. We hope our work\ncould facilitate more research on how to improve\nexisting trained models after all the previous PLM\nweights are expensive and knowledgeable.\nLimitations\nOur work has several potential limitations. First,\ngiven the limited computational budget, we only\nvalidate our self-evolution learning on the Large\nand Base sizes. It will make our work more con-\nvincing if scaling the experiments up to the larger\nmodel size and training corpus. On the other hand,\nbesides the improved commonsense knowledge\nlearning ability, we believe that there are still other\nabilities, e.g., mathematical word problems, of\nPLMs that can be improved by our method, which\nare not fully explored in this work.\nEthics and Reproducibility Statements\nEthics We take ethical considerations very seri-\nously, and strictly adhere to the ACL Ethics Policy.\nThis paper focuses on higher data and model effi-\nciency for discriminative pretrained language mod-\nels, but not capturing the privacy knowledge. Both\nthe pretraining datasets and models used in this\npaper are publicly available and have been widely\nadopted by researchers. Therefore, we believe that\nthis research will not pose ethical issues.\nReproducibility We will publicly release\nour code in https://github.com/WHU-ZQH/\nSE4PLMs to help reproduce the experimental\nresults of this paper.\nAcknowledgements\nWe are grateful to the anonymous reviewers and the\narea chair for their insightful comments and sug-\ngestions. This work was supported in part by the\nNational Natural Science Foundation of China un-\nder Grants 62225113 and 62076186, and in part by\nthe Science and Technology Major Project of Hubei\nProvince (Next-Generation AI Technologies) under\nGrant 2019AEA170. The numerical calculations in\nthis paper have been done on the supercomputing\nsystem in the Supercomputing Center of Wuhan\nUniversity.\n4138\nReferences\nKenneth Church and Patrick Hanks. 1990. Word associ-\nation norms, mutual information, and lexicography.\nComputational linguistics.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL.\nYiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu.\n2022. Lert: A linguistically-motivated pre-trained\nlanguage model. arXiv.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Investi-\ngating projection in naturally occurring discourse. In\nproceedings of Sinn und Bedeutung.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL.\nLiang Ding, Longyue Wang, Xuebo Liu, Derek F\nWong, Dacheng Tao, and Zhaopeng Tu. 2021. Un-\nderstanding and improving lexical choice in non-\nautoregressive translation. In ICLR.\nLiang Ding, Longyue Wang, Shuming Shi, Dacheng\nTao, and Zhaopeng Tu. 2022. Redistributing low-\nfrequency words: Making the most of monolingual\ndata in non-autoregressive translation. In ACL.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third International Workshop on Paraphrasing\n(IWP2005).\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nWilliam B Dolan. 2007. The third pascal recognizing\ntextual entailment challenge. In ACL.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In ICML.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. In ICLR.\nRuidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng\nDing, Liying Cheng, Jiawei Low, Lidong Bing, and\nLuo Si. 2021. On the effectiveness of adapter-based\ntuning for pretrained language model adaptation. In\nACL.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In NAACL.\nLan Jiang, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou,\nand Rui Jiang. 2022. Rose: Robust selective fine-\ntuning for pre-trained language models. arXiv.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. TACL.\nSolomon Kullback and Richard A Leibler. 1951. On\ninformation and sufficiency. The annals of mathe-\nmatical statistics.\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend,\nKevin Leyton-Brown, Moshe Tennenholtz, and Yoav\nShoham. 2020. Pmi-masking: Principled masking of\ncorrelated spans. In ICLR.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and\nTom Goldstein. 2018. Visualizing the loss landscape\nof neural nets. In NeurIPS.\nShaobo Li, Xiaoguang Li, Lifeng Shang, Chengjie Sun,\nBingquan Liu, Zhenzhou Ji, Xin Jiang, and Qun Liu.\n2022. Pre-training language models with determinis-\ntic factual knowledge. EMNLP.\nJianhua Lin. 1991. Divergence measures based on the\nshannon entropy. IEEE Transactions on Information\ntheory.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In ICLR.\nChristopher D Manning, Mihai Surdeanu, John Bauer,\nJenny Rose Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The stanford corenlp natural language\nprocessing toolkit. In ACL.\nMengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua\nZhou, and Jie Zhou. 2021. Prevent the language\nmodel from being overconfident in neural machine\ntranslation. In ACL.\nSeo Yeon Park and Cornelia Caragea. 2022. On the cal-\nibration of pre-trained language models using mixup\nguided by area under the margin and saliency. In\nACL.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In EMNLP.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. Wic: the word-in-context dataset for evaluat-\ning context-sensitive meaning representations. In\nNAACL.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\nfor squad. In ACL.\n4139\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nLynne M Reder, Xiaonan L Liu, Alexander Keinath,\nand Vencislav Popov. 2016. Building knowledge\nrequires bricks, not sand: The critical role of familiar\nconstituents in learning. Psychonomic bulletin &\nreview, 23(1):271–277.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In AAAI.\nNafis Sadeq, Canwen Xu, and Julian McAuley. 2022.\nInformask: Unsupervised informative masking for\nlanguage model pretraining. arXiv.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nEMNLP.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nCVPR.\nWilson L Taylor. 1953. “cloze procedure”: A new tool\nfor measuring readability. Journalism quarterly.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In EMNLP.\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan,\nYu Cheng, Jianfeng Gao, Ahmed Hassan Awadal-\nlah, and Bo Li. 2021. Adversarial glue: A multi-\ntask benchmark for robustness evaluation of language\nmodels. In NeurIPS Datasets and Benchmarks Track.\nWenxuan Wang, Wenxiang Jiao, Yongchang Hao, Xing\nWang, Shuming Shi, Zhaopeng Tu, and Michael R.\nLyu. 2022. Understanding and improving sequence-\nto-sequence pretraining for neural machine transla-\ntion. In ACL.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTACL.\nRunxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,\nBaobao Chang, Songfang Huang, and Fei Huang.\n2021. Raise a child in large language model: To-\nwards effective and generalizable fine-tuning. In\nEMNLP.\nLi Yuan, Francis EH Tay, Guilin Li, Tao Wang, and\nJiashi Feng. 2020. Revisiting knowledge distillation\nvia label smoothing regularization. In CVPR.\nChangtong Zan, Liang Ding, Li Shen, Yu Cao, Weifeng\nLiu, and Dacheng Tao. 2022. On the complemen-\ntarity between pre-training and random-initialization\nfor resource-rich machine translation. In COLING.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. SW AG: A large-scale adversarial dataset\nfor grounded commonsense inference. In EMNLP.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2022a. Panda: Prompt transfer meets\nknowledge distillation for efficient model adaptation.\narXiv.\nQihuang Zhong, Liang Ding, Juhua Liu, Xuebo Liu,\nMin Zhang, Bo Du, and Dacheng Tao. 2023a. Re-\nvisiting token dropping strategy in efficient bert pre-\ntraining. In ACL.\nQihuang Zhong, Liang Ding, Keqin Peng, Juhua Liu,\nBo Du, Li Shen, Yibing Zhan, and Dacheng Tao.\n2023b. Bag of tricks for effective language model\npretraining and downstream adaptation: A case study\non glue. arXiv.\nQihuang Zhong, Liang Ding, Li Shen, Peng Mi, Juhua\nLiu, Bo Du, and Dacheng Tao. 2022b. Improving\nsharpness-aware minimization with fisher mask for\nbetter generalization on language models. In Find-\nings of EMNLP.\nQihuang Zhong, Liang Ding, Yibing Zhan, Yu Qiao,\nYonggang Wen, Li Shen, Juhua Liu, Baosheng Yu,\nBo Du, Yixin Chen, et al. 2022c. Toward efficient\nlanguage model pretraining and downstream adapta-\ntion via self-evolution: A case study on superglue.\narXiv.\nA Appendix\nA.1 Details of Tasks and Datasets\nHere, we introduce the descriptions of all down-\nstream tasks and datasets in detail. Firstly, we\npresent the statistics of all datasets in Table 7. Then,\neach task is described as:\nCoLA Corpus of Linguistic Acceptabil-\nity (Warstadt et al., 2019) is a binary single-\nsentence classification task to determine whether a\ngiven sentence is linguistically “acceptable”.\nMRPC Microsoft Research Paraphrase Cor-\npus (Dolan and Brockett, 2005) is a task to predict\nwhether two sentences are semantically equivalent.\n4140\nTask #Train #Dev #Class LR BSZ Epochs/Steps\nGLUE\nCoLA 8.5K 1,042 2 2e-5 32 2668 steps\nMRPC 3.7K 409 2 1e-5 32 1148 steps\nRTE 2.5K 278 2 1e-5 16 2036 steps\nSuperGLUE\nBoolQ 9.4K 3,270 2 1e-5 16 10 epochs\nCB 250 57 2 2e-5 16 20 epochs\nWiC 6K 638 2 2e-5 16 10 epochs\nCOPA 400 100 2 2e-5 16 10 epochs\nCommonsense QA SQuAD2.0 130K 11,873 - 3e-5 12 2 epochs\nSW AG 73K 20K - 5e-5 16 3 epochs\nLAMA Google-RE 60K - N/A\nTable 7: Data statistics and fine-tuning hyper-parameters of all used tasks in this paper. “Class” refers to the label\nclass, “LR” means the learning rate and “BSA” denotes the batch size. Note that the LAMA benchmark is wrapped\ninto a cloze test to probe the PLM without fine-tuning.\nMethod BERTbase ROBERTAbase\nRTE SST-2 QNLI MNLI QQP Avg. RTE SST-2 QNLI MNLI QQP Avg.\nBaseline 32.8 29.7 40.5 22.8 38.5 32.9 47.1 41.9 36.2 21.0 30.9 35.4\n-w/ SE 33.3 28.4 42.6 23.5 42.3 34.0 45.7 37.8 32.4 23.5 38.5 35.6\nMethod BERTlarge ROBERTAlarge\nRTE SST-2 QNLI MNLI QQP Avg. RTE SST-2 QNLI MNLI QQP Avg.\nBaseline 45.6 35.8 41.4 25.3 45.4 38.7 64.1 43.9 61.5 33.9 44.9 49.7\n-w/ SE 53.1 35.1 45.3 24.7 50.0 41.6 67.9 48.6 58.1 34.6 55.1 52.9\nTable 8: Comparison between SE and vanilla method applied to all PLMs on AdvGLUE (Wang et al., 2021)\nbenchmark. Average scores on all tasks are underlined. The best results are given in bold.\nRTE Recognizing Textual Entailment (Giampic-\ncolo et al., 2007), given a premise and a hypothesis,\nis a task to predict whether the premise entails the\nhypothesis.\nQNLI Question Natural Language Inference\nis a binary classification task constructed from\nSQuAD (Rajpurkar et al., 2016), which aims to\npredict whether a context sentence contains the\nanswer to a question sentence.\nCB CommitmentBank (De Marneffe et al., 2019)\ncan be framed as three-class textual entailment on\na corpus of 1,200 naturally occurring discourses.\nBoolQ Boolean Question (Clark et al., 2019)\nis a question answering task where each sample\nconsists of a short passage and a yes/no question\nabout the passage.\nWiC Word-in-Context (Pilehvar and Camacho-\nCollados, 2019) is a word sense disambiguation\ntask that aims to predict whether the word is used\nwith the same sense in sentence pairs.\nCOPA Choice of Plausible Alterna-\ntives(Roemmele et al., 2011) is a causal reasoning\ntask in which a system is given a premise sentence\nand must determine either the cause or effect of\nthe premise from two possible choices.\nSQuAD2.0 The latest version of the Stanford\nQuestion Answering Dataset (Rajpurkar et al.,\n2018) is one of the most widely-used reading com-\nprehension benchmarks that require the systems to\nacquire knowledge reasoning ability.\nSWAG Situations With Adversarial Genera-\ntions (Zellers et al., 2018) is a task of grounded\ncommonsense inference, which unified natural lan-\nguage inference and commonsense reasoning. It is\nalso widely used to evaluate the ability of PLMs on\ncommonsense knowledge reasoning.\nGoogle-RE The Google-RE corpus contains\n60K facts manually extracted from Wikipedia. The\nLAMA (Petroni et al., 2019) benchmark manually\ndefines a template for each considered relation, e.g.,\n“[S] was born in [O]” for “place of birth”. Each fact\nin the Google-RE dataset is, by design, manually\naligned to a short piece of Wikipedia text support-\ning it. There is no training process and during\ninference, we query the PLMs using a standard\ncloze template for each relation. It is widely used\nto probe the model’s world knowledge, especially\nfactual knowledge.\n4141\nMethod GLUE/SGLUE SQuAD2.0/SW AG\nAvg. (∆) Avg. ( ∆)\nBERTbase 74.10 74.93\nBERT-SEbase\nTl = 0.05 74.63 (+0.53) 75.44 (+0.51)\nTl = 0.1 75.45 (+1.35) 75.48 (+0.55)\nTl = 0.5 73.93 (-0.17) 75.22 (+0.29)\nTl = 1 74.02 (-0.08) 75.37 (+0.44)\nTable 9: Parameter analysis on the threshold Tl used in\nself-questioning stage. The “Correctness-based” metric\nis used in this study. Full results are in Table 12.\nA.2 Hyper-parameters of Fine-tuning\nFor fine-tuning, we use the BERT and RoBERTa\nmodels as the backbone PLMs and conduct ex-\nperiments using the open-source toolkit fairseq9\nand transformers10. Notably, we apply the same\nhyper-parameters to all PLMs for simplicity. The\ntraining epochs/steps, batch size, and learning rate\nfor each downstream task are listed in Table 7.\nA.3 Does SE Improve the Robustness?\nHere, we conduct experiments to verify whetherSE\nimproves the robustness of PLMs. In practice, fol-\nlowing Jiang et al. (2022), we use the Adversarial\nGLUE (AdvGLUE) (Wang et al., 2021), which is\na robustness benchmark that was created by apply-\ning 14 textual adversarial attack methods to GLUE\ntasks, to measure the robustness in this study. Ta-\nble 8 lists the results on all PLMs. With the help of\nour SE method, the PLMs achieve consistent im-\nprovements on the AdvGLUE benchmark. These\nresults prove that our SE method is beneficial to\nthe robustness of PLMs.\nA.4 Parameter Analyses on Tl and Te\nAs stated in §3.2, we respectively set a thresholdTl\nand Te for the Correctness-based and Confidence-\nbased metrics to select the hard-to-learn tokens.\nHere, we analyze the influence of different T\nin detail. In practice, taking the Tl as an exam-\nple, we train the BERT base with different Tl (in\n{0.05,0.1,0.5,1}) and evaluate the performance on\na combination of GLUE, SuperGLUE (SGLUE for\nshort), SQuAD2.0 and SW AG benchmarks. Table 9\nlists the average scores of these benchmarks.\nSpecifically, when the Tl (i.e., 0.05) is too small,\nthere may be too many easy-to-learn tokens se-\n9https://github.com/facebookresearch/fairseq\n10https://github.com/huggingface/transformers\nJS(P1||P2) KL(P2||P1) KL( P1||P2)\n0.1681 0.3875 0.7506\nTable 10: Distribution difference between vocabulary\ndistributions selected by Correctness-based “ P1” and\nConfidence-based “P2” metrics. BERT-SElarge is used.\nlected by the metric, which could make the PLM\npay less attention to the target hard-to-learn tokens\nand thus slightly affect the efficacy of SE mecha-\nnism. On the other hand, increasing the Tl makes\nit hard to learn the few amounts but greatly chal-\nlenging tokens, thus slightly harming the perfor-\nmance on GLUE/SGLUE. Among them, Tl = 0.1\nachieves the best, thus leaving as the default setting\nfor correctness-based metric11.\nA.5 Analysis of non-complementarity\nbetween token-selecting metrics.\nAs aforementioned in the ablation study, costly\ncombining both correctness- and confidence-based\nmetrics to select the tokens in the self-questioning\nstage does not show complementarity, having not\noutperformed the default one (correctness-based).\nTo explain their non-complementarity, we quanti-\ntatively analyze the difference in their vocabulary\ndistributions in Table 10.\nSpecifically, let P1 and P2 denote the token fre-\nquency distributions of “Correctness-based” and\n“Confidence-based” metrics, respectively. We first\nuse the Jensen-Shannon (JS) divergence (Lin, 1991)\nto measure the overall difference between P1 and\nP2. It can be found that the JS( P1||P2) is only\n0.1681, indicating that both distributions are over-\nall similar. Furthermore, to fine-grained analyze\nthe impact of both distributions on each other,\nwe compute the KL divergence (Kullback and\nLeibler, 1951) for P1 − →P2 (i.e., KL(P2||P1)) and\nP2 − →P1 (i.e., KL(P1||P2)), respectively. Clearly,\nestimating P2 based on P1 is much easier than the\nopposite direction, i.e., KL(P2||P1) <KL(P1||P2),\nindicating that tokens selected by the correctness-\nbased metric contain most of those selected by\nconfidence-based metric. These statistics nicely\nexplain the empirical superiority of the correctness-\nbased metric in Table 4.\n11We ablate theTe spanning {0.05, 0.1, 0.5, 1, 5, 10} on the\nconfidence-based metric, and observe the similar trend, where\nthe best setting is Te = 1.\n4142\nCoLA MRPC RTE BoolQ CB WiC COPA Score\nMethod Mcc. Acc. Acc. Acc. Acc. Acc. Acc. Avg. ∆\nBaseline PLMs\nBERTbase 62.33 88.97 76.89 75.05 85.71 66.77 63.00 74.10 –\nBERTlarge 63.00 87.25 83.80 78.40 91.07 67.24 72.00 77.54 –\nRoBERTabase 62.00 90.20 83.12 78.72 83.93 69.12 70.00 76.73 –\nRoBERTalarge 64.73 90.69 88.44 84.37 91.07 69.90 78.00 81.03 –\n“Randomly selecting”\nBERT-SEbase 63.23 87.01 76.17 74.83 85.70 68.00 62.00 73.85 -0.25\nBERT-SElarge 65.28 87.74 84.12 80.10 92.90 68.8 69.00 78.28 +0.74\nRoBERTa-SEbase 63.78 88.73 81.59 78.83 89.29 69.43 68.00 77.09 +0.36\nRoBERTa-SElarge 63.42 90.20 89.89 84.13 92.86 71.00 80.00 81.64 +0.61\n“Correctness-based” metric\nBERT-SEbase 63.63 89.50 77.98 74.37 89.29 67.40 66.00 75.45 +1.35\nBERT-SElarge 65.66 88.23 85.20 80.18 92.86 68.34 78.00 79.78 +2.24\nRoBERTa-SEbase 62.11 89.71 84.12 79.39 92.86 71.40 74.00 79.08 +2.35\nRoBERTa-SElarge 67.80 91.91 90.25 84.56 96.40 70.53 80.00 83.06 +2.03\n“Confidence-based” metric\nBERT-SEbase 63.17 89.22 80.51 73.98 89.29 67.24 67.00 75.77 +1.67\nBERT-SElarge 64.07 88.48 84.84 79.30 92.86 69.59 73.00 78.88 +1.34\nRoBERTa-SEbase 64.06 89.71 83.03 78.10 85.71 69.44 75.00 77.86 +1.13\nRoBERTa-SElarge 64.17 89.71 90.61 84.13 96.40 70.22 82.00 82.46 +1.43\nTable 11: Full comparison results (corresponding to the average results in Table 4) between different metrics used\nto select the hard-to-learn tokens on the combination of GLUE and SuperGLUE benchmarks. “ ∆” denotes the\nimprovement of SE methods compared to the baseline PLMs. Average scores on all tasks are underlined.\nCoLA MRPC RTE BoolQ CB WiC COPA SQuAD2.0 SW AG\nMethod Mcc. Acc. Acc. Acc. Acc. Acc. Acc. EM F1 Acc.\nRoBERTabase 62.00 90.20 83.12 78.72 83.93 69.12 70.00 78.79 81.92 79.69\nRoBERTa-SEbase\n-w/ vanilla LS 63.18 89.71 83.39 78.29 89.29 69.75 75.00 79.01 82.12 79.97\n-w/ TLS (Ours) 62.11 89.71 84.12 79.39 92.86 71.40 74.00 79.41 82.55 79.88\nBERTbase 62.33 88.97 76.89 75.05 85.71 66.77 63.00 72.85 75.63 77.83\nBERT-SEbase\nTl = 0.05 63.30 87.50 77.26 73.91 85.71 67.71 67.00 72.85 75.63 77.83\nTl = 0.1 63.63 89.50 77.98 74.37 89.29 67.40 66.00 72.89 75.64 77.91\nTl = 0.5 61.31 88.24 77.26 73.88 85.71 67.08 64.00 72.46 75.36 77.84\nTl = 1 62.06 88.24 77.26 73.82 82.14 68.65 66.00 72.62 75.56 77.93\nTable 12: Full comparison results (corresponding to the average results in Table 5 and 9, respectively) on the\ncombination of GLUE, SuperGLUE, SQuAD2.0 and SW AG benchmarks. Best results are given inbold.\n4143\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix A1\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4144\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.2\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4 and Appendix A1\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4145",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.775605320930481
    },
    {
      "name": "Masking (illustration)",
      "score": 0.7566133737564087
    },
    {
      "name": "Discriminative model",
      "score": 0.7214776277542114
    },
    {
      "name": "Generalization",
      "score": 0.580158531665802
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5743045210838318
    },
    {
      "name": "Exploit",
      "score": 0.5688876509666443
    },
    {
      "name": "Security token",
      "score": 0.5578119158744812
    },
    {
      "name": "Language model",
      "score": 0.5253790616989136
    },
    {
      "name": "Smoothing",
      "score": 0.5196141600608826
    },
    {
      "name": "Softmax function",
      "score": 0.46623656153678894
    },
    {
      "name": "Machine learning",
      "score": 0.43597447872161865
    },
    {
      "name": "Natural language processing",
      "score": 0.42792195081710815
    },
    {
      "name": "Sentence",
      "score": 0.4242139756679535
    },
    {
      "name": "Speech recognition",
      "score": 0.36364954710006714
    },
    {
      "name": "Deep learning",
      "score": 0.17924639582633972
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}