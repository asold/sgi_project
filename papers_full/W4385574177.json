{
  "title": "Retrieval Augmented Visual Question Answering with Outside Knowledge",
  "url": "https://openalex.org/W4385574177",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4208883628",
      "name": "Lin, W.",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4327992391",
      "name": "Byrne, B.",
      "affiliations": [
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2971986145",
    "https://openalex.org/W2964303913",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4309067651",
    "https://openalex.org/W2979382951",
    "https://openalex.org/W2998374885",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2962967746",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W2740432174",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W3161051151",
    "https://openalex.org/W3097986428",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3035497460",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4226321975",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3093200502",
    "https://openalex.org/W3196798856",
    "https://openalex.org/W3035454069",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2747623286",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4312971273",
    "https://openalex.org/W3195541832",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3204877056",
    "https://openalex.org/W3172845486",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3101703188",
    "https://openalex.org/W102708294"
  ],
  "abstract": "Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA task that requires retrieval of external knowledge to answer questions about images. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve documents from external knowledge bases, such as Wikipedia, but with DPR trained separately from answer generation, introducing a potential limit on the overall system performance. Instead, we propose a joint training scheme which includes differentiable DPR integrated with answer generation so that the system can be trained in an end-to-end fashion. Our experiments show that our scheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also introduce new diagnostic metrics to analyze how retrieval and generation interact. The strong retrieval ability of our model significantly reduces the number of retrieved documents needed in training, yielding significant benefits in answer quality and computation required for training.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11238–11254\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nRetrieval Augmented Visual Question Answering with Outside Knowledge\nWeizhe Lin\nDepartment of Engineering\nUniversity of Cambridge\nUnited Kingdom\nwl356@cam.ac.uk\nBill Byrne\nDepartment of Engineering\nUniversity of Cambridge\nUnited Kingdom\nbill.byrne@eng.cam.ac.uk\nAbstract\nOutside-Knowledge Visual Question Answer-\ning (OK-VQA) is a challenging VQA task that\nrequires retrieval of external knowledge to an-\nswer questions about images. Recent OK-VQA\nsystems use Dense Passage Retrieval (DPR) to\nretrieve documents from external knowledge\nbases, such as Wikipedia, but with DPR trained\nseparately from answer generation, introducing\na potential limit on the overall system perfor-\nmance. Instead, we propose a joint training\nscheme which includes differentiable DPR in-\ntegrated with answer generation so that the sys-\ntem can be trained in an end-to-end fashion.\nOur experiments show that our scheme out-\nperforms recent OK-VQA systems with strong\nDPR for retrieval. We also introduce new di-\nagnostic metrics to analyze how retrieval and\ngeneration interact. The strong retrieval ability\nof our model significantly reduces the num-\nber of retrieved documents needed in training,\nyielding significant benefits in answer quality\nand computation required for training.\n1 Introduction\nVisual Question Answering (VQA) is a challenging\nproblem that lies at the intersection of Computer\nVision, Natural Language Processing, and Infor-\nmation Retrieval. The objective in VQA is to read\nan image and provide an answer to an accompa-\nnying question about the image content. Current\napproaches to VQA employ deep-learning-based\nsystems to jointly understand images and text.\nVQA is particularly challenging when the an-\nswer to the question is not directly available in\nthe image. In Knowledge-based VQA (KB-VQA),\nthe VQA system must access external knowledge\nsources to find a correct and complete answer. The\nOuside-Knowledge VQA task (OK-VQA) (Marino\net al., 2019) consists of questions that requires gen-\neral knowledge and simple inference to answer\n(Fig. 1). Such questions are even hard for humans.\nUnlike other KB-VQA datasets (e.g. FVQA (Wang\net al., 2017)) which provide an associated knowl-\nedge base, OK-VQA encourages using any outside\nknowledge in answering questions.\nQuestion  : How many\nteeth does this animal use\nto have?  \nAnswer : 26\nFigure 1: OK-VQA contains questions whose answer\ncannot be found within the image.\nThe need to adapt and refresh knowledge sources\nmotivates the study of KB-VQA systems that can\nextract knowledge from both structured (e.g. Con-\nceptNet (Speer et al., 2017)) and unstructured\nknowledge representations (e.g. Wikipedia pas-\nsages). Recent designs (Luo et al., 2021; Gao et al.,\n2022) approach VQA in two distinct steps: (1)\nKnowledge Retrieval extracts documents from a\nlarge knowledge base; (2) Answer Generation pro-\nduces an answer from these documents. Knowl-\nedge Retrieval can be done via Dense Passage Re-\ntrieval (DPR) (Karpukhin et al., 2020), which con-\nsists of a question encoder and a document encoder\n(both Transformer-based) that encode questions\nand documents into separate dense representations.\nThe DPR system is trained to assign higher scores\nto documents intended to be helpful in answering\nquestions, so that document sets can be retrieved\nand passed to Answer Generation.\nKnowledge Retrieval based on DPR is powerful\nbut has some readily observed limitations, particu-\nlarly in model training. Firstly, whether a retrieved\ndocument is useful in answering a question cannot\nbe easily determined, even if an answer is provided.\nPrior work (Qu et al., 2021; Luo et al., 2021) has\naddressed this problem using “Pseudo Relevance\nLabels” which are based on whether a document\ncontains a given answer. However, these are only\n11238\na weak signal of potential relevance and may en-\ncourage DPR to retrieve misleading documents.\nSecondly, the document retriever and answer gen-\nerator are trained separately. To ensure that the an-\nswer generator sees relevant documents in training,\nsystems can retrieve large numbers of documents\n(∼50+) (Gao et al., 2022; Gui et al., 2021), but at\nthe cost of slower training and more GPU usage,\nand also possibly presenting misleading material\nto the answer generator.\nJoint training of the retriever and answer genera-\ntor offers a solution to these problems. The aim is\ntwofold: (1) to improve the retrieval of documents\ntruly relevant to providing a given answer; and (2)\nto reject documents with pseudo relevance but not\nactual relevance.\nRetrieval Augmented Generation (RAG) (Lewis\net al., 2020) has shown that end-to-end joint train-\ning of a DPR-based QA system can outperform\nbaseline two-step systems. A notable feature of\nRAG is a loss function that incorporates marginal-\nized likelihoods over retrieved documents such that\nthe training score of a document is increased when-\never it improves prediction.\nHowever, in preliminary OK-VQA experiments\nwe found that RAG did not perform well. Our in-\nvestigations found that a good portion of OK-VQA\ntraining questions are answerable in closed-book\nform (i.e. using pre-trained models such as T5 (Raf-\nfel et al., 2020)) with information extracted only\nfrom the image, with the unintended consequence\nthat the RAG loss function awards credit to docu-\nments that did not actually contribute to answering\na question. We also found that difficult questions\nthat are unanswerable with the knowledge avail-\nable to retrieval were more prevalent in OK-VQA\nthan in the Open QA datasets (e.g. Natural Ques-\ntions (Kwiatkowski et al., 2019)) on which RAG\nwas developed. In both of these scenarios, the RAG\nloss function leads to counter-intuitive adjustments\nto the document scores used in training the retrieval\nmodel, leading to decreased VQA performance.\nMotivated by these findings, we propose a novel\nneural-retrieval-in-the-loop framework for joint\ntraining of the retriever and the answer generator.\nWe formulate a loss function that avoids sending\nmisleading signals to the retrieval model in the\npresence of irrelevant documents. This formalism\ncombines both pseudo relevance labels and model\npredictions to refine document scores in training.\nWe find significantly better performance on OK-\nVQA compared to RAG. In this paper:\n• We present a novel joint training frame-\nwork Retrieval Augmented Visual Question\nAnswering (RA-VQA) for Knowledge Re-\ntrieval and Answer Generation that improves\nover RAG and two-step baseline systems\nbased on DPR (Karpukhin et al., 2020).\n• We investigate visually grounded features\ntransformed into ‘language space’ and assess\ntheir contribution to OK-VQA performance.\n• We study the role of document retrieval in\nKB-VQA and evaluate its interaction with\nretrieval-augmented generation. We also show\nthat retrieval becomes more efficient in joint\ntraining, requiring retrieval of relatively few\n(∼5) documents in training.\n2 Related Work\nOpen-domain QA systems. These QA systems are\ndesigned to answer questions from datasets such\nas Natural Questions (Kwiatkowski et al., 2019).\nThe knowledge needed to answer questions can\nbe in pre-trained models (Roberts et al., 2020),\nknowledge-graphs (KGs) (Lin et al., 2019; Feng\net al., 2020; Lv et al., 2020; Saffari et al., 2021) or\ndocument collections (Chen et al., 2017; Izacard\nand Grave, 2021; Guu et al., 2020; Lee et al., 2019;\nLewis et al., 2020). In retrieval-based systems,\ndifferential retrieval can be combined with extrac-\ntive question answering, as in REALM (Guu et al.,\n2020) and ORQA (Lee et al., 2019), as well as with\ngenerative answer generation, as in RAG (Lewis\net al., 2020).\nVQA Systems. Modelling vision and language\nis central to VQA. Models can aggregate visual\nand textual features via cross-modality fusion (Yu\net al., 2018; Singh et al., 2019; Yu et al., 2019;\nJiang et al., 2020; Guo et al., 2021). Systems can\nalso be pre-trained on large vision-and-language\ncollections (Jia et al., 2021) and then fine-tuned\nfor VQA tasks (Tan and Bansal, 2019; Chen et al.,\n2020; Gan et al., 2020; Li et al., 2020b; Wang et al.,\n2022; Zhang et al., 2021; Li et al., 2021) with VQA\ndatasets such as VQA 2.0 (Antol et al., 2015).\nKnowledge-based VQA Systems. KB-VQA can\naccess both structured data, such as ConceptNet\nand other KGs (Narasimhan et al., 2018a; Garderes\net al., 2020; Li et al., 2020a; Wu et al., 2022;\nMarino et al., 2021), as well as unstructured data\nsuch as Wikipedia passages (Wu et al., 2022; Gao\n11239\net al., 2022; Gui et al., 2021). A variety of multi-\nmodal approaches have been explored to access\nexternal knowledge. ConceptBERT (Garderes\net al., 2020) uses attention to aggregate graph node\nembeddings from ConceptNet. KRISP (Marino\net al., 2021) uses a “symbolic knowledge mod-\nule” to match ConceptNet KG entities with lan-\nguage/visual elements in questions. MA VEx (Wu\net al., 2022) uses multiple information sources\n(Google Images, Wikipedia sentences, and Con-\nceptNet) to validate promising answer candidates.\nVRR (Luo et al., 2021) uses Google Search in a\nretriever-reader pipeline to perform open-ended an-\nswer generation.\nWe also note unpublished contemporaneous\nwork on OK-VQA at the time of submission.\nTRiG (Gao et al., 2022) shows that it is feasible\nto transform images into textual features for VQA.\nThe features used are similar to those presented\nhere, although without an emphasis on the role\nof knowledge retrieval. PICa (Yang et al., 2022)\n‘prompts’ GPT-3 with descriptive captions gener-\nated from images, and KAT (Gui et al., 2021) ex-\nploits an ensemble of DPR, T5, and GPT-3 to im-\nprove OK-VQA performance.\n3 Methodology\nWe present our RA-VQA framework that con-\nsists of: (1) Vision-to-Language Transformation\n(Sec. 3.1); (2) Weakly-supervised Dense Passage\nRetrieval (Sec. 3.2); (3) Joint Training of Retrieval\nand Answer Generation (Sec. 3.3).\n3.1 Vision-to-Language Transformation\nPrior work has established that images can be\ntransformed into text such that large pre-trained\nlanguage-based Transformers (e.g. BERT (Devlin\net al., 2019), GPT-2 (Radford et al., 2019), and T5)\ncan be applied to VQA tasks (Luo et al., 2021; Yang\net al., 2022). Systems can be based on straightfor-\nward image caption, but we have found improve-\nments by introducing additional visually-grounded\nfeatures. In RA-VQA, each image is represented\nby visual objects and their attributes, image cap-\ntion, and any text strings detected within the image.\nWe use an object detection model VinVL (Zhang\net al., 2021) that was pre-trained on large object\ndetection datasets to extract visual elements and\ntheir attributes (e.g. color and material).\nFormally, for an image I we use VinVL to ex-\ntract a set of visual objects {oi}, along with a set of\ntext attributes for each visual object {ai,j}. Visual\nobjects and their attributes are extracted by VinVL\nat confidence thresholds 0.8 and 0.6, respectively.\nImage captioning is performed to extract rela-\ntionships and interactions among visual elements\nsuch as “a woman holding a knife cuts a cake”.\nThe pre-trained captioning model Oscar+ (Zhang\net al., 2021) is applied to process visual features\nextracted from the VinVL model to generate a cap-\ntion for the image. To answer questions related\nto text strings in images (e.g. “which language is\nthe book written in?”), Google OCR (Optical Char-\nacter Recognition) APIs are used to extract text\nstrings from each image.\nHence, a VQA training set {(I,q, S)}, where\nSis a set of answers to a question qabout I, can\nbe transformed into a text-only training set T =\n{(x,S)}that we use for RA-VQA. The string x\ncontains all the text features extracted from the\nimage (the question, the textual attributes for each\nidentified visual object, the generated caption, and\nany OCR’d text), with special tokens marking the\nstart and end of each type of feature (Fig. 2).\n3.2 Weakly-supervised Dense Passage\nRetrieval\nDense Passage Retrieval in RA-VQA consists of\na query encoder Fq and a document encoder Fd,\nboth as Transformer-like encoders. The aim is to\nretrieve Kdocuments from an external knowledge\ndatabase Z= {zi}Nd\ni=1 (e.g. Wikipedia passages)\nthat are expected to be useful for answering a ques-\ntion. DPR encodes questions and documents sepa-\nrately into dense feature vectors Fq(x) ∈Rh and\nFd(z) ∈Rh. A scoring function is used to retrieve\ndocuments for each question as the inner product\nbetween the representations of xand z\nr(x,z) =F⊤\nq (x)Fd(z) (1)\nRA-VQA training aims to maximize r(x,z) when\ndocument zis relevant to answering the question.\nAs discussed in Sec. 1, the relevance betweenqand\nzcannot be easily obtained and “pseudo relevance\nlabels” serve as a proxy. We use a pseudo relevance\nfunction H(z,S) which is 1 ifzcontains an answer\nin S(by string match), and 0 otherwise.\nFor each question-answer pair (x,S) one posi-\ntive document z+(x) is extracted for training. In-\nbatch negative sampling is used: all documents in\na training batch other than z+(x) are considered\nto be negative for (x,S) (Karpukhin et al., 2020).\n11240\n       <BOK> flash floods occur within six hours of a\nrain event, or after a dam or levee failure, and flash\nfloods can catch people unprepared. ... <EOK>\n       <BOK> these types of storm are hard to predict\n... flooding floods occur due to rain and other water\nrising faster than the drains can handle. <EOK>\nDocument\nEncoder\nQuestion\nEncoder\nMIPS\nIndex\n.....\nT ransformer\nflood\nrain\n<BOQ> What weather phenomenon\nmost likely happened? <EOQ>\n<BOC> a man sitting on a bench in\na flooded park. <EOC>\n<BOV>wood brown red bench\n<SOV> large tall green tree <SOV>\ncalm gray water <SOV> white\ndocked boat <SOV> cloudy gray\nwhite sky ...... <SOV> [OCR texts if\nexists] <EOV>1 Image-to-T ext T ransform\n2 Dense Passage Retrieval\nKnowledge\nDatabase\nstorm\nPseudo Relevance\nAnswers: flood, hurricane, rain\n3 Joint T raining of  \nBackpropogation \n         <BOK> the most common cause of flooding is\nwater due to rain and/or snowmelt that accumulates\nfaster than soils can absorb it or rivers can carry it\naway . <EOK>\nGradient\nBackpropogate \nT rainable\nParameters\nNon-parametric\nT ransform\nflood\nmax joint\nprobability\n4 Prediction\nFigure 2: Model overview. (1) Using object detection/image captioning/Optical Character Recognition to transform\nvisual signals into language space. (2) Dense Passage Retrieval retrieves documents that are expected to be helpful\nfrom the knowledge database; (3) Training the retriever pθ and the answer generator pϕ together using our proposed\nRA-VQA loss. (4) The answer with highest joint probability pθ(zi|x)pϕ(yi|x,zi) is selected.\nDenoting the negative documents as N(x,S) and\nthe score of the positive document as ˆr+(x) leads\nto the DPR loss LDPR :\n−\n∑\n(x,S)∈T\nlog exp (ˆr+(x))\nexp (ˆr+(x)) +\n∑\nz∈N(x,S)\nexp (ˆr(x,z))\n(2)\n3.3 RA-VQA: Joint Training of Document\nRetrieval and Answer Generation\nGiven a full query string x extracted from the\nimage-question pair (I,q), DPR returns the K\nhighest scoring documents {zk}K\nk=1. The score\nassigned by the document retriever pθ(·|x) to a\nretrieved document is\npθ(zk|x) = exp(ˆr(x,zk))∑K\nj=1 exp(ˆr(x,zj))\n(3)\nOpen-ended answer generation for each re-\ntrieved document zk is performed with a generative\nmodel, such as T5, with parameters ϕ:\nyk = argmax\ny\npϕ(y|x,zk) (4)\nFor each document zk retrieved for a training\nitem (x,S), we train the answer generator to pro-\nduce the answer string s∗\nk from the concatenation\nof xand zk (as shown in Fig. 2). We select the most\npopular1 human response s∗\nk from Ssuch that s∗\nk is\ncontained in zk; in the case that zk does not contain\nany answer, the most popular answer s∗∈S is se-\nlected s∗\nk = s∗. Through this design, we customize\n1There are 5 annotators for each OKVQA question. The\npopularity of an answer is measured by the number of annota-\ntors who voted for it.\nthe generation target s∗\nk for each retrieved docu-\nment instead of training all (x,zk) pairs towards\nthe most popular human response s∗. This has\nbeen proved to improve the system performance\n(Appendix B.1).\nWe identify two subsets of the retrieved docu-\nments {zk}K\nk=1 based on pseudo relevance labels\nand model predictions:\nP+(x,S) ={k: yk = s∗\nk ∧H(zk,S) = 1};\nP−(x,S) ={k: yk ̸= s∗\nk ∧H(zk,S) = 0}. (5)\nP+ are indices of pseudo relevant documents that\nalso help the model generate popular answers\nwhereas P−identifies documents not expected to\nbenefit answer generation. In joint training, we\nintend to increase the scores of documents in P+\nwhile decreasing the scores for those in P−. zk\nwill be put into the negative set if it does not con-\ntain any answer (H(zk,S) = 0) and the generation\nis incorrect (yk ̸= s∗\nk).2 This is motivated by our\nintention to reduce scores for those documents that\ncontain no answers and fail to answer questions.\nFormally, joint training of retrieval and answer\ngeneration is achieved with a loss LRA−VQA that\nreflects both model predictions and pseudo rele-\nvance:\n−\n∑\n(x,S)∈T\n(K∑\nk=1\nlog pϕ(s∗\nk|x,zk)\n+\n∑\nk∈P+(x,S)\nlog pθ(zk|x) −\n∑\nk∈P−(x,S)\nlog pθ(zk|x)\n)(6)\n2Note that in this case H(zk, S) = 0already implies that\nzk does not contain any answer and thus s∗\nk = s∗.\n11241\nThe first term in the loss improves answer gener-\nation from queries and retrieved documents, taken\ntogether. The remaining terms affect document\nretrieval: the second term encourages retrieval of\ndocuments that are not only pseudo relevant but\nalso lead to production of correct answers, while\nthe third term works to remove irrelevant items\nfrom the top ranked retrieved documents. The in-\nR etriev er\nAnswer\nGener ator\nCustomiz ed\nGener ation T ar gets\nModel Pr edictions\nPseudo R elev ance Labels\nFigure 3: Information flow between the retriever and\nthe answer generator.\nformation flow is demonstrated in Fig. 3. Retrieval\nand generation complement each other in train-\ning: pseudo relevance labels and model predictions\nprovide positive and negative signals to improve\nretrieval, and the improved retrieval leads to im-\nproved answer generation by training towards s∗\nk,\na customized generation target for each retrieved\ndocument zk.\n3.4 RA-VQA Generation\nGiven an image query (I,q), a full query xis cre-\nated (Sec. 3.1) and answer generation searches for\nthe answer with the highest joint probability:\n{zk}K\nk=1 = argmax\nz\nKpθ(z|x)\nˆy,ˆz= argmax\ny,zk\npϕ(y|x,zk) pθ(zk|x) (7)\nAnswers reflect both generation and retrieval mod-\nels and retrieval confidence plays a strong role,\nunlike some prior work such as Luo et al. (2021).\n3.5 Pre-Computed FAISS Document Indices\nSince repeated computation of embeddings for all\ndocuments is costly, we follow Lewis et al. (2020)\nwho find that it is enough to train only the ques-\ntion encoder Fq and leave document encoder Fd\nfixed. As shown in Fig. 2, document embeddings\nare pre-extracted with a pre-trained DPR document\nencoder. The FAISS system (Johnson et al., 2019)\nis used to index all document embeddings which en-\nables fast nearest neighbour search with sub-linear\ntime complexity. In training, question embeddings\nare generated dynamically and documents with\nhighest scores are retrieved using the pre-computed\nindex.\n4 Experiments\n4.1 Datasets and RA-VQA Configurations\nOK-VQA (Marino et al., 2019) is currently the\nlargest knowledge-based VQA dataset. It consists\nof 14,031 images and 14,055 questions. These\nquestions are split into a training set (9,009 ques-\ntions) and a test set (5046 questions). In addition\nto understanding images and questions, external\nknowledge sources are needed to answer questions.\nAs outside knowledge we use the knowledge\ncorpus collected by Luo et al. (2021) from Google\nSearch. We use the corpus GS-full which con-\nsists of 168,306 documents covering training and\ntest questions. In Appendix B.1 we also report on\nGS-train, which contains documents relevant to\nOK-VQA training set questions only.\nPre-training: We start with pre-trained versions\nof BERT-base and T5-large as the document re-\ntriever and the answer generator, respectively. The\nretriever was refined by training it on GS-full un-\nder the DPR loss (Equation 2) with pseudo rele-\nvance labels released by Luo et al. (2021). The\nalready strong retriever serves as a good starting\npoint for all DPR-based models presented in this\npaper (including RA-VQA and our replication of\nbaselines in the literature).\nOK-VQA Fine-tuning: Our RA-VQA frame-\nwork trains the answer generator and the retriever\njointly under Equation 6.\nWe also report on variants of RA-VQA, to in-\nvestigate the contribution of various model compo-\nnents to overall performance:\nRA-VQA-NoDPR omits retrieval entirely so that\nanswers are generated by the fine-tuned T5 alone.\nRA-VQA generation in Equation 7 simplifies to\nˆyNoDPR = argmax\ny\npϕ(y|x) (8)\nRA-VQA-FrDPR leaves the retriever frozen after\npre-training and fine-tunes the generator only.\nRA-VQA-NoPR is a version of RA-VQA in which\ndocument retrieval is trained only with model pre-\ndictions. The loss function is as Equation 6, but\nwith positive and negative document sets defined\nas P+\nNoPR(x,S) ={k: yk = s∗\nk};\nP−\nNoPR(x,S) ={k: yk ̸= s∗\nk}. (9)\nRA-VQA-NoCT replaces the customized gener-\nation targets by the single most popular response\n11242\n(s∗\nk becomes s∗in Equation 6) so that the generator\nis trained to produce the same answer from every\nretrieved document.\n4.2 Evaluation\nThe following metrics are applied to assess the\nquality of individual answers generated and docu-\nments retrieved. Average scores are then computed\nover the evaluation set. The average of 3 runs with\ndifferent seeds is reported.\n4.2.1 Answer Evaluation\nVQA Score: We follow Marino et al. (2019) to\ncompute VQA Scores using pre-processed human\nannotations S:\nVQAScore(y,S) = min\n(#S(y)\n3 ,1\n)\n, (10)\nwhere #S(y) is the number of annotators who\nanswered y. This score ensures that a model is\npartially rewarded even if it generates one of the\nless popular answers from amongst the human re-\nsponses.\nExact Match (EM) treats annotated answers\nequally: EM(y,S) = min(#S(y),1) .\n4.2.2 Retrieval Evaluation\nFollowing Luo et al. (2021), we use pseudo rele-\nvance to ascertain whether the retrieved documents\nare relevant to the response. It concerns pseudo\nrelevance instead of actual relevance but is still a\nreasonable metric for retrieval evaluation.\nPseudo Relevance Recall (PRRecall)@K mea-\nsures how likely the retrieved K documents con-\ntains at least one positive document:\nPRRecall@K = min\n(K∑\nk=1\nH(zk,S),1\n)\n. (11)\n4.2.3 Integrated System Evaluation\nThe above methods evaluate retrieval and answer\ngeneration as separate processes. We propose ad-\nditional metrics that assess how the two processes\nbehave in an integrated VQA system.\nThe Hit Success Ratio (HSR) counts questions\nthat require external knowledge to answer:\nHSR = 1\n{\nˆy∈S∧ ˆyNoDPR /∈S\n}\n. (12)\nHSR reflects the value of incorporating external\ndocuments into answer generation.\nBy contrast, Free Success Rate (FSR) counts\nquestions that can be answered without external\nknowledge.\nFSR = 1\n{\nˆy∈S∧ ˆyNoDPR ∈S\n}\n. (13)\nA high FSR suggests a model can generate cor-\nrect answers ‘freely’ without being distracted by\nretrieved documents if they are not needed.\nWe also assess performance as a function of the\nnumber of documents retrieved during training and\ntesting, Ktrain and Ktest. In practice, Ktrain has the\ngreater effect on GPU usage, since a large Ktrain re-\nquires at least Ktrain forward passes for each ques-\ntion and an Adam-like optimizer must compute\nand store the associated gradients (Kingma and Ba,\n2015). In contrast, GPU memory required during\ntesting is significantly less, as there is no optimizer\ninvolved. We are in particular interested in the abil-\nity of knowledge-augmented systems that can be\nrobustly trained with small Ktrain while yielding\nimproved performance with large Ktest.\n4.3 Baseline Systems\n4.3.1 Retrieval Augmented Generation\nRAG (Lewis et al., 2020) is based on DPR and an\nanswer generator that are trained jointly by approx-\nimately marginalizing the probability of yover the\nretrieved documents. In the notation of Sec. 3:\npRAG(y|x) ≈\nK∑\nk=1\npϕ(y|x,zk) pθ(zk|x) (14)\nThe answer generator and the retriever are\njointly trained by optimizing the RAG loss:\n−∑\n(x,S)∈T log\n(\npRAG(s∗|x)\n)\n. The rationale is\nthat pθ(zk|x) will increase if zk has a positive im-\npact on answer generation (Lewis et al., 2020). We\nconsider RAG as an important baseline and have\ncarefully replicated its published implementation3.\n4.3.2 Baseline Systems in the Literature\nWe compare against the published OK-VQA results\nfrom systems described in Sec. 2: ConceptBERT,\nKRISP, MA VEx, and VRR. We also report per-\nformance against unpublished (non peer-reviewed)\nsystems TRiG4, PICa, and KAT. TRiG uses a\nsimilar image-to-text transform as this work, so to\nenable fair comparison with our model we replicate\ntheir knowledge fusing method with our features.\nBaseline results are reported in Table 1; baseline\nresults marked * are our own. TRiG*, our own\nimplementation of TRiG, concatenates Kencoder\noutputs for the decoder to use in generation.\nWe make some particular observations. Our\nTRiG* improves over the results released in its\n3The authors released RAG in huggingface.\n4At the time of submission, TRiG has not been published.\n11243\nModel T5 GPT-3 Ktrain Ktest Knowl. Src. PRRecall HSR / FSR H/F EM VQA\nConceptBERT × × - - C 33.66\nKRISP × × - - C + W 38.35\nVRR × × 100 100 GS 45.08\nMA VEx × × - - W + C + GI 39.40\nKAT-T5 ✓ × 40 40 W 44.25\nTRiG ✓ × 5 5 W 49.21 45.51\nTRiG ✓ × 100 100 W 53.59 49.35\nTRiG-Ensemble ✓ × 100 100 W 54.73 50.50\nTRiG* ✓ × 5 5 GS 52.79 48.32\nRAG* ✓ × 5 5 GS 82.34 12.28 / 40.24 0.31 52.52 48.22\nRA-VQA (Ours) ✓ × 5 5 GS 82.84 16.75 / 41.97 0.40 58.72 53.81\nRA-VQA (Ours) ✓ × 5 50 GS 96.55 17.32 / 42.09 0.41 59.41 54.48\nAblation Study\nRA-VQA-FrDPR ✓ × 5 5 GS 81.25 15.01 / 40.76 0.37 55.77 51.22\nRA-VQA-NoPR ✓ × 5 5 GS 77.67 15.97 / 41.83 0.38 57.80 52.98\nRA-VQA-NoCT ✓ × 5 5 GS 83.77 14.55 / 42.96 0.33 57.51 52.67\nGPT-3-based Systems (>175 Billion Parameters)\nPICa × ✓ - - GPT-3 48.00\nKAT-Knowledge-T5 ✓ ✓ 40 40 W + GPT-3 51.97\nKAT-Ensemble ✓ ✓ 40 40 W + GPT-3 54.41\nTable 1: RA-VQA vs. Baseline Systems. Knowledge Sources: ConceptNet; Wikipedia; Google Search; Google\nImages; GPT-3 closed book knowledge. H/F: HSR to FSR ratio. PRRecall, HSR, FSR, and EM are reported in\npercentage (%). PRRecall is reported at the corresponding Ktest.\npaper (VQA Score of 48.32 vs 45.51) at Ktrain =\nKtest = 5; TRiG and TRiG Ensemble both bene-\nfit from more retrieved documents in training and\ntesting (Ktrain = Ktest = 100), although at great\ncomputational cost. Best performance with KAT-\nT5 and VRR similarly requires large document\ncollections in training and in test.\nWe include results from GPT-3 based systems\nbecause they are amongst the best in the literature,\nbut we note that GPT-3 is so much bigger than\nT5 (175 billion parameters in GPT-3 v.s. 770 mil-\nlion in T5-large) that simply switching a system\nimplementation from T5 to GPT-3 can give sig-\nnificant improvements: KAT-T5 achieved a 44.25\nVQA Score while ensembling it with GPT-3 yields\n54.41; and GPT-3 alone already achieved good per-\nformance with prompting (PICa with 48.00 VQA\nScore). Our RA-VQA system is based on T5, but\nwe still find competitive results even in comparison\nto systems incorporating GPT-3 (54.48 vs 54.41 of\nKAT-Ensemble).\n4.4 RA-VQA Performance Analysis\nWe find that RA-VQA matches or improves over all\nbaseline systems with a VQA Score of 54.48. This\nis with a configuration of Ktrain = 5and Ktest =\n50, thus validating our claim that RA-VQA can use\na large number of retrieved documents in testing\n(50) while using relatively few retrieved documents\nin training (5). We find that reducing the number\nof retrieved documents in test (Ktest = 5) reduces\nthe VQA Score, but still yields performance better\nthan all baselines except the KAT ensemble.\nWe also find that RA-VQA performs well rela-\ntive to GPT-3 baselines. RA-VQA yields a higher\nVQA score than KAT-Knowledge-T5 (54.48 vs.\n51.97) and matches the KAT-Ensemble system. We\nemphasize that RA-VQA is significantly smaller\nin terms of parameters (and in model pre-training\ndata) than these GPT-3 based systems and that\ntraining RA-VQA requires much less memory\n(Ktrain = 5vs Ktrain = 40).\n4.4.1 Contributions of Query Features and\nDPR to Overall Performance\nModel Q O A C T VQA Score\nRA-VQA-NoDPR ✓ × × × × 28.05\nRA-VQA-NoDPR ✓ ✓ × × × 40.95\nRA-VQA-NoDPR ✓ ✓ ✓ × × 42.14\nRA-VQA-NoDPR ✓ ✓ ✓ ✓ × 45.31\nRA-VQA-NoDPR ✓ ✓ ✓ ✓ ✓ 46.16\nRA-VQA-FrDPR ✓ ✓ ✓ ✓ ✓ 51.22\nRA-VQA ✓ ✓ ✓ ✓ ✓ 53.81\nTable 2: Ablation study on input features and system\nconfigurations: Questions; Objects; Attributes associ-\nated with objects; Captions; visible Text from OCR.\nK = 5in RA-VQA and RA-VQA-FrDPR.\nA detailed ablation study on input features is\npresented in Table 2. As shown, the T5 model\n11244\nfine-tuned on OK-VQA achieves a 28.05 VQA\nScore. The VQA Score increases to 46.16 as ob-\njects, object attributes, image captions, and OCR\ntexts are incorporated into RA-VQA-NoDPR. With\n5 retrieved documents, RA-VQA-FrDPR yields\na 51.22 VQA Score, with a further improvement\n(53.81 VQA Score) in full training of retrieval and\nanswer generation, confirming that outside knowl-\nedge is needed to answer OK-VQA questions.\n4.4.2 Benefits of Integrated Training\nJoint training is a key benefit of our proposed RA-\nVQA framework: model predictions combine with\npseudo relevance labels to improve retrieval, and\nthe resulting improved retrieval in turn provides\ncustomized answer generation targets. To quan-\ntify these effects, we take RA-VQA-FrDPR as a\nstarting point (Table 1). Comparing it with other\nRA-VQA models suggests that DPR training in\nitself is necessary, as using only pre-trained DPR\n(RA-VQA-FrDPR) leads to weaker VQA Score\n(51.22). Using model predictions alone in joint\nDPR training (RA-VQA-NoPR) leads to a higher\nVQA Score (52.98 vs 51.22), but a significantly\nlower PRRecall (77.67% vs 81.25%). The model\ndecides to remove some pseudo relevant documents\nbut achieves better performance. This points to a\npotential problem that can arise. Pseudo relevance\nis only an imperfect indication of true relevance\nand so is not an ideal criteria on its own. Training\nDPR to retrieve pseudo relevant documents could\nresult in misleading documents being used in an-\nswer generation.\nUsing both pseudo relevance labels and model\npredictions in DPR training (RA-VQA) improves\nVQA Score to 53.81 and notably improves\nPRRecall to 82.84%. Including pseudo relevance\nensures that potentially useful documents are re-\ntained, even while the generator is still learning to\nuse them.\nWe also note that when generation targets are\nnot customized for each retrieved document (RA-\nVQA-NoCT), VQA Score drops by 1.14 relative\nto RA-VQA, showing that customized generation\ntargets play an important role in the overall sys-\ntem: by training the model to extract the reliable\nanswers available in retrieved documents, answer\ngeneration and retrieval are both improved.\n4.4.3 Interaction of Retrieval and Generation\nTable 1 also reports our investigation into the inter-\naction between document retrieval and answer gen-\neration. In comparing RA-VQA-FrDPR (frozen\nDPR) to RA-VQA, we see that joint training of\nDPR yields not only improved EM but also signif-\nicantly higher HSR (+1.74%) and FSR (+1.21%).\nManual inspection of OK-VQA reveals that there\nare many general knowledge questions. For ex-\nample, document retrieval is not needed to answer\nthe question \"Is this television working?\" in refer-\nence to a picture of a broken television lying in a\nfield. A high FSR indicates good performance on\nsuch questions. By contrast, a high HSR reflects\nthe ability to use document retrieval to answer the\nquestions that truly require external documents.\nBoth EM and HSR are further improved for\nKtest = 50in RA-VQA, with little change in FSR.\nThe increased HSR to FSR ratio (0.41 vs. 0.40)\nindicates that RA-VQA is using these additional\nretrieved documents to answer the questions that\nneed outside knowledge.\nHSR and FSR also explain the relatively weak\nperformance of RAG*. We see that although RAG*\nand RA-VQA-FrDPR have similar FSRs, RAG*\nhas higher PRRecall but lower HSR (by -2.73%).\nThis suggests RAG*’s DPR model is not well\nmatched to its answer generator. The result is that\nretrieved documents remain unexploited. In man-\nual examination of gradients of document scores\nin training, we find anecdotally that adjustments to\ndocument scores are often counter-intuitive: doc-\numents that do not contain answers can still have\ntheir scores upvoted if the answer generator hap-\npens to find a correct answer by relying only on the\nability of T5 model. This works against a model’s\nability to find answers in retrieved documents even\nwhen those documents are relevant.\n4.4.4 Effects of Ktrain\nAs noted, retrieving a large collection of documents\nin training is costly (large Ktrain). Fig. 4 shows\nthat RA-VQA can be trained with relatively few\nretrieved documents (Ktrain = 5). We gradually in-\ncrease Ktrain while fixing Ktest = Ktrain (dash lines)\nand Ktest = 50 (solid lines). RA-VQA achieves\nconsistent performance ( ∼54.4 VQA Score) at\nKtrain ≥5 and Ktest = 50, which suggests that\nour joint training scheme is able to gather most\nuseful knowledge into a top-50 list even when the\nmodel is trained to retrieve fewer documents. This\nis not the case for the frozen DPR systems which re-\nquire increasing Ktrain to obtain best performance.\nRA-VQA’s superior performance shows that joint\ntraining of retrieval and generation yields clear ben-\n11245\n2 3 4 5 10 15 20 30\nKtrain\n50\n51\n52\n53\n54\n55VQA Score\n50.5\n51.5 51.7 51.8 51.8 51.9\n52.4 52.3\n51.2\n51.9 52.0 52.0 52.2\n52.4\n52.8 52.8\n51.3\n52.3\n53.2\n53.8 54.0 54.0 54.1 53.9\n52.6\n53.4\n54.0\n54.5 54.4 54.3 54.4 54.4\nRA-VQA-FrDPR(Ktest = Ktrain)\nRA-VQA-FrDPR(Ktest = 50)\nRA-VQA(Ktest = Ktrain)\nRA-VQA(Ktest = 50)\nFigure 4: VQA Scores against Ktrain. Dashed line:\nKtest = Ktrain; solid line: Ktest = 50. Our proposed\nmodel achieves the best performance when additional\ndocuments are retrieved in test (Ktest = 50). This holds\neven for models trained to retrieve fewer documents.\nefits in computation and answer quality.\nWe also note thatKtrain = 5is an optimal design\nchoice that strikes a balance between training com-\nputational cost and system performance. The green\ncurves at Ktrain <5 also suggest that it is beneficial\nto include at least 5 documents in training. This is\nbecause Ktrain ≥5 offers a good PRRecall (over\n80%), which provides documents of higher quality\nfor training the system.\n4.5 Additional Analyses\nWe redirect readers to other interesting analyses\n(e.g. an analysis of computational cost) and case\nstudies in Appendices B-E.\nIn addition, in Appendix F, we evaluate\nour proposed framework on another popular\nKnowledge-based VQA dataset, FVQA (Fact-\nbased VQA) (Wang et al., 2017). Similarly, the RA-\nVQA framework with joint training achieves bet-\nter results over the baseline systems with a frozen\nDPR component, showing the generalizability of\nour proposed framework.\n5 Conclusion\nRetrieval-Augmented Visual Question Answering\nis a novel modelling framework for integrated train-\ning of DPR and answer generation. We have evalu-\nated RA-VQA on the OK-VQA task and we find\nsignificantly better performance than the indepen-\ndent training of component system. Through diag-\nnostic metrics such as HSR and FSR we analysed\nthe interaction between retrieval and generation,\nand have also shown how RA-VQA’s gains arise\nrelative to other approaches, such as RAG. As a\nfurther practical benefit, we found that RA-VQA\ncan be used with larger numbers of retrieved docu-\nments than were used in system training, yielding\ncomputational savings without sacrificing perfor-\nmance.\nThe code for this paper will be released\nat https://github.com/LinWeizheDragon/Retrieval-\nAugmented-Visual-Question-Answering.\n6 Acknowledgement\nW. Lin was supported by a Research Stu-\ndentship funded by Toyota Motor Europe\n(RG92562(24020)). We thank our colleagues,\nDaniel Olmeda Reino (Toyota Motor Europe) and\nJonas Ambeck (Toyota Motor Europe), who pro-\nvided insight and expertise in this project.\nWe thank Zhilin Wang (University of Wash-\nington) and Alexandru Coca (University of Cam-\nbridge) for comments that greatly improved the\nmanuscript. We would also like to thank all the\nreviewers for their knowledgeable reviews.\n7 Limitations\nOne possible limitation is that some relevant infor-\nmation (such as relative positioning of objects in\nthe image) could be lost in transforming images\nindependently of the information being sought. Ex-\ntracting visual features based on queries could be a\nnatural next step, although query-specific process-\ning of the image collection would be computation-\nally expensive.\nWe selected the Google Search corpus (Luo\net al., 2021) as the knowledge base for our ques-\ntion answering system. Its advantages are that it is\nlarge, openly available, and can be readily used\nto replicate the results in this paper. However\nsome visual question types (e.g. ‘Is the athlete\nright or left handed?’) could plausibly require both\ncomplex reasoning and more closely relevant doc-\numents from additional knowledge sources (such\nas Wikipedia). Our system may be limited with\nrespect to these considerations.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. Vqa: Visual question answering.\nIn Proceedings of the IEEE international conference\non computer vision, pages 2425–2433.\n11246\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\nDbpedia: A nucleus for a web of open data. In The\nsemantic web, pages 722–735. Springer.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European conference on\ncomputer vision, pages 104–120. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics. ACL.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng\nWang, Jun Yan, and Xiang Ren. 2020. Scalable multi-\nhop relational reasoning for knowledge-aware ques-\ntion answering. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1295–1309, Online. As-\nsociation for Computational Linguistics.\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu,\nYu Cheng, and Jingjing Liu. 2020. Large-scale adver-\nsarial training for vision-and-language representation\nlearning. Advances in Neural Information Process-\ning Systems, 33:6616–6628.\nFeng Gao, Qing Ping, Govind Thattai, Aishwarya Re-\nganti, Ying Nian Wu, and Prem Natarajan. 2022.\nTransform-retrieve-generate: Natural language-\ncentric outside-knowledge visual question answer-\ning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n5067–5077.\nFrançois Garderes, Maryam Ziaeefard, Baptiste Abe-\nloos, and Freddy Lecue. 2020. Conceptbert:\nConcept-aware representation for visual question an-\nswering. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nFindings, pages 489–498.\nLiangke Gui, Borui Wang, Qiuyuan Huang, Alex Haupt-\nmann, Yonatan Bisk, and Jianfeng Gao. 2021. Kat:\nA knowledge augmented transformer for vision-and-\nlanguage. arXiv preprint arXiv:2112.08614.\nDalu Guo, Chang Xu, and Dacheng Tao. 2021. Bilinear\ngraph networks for visual question answering. IEEE\nTransactions on Neural Networks and Learning Sys-\ntems.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In Proceed-\nings of the 37th International Conference on Machine\nLearning, ICML’20. JMLR.org.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nHuaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik\nLearned-Miller, and Xinlei Chen. 2020. In defense\nof grid features for visual question answering. InPro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10267–10276.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453–\n466.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\n11247\nGuohao Li, Xin Wang, and Wenwu Zhu. 2020a. Boost-\ning visual question answering with context-aware\nknowledge aggregation. In Proceedings of the 28th\nACM International Conference on Multimedia, pages\n1227–1235.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao\nLiu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2021.\nUNIMO: Towards unified-modal understanding and\ngeneration via cross-modal contrastive learning. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2592–\n2607, Online. Association for Computational Lin-\nguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020b. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision,\npages 121–137. Springer.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. KagNet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2829–2839, Hong Kong,\nChina. Association for Computational Linguistics.\nMan Luo, Yankai Zeng, Pratyay Banerjee, and Chitta\nBaral. 2021. Weakly-supervised visual-retriever-\nreader for knowledge-based question answering. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n6417–6431, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nShangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan\nDuan, Ming Gong, Linjun Shou, Daxin Jiang, Gui-\nhong Cao, and Songlin Hu. 2020. Graph-based rea-\nsoning over heterogeneous external knowledge for\ncommonsense question answering. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 34, pages 8449–8456.\nKenneth Marino, Xinlei Chen, Devi Parikh, Abhinav\nGupta, and Marcus Rohrbach. 2021. Krisp: Inte-\ngrating implicit and symbolic knowledge for open-\ndomain knowledge-based vqa. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14111–14121.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n3195–3204.\nMedhini Narasimhan, Svetlana Lazebnik, and Alexan-\nder Schwing. 2018a. Out of the box: Reasoning with\ngraph convolution nets for factual visual question an-\nswering. Advances in neural information processing\nsystems, 31.\nMedhini Narasimhan, Svetlana Lazebnik, and Alexan-\nder Schwing. 2018b. Out of the box: Reasoning\nwith graph convolution nets for factual visual ques-\ntion answering. In Advances in Neural Information\nProcessing Systems, volume 31. Curran Associates,\nInc.\nChen Qu, Hamed Zamani, Liu Yang, W Bruce Croft,\nand Erik Learned-Miller. 2021. Passage retrieval for\noutside-knowledge visual question answering. In\nProceedings of the 44th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 1753–1757.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nAmir Saffari, Armin Oliya, Priyanka Sen, and Tom Ay-\noola. 2021. End-to-end entity resolution and question\nanswering using differentiable knowledge graphs.\nIn Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n4193–4200, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAmanpreet Singh, Vivek Natarajan, Meet Shah,\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. 2019. Towards vqa models\nthat can read. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 8317–8326.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-first AAAI conference on\nartificial intelligence.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for Com-\nputational Linguistics.\n11248\nNiket Tandon, Gerard De Melo, and Gerhard Weikum.\n2017. Webchild 2.0: Fine-grained commonsense\nknowledge distillation. In Proceedings of ACL 2017,\nSystem Demonstrations, pages 115–120.\nPeng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and\nAnton Van Den Hengel. 2017. Fvqa: Fact-based\nvisual question answering. IEEE transactions on pat-\ntern analysis and machine intelligence, 40(10):2413–\n2427.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2022. SimVLM: Simple\nvisual language model pretraining with weak super-\nvision. In International Conference on Learning\nRepresentations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nJialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh\nMottaghi. 2022. Multi-modal answer validation for\nknowledge-based vqa. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36,\npages 2712–2721.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of gpt-3 for few-shot knowledge-\nbased vqa. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 36, pages 3081–\n3089.\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.\n2019. Deep modular co-attention networks for visual\nquestion answering. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recogni-\ntion, pages 6281–6290.\nZhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and\nDacheng Tao. 2018. Beyond bilinear: Generalized\nmultimodal factorized high-order pooling for visual\nquestion answering. IEEE transactions on neural\nnetworks and learning systems, 29(12):5947–5959.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\ntions in vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5579–5588.\nZihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue\nHu, and Qi Wu. 2020. Mucko: Multi-layer cross-\nmodal knowledge reasoning for fact-based visual\nquestion answering. In Proceedings of the Twenty-\nNinth International Joint Conference on Artificial\nIntelligence, IJCAI-20, pages 1097–1103. Interna-\ntional Joint Conferences on Artificial Intelligence\nOrganization. Main track.\nA Training Details and Artifacts\nAdam (Kingma and Ba, 2015) was used in this pa-\nper. In DPR pre-training, the retriever was trained\nfor 6 epochs with a constant learning rate 10−5. In\nRA-VQA training (including RAG*), the learning\nrates are 10−5 for the retriever, and 6 ×10−5 for\nthe answer generator, linearly decaying to 0 after\n10 epochs. In the training of RA-VQA-NoDPR\nand TRiG*, the initial learning rate is 6 ×10−5.\nEmpirically, the checkpoints at epoch 6 were used\nin testing. All experiments were run on Nvidia A-\n100 GPU clusters. With Ktrain = 5, the RA-VQA\ntraining takes around 5 hours (10 epochs) while\ntesting takes 5 minutes. The time cost increases as\nKtrain increases, approximately linearly.\nPre-trained model parameters (e.g. T5-large and\nBERT-base) are provided by huggingface (Wolf\net al., 2020) accompanied by Python libraries (un-\nder Apache License 2.0). FAISS (Johnson et al.,\n2019) is under MIT License.\nB Supplementary Tables and Figures\nLimited by space available, we present supplemen-\ntary tables and figures in this section, offering more\nfindings to readers.\nB.1 Full Version of Table 1\nWe provide the full version of Table 1 in Table 8.\nSome more discussions about customized genera-\ntion target in joint training is provided.\nAs noted, RA-VQA improves retrieval with the\nfeedback of model predictions, and in turn the im-\nproved retrieval leads to improved answer gener-\nation by training towards s∗\nk, a customized gener-\nation target for each retrieved document zk. We\nremove this interaction from RA-VQA models by\nenforcing s∗\nk = s∗ (the most popular human re-\nsponse), independent of the retrieved zk. The ab-\nlated models are denoted with a *-NoCT suffix.\nAs shown in Table 8, customizing generation\ntargets for each retrieved zk in training yields per-\nformance boost for both RA-VQA-FrDPR and RA-\nVQA, showing that this supervision signal is benefi-\ncial to overall system performance. We also notice\nthat the improvement to RA-VQA (+1.14 VQA\n11249\nModels K = 5 K = 10 K = 20 K = 50 Ktest = 5\nP R P R P R P R EM VQA Score\nVRR (Luo et al., 2021) - 80.40 - 88.55 - 93.22 - 97.11 - 42.54\nRA-VQA-FrDPR 51.82 81.25 49.20 88.51 45.98 92.98 41.24 96.75 55.77 51.22\nRA-VQA 57.39 82.84 54.83 89.00 51.48 93.62 46.36 96.47 58.72 53.81\nTable 3: Comparing retrieval performance of VRR and our RA-VQA models. The same knowledge corpus\n(GS-full) was used. P: Pseudo Relevance Precision; R: Pseudo Relevance Recall; EM: Exact Match. P under\nK = 5refers to PRPrec@5. VRR was trained on Ktrain = 100, while RA-VQAs were trained on Ktrain = 5.\nScore) is larger compared to RA-VQA-FrDPR\n(+0.56 VQA Score), showing that customizing the\ngeneration target brings more benefits when the re-\ntrieval is improved within our proposed RA-VQA\njoint training framework. This further confirms that\nthe two components, retrieval and answer genera-\ntion, complement each other bi-directionally.\nB.2 Retrieval Performance of RA-VQA\nIn addition to Pseudo Relevance Recall (PRRecall)\nintroduced in the paper, we further evaluate re-\ntrieval performance with Pseudo Relevance Preci-\nsion (PRPrec)@K, which is calculated as the rate\nof pseudo positive documents in all the K docu-\nments retrieved for a question:\nPRPrec@K = 1\nK\nK∑\nk=1\nH(zk,S) (15)\nwhere H(·) is the pseudo relevance function intro-\nduced in Sec. 3.2.\nThe success of our RA-VQA model can be fur-\nther explained by Table 3. As expected, RA-VQA-\nFrDPR (pre-trained DPR) achieves similar retrieval\nperformance as VRR (Luo et al., 2021) since they\nare both based on DPR and are trained with the\nsame pseudo-relevance-based labels. Our proposed\nRA-VQA, with a substantial improvement in Re-\ncall over RA-VQA-FrDPR (82.84 PRRecall@5 vs\n81.25 PRRecall@5), achieves significantly higher\nPrecision (57.39 PRPrec@5 vs 51.82 PRPrec@5).\nThis also yields substantial improvements to both\nEM (+3.05%) and VQA Score (+2.59%). This\nsuggests that training the retriever jointly presents\nmore potentially relevant documents to answer gen-\neration, improving the quality of the top-ranked\ndocuments.\nB.3 Effects of Retrieving More Documents in\nTest\nFig. 5 presents the change of VQA Score and\nPRRecall as additional documents are retrieved\nin test (increasing Ktest).\n5 10 20 50 80 100\nKtest\n50\n51\n52\n53\n54VQA Score\n51.2\n51.6\n53.0\n53.5\n53.8\n54.5\nRA-VQA-FrDPR (baseline)\nRA-VQA-NoPR (ablation)\nRA-VQA\n(a) VQA Score vs Ktest\n5 10 20 50 80 100\nKtest\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975Recall@5\nRA-VQA-FrDPR (baseline)\nRA-VQA-NoPR (ablation)\nRA-VQA\n(b) Recall vs Ktest\nFigure 5: Comparison of model performance as more\ndocuments are retrieved in testing. These models are\nall trained with Ktrain = 5. In RA-VQA full joint train-\ning (green), combining model predictions with pseudo\nrelevance labels yields higher PRRecall at low Ktest,\nshowing that full joint training improves retrieval; RA-\nVQA-NoPR (orange), which uses only model predic-\ntions in training, achieves a higher VQA Score with\nlower Pseudo Relevance Recall compared to the RA-\nVQA-FrDPR with frozen DPR in training (blue), which\nsuggests that Pseudo Relevance is only an approximate\nmeasurement of actual relevance.\nPRRecall is improved dramatically as Ktest in-\ncreases from 5 to 50, after which only marginal im-\nprovement is observed. Similarly, the VQA Score\nof these models is improved as more documents\nare presented in test, and the performance peaks\nat Ktest ∼50. This suggests that including more\n11250\nadditional documents in test is more likely to in-\nclude the truly relevant document to help answer\nthe question yet along with more distracting and\nmisleading documents.\nRA-VQA-NoPR (introduced in Sec. 4.1), which\nuses only model predictions in training to adjust\ndocument scores without pseudo relevance labels,\nyields a significantly lower PRRecall curve (orange\ncurve in Fig. 5(b)) than RA-VQA-FrDPR (blue\ncurve) across all Ktests, but achieves much higher\nVQA performance (Fig. 5(a)). This further con-\nfirms that Pseudo Relevance Labels are a weak\nsignal and a high PRRecall does not necessarily\nguarantee to gather truly relevant knowledge in\nretrieval.\nC Evaluation of Computational Cost\nModel GPU memory Wall time Batch Size\nRA VQA 70G 180 min 2 × 16\nRA VQA-FrDPR 67G 160 min 4 × 8\nTRiG* 70G 220 min 16 × 2\nTable 4: Comparing computational cost of models. GPU\nmemory: total GPU memory occupied in runtime (ap-\nproximately); Wall time: the required training time\nfor 8 epochs; Batch Size: per_device_batch_size×\ngradient_accumulation_steps. The statistics was\ncollected using one Nvidia A100 GPU (80G).\nAs shown in Fig. 4, comparing with the DPR\nbaseline, the wall time is increased by only 20\nmins for RA VQA joint training. Our method does\nnot significantly increase the computation needs.\nIn comparing with TRiG, the total training time is\nalso reduced; this is because TRiG concatenates\nall Khidden states for decoding (Gao et al., 2022),\nwhich is computationally expensive.\nModel Wall Time (8 epochs)\nRA VQAKtrain = 5 3 hrs\nRA VQAKtrain = 10 7 hrs\nRA VQAKtrain = 15 12 hrs\nTable 5: Wall time of the RA VQA model with an in-\ncreasing Ktrain. More time is required for training 8\nepochs as Ktrain increases.\nAs K increases, the DPR-based system takes\nsignificantly more time to train. This is a real issue\nfor other DPR-based systems that use very high\nKtrain (e.g. TRiG, KAT), but not the case for our\nframework: we verified in Sec.4.4.4 that K = 5\nachieves almost the same results asK ≥10, which\nis a desirable feature that can significantly reduce\nthe required computation cost for achieving the\nbest performance.\nD Random Guess with OK-VQA\nQuestions\nQuestion Prediction\nWhat type of bird is this? hawk\nWhat time of day is it? afternoon\nWhat kind of dog is this? chihuaha\nWhat kind of bird is this? hawk\nWhat sport is this? horse race\nWhat breed of horse is that? clydesdale\nWhat century is this? 19th\nWhat kind of birds are these? pigeon\nWhat city is this? new york\nWhat is the weather like? rainy\nWhat do these animals eat? grass\nWhat activity is this? skateboard\nHow long do these animals live? 20 years\nWhat type of train is this? passenger\nWhat is this used for? travel\nWhat is this room used for? sleep\nWhat kind of bird is that? hawk\nWhat food does the animal eat? cat food\nWhat type of dog is this? chihuaha\nWhat food do these animals eat? cat food\nWhat place is this? switzerland\nWhat breed of cat is this? calico\nWhat season is this? winter\nTable 6: Example of random guesses with only question\ninput. Random guess achieved a good VQA Score by\nmatching to the answers by chance. But the OK-VQA\nquestions are still not directly answerable without access\nto the associated images.\nFrom the feature ablation study we found that\nour RA-VQA-NoDPR achieved ∼28 VQA Score\nrelying on only questions. This is due to the fact\nthat ∼75% of answers to training questions ap-\npear in the answers to test questions. As shown\nin Table 6, for each distinct question, the model\nlearned to generate the same answer without access\nto the associated images. These random guesses\ncan match to the answers of some test questions by\nchance, leading to a good VQA Score. By inspec-\ntion we report that most of the successful cases are\nrandom guesses, and these questions are still not\ndirectly answerable without reading the associated\nimages.\nE Case Study\nWe present a case study in Fig. 6 to compare RA-\nVQA-FrDPR and our proposed RA-VQA frame-\nwork. Conclusions are provided to each case in the\nfigure.\n11251\nHow many teeth does this animal\nuse to have?\nRAVQA-Frozen\nRA-VQA sucessfully retrieved more relevant documents.\nan adult dog should have 42 teeth in total: that's 20 on top of their jaw and 22 on the bottom.Pred: 42\ncats have 30 teeth and dogs have 42. Pred: 30\nPred: 32\nwe humans have 32 pearly whites. horses have 44 chompers, dolphins can have as many as\n250 teeth and, it's hard to believe, but snails can have more than 20,000 tiny, very sharp teeth\n— located on their tongues \nRAVQA (ours)\ncats have 26 deciduous teeth and 30 permanent teeth.  Pred: 26\nthis is true. as dogs grow older, they will have 42 permanent teeth while cats will have 30. Pred: 30\nPred: 26cats have 26 deciduous teeth consisting of: 12 incisors; 4 canines; 10 premolars. permanent\nteeth begin to erupt from the age of 11-12 weeks. \nWhat position does the man with\nthe bat play?\nRAVQA-Frozen\nEven with the same retrieved document, RA-VQA learned to retrieve correct answers.\ncatcher is a position for a baseball or softball player. when a batter takes their turn to hit, the\ncatcher crouches behind home plate, in front of the (home) umpire, and receives the ball\nfrom the pitcher..... \nPred: Catcher\n.....\nRAVQA (ours)\ncatcher is a position for a baseball or softball player. when a batter takes their turn to hit, the\ncatcher crouches behind home plate, in front of the (home) umpire, and receives the ball\nfrom the pitcher...... \nPred: Batter\n.....\nWhat is the active ingredient in\nthis?\nRAVQA-Frozen\ni credit this “secret ingredient” for being the greatest offender in this recipe. Pred: toothpaste\ningredients on the list that end in “ose”—fructose, maltose, sucrose...... Pred: toothpaste\nPred: toothpastenatural cheese is made from only four ingredients: milk, salt, starter culture (good bacteria)\nand rennet (an enzyme) \nRAVQA (ours)\nactive ingredient sodium fluoride 0.21% (0.12% w/v fluoride ion) purpose anticavity\ntoothpaste use helps protect teeth and roots against cavities warnings keep out of reach of\nchildren under 6 years of age. \nPred: fluoride\ningredients. active ingredient - purpose. sodium fluoride (0. 24%) - anticavity toothpaste.\ninactive ingredients: sorbitol, water, hydrated silica, peg-32, sodium lauryl sulfate, ...... Pred: fluoride\nPred: toothpastefluoride-containing compounds in the form of sodium monofluorophosphate, sodium\nfluoride and stannous fluoride are used as anticaries agents in toothpastes. \nRetrieval was improved by RA-VQA, which leads to sucessful answering with given knowledge.\nQuestions about \"ingredient\" are common in food domain, and misleading material may be\npresented by Pseudo Relevance Labels to Answer Generator, leading to failed answering.\nPred: answer Prediction (correct / wrong)\nwith the given knowledge\nPred: answer Selected as final prediction\nNotations \nFigure 6: A case study comparing RA-VQA-FrDPR (baseline) and our RA-VQA that benefits from joint training of\nretrieval and answer generation.\n11252\nF Generalizing RA VQA to Other Datasets\nWe are also interested in whether this approach is\nalso generalisable to other similar VQA tasks that\nmay benefit from improved passage retrieval.\nWe implement our framework on another\nknowledge-based VQA task, Fact-based VQA\n(FVQA) (Wang et al., 2017). This dataset con-\ntains commonsense factoid VQA questions, such as\n“Question: which object in the image can cut you?\nAnswer: the knife”. In contrast to OKVQA where\nno knowledge base is provided, FVQA grounds\neach question-answer pair with a fact (a triplet from\nseveral ‘common sense’ knowledge bases, includ-\ning ConceptNet (Speer et al., 2017), Webchild (Tan-\ndon et al., 2017), and DBpedia (Auer et al., 2007)).\nA triplet contains a head node, a relation, and a tail\nnode (e.g. [Car] /r/HasA [4 wheels]). To cope with\npassage retrieval, these knowledge triplets are flat-\ntened into surface texts (e.g. “[car] has [4 wheels]”)\nsuch that DPR can be directly applied to retrieve\nthem. We replace pseudo relevance with ground-\ntruth relevance since relevant triplets for answering\nquestions are given.\nThe metrics used for assessing performance are\nAccuracy and Recall, with their standard devia-\ntions of 5 splits. Accuracy counts the portion of\nquestions that are successfully answered, while\nRecall@K measures how likely the retrieved K\nknowledge triplets contain the answer node. Since\nFVQA was designed for answer selection instead\nof open-ended answer generation, prior works used\naccuracy as “whether the answer node is success-\nfully selected from all KG nodes”. To enable fair\nevaluation with our open-ended framework, in cal-\nculating accuracy, a question is considered success-\nfully answered if the answer node is the closest\nnode to the generated answer string (shortest in\nLevenshtein distance). For example, the generated\nanswer ‘knives’ is still a valid answer since the an-\nswer node ‘[knife]’ can be matched with a shortest\nLevenshtein distance.\nThe significance of performance is guaranteed\nby reporting the average of 5 splits (as in the official\nFVQA evaluation). In total we trained 5 DPR mod-\nels and 5 ×3 models (RA VQA, RA VQA-FrDPR,\nand RA VQA-NoDPR) with the same hyperparam-\neters. Each split has approximately half questions\nfor training and the remaining for testing.\nWe compare with three systems in prior work:\n(1) FVQA (Wang et al., 2017): the baseline sys-\ntem provided in the official FVQA dataset paper.\n(2) GCN (Narasimhan et al., 2018b): a\nmodel that leverages graph convolutional net-\nworks (GCNs) to aggregate features from vi-\nsual/language/fact modalities.\n(3) Mucko (Zhu et al., 2020): the current state-\nof-the-art system that uses GCNs to combine visual,\nfact, and semantic graphs.\nModel Accuracy (Std.) Recall@5 (Std.)\nMucko 73.06 ( – ) -\nRA VQA 69.88 (0.13) 68.77 (0.87)\nGCN 69.35 ( – ) -\nRA VQA-FrDPR 68.81 (0.59) 64.54 (0.80)\nRA VQA-NoDPR 67.93 (0.82) -\nFVQA 58.76 (0.92) -\nTable 7: Model performance on the FVQA dataset\n(sorted by accuracy). Our proposed systems are in bold.\nAs shown in Table 7, RA VQA-NoDPR achieves\nan already strong result (67.93% accuracy) com-\npared to early work in FVQA, showing that the\nextracted vision-to-language features are useful\nand text-based Transformers are able to learn to\nanswer commonsense VQA questions well with-\nout accessing the provided knowledge graph (Con-\nceptNet). The incorporation of DPR boosts the\nperformance to 68.81% with 64.54% Recall@5,\nshowing that retrieval works as expected and the re-\ntrieved knowledge triplets are exploited in answer\ngeneration. The joint training scheme improves the\nretrieval (64.54% to 68.77% Recall@5) as well as\nthe overall performance (68.81% to 69.88% Accu-\nracy). This demonstrates that our proposed joint\ntraining framework is generalizable to other KB-\nVQA tasks, though the passages used in retrieval\nare simply flattened surface texts of KG triplets.\nIn comparing with other systems in the FVQA\nbenchmark, our best system ranks second without\nan explicit design for leveraging KG structures.\nThis shows the power of open-ended answer gen-\neration with text-based Transformers. But we em-\nphasise that better performance could be achieved\nthrough designing a more specialised retrieval com-\nponent for the structured knowledge base used in\nthis task.\nTo summarise, our system shows great gener-\nalizability in an external KB-VQA task that was\nconstructed very differently. Therefore, the pro-\nposed framework can serve as a strong basis for\nfuture improvements.\n11253\nModel T5 GPT-3 Ktrain Ktest Knowl. Src. PRRecall EM VQA Score\nConceptBERT × × - - C 33.66\nKRISP × × - - C + W 38.35\nVRR × × 100 100 GS (trn/full) 39.22 / 45.08\nMA VEx × × - - W + C + GI 39.40\nKAT-T5 ✓ × 40 40 W 44.25\nTRiG ✓ × 5 5 W 49.21 45.51\nTRiG ✓ × 100 100 W 53.59 49.35\nTRiG-Ensemble ✓ × 100 100 W 54.73 50.50\nTRiG* ✓ × 5 5 GS (trn/full) 52.79 45.11 / 48.32\nRAG* ✓ × 5 5 GS (trn/full) 82.34 52.52 44.90 / 48.22\nRA-VQA (Ours) ✓ × 5 5 GS (trn/full) 82.84 58.72 48.77 / 53.81\nRA-VQA (Ours) ✓ × 5 50 GS (trn/full) 96.55 59.41 49.24 / 54.48\nAblation Study\nRA-VQA-FrDPR ✓ × 5 5 GS (trn/full) 81.25 55.77 47.05 / 51.22\nRA-VQA-NoPR ✓ × 5 5 GS (full) 77.67 57.80 52.98\nRA-VQA-FrDPR-NoCT ✓ × 5 5 GS (full) 81.25 54.99 50.66\nRA-VQA-NoCT ✓ × 5 5 GS (full) 83.77 57.51 52.67\nGPT-3-based Systems (>175 Billion Parameters)\nPICa × ✓ - - GPT-3 48.00\nKAT-Knowledge-T5 ✓ ✓ 40 40 W + GPT-3 51.97\nKAT-Ensemble ✓ ✓ 40 40 W + GPT-3 54.41\nTable 8: Full table of RA-VQA vs Baseline Systems. Knowledge Sources: ConceptNet; Wikipedia; Google Search;\nGoogle Images; GPT-3 closed book knowledge. ‘GS (trn/full)’ indicates if the Google Search data is constrained to\ncontain only documents relevant to training questions. PRRecall, HSR, FSR, and EM are reported on ‘GS (full)’\nsystems as percentage (%). PRRecall is reported at the corresponding Ktest.\n11254",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8799917697906494
    },
    {
      "name": "Computer science",
      "score": 0.8278272151947021
    },
    {
      "name": "Information retrieval",
      "score": 0.6153052449226379
    },
    {
      "name": "Task (project management)",
      "score": 0.610495388507843
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.5906782150268555
    },
    {
      "name": "Limit (mathematics)",
      "score": 0.4368270933628082
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.426802396774292
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37020450830459595
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}