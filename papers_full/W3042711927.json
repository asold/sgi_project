{
  "title": "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training",
  "url": "https://openalex.org/W3042711927",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3163607453",
      "name": "Chi, Zewen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097573093",
      "name": "Dong Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2389670735",
      "name": "Wei, Furu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979826969",
      "name": "Yang Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224447770",
      "name": "Singhal, Saksham",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1496033914",
      "name": "Wang Wenhui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108846051",
      "name": "Song Xia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221500202",
      "name": "Mao, Xian-Ling",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2374814473",
      "name": "Huang Heyan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102363648",
      "name": "Zhou Ming",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3095349973",
    "https://openalex.org/W3035058308",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2960374072",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W1993284418",
    "https://openalex.org/W2949517790",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2995040292",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W2066873261",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2964184826",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2971155163",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2166944917",
    "https://openalex.org/W3014635508",
    "https://openalex.org/W2951873722",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2944828972",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3016973796",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2917551568",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W3023063853",
    "https://openalex.org/W2963877297",
    "https://openalex.org/W3039695075",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2799518931",
    "https://openalex.org/W3034469191"
  ],
  "abstract": "In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm.",
  "full_text": "INFO XLM: An Information-Theoretic Framework for\nCross-Lingual Language Model Pre-Training\nZewen Chi†‡∗, Li Dong‡, Furu Wei‡, Nan Yang‡, Saksham Singhal‡, Wenhui Wang‡\nXia Song‡, Xian-Ling Mao†, Heyan Huang†, Ming Zhou‡\n†Beijing Institute of Technology\n‡Microsoft Corporation\n{czw,maoxl,hhy63}@bit.edu.cn\n{lidong1,fuwei,nanya,saksingh,wenwan,xiaso}@microsoft.com\nAbstract\nIn this work, we present an information-\ntheoretic framework that formulates cross-\nlingual language model pre-training as\nmaximizing mutual information between\nmultilingual-multi-granularity texts. The\nuniﬁed view helps us to better understand the\nexisting methods for learning cross-lingual\nrepresentations. More importantly, inspired\nby the framework, we propose a new pre-\ntraining task based on contrastive learning.\nSpeciﬁcally, we regard a bilingual sentence\npair as two views of the same meaning and\nencourage their encoded representations to\nbe more similar than the negative examples.\nBy leveraging both monolingual and parallel\ncorpora, we jointly train the pretext tasks to\nimprove the cross-lingual transferability of\npre-trained models. Experimental results on\nseveral benchmarks show that our approach\nachieves considerably better performance.\nThe code and pre-trained models are available\nat https://aka.ms/infoxlm.\n1 Introduction\nLearning cross-lingual language representations\nplays an important role in overcoming the language\nbarrier of NLP models. The recent success of cross-\nlingual language model pre-training (Devlin et al.,\n2019; Conneau and Lample, 2019; Conneau et al.,\n2020a; Chi et al., 2020; Liu et al., 2020) signif-\nicantly improves the cross-lingual transferability\nin various downstream tasks, such as cross-lingual\nclassiﬁcation, and question answering.\nState-of-the-art cross-lingual pre-trained mod-\nels are typically built upon multilingual masked\nlanguage modeling (MMLM; Devlin et al. 2019;\nConneau et al. 2020a), and translation language\nmodeling (TLM; Conneau and Lample 2019). The\ngoal of both pretext tasks is to predict masked to-\nkens given input context. The difference is that\n∗Contribution during internship at Microsoft Research.\nContact person: Li Dong and Furu Wei.\nMMLM uses monolingual text as input, while TLM\nfeeds bilingual parallel sentences into the model.\nEven without explicit encouragement of learning\nuniversal representations across languages, the de-\nrived models have shown promising abilities of\ncross-lingual transfer.\nIn this work, we formulate cross-lingual pre-\ntraining from a uniﬁed information-theoretic per-\nspective. Following the mutual information maxi-\nmization principle (Hjelm et al., 2019; Kong et al.,\n2020), we show that the existing pretext tasks can\nbe viewed as maximizing the lower bounds of\nmutual information between various multilingual-\nmulti-granularity views.\nSpeciﬁcally, MMLM maximizes mutual infor-\nmation between the masked tokens and the con-\ntext in the same language while the anchor points\nacross languages encourages the correlation be-\ntween cross-lingual contexts. Moreover, we present\nthat TLM can maximize mutual information be-\ntween the masked tokens and the parallel context,\nwhich implicitly aligns encoded representations\nof different languages. The uniﬁed information-\ntheoretic framework also inspires us to propose\na new cross-lingual pre-training task, named as\ncross-lingual contrast (XLCO). The model learns\nto distinguish the translation of an input sentence\nfrom a set of negative examples. In comparison\nto TLM that maximizes token-sequence mutual\ninformation, XLCO maximizes sequence-level mu-\ntual information between translation pairs which\nare regarded as cross-lingual views of the same\nmeaning. We employ the momentum contrast (He\net al., 2020) to realize XLCO. We also propose\nthe mixup contrast and conduct the contrast on the\nuniversal layer to further facilitate the cross-lingual\ntransferability.\nUnder the presented framework, we develop a\ncross-lingual pre-trained model ( INFO XLM ) to\nleverage both monolingual and parallel corpora.\nWe jointly train INFO XLM with MMLM, TLM\narXiv:2007.07834v2  [cs.CL]  7 Apr 2021\nand XLCO. We conduct extensive experiments on\nseveral cross-lingual understanding tasks, includ-\ning cross-lingual natural language inference (Con-\nneau et al., 2018), cross-lingual question answer-\ning (Lewis et al., 2020), and cross-lingual sentence\nretrieval (Artetxe and Schwenk, 2019). Experimen-\ntal results show that INFO XLM outperforms strong\nbaselines on all the benchmarks. Moreover, the\nanalysis indicates that INFO XLM achieves better\ncross-lingual transferability.\n2 Related Work\n2.1 Cross-Lingual LM Pre-Training\nMultilingual BERT (mBERT; Devlin et al. 2019)\nis pre-trained with the multilingual masked lan-\nguage modeling (MMLM) task on the monolingual\ntext. mBERT produces cross-lingual representa-\ntions and performs cross-lingual tasks surprisingly\nwell (Wu and Dredze, 2019). XLM (Conneau and\nLample, 2019) extends mBERT with the translation\nlanguage modeling (TLM) task so that the model\ncan learn cross-lingual representations from par-\nallel corpora. Unicoder (Huang et al., 2019) tries\nseveral pre-training tasks to utilize parallel corpora.\nALM (Yang et al., 2020) extends TLM to code-\nswitched sequences obtained from translation pairs.\nXLM-R (Conneau et al., 2020a) scales up MMLM\npre-training with larger corpus and longer training.\nLaBSE (Feng et al., 2020) learns cross-lingual sen-\ntence embeddings by an additive translation rank-\ning loss.\nIn addition to learning cross-lingual encoders,\nseveral pre-trained models focus on generation.\nMASS (Song et al., 2019) and mBART (Liu et al.,\n2020) pretrain sequence-to-sequence models to im-\nprove machine translation. XNLG (Chi et al., 2020)\nfocuses on the cross-lingual transfer of language\ngeneration, such as cross-lingual question genera-\ntion, and abstractive summarization.\n2.2 Mutual Information Maximization\nVarious methods have successfully learned visual\nor language representations by maximizing mutual\ninformation between different views of input. It is\ndifﬁcult to directly maximize mutual information.\nIn practice, the methods resort to a tractable lower\nbound as the estimator, such as InfoNCE (Oord\net al., 2018), and the variational form of the KL\ndivergence (Nguyen et al., 2010). The estimators\nare also known as contrastive learning (Arora et al.,\n2019) that measures the representation similarities\nbetween the sampled positive and negative pairs.\nIn addition to the estimators, various view pairs are\nemployed in these methods. The view pair can be\nthe local and global features of an image (Hjelm\net al., 2019; Bachman et al., 2019), the random\ndata augmentations of the same image (Tian et al.,\n2019; He et al., 2020; Chen et al., 2020), or differ-\nent parts of a sequence (Oord et al., 2018; Henaff,\n2020; Kong et al., 2020). Kong et al. (2020) show\nthat learning word embeddings or contextual em-\nbeddings can also be uniﬁed under the framework\nof mutual information maximization.\n3 Information-Theoretic Framework for\nCross-Lingual Pre-Training\nIn representation learning, the learned representa-\ntions are expected to preserve the information of\nthe original input data. However, it is intractable\nto directly model the mutual information between\nthe input data and the representations. Alterna-\ntively, we can maximize the mutual information\nbetween the representations from different views\nof the input data, e.g., different parts of a sentence,\na translation pair of the same meaning.\nIn this section, we start from a uniﬁed\ninformation-theoretic perspective, and formulate\ncross-lingual pre-training with the mutual infor-\nmation maximization principle. Then, under the\ninformation-theoretic framework, we propose a\nnew cross-lingual pre-training task, named as cross-\nlingual contrast (XLCO). Finally, we present the\npre-training procedure of our INFO XLM.\n3.1 Multilingual Masked Language Modeling\nThe goal of multilingual masked language mod-\neling (MMLM; Devlin et al. 2019) is to recover\nthe masked tokens from a randomly masked se-\nquence. For each input sequence of MMLM, we\nsample a text from the monolingual corpus for pre-\ntraining. Let (c1, x1) denote a monolingual text\nsequence, where x1 is the masked token, and c1\nis the corresponding context. Intuitively, we need\nto maximize their dependency (i.e., I(c1; x1)), so\nthat the context representations are predictive for\nmasked tokens (Kong et al., 2020).\nFor the example pair (c1, x1), we construct a set\nNthat contains x1 and |N|− 1 negative samples\ndrawn from a proposal distribution q. According to\nthe InfoNCE (Oord et al., 2018) lower bound, we\nhave:\nI(c1; x1)\n⩾ E\nq(N)\n[\nlog fθ(c1, x1)∑\nx′∈Nfθ(c1, x′)\n]\n+ log|N| (1)\nwhere fθ is a function that scores whether the input\nc1 and x1 is a positive pair.\nGiven context c1, MMLM learns to minimize\nthe cross-entropy loss of the masked token x1:\nLMMLM = −log exp(gθT (c1)⊤gθE (x1))∑\nx′∈Vexp(gθT (c1)⊤gθE (x′))\n(2)\nwhere Vis the vocabulary, gθE is a look-up func-\ntion that returns the token embeddings, gθT is\na Transformer that returns the ﬁnal hidden vec-\ntors in position of x1. According to Equation (1)\nand Equation (2), if N = Vand fθ(c1, x1) =\nexp(gθT (c1)⊤gθE (x1)), we can ﬁnd that MMLM\nmaximizes a lower bound of I(c1; x1).\nNext, we explain why MMLM can implicitly\nlearn cross-lingual representations. Let (c2, x2)\ndenote a MMLM instance that is in different lan-\nguage as (c1, x1). Because the vocabulary, the po-\nsition embedding, and special tokens are shared\nacross languages, it is common to ﬁnd anchor\npoints (Pires et al., 2019; Dufter and Sch¨utze, 2020)\nwhere x1 = x2 (such as subword, punctuation, and\ndigit) or I(x1, x2) is positive (i.e., the representa-\ntions are associated or isomorphic). With the bridge\neffect of {x1, x2}, MMLM obtains a v-structure de-\npendency “c1 →{x1, x2}← c2”, which leads to a\nnegative co-information (i.e., interaction informa-\ntion) I(c1; c2; {x1, x2}) (Tsujishita, 1995). Specif-\nically, the negative value ofI(c1; c2; {x1, x2}) in-\ndicates that the variable {x1, x2}enhances the cor-\nrelation between c1 and c2 (Fano, 1963).\nIn summary, although MMLM learns to maxi-\nmize I(c1, x1) and I(c2, x2) in each language, we\nargue that the task encourages the cross-lingual\ncorrelation of learned representations. Notice that\nfor the setting without word-piece overlap (Artetxe\net al., 2020; Conneau et al., 2020b; K et al., 2020),\nwe hypothesize that the information bottleneck\nprinciple (Tishby and Zaslavsky, 2015) tends to\ntransform the cross-lingual structural similarity\ninto isomorphic representations, which has sim-\nilar bridge effects as the anchor points. Then we\ncan explain how the cross-lingual ability is spread\nout as above. We leave more discussions about the\nsetting without word-piece overlap for future work.\n3.2 Translation Language Modeling\nSimilar to MMLM, the goal of translation language\nmodeling (TLM; Conneau and Lample 2019) is\nalso to predict masked tokens, but the prediction is\nconditioned on the concatenation of a translation\npair. We try to explain how TLM pre-training en-\nhances cross-lingual transfer from an information-\ntheoretic perspective.\nLet c1 and c2 denote a translation pair of sen-\ntences, and x1 a masked token taken in c1. So c1\nand x1 are in the same language, while c1 and c2\nare in different ones. Following the derivations of\nMMLM in Section 3.1, the objective of TLM is\nmaximizing the lower bound of mutual informa-\ntion I(c1, c2; x1). By re-writing the above mutual\ninformation, we have:\nI(c1, c2; x1) =I(c1; x1) +I(c2; x1|c1) (3)\nThe ﬁrst term I(c1; x1) corresponds to MMLM,\nwhich learns to use monolingual context. In con-\ntrast, the second term I(c2; x1|c1) indicates cross-\nlingual mutual information between c2 and x1 that\nis not included by c1. In other words, I(c2; x1|c1)\nencourages the model to predict masked tokens by\nusing the context in a different language. In con-\nclusion, TLM learns to utilize the context in both\nlanguages, which implicitly improves the cross-\nlingual transferability of pre-trained models.\n3.3 Cross-Lingual Contrastive Learning\nInspired by the uniﬁed information-theoretic frame-\nwork, we propose a new cross-lingual pre-training\ntask, named as cross-lingual contrast (XLCO). The\ngoal of XLCO is to maximize mutual information\nbetween the representations of parallel sentences\nc1 and c2, i.e., I(c1, c2). Unlike maximizing token-\nsequence mutual information in MMLM and TLM,\nXLCO targets at cross-lingual sequence-level mu-\ntual information.\nWe describe how the task is derived as follows.\nUsing InfoNCE (Oord et al., 2018) as the lower\nbound, we have:\nI(c1; c2) ⩾ E\nq(N)\n[\nlog fθ(c1, c2)∑\nc′∈Nfθ(c1, c′)\n]\n+ log|N|\n(4)\nwhere Nis a set that contains the positive pair c2\nand |N|−1 negative samples. In order to maximize\nthe lower bound of I(c1; c2), we need to design the\nfunction fθ that measures the similarity between\nthe input sentence and the proposal distribution\nq(N). Speciﬁcally, we use the following similarity\nfunction fθ:\nfθ(c1, c2) = exp(gθ(c1)⊤gθ(c2)) (5)\nwhere gθ is the Transformer encoder that we are\npre-training. Following (Devlin et al., 2019), a\nspecial token [CLS] is added to the input, whose\nhidden vector is used as the sequence representa-\ntion. Additionally, we use a linear projection head\nafter the encoder in gθ.\nMomentum Contrast Another design choice is\nhow to construct N. As shown in Equation (4),\na large |N| improves the tightness of the lower\nbound, which has been proven to be critical for\ncontrastive learning (Chen et al., 2020).\nIn our work, we employ the momentum con-\ntrast (He et al., 2020) to construct the set N, where\nthe previously encoded sentences are progressively\nreused as negative samples. Speciﬁcally, we con-\nstruct two encoders with the same architecture\nwhich are the query encoder gθQ and the key en-\ncoder gθK . The loss function of X LCO is:\nLXLCO = −log exp(gθQ(c1)⊤gθK (c2))∑\nc′∈Nexp(gθQ(c1)⊤gθK (c′))\n(6)\nDuring training, the query encoder gθQ encodes\nc1 and is updated by backpropagation. The key\nencoder gθK encodes Nand is learned with mo-\nmentum update (He et al., 2020) towards the query\nencoder. The negative examples in N are orga-\nnized as a queue, where a newly encoded example\nis added while the oldest one is popped from the\nqueue. We initialize the query encoder and the\nkey encoder with the same parameters, and pre-ﬁll\nthe queue with a set of encoded examples until it\nreaches the desired size |N|. Notice that the size\nof the queue remains constant during training.\nMixup Contrast For each pair, we concatenate\nit with a randomly sampled translation pair from\nanother parallel corpus. For example, consider\nthe pairs ⟨c1, c2⟩and ⟨d1, d2⟩sampled from two\ndifferent parallel corpora. The two pairs are con-\ncatenated in a random order, such as ⟨c1d1, c2d2⟩,\nand ⟨c1d2, d1c2⟩. The data augmentation of mixup\nencourages pre-trained models to learn sentence\nboundaries and to distinguish the order of multilin-\ngual texts.\nContrast on Universal LayerAs a pre-training\ntask maximizing the lower bound of sequence-\nlevel mutual information, XLCO is usually jointly\nlearned with token-sequence tasks, such as\nMMLM, and TLM. In order to make XLCO more\ncompatible with the other pretext tasks, we propose\nto conduct contrastive learning on the most univer-\nsal (or transferable) layer in terms of MMLM and\nTLM.\nIn our implementations, we instead use the hid-\nden vectors of [CLS] at layer 8 to perform con-\ntrastive learning for base-size (12 layers) models,\nand layer 12 for large-size (24 layers) models. Be-\ncause previous analysis (Sabet et al., 2020; Dufter\nand Sch¨utze, 2020; Conneau et al., 2020b) shows\nthat the speciﬁc layers of MMLM learn more uni-\nversal representations and work better on cross-\nlingual retrieval tasks than other layers. We choose\nthe layers following the same principle.\nThe intuition behind the method is that MMLM\nand TLM encourage the last layer to produce\nlanguage-distinguishable token representations be-\ncause of the masked token classiﬁcation. But\nXLCO tends to learn similar representations across\nlanguages. So we do not directly use the hidden\nstates of the last layer in XLCO.\n3.4 Cross-Lingual Pre-Training\nWe pretrain a cross-lingual model INFO XLM by\njointly maximizing the lower bounds of three\ntypes of mutual information, including monolin-\ngual token-sequence mutual information (MMLM),\ncross-lingual token-sequence mutual information\n(TLM), and cross-lingual sequence-level mutual\ninformation (XLCO). Formally, the loss of cross-\nlingual pre-training in INFO XLM is deﬁned as:\nL= LMMLM + LTLM + LXLCO (7)\nwhere we apply the same weight for the loss terms.\nBoth TLM and XLCO use parallel data. The\nnumber of bilingual pairs increases with the square\nof the number of languages. In our work, we set\nEnglish as the pivot language following (Conneau\nand Lample, 2019), i.e., we only use the parallel\ncorpora that contain English.\nIn order to balance the data size between high-\nresource and low-resource languages, we apply a\nmultilingual sampling strategy (Conneau and Lam-\nple, 2019) for both monolingual and parallel data.\nAn example in the language l is sampled with the\nprobability pl ∝(nl/n)0.7, where nl is the number\nof instances in the language l, and n refers to the\ntotal number of data. Empirically, the sampling\nalgorithm alleviates the bias towards high-resource\nlanguages (Conneau et al., 2020a).\n4 Experiments\nIn this section, we ﬁrst present the training conﬁg-\nuration of INFO XLM . Then we compare the ﬁne-\ntuning results of INFO XLM with previous work on\nthree cross-lingual understanding tasks. We also\nconduct ablation studies to understand the major\ncomponents of INFO XLM.\n4.1 Setup\nCorpus We use the same pre-training corpora\nas previous models (Conneau et al., 2020a; Con-\nneau and Lample, 2019). Speciﬁcally, we recon-\nstruct CC-100 (Conneau et al., 2020a) for MMLM,\nwhich remains 94 languages by ﬁltering the lan-\nguage code larger than 0.1GB. Following (Con-\nneau and Lample, 2019), for the TLM and XLCO\ntasks, we employ 14 language pairs of parallel data\nthat involves English. We collect translation pairs\nfrom MultiUN (Ziemski et al., 2016), IIT Bom-\nbay (Kunchukuttan et al., 2018), OPUS (Tiede-\nmann, 2012), and WikiMatrix (Schwenk et al.,\n2019). The size of parallel corpora is about 42GB.\nMore details about the pre-training data are de-\nscribed in the appendix.\nModel Size We follow the model conﬁgurations\nof XLM-R (Conneau et al., 2020a). For the Trans-\nformer (Vaswani et al., 2017) architecture, we use\n12 layers and 768 hidden states for INFO XLM (i.e.,\nbase size), and 24 layers and 1,024 hidden states\nfor INFO XLM LARGE (i.e., large size).\nHyperparameters We initialize the parameters\nof INFO XLM with XLM-R. We optimize the\nmodel with Adam (Kingma and Ba, 2015) using\na batch size of 2048 for a total of 150K steps for\nINFO XLM, and 200K steps for I NFO XLM LARGE .\nThe same number of training examples are fed to\nthree tasks. The learning rate is scheduled with a\nlinear decay with 10K warmup steps, where the\npeak learning rate is set as 0.0002 for INFO XLM ,\nand 0.0001 for INFO XLM LARGE . The momen-\ntum coefﬁcient is set as 0.9999 and 0.999 for IN-\nFOXLM and INFO XLM LARGE , respectively. The\nlength of the queue is set as 131, 072. The train-\ning procedure takes about 2.3 days ×2 Nvidia\nDGX-2 stations for INFO XLM , and 5 days ×16\nNvidia DGX-2 stations for INFO XLM LARGE . De-\ntails about the pre-training hyperparameters can be\nfound in the appendix.\n4.2 Evaluation\nWe conduct experiments over three cross-lingual\nunderstanding tasks, i.e., cross-lingual natural lan-\nguage inference, cross-lingual sentence retrieval,\nand cross-lingual question answering.\nCross-Lingual Natural Language Inference\nThe Cross-Lingual Natural Language Inference cor-\npus (XNLI; Conneau et al. 2018) is a widely used\ncross-lingual classiﬁcation benchmark. The goal\nof NLI is to identify the relationship of an input\nsentence pair. We evaluate the models under the\nfollowing two settings. (1) Cross-Lingual Transfer:\nﬁne-tuning the model with English training set and\ndirectly evaluating on multilingual test sets. (2)\nTranslate-Train-All: ﬁne-tuning the model with the\nEnglish training data and the pseudo data that are\ntranslated from English to the other languages.\nCross-Lingual Sentence Retrieval The goal of\nthe cross-lingual sentence retrieval task is to extract\nparallel sentences from bilingual comparable cor-\npora. We use the subset of 36 language pairs of the\nTatoeba dataset (Artetxe and Schwenk, 2019) for\nthe task. The dataset is collected from Tatoeba 1,\nwhich is an open collection of multilingual parallel\nsentences in more than 300 languages. Follow-\ning (Hu et al., 2020), we use the averaged hidden\nvectors in the seventh Transformer layer to com-\npute cosine similarity for sentence retrieval.\nCross-Lingual Question Answering We\nuse the Multilingual Question Answering\n(MLQA; Lewis et al. 2020) dataset for the cross-\nlingual QA task. MLQA provides development\nand test data in seven languages in the format of\nSQuAD v1.1 (Rajpurkar et al., 2016). We follow\nthe ﬁne-tuning method introduced in (Devlin et al.,\n2019) that concatenates the question-passage pair\nas the input.\n4.3 Results\nWe compare INFO XLM with the following pre-\ntrained Transformer models: (1) Multilingual\nBERT (MBERT; Devlin et al. 2019) is pre-trained\nwith MMLM on Wikipedia in 102 languages; (2)\nXLM (Conneau and Lample, 2019) pretrains both\nMMLM and TLM tasks on Wikipedia in 100\n1https://tatoeba.org/eng/\nModels #M en fr es de el bg ru tr ar vi th zh hi sw ur Avg\nFine-tune multilingual model on English training set (Cross-lingual Transfer)\nMBERT* N 82.1 73.8 74.3 71.1 66.4 68.9 69.0 61.6 64.9 69.5 55.8 69.3 60.0 50.4 58.0 66.3\nXLM (w/o TLM)* N 83.7 76.2 76.6 73.7 72.4 73.0 72.1 68.1 68.4 72.0 68.2 71.5 64.5 58.0 62.4 71.3\nXLM* N 85.0 78.7 78.9 77.8 76.6 77.4 75.3 72.5 73.1 76.1 73.2 76.5 69.6 68.4 67.3 75.1\nXLM (w/o TLM)* 1 83.2 76.7 77.7 74.0 72.7 74.1 72.7 68.7 68.6 72.9 68.9 72.5 65.6 58.2 62.4 70.7\nUNICODER 1 85.4 79.2 79.8 78.2 77.3 78.5 76.7 73.8 73.9 75.9 71.8 74.7 70.1 67.4 66.3 75.3\nXLM-R* 1 85.8 79.7 80.7 78.7 77.5 79.6 78.1 74.2 73.8 76.5 74.6 76.7 72.4 66.5 68.3 76.2\nXLM-R (reimpl) 1 84.7 79.1 79.4 77.4 76.6 78.4 76.0 73.5 72.6 75.5 73.0 74.5 71.0 65.7 67.6 75.0\nINFOXLM 1 86.4 80.3 80.9 79.3 77.8 79.3 77.6 75.6 74.2 77.1 74.6 77.0 72.2 67.5 67.3 76.5\n−XLCO 1 86.5 80.5 80.3 78.7 77.3 78.8 77.4 74.6 73.8 76.8 73.7 76.7 71.8 66.3 66.4 76.0\nXLM-RLARGE* 1 89.1 84.1 85.1 83.9 82.9 84.0 81.2 79.6 79.8 80.8 78.1 80.2 76.9 73.9 73.8 80.9\nXLM-RLARGE(reimpl) 1 88.9 83.6 84.8 83.1 82.4 83.7 80.7 79.2 79.0 80.4 77.8 79.8 76.8 72.7 73.3 80.4\nINFOXLMLARGE 1 89.7 84.5 85.5 84.1 83.4 84.2 81.3 80.9 80.4 80.8 78.9 80.9 77.9 74.8 73.781.4\nFine-tune multilingual model on all training sets (Translate-Train-All)\nXLM (w/o TLM)* 1 84.5 80.1 81.3 79.3 78.6 79.4 77.5 75.2 75.6 78.3 75.7 78.3 72.1 69.2 67.7 76.9\nXLM* 1 85.0 80.8 81.3 80.3 79.1 80.9 78.3 75.6 77.6 78.5 76.0 79.5 72.9 72.8 68.5 77.8\nXLM-R* 1 85.4 81.4 82.2 80.3 80.4 81.3 79.7 78.6 77.3 79.7 77.9 80.2 76.1 73.1 73.0 79.1\nXLM-R (reimpl) 1 85.0 81.0 81.9 80.6 79.7 81.4 79.5 77.7 77.3 79.5 77.5 79.1 75.3 72.2 70.9 78.6\nINFOXLM 1 86.5 82.6 83.0 82.3 81.3 82.4 80.6 79.5 78.9 81.0 78.9 80.7 77.8 73.3 71.6 80.0\nTable 1: Evaluation results on XNLI cross-lingual natural language inference. We report test accuracy in 15\nlanguages. The model number #M=N indicates the model selection is done on each language’s validation set (i.e.,\neach language has a different model), while #M=1 means only one model is used for all languages. Results with\n“*” are taken from Conneau et al. (2020a). “(reimpl)” is our reimplementation of ﬁne-tuning, which is the same\nas I NFO XLM. Results of I NFO XLM and XLM-R (reimpl) are averaged over ﬁve runs. “ −XLCO” is the model\nwithout cross-lingual contrast.\nlanguages; (3) XLM-R (Conneau et al., 2020a)\nscales up MMLM to the large CC-100 corpus\nin 100 languages with much more training steps;\n(4) UNICODER (Liang et al., 2020) continues\ntraining XLM-R with MMLM and TLM. (5) IN-\nFOXLM−XLCO continues training XLM-R with\nMMLM and TLM, using the same pre-training\ndatasets with INFO XLM.\nCross-Lingual Natural Language Inference\nTable 1 reports the classiﬁcation accuracy on each\ntest of XNLI under the above evaluation settings.\nThe ﬁnal scores on test set are averaged over ﬁve\nrandom seeds. INFO XLM outperforms all base-\nline models on the two evaluation settings of XNLI.\nIn the cross-lingual transfer setting, INFO XLM\nachieves 76.5 averaged accuracy, outperforming\nXLM-R (reimpl) by 1.5. Similar improvements can\nbe observed for large-size models. Moreover, the\nablation results “−XLCO” show that cross-lingual\ncontrast is helpful for zero-shot transfer in most\nlanguages. We also ﬁnd that INFO XLM improves\nthe results in the translate-train-all setting.\nCross-Lingual Sentence Retrieval In Table 2\nand Table 3, we report the top-1 accuracy scores of\ncross-lingual sentence retrieval with the base-size\nmodels. The evaluation results demonstrate that\nINFO XLM produces better aligned cross-lingual\nsentence representations. On the 14 language pairs\nthat are covered by parallel data, INFO XLM ob-\ntains 77.8 and 80.6 averaged top-1 accuracies in\nthe directions of xx →en and en →xx, outper-\nforming XLM-R by 20.2 and 21.1. Even on the\n22 language pairs that are not covered by parallel\ndata, INFO XLM outperforms XLM-R on 16 out of\n22 language pairs, providing 8.1% improvement\nin averaged accuracy. In comparison, the ablation\nvariant “−XLCO” (i.e., MMLM +TLM) obtains\nbetter results than XLM-R in Table 2, while getting\nworse performance than XLM-R in Table 3. The\nresults indicate that XLCO encourages the model\nto learn universal representations even on the lan-\nguage pairs without parallel supervision.\nCross-Lingual Question Answering Table 4\ncompares INFO XLM with baseline models on\nMLQA, where we report the F1 and the exact\nmatch (EM) scores on each test set. Both IN-\nFOXLM and INFO XLM LARGE obtain the best re-\nsults against the four baselines. In addition, the\nresults of the ablation variant “−XLCO” indicate\nthat the proposed cross-lingual contrast is beneﬁ-\ncial on MLQA.\nModels Direction ar bg zh de el fr hi ru es sw th tr ur vi Avg\nXLM-R xx →en 36.8 67.6 60.7 89.9 53.7 74.1 54.2 72.5 74.0 18.7 38.3 61.1 36.6 68.4 57.6\nINFOXLM xx →en 59.0 78.6 86.3 93.9 62.1 79.4 87.1 83.8 88.2 39.5 84.9 83.3 73.0 89.6 77.8\n−XLCO xx→en 42.9 65.5 69.5 91.1 55.6 76.4 71.6 74.9 74.8 20.5 68.1 69.8 51.6 81.8 65.3\nXLM-R en →xx 38.6 69.9 60.3 89.4 57.3 74.3 49.3 73.0 74.6 14.4 58.4 64.0 36.9 72.5 59.5\nINFOXLM en →xx 68.6 78.6 86.4 95.1 72.6 84.0 88.3 85.7 87.2 40.8 91.2 84.7 73.3 92.0 80.6\n−XLCO en→xx 45.4 64.0 69.3 88.1 56.5 72.3 69.6 73.6 71.5 22.1 79.7 64.3 48.2 79.8 64.6\nTable 2: Evaluation results on Tatoeba cross-lingual sentence retrieval. We report the top-1 accuracy of14 language\npairs that are covered by parallel data.\nModels Direction af bn et eu ﬁ he hu id it jv ja ka kk ko ml mr nl fa pt ta te tl Avg\nXLM-R xx →en 55.2 29.3 49.3 33.5 66.7 53.9 61.6 70.8 68.2 15.1 57.2 41.4 40.3 51.6 56.5 46.0 79.5 68.0 80.6 25.7 32.5 31.2 50.6\nINFO XLM xx →en 48.6 49.6 38.3 36.7 65.7 62.9 61.7 79.9 72.2 13.2 78.3 57.4 49.2 74.5 76.6 72.0 80.8 82.2 84.7 53.7 53.0 42.1 60.6\n−XLCO xx →en 33.1 33.5 25.9 20.8 48.4 49.1 46.1 68.5 60.4 12.2 60.6 38.6 35.1 60.6 57.8 49.1 72.2 66.0 75.3 36.5 38.0 25.5 46.1\nXLM-R en →xx 55.0 27.9 50.2 32.5 72.9 63.2 67.1 71.9 68.0 9.8 58.2 52.0 41.7 58.3 60.8 42.1 78.9 69.6 82.1 33.2 38.9 29.7 52.9\nINFO XLM en →xx 51.8 49.1 35.2 28.6 65.6 66.5 61.7 80.1 72.8 7.8 80.4 61.9 50.6 79.6 78.7 68.1 81.8 82.8 86.5 63.5 53.0 35.5 61.0\n−XLCO en →xx 28.1 23.5 19.0 12.6 45.2 49.7 40.8 62.8 57.5 3.4 58.2 38.9 31.3 61.0 57.5 37.2 67.8 66.4 75.0 43.0 31.6 17.9 42.2\nTable 3: Evaluation results on Tatoeba cross-lingual sentence retrieval. We report the top-1 accuracy scores of 22\nlanguage pairs that are not covered by parallel data.\n4.4 Analysis and Discussion\nTo understand INFO XLM and the cross-lingual\ncontrast task more deeply, we conduct analysis\nfrom the perspectives of cross-lingual transfer and\ncross-lingual representations. Furthermore, we per-\nform comprehensive ablation studies on the ma-\njor components of INFO XLM, including the cross-\nlingual pre-training tasks, mixup contrast, the con-\ntrast layer, and the momentum contrast. To reduce\nthe computation load, we use INFO XLM15 in our\nablation studies, which is trained on 15 languages\nfor 100K steps.\nCross-Lingual Transfer Gap Cross-lingual\ntransfer gap (Hu et al., 2020) is the difference\nbetween the performance on the English test set\nand the averaged performance on the test sets of\nall other languages. A lower cross-lingual transfer\ngap score indicates more end-task knowledge\nfrom the English training set is transferred to\nother languages. In Table 5, we compare the\ncross-lingual transfer gap scores of INFO XLM\nwith baseline models on MLQA and XNLI. Note\nthat we do not include the results of XLM because\nit is pre-trained on 15 languages or using #M=N.\nThe results show that INFO XLM reduces the gap\nscores on both MLQA and XNLI, providing better\ncross-lingual transferability than the baselines.\nCross-Lingual Representations In addition to\ncross-lingual transfer, learning good cross-lingual\nrepresentations is also the goal of cross-lingual pre-\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n20\n40\n60\n80Averaged Accuracy\nXLM-R\nInfoXLM\n- TLM\n- XlCo\nFigure 1: Evaluation results of different layers on\nTatoeba cross-lingual sentence retrieval.\ntraining. In order to analyze how the cross-lingual\ncontrast task affects the alignment of the learned\ncross-lingual representations, we evaluate the repre-\nsentations of different middle layers on the Tatoeba\ntest sets of the 14 languages that are covered by\nparallel data. Figure 1 presents the averaged top-\n1 accuracy of cross-lingual sentence retrieval in\nthe direction of xx →en. INFO XLM outperforms\nXLM-R on all of the 12 layers, demonstrating\nthat our proposed task improves the cross-lingual\nalignment of the learned representations. From the\nresults of XLM-R, we observe that the model suf-\nfers from a performance drop in the last few layers.\nThe reason is that MMLM encourages the repre-\nsentations of the last hidden layer to be similar to\ntoken embeddings, which is contradictory with the\ngoal of learning cross-lingual representations. In\nModels en es de ar hi vi zh Avg\nMBERT* 77.7 / 65.2 64.3 / 46.6 57.9 / 44.3 45.7 / 29.8 43.8 / 29.7 57.1 / 38.6 57.5 / 37.3 57.7 / 41.6\nXLM* 74.9 / 62.4 68.0 / 49.8 62.2 / 47.6 54.8 / 36.3 48.8 / 27.3 61.4 / 41.8 61.1 / 39.6 61.6 / 43.5\nUNICODER 80.6 / - 68.6 / - 62.7 / - 57.8 / - 62.7 / - 67.5 / - 62.1 / - 66.0 / -\nXLM-R 77.1 / 64.6 67.4 / 49.6 60.9 / 46.7 54.9 / 36.6 59.4 / 42.9 64.5 / 44.7 61.8 / 39.3 63.7 / 46.3\nXLM-R (reimpl) 80.2 / 67.0 67.7 / 49.9 62.1 / 47.7 56.1 / 37.2 61.1 / 44.0 67.0 / 46.3 61.4 / 38.5 65.1 / 47.2\nINFOXLM 81.6 / 68.3 69.8 / 51.6 64.3 / 49.4 60.6 / 40.9 65.2 / 47.1 70.2 / 49.0 64.8 / 41.3 68.1/49.6\n−XLCO 81.2 / 68.1 69.6 / 51.9 64.0 / 49.3 59.7 / 40.2 64.0 / 46.3 69.3 / 48.0 64.1 / 40.6 67.4 / 49.2\nXLM-RLARGE 80.6 / 67.8 74.1 / 56.0 68.5 / 53.6 63.1 / 43.5 69.2 / 51.6 71.3 / 50.9 68.0 / 45.4 70.7 / 52.7\nXLM-RLARGE(reimpl) 84.0 / 71.1 74.4 / 56.4 70.2 / 55.0 66.5 / 46.3 71.1 / 53.2 74.4 / 53.5 68.6 / 44.6 72.7 / 54.3\nINFOXLMLARGE 84.5 / 71.6 75.1 / 57.3 71.2 / 56.2 67.6 / 47.6 72.5 / 54.2 75.2 / 54.1 69.2 / 45.473.6/55.2\nTable 4: Evaluation results on MLQA cross-lingual question answering. We report the F1 and exact match\n(EM) scores. Results with “*” are taken from (Lewis et al., 2020). “(reimpl)” is our reimplementation of ﬁne-\ntuning, which is the same as I NFO XLM. Results of I NFO XLM and XLM-R (reimpl) are averaged over ﬁve runs.\n“−XLCO” is the model without cross-lingual contrast.\nModels MLQA XNLI Average\nMBERT 23.3 16.9 20.1\nXLM-R 17.6 10.4 14.0\nINFOXLM 15.8 10.3 13.1\n−XLCO 16.1 11.0 13.6\nTable 5: Cross-lingual transfer gap scores, i.e., aver-\naged performance drop between English and other lan-\nguages in zero-shot transfer. Smaller gap indicates\nbetter transferability. “ −XLCO” is the model without\ncross-lingual contrast.\ncontrast, INFO XLM still provides high retrieval\naccuracy at the last few layers, which indicates\nthat INFO XLM provides better aligned represen-\ntations than XLM-R. Moreover, we ﬁnd that the\nperformance is further improved when removing\nTLM, demonstrating that XLCO is more effective\nthan TLM for aligning cross-lingual representa-\ntions, although TLM helps to improve zero-shot\ncross-lingual transfer.\nEffect of Cross-Lingual Pre-training TasksTo\nbetter understand the effect of the cross-lingual\npre-training tasks, we perform ablation studies on\nthe pre-training tasks of INFO XLM , by remov-\ning XLCO, TLM, or both. We present the ex-\nperimental results in Table 7. Comparing the re-\nsults of −TLM and −XLCO with the results of\n−TLM−XLCO, we ﬁnd that both XLCO and TLM\neffectively improve cross-lingual transferability of\nthe pre-trained INFO XLM model. TLM is more ef-\nfective for XNLI while XLCO is more effective for\nMLQA. Moreover, the performance can be further\nimproved by jointly learning XLCO and TLM.\nEffect of Contrast on Universal LayerWe con-\nduct experiments to investigate whether contrast\nModel X LCOLayer XNLI MLQA\nINFOXLM15 8 76.45 67.87 / 49.58\nINFOXLM15 12 76.12 67.83 / 49.50\nINFOXLM15−TLM 8 75.58 67.42 / 49.27\nINFOXLM15−TLM 12 75.85 67.84 / 49.54\nTable 6: Contrast on the universal layer v.s. on the last\nlayer. Results are averaged over ﬁve runs. “−TLM” is\nthe ablation variant without TLM.\nModel XNLI MLQA\n[0] INFOXLM15 76.45 67.87/ 49.58\n[1] [0] −XLCO 76.24 67.43 / 49.23\n[2] [0] −TLM 75.85 67.84 / 49.54\n[3] [2] −XLCO 75.33 66.86 / 48.82\n[4] [2] −Mixup 75.43 67.21 / 49.19\n[5] [2] −Momentum 75.32 66.58 / 48.66\nTable 7: Ablation results on components of INFO XLM.\nResults are averaged over ﬁve runs.\non the universal layer improves cross-lingual pre-\ntraining. As shown in Table 6, we compare the\nevaluation results of four variants of INFO XLM ,\nwhere XLCO is applied on the layer 8 (i.e., uni-\nversal layer) or on the layer 12 (i.e., the last layer).\nWe ﬁnd that contrast on the layer 8 provides bet-\nter results for INFO XLM . However, conducting\nXLCO on layer 12 performs better when the TLM\ntask is excluded. The results show that maximiz-\ning context-sequence (TLM) and sequence-level\n(XLCO) mutual information at the last layer tends\nto interfere with each other. Thus, we suggest ap-\nplying XLCO on the universal layer for pre-training\nINFO XLM.\nEffect of Mixup Contrast We conduct an abla-\ntion study on the mixup contrast strategy. We pre-\ntrain a model that directly uses translation pairs for\nXLCO without mixup contrast ( −TLM−Mixup).\nAs shown in Table 7, we present the evaluation re-\nsults on XNLI and MLQA. We observe that mixup\ncontrast improves the performance of INFO XLM\non both datasets.\nEffect of Momentum ContrastIn order to show\nwhether our pre-trained model beneﬁts from mo-\nmentum contrast, we pretrain a revised version of\nINFO XLM without momentum contrast. In other\nwords, the parameters of the key encoder are al-\nways the same as the query encoder. As shown\nin Table 7, we report evaluation results (indicated\nby “−TLM−Momentum”) of removing momen-\ntum contrast on XNLI and MLQA. We observe a\nperformance descent after removing the momen-\ntum contrast from INFO XLM, which indicates that\nmomentum contrast improves the learned language\nrepresentations of INFO XLM.\n5 Conclusion\nIn this paper, we present a cross-lingual pre-trained\nmodel INFO XLM that is trained with both mono-\nlingual and parallel corpora. The model is mo-\ntivated by the uniﬁed view of cross-lingual pre-\ntraining from an information-theoretic perspective.\nSpeciﬁcally, in addition to the masked language\nmodeling and translation language modeling tasks,\nINFO XLM is jointly pre-trained with a newly intro-\nduced cross-lingual contrastive learning task. The\ncross-lingual contrast leverages bilingual pairs as\nthe two views of the same meaning, and encourages\ntheir encoded representations to be more similar\nthan the negative examples. Experimental results\non several cross-lingual language understanding\ntasks show that INFO XLM can considerably im-\nprove the performance.\n6 Ethical Considerations\nCurrently, most NLP research works and applica-\ntions are English-centric, which makes non-English\nusers hard to access to NLP-related services. Our\nwork focuses on cross-lingual language model pre-\ntraining. With the pre-trained model, we are able\nto transfer end-task knowledge from high-resource\nlanguages to low-resource languages, which helps\nto build more accessible NLP applications. Addi-\ntionally, incorporating parallel corpora into the pre-\ntraining procedure improves the training efﬁciency,\nwhich potentially reduces the computational cost\nfor building multilingual NLP applications.\nAcknowledgements\nWe appreciate the helpful discussions with Bo\nZheng, Shaohan Huang, Shuming Ma, and Yue\nCao.\nReferences\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail\nKhodak, Orestis Plevrakis, and Nikunj Saunshi.\n2019. A theoretical analysis of contrastive unsuper-\nvised representation learning. In International Con-\nference on Machine Learning.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 4623–4637. Association for Computa-\ntional Linguistics.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7(0):597–610.\nPhilip Bachman, R Devon Hjelm, and William Buch-\nwalter. 2019. Learning representations by maximiz-\ning mutual information across views. In Advances\nin Neural Information Processing Systems , pages\n15509–15519.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework\nfor contrastive learning of visual representations. In\nProceedings of the 37th International Conference\non Machine Learning , volume 119 of Proceedings\nof Machine Learning Research , pages 1597–1607.\nPMLR.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nLing Mao, and Heyan Huang. 2020. Cross-lingual\nnatural language generation via pre-training. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7570–7577. AAAI Press.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7057–7067. Curran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\ncross-lingual structure in pretrained language mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 6022–6034, Online. Association for Compu-\ntational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nPhilipp Dufter and Hinrich Sch ¨utze. 2020. Identifying\nelements essential for BERT’s multilinguality. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4423–4437, Online. Association for Computa-\ntional Linguistics.\nRobert M. Fano. 1963. Transmission of Information: A\nStatistical Theory of Communications. M.I.T. Press.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2020. Language-\nagnostic BERT sentence embedding. arXiv preprint\narXiv:2007.01852.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for unsu-\npervised visual representation learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR).\nOlivier Henaff. 2020. Data-efﬁcient image recognition\nwith contrastive predictive coding. In Proceedings\nof the 37th International Conference on Machine\nLearning, volume 119 of Proceedings of Machine\nLearning Research, pages 4182–4192. PMLR.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-\nMarchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. 2019. Learning deep\nrepresentations by mutual information estimation\nand maximization. In International Conference on\nLearning Representations.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research , pages\n4411–4421. PMLR.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing, pages 2485–2494, Hong Kong, China.\nAssociation for Computational Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual bert: An empirical study. In International Con-\nference on Learning Representations.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, San\nDiego, CA.\nLingpeng Kong, Cyprien de Masson d’Autume, Lei Yu,\nWang Ling, Zihang Dai, and Dani Yogatama. 2020.\nA mutual information maximization perspective of\nlanguage representation learning. In International\nConference on Learning Representations.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-\ntacharyya. 2018. The IIT Bombay English-Hindi\nparallel corpus. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation, Miyazaki, Japan. European Language\nResources Association.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie\nWu, Shuguang Liu, Fan Yang, Daniel Campos, Ran-\ngan Majumder, and Ming Zhou. 2020. XGLUE: A\nnew benchmark datasetfor cross-lingual pre-training,\nunderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8(0):726–742.\nXuanlong Nguyen, Martin J Wainwright, and Michael I\nJordan. 2010. Estimating divergence functionals\nand the likelihood ratio by convex risk minimiza-\ntion. IEEE Transactions on Information Theory ,\n56(11):5847–5861.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMasoud Jalili Sabet, Philipp Dufter, and Hinrich\nSch¨utze. 2020. Simalign: High quality word align-\nments without parallel training data using static\nand contextualized embeddings. arXiv preprint\narXiv:2004.08728.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm ´an. 2019. Wiki-\nMatrix: Mining 135M parallel sentences in 1620\nlanguage pairs from wikipedia. arXiv preprint\narXiv:1907.05791.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: Masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, volume 97 ofProceedings of Ma-\nchine Learning Research, pages 5926–5936. PMLR.\nYonglong Tian, Dilip Krishnan, and Phillip Isola.\n2019. Contrastive multiview coding. ArXiv,\nabs/1906.05849.\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation, pages 2214–2218, Istanbul, Turkey. Eu-\nropean Language Resources Association.\nNaftali Tishby and Noga Zaslavsky. 2015. Deep learn-\ning and the information bottleneck principle. 2015\nIEEE Information Theory Workshop (ITW), pages 1–\n5.\nT. Tsujishita. 1995. On triple mutual information. Ad-\nvances in Applied Mathematics, 16(3):269 – 274.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998–6008. Curran As-\nsociates, Inc.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, pages 833–844, Hong Kong,\nChina. Association for Computational Linguistics.\nJian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi\nWu, Zhoujun Li, and Ming Zhou. 2020. Alternating\nlanguage modeling for cross-lingual pre-training. In\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel corpus\nv1. 0. In LREC, pages 3530–3534.\nA Pre-Training Data\nWe reconstruct CCNet2 and follow (Conneau et al.,\n2020a) to reproduce the CC-100 corpus for mono-\nlingual texts. The resulting corpus contains 94 lan-\nguages. Table 8 reports the language codes and data\nsize in our work. Notice that several languages can\nshare the same ISO language code, e.g., zh rep-\nresents both Simpliﬁed Chinese and Traditional\nChinese. Moreover, Table 9 shows the statistics of\nthe parallel data.\nCode Size (GB) Code Size (GB) Code Size (GB)\naf 0.2 hr 1.4 pa 0.8\nam 0.4 hu 9.5 pl 28.6\nar 16.1 hy 0.7 ps 0.4\nas 0.1 id 17.2 pt 39.4\naz 0.8 is 0.5 ro 11.0\nba 0.2 it 47.2 ru 253.3\nbe 0.5 ja 86.8 sa 0.2\nbg 7.0 ka 1.0 sd 0.2\nbn 5.5 kk 0.6 si 1.3\nca 3.0 km 0.2 sk 13.6\nckb 0.6 kn 0.3 sl 6.2\ncs 14.9 ko 40.0 sq 3.0\ncy 0.4 ky 0.5 sr 7.2\nda 6.9 la 0.3 sv 60.4\nde 99.0 lo 0.2 sw 0.3\nel 13.1 lt 2.3 ta 7.9\nen 731.6 lv 1.3 te 2.3\neo 0.5 mk 0.6 tg 0.7\nes 85.6 ml 1.3 th 33.0\net 1.4 mn 0.4 tl 1.2\neu 1.0 mr 0.5 tr 56.4\nfa 19.0 ms 0.7 tt 0.6\nﬁ 5.9 mt 0.2 ug 0.2\nfr 89.9 my 0.4 uk 13.4\nga 0.2 ne 0.6 ur 3.0\ngl 1.5 nl 25.9 uz 0.1\ngu 0.3 nn 0.4 vi 74.5\nhe 4.4 no 5.5 yi 0.3\nhi 5.0 or 0.3 zh 96.8\nTable 8: The statistics of CCNet used corpus for pre-\ntraining.\nISO Code Size (GB) ISO Code Size (GB)\nen-ar 5.88 en-ru 7.72\nen-bg 0.49 en-sw 0.06\nen-de 4.21 en-th 0.47\nen-el 2.28 en-tr 0.34\nen-es 7.09 en-ur 0.39\nen-fr 7.63 en-vi 0.86\nen-hi 0.62 en-zh 4.02\nTable 9: Parallel data used for pre-training.\nB Results of Training From Scratch\nWe conduct experiments under the setting of train-\ning from scratch. The Transformer size and hy-\nperparameters follow BERT-base (Devlin et al.,\n2019). The parameters are randomly initialized\nfrom U[−0.02, 0.02]. We optimize the models with\n2https://github.com/facebookresearch/\ncc_net\nModel XNLI MLQA\nMetrics Acc. F1 / EM\nMMLMSCRATCH 69.40 55.02 / 37.90\nINFO XLM SCRATCH 70.71 59.71 / 41.46\n−XLCO 70.64 57.70 / 40.21\n−TLM 69.76 58.22 / 40.78\n−MMLM 63.06 52.81 / 35.01\nTable 10: Ablation results of the models pre-trained\nfrom scratch. Results are averaged over ﬁve runs.\nAdam using a batch size of 256 for a total of 1M\nsteps. The learning rate is scheduled with a lin-\near decay with 10K warmup steps, where the peak\nlearning rate is set as 0.0001. For cross-lingual\ncontrast, we set the queue length as 16, 384. We\nuse a warmup of 200K steps for the key encoder\nand then enable cross-lingual contrast. We use an\ninverse square root scheduler to set the momen-\ntum coefﬁcient, i.e., m = min(1−t−0.51, 0.9995),\nwhere t is training step.\nTable 10 shows the results ofINFO XLM SCRATCH\nand various ablations. INFO XLM SCRATCH signiﬁ-\ncantly outperforms MMLMSCRATCH on both XNLI\nand MLQA. We also evaluate the pre-training ob-\njectives of INFO XLM , where we ablate XLCO,\nTLM, and MMLM, respectively. The ﬁndings\nagree with the results in Table 7.\nC Hyperparameters for Pre-Training\nAs shown in Table 11, we present the hyperparam-\neters for pre-training INFO XLM. We use the same\nvocabulary with XLM-R (Conneau et al., 2020a).\nHyperparameters F ROM SCRATCH BASE LARGE\nLayers 12 12 24\nHidden size 768 768 1,024\nFFN inner hidden size 3,072 3,072 4,096\nAttention heads 12 12 16\nTraining steps 1M 150K 200K\nBatch size 256 2,048 2,048\nAdam ϵ 1e-6 1e-6 1e-6\nAdam β (0.9, 0.999) (0.9, 0.98) (0.9, 0.98)\nLearning rate 1e-4 2e-4 1e-4\nLearning rate schedule Linear Linear Linear\nWarmup steps 10,000 10,000 10,000\nGradient clipping 1.0 1.0 1.0\nWeight decay 0.01 0.01 0.01\nMomentum coefﬁcient 0.9995* 0.9999 0.999\nQueue length 16,384 131,072 131,072\nUniversal layer 8 8 12\nTable 11: Hyperparameters used for I NFO XLM pre-\ntraining. *: the momentum coefﬁcient uses an inverse\nsquare root scheduler m = min(1−t−0.51, 0.9995).\nXNLI MLQA\nBatch size 32 {16, 32}\nLearning rate {5e-6, 7e-6, 1e-5} { 2e-5, 3e-5, 5e-5}\nLR schedule Linear Linear\nWarmup 12,500 steps 10%\nWeight decay 0 0\nEpochs 10 {2, 3, 4}\nTable 12: Hyperparameters used for ﬁne-tuning BASE-\nsize models on XNLI and MLQA.\nXNLI MLQA\nBatch size 32 32\nLearning rate {4e-6, 5e-6, 6e-6} { 2e-5, 3e-5, 5e-5}\nLR schedule Linear Linear\nWarmup 5,000 steps 10%\nWeight decay {0, 0.01} 0\nEpochs 10 {2, 3, 4}\nTable 13: Hyperparameters used for ﬁne-tuning\nLARGE-size models on XNLI and MLQA.\nD Hyperparameters for Fine-Tuning\nIn Table 12 and Table 13, we present the hyperpa-\nrameters for ﬁne-tuning on XNLI and MLQA. For\neach task, the hyperparameters are searched on the\njoint validation set of all languages (#M=1). For\nXNLI, we evaluate the model every 5,000 steps,\nand select the model with the best accuracy score\non the validation set. For MLQA, we directly use\nthe ﬁnal learned model. The ﬁnal scores are aver-\naged over ﬁve random seeds.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8395878672599792
    },
    {
      "name": "Natural language processing",
      "score": 0.6360054612159729
    },
    {
      "name": "Task (project management)",
      "score": 0.5827725529670715
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5745385885238647
    },
    {
      "name": "Meaning (existential)",
      "score": 0.526630699634552
    },
    {
      "name": "Sentence",
      "score": 0.4785783290863037
    },
    {
      "name": "Code (set theory)",
      "score": 0.4528461694717407
    },
    {
      "name": "Granularity",
      "score": 0.44755059480667114
    },
    {
      "name": "Programming language",
      "score": 0.07972130179405212
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": []
}