{
  "title": "Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models",
  "url": "https://openalex.org/W3199674690",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2647890403",
      "name": "Taichi Iki",
      "affiliations": [
        "The Graduate University for Advanced Studies, SOKENDAI",
        "National Institute of Informatics"
      ]
    },
    {
      "id": "https://openalex.org/A2167769188",
      "name": "Akiko Aizawa",
      "affiliations": [
        "The Graduate University for Advanced Studies, SOKENDAI",
        "National Institute of Informatics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W131533222",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2964260310",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2890399523",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3110570034",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3129436779",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2949554544",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2960994197",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2963491027",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2746097825",
    "https://openalex.org/W2251512949",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3106784008",
    "https://openalex.org/W2998536339",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W3171096271",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3173585224",
    "https://openalex.org/W4287547182",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2952316487"
  ],
  "abstract": "A method for creating a vision-and-language (V&L) model is to extend a language model through structural modifications and V&L pre-training. Such an extension aims to make a V&L model inherit the capability of natural language understanding (NLU) from the original language model. To see how well this is achieved, we propose to evaluate V&L models using an NLU benchmark (GLUE). We compare five V&L models, including single-stream and dual-stream models, trained with the same pre-training. Dual-stream models, with their higher modality independence achieved by approximately doubling the number of parameters, are expected to preserve the NLU capability better. Our main finding is that the dual-stream scores are not much different than the single-stream scores, contrary to expectation. Further analysis shows that pre-training causes the performance drop in NLU tasks with few exceptions. These results suggest that adopting a single-stream structure and devising the pre-training could be an effective method for improving the maintenance of language knowledge in V&L extensions.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2189–2196\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n2189\nEffect of Visual Extensions on Natural Language Understanding in\nVision-and-Language Models\nTaichi Iki1,2 and Akiko Aizawa1,2\n1National Institute of Informatics, Chiyoda-ku, Tokyo, Japan\n2Graduate University for Advanced Studies, Hayama, Kanagawa, Japan\n{iki,aizawa}@nii.ac.jp\nAbstract\nA method for creating a vision-and-language\n(V&L) model is to extend a language model\nthrough structural modiﬁcations and V&L pre-\ntraining. Such an extension aims to make a\nV&L model inherit the capability of natural\nlanguage understanding (NLU) from the orig-\ninal language model. To see how well this is\nachieved, we propose to evaluate V&L mod-\nels using an NLU benchmark (GLUE). We\ncompare ﬁve V&L models, including single-\nstream and dual-stream models, trained with\nthe same pre-training. Dual-stream mod-\nels, with their higher modality independence\nachieved by approximately doubling the num-\nber of parameters, are expected to preserve the\nNLU capability better. Our main ﬁnding is\nthat the dual-stream scores are not much dif-\nferent than the single-stream scores, contrary\nto expectation. Further analysis shows that\npre-training causes the performance drop in\nNLU tasks with few exceptions. These results\nsuggest that adopting a single-stream structure\nand devising the pre-training could be an effec-\ntive method for improving the maintenance of\nlanguage knowledge in V&L extensions.\n1 Introduction\nPre-trained vision-and-language (V&L) models im-\nprove the performance of tasks that require an un-\nderstanding of the V&L grounding, including vi-\nsual question answering (Antol et al., 2015), re-\nferring expression comprehension (Kazemzadeh\net al., 2014), and image-text matching (ITM) (Suhr\net al., 2019). Recent V&L tasks, such as multi-\nmodal reading comprehension (Kembhavi et al.,\n2017; Yagcioglu et al., 2018; Hannan et al., 2020;\nTanaka et al., 2021) and dialogue (Ilinykh et al.,\n2019; Haber et al., 2019; Udagawa and Aizawa,\n2019), require a deeper NLU as well as the ground-\ning. Extending pre-trained language models (LMs)\nis an option for those tasks as this allows V&L\nmodels to inherit language knowledge from their\nsource LMs. The typical extending consists of vi-\nsual pre-training and structure such as the stream\ntype; the single-stream inserts vision tokens into\nthe input sequence of the LM, and the dual-stream\nuses another sequence for early visual encoding.\nOne of the remaining challenges is to understand\nhow such extensions affect the pre-trained language\nknowledge. For example, Lu et al. (2019) proposed\nthe dual-stream model where part of the goal was\nto protect the learned LMs. The authors focused\non evaluation with V&L tasks and did not evaluate\ntheir models with language-only tasks. Cao et al.\n(2020) evaluated the extent of language knowl-\nedge loss in the single/dual-stream models against\nthe source LM using language-only tasks. How-\never, the difference between single-stream and dual-\nstream models was unclear because the pre-training\nwas also different in their models.\nIn this paper, we investigate the effect of visual\nextensions in V&L models on language-only tasks1.\nBugliarello et al. (2020) proposed a framework to\nunify transformer-based V&L models and com-\npared some single/dual-stream models in the same\nsetup. Based on their work, our study shows how\nthese structural differences affect the performance\nof NLU using the GLUE (Wang et al., 2019) tasks.\nIn our experiments, ﬁne-tuning of pre-trained\nV&L models shows that both single/dual-stream\nmodels perform worse than the source LM and that\nsingle-stream models perform slightly better than\ndual-stream models. Further, we ﬁne-tune the mod-\nels created by only structural modiﬁcations without\npre-training. We observe that the single/dual modi-\nﬁcation alone has little effect on the GLUE scores,\nindicating the performance degradation is primarily\ncaused by pre-training. We also see how the V&L\nmodels changed from the source LM by analyzing\nthe changes in the model parameters and the prob-\nlem sets that each model can solve. Our results\n1The source code for our experiments is available at\nhttps://github.com/Alab-NII/eval_vl_glue\n2190\nsuggest that it would be more effective to adopt a\nsingle stream, and devise pre-training strategies for\nmaintaining language knowledge.\n2 Controlled V&L Models\nIn this section, we describe the pre-trained V&L\nmodels used in our experiments. Bugliarello et al.\n(2020) proposed a framework for V&L models that\nconsider a sequence of tokens in sentences as lan-\nguage information, and a sequence of recognized\nobject regions as visual information. In their frame-\nwork, they reproduced ﬁve existing models, Visual-\nBERT (Li et al., 2019), Uniter (Chen et al., 2020),\nVL-BERT (Su et al., 2020), ViLBERT (Lu et al.,\n2019), and LXMERT (Tan and Bansal, 2019), and\nmade their controlled versions by modifying some\nparts for a fairer and easier comparison. We use\nthese controlled versions.\n2.1 Structural Modiﬁcation\nWe describe streams and embeddings, which are\nthe basic factors of the model structures. We sum-\nmarize the model structures in the controlled setup\nused in this experiment in Table 1.\nStreams. V&L models can be divided into two\ncategories based on how the vision and language se-\nquences are encoded. Single-stream models, Visu-\nalBERT, Uniter, and VL-BERT, jointly process the\nvision and language sequences in a single encoder.\nDual-stream models, ViLBERT and LXMERT, en-\ncode those sequences separately before encoding\nthem jointly. ViLBERT is an early example of\nthe dual-stream models and was proposed mainly\nto account for the differences in abstraction lev-\nels between vision and language, and to protect\nlearned language models. In the controlled setup\nof Bugliarello et al. (2020), the stream type is iden-\ntical to the original one in all models.\nEmbeddings. The major difference in embed-\ndings is the use of global visual feature. The origi-\nnal VisualBERT, Uniter, and LXMERT do not use\nthe global visual feature. ViLBERT has a token\nthat represents the global visual feature at the be-\nginning of vision sequences. VL-BERT inserts the\nglobal visual feature to the last of vision sequences\nand also adds the global visual feature to each to-\nken embedding in the language sequence. Object\nlocation is also expressed differently. The original\nVL-BERT and LXMERT use four attributes (left,\ntop, right, bottom). In addition to the four attributes,\nthe original ViLBERT uses area, and the original\nUniter uses width, height, and area. VisualBERT\ndoes not use location information2.\nThe controlled setup is based on the structure of\nViLBERT. For the global image feature, the setup\ninserts the average of vision tokens to the head of\nthe vision sequence for all models. In addition to\ninserting the global visual feature, the controlled\nVL-BERT adds it to the respective tokens in the\nlanguage sequence. For location, VisualBERT’s\nsetup that do not use location information remain\nthe same, while the other models use the ﬁve at-\ntributes. The ﬁve attributes are normalized by width\nor height. Another point is the token type for the vi-\nsion tokens. In the controlled setup, the token type\nis not added for ViLBERT and LXMERT because\nthey have separate streams. Of the single-stream\nmodels, VisualBERT and Uniter use BERT’s token\ntype ID to specify vision tokens, while VL-BERT\nadds a new embedding to represent vision tokens.\n2.2 Pre-training\nWe summarize the pre-training used in the con-\ntrolled setup to train the ﬁve model structures de-\nscribed above. Note that we omit the detail of the\npre-training used in each original paper here.\nThe ﬁve models were pre-trained on Google’s\nConceptual Captions (Sharma et al., 2018) corpus,\nwhich was collected from Web images and their\nalt-text HTML attributes. The corpus was ﬁltered\nbefore training, and the size was approximately\n2.7 M pairs as a result. Three tasks, masked lan-\nguage modelling (MLM), masked object classiﬁca-\ntion (MOC), and ITM, were made from image-text\npairs in the corpus. Given an image-text pair, the\nmodel predicts masked language tokens for MLM,\nthe object class of masked vision tokens for MOC,\nand whether the pair is correct or not for ITM.\nThe weights of the ﬁve models were initialized\nwith the pre-trained weights of BERTBASE if the\ncorresponding weights were in BERTBASE; other-\nwise (e.g., the weights of the vision encoder in dual-\nstream models), they were initialized randomly.\n3 Experiment with GLUE\n3.1 Datasets\nThe GLUE benchmark (Wang et al., 2019) is a col-\nlection of diverse tasks for studying NLU systems.\n2If alignments between words and regions are provided,\nVisualBERT adds the same position embeddings to matched\nword and region tokens instead.\n2191\nStructure Abbreviation\nin this paper Stream #param Location\nformat\nGlobal\nimage feat.\nVision\ntype ID\nOriginal\npaper\nVisualBERTCTRL VISCTRL\nSingle\n112M not used head from BERT Li et al. (2019)\nUniterCTRL UNICTRL 112M LTRBA head from BERT Chen et al. (2020)\nVL-BERTCTRL VLCTRL 114M LTRBA head + added\nto each word extended Su et al. (2020)\nViLBERTCTRL VILCTRL Dual 240M LTRBA head not used Lu et al. (2019)\nLXMERTCTRL LXCTRL 209M LTRBA head not used Tan and Bansal (2019)\nTable 1: Comparison of the structures in the controlled setup used in this study. L, T, R, B, and A in the location\nformat column denote left, top, right, bottom, and area, respectively. Bugliarello et al. (2020)’s controlled setup\nuniﬁes the use of the location format and global visual features, which were different in the original proposals.\nIt consists of nine tasks: CoLA (Warstadt et al.,\n2019), SST-2 (Socher et al., 2013), MRPC (Dolan\nand Brockett, 2005), QQP 3, STS-B (Cer et al.,\n2017), MNLI (Nangia et al., 2017), QNLI (Ra-\njpurkar et al., 2016), RTE (Dagan et al., 2005;\nBar Haim et al., 2006; Giampiccolo et al., 2007;\nBentivogli et al., 2009), and WNLI (Levesque et al.,\n2012). STS-B is a single-valued regression task,\nand the others are classiﬁcation tasks. We train\nthe controlled pre-trained models on the training\nsets and evaluate them with the development sets.\nFigure 1 (left) shows the number of the training\nsentences in the corpora and their word overlap\nbetween the corpus used in the V&L pre-training.\n3.2 Implementation Details\nWe ﬁne-tuned pre-trained models published by\nBugliarello et al. (2020) 4. To use a script for\nthe GLUE benchmark, we modiﬁed the model\ncodes for Huggingface’s Transformers (Wolf et al.,\n2020)5. We used the BERT-uncased tokenizer to\ntokenize sentences.\nImage inputs. Because the GLUE tasks have no\nimage input, we used a black image (of 224 × 224\npixels) in our experiments. We followed the\nmethod of Bugliarello et al. (2020) for image pro-\ncessing; we input the images to the Faster R-CNN\nobject detector (Ren et al., 2015) trained for the\nBottom-Up and Top-Down model (Anderson et al.,\n2018), and used the top 36 detected results (bound-\ning boxes and feature vectors) as vision tokens 6.\nWe used the average of the vision tokens as a global\nvisual token. Those vision tokens were ﬁxed and\nused for both training and evaluation in all models.\nIn this study, we tried image completion with\n3https://www.kaggle.com/c/\nquora-question-pairs\n4https://github.com/e-bug/volta/blob/\nmain/MODELS.md\n5We checked that our implementation reproduced original\nresults with a V&L task, NLVR2 (Suhr et al., 2019).\n6Although the image was monochromatic black, 36 bound-\ning boxes with different features were detected.\nblack images for tasks where no image is provided\nas a simple way to preserve the input format used\nin pre-training. However, there are many possible\nmethods for complementing the image input. For\nexample, a method as simple as the present one\ncan use other images, a noise input, or learnable\nparameters. Examining the impact of image input\ncompletion methods remains as future work.\nHead for classiﬁcation. We adopted the method\nused in Bugliarello et al. (2020) for V&L tasks.\nWe used a learnable linear layer to calculate the\nlikelihood of document classes, such as entail-\nment/neutral/contradiction. We input the element-\nwise product of two vectors made from the model’s\noutput sequence into the linear layer. For those\ntwo vectors, we pooled the portions of the model’s\noutput sequence that correspond to the vision input\nand to the language input, respectively, by taking\nthe ﬁrst token of each portion. This corresponds\nto taking the outputs of the [CLS] token (in the\nlanguage sequence) and the global visual token.\nHyperparameters for ﬁne-tuning. We used a\nbatch size of 64 and Adam for optimization. The\nlearning rate was initialized at 2e-5 and decreased\nlinearly. We trained for ﬁve epochs, evaluating\nthe loss on the dev sets at the end of each epoch.\nFinally, we adopted the model with the lowest loss.\n3.3 Overall Result\nTable 2 shows the results of the GLUE benchmark.\nIn our experiment, we ﬁne-tuned ﬁve V&L models\nand their source language model–BERTBASE. We\nalso cited the BiLSTM baseline from the GLUE\npaper. The Glue avg of ﬁve V&L models decrease\ncompared to BERTBASE. We can see a trend where\nthe single-stream models perform slightly better\nthan the dual-stream models. Note that this trend\nis consistent with the results of Cao et al. (2020)\nfor linguistic probing of the original Uniter and\nLXMERT. Although the difference is small, this\nsuggests that the single-stream models can main-\n2192\nGLUE (Language) V&L\navg↑ (SD) CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE WNLI avg ↑\nBiLSTM 66.7 17.6 87.5 77.9/85.1 85.3/82.0 71.6/72.0 66.7 77.0 58.5 56.3\nBERTBASE 77.3 (0.8) 54.6 92.5 81.9/87.6 90.6/87.4 88.2/87.9 84.4 91.0 62.5 48.8\nModel avg 71.6 38.0 88.9 70.1/80.8 89.0/85.5 78.5/78.7 81.0 85.5 55.7 52.6 68.0\nVISCTRL 72.5(1.2) 38.6 89.4 71.9/82.1 89.4/86.0 81.8/81.7 81.8 87.0 56.6 53.1 69.2\nUNICTRL 71.4 (0.3) 37.4 89.7 69.3/80.3 89.2/85.7 74.9/75.6 81.2 86.0 55.6 55.4 69.7\nVLCTRL 72.4(0.8) 38.7 89.8 70.6/81.8 89.0/85.4 82.9/82.8 81.4 86.3 55.7 53.1 67.7\nVILCTRL 70.9 (0.8) 36.1 90.4 69.0/79.4 88.6/85.0 77.7/78.0 80.1 83.8 53.7 55.4 69.8\nLXCTRL 70.5 (0.2) 39.0 90.2 69.8/80.4 89.0/85.4 75.3/75.3 80.7 84.2 57.2 46.0 63.6\nTable 2: Performance of the development sets of the GLUE tasks (single-task training). The best scores among\nthe ﬁve V&L models are shown in bold. We report the Matthews correlation for CoLA; accuracy/F1 for MRPC\nand QQP; the Pearson/Spearman correlation for STS-B; and the accuracy for all other tasks. For MNLI, we show\naccuracy averaged over the matched and mismatched sets. The values of BiLSTM are cited from Wang et al.\n(2019). The other values related to GLUE are our results. We ﬁne-tuned the pre-trained models for each task three\ntimes with different random seeds. We show the standard deviation in parentheses for avg and in Appendix B\nfor each task. In the last column, we also show the scores of V&L tasks calculated by averaging the results in\nBugliarello et al. (2020). The detail is described in Section 4.3.\nFigure 1: Left: The number of training sentences\nvs. the Simpson coefﬁcient between the GLUE and\nCC (training) corpora. Right: The correlation between\nthe Simpson coefﬁcient and the model score. The\nmodel scores were averaged over the ﬁve V&L models\nand normalized with BERTBASE’s score.\ntain more of BERTBASE’s knowledge.\nPerformance of each task. V&L models per-\nform lower than the BiLSTM baseline for some\ntasks, including MRPC, RTE, and WNLI. Figure 1\n(right) shows the correlation between the word\noverlap between the corpus for pre-training and\nthe GLUE task corpora and the GLUE score. We\ncan see a positive correlation between those two\nvariables. Although we do not conclude clearly\nbecause word overlap and the number of training\ndata also correlate, word overlap could have a large\nimpact on task performance.\n4 Analysis\n4.1 Amount of Change in Parameters\nWe expected the model inference to be closer to\nBERT’s inference if a model has parameters closer\nto BERT. Therefore, we calculated the cosine simi-\nlarity of the corresponding parameters between pre-\ntrained models and BERT to indicate the degree to\nwhich the parameters had changed. Table 3 shows\nthe averaged cosine similarity. We ﬂattened param-\nweight weight (LN) bias bias (LN)\n#layers 75 25 72 25\nVISCTRL 0.9218 0.9999 0.9963 0.9973\nUNICTRL 0.9197 0.9999 0.9966 0.9971\nVLCTRL 0.9193 0.9999 0.9964 0.9968\nVILCTRL 0.9218 0.9999 0.9934 0.9895\nLXCTRL 0.9208 0.9998 0.9926 0.9935\nTable 3: Averaged cosine similarity between the corre-\nsponding parameters in the BERTBASE and V&L mod-\nels. #layers represents the number of layers transferred\nfrom BERTBASE to V&L models. We computed the av-\neraged similarity of the weights and biases in the layer\nnormalization (LN) layers and the other layers.\nSuccessful models\nBoth BERT BASE V&L Neither\nVISCTRL 0.722 0.080 0.049 0.150\nUNICTRL 0.717 0.085 0.050 0.149\nVLCTRL 0.700 0.102 0.053 0.146\nVILCTRL 0.710 0.091 0.049 0.150\nLXCTRL 0.691 0.111 0.065 0.134\nTable 4: Analysis of which models were successful in\nanswering the classiﬁcation task. STS-B was excluded\nbecause it is a regression task. We deﬁned success in\na problem as answering correctly in at least two out of\nthree runs.\neters and calculated their similarity as vectors. We\ncan see that the parameters of the single-stream and\ndual-stream models changed by the same extent.\nThis suggests that separating streams alone may\nnot be sufﬁcient for knowledge maintenance.\n4.2 Breakdown of Classiﬁcation Results\nTable 4 shows the results of aggregating the GLUE\nclassiﬁcation task problems into four categories:\nsolvable by both BERT BASE and V&L models,\nBERTBASE only, V&L model only, and neither\nmodel. We deﬁned success in a given problem\nas answering correctly in at least two out of three\nexperimental runs. To make Table 4, we ﬁrst cal-\n2193\nMod. only Mod.+V&L PT\nVISCTRL 77.4 (1.00) 72.5 (0.94)\nUNICTRL 77.9 (1.01) 71.4 (0.92)\nVLCTRL 39.5 (0.51) 72.4 (0.94)\nVILCTRL 75.6 (0.98) 70.9 (0.92)\nLXCTRL 78.4 (1.01) 70.5 (0.91)\nTable 5: Effect of V&L pre-training on the averaged\nGLUE score. Values in parentheses are scores that have\nbeen normalized by the BERTBASE scores.\nculated the tables of successful models for each\nGLUE task and V&L model and second averaged\nthe tables for the tasks. For all ﬁve models, there\nare approximately 5% of problems that they only\ncan solve. This category shows the positive impact\nof V&L pre-training on NLU. Problems that both\nmodels can solve tended to be more common for\nthe single-stream models. This supports the ﬁnding\nthat these models retain more language knowledge.\nThe difference of corpora for the last pre-training\nbetween BERT (mainly English Wikipedia) and the\nV&L models (images’ alt-texts) might affect the\ncomplexity of the sentences in the problem sets that\ncan be solved only by BERT and only by the V&L\nmodels. Thus, we analyzed the distributions of\nsome metrics (sentence length, readability). How-\never, we found no signiﬁcant difference between\nthe two sets in each model. We show the distribu-\ntions in Appendix C.\n4.3 Language and V&L Tasks\nThe last column of Table 2 shows the V&L scores\nfor the V&L models. We calculated these scores\nby averaging the results on the ﬁve V&L tasks\nreported in Bugliarello et al. (2020). Their tasks\ncover four groups widely used to test V&L models:\nVQA, image–text retrieval, referring expressions,\nand multi-modal veriﬁcation. Comparing the V&L\nand GLUE scores, we cans see that no model is\nbest in both respects at the same time. There is\nroom for improvement in the V&L extension.\n4.4 Structural Modiﬁcation or Pre-training:\nWhich Has the Greater Impact?\nTo further analyze the impact of structural modiﬁ-\ncation, we ﬁne-tuned models with only structural\nmodiﬁcations (Mod. only). Table 5 shows a com-\nparison between the GLUE scores of the Mod-only\nmodels and the full models (Mod+V&L-PT). Ex-\ncept for VLCTRL, the Mod-only models achieve a\nscore comparable to BERT BASE, and the GLUE\nscore decreases for the Mod+V&L-PT models. The\nfact that the structural modiﬁcation preserves the\nscore of the GLUE tasks in most cases suggests that\nthe main factor for the drop in the GLUE tasks is\nV&L pre-training. This observation emphasizes the\nimpact of pre-training on maintaining the language\nknowledge. Note that a possible reason for the ex-\nception of VLCTRL is that the global visual feature\nadded to the language embeddings may break the\nlanguage knowledge.\n5 Discussion and Conclusion\nThe number of V&L model works that focus on\nboth V&L tasks and language-only tasks has in-\ncreased (Ororbia et al., 2019; Lin et al., 2021; Li\net al., 2020; Hu and Singh, 2021). Ororbia et al.\n(2019) proposed a V&L neural architecture and\ntrained it on a language model in a visual context.\nThey demonstrated that their architecture outper-\nforms its equivalent trained on language alone in\nperplexity and stated that language is inseparable\nfrom its physical context. Although it is not clear\nwhether methods that improve the perplexity of\nlanguage modeling can also apply to maintain the\nperformance of downstream tasks, the strategy of\nimproving models with reference to human cog-\nnition would be an important direction. More re-\ncently, Li et al. (2020) achieved better performance\non language-only tasks than their base model with\npre-training on three types of corpora (text, image,\nand image–text pairs) at the same time. Lin et al.\n(2021) reported that adding separated extractors\nfor vision and language on top of a single-stream\nencoder can help maintain language knowledge.\nIn this paper, we ﬁne-tuned V&L models ex-\ntended from a language model (LM) to an NLU\nbenchmark to compare their NLU performance.\nWe used ﬁve V&L models, including single-stream\nand dual-stream models, pre-trained in the same\nsetup. The benchmark scores of those models de-\ncreased compared with their source LM. We also\nfound that the single-stream models tended to re-\ntain (slightly) more language knowledge than the\ndual-stream models, and that the main cause of the\ndrop in the NLU tasks can be pre-training. Our ob-\nservations suggest that adopting a single stream and\ndevising pre-training strategies could be effective,\nat least for preserving the language knowledge.\nAcknowledgements\nWe would like to thank Takuma Udagawa, Taku\nSakamoto, and the anonymous reviewers for their\ninsightful comments. This work was supported by\nJSPS KAKENHI Grant Number 21H03502.\n2194\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6077–6086.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: Visual Question An-\nswering. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2425–2433.\nRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nFifth PASCAL Recognizing Textual Entailment\nChallenge. In Proceedings of the Text Analysis Con-\nference (TAC’09).\nEmanuele Bugliarello, Ryan Cotterell, Naoaki\nOkazaki, and Desmond Elliott. 2020. Multimodal\nPretraining Unmasked: Unifying the Vision and\nLanguage BERTs. arXiv preprint:2011.15124.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-\nChun Chen, and Jingjing Liu. 2020. Behind the\nscene: Revealing the secrets of pre-trained vision-\nand-language models. In The 2020 European Con-\nference on Computer Vision, pages 565–580.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\nTask 1: Semantic Textual Similarity Multilingual\nand Crosslingual Focused Evaluation. In Proceed-\nings of the 11th International Workshop on Semantic\nEvaluation, pages 1–14.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: Learning UNiversal\nimage-TExt representations. In The 2020 European\nConference on Computer Vision, pages 104–120.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL Recognising Textual Entail-\nment Challenge. In Proceedings of the First In-\nternational Conference on Machine Learning Chal-\nlenges: Evaluating Predictive Uncertainty Visual\nObject Classiﬁcation, and Recognizing Textual En-\ntailment, page 177–190.\nWilliam B. Dolan and Chris Brockett. 2005. Auto-\nmatically Constructing a Corpus of Sentential Para-\nphrases. In Proceedings of the Third International\nWorkshop on Paraphrasing.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The Third PASCAL Recog-\nnizing Textual Entailment Challenge. In Proceed-\nings of the ACL-PASCAL Workshop on Textual En-\ntailment and Paraphrasing, pages 1–9.\nJanosch Haber, Tim Baumgärtner, Ece Takmaz, Lieke\nGelderloos, Elia Bruni, and Raquel Fernández. 2019.\nThe PhotoBook Dataset: Building Common Ground\nthrough Visually-Grounded Dialogue. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 1895–1910.\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\nManyModalQA: Modality Disambiguation and QA\nover Diverse Inputs. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 34,\npages 7879–7886.\nRonghang Hu and Amanpreet Singh. 2021. Trans-\nformer is All You Need: Multimodal Multitask\nLearning with a Uniﬁed Transformer. arXiv\npreprint arXiv:2003.13198.\nNikolai Ilinykh, Sina Zarrieß, and David Schlangen.\n2019. Meetup! a corpus of joint activity dia-\nlogues in a visual environment. arXiv preprint\narXiv:1907.05084.\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten,\nand Tamara Berg. 2014. ReferItGame: Referring to\nObjects in Photographs of Natural Scenes. In Pro-\nceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing , pages 787–\n798.\nAniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Ha-\njishirzi. 2017. Are you smarter than a sixth grader?\ntextbook question answering for multimodal ma-\nchine comprehension. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pages 4999–5007.\nHector J. Levesque, Ernest Davis, and Leora Mor-\ngenstern. 2012. The Winograd Schema Challenge.\nIn Proceedings of the Thirteenth International Con-\nference on Principles of Knowledge Representation\nand Reasoning, page 552–561.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. VisualBERT: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao\nLiu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2020.\nUNIMO: Towards Uniﬁed-Modal Understanding\nand Generation via Cross-Modal Contrastive Learn-\ning. arXiv preprint arXiv:2012.15409.\nJunyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren\nZhou, and Hongxia Yang. 2021. M6-v0: Vision-\nand-Language Interaction for Multi-modal Pretrain-\ning. arXiv preprint arXiv:2003.13198.\n2195\nJiasen Lu, Dhruv Batra, Devi Parikh, and Ste-\nfan Lee. 2019. ViLBERT: Pretraining Task-\nAgnostic Visiolinguistic Representations for Vision-\nand-Language Tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 32.\nNikita Nangia, Adina Williams, Angeliki Lazaridou,\nand Samuel Bowman. 2017. The RepEval 2017\nShared Task: Multi-Genre Natural Language Infer-\nence with Sentence Representations. In Proceedings\nof the 2nd Workshop on Evaluating Vector Space\nRepresentations for NLP, pages 1–10.\nAlexander Ororbia, Ankur Mali, Matthew Kelly, and\nDavid Reitter. 2019. Like a baby: Visually situated\nneural language acquisition. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5127–5136.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2383–2392.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster R-CNN: Towards Real-Time Ob-\nject Detection with Region Proposal Networks. In\nAdvances in Neural Information Processing Systems,\nvolume 28.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual Captions: A\nCleaned, Hypernymed, Image Alt-text Dataset For\nAutomatic Image Captioning. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2556–2565.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: Pre-\ntraining of Generic Visual-Linguistic Representa-\ntions. In International Conference on Learning Rep-\nresentations.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A Corpus for\nReasoning about Natural Language Grounded in\nPhotographs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 6418–6428.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learn-\ning Cross-Modality Encoder Representations from\nTransformers. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, pages 5100–\n5111.\nRyota Tanaka, Kyosuke Nishida, and Sen Yoshida.\n2021. VisualMRC: Machine Reading Comprehen-\nsion on Document Images. In the AAAI Conference\non Artiﬁcial Intelligence.\nTakuma Udagawa and Akiko Aizawa. 2019. A nat-\nural language corpus of common grounding under\ncontinuous and partially-observable context. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 33, pages 7120–7127.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding. In Inter-\nnational Conference on Learning Representations.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural Network Acceptability Judg-\nments. Transactions of the Association for Compu-\ntational Linguistics, 7:625–641.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-Art Natural Language Process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45.\nSemih Yagcioglu, Aykut Erdem, Erkut Erdem, and Na-\nzli Ikizler-Cinbis. 2018. RecipeQA: A Challenge\nDataset for Multimodal Comprehension of Cooking\nRecipes. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1358–1368.\nA Dataset Statistics\nDataset Task Size #vocab. Word ov\nbtw CC\nV&L pre-training\nCC CAP 2.8M 48,360 1\nCC (val) CAP 14K 10,442 0.63\nGLUE benchmark\nWNLI NLI 635 1.622 0.08\nRTE NLI 2.5K 23,341 0.24\nMRPC P/S 3.7K 13,926 0.26\nSTS-B P/S 5.7K 16,436 0.25\nCoLA SS 8.6K 7,845 0.19\nSST-2 SS 67K 14,816 0.26\nQNLI NLI 104K 148,413 0.29\nQQP P/S 364K 193,041 0.28\nMNLI NLI 393K 167,790 0.34\nTable 6: Training dataset statistics. CC: The Con-\nceptual Captions dataset (Sharma et al., 2018). CAP:\nimage captioning, P/S: paraphrase/similarity task, SS:\nsingle-sentence task.\n2196\nB Additional Data for Overall Results\nTable 7 shows the SDs to the averaged scores of\nV&L models on the GLUE tasks’ development\nsets.\navg CoLA SST-2\nBERTBASE\n77.3\n(0.8)\n54.6\n(1.1)\n92.5\n(0.1)\nVISCTRL\n72.5\n(1.2)\n38.6\n(7.3)\n89.4\n(0.4)\nUNICTRL\n71.4\n(0.3)\n37.4\n(6.5)\n89.7\n(0.5)\nVLCTRL\n72.4\n(0.8)\n38.7\n(1.5)\n89.8\n(0.9)\nVILCTRL\n70.9\n(0.8)\n36.1\n(6.0)\n90.4\n(0.5)\nLXCTRL\n70.5\n(0.2)\n39.0\n(6.1)\n90.2\n(0.5)\nMRPC QQP STS-B\nBERTBASE\n81.9 / 87.6\n(0.6) / (0.5)\n90.6 / 87.4\n(0.0) / (0.1)\n88.2 / 87.9\n(0.3) / (0.3)\nVISCTRL\n71.9 / 82.1\n(1.4) / (0.8)\n89.4 / 86.0\n(0.1) / (0.1)\n81.8 / 81.7\n(4.0) / (3.6)\nUNICTRL\n74.9 / 75.6\n(2.0) / (2.2)\n69.3 / 80.3\n(0.8) / (0.7)\n89.2 / 85.7\n(0.1) / (0.1)\nVLCTRL\n70.6 / 81.8\n(0.5) / (0.3)\n89.0 / 85.4\n(0.3) / (0.4)\n82.9 / 82.8\n(2.3) / (1.9)\nVILCTRL\n69.0 / 79.4\n(1.3) / (2.1)\n88.6 / 85.0\n(0.2) / (0.1)\n77.7 / 78.0\n(1.2) / (0.9)\nLXCTRL\n69.8 / 80.4\n(1.3) / (1.1)\n89.0 / 85.4\n(0.1) / (0.2)\n75.3 / 75.3\n(0.8) / (0.7)\nMNLI QNLI RTE WNLI\nBERTBASE\n84.2\n(0.1)\n91.0\n(0.4)\n62.5\n(1.5)\n48.8\n(5.8)\nVISCTRL\n81.6\n(0.2)\n87.0\n(1.1)\n56.6\n(1.9)\n53.1\n(4.6)\nUNICTRL\n80.9\n(0.4)\n86.0\n(1.0)\n55.6\n(2.4)\n55.4\n(1.3)\nVLCTRL\n81.2\n(0.2)\n86.3\n(0.1)\n55.7\n(1.4)\n53.1\n(3.5)\nVILCTRL\n79.9\n(0.5)\n83.8\n(0.6)\n53.7\n(0.9)\n55.4\n(1.8)\nLXCTRL\n80.4\n(0.2)\n84.2\n(0.2)\n57.2\n(3.4)\n46.0\n(9.2)\nTable 7: Standard deviations of our results in the perfor-\nmance on the GLUE tasks’ development sets (Table 2).\nSDs are shown in parentheses below each value. We\nran three experiments for each task.\nC Additional Data for Analysis\nWe show the distributions of sentence length and\nreadability mentioned in Section 4.2 in Figure 2\nand Figure 3, respectively.\nsingle-streamdual-stream\nFigure 2: The sentence length distributions in the prob-\nlem sets solved only by the V&L model and only by\nBERT. In each plot, the area of the distribution is nor-\nmalized to 1. The range of the vertical axis is [0, 0.020].\nsingle-streamdual-stream\nFigure 3: The Flesch–Kincaid Grade Level distribu-\ntions of sentences in the problem sets solved only by\nthe V&L model and only by BERT. In each plot, the\narea of the distribution is normalized to 1. The range of\nthe vertical axis is [0, 0.15].",
  "topic": "Natural language understanding",
  "concepts": [
    {
      "name": "Natural language understanding",
      "score": 0.7984826564788818
    },
    {
      "name": "Computer science",
      "score": 0.768704891204834
    },
    {
      "name": "Language model",
      "score": 0.6316744089126587
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.6028331518173218
    },
    {
      "name": "Natural language",
      "score": 0.54966801404953
    },
    {
      "name": "Independence (probability theory)",
      "score": 0.49811363220214844
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4878980815410614
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.47647520899772644
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.46668490767478943
    },
    {
      "name": "Contrast (vision)",
      "score": 0.4527930021286011
    },
    {
      "name": "Natural language processing",
      "score": 0.4286278486251831
    },
    {
      "name": "Mathematics",
      "score": 0.11910519003868103
    },
    {
      "name": "Statistics",
      "score": 0.11064350605010986
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}