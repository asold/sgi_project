{
    "title": "Automatic Identifier of Socket for Electrical Vehicles Using SWIN-Transformer and SimAM Attention Mechanism-Based EVS YOLO",
    "url": "https://openalex.org/W4387247756",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5092984118",
            "name": "V. C. Mahaadevan",
            "affiliations": [
                "SRM Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5023804505",
            "name": "R. Narayanamoorthi",
            "affiliations": [
                "SRM Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5030895613",
            "name": "Radom√≠r Go≈ào",
            "affiliations": [
                "VSB - Technical University of Ostrava"
            ]
        },
        {
            "id": "https://openalex.org/A5087742170",
            "name": "Petr Mold≈ô√≠k",
            "affiliations": [
                "VSB - Technical University of Ostrava"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2783348408",
        "https://openalex.org/W3126779847",
        "https://openalex.org/W2924035145",
        "https://openalex.org/W2988946191",
        "https://openalex.org/W2762248135",
        "https://openalex.org/W2161969291",
        "https://openalex.org/W2151103935",
        "https://openalex.org/W2183182206",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W4286801158",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2193145675",
        "https://openalex.org/W2570343428",
        "https://openalex.org/W6750227808",
        "https://openalex.org/W3018757597",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W3034681889",
        "https://openalex.org/W4289752563",
        "https://openalex.org/W3177052299",
        "https://openalex.org/W3048201280",
        "https://openalex.org/W2995594259",
        "https://openalex.org/W2991192987",
        "https://openalex.org/W4241771866",
        "https://openalex.org/W2999847126",
        "https://openalex.org/W2990688248",
        "https://openalex.org/W2936718694",
        "https://openalex.org/W3135934332",
        "https://openalex.org/W3153249728",
        "https://openalex.org/W4291805279",
        "https://openalex.org/W3210586215",
        "https://openalex.org/W4309570031",
        "https://openalex.org/W4317383044",
        "https://openalex.org/W3015064087",
        "https://openalex.org/W2409906311",
        "https://openalex.org/W6736157709",
        "https://openalex.org/W4281291638",
        "https://openalex.org/W4309225348",
        "https://openalex.org/W2963857746",
        "https://openalex.org/W3042011474",
        "https://openalex.org/W2962949934",
        "https://openalex.org/W4291819654",
        "https://openalex.org/W3215034392",
        "https://openalex.org/W2982363097",
        "https://openalex.org/W6796538260",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2919522067",
        "https://openalex.org/W3207050304",
        "https://openalex.org/W3202546031",
        "https://openalex.org/W4220929648",
        "https://openalex.org/W4225304548",
        "https://openalex.org/W4321377727",
        "https://openalex.org/W2964254151",
        "https://openalex.org/W3106250896",
        "https://openalex.org/W4293584584"
    ],
    "abstract": "Electric vehicle (EV) technology is emerging as one of the most promising solutions for&#13;\\ngreen transportation. The same growth occurs in the charging infrastructure development and automating&#13;\\nthe EV charging process. Globally, EVs has different types of charging sockets and it‚Äôs located at the&#13;\\nvarious positions in the Vehicle. In simple, EV has a diversity in socket type and socket location. Hence,&#13;\\ncorrectly identifying the socket type and location is mandatory to automate the charging process. The&#13;\\nrecent development in computer vision and robotic systems helps to automate EV charging without human&#13;\\nintervention. Image processing and deep learning-based socket identification can help the EV charging&#13;\\ninfrastructure providers automate the process. Moreover, the deep learning techniques should be simple&#13;\\nenough to implement in the real-time processing boards for experimental viability. Hence, this paper proposes&#13;\\na new You Only Look Once (YOLO) model called the Electric Vehicle Socket (EVS) YOLO that uses&#13;\\nYOLOv5 as its base architecture with the addition of a vision-type transformer called the SWIN-Transformer&#13;\\nand an attention mechanism called SimAM for better performance of the model in detecting the correct&#13;\\ncharging port. A dataset of 2700 images with six types of classes has been used to test the model, and the&#13;\\nEVS -YOLO also evaluated with varying mechanisms of attention positioned at various places along the&#13;\\nhead. The paper contrasts the suggested model with alternative deep learning architectures and analyzes&#13;\\nrespective performances.",
    "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nAutomatic Identifier of Socket for Electrical \nVehicles using SWIN-Transformer and SimAM \nAttention Mechanism-based EVS YOLO \nV.C. Mahaadevan1, Narayanamoorthi R1, Radomir Gono2, Petr Moldrik2 \n1Electric Vehicle Charging Research Centre, Department of Electrical and Electronics Engineering, SRM Institute of Science and Technology, Chennai, India. \n2Faculty of Electrical Engineering and Computer Science, VSB-Technical University of Ostrava, 708 00 Ostrava, Czech Republic \nCorresponding author: Narayanamoorthi R1 (e-mail: narayanamoorthi.r@gmail.com). \nABSTRACT Electric vehicle (EV) technology is emerging as one of the most promising solutions for green \ntransportation. The same growth occurs in the charging infrastructure development and automating the EV charging \nprocess. Globally, EVs has different types of charging sockets and it‚Äôs located at the various positions in the Vehicle. \nIn simple, EV has a diversity in socket type and socket location. Hence, correctly identifying the socket type and location \nis mandatory to automate the charging process. The recent development in computer vision and robotic systems helps \nto automate EV charging without human intervention. Image  processing and deep learning-based socket identification \ncan help the EV charging infrastructure providers automate the process. Moreover, the deep learning techniques should \nbe simple enough to implement in the real-time processing boards for experimental viability. Hence, this paper proposes \na new You Only Look Once (YOLO) model called the Electri c Vehicle Socket (EVS) YOLO that uses YOLOv5 as its \nbase architecture with the addition of a vision -type transformer called the SWIN -Transformer and an attention \nmechanism called SimAM for better performance of the model in detecting the correct charging port. A dataset of 2700 \nimages with six types of  classes has been used to test the model, and the EVS  -YOLO also evaluated with varying \nmechanisms of attention positioned at various places along the head. The paper contrasts the suggested model with \nalternative deep learning architectures and analyzes respective performances.   \n \nINDEX TERMS SWIN-Transformer, Attention mechanism, YOLOv5, Electric Vehicles, Socket detection, SimAM.  \n \nI. INTRODUCTION \nThe current technological revolution, the electric vehicle, uses \nbatteries instead of fuel -based technologies to help reduce \nemissions [1], [2]. Since then, EVs have advanced \nsignificantly, from efficient designs to many sorts of charging \nsockets [3], [4]. Even though there has been growth since \ndifferent EV models employ various kinds of sockets, the \ntechniques used to charge electric vehicles now rely \nsignificantly on manual operation, which can result in \nproblems including parking the vehicle according t o socket \nlocation, heavyweight handling approx. 1.5 kg and choose the \ncorrect plug and docking of charging ports, which needs some \neffort. The EV has a charging port at five locations: front right \n& left, rear right & left, and front middle  [5], as shown i n \nFigure 1. Most vehicles' charging ports are located at the right \nrear at 36%, followed by 28% at the left end and 22% at the \nleft front. Only 10 % of the global vehicle has front -end \ncharging, which enable ease of parking, and the rest need some \neffort to align the vehicle according to the charging station \ngun. From this study, there is a need for automated charging \nrobots enabled by machine vision. Electric shock is another \npossible safety risk brought on by old insulation and manual \noperation. The need for intelligent and humanized services [6] \nin the electric car \n \n \n  \n  FIGURE 1. Global Charging port position \n \ncharging sector is rising in the age of intelligence. To enhance \nthe charging experience for owners of electric vehicles and to \nreduce the hazards [7], research on automated identification \nmethods based on image recognition and the creation of \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n2 \n \nautomatic charging control systems. Robotic, automatic \ncharging is one of the many methods investigated in the study \nof electric cars automated charging systems. For instance, \nTESLA [8] has created an autonomous charging robot as a \nsnake that can stoop down to locate the charging port and \nattach the charging gun to the c harging socket. The \nVolkswagen Automotive Company created the E -Smart \nConnect system, employing a camera to detect the vehicle's \nlocation and interface while also controlling a KUKA robot to \ncharge [9]. \nOther studies have concentrated on very accurate \ncharging port locations utilizing binocular vision or ultrasonic-\nbased techniques [10], [11], [12]. [13] Samsung's EVAR is the \nlatest autonomous charging system for electric cars. [14] \nHyundai Motors recently developed an Automatic charging \nrobot (ACR). Most automated charging systems proposed, \nconceived & developed are limited to specific sockets & \nconnectors and particular locations at the vehicle and more \nend-user involvement. Hence, a 6 DOF robot with a 3D vision \nsensor mount on AGV is proposed that moves in the parking \nline as guided to ensure the entire system moves around the \nvehicle. The rob ot's vision sensor with image processing \ncapability will detect the charging port by moving the whole \nrobot system on the parking line, get the correct charging gun \nfrom the station through a suitable control system, and achieve \ncharging by plug -in with im age processing support. The \nprocess will reverse to keep the charger gun back at the station. \nThrough this proposed automated system, any electric car with \nany charging socket located in any location of a vehicle can be \ncharged.  \nThe image processing area alone is discussed in this \npaper. Safety is a top priority when designing automated \ncharging systems for electric vehicles [15]. Hence, it is crucial \nto distinguish between purposeful encounters and \nunintentional accidents when robots and vehicles come int o \ntouch during the charging process. Despite research advances, \nlittle is known about these vehicle -robot interaction \ncomponents. Given the significance of new energy -based \nvehicles and the rising demand for intelligent charging \nsolutions, there is a need for more research and development \nin electric car charging facilities. The electric vehicle charging \nindustry can enhance the effectiveness, security, and user \nexperience of electric vehicle charging by utilizing cutting -\nedge technologies like image recogn ition, robotics, and \nautomation. It will help to promote the overall growth of the \nelectric vehicle ecosystem and the international efforts to \nconserve energy and reduce emissions.  \nTo address such concerns, applying deep learning \nthrough object detection can play a role by automatically \ndetecting the type of charging socket required by the electric \nvehicle, eliminating the need for the driver to select the \nappropriate one. s manually. Using deep learning algorithms, \nEV charging stations can detect a wide r ange of charging \nsocket types, including older or less common types, making it \neasier for EV drivers to charge their vehicles. By automating \nthe socket detection process, EV charging stations can reduce \nthe need for additional personnel, resulting in opera tor cost \nsavings. It is possible through object detection, which forms \nthe basis of an automatic EV charging system. Similarly, \ncharging sockets can also be detected. Every sort of socket will \nhave a unique quality. For example, the type 1 plug, which can \nsupport a power injection of up to 7.4 kW (230 V, 32 A), is a \nsingle-phase connector. \nType 2 plug is a triple-phase plug. Type 2 sockets at \npublic charging stations provide power levels of up to 43 kW \n(400 V, 63 A, AC). The Combined Charging System plugs in \nshort Combination plugs or CCS with power levels of up to \n170 kW. However, in practical scenarios, the value frequently \nremains in the vicinity of 50 kW. A wide variety of electric \nvehicles can use the CHAdeMO plug, which is built for them \nand offers a charge of 50kW, designed specifically for Tesla \nelectric vehicles. These chargers come with a charging \ncapacity between 150kW and 250kW. \nIdentifying the existence and placement of objects in \nan image or video is object detection in computer vision. \nObject detection algorithms coupled with a label specifying \nthe item type produce Bounding boxes that encircle the things \nin the picture. Some challenges associated with this task \ninclude occlusions, background clutter, and scale variations. \nSeveral approaches are  developed, which typically revolve \naround learning complex representations of objects and their \nfeatures to solve these issues. In deep learning, a feature refers \nto a pattern or characteristic detected or extracted from input \ndata relevant to the problem  being solved. Features are \nfrequently utilized to express the input data more concisely \nand appropriately to make it simpler. For instance, in image \nrecognition, a feature may be a specific edge or texture pattern \ntypical of a particular class of objects. A distinctive word or \nphrase to a feeling or topic might be a feature in natural \nlanguage processing. Feature methods like Histogram of \nOriented Gradients (HOG) [16] and Scale -Invariant Feature \nTransform (SIFT) [17] are categorized as handcrafted feature \nextraction methods.  \nHandmade feature extraction methods involve \nmanually designing algorithms or heuristics to identify and \nextract features from input data. These methods typically use \ndomain knowledge and human expertise to develop effective \nfeature extractors. HOG and SIFT  are examples of widely \nused handcrafted feature extraction methods in computer \nvision. HOG works by computing the distribution of gradient \norientations in an image, while SIFT extracts features by \ndetecting key points and describing their local appearance  \nusing scale-invariant descriptors. The capacity of more recent \ndeep learning techniques to automatically learn the most \npertinent and discriminative features from the data, without \nthe need for human skill or domain knowledge, is one of their \nkey benefits  over more traditional feature extraction \ntechniques like HOG and SIFT.  \nDeep learning models can achieve cutting -edge \nperformance on a broad range of tasks by learning to extract \ncomplicated and abstract characteristics from the data by \ntraining on vast v olumes of labeled data. The main \ncontribution of this paper includes:   \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n3 \n \n‚Ä¢ It effectively detects charging ports embedded in \nelectric vehicles using a novel network architecture \nEVS-YOLO that uses a Swin transformer to improve \nself-attention. \n‚Ä¢ A SimAM attentio n mechanism is added in 3 \nlocations across the detection heads in the \narchitecture, and a comparison of different attention \nmechanisms is to come to a consensus on using the \nSimAM attention module. \n‚Ä¢ A dataset [18] of 2700 images with six types of classes \nwas used and the same was operated on different \nmodels and results are compared between proposed \nEVS-YOLO network architecture and existing object \ndetectors. The proposed EVS -YOLO model has a \nMean Average Precision (mAP) of 81.4, the highest \ncompared to the other object detectors. \n \nII. LITERATURE SURVEY \n \nA. Real-Time object detectors \n \nModern-day deep learning frameworks are aggregated into \ntwo types: single-stage and two-stage detectors. The bounding \nboxes and class probabilities of objects in an image are directly \npredicted by a single-stage detector, sometimes called a one-\nstage detec tor. These versions are perfect for real -time \napplications since they are often quicker and easier to use than \ntwo-stage detectors. Contrarily, two-stage detectors have two \nsteps. The model creates region proposals in the first step, \npotential placements for objects in the picture. The model uses \nthese suggestions in the second step to categorize items and \nimprove the bounding box coordinates. An example of this \ntype is R-CNN, proposed by Girshick et al. [19] This approach \nuses a method to extract features from an image, followed by \na step of the regional proposal to identify potential object \nlocations and a classifier to classify objects in each proposed \nregion.  \n Fast RCNN [20], [ 21] improved upon R -CNN by \nusing a single CNN to extract features for both th e region \nproposal and object classification stages, resulting in faster \ntraining and inference. One another example that portrays the \ntwo-stage detector method is Faster RCNN [2 2], which \nintroduced a Region Proposal Network (RPN) that shares \nconvolutional features with the object detection network, \nallowing for even faster training and inference.  \nThe RPN generates region proposals more efficiently \nthan the external region proposal algorithms used in R-CNN. \nEven though R -CNN was a significant advance in obj ect \ndetection, it had several limitations, including slow training \nand inference and limited flexibility. Its successors, such as \nFast R-CNN Faster R-CNN, addressed these limitations and \nsignificantly improved the accuracy and efficiency of object \ndetection models. On the other hand, in SSD [23] and YOLO \n[24], [25], [26] the input images are fragmented into a grid of \ncells, and for each cell, bounding boxes and probabilities of \nclass are predicted. On the other hand, SSD uses multiple \nlayers with different aspect ratios to manage objects of \ndifferent shapes and scales. RetinaNet [27] uses an FPN \n0662to identify objects at various dimensions and resolutions. \nIt also includes a novel focal loss function that addresses the \nclass imbalance problem in object detection, where there are \nmany more background pixels than object pixels in an image.  \n \nB. Attention modules \n \nAttention modules are utilized in object identification to assist \nthe model in concentrating on the areas of an image that are \nmost important for detecting objects. According to their \nimportance to the task, distinct parts of a picture are given \nvaried weights by attention modules. The model's attention is \ndirected to the areas of a concept that are the most informative \nusing weights learned during training. It enables the model to \nconcentrate on an image's most crucial elements while \navoiding unimportant or distracting aspects. To upgrade the \nmodel's capacity to focus on critical properties of the input \ndata, deep learning models employ two different types of \nattention methodologies: channel and spatial.  \nA CNN's channel attention mechanism may be \ntrained t o highlight or suppress particular feature map \nchannels selectively. Contrarily, a spatial attention technique \nmay be instructed to selectively emphasize or hide certain \nspatial positions in a CNN's feature maps. SE attention [2 8] \nmodule consists of the sq ueeze and excites operations. The \nsqueezing process aggregates the spatial dimensions of each \nfeature map into a single scalar value by a global average \npooling technique. It decreases the dimensions of the feature \nmaps while retaining the channel-wise information. The final \nstep is to apply the generated vector to a two-layer perceptron \nthat learns to represent the channel -wise correlations. The \nexcitation operation, a sigmoid activation function, scales the \nlearned attention map produced by a two -layer perceptron. \nCBAM [29] attention module consists of channel and spatial \nattention modules, wherein the SAM records the spatial \ndependencies, and the CAM learns the channel -wise feature \nmaps. Therefore, the CBAM attention module can adaptively \nrecalibrate the feature maps across both spatial and channel \ndimensions by integrating the CAM and SAM processes.  \n The coordinate attention [ 30] module aims to \nimprove by ingraining positional information with channel \nattention with the help of two blocks: coordinate information \nembedding and coordinate attention generation blocks. The \nfirst block replaces the typically used global pooling operation \nwith a two 1 - D encoding structure to better preserve the \npositional information.  \nThe second block lets the model concentrate on an \nimage's most crucial elements while avoiding unimportant or \ndistracting aspects. Deep learning models employ two types of \nattention methodologies, channel and spatial, to upgrade the \nmodel's capacity to focus on key properties of the input data. \nA CNN's channel attention mechanism may be trained to \nhighlight or suppress particular feature map channels \nselectively. Contrarily, a technique called spatial attention the \ncaptured positional data to identify the r egions of interest \nprecisely, and it is also capable of effectively capturing inter-\nchannel relationships. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n4 \n \n \nTABLE 1.   Summary of related works.\nC. Detection methods based on the YOLO series \n \nThe popularity of the YOLO series due to its speed, its real -\ntime inference of images and videos as it is a one -stage \ndetector, its accuracy, its versatility to be trained on different \ndatasets, and its ability to be able to detect a wide variety of \nobjects it is has been employed in many fields [31], [32], [33], \n[34], [35], [36], [37], [38] and performs very well concerning \nother methods. Tian et al. [3 9] Proposed a method for \ndetecting apple lesions using a YOLOv3 -dense network; it \nalso included the usage of cycle GAN as an image \naugmentation tool to improve the results as traditional \nYOLOv3 networks don't have the inbuilt ability for data \naugmentation.  \nThe consequent model in the YOLO series is \nYOLOv4, which introduced a CSP -based structure into the \nbackbone and added SPP and PAN to improve the multi-scale \nfusion of features. It also added an improved head and data \naugmentation technique for better performance. Cai et al. [40] \nintroduced YOLOv4-5D for object detection in autonomous \ndriving in which the existing CSPdarknet53 backbone was \ncombined with deformable convolutional layers, and the \nnetwork was modified by adding two large-scale layers to be \neffective in detecting smaller objects. The proposed model \nwas also pruned in terms of parameters. Since the proposed \nmodel had five different detection scales, the feature fusion \nnetwork was modified to accompany that change. One another \nexample that shows the versatility of the YOLO series is [41] \nin the field of marine target detection, wherein the CBAM \nattention module was added to the three branches of the feature \nfusion network to improve the accuracy of the result. Guney \net al. [42], [43] used YOLO v5 for the ADAS system in cars \nto recognize sign boards on roads to assist drivers in real-time.  \nYOLOv5 was improved upon the predecessor by introducing \nchanges to its backbone; it is also a shallower model, thus \nmaking it faster to train on. In 2021, Zhu et al. [44] introduced \na modified YOLOv5 network called transformer prediction \nheads YOLOv5 to aid object detection in drone scenarios. It \nwas improved by the addition of transformer heads for \nprediction and the addition of the CBAM attention module. \nTherefore, when tested on the VisDrone dataset, it \noutperformed the base model by 7%. Due to the ability of real-\ntime and fast inference, the YOLO models are helpful in \napplications involving monitoring and sorting. Wang et al. \n[45] portray the application of a modified YOLOv5 network \ncalled YOLO-BS which includes a SimAM attention module \nby detecting more giant coal blocks to aid congestion in \nunderground mine scraper conveyors. \n \nD. Related works on EV charging systems  \n \nThe capacity to identify and locate the charging outlet is \nessential for autonomous charging as it also affects the \nsystem's dependability. It is critical to attenuate the negative \nimpacts of a complicated environment by using the necessary \nalgorithms. For an autonomous vehicle charging system to \nwork efficiently, detecting the charging port used in an electric \nvehicle is essential; therefore, the algorithm used for the \nReferences Analysis type Models Techniques Reference \nZhang et al. 2016 Images Based on HSI Color Model Image filtering, threshold segmentation, morphology processing, edge detection, feature \nextraction [47]\nMiseikis et al. 2017 Images CATIA  v5 Region-based Convolutional Neural Networks, Deep learning [48]\nJiang et al. 2019 Image , Video Frames YOLO v3 Multi-agent deep reinforcement learning [60]\nBochkovskiy  et al. 2020 Video Frames YOLO v4 Bag of Freebies (BoF) and Bag of Specials (BoS), Data Augmentation, CmBN [25]\nPan et al.  2020 Images Based on CNN Convolution neural network, Deep learning [46]\nDirir  et al.  2021 Video Frames , Images YOLO v2 Region-based Convolutional Neural Networks, Single Shot Detector [56]\nZhou et al.  2021 3D point cloud data Based on PV-RCNN model 3D point cloud technique [61]\nShibl et al.  2021 Images Based on LSTM K-Nearest Neighbors, Deep learning [62]\nPark et al. 2022 Video Frames DQN,VCE Deep Reinforcement Learning, convolution neural network,RNN [63]\nGuney et al.  2022 Images Based on YOLO v4, CNN HSV Color Space [50]\nLin et al. 2022 Images Based on CNN, LSTM Deep Convolutional Neural Network, Support Vector Machine, KNA [64]\nLi et al.  2022 Images DUAL-200M-030T160 Semi-global block matching, Fast Library for Approximate Nearest Neighbors [49]\nGuney el al.2022 Images YOLO v5 Portable and image-based ADAS system for real-time detection of traf c signs, vehicles, and \npedestrians. [42]\nElKashlan  et al. 2023 Network traffic data(IoT) Based on CNN Convolution neural network, Deep learning [65]\nKaranam et al.  2023 Time series data LSTM, HMM Recurrent neural network, SVM [66]\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n5 \n \ndetection must aid the robotic system accurately to get the best \nresults. Zhao et al. [4 6] proposed a method for fast \nidentification and localized detection of the socket with a \ncombination of a modified YOLOv4 network for quick \nrecognition and mean shift clustering to improve the success \nrate by removing noise and an affine transformation me thod \nfor the correction of coordinates. Mingqiang et al. [47] use a \nmodified Lenet-5 model in which the ReLU function is used, \nthe dimensionality of the Final convolution is changed, and the \nlearning rate is optimized for the recognition of the socket. The \nsocket was located by using a feature circle method. Zhang et \nal. [4 8] describe a process for using the vision software \nHALCON to automatically extract the characteristic \nparameters of an electric vehicle charging hole. The method \ninvolves filtering the o riginal image using various image \nprocessing techniques.  \nMiseikis et al. [4 9] presented an approach that \ncombines shape-based template matching, stereo cameras, and \na robot with a connector plug to localize and approach the \ncharging socket of an EV or PHEV. The method uses marker \nless eye-to-hand calibration to estimate the location and \norientation of the charging socket and observes the forces \nexerted on the robot's end -effector to prevent misalignment. \nThe approach has succeeded in lab conditions using a custom-\nmade charging port holder and indoor illumination. T.Li et al. \n[50] propose a method for accurately identifying and \npositioning charging ports, even under varying light intensities \nand backgrounds. The technique uses the SIFT feature \nextraction algorithm and FLANN matching algorithm to attain \na high -precision mapping of points. It then employs the \nSGBM algorithm for binocular ranging for calculating the \ndepth of the socket. The proposed method was validated \nthrough binocular range and image identification experiments, \ndemonstrating high-precision results. Behl et al. [ 51], This \ntechnology employs HSV color space to recognize and \nmonitor the location of the female socket. It combines a \nmobile male socket on the shore charging station with a \nstationary female socket aboard the ship. The system was \ntrained using the YOLO model for quicker and more precise \nidentification, and an application interface was created for \nreal-time monitoring. Table 1 depicts the summary of related \nworks that clearly illuminate s data such as method, \ntechnology, and description of various studies. \n \nIII. METHODOLOGY \n \nA. YOLOv5 structure \n \nYOLOv5, being one of the widely used object detection \nalgorithms due to its speed and effectiveness, has intrigued \nresearchers to explore its possibilities in bringing change in its  \noverall architecture, which comprises the head, neck, and \nbackbone. Feature information gets extracted in the backbone \nand gets gathered in the neck. Based on the feature maps \ncreated, the head detects the predictions. CSPDarket53 \nframework with Spatial Pyramid Pooling-Fast (SPPF) layer \nmakes up the backbone gets employed, PANe t as neck and \ndetection head completes the YOLOv5 basic architecture. The \nfeature pyramids are obtained using the PANet. With the \naccuracy and speed of a pyramid in mind, the Feature Pyramid \nNetwork (FPN) [ 52] feature extractor was developed. As \ncompared to older models like the quicker RCNN, it creates \nnumerous layers of feature maps with greater quality \ninformation than the usual feature pyramid. The FPN is made \nup of top-down feature pyramids and a bottom-up path. The \nbottom-up method extracts features u sing a typical \nconvolutional network. \nAs we climb, the spatial resolution decreases. Each \nlayer becomes more important semantically when more high-\nlevel structures are found. The YOLO deep network uses \nresidual and dense blocks to overcome the vanishing gradient \nproblem, enabling information to go to the deepest layers. \nHowever, one advantage of having thick and residual blocks \nis the problem of recurring gradients. CSPNet [53] solves this \nproblem by discretizing the gradient flow. Convolutional \nneural networks are designed to perform better, and one sort \nof feature aggregation module, CSPNet, seeks to do just that \n(CNNs) [54]. The CSPNet module serves as the backbone of \nthe network architecture in YOLOv5. The object detection \nhead uses feature maps produced by the spine to forecast the \npredictions. The three phases of the CSPNet module in \nYOLOv5 [55], [56] each feature a set of convolutional layers \nfollowed by a cross-pathway link. It achieves this by dividing \nthe network into central and cross pathways. \nThe main pathway analyses the input data and creates \nfeature maps. More information flow is made possible by \ndoing this, which also lessens the chance that important data \nmay be lost while being processed. Incorporating CSPNet in \nYOLOv5 enables maintaining  a smaller size and a higher \ninference speed. This is because CSPNet enhances the \nnetwork's information flow and enables more effective feature \naggregation. The final product is an application-friendly object \ndetection model that is very effective and accu rate. CSPNet \ndecreases the model's parameters and Flops, which not only \nenhances inference speed and accuracy but also addresses the \nproblems with recurrent gradient information in large -scale \nbackbones and trims down the model's size. \nFast and accurate data detection is essential, and the \nmodel's size also determines the effectiveness of its inference \non devices with minimal computational resources. The SPPF \nblock produces a fixed-length result after it has combined the \ndata it received from the inputs. As a result, without degrading \nthe network's performance, it has the advantage of greatly \nincreasing the receptive field using this block in earlier \niterations of YOLO; however, to increase network speed in \nYOLOv5, SPPF just another variation of the SPP bloc k was \nutilized. The classes of the discovered objects, their bounding \nboxes, and the abjectness scores are the three outputs. \nYOLOv5 produces. In calculating the location loss, CIoU [57] \nloss is used. The following equation provides the ultimate loss \nformula.       \nLoss =  Œª1Lcls +  Œª2Lobj +  Œª3Lloc       (1) \nDifferent activation functions, attention \nmechanisms, and modifying the backbone have been done \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n6 \n \n \nFIGURE 2. Base architecture of yolov5 showing three regions backbone, neck and head. \n \npreviously to improve the performance of the YOLOv5 \ndetection algorithm. Sigmoid linear units are the standard \nactivation function used in YOLOv5. YOLOv5 comes in five \ndifferent sizes, namely. YOLOv5x, YOLOv5N, YOLOv5M, \nYOLOv5L, and YOLOv5s. The model architecture in all five \nmodels remains the same, but changes in the width and depths \ncan be noticed. YOLOv5X being the largest and YOLOv5n \nbeing the smallest. The larger models tend to perform better, \nbut they are computationally intensive. We use the YOLOv5s \narchitecture a base for our proposed EVS-YOLO architecture. \nThe architecture of the YOLOv5 model is depicted in Figure \n2. Base architecture of yolov5, which is predominantly made \nof th ree regions backbone, neck and head, wherein CBL \nconsists of convolution, batch normalization and activation \nlayer. C3 refers to a cross stage partial network with 3 \nconvolutions and concat refers to the concatenation operation. \nSPPF refers to a faster ver sion of spatial pyramid pooling \nwhich reduces the dimensionality and improves the network \nspeed. \n \nB. SWIN TRANSFORMER \n \nIn addition to effectively modeling global contextual \ninformation, the transformer also exhibits great transferability \nto downstream tasks w hen pretraining on a large scale. It \noffers new opportunities for visual feature learning and has \nobserved the performance of the transformer in various deep-\nlearning avenues. The transformer creates a method for global \ninformation exchange that aids in cr eating a suitable feature \nrepresentation. Nevertheless, using the transformer for visual \ntasks has two big drawbacks. On the one hand, the \ntransformer's use is severely constrained by the high \ncomputing cost of its transformation, which employs \nsequences as input. \n \nOn the other hand, Transformer mines correlations from \nglobal linkages instead of local inductive bias in convolution \nand needs training with a lot of data to provide good results. \nThe introduction of the Swin transformer expands the \npotential us es of transforms in visual activities. The \ncomputational overhead of the swing transformer is minimal. \nHierarchical structures are built as it analyses pictures, \nenabling the swing -based model to tackle multiscale heavy \njobs. The proposed work modifies the  YOLOv5s network \ntopology to incorporate the Swin module, allowing the \nnetwork to do global modeling while using fewer computing \nresources. The window based multi -head self-attention (W-\nMSA) module was proposed by Swin Transformer. There are \nseveral windows split up into the image. Swin Transformer \nreduces the computing complexity to a linear relationship by \nperforming attention computations exclusively on the window \npixel areas. Importantly, the Swin Transformer interacts with \ninformation across non -overlapping windows utilizing a \nmulti-head self-attention module for shifted windows (SW -\nMSA). A shifted window partitioning strategy in the Swin \ntransformer is incorporated such that the switches between \ntwo partitioning configurations in subsequent Swin \nTransformer blocks allow for cross -window connections to \nensure the effectiveness of non -overlapping window \ncomputation. It is difficult to gather global contextual \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n7 \n \ninformation due to CNN's constrained perceptual field. The \nSwin Transformer, in comparison, makes  use of more \nadaptable self -attention information transmission and \nperforms very well in obtaining global semantic information \nand effectiveness. \n \n \nFIGURE 3. The Swin transformer with W-MSA and SW-MSA.  \nThe architecture of the Swin Transformer is shown in Figure \n3. The Swin transformer predominantly contains two blocks \nnamely Windows Multi -Head Self -Attention module             \n(W-MSA) and Shifted Windows Multi -Head Self-Attention \nmodule (SW-MSA). The SW -MSA is crucial to know  the \nfunctions of Swin transformer as it allows for information \nexchange across non  overlapping windows.  The \ncomputational complexity of a global MSA module and a \nwindow based are given as \nŒ©(MSA)  =  4hùìåC2 +  2(hùìå)2C    (2) \nŒ© (W ‚àí  MSA)  =  4hùìåC2 +  2M2hùìåC      (3) \nThe shifted window partitioning approach is used to compute \nthe successively aligned Swin transformer blocks. \nzÃÇl =  W ‚àí  MSA(LN(zl ‚àí 1))  +  zl ‚àí 1                    (4) \nzl =  MLP(LN(zÃÇl )) +  zÃÇl     (5) \nzÃÇl + 1 =  SW ‚àí  MSA(LN(zl )) +  zl    (6) \nzl + 1 =  MLP(LN(zÃÇl + 1))  +  zÃÇl + 1    (7) \nwhere, ùëßÃÇùëô and ùëßùëô stands for the respective block ùëô output \ncharacteristics of the SW-MSA module and MLP module. \n \nC. SimAM attention mechanism \n \nAttention mechanisms are a great way to utilize the most \nsignificant features in an input sequence by using the weighted \ncombination of the input vectors. For our proposed model, we \nhave incorporated the SimAM attention mechanism in the \nYOLOv5 structure to improve its overall performance. The \nattention module in the system uses a complex set of filters to \nfocus on a single object while having a variety of different \nthings in our field of view. To filter out feature combinations \nthat are helpful to the recognition of the feature, we include a \n3D attention module called SimAM here. Moreover, the issue \nof feature misalignment brought on by the direct stacking of \ncomponents with various scales may be resolved. To capitalize \non the value of neurons, the SimAM mod ule suggests an \nimproved energy function based on neuroscience theory.  \n \n \nFIGURE 4. SimAM module. \nIt then generates an analytical solution to the energy function \nto expedite the calculation of attention weights. SimAM is a \nunique attention mechanism because of its complete usage of \n3D weights and the energy function, which accelerates the \ncomputation of weights. The entire SimAM is a lightweight \nmodule as it is non -parametric; the number  of parameters \naccounts for zero compared to the other attention mechanisms. \nThe SimAM attention module will be placed in different \nplaces along the head, and the performance will be tabulated \nfor each positioning. The architecture of the SimAM attention \nmodule can be seen in Figure 4. SimAM completely uses the \n3d weights and the energy function to accelerate the weights \ncalculation along with an added advantage of it being a non-\nparametric module \n \nEnergy function of each neuron will be: \nùëíùë°(ùìåùë°,   ùëèùë°,   ùë¶,   ùë•ùëñ ) =  (ùë¶ùë°  ‚àí ùë°ÃÇ)2 + 1\nùëÄ ‚àí 1 ‚àë (ùë¶ùëú  ‚àí\n ùë•ÃÇùëñ\n)\n2ùëÄ‚àí1\nùëñ=1\n            (8)   \nHere, ùë°ÃÇ =  ùìåùë° ùë° + ùëèùë° and ùë•ÃÇùëñ =  ùìåùë° ùë•ùëñ +  ùëèùë° are linear \ntransformations of ùë° and ùë•ùëñ, where ùë°  is the target neuron and \nùë•ùëñ is the other neurons in a single channel of the input feature. \nFinal energy function will be: \nùëíùë°(ùìåùë°,   ùëèùë°,   ùë¶,   ùë•ùëñ ) =  1\nùëÄ ‚àí 1 ‚àë (‚àí1 ‚àí (ùìåùë°ùë•ùëñ  + ùëèùë°))2\nùëÄ‚àí1\nùëñ=1\n+ (1 ‚àí (ùìåùë°ùë° + ùëèùë°))2 +  ùúÜùìåùë°\n2                 (9)    \nwhere, ùìåùë° and ùëèùë° can be easily obtained by: \nùìåùë° =  ‚àí 2(ùë° ‚àí ùúáùë°)\n(ùë° ‚àí ùúáùë°)2 + 2ùúéùë°\n2 + 2ùúÜ                                                    (10) \nùëèùë° =  ‚àí 1\n2 (ùë° + ùúáùë°)ùìåùë°                                                                       (11) \nwhere,  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n8 \n \nùúáùë° =\n1\nùëÄ‚àí1  ‚àë ùë•ùëñ                                                                            (12)ùëÄ‚àí1\nùëñ=1   \nùúéùë°\n2 =  \n1\nùëÄ‚àí1  ‚àë (ùë•ùë° ‚àí ùúáùë°)2ùëÄ‚àí1\nùëñ                                             (13) \nare mean and variance calculated for all neurons except t. \nMinimal energy can be computed with: \nùëíùë°\n‚àó =  4(ùúéÃÇ2 +  ùúÜ)\n(ùë° ‚àí ùúáÃÇ)2 + 2ùúéÃÇ2 + 2ùúÜ                                                            (14) \nwhere,  \nùúáÃÇ =\n1\nùëÄ   ‚àë ùë•ùëñ\nùëÄ\nùëñ=1                                                                  (15) \nùúéÃÇ2 =  \n1\nùëÄ  ‚àë (ùë•ùëñ ‚àí ùúáÃÇ)2ùëÄ\nùëñ=1 .                                                  (16) \n \nThe above equation (14) depicts the lower energy ùëíùë°\n‚àó, the \nneuron ùë° is more unique from neighbouring neurons. \n \nD. Proposed EVS-YOLO architecture \n \nIn the proposed EVS-YOLO model, we use the Swin module \nin the backbone of the YOLOv5S architecture. Adding a \ntransformer in the backbone improves the model's overall \nperformance by focusing on the necessary information rather \nthan focusing on every aspect of the image that includes the \nunwanted part, thus decreasing the overall accuracy and \nperformance. Researchers have created a better network by \nfusing CNN with the transformer ins pired by the visual \ntransformer. The transformer works better for dense and \nobstructed images and scenarios. It has a more significant \ncapacity to collect global information than CNN, thus being \nan essential addition to the overall network in enhancing its  \nperformance. Further, to push the model's performance, the \nSimAM module is embedded before the three detection heads \nin the architecture.  On using an attention mechanism, the \nEVS-YOLO model tends to focus better on the target rather  \n \n \nthan concentrating more on the unimportant features, thus \nenhancing the model's overall performance. Attention \nmechanisms can selectively attend to informative regions of \nthe input image, which can improve the discrimination ability \nof the model. By listening to relevant re gions, the attention \nmodule can help the YOLO model better distinguish between \nobjects with similar features, reducing false positives or false \nnegatives. The SimAM attention [58] is a lightweight and non-\nparametric algorithm. It directly uses 3D - weights, thus \nmaking it robust attention to be embedded along with the Swin \nTransformer [59] in the YOLO architecture. The proposed \nEVS-YOLO architecture is depicted in Figure 5  with SWIN \ntransformer module embedded in the backbone of the network \nbefore the poolin g operation done by the SPPF layer. The \nSimAM attention modules are added in the neck before \npassing to the head through the different detection heads. \n \nIV. EXPERIMENT \n \nA. Dataset \nTo evaluate the performance and robustness of the EVS -\nYOLO algorithm for detecti ng the EV charging socket, we \nused a dataset derived from two publicly available datasets \nconsisting of over 2,500 images. The dataset consists of \ndifferent EV charging sockets, and it was categorized into six \ndifferent types of classes manually. The dataset consists of 6 \nclasses, namely CCS1, CCS2, Type1, Type2, Tesla, and \nCHAdeMO. All the images in the combined dataset have been \nmanually labeled and then categorized into their respective \ntype of sockets. The dataset is partitioned in a ratio of 8:1:1 for \nthe training, validation, and testing sets, respectively. \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFIGURE 5. Proposed EVS-YOLO architecture with SWIN transformer and SimAM attention modules.  \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n9 \n \n \nFIGURE. 6. Instances of each class of the dataset.  \n \nFigure 6 represents the instances of different types of classes \nin the dataset used for the analysis of EVS-YOLO model. An \noverview of the dataset showing different sockets namely \nCCS1, CCS2, CHAdeMO, Type1, Type2 and Tesla is \nillustrated in Figure 7. Dataset preparation includes images \nwith different conditions and scenarios as shown in Figure 8 \nfor calculation the accuracy of the proposed model so as to \nattain the real-time implementation success containing all the \ndifferent cases such as image samples with different camera \nangles, different brightness, weather conditions, different \ncolors, environments and images with socket and camera - \ndirty, clean conditions. \n \nB. Experimental and environmental settings \n \nTo confirm the efficacy and reliability of the EVS -YOLO \nalgorithm, we performed ablation and comparison tests on the \ndataset. Windows 11 is the operating system employed for the \ntrials. The processor is an Intel 10th Gen H type. Python 3.8.13 \nis the programming language used. Pytorch 1.10.0 is the deep \nlearning framework, and the acceleration environment is \nCUDA 11.4. For the analysis, the epochs were set at 125, the \nweights used were YOLOv5.pt, and the batch size was set as \n16. Figure 9, the overall representation and distribution of the \nobjects in the dataset are uniform. \n \nV. RESULTS AND DICUSSION \n \nTo analyze the improvement that happens with adding \ndifferent modules in the ES-YOLO algorithm, we performed \nablation experiments that would better help us understand how \nadding each module improves the base model. Table 2 shows \nthe experimental results of  proposed EVS -YOLO                                                                                                                                                     \nablation on the respective datasets. The general trend is that \nthe base model's performan ce is enhanced by including a \nprocess in terms of precision, recall, mean average accuracy, \nand F1 score. It lags in inference speed as there are many more \nmathematical operations to be performed, as indicated by the \nGflops column. In scheme 2, the SWIN transformer module is \nadded to the backbone of the base network to improve the \nmodel's ability to capture long -range dependencies and \ncontextual information. This enhanced precision by 1.6%, \nrecall by 6.2%, mAP by 1.0%, and f1 score by 4.79%, but \nconversely, the inference speed increased to 7.9ms from 7.7ms \nin scheme 1. Gigaflops (GFlops) refer to the number of \nmathematical operations required for the model to have an \nentire pass. These metric increases, so there is an increase in \ninference speed. The lower the inference speed, the faster we \nhave the result. In scheme three, along with the Swin \ntransformer in the backbone,  SimAM attention module is \nadded to three detection heads, allowing the neural network to \nselectively focus on different parts of the input data, assigning \nvarying levels of importance to other regions or features, \nhelping the network capture relevant information while \nfiltering out irrelevant or redundant information. Therefore, \nthis improves precision by 5.6%, recall by 0.5%, mAP by \n1.6%, and F1 score by 2.59%, and is slightly slower as there \nis an increase in inference speed. On comparison of the results \nof scheme one and scheme 3, we can infer that the EVS -\nYOLO model outperforms the YOLOv5s model in all \nevaluation indicators. The precision reached 95.2%, the recall \nreached 78.7%, and the mAP and F1 scores reached 81.4% \nand 86.16%, respectively, with a slight increase in inference \nspeed but still fast enough for real-time applications. Figure 10 \nrepresents the class -wise mAP comparison betwee n the \nEVSYOLO and base YOLOv5s. Drastic changes is observed \nin the tesla and CHAdeMO classes where the base model \nsignificantly underperforms. For other classes, the proposed \nmodel either slightly improves or has similar accuracy values. \nThe confusion matrix of the YOLOv5s model and the \nproposed EVS-YOLO model is shown in Figure 11. It can be \nseen that the EVS-YOLO model outperforms the YOLOv5s \nmodel in every class. Table 3 represents the performance of \ndifferent attention modules with a Swin transformer backbone. \nThe other attention models are placed before the three \ndetection heads of the structure. The proposed model \ncomprises a SimAM attention module, a lightweight and non-\nparametric module, reflected in the results as the proposed \nmodel has the lowest number of parameters at 7,163,019. \nCompared to the next best -performing model in terms of \naccuracy with CBAM, which has 7207276 parameters, it is \n44,257 less. The proposed model also performs the best in \nprecision, recall, mAP, and f1 scores. It outperforms the next \nbest model, which includes CBAM as its attention module in \nterms of accuracy by 16.8%, mAP by 0.3%, and f1 score by \n6.3%. \nIt significantly outperforms when compared to the \nmodel which includes the SK Attention module, as it has five \ntimes less the number of parameters and twice as few Gflops, \nand it improves on precision by 5.8%, recall by5.0%, mAP by \n5.7%, f1 score by 5.3%. Typically, the lower the parameters, \nthe faster the model is in inference. The addition of SimAM \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n10 \n \nimproves the accuracy compared with another attention \nmodule called the Coordinate attention module by 0.8% while \nhaving 35,680 lesser parameters. It also outperforms attention \nmodules like NAM and SE attention modules in all metrics \nwhile still having fewer parameters. Figure 12 shows the class \nwise comparison of Mean Average Precision for different \nattention modules with Swin Transformer embedded in the \nbackbone. It justifies that adding the SimAM attention module \nimproves precision for certain classes  such as CCS1, \nCHAdeMO and Tesla. YOLOv5 uses three different detection \nheads, each responsible for predicting objects at different \nscales or resolutions in the input image. Th e large Detection \nHead predicts things at the highest resolution in the input \nimage. It has a larger receptive field, which allows it to detect \nsmaller objects with fine details. It outputs a tensor with higher \nspatial resolution and smaller object anchor boxes for \ndetecting smaller objects. Medium Detection Head operates at \nan intermediate resolution in the input image and is \nresponsible for detecting objects of medium size. It has a \nmedium-sized receptive field, allowing it to detect moderate-\nsized objects. The Small Detection Head operates at the lowest \nresolution in the input image and detects larger objects. Its \nlarger receptive field will enable it to detect objects of larger \nsize with coarser details. Combining these three detection \nheads at different  resolutions helps YOLOv5 to accurately \ndetect objects of varying sizes and scales. Table 4 represents \nthe results of the influence of the positioning of the SimAM \nattention module in these three detection heads. Since there are \nthree detection heads, seve n possible combinations of \npositions are likely. The best -performing model is scheme \nseven, where an attention module is present in all three places \nwith a precision, recall, mAP, and f1 score value of 95.2%, \n78.7%, 81.4%, and 86.16% as there is a combinat ion of all \nthree detection scales. The addition of attention modules in the \nsmall and large detection heads has similar results in terms of \nmAP at 80.6% in both cases. Still, including an attention \nmethod in the more giant head outperforms precision, recall, \nand f1 scores by 2.5%, 4.2%, and 3.56%. In terms of \npositioning in two detection heads, adding attention modules \nin the small and large detection heads gives the best \noutperforming the other schemes with similar positioning but \nstill less than other positioning methods. \nTable 5 compares the proposed model with other \ndetection methods. It outperforms traditional methods like \nFaster RCNN and Retinanet regarding precision, recall, mAP, \nand F1 score by a considerable margin, and typically, stage \nmethods are faster than two-stage methods, so this is an added \nadvantage. Compared with other YOLO models, it \noutperforms YOLOv3 by 7% in terms of mAP and has a better \nF1 score, which is 3.7% better than YOLOv3. Its successor, \nYOLOv4, is slightly improved over YOLOv3 but still needs \nto catch up by 5.3% in average precision. Even though the \nYOLOX model has a better F1 score, it still needs to improve \nin accuracy by 2.0%.  \nFIGURE 7. A Representation of different classes of EV sockets present in the dataset [18]\n  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n11 \n \nWhen compared to other YOLOv5 models, the proposed \nmodel outperformsv5n, v5s, and v5m with regards to mean \naverage precision by 4.1%, 2.6 %,4.5%, and f1 scores by \n8.06%, 7.37%,2.11%. The YOLOv5x model outperforms the \nproposed model in terms of mAP by 1.9%. Inference speed \nand parameters comparison of proposed EVS -YOLO               \nmodel and other detection models are tabulated in Table 6. \n \n \nFIGURE 8. Sample images taken under different environmental \nconditions Source: ¬© Roboflow.com [18] \n \nEven though the proposed model has significantly fewer \nparameters, it performed better compared to most one -stage \nmethods, and it also performs well with inference speed, with \nonly YOLOv5s and YOLOv5n having lesser inference speed \nowing to their more secondary parameters. Figure 13 depicts \nthe graphical representation of various evaluation metrics such \nas precision, recall, mAP, and F1 score of the proposed EVS-\nYOLO model with existing ob ject detection models. The \nproposed EVS Yolo model outperforms other models, as the \ngraph shows. The subjective detection results of the proposed \nEVS-YOLO model with other object detection models for all \nsix classes shown in Figure 14. \n \n \nFIGURE 9. Location size and distribution of objects. \n \nFIGURE 10. Class wise performance comparison between  \n                      EVS-YOLO and YOLOv5s model. \n \nTABLE 2. Experimental results of the proposed EVS-YOLO ablation on      \n                 the respective dataset. \n \n \nTABLE 3. Comparison of different positioning of SimAM attention \nmodule on different detection heads on the EVS-YOLO model. \n \n \n \n \n \n \n \n1 ‚úì¬† ¬† - - 87 72 80.8 78.79 15.8 7.7\n2 ‚úì¬† ¬† ‚úì¬† ¬† - 89.6 78.2 77.8 83.51 55.7 7.9\n3 ‚úì¬† ¬† ‚úì¬† ¬† ‚úì¬† ¬† 95.2 78.7 81.4 86.16 55.7 8.3\nmAP \n(0.50) gflopF1 Score \n(%)\nSpeed \n(ms)YOLOv5s Swin SimAM Precision RecallScheme\nBackbone Attentions \n Model Precision Recall mAP \n(0.50)\nF1 Score \n(%) Param gflop\nNAM 91.7 76.8 78.2 83.59 7164811 55.7\nSE 91.6 98.1 99.4 94.73 7206027 55.8\nCA 95.8 79.1 80.6 86.65 7198699 55.8\nCBAM 78.4 80.7 81.1 79.53 7207276 55.8\nSKATTNTI\nON 89.4 73.7 75.7 80.79 3.6E+07 108.7\nSIMAM 95.2 78.7 81.4 86.16 7163019 55.7\n      YOLOv5               \n            +          \nSwin\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n12 \n \nFIGURE 11. Confusion matrix obtained from (a).  YOLOv5s base model and (b). proposed EVS-YOLO model. \n \n \n \nFIGURE 12. Class wise comparison of Mean Average Precision for different attention modules with Swin Transformer embedded in the backbon e. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFIGURE 13. Graphical representation of various evaluation metrics of proposed EVS-YOLO model with existing object detection models. \n \nTABLE 4. Performance comparison for different attention modules with \nSwin transformer embedded in the backbone.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 15 shows the performance comparison of all the losses \nin the training and validation set, such as bounding box, class, \nand object loss for the proposed EVS -YOLO on the dataset \nused. The first row is the loss of the validation set. The three \nfigures in the first row from left to right are class loss, object \nloss, and box loss. The second row is the loss of the training \nset for the EVS -YOLO model on the dataset used, and the \nthree figures in the second row from the left are the same as \nhose mentioned in the first row. Be it the validation set or the \ntraining set, the loss tends to decrease and eventually stabilize. \nF1 Score\nSmall Medium Large (%)\n1 ‚úì¬† ¬† - - 90.1 72.5 80.6 80.34\n2 - ‚úì¬† ¬† - 91.8 78.1 77.8 84.39\n3 - - ‚úì¬† ¬† 92.6 76.7 80.6 83.9\n4 ‚úì¬† ¬† ‚úì¬† ¬† - 92.6 75.9 79.2 83.42\n5 ‚úì¬† ¬† - ‚úì¬† ¬† 91.3 79.4 80.1 84.93\n6 - ‚úì¬† ¬† ‚úì¬† ¬† 76.7 79.3 79.4 77.97\n7 ‚úì¬† ¬† ‚úì¬† ¬† ‚úì¬† ¬† 95.2 78.7 81.4 86.16\nRecall mAP \n(0.50)Scheme\nAttention Module Positioning\nPrecision\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n13 \n \n \nFIGURE 14. Class wise detection results of the proposed EVS -YOLO model with other object detection models.\n \n \nTABLE 5. Performance comparison of different detection models  \n                  with EVS-YOLO. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTable 6. Inference speed and parameters comparison of EVS-YOLO       \n              model with other detection models. \n \n \n \n \nDetection Models Params gflops  speed \n(ms)\nYolov5m 20873139 47.9 13.9\nYolov5n 1772035 4.2 6\nYolov5s 7026307 15.8 7.7\nYolov5x 86207059 203.9 24.1\nYoloR 36864968 80.6 16.2\nYoloX 8042561 21.6 9.8\nYolov3 61524355 154.6 20.5\nYolov4 9124115 20.6 9.6\nEVS-Yolo 7163019 55.7 8.3\nF1 Score\n(%)\nFaster \nRCNN 51.3 48.6 52.2 49.91\nRetinaNet 69.3 53.2 69.3 60.19\nYolov5m 92.4 77.1 76.9 84.05\nYolov5n 86.2 71.4 77.3 78.1\nYolov5s 87 72 78.8 78.79\nYolov5x 91.5 76.4 81.3 83.27\nYoloR 89.8 71.4 74.3 79.54\nYoloX 95.1 78.1 79.4 86.01\nYolov3 92.8 74.5 74.4 82.64\nYolov4 92.9 76.7 76.1 84.02\nProposed    \nEVS-Yolo 95.2 78.7 81.4 86.16\nDetection \nModels Precision Recall mAP \n(0.50)\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n14 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFIGURE 15. Performance comparison of three kinds of loss for the EVS -YOLO on the dataset used. \n \nDespite the curve being close, when we compare the bounding \nbox set of the validation loss, EVS -YOLO is comparatively \nlow compared to the YOLOv5s, i.e., the base model. YOLOv5 \nloss stands at 0.0048 for 125 epochs, whereas EVS-YOLO is \nat 0.0042 for 125 periods, which means the proposed model \naccelerates the speed and tends to converge a lower loss value. \nThe terms \"bounding box loss,\" \"class loss,\" and \"object loss\" \nare used in object detection with deep learning to represen t \ndifferent types of losses that are computed during the model's \ntraining and validation stages. The bounding box loss is a \nspecific type of loss that assesses the error in anticipating the \nbounding box coordinates of an object. The model is penalized \nwhen the expected coordinates differ from the actual \ncoordinates, and the loss is frequently calculated utilizing a \nregression loss function like mean squared error or smooth L1 \nloss.       \n On the other hand, class loss is a type of loss that \nmeasures the mod el's error in predicting the object's class \ninside the bounding box. The model is punished when the \npredicted label varies from the true label. The loss is typically \ncalculated using a classification function such as cross-entropy \nloss. Lastly, object loss  is a type of loss that quantifies the \nmodel's error in detecting the object inside a boundary. The \nmodel is punished when it fails to detect an object's presence \ninside a bounding box or mistakenly detects a false positive. \nThe loss is generally calculated using a binary classification \nfunction such as binary cross-entropy loss. During the training \nphase, the model tries to minimize the overall loss, an \naggregated sum of the bounding box, class, and object loss. \nThe weights are commonly chosen based on the  relative \nimportance of each loss term. During the validation phase, the \nloss terms are computed on a separate set of validation data to \ndetermine the model's performance. The end goal is to \nminimize the overall training loss and validation sets to \nachieve a high level of generalization performance. \nVI. CONCLUSION AND FUTURE WORKS \n \nAn object detection algorithm called EVS -Yolo has been \nproposed to address the problems of identification of Electric \nVehicle charging sockets, which hurdles the experience of the \ncharging port efficiently for the users. We introduced a \nSimAM attention mechanism in the backbone section to \nenhance the network's ability to aggregate features and focus \nmore on the object than the background. SimAM attention was \napplied in three positions across the proposed architecture to \nfind the best possible result. We also embedded the Swin \ntransformer module into the backbone part of the network, \nwhich can extract contextual features. To verify the \nsatisfactory work of the algorithm, ablation experiments were \nperformed. The experiment results show that the EVS-YOLO \nalgorithm achieves an average detection accuracy of 81.4% on \nthe test set, an improvement of 2.6%. The EVS-YOLO model \nachieves an inference speed of 8.3ms, attaining the accuracy \nand requirements for detecting Electric vehicle sockets. The \nEVSYOLO method is more suitable for this application than \nother object detection algorithms due to higher accuracy and \nreal-time inference.  \n Further, the EVS-YOLO algorithm can apply to real-\ntime samples with an automated application, which might act \nas a catalyst for improving the overall user Experience. \nFurther, all possible environmental impacts will be considered \nto mature the system for robustness \n \nACKNOWLEDGEMENT \nThis work was partly supported by the Government of India, \nDepartment of Science and Technology (DST) Science and \nEngineering Research Board (SERB) Core Research \nCRG/2020/004073. This research also was supported by the \nSGS grant from VSB -Technical University of Ostrava under \ngrant number SP2023/005. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n15 \n \nREFERENCES  \n[1] Z. Liu, H. Hao, X. Cheng, and F. Zhao, ‚Äò‚ÄòCritical issues of \nenergy efficient and new energy vehicles development in \nChina,‚Äô‚Äô Energy Policy, vol. 115, pp. 92‚Äì97, Apr. 2018. \n[2] D. Guo, W. Yan, X. Gao, Y. Hao, Y. Xu, X. Tan, and T. Zhang, \n‚Äò‚ÄòForecast of passenger car market structure and environmental \nimpact analysis in China,‚Äô‚Äô Sci. Total Environ., vol.772, Jun. \n2021, Art. no. 144950. \n[3] Yu, P.; Zhang,  J.; Yang, D.; Lin, X.; Xu, T. The Evolution of \nChina‚Äôs New Energy Vehicle Industry from the Perspective of a \nTechnology‚ÄìMarket‚ÄìPolicy Framework. Sustainability 2019, \n11, 1711. \n[4] Z. Liu, H. Hao, X. Cheng, and F. Zhao, ‚Äò‚ÄòCritical issues of \nenergy efficient an d new energy vehicles development in \nChina,‚Äô‚Äô Energy Policy, vol. 115, pp. 92‚Äì97, Apr. 2018. \n[5] D. Guo, W. Yan, X. Gao, Y. Hao, Y. Xu, X. Tan, and T. Zhang, \n‚Äò‚ÄòForecast of passenger car market structure and environmental \nimpact analysis in China,‚Äô‚Äô Sci. Total Environ., vol.772, Jun. \n2021, Art. no. 144950. \n[6] Yu, P.; Zhang, J.; Yang, D.; Lin, X.; Xu, T. The Evolution of \nChina‚Äôs New Energy Vehicle Industry from the Perspective of a \nTechnology‚ÄìMarket‚ÄìPolicy Framework. Sustainability 2019, \n11, 1711. \n[7] Chen, Z., Huang, X.: ‚ÄòChallenges and opportunities for the \ndevelopment of EV in large scale‚Äô, J. Electr. Eng., 2015, 10, (4), \npp. 35‚Äì44. \n[8] https://www.gridserve.com/2023/03/06/where-is-the-charge-\nport-on-my-electric-car/ \n[9] Dixon, J.; Andersen, P.B.; Bell, K.; Tr√¶holt, C. On the ease of \nbeing green: An investigation of the inconvenience of electric \nvehicle charging. Appl. Energy 2020, 258, 114090. \n[10] He, C.; Chen, J.; Feng, Q.; Yin, X.; Li, X. Safety Analysis and \nSolution of Electric Vehicle Charging. Distrib. Util. 2017, 34, \n12‚Äì18 \n[11] Tesla develops \"snake -shaped\" metal charging robot that can \nautomatically charge. Available online: \nhttp://www.xinhuanet.com/world/2015-8/10/c_128111025.htm \n(accessed on 15 January 2022). \n[12] Coming! The latest development of the KUKA & Volkswagen \nGroup ‚ÄúE -smart Connect‚Äù project. Available online: \nhttps://www.imrobotic.com/news/detail/5755 (accessed on 16 \nJanuary 2022). \n[13] Shi, Y.: ‚ÄòResearch on robot -based electric vehicle charging \nsystem and its automatic plugin‚Äô(Harbin Institute of Technology, \nPeople's Republic of China, 2016). \n[14] Ma, L.: ‚ÄòUltrasonic -based electric vehicle chargin g port \npositioning technology‚Äô (Harbin Institute of Technology, \nPeople's Republic of China, 2018) \n[15] Zhao, L.Y. Research on the Classification, Identification and \nDeblurring Algorithm of Electric Vehicle Charging Ports Based \non Deep Learning. Master‚Äôs Thesis,  Harbin Institute of \nTechnology, Harbin, China, 2021. \n[16] Haddadin, S.; De Luca, A.; Albu-Schaffer, A. Robot Collisions: \nA Survey on Detection, Isolation, and Identification. IEEE \nTrans. Robot. 2017, 33, 1292‚Äì1312. \n[17] KoreaTechDesk, ‚ÄúEVAR: Samsung electronics‚Äô sp inoff brings \nan autonomous robotic charger for electric vehicles,‚Äù Nov. 22, \n2018. [Online]. Available: https://koreatechdesk.com/evar- \nsamsungelectronics-spinoff-brings-an-autonomous-robotic-\ncharger-for electric-vehicles/, 2018 October, Accessed on: Jul. \n29, 2019. \n[18] https://www.hyundaimotorgroup.com/tv/CONT000000000008\n2225 \n[19] N. Dalal and B. Triggs, \"Histograms of oriented gradients for \nhuman detection,\" 2005 IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition (CVPR'05), San \nDiego, CA, USA, 2005, pp. 886 -893 vol. 1, doi: \n10.1109/CVPR.2005.177. \n[20] Z. Liu, H. Hao, X. Cheng, and F. Zhao, ‚Äò‚ÄòCritical issues of \nenergy efficient and new energy vehicles development in \nChina,‚Äô‚Äô Energy Policy, vol. 115, pp. 92‚Äì97, Apr. 2018. \n[21] D. Guo, W. Yan, X. Gao, Y. Hao, Y. Xu, X. Tan, and T. Zhang, \n‚Äò‚ÄòForecast of passenger car market structure and environmental \nimpact analysis in China,‚Äô‚Äô Sci. Tota l Environ., vol.772, Jun. \n2021, Art. no. 144950. \n[22] Yu, P.; Zhang, J.; Yang, D.; Lin, X.; Xu, T. The Evolution of \nChina‚Äôs New Energy Vehicle Industry from the Perspective of a \nTechnology‚ÄìMarket‚ÄìPolicy Framework. Sustainability 2019, \n11, 1711. \n[23] Chen, Z., Huang,  X.: ‚ÄòChallenges and opportunities for the \ndevelopment of EV in large scale‚Äô, J. Electr. Eng., 2015, 10, (4), \npp. 35‚Äì44. \n[24] https://www.gridserve.com/2023/03/06/where-is-the-charge-\nport-on-my-electric-car/ \n[25] Dixon, J.; Andersen, P.B.; Bell, K.; Tr√¶holt, C. On the ease of \nbeing green: An investigation of the inconvenience of electric \nvehicle charging. Appl. Energy 2020, 258, 114090. \n[26] He, C.; Chen, J.; Feng, Q.; Yin, X .; Li, X. Safety Analysis and \nSolution of Electric Vehicle Charging. Distrib. Util. 2017, 34, \n12‚Äì18 \n[27] Tesla develops \"snake -shaped\" metal charging robot that can \nautomatically charge. Available online: \nhttp://www.xinhuanet.com/world/2015-8/10/c_128111025.htm \n(accessed on 15 January 2022). \n[28] Coming! The latest development of the KUKA & Volkswagen \nGroup ‚ÄúE -smart Connect‚Äù project. Available online: \nhttps://www.imrobotic.com/news/detail/5755 ( accessed on 16 \nJanuary 2022). \n[29] Shi, Y.: ‚ÄòResearch on robot -based electric vehicle charging \nsystem and its automatic plugin‚Äô(Harbin Institute of Technology, \nPeople's Republic of China, 2016). \n[30] Ma, L.: ‚ÄòUltrasonic -based electric vehicle charging port \npositioning technology‚Äô (Harbin Institute of Technology, \nPeople's Republic of China, 2018) \n[31] Zhao, L.Y. Research on the Classification, Identification and \nDeblurring Algorithm of Electric Vehicle Charging Ports Based \non Deep Learning. Master‚Äôs Thesis, Harbin  Institute of \nTechnology, Harbin, China, 2021. \n[32] Haddadin, S.; De Luca, A.; Albu-Schaffer, A. Robot Collisions: \nA Survey on Detection, Isolation, and Identification. IEEE \nTrans. Robot. 2017, 33, 1292‚Äì1312. \n[33] KoreaTechDesk, ‚ÄúEVAR: Samsung electronics‚Äô spinoff b rings \nan autonomous robotic charger for electric vehicles,‚Äù Nov. 22, \n2018. [Online]. Available: https://koreatechdesk.com/evar- \nsamsungelectronics-spinoff-brings-an-autonomous-robotic-\ncharger-for electric-vehicles/, 2018 October, Accessed on: Jul. \n29, 2019. \n[34] https://www.hyundaimotorgroup.com/tv/CONT000000000008\n2225 \n[35] N. Dalal and B. Triggs, \"Histograms of oriented gradients for \nhuman detection,\" 2005 IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition (CVPR'05), San \nDiego, CA, USA, 2005, pp. 886 -893 vol. 1, doi: \n10.1109/CVPR.2005.177. \n[36] Lowe, David G. \"Distinctive image features from scale-invariant \nkeypoints. \" International journal of computer vision 60 (2004): \n91-110. https://doi.org/10.1023/B:VISI.0000029664. 99615.94. \n[37] Dwyer, B., Nelson, J. (2022), Solawetz, J., et. al. Roboflow \n(Version 1.0) [Software]. Available from https://roboflow.com. \ncomputer vision. \n[38] R. Girshick, J. Donahue, T. Darrell and J. Malik, \"Region-Based \nConvolutional Networks for Accurate Object Detection and \nSegmentation,\" in IEEE Transactions on Pattern Analysis and \nMachine Intelligence, vol. 38, no. 1, pp. 142 -158, 1 Jan. 2016, \ndoi: 10.1109/TPAMI.2015.2437384. \n[39] R. Girshick, \"Fast R -CNN,\" 2015 IEEE International \nConference on Computer Vision (ICCV), Santiago, Chile, 2015, \npp. 1440-1448, doi: 10.1109/ICCV.2015.169. \n[40] G√ºney, Emin, and C√ºneyt BAYILMI≈û. \"An implementation of \ntraffic signs and road objects detection using faster R -CNN.\" \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n16 \n \nSakarya University Journal of Computer and Information \nSciences 5, no. 2 (2022): 216-224. \n[41] Ren, K. He, R. Girshick and J. Sun, \"Faster R -CNN: Towards \nReal- Time Object Detection with Region Proposal Networks,\" \nin IEEE Transactions on Pattern Analysis and Machine \nIntelligence, vol. 39, no. 6, pp. 1137 - 1149, 1 June 2017, \ndoi:10.1109/TPAMI.2016.2577031. \n[42] Liu, W. et al. (2016). SSD: Single Shot MultiBox Detect or. In: \nLeibe, B., Matas, J., Sebe,N., Welling, M. (eds) Computer \nVision ‚Äì ECCV 2016. ECCV 2016. Lecture Notes in Computer \nScience (), vol 9905. Springer, Cham. \nhttps://doi.org/10.1007/978-3-319-46448- 0_2S. \n[43] Redmon J, Farhadi A (2017) YOLO9000: better, faster, stronger. \nIn: Computer Vision and Pattern Recognition (CVPR). Vol. 1, \npp. 6517‚Äì6525 \n[44] Redmon J, Farhadi A (2018) YOLOv3: an incremental \nimprovement. Technical Report. Accessed \nhttps://arxiv.org/abs/1804.02767  \n[45] Bochkovskiy A, Wang CY, Liao HYM (2020) YOLOv4: \noptimal speed and accuracy of object detection. Accessed \nhttps://arxiv.org/abs/2004.10934 \n[46] T. -Y. Lin, P. Goyal, R. Girshick, K. He and P. Doll√°r, \"Focal \nLoss for Dense Object Detection,\" in IEEE Transactions on \nPattern Analysis and Machine Intelligence, vol. 42, no. 2, pp. \n318-327, 1 Feb. 2020, doi: 10.1109/TPAMI.2018.2858826. \n[47] Zhong, Zilong, Zhong Qi u Lin, Rene Bidart, Xiaodan Hu, \nIbrahim Ben Daya, Zhifeng Li, Wei-Shi Zheng, Jonathan Li, and \nAlexander Wong. \"Squeeze-and-attention networks for semantic \nsegmentation.\" In  Proceedings of the IEEE/CVF conference \non computer vision and pattern recognition,  pp. 13065-13074. \n2020. \n[48] Woo, Sanghyun, Jongchan Park, Joon -Young Lee, and In So \nKweon. \"Cbam: Convolutional block attention module.\" In \nProceedings of the European conference on computer vision \n(ECCV), pp. 3-19. 2018 \n[49] Hou, Qibin, Daquan Zhou, and Jiashi Fen g. \"Coordinate \nattention for efficient mobile network design.\" In Proceedings of \nthe IEEE/CVF conference on computer vision and pattern \nrecognition, pp. 13713-13722. 2021. \n[50] Cao Z et al (2020) Detecting the shuttlecock for a badminton \nrobot: a YOLO based approach. Expert Syst Appl 164:113833. \n[51] Fikri RM, Byungwook K, Mintae H (2020) Waiting time \nestimation of hydrogen -fuel vehicles with YOLO real -time \nobject detection. Information science and applications. Springer, \nSingapore, pp 229‚Äì237. \n[52] Jamtsho Y, Panomkhawn R, Rattapoom W (2020) Real -time \nBhutanese license plate localization using YOLO. ICT Express \n6(2):121‚Äì124 \n[53] Kalhagen ES, √òrjan LO (2020) Hierarchical fsh species \ndetection in real-time video using YOLO. MS Thesis. University \nof Agder \n[54] Mohd P, Nurul PA (2020) A real -time trafc sign recognition \nsystem for autonomous vehicle using Yolo. Diss. Universiti \nTeknologi MARA, Cawangan Melaka. \n[55] Ren P et al (2020) A novel squeeze YOLO -based real -time \npeople counting approach. Int J Bio Inspir Comput  16(2):94‚Äì\n101. \n[56] Shi R, Tianxing L, Yasushi Y (2020) An attribution -based \npruning method for real -timemango detection with YOLO \nnetwork. Comput Electron Agric 169:105214. \n[57] Wang J et al (2020) Real-time behavior detection and judgment \nof egg breeders based on YOLO v3. Neural Comput Appl \n32(10):5471‚Äì5481. \n[58] Tian, Yunong & Yang, Guodong & Wang, Zhe & Li, En & \nLiang, Zize. (2019). Detectionof Apple Lesions in Orchards \nBased on Deep Learning Methods of CycleGAN andYOLOV3-\nDense. Journal of Sensors. 2019. 10.1155/2019/7630926. \n[59] Y. Cai et al., \"YOLOv4 -5D: An Effective and Efficient Object \nDetector for Autonomous Driving,\" in IEEE Transactions on \nInstrumentation and Measurement, vol. 70, pp. 1 -13,2021, Art \nno. 4503613, doi: 10.1109/TIM.2021.3065438. \n[60] Fu, Huixuan, Guoqing So ng, and Yuchao Wang. 2021. \n\"Improved YOLOv4 Marine Target Detection Combined with \nCBAM\" Symmetry 13, no. 4: 623, \nhttps://doi.org/10.3390/sym13040623 \n[61] G√ºney, Emin, C√ºneyt Bayilmi≈ü, and Batuhan √áakan. \"An \nimplementation of real -time traffic signs and road objects \ndetection based on mobile GPU platforms.\" Ieee Access 10 \n(2022): 86191-86203. \n[62] G√ºney, Emin, C√ºneyt Bayilmi≈ü, and Batuhan Cakan. \n\"Corrections to ‚ÄúAn Implementation of Real-Time Traffic Signs \nand Road Objects Detection Based on Mobile GPU Platforms‚Äù.\" \nIeee Access 10 (2022): 103587-103587. \n[63] X. Zhu, S. Lyu, X. Wang and Q. Zhao, \"TPH -YOLOv5: \nImproved YOLOv5 Based on Transformer Prediction Head for \nObject Detection on Drone -captured Scenarios,\" 2021 \nIEEE/CVF International Conference on Computer Vision \nWorkshops (ICCVW), Montreal,BC, Canada, 2021, pp. 2778 -\n2788, doi: 10.1109/ICCVW54120.2021.00312. \n[64] Wang, Yuan, Wei Guo, Shuanfeng Zhao, Buqing Xue, Wugang \nZhang, and Zhizhong Xing. 2022. \"A Big Coal Block Alarm \nDetection Method for Scraper Conveyor Based on YOLOBS\" \nSensors 22, no. 23: 9052. https://doi.org/10.3390/s22239052 \n[65] P. Zhao, X. Chen, S. Tang, Y. Xu, M. Yu and P. Xu, \"Fast \nRecognition and Localization of Electric Vehicle Charging \nSocket Based on Deep Learning  and Affine Correction,\" 2022 \nIEEE International Conference on Robotics and Biomimetics \n(ROBIO), Jinghong, China, 2022, pp. 2140 -2145, doi: \n10.1109/ROBIO55434.2022.10011985. \n[66] Mingqiang, Pan & Sun, Cheng & Liu, Ji & Wang, Yangjun. \n(2020). Automatic recogniti on and location system for electric \nvehicle charging port in complex environment. IET Image \nProcessing. 14. 10.1049/iet-ipr.2019.1138. \n[67] Zhang, Hui & Xiating, Jin. (2016). A Method for New Energy \nElectric Vehicle Charging Hole Detection and Location Based \non Machine Vision. 10.2991/emcpe-16.2016.84. \n[68] Miseikis, Justinas, Matthias Ruther, Bernhard Walzel, Mario \nHirz, and Helmut Brunner. \"3D vision guided robotic charging \nstation for electric and plug-in hybrid vehicles.\" ArXiv preprint \narXiv:1703.05381 (2017). M iseikis, Justinas, Matthias Ruther, \nBernhard Walzel, Mario Hirz, and Helmut Brunner. \"3D vision \nguided robotic charging station for electric and plug -in hybrid \nvehicles.\" arXiv preprint arXiv:1703.05381 (2017). \n[69] Li, Taoyong, Chunlei Xia, Ming Yu, Panpan Tang, Wei Wei, and \nDongmei Zhang. \"Scale - Invariant Localization of Electric \nVehicle Charging Port via Semi -Global Matching of Binocular \nImages.\" Applied Sciences 12, no. 10 (2022): 5247. \n[70] Guney, Emin, Ismail Hakki Sahin, Serap Cakar, Ozhan Atmaca, \nErdeniz Ero l, MertDoganli, and Cuneyt Bayilmis. \"Electric \nShore-to-Ship Charging Socket Detection Using Image \nProcessing and YOLO.\" In 2022 International Symposium on \nMultidisciplinary Studies and Innovative Technologies \n(ISMSIT), pp. 1069-1073. IEEE, 2022. \n[71] Liu, Shu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. \"Path \naggregation network forinstance segmentation.\" In Proceedings \nof the IEEE conference on computer vision andpattern \nrecognition, pp. 8759-8768. 2018. \n[72] Wang, Chien-Yao, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-\nYang Chen, Jun -WeiHsieh, and I -Hau Yeh. \"CSPNet: A new \nbackbone that can enhance learning capability ofCNN.\" In \nProceedings of the IEEE/CVF conference on computer vision \nand pattern recognition workshops, pp. 390-391. 2020. \n[73] Gu J, Wang Z, Kuen J, Ma L,  Shahroudy A, Shuai B, Liu T, \nWang X, Wang G, Cai J, Chen T (2018) Recent advances in \nconvolutional neural networks. Pattern Recogn 77:354 ‚Äì377. \nhttps://doi.org/10.1016/j.patcog.2017.10.013. \n[74] Diwan, T., Anirudh, G. & Tembhurne, J.V. Object detection \nusing YOLO: challenges, architectural successors, datasets and \napplications. Multimed Tools Appl 82, 9243 ‚Äì9275 (2023). \nhttps://doi.org/10.1007/s11042-022-13644-y. \n[75] Dirir, Ahmed, Henry Ignatious, Hesham El sayed, Manzoor \nKhan, Mohammed Adib, Anas Mahmoud, and Moatasem Al -\nGunaid. 2021. \"An Advanced Deep Learning Approach for \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n Author Name: Preparation of Papers for IEEE Access (February 2017)  \n17 \n \nMulti-Object Counting in Urban Vehicular Environments\" \nFuture Internet 13, no. 12: 306. \nhttps://doi.org/10.3390/fi13120306 \n[76] Zhou, Dingfu, Jin Fang, Xibin Song, Chenye Guan, Junbo Yin, \nYuchao Dai, and Ruigang Yang. \"Iou loss for 2d/3d object \ndetection.\" In 2019 International Conference on 3D Vision \n(3DV), pp. 85-94. IEEE, 2019. \n[77] Yang, Lingxiao, Ru -Yuan Zhang, Lida Li, and Xiaohua Xie. \n\"Simam: A simple, parameter free attention module for \nconvolutional neural networks.\" In International conference on \nmachine learning, pp. 11863-11874. PMLR, 2021. \n[78] Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng \nZhang, Stephen Lin, and Baining Guo. \"Swin transformer: \nHierarchical vision transformer using shifted windows.\" In \nProceedings of the IEEE/CVF international conference on \ncomputer vision, pp. 10012- 10022. 2021. \n[79] Jiang, Mingxin, Tao Hai, Zhigeng Pan, Haiyan Wang, Yinjie Jia, \nand Chao Deng. \"Multi -agent deep reinforcement learning for \nmulti-object tracker.\" IEEE Access 7 (2019): 32400-32407. \n[80] Zhou, Zhengxue, Leihui Li, Riwei Wang, and Xuping  Zhang. \n\"Deep learning on 3d object detection for automatic plug -in \ncharging using a mobile manipulator.\" In  2021 IEEE \nInternational Conference on Robotics and Automation (ICRA), \npp. 4148-4154. IEEE, 2021. \n[81] Shibl, Mostafa, Loay Ismail, and Ahmed Massoud. 20 21. \n\"Electric Vehicles Charging Management Using Machine \nLearning Considering Fast Charging and Vehicle -to-Grid \nOperation\" Energies 14, no. 19: 6199. \nhttps://doi.org/10.3390/en14196199. \n[82] Park J -H, Farkhodov K, Lee S -H, Kwon K -R. Deep \nReinforcement Learning -Based DQN Agent Algorithm for \nVisual Object Tracking in a Virtual Environmental \nSimulation. Applied Sciences . 2022; 12(7):3220. \nhttps://doi.org/10.3390/app12073220 \n[83] Lin, Haoyu, Pengkun Quan, Zhuo Liang, Ya‚Äônan Lou, Dongbo \nWei, and Shichun Di. 2022. \"Collisi on Localization and \nClassification on the End -Effector of a Cable -Driven \nManipulator Applied to EV Auto -Charging Based on DCNN ‚Äì\nSVM\" Sensors 22, no. 9: 3439. \nhttps://doi.org/10.3390/s22093439 \n[84] ElKashlan, Mohamed, Mahmoud Said Elsayed, Anca Delia \nJurcut, and Marianne Azer. 2023. \"A Machine Learning -Based \nIntrusion Detection System for IoT Electric Vehicle Charging \nStations (EVCSs)\"  Electronics 12, no. 4: 1044. \nhttps://doi.org/10.3390/electronics12041044 \n[85] Karanam, Vaishnavi, and Gil Tal. \"Developing a Deep Learning \nTool to Detect Electric Vehicle Supply Equipment Failures.\" \n \nV. C. MAHAADEVAN  received his B. E \nMechanical Engineering degree from University \nof Madras and MBA Marketing from \nPondicherry University and followed by M. \nTech in CAD from SRM university. He is \nhaving more than 23 years of experience in \nAutomotive industry with Quality Management \nSystem, Product development and Supplier \nchain management.  He stared carrier in Tier 2 \nindustry and moved to Hyundai ancillary for \nInterior & Seating system and currently working in Renault Nissan as \nGlobal Benchmark pilot & team leader for body  in white costing team. \nHe is pursuing Ph. D at SRM Institute of Science and Technology, \nChennai in the school of electrical and electronics engineering. \n \nNARAYANAMOORTHI R received the \nbachelor‚Äôs degree in electrical engineering and \nthe master‚Äôs degree i n control and \ninstrumentation from Anna University, India, in \n2009 and 2011, respectively, and the doctoral \ndegree from the SRM Institute of Science and \nTechnology, India, in 2019. He is an Associate \nProfessor with the Department of Electrical and \nElectronics Engineering, SRM Institute of \nScience and Technology. His research interests \ninclude wireless power transfer, electric  \nvehicles, power electronics, artificial  \nintelligence and machine learning in renewable energy systems, and \nembedded system for smart sensors.  \n \nRADOMIR GONO  (IEEE M‚Äô12 -SM‚Äô16) \nreceived his M.Sc., Ph.D., Habilitate Doctorate \nand Professor degrees, all in Electrical Power \nEngineering, in 1995, 2000, 2008, and 2019 \nrespectively. He has been with the Department of \nElectrical Power Engineerin g, VSB ‚ÄìTechnical \nUniversity of Ostrava, Czech Republic, since 1999 \nwhere he currently holds the position of Professor \nand Vice -head of the department. His current \nresearch interests are in the areas of electric power \nsystems reliability, optimization of ma intenance and renewable energy \nsources. \n \n                                \nPETR MOLDRIK  graduated from the faculty of \nelectrical engineering and computer science, VSB - \nTechnical university of Ostrava, Czech Republic, \nfrom Electrical power engineering branch, in 2003, \nand received the Ph.D. degree in Electrotechnics, \ncommunication and computer engineering, in 2008. \nSince 2003 he has been with the Department of \nelectrical power engineering. His research activity is \nfocused on the issue of the application of hydrogen \nfuel cells and related technologies, especially in the \nenergy sector, in the field of hybrid Electric Vehicles. \n \n \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321290\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}