{
  "title": "An Empirical Study on the Usage of Transformer Models for Code Completion",
  "url": "https://openalex.org/W3189653670",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4224001648",
      "name": "Ciniselli, Matteo",
      "affiliations": [
        "Università della Svizzera italiana"
      ]
    },
    {
      "id": "https://openalex.org/A4223219790",
      "name": "Cooper, Nathan",
      "affiliations": [
        "William & Mary"
      ]
    },
    {
      "id": "https://openalex.org/A3179307287",
      "name": "Pascarella, Luca",
      "affiliations": [
        "Università della Svizzera italiana"
      ]
    },
    {
      "id": "https://openalex.org/A4223120940",
      "name": "Mastropaolo, Antonio",
      "affiliations": [
        "Università della Svizzera italiana"
      ]
    },
    {
      "id": "https://openalex.org/A4222651660",
      "name": "Aghajani, Emad",
      "affiliations": [
        "Università della Svizzera italiana"
      ]
    },
    {
      "id": "https://openalex.org/A2236283596",
      "name": "Poshyvanyk, Denys",
      "affiliations": [
        "William & Mary"
      ]
    },
    {
      "id": "https://openalex.org/A2237772633",
      "name": "Di Penta, Massimiliano",
      "affiliations": [
        "University of Sannio"
      ]
    },
    {
      "id": "https://openalex.org/A2237111736",
      "name": "Bavota, Gabriele",
      "affiliations": [
        "Università della Svizzera italiana"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3108032709",
    "https://openalex.org/W2059556545",
    "https://openalex.org/W6783958930",
    "https://openalex.org/W2134092629",
    "https://openalex.org/W2096061896",
    "https://openalex.org/W2165747537",
    "https://openalex.org/W3174414731",
    "https://openalex.org/W3146720657",
    "https://openalex.org/W2106259924",
    "https://openalex.org/W4244284466",
    "https://openalex.org/W6783201201",
    "https://openalex.org/W2954823997",
    "https://openalex.org/W2122076271",
    "https://openalex.org/W2157976942",
    "https://openalex.org/W2402619042",
    "https://openalex.org/W2954451301",
    "https://openalex.org/W2740130862",
    "https://openalex.org/W1970018430",
    "https://openalex.org/W1977971855",
    "https://openalex.org/W6712677135",
    "https://openalex.org/W2084413241",
    "https://openalex.org/W3150708171",
    "https://openalex.org/W2797862469",
    "https://openalex.org/W6671236358",
    "https://openalex.org/W6601894380",
    "https://openalex.org/W2478709442",
    "https://openalex.org/W2266912522",
    "https://openalex.org/W2170460608",
    "https://openalex.org/W2138318139",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W2143861926",
    "https://openalex.org/W2907705732",
    "https://openalex.org/W3176740355",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3173543662",
    "https://openalex.org/W4232728046",
    "https://openalex.org/W6677035689",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W6636915900",
    "https://openalex.org/W3161997752",
    "https://openalex.org/W2883889688",
    "https://openalex.org/W4232188856",
    "https://openalex.org/W2953495996",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W3081454995",
    "https://openalex.org/W3121707215",
    "https://openalex.org/W2768572539",
    "https://openalex.org/W2052456234",
    "https://openalex.org/W1980450075",
    "https://openalex.org/W2768007977",
    "https://openalex.org/W6776498265",
    "https://openalex.org/W3160765131",
    "https://openalex.org/W2979679630",
    "https://openalex.org/W2080579313",
    "https://openalex.org/W3005628256",
    "https://openalex.org/W6730121546",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778205038",
    "https://openalex.org/W4245415816",
    "https://openalex.org/W1970607969",
    "https://openalex.org/W3161677724",
    "https://openalex.org/W3195421019",
    "https://openalex.org/W2972082064",
    "https://openalex.org/W4230312341",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W6779831603",
    "https://openalex.org/W3162689995",
    "https://openalex.org/W2110065044",
    "https://openalex.org/W3150567095",
    "https://openalex.org/W2964150020",
    "https://openalex.org/W3011564318",
    "https://openalex.org/W6779580516",
    "https://openalex.org/W1985514943",
    "https://openalex.org/W6760150090",
    "https://openalex.org/W2999760805",
    "https://openalex.org/W6768003788",
    "https://openalex.org/W2884670025",
    "https://openalex.org/W2964322208",
    "https://openalex.org/W1585991568",
    "https://openalex.org/W2973529529",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2557805692",
    "https://openalex.org/W3105903381",
    "https://openalex.org/W3105398568",
    "https://openalex.org/W3028741749",
    "https://openalex.org/W3015810308",
    "https://openalex.org/W2018389835",
    "https://openalex.org/W2142403498",
    "https://openalex.org/W2083878868",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1857789879",
    "https://openalex.org/W2115259925",
    "https://openalex.org/W3021206621",
    "https://openalex.org/W3034549508",
    "https://openalex.org/W3085219557",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1974020522",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W3035300716",
    "https://openalex.org/W2400997415",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3099302725",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2153943889",
    "https://openalex.org/W2921792613"
  ],
  "abstract": "Code completion aims at speeding up code writing by predicting the next code token(s) the developer is likely to write. Works in this field focused on improving the accuracy of the generated predictions, with substantial leaps forward made possible by deep learning (DL) models. However, code completion techniques are mostly evaluated in the scenario of predicting the next token to type, with few exceptions pushing the boundaries to the prediction of an entire code statement. Thus, little is known about the performance of state-of-the-art code completion approaches in more challenging scenarios in which, for example, an entire code block must be generated. We present a large-scale study exploring the capabilities of state-of-the-art Transformer-based models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). We experimented with several variants of two recently proposed Transformer-based models, namely RoBERTa and the Text-To-Text Transfer Transformer (T5), for the task of code completion. The achieved results show that Transformer-based models, and in particular the T5, represent a viable solution for code completion, with perfect predictions ranging from ~29%, obtained when asking the model to guess entire blocks, up to ~69%, reached in the simpler scenario of few tokens masked from the same code statement.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 1\nAn Empirical Study on the Usage of Transformer\nModels for Code Completion\nMatteo Ciniselli, Nathan Cooper, Luca Pascarella, Antonio Mastropaolo, Emad Aghajani,\nDenys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota\nAbstract—Code completion aims at speeding up code writing by predicting the next code token(s) the developer is likely to write. Works\nin this ﬁeld focused on improving the accuracy of the generated predictions, with substantial leaps forward made possible by deep\nlearning (DL) models. However, code completion techniques are mostly evaluated in the scenario of predicting the next token to type,\nwith few exceptions pushing the boundaries to the prediction of an entire code statement. Thus, little is known about the performance of\nstate-of-the-art code completion approaches in more challenging scenarios in which, for example, an entire code block must be generated.\nWe present a large-scale study exploring the capabilities of state-of-the-art Transformer-based models in supporting code completion at\ndifferent granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block\nof a for loop). We experimented with several variants of two recently proposed Transformer-based models, namely RoBERTa and the\nText-To-Text Transfer Transformer (T5), for the task of code completion. The achieved results show that Transformer-based models, and in\nparticular the T5, represent a viable solution for code completion, with perfect predictions ranging from ∼29%, obtained when asking the\nmodel to guess entire blocks, up to ∼69%, reached in the simpler scenario of few tokens masked from the same code statement.\nIndex Terms—Code Completion, Deep Learning, Empirical Software Engineering\n!\n1 I NTRODUCTION\nCode completion is considered as one of the “killer” fea-\ntures of modern Integrated Development Environments\n(IDEs) [18], [49], [72]: It can provide developers with pre-\ndictions about the next code token ( e.g., a method call)\nto write given the code already written in the IDE, thus\nspeeding up software development and preventing potential\nmistakes [33], [35].\nSeveral works in this ﬁeld have been proposed. Most\nof them aim at advancing the performance of code comple-\ntion tools, especially in terms of prediction accuracy. Such\nresearch has allowed moving from simple alphabetically\nranked lists of recommendations for completing what a\ndeveloper is typing (e.g., a list of possible method calls match-\ning what has been typed by the developer) to “intelligent”\n• M. Ciniselli is with SEART @ Software Institute, Università della Svizzera\nitaliana, Switzerland.\nE-mail: matteo.ciniselli@usi.ch\n• N. Cooper is with SEMERU @ William & Mary, USA.\nE-mail: nacooper01@email.wm.edu\n• L. Pascarella is with SEART @ Software Institute, Università della Svizzera\nitaliana, Switzerland.\nE-mail: luca.pascarella@usi.ch\n• A. Mastropaolo is with SEART @ Software Institute, Università della\nSvizzera italiana, Switzerland.\nE-mail: antonio.mastropaolo@usi.ch\n• E. Aghajani is with SEART @ Software Institute, Università della Svizzera\nitaliana, Switzerland.\nE-mail: emad.aghajani@usi.ch\n• D. Poshyvanyk is with SEMERU @ William & Mary, USA.\nE-mail: denys@cs.wm.edu\n• M. Di Penta is with University of Sannio, Italy.\nE-mail: dipenta@unisannio.it\n• G. Bavota is with SEART @ Software Institute, Università della Svizzera\nitaliana, Switzerland.\nE-mail: gabriele.bavota@usi.ch\ncompletions considering the context surrounding the code\n[18], [72], the history of code changes [72], and/or coding\npatterns mined from software repositories [10], [36], [39],\n[62], [63], [65], [77]. Last, but not least, Deep Learning (DL)\nmodels have been applied to code completion [8], [22], [47],\n[49], [73], [84], setting new standards in terms of prediction\nperformance. Although the performance of code completion\ntechniques has substantially improved over time, the type\nof support they provide to developers has not evolved\nat the same pace. Indeed, besides a few works focusing\non predicting multiple code tokens ( e.g., [8], [73]) or even\nrecommending entire statements ( e.g., [11], [83]), most of\nthe approaches presented in the literature have only been\nexperimented in the speciﬁc scenario in which the next token\nthe developer is likely to type must be predicted. This leaves\nthe following question partially unanswered: how far can we\ngo with DL-based token prediction (even beyond the source code\nline boundary)?\nWe present a large-scale empirical study exploring the\nlimits and capabilities of state-of-the-art DL models to sup-\nport code completion. Besides generating the next token(s)\nthe developer is likely to write, we apply DL models to\ngenerate entire statements and code blocks ( e.g., the body\nof an if statement). Among the many DL models proposed\nin the literature, we focus on models using the Transformer\narchitecture [81]. In particular, in our recent work published\nat MSR 2021 [22] we evaluated the performance of a RoBERTa\nmodel [55] in the code completion tasks described above.\nRoBERTa is a BERT (Bidirectional Encoder Representations\nfrom Transformers) model [24] using a pre-training task in\nwhich random words in the input sentences are masked out\nusing a special <MASK> token, with the model in charge of\npredicting the masked words. While experimenting with\nRoBERTa for the task of code completion, we faced an\narXiv:2108.01585v2  [cs.SE]  18 Nov 2021\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 2\nimportant limitation that did not make it suitable for the\nstudy we wanted to perform ( i.e., the prediction of multiple\nmasked tokens): In the RoBERTa pre-training task n<MASK>\ntokens must be used to mask ncode tokens, thus implicitly\nsuggesting to the model how many code tokens must\nbe generated to autocomplete the masked statement. This\nwould not be realistic in a real usage scenario, in which the\ncode completion engine must guess the tokens to generate,\nwithout the developer suggesting how many tokens must be\ngenerated. To overcome this limitation, we had to adapt the\nRoBERTa pre-training objective to be able to guess, from a\nsingle <MASK> token masking one or more code tokens in the\ngiven statements, which and how many code tokens must be\ngenerated [22]. The adaptation of the RoBERTa pre-training\nobjective was inspired by the recently proposed Text-To-Text\nTransfer Transformer (T5) architecture [68], suggesting this\nas a good ﬁt for the task of code completion.\nIn this work, we extend our MSR 2021 paper [22] by\nshowing that the T5 substantially overcomes the performance\nof the RoBERTa model, being able to correctly predict even\nentire code blocks, something that we found to be not\nachievable with RoBERTa. As in [22], we focus on three\ncode prediction scenarios: (i) token-level predictions, namely\nclassic code completion in which the model is used to guess\nthe last ntokens in a statement the developer started writing;\n(ii) construct-level predictions, in which the model is used to\npredict speciﬁc code constructs ( e.g., the condition of an if\nstatement) that can be particularly useful to developers while\nwriting code; and (iii) block-level predictions, with the masked\ncode spanning one or more entire statements composing a\ncode block (e.g., the iterated block of a for loop).\nWe compare the performance of several models. First, we\nuse the RoBERTa model as presented in [22] as representative\nof BERT-like models. Second, we use T5 model for the task\nof code completion for the ﬁrst time in this paper. The\nT5 has been recently shown to outperform many state-of-\nthe-art techniques in code-related tasks [59]. In particular,\nMastropaolo et al. [59] showed the possibility to train a single\nT5 model dealing with four code-related tasks, namely bug\nﬁxing, injection of code mutant, assert statements generation,\nand code summarization. Third, we experiment with an n-\ngram model as a baseline for DL-based models, also showing\nthe impact on its performance of using a caching mechanism\nas proposed by Hellendoorn and Devanbu [36].\nBoth RoBERTa and T5 models are trained in two phases:\npre-training which allows deﬁning a shared knowledge-base\nuseful for a large class of sequence-to-sequence tasks ( e.g.,\nguessing masked words in English sentences to learn about\nthe language), and ﬁne-tuning which specializes the model\non a speciﬁc downstream task ( e.g., learning the translation\nof sentences from English to German).\nSeveral tasks can be used in the ﬁne-tuning, to possibly\ntake advantage of transfer learning (i.e., the knowledge\nacquired on one task can be reused by the model for another\ntask). For example, a single model trained on multiple\ntranslation tasks ( e.g., from English to German, English to\nFrench, and French to German) could be more effective than\nthree different models each trained on a speciﬁc translation\ntask (e.g., English to German).\nIn our work, we want to investigate the performance of\nthe two transformer-based models by also looking at the role\nplayed on the models’ performance by the pre-training task\nand the transfer learning across different tasks. However,\nsince this requires the training of many different variants of\nthe experimented models, we adopt the following strategy.\nFirst, we compare RoBERTa and T5 by training three different\nmodels for the three code completion scenarios ( i.e., token-\nlevel, construct-level, and block-level) we experiment with.\nThis implies creating three different RoBERTa and T5 models\n(six models overall). Then, we take the best performing\none (T5) and we show that using pre-training increases its\nperformance, even though the impact is limited. Finally,\nwe show that ﬁne-tuning a single T5 model to support all\nthree prediction tasks boosts performance conﬁrming transfer\nlearning across the three very similar tasks ( i.e., knowledge\nacquired in one task can be used to perform another task).\nThe achieved results show that, for a typical code\ncompletion task ( i.e., token-level ), T5 correctly guesses all\nmasked tokens in 66% to 69% of cases (depending on the\nused dataset), while RoBERTa achieving 39% to 52% and the\nn-gram model 42% to 44%. In the most challenging prediction\nscenario, in which we mask entire blocks, RoBERTa and the\nn-gram model show their limitations, being able to only\ncorrectly reconstruct the masked block in less than 12% of\nthe cases, while the T5 achieves 30% of correct predictions.\nIt is worth noting that the goal of our study is not to show\nthat the T5 model is the best option for neural-based code\ncompletion. Our work focuses on empirically exploring the\ncapabilities of learning-based code completion techniques,\nand T5, RoBERTa, and the n-gram model have been chosen\nas representatives of the state-of-the-art.\nIn summary, as compared to our MSR 2021 paper [22],\nthe contributions of this work are as the following: (i) we\nperform a comprehensive empirical study with an additional\nstate-of-the-art approach, namely the T5 model, showing its\nvery promising performance for the code completion task;\n(ii) differently from [22] in which three different RoBERTa\nmodels have been ﬁne-tuned on the three code completion\nscenarios (i.e., token-level, construct-level, and block-level)\nwithout pre-training and without testing the impact of\ntransfer learning, we pre-train and ﬁne-tune several versions\nof the best performing model ( i.e., the T5), to investigate\nthese aspects; (iii) for the best performing model, we also\nexplore the possibility of exploiting the conﬁdence of the\npredictions as a measure of the prediction quality, showing\nthe reliability of such an indicator.\nThe source code and data used in our work are publicly\navailable in a comprehensive replication package [21].\n2 R ESEARCH QUESTIONS AND CONTEXT\nThe study goal is to assess the effectiveness of Transformer-\nbased DL models in predicting masked code tokens at dif-\nferent granularity levels. We address the following research\nquestions (RQs):\nRQ1: To what extent are transformer models a viable approach\nto learn how to autocomplete code? This RQ investigates the\nextent to which T5 and RoBERTa can be used for predicting\nmissing code tokens. We assess the quality of the generated\npredictions from both a quantitative ( i.e., BLEU score [25],\nLevenshtein distance [51]) and a qualitative ( i.e., perfect\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 3\npredictions, potential usefulness of wrong predictions) point\nof view. RQ1 is further detailed in the following two sub-RQs:\nRQ1.1: To what extent does the number of masked tokens\nimpact the prediction quality? We train and test the approaches\nwe experiment with on datasets in which masked code tokens\nspan from few contiguous tokens in a given statement to\nmultiple missing statements composing a code block. RQ 1.1\nexplores the limits of Transformer models when considering\nsimple and more challenging code completion scenarios.\nRQ1.2: To what extent are the performance of the models\ninﬂuenced by the speciﬁcity of the dataset employed for training and\ntesting it? While it is reasonable to expect that larger training\ndatasets tend to help deep learning models, we are interested\nin answering RQ1.2 from a different perspective. To address\nthis RQ, we compare the autocompletion performance on\ntwo different datasets: a ﬁrst, more general one, composed\nof Java methods; and a second, more speciﬁc one, composed\nof methods from Android apps. While the programming\nlanguage is the same, the granularity of the two datasets\nis the same (i.e., method-level granularity), methods in the\nsecond dataset make heavy use of Android APIs, and the\nsame APIs are likely to be used for similar purposes, e.g., app\nfeatures dealing with GPS positioning share common API\nusages. We expect this to create “regularities” in the Android\ndataset to help model learning.\nRQ2: What is the role of pre-training and transfer learning\nin the performance of Transformer-based models? As explained\nin Section 1, both RoBERTa and T5 can be pre-trained and\nthen ﬁne-tuned on several tasks. RQ 2 investigates the boost\nin performance (if any) brought by (i) pre-training of the\nmodels, and (ii) ﬁne-tuning a single model on several tasks\nto take advantage of transfer learning. Such an additional\nanalysis has been performed only for the best-performing\nmodel (i.e., the T5).\nRQ3: How do transformer models compare to a state-of-the-art\nn-gram model? An alternative to DL models is represented by\nstatistical language models based on n-grams. In this research\nquestion, we compare the DL models to (i) a classical n-gram\nmodel and, (ii) in a smaller study, to the state-of-the-art\nn-gram cached model [36].\n2.1 Context Selection: Datasets\nOur study involves two datasets. The ﬁrst one comes from\nour MSR’21 paper [22] and is used to ﬁne-tune the RoBERTa\nand T5 models and to train the n-gram model. We refer\nto this dataset as ﬁne-tuning dataset and it includes both\na Java and an Android dataset to allow answering RQ 1.2.\nThe ﬁne-tuning dataset has been built starting from the\nCodeSearchNet dataset [41], which features Java methods\nmined from open source projects. The second dataset has\nbeen built speciﬁcally to answer RQ 2, i.e., to have a different\ndataset that can be used to pre-train the best performing\nmodel among RoBERTa and T5 (i.e., pre-training dataset). The\nfollowing section describes how the datasets have been built.\n2.1.1 Fine-tuning dataset\nTo create the Java dataset, we started from the CodeSearchNet\nJava Dataset provided by Husain et al. [41]. We decided to\nstart from CodeSearchNet rather than from other datasets\nproposed in the literature (see e.g., [58], [67]) since Code-\nSearchNet has been already subject to cleaning steps making\nit suitable for applications of machine learning on code.\nAlso, CodeSearchNet is already organized at method-level\ngranularity ( i.e., one instance is a method), while other\ndatasets, such as the 50k [58], collect whole repositories. In\nparticular, CodeSearchNet contains over 1.5M Java methods\ncollected from open-source, non-fork, GitHub repositories.\nFor details on how the dataset has been built, see the report\nby Husain et al. [41]. For our work, the most important criteria\nused in the dataset construction are: (i) excluding methods of\nfewer than three lines; (ii) removing near-duplicate methods\nusing the deduplication algorithm from CodeSearchNet; this\nis done to not inﬂate the performance of the models as a\nresult of overlapping instances between training and test sets\n[7] and (iii) removing methods with the name containing\nthe “test” substring in an attempt to remove test methods;\nmethods named “toString” are removed as well. The latter\nare often automatically generated by the IDEs with a very\nsimilar structure (e.g., mostly concatenating class attributes).\nThus, they rarely represent a challenging code completion\nscenario and can result in inﬂating the prediction accuracy.\nTo build the Android dataset we adopted a similar proce-\ndure. We cloned the set of 8,431 open-source Android apps\nfrom GitHub available in the AndroidTimeMachine dataset\n[28]. Then, we extracted from each project’s latest snapshot\nthe list of methods. This resulted in a total of ∼2.2M methods.\nThen, we applied the same ﬁltering heuristics deﬁned for the\nJava dataset, ending up with 654,224 methods. Since one of\nthe goals of our study is also to compare the performance\nof the models when applied on a more generic (Java) and\na more speciﬁc (Android) dataset, we randomly selected\n(using the random Python function) 654,224 methods from\nthe Java dataset, to match the size of the Android dataset.\nIn our MSR paper [22], we also experimented with code\nabstraction as used in the previous studies [79], [80] to\navoid the open vocabulary problem. However, new DL-\nbased models do not suffer from this limitation anymore\nthanks to the usage of tokenizers exploiting techniques such\nas Byte Pair Encoding (BPE) [27]. For this reason, while in\n[22] we built two versions of the ﬁne-tuning dataset (with\nand without abstraction), in this work we only focus on\nthe datasets using raw source code since this is the real\nscenario in which code completion techniques are used. Such\nclariﬁcation is needed since, when building the ﬁne-tuning\ndataset, methods for which parsing errors occurred during\nthe abstraction process were excluded [22], leaving the Java\ndataset with 634,799 methods, and the Android one with\n532,096.\nThen, the three versions of each dataset (Java and\nAndroid) summarized in Table 1 were created using the\nfollowing masking processes (note that Table 1 reports the\nnumber of instances in each dataset after the ﬁltering steps\ndescribed below):\nToken masking. For each code line l in each method\nhaving more than one token we mask its last xtokens, where\nxis a random number between 1 ...n −1, where nis the\nnumber of tokens composing l. Given a method mhaving k\nlines with more than one token, we generate kversions of m,\neach of them having one line with the last xtokens masked\nand the remaining k−1 reported without any change ( i.e.,\nno masked tokens, just the original raw source code). We\nset the maximum number of masked tokens to 10 ( i.e., if\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 4\nx> 10 then x= 10). This scenario simulates the classic code\ncompletion task in which a developer is writing a statement\nand the code completion tool is in charge of autocompleting\nit.\nConstruct masking. We selected a number of code\nconstructs for which it could be particularly useful to\nbe supported with automated code completion. Given a\nmethod m, we use the scrML [4] toolkit to identify all m’s\ntokens used to: (i) deﬁne the complete condition of an if\nstatement or of a while/for loop (e.g., in a statement having\nfor(int i=0; i<data.size(); i++) we identify all\ntokens between parenthesis as those used to deﬁne the for\nloop); (ii) deﬁne the parameters in a method call ( e.g., in\ncopyFile(source, target) the tokens “source”, “,”,\nand “target” are identiﬁed); and (iii) deﬁne the exception\ncaught in a catch statement (e.g., in catch(IOException\nio) we identify IOException io as the involved tokens).\nFor m this results in a set S={T1, T2, ... , Tn}, where Ti\nrepresents a set of relevant tokens for one of the previously\nmentioned constructs ( e.g., Ti is the set of tokens used to\ndeﬁne the for loop condition).\nGiven m, we generate |S|versions of it, each one having\none of the subject constructs masked. Also, in this case we set\nthe maximum number of masked tokens to 10. This means\nthat if a construct requires more than 10 tokens to be masked\n(this happened for 9.38% of the constructs in our dataset), it\nis not masked in our dataset.\nThe code completion tasks simulated by the construct\nmasking resemble cases in which the developer uses the\ntechnique to get recommendations about non-trivial code\ntokens, representing decision points in the program ﬂow\n(e.g., condition of if statement) or error-handling cases (e.g.,\nexceptions to catch).\nBlock masking.We use srcML to identify in each method\nm its code blocks. We deﬁne a code block as the code\nenclosed between two curly brackets. For example, a block\nmay be, besides the method body itself, the code executed in\na for/while loop, when an if/else/else if condition\nis satisﬁed, etc. Then, given kthe number of blocks identiﬁed\nin m, we create kversions of meach one having a speciﬁc\ncode block masked. We set the maximum size of the masked\nblock to two complete statements. This means that if a block\nis composed of more than two statements (which happened\nfor 49.29% of the blocks in our dataset), it is not masked. This\nis the most challenging code completion scenario in which\nwe test the experimented techniques. If successful in this\ntask, code completion techniques could substantially speed\nup code implementation activities.\nIn summary, there are six ﬁne-tuning datasets: For each of\nthe two domains (Java or Android), there are three different\nmasking levels (token, construct, block). These masking\nlevels have been pick to simulate code completion tasks\nhaving different complexity (with token masking expected to\nbe the simplest and block-masking the most complex).\nStarting from the six datasets, we created the training,\nevaluation, and test sets in Table 1. As a ﬁrst step, we ﬁltered\nout speciﬁc instances from our datasets. First, when using\ngenerative deep learning models, the variability in length\nof the sentences (in our case, methods) provided as input\ncan affect the training and performance of the model, even\nwhen techniques such as padding are employed. For this\nTABLE 1\nStudy datasets. One instance corresponds to a method with masked\ntoken(s).\nDomain Masking Dataset #Instances #TokensLevel\nToken\nTraining 750k 46.9M\nEvaluation 215k 13.4M\nTest 219k 13.6M\nConstruct\nTraining 750k 48.2M\nJava Evaluation 104k 6.7M\nTest 106k 6.7M\nBlock\nTraining 298k 19.1M\nEvaluation 39k 2.5M\nTest 40k 2.5M\nToken\nTraining 750k 47.4M\nEvaluation 198k 12.5M\nTest 201k 12.6M\nConstruct\nTraining 750k 48.9M\nAndroid Evaluation 99k 6.4M\nTest 101k 6.5M\nBlock\nTraining 205k 13.4M\nEvaluation 27k 1.7M\nTest 27k 1.8M\nreason, we analyzed the distribution of methods length in\nour dataset, ﬁnding that two-thirds of them are composed\nof at most 100 tokens. For this reason, as done by Tufano\net al. [80], we excluded from our datasets all the methods\nhaving more than 100 tokens. Second, RoBERTa cannot\nefﬁciently handle cases in which the masked tokens are more\nthan the non-masked tokens. This often happens, for example,\nwhen masking the entire method body in the block-level\nmasking approach. Thus, those instances are excluded as\nwell.\nAfter the ﬁltering steps, we split each of the six datasets\ninto training (80%), evaluation (10%), and test (10%) sets.\nWhile the methods in the dataset are randomly ordered, the\nsplitting we performed was not random to avoid biasing the\nlearning. To explain this point, let us consider the case of the\nblock masking dataset. Given a method mhaving kblocks in\nit, we add in the dataset k versions of m, each having one\nand only one block masked. Suppose that mcontains two\nblocks b1 and b2, thus leading to two versions of m: One in\nwhich b1 is masked (mb1 ) and b2 is not and vice versa (mb2 ).\nWith a random splitting, it could happen that mb1 is assigned\nto the training set and mb2 to the test set. However, in mb1\nthe b2 is not masked. Thus, when the model has to guess\nthe tokens masked in mb2 it would have the solution in the\ntraining set, resulting in boosted prediction performance. For\nthis reason, we randomly select 80% of the methods in each\ndataset and assign all of their masked versions to the training\nset. Then, we proceed in the same way with evaluation and\ntest sets.\nUsing this procedure, we obtained the datasets in Table 1.\nImportant to note is that, given the original size of the\ndatasets using token-level and construct-level masking, we\ndecided to cap the training set to 750k instances (no changes\nwere done in the evaluation and test sets). This was necessary\ngiven the computationally expensive process of training\nseveral DL models (as it will be clear later, our study required\nthe training of 19 different DL-based models). Also, the size\nof the evaluation and test sets is slightly different since, as\nexplained before, we split the dataset based on the methods\n(not on their masked versions) and each method can result\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 5\nin a different number of its generated masked versions.\n2.1.2 Pre-training dataset\nTo build the pre-training dataset, we used the GitHub Search\nplatform [23] to identify all Java repositories having at least\n100 commits, 10 contributors, and 10 stars. These ﬁltering\ncriteria only aim at reducing the likelihood of selecting toy\nand personal projects for the building of this dataset. We\nsorted the projects by their number of stars, cloning the\ntop 6,000 and extracting from each of them the methods\nin the latest snapshot tagged as a release, to only rely on\nmethods likely to be syntactically correct. Repositories for\nwhich no snapshot was tagged as a release were excluded,\nleaving 3,175 repositories. Finally, since we wanted to avoid\nextremely large projects to inﬂuence the dataset too much\n(i.e., to contribute too many methods to the dataset), we\ncap the maximum number of methods to extract from each\nrepository to 1,500. This was also due to limit the number of\nthe pre-training instances to a manageable size according to\nour available hardware resources. In addition to the ﬁlters\nused while building the ﬁne-tuning dataset (see Section 2.1.1),\nwe also removed test methods identiﬁed as all those using the\n@test annotation or containing the word “test” in the method\nname after camel case splitting ( i.e., we do not exclude\nupdateStatus). Also, since the goal of the pre-training dataset\nis to provide instances in which random tokens are masked\nto make the model “familiar” with a speciﬁc context ( i.e., the\nJava language in our case), we excluded very short methods\n(<15 tokens) not having enough elements to mask and, for\nthe same reasons explained for the ﬁne-tuning dataset, long\nmethods (in this case, >200 tokens).\nWe then removed all the exact duplicates within the\npre-training dataset, keeping in the dataset only the ﬁrst\noccurrence of each duplicate. After having removed the\nduplicates, the dataset contained 1,874,338 different methods.\nFinally, we ensured that the pre-training dataset does not\ncontain any methods belonging to the ﬁne-tuning dataset\n(neither in the training, evaluation, or test sets). We found a\ntotal of 23,977 duplicates between the pre-training and the\nﬁne-tuning datasets, leading to a ﬁnal number of 1,850,361\ninstances in the pre-training dataset.\n2.2 Context Selection: Techniques\nIn this section we overview three experimented techniques,\ni.e.,RoBERTa [55], T5 [68], and n-gram [36]. We refer to the\noriginal papers presenting them for additional details.\n2.2.1 RoBERTa\nThe ﬁrst Transformer-based model leverages the off-the-\nshelf RoBERTa model, which is an Encoder-Transformer\narchitecture. Details about the RoBERTa model are provided\nin a report by Liu et al. [55], while here, we mainly focus\non explaining why it represents a suitable choice for code\ncompletion. BERT-based models, such as RoBERTa, use\na special pre-training where random words in the input\nsentence are masked out with a special <MASK> token. This\npre-training task is very well-suited to simulate a code\ncompletion task, in which the input is an incomplete code\nsnippet the developer is writing and the masked tokens\nrepresent the code needed to autocomplete the snippet.\nHowever, one limitation of such a pre-training is that when\nattempting to predict multiple tokens, e.g., an entire masked\nif condition, it requires the number of tokens to generate to\nbe known, due to the ﬁxed sequence length of Transformers\n[81]. To overcome this issue, we modify such an objective by\nmasking spans of tokens using a single <MASK> token.\nAs previously explained, BERT models (such as RoBERTa)\ncan be pre-trained and ﬁne-tuned on several tasks [24]. The\nresult will be a single model able to support different tasks\nand, possibly, taking advantage of what it learned for a\nspeciﬁc task to also improve its performance in a different\ntask. In our study, we start by comparing the RoBERTa\nand the T5 models in a scenario in which no pre-training\nis performed and a single model is built for each of the\nthree code completion tasks previously described ( i.e., token,\nconstruct, and block masking) by using the ﬁne-tuning dataset.\nThen, for the best performing model among the two ( i.e.,\nT5), we also experiment with pre-training and multi-task\nﬁne-tuning. We trained six RoBERTa models, one for each\ndataset in Table 1.\nAs for the implementation of the RoBERTa model, we\nused the one provided in the Python transformers library [86].\nWe also train a tokenizer for each model to overcome the out-\nof-vocabulary problem. The out-of-vocabulary problem happens\nwhen a machine learning model deals with terms that were\nnot part of the training set but appear in the test set. We\ntrained a Byte Pair Encoding (BPE) [27] model using the\nHuggingFace’stokenizers Python library [2]. BPE uses bytes\nas vocabulary, allowing it to tokenize every text without\nrequiring the unknown token often used in applications of\nDL to NLP , thus overcoming the out-of-vocabulary problem.\nWhen used on source code [46], BPE has been shown to\naddress the out-of-vocabulary problem.\n2.2.2 T5\nRaffel et al. [68] presented the T5 model that leverages\nmulti-task learning to implement transfer learning in the\nNLP domain. The T5 has been presented in ﬁve pre-deﬁned\nvariants [68]: small, base, large, 3 Billion, and 11 Billion that\ndiffer in complexity, size, and, as a consequence, training\ntime. T5small, the smaller variant, has 60 million parameters\nwhile T511B, the largest, has 11 billion parameters. Despite\nRaffel et al. [68] report that highlights the largest model offers\nthe best accuracy, its training time is sometimes too high to\njustify its use. Given our computational resources, we opted\nfor the T5small model; therefore, we expect that our results\nrepresent a lower bound for the performance of a T5-based\nmodel.\nT5 offers two advantages as compared to other DL\nmodels: (i) it is usually more efﬁcient than RNNs since it\nallows to compute the output layers in parallel, and (ii) it can\ndetect hidden and long-ranged dependencies among tokens,\nwithout assuming that nearest tokens are more related than\ndistant ones. The latter is particularly relevant in code-related\ntasks. For example, a local variable could be declared at\nthe beginning of a method (ﬁrst statement), used in the\nbody inside an if statement, and ﬁnally returned in the\nlast method’s statement. Capturing the dependency existing\nbetween these three statements, that might even be quite\nfar from each other ( e.g., variable declaration and return\nstatement), can help in better modeling the source code with\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 6\na consequent boost of performance for supporting code-\nrelated tasks.\nFor additional details about the T5 architecture, we refer\nthe reader to the original work presenting this model [68].\n2.2.3 n-gram\nAs a baseline for comparison, we used the widely studied\nstatistical language models based on n-gram. An n-gram\nmodel can predict a single token following the n−1 tokens\npreceding it. Even though the n-gram model is meant to\npredict a single token given the n−1 preceding tokens, we\ndesigned a fair comparison for the scenario in which we mask\nmore than one token. In particular, we use the n-gram model\nin the following way: Let us assume that we are predicting,\nusing a 3-gram model, how to complete a statement having\nﬁve tokens T, of which the last two are masked ( M): <T1, T2,\nT3, M4, M5>, with M4 and M5 masking T4 and T5, respectively.\nWe provide as input to the model T2 and T3 to predict M4,\nobtaining the model prediction P4. Then, we use T3 and T4\nto predict M5, thus obtaining the predicted sentence <T1, T2,\nT3, P4, P5>. Basically, all predictions are joined to predict\nmultiple contiguous tokens.\nThe n-gram models are trained on the same training sets\nused for the ﬁne-tuning of the DL models without, however,\nmasked tokens. We experiment with both the standard n-\ngram model ( i.e., the one discussed above) as well as, in a\nsmaller study, with the n-gram cached model proposed by\nHellendoorn and Devanbu [36].\n3 D ATA COLLECTION AND ANALYSIS\nIn this section we detail the data collection and analysis pro-\ncedure adopted to answer the research questions described\nin Section 2.\n3.1 Training of Models\nWe detail the process used for the training and hyperpa-\nrameters tuning of the two deep learning models that we\nexperimented with.\nTABLE 2\nHyperparameters Tuned for the RoBERTa Models.\nHyperparameter Experimented Values Best\nLearning rate { 5e−5, 3e−5, 2e−5} 5e−5\nBatch size {16, 32, 64} 64\n# Hidden Layers {6, 12, 16} 12\n# Attention Heads {6, 12, 16} 16\nHidden Layer Size {256, 512, 768, 1024} 768\nIntermediete Size {3072, 4096} 4,096\n3.1.1 RoBERTa\nWe performed hyperparameter tuning using the Weights &\nBiases’s [5] Python library on a Linux server with an Nvidia\nRTX Titan GPU. Table 2 reports the hyperparameters we\ntuned, the range of values we tested for them, and the value\nin the best conﬁguration we found. Besides those parameters,\nwe used an attention dropout probability of 0.1, and a\nhidden layer dropout probability of 0.3. For the tokenizer, the\nvocabulary size was set to 50k. The hyperparameter search\nwas performed using the training and the evaluation sets\nof the Android dataset with token masking. We picked as\nthe best conﬁguration the one that, when applied to the\nevaluation set, was able to obtain the highest number of\n“perfect predictions”. We deﬁne as “perfect” a prediction that\nexactly matches the code written by the developers. Thus,\nthe model correctly guesses all masked tokens. If one of the\nmasked tokens is different we do not consider the prediction\nas “perfect”. While, in principle, a different hyperparameter\ntuning would be necessary for each dataset, such a process\nis extremely expensive, and preliminary investigations we\nperformed on a subset of the other datasets showed minor\ndifferences in the achieved best conﬁguration.\nThe training was performed across servers using their\nGPUs. The ﬁrst was equipped with an Nvidia Tesla V100S,\nthe second with an Nvidia RTX Titan, and the third with\n3 Nvidia GTX 1080Ti. The training time strongly depends\non the size of the dataset and the used server but ranged\nbetween 28 and 114 hours per model. Note that, once trained,\neach model can be used to perform predictions in the split\nof a second (on average, 0.12 seconds on a laptop CPU),\nthus making them a viable solution for “real-time” code\ncompletion.\nWe train each model for a maximum of 50 epochs.\nHowever, we adopted the following stopping condition. At\nthe end of each training epoch, we executed the model on\nthe evaluation set and we compute the number of perfect\npredictions. If we observe that, during the training, the\nperformance of the model is worsening in terms of perfect\npredictions on the evaluation set ( i.e., the model is likely\noverﬁtting to the training set), we stop the training. In\nparticular, given a model trained for nth epoch, we stop the\ntraining if the number of perfect predictions on the evaluation\nset is lower than the number of perfect predictions achieved\nafter the n−4 epoch. This ensures that the models can have\nsome ﬂuctuations in performance for up to three epochs.\nThen, if it is still not improving, we stop its training and\ntake the best model (in terms of perfect predictions on the\nevaluation test) obtained up to that moment. None of the\nmodels were trained for the whole 50 epochs.\nTABLE 3\nHyperparameters Tuned for the T5 Models.\nLearning Rate Type Parameters\nConstant (C-LR) LR = 0.001\nSlanted Triangular (ST-LR) LRstarting = 0.001\nLRmax = 0.01\nRatio = 32\nCut = 0.1\nInverse Square Root (ISQ-LR) LRstarting = 0.01\nWarmup = 10, 000\nPolynomial Decay (PD-LR) LRstarting = 0.01\nLRend = 1e−06\nPower = 0.5\n3.1.2 T5\nWe rely on the same conﬁgurations used by Mastropaolo\net al. [59]. In particular, concerning the pre-training, we do\nnot tune the hyperparameters of the T5 model because the\npre-training step is task-agnostic, and this would provide\nlimited beneﬁts. Instead, we experiment with four different\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 7\nlearning rate schedules for the ﬁne-tuning phase, using the\nconﬁgurations reported in Table 3, and identify the best-\nperforming conﬁguration in terms of perfect predictions on\nthe evaluation sets. Each of the four experimented conﬁgu-\nrations has been trained for 100k steps ( ∼7 epochs) before\nassessing its performance on the evaluation sets. Across\nall six evaluation datasets (Table 1), the best performing\nconﬁguration was the one using the Slanted Triangular\nlearning rate, conﬁrming the ﬁndings in [59]. Also, all T5\nmodels we built use a SentencePiece [50] tokenizer trained\non the pre-training dataset and are composed of 32k word\npieces [59].\nThe best conﬁguration we identiﬁed has been used to\ntrain six different T5 models ( i.e., one for each dataset in\nTable 1) and assess their performance on the corresponding\ntest set. These results can be used to compare directly the\nT5 and the RoBERTa model when ﬁne-tuned without pre-\ntraining and in a single-task setting (i.e., no transfer learning).\nSince we found the T5 to perform better than RoBERTa, we\nalso use this model to answer RQ2. Thus, in addition to these\nsix models, we also built additional seven models: six of\nthem leverage pre-training plus single-task ﬁne-tuning. In\nother words, they are the equivalent of the ﬁrst six models\nwe built, with the addition of a pre-training phase.\nFor pre-training the T5 model, we randomly mask 15%\nof the tokens in each instance (method) of the pre-training\ndataset. The pre-training has been performed for 200k steps\n(∼28 epochs), since we did not observe any improvement\ngoing further. We used a 2x2 TPU topology (8 cores) from\nGoogle Colab to train the model with a batch size of 256,\nwith a sequence length (for both inputs and targets) of 256\ntokens. As a learning rate, we use the Inverse Square Root\nwith the canonical conﬁguration [68]. The training requires\naround 26 seconds for 100 steps.\nFinally, we created a T5 model exploiting both pre-\ntraining and multi-task ﬁne-tuning ( i.e., a single model was\nﬁrst pre-trained, and then ﬁne-tuned on all six datasets in\nTable 1). This was done to check the impact of transfer\nlearning on the model performance. Overall, we trained\n13 T5 models: six with no pre-training and single-task ﬁne-\ntuning, six with pre-training and single-task ﬁne-tuning, and\none with pre-training and multi-task ﬁne-tuning.\n3.2 Analysis of Results\nTo answer RQ 1 we compute the metrics summarized in\nTable 4 by running each trained model on the test sets in\nTable 1.\nThe ﬁrst metric, Bilingual Evaluation Understudy (BLEU)-n\nscore, assesses the quality of automatically translated text\n[25]. The BLEU score computes the weighted percentage ( i.e.,\nconsidering the number of occurrences) of words appearing\nin translated text and the reference text. We use four variants\nof BLEU, namely BLEU-1, BLEU-2, BLEU-3, and BLEU-4. A\nBLEU-n variant computes the BLEU score by considering\nthe n-grams in the generated text. Most of the previous\nwork in the SE literature adopts the BLEU-4 score [31], [43],\n[82]. However, such a variant cannot be computed when\nthe target prediction (in our case, the number of masked\ntokens) is lower than four. For this reason, we compute\nfour different versions from BLEU-1 to BLEU-4. BLEU-1 can\nbe computed for all predictions, while BLEU-n with n >1\nonly for predictions having a length ( i.e., number of tokens)\nhigher or equal than n. The BLEU score ranges between 0%\nand 100%, with 100% indicating, in our case, that the code\ngenerated for the masked tokens is identical to the reference\none.\nThe Levenshtein distance [51]. To provide a proxy measure\nof the effort needed by developers to convert a prediction\ngenerated by the model into the reference (correct) code,\nwe compute the Levenshtein distance at token-level: This\ncan be deﬁned as the minimum number of token edits\n(insertions, deletions or substitutions) needed to transform\nthe predicted code into the reference one. Since such a\nmeasure is not normalized, it is difﬁcult to interpret it in\nour context. Indeed, saying that ﬁve tokens must be changed\nto obtain the reference code tells little without knowing the\nnumber of tokens in the reference code. For this reason, we\nnormalize such a value by dividing it by the number of\ntokens in the longest sequence among the predicted and the\nreference code.\nThe percentage of perfect predictions tells us about the cases\nin which the experimented model can recommend the very\nsame sequence of tokens which were masked in the target\ncode.\nWe statistically compare the results achieved by RoBERTa\nand T5 using different statistical analyses. We assume a\nsigniﬁcance level of 95%. As explained below, we use both\ntests on proportions and non-parametric tests for numerical\nvariables; parametric tests cannot be used because all our\nresults in terms of BLEU score or Levenshtein distance\ndeviate from normality, according to the Anderson-Darling\ntest [6] ( p-values<0.001). Whenever an analysis requires\nrunning multiple test instances, we adjust p-values using\nthe Benjamini-Hochberg procedure [87].\nTo (pairwise) compare the perfect predictions of RoBERTa\nand T5, we use the McNemar’s test [61], which is a\nproportion test suitable to pairwise compare dichotomous\nresults of two different treatments. To compute the test\nresults, we create a confusion matrix counting the number\nof cases in which (i) both T5 and RoBERTa provide a perfect\nprediction, (ii) only T5 provides a perfect prediction, (iii)\nonly RoBERTa provides a perfect prediction, and (iv) neither\nT5 nor RoBERTa provides a perfect prediction. Finally, we\ncomplement the McNemar’s test with the Odds Ratio (OR)\neffect size.\nThe comparison between different datasets, aimed at\naddressing RQ1.2, is performed, again, through a proportion\ntest, but this time, being the analysis unpaired ( i.e., we\nare comparing results over two different datasets), we use\nFischer’s exact test (and related OR) on a matrix containing,\nfor different approaches and for different masking levels, the\nnumber of correct and incorrect predictions achieved on Java\nand Android.\nTo compare results of T5 and RoBERTa in terms of BLEU-\nn score and Levenshtein distance, we use the Wilcoxon\nsigned-rank test [85] and the paired Cliff’s delta [30] effect\nsize. Similarly, the comparison between datasets in terms of\nBLUE-n score and Levenshtein distance, being unpaired, is\nperformed using the Wilcoxon rank-sum test [85] and the\nunpaired Cliff’s delta effect size.\nFor the T5, we also statistically compare the performance\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 8\nTABLE 4\nSummary of the evaluation metrics used in our study\nMetric Purpose\nBLEU score Overall prediction quality for different prediction lengths\nLevenshtein distance Proxy of the effort needed to adapt the prediction\n% of perfect predictions To what extent is the approach able to generate predictions that need human intervention\nachieved (i) with/without pre-training, and (ii) with/without\ntransfer learning. Also in this case, McNemar’s test is used\nto compare perfect predictions.\nFinally, we take the best performing model ( i.e., T5\nwith pre-training and multi-task ﬁne-tuning) and we check\nwhether the conﬁdence of the predictions can be used as a\nreliable proxy for the “quality” of the predictions. If this\nis the case, this means that in a recommender system built\naround the trained model, the developer could decide to\nreceive recommendations only when their conﬁdence is\nhigher than a speciﬁc threshold. T5 returns a score for each\nprediction, ranging from minus inﬁnity to 0. This score is the\nlog-likelihood (ln) of the prediction. Thus, if it is 0, it means\nthat the likelihood of the prediction is 1 ( i.e., the maximum\nconﬁdence, since ln(1) = 0), while when it goes towards\nminus inﬁnity, the conﬁdence tends to 0.\nWe split the predictions performed by the model into ten\nintervals, based on their conﬁdence cgoing from 0.0 to 1.0 at\nsteps of 0.1 (i.e., ﬁrst interval includes all predictions having\na conﬁdence c with 0 ≤c < 0.1, last interval has 0.9 ≤c).\nThen, we report for each interval the percentage of perfect\npredictions.\nTo corroborate our results with a statistical analysis, we\nreport the OR obtained by building a logistic regression\nmodel relating the conﬁdence (independent variable) with\nthe extent to which the prediction achieved a perfect predic-\ntion (dependent variable). Given the independent variable\nestimate βi in the logistic regression model, the OR is given\nby eβi , and it indicates the odds increase corresponding to a\nunit increase of the independent variable. We also determine\nthe extent to which the conﬁdence reported by the model\ncorrelates with the number of masked tokens. To this extent,\nwe use the Kendall’s correlation [48], which does not suffer\nfrom the presence of ties (occurring in our dataset) as other\nnon-parametric correlations.\nTo address RQ 3, for all the datasets, we compare the\nperformance of the DL-based models with that of an n-\ngram model. In particular, we perform a ﬁrst large-scale\ncomparison using a standard n-gram language model and,\non a smaller dataset, we also compare the experimented\ntechniques with the state-of-the-art cached n-gram model [36]\nusing the implementation made available by the authors\n[3]. We detail later why the cached n-gram model was too\nexpensive to run on the entire dataset.\nWe tried to design a fair comparison, although the n-\ngram model is designed to predict a single token given\nthe ntokens preceding it. Thus, in a scenario in which we\nmask more than one token, we use the n-gram model in\nthe following way: We run it to predict each masked token\nin isolation. Then, we join all predictions to generate the\nﬁnal string ( i.e., the set of previously masked tokens). The\nn-gram models are trained on the same training sets used for\nthe ﬁne-tuning of the DL-based models without, however,\nmasked tokens. We compare the three approaches in terms\nof perfect predictions generated on the test sets. A statistical\ncomparison is performed using the McNemar’s test [61] and\nORs.\n4 R ESULTS DISCUSSION\nWe start by contrasting the performances of T5 and RoBERTa\n(Section 4.1). Then, we show how the n-gram model com-\npares with the DL-based ones (Section 4.2). Finally, Section 4.3\npresents qualitative examples of correct predictions made\nby the models and discusses the semantic equivalence of\nnon-perfect predictions.\nNote that, upon interpreting the achieved results, and\nespecially those concerning the perfect (correct) predictions,\na direct comparison with the results achieved in previous\nworks on code completion is not possible. This is because\nmost of the studies in the literature experiment with code\ncompletion models when predicting a single next token the\ndeveloper is likely to write. As we will show, in such a\nspeciﬁc scenario the models we experiment with can achieve\nextremely high accuracy ( > 95% of correct predictions).\nHowever, their performance strongly decreases when pre-\ndicting longer sequences composed of multiple tokens or\neven multiple statements.\n4.1 DL-based models performance comparison (RQ1)\nFig. 1 depicts the results achieved by DL-based models in\nterms of perfect predictions for different masking approaches,\nnamely (from left to right) token-masking, construct-masking,\nand block-masking. The plots show the percentage of perfect\npredictions (yaxis) by the number of masked tokens ( xaxis).\nFor example, in thetoken masking scenario we randomly mask,\nfor each source code line lhaving more than one token, its\nlast x tokens, where x is a random number between 1 ...\nn−1, with n being the number of tokens of l, and x is\ncapped to a maximum of 10. The results achieved by the\nT5 are reported in orange while those for RoBERTa in red;\ncontinuous lines represent the results achieved on the Java\ndataset, while the dashed lines are used for the Android\ndataset.\nThe left-side graph in Fig. 1 shows the percentage of\nperfect predictions when we only mask the last token ( i.e.,\none masked token), the last two tokens, etc.. The scale on\nthe xaxis is different when dealing with the block masking\nscenario since here we mask entire blocks thus having, in\nsome cases, dozens of masked tokens. Each point indicates\nthat between x−5 and xtokens were masked, e.g., for the\nﬁrst data point at most 5 tokens were masked, for the second\nbetween 5 and 10, etc..\nTable 5 reports the average BLEU score in the four\nconsidered variants and the average normalized Levenshtein\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 9\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n2 3 4 5 6 7 8 9\nmasked tokens\n101\nperfect predictions\nT5 Java\nRoBERTa Java\nT5 Android\nRoBERTa Android\nConstruct Masking\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n2 3 4 5 6 7 8 9 101\nperfect predictions\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n10 15 20 25 30 35 40 45 505\nperfect predictions\nT5 Java\nRoBERTa Java\nT5 Android\nRoBERTa Android\nT5 Java\nRoBERTa Java\nT5 Android\nRoBERTa Android\nToken Masking Block Masking\nmasked tokens masked tokens\nFig. 1. Percentage of perfect predictions achieved by T5 and RoBERTa\nTABLE 5\nBLEU score and Levenshtein distance for T5 and RoBERTa.\nToken masking\nJava Android\nT5 RoBERTa T5 RoBERTa\nBLEU-1 0.83 0.60 0.85 0.73\nBLEU-2 0.73 0.43 0.76 0.61\nBLEU-3 0.60 0.23 0.64 0.44\nBLEU-4 0.47 0.10 0.51 0.28\nLevenshtein 0.16 0.35 0.14 0.24\nConstruct masking\nJava Android\nT5 RoBERTa T5 RoBERTa\nBLEU-1 0.68 0.51 0.68 0.57\nBLEU-2 0.55 0.34 0.57 0.43\nBLEU-3 0.48 0.24 0.49 0.33\nBLEU-4 0.37 0.14 0.43 0.26\nLevenshtein 0.32 0.48 0.32 0.41\nBlock masking\nJava Android\nT5 RoBERTa T5 RoBERTa\nBLEU-1 0.65 0.44 0.62 0.44\nBLEU-2 0.57 0.32 0.54 0.31\nBLEU-3 0.49 0.21 0.46 0.21\nBLEU-4 0.41 0.13 0.38 0.13\nLevenshtein 0.35 0.54 0.37 0.55\ndistance achieved by T5 and RoBERTa. Also in this case the\nresults are grouped based on the masking level and dataset.\nThe results in Fig. 1 and Table 5 are achieved by the\nDL-based models in the simplest scenario, i.e., single-task\nwithout pretraining. To answer RQ 1.3 we run additional\nexperiments for the best model ( i.e., T5). The results of such\nexperiments are provided in Table 6 as the percentage of\nperfect predictions for different variants of the T5 model, i.e.,\nwith/without pretraining and using single- and multi-task\nﬁne-tuning. Table 6 also reports the results achieved with\nthe RoBERTa model in the simplest scenario to simplify the\ndiscussion of the results.\n4.1.1 Impact of number of masked tokens (RQ 1.1) and\nspeciﬁcity of the dataset (RQ1.2)\nThree ﬁndings immediately emerge from the analysis of\nFig. 1: (i) as expected, the higher the number of masked\ntokens, the lower the performance of the models; (ii) the\nresults achieved on the more speciﬁc dataset ( i.e., Android,\ndashed lines in Fig. 1) are substantially better as compared to\nthe ones achieved for Java only in the token-masking scenario\nwith the RoBERTa model (see statistics in Table 9); (iii) the\nT5 model (orange lines in Fig. 1) substantially outperforms\nRoBERTa (see statistics in Table 7 and Table 8). Also, the\nperformance of RoBERTa drops more steadily as compared\nto that of T5 when the number of masked tokens increases.\nTable 7 reports results of the McNemar’s test and ORs for\nthe comparison between T5 and RoBERTa in terms of their\nability to perform perfect predictions. As it can be seen, the\n(adjusted) p-values always indicate a statistically signiﬁcant\ndifference, and the ORs indicate that T5 has between 2.94\nand 8.87 higher odds to provide a perfect prediction than\nRoBERTa.\nConcerning the comparison of BLEU scores or Leven-\nshtein distances (whose average values are reported in Ta-\nble 5) between T5 and RoBERTa, statistical results (Wilcoxon\nsigned-rank test adjusted p-values and Cliff’s d) are in\nTable 8. Also in this case, differences are always statistically\nsigniﬁcant, with varying effect sizes (generally larger for\ngreater levels of BLEU score, and for Java than Android) in\nfavor of T5 (for the Levenshtein distance a negative dis in\nfavor of T5, as it is a distance).\nToken masking. The left part of Fig. 1 shows that, as\nexpected, the lower the number of masked tokens the higher\nthe perfect predictions. Not surprisingly, the models are very\neffective when we only mask the last token in a statement.\nIndeed, in most cases, this will be a semicolon, a parenthesis,\nor a curly bracket. Thus, it is easy for the model to guess the\nlast token. When moving to more challenging scenarios like\nthe last ﬁve tokens masked in a statement, the percentage of\nperfect predictions for RoBERTa on the Java dataset drops to\nless than 10%, a major gap with the T5 model that keeps a\npercentage of perfect predictions higher than 40%. As for the\ndataset, both models achieve signiﬁcantly better performance\non the Android dataset (Fisher’s test p-value <0.001 and\nOR< 1), which is more speciﬁc and, thus, more subject to\nregularities in the source code. However, the gap in terms of\nperfect predictions between the Java and the Android dataset\nis much more marked for the RoBERTa model (e.g., ∼20% at\nx= 5against a ∼6% for the T5).\nLooking at Table 5, the BLEU scores and the Levenshtein\ndistance conﬁrm what was observed for perfect predictions:\nperformances for the Android dataset are better than for\nthe Java one. According to Wilcoxon rank-sum test, all\ndifferences, except for RoBERTa at Block level, are statistically\nsigniﬁcant, yet with a negligible/small Cliff’s d (detailed\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 10\nTABLE 6\nPerfect predictions of T5 models with different ﬁne-tuning strategies, and RoBERTa model\nDataset and Masking Level\nT5 RoBERTa\nWith Pretraining No Pretraining No Pretraining\nSingle-task Multi-task Single-task Single-task\nJava\nToken 62.9% 66.3% 61.0% 38.9%\nConstruct 51.2% 53.0% 48.4% 33.4%\nBlock 27.2% 28.8% 22.9% 8.7%\nAndroid\nToken 64.8% 69.3% 63.8% 51.8%\nConstruct 49.3% 50.8% 46.8% 37.4%\nBlock 27.5% 29.7% 22.8% 9.4%\nOverall 56.2% 59.3% 54.1% 38.7%\nTABLE 7\nPerfect prediction: Mcnamar’s test comparison between T5 and\nRoBERTa\nDataset Masking p-value OR\nJava\nToken <0.001 8.87\nConstruct <0.001 4.69\nBlock <0.001 8.14\nAndroid\nToken <0.001 4.47\nConstruct <0.001 2.94\nBlock <0.001 7.61\nstatistical results are in the online appendix).\nConstruct masking. In this scenario (see central sub-\ngraph in Fig. 1), T5 and RoBERTa achieve respectively above\n65% and 55% of perfect predictions when a single token is\nmasked for both datasets. Note that, in this scenario, also a\nsingle-token prediction is not trivial since we are in a context\nin which such a single token represents (i) the complete\ncondition of an if statement or a while/for loop, or (ii)\nthe parameters in a method call, or (iii) the exception caught\nin a catch statement. When the prediction is represented by\na single token, it is usually related to a Boolean used in an\nif condition (e.g.,if(true), if(valid), etc.) or the single\nparameter needed for a method invocation.\nAlso in this case, a higher number of masked tokens\nimplies lower performance, and again the T5 outperforms\nRoBERTa signiﬁcantly for both datasets although the gap is\nsmaller. Finally, as shown in Table 9, while with RoBERTa\nresults for Android are better, for T5 we achieve an OR ≃1.\nIn terms of BLEU score and Levenshtein distance, the\nachieved values are worse as compared to the token-level\nmasking, conﬁrming the more challenging prediction sce-\nnario represented by the construct-level masking. On average,\nthe developer may need to modify ∼40% and ∼30% of\nthe predicted tokens to obtain the reference code (small\nvariations are observed between Java and Android) when\nusing RoBERTa and T5, respectively.\nBlock masking. This represents the most challenging\nprediction scenario: The masked part can involve an entire\nstatement or even span over two statements (maximum\nboundary we set). The performance of T5 and RoBERTa\nin terms of perfect predictions are respectively above 50%\nand 35% when dealing with small masked blocks, up to ﬁve\ntokens. These blocks are mostly related toreturn statements\nrepresenting a code block (e.g., the value to return when an\nif condition is satisﬁed), such as { return false; },\n{ return null; }, etc.\nFor longer blocks, the performance substantially drops.\nprotected void fireScriptEnded(String plugin, Hook hook, Script script) \n{ Object[] listeners = _listeners.getListenerList(); \nfor (int i = listeners.length-2; i>=0; i-=2) <MASK> }\n{ if (listeners[i]==ScriptListener.class) \n{ ((ScriptListener)listeners[i+1]).scriptEnded(plugin, hook, script); } }\nFig. 2. Perfect prediction of 36 tokens generated by T5 in the Android\ndataset\nWhen considering blocks having between six and ten masked\ntokens, RoBERTa is able to generate a correct prediction in\n∼5% of cases, as compared to the ∼25% achieved by the T5.\nThe largest masked block reporting a perfect prediction for\nthe T5 model is composed of 36 and 39 tokens for Android\n(see Fig. 2) and Java datasets respectively, compared to the\n13 and 15 tokens achieved with the RoBERTa model.\nAt this level (see Table 9), the difference in terms of\nperformance between Java and Android is not so evident,\nand even insigniﬁcant for T5.\nAs expected, the BLEU scores are the lowest in this\nscenario (Table 5), and the developer may need to revise,\non average, ∼ 50% and ∼ 35% of the predicted tokens,\nindependently from the dataset of interest, when using\nRoBERTa and T5, respectively.\nAnswer to RQ1.1: As the number of masked tokens increases,\nthe DL-based models have a harder time generating correct\npredictions. Still, the performance achieved by the T5 model looks\npromising and, as we will discuss later, can be further pushed\nthrough proper pretraining and multi-task ﬁne-tuning.\nAnswer to RQ1.2: When looking at the best model ( i.e., the\nT5), its performance on the two datasets is quite similar, with no\nmajor differences observed. A strong difference in performance is\nonly observed in the token-masking scenario with the RoBERTa\nmodel.\n4.1.2 Impact of pre-training and transfer learning (RQ 2)\nAs explained in Section 3.1.2, we trained seven additional\nT5 models to assess the impact of pretraining and transfer\nlearning on its performance. First, we added to the six models\nfor which we previously discussed the T5 performance ( i.e.,\nno pretraining, single-task) the pretraining phase (obtaining a\npre-trained model in the single-task scenario, i.e., no transfer\nlearning). Then, we take the pre-trained model, and ﬁne-\ntuned it in a multi-task setting, investigating the impact of\ntransfer learning.\nTable 6 shows the achieved results also reporting the\nperformance of the previously discussed T5 and RoBERTa\nmodels (i.e., no pretraining, single-task in Table 6). Results\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 11\nTABLE 8\nBLEU score and Levensthein distance comparison between T5 and RoBERTa: Wilcoxon signed-rank and Cliff’s delta (N: negligible, S: small, M:\nmedium, L: large)\nDataset Masking BLEU 1 BLEU 2 BLEU 3 BLEU 4 Levenshtein\np-value d p -value d p -value d p -value d p value d\nJava\nToken <0.001 0.33 (S) <0.001 0.41 (M) <0.001 0.51 (L) <0.001 0.62 (L) <0.001 -0.32 (S)\nConstruct <0.001 0.22 (S) <0.001 0.30 (S) <0.001 0.32 (S) <0.001 0.35 (M) <0.001 -0.21 (S)\nBlock <0.001 0.39 (M) <0.001 0.43 (M) <0.001 0.47 (M) <0.001 0.49 (L) <0.001 -0.38 (M)\nAndroid\nToken <0.001 0.17 (S) <0.001 0.21 (S) <0.001 0.27 (S) <0.001 0.34 (M) <0.001 -0.17 (S)\nConstruct <0.001 0.14 (N) <0.001 0.20 (S) <0.001 0.22 (S) <0.001 0.27 (S) <0.001 -0.14 (N)\nBlock <0.001 0.33 (M) <0.001 0.39 (M) <0.001 0.42 (M) <0.001 0.44 (M) <0.001 -0.34 (M)\nTABLE 9\nComparison between different datasets for perfect predictions - results of\nFisher’s exact test (OR<1 indicate better performances for Android)\nMasking Method p-value OR\nToken T5 <0.001 0.89\nRoBERTa <0.001 0.59\nConstruct T5 <0.001 1.07\nRoBERTa <0.001 0.84\nBlock T5 0.67 1.01\nRoBERTa 0.01 0.93\nTABLE 10\nEffect of different pretraining levels for T5: McNemar’s test results. None\nindicates the T5 model with no pre-training and single-task ﬁnetuning.\nSingle and Multi indicates the pre-trained model with single- and\nmulti-task ﬁne-tuning, respectively.\nDataset Masking Comparison p-value OR\nJava\nToken\nsingle vs. none <0.001 1.44\nmulti vs. single <0.001 1.81\nmulti vs none <0.001 2.33\nConstruct\nsingle vs. none <0.001 1.61\nmulti vs. single <0.001 1.34\nmulti vs none <0.001 1.92\nBlock\nsingle vs. none <0.001 2.19\nmulti vs. single <0.001 1.32\nmulti vs none <0.001 2.32\nAndroid\nToken\nsingle vs. none <0.001 1.23\nmulti vs. single <0.001 2.27\nmulti vs none <0.001 2.61\nConstruct\nsingle vs. none <0.001 1.58\nmulti vs. single <0.001 1.28\nmulti vs none <0.001 1.81\nBlock\nsingle vs. none <0.001 2.14\nmulti vs. single <0.001 1.39\nmulti vs none <0.001 2.39\nof a statistical comparison made using McNemar’s test are\nreported in Table 10. As it is shown, the pretraining has a\npositive (OR>1) and statistically signiﬁcant effect in all cases,\nand the ﬁne-tuning in a multi-task setting outperforms the\nsingle-task pretraining. Looking at Table 6, the pretraining\nhad a positive impact on the accuracy of T5, boosting the\npercentage of perfect predictions from 1% to 4.7%, depending\non the test dataset. The beneﬁt of pretraining is more evident\nin the most challenging block-level scenario ( ∼5%). Overall,\nwhen considering all test datasets as a whole, the percentage\nof perfect predictions increases from 54.1% to 56.2% (+2.1%).\nBy training a single model on the six training datasets, the\npercentage of perfect predictions further increases, going up\nto an overall 59.3%. Note that improvements can be observed\non all test datasets and, for the token-masking scenario, they\ncan reach ∼5%.\nThe performance improvement is also conﬁrmed by the\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nConﬁdence\n10.1\nperfect predictions\n% perfect predictions in the conﬁdence interval\n% perfect predictions out of the total\nFig. 3. Perfect predictions by the conﬁdence of the model\nresults achieved in terms of BLEU score and the Levenshtein\ndistance that, for the sake of brevity, we report in our\nreplication package [21].\nAnswer to RQ2: We found both pretraining and multi-task\nﬁne-tuning to have a positive impact on the T5 performance.\nOverall, such an improvement accounts for +5.2% in terms of\nperfect predictions (36,009 additional instances correctly predicted).\n4.1.3 T5 Conﬁdence Level\nThe T5 returns a score for each prediction, ranging from\nminus inﬁnity to 0. This score is the log-likelihood of the\nprediction itself. If the score is -2 then it means that the log-\nlikelihood of the prediction is -2. Hence, the likelihood is 0.14\n(ln(x) =−2 =⇒x= 0.14) and this implies that the model\nhas a conﬁdence of 14% for the prediction to be correct. If\nthe score is 0, repeating the same computation as above, the\nmodel has the conﬁdence of 100% about the prediction itself.\nFig. 3 reports the relationship between the percentage of\nperfect predictions and the conﬁdence of the model. The\norange line shows the percentage of perfect predictions\nwithin each conﬁdence interval ( e.g., 90% of predictions\nhaving a conﬁdence higher than 0.9 are correct), while the\nred line reports the percentage of perfect predictions that are\ndue to predictions in that conﬁdence interval out of the total\n(e.g., 78% of all perfect predictions have a conﬁdence higher\nthan 0.9).\nFig. 3 shows a strong relationship between the conﬁdence\nof the model and the correctness of the prediction. While this\nresult might look minor, it has an important implication: It\nwould be possible to build a reliable code completion tool\naround the T5 model. Indeed, the tool could be conﬁgured\nto only trigger recommendations when the conﬁdence of the\nprediction is higher than a given threshold ( e.g., 0.9). This\nwould result in an extremely high precision.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 12\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nConﬁdence\n10.1\nNumber of tokens\nWrong predictions\nOverall prediction\nPerfect predictions\nFig. 4. Average length (in tokens) of the predictions by conﬁdence\nFrom a statistical perspective, a logistic regression model\ncorrelating the conﬁdence level and the perfect prediction\noutcome indicates a statistically signiﬁcant ( p-value <0.001)\ncorrelation, and an estimate of 6.58, which means 720 higher\nodds of a perfect prediction for each unit increase of the\nconﬁdence, i.e., 72 higher odds of a perfect prediction for\na 0.1 increase of the conﬁdence, i.e., a tick on the x-axis of\nFig. 3.\nFig. 4 analyzes the average length, in tokens, of the perfect\npredictions (yellow line), wrong predictions (orange line),\nand for all the predictions (red line) among all conﬁdence\nintervals. It is clear that the length of the prediction is related\nto the conﬁdence, since the model has higher conﬁdence for\nshorter predictions. Indeed, the average number of tokens\nin perfect predictions for the highest conﬁdence interval ( i.e.,\n3 tokens) is much lower than the average number of tokens\nin perfect predictions for the lowest conﬁdence interval ( i.e.,\n6 tokens). This conﬁrms previous ﬁndings showing that the\nmodel is more likely to correctly predict shorter statements.\nFrom a statistical perspective, this is conﬁrmed by a sig-\nniﬁcant (p-value <0.001), negative, and moderate Kendall’s\ncorrelation (τ=-0.36).\nTABLE 11\nPerfect predictions of the three models\nDataset and Masking Level T5 RoBERTa n-gram\nJava\nToken 61.0% 38.9% 30.4%\nConstruct 48.8% 33.9% 12.5%\nBlock 22.9% 8.7% 4.6%\nAndroid\nToken 63.8% 51.9% 35.4%\nConstruct 47.1% 37.8% 17.6%\nBlock 22.8% 9.4% 6.6%\nOverall 54.3% 38.8 24.9%\n4.2 Comparison with an n-gram Model\nWe answer RQ3 by comparing the DL-based models without\npretraining and in the single-task setting to then-gram model.\nWe opted for this comparison for the sake of fairness, since\nin this way the n-gram model has been trained on exactly\nthe same dataset as the two DL-based models.\nTable 11 reports the comparison in terms of perfect\npredictions between T5, RoBERTa and the n-gram model in\ndifferent evaluation scenarios, as well as the overall results.\nTABLE 12\nComparison with the n-grams model: results of McNemar’s test\nDataset Masking Comparison p-value OR\nJava\nToken\nT5 vs. RoBERTa <0.001 8.93\nRoBERTa vs. n-grams <0.001 2.21\nT5 vs. n-grams <0.001 10.31\nConstruct\nT5 vs. RoBERTa <0.001 4.65\nRoBERTa vs. n-grams <0.001 5.29\nT5 vs. n-grams <0.001 11.62\nBlock\nT5 vs. RoBERTa <0.001 8.15\nRoBERTa vs. n-grams <0.001 2.85\nT5 vs. n-grams <0.001 14.38\nAndroid\nToken\nT5 vs. RoBERTa <0.001 4.47\nRoBERTa vs. n-grams <0.001 4.26\nT5 vs. n-grams <0.001 10.14\nConstruct\nT5 vs. RoBERTa <0.001 2.91\nRoBERTa vs. n-grams <0.001 5.30\nT5 vs. n-grams <0.001 9.04\nBlock\nT5 vs. RoBERTa <0.001 7.62\nRoBERTa vs. n-grams <0.001 1.90\nT5 vs. n-grams <0.001 10.00\nFor example, T5 produced 61% perfect predictions on the\nJava dataset when using token masking. Results of statistical\ntests (McNemar’s test) are in Table 12.\nOne important clariﬁcation is needed to properly inter-\npret the results of Table 11. Since the n-gram model uses\na different script to tokenize the code, we excluded from\nthe test sets cases in which the tokens to predict ( i.e., the\nmasked ones) are tokenized differently between the DL-\nbased approaches and n-gram one ( e.g., one identiﬁes 4\ntokens and the other one 5). This resulted in the exclusions of\na few hundred instances from each test set and explains the\nslightly different performances reported for T5 and RoBERTa\nbetween Table 11 and Fig. 1.\nTable 12 reports results of the statistical comparison\namong the three models, using McNemar’s test. DL-based\nmodels achieve better performance in all experimented\ndatasets, and McNemar’s tests always indicate statistically\nsigniﬁcant differences, with ORs ranging between 1.90\n(RoBERTa vs n-grams, block masking for Android) and 14.38\n(block masking, T5 vs n-grams for Java).\nIn the token masking scenario, the performance of the\nn-gram model is very competitive when compared with\nRoBERTa, while the T5 performs substantially better. When\nmasking speciﬁc constructs, the gap in performance becomes\nstronger (see Table 11) with a substantial gap, especially\nbetween T5 and n-gram. Finally, in the block masking\nexperiment, RoBERTa and n-gram techniques struggle to\nobtain a high percentage of perfect predictions, with the\nT5 performing better achieving more than twice the num-\nber of perfect predictions as compared to the competitive\ntechniques.\nWhile the DL-based models showed superior perfor-\nmance, there are two important aspects to consider. First, the\nn-gram model allows for faster training. We estimate four to\nﬁve times less training time needed for the n-gram model as\ncompared to the DL-based models. We do not report precise\ndata since such a study would require executing the training\nmany times on the same machine, and such an analysis\nis out of the scope of this work. Once trained all models\ncan generate predictions in fractions of a second. Second,\nthe comparison presented as of now concerns the standard\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 13\nn-gram model. However, we also experimented with the\ncached n-gram model [36], which can leverage information\nabout other code components coming from the same project\n(e.g., same ﬁle or package [36]) of the method in which the\nprediction is performed. This is one of the advantages of\nthe cache model [36] and, in a real scenario, it should be\npossible to use this information assuming that the method\non which the prediction is performed is not the ﬁrst one\nwritten in the whole system. However, such experimentation\nis quite expensive to perform since it requires the cloning\nof the whole repositories hosting every test method. This is\nwhy it has only been performed on a small sample of our\ndataset.\nTABLE 13\nPerfect predictions of n-gram model when providing the cloned\nrepository (WC) vs. when not providing (NC). In comparison to DL-based\nmodels (200 methods)\nDataset and Masking Level T5 RoBERTa n-gram\nNC WC\nJava\nToken 65.5% 42.2% 32.5% 43.9%\nConstruct 56.0% 38.0% 14.5% 20.5%\nBlock 25.8% 8.5% 5.2% 8.5%\nAndroid\nToken 69.9% 50.9% 35% 42.2%\nConstruct 52.8% 37.8% 13.9% 22.0%\nBlock 33.6% 13.0% 9% 11.9%\nOverall 57.7% 38.2% 23.9% 31.5%\nFor a given method mt in the test set, we clone its\nrepository and check if the source code of mt in the latest\nsystem snapshot is exactly the same as in the test set. If this\nis the case, we run the prediction on mt providing the cloned\nrepository as a test folder, in such a way that it is leveraged\nby the cache model (this is done through the implementation\nof Hellendoorn et al. [36]). If the method changed, we discard\nit and move to the next one. Since such a process is very\nexpensive, we collected 200 methods from each test set, and\nwe compare the performance of the n-gram model when\nsuch additional information is provided (and not) on these\ninstances.\nTable 13 reports the achieved results. As expected, the\nperformance of the n-gram model increase thanks to the\nuse of the information in the test project. On these same\ninstances, the performance of T5 and RoBERTa models are\nalways superior but in the case of Java token and block\nmasking for RoBERTa.\nAnswer to RQ3: The n-gram model is a competitive alterna-\ntive to RoBERTa, while the T5 conﬁrms its superior performance.\nIt is worth highlighting the much cheaper cost of training (and\npossibly re-training several times) an n-gram model as compared\nto a DL-based approach.\n4.3 Qualitative Results\nTo give a better idea to the reader about the capabilities of\nthe experimented models in supporting code completion,\nwe report in Fig. 5 examples of correct predictions for\nthe T5 model in different scenarios/datasets. Examples of\npredictions for the RoBERTa and n-gram model are available\nin the replication package [21].\nGiven the achieved results showing the superiority of\nthe T5 model, we had a better look at a sample of the\nAndroid Token\npublic SongViewHolder(View itemView) { super(itemView); albumSongNameTextView = \n(TextView) itemView <MASK> }\n.findViewById(R.id.albumSongNameTextView);\nJava Token\npublic Sheet getSheet() { if (sheet != null) { return sheet; } UIComponent \nparent = getParent(); while (parent != <MASK> parent = parent.getParent(); } \nreturn (Sheet) parent; }\nnull && !(parent instanceof Sheet)) {\nAndroid Construct\npublic static int i(String tag, String msg) { _log( <MASK>); return \nLog.i(tag, msg); }\n_prioToLevel(Log.INFO), tag, msg\nJava Construct\npublic DoubleToLongFunction mask(ThrowingDoubleToLongFunction<? extends X> \nfunction) { Objects.requireNonNull(function); return d -> \nmaskException( <MASK>); }\n() -> function.applyAsLong(d)\nAndroid Block\npublic void setAnchor(float anchorU, float anchorV) { if (marker != null) <MASK> else \n{ markerOptions.anchor(anchorU, anchorV); } }\n{ marker.setAnchor(anchorU, anchorV); }\nJava Block\nprivate void calculateMean() { double sum = 0; Integer count = 0; for(int i=0; \ni<data.length; i++) { if (calibrationFlag[i]) <MASK> } mean = sum / \ncount.doubleValue(); }\n{ sum += data[i]; count++; }\nFig. 5. Examples of perfect predictions generated by T5\nwrong predictions it generates, to see whether some of them\nare semantically correct ( e.g., return 0x0; is equivalent to\nreturn 0; ) despite being different from the reference code\nwritten by the developers. The ﬁrst author looked at 200\nwrong predictions generated within the highest conﬁdence\ninterval, ﬁnding that only in three cases the prediction was\nsemantically equivalent, with the reference code including\nextra (unnecessary) brackets not generated by the T5 model\n(e.g., T5 predicts entry; instead of (entry);). Overall, it appeared\nthat several of the generated predictions, while wrong, might\nstill speed up the implementation process, for example when\nn−1 out of the nparameters needed for a method invocation\nare correctly predicted. Clearly, only a user study with\ndevelopers can help in assessing the actual usefulness of\nthese predictions during real coding activities.\nSince we found cases in which the perfect predictions of\nthe T5 spanned across dozens of tokens, being almost unreal-\nistic, we checked whether the 21 perfect predictions having\nmore than 30 tokens were already present in the training\nset. Indeed, while we ensure that there are no duplicated\nmethods between training and test, it is possible that two\ndifferent methods m1 and m2 have the same masked part\n(i.e., the two methods are different in the non-masked part but\nthey have the same set of masked tokens). Only one out of\nthe 21 inspected cases was already present in the training set\nand related to the transpose of a matrix. The model was able\nto correctly predict very complex masked parts such as \"{ if\n(defaultProviders != null && index < defaultProviders.length) {\nreturn defaultProviders[index].getRebuild(defaultProviders, index\n+ 1); } } \".\nFinally, it is worth commenting on the possible reasons\nbehind the superior performance we observed for the T5\nas compared to RoBERTa and for the DL-based models as\ncompared to the n-gram model. RoBERTa predicts all of\nthe masked tokens at the same time, whereas T5 predicts\nthem one by one. This means that RoBERTa cannot use the\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 14\npreviously generated tokens to predict the next one, while\nthe T5 exploits this additional information. Concerning the\nsuperior performance of the DL-based model as compared\nto the n-grams, this most likely comes down to the context\nwindow it is able to see. Indeed, the n-gram model can only\nsee (and leverage) a few tokens when predicting the next\none, while both T5 and RoBERTa have a better view of the\ncoding context, seeing all the tokens surrounding the masked\nones (which could be hundreds). A solution could be to scale\nup the n-gram model which, however, would become too\ndemanding in terms of computational cost.\n5 T HREATS TO VALIDITY\nThreats to construct validity concern the relationship between\ntheory and observation. One threat, also discussed by Hellen-\ndoorn et al. [37], is related to how we simulate the extent to\nwhich code completion intervenes during development, i.e.,\nby masking source code elements. As explained in Section 2.1,\nwe consider different masking levels, not only to evaluate\nthe amount of code completion that can be predicted but also\nto simulate different ways a developer writes source code,\nespecially because we cannot assume this is done sequentially.\nHowever, we are aware that the considered masking levels\ncover a limited number of cases that may not completely\nreﬂect how developers write code.\nAnother threat is related to how we assess the code\ncompletion performances. On the one hand, 100% BLEU\nscore clearly reﬂects a perfect prediction. However, the BLEU\nscore may be sufﬁcient to assess the performance of code-\nrelated tasks [71] and, in general, it is difﬁcult to evaluate the\nusefulness of semantic equivalent predictions or imperfect\nyet useful. To mitigate this threat, we report some qualitative\nexamples, indicating how partially-complete recommenda-\ntions could still be useful.\nThreats to internal validity concern factors, internal to\nour study, that could inﬂuence its results. To this extent,\nan important factor that inﬂuences DL performance is the\ncalibration of hyperparameters, which has been performed\nas detailed in Section 3.2. We are aware that due to feasibility\nreasons we only performed a limited calibration of the\nhyperparameters. Hence, it is possible that a more detailed\ncalibration would produce better performances. Also, note\nthat we did not experiment with a pre-trained version of\nRoBERTa. Indeed, to simplify our experimental design and\nreduce the training cost we decided to only pre-train the\nbest-performing model (i.e., T5).\nWhen building the pre-training dataset we capped to\n1,500 the maximum number of instances that a single project\ncan contribute to our dataset. This has been done to avoid a\nhandful of projects strongly inﬂuencing the training of the\nmodel. We acknowledge that different (and maybe better)\nresults could be obtained by considering the whole code base\nof each project for pre-training.\nThreats to conclusion validity concern the relationship\nbetween evaluation and outcome. As explained in Section 3.2\nwe used appropriate statistical procedures, also adopting\np-value adjustment when multiple tests were used within\nthe same analysis.\nThreats to external validity are related to the generaliz-\nability of our ﬁndings. On the one hand, we have evaluated\nthe performances of the models on two large datasets. At\nthe same time, we do not know whether the obtained\nresults generalize to different domains than Android, and\nother programming languages than Java. A further threat is\nthat our study is limited to the RoBERTa and T5 models\nfor DL and, as a baseline for n-gram models, the one\nby Hellendoorn and Devanbu [36]. While we claim such\nmodels are well-representative of the current state-of-the-\nart, it would be desirable to investigate how alternative\napproaches would work for the different evaluation scenarios.\nAlso, when building our ﬁne-tuning dataset, we started\nfrom the CodeSearchNet Java Dataset provided by Husain\net al. [41]. In this dataset, short methods (those having less\nthan three lines), as well as methods containing test in their\nname have been excluded. This means that the results of our\nstudy do not generalize, for example, to very short methods\nimplementing critical tasks in less than three lines of code.\n6 R ELATED WORK\nWe start by detailing the literature related to code com-\npletion techniques and, more speciﬁcally, we highlight the\napproaches aimed at (partially) automating code writing.\nThen, we present studies investigating the effectiveness of\ncode completion techniques. For the sake of brevity, we do\nnot discuss recently proposed techniques for automating\nbug-ﬁxing [14], [20], [80], modeling activities [56], learning\ncode changes [17], [79], as well as source code search engines\nthat can be used to identify pieces of code for reuse [15], [29],\n[60], [70], [75], [76].\n6.1 Code Completion Approaches\nThe Prospector tool by Mandelin et al. [57] is one of the ﬁrst\ntechniques aimed at supporting code completion by suggest-\ning within the IDE variables or method calls from the user’s\ncode base. Prospector was then followed by improvements\nsuch as the InSynth tool by Gvero et al. [32] which, given a\ntype expected at a given point in the source code, searches\nfor type-compatible expressions. Other approaches focus on\nspeciﬁc elements of API usage completion. The work from\nZhang et al. [88] aims at recommending parameter usages,\nachieving 64% of useful recommendations and 53% of perfect\nones.\nHill and Rideout [38] proposed a technique to automat-\nically complete the body of a method. Their approach can\nsupport such a completion for what the authors deﬁne as\n“atomic clones” (i.e., small units of implementation that are\nunavoidable in Java to implement speciﬁc requirements). The\npresented tool uses the K-Nearest Neighbour to identify a\nclone of a method under development. Such a clone is then\nused to recommend the completion of the method body.\nBruch et al. [18] introduced the intelligent code completion\nsystem, able to ﬁlter out from the list of candidate method\ncalls recommended by the IDE those that are more relevant to\nthe current working context. Their results show the capability\nto correctly predict up to 82% of method calls actually needed\nby developers, and up to 72% of those that are relevant to the\ncurrent development context. The approach by Bruch et al.\nhas been improved by Proksch et al. [66], by adding further\ncontextual information and by proposing a Pattern-based\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 15\nBayesian Networks approach. As a result, Proksch et al. were\nable to substantially reduce the model size while keeping\nabout the same level of prediction accuracy. Differently from\nthe aforementioned approaches, we do not restrict code\ncompletion to method calls.\nHan et al. [34] proposed a technique exploiting a Hidden\nMarkov Model (HMM) to autocomplete multiple keywords\nstarting from abbreviated inputs. This means that the user\n(i.e., the developer) only writes a few characters of the\nkeyword of interest that is then expanded by the HMM.\nThe authors show that their model can save up to 41% of\nkeystrokes.\nRobbes and Lanza [72] used information extracted from\nthe change history of software systems to support the code\ncompletion of method calls and class names. Their approach\nhas been implemented in a tool named OCompletion, and\nthe performed empirical evaluation demonstrated its ability\nto propose a correct match in the top-3 results in 75% of\ncases.\nAsaduzzaman et al. [10] proposed a technique named\nCSCC (Context Sensitive Code Completion). They collect\ncode examples from software repositories and, for each\nmethod call, represent its context as a set of methods,\nkeywords, class, and interface names appearing within four\nlines of code. This contextual information is then used to\nﬁlter out method call recommendations. The assumption\nis that similar contexts imply similar method calls. CSCC\noutperforms previous approaches, achieving 86% precision\nand 99% recall.\nHindle et al. [39] pioneered the work on statistical\nlanguage models applied to software. They conceived the\nidea of “naturalness of source code” and used n-gram models\nto create a language-agnostic algorithm that is able to predict\nthe next token in a given statement. The trained model’s\naverage entropy is between three and four bits, indicating a\nhigh degree of naturalness.\nRaychev et al. [69] approach the code completion problem\nthrough statistical language models. They extract sequences\nof method calls from a large code base, and use this dataset to\ntrain a language model able to predict API calls. Their model\nachieves a 90% accuracy in the top-3 recommendations.\nNguyen et al. [63] proposed GraPacc, a context-sensitive\ncode completion model trained on a database of API usage\npatterns. These patterns are then matched to a given code\nunder development to support code completion. GraPacc\nachieves up to 95% precision and 92% recall. A similar\napproach was later on proposed by Niu et al. [65] for API\ncompletion in Android: Given an API method as a query,\ntheir approach recommends a set of relevant API usage pat-\nterns. They report an 18% improvement of F-Measure when\ncomparing to pattern extraction using frequent-sequence\nmining.\nTu et al. [77] introduced a cache component to exploit\nthe “localness of code” in the n-gram model. Results show\nthat since the code is locally repetitive, localized information\ncan be used to improve performance. The enhanced model\noutperforms standard n-gram models by up to 45% in\naccuracy. In a related work, Franks et al. [26] implemented\nCACHECA, an Eclipse auto-completion plugin exploiting the\naforementioned cache language model [77]. In comparison to\nEclipse built-in suggestions, their tool improves the accuracy\nof top 1 and top 10 suggestions by 26% and 34%, respectively.\nNguyen et al. [64] presented GraLan, a graph-based\nstatistical language model that the authors instantiated to\nrecommend the next API element needed in a given code,\nwhere an API element is a method call together with the\ncontrol units ( e.g., if statements) needed for its usage.\nThe reported empirical evaluation showed that GraLan can\ncorrectly recommend the correct API element in 75% of cases\nwithin the ﬁrst ﬁve candidates.\nHou and Pletcher [40] evaluated three mechanisms to en-\nhance code completion techniques, namely sorting, ﬁltering,\nand grouping. Also this works focuses on code completion\nrelated to API methods and the outcome of their study\nis an assessment of the effectiveness of fourteen different\nconﬁgurations of the three mechanisms.\nAsaduzzaman et al. [11] proposed a technique to recom-\nmend developers with examples of framework extensions.\nGiven a class under development, the approach recommends\ncode examples showing how to integrate frameworks in spe-\nciﬁc extension points. While the approach by Asaduzzaman\net al. recommends relatively large code completion fragments,\nit is limited to a speciﬁc scenario, i.e., framework extension\npoints, whereas the approaches we experiment with are more\ngeneral in that respect.\nHellendoorn and Devanbu [36] proposed further im-\nprovements to the cached models aimed at considering\nspeciﬁc characteristics of code ( e.g., unlimited, nested, and\nscoped vocabulary). Then, they compare their model with\nDL-based models, showing its superiority. Also, they show\nthat the two families of techniques can be combined together,\nleading to an unprecedented 1.25 bits of entropy per token.\nKarampatsis et al. [47], a few years later, suggested instead\nthat neural networks are the best language-agnostic algo-\nrithm for code completion. They proposed to overcome the\nout-of-vocabulary problem by using Byte Pair Encoding [27]. In\naddition, the proposed neural network is able to dynamically\nadapt to different projects. Their best model outperforms\nn-gram models, achieving an entropy of 1.03 bits.\nKim et al. [49] leveraged the Transformers neural network\narchitecture for code completion. They provide the syntactic\nstructure of code to the network by using information\nfrom the Abstract Syntax Tree to fortify the self-attention\nmechanism. Among the several models they experiment\nwith, the best one reached a MRR up to 74.1% in predicting\nthe next token.\nAlon et al. [8] addressed the problem of code completion\nwith a language agnostic approach named Structural Lan-\nguage Model. It leverages the syntax to model the code snip-\npet as a tree. The model, based on LSTMs and Transformers,\nreceives an AST representing a partial expression (statement),\nwith some missing consecutive tokens to complete. Their best\nmodel reached state-of-the-art performance with an exact\nmatch accuracy for the top prediction of 18.04%.\nSvyatkovskiy et al. [73] introduced IntelliCode Compose,\na general-purpose multilingual code completion tool capable\nof predicting code sequences of arbitrary token types. They\ndo not leverage high-level structural representation, such as\nAST, and use subtokens to overcome the out-of-vocabulary\nproblem. Their model can recommend an entire statement, and\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 16\nachieves a perplexity of 1.82 for the Python programming\nlanguage.\nLiu et al. [53] presented a Transformer-based neural\narchitecture pre-trained with the goal of incorporating both\ncode understanding and generation tasks. Afterwards, the\nmodel was then ﬁne-tuned on the classic code completion\ntask (i.e., predicting the next token to write).\nA problem related to code completion has also been\ntackled by Watson et al. [82]: The authors exploit a sequence-\nto-sequence model to recommend assert statements for a\ngiven Java test case. This technique is able to generate a\nspeciﬁc type of code statement, with a top-1 accuracy of\n31%. Also, Kanade et al. [45] show how code embeddings\ncan support code-related tasks, including variable misuse and\nrepair, related to code completion when focusing on a single\ntoken.\nSvyatkovskiy et al. [74] proposed a different perspective\non neural code completion, shifting from a generative task\nto a learning-to-rank task. Their model is used to rerank the\nrecommendations provided via static analysis, being cheaper\nin terms of memory footprint than generative models. To this\naim, Avishkar et al. [16] proposed a neural language model\nfor code suggestion in Python, aiming to capture long-range\nrelationships among identiﬁers exploiting a sparse pointer\nnetwork.\nTo address the out-of-vocabulary problem in standard\nneural language models, Jian et al. [52] proposed a pointer\nmixture deep learning model for Python beneﬁting from\nthe pointer copy mechanism. Such architecture helps the\nmodel to generate an out-of-vocabulary word from local\ncontext through a pointer component when generating a\nwithin-vocabulary token is not possible.\nA considerable step forward, has been taken recently by\nAye and Kaiser [12] proposing a novel language model to\npredict the next top-k tokens while taking into consideration\nsome real-world constraints such as (i) prediction latency, (ii)\nsize of the model and its memory footprint, and (iii) validity\nof suggestions. Chen et al. [19] proposed a deep learning\nmodel for API recommendation combining structural and\ntextual code information based on an API context graph\nand code token network. The evaluation model signiﬁcantly\noutperforms the existing graph-based statistical approach\nand the tree-based deep learning approach for API recom-\nmendation.\nTo the best of our knowledge, our work is the ﬁrst\nto present a comprehensive study on the effectiveness of\nTransformer models for code completion tasks, pushing this\nproblem forward by attempting the automatic generation of an\nentire code block (e.g., the body of a for statement).\n6.2 Studies About the Effectiveness of Code Comple-\ntion Approaches\nAlthough code completion techniques are likely to be beneﬁ-\ncial for developers, their limitations ( e.g., prediction latency,\naccuracy) can bound their practical usefulness. For this\nreason, several studies investigated the effectiveness of code\ncompletion techniques.\nJin and Servant [44] investigated the effect of different\nrecommendation list lengths on the developers’ productivity.\nThey found that lengthy suggestion lists are not uncommon\nand reduce the developer’s likelihood of selecting one of the\nrecommendations.\nLin et al. [42] focus on the performance of a code2vec\n[9] model, in the context of method name recommendation.\nThe authors retrain the model on a different dataset and\nassess it in a more realistic setting where the training dataset\ndoes not contain any record from evaluation projects. The\nresults suggest that while the dataset change had little\nimpact on the model’s accuracy, the newproject-based setting\nnegatively impacted the model. Lin et al. [42] also evaluated\nthe usefulness of code2vec suggestions by asking developers\nto assess the quality of suggestions for non-trivial method\nnames. The evaluation results show the model rarely works\nwhen it is needed in practice. Further investigation also\nrevealed that around half of successful recommendations\n(48%) occur for simpler scenarios, such as setter/getter\nmethods or when the recommended name is copied from the\nmethod body source code.\nHellendoorn et al. [37] studied 15,000 real code com-\npletions from 66 developers founding that typically-used\ncode completion benchmarks — e.g., produced by artiﬁcially\nmasking tokens — may misrepresent actual code completion\ntasks. The study by Hellendoorn et al. suggests that further\nresearch is needed to assess the actual applicability of DL-\nbased code completion to the real-world. This is however\nout of scope for our work, because our aim is to assess the\ncapability of DL models to predict non-trivial portions of\ncode going beyond a single method call or parameter.\nLiu et al. [54] investigate the performance of deep learning-\nbased approaches for generating code from requirement\ntexts. For that, they assessed ﬁve state-of-the-art approaches\non a larger and more diverse dataset of pairs of software\nrequirement texts and their validated implementation as\ncompared to those used in the literature. The evaluation\nresults suggest that the performance of such approaches, in\nterms of common metrics (e.g., BLEU score), is signiﬁcantly\nworse than what was reported in the literature. The authors\nattribute this observation to the relatively small datasets on\nwhich such models are evaluated.\nSimilarly, Aye et al. [13] investigate the impact of using\nreal-world code completion examples (i.e., code completion\nacceptance events in the past) for training models instead of\nartiﬁcial examples sampled from code repositories. The usage\nof such realistic data on n-gram and transformer models\nsuggests a signiﬁcant accuracy decrease. Later, an A/B test\nconducted with Facebook developers conﬁrmed that the\nautocompletion usage increases by around 6% for models\ntrained on real-world code completion examples.\nOur work, differently from previous studies, aims at\nassessing the capability of state-of-the-art Transformer-based\nmodels in predicting non-trivial snippets of code. In contrast,\nit is out of this study scope to assess the developer’s\nperception of the prediction models that would require an\nextensive study with developers.\n7 C ONCLUSION\nWe investigated the ability of Transformer-based DL-models\nin dealing with code completion tasks having a different\nlevel of difﬁculty, going from the prediction of a few tokens\nwithin the same code statement, up to the entire code blocks\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 17\nwe masked. Among the three models we experimented with,\nnamely T5 [68], RoBERTa [24], and the cached n-gram model\n[36], the T5 resulted to be the most effective in supporting\ncode completion.\nOur study provided a series of highlights that will guide\nour future research. First, when the code to complete spans\nover multiple statements (two in the case of our experiments),\nthese models, with the training we performed, are still far\nfrom being a valuable solution for software developers.\nIndeed, even the best-performing model (T5) struggles in\nguessing entire code blocks. However, the performance we\nreported should not be seen as an “upper bound” for these\ntechniques, since larger models may be trained on more data\ncan be adopted (e.g., the recently proposed GitHub Copilot\n[1]) and different training strategies could help in achieving\nbetter results (e.g., Tufano et al. [78] showed that pre-training\non English text helps transformer models in improving\nperformance even in code-related tasks). Besides working\non these research directions we also plan to investigate\nalternative solutions mixing, for example, retrieval-based\nand DL-based solutions.\nSecond, the conﬁdence of the predictions generated by the\nT5 turned out to be a very reliable proxy for the quality of its\npredictions. This is something fundamental for building tools\naround this model, as it can be used by developers to just\nignore low-conﬁdence recommendations. Future studies will\ninvestigate how the developers perceive the usefulness of\nrecommendations having different characteristics, including\nlength, conﬁdence, and covered code constructs.\nFinally, a user study is also needed to understand what is\nthe level of accuracy (in terms of perfect predictions) needed\nto consider tools built around these models as effective for\ndevelopers. In other words, it is important to understand the\n“percentage of wrong predictions” a developer can accept\nbefore considering the tool counterproductive. Such a study\nis also part of our research agenda.\nACKNOWLEDGMENT\nThis project has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Horizon\n2020 research and innovation programme (grant agreement\nNo. 851720). W&M co-authors have been supported in part\nby the NSF CCF-1955853 and CCF-2007246 grants. Any\nopinions, ﬁndings, and conclusions expressed herein are the\nauthors’ and do not necessarily reﬂect those of the sponsors.\nREFERENCES\n[1] “Github copilot https://copilot.github.com.”\n[2] Hugging Face’s Tokenizer Repositor , https://github.com/\nhuggingface/tokenizers.\n[3] N-gram Cached Model, https://github.com/SLP-team/SLP-Core.\n[4] ScrML Website, https://www.srcml.org/.\n[5] Weights and Biases Website, https://www.wandb.com/.\n[6] Anderson–Darling Test. New York, NY: Springer New York,\n2008, pp. 12–14. [Online]. Available: https://doi.org/10.1007/\n978-0-387-32833-1_11\n[7] M. Allamanis, CodeSearchNet Deduplication Algorithm ,\nhttps://github.com/github/CodeSearchNet/blob/master/\nsrc/dataextraction/dedup_split.py.\n[8] U. Alon, R. Sadaka, O. Levy, and E. Yahav, “Structural language\nmodels of code,” in International Conference on Machine Learning .\nPMLR, 2020, pp. 245–256.\n[9] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “code2vec: Learning\ndistributed representations of code,” Proceedings of the ACM on\nProgramming Languages, vol. 3, no. POPL, pp. 1–29, 2019.\n[10] M. Asaduzzaman, C. K. Roy, K. A. Schneider, and D. Hou, “Context-\nsensitive code completion tool for better api usability,” in2014 IEEE\nInternational Conference on Software Maintenance and Evolution , 2014,\npp. 621–624.\n[11] M. Asaduzzaman, C. K. Roy, K. A. Schneider, and D. Hou,\n“Recommending framework extension examples,” in 2017 IEEE\nInternational Conference on Software Maintenance and Evolution (IC-\nSME), 2017, pp. 456–466.\n[12] G. A. Aye and G. E. Kaiser, “Sequence model design for code\ncompletion in the modern ide,” arXiv preprint arXiv:2004.05249 ,\n2020.\n[13] G. A. Aye, S. Kim, and H. Li, “Learning autocompletion from real-\nworld datasets,” in 2021 IEEE/ACM 43rd International Conference on\nSoftware Engineering: Software Engineering in Practice (ICSE-SEIP) .\nIEEE, 2021, pp. 131–139.\n[14] J. Bader, A. Scott, M. Pradel, and S. Chandra, “Getaﬁx: learning\nto ﬁx bugs automatically,” Proc. ACM Program. Lang. , vol. 3, no.\nOOPSLA, pp. 159:1–159:27, 2019.\n[15] S. Bajracharya, T. Ngo, E. Linstead, Y. Dou, P . Rigor, P . Baldi,\nand C. Lopes, “Sourcerer: A search engine for open source\ncode supporting structure-based search,” in Companion to the 21st\nACM SIGPLAN Symposium on Object-Oriented Programming Systems,\nLanguages, and Applications , ser. OOPSLA ’06. ACM, 2006, p.\n681–682.\n[16] A. Bhoopchand, T. Rocktäschel, E. Barr, and S. Riedel, “Learning\npython code suggestion with a sparse pointer network,” arXiv\npreprint arXiv:1611.08307, 2016.\n[17] S. Brody, U. Alon, and E. Yahav, “Neural edit completion,”arXiv\npreprint arXiv:2005.13209, 2020.\n[18] M. Bruch, M. Monperrus, and M. Mezini, “Learning from examples\nto improve code completion systems,” in Proceedings of the 7th Joint\nMeeting of the European Software Engineering Conference and the ACM\nSIGSOFT Symposium on The Foundations of Software Engineering , ser.\nESEC/FSE 2009, 2009, pp. 213–222.\n[19] C. Chen, X. Peng, Z. Xing, J. Sun, X. Wang, Y. Zhao, and W. Zhao,\n“Holistic combination of structural and textual code information for\ncontext based api recommendation,” IEEE Transactions on Software\nEngineering, 2021.\n[20] Z. Chen, S. J. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshy-\nvanyk, and M. Monperrus, “Sequencer: Sequence-to-sequence\nlearning for end-to-end program repair,” IEEE Transactions on\nSoftware Engineering, 2019.\n[21] M. Ciniselli, “Replication package https://github.com/mciniselli/\nT5_Replication_Package.git.”\n[22] M. Ciniselli, N. Cooper, L. Pascarella, D. Poshyvanyk, M. Di Penta,\nand G. Bavota, “An empirical study on the usage of bert models\nfor code completion,” in Proceedings of the 18th Working Conference\non Mining Software Repositories, ser. MSR ’21, 2021, p. To Appear.\n[23] O. Dabic, E. Aghajani, and G. Bavota, “Sampling projects in github\nfor MSR studies,” in 18th IEEE/ACM International Conference on\nMining Software Repositories, MSR 2021. IEEE, 2021, pp. 560–564.\n[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers). Association\nfor Computational Linguistics, Jun. 2019, pp. 4171–4186.\n[25] M. Dreyer and D. Marcu, “HyTER: Meaning-equivalent semantics\nfor translation evaluation,” in Proceedings of the 2012 Conference of the\nNorth American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies. Montréal, Canada: Association\nfor Computational Linguistics, Jun. 2012, pp. 162–171. [Online].\nAvailable: https://www.aclweb.org/anthology/N12-1017\n[26] C. Franks, Z. Tu, P . Devanbu, and V . Hellendoorn, “Cacheca:\nA cache language model based code suggestion tool,” in 2015\nIEEE/ACM 37th IEEE International Conference on Software Engineering,\nvol. 2. IEEE, 2015, pp. 705–708.\n[27] P . Gage, “A new algorithm for data compression,”C Users J., vol. 12,\nno. 2, p. 23?38, 1994.\n[28] F.-X. Geiger, I. Malavolta, L. Pascarella, F. Palomba, D. D. Nucci,\nand A. Bacchelli, “A graph-based dataset of commit history of\nreal-world android apps,” in Proceedings of the 15th International\nConference on Mining Software Repositories, MSR. ACM, May 2018.\n[Online]. Available: https://androidtimemachine.github.io\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 18\n[29] M. Grechanik, C. Fu, Q. Xie, C. McMillan, D. Poshyvanyk, and\nC. Cumby, “A search engine for ﬁnding highly relevant applica-\ntions,” in Proceedings of the 32nd ACM/IEEE International Conference\non Software Engineering - Volume 1 , ser. ICSE ’10. ACM, 2010, p.\n475–484.\n[30] R. J. Grissom and J. J. Kim, Effect sizes for research: A broad practical\napproach, 2nd ed. Lawrence Earlbaum Associates, 2005.\n[31] X. Gu, H. Zhang, D. Zhang, and S. Kim, “Deep api learning,” in\nProceedings of the 2016 24th ACM SIGSOFT International Symposium\non Foundations of Software Engineering , ser. FSE 2016. New\nYork, NY, USA: ACM, 2016, pp. 631–642. [Online]. Available:\nhttp://doi.acm.org.proxy.wm.edu/10.1145/2950290.2950334\n[32] T. Gvero, V . Kuncak, I. Kuraj, and R. Piskac, “Complete comple-\ntion using types and weights,” in ACM SIGPLAN Conference on\nProgramming Language Design and Implementation, PLDI ’13, Seattle,\nWA, USA, June 16-19, 2013, 2013, pp. 27–38.\n[33] S. Han, D. R. Wallace, and R. C. Miller, “Code completion from\nabbreviated input,” in 2009 IEEE/ACM International Conference on\nAutomated Software Engineering. IEEE, 2009, pp. 332–343.\n[34] ——, “Code completion from abbreviated input,” in 2009\nIEEE/ACM International Conference on Automated Software Engineer-\ning, 2009, pp. 332–343.\n[35] ——, “Code completion of multiple keywords from abbreviated\ninput,” Automated Software Engineering, vol. 18, no. 3-4, pp. 363–398,\n2011.\n[36] V . J. Hellendoorn and P . Devanbu, “Are deep neural networks\nthe best choice for modeling source code?” in Proceedings of the\n2017 11th Joint Meeting on Foundations of Software Engineering , ser.\nESEC/FSE 2017, 2017, p. 763?773.\n[37] V . J. Hellendoorn, S. Proksch, H. C. Gall, and A. Bacchelli, “When\ncode completion fails: a case study on real-world completions,” in\nProceedings of the 41st International Conference on Software Engineering,\nICSE 2019, Montreal, QC, Canada, May 25-31, 2019, 2019, pp. 960–970.\n[38] R. Hill and J. Rideout, “Automatic method completion,” in Proceed-\nings. 19th International Conference on Automated Software Engineering,\n2004., 2004, pp. 228–235.\n[39] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P . Devanbu, “On the\nnaturalness of software,” in Proceedings of the 34th International\nConference on Software Engineering, ser. ICSE 2012. IEEE Press, 2012,\npp. 837–847.\n[40] D. Hou and D. M. Pletcher, “An evaluation of the strategies of\nsorting, ﬁltering, and grouping api methods for code completion,”\nin 2011 27th IEEE International Conference on Software Maintenance\n(ICSM). IEEE, 2011, pp. 233–242.\n[41] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n“Codesearchnet challenge: Evaluating the state of semantic code\nsearch,” CoRR, vol. abs/1909.09436, 2019. [Online]. Available:\nhttp://arxiv.org/abs/1909.09436\n[42] L. Jiang, H. Liu, and H. Jiang, “Machine learning based recommen-\ndation of method names: how far are we,” in 2019 34th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE) .\nIEEE, 2019, pp. 602–614.\n[43] S. Jiang, A. Armaly, and C. McMillan, “Automatically generating\ncommit messages from diffs using neural machine translation,” in\n2017 32nd IEEE/ACM International Conference on Automated Software\nEngineering (ASE), ser. ASE’17, Oct. 2017, pp. 135–146, iSSN:.\n[44] X. Jin and F. Servant, “The hidden cost of code completion:\nUnderstanding the impact of the recommendation-list length on\nits efﬁciency,” in Proceedings of the 15th International Conference on\nMining Software Repositories, 2018, pp. 70–73.\n[45] A. Kanade, P . Maniatis, G. Balakrishnan, and K. Shi, “Learning and\nevaluating contextual embedding of source code,” 2020.\n[46] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes,\n“Big code != big vocabulary: Open-vocabulary models for source\ncode,” in Proceedings of the 42nd International Conference on Software\nEngineering, ICSE 2020, 2020, p. To Appear.\n[47] R. Karampatsis and C. A. Sutton, “Maybe deep neural\nnetworks are the best choice for modeling source code,”\nCoRR, vol. abs/1903.05734, 2019. [Online]. Available: http:\n//arxiv.org/abs/1903.05734\n[48] M. Kendall, “A new measure of rank correlation,” Biometrika, 1938.\n[49] S. Kim, J. Zhao, Y. Tian, and S. Chandra, “Code prediction by\nfeeding trees to transformers,” in 2021 IEEE/ACM 43rd International\nConference on Software Engineering (ICSE). IEEE, 2021, pp. 150–162.\n[50] T. Kudo and J. Richardson, “Sentencepiece: A simple and language\nindependent subword tokenizer and detokenizer for neural text\nprocessing,” CoRR, vol. abs/1808.06226, 2018.\n[51] V . Levenshtein, “Binary Codes Capable of Correcting Deletions,\nInsertions and Reversals,” Soviet Physics Doklady , vol. 10, p. 707,\n1966.\n[52] J. Li, Y. Wang, M. R. Lyu, and I. King, “Code completion with neural\nattention and pointer networks,” arXiv preprint arXiv:1711.09573,\n2017.\n[53] F. Liu, G. Li, Y. Zhao, and Z. Jin, “Multi-task learning based pre-\ntrained language model for code completion,” in Proceedings of\nthe 35th IEEE/ACM International Conference on Automated Software\nEngineering, ser. ASE 2020. Association for Computing Machinery,\n2020.\n[54] H. Liu, M. Shen, J. Zhu, N. Niu, G. Li, and L. Zhang, “Deep learning\nbased program generation from requirements text: Are we there\nyet?” IEEE Transactions on Software Engineering, 2020.\n[55] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly\noptimized BERT pretraining approach,” CoRR, vol. abs/1907.11692,\n2019. [Online]. Available: http://arxiv.org/abs/1907.11692\n[56] P . Mäder, T. Kuschke, and M. Janke, “Reactive auto-completion\nof modeling activities,” IEEE Transactions on Software Engineering,\n2019.\n[57] D. Mandelin, L. Xu, R. Bodík, and D. Kimelman, “Jungloid mining:\nhelping to navigate the API jungle,” in Proceedings of the ACM\nSIGPLAN 2005 Conference on Programming Language Design and\nImplementation, Chicago, IL, USA, June 12-15, 2005 , 2005, pp. 48–61.\n[58] P . Martins, R. Achar, and C. V . Lopes, “50k-c: A dataset of\ncompilable, and compiled, java projects,” in 2018 IEEE/ACM 15th\nInternational Conference on Mining Software Repositories (MSR) , 2018,\npp. 1–5.\n[59] A. Mastropaolo, S. Scalabrino, N. Cooper, D. N. Palacio, D. Poshy-\nvanyk, R. Oliveto, and G. Bavota, “Studying the usage of text-to-\ntext transfer transformer to support code-related tasks,” in 2021\nIEEE/ACM 43rd International Conference on Software Engineering\n(ICSE). IEEE, 2021, pp. 336–347.\n[60] C. McMillan, M. Grechanik, D. Poshyvanyk, C. Fu, and Q. Xie,\n“Exemplar: A source code search engine for ﬁnding highly relevant\napplications,” IEEE Transactions on Software Engineering , vol. 38,\nno. 5, pp. 1069–1087, 2012.\n[61] Q. McNemar, “Note on the sampling error of the difference between\ncorrelated proportions or percentages,” Psychometrika, vol. 12, no. 2,\npp. 153–157, 1947.\n[62] A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, “A large-scale\nstudy on repetitiveness, containment, and composability of routines\nin open-source projects,” in Proceedings of the IEEE/ACM 13th\nWorking Conference on Mining Software Repositories (MSR 2016), 2016,\npp. 362–373.\n[63] A. T. Nguyen, T. T. Nguyen, H. A. Nguyen, A. Tamrawi, H. V .\nNguyen, J. Al-Kofahi, and T. N. Nguyen, “Graph-based pattern-\noriented, context-sensitive source code completion,” in 2012 34th\nInternational Conference on Software Engineering (ICSE) , 2012, pp.\n69–79.\n[64] A. T. Nguyen and T. N. Nguyen, “Graph-based statistical lan-\nguage model for code,” in 2015 IEEE/ACM 37th IEEE International\nConference on Software Engineering, vol. 1. IEEE, 2015, pp. 858–868.\n[65] H. Niu, I. Keivanloo, and Y. Zou, “Api usage pattern recommenda-\ntion for software development,” Journal of Systems and Software, vol.\n129, pp. 127–139, 2017.\n[66] S. Proksch, J. Lerch, and M. Mezini, “Intelligent code completion\nwith bayesian networks,” ACM Trans. Softw. Eng. Methodol., vol. 25,\nno. 1, pp. 3:1–3:31, 2015.\n[67] S. Raemaekers, A. van Deursen, and J. Visser, “The maven reposi-\ntory dataset of metrics, changes, and dependencies,” in 2013 10th\nWorking Conference on Mining Software Repositories (MSR) , 2013, pp.\n221–224.\n[68] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer,” 2019.\n[69] V . Raychev, M. Vechev, and E. Yahav, “Code completion with statis-\ntical language models,” in Proceedings of the 35th ACM SIGPLAN\nConference on Programming Language Design and Implementation , ser.\nPLDI 2014, 2014, pp. 419–428.\n[70] S. P . Reiss, “Semantics-based code search,” inProceedings of the 31st\nInternational Conference on Software Engineering, ser. ICSE ’09. IEEE\nComputer Society, 2009, p. 243–253.\n[71] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan,\nM. Zhou, A. Blanco, and S. Ma, “Codebleu: a method for automatic\nevaluation of code synthesis,” CoRR, vol. abs/2009.10297, 2020.\n[Online]. Available: https://arxiv.org/abs/2009.10297\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 19\n[72] R. Robbes and M. Lanza, “Improving code completion with\nprogram history,” Automated Software Engineering, vol. 17, no. 2, pp.\n181–212, 2010.\n[73] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intellicode\ncompose: Code generation using transformer,” in Proceedings of the\n28th ACM Joint Meeting on European Software Engineering Conference\nand Symposium on the Foundations of Software Engineering , 2020, pp.\n1433–1443.\n[74] A. Svyatkovskiy, S. Lee, A. Hadjitoﬁ, M. Riechert, J. Franco, and\nM. Allamanis, “Fast and memory-efﬁcient neural code completion,”\n2020.\n[75] S. Thummalapenta and T. Xie, “Parseweb: A programmer assistant\nfor reusing open source code on the web,” in Proceedings of the\nTwenty-Second IEEE/ACM International Conference on Automated\nSoftware Engineering, ser. ASE ’07. Association for Computing\nMachinery, 2007, p. 204–213.\n[76] ——, “Spotweb: Detecting framework hotspots and coldspots via\nmining open source code on the web,” in 2008 23rd IEEE/ACM\nInternational Conference on Automated Software Engineering, 2008, pp.\n327–336.\n[77] Z. Tu, Z. Su, and P . Devanbu, “On the localness of software,” in\nProceedings of the 22nd ACM SIGSOFT International Symposium on\nFoundations of Software Engineering, ser. FSE 2014. New York, NY,\nUSA: Association for Computing Machinery, 2014, p. 269–280.\n[Online]. Available: https://doi.org/10.1145/2635868.2635875\n[78] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and\nN. Sundaresan, “Unit test case generation with transformers,”\nCoRR, vol. abs/2009.05617, 2020. [Online]. Available: https:\n//arxiv.org/abs/2009.05617\n[79] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshy-\nvanyk, “On learning meaningful code changes via neural machine\ntranslation,” in Proceedings of the 41st International Conference on\nSoftware Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31,\n2019, 2019, pp. 25–36.\n[80] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and\nD. Poshyvanyk, “An empirical study on learning bug-ﬁxing patches\nin the wild via neural machine translation,” ACM Trans. Softw. Eng.\nMethodol., vol. 28, no. 4, pp. 19:1–19:29, 2019.\n[81] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention\nis all you need,” in Advances in Neural Information Processing\nSystems 30, I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran\nAssociates, Inc., 2017, pp. 5998–6008. [Online]. Available:\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n[82] C. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk,\n“On learning meaningful assert statements for unit test cases,”\nin Proceedings of the 42nd International Conference on Software\nEngineering, ICSE 2020, 2020, p. To Appear.\n[83] F. Wen, E. Aghajani, C. Nagy, M. Lanza, and G. Bavota,\n“Siri, write the next method,” in 43rd IEEE/ACM International\nConference on Software Engineering, ICSE 2021, Madrid, Spain, 22-\n30 May 2021 . IEEE, 2021, pp. 138–149. [Online]. Available:\nhttps://doi.org/10.1109/ICSE43902.2021.00025\n[84] M. White, C. Vendome, M. Linares-Vásquez, and D. Poshyvanyk,\n“Toward deep learning software repositories,” in Proceedings of the\n12th Working Conference on Mining Software Repositories , ser. MSR\n’15. Piscataway, NJ, USA: IEEE Press, 2015, pp. 334–345. [Online].\nAvailable: http://dl.acm.org/citation.cfm?id=2820518.2820559\n[85] F. Wilcoxon, “Individual comparisons by ranking methods,” Bio-\nmetrics Bulletin, vol. 1, no. 6, pp. 80–83, 1945.\n[86] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,\nP . Cistac, T. Rault, R. Louf, M. Funtowicz, and J. Brew, “Hugging-\nface’s transformers: State-of-the-art natural language processing,”\nArXiv, vol. abs/1910.03771, 2019.\n[87] B. Yoav and H. Yosef, “Controlling the false discovery rate: A\npractical and powerful approach to multiple testing,” Journal of the\nRoyal Statistical Society. Series B (Methodological), vol. 57, no. 1, pp.\n289–300, 1995.\n[88] C. Zhang, J. Yang, Y. Zhang, J. Fan, X. Zhang, J. Zhao, and P . Ou,\n“Automatic parameter recommendation for practical API usage,”\nin 34th International Conference on Software Engineering, ICSE 2012,\nJune 2-9, 2012, Zurich, Switzerland, 2012, pp. 826–836.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, MONTH XXXX 20\nMatteo Ciniselliis a Ph.D. student in the Faculty\nof Informatics at the Università della Svizzera\nitaliana (USI), Switzerland, where he is part of\nthe Software Institute. He received his MSc. in\nMathematical Engineering from Politecnico di\nMilano, Italy, in April 2015. His research interests\ninclude the study of deep-learning models to\nsupport code-related tasks. More information\navailable at: https://www.inf.usi.ch/phd/cinism.\nNathan Cooperreceived a B.S. degree in Soft-\nware Engineering from the University of West\nFlorida in 2018. He is currently a Ph.D. candidate\nin Computer Science at William & Mary under\nthe advisement of Dr. Denys Poshyvanyk and\nis a member of the Semeru Research group.\nHe has research interests in Software Engineer-\ning, Machine / Deep Learning applications for\nSoftware Engineering, information retrieval, and\nquestion & answering applications for Software\nEngineering. He has published in the top peer-\nreviewed Software Engineering venues ICSE and MSR. He has also\nreceived the ACM SIGSOFT Distinguished paper award at ICSE’20.\nMore information is available at https://nathancooper.io/#/.\nLuca Pascarella is a postdoctoral researcher\nat the Università della Svizzera italiana (USI),\nSwitzerland, where he is part of the Software\nInstitute. He received his Ph.D. in Computer\nScience from the Delft University of Technol-\nogy (TU Delft), The Netherlands, in 2020. His\nbroader mission aims to smooth engineering\ntasks through data-driven algorithms, which lever-\nage the large amount of information recorded dur-\ning modern engineering processes. His research\ninterests include empirical software engineering,\nmining software repository, and code review. He received an ACM\nSIGSOFT Distinguished Paper Award at MSR 2017 and a Best Paper\nAward Honorable Mention at CSCW2018. More information available at:\nhttps://lucapascarella.com.\nAntonio Mastropaolois a Ph.D. student in the\nFaculty of Informatics at the Università della\nSvizzera italiana (USI), Switzerland, where he\nis part of the Software Institute. He received his\nMSc. in Software System Security from Università\ndegli studi del Molise, Italy, in July 2020. His\nresearch interests include the study and the\napplication of deep-learning techniques to foster\ncode-related tasks. More information available at:\nhttps://antoniomastropaolo.com.\nEmad Aghajaniis a postdoctoral researcher in\nthe SEART research group, at Software Insti-\ntute, the Università della Svizzera italiana (USI),\nSwitzerland. He ﬁnished his Ph.D. studies in\nComputer Science in 2020 under the supervision\nof Prof. Michele Lanza and Prof. Gabriele Bavota\nat USI. He received his M.Sc. in Software Engi-\nneering from Sharif University of Technology, Iran,\nin 2016. His research interests include software\nevolution, software maintenance, mining software\nrepositories, and empirical software engineering.\nMore information is available at: https://emadpres.github.io/.\nDenys Poshyvanykis a Professor of Computer\nScience at William and Mary. He received the\nMS and MA degrees in Computer Science from\nthe National University of Kyiv-Mohyla Academy,\nUkraine, and Wayne State University in 2003 and\n2006, respectively. He received the PhD degree\nin Computer Science from Wayne State Univer-\nsity in 2008. He served as a program co-chair\nfor ASE’21, MobileSoft’19, ICSME’16, ICPC’13,\nWCRE’12 and WCRE’11. He currently serves\non the editorial board of IEEE Transactions on\nSoftware Engineering (TSE), ACM Transactions on Software Engineering\nand Methodology (TOSEM), Empirical Software Engineering Journal\n(EMSE, Springer), Journal of Software: Evolution and Process (JSEP ,\nWiley) and Science of Computer Programming. His research interests\ninclude software engineering, software maintenance and evolution,\nprogram comprehension, reverse engineering and software repository\nmining. His research papers received several Best Paper Awards\nat ICPC’06, ICPC’07, ICSM’10, SCAM’10, ICSM’13, CODAPSY’19\nand ACM SIGSOFT Distinguished Paper Awards at ASE’13, ICSE’15,\nESEC/FSE’15, ICPC’16, ASE’17, ESEC/FSE’19 and ICSE’20. He also\nreceived the Most Inﬂuential Paper Awards at ICSME’16, ICPC’17 and\nICPC’20. He is a recipient of the NSF CAREER award (2013). He\nis a member of the IEEE and ACM. More information is available at:\nhttp://www.cs.wm.edu/~denys/.\nMassimiliano Di Pentais a full professor at the\nUniversity of Sannio, Italy. His research inter-\nests include software maintenance and evolution,\nmining software repositories, empirical software\nengineering, search-based software engineering,\nand service-centric software engineering. He is\nan author of over 300 papers appeared in inter-\nnational journals, conferences, and workshops.\nHe serves and has served in the organizing and\nprogram committees of more than 100 confer-\nences, including ICSE, FSE, ASE, ICSME. He is\neditor-in-chief of the Journal of Software: Evolution and Processes edited\nby Wiley. He is in the editorial board of ACM Transactions on Software\nEngineering and Methodology, and of the Empirical Software Engineering\nJournal. He has served the editorial board of the IEEE Transactions on\nSoftware Engineering.\nGabriele BavotaGabriele Bavota is an associate\nprofessor at the Faculty of Informatics of the\nUniversità della Svizzera italiana (USI), Switzer-\nland, where he is part of the Software Institute\nand he leads the SEART research group. He\nreceived the PhD in Computer Science from the\nUniversity of Salerno, Italy, in 2013. His research\ninterests include software maintenance and evo-\nlution, code quality, mining software repositories,\nand empirical software engineering. On these\ntopics, he authored over 140 papers appeared\nin international journals and conferences and has received four ACM\nSigsoft Distinguished Paper awards at the three top software engineering\nconferences: ASE 2013 and 2017, ESEC-FSE 2015, and ICSE 2015. He\nalso received the best/distinguished paper award at SCAM 2012, ICSME\n2018, MSR 2019, and ICPC 2020. He is the recipient of the 2018 ACM\nSigsoft Early Career Researcher Award for outstanding contributions in\nthe area of software engineering as an early career investigator and the\nprincipal investigator of the DEVINTA ERC project. More information is\navailable at: https://www.inf.usi.ch/faculty/bavota/.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7749593257904053
    },
    {
      "name": "Security token",
      "score": 0.6003512740135193
    },
    {
      "name": "Code (set theory)",
      "score": 0.5686300992965698
    },
    {
      "name": "Transformer",
      "score": 0.5630666017532349
    },
    {
      "name": "Source code",
      "score": 0.4628317654132843
    },
    {
      "name": "Programming language",
      "score": 0.4054209291934967
    },
    {
      "name": "Engineering",
      "score": 0.08659583330154419
    },
    {
      "name": "Operating system",
      "score": 0.08120143413543701
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 17
}