{
  "title": "PhoBERT: Pre-trained language models for Vietnamese",
  "url": "https://openalex.org/W3007955273",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2103676582",
      "name": "Dat Quoc Nguyen",
      "affiliations": [
        "VinUniversity"
      ]
    },
    {
      "id": "https://openalex.org/A1983959534",
      "name": "Anh Tuan Nguyen",
      "affiliations": [
        "Nvidia (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2964336292",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3104453603",
    "https://openalex.org/W2963841919",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2922722434",
    "https://openalex.org/W2973039175",
    "https://openalex.org/W3004142601",
    "https://openalex.org/W1597333669",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2953109491",
    "https://openalex.org/W563686125",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3121547375",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2251400573",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2963979075",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2989994479",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2996580882"
  ],
  "abstract": "We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https://github.com/VinAIResearch/PhoBERT",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1037–1042\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n1037\nPhoBERT: Pre-trained language models for Vietnamese\nDat Quoc Nguyen1 and Anh Tuan Nguyen2,∗\n1VinAI Research, Vietnam; 2NVIDIA, USA\nv.datnq9@vinai.io, tuananhn@nvidia.com\nAbstract\nWe present PhoBERT with two versions—\nPhoBERTbase and PhoBERT large—the first\npublic large-scale monolingual language mod-\nels pre-trained for Vietnamese. Experimental\nresults show that PhoBERT consistently out-\nperforms the recent best pre-trained multilin-\ngual model XLM-R (Conneau et al., 2020)\nand improves the state-of-the-art in multi-\nple Vietnamese-specific NLP tasks includ-\ning Part-of-speech tagging, Dependency pars-\ning, Named-entity recognition and Natural lan-\nguage inference. We release PhoBERT to fa-\ncilitate future research and downstream appli-\ncations for Vietnamese NLP. Our PhoBERT\nmodels are available at: https://github.\ncom/VinAIResearch/PhoBERT.\n1 Introduction\nPre-trained language models, especially BERT\n(Devlin et al., 2019)—the Bidirectional Encoder\nRepresentations from Transformers (Vaswani\net al., 2017), have recently become extremely pop-\nular and helped to produce significant improve-\nment gains for various NLP tasks. The success\nof pre-trained BERT and its variants has largely\nbeen limited to the English language. For other\nlanguages, one could retrain a language-specific\nmodel using the BERT architecture (Cui et al.,\n2019; de Vries et al., 2019; Vu et al., 2019; Martin\net al., 2020) or employ existing pre-trained mul-\ntilingual BERT-based models (Devlin et al., 2019;\nConneau and Lample, 2019; Conneau et al., 2020).\nIn terms of Vietnamese language modeling, to\nthe best of our knowledge, there are two main con-\ncerns as follows:\n•The Vietnamese Wikipedia corpus is the only\ndata used to train monolingual language models\n(Vu et al., 2019), and it also is the only Viet-\nnamese dataset which is included in the pre-\ntraining data used by all multilingual language\n∗Work done during internship at VinAI Research.\nmodels except XLM-R. It is worth noting that\nWikipedia data is not representative of a general\nlanguage use, and the Vietnamese Wikipedia\ndata is relatively small (1GB in size uncom-\npressed), while pre-trained language models can\nbe significantly improved by using more pre-\ntraining data (Liu et al., 2019).\n•All publicly released monolingual and multi-\nlingual BERT-based language models are not\naware of the difference between Vietnamese syl-\nlables and word tokens. This ambiguity comes\nfrom the fact that the white space is also\nused to separate syllables that constitute words\nwhen written in Vietnamese. 1 For example, a\n6-syllable written text “Tôi là một nghiên cứu\nviên” (I am a researcher) forms 4 words “Tôi I\nlàam mộta nghiên_cứu_viênresearcher”.\nWithout doing a pre-process step of Vietnamese\nword segmentation, those models directly apply\nByte-Pair encoding (BPE) methods (Sennrich\net al., 2016; Kudo and Richardson, 2018) to the\nsyllable-level Vietnamese pre-training data.2 In-\ntuitively, for word-level Vietnamese NLP tasks,\nthose models pre-trained on syllable-level data\nmight not perform as good as language models\npre-trained on word-level data.\nTo handle the two concerns above, we train the\nfirst large-scale monolingual BERT-based “base”\nand “large” models using a 20GB word-level Viet-\nnamese corpus. We evaluate our models on four\ndownstream Vietnamese NLP tasks: the common\nword-level ones of Part-of-speech (POS) tagging,\nDependency parsing and Named-entity recogni-\n1Thang et al. (2008) show that 85% of Vietnamese word\ntypes are composed of at least two syllables.\n2Although performing word segmentation before apply-\ning BPE on the Vietnamese Wikipedia corpus, ETNLP (Vu\net al., 2019) in fact does not publicly release any pre-trained\nBERT-based language model ( https://github.com/\nvietnlp/etnlp). In particular, Vu et al. (2019) release a\nset of 15K BERT-based word embeddings specialized only\nfor the Vietnamese NER task.\n1038\ntion (NER), and a language understanding task of\nNatural language inference (NLI) which can be\nformulated as either a syllable- or word-level task.\nExperimental results show that our models obtain\nstate-of-the-art (SOTA) results on all these tasks.\nOur contributions are summarized as follows:\n•We present the first large-scale monolingual\nlanguage models pre-trained for Vietnamese.\n•Our models help produce SOTA performances\non four downstream tasks of POS tagging, De-\npendency parsing, NER and NLI, thus show-\ning the effectiveness of large-scale BERT-based\nmonolingual language models for Vietnamese.\n•To the best of our knowledge, we also perform\nthe first set of experiments to compare monolin-\ngual language models with the recent best multi-\nlingual model XLM-R in multiple (i.e. four) dif-\nferent language-specific tasks. The experiments\nshow that our models outperform XLM-R on all\nthese tasks, thus convincingly confirming that\ndedicated language-specific models still outper-\nform multilingual ones.\n•We publicly release our models under the name\nPhoBERT which can be used with fairseq\n(Ott et al., 2019) and transformers (Wolf\net al., 2019). We hope that PhoBERT can serve\nas a strong baseline for future Vietnamese NLP\nresearch and applications.\n2 PhoBERT\nThis section outlines the architecture and de-\nscribes the pre-training data and optimization\nsetup that we use for PhoBERT.\nArchitecture: Our PhoBERT has two versions,\nPhoBERTbase and PhoBERT large, using the same\narchitectures of BERT base and BERT large, respec-\ntively. PhoBERT pre-training approach is based on\nRoBERTa (Liu et al., 2019) which optimizes the\nBERT pre-training procedure for more robust per-\nformance.\nPre-training data: To handle the first con-\ncern mentioned in Section 1, we use a 20GB\npre-training dataset of uncompressed texts. This\ndataset is a concatenation of two corpora: (i)\nthe first one is the Vietnamese Wikipedia corpus\n(∼1GB), and (ii) the second corpus ( ∼19GB) is\ngenerated by removing similar articles and dupli-\ncation from a 50GB Vietnamese news corpus. 3 To\n3https://github.com/binhvq/news-corpus,\ncrawled from a wide range of news websites and topics.\nTask #training #valid #test\nPOS tagging† 27,000 870 2,120\nDep. parsing† 8,977 200 1,020\nNER† 14,861 2,000 2,831\nNLI‡ 392,702 2,490 5,010\nTable 1: Statistics of the downstream task datasets.\n“#training”, “#valid” and “#test” denote the size of the\ntraining, validation and test sets, respectively. †and ‡\nrefer to the dataset size as the numbers of sentences\nand sentence pairs, respectively.\nsolve the second concern, we employ RDRSeg-\nmenter (Nguyen et al., 2018) from VnCoreNLP\n(Vu et al., 2018) to perform word and sentence\nsegmentation on the pre-training dataset, resulting\nin ∼145M word-segmented sentences ( ∼3B word\ntokens). Different from RoBERTa, we then apply\nfastBPE (Sennrich et al., 2016) to segment these\nsentences with subword units, using a vocabulary\nof 64K subword types. On average there are 24.4\nsubword tokens per sentence.\nOptimization: We employ the RoBERTa imple-\nmentation in fairseq (Ott et al., 2019). We set\na maximum length at 256 subword tokens, thus\ngenerating 145M ×24.4 / 256 ≈13.8M sentence\nblocks. Following Liu et al. (2019), we optimize\nthe models using Adam (Kingma and Ba, 2014).\nWe use a batch size of 1024 across 4 V100 GPUs\n(16GB each) and a peak learning rate of 0.0004 for\nPhoBERTbase, and a batch size of 512 and a peak\nlearning rate of 0.0002 for PhoBERT large. We run\nfor 40 epochs (here, the learning rate is warmed\nup for 2 epochs), thus resulting in 13.8M ×40\n/ 1024 ≈540K training steps for PhoBERT base\nand 1.08M training steps for PhoBERT large. We\npre-train PhoBERT base during 3 weeks, and then\nPhoBERTlarge during 5 weeks.\n3 Experimental setup\nWe evaluate the performance of PhoBERT on four\ndownstream Vietnamese NLP tasks: POS tagging,\nDependency parsing, NER and NLI.\nDownstream task datasets\nTable 1 presents the statistics of the experimental\ndatasets that we employ for downstream task eval-\nuation. For POS tagging, Dependency parsing and\nNER, we follow the VnCoreNLP setup (Vu et al.,\n2018), using standard benchmarks of the VLSP\n2013 POS tagging dataset, 4 the VnDT dependency\n4https://vlsp.org.vn/vlsp2013/eval\n1039\nPOS tagging(word-level) Dependency parsing(word-level)\nModel Acc. Model LAS / UAS\nRDRPOSTagger (Nguyen et al., 2014a) [♣] 95.1 _ _\nBiLSTM-CNN-CRF (Ma and Hovy, 2016) [♣] 95.4 VnCoreNLP-DEP (Vu et al., 2018) [⋆] 71.38 / 77.35\nVnCoreNLP-POS (Nguyen et al., 2017) [♣] 95.9 jPTDP-v2 [ ⋆] 73.12 / 79.63\njPTDP-v2 (Nguyen and Verspoor, 2018) [⋆] 95.7 jointWPD [ ⋆] 73.90 / 80.12\njointWPD (Nguyen, 2019) [⋆] 96.0 Biaffine (Dozat and Manning, 2017) [ ⋆] 74.99 / 81.19\nXLM-Rbase(our result) 96.2 Biaffine w/ XLM-R base(our result) 76.46 / 83.10\nXLM-Rlarge(our result) 96.3 Biaffine w/ XLM-R large(our result) 75.87 / 82.70\nPhoBERTbase 96.7 Biaffine w/ PhoBERTbase 78.77/ 85.22\nPhoBERTlarge 96.8 Biaffine w/ PhoBERTlarge 77.85/ 84.32\nTable 2: Performance scores (in %) on the POS tagging and Dependency parsing test sets. “Acc.”, “LAS” and\n“UAS” abbreviate the Accuracy, the Labeled Attachment Score and the Unlabeled Attachment Score, respectively\n(here, all these evaluation metrics are computed on all word tokens, including punctuation). [ ♣] and [ ⋆] denote\nresults reported by Nguyen et al. (2017) and Nguyen (2019), respectively.\ntreebank v1.1 (Nguyen et al., 2014b) with POS\ntags predicted by VnCoreNLP and the VLSP 2016\nNER dataset (Nguyen et al., 2019a).\nFor NLI, we use the manually-constructed Viet-\nnamese validation and test sets from the cross-\nlingual NLI (XNLI) corpus v1.0 (Conneau et al.,\n2018) where the Vietnamese training set is re-\nleased as a machine-translated version of the cor-\nresponding English training set (Williams et al.,\n2018). Unlike the POS tagging, Dependency pars-\ning and NER datasets which provide the gold word\nsegmentation, for NLI, we employ RDRSegmenter\nto segment the text into words before applying\nBPE to produce subwords from word tokens.\nFine-tuning\nFollowing Devlin et al. (2019), for POS tagging\nand NER, we append a linear prediction layer\non top of the PhoBERT architecture (i.e. to the\nlast Transformer layer of PhoBERT) w.r.t. the first\nsubword of each word token. 5 For dependency\nparsing, following Nguyen (2019), we employ a\nreimplementation of the state-of-the-art Biaffine\ndependency parser (Dozat and Manning, 2017)\nfrom Ma et al. (2018) with default optimal hyper-\nparameters. We then extend this parser by replac-\ning the pre-trained word embedding of each word\nin an input sentence by the corresponding contex-\ntualized embedding (from the last layer) computed\nfor the first subword token of the word.\nFor POS tagging, NER and NLI, we employ\ntransformers (Wolf et al., 2019) to fine-tune\nPhoBERT for each task and each dataset indepen-\ndently. We use AdamW (Loshchilov and Hutter,\n5In our preliminary experiments, using the average of con-\ntextualized embeddings of subword tokens of each word to\nrepresent the word produces slightly lower performance than\nusing the contextualized embedding of the first subword.\n2019) with a fixed learning rate of 1.e-5 and a\nbatch size of 32 (Liu et al., 2019). We fine-tune in\n30 training epochs, evaluate the task performance\nafter each epoch on the validation set (here, early\nstopping is applied when there is no improvement\nafter 5 continuous epochs), and then select the best\nmodel checkpoint to report the final result on the\ntest set (note that each of our scores is an average\nover 5 runs with different random seeds).\n4 Experimental results\nMain results\nTables 2 and 3 compare PhoBERT scores with the\nprevious highest reported results, using the same\nexperimental setup. It is clear that our PhoBERT\nhelps produce new SOTA performance results for\nall four downstream tasks.\nFor POS tagging , the neural model jointWPD\nfor joint POS tagging and dependency pars-\ning (Nguyen, 2019) and the feature-based model\nVnCoreNLP-POS (Nguyen et al., 2017) are the\ntwo previous SOTA models, obtaining accuracies\nat about 96.0%. PhoBERT obtains 0.8% absolute\nhigher accuracy than these two models.\nFor Dependency parsing , the previous highest\nparsing scores LAS and UAS are obtained by the\nBiaffine parser at 75.0% and 81.2%, respectively.\nPhoBERT helps boost the Biaffine parser with\nabout 4% absolute improvement, achieving a LAS\nat 78.8% and a UAS at 85.2%.\nFor NER , PhoBERT large produces 1.1 points\nhigher F 1 than PhoBERT base. In addition,\nPhoBERTbase obtains 2+ points higher than the\nprevious SOTA feature- and neural network-based\nmodels VnCoreNLP-NER (Vu et al., 2018) and\nBiLSTM-CNN-CRF (Ma and Hovy, 2016) which\n1040\nNER (word-level) NLI (syllable- or word-level)\nModel F 1 Model Acc.\nBiLSTM-CNN-CRF [♦] 88.3 _ _\nVnCoreNLP-NER (Vu et al., 2018) [♦] 88.6 BiLSTM-max (Conneau et al., 2018) 66.4\nVNER (Nguyen et al., 2019b) 89.6 mBiLSTM (Artetxe and Schwenk, 2019) 72.0\nBiLSTM-CNN-CRF + ETNLP [♠] 91.1 multilingual BERT (Devlin et al., 2019) [ ■] 69.5\nVnCoreNLP-NER + ETNLP [♠] 91.3 XLM MLM+TLM (Conneau and Lample, 2019) 76.6\nXLM-Rbase (our result) 92.0 XLM-R base (Conneau et al., 2020) 75.4\nXLM-Rlarge (our result) 92.8 XLM-R large (Conneau et al., 2020) 79.7\nPhoBERTbase 93.6 PhoBERTbase 78.5\nPhoBERTlarge 94.7 PhoBERTlarge 80.0\nTable 3: Performance scores (in %) on the NER and NLI test sets. [ ♦], [ ♠] and [ ■] denote results reported by\nVu et al. (2018), Vu et al. (2019) and Wu and Dredze (2019), respectively. Note that there are higher Vietnamese\nNLI results reported for XLM-R when fine-tuning on the concatenation of all 15 training datasets from the XNLI\ncorpus (i.e. TRANSLATE-TRAIN-ALL: 79.5% for XLM-R base and 83.4% XLM-R large). However, those results\nmight not be comparable as we only use the monolingual Vietnamese training data for fine-tuning.\nare trained with the set of 15K BERT-based\nETNLP word embeddings (Vu et al., 2019).\nFor NLI , PhoBERT outperforms the multilin-\ngual BERT (Devlin et al., 2019) and the BERT-\nbased cross-lingual model with a new transla-\ntion language modeling objective XLM MLM+TLM\n(Conneau and Lample, 2019) by large margins.\nPhoBERT also performs better than the recent\nbest pre-trained multilingual model XLM-R but\nusing far fewer parameters than XLM-R: 135M\n(PhoBERTbase) vs. 250M (XLM-R base); 370M\n(PhoBERTlarge) vs. 560M (XLM-R large).\nDiscussion\nWe find that PhoBERT large achieves 0.9% lower\ndependency parsing scores than PhoBERT base.\nOne possible reason is that the last Transformer\nlayer in the BERT architecture might not be the\noptimal one which encodes the richest informa-\ntion of syntactic structures (Hewitt and Manning,\n2019; Jawahar et al., 2019). Future work will\nstudy which PhoBERT’s Transformer layer con-\ntains richer syntactic information by evaluating the\nVietnamese parsing performance from each layer.\nUsing more pre-training data can significantly\nimprove the quality of the pre-trained language\nmodels (Liu et al., 2019). Thus it is not surprising\nthat PhoBERT helps produce better performance\nthan ETNLP on NER, and the multilingual BERT\nand XLM MLM+TLM on NLI (here, PhoBERT uses\n20GB of Vietnamese texts while those models em-\nploy the 1GB Vietnamese Wikipedia corpus).\nFollowing the fine-tuning approach that we use\nfor PhoBERT, we carefully fine-tune XLM-R for\nthe remaining Vietnamese POS tagging, Depen-\ndency parsing and NER tasks (here, it is applied\nto the first sub-syllable token of the first sylla-\nble of each word). 6 Tables 2 and 3 show that\nour PhoBERT also does better than XLM-R on\nthese three word-level tasks. It is worth noting that\nXLM-R uses a 2.5TB pre-training corpus which\ncontains 137GB of Vietnamese texts (i.e. about\n137 / 20 ≈7 times bigger than our pre-training\ncorpus). Recall that PhoBERT performs Viet-\nnamese word segmentation to segment syllable-\nlevel sentences into word tokens before applying\nBPE to segment the word-segmented sentences\ninto subword units, while XLM-R directly ap-\nplies BPE to the syllable-level Vietnamese pre-\ntraining sentences. This reconfirms that the ded-\nicated language-specific models still outperform\nthe multilingual ones (Martin et al., 2020). 7\n5 Conclusion\nIn this paper, we have presented the first large-\nscale monolingual PhoBERT language mod-\nels pre-trained for Vietnamese. We demonstrate\nthe usefulness of PhoBERT by showing that\nPhoBERT performs better than the recent best\nmultilingual model XLM-R and helps produce the\nSOTA performances for four downstream Viet-\nnamese NLP tasks of POS tagging, Dependency\nparsing, NER and NLI. By publicly releasing\nPhoBERT models, we hope that they can foster fu-\nture research and applications in Vietnamese NLP.\n6For fine-tuning XLM-R, we use a grid search on the val-\nidation set to select the AdamW learning rate from {5e-6,\n1e-5, 2e-5, 4e-5} and the batch size from {16, 32}.\n7Note that Martin et al. (2020) only compare their model\nCamemBERT with XLM-R on the French NLI task.\n1041\nReferences\nMikel Artetxe and Holger Schwenk. 2019. Massively\nMultilingual Sentence Embeddings for Zero-Shot\nCross-Lingual Transfer and Beyond. TACL, 7:597–\n610.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of ACL, pages 8440–8451.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual Language Model Pretraining. In Proceed-\nings of NeurIPS, pages 7059–7069.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Hol-\nger Schwenk, Ves Stoyanov, Adina Williams, and\nSamuel R. Bowman. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nEMNLP, pages 2475–2485.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-Training with Whole Word Masking for Chinese\nBERT. arXiv preprint, arXiv:1906.08101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL, pages 4171–\n4186.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep Biaffine Attention for Neural Dependency\nParsing. In Proceedings of ICLR.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word repre-\nsentations. In Proceedings of NAACL, pages 4129–\n4138.\nGanesh Jawahar, Beno ˆıt Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of ACL, pages 3651–\n3657.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nMethod for Stochastic Optimization. arXiv preprint,\narXiv:1412.6980.\nTaku Kudo and John Richardson. 2018. Sentence-\nPiece: A simple and language independent subword\ntokenizer and detokenizer for Neural Text Process-\ning. In Proceedings of EMNLP: System Demonstra-\ntions, pages 66–71.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv preprint, arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization. In Proceedings of\nICLR.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of ACL, pages 1064–1074.\nXuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,\nGraham Neubig, and Eduard Hovy. 2018. Stack-\nPointer Networks for Dependency Parsing. In Pro-\nceedings of ACL, pages 1403–1414.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de la Clergerie, Djamé Seddah, and Beno ˆıt\nSagot. 2020. CamemBERT: a Tasty French Lan-\nguage Model. In Proceedings of ACL, pages 7203–\n7219.\nDat Quoc Nguyen. 2019. A neural joint model for Viet-\nnamese word segmentation, POS tagging and depen-\ndency parsing. In Proceedings of ALTA, pages 28–\n34.\nDat Quoc Nguyen, Dai Quoc Nguyen, Dang Duc Pham,\nand Son Bao Pham. 2014a. RDRPOSTagger: A Rip-\nple Down Rules-based Part-Of-Speech Tagger. In\nProceedings of the Demonstrations at EACL, pages\n17–20.\nDat Quoc Nguyen, Dai Quoc Nguyen, Son Bao Pham,\nPhuong-Thai Nguyen, and Minh Le Nguyen. 2014b.\nFrom Treebank Conversion to Automatic Depen-\ndency Parsing for Vietnamese. In Proceedings of\nNLDB, pages 196–207.\nDat Quoc Nguyen, Dai Quoc Nguyen, Thanh Vu, Mark\nDras, and Mark Johnson. 2018. A Fast and Accu-\nrate Vietnamese Word Segmenter. In Proceedings\nof LREC, pages 2582–2587.\nDat Quoc Nguyen and Karin Verspoor. 2018. An\nimproved neural network model for joint POS tag-\nging and dependency parsing. In Proceedings of the\nCoNLL 2018 Shared Task, pages 81–91.\nDat Quoc Nguyen, Thanh Vu, Dai Quoc Nguyen, Mark\nDras, and Mark Johnson. 2017. From word segmen-\ntation to POS tagging for Vietnamese. In Proceed-\nings of ALTA, pages 108–113.\nHuyen Nguyen, Quyen Ngo, Luong Vu, Vu Tran, and\nHien Nguyen. 2019a. VLSP Shared Task: Named\nEntity Recognition. Journal of Computer Science\nand Cybernetics, 34(4):283–294.\nKim Anh Nguyen, Ngan Dong, and Cam-Tu Nguyen.\n2019b. Attentive Neural Network for Named En-\ntity Recognition in Vietnamese. In Proceedings of\nRIVF.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations, pages 48–53.\n1042\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of ACL, pages\n1715–1725.\nDinh Quang Thang, Le Hong Phuong, Nguyen\nThi Minh Huyen, Nguyen Cam Tu, Mathias Rossig-\nnol, and Vu Xuan Luong. 2008. Word segmentation\nof Vietnamese texts: a comparison of approaches. In\nProceedings of LREC, pages 1933–1936.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. BERTje: A Dutch BERT\nModel. arXiv preprint, arXiv:1912.09582.\nThanh Vu, Dat Quoc Nguyen, Dai Quoc Nguyen, Mark\nDras, and Mark Johnson. 2018. VnCoreNLP: A\nVietnamese Natural Language Processing Toolkit.\nIn Proceedings of NAACL: Demonstrations, pages\n56–60.\nXuan-Son Vu, Thanh Vu, Son Tran, and Lili Jiang.\n2019. ETNLP: A visual-aided systematic approach\nto select pre-trained embeddings for a downstream\ntask. In Proceedings of RANLP, pages 1285–1294.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A Broad-Coverage Challenge Corpus for Sen-\ntence Understanding through Inference. In Proceed-\nings of NAACL, pages 1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. arXiv preprint, arXiv:1910.03771.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of EMNLP-IJCNLP, pages 833–844.",
  "topic": "Vietnamese",
  "concepts": [
    {
      "name": "Vietnamese",
      "score": 0.9670787453651428
    },
    {
      "name": "Computer science",
      "score": 0.7828763723373413
    },
    {
      "name": "Natural language processing",
      "score": 0.7539684772491455
    },
    {
      "name": "Dependency (UML)",
      "score": 0.6901636123657227
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6723268628120422
    },
    {
      "name": "Language model",
      "score": 0.6260464787483215
    },
    {
      "name": "Parsing",
      "score": 0.5972562432289124
    },
    {
      "name": "Inference",
      "score": 0.591335654258728
    },
    {
      "name": "Dependency grammar",
      "score": 0.5369365811347961
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5203563570976257
    },
    {
      "name": "Entity linking",
      "score": 0.4289707541465759
    },
    {
      "name": "Natural language understanding",
      "score": 0.4175518751144409
    },
    {
      "name": "Sequence labeling",
      "score": 0.4149458408355713
    },
    {
      "name": "Natural language",
      "score": 0.36901238560676575
    },
    {
      "name": "Linguistics",
      "score": 0.23488035798072815
    },
    {
      "name": "Knowledge base",
      "score": 0.17920222878456116
    },
    {
      "name": "Task (project management)",
      "score": 0.05227521061897278
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I1304085615",
      "name": "Nvidia (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 35
}