{
  "title": "Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning",
  "url": "https://openalex.org/W3195923413",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2353051423",
      "name": "Wang, Cunxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2295850553",
      "name": "Zheng, Boyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744164729",
      "name": "Niu Yuchen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1920224989",
      "name": "Zhang Yue",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W3089285634",
    "https://openalex.org/W2905270607",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3048585918",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3085225979",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W3099624838",
    "https://openalex.org/W3035267217",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W3113425182",
    "https://openalex.org/W2978518939",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W3133029875",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "To quantitatively and intuitively explore the generalization ability of pre-trained language models (PLMs), we have designed several tasks of arithmetic and logical reasoning. We both analyse how well PLMs generalize when the test data is in the same distribution as the train data and when it is different, for the latter analysis, we have also designed a cross-distribution test set other than the in-distribution test set. We conduct experiments on one of the most advanced and publicly released generative PLM - BART. Our research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.",
  "full_text": "Exploring Generalization Ability of Pretrained Language Models on\nArithmetic and Logical Reasoning\nCunxiang Wang♠♣, Boyuan Zheng♣♦, Yuchen Niu♣♭ and Yue Zhang♣♥∗\n♠Zhejiang University, China\n♣School of Engineering, Westlake University, China\n♥Institute of Advanced Technology, Westlake Institute for Advanced Study, China\n♦Johns Hopkins University\n♭Imperial College London\n{wangcunxiang, zhangyue, zhengboyuan, niuyuchen}@westlake.edu.cn\nAbstract\nTo quantitatively and intuitively explore the\ngeneralization ability of pre-trained language\nmodels (PLMs), we have designed several\ntasks of arithmetic and logical reasoning. We\nboth analyse how well PLMs generalize when\nthe test data is in the same distribution as\nthe train data and when it is different, for\nthe latter analysis, we have also designed a\ncross-distribution test set other than the in-\ndistribution test set. We conduct experiments\non one of the most advanced and publicly re-\nleased generative PLM - BART. Our research\nﬁnds that the PLMs can easily generalize when\nthe distribution is the same, however, it is still\ndifﬁcult for them to generalize out of the dis-\ntribution.\n1 Introduction\nNeural networks have shown strong capabilities\nin a range of NLP tasks (Sutskever et al., 2014;\nVaswani et al., 2017). Recently, pretrained lan-\nguage models (PLMs) have achieved signiﬁcantly\nlevels of performance gains on many benchmark\ndatasets (Devlin et al., 2019; Lewis et al., 2020a;\nRadford et al., 2019). Recently, some work\nshows that neural networks are lack of generaliza-\ntion ability in mathematical and logical reasoning\n(Nogueira et al., 2021; Madsen and alexander rosen-\nberg johansen, 2019). This can lead to more un-\nderstanding of the limitation of existing models\nand motivate future work. However, no work has\nbeen done to quantitatively or intuitively explore\nthe conditions under which PLMs can generalize,\nin terms of whether PLMs can understand the in-\nternal mathematical rules and logical rules. The\nexample of mathematical rules is shown in Figure 1.\nWe suppose that if the model can effectively learn\nthe underlying rules of Addition and Subtraction\n∗The corresponding author\nFigure 1: Example mathematical rules for Addition and\nSubtraction. If the model can master these rules, we\nsuppose it can generalize well on all two-number addi-\ntion and subtraction samples.\nwhen giving sufﬁcient training data, it can gener-\nalize to all two-number addition and subtraction\ncalculation.\nTo this end, we conduct quantitative insights by\ndesigning a series of tasks for simple mathematical\noperations and logical reasoning, which includes\nnumbering, addition, subtraction, comparison, and\nsymbolic logic. We construct a set of correspond-\ning datasets, where instances are in the form of\ntext or mathematical expressions. Some examples\nare shown in the next section. For example, in the\nAddition task, ‘100 + 200’ is the question and ‘300’\nis the answer.\nThere are various types of generalization\n(Linzen, 2020; Lake and Baroni, 2018), such\nas question generalization on distribution differ-\nences between training set and test set (Wallace\net al., 2019), and answer generalization on distri-\nbution differences between training set and test set\n(Nogueira et al., 2021). For example, in the Ad-\ndition task, if the question and answer numbers\nin training data are of three-digit, but the question\nand answer numbers in the testing data are of two-\nor four-digit, they are in different distribution. To\ncover each type of generalization, we use different\nkinds of tasks and corresponding dataset. For ex-\nample, we use addition to test the generalization on\nthe question distribution differences data between\narXiv:2108.06743v2  [cs.CL]  19 Oct 2021\nFigure 2: The Numbering task has two subtasks,\nnamely Counting and Listing.\ntraining and testing. In this task, the numbers in\nthe training set and development have three digits.\nHowever, the numbers in test set is set to consist of\ntwo, three, and four digits.\nWe conduct experiments using BART (Lewis\net al., 2020a) since they can generate arbitrary text\nsequences and have been shown to achieve the state-\nof-art results on numerous Natural Language Pro-\ncessing (NLP) tasks. For each task, we ﬁne-tune\nBART with training data, validate on the develop-\nment set and ﬁnally evaluate on the test set. We ﬁnd\nthat strong PLMs can address simple generaliza-\ntion of the same answer distribution for counting,\narithmetic and logic tasks. But they cannot mas-\nter the underlying rules of arithmetic reasoning,\nfor example, the model trained on 3-digit addition\ncan handle the addition expressions with 2-digit or\n4-digit.\nWe will release all the code and data set for\nfuture study.\n2 Task\nWe construct ﬁve tasks related to algebraic and\nlogical reasoning, namely Numbering, Addition,\nSubtraction, Comparison, Symbolic Logic. In\norder to test the generalization ability of models\non the data with the same distribution and on the\ndata with the different distribution, we create an in-\ndistribution dataset and a cross-distribution dataset\nfor each task. The in-distribution dataset contains\ntrain set, development set and test set that are in the\nsame distribution. The cross-distribution dataset\nonly serves as the test set and it is in the different\ndistribution in contrast to the in-distribution dataset.\nWe believe that if the model can understand the un-\nderlying rules of arithmetic and logical Reasoning,\nit can both generalize well on in-distribution and\ncross-distribution test set.\n2.1 Numbering\nThis task comprises two symmetric subtasks,\nnamely Counting and Listing. Examples are\n(a) Description of Addition\ntask.\n(b) Description of Subtrac-\ntion task.\n(c) Description of Compari-\nson task.\n(d) Description of Symbolic\nLogic task.\nshown in Figure 2. The Counting task asks the\nmodel to count the number of characters in the in-\nput sequence. For example, ‘A A A A A A’ is a\nsequence with length ‘6’. The Listing task asks\nthe model to output a list with a speciﬁc length\nand character. For example, the model receives a\ncommand ‘Generate a list of 6 A’ and the result is\n‘A A A A A A’.\n2.2 Addition\nThe Addition task is the standard summation of\ntwo input numbers. In order to make sure that\nall numbers are in the same distribution during\ntraining, we use only the equations whose left-\nhand-side and right-hand-side are both three dig-\nits in the in-distribution dataset. We also adopt\ntwo-digit and four-digit numbers on both sides in\ncross-distribution test set to further test the general-\nization ability of models. One example is shown in\nFigure 3a.\n2.3 Subtraction\nThe Subtraction task the standard tack(?) to sub-\ntract a subtrahend from a minuend. In order to\nmake sure that all numbers are in same distribu-\ntion during training, we use only equations whose\nleft-hand-side and right-hand-side are both three\ndigits in the in-distribution dataset. We also adopt\ntwo-digit and four-digit numbers on both sides in\ncross-distribution test set to further test the general-\nization ability of models. A example of Subtraction\ntask is shown in Figure 3b.\n2.4 Comparison\nThe Comparison task is to determine which of the\ntwo numbers is greater or smaller. In order to\nmake sure all numbers are in same distribution\nduring training, we use only equations whose left-\nTask Train Set Dev Set\nIn- + Cross-\nDistribution\nTest Set\nIn- + Cross-\nDistribution\nDataset\nNumbering - Counting 3,744 468 468 + 2,030 4,680 + 2,030\nNumbering - Listing 3,744 468 468 4,680\nAddition 256,320 32,040 32,040 + 4,000 320,400 + 4,000\nSubtraction 256,320 32,040 32,040 + 4,000 320,400 + 4,000\nComparison 648,000 81,000 81,000 + 5,600 810,000 + 5,600\nSymbolic Logic 40,000 5,000 5,000 + 2,200 50,000 + 2,200\nTable 1: Data statistics of each task. For each task, we list the in-distribution dataset and cross-distribution test set.\nhand-side and right-hand-side are both three dig-\nits in the in-distribution dataset. We also adopt\ntwo-digit and four-digit numbers on both sides in\ncross-distribution test set to further test the general-\nization ability of models. One example is shown in\nFigure 3c.\n2.5 Symbolic Logic\nAs shown in Figure 3d, this task is to reason over\nsymbolic logic expressions. The input question ex-\npression consists of six basic components, which\nare ‘0’, ‘1’, ‘&’, ‘|’, ‘¬’ and ‘→’, representing\nFALSE, TRUE, AND, OR, NOT and IMPLY, respec-\ntively. The output answer is either 0 or 1, which\nrepresent FALSE and TRUE, respectively. This task\nasks the model to reason over the input logic ex-\npression and determine whether it is true or false.\nIn order to make sure all expressions are in the\nsame distribution during training, we use only the\nexpressions that contain 6 - 10basic ‘0’ and ‘1’\ncomponents. For testing the generalization ability\nof models, we also adopt the some expressions with\n1 - 15 basic ‘0’ and ‘1’ components in the test set.\nDifferent from the other tasks, we select a sub-\nset from the overall dataset to serve as the in-\ndistribution dataset because the data is large. We\ntake only 10,000 of expressions with X basic com-\nponents, where X is a number between 6 - 10, re-\nspectively. So, we end up with 50,000 samples in\nthe in-distribution dataset.\n2.6 Metrics\nWe use Exact Match to compute accuracy for\nNumbering, Addition, Subtraction and Compar-\nison tasks. However, for the Symbolic Logic task,\nsince the answer distribution is unbalanced (84%\nanswers are ‘1’), we use the F1 score as the metric.\n3 Experiments\nIn this section, we separate the generalization ex-\nperiments to In-Distribution Generalizationex-\nperiments and Cross-Distribution experiments. In\nthe former, the testing data is in the same distribu-\ntion with the training data. In the latter, the testing\ndata is in the different distribution from the train-\ning data. We suppose that if the model can master\nthe underlying rules of the mathematical and log-\nical reasoning, it should achieve 100% accuracy\non both In-Distribution Generalization experiments\nand Cross-Distribution experiments.\nWe have organized the details of in-distribution\ndata and cross-distribution data in this section. In\naddition, We also sorted out the examples of them\nand put the examples in the Appendix Table 1.\n3.1 Experimental Settings\nWe adopt BART (Lewis et al., 2020a) namely due\nto the following reasons. First, it is a generative\npretrained language model, which means that they\ncan generate arbitrary sequences of tokens. This\nis essential for the addition and subtraction task.\nSecond, it has achieved state-of-art results on nu-\nmerous tasks and they has received much research\nattention. Last, it has released model checkpoints,\nthus it can be more standardized and more fair can\nevaluate them.\nFor the BART (Lewis et al., 2020a) model,\nwe conduct experiments on the publicly released\n‘BART-Large’ checkpoint1. We insert spaces be-\ntween numbers while representing them in the data.\nFor example, ‘111’ is written as ‘1 1 1’ both in the\nquestion and answer. For the character sequence in\nthe Numbering task, we also insert spaces between\nthe sequence, such as ‘A A A’.\n3.2 In-Distribution Generalization\nIn this subsection, we mainly explore models’ gen-\neralization ability on test data which in the same\ndistribution with train data. For the Counting sub-\ntask of the Numbering task, each question is a se-\nquence with 10-99 same character which is one\ncharacter among the alphabet; each answer is an\n1https://huggingface.co/facebook/bart-large/tree/main\n(a) The results of counting\ntask.\n(b) The results of listing\ntask.\n(c) The results of addition\ntask.\n(d) The results of subtrac-\ntion task.\n(e) The results of compari-\nson task.\n(f) The results of symbolic\nlogic.\nFigure 4: The in-distribution results on each task.\ninteger between 10 and 99. For the Listing task,\neach question is a textual sequence ‘Generate a list\nwith X Y’, where X is an integer between 10 and\n99 and Y is one character among the alphabet; each\nanswer is a sequence with 10-99 same characters.\nFor the Addition task, each question is an addition\nexpression, and the answer is a sum number. Each\nnumber in the question and answer is three digits.\nFor the Subtraction task, each question is an sub-\ntraction expression , and the answer is a difference\nnumber. Each number in the question and answer\nis of three-digit. For the Comparison task, each\nquestion is made of two numbers and each answer\nis a single symbol which is either ‘>’ or ‘<’ or ‘=’.\nThe numbers in the question are all of three-digit.\nFor the Symbolic Logic task, each question is a\nsequence with 5-10 basic ‘0’ and ‘1’ components;\neach answer is either 0 or 1.\nFor testing generalization ability on the same\ndistributional data, we explore how the num-\nber of training samples affects the generalization.\nFor each task, we extract subsets from the in-\ndistribution train set and train on the subsets, but\nkeep the distribution of development set and test\nset the same. Thus, we analyse how the number of\ntraining samples inﬂuences the performance, which\nalso indicate the generalization ability of models\non the data with the same distribution.\nThe in-distribution results on the Numbering task\nare shown in Figure 4b Figure 4a. For the Listing\nsubtask, we ﬁnd that the model’s generation results\nare very unstable, which means that the outputs\noften contain other tokens other than the needed\ncharacter. For example, when the input is ‘Generate\na list of 6 A’, the output can be ‘A A a Aa E T A’.\nWhen the sequence length increases, this kind of\ndisruption will be more likely to occur. So, results\nare always around zero. We suppose this result is\nresult from the instability of the generative model\nitself, because we also observe this situation from\nother generative models, such as T5 (Raffel et al.,\n2020). So, we mainly analyse the Counting task\nrather the Listing task in the following sections.\nIt can be seen that when the number of training\nsamples increase, the performance of Counting will\nalso improve.\nThe in-distribution results on the Addition task\nare shown in Figure 4c. We can seen that when\nthe number of training samples is 1600 (0.5% of\nthe dataset), the model can achieve 99% accuracy;\neven when the number of training samples is re-\nduced to 160 (0.05% of the dataset), the model\ncan still achieve around 40% accuracy. The in-\ndistribution results on the Subtraction, Compari-\nson, Symbolic Logictask are shown in Figure 4d,\nFigure 4e and Figure 4f, respectively. It can be seen\nfrom the ﬁgures that when the number of training\nsamples increase, the model can perform better in\nthe in-distribution test set. And when the training\nsamples increase to several hundreds, the model\ncan achieve around 100% accuracy or F1, showing\nBART’s ability on the in-distribution generaliza-\ntion. Thus, we are wondering whether the model\nhas truly learn the underlying rules of these tasks\nor they just use some spurious correlations to solve\nthese questions, so, we design cross-distribution\ngeneralization test set to further explore the model’s\ngeneralization ability in the following section.\n3.3 Cross-Distribution Generalization\nIn this section, we analyse how models generalize\n(1) when test question distribution is different from\ntrain question while the test answer distribution\nis the same; (2) when test answer distribution is\ndifferent from the train answer while the test ques-\ntion distribution keeps the same; (3) when the test\nquestion distribution and test answer distribution\nare both different from train set. We have designed\ntesting data for different types of cross-distribution\non each task and list examples of the testing data\nin this section.\n3.3.1 Varying Questions\nIn this part, we mainly talk about when the test\nquestion distribution is different from the train\nquestion while the test answer distribution keeps\nthe same, how strong is the model’s generalization\nability. So, we use the Counting, Addition, Sub-\ntraction, Comparison, and Symbolic Logic tasks\nto analyse. For the Counting task, we use the in-\nstances whose character is not in letters of an al-\nphabet while the number is still of two-digit. For\nexample, the question is ‘@ @ @ @ @ @ @ @ @ @’\nand the answer is ‘10’. For the Addition task, we\nuse the instances whose at least one added num-\nber is of two-digit. But we make sure answers of\nselected equations are all of three-digit. For exam-\nple, the question is ‘50 + 170’, the answer is ‘220’.\nFor the Subtraction task, the situation is similar to\nthe Addition task, we use the instances whose at\nleast one number is of four-digit. But we make\nsure answers of selected instances are all of three-\ndigit. For example, the question is ‘1000 - 500’, the\nanswer is ‘500’. For the Comparison task, the situ-\nation is also similar, we use the instances whose at\nleast one number is of two-digit or four-digit. For\nexample, the question is ‘56 176’, the answer is\n‘<’. For the Symbolic Logic task, the situation is\nalso similar, we use the instances which has 1 - 5 or\n11 -15 basic ‘0’ and ‘1’ components. For example,\nthe question is ‘not 0 and 1 or 0’, the answer is\n‘1’.\n3.3.2 Varying Answers\nIn this part, we mainly talk about when the test an-\nswer distribution is different from the train answer\nwhile the test question distribution keeps the same,\nhow strong is the model’s generalization ability.\nAs a result, we use the Addition and Subtraction to\nanalyse.\nFor the Addition task, we use the instances\nwhose two numbers are of three-digit while the\nanswer is of four-digit. For example, the question\nis ‘500 + 600’, the answer is ‘1100’. For the Sub-\ntraction task, the situation is similar to the Addition\ntask, we use the instances whose two numbers are\nof three-digit while the answer is of two-digit. For\nexample, the question is ‘550 - 500’, the answer is\n‘50’.\n3.3.3 Varying Instances\nIn this part, we mainly talk about hen the test ques-\ntion distribution and test answer distribution are\nboth different from the train set, how strong is\nthe model’s generalization ability. So, we use the\nCounting, Addition and Subtraction tasks to anal-\nyse.\nFor the Counting task, we use the instances\nwhose character is not in letters of an alphabet\nand number is not of two-digit. For example, the\nquestion is ‘@ @ @ @ @ @ @ @ @’ and the answer\nis ‘9’. For the Addition task, we use the instances\nwhose at least one number in question is of two-\nor four-digit and the answer number is also of two-\nor four-digit. For example, the question is ‘50 +\n960’, the answer is ‘1010’. For the Subtraction\ntask, the situation is similar to the Addition task,\nwe use the instances whose at least one number is\nof two- or four-digit and the answer is also of two-\nor four-digit. For example, the question is ‘1100 -\n50’, the answer is ‘1050’.\n3.3.4 Analysis on Different\nCross-Distributions\nThe model’s performance on the test set of differ-\nent types of cross-distributions is shown in Table 2.\nFrom the table, we can see that although BART\nhas achieved 100% accuracy on the in-distribution\ntesting data, it fails to generalize on the cross-\ndistribution testing data of arithmetic reasoning\ntasks.\nResults of Counting and Symbolic Logic task on\ncross-distribution testing data are quite high. How-\never, for Counting task, all correct instances are the\ninstances which have different length but have the\nsame character distributions with the training data.\nIn addition, the cross-distribution testing data only\nhave length difference from the training data. Thus,\nwe can conclude that the model is not sensitive to\nthe length of question if the basic components does\nnot change. This conclusion is also consistent with\nthe result of (Clark et al., 2020). In addition, the\nresults show that the model is especially weak in\ngeneralizing to the instances with different answer\ndistributions.\nTo conclude, the model is still struggling on\ncross-distribution generalization, especially the car-\nrying and borrowing in Addition and Subtraction\ntasks.\nTask Question\nCross-Distribution\nAnswer\nCross-Distribution\nInstance\nCross-Distribution\nCounting 316/320(98.8%) / 0/1710\nAddition 15/1,500 (1.0%) 0/1,500 0/1,000\nSubtraction 13/1,500 (0.87%) 0/1,500 1/1,000 (0.1%)\nComparison 2,555/5,600(45.63%) / /\nSymbolic Logic 2,200/2,200 (100%) / /\nTable 2: The performance of BART on cross-distribution test set. For each task and different distribution type,\nwe select the model checkpoint which has achieved 100% accuracy/F1 on the corresponding in-distribution test\nset. Note that the random result on Comparison is around 49.9%. Data samples that models answer correctly\non Addition and Subtraction task in the Cross-Distribution experiment can be found in Appendix A[Some of the\nexamples here are deleted since they do not conform the fomat]\n3.4 Case Study on GPT-3\nGPT-3 (Brown et al., 2020) has received a lot of\nattention since it was born. And it has shown strong\nabilities on every single NLP task as well as on gen-\neralization. Thus, we also conduct some case study\nexperiments of arithmetic calculation on GPT-3\n2. We ﬁnd that GPT-3 can handle the addition\nand subtraction calculation with 1,000 perfectly,\nbut when the number increases, GPT-3 starts to\nlose its ability, it can only get some very speciﬁc\ninstances correctly. A interesting case is that it\ncan get correct result ‘9999999’ from ‘12345678 +\n87654321’, however, when we give it ‘12345678\n+ 8765432’, it still answers ‘99999999’. We guess\nthat the model does not have calculation ability,\nbut rather remembers some examples that have ap-\npeared before, since each calculation with 1000 and\n‘12345678 + 87654321’ may appear in the Internet\nfor many times while ‘12345678 + 8765432’ may\nnot so frequently appear.\n3.5 Overlap Analysis\nWe have also explored how overlaps inﬂuence mod-\nels’ performance.\nFollowing (Lewis et al., 2020b) and (Wang et al.,\n2021), if one test question appears in the questions\nof train set, we call it as question overlap, other-\nwise it is question non-overlap. Similarly, if one\ntest answer appears in the answers of train set, we\ncall it as answer overlap, otherwise it is answer\nnon-overlap. If one test instance is both question\noverlap and answer overlap, we call it is instance\noverlap, otherwise it is instance non-overlap.\nWe mainly use results of the Addition task to\n2The experiments are conducted on\nhttps://beta.openai.com/examples/default-qa . But since the\nOpenAI has not released the whole API, we cannot ﬁnetune\nthe model or do large scale experiments.\nillustrate this problem. However, for these two\ntask, the question overlap is sightly different, if the\ntwo numbers of one test question both appear in the\nnumbers of train set, we call it isquestion overlap,\notherwise it is question non-overlap. Since one\nanswer of these two tasks only contain one number,\nthe situation is same with the original deﬁnition.\nWe choose the two results which using 1920 in-\nstances (0.6% for the dataset) for training in the\nAddition task, because it has achieved 68% accu-\nracy, which means that the results have both correct\nand incorrect instances.\nThe results are shown in Table 3. From the ta-\nble, we can see that, unlike results from (Lewis\net al., 2020b) and (Wang et al., 2021), the over-\nlap and non-overlap do not inﬂuence the models’\nperformance.\n4 Related Work\nSome works have investigated in Mathematical\nproblems in NLP (Dua et al., 2019; Wang et al.,\n2017; Zhao et al., 2020). DROP (Dua et al., 2019)\nis a reading comprehension dataset comprising sev-\neral kinds of mathematical tasks, such as Subtrac-\ntion and Selection. However, all answers of its\nquestions can be directly or indirectly found in the\ncorresponding passages. Math23L (Wang et al.,\n2017) is simple math word problem dataset with\n23k problems. Its problem is of the simple En-\nglish context format, along with the equation and\nthe answer. Ape210K (Zhao et al., 2020) is a Chi-\nnese simple math word problem dataset with 210k\nquestions. The questions are similar to Math23L’s\nquestions. The data are taken from some elemen-\ntary school math word problems. These datasets do\nnot contain a generalization test set, the test set is in\nthe same distribution with the train set. In addition,\nCorrect Incorrect\nOverlap 1,083 524\nNon-\nOverlap 20,858 9,575\n(a) Instance Overlap\nCorrect Incorrect\nOverlap 4,538 1,846\nNon-\nOverlap 12,403 8,253\n(b) question Overlap\nCorrect Incorrect\nOverlap 6,066 3,070\nNon-\nOverlap 15,873 7,029\n(c) answer Overlap\nTable 3: The overlap analysis.\nthe often used methods for these datasets are ﬁrst to\npredict the equations or expression for the question\nand then to use calculation tool to get the result\n(Wang et al., 2017; Wangperawong, 2018). How-\never, our work concentrate on the generalization\nability of models. Thus, we have designed test set\nwith different distribution. In addition, we try to\nuse the model to directly solve the questions, aim-\ning test model’s internal ability of understanding\nthe deep rules of arithmetic and logical reasoning.\nSome works have researched on models’ the\ninternal ability of solving mathematical expres-\nsions. Wallace et al. (2019) has investigated that\nhow will different types of embedding, such as\nBERT (Devlin et al., 2019) and GloVe (Pennington\net al., 2014), affect the performance of the same\nNAQANet model (Dua et al., 2019) on the same\ntasks including List Maximum, Decoding and Ad-\ndition. Besides, Wallace et al. (2019) also explores\nthat how the the way numbers are represented and\nthe way to do tokenization affect the performance\nof models. Geva et al. (2020) try to inject numeri-\ncal reasoning skill by adding a calculation module\ninto the PLMs, which helps the performance on\nDROP (Dua et al., 2019) dataset.\nThere are also some works research focusing\non the generalization ability of neural network\nmodels. Lake and Baroni (2018) research on the\ncompositional generalization skills of sequence-\nto-sequence models, such as LSTM (Hochreiter\nand Schmidhuber, 1997) and GRU (Chung et al.,\n2014). Linzen (2020) explain that the generaliza-\ntion test in machine learning (ML) is not very rea-\nsonable, they put forward seven suggestions to bet-\nter evaluate the generalization ability of ML mod-\nels. Lewis et al. (2020b) and Wang et al. (2021) ﬁnd\nthat the PLMs cannot generalize well on Closed-\nbook QA task (Roberts et al., 2020), the model\ncan handle the test instances which overlap with\nthe train data, however, they cannot solve the non-\noverlapped instances. McCoy et al. (2020) ﬁnd that\neven when the model’s architecture is set, the gen-\neralization ability of the model is still inﬂuenced\nlargely by the random luck, the random initialized\nweights and other things. Clark et al. (2020) per-\nform Transformer-based models on simple logic\nreasoning test, and their results show that the model\ncan get quite promising results and the model is not\nsensitive to the question length. Though Wang et al.\n(2020a) proves that pretrained language models\n(Liu et al., 2019; Lan et al., 2020) can general-\nize well on textual commonsense reasoning tasks\n(Wang et al., 2019), Wang et al. (2020b) ﬁnds that\ntransformer models (Bosselut et al., 2019) may not\ngeneralize well on commonsense knowledge graph\n(Sap et al., 2019) reasoning. Zhang et al. (2020)\nanalyses the generalization ability on the relation\nextraction task and ﬁnd some speciﬁc problems can\ninduce a signiﬁcant decline in model performance.\n5 Conclusion\nWe have designed a series of tasks for evaluat-\ning BART on simple mathematical operations and\nlogic reasoning, which includes numbering, addi-\ntion, subtraction, comparison, and symbolic logic.\nWe constructed a corresponding in-distribution\ndatasets, and also designed cross-distribution test\nset to further evaluate the model’s generalization\nability. If the model can understand the under-\nlying rules of these mathematical operations and\nlogic reasoning, it can generalize well on both in-\ndistribution and cross-distribution test set. Our ex-\nperiments showed that BART can only generalize\non the in-distribution test set but cannot perform\nwell on the cross-distribution test set, showing that\nthe most advanced PLM still cannot understand the\nunderlying rules of simple mathematical operations\nand logic reasoning.\nReferences\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 4762–4779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nJ. Chung, C ¸ aglar G ¨ulc ¸ehre, Kyunghyun Cho, and\nYoshua Bengio. 2014. Empirical evaluation of gated\nrecurrent neural networks on sequence modeling.\nArXiv, abs/1412.3555.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\n2020. Transformers as soft reasoners over language.\nIn Proceedings of the Twenty-Ninth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI-\n20, pages 3882–3890. International Joint Confer-\nences on Artiﬁcial Intelligence Organization. Main\ntrack.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 946–958, Online. Association for Com-\nputational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nB. Lake and Marco Baroni. 2018. Generalization with-\nout systematicity: On the compositional skills of\nsequence-to-sequence recurrent networks. In ICML.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2020. Albert: A lite bert for self-supervised\nlearning of language representations. ArXiv,\nabs/1909.11942.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2020b. Question and answer test-train overlap in\nopen-domain question answering datasets.\nTal Linzen. 2020. How can we accelerate progress to-\nwards human-like linguistic generalization? In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5210–\n5217, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\nAndreas Madsen and alexander rosenberg johansen.\n2019. Measuring arithmetic extrapolation perfor-\nmance. ArXiv, abs/1910.01888.\nR. Thomas McCoy, Junghyun Min, and Tal Linzen.\n2020. BERTs of a feather do not generalize to-\ngether: Large variability in generalization across\nmodels with similar test set performance. In Pro-\nceedings of the Third BlackboxNLP Workshop on An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 217–227, Online. Association for Computa-\ntional Linguistics.\nRodrigo Nogueira, Zhiying Jiang, and J. Li. 2021. In-\nvestigating the limitations of the transformers with\nsimple arithmetic tasks. ArXiv, abs/2102.13019.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A. Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. ArXiv, abs/1811.00146.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\narXiv preprint arXiv:1409.3215.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undeﬁne-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’17, page 6000–6010, Red Hook, NY ,\nUSA. Curran Associates Inc.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know\nnumbers? probing numeracy in embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5307–\n5315, Hong Kong, China. Association for Computa-\ntional Linguistics.\nCunxiang Wang, Shuailong Liang, Yili Jin, Yi-\nlong Wang, Xiao-Dan Zhu, and Y . Zhang. 2020a.\nSemeval-2020 task 4: Commonsense validation and\nexplanation. In SEMEVAL.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiao-\nnan Li, and Tian Gao. 2019. Does it make sense?\nand why? a pilot study for sense making and ex-\nplanation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4020–4026, Florence, Italy. Association for\nComputational Linguistics.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book QA? In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3241–3251,\nOnline. Association for Computational Linguistics.\nCunxiang Wang, Jinhang Wu, Luxin Liu, and Yue\nZhang. 2020b. Commonsense knowledge graph rea-\nsoning by selection or generation? why? ArXiv,\nabs/2008.05925.\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 845–\n854, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nA. Wangperawong. 2018. Attending to math-\nematical language with transformers. ArXiv,\nabs/1812.02825.\nNingyu Zhang, Luoqiu Li, Shumin Deng, H. Yu,\nXu Cheng, W. Zhang, and Huajun Chen. 2020. Can\nﬁne-tuning pre-trained models lead to perfect nlp?\na study of the generalizability of relation extraction.\nArXiv, abs/2009.06206.\nWei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and\nJingming Liu. 2020. Ape210k: A large-scale and\ntemplate-rich dataset of math word problems.CoRR,\nabs/2009.11506.\nA Correct Cases of Cross-Distribution\nExperiments\nA.1 Subtraction\n1101-974=127\n1070-955=115\n1069-959=110\n1111-991=120\n190-11=179\n222-99=123\nA.2 Addition\n75+653=728\n77+849=926\n73+432=505\n42+668=710\n82+886=968\n80+874=954\nTask Training Data &\nIn-Distribution Test Cross-Distribution Test\nQuestion\nCross-\nDistribution\nAnswer\nCross-\nDistribution\nInstance\nCross-\nDistribution\nCounting\na sequence\nof [A-Z, a-z]\nwith a\nlength of\n10 to 99.\n(B B B\nB B B\nB B B B)\na sequence\nof special\ncharacters\nwith a\nlength of\n10 to 99\n(@ @ @\n@ @ @\n@ @ @ @)\na sequence\nof special\ncharacters\nwith a\nlength of\n1 to 9 or\n100 to 1000.\n(@ @)\nAddition\n3-digit\naddition.\n(100 + 200\n= 300)\nat least\none addend\nis 2 digits.\n(50 + 170\n= 220)\nthe answer\nis 4 digits.\n(500 + 600\n= 1100)\nat least one\nnumber is\n2 or 4 digits.\n(50 + 960\n= 1010)\nSubtraction\n3-digit\nsubtraction.\n(200 - 100\n= 100)\nat least\none number\nis 4 digits,\nbut the answer\nis still 3 digits.\n(1000 - 500\n= 500)\nthe answer\nis 2 digits.\n(550 - 500\n= 50)\nat least one\nnumber is\n2 or 4 digits.\n(1100 - 50\n= 1050)\nComparison\n3-digit\ncomparison.\n(100 <200)\nat least\none number\nis not 3-digit.\n(100 <2000)\nSymbolic\nLogic\nan equation\nconsists of\n6 to 10\n”0”s or ”1”s.\n(¬ 0 & 1 — 0\n& 1 — 1 — 0 is 1)\nan equation\nconsists of\n1 to 5\nor 11 to 15\n”0”s or ”1”s.\n(¬ 0 & 1 — 0 is 1)\nTable 4: Examples of training data, In-distribution test data, three kinds of cross-distribution tests. Note that the\ntraining data and in-distribution test share the same distribution.",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.8384095430374146
    },
    {
      "name": "Computer science",
      "score": 0.607704758644104
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5874055624008179
    },
    {
      "name": "Logical reasoning",
      "score": 0.5594454407691956
    },
    {
      "name": "Generative grammar",
      "score": 0.5500921607017517
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5189892649650574
    },
    {
      "name": "Test (biology)",
      "score": 0.5016965866088867
    },
    {
      "name": "Natural language processing",
      "score": 0.4915536344051361
    },
    {
      "name": "Distribution (mathematics)",
      "score": 0.46567508578300476
    },
    {
      "name": "Generative model",
      "score": 0.44435709714889526
    },
    {
      "name": "Arithmetic",
      "score": 0.4127247631549835
    },
    {
      "name": "Machine learning",
      "score": 0.365949809551239
    },
    {
      "name": "Mathematics",
      "score": 0.24605464935302734
    },
    {
      "name": "Programming language",
      "score": 0.18667805194854736
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": []
}