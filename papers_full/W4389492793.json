{
  "title": "A Large and Diverse Arabic Corpus for Language Modeling",
  "url": "https://openalex.org/W4389492793",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2228581002",
      "name": "Abbas Raza Ali",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2489882387",
      "name": "Muhammad Ajmal Siddiqui",
      "affiliations": [
        "Inception Institute of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5093450697",
      "name": "Rema Algunaibet",
      "affiliations": [
        "Saudi Aramco (Saudi Arabia)"
      ]
    },
    {
      "id": "https://openalex.org/A5105990460",
      "name": "Hasan Raza Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228581002",
      "name": "Abbas Raza Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2489882387",
      "name": "Muhammad Ajmal Siddiqui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093450697",
      "name": "Rema Algunaibet",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5105990460",
      "name": "Hasan Raza Ali",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6691249888",
    "https://openalex.org/W6755657607",
    "https://openalex.org/W6784433051",
    "https://openalex.org/W6767229672",
    "https://openalex.org/W3123572135",
    "https://openalex.org/W6656482109",
    "https://openalex.org/W6788007171",
    "https://openalex.org/W6754753029",
    "https://openalex.org/W6731628129",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6691892052",
    "https://openalex.org/W6732095968",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6775216148",
    "https://openalex.org/W6843858628",
    "https://openalex.org/W6767494904",
    "https://openalex.org/W6601894380",
    "https://openalex.org/W6788175385",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W6733303045",
    "https://openalex.org/W6785714105",
    "https://openalex.org/W6677481393",
    "https://openalex.org/W6763361438",
    "https://openalex.org/W6818723395",
    "https://openalex.org/W3209665139",
    "https://openalex.org/W6785649235",
    "https://openalex.org/W2585345551",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6770212971",
    "https://openalex.org/W6767705600",
    "https://openalex.org/W6731744468",
    "https://openalex.org/W3153642904",
    "https://openalex.org/W3106433641",
    "https://openalex.org/W4255562757",
    "https://openalex.org/W1986398087",
    "https://openalex.org/W2530385328",
    "https://openalex.org/W2117010802",
    "https://openalex.org/W1524281572",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3083568238",
    "https://openalex.org/W2892181857",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4240501285",
    "https://openalex.org/W2599674900",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2770803436",
    "https://openalex.org/W4256424337",
    "https://openalex.org/W2785383552",
    "https://openalex.org/W4297663785"
  ],
  "abstract": "Large Language Models (LLMs) have ushered in a major paradigm shift in Natural Language Processing (NLP), where large pre-trained Language models (LMs) have become a fundamental component of most NLP tasks. These models are intelligent enough to find relevant and meaningful representations of a language without any supervision. They are used to fine-tune typical NLP tasks with substantially higher precision than conventional shallow learning techniques. However, training these models requires a massively large corpus that adequately represents a language. Due to the availability of enormous corpora, English LLMs typically perform better than their counterparts. This effort focuses on the design and development of a large Arabic corpus. The corpus comprises over 500 GB of Arabic cleaned text, intended to improve cross-domain knowledge and downstream generalization capability of LLMs. The corpus was employed in the training of a large Arabic LLM. In order to assess the efficacy of the LLM, a variety of typical NLP tasks were fine-tuned. The fine-tuned tasks exhibited a significant boost in accuracy ranging between 4.5 and 8.5%, when compared to those downstreamed from multi-lingual BERT (mBERT). To the best of our knowledge, this is currently the largest clean and diverse Arabic corpus ever assembled.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9022365808486938
    },
    {
      "name": "Natural language processing",
      "score": 0.6981797218322754
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6793332695960999
    },
    {
      "name": "Arabic",
      "score": 0.5648548007011414
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4843686521053314
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.4764944911003113
    },
    {
      "name": "Language model",
      "score": 0.4684818983078003
    },
    {
      "name": "Generalization",
      "score": 0.4501502513885498
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.42383480072021484
    },
    {
      "name": "Linguistics",
      "score": 0.20824965834617615
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}