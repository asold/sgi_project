{
  "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior",
  "url": "https://openalex.org/W3106070274",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2099841324",
      "name": "Zi Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2611359558",
      "name": "Jeremiah Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100374689",
      "name": "Zi Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1852764121",
      "name": "Nan Hua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122007671",
      "name": "Dan Roth",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2889467719",
    "https://openalex.org/W2964294232",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2997710335",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2933374552",
    "https://openalex.org/W2963000224",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2774627531",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3000103182",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2732724430",
    "https://openalex.org/W2898631838",
    "https://openalex.org/W2976833415",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2529714286",
    "https://openalex.org/W2279098554",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2964236304",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4394643672",
    "https://openalex.org/W2963540976",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2900578291",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2963836885",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2963739978",
    "https://openalex.org/W4300167250",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3034560159",
    "https://openalex.org/W2964120007",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2785678896",
    "https://openalex.org/W2897507397",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2134797427",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2946794439"
  ],
  "abstract": "Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which penalizes an entire residual module in a Transformer model toward an identity mapping. Our method identifies and discards unimportant non-linear mappings in the residual connections by applying a thresholding operator on the function norm, and is applicable to any structured module including a single attention head, an entire attention blocks, or a feed-forward subnetwork. Furthermore, we introduce spectral normalization to stabilize the distribution of the post-activation values of the Transformer layers, further improving the pruning effectiveness of the proposed methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to demonstrate that SNIP achieves effective pruning results while maintaining comparable performance. Specifically, we improve the performance over the state-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 719–730\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n719\nPruning Redundant Mappings in Transformer Models\nvia Spectral-Normalized Identity Prior\nZi Lin∗\nGoogle Research\nlzi@google.com\nJeremiah Zhe Liu†\nGoogle Research & Harvard University\njereliu@google.com\nZi Yang\nGoogle Research\nziy@google.com\nNan Hua\nGoogle Research\nnhua@google.com\nDan Roth\nUniversity of Pennsylvania\ndanroth@seas.upenn.edu\nAbstract\nTraditional (unstructured) pruning methods for\na Transformer model focus on regularizing\nthe individual weights by penalizing them to-\nward zero. In this work, we explore spectral-\nnormalized identity priors(SNIP), a structured\npruning approach that penalizes an entire resid-\nual module in a Transformer model toward an\nidentity mapping. Our method identiﬁes and\ndiscards unimportant non-linear mappings in\nthe residual connections by applying a thresh-\nolding operator on the function norm. It is\napplicable to any structured module, includ-\ning a single attention head, an entire attention\nblock, or a feed-forward subnetwork. Further-\nmore, we introduce spectral normalizationto\nstabilize the distribution of the post-activation\nvalues of the Transformer layers, further im-\nproving the pruning effectiveness of the pro-\nposed methodology. We conduct experiments\nwith BERT on 5 GLUE benchmark tasks to\ndemonstrate that SNIP achieves effective prun-\ning results while maintaining comparable per-\nformance. Speciﬁcally, we improve the perfor-\nmance over the state-of-the-art by 0.5 to 1.0%\non average at 50% compression ratio.\n1 Introduction\nNatural Language Processing (NLP) has recently\nachieved great success by using the Transformer-\nbased pre-trained models (Radford et al., 2019;\nDevlin et al., 2018; Yang et al., 2019; Clark et al.,\n2020). However, these models often consume con-\nsiderable storage, memory bandwidth, and compu-\ntational resource. To reduce the model size and in-\ncrease the inference throughput, compression tech-\nniques such as knowledge distillation (Sanh et al.,\n2019; Sun et al., 2019; Tang et al., 2019; Jiao et al.,\n2019; Sun et al., 2020) and weight pruning (Guo\n∗Work done as part of the Google AI Residency.\n†Work done at Google Research.\n20 40 60 80 100\nRemaining weights %\n70\n75\n80\n85Accuracy\nMNLI-mm\nSNIP (Ours)\nIterative Pruning\nDistilBERT\nBERT-PKD\nFigure 1: Comparison of selected knowledge distil-\nlation methods (DistilBERT (Sanh et al., 2019) and\nBERT-PKD (Sun et al., 2019)) and iterative pruning\nmethods (Iterative Pruning (Guo et al., 2019) and our\nproposed method) in terms of accuracy at various com-\npression rate using MNLI test set. knowledge distil-\nlation methods require re-distillation from the teacher\nto get each single data point, whereas iterative pruning\nmethods can produce continuous curves at once.\net al., 2019; Wang et al., 2019; Gordon et al., 2020;\nSanh et al., 2020) have recently been developed.\nKnowledge distillation methods require the spec-\niﬁcation of a student network with a smaller archi-\ntecture, which often has to be identiﬁed by a tedious\nsequence process of trial-and-error based decisions.\nBy comparison, the iterative pruning methods grad-\nually prune the redundant model weights or layers\nfrom the full-size model, and provide a full picture\nof the trade-off between the task performance and\nthe model size with a single training process, as\nillustrated in Figure 1. This allows the iterative\npruning methods to easily determine the most com-\npact architecture given a required level of model\nperformance.\nHowever, many of the existing pruning methods\nrely on classic regularizers that act on the individ-\nual weights by penalizing them to zero (Guo et al.,\n2019; Sanh et al., 2020). As a result, the pruned\n720\nmodel tends to maintain the same architecture as\nthe original model despite the reduced parameter\ncount, which does not practically lead to an im-\nprovement in inference latency (Wen et al., 2016).\nThis leads to a question: is it possible to perform\nmore structured pruning on a Transformer model to\nmodify its model architecture (e.g., reducing width\nand depth)?\nTo this end, we notice that many previous works\nhave suggested that the learned Transformer mod-\nels often have much redundancy (Tenney et al.,\n2019; Liu et al., 2019a; Jawahar et al., 2019; Koval-\neva et al., 2019). For example, Michel et al. (2019);\nV oita et al. (2019) found that most of the attention\nheads in a Transformer model can be removed with-\nout signiﬁcantly impacting accuracy, and Tenney\net al. (2019) found that while the earlier layers and\nthe later layers in a BERT model play clear roles\nin extracting either low-level or task-speciﬁc lin-\nguistic knowledge, the roles of the intermediate\nlayers are less important. These observations have\nmotivated the idea that Transformer models may\nexhibit considerable structural redundancy- i.e.,\nsome layers can be removed during the training\nwithout harming the ﬁnal performance.\nIn this paper, we propose a structured pruning\nmethod that reduces the architecture of a Trans-\nformer by directly targeting its sub-modules as a\nwhole - for example, a single attention head, an\nattention module, a feed-forward subnetwork, etc.\nWe take an approach we call spectral-normalized\nidentity prior(SNIP), which imposes a function-\nlevel prior centered around the identity function on\nthe aforementioned modules.\nSpeciﬁcally, we take the advantage of the resid-\nual blocks ( F(x) + x) within a Transformer\nlayer and compress them to strict identity map-\npings (Yu et al., 2018) by identifying the residual\nblocks whose nonlinear mapping’s absolute values\n(|F(x)|) mostly fall below a threshold ϵ. With this\nstrategy, the weights of the Transformer model can\nstill be under-regularized when using simple L1\nor L2 based regularizers, which cause the distribu-\ntion of the post-activation values prone to be noisy\neven after layer normalization (Ba et al., 2016).\nTo address this issue, we further leveragespectral\nnormalization (Miyato et al., 2018) to stabilize the\ndistribution of the post-activation values by reg-\nularizing the largest singular value of the weight\nmatrices.\nWe use BERT (Devlin et al., 2018) as a case\nstudy in this paper. Across multuple tasks in the\nGLUE benchmark (Wang et al., 2018), SNIP im-\nproves the performance over the state-of-the-art by\n0.5 to 1.0% on average at 50% compression ratio.\n1 We also show that spectral normalization results\nin more sparse and regulated layer mappings dur-\ning pre-training. We compare the remaining model\ncomponents across the tasks at a ﬁxed compres-\nsion ratio in an ablation study, and show that the\nremaining components are similar but not identical.\nOur contributions are three-fold: First, we intro-\nduce identity-inducing prior, a structured pruning\napproach that imposes identity-inducing regular-\nization on the Transformer mappings as a whole\nrather than its individual weights. Second, we\nshow that through a novel combination with the\nspectral normalization regularization, the result-\ning spectral-normalized identity prior(SNIP) leads\nto well-regularized weight distribution and sparse\nlayer mappings in a BERT model. Finally, we con-\nduct thorough experiments to validate the SNIP\napproach over 5 standard NLU tasks. Our re-\nsults suggest that different model components in\na Transformer play critically different roles across\ntasks, suggesting the importance of performing\ntask-speciﬁc pruning to obtain an architecture that\nis the most suitable for the target task.\n2 Related Work\nPre-trained Language Model Compression\nThe major existing efforts to compress pre-trained\nlanguage models such as BERT include knowledge\ndistillation (Ba and Caruana, 2014; Hinton et al.,\n2015) and pruning (Iandola et al., 2016; Veit and\nBelongie, 2017).\nThe knowledge distillation approach enables the\ntransfer of knowledge from a large teacher model\nto a smaller student model. Such attempts have\nbeen made to distill BERT models, e.g., Distil-\nBERT (Sanh et al., 2019), BERT-PKD (Sun et al.,\n2019), Distilled BiLSTM (Tang et al., 2019), Tiny-\nBERT (Jiao et al., 2019), MobileBERT (Sun et al.,\n2020), etc. All of these methods require carefully\ndesigning the student architecture. Furthermore,\nto choose which intermediate results that the stu-\ndent model can learn from, e.g., the outputs of each\nlayer, the attention maps, is still under discussion.\nSimilar to other pruning-based methods, our\n1Open-source code can be found at https://github.\ncom/google-research/google-research/\ntree/master/snip\n721\nmethod can iteratively remove the least important\nweights or connections, explore the full spectrum\nof trade-offs, and ﬁnd the best affordable architec-\nture in one shot.\nMany language representation model pruning\nmethods focus on individual components of the\nweight matrices. For example, Guo et al. (2019)\nintegrates reweighted L1 minimization with a prox-\nimal algorithm to search sparsity patterns in the\nmodel; Gordon et al. (2020) uses magnitude weight\npruning, which compresses the model by remov-\ning weights close to 0; Sanh et al. (2020) applies\ndeterministic ﬁrst-order weight pruning method\nwhere both weights with low and high values can\nbe pruned. A very few works try structured weight\npruning, e.g., Wang et al. (2019) proposes a struc-\ntured pruning approach based on low-rank factor-\nization and augmented Lagrangian L0 norm reg-\nularization. On the other hand, there also exist\nworks that prune a coherent set of sub-modules\nin the Transformer model. For example, Michel\net al. (2019) and V oita et al. (2019) propose to\nprune individual attention heads either manually\nvia head importance score, or automatically via a\nrelaxed L0 regularization. Fan et al. (2020) applies\nrandom pruning to the entire layers. In contrast,\nour method allows ﬁner-grained structured pruning\non Transformer modules (i.e., both attention heads\nand feed-forward layers) and propose to improve\nthe mathematical property of a Transformer (i.e.,\nLipschitz condition) for more effective pruning.\nOther compression approaches include weight\nsharing (Liu et al., 2019b), quantization (Zafrir\net al., 2019; Shen et al., 2019) and neural architec-\nture search (Chen et al., 2020), but are not within\nthe discussion of this paper. We refer interested\nreaders to Ganesh et al. (2020) for further details.\nApplications of Spectral Normalization Spec-\ntral normalization is ﬁrst proposed for generative\nadversarial network (GAN) as a regularization tech-\nnique to stabilize the discriminator training (Miyato\net al., 2018). It was later applied to improve the\nperformance of the other types of generative neu-\nral networks (Zhao et al., 2018; Behrmann et al.,\n2019), and was analyzed theoretically in the con-\ntext of adversarial robustness and generalization\n(Farnia et al., 2018).\nSpectral normalization regularizes the Lipschitz\ncondition of the model mappings and is known to\nbeneﬁt model generalization under both the classic\nand the adversarial settings (Sokoli ´c et al., 2017;\nCisse et al., 2017; Oberman and Calder, 2018;\nNeyshabur et al., 2017). In this paper, we will\nexplore the beneﬁt of spectral regularization for\nimproving the effectiveness of pruning.\n3 Methods\nIn this section, we ﬁrst brieﬂy review the basic\nTransformer layers in Vaswani et al. (2017) (3.1).\nWe then introduce our identity prior into Trans-\nformer’s residual connections using ϵ threshold\n(3.2). In section 3.3, we give mathematical founda-\ntions to the spectral normalization and show how\nit could help with our identity prior. Finally, to put\nit all together, we establish our structured iterative\npruning methods for BERT ﬁne-tuning (3.4).\n3.1 Background: Transformer Layer\nTransformer-based models are usually comprised\nof a stack of Transformer layers. A Transformer\nlayer takes on a sequence of vectors as input, ﬁrst\npasses it through a (multi-head) self-attention sub-\nlayer, followed by a position-wise feed-forward\nnetwork sub-layer.\nSelf-attention sub-layer The attention mecha-\nnism can be formulated as querying a dictionary\nwith key-value pairs. Formally,\nAtt(Q,K,V ) =softmax(QKT/\n√\ndH) ·V\nwhere dH is the dimension of the hidden represen-\ntations. Q, K, and V represent query, key, and\nvalue. The multi-head variant of attention (MHA)\nallows the model to jointly attend to information\nfrom different representation sub-spaces, deﬁned\nas\nMHA(Q,K,V ) = [head1,..., headA]WO\nheadk = Att(QWQ\nk ,KW K\nk ,VW V\nk )\nwhere [·,·] is the concatenation operator, WQ\nk ∈\nRdH×dK , WK\nk ∈RdH×dK , WV\nk ∈RdH×dV ,and\nWO ∈RHdV ×dH are projection parameter matri-\nces, Ais the number of heads, and dK and dV are\nthe dimensions of key and value.\nPosition-wise FFN sub-layer In addition to the\nself-attention sub-layer, each Transformer layer\nalso contains a fully connected feed-forward net-\nwork, which is applied to each position separately\nand identically. This feed-forward network consists\nof two linear transformations with an activation\nfunction σ in between. Specially, given vectors\n722\nx1,..., xn, a position-wise FFN sub-layer trans-\nforms each xias FFN(xi) =σ(xiW1+b1)W2+b2,\nwhere W1,W2,b1 and b2 are parameters.\nWe should also emphasize that a residual con-\nnection (He et al., 2016b) and a layer normalization\n(Ba et al., 2016) are applied to the output of both\nMHA or FNN sub-layers. The residual connection\nplays a key role in learning strict identity mapping\n(detailed in Section 3.2), while layer normalization\nand spectral normalization (detailed in Section 3.3)\ntogether ensure regulated magnitude of activation\noutputs for improved pruning stability.\n3.2 Identity-inducing Prior for Transformer\nThe design of residual connection can provide us\nwith a promising way to ﬁnd identity mappings\nfor the Transformer model. Speciﬁcally, residual\nconnection (He et al., 2016b) can be formalized\nas H(x) = F(x) +x, where Fcould be either\nMHA or FFN and His the sub-layer output. As\nillustrated in He et al. (2016a), if an identity map-\nping is optimal, it is easier to push the residual to\nzero than to ﬁt an identity mapping by a stack of\ntraditional non-linear layers.\nWe leverageϵ-ResNet (Yu et al., 2018), a strict\nidentity mapping mechanism that sparsiﬁes the\nlayer output by inducing a speciﬁc threshold ϵas\nthe identity prior. Speciﬁcally, we turn the residual\nconnection H(x) into Sϵ(F(x)) +x, where\nSϵ(v) =\n{ 0 if |vi|<ϵ, ∀i∈1,..., |v|,\nv otherwise,\nwhere vi is the i-th element of the vector v. Here\na sparsity-promoting function Sis applied to dy-\nnamically discard the non-linearity term based on\nthe activations. When all the responses in the non-\nlinear mapping F(x) is below a threshold ϵ, then\nS(F(x)) = 0, otherwise, the original mapping\nS(F(x)) =F(x) was used as the standard resid-\nual network.\nTo implement S, we put an extra binary gate\nlayer tϵ upon F(x) by stacking additional recti-\nﬁed linear units (ReLU), following Srivastava et al.\n(2015). In particular,\ntϵ(v) = 1−ReLU(1 −L max\n1≤i≤|v|\nReLU(|vi|−ϵ))\nwhere L refers to a very large positive constant\n(e.g., 1e5 in our experiments). Then, Sϵ(F(v)) has\nthe following form:\nSϵ(F(x)) =tϵ(F(x))F(x)\nRecall that each layer of Transformer consists\nof two residual blocks, namely, the self-attention\nsub-layer and the position-wise FFN sub-layer. We\napply the ϵnetwork directly to the residual block\nin the FFN sub-layer, i.e.,\nHFFN(x) =SϵFFN (FFN(x)) +x,\nWhen applying it to the attention sub-layer, we\nplace Sto each single attention head, which allows\nus to prune any subset of attention heads, i.e.,\nHATT(x) =\nA∑\ni=1\nSϵATT(headiWO\ni ) +x\nHere, WO\ni is the output weight assign to the i-th\nattention head, i.e., WO = [WO\nI ,WO\n2 ,...,W O\nA].\nIf SϵATT(headiWO\ni ) = 0, this means that the i-th\nattention head does not contribute to the output of\nthe attention layer and thus could be pruned out.\nIn our experiment, we set different values to\nϵATT and ϵFFN, since the absolute outputs of at-\ntention and FFN layers lay in different scalars, as\nillustrated in Figure 2.\n3.3 Spectral Normalization\nThe sparsity-inducing function Sϵ(F(x)) in the ϵ-\nResNet has been found to work well for randomly\ninitialized neural network, where the initial weight\nmatrix of the non-linear mappings for all layers\nwas distributed within a consistent range, and thus\nfacilitates a natural separation between the func-\ntion norms |F|of the important and unimportant\nnon-linear mappings in the residual blocks during\ntraining (Yu et al., 2018). This is, however, not the\ncase for the weight distribution of a pre-trainined\nmodel like BERT, where the weight distributions\nbetween different layers have already diverged dur-\ning pre-training, which is likely due to the special-\nization of layer functionalities under the masked\nlanguage modeling (MLM) training (Tenney et al.,\n2019).\nIndeed, in our preliminary experiments, we ob-\nserved that the proposed identity-inducing prior ϵ\nis not effective for a BERT model initialized from a\nclassic pre-training checkpoint. As shown in Table\n1, we found the distribution of the function norms\nP(|F(x)i|) for the attention layers to be densely\nclustered within a small range ((0,2)) and with no\nclear separation between the function norm for the\nimportant and unimportant non-linear residual map-\npings. On the other hand, the norm distributions\n723\n4\n 3\n 2\n 1\n 0 1 2\nthe log of max absolute value of attention outputs\n0\n100\n101\nw/o Spectral Normalization (SN)\nlayer_0\nlayer_1\nlayer_2\nlayer_3\nlayer_4\nlayer_5\nlayer_6\nlayer_7\nlayer_8\nlayer_9\nlayer_10\nlayer_11\n2 3 4 5 6 7\nthe log of max absolute value of FFN outputs\n0\n100\n101\nw/o Spectral Normalization (SN)\nlayer_0\nlayer_1\nlayer_2\nlayer_3\nlayer_4\nlayer_5\nlayer_6\nlayer_7\nlayer_8\nlayer_9\nlayer_10\nlayer_11\n3\n 2\n 1\n 0 1 2 3 4\nthe log of max absolute value of attention outputs\n0\n100\n101\n102\n103\nw/ Spectral Normalization (SN)\nlayer_0\nlayer_1\nlayer_2\nlayer_3\nlayer_4\nlayer_5\nlayer_6\nlayer_7\nlayer_8\nlayer_9\nlayer_10\nlayer_11\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5\nthe log of max absolute value of FFN outputs\n0\n100\n101\n102\n103\nw/ Spectral Normalization (SN)\nlayer_0\nlayer_1\nlayer_2\nlayer_3\nlayer_4\nlayer_5\nlayer_6\nlayer_7\nlayer_8\nlayer_9\nlayer_10\nlayer_11\nFigure 2: Distribution of the max absolute value of the attention/FFN outputs on the Stanford Sentiment Treebank\n(SST) training set, based on model BERTBASE (w/o SN) and spectral normalized BERTBASE (w/ SN). For attention\nlayers, different heads are plotted separately but with the same color in the same layer for clarity. After SN, the\nnorm distribution P(|Fk(x)|) shows a better separation (bottom v.s. top), and the norms distribution for the FFN\nlayers are stabilized within a much smaller range (bottom right v.s. top right).\nSN MLM SST-2 QQP MRPC QNLI MNLI\nw/o 71.93 92.7 90.6 90.9 91.6 84.7\nw/ 74.16 91.9 90.1 90.8 91.6 85.1\nTable 1: Evaluate the performance of BERT BASE (w/o\nSN) and the spectral normalized BERTBASE (w/ SN, us-\ning λ(W) = 5), respectively masked language model-\ning accuracy on pre-training data and accuracy of ﬁne-\ntuning on 5 natural language understanding tasks (de-\ntails could be found in Section 4.1).\nfor different FFN layers were found to vary wildly,\ncreating challenges for selecting a proper set of ϵ’s\nin practice.\nThe above observations motivate us to identify\nan effective method to stabilize the norm distribu-\ntions of BERT model layers. In this work, we con-\nsider spectral normalization(SN), an approach that\ndirectly controls the Lipschitz norm of a non-linear\nmapping Fby regularizing the spectral behavior\nof its weight matrices (Miyato et al., 2018).\nSpeciﬁcally, for a weight matrix W, its spectral\nnorm λ(W) is deﬁned as its largest singular value:\nλ(W) =max\nx̸=0\n||Wx||2\n||x||2\n.\nWe say a function Fis L-Lipschitz if |F(x1) −\nF(x2)|/||x1 −x2||≤ L, for all possible (x1,x2)\npairs from the feature space, and we call the small-\nest possible Lthe Lipschitz norm of F, denoted as\n|F|Lip. Consequently, for a neural network map-\nping F(x) =σ(Wx + b) with an contractive ac-\ntivation function σ, its Lipschitz norm is upper-\nbounded by λ(W) (Miyato et al., 2018):\n|F|Lip = |σ(Wx+b)|Lip ≤|Wx+b|Lip ≤λ(W).\nFor BERT models, since the layer input x fol-\nlows a distribution of zero mean and unit variance\ndue to layer normalization (Ba et al., 2016), a non-\nlinear mapping’sL1 norm |F|is roughly propor-\ntional to its Lipschitz norm |F|Lip, which is con-\ntrolled by λ(W). Therefore, we are able to have a\nbetter control of the maximum of |F(x)|for iden-\ntifying a good ϵ. Furthermore, the regularization\nis achieved by that the layer weights are simply\ndivided by their corresponding spectral norm in\nSN, i.e., ˆW = W/ˆλ(W), adding no additional\ntrainable parameter to the original model.\nWe apply SN to both pre-training and ﬁne-tuning\nof the BERT model, and on the weights in both the\nattention and the FFN layers. As shown in Table\n1, compared to the original BERTBASE without SN,\nadding SN to a BERTBASE model has resulted in im-\nproved pre-training performance and competitive\nﬁne-tuning performance. To illustrate the effect\n724\n0 20 40 60 80 100\nPercentage pruned\n70\n75\n80\n85\n90\n95Accuracy\nEvolution of accuracy on SST\nATT\nATT*\nFFN\nFFN*\nJoint\nJoint*\n0 20 40 60 80 100\nPercentage pruned\n65\n70\n75\n80\n85\n90Accuracy\nEvolution of accuracy on QQP\nATT\nATT*\nFFN\nFFN*\nJoint\nJoint*\n0 20 40 60 80 100\nPercentage pruned\n0\n20\n40\n60\n80F1\nEvolution of F1 on MRPC\nATT\nATT*\nFFN\nFFN*\nJoint\nJoint*\n0 20 40 60 80 100\nPercentage pruned\n50\n60\n70\n80\n90Accuracy\nEvolution of accuracy on QNLI\nATT\nATT*\nFFN\nFFN*\nJoint\nJoint*\n0 20 40 60 80 100\nPercentage pruned\n30\n40\n50\n60\n70\n80Accuracy\nEvolution of accuracy on MNLI-mm\nATT\nATT*\nFFN\nFFN*\nJoint\nJoint*\nFigure 3: Evolution of performance when separately pruning attention heads (ATT) and FFN layers (FFN), and\njoint pruning ATT and FFN (Joint) on 5 GLUE dev sets. * means applying spectral normalization. The percentage\npruned (x-axis) is calculated based on the parameters of attention and FFN sub-layers as the full set (excluding\nembedding, pooler and classiﬁer layers). Joint pruning with spectral normalization (Joint*) could prune more\nparameters than the original BERT (Joint) to preserve the same accuracy.\nof SN on the distribution of function norms of the\nBERT model, we plot |F(x)|for the training set of\nthe Stanford Sentiment Treebank in Figure 2. As\nshown, after SN, the norm distribution P(|Fk(x)|)\nbetween the important and the unimportant nonlin-\near mappings shows a better separation (Figure 2,\nbottom), the norms distribution for the FFN layers\nare now stabilized within a much smaller range\n(Figure 2, bottom right).\n3.4 Structured Iterative Pruning\nWe use a simple pruning method that greedily and\niteratively prunes away attention heads and FFN\nlayers to avoid impractical combinatorial search,\nwhere two dynamic estimations are conducted forϵ\nand model architecture respectively. One iteration\ncontains four substeps:\n1. Estimate ϵgiven current model architecture\nand training data. Speciﬁcally, We sort the\nattention heads and FFN layers by their mean\nactivation outputs, and set ϵto the k-th small-\nest mean activation. Larger kleads to more\nmappings being pruned in one iteration, which\nmakes the retraining more difﬁcult to recover\nthe performance, but leads to fewer pruning\niterations.\n2. Train the model with identity-inducing prior\nby using the selected ϵ.\n3. Estimate a smaller architecture given current\nϵand training data. Speciﬁcally, we estimate\nthe module usage by counting the number of\ntimes each residual block has been learned to\nbecome a strict identity mapping across mini-\nbatches in the training set. We prune residual\nblocks whose usage rate below a threshold\nθ. When a residual block produces a negli-\ngible response, the ϵfunction will start pro-\nducing 0 outputs. As a result, the weights in\nthis block will stop contributing to the cross-\nentropy term. Consequently, the gradients will\nonly be based on the regularization term and\nlead to weight collapse.\n4. Retrain the model with the pruned residual\nblocks completely removed from the architec-\nture. This is critical — if the pruned network\nis used without retraining, accuracy is signif-\nicantly impacted. Also, during retraining, it\nis better to retain the weights from the initial\ntraining phase for the connections that sur-\nvived pruning than it is to re-initialize them\n(Han et al., 2015).\n4 Experiments\n4.1 Experimental Settings\nIn the experiments, we apply the same architecture\nand the base settings from the original BERTBASE\n725\nModel FLOPS SST-2 acc QQPacc MRPCF1 QNLIacc MNLI-mmacc\nBERTBASE 22.5B -0.0%/92.7 -0.0%/90.6 -0.0%/90.9 -0.0%/91.6 -0.0%/84.7\nTinyBERT6 w/o DA (Jiao et al., 2019)2 1.2B - - -50.0%/86.0 - -50.0%/84.4\nDistilBERT6 (Sanh et al., 2019) 11.3B -50.0%/91.3 -50.0%/88.5 -50.0%/87.5 -50.0%/89.2 -50.0%/82.2\nBERT-PKD6 (Sun et al., 2019) 11.3B -50.0%/91.5 -50.0%/88.9-50.0%/86.2 -50.0%/89.0 -50.0%/81.0\nBERT-PKD3 (Sun et al., 2019) 7.6B -75.0%/87.5 -75.0%/87.8 -75.0%/80.7 -75.0%/84.7 -75.0%/76.3\nFLOP (Wang et al., 2019) 15B -35.0%/92.1 - -35.0%/88.6 -35.0%/89.0 -\nMvP (Sanh et al., 2020)† N/A - -97.0%/89.2 - - -97.0%/79.7\nSNIP (w/ SN) 13.2 - 14.5B -50.0%/91.8 -50.0%/88.9 -50.0%/88.1 -50.0%/89.5-50.0%/82.8\nSNIP (w/ SN) 8.3 - 9.1B -75.0%/88.4 -75.0%/87.8 -75.0%/81.2 -75.0%/84.6 -75.0%/78.3\nSNIP (w/o SN) 16.8 - 18.2B -30.0%/91.3 -38.7%/89.5 -39.7%/89.9 -26.1%/90.8 -32.3%/83.5\nSNIP (w/ SN) 13.4 - 16.1B -56.7%/91.7 -40.7%/89.7 -46.7%/89.9 -36.0%/90.7 -39.3%/83.9\nTable 2: The compression results including model efﬁciency (percentage of reduced parameters) and performance\nfrom the GLUE dev results, and the MNLI result is evaluate for mismatched-accuracy (MNLI-mm). BERT BASE\nindicates the results of the ﬁne-tuned BERTBASE in our implementation. The number of model parameters includes\nthe attention and FFN sub-layers but excludes the embedding, pooler and classiﬁer layers. The bold numbers\nindicate the best performance for keeping 50% of the parameters respectively. Rows 1–5 are knowledge distillation\nmethods, and Rows 6 and 7 are pruning methods, where †means unstructured pruning 3.\n(Devlin et al., 2018), and ﬁne-tune each task in-\ndependently. More details could be found in Ap-\npendix.\nFor pre-training, we use the same data as\nBERT, which consists of 3.3 Billion tokens from\nWikipedia and BooksCorpus (Zhu et al., 2015).\nSimilar to the standard BERT practice, we conduct\nthe pre-training only once and from scratch (i.e., no\nsecond pre-training). We use dynamic token mask-\ning with the masked positions decided on-the-ﬂy\ninstead of during preprocessing. Also, we did not\nuse the next sentence prediction objective proposed\nin the original BERT paper, as recent work has sug-\ngested it dost not improve the scores (Yang et al.,\n2019; Liu et al., 2019b).\nFor ﬁne-tuning tasks, we focus on the Gen-\neral Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2018) in the main text\nsince it is a thoroughly studied setting in many\npruning/distillation work, thereby allowing com-\nprehensive comparison. We conduct experiments\non the subset of GLUE , classiﬁed into three cate-\ngories:\n1. Sentiment analysis: Stanford Sentiment Tree-\nbank (SST) (Socher et al., 2013);\n2. Paraphrasing: Quora Question Pairs (QQP)\n(Chen et al., 2018) and Microsoft Research\nParaphrase Corpus (MRPC) (Dolan and\nBrockett, 2005);\n3. Natural language inference: Question Natu-\nral Language Inference (QNLI) (Chen et al.,\n2018), and Multi-genre Natural Language In-\nference (MNLI) (Williams et al., 2017).\nThe detailed description of downstream tasks could\nbe found in Appendix. The reason for choosing this\nsubset is that we found the variance in performance\nfor those tasks lower than the other GLUE tasks.\nWe used the hyperparameters from Clark et al.\n(2020) for the most part. Since we run the training\niteratively, we set train epoch as 1 for most of the\ntask but 3 for task MRPC consider that the size of\nthe datasets is much smaller than other tasks.\nFor ϵand architecture estimations in Section 3.4,\nwe set k= 1and θ= 0.95 in our experiment.\n4.2 Results\nWe compare the pruning results on non-normalized\nand normalized BERT models on the 5 GLUE tasks,\nas shown in Figure 3, which includes separate prun-\ning for attention heads and FFN sub-layers and\njoint pruning of both modules. The results demon-\nstrate the advantage of spectral normalization.\nTo put it all together, in Table 2, we further show\nthe simplest architecture we could get when al-\nlowed at most 1% in terms of performance degra-\ndation. This also means that further slight pruning\nwill have a noticeable impact on the ﬁnal results.\nWe ﬁnd that spectral normalization can lead to a\nbetter trade-off between parameter size and per-\nformance. Speciﬁcally, for the same ideal perfor-\nmance shown in the last two rows in Table 2, spec-\n2TinyBERT (Jiao et al., 2019) utilizes data augmentation\n(DA), which makes it unfair to have a direct comparison. We\nonly listed their results without DA.\n3We do not list the results in Guo et al. (2019); Gordon\net al. (2020) since they show the results in scatter plots instead\nof the exact numbers, and similar to MvP (Sanh et al., 2020),\nthey are of unstructured pruning, which could not speed up\nthe inference in practice.\n726\nSST MRPC MNLI\n16.67%\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn33.33%\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn50.00%\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn\n0 1 2 3 4 5 6 7 8 9 10 11\nheads\n01234567891011\nlayers\natt meanffn\nFigure 4: Pruning map of attention heads ( att) and FFN layers ( ffn) during pruning on task SST-2, MRPC\nand MNLI dev sets. mean is the mean of attention heads of each layers. The left column shows the percentage\nof pruned parameters. Color white indicates that this attention head or FFN layer has been entirely pruned. For\ndifferent tasks, the distribution of important attention heads is similar, but not identical, and certain FFN sub-layers\nmight be more amenable to compression, even if the corresponding self-attention sub-unit is not.\ntral normalized BERT could on average be pruned\n12% parameters more than the original BERT.\nWe also list other compression methods in the\ntable for comparison while most of them are of\nknowledge distillation and have a pre-deﬁned ﬁxed\nsize of the compressed model. Our methods pro-\nvide the ﬂexibility to choose the best architecture\nand take advantage of ﬁnding the inﬂection point\nduring pruning, while compared with other pruning\nmethods we could practically speed up the infer-\nence time since we use a structured pruning.\n4.3 Analysis\nIn this section, we investigate the contribution of:\n(1) single attention head pruning, and (2) split prun-\ning for attention heads and FFN sub-layers.\nSingle Head Attention Pruning Multi-head\nself-attention is a key component of Transformer,\nwhere each attention head potentially focuses on\ndifferent parts of the inputs.\nThe analysis of multi-head attention and its im-\nportance is challenging. Previous analysis of multi-\nhead attention considered the average of attention\nweights over all heads at given position or focused\nonly on the maximum attention weights (V oita\net al., 2018; Tang et al., 2018), or explicitly takes\ninto account the varying importance of different\nheads (V oita et al., 2019). Michel et al. (2019) has\nproved that attention heads can be removed with-\nout signiﬁcantly impacting performance, but they\nmainly focus on machine translation and NLI.\nTo understand whether and to what extent atten-\ntion heads play consistent and interpretable roles\nwhen trained on different downstream tasks, we\n727\n0 20 40 60 80 100\nPercentage pruned\n50\n60\n70\n80\n90Accuracy\nEvolution of Accuracy on SST\nHA\nMHA\nLayer pruning\nSeparate pruning\nFigure 5: Evolution of accuracy on the dev set of SST-\n2 for (1) pruning the whole attention layers (HA); (2)\npruning single attention heads (MHA); (3) pruning the\nwhole layer (Layer pruning) and (4) pruning the atten-\ntion and FFN sub-layers separately (Separate pruning).\npick one task from the sentiment analysis, para-\nphrasing and natural language inference respec-\ntively, plot the pruned attention heads during train-\ning and show the dynamic process in Figure 4. We\ncan ﬁnd that though for different tasks, the distri-\nbutions of important attention heads are similar, as\nreﬂected in the mean of each attention layer, the\nexact pruned attention heads are actually different.\nThis also indicates that splitting the pruning for\nattention heads could have more ﬂexibility for the\nmodel to ﬁnd an optimal architecture.\nSeparate Pruning for Attention and FFNDe-\ncoupling and then individually studying the self-\nattention and the FFN sub-layers is important for\nunderstanding the progressive improvement they\nprovide. As can be observed from Figure 3, for\nmost of the tasks, pruning FFN layers damages\nthe performance more than the attention layers,\nindicating that the compression technique for the\nTransformer model tends to be more effective on\nthe attention layers (V oita et al., 2018; Michel et al.,\n2019), than the FFN layers (Ganesh et al., 2020).\nIn Figure 4, similar to attention heads, we further\nplot the pruning map of FFN layers. We ﬁnd that,\ncertain FFN sub-layers might be more amenable\nto compression, even if the corresponding self-\nattention sub-unit is not. For example, in all the\ntasks, the FFN layers near the ends of input or out-\nput are more likely to be pruned, while this does\nnot hold for the corresponding self-attention layers.\nFinally, we compare between single head at-\ntention pruning and separate pruning for atten-\ntion/FFN, and show the evolution of performance\nfor single head attention pruning (HA), multi-head\nattention pruning (MHA), separate attention/FFN\npruning, and whole layer pruning respectively in\nFigure 5. We ﬁnd that MHA and separate pruning\nperform much better than HA and layer pruning.\n5 Conclusion\nIn this work, we propose a structured prun-\ning method for compressing Transformer models,\nwhich prunes redundant mappings via spectral-\nnormalized identity priors (SNIP). We achieve ef-\nfective pruning results on BERT ﬁne-tuning while\nmaintaining comparable performance. Our work\nshows the importance of the mathematical prop-\nerties of the Transformer model (speciﬁcally, the\nLipschitz condition) on the effectiveness of prun-\ning.\nAdditionally, we quantify task-speciﬁc trade-offs\nbetween model complexity and task performance,\nas well as the progressive improvement provided\nby Multi-head Attention (MHA) and Feedforward\nNetworks (FFN). Our results show that applying\npruning at the level of mappings instead of individ-\nual weights allows for better model compression,\nwhen combined with the appropriate regularization.\nThis suggests that developing more global prun-\ning strategies may be a fruitful avenue for future\nresearch.\nIn the future, we plan to apply a similar approach\nto further reduce the width of Transformer lay-\ners, i.e., the hidden dimension, to achieve an even\nhigher compression ratio. We are also interested\nin jointly using the proposed approach with other\ncompression methods.\nAcknowledgement\nWe thank Atish Agarwala, Xikun Zhang, and\nanonymous reviewers for helpful feedback.\nReferences\nJimmy Ba and Rich Caruana. 2014. Do deep nets really\nneed to be deep? In Advances in neural information\nprocessing systems, pages 2654–2662.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nJens Behrmann, Will Grathwohl, Ricky T. Q. Chen,\nDavid Duvenaud, and Joern-Henrik Jacobsen. 2019.\nInvertible Residual Networks. In International\nConference on Machine Learning, pages 573–582.\nISSN: 1938-7228 Section: Machine Learning.\n728\nDaoyuan Chen, Yaliang Li, Minghui Qiu, Zhen\nWang, Bofang Li, Bolin Ding, Hongbo Deng, Jun\nHuang, Wei Lin, and Jingren Zhou. 2020. Ad-\nabert: Task-adaptive bert compression with differ-\nentiable neural architecture search. arXiv preprint\narXiv:2001.04246.\nZihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi\nZhao. 2018. Quora question pairs.\nMoustapha Cisse, Piotr Bojanowski, Edouard Grave,\nYann Dauphin, and Nicolas Usunier. 2017. Parse-\nval networks: Improving robustness to adversarial\nexamples. arXiv preprint arXiv:1704.08847.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nAngela Fan, Edouard Grave, and Armand Joulin.\n2020. Reducing Transformer Depth on Demand\nwith Structured Dropout. In International Confer-\nence on Learning Representations.\nFarzan Farnia, Jesse Zhang, and David Tse. 2018. Gen-\neralizable adversarial training via spectral normal-\nization. In International Conference on Learning\nRepresentations.\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali\nKhan, Yin Yang, Deming Chen, Marianne Winslett,\nHassan Sajjad, and Preslav Nakov. 2020. Compress-\ning large-scale transformer-based models: A case\nstudy on bert. arXiv preprint arXiv:2002.11985.\nMitchell A Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. arXiv preprint\narXiv:2002.08307.\nFu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin,\nand Yanzhi Wang. 2019. Reweighted proximal prun-\ning for large-scale language representation. arXiv\npreprint arXiv:1909.12486.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefﬁcient neural network. In Advances in neural in-\nformation processing systems, pages 1135–1143.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016a. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016b. Identity mappings in deep residual net-\nworks. In European conference on computer vision,\npages 630–645. Springer.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nForrest N Iandola, Song Han, Matthew W Moskewicz,\nKhalid Ashraf, William J Dally, and Kurt Keutzer.\n2016. Squeezenet: Alexnet-level accuracy with 50x\nfewer parameters and¡ 0.5 mb model size. arXiv\npreprint arXiv:1602.07360.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does bert learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. arXiv preprint arXiv:1908.08593.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew Peters, and Noah A Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. arXiv preprint arXiv:1903.08855.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems,\npages 14014–14024.\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama,\nand Yuichi Yoshida. 2018. Spectral normalization\nfor generative adversarial networks. arXiv preprint\narXiv:1802.05957.\nBehnam Neyshabur, Srinadh Bhojanapalli, David\nMcAllester, and Nati Srebro. 2017. Exploring gen-\neralization in deep learning. In Advances in neural\ninformation processing systems, pages 5947–5956.\nAdam M Oberman and Jeff Calder. 2018. Lipschitz\nregularized deep neural networks converge and gen-\neralize. arXiv preprint arXiv:1808.09540.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\n729\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nVictor Sanh, Thomas Wolf, and Alexander M Rush.\n2020. Movement pruning: Adaptive sparsity by ﬁne-\ntuning. arXiv preprint arXiv:2005.07683.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and\nKurt Keutzer. 2019. Q-bert: Hessian based ultra\nlow precision quantization of bert. arXiv preprint\narXiv:1909.05840.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nJure Sokoli ´c, Raja Giryes, Guillermo Sapiro, and\nMiguel RD Rodrigues. 2017. Robust large margin\ndeep neural networks. IEEE Transactions on Signal\nProcessing, 65(16):4265–4280.\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\nSchmidhuber. 2015. Highway networks. arXiv\npreprint arXiv:1505.00387.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert:\na compact task-agnostic bert for resource-limited de-\nvices. arXiv preprint arXiv:2004.02984.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2018.\nAn analysis of attention mechanisms: The case of\nword sense disambiguation in neural machine trans-\nlation. arXiv preprint arXiv:1810.07595.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from bert into simple neural net-\nworks. arXiv preprint arXiv:1903.12136.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. arXiv\npreprint arXiv:1905.05950.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAndreas Veit and Serge Belongie. 2017. Convolutional\nnetworks with adaptive computation graphs. arXiv\npreprint arXiv:1711.11503, 2.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. arXiv preprint\narXiv:1805.10163.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. 2019.\nStructured pruning of large language models. arXiv\npreprint arXiv:1910.04732.\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen,\nand Hai Li. 2016. Learning structured sparsity in\ndeep neural networks. In Advances in neural infor-\nmation processing systems, pages 2074–2082.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nXin Yu, Zhiding Yu, and Srikumar Ramalingam. 2018.\nLearning strict identity mappings in deep residual\nnetworks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n4432–4440.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert.\narXiv preprint arXiv:1910.06188.\nJunbo Zhao, Yoon Kim, Kelly Zhang, Alexander Rush,\nand Yann LeCun. 2018. Adversarially Regularized\nAutoencoders. In International Conference on Ma-\nchine Learning, pages 5902–5911. ISSN: 1938-\n7228 Section: Machine Learning.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–\n27.\n730\n0 2 4 6 8 10 12\nTraining steps\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0largest singular value\nAttention\nlayer_10\nlayer_11\nlayer_0\nlayer_7\nlayer_5\nlayer_3\nlayer_9\nlayer_4\nlayer_8\nlayer_1\nlayer_2\nlayer_6\n0 2 4 6 8 10 12\nTraining steps\n10\n11\n12\n13largest singular value\nFFN intermidiate\nlayer_4\nlayer_10\nlayer_1\nlayer_11\nlayer_9\nlayer_6\nlayer_8\nlayer_3\nlayer_5\nlayer_0\nlayer_7\nlayer_2\n0 2 4 6 8 10 12\nTraining steps\n6\n7\n8\n9\n10largest singular value\nFFN output\nlayer_10\nlayer_9\nlayer_4\nlayer_3\nlayer_1\nlayer_11\nlayer_6\nlayer_2\nlayer_5\nlayer_7\nlayer_8\nlayer_0\nFigure 6: The largest singular value of attention output weights, and FFN intermediate and output weights for the\noriginal BERTBASE during the ﬁne-tuning on the SST-2 task.\nHyperparameter Pre-trainingFine-tuning\nNumber of layers 12 12\nHidden size 768 768\nFFN inner hidden size3072 3072\nAttention heads 12 12\nAttention head size64 64\nEmbedding size 768 768\nMask percent 15 -\nLearning rate decaylinear linear\nLayerwise LR decay- 0.8\nWarmup steps 10000 -\nWarmup fraction - 0.1\nLearning Rate 2e-4 1e-4\nAdamϵ 1e-6 1e-6\nAdamβ1 0.9 0.9\nAdam 0.999 0.999\nAttention dropout 0.1 0.1\nDropout 0.1 0.1\nWeight decay 0.01 0.01\nl1 regularization factor- 0.01\nBatch size 2048 32\nTrain steps 1M -\nTrain epochs - 3.0/iter for MRPC,\n1.0 for others\nTable 3: Pre-training and Fine-tuning hyperparameters\nA Appendices\nA.1 GLUE Dataset\nWe provide a brief description of the 5 tasks in our\nexperiments from the GLUE benchmarks (Wang\net al., 2018).\nSST-2 The Stanford Sentiment Treebank (Socher\net al., 2013) consists of sentences from movie re-\nviews and human annotations of their sentiment.\nThe task is to predict the sentiment of a given sen-\ntence (positive/negative). The performance is eval-\nuated by the accuracy.\nQQP The Quora Question Pairs dataset (Chen\net al., 2018) is a collection of question pairs from\nthe community question-answering website Quora.\nThe task is to determine whether a pair of questions\nare semantically equivalent. The performance is\nevaluated by the accuracy.\nMRPC The Microsoft Research Paraphrase Cor-\npus (Dolan and Brockett, 2005) is a corpus of sen-\ntence pairs automatically extracted from online\nnews sources, with human annotations for whether\nthe sentences in the pair are semantically equiva-\nlent, and the task is to predict the equivalence. The\nperformance is evaluated by both the F1 score.\nQNLI The Question-answering NLI dataset\n(Chen et al., 2018) is converted from the Stanford\nQuestion Answering Dataset (SQuAD) to a classi-\nﬁcation task. The performance is evaluated by the\naccuracy.\nMNLI The Multi-Genre Natural Language Infer-\nence Corpus (Williams et al., 2017) is a crowd-\nsourced collection of sentence pairs with textual\nentailment annotations. Given a premise sentence\nand a hypothesis sentence, the task is to predict\nwhether the premise entails the hypothesis (entail-\nment), contradicts the hypothesis (contradiction),\nor neither (neutral). The performance is evaluated\nby the test accuracy on both matched (in-domain)\nand mismatched (cross-domain) sections of the test\ndata.\nA.2 Experiment Settings\nThe full set of hyperparameters for pre-training and\nﬁne-tuning are listed in Table 3.\nA.3 Spectral Norm of Weights during\nTraining\nWe show the largest singular values of the weight\nmetrics in the original BERT model during ﬁne-\ntuning on the task SST-2 in Figure 6. As can be\nseen from the ﬁgure, the norm of the weights with-\nout spectral normalization is obviously out of con-\ntrol.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6565937399864197
    },
    {
      "name": "Transformer",
      "score": 0.612364649772644
    },
    {
      "name": "Thresholding",
      "score": 0.5692721009254456
    },
    {
      "name": "Subnetwork",
      "score": 0.5646297931671143
    },
    {
      "name": "Residual",
      "score": 0.5521154999732971
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.5131390690803528
    },
    {
      "name": "Prior probability",
      "score": 0.48845377564430237
    },
    {
      "name": "Algorithm",
      "score": 0.40130487084388733
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3763849139213562
    },
    {
      "name": "Bayesian probability",
      "score": 0.28547972440719604
    },
    {
      "name": "Voltage",
      "score": 0.13702329993247986
    },
    {
      "name": "Engineering",
      "score": 0.08402296900749207
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}