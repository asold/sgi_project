{
  "title": "Memformer: The Memory-Augmented Transformer",
  "url": "https://openalex.org/W3093260394",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2121355092",
      "name": "Qingyang Wu",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A2115082539",
      "name": "Zhen-zhong Lan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099504887",
      "name": "Jing Gu",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A2037128165",
      "name": "Zhou Yu",
      "affiliations": [
        "University of California, Davis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Transformer models have obtained remarkable accomplishments in various NLP tasks. However, these models have efficiency issues on long sequences, as the complexity of their self-attention module scales quadratically with the sequence length. To remedy the limitation, we present Memformer, a novel language model that utilizes a single unified memory to encode and retrieve past information. It includes a new optimization scheme, Memory Replay Back-Propagation, which promotes long-range back-propagation through time with a significantly reduced memory requirement. Memformer achieves O(n) time complexity and O(1) space complexity in processing long sequences, meaning that the model can handle an infinite length sequence during inference. Our model is also compatible with other self-supervised tasks to further improve the performance on language modeling. Experimental results show that Memformer outperforms the previous long-range sequence models on WikiText-103, including Transformer-XL and Compressive Transformer.",
  "full_text": "Memformer: A Memory-Augmented Transformer for Sequence Modeling\nQingyang Wu 1, Zhenzhong Lan 2, Kun Qian 1 Jing Gu 1\nAlborz Geramifard 3 Zhou Yu 1\n1 University of California, Davis, 2 Westlake University,3 Facebook AI\n{wilwu,qkun,jkgu,joyu}@ucdavis.edu\nlanzhenzhong@westlake.edu.cn,alborzg@fb.com\nAbstract\nTransformers have reached remarkable suc-\ncess in sequence modeling. However, these\nmodels have efﬁciency issues as they need\nto store all the history token-level representa-\ntions as memory. We present Memformer, an\nefﬁcient neural network for sequence model-\ning, that utilizes an external dynamic mem-\nory to encode and retrieve past information.\nOur model achieves linear time complexity\nand constant memory space complexity when\nprocessing long sequences. We also pro-\npose a new optimization scheme, memory re-\nplay back-propagation (MRBP), which pro-\nmotes long-range back-propagation through\ntime with a signiﬁcantly reduced memory re-\nquirement. Experimental results show that\nMemformer has achieved comparable perfor-\nmance compared against the baselines by us-\ning 8.1x less memory space and 3.2x faster\non inference. Analysis of the attention pattern\nshows that our external memory slots can en-\ncode and retain important information through\ntimesteps.\n1 Introduction\nMemory plays a fundamental role in human cogni-\ntion. Humans perceive and encode sensory infor-\nmation into a compressed representation stored in\nneurons, and later we effectively retrieve the stored\ninformation to accomplish various tasks. The for-\nmation of memory involves complex cognitive pro-\ncesses. Modeling and studying the behavior of\nhuman memory is still a challenging research prob-\nlem in many areas.\nMany researchers have attempted to incorpo-\nrate memory systems in artiﬁcial neural networks.\nEarly works like recurrent neural networks (RNN)\n(Rumelhart et al., 1988) including LSTM (Hochre-\niter and Schmidhuber, 1997) and GRU (Chung\net al., 2014) model temporal sequences with their\ninternal compressed state vector as memory. How-\never, they are limited in preserving the long-term\ninformation due to the memory bottleneck. To al-\nleviate this limitation, more powerful memory net-\nwork architectures such as Neural Turing Machine\n(NTM) (Graves et al., 2014), Differential Neural\nComputer (DNC) (Graves et al., 2016) have been\nproposed by leveraging a large external dynamic\nmemory. Unfortunately, due to their complex mem-\nory interaction mechanism, they are not widely\nused for down-stream tasks at present.\nMore recently, Vaswani et al. (2017) propose\nTransformer by discarding the use of recurrence\nand memory. Instead, it computes all the O(N2)\npaired dependencies in a sequence with self-\nattention (Bahdanau et al., 2015). Transform-\ners have achieved great success in various natu-\nral language processing tasks. Nevertheless, the\nquadratic computation complexity can be costly.\nSome works try to address the limitations of self-\nattention, including Reformer, Sparse Transformer,\nLongformer, Linformer (Child et al., 2019; Kitaev\net al., 2020; Wang et al., 2020), etc. They success-\nfully reduce the complexity of self-attention and\nthus enable processing longer sequences. However,\nmost of them still require linear memory space\ncomplexity.\nTransformer-XL (Dai et al., 2019) re-introduces\nthe concept of memory and recurrence. It caches\neach layer’s hidden states of self-attention into a\nﬁxed-size queue and re-uses them in the later at-\ntention computation. However, the memory as\nraw hidden states cannot effectively compress high-\nlevel information. Thus, Transformer-XL in prac-\ntice needs a massive memory size to perform well,\nand spends huge computation in using its mem-\nory. Compressive Transformer (Rae et al., 2020)\nimproves upon Transformer-XL by further com-\npressing its memories into fewer vectors via a com-\npression network. However, as mentioned in the pa-\npers, both Transformer-XL and Compressive Trans-\nformer discard the information from the distant\npast, which causes a theoretical maximum tempo-\narXiv:2010.06891v2  [cs.CL]  12 Apr 2022\nral range given the ﬁxed memory size.\nInspired by the previous external memory net-\nworks, we propose Memformer, which incorporates\na ﬁxed-size external dynamic memory combined\nwith the recent Transformer architecture. Mem-\nformer interacts with its external dynamic mem-\nory through the memory reading and writing mod-\nules. Also, we introduce a forgetting mechanism\nto improve the effectiveness of memorizing new\ninformation. By utilizing recurrence and a ﬁxed-\nsize memory, our model has a theoretically inﬁ-\nnite temporal range of memorization and implies a\nlinear computation complexity and constant mem-\nory space complexity. As the traditional back-\npropagation through time (BPTT) has an unafford-\nable memory cost in our model, we introduce a\nnew optimization scheme, memory replay back-\npropagation (MRBP), to signiﬁcantly reduce the\nmemory cost in training recurrent neural networks\nwith large size of memory representations.\nWe evaluate Memformer on the autoregressive\nimage generation and language modeling task. Ex-\nperimental results show that Memformer performs\non par with Transformer and Transformer XL with\nlarge memory size, while being much more efﬁ-\ncient in terms of computation speed and memory\nspace consumption. We also conduct an analysis\nshowing that Memformer can retain information\nfor an extended period.\n2 Related Work\nThis section introduces some recent research direc-\ntions that aim to alleviate the quadratic cost of self-\nattention. Moreover, we analyze their assumptions\nand limitations under the autoregressive setting to\nprovide a broader view of these models.\n2.1 Sparse Attention\nOne inﬂuential direction is to replace the full self-\nattention with sparse attention patterns to speed\nup the computation. Child et al. (2019) proposed\nSparse Transformer, using a block sparse atten-\ntion pattern to reduce the computation complexity\nto O(N\n√\nN). Later, Longformer (Beltagy et al.,\n2020) and Big Bird (Zaheer et al., 2020) further\nexplored this direction and proposed an even more\nsparse attention pattern to reduce the cost to O(N).\nThey introduced global tokens to encode the infor-\nmation from the entire sequence and kept the self-\nattention to the closest ktokens and the global to-\nkens to achieve linear complexity. Although linear\nsparse attention’s theoretical soundness is proven\nfor bidirectional encoders, it does not hold for the\ndecoder. The main reason is that the global to-\nkens cannot leak information to the future tokens\nin the autoregressive setting, where all the tokens\ncan only see their previous tokens. Thus, linear\nsparse attention cannot guarantee a token to see its\nall past tokens. Only Sparse Transformer here with\nO(N\n√\nN) complexity can theoretically cover all\nthe past tokens for the sequence generation.\n2.2 Linear Attention\nAnother direction is focusing on improving the\nsoftmax operation in the self-attention. Linformer\n(Wang et al., 2020) reduced the complexity to\nO(N) by projecting the entire sequence to a con-\nstant size of keys and values, but this method\nhas not been applied to autoregressive decoding.\nPerformer (Choromanski et al., 2020) and Linear\nTransformer (Katharopoulos et al., 2020) used a\nlinear dot-product of kernel feature maps to replace\nsoftmax. However, for Linear Transformer under\nthe autoregressive setting, it needs to compute the\ncumulative summation to aggregate the history in-\nformation. This assumption is too strong if the\ninput sequence is long and the length is not ﬁxed.\nAfter thousands of steps, the numerical values can\nbecome very large due to the summation, causing\noverﬂow and gradient instability.\n2.3 Recurrence and Memory\nApplying recurrence and memory to Transformers\nis an orthogonal direction comparing to the efﬁ-\ncient attention approaches. If the memory size is\nconstant, recurrence enables the model to have con-\nstant memory complexity during inference. There\nare mainly two works exploring this direction.\nTransformer-XL (Dai et al., 2019) used relative\npositional encoding and consisted of a segment-\nlevel recurrence mechanism to encode beyond a\nﬁxed-length context. Compressive Transformer\n(Rae et al., 2020) extended from Transformer-XL\nby further compressing the previous cached hid-\nden states to achieve a longer context. However,\nusing past hidden states as memory would cause\na theoretical maximum temporal range of context,\nmeaning that a token is not guaranteed to see all\nthe past tokens. Thus, in practice, Transformer-XL\nand Compressive Transformer need huge memory\nsize to achieve good performance.\nFigure 1: Memformer overall architecture for the en-\ncoder (left) and decoder (right). Transformer encoder\nis responsible to interact with the memory. Sequence\nmodeling is achieved by predicting the next segment\nconditioned to the current segment and memory.\n2.3.1 Dynamic Memorization\nWithin the scope of memory networks, there are\ndynamic memorization techniques. Different from\nTransformer-XL which stores the token-level his-\ntory representations as memory, dynamic memo-\nrization does not have a theoretical upper bound for\nthe temporal range. Neural Turing Machine (NTM)\n(Graves et al., 2014) and Differential Neural Com-\nputer (DNC) (Graves et al., 2016) are two early\nmodels that can control external memory resources\nto achieve long-lasting memory. However, their\ncomplex memory mechanisms cause them to be\nslow and unstable during training. In this work,\nwe propose a dynamic memorization mechanism\nto achieve more efﬁcient memory representations.\n3 Methods\nIn this section, we ﬁrst formalize the segment-level\nsequence modeling. Then, we present the memory\nreading and writing modules. Finally, we explain\nthe memory replay back-propagation (MRBP) al-\ngorithm used for training.\n3.1 Segment-level Sequence Modeling\nGiven a sequence of N tokens x1,x2,...,x N , an\nstandard language model learns the joint probabil-\nity of the sequence by taking the product of each\ntoken’s probability conditioned to the previous to-\nkens, which is deﬁned as:\nP(x) =\n∏\nt\nP(xt|x<t)\nWhen we have a large external memory sys-\ntem to store the history information, we cannot\nafford to interact with memory for every token.\nThe workaround is to process a long sequence\nat the segment level. We can split a sequence\ninto T segments and each segment has Ltokens:\nst = {xt,1,xt,2,...x t,L}.\nBecause a bidirectional encoder is better at ex-\ntracting word representations, we apply a Trans-\nformer encoder-decoder here. The encoder’s role is\nto encode the segment st and inject the information\ninto the memory Mt, while it also retrieves past\ninformation from the previous timestep’s memory\nMt−1. The encoder’s ﬁnal output will be fed into\nthe decoder’s cross attention layers to predict the\ntoken probabilities of the next timestep’s segment\nst+1 with standard language modeling.\nMt = Encoder(st,Mt−1)\nP(st|s<t) =\n∏\nn=1:L\nPDecoder(xt,n |xt,<n,Mt−1)\nP(x) =\n∏\nt=1:T\nPModel(st|s<t)\nAt each timestep, given a segment as the input,\nthe model needs to continue that segment by gener-\nating the next text segment, and the generated seg-\nment will be fed back into the model again. Since\nthe memory stores all the past information, we can\nautoregressively generate all the token segments\nin a sequence. In this fashion, we can model the\nentire long sequence.\nFigure 1 shows the overall architecture of Mem-\nformer. We will further explain each component\nand the implementation in the following sections.\n3.2 External Dynamic Memory Slots\nExternal dynamic memory (EDM) is a data struc-\nture that stores high-level representations of past\ninputs. “Dynamic” means that the model interac-\ntively encodes and retrieves the information from\nmemory in a recurrent manner. This contrasts with\nstatic memory design, where the memory is stored\nstatically and does not change during the inference.\nIn our design, we allocate a constantknumber of\nvectors as the external dynamic memory. At each\nFigure 2: Memory Reading. The input sequence xat-\ntends over all the memory slots to retrieve the history\ninformation.\ntimestep t, we can have Mt = [m0\nt ,m0\nt ,...,m k\nt ].\nFor each sample in the batch, they have separate\nmemory representations. Therefore, similar to\nRNN during inference, the memory consumption\nwill be constant no matter how long the input se-\nquence is. We name it memory slots because each\nslot is working individually to have different repre-\nsentations. The following sections will explain how\nthe model manages to read and write this memory.\n3.3 Memory Reading\nFor each input segment sequence, the model needs\nto read the memory to retrieve relevant past infor-\nmation. We leverage the cross attention to achieve\nthis function:\nQx,KM ,VM = xWQ,MtWK,MtWV (1)\nAx,M = MHAttn(Qx,KM ) (2)\nHx = Softmax(Ax,M ) VM (3)\nMHAttn refers to Multi-Head Attention. Mem-\nory slot vectors are projected into keys and values,\nand the input sequence xis projected into queries.\nThen the input sequence’s queries attend over all\nthe memory slots’ key-value pairs to output the ﬁ-\nnal hidden states. This enables the model to learn\nthe complex association of the memory. Figure 2\nshows the illustration.\nMemory reading occurs multiple times as ev-\nery encoder layer incorporates a memory reading\nmodule. This process ensures a higher chance of\nsuccessfully retrieving the necessary information\nfrom a large memory.\n3.4 Memory Writing\nMemory writing involves a slot attention module\nto update memory information and a forgetting\nmethod to clean up unimportant memory informa-\ntion. Contrary to memory reading, memory writing\nonly happens at the last layer of the encoder. This\nhelps to store the high-level contextual represen-\ntations into the memory. In practice, we append\nsome classiﬁcation tokens to the input sequence to\nbetter extract the sequence representations.\nFigure 3: Memory Writing. Each memory slot attends\nover itself and the input sequence representations to\nproduce the next timestep’s memory slot.\n3.4.1 Update via Memory Slot Attention\nFigure 3 shows how memory is updated with the\ncurrent segment’s information. Each slot is sepa-\nrately projected into queries and keys. The segment\ntoken representations are projected into keys and\nvalues. Slot attention means that each memory slot\ncan only attend to itself and the token representa-\ntions. Thus, each memory slot cannot write its own\ninformation to other slots directly, as memory slots\nshould not be interfering with each other.\nQmi ,Kmi = miWQ,miWK (4)\nKx,Vx = xWK,xWV (5)\nA′\nmi =MHAttn(Qmi ,[Kmi ; Kx]) (6)\nWhen we compute the ﬁnal attention scores, we\ndivide the raw attention logits with a temperature\nτ (τ <1). This operation sharpens the attention\ndistribution, which makes the writing focusing on\nfewer slots or token outputs.\nAmi = exp(A′\ni/τ)∑\nj exp(A′\nj/τ) (7)\nFinally, the next timestep’s memory is collected\nwith by attention.\nmi\nt+1\n′\n= Softmax(Ax,M ) [mi\nt; Vx] (8)\nThe attention mechanism helps each memory slot\nto choose to whether preserve its old information\nor update with the new information.\nFigure 4: Illustration of forgetting. Memory slot ma is\neasy to be forgotten, while mb is hard to be forgotten.\n3.4.2 Implementation of Memory Writer\nSince each memory slot stores the information in-\ndependently, we design a special type of sparse\nattention pattern. Each slot in the memory can only\nattend over itself and the encoder outputs. It aims\nto preserve the information in each slot longer over\nthe time horizon. When a slot only attends itself\nduring writing, the information will not be changed\nin the next timestep.\n3.4.3 Forgetting Mechanism\nForgetting is crucial for learning as it helps to ﬁlter\nout trivial and temporary information to memorize\nmore important information. LSTM introduces the\nforget gate (Gers et al., 2000) to reset its mem-\nory state, and the forget gate is proven to be the\nmost important component in the LSTM (van der\nWesthuizen and Lasenby, 2018).\nIn this work, we introduce a forgetting mecha-\nnism called Biased Memory Normalization(BMN),\nspeciﬁcally designed for our slot memory represen-\ntations. We normalize the memory slots for every\nstep to prevent memory weights from growing in-\nﬁnitely and maintain gradient stability over long\ntimesteps. To help forget the previous information,\nwe add a learnable vector vbias to it. Also, naturally\nthe initial state vi\nbias is after normalization.\nmi\nt+1 ←mi\nt+1 + vi\nbias\nmi\nt+1 ← mi\nt+1\n||mi\nt+1||\nmi\n0 ← vi\nbias\n||vi\nbias||\nIn Figure 4, we illustrate the forgetting mecha-\nnism with the learnable bias vector vbias. Because\nof the normalization, all memory slots will be pro-\njected onto a sphere distribution. Here, we demon-\nstrate with a 2D sphere for simplicity.\nvbias here controls the speed and the direction\nof forgetting. When adding vbias to the memory\nAlgorithm 1: Memformer Update\nInput: rollout=[xt,xt+1,...,x T ]: a\nlist containing previous\ninputs\nmemories=[Mt,Mt+1,...,M T ]:\nmemory from the previous\n⊿ Initialize a list for\nback-propagation\n1 replayBuffer = [Mt]\n⊿ Forward pass & no gradient\n2 for t= t,t + 1,...,T −1 do\n3 Mt+1, _ = Model(xt, Mt)\n4 replayBuffer.append(Mt+1)\n5 end\n⊿ Backward pass with gradient\n6 ∇Mt+1 = 0\n7 for t= T,T −1,...,t + 1,t do\n⊿ Recompute\n8 Mt+1, Ot = Model(xt, Mt)\n9 loss= floss(Ot)\n10 loss.backward()\n11 Mt+1.backward(∇Mt+1)\n12 ∇Mt+1 = ∇Mt\n13 end\n⊿ Update and pop the oldest\nmemories\n14 memories = replayBuffer\n15 memories.pop()\nslot, it would cause the memory to move along\nthe sphere and forget part of its information. If a\nmemory slot is not updated for many timesteps, it\nwill eventually reach the terminal state T unless\nthe new information is injected. The terminal state\nis also the initial state, and it is learnable.\nThe speed of forgetting is controlled by the mag-\nnitude of vbias and the cosine distance between\nm′\nt+1 and vbias. For example, mb is nearly opposite\nto the terminal state, and thus would be hard to\nforget its information. ma is closer to the terminal\nstate and thus easier to forget.\n3.5 Memory Replay Back-Propagation\nMemformer relies on the external memory to pro-\ncess a sequence. At inference time, there is no addi-\ntional memory cost because of the ﬁxed-size mem-\nory design. Nevertheless, during training, it would\nrequire back-propagation through time (BPTT) so\nthat the memory writer network can be trained to\nretain long-term information. The problem with\ntraditional BPTT is that it unrolls the entire compu-\ntational graph during the forward pass and stores\nall the intermediate activations. This process would\nlead to impractically huge memory consumption\nfor Memformer.\nA favorable existing approach to eliminate this\nproblem is gradient checkpointing (Chen et al.,\n2016). The algorithm can signiﬁcantly reduce the\nmemory cost of a large neural network. However,\nthe standard gradient checkpointing still needs to\ncompute all the nodes in the computational graph\nand store unnecessary activations during the for-\nward pass. We propose Memory Replay Back-\nPropagation (MRBP), a more efﬁcient variant of\ngradient checkpointing, by replaying the mem-\nory at each timestep to accomplish gradient back-\npropagation over long unrolls.\nThe algorithm takes an input with a roll-\nout xt,xt+1,...,x T and the previous memories\nMt,Mt+1,...,M T if already being computed.\nMRBP only traverses the critical path in the compu-\ntational graph during the forward pass and recom-\nputes the partial computational graph for the local\ntimestep during the backward pass. It then obtains\neach timestep’s memory and stores those memories\nin the replay buffer. The full algorithm is described\nin Algorithm 1. The experiments of memory cost\nreduction with MRBP is in the Appendix A.\n4 Experiments\n4.1 Computation and Memory Cost\nWe experimented the computation and memory\ncost of Vanilla Transformer, Transformer-XL, and\nMemformer. For Vanilla Transformer, it has to in-\ncrease the input sequence length to encode more\ntokens. Its cost is O(N2) where N is the sequence\nlength. Transformer-XL and Memformer use mem-\nory to store the history information, and the input\nsequence length is a constant value. Thus, their\ncomputation complexity is O(N).\nAs a trade-off, for both Transformer-XL and\nMemformer, the memory size is then an important\nfactor to affect the capacity of storing the history\ninformation. Transformer-XL stores the past hid-\nden states for all layers as memory. If L is the\nnumber of layers, and Kis the memory size, then\nthe memory cost is O(K×L). Memformer only\nstores Kvectors as memory with cost O(K).\nTo better illustrate the difference, Figure 5 shows\nthe number of FLOPs (ﬂoating-point operations)\nversus sequence length (left) and the GPU mem-\nory consumption versus memory size on the ac-\ntual models (right). The sequence length is in-\ncreased from 128 to 8,192. Here, Memformer and\nTransformer-XL had the same number of param-\neters. From the ﬁgure, Vanilla Transformer has\nthe largest computation cost growth. Memformer’s\ncosts grew linearly with the sequence length and\nachieved better efﬁciency than Transformer-XL.\nThen, we compared the GPU memory consump-\ntion. We tested the memory size ranging from 64\nto 2,048, with a batch size 16 for better visibil-\nity of memory cost difference. Transformer-XL’s\nmemory consumption grew rapidly with the mem-\nory size, while Memformer is more efﬁcient with\nlarge memory size. In large memory size setting,\nMemformer uses 8.1x less memory space.\n4.2 Autoregressive Image Generation\nModel #FLOPs (B) Perplexity↓\nLSTM 52.5 1 .698\nTransformer Decoder 41.3 1 .569\nTransformer-XL\nmemory=56 5.6 1 .650\nmemory=224 15.6 1 .618\nmemory=784 49.1 1 .611\nMemformer\n4 encoder+8 decoder 5.0 1 .555\nMemformer Ablation\n2 encoder+6 decoder\nmemory=64 3.9 1 .594\nmemory=32 3.9 1 .600\nmemory=16 3.9 1 .604\nmemory=1 3.9 1 .627\n4 encoder+4 decoder 3.6 1 .628\nw/o memory 1.8 1 .745\ntemperature=1.0 3.9 1 .612\nw/o forgetting 3.9 1 .630\nw/o multi-head 3.9 1 .626\nTable 1: Results for autoregressive image generation.\nOur method only takes about 10% FLOPs of the best\nTransformer-XL model.\nRecent research (Ramesh et al., 2021) demon-\nstrates the approach of treating an image as a long\nsequence for image generation. Thus, we evalu-\nated our model on the MNIST (LeCun and Cortes,\n2010) image generation task with sequence model-\ning. Each image of size 28 ×28 was reshaped into\na sequence of 784 tokens, and the 8-bit gray-scale\nwas turned to a 256 vocabulary size.\nFor the baselines, LSTM had 4 layers and 512\nhidden size. Transformer Decoder had 8 layers\nFigure 5: Comparison of the number of FLOPs and GPU memory consumption for Vanilla Transformer,\nTransformer-XL, and Memformer.\nand could take all the 784 tokens as the input.\nTransformer-XL had 8 layers. All the models had\nthe same 128 hidden size, 4 attention heads, 32\nhead size, and 256 feedforward size. Memformer\nwas tested with default memory size 64. The de-\nfault memory writer temperature was set to 0.25.\nWe also conducted ablation studies to examine the\ncontribution of various components.\nModel #FLOPs (B) PPL↓\nTransformer-XL base\nmemory=1600 250 23 .95\nmemory=1024 168 23 .67\nmemory=512 94 23 .94\nmemory=256 58 25 .39\nmemory=128 39 25 .60\nmemory=32 26 27 .22\nCompressive Transformer\nmemory= 512 compress=512 172 23.23\nMemformer\n4 encoder + 16 decoder 54 22 .74\nMemformer Ablation\n4 encoder + 12 decoder 48 23 .91\nmemory=512 35 23 .30\nw/o memory 31 25 .57\nTable 2: Experimental results on language modeling.\nOur method is 3.2 times faster here.\nTable 1 shows the experimental results. We re-\nport median from three trials. Our Memformer\nwith 4 layers of encoder and 8 layers of decoder\nachieved the best performance (1.555), while only\nusing nearly 10% of FLOPs compared to the best\nTransformer XL baseline with memory size of\n784 (1.611). Its performance was even better than\nthe Transformer Decoder with the entire input se-\nquence. We hypothesized that this observation was\ndue to the extra parameters from the 4 layers of en-\ncoder. Therefore, we conducted an ablation study\nby having various numbers of encoder and decoder\nlayers. If we reduce the number of decoder layers\nin Memformer (4 encoder+4 decoder), the perfor-\nmance dropped as shown ( 1.628). Results indi-\ncated that the number of decoder layers was im-\nportant for the performance. Overall, Memformer\noutperformed Transformer-XL with a much lower\ncomputation cost.\nThe performance increased as the memory size\nincreased. Moreover, when we completely re-\nmoved the memory, Memformer performed terribly,\nsignifying the importance of the encoded informa-\ntion in the memory. Other components such as\nforgetting mechanism, memory writer temperature,\nmulti-head attention were proven to contribute to\nthe ﬁnal performance as well.\n4.3 Language Modeling\nWe also conducted experiments on WikiText-103\n(Merity et al., 2017), which is a long-range lan-\nguage modeling benchmark. It contains 28K ar-\nticles with an average length of 3.6K tokens per\narticle. Due to the limitation of computational re-\nsources, we are unable to experiment on the more\nrecent PG19 (Rae et al., 2020) dataset. To study\nthe computation cost and memory efﬁciency, we\ntest with Transformer-XL base with 16 layers, 512\nhidden size, 2,048 feedforward size, 64 head size,\nand 8 heads. The details are in the Appendix.\nMemformer has the same hidden size, feedfor-\nward size, head size, and number of heads. We\nalso re-implement a version of Compressive Trans-\nformer of the same size as there is no ofﬁcial imple-\nmentation. The memory length is set to 512, and\nthe compressive memory length is 512. The com-\npression ratio is 4. The target sequence length for\nall models was set to 128. We test the performance\nunder various memory sizes.\nTable 2 summarizes the results on WikiText-103\ntest set. We report the number of inference FLOPs\n(billions) and perplexity median from three trials.\nAs Transformer-XL’s memory size increased, the\nperplexity dropped as expected, but the the num-\nber of FLOPs grew quickly because the attention\nlength was also increased. The perplexity stopped\ndecreasing after we increased the memory size to\n1,600. We suspect that since the average num-\nber of tokens in WikiText-103 is 3,600, a larger\nmemory size would bring noises and hence did not\nfurther improve the performance compared to a\nsmaller memory size (1,024). Compressive Trans-\nformer achieves slightly better performance with\nextra FLOPS compared to Transformer XL with\nmemory size 1024.\nMemformer with 4 encoders, 16 decoders, and\n1,024 memory size achieved the best performance.\nIt required much less computation cost ( 54) and\nperformed much better than Transformer-XL with\n1,024 memory size, supporting that Memformer\nhas a more efﬁcient memory representation.\nIn the ablation studies, to compensate for the ex-\ntra number of encoder layers, we reduced the num-\nber of decoder layers to 12. The ﬁnal performance\nwas close to Transformer-XL, but Memformer used\na much smaller number of FLOPs. Also, memory\nsize was important for Memformer, as the perfor-\nmance dropped after the memory size is reduced\nto 512. When we completely removed the memory\nmodule by removing the memory writer and mem-\nory reading cross attention, the perplexity increased\nto 25.57, which is similar to Transformer-XL with\na memory size of 128.\n4.3.1 Memory Writer Analysis\nFigure 6: Visualization of three types of memory slots.\nIt is interesting to interpret how memory writer\nupdates the memory slots. We analyzed the atten-\ntion outputs from the memory writer. We roughly\ncategorized the memory slots into three different\ntypes and visualized three examples with normal-\nized attention values in Figure 6.\nWe picked the memory slot m250, m300 , and\nm355. During the middle of processing a docu-\nment, around 60% to 80% of the memory slots are\nlike m300. Their attention focused on themselves,\nmeaning that they were not updating for the current\ntimestep. This suggests that the memory slots can\ncarry information from the distant past.\nFor the second type, the memory slot m250 had\nsome partial attention over itself and the rest of\nattention over other tokens. This type of memory\nslots is transformed from the ﬁrst type of memory\nslots, and at the current timestep they aggregate\ninformation from other tokens.\nThe third type of memory slot looks likem355. It\ncompletely attended to the input tokens. At the be-\nginning, nearly all memory slots belong to this type,\nbut later only 5% to 10% of the total memory slots\naccount for this type. We also found that the forget-\nting vector’s bias form355 had a larger magnitude\n(3.20) compared to some other slots ( 1.15), sug-\ngesting that the information was changing rapidly\nfor this memory slot.\nFigure 7: Visualization of the memory writer’s atten-\ntion.\nTo better understand how the slot m355 update\nits information, we visualized its attention on an\nexample input sequence in Figure 7. It shows that\nthis slot learned a compressed representation of the\nsentence by attending over some named entities and\nverbs, which is consistent with human cognition.\n5 Conclusion\nWe presented Memformer, an autoregressive model\nwhich utilizes an external dynamic memory to\nefﬁciently process long sequences with a linear\ntime complexity and constant memory complex-\nity. Along with Memformer, we introduced a\nnew optimization scheme, Memory Replay Back-\npropagation, which enables training recurrent neu-\nral networks with large memory. Experimental\nresults showed that Memformer achieved compa-\nrable performance with great efﬁciency, and was\nable to preserve information from the distant past.\nWith the enhanced memory capacity, we believe\nthat Memformer can spark interesting works that\nrely on recurrence and autoregressive modeling,\nwhich will beneﬁt tasks such as dialog and interac-\ntive systems.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\nCoRR, abs/2004.05150.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\nGuestrin. 2016. Training deep nets with sublinear\nmemory cost. CoRR, abs/1604.06174.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. URL\nhttps://openai.com/blog/sparse-transformers.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamás Sar-\nlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2020. Rethinking attention with per-\nformers. CoRR, abs/2009.14794.\nJunyoung Chung, Caglar Gulcehre, Kyunghyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence mod-\neling. In NIPS 2014 Workshop on Deep Learning,\nDecember 2014.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a ﬁxed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n2978–2988. Association for Computational Linguis-\ntics.\nFelix A. Gers, Jürgen Schmidhuber, and Fred A. Cum-\nmins. 2000. Learning to forget: Continual predic-\ntion with LSTM. Neural Comput., 12(10):2451–\n2471.\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines. CoRR, abs/1410.5401.\nAlex Graves, Greg Wayne, Malcolm Reynolds,\nTim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwinska, Sergio Gomez Colmenarejo, Edward\nGrefenstette, Tiago Ramalho, John P. Agapiou,\nAdrià Puigdomènech Badia, Karl Moritz Hermann,\nYori Zwols, Georg Ostrovski, Adam Cain, Helen\nKing, Christopher Summerﬁeld, Phil Blunsom, Ko-\nray Kavukcuoglu, and Demis Hassabis. 2016. Hy-\nbrid computing using a neural network with dy-\nnamic external memory. Nat., 538(7626):471–476.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput., 9(8):1735–\n1780.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear at-\ntention. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-\n18 July 2020, Virtual Event, volume 119 of Proceed-\nings of Machine Learning Research, pages 5156–\n5165. PMLR.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nYann LeCun and Corinna Cortes. 2010. MNIST hand-\nwritten digit database.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nChloe Hillier, and Timothy P. Lillicrap. 2020. Com-\npressive transformers for long-range sequence mod-\nelling. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gener-\nation.\nDavid E. Rumelhart, Geoffrey E. Hinton, and Ronald J.\nWilliams. 1988. Learning Representations by Back-\nPropagating Errors, page 696–699. MIT Press,\nCambridge, MA, USA.\nJos van der Westhuizen and Joan Lasenby. 2018. The\nunreasonable effectiveness of the forget gate. CoRR,\nabs/1804.04849.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity. CoRR, abs/2006.04768.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nA MRBP Efﬁciency Test\nIn this section, we test MRBP’s efﬁciency by comparing against the standard back-propagation through\ntime (BPTT) and the standard gradient checkpointing (GC) algorithm. This algorithm is useful for\nMemformer to reduce memory requirement because of the back-propagation through several timesteps.\nWe use the Memformer model and set all the hyper-parameters to be the same.\nMethod GPU Memory (MB) Speed (relative)\nBPTT 16,177 x1.00\nGC 9,885 x0.48\nMRBP 7,229 x0.90\nTable 3: Memory Replay Back-Propagation performance comparison. Evaluation speed is based on seconds per\nsample. BPTT means back-propagation through time. GC means gradient checkpointing.\nThe back-propagation through time (BPTT) approach is the fastest because it does not need re-\ncomputation. However, it costs the most amount of memory due to unrolling the entire computational\ngraph. While gradient checkpointing can save huge amount of memory, it is much slower than the\nother two methods (x0.48). In contrast, our MRBP saves more GPU memory with only slight speed\ndegeneration (x0.90).\nB Training Details\nImage Generation Language Modeling\nbatch size 256 128\nwarm-up steps 1,000 1,0000\nlearning rate 1e-3 1e-3\ndropout 0.1 0.1\nmemory length 8 1,024\ntemperature 0.25 0.125\ntime horizon 8 8\nweight decay 0.01 0.01\nmax gradient norm 1.0 1.0\ntraining steps 10,000 150,000\nTable 4: Training Details\nWe trained our model on NVIDIA V100 16GB and 2080Ti 11GB. The training for image generation\ntook about one day on one GPU. The training for language modeling took approximately four days on\nfour GPUs.\nC Effects of Time Horizon and Memory Size\nWe test how the time horizon for back-propagation affects the performance. We test on a smaller\nMemformer model for the efﬁciency. The results are shown in Figure 8a. We vary the back-propagation\ntime horizon from 1 to 32. When the time horizon is set to 1, back-propagation cannot pass gradients\nthrough memory to the previous timestep. Thus, we observe the performance is the worst when the time\nhorizon is 1. As we increase the time horizon, the model achieves better perplexity scores. When the time\nhorizon is increased to 32, we observe the marginal improvement on perplexity is almost gone. A large\nmemory size ideally helps to store more information. From Table 8b, we can see a huge improvement\nwhen increasing the memory size from 1 to 8. Furhter increasing the memory size has a smaller effects on\nthe performance, and we suspect that this is due to the size of the model.\n(a) Effects of different time horizons\n (b) Effects of different memory sizes\nFigure 8: Effects of different conﬁgurations. (a) shows the effects of changing time horizon. (b) shows the effects\nof changing memory size.\nFigure 9: Memory Writer’s Attention\nD Implementation of Memory Writer\nMemory Slot Attention in Figure 9 produces the next timestep’s memory Mt+1. This module takes\nthe inputs of the previous timestep’s memoryMt and the encoder’s ﬁnal hidden states. It then projects\nthe memory into queries, keys, and values, while the encoder outputs are into keys and values. Since\neach memory slot should not be interfering with other memory slots, we design a special type of sparse\nattention pattern. Thus, each slot in the memory can only attend over itself and the encoder outputs. This\nis to preserve the information in each slot longer over the time horizon. For example, if one slot only\nattends itself, then the information in that slot will not change in the next timestep.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7519053220748901
    },
    {
      "name": "Computer science",
      "score": 0.7000682353973389
    },
    {
      "name": "ENCODE",
      "score": 0.656162679195404
    },
    {
      "name": "Language model",
      "score": 0.5866509675979614
    },
    {
      "name": "Inference",
      "score": 0.5602542757987976
    },
    {
      "name": "Quadratic growth",
      "score": 0.5539240837097168
    },
    {
      "name": "Sequence (biology)",
      "score": 0.43451589345932007
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4298032820224762
    },
    {
      "name": "Algorithm",
      "score": 0.40432804822921753
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3439266085624695
    },
    {
      "name": "Speech recognition",
      "score": 0.33883535861968994
    },
    {
      "name": "Voltage",
      "score": 0.10487672686576843
    },
    {
      "name": "Electrical engineering",
      "score": 0.09925085306167603
    },
    {
      "name": "Engineering",
      "score": 0.08707839250564575
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84218800",
      "name": "University of California, Davis",
      "country": "US"
    }
  ]
}