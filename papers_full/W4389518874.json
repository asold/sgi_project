{
  "title": "A Closer Look into Using Large Language Models for Automatic Evaluation",
  "url": "https://openalex.org/W4389518874",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222756294",
      "name": "Cheng-Han Chiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2514219681",
      "name": "Hung-Yi Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250234233",
    "https://openalex.org/W4327811957",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2915756181",
    "https://openalex.org/W2972664115",
    "https://openalex.org/W4308902180",
    "https://openalex.org/W2294699749",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4223908421"
  ],
  "abstract": "Using large language models (LLMs) to evaluate text quality has recently gained popularity. Some existing prior works explore the idea of using LLMs for evaluation, while they differ in some details of the evaluation process. In this paper, we analyze *LLM evaluation* and *G-Eval*, and we discuss how those details in the evaluation process change how well the ratings given by LLMs correlate with human ratings. We find that the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more aligned with human ratings. We also show that forcing the LLM to output only a numeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8928–8942\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nA Closer Look into Automatic Evaluation Using Large Language Models\nCheng-Han Chiang\nNational Taiwan University,\nTaiwan\ndcml0714@gmail.com\nHung-yi Lee\nNational Taiwan University,\nTaiwan\nhungyilee@ntu.edu.tw\nAbstract\nUsing large language models (LLMs) to evalu-\nate text quality has recently gained popularity.\nSome prior works explore the idea of using\nLLMs for evaluation, while they differ in some\ndetails of the evaluation process. In this pa-\nper, we analyze LLM evaluation (Chiang and\nLee, 2023)1 and G-Eval (Liu et al., 2023), and\nwe discuss how those details in the evaluation\nprocess change how well the ratings given by\nLLMs correlate with human ratings. We find\nthat the auto Chain-of-Thought (CoT) used in\nG-Eval does not always make G-Eval more\naligned with human ratings. We also show that\nforcing the LLM to output only a numeric rat-\ning, as in G-Eval, is suboptimal. Last, we re-\nveal that asking the LLM to explain its own\nratings consistently improves the correlation\nbetween the ChatGPT and human ratings and\npushes state-of-the-art (SoTA) correlations on\ntwo meta-evaluation datasets.\n1 Introduction\nLarge language models (LLMs) trained with task\ninstructions and human feedback can follow natu-\nral language instructions to complete a task (Askell\net al., 2021; Sanh et al., 2022; Wei et al., 2022a;\nOuyang et al., 2022). Recently, the instruction-\nfollowing ability of LLMs makes them promising\ncandidates for automatic evaluation (Chiang and\nLee, 2023; Liu et al., 2023; Wang et al., 2023;\nHuang et al., 2023). By simply instructing the\nLLMs on how to rate and giving the LLMs the sam-\nple to be rated, the LLM can follow the instructions\nand provide a rating of the sample.\nChiang and Lee (2023) propose LLM evaluation\nand Liu et al. (2023) proposeG-Eval; both of which\nuse LLMs to evaluate samples by giving the LLM\ninstructions, and they both show that some LLMs\ncan yield evaluation results that are aligned to the\n1In this paper, the term LLM evaluation is used to refer to\nthe specific method proposed by Chiang and Lee (2023).\nevaluation results of humans. Still, LLM evaluation\nand G-Eval differ in some specific design choices\nin the evaluation procedure. Since Chiang and Lee\n(2023) and Liu et al. (2023) use distinct tasks, it is\nhard to know how the differences between LLM\nevaluation and G-Eval affect the evaluation results.\nThis makes practitioners in the future hard to deter-\nmine how to conduct an automatic evaluation using\nLLMs.\nGiven that LLM evaluation and G-Eval have al-\nready received significant attention shortly after\npublication, these methods will likely revolution-\nize the evaluation in NLP. Therefore, conducting\na detailed analysis of these approaches is essential\nand timely. This paper aims to identify the crucial\ncomponents in LLM evaluation and G-Eval that\ncontribute to stronger correlations with human rat-\nings. Based on our analysis, we provide guidelines\non how to use LLMs for automatic evaluations. We\nhave the following findings:\n• Auto-CoT (proposed by G-Eval) does not al-\nways improve the correlation between LLM\nand human ratings.\n• Making the LLMs output only a single nu-\nmeric rating is suboptimal.\n• Asking the LLMs to rationalize their own rat-\nings significantly improves the correlation be-\ntween the LLMs’ ratings and human ratings.\n• On two datasets, we improve the best correla-\ntion that ChatGPT’s rating can achieve, and\nsome correlations even exceed prior SoTA cor-\nrelations obtained using the ratings of GPT-4\nin Liu et al. (2023).\n2 Experiment Setup\nOur paper studies what components in LLM eval-\nuation and G-Eval make the ratings generated by\nLLM correlate with human ratings better, and we\naim to improve the correlation.\n8928\n2.1 LLM as an Automatic Evaluation Metric\nBoth LLM evaluation (Chiang and Lee, 2023) and\nG-Eval (Liu et al., 2023) propose to ask LLMs\nto rate a sample regarding some attributes of the\nsample (e.g., fluency, grammaticality) using a k-\npoint Likert scale. They give the LLMs (1) de-\nscriptions of the rating task, (2) the definition\nand rating criteria of the attribute to be rated, (3)\nthe sample to be rated, and (4) a sentence that\nprompts the LLM to give the rating2. The LLM\noutputs a sequence containing the rating. Unless\nspecified, we follow prior works to sampleN = 20\nsequences from the LLM and average those ratings\nas the final rating. While the two methods share\nthe core concept, they differ in two details.\nDifference 1: Auto Chain-of-Thought The\ntask descriptions and rating criteria in LLM eval-\nuation and G-Eval are all human-written. How-\never, Liu et al. (2023) argue that some evaluated\nattributes require more than simple definition and\nevaluation criteria, so they use LLMs to determine\nthe evaluation steps. Specifically, they concatenate\nthe task description, definition, and criteria of the\nattributes and append a line \"Evaluation steps:\"\nto prompt the LLM. The LLM then generates an\nordered list containing the step-by-step evaluation\nsteps. They dub this process auto chain-of-thought\n(CoT). G-Eval uses human-written task instructions\nand auto-CoT-generated evaluation steps to prompt\nthe LLM to rate the sample.\nDifference 2: Prompts for Output At the end\nof the input to LLMs, G-Eval uses the prompt\n\"{{placeholder}} (score only): \" to restrict\nthe LLM to output only the numeric rating; the\nplaceholder will be replaced by the evaluated at-\ntributes. In contrast, LLM evaluation uses the fol-\nlowing question to ask the LLM to assign the rating:\n\"How {{placeholder}} is the sample? (on\na scale of 1-k, with 1 being the lowest)\".\nThe LLM’s output form is not restricted.\n2.2 Meta-Evaluating an Evaluation Metric\nGiven a sample, an evaluation metric assigns it a\nrating. To evaluate an evaluation metric, we need\na dataset containing human ratings for samples in\nthe dataset. We calculate the correlation coefficient\nbetween the ratings obtained by the evaluation met-\nric and the human ratings. A higher correlation\n2In our paper, we use different highlight colors to repre-\nsent different parts of the prompt, as shown in the above text.\nAdditionally, we use cyan to represent the parts generated by\nauto Chain-of-Thought\nindicates the evaluation metric better aligns with\nhuman ratings. We adopt Pearson rand Kendall’s\nτ as they are widely used in meta-evaluations (Gra-\nham et al., 2015; Bojar et al., 2017; Zhang* et al.,\n2020). In our paper, all the correlation refers to\nthe correlation coefficient between the ratings of\nLLM and human ratings. Details on the calcula-\ntion of correlation coefficients are in Appendix C.\nWe use SummEval (Fabbri et al., 2021)\nand Topical-Chat (Gopalakrishnan et al., 2019;\nMehri and Eskenazi, 2020) as the meta-evaluation\ndatasets, following Liu et al. (2023). SummEval\nis a meta-evaluation dataset for summarization de-\nrived from the CNN/DailyMail dataset (Hermann\net al., 2015). Each summary in SummEval is rated\nby humans based on the coherence, consistency,\nfluency of the summary, and relevance between the\nsummary and the source document. Topical-Chat\nis a dataset that evaluates the quality of a response\ngiven the dialogue history and a piece of knowl-\nedge relating to the dialogue. We follow Zhong\net al. (2022) to evaluate the naturalness, coherence,\nengagingness, and groundedness (whether the re-\nsponse is grounded on the provided knowledge) of\nthe response. The dataset details are in Appendix E.\n2.3 Large Language Models\nAn LLM used as an evaluation metric should be\naffordable and accessible to whoever wants to\nuse it. Based on this principle, we use ChatGPT\n(gpt3.5-turbo-0613) (OpenAI, 2022) for evalu-\nation since it has lower cost and improved perfor-\nmance compared with other GPT-3.5 models. Chat-\nGPT is also used in LLM evaluation and G-Eval.\nWhile Liu et al. (2023) further use GPT-4 (OpenAI,\n2023) in their experiments, we cannot use GPT-4\nin our experiments since most people, including\nus, have limited or no access to GPT-4, making it\nutterly unsuitable as an evaluation metric.\nIn our preliminary experiments, we also try to\nuse the best open LLM (at the time of writing\nthis manuscript) on Open LLM leaderboard, the\nfalcon-40b-instruct model (Almazrouei et al.,\n2023), but we find it cannot follow the instructions\nand rate the samples very well. Hence, we exclude\nopen LLMs in our paper.\n3 Better Usage of LLM for Evaluation\n3.1 Is Auto CoT Always Useful?\nLiu et al. (2023) shows that adding the evaluation\nsteps generated by auto CoT improves the correla-\n8929\nSec. Ablations Coherence Consistency Fluency Relevance\nCoT Output r τ r τ r τ r τ\nGPT-4† ?‡ Score only 0.581 0.463 0.575 0.419 0.6 0.457 0.599 0.409\n✓ Score only 0.45 0.359 0.37 0.286 0.319 0.203 0.403 0.3273.1 ✗ 0.344 0.248 0.328 0.185 0.361 0.177 0.353 0.248\n✗ Score only 0.344 0.248 0.328 0.185 0.361 0.177 0.353 0.248\n✗ Free Text 0.46 0.342 0.476 0.334 0.477 0.273 0.324 0.228\n✗ Rate-explain 0.557 0.44 0.473 0.337 0.451 0.306 0.509 0.3483.2\n✗ Analyze-rate 0.635 0.476 0.537 0.34 0.479 0.302 0.444 0.305\nTable 1: The Pearson’s rand Kendall’s τ correlation coefficient between LLMs’ ratings and human ratings for\nSummEval. All the results in this table, except the first row, are from ChatGPT. We consider auto CoT + score\nonly using ChatGPT proposed in G-Eval as the baseline of this paper. We boldface the Pearson’s rstatistically\nsignificantly higher than the baseline (except GPT-4). †: results from Liu et al. (2023). Some numbers are different\nbecause we re-calculate the correlations based on the GPT-4 responses Liu et al. (2023) released. ‡: The results of\nGPT-4 cannot serve as a reasonable comparison since we find something odd in the prompts Liu et al. (2023) use,\nwhich we elaborate in Appendix A.\ntion on SummEval when using GPT-4 for evalua-\ntion. By scrutinizing their results, we find that the\ncorrelations when using auto CoT and not using\nit often differ by less than 0.02. This raises two\nquestions: (1) Is this difference statistically signifi-\ncant? (2) Does auto CoT yield higher correlations\nfor different LLMs and datasets? To answer these\nquestions, we use ChatGPT to rate the samples\nin SummEval and Topical-Chat using two sets of\nprompts, one with the evaluation steps generated\nusing auto CoT and one without those evaluation\nsteps. In this experiment, we follow G-Eval and\nrestrict ChatGPT to output only a numeric score.\nFollowing Graham and Baldwin (2014), we use\nWilliam’s test for significance to see if the Pear-\nson’s rof using and not using auto CoT is statisti-\ncally significantly different. We try to follow the\nprompts used in G-Eval when possible; still, we\nhave to construct some prompts since Liu et al.\n(2023) only release part of the prompts and some\nof which are problematic. We list all the prompts\nand how they are obtained in Appendix F.\nThe experiment results for SummEval are shown\nin the block in blue in Table 1. We also list the\nbest results of G-Eval using GPT-4 from Liu et al.\n(2023) in the first row of Table 1 only for refer-\nence. Comparing our results with GPT-4 is unfair\nsince we use ChatGPT, which is weaker than GPT-\n4. A more reasonable baseline for our paper is\nthe \"auto CoT + score only\" using ChatGPT on\nthe second row, which is the method proposed by\nG-Eval and shows the highest correlation that Chat-\nGPT can achieve in Liu et al. (2023). The numbers\nhere differ from results in Liu et al. (2023) because\nwe carefully reproduce their results ourselves.\nBack to Table 1, we can see that auto CoT leads\nto higher correlations for coherence, consistency,\nand relevance. By William’s test, these higher cor-\nrelations reach statistical significance withp-values\nless than 0.05. However, using auto CoT results in\na lower Pearson’s rfor fluency, and this inferiority\nin Pearson’s ris also statistically significant.\nThe results for Topical-Chat are illustrated in\nTable 2. For Topical-Chat, the Pearson’s rof us-\ning and not using auto CoT are very close for all\nfour attributes except groundedness, with differ-\nences less than 0.025, and these differences are\nnot statistically significant. For groundedness, auto\nCoT even drastically decreases the correlation. In\nsummary, using auto CoT does not yield consis-\ntent and meaningful improvements compared with\nnot using CoT. This should not be surprising since\nthe evaluation steps generated with auto CoT of-\nten merely paraphrases the evaluation criterion and\ninstructions given to the LLM.\n3.2 Prompt for Outputs\nIn this section, we explore if the difference in how\nChatGPT is prompted to output makes it’s ratings\nbetter aligned with human ratings. We use two\nsets of prompts that share the same task descrip-\ntions and evaluation criteria but differ in how they\nprompt the LLM to generate the output. One uses\n\"score only\", as in G-Eval. The other replaces\nthe \"score only \" with \" How {{placeholder}}\nis the sample? (on a scale of 1-k, with 1\nbeing the lowest)\", as in LLM evaluation. We\ncall the latter prompts free text since they do not\n8930\nSec. Ablations Naturalness Coherence Engagingness Groundedness\nCoT Output r τ r τ r τ r τ\n✓ Score only 0.393 0.358 0.468 0.391 0.549 0.513 0.311 0.5663.1 ✗ 0.408 0.331 0.443 0.404 0.557 0.535 0.358 0.582\n✗ Score only 0.408 0.331 0.443 0.404 0.557 0.535 0.358 0.582\n✗ Free Text 0.464 0.476 0.524 0.426 0.611 0.557 0.563 0.666\n✗ Rate-explain 0.524 0.47 0.477 0.416 0.567 0.524 0.58 0.693\n3.2\n✗ Analyze-rate 0.573 0.47 0.486 0.416 0.628 0.524 0.725 0.693\nTable 2: The Pearson’s rand Kendall’s τ correlation coefficient between LLMs’ ratings and human ratings for\nTopical-Chat. All the results in this table, except the first row, are from ChatGPT. We boldface the Pearson’s r\nstatistically significantly higher than auto CoT + score only. We underline the Pearson’s rcomparable auto CoT +\nscore only.\nrestrict the output form.\nThe results for SummEval are shown in the yel-\nlow blocks in Table 1, and the results for Topical-\nChat are shown in Table 2. We find that allowing\nChatGPT to respond to the question freely yields\nPearson’s rand Kendall’s τ much higher than re-\nstricting the model to output a single numeric score\nfor almost all attributes of both datasets. The higher\nPearson’s rof free text compared with score only\nis statistically significant. The only exception is\nthe relevance of SummEval, where free text yields\nslightly lower correlations.\nInitially, we thought ChatGPT aligns better with\nhuman ratings in free text because it can generate\nnatural language explanations to justify their rating,\nmaking the ratings more correlated with human\nratings. However, we observe that the responses\nof ChatGPT when prompted with free text mostly\ncontain a single numeric rating, which is the same\nbehavior when it is instructed by score only. This\nmeans that what the model is allowed to generate\nis more important than what it really generates.\nThe above observations make us curious if the\ncorrelations can be higher if ChatGPT is instructed\nto justify its ratings. Inspired by chain-of-thought\nin Wei et al. (2022b) and Kojima et al. (2022)\n(not the auto CoT in G-Eval), we ask ChatGPT\nto provide their reasoning and rationales on the\nratings. Instead of asking ChatGPT to output only\na score, we construct two types of prompts that ask\nChatGPT to rationalize its decision. The first type\nof prompt, called analyze-rate, asks ChatGPT to\nanalyze the samples regarding the evaluated criteria\nfirst and give the rating. The second type of prompt,\ncalled rate-explain, asks ChatGPT to provide the\nnumeric ratings first and explain why it gives such\na rating. analyze-rate is more like the zero-shot\nchain-of-thought (Kojima et al., 2022). Refer to\nAppendix F.1.1 for the exact prompts we use.\nThe results of asking ChatGPT to explain/ana-\nlyze how they rate the sample are shown in the last\ntwo rows in Table 1 and Appendix Table 2. We find\nthat for all attributes of both datasets, rate-explain\nand anlyze-rate both lead to correlations stronger\nthan or at least comparable to the correlation of ask-\ning ChatGPT to output only a numeric rating (score\nonly). By asking ChatGPT to explain/analyze, we\nimprove the best correlations that can be achieved\nby ChatGPT in Liu et al. (2023) (the Auto-CoT +\nscore only). Moreover, when asked to explain/an-\nalyze when rating, ChatGPT’s correlation can be\nbetter than or comparable to the state-of-the-art cor-\nrelation coefficients obtained from GPT-4 in Liu\net al. (2023) for coherence of SummEval and three\nattributes of Topical-Chat. We hypothesize that\nsome attributes (e.g., coherence for SummEval) are\nharder for ChatGPT to rate, so the correlations for\nthese attributes show a larger improvement when\nChatGPT explains how it rates the sample.\nIn rate-explain, the output of ChatGPT contains\na numeric rating followed by some explanations.\nAs an auto-regressive language model, ChatGPT\ncannot depend on the explanation when generating\nthe rating due to causal attention. If we stop the\ngeneration after ChatGPT generates the ratings,\nthe output of rate-explain will only contain the\nratings, just like the output forms in score only.\nAlthough the ratings in rate-explain do not depend\non ChatGPT’s rationales for the ratings, the ratings\nstill correlate better with human ratings, compared\nwith the ratings in score only. We think this is\nbecause when ChatGPT knows it needs to explain\nthe ratings, it tends to generate ratings that are\neasier for it to explain, and a rating that is more\n8931\naligned to humans’ rating is easier for ChatGPT to\nexplain.\n3.3 Empirical Guidelines\nBased on the analysis and results in this section,\nwe provide the following guideline: Always ask\nChatGPT to explain/analyze when rating. We\ndo not see rate-explain to be significantly better (or\nworse) than analyze-rate, so it is hard to determine\nwhich one to use. A valid method is sampling\nsome ratings using rate-explain and sampling some\nratings using analyze-rate and averaging the ratings\nfrom the two prompts as the final rating. Using\nauto CoT is optional since it does not always lead\nto higher correlations with human ratings. We also\nfind that using auto CoT does not always improve\nthe correlations when ChatGPT is asked to explain;\nthis result is shown in Appendix Table 3.\n3.4 Robustness of the Guidelines\nLLMs are notorious for their performance fluctu-\nation due to the input prompts, and the sequence\ngenerated by LLMs can be different when changing\nthe hyperparameters used in decoding. To verify\nthe validity of our empirical guidelines, we con-\nduct the following two sets of experiments: (1) we\nvary the temperature used in sampling the output\nfrom ChatGPT, and (2) we vary the prompt given\nto ChatGPT.\n3.4.1 Varying the Temperature\nWe check if our guideline holds if we change the\ntemperature T during generation. We compare\nPearson’s r when using the method proposed in\nG-Eval (Auto-CoT + score only) with rate-explain\nand analyze-rate under different temperatures used\nwhen generating the output from ChatGPT. We\nfollow Chiang and Lee (2023) and use two temper-\natures: 0.7 and 0.3.\nThe results are shown in Appendix Table 5 and\nsummarized as follows: First, when fixing the sam-\npling temperature, we find that rate-explain and\nanalyze-rate always achieve a higher correlation\ncompared with G-Eval. This supports our guide-\nline that \"asking the LLM to explain/analyze out-\nperforms the method proposed in G-Eval .\" Next,\nwe observe that the correlation of G-Eval when\nT = 0.3 is much lower than that of T = 1.0.\nThis shows that G-Eval is not robust to sampling\ntemperature. Contrarily, we find that the corre-\nlations obtained by rate-explain and analyze-rate\ndo not significantly change for different sampling\ntemperatures for almost all cases. This shows that\nrate-explain and analyze-rate are more robust than\nG-Eval with respect to the sampling temperature.\n3.4.2 Changing the Prompts\nWe check if our guideline holds if we change the\nprompt given to ChatGPT. In this experiment, we\nchanged the prompts to ChatGPT by appending\nsome instructions before the descriptions of the rat-\ning task. We tried with two prompts: (1) the HHH\nprompts and (2) the human annotator prompts. The\nHHH prompt is designed by Bai et al. (2022) to\nalign the output of LLMs to be more harmless, hon-\nest, and helpful. The human annotator prompt is\ninspired by Chiang and Lee (2023), who use a sim-\nilar prompt to make the LLM behave as a human\nannotator. These two prompts will be inserted be-\nfore the prompt we originally used in our paper.\nWe use these two prompts to inject persona into\nthe LLM. This is inspired by Zeng et al. (2023),\nwhich shows that the output of GPT3 can be differ-\nent when prompted with a different persona. The\nprompts are detailed in Appendix F.3.\nThe results are shown in Table 6 and summa-\nrized as follows: rate-explain and analyze-rate con-\nsistently outperform the G-eval when using the\nhuman annotator prompts and the HHH prompts.\nThis indicates that our guidelines are robust toward\ndifferent prompts. We also find that the correla-\ntions of G-Eval significantly drop when adding the\nhuman-annotator prompts or HHH prompts. On\nthe other hand, the correlation for rate-explain and\nanalyze-rate do not significantly decrease when\nadding the human-annotator prompt and the HHH\nprompt. This shows that asking the LLM to explain\nis more robust to the variation of the prompts.\n4 Conclusion\nWe study how to better use ChatGPT as an auto-\nmatic evaluation tool by scrutinizing LLM evalua-\ntion and G-Eval. We provide concrete guidelines\nand show that by using those guidelines, the cor-\nrelations of several evaluated attributes given by\nChatGPT, a publicly usable model, can be higher\nthan or comparable to the ratings given by GPT-4,\na highly restricted and pricey model. We also show\nthat the evaluation results based on our guidelines\nimprove the best correlation that ChatGPT’s rating\ncan achieve. We believe our results and guidelines\nhelp future researchers better use LLMs for evalua-\ntion.\n8932\nLimitations\nThere are three main limitations of this paper.\n1. We only use ChatGPT to conduct the experi-\nments in this paper. We explain why we chose\nChatGPT in Section 2.3. We believe that us-\ning ChatGPT is already enough since we show\nthat the correlations obtained by using Chat-\nGPT are already comparable to or better than\nthe previous SoTA results obtained by GPT-4.\n2. We only conduct analysis using two tasks,\nwhile we know that NLP has more diverse\ntasks. We do not guarantee that our observa-\ntions can generalize to all the other datasets.\nWe recommend the users verify the effective-\nness of using LLM to evaluate the tasks of\ninterest.\n3. We cannot fairly compare our results with Liu\net al. (2023), the previous SoTA results, due\nto multiple reasons. We explain those reasons\nin Appendix A.\nEthics Statement\nOur paper follows the ACL Code of Ethics.\nWe do not see a particular harmful out-\ncome of our paper. The code and datasets\nfor reproducing our experiments can be\nfound at https://github.com/d223302/\nA-Closer-Look-To-LLM-Evaluation/ .\nAcknowledgements\nWe want to thank the reviews for providing de-\ntailed feedback and actionable suggestions, which\nhelped us strengthen our paper. We also want to\nthank the senior committee members for monitor-\ning the reviewing process. Cheng-Han Chiang is\nsupported by a Ph.D. scholarship program by Delta\nElectronics.\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, et al. 2021. A\ngeneral language assistant as a laboratory for align-\nment. arXiv preprint arXiv:2112.00861.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022. Training a help-\nful and harmless assistant with reinforcement learn-\ning from human feedback.\nOndˇrej Bojar, Yvette Graham, and Amir Kamran. 2017.\nResults of the WMT17 metrics shared task. In Pro-\nceedings of the Second Conference on Machine Trans-\nlation, pages 489–513, Copenhagen, Denmark. Asso-\nciation for Computational Linguistics.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15607–15631, Toronto,\nCanada. Association for Computational Linguistics.\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. Summeval: Re-evaluating summariza-\ntion evaluation. Transactions of the Association for\nComputational Linguistics, 9:391–409.\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qinlang\nChen, Anna Gottardi, Sanjeev Kwatra, Anushree\nVenkatesh, Raefer Gabriel, and Dilek Hakkani-Tür.\n2019. Topical-chat: Towards knowledge-grounded\nopen-domain conversations.\nYvette Graham and Timothy Baldwin. 2014. Testing\nfor significance of increased correlation with human\njudgment. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 172–176, Doha, Qatar. Association\nfor Computational Linguistics.\nYvette Graham, Timothy Baldwin, and Nitika Mathur.\n2015. Accurate evaluation of segment-level machine\ntranslation metrics. In Proceedings of the 2015 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1183–1191, Denver, Col-\norado. Association for Computational Linguistics.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. Advances in neural information\nprocessing systems, 28.\nFan Huang, Haewoon Kwak, and Jisun An. 2023. Is\nchatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate\nspeech. arXiv preprint arXiv:2302.07736.\n8933\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nMatouš Macháˇcek and Ondˇrej Bojar. 2014. Results of\nthe WMT14 metrics shared task. In Proceedings of\nthe Ninth Workshop on Statistical Machine Trans-\nlation, pages 293–301, Baltimore, Maryland, USA.\nAssociation for Computational Linguistics.\nShikib Mehri and Maxine Eskenazi. 2020. Usr: An\nunsupervised and reference free evaluation metric\nfor dialog generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 681–707.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue. Accessed on January 10, 2023.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022a. Finetuned language\nmodels are zero-shot learners. In International Con-\nference on Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nAndy Zeng, Maria Attarian, brian ichter,\nKrzysztof Marcin Choromanski, Adrian Wong,\nStefan Welker, Federico Tombari, Aveek Purohit,\nMichael S Ryoo, Vikas Sindhwani, Johnny Lee, Vin-\ncent Vanhoucke, and Pete Florence. 2023. Socratic\nmodels: Composing zero-shot multimodal reasoning\nwith language. In The Eleventh International\nConference on Learning Representations.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022. Towards a unified multi-\ndimensional evaluator for text generation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2023–\n2038, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nA Why We Cannot Fairly Compare with\nthe Results in Liu et al. (2023)\nAs a work highly related to G-Eval, we would re-\nally like to compare our results with G-Eval. How-\never, we encounter difficulties when comparing\nour results with those in Liu et al. (2023) for the\nfollowing reasons.\n• G-Eval proposes to use GPT-4 as the evalu-\nation tool, while it is currently a highly re-\nstricted model, and we only have limited ac-\ncess to it.\n• G-Eval only releases the prompts for Sum-\nmEval. We need to construct the prompts for\nTopical-Chat based on the human evaluation\ninstructions released by Mehri and Eskenazi\n(2020). It is possible that the prompts we\nuse for Topical-Chat are different from the\nprompts used in Liu et al. (2023), making\ntheir results incomparable to ours.\n• The prompts of fluency in SummEval released\nby Liu et al. (2023) in here is problematic so\nwe need to construct new prompts for fluency.\nRefer to Appendix F.1 for detailed explana-\ntions. This makes us unable to directly com-\npare our results with the results in Liu et al.\n(2023).\n• We cannot reproduce the numbers on the pa-\nper of G-Eval even when using their official\nimplementation and the GPT-4 responses they\nrelease. This means that the only thing we\n8934\ndo is calculate the correlation coefficient us-\ning the data and code released on the official\nGitHub of G-Eval, but the numbers are quite\ndifferent from the results in G-Eval’s paper.\nMoreover, the results of fluency they provide\nis the result not using auto CoT, but the results\nof the other three attributes for SummEval use\nauto CoT. That is why we use a question mark\nfor the auto CoT field in Table 1.\n• The Table 2 in Liu et al. (2023) seems to\nbe wrong. The caption (Spearman’s ρ and\nKendall’s τ) does not match the headers ( r\nand ρ). This makes us hard to compare their\nresults with ours reliably.\nB Supplementary Results for\nTopical-Chat\nTable 2 is the supplementary results for Topical-\nChat that we referred to in the main content. We\nplan to move Table 2 to the main content using the\nadditional one page in the camera-ready version\nif the paper is accepted. See how Pearson’s rand\nKendall’s τ are calculated in Appendix C.\nB.1 Is Auto CoT Useful When ChatGPT Is\nAsked to Explain?\nIn Table 3, we show the results when we add the\nevaluation steps generated by auto CoT when we\nask ChatGPT when prompting with (rate-explain).\nWe find that on groundedness, using auto CoT is\nworse. However, for the other three attributes, us-\ning auto CoT is better. This again shows that auto\nCoT is not particularly useful.\nC Calculation of Correlation Coefficient\nIn this paper, we calculate Pearson’s r and\nKendall’s τ between human ratings and ChatGPT’s\nratings. Whether to use Spearman’s rank corre-\nlation or Pearson’s (linear) correlation to evalu-\nate the alignment between human ratings and an\nautomatic evaluation metric is long-standing, but\nthere has been an increasing trend towards Pear-\nson’s correlation since 2014 (Macháˇcek and Bojar,\n2014; Graham and Baldwin, 2014; Zhang* et al.,\n2020). We use the pearsonr and kendalltau in\nscipy.stats for calculating the correlation coeffi-\ncients. For each attribute of each sample, the rating\nof ChatGPT is obtained by 20 samples; we set the\ndecoding temperature to 1 and the top-pin nucleus\nsampling to 1, following G-Eval (Liu et al., 2023).\nConsider a dataset with N source documents,\nand each source document has M corresponding\ntarget documents. We also have the human ratings\nfor N ·M target documents on a specific attribute.\nWhile each attribute of each target document is\nrated by more than one human rater, we average\nthose ratings when calculating the correlation coef-\nficient. So the N·M ratings are the average ratings\nfrom different raters. In the case of SummEval, we\nhave N = 100 source documents and M = 16\nsummaries generated by 16 summarization mod-\nels. There are two different methods for calculating\ncorrelation coefficients.\nC.0.1 Method 1: Dataset-Level Correlation\nCoefficient\nIn this method, we first obtain the ratings onN·M\ntarget documents from ChatGPT. We then calcu-\nlate the correlation coefficient between the N ·M\nChatGPT’s ratings and the N ·M average human\nratings. In this case, the correlation coefficient is\ncalculated among two N·M vectors, meaning that\nthe correlation coefficient is calculated across the\nentire dataset.\nC.0.2 Method 2: Document-Level Correlation\nCoefficient\nIn this method, for each source document, we ob-\ntain the ratings of its M target documents using\nChatGPT. Next, we calculate the correlation coef-\nficient between these M ChatGPT ratings and the\ncorresponding M human ratings. After iterating\nthe above process over all theN source documents,\nwe obtain the N correlation coefficients. We av-\nerage the N correlation coefficients as the final\ncorrelation coefficient. In this case, the correlation\ncoefficient is calculated at the document-level and\naveraged over the whole dataset.\nC.1 How We Calculate the Correlation\nCoefficient\nIn Table 1 and 2 in this paper, we use Method\n1 (Subsection C.0.1) to calculate Pearson’s corre-\nlation, following the recommendation in Graham\net al. (2015). Calculating the correlation coefficient\non the dataset level is also used in LLM evalua-\ntion (Chiang and Lee, 2023). Calculating a single\ncorrelation coefficient on the dataset level allows\nus to use William’s test to test whether two Pear-\nson’s rare significantly different.\nFor Kendall’sτ in Table 1 and 2, we follow most\nprior works (Zhong et al., 2022; Liu et al., 2023) to\n8935\nSec. Ablations Naturalness Coherence Engagingness Groundedness\nCoT Output r τ r τ r τ r τ\n✗ Score only 0.393 0.358 0.468 0.391 0.549 0.513 0.311 0.566\n✓ rate-explain 0.554 0.478 0.512 0.429 0.613 0.566 0.555 0.6643.2\n✗ rate-explain 0.524 0.47 0.477 0.416 0.567 0.524 0.58 0.693\nTable 3: The Pearson’s rand Kendall’s τ correlation coefficient between LLMs’ ratings and human ratings for\nTopical-Chat. All the results in this table, except the first row, are from ChatGPT. We boldface the Pearson’s r\nstatistically significantly higher than auto CoT + score only. We underline the Pearson’s rcomparable auto CoT +\nscore only.\ncalculate Kendall’s τ using Method 2 (document-\nlevel, Section C.0.2) to understand if ChatGPT can\ndifferentiate the quality difference between differ-\nent system outputs for the same source document.\nIn fact, we find that Pearson’s r calculated by\nMethod 1 and Method 2 are highly correlated. In\nTable 4, we show the result of Topical-Chat while\nwe use Method 2 to calculate Pearson’sr; Kendall’s\nτ is still calculated by Method 2. Comparing the\nresults of Pearson’s rin Table 2 and Table 4, one\ncan easily see that when a method have signifi-\ncantly higher Pearson’s r in Table 2, it will also\nhave significantly higher Pearson’s r. We present\nthe r calculated by Method 1 because it makes\nmore sense when calculating statistical significance\nwhen the correlation coefficient is calculated at the\ndataset-level (Graham et al., 2015).\nD Results of Changing the Temperature\nand Prompts\nWe show the results of varying the temperature\nused to sample the ChatGPT output in Table 5. In\nthe experiments in this section, we only sample\nN = 5 samples from the ChatGPT since we find\nthat G-eval and our proposed guidelines are quite\nrobust to the number of samples when N ≥5.\nE Datasets\nE.1 SummEval\nSummEval (Fabbri et al., 2021) is a dataset for\nthe meta-evaluation of summarization. It contains\n100 source documents, each with 16 summaries ob-\ntained from different summarization models. Each\nof the 1600 summaries is rated by three workers\nrecruited on Amazon Mturk and two experts in\nsummarization. Each summary in SummEval is\nrated by humans based on the coherence, consis-\ntency, fluency of the summary, and relevance be-\ntween the summary and the source document. Each\nattribute is rated based on a 5-point Likert scale.\nWe download the source documents, summaries,\nand human ratings from the GitHub repository of\nG-Eval (https://github.com/nlpyang/geval/\ntree/8f54105/data). SummEval was released\nunder MIT License, and our usage for research\ndoes not violate the dataset’s initial intention.\nE.2 Topical-Chat\nTopical-Chat (Gopalakrishnan et al., 2019) is\na knowledge-grounded open-domain dialogue\ndataset. The dataset consists of a dialogue context\n(history), an interesting fact related to the topic\nof the conversation, and a response. Mehri and\nEskenazi (2020) releases high-quality human\nannotations on the quality of responses. They\nconstruct the dataset as follows: they first sample\n60 dialogues context from Topical-Chat, and for\neach dialogue context and corresponding fun fact,\nthey use a transformer model to generate four\nresponses using four decoding methods. Each\ndialogue content has two additional responses: the\nhuman response and the ground truth response.\nThus, there are a total of 360 dialogue-response\npairs. Those pairs are evaluated based on six\nattributes, and we follow Zhong et al. (2022)\nand Liu et al. (2023) to only use four attributes:\nnaturalness, coherence, engagingness, and\ngroundedness (whether the response is grounded\non the provided knowledge). We obtain the\nhuman ratings of Topical-Chat from the Github\nrepository of UniEval (Zhong et al., 2022):\nhttps://github.com/maszhongming/UniEval/\nblob/main/reproduce/data/dialogue/\ntopical_chat.json.\nF Prompts\nWe list the prompts we use in this section. In the\nmain content of the paper and in the following parts,\nwe use different highlight colors to represent dif-\nferent parts of the prompt. A prompt is composed\n8936\nSec. Ablations Naturalness Coherence Engagingness Groundedness\nCoT Output r τ r τ r τ r τ\nGPT-4† ✓ Score only 0.549 - 0.594 - 0.627 - 0.531 -\n✓ Score only 0.445 0.358 0.498 0.391 0.579 0.513 0.685 0.5663.1 ✗ 0.431 0.331 0.507 0.404 0.631 0.535 0.666 0.582\n✗ Score only 0.431 0.331 0.507 0.404 0.631 0.535 0.666 0.582\n✗ Free Text 0.572 0.476 0.523 0.426 0.676 0.557 0.747 0.666\n✗ Rate-explain 0.621 0.512 0.472 0.425 0.61 0.509 0.771 0.6633.2\n✗ Analyze-rate 0.573 0.47 0.486 0.416 0.628 0.524 0.725 0.693\nTable 4: The Pearson’s rand Kendall’s τ correlation coefficient between LLMs’ ratings and human ratings for\nTopical-Chat. Note that in this table, both Pearson’srand Kendall’sτ are calculated by Method 2 in Appendix C.0.2.\nAll the results in this table, except the first row, are from ChatGPT. The results of GPT-4 are from Liu et al. (2023)\nbut should not be compared with our results since the prompts they use may be different from the prompt we use.\nStill, we can see that for naturalness, engagingness, and grounedness, the results of rate-explain and analyze-rate is\nbetter or comparable to GPT-4.\nof four parts: (1) the descriptions of the rating\ntask, (2) the definition and rating criteria of the\nattribute to be rated, (3) the sample to be rated ,\nand (4) a sentence used to prompt the LLM to\ngive the rating.\nThe prompts for different attributes of the same\ndataset share the same descriptions of the rating\ntask. Different attributes use different definition\nand rating criteria. In G-Eval, the prompts also\ncompose of the evaluation steps generated by auto\nCoT.\nF.1 Prompts for SummEval\nThe descriptions of the rating task, the\ndefinition and rating criteria, the evalua-\ntion steps for coherence, consistency, and\nrelevance in SummEval is from the prompts\nreleased by G-Eval in their GitHub repository\n(https://github.com/nlpyang/geval/tree/\n8f54105/prompts/summeval). While G-Eval\nalso releases the prompt they use for fluency,\nwe find something highly problematic in the\nprompt they use. The prompt for fluency asks\nthe LLM to rate fluency on a scale of 1 to 3\n(https://github.com/nlpyang/geval/blob/\n8f54105061e00377fbb909153892d5bfb5b3623a/\nprompts/summeval/flu_detailed.txt), while\nthe original rating scale in SummEval is 1 to 5 .\nWe also find that the original rating criteria used in\nG-Eval for fluency differ largely from the rating\ncriteria of fluency used for human evaluation in\nSummEval. Through our experiment, we find\nthat the misalignment of evaluation criteria and\nevaluation scale significantly decreases Pearson’s\nrwith human ratings when using analyze-rate to\nprompt ChatGPT to output. This is likely because\nChatGPT tends to stick to the rating criteria when\nprompted with analyze-rate, and when using\nthe rating criteria different from the criteria that\nare used to instruct the human raters, the scores\ngenerated by ChatGPT deviates more from the\nhuman ratings. This highlights the importance of\nusing the same instructions to the LLM as those\ninstructions used in the human evaluation, as\nemphasized in Chiang and Lee (2023).\nFirst, we show an example prompt forcoherence.\nThis prompt corresponds to the score only + auto\nCoT in Table 1.\nCoherence\nYou will be given one summary written for\na news article.\nYour task is to rate the summary on one\nmetric.\nPlease make sure you read and understand\nthese instructions carefully. Please keep\nthis document open while reviewing, and\nrefer to it as needed.\nEvaluation Criteria:\nCoherence (1-5) - the collective quality\nof all sentences. We align this dimension\nwith the DUC quality question of struc\nture and coherence whereby \"the summary\nshould be well-structured and well-orga\nnized. The summary should not just be a\nheap of related information, but should\nbuild from sentence to a coherent body of\ninformation about a topic.\"\nEvaluation Steps:\n1. Read the news article carefully and\n8937\nAuto-CoT Output Coherence Consistency Fluency Relevance\n✓ Score only 0.356 0.290 0.261 0.263\n✗ Rate-explain 0.548 0.482 0.423 0.487\n✗ Analyze-rate 0.589 0.439 0.438 0.319\n(a) Temperature T = 0.3\nAuto-CoT Output Coherence Consistency Fluency Relevance\n✓ Score only 0.394 0.256 0.288 0.334\n✗ Rate-explain 0.526 0.468 0.414 0.485\n✗ Analyze-rate 0.605 0.448 0.441 0.392\n(b) Temperature T = 0.7\nAuto-CoT Output Coherence Consistency Fluency Relevance\n✓ Score only 0.450 0.370 0.319 0.403\n✗ Rate-explain 0.557 0.473 0.452 0.509\n✗ Analyze-rate 0.635 0.534 0.479 0.444\n(c) Temperature T = 1.0 (The result in Table 1)\nTable 5: Comparing G-Eval (Auto-CoT + score only) with rate-explain and analyze-rate at different temperatures.\nWe boldface Pearson’s r statistically significantly higher than the baseline (the first row in each subtable).\nAuto-CoT Output Coherence Consistency Fluency Relevance\n✓ Score only 0.308 0.248 0.265 0.345\n✗ Rate-explain textbf0.526 0.468 0.414 0.485\n✗ Analyze-rate 0.589 0.524 0.459 0.416\n(a) Results when prompted with the human evaluator prompts.\nAuto-CoT Output Coherence Consistency Fluency Relevance\n✓ Score only 0.325 0.206 0.281 0.301\n✗ Rate-explain 0.596 0.465 0.403 0.478\n✗ Analyze-rate 0.596 0.493 0.475 0.406\n(b) Results when prompted with the HHH prompts.\nTable 6: Comparing G-Eval (Auto-CoT + score only) with rate-explain and analyze-rate when using different\nprompts. We boldface Pearson’s r statistically significantly higher than the baseline (the first row in each subtable).\n8938\nidentify the main topic and key points.\n2. Read the summary and compare it to the\nnews article. Check if the summary cov\ners the main topic and key points of the\nnews article, and if it presents them in\na clear and logical order.\n3. Assign a score for coherence on a scale\nof 1 to 5, where 1 is the lowest and 5\nis the highest based on the Evaluation\nCriteria.\nExample:\nSource Text: {{Document}}\nSummary: {{Summary}}\nEvaluation Form (scores ONLY):\n- Coherence:\nF.1.1 Different Output Prompts\nFor different output prompts, which is the ablation\nin Section 3.2 and the last block in Table 1 and 2,\nwe only change the yellow parts (the last part) in\nthe example prompt above. There are four output\nprompts used in Section 3.2: score only, free text,\nrate-explain, and analyze-rate. The prompts for\nfree text is attribute-dependent, and we list them in\nthe Their corresponding output prompts are listed\nas follows:\nScore only\nEvaluation Form (scores ONLY):\n- {Attribute}:\nRate-explain\nEvaluation Form (Answer by starting with\n\"Rating:\" and then give the explanation\nof the rating on the next line by \"Ratio\nnale:\"):\n- {Attribute}:\nAnalyze-rate\nEvaluation Form (Answer by starting with\n\"Analysis:\" to analyze the given example\nregarding the evaluation criteria as con\ncise as possible, and then give the nu\nmeric rating on the next line by \"Rat\ning:):\n- {Attribute}:\nF.1.2 Attribute-Dependent Prompts\nThe definition and rating criteria of the attribute\nto be rated, the evaluation steps generated by auto\nCoT, and output prompt for text-free are attribute-\ndependent, and we list them as follows. We use dif-\nferent colors to denote different parts in the prompt.\nNote that the following prompts are not the com-\nplete prompts used as the model input; they need\nto be used with the descriptions of the rating task\nand the sample to be rated.\nCoherence\nEvaluation Criteria:\nCoherence (1-5) - the collective quality\nof all sentences. We align this dimen\nsion with the DUC quality question of\nstructure and coherence whereby \"the\nsummary should be well-structured and\nwell-organized. The summary should not\njust be a heap of related information,\nbut should build from sentence to a\ncoherent body of information about a\ntopic.\"\nEvaluation Steps:\n1. Read the news article carefully and\nidentify the main topic and key points.\n2. Read the summary and compare it to\nthe news article. Check if the summary\ncovers the main topic and key points of\nthe news article, and if it presents them\nin a clear and logical order.\n3. Assign a score for coherence on a\nscale of 1 to 5, where 1 is the lowest and\n5 is the highest based on the Evaluation\nCriteria.\nQuestion:\nHow coherent is the summary? That is,\nhow well do the sentences in the summary\nfit together? (On a scale of 1-5, with 1\nbeing the lowest)\nConsistency\nEvaluation Criteria:\nConsistency (1-5) - the factual alignment\nbetween the summary and the summarized\nsource. A factually consistent summary\ncontains only statements that are en\ntailed by the source document. Annotators\nwere also asked to penalize summaries\nthat contained hallucinated facts.\nEvaluation Steps:\n1. Read the news article carefully and\nidentify the main facts and details it\npresents.\n2. Read the summary and compare it to the\n8939\narticle. Check if the summary contains\nany factual errors that are not supported\nby the article.\n3. Assign a score for consistency based\non the Evaluation Criteria.\nQuestion:\nHow consistent is the summary with the\nsource document in terms of the factual\nalignment? (On a scale of 1-5, with 1\nbeing the lowest)\nFluency\nEvaluation Criteria:\nFluency (1-5): This rating measures the\nquality of individual sentences, are\nthey well-written and grammatically cor\nrect. Consider the quality of individual\nsentences.\nEvaluation steps:\n1. Read the given summary.\n2. Evaluate the fluency of the summary\non a scale of 1-5 based on the criteria\nprovided.\n3. Provide the rating.\nQuestion:\nBased on the evaluation criteria, how\nfluent is the summary? (On a scale of\n1-5, with 1 being the lowest)\nRelevance\nEvaluation Criteria:\nRelevance (1-5) - selection of important\ncontent from the source. The summary\nshould include only important informa\ntion from the source document. Annotators\nwere instructed to penalize summaries\nwhich contained redundancies and excess\ninformation.\nEvaluation Steps:\n1. Read the summary and the source\ndocument carefully.\n2. Compare the summary to the source\ndocument and identify the main points of\nthe article.\n3. Assess how well the summary covers\nthe main points of the article, and how\nmuch irrelevant or redundant information\nit contains.\n4. Assign a relevance score from 1 to 5.\nQuestion:\nOn a scale of 1-5, with 1 being the\nlowest, is the summary relevant to the\nsource document and does the summary only\ncontain the important information of the\nsource document?\nF.2 Prompts for Topical-Chat\nFirst, we show an example prompt for naturalness.\nThis prompt corresponds to the score only + auto\nCoT in Table 2.\nNaturalness\nYou will be given a conversation between\ntwo individuals. You will then be given\none potential response for the next\nturn in the conversation. The response\nconcerns an interesting fact, which will\nbe provided as well.\nYour task is to rate the responses on\none metric.\nPlease make sure you read and understand\nthese instructions carefully. Please\nkeep this document open while reviewing,\nand refer to it as needed.\nEvaluation Crieteria:\nNaturalness (1-3) Is the response natu\nrally written??\n- A score of 1 (bad) means that the\nresponse is unnatural.\n- A score of 2 (ok) means the response\nis strange, but not entirely unnatural.\n- A score of 3 (good) means that the\nresponse is natural.\nEvaluation Steps:\n1. Read the conversation between the two\nindividuals.\n2. Read the potential response for the\nnext turn in the conversation.\n3. Evaluate the response based on its\nnaturalness, using the provided criteria.\n4. Assign a rating score of 1, 2, or 3\nbased on the evaluation.\nExample:\nConversation History:\n{{Document}}\nCorresponding Fact:\n8940\n{{Fact}}\nResponse:\n{{Response}}\nEvaluation Form (scores ONLY):\n- Naturalness:\nF.2.1 Different Output Prompts\nFor Topical-Chat, we also conduct ablations on\ndifferent output prompts. Those different output\nprompts for score only, rate-explain, analyze-rate\nare the same as those listed in Section F.1.1. We do\nnot list them here to save some space. The exact\nprompts we use can be found in the supplementary\ndata of this paper.\nF.2.2 Attribute-Dependent Prompts\nThe definition and rating criteria of the attribute\nto be rated, the evaluation steps generated by auto\nCoT, and output prompt for text-free are attribute-\ndependent, and we list them as follows. Again, the\nfollowing prompts are not the complete prompts\nused as the model input; they need to be used with\nthe descriptions of the rating task and the sample\nto be rated.\nNaturalness\nEvaluation Crieteria:\nNaturalness (1-3) Is the response natu\nrally written??\n- A score of 1 (bad) means that the\nresponse is unnatural.\n- A score of 2 (ok) means the response\nis strange, but not entirely unnatural.\n- A score of 3 (good) means that the\nresponse is natural.\nEvaluation Steps:\n1. Read the conversation between the two\nindividuals.\n2. Read the potential response for the\nnext turn in the conversation.\n3. Evaluate the response based on its\nnaturalness, using the provided criteria.\n4. Assign a rating score of 1, 2, or 3\nbased on the evaluation.\nQuestion:\nHow natural is the reponse? (On a scale\nof 1-3, with 1 being the lowest)\nCoherence\nEvaluation Crieteria:\nCoherence (1-3) Does the response serve\nas a valid continuation of the conversa\ntion history?\n- A score of 1 (no) means that the\nresponse drastically changes topic or\nignores the conversation history.\n- A score of 2 (somewhat) means the\nresponse refers to the conversation\nhistory in a limited capacity (e.g., in a\ngeneric way) and shifts the conversation\ntopic.\n- A score of 3 (yes) means the response\nis on topic and strongly acknowledges\nthe conversation history.\nEvaluation Steps:\n1. Read the conversation history.\n2. Read the potential response.\n3. Evaluate the coherence of the response\nbased on the conversation history.\n4. Assign a score of 1, 2, or 3 for\ncoherence.\nQuestion:\nDoes the response serve as a valid con\ntinuation of the conversation history?\n(On a scale of 1-3, with 1 meaning the\nresponse is invalid and 3 meaning the\nresponse is coherent)\nEngagingness\nEvaluation Crieteria:\nEngagingness (1-3) Is the response dul\nl/interesting?\n- A score of 1 (dull) means that the\nresponse is generic and dull.\n- A score of 2 (somewhat interesting)\nmeans the response is somewhat inter\nesting and could engage you in the\nconversation (e.g., an opinion, thought)\n- A score of 3 (interesting) means the\nresponse is very interesting or presents\nan interesting fact\nEvaluation Steps:\n1. Read the conversation, the correspond\ning fact and the response carefully.\n2. Rate the response on a scale of\n1-3 for engagingness, according to the\ncriteria above.\nQuestion:\n8941\nIs the response interesting and engaging?\n(On a scale of 1-3, with 1 meaning dull\nand 3 meaning interesting)\nGroundedness\nEvaluation Crieteria:\nGroundedness (0-1) given the fact that\nthis response is conditioned on, deter\nmine whether this response uses that\nfact.\n- A score of 0 (no) means the response\ndoes not mention or refer to the fact at\nall\n- A score of 1 (yes) means the response\nuses the fact well\nEvaluation Steps:\n1. Read the conversation between the two\nindividuals.\n2. Identify the fact that is provided\nfor the potential response.\n3. Read the potential response.\n4. Determine if the potential response\nuses or mentions the fact.\n5. Assign a score of 0 or 1 for grounded\nness based on whether the response uses\nthe fact.\nQuestion:\nGiven the fact that this response is\nconditioned on, does the response use\nthe fact? (On a scale of 0-1, with 0\nmeaning no and 1 meaning yes)\nF.3 Prompts for Section 3.4.2\nHHH prompts You are an AI assistant.\nThe AI tries to be helpful, polite,\nhonest, sophisticated, emotionally aware,\nand humble-but-knowledgeable. The\nassistant is happy to help with almost\nanything, and will do its best to\nunderstand exactly what is needed.\nHuman annotator prompts Assume that\nyou are a professional and careful human\nevaluator. You are recruited and paid to\nconduct the following task. You need to\nstrictly follow the task instruction and\nensure that you are doing the job with\nhigh-quality.\n8942",
  "topic": "Popularity",
  "concepts": [
    {
      "name": "Popularity",
      "score": 0.8758845925331116
    },
    {
      "name": "Computer science",
      "score": 0.6217840313911438
    },
    {
      "name": "Forcing (mathematics)",
      "score": 0.5629063844680786
    },
    {
      "name": "Process (computing)",
      "score": 0.5037664771080017
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4919445216655731
    },
    {
      "name": "Natural language processing",
      "score": 0.4687647521495819
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4350486099720001
    },
    {
      "name": "Machine learning",
      "score": 0.3366987705230713
    },
    {
      "name": "Psychology",
      "score": 0.28915780782699585
    },
    {
      "name": "Social psychology",
      "score": 0.18582594394683838
    },
    {
      "name": "Mathematics",
      "score": 0.11568111181259155
    },
    {
      "name": "Epistemology",
      "score": 0.07234320044517517
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ],
  "cited_by": 22
}