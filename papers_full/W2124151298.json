{
    "title": "Modeling language and cognition with deep unsupervised learning: a tutorial overview",
    "url": "https://openalex.org/W2124151298",
    "year": 2013,
    "authors": [
        {
            "id": "https://openalex.org/A5008796300",
            "name": "Marco Zorzi",
            "affiliations": [
                "IRCCS San Camillo Hospital",
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A5027874987",
            "name": "Alberto Testolin",
            "affiliations": [
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A5000360625",
            "name": "Ivilin Peev Stoianov",
            "affiliations": [
                "Institute of Cognitive Sciences and Technologies",
                "National Research Council",
                "University of Padua"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2042492924",
        "https://openalex.org/W6986839675",
        "https://openalex.org/W6959456255",
        "https://openalex.org/W2181347294",
        "https://openalex.org/W4231109964",
        "https://openalex.org/W6676481782",
        "https://openalex.org/W2162693531",
        "https://openalex.org/W2046809950",
        "https://openalex.org/W6674968403",
        "https://openalex.org/W2153791616",
        "https://openalex.org/W2026799324",
        "https://openalex.org/W6684859321",
        "https://openalex.org/W2162950292",
        "https://openalex.org/W2005763686",
        "https://openalex.org/W2167044231",
        "https://openalex.org/W6676391964",
        "https://openalex.org/W2127958135",
        "https://openalex.org/W2020999234",
        "https://openalex.org/W2083227311",
        "https://openalex.org/W2167956961",
        "https://openalex.org/W6605580055",
        "https://openalex.org/W2016534914",
        "https://openalex.org/W6680086184",
        "https://openalex.org/W2084336274",
        "https://openalex.org/W2116064496",
        "https://openalex.org/W2124537004",
        "https://openalex.org/W44815768",
        "https://openalex.org/W2119574220",
        "https://openalex.org/W1981814724",
        "https://openalex.org/W6758710848",
        "https://openalex.org/W2136922672",
        "https://openalex.org/W2100495367",
        "https://openalex.org/W323291900",
        "https://openalex.org/W4230449744",
        "https://openalex.org/W2106145030",
        "https://openalex.org/W1997128234",
        "https://openalex.org/W2620776971",
        "https://openalex.org/W6633648486",
        "https://openalex.org/W1990517717",
        "https://openalex.org/W1511986666",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2120480077",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W2130325614",
        "https://openalex.org/W2092873805",
        "https://openalex.org/W2170014483",
        "https://openalex.org/W1597739853",
        "https://openalex.org/W2169281351",
        "https://openalex.org/W2073257493",
        "https://openalex.org/W2086789740",
        "https://openalex.org/W6683819664",
        "https://openalex.org/W2567948266",
        "https://openalex.org/W2432567885",
        "https://openalex.org/W2145889472",
        "https://openalex.org/W2152742558",
        "https://openalex.org/W2153758134",
        "https://openalex.org/W2159080219",
        "https://openalex.org/W2095722689",
        "https://openalex.org/W2117350000",
        "https://openalex.org/W2022476005",
        "https://openalex.org/W2161070585",
        "https://openalex.org/W2007800656",
        "https://openalex.org/W2120432001",
        "https://openalex.org/W2119885245",
        "https://openalex.org/W2149194912",
        "https://openalex.org/W2040870580",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W2037139345",
        "https://openalex.org/W2541474375",
        "https://openalex.org/W1490454746",
        "https://openalex.org/W6607775107",
        "https://openalex.org/W2169214866",
        "https://openalex.org/W2030831236",
        "https://openalex.org/W2064658612",
        "https://openalex.org/W2614910609",
        "https://openalex.org/W2017158895",
        "https://openalex.org/W2135341757",
        "https://openalex.org/W6689723076",
        "https://openalex.org/W2158164339",
        "https://openalex.org/W2296248009",
        "https://openalex.org/W2108379336",
        "https://openalex.org/W2116945806",
        "https://openalex.org/W1603152151",
        "https://openalex.org/W2004503299",
        "https://openalex.org/W6712427120",
        "https://openalex.org/W2072128103",
        "https://openalex.org/W3207342693",
        "https://openalex.org/W4301630257",
        "https://openalex.org/W137420995",
        "https://openalex.org/W4366495025",
        "https://openalex.org/W2133257461",
        "https://openalex.org/W1997063559",
        "https://openalex.org/W2504108613",
        "https://openalex.org/W4254816979",
        "https://openalex.org/W2161893161",
        "https://openalex.org/W2253807446",
        "https://openalex.org/W2110798204",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W53417064",
        "https://openalex.org/W4214585779",
        "https://openalex.org/W2112129677",
        "https://openalex.org/W189596042",
        "https://openalex.org/W1564529390",
        "https://openalex.org/W2398477990",
        "https://openalex.org/W2068868410",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2158572527",
        "https://openalex.org/W2168231600",
        "https://openalex.org/W2074376560",
        "https://openalex.org/W2545004184",
        "https://openalex.org/W4298069009",
        "https://openalex.org/W2913660729",
        "https://openalex.org/W2045908891",
        "https://openalex.org/W60493759",
        "https://openalex.org/W2326588846",
        "https://openalex.org/W2164009308",
        "https://openalex.org/W2163922914",
        "https://openalex.org/W2085536277",
        "https://openalex.org/W2135194391",
        "https://openalex.org/W2133671888",
        "https://openalex.org/W4248436593",
        "https://openalex.org/W2097552562",
        "https://openalex.org/W4210369895",
        "https://openalex.org/W2912225506"
    ],
    "abstract": "Deep unsupervised learning in stochastic recurrent neural networks with many layers of hidden units is a recent breakthrough in neural computation research. These networks build a hierarchy of progressively more complex distributed representations of the sensory data by fitting a hierarchical generative model. In this article we discuss the theoretical foundations of this approach and we review key issues related to training, testing and analysis of deep networks for modeling language and cognitive processing. The classic letter and word perception problem of McClelland and Rumelhart (1981) is used as a tutorial example to illustrate how structured and abstract representations may emerge from deep generative learning. We argue that the focus on deep architectures and generative (rather than discriminative) learning represents a crucial step forward for the connectionist modeling enterprise, because it offers a more plausible model of cortical learning as well as a way to bridge the gap between emergentist connectionist models and structured Bayesian models of cognition.",
    "full_text": "HYPOTHESIS AND THEORY ARTICLE\npublished: 20 August 2013\ndoi: 10.3389/fpsyg.2013.00515\nModeling language and cognition with deep unsupervised\nlearning: a tutorial overview\nMarco Zorzi1,2*, Alberto T estolin1 and Ivilin P . Stoianov1,3\n1 Computational Cognitive Neuroscience Lab, Department of General Psychology, University of Padova, Padova, Italy\n2 IRCCS San Camillo Neurorehabilitation Hospital, Venice-Lido, Italy\n3 Institute of Cognitive Sciences and Technologies, National Research Council, Rome, Italy\nEdited by:\nJulien Mayor, University of Geneva,\nSwitzerland\nReviewed by:\nBradley Love, University College\nLondon, UK\nAngelo Cangelosi, University of\nPlymouth, UK\n*Correspondence:\nMarco Zorzi, Computational\nCognitive Neuroscience Lab,\nDepartment of General Psychology,\nUniversity of Padova, Via Venezia 12,\nPadova 35131, Italy\ne-mail: marco.zorzi@unipd.it\nDeep unsupervised learning in stochastic recurrent neural networks with many layers of\nhidden units is a recent breakthrough in neural computation research. These networks\nbuild a hierarchy of progressively more complex distributed representations of the sensory\ndata by ﬁtting a hierarchical generative model. In this article we discuss the theoretical\nfoundations of this approach and we review key issues related to training, testing and\nanalysis of deep networks for modeling language and cognitive processing. The classic\nletter and word perception problem of McClelland and Rumelhart (1981) is used as a\ntutorial example to illustrate how structured and abstract representations may emerge\nfrom deep generative learning. We argue that the focus on deep architectures and\ngenerative (rather than discriminative) learning represents a crucial step forward for the\nconnectionist modeling enterprise, because it offers a more plausible model of cortical\nlearning as well as a way to bridge the gap between emergentist connectionist models\nand structured Bayesian models of cognition.\nKeywords: neural networks, connectionist modeling, deep learning, hierarchical generative models, unsupervised\nlearning, visual word recognition\nINTRODUCTION\nA fundamental issue in the study of human cognition is what\ncomputations are carried out by the brain to implement cognitive\nprocesses. The connectionist fr amework assumes that cognitive\nprocesses are implemented in terms of complex, non-linear inter-\nactions among a large number of simple, neuron-like processing\nunits that form a neural network ( Rumelhart and McClelland,\n1986). This approach has been used in cognitive psychology—\noften with success—to develop functional models that clearly\nrepresent a great advance over previous verbal-diagrammatic\nmodels because they can produce simulations of learning, skilled\nperformance, and breakdowns of processing after brain dam-\nage. One paradigmatic example is the connectionist modeling of\nvisual word recognition and reading aloud, which has often pro-\nvided key theoretical and methodological advances with broad\ninﬂuences well-beyond the language domain (e.g., McClelland\nand Rumelhart, 1981; Seidenberg and McClelland, 1989; Plaut\nand Shallice, 1993; Plaut et al., 1996 ). Connectionist models of\nthe reading processes can produce highly detailed simulations of\nhuman performance, accounting for a wide range of empirical\ndata that include reaction times and accuracy of skilled readers\nat the level of individual words, the development of reading skills\nin children, and the impaired performance of dyslexic individuals\n(Plaut et al., 1996; Zorzi et al., 1998; Harm and Seidenberg, 1999,\n2004; Perry et al., 2007, 2010, 2013 ). Despite signiﬁcant progress\nin the attempt to improve the architectural and learning princi-\nples incorporated in neural network models (see O’Reilly, 1998;\nO’Reilly and Munakata, 2000), much modeling work in psychol-\nogy is still based on the classic neural network with one layer of\nhidden units (i.e., a “shallow” architecture) and error backprop-\nagation ( Rumelhart et al., 1986 ) as learning algorithm—a choice\nthat is typically seen as a compromise to achieve efﬁcient learn-\ning of complex cognitive tasks. We argue below that a key step\nforward for connectionist modeling is the use of networks with a\n“deep” architecture (Hinton, 2007, 2013 )a n dw h e r em o s to ft h e\nlearning is generative rather than discriminative ( Box 1).\nThe shallow architecture of the prototypical multi-layer neural\nnetwork (Rumelhart et al., 1986 ) does not capture the hierarchi-\ncal organization of the cerebral cortex. Hierarchical processing is\nthought to be a fundamental characteristic of cortical computa-\ntion ( Hinton, 2007; Clark, 2013 ) and it is a key feature of bio-\nlogically inspired computational models of vision ( Riesenhuber\nand Poggio, 1999 ). The idea of a deep network with a hierar-\nchy of increasingly complex feature detectors can be traced back\nto the Interactive Activation Model (IAM) of letter and word\nperception ( McClelland and Rumelhart, 1981 ), but this semi-\nnal proposal did not transfer to connectionist learning models\nbecause the error backpropagation algorithm had little success in\ntraining networks with many hidden layers ( Hinton, 2007, 2013 ).\nAnother key assumption of the IAM that did not readily trans-\nfer to connectionist learning models is the mixing of bottom–up\nand top–down processing through recurrent feedback. Finally, the\nwidespread use of the error backpropagation algorithm in con-\nnectionist modeling, leaving aside its lack of biological plausibility\n(O’Reilly, 1998), implies subscription to the dubious assumption\nthat learning is largely discriminative (e.g., classiﬁcation or func-\ntion learning) and that an external teaching signal is available\nat each learning event (that is, all training data is labeled). This\nwww.frontiersin.org August 2013 | Volume 4 | Article 515| 1\n\nZorzi et al. Deep learning models of language and cognition\nBox 1 | Glossary.\nBOLTZMANN MACHINE\nStochastic neural network of symmetrically connected, neuron-like\nunits whose dynamics is governed by an energy function. The\ninput to the network is given through a layer of visible units, while\nanother layer of hidden units is used to model the latent causes\nof the data. A variant known as Restricted Boltzmann Machine\n(RBM) is obtained by removing within-layer lateral connections to\nform a bipartite graph, allowing to perform efﬁcient inference and\nlearning.\nCONTRASTIVE DIVERGENCE\nObjective function that allows to efﬁciently train RBMs by approx-\nimating the log-likelihood gradient, without requiring to run a\nMarkov chain to convergence.\nDEEP BELIEF NETWORK\nHierarchical generative model composed of a stack of RBMs,\nwhich can be greedily trained layer-wise in an unsupervised\nfashion. The whole network can be eventually ﬁne-tuned with\nsupervised learning to perform discriminative tasks.\nDEEP LEARNING\nMachine learning framework that exploits multiple layers of hidden\nunits to build hierarchical internal representations of the input data.\nDISCRIMINATIVE LEARNING\nLearning approach whose objective is to map the observed vari-\nables X into corresponding output variables Y, usually by mod-\neling the conditional distributionP(Y |X ), optimizing classiﬁcation\nboundaries, or by approximating a functionY = f(X ). This approach\nrequires labeled examples (i.e, a teaching signal for supervised\nlearning).\nGENERATIVE LEARNING\nLearning approach whose objective is to model the joint distri-\nbution P(X, Y ) of observed and latent variables, typically using a\nlikelihood-based criterion. This approach does not require labeled\ndata (i.e., learning is unsupervised).\nGRAPHICAL MODELS\nProbabilistic models in which the topology of a graph deﬁnes con-\nditional independecies between random variables, allowing to efﬁ-\nciently represent complex joint distributions through factorization.\nlearning regimen is exceptional in the real world. Reinforcement\nlearning ( Sutton and Barto, 1998 )i sap l a u s i b l ea l t e r n a t i v e ,b u t\nthere is a broad range of situations where learning is fully unsu-\npervised and its only objective is that of building rich internal\nrepresentations of the sensory world ( Hinton and Sejnowski,\n1999). Notably, the learned internal model can then be used to\ninfer causes and make predictions ( Dayan et al., 1995; Hinton\nand Ghahramani, 1997; Friston, 2005; Hinton, 2010b; Huang and\nRao, 2011; Clark, 2013 ).\nUnsupervised learning has a long history, but the classic\nlearning algorithms have important limitations. Some develop\na representation that is distributed but also linear ( Oja, 1982 ),\nwhich implies that higher-order information remains invisible.\nOthers develop a representation that is non-linear but also local-\nist, that is one in which each observation is associated to a\nsingle hidden unit ( Rumelhart and Zipser, 1985; Kohonen, 1990 ).\nFor these reasons, their application to modeling complex cog-\nnitive functions has been limited. An important breakthrough\nin unsupervised learning is the use of statistical principles such\nas maximum likelihood and Bayesian estimation to develop\ngenerative models that discover representations that are both\ndistributed and non-linearly related to the input data ( Hinton\nand Ghahramani, 1997 ). A generative model is a probabilis-\ntic model that captures the hidden (latent) causes of the data,\nthereby providing a sensible objective function for unsuper-\nvised learning. In other words, the “learner” estimates a model,\nwithout any supervision or reward, that represents the proba-\nbility distribution of the data. Generative models are appealing\nbecause they make strong suggestions about the role of feedback\nconnections in the cortex and are consistent with neurobio-\nlogical theories that emphasize the mixing of bottom–up and\ntop–down interactions in the brain: bottom–up inputs convey\nsensory information, whereas internal representations form a\ngenerative model that predicts the sensory input via top–down\nactivation ( Hinton and Ghahramani, 1997 ). Learning can be\nviewed as maximizing the likelihood of the observed data under\nthe generative model, which is equivalent to discovering efﬁ-\ncient ways of coding the sensory data ( Ghahramani et al., 1999 ).\nNotably, the application of these algorithms to natural images\nhas been shown to generate receptive ﬁeld properties simi-\nlar to those observed in the visual cortex ( Rao and Ballard,\n1999).\nGenerative learning can be implemented in the framework of\nrecurrent stochastic neural networks with hidden units ( Hinton,\n2002) .H o w e v e r ,o n eh i d d e nl a y e rc a nb ei n s u f ﬁ c i e n tf o rm o d -\neling structured and high-dimensional sensory data. In contrast,\na network with many hidden layers, that is a deep network, can\nlearn a more powerful hierarchical generative model (Hinton and\nSalakhutdinov, 2006; Hinton et al., 2006 ). Note that a good gen-\nerative model of the data can be a very useful starting point for\nlater discriminative learning ( Hinton, 2007; Stoianov and Zorzi,\n2012). The internal representations obtained from generative\nlearning can be the input to a variety of classiﬁcation or func-\ntion learning tasks, thereby exploiting re-use of learned features\n(Bengio et al., 2012 ). Moreover, the internal model might be\nreﬁned through supervised learning to strengthen the features\nthat are most informative for solving a speciﬁc classiﬁcation task\n(Hinton and Salakhutdinov, 2006 ;a l s os e e Love et al., 2004 ,f o r\na related modeling approach to category learning). Indeed, it has\nbeen shown that human category learning implies ﬂexibility in\nthe use and creation of perceptual features ( Schyns et al., 1998 )\nand that different types of features might be extracted accord-\ning to the nature of the learning task (e.g., unsupervised vs.\nsupervised; Love, 2002).\nThe goal of the present article is to provide a tutorial\noverview of generative learning in deep neural networks to high-\nlight its appeal for modeling language and cognition. We start\nwith a brief review of the theoretical foundations of generative\nFrontiers in Psychology| Language Sciences August 2013 | Volume 4 | Article 515| 2\nZorzi et al. Deep learning models of language and cognition\nlearning and deep networks. We then discuss various practi-\ncal aspects related to training, testing and analyzing deep net-\nworks, using the classic letter and word perception problem\nof McClelland and Rumelhart (1981) as a tutorial example.\nThe emergence of a hierarchy of orthographic representations\nthrough deep unsupervised learning is particularly interesting\n(also see Di Bono and Zorzi, under review) because it can\nrevisit the hard-wired architecture of the IAM. The idea that\nperception of written words involves the sensitivity to increas-\ningly larger orthographic units is also supported by recent\nneuroimaging ﬁndings ( Dehaene et al., 2005; Vinckier et al.,\n2007).\nLEARNING A GENERATIVE MODEL: RESTRICTED\nBOLTZMANN MACHINES\nHere we consider a class of neural networks known as Boltzmann\nMachines (hereafter BM; Ackley et al., 1985 ). These are stochas-\ntic associative networks that observe and model data by using\nlocal signals only. BMs can be interpreted as undirected graphical\nmodels ( Jordan and Sejnowski, 2001 ;s e e Box 2) where learning\ncorresponds to ﬁtting a generative model to the data. Despite the\nappeal of BMs as plausible models of cortical learning, their use\nwas strongly discouraged by the very high computational demand\nof the original learning algorithm, until the recent development\nof contrastive divergence (CD) learning (Hinton, 2002). CD makes\nlearning of BMs practical, even for large networks (see below).\nBMs consist of a set of stochastic units, fully connected with\nsymmetric weights and without self-connections, where each unit\nﬁres with a probability depending on the weighted sum of its\ninputs. Data patterns are represented by the activation of “visible”\nunits. An additional layer of “hidden” units captures high-order\nstatistics and represent the latent causes of the data. Inspired by\nstatistical mechanics, the model behavior is driven by an energy\nfunction E that describes which conﬁgurations of the units are\nmore likely to occur by assigning them a certain probability value:\np (v, h) = e\n−E(v, h)\nZ\nwhere v and h are, respectively, the visible and hidden units and Z\nis a normalizing factor known as partition function,w h i c he n s u r e s\nthat the values of p constitute a legal probability distribution\n(i.e., summing up to 1). The network state changes in a way that\nallows the gradual decrease of the associated energy, modulated\nby a “temperature” parameter T so that at higher temperatures\nan occasional increase of energy is also permitted to avoid local\nminima. T o achieve local energy minimum (equilibrium), T is\nBox 2 | Probabilistic Graphical Models.\nThe framework of probabilistic graphical models ( Koller and\nFriedman, 2009) provides a general approach to model arbitrarily\ncomplex statistical distributions, which can involve a large num-\nber of stochastic variables that interact together. Graphical models\nallow us to describe complex relations between variables by\nexploiting the structure of their joint distribution, since in general\ntheir interactions are not globally deﬁned but instead each variable\nis only inﬂuenced by a limited subset of “neighbors. ” The topology\nof a graphical model explicitly deﬁnes the scope of interaction of\neach variable (represented by a node in the graph) by highlighting\nthe set of independecies that hold in the distribution. This allows\nto factorize a joint probability distribution using local conditional\nprobabilities.\nGraphical models can havedirected connections between vari-\nables, such as in Bayesian networks (Figure 1A), or undirected\nconnections, such as in Markov networks (Figure 1B). Both types\nof connections might be present in the same graph, thus forming a\nhybrid model. Although they share the same underlying theoretical\nframework, Bayesian and Markov networks have rather differ-\nent representational and computational characteristics. In directed\nmodels, the semantic of connections deﬁnes a “parent of” rela-\ntionships between linked variables, while in undirected models\nthe connections are symmetric and therefore only encode a sort\nof “degree of afﬁnity” between linked variables. This leads to a\ndifferent representation of independencies between nodes of the\ngraph: in Bayesian networks, each node is conditionally indepen-\ndent from all the others given its parents, its children and the\nparents of its children, while in undirected models each node\nis conditionally independent from all the others given the nodes\ndirectly connected to it [i.e., its “Markov blanket” (Pearl, 1988),\nhighlighted inFigure 1]. In both cases, these conditional indepen-\ndencies can be exploited to derive efﬁcient inference and learning\nprocedures even in the presence of a large number of variables,\nbecause only the Markov blanket of a certain node is required in\norder to sample from its conditional distribution.\nIn the case of undirected graphical models, each edge is asso-\nciated with a certain function, known as factor,w h i c ht a k e sa s\ninput the values of the nodes connected by the edge and gives as\noutput a scalar value that represents the afﬁnity between them: a\nhigh value indicates that the two variables are likely to be strongly\nrelated, while a low value indicates a weak relation. The joint dis-\ntribution of all the variables in the graph can be efﬁciently deﬁned\nas a product of such local factors:\nP(X\n1,X2,..., Xn) = 1\nZ\n∏\ni\nφi (Di )\nwhere Di represents the scope of each factorφi (i.e., which vari-\nables it involves) andZ is a global normalization constant called\npartition function, which ensures dealing with legal probabilities\nsumming up to 1 .\nFIGURE 1 | (A)A directed graphical model, also known as Bayesian network.\n(B) An undirected graphical model, also known as Markov network. In both\ngraphs, the dashed line highlights the Markov blanket of the blue node.\nwww.frontiersin.org August 2013 | Volume 4 | Article 515| 3\nZorzi et al. Deep learning models of language and cognition\ngradually decreased ( simulated annealing) . The learning proce-\ndure minimizes the Kullback-Liebler divergence between the data\ndistribution and the model distribution. Accordingly, for each\npattern the network performs a data-driven, positive phase (+)\nand a model-driven, negative phas e (–). In the positive phase\nthe visible units are clamped to the current pattern and the\nhidden layer settles to a stable activation state. In the negative\nphase all units are unclamped and the network is run [using\na Markov Chain Monte Carlo (MCMC) algorithm; see Box 3]\nuntil it settles on a stable activation state over visible and hid-\nden units, which reﬂects the model beliefs. After each phase,\ncorrelations between the activations of each pair of connected\nunits are collected and used to update the network weights. Note\nthat learning is unsupervised (i.e., the network does not learn an\ninput–output mapping like typical multilayer networks trained\nwith error backpropagation) and it uses only local signals and\nHebbian rules. A similar form of contrastive Hebbian learning\nis also used in the generalized recirculation algorithm and in\nLeabra ( O’Reilly, 1998, 2001). Learning the connection weights\nin the original BM is based on a maximum likelihood learning\nrule that is very simple and locally optimal, but unfortunately\nthe learning algorithm is also very slow because it implies run-\nning a Markov chain until convergence (which may require an\nexponential time).\nThe breakthrough that led to CD learning ( Hinton, 2002;a l s o\nsee Welling and Hinton, 2002 ; for a mean ﬁeld version) is the\nﬁnding that the negative phase does not need to be run until equi-\nlibrium (i.e., full convergence). If sampling starts from the hidden\nunit state computed in the positive phase (i.e., driven by the data),\ncorrelations computed after a ﬁxed number of steps in the Markov\nchain are sufﬁcient to drive the weights toward a state in which the\ninput data will be accurately reconstructed. Hence, CD learning\napproximates the gradient of the log-likelihood of the learning\ndata by performing only few iterations, which in practice gives\ngood results even with a single step ( CD-1). After computing\nthe model’s reconstruction, weights are updated by contrasting\nvisible-hidden correlations computed on the data vector ( v\n+h+)\nwith visible-hidden correlations computed on the reconstruction\n(v−h−):\n/Delta1W = η(v+h+ − v−h−)\nwhere η is the learning rate. Importantly, a restriction to the archi-\ntecture of the BM by not allowing intra-layer connections (RBM;\nHinton, 2002) makes learning extremely fast. The energy function\nf o rR B M si sd e ﬁ n e da s :\nE(v, h) =− bTv − cT h − hTWv\nwhere W is the matrix of connections weights and b and c are\nthe biases of visible and hidden units, respectively. In RBMs, the\nupdate of units in one layer no longer requires any iterative set-\ntling because they are conditionally independent given the state of\nthe other layer. That is, the sampling process is speeded up by per-\nforming block Gibbs sampling (see Box 3) over visible and hidden\nunits (i.e., all units in a layer are sampled in a single step).\nExamples of application of CD learning in connectionist mod-\neling studies include numerical cognition ( Stoianov et al., 2002,\n2004; Zorzi et al., 2005 ) and space coding for sensorimotor\ntransformations (De Filippo De Grazia et al., 2012 ).\nBox 3 | Block Gibbs sampling in RBMs.\nIn a probabilistic graphical model, we are often interested in gen-\nerating samples from the model distribution. A general-purpose,\npowerful method is the Gibbs sampling algorithm, which gener-\nates a sequence of observations that progressively approximate a\nspeciﬁed multivariate probability distribution (Geman and Geman,\n1984). Gibbs sampling belongs to the family of MCMC methods,\nwhich draw samples from a probability distribution by constructing\na Markov chain that has the desired distribution as its equilibrium\ndistribution (Andrieu et al., 2003). Under certain conditions, after\nan initial burn-in phase the Markov chain will converge to the sta-\nble distribution. The basic idea of Gibbs sampling is to construct\nthe Markov chain so that one particular variable is sampled at each\nstep given the current values ofall the other variables. After repeat-\ning this process iteratively for enough time, the chain will generate\nsamples from the target joint distribution. Notably, Gibbs sampling\ncan exploit the structure of the graph (i.e., the conditional inde-\npendecies between variables) to speed up this process: since the\nvalue of each node is only inﬂuenced by its Markov blanket (see\nBox 2), if two variables are conditionally independent given the cur-\nrent evidence (i.e., their Markov blanket is observed) they can be\nsampled at the same time. This variant of the algorithm is known\nas block Gibbs sampling.\nIn the case of Boltzmann Machines, learning requires sampling\nfrom the joint distribution of visible and hidden variables in order\nto compute visible-hidden correlations on the model expectations.\nIf the connectivity of the network is restricted, as in the RBM, the\nsampling process can be signiﬁcantly speeded up by using block\nGibbs sampling. Indeed, the units of the same layer become con-\nditionally independent if there are no intra-layer connections; that\nis, in RBMs the Markov blanket of a hidden unit corresponds to\nthe visible layer, and vice versa (Figure 2). This allows to sample\nall units of the same layer in parallel.\nFIGURE 2 | Graphical representation of a Restricted Boltzmann\nMachine. The dashed line highlights the Markov blanket of the blue\nhidden unit, which corresponds to the whole layer of visible units.\nFrontiers in Psychology| Language Sciences August 2013 | Volume 4 | Article 515| 4\nZorzi et al. Deep learning models of language and cognition\nLEARNING A HIERARCHICAL GENERATIVE MODEL: DEEP\nBELIEF NETWORKS\nRBMs can be used as building blocks of more complex archi-\ntectures, where the hidden variables of the generative model can\nbe organized into layers of a hierarchy ( Figure 3A). The result-\ning architecture is referred to as a “deep network.” In particular,\nthe Deep Belief Network (DBN; Hinton and Salakhutdinov, 2006;\nHinton et al., 2006 ) is a stack of RBMs that can be trained layer\nby layer in a greedy, unsupervised way. The main intuition behind\ndeep learning is that, by training a generative model at level l using\nas input the hidden causes discovered at level l–1,t h en e t w o r kw i l l\nprogressively build more structured and abstract representations\nof the input data. Importantly, architectures with multiple pro-\ncessing levels permit an efﬁcient encoding of information by\nexploiting re-use of features among different layers: simple fea-\ntures extracted at lower levels can be successively combined to\ncreate more complex features, which will eventually unravel the\nmain causal factors underlying the data distribution. Indeed, it\nhas been shown that functions that can be compactly represented\nby a depth k architecture might require an exponential num-\nber of computational elements to be represented by a depth k–1\narchitecture (Bengio, 2009 ). Moreover, adding a new layer to the\narchitecture increases a lower bound on the log-likelihood of the\ngenerative model ( Hinton et al., 2006 ), thus improving the over-\nall capacity of the network. After learning of all layers, the deep\narchitecture can be used as a generative model by reproducing\nthe data when sampling from the model, that is by feeding the\nactivations of the deepest layer all the way back to the input layer.\nNote that the hierarchical structure of the internal representations\nis an emergent property of the learning algorithm. In contrast,\nhierarchy in classic connectionist models is typically built in by\nstipulating the representations to be used at more than one layer\n(e.g., Rumelhart and Todd, 1993; Perry et al., 2013 ); indeed, train-\ning of deep multi-layer perceptrons using error backpropagation\nis very difﬁcult because the error gradient tends to vanish when\npropagated backwards through more than one hidden layer (see\nHinton, 2013, for further discussion).\nAn important advantage of deep unsupervised learning is that\nthe internal representations discovered by the network are not\ntied to a particular discriminative task, because the objective of\nlearning is only to model the hidden causes of the data. However,\nonce the system has developed expressive abstract representa-\ntions, possible supervised tasks can be carried out by introducing\nadditional modules, which directly operate on such high-level\nrepresentations of the data and can therefore yield excellent per-\nformance in classiﬁcation or function learning ( Figure 3B). For\nexample, on a popular handwritten digit recognition problem\n(MNIST dataset; LeCun et al., 1998 ), high discriminative accu-\nracy can be obtained even by a linear classiﬁer applied on the\ntop-level internal representations of a DBN that was only trained\nto reconstruct the digit images ( Testolin et al., 2013 ;e x a m p l e so f\ndigits reconstructed by the network are reported in Figure 3C).\nWithin this perspective, the use of an additional ﬁne-tuning phase\nof the whole deep network using error backpropagation (as done\nin Hinton and Salakhutdinov, 2006 ) might be unwarranted, not\nonly because of the biological implausibility of the learning algo-\nrithm, but also because the network would become speciﬁcally\ntuned to a particular task. Indeed, the idea that high-level repre-\nsentations obtained from (unsupervised) model learning should\nbe usable across several tasks ( Figure 3B) is referred to as “trans-\nfer learning” and it is a hot topic for the machine learning\ncommunity ( Bengio, 2009; Bengio et al., 2012 ). It is worth men-\ntioning that machine learning researchers have recently inves-\ntigated deep networks built through greedy layer-wise training\nof stacked autoencoders, where each autoencoder is a multi-\nlayer perceptron trained to auto-associate the input ( Bengio and\nLamblin, 2007; Baldi, 2012 ). This approach has been successful\nin terms of machine learning benchmarks, but it is less appeal-\ning than DBNs for cognitive modeling purposes because learning\nis based on error backpropagation and it is not grounded in a\nFIGURE 3 | (A)Architecture of the DBN with three hidden layers\nused in the MNIST handwritten digit recognition problem ( Hinton\nand Salakhutdinov, 2006). (B) A typical transfer learning scenario,\non which high-level, abstract representations are ﬁrst extracted\nby deep unsupervised learning and then used to perform a\nvariety of supervised t asks [adapted from Bengio et al. (2012) ].\n(C) Reconstructions of MNIST digit images made by the deep\nnetwork.\nwww.frontiersin.org August 2013 | Volume 4 | Article 515| 5\nZorzi et al. Deep learning models of language and cognition\nsound probabilistic framework. Moreover, deep autoencoders are\nnot used as generative models to produce predictions based on\ntop–down signals.\nA ﬁnal consideration concerns the computational complexity\nof deep learning: thanks to its efﬁciency, the algorithm proposed\nby Hinton et al. (2006) solves the problem of learning in densely\nconnected networks that have many hidden layers. If imple-\nmented on multicore hardware, deep learning is practical even\nwith billions of connections, thereby allowing the development of\nvery-large-scale simulations ( Raina et al., 2009; Dean et al., 2012;\nLe et al., 2012 ). Medium-to-large-scale simulations can even be\nperformed on a desktop PC equipped with a low-cost graphic\ncard (Testolin et al., 2013 ;s e eb e l o w ) .\nCONNECTIONIST MODELING WITH DEEP NETWORKS: A\nTUTORIAL\nIn this section we provide a practical overview on how to con-\nstruct a complete DBN simulation. We illustrate how to train,\ntest and analyze a deep network model using the classic letter and\nword perception problem of McClelland and Rumelhart (1981) .\nWritten word perception is particularly representative because it\ncan be linked to one of the most inﬂuential models of language\nprocessing, McClelland and Rumehart’s IAM, and more speciﬁ-\ncally to its two key assumptions: (1) a hierarchical organization\nof the network, with increasingly more complex levels of rep-\nresentation, and (2) the mixing of bottom–up and top–down\nprocessing (i.e., interactivity) t or e s o l v ea m b i g u i t yo ft h es e n s o r y\ninput. Interestingly, a recent re-formulation of the IAM as a prob-\nabilistic generative model ( Khaitan and McClelland, 2010 )w a s\nshown to perform optimal Bayesian inference, thereby supporting\nthe appeal of the hierarchical interactive architecture ( Mirman\net al., in press ). A deep learning model would therefore represent\nan important step forward, because the hard-wired architecture of\nthe IAM might be replaced by the hierarchical generative model\nlearned in a DBN. In this regard, learning word perception can\nbe seen as a stochastic inference problem where the goal is to\nestimate the posterior distribution over latent variables given the\nimage of a word as input.\nThough written word perception is an excellent candidate for\ndeep learning, the complexity of the problem makes realistic sim-\nulations difﬁcult to handle. For example, high-resolution images\nof whole words would require a very large network, with tens\nof thousands of visible units (e.g., 20,000 units for a 400 by\n50 pixels image), many hidden layers and billions of connec-\ntions (see Krizhevsky et al., 2012 , for deep learning on a realistic\nobject recognition problem). One possible simpliﬁcation would\nbe to split words into letter constituents and ﬁrst model the\nperception of single letters. This might lead to sensible internal\nletter representations that are invariant to position, size, rota-\ntion, and noise (i.e., abstract letter identities; McClelland and\nRumelhart, 1981). Alternatively, written words can be represented\nusing small resolution images, with letters encoded as combina-\ntions of simple geometric features (the “Siple” font; McClelland\nand Rumelhart, 1981 ). We employed the latter solution for the\nsimulations presented here.\nIn this tutorial we also consider deep learning of handwrit-\nten digits (MNIST database; LeCun et al., 1998 )a n dv i s u a l\nnumerosity estimation ( Stoianov and Zorzi, 2012 )i nr e l a t i o nt o\nthe analysis of DBNs, because they represent more realistic per-\nception problems that involve training on thousands of images.\nTraining on a large dataset can be important for the emergence of\na richer hierarchical structure of features.\nTRAINING A DBN\nAs in other connectionist models, input to the network is pro-\nvided as pattern of activations over visible units. Note that 2D\nimages are vectorized; this implies that the spatial structure\nremains only implicit in the co-activation of neighboring visible\nunits, but it can emerge during learning in the form of statistical\nregularities (see examples below). Learning a generative model\ndoes not require labeled data, that is, unlike supervised learn-\ning, each pattern does not need to possess a class label or any\nother form of associated target state. Nevertheless, this kind of\ninformation might still be useful for testing and analyzing the\nnetwork. Note that realistic, large-scale simulations often imply\nabundance of unlabeled data and only a limited sample of pre-\nclassiﬁed learning examples (see Le et al., 2012 , for deep learning\non millions of images randomly extracted from videos on the\nInternet).\nA ready-to-use parallel implementation of deep unsupervised\nlearning on graphic cards is described in Testolin et al. (2013),a n d\nit is publicly available for download\n1.\nNetwork architecture\nThe learning algorithm tunes the parameters (i.e., weights) of\naD B Nw i t hag i v e ns t r u c t u r et h a ts h o u l db es p e c i ﬁ e da f t e r\nestablishing the input domain. Here we only consider network\narchitectures with fully connected pairs of layers ( Figure 3A), but\nalternatives based on weights sharing like convolutional networks\n(LeCun et al., 1998 ) can simplify the learning problem by assum-\ning identical processing applied to different portions of the image,\nthereby reducing the number of parameters of the model. In gen-\neral, the size of a given hidden layer might be proportional to\nthe expected number of features describing the data at a cer-\ntain processing level. Intuitively, many hidden units will allow for\nthe encoding of more speciﬁc characteristics of the data, whereas\nfewer units imply a greater compression of the representation and\nhence increase the generality of the features. A more neutral strat-\negy with regard to the architectural choices is to keep the size of\nfew consecutive layers constant. Finally, a large top hidden layer\ncan be useful to unfold categories and classes, thereby facilitating\nlinear associations to categories or other processing domains (as\nwe will discuss in the following sections). At any rate, we advise\nto try several architectures, gradually increasing the number of\nlayers and units per layer, until sa tisfactory results are obtained.\nLearning tasks\nWe illustrate the tutorial with examples of increasing complex-\nity. The ﬁrst toy example is the visual perception of single letters\nwith input consisting of black and white (b/w) images of size\n1A variety of multicore implementations (MATLAB and Python on graphic\ncards; Octave/MPI on a multi-core cluster) is described in Testolin et al.\n(2013) and the source codes can be found at: http://ccnl .psy.unipd.it/research/\ndeeplearning\nFrontiers in Psychology| Language Sciences August 2013 | Volume 4 | Article 515| 6\nZorzi et al. Deep learning models of language and cognition\n7 × 7 pixels (i.e., patterns over 49 visible units). The dataset con-\ntains the images of 26 capital letters created with the schematic\n“Siple” font, composed of 14 basic visual features ( Rumelhart\nand Siple, 1974 ). We found that a small two-layer DBN net-\nwork with as few as 10 units in the ﬁrst layer and 30 units in\nthe second layer was sufﬁcient to discover the underlying visual\nfeatures. The second example extends the problem above to the\nvisual perception of four-letter words, using the classic dataset of\n1180 words employed by McClelland and Rumelhart (1981) in\nt h eI A M .I n p u ta r eb / wi m a g e so fs i z e2 8× 7 pixels (i.e., patterns\nover 196 visible units) of words printed with the Siple font. This\nproblem required a DBN with more hidden units: 120 in the ﬁrst\nhidden layer and 200 in the second one (see Figure 4).\nTwo additional examples approach realistic problems: the per-\nception of handwritten digits and visual numerosity perception.\nThe training datasets for these problems contain thousands of\nsamples per category (i.e., digits or numerosity levels) and pro-\nvide a rich variety of different instances. In the handwritten digit\nrecognition problem, input data consists of 50,000 vectorized\ngray-level images of size 28 × 28 pixels (i.e., patterns over 784\nvisible units) that contain handwritten digits from zero to nine\n(MNIST dataset; LeCun et al., 1998 ). A robust model of this data\nwould beneﬁt from a hierarchical process that extracts increas-\ningly more complex features (e.g., Gabor ﬁlters at the ﬁrst level,\nedge detectors in the following layers, etc.). We used the DBN\narchitecture proposed by Hinton and Salakhutdinov (2006) for\nthis task, with three hidden layers of size 500, 500, and 2000 units,\nrespectively. The data of the numerosity perception problem con-\nsists of 51,200 vectorized b/w images of size 30 × 30 pixels (i.e.,\npatterns over 900 visible units) that contain up to 32 rectangular\nobjects of variable size. We used the DBN architecture proposed\nby Stoianov and Zorzi (2012) , consisting of two hidden layers\nof size 80 and 400 units, which was shown to extract abstract\nnumerosity information.\nLearning parameters\nThe DBN learning algorithm is governed by few meta parame-\nters. First, the learning rate should be small, typically in the range\n0.01–0.1. Second, the use of a momentum coefﬁcient (i.e., a frac-\ntion of the previous weight update) is also critical to avoid local\nminima, and it is usually set to 0.5 at the beginning of training\nFIGURE 4 | Architecture of the DBN with two hidden layers used in the\nwritten word perception problem.\nand then increased up to 0.9. Third, network weights should be\nregularized, that is kept relatively small, by applying a constant\nweight decrease in the form of a small weight-decay factor of\nabout 0.0001. Finally, weights should be initialized with small\nrandom values drawn from a zero -mean Gaussian distribution\nwith standard deviation of 0.01. The initial values of the bias can\nbe set to zero. These and other issues related to training RBMs are\ndiscussed in a comprehensive practical guide by Hinton (2010a).\nDBNs are trained with the CD learning algorithm, one RBM\nlayer at a time, using as input either the sensory data (ﬁrst RBM)\nor the activations of the previous hidden layer (deeper RBMs).\nThis greedy, layer-wise learning procedure can be performed in\na completely iterative way, by updating the network weights after\neach pattern (on-line learning). A complete sweep over all training\npatterns constitutes a learning epoch. In batch ( off-line)l e a r n i n g ,\ninstead, weights updates are computed over the whole training\nset. A good compromise between these two approaches is to\nuse a mini-batch learning scheme, in which the dataset is par-\ntitioned into small subsets (i.e., mini-batches) and the weights\nare updated with the average gradient computed on each subset\n(Neal and Hinton, 1998 ). This latter strategy is highly recom-\nmended, because it improves the quality of learning by avoiding\nlocal minima and it also allows to signiﬁcantly speed-up the learn-\ning phase on multicore parallel implementations (see Testol i n\net al., 2013 , for a mini-batch GPU implementation of deep net-\nworks). The mini-batch size should be set between 10 and few\nhundred patterns.\nMonitoring learning\nThe learning progress can be monitored by analyzing the recon-\nstruction error on the training patterns. The mean reconstruction\nerror on the entire training set should fall rapidly at the beginning\nof learning and then gradually stabilize. However, this measure\ncan be misleading because it is not the objective function opti-\nmized by the CD- n algorithm, especially for large n (Hinton,\n2010a) .Am o r ep r e c i s em e a s u r eo ft h ep e r f o r m a n c eo ft h en e t -\nwork is to compare the free energy of the training data with\nthat of a sample of held-out patterns ( Hinton, 2010a ). A ﬁnal\napproach to monitor the quality of learning is to regularly per-\nform an additional discriminative task over the learned internal\nrepresentations, as we will discuss at length below.\nSparsity constraints on internal representations\nAn interesting variant of standard RBMs (and, consequently,\nDBNs) consists in forcing the network’s internal representations\nto rely on a limited number of active hidden units. In this case\nthe network develops sparse distributed representations, which\nhave many useful properties and appear to be a coding strategy\nadopted by the brain ( Olshausen and Field, 1996 ;s e e Olshausen\nand Field, 2004, for review). Forcing sparseness within a network’s\nhidden layer can be interpreted in terms of inhibitory competition\nbetween units ( O’Reilly, 2001). A sparse-coding version of the\nRBM encourages the development of more orthogonal features,\nwhich can allow a better pattern discriminability and a more intu-\nitive interpretation of what each unit is representing. In RBMs,\nsparsity can be obtained by driving the probability q of a unit to\nbe active to a certain desired (low) probability p (Lee et al., 2008;\nwww.frontiersin.org August 2013 | Volume 4 | Article 515| 7\nZorzi et al. Deep learning models of language and cognition\nNair and Hinton, 2009 ). For logistic units, this can be practically\nimplemented by ﬁrst calculating the quantity q-p,w h i c hi st h e n\nmultiplied by a scaling factor and added to the biases (and, possi-\nbly, to each incoming weight) of the hidden units at every weight\nupdate. Depending on the number of hidden units, the desired\nsparsity level ( p) can be set in the range of 0.01–0.1. Monitoring\nthe distribution of the hidden units activity can be useful to ver-\nify that the desired sparsity level is obtained and that the scaling\nfactor is correctly set so that the probability that a unit is active is\nclose to p while learning is not hindered ( Hinton, 2010a).\nTESTING A DBN: READ-OUT OF INTERNAL REPRESENTATIONS\nWhen performing a discriminative task, one of the simplest meth-\nods is to exploit a linear classiﬁer (e.g., Rosenblatt, 1958), to assign\na certain class to each input pattern. The classiﬁer makes a deci-\nsion by using a linear combination of the input features and this\nrepresents its main limitation ( Minsky and Papert, 1969 ). In the\ncase of real sensory signals, this shortcoming is exacerbated by\nthe fact that the feature vectors are high-dimensional and usually\nlie on highly curved and tangled manifolds ( DiCarlo et al., 2012 ).\nHowever, deep belief networks perform a non-linear projection of\nthe feature vector at each hidden layer, gradually building increas-\ningly more complex and abstract representations of the data that\neventually make explicit the latent causes of the sensory signal.\nThis hierarchical organization suggests that a linear “read-out”\nof hidden unit representations should become increasingly more\naccurate as a function of layer depth. In this perspective, accu-\nracy of linear read-out can be considered as a coarse measure of\nhow well the relevant features are explicitly encoded at a given\ndepth of the hierarchical generative model (see, e.g., Stoianov and\nZorzi, 2012 ; Di Bono and Zorzi, under review). As noted above,\nlinear read-out can also be used to monitor the quality of the rep-\nresentations developed by the deep network during unsupervised\ngenerative learning.\nThe linear read-out on internal representations can be eas-\nily implemented using another connectionist module, such as a\nlinear network trained with the delta rule, thereby preserving\nthe biological plausibility of the model. The linear network can\nalso be considered as a response module that supports a particu-\nlar behavioral task, so that its responses can be assessed against\nthe human data (e.g., numerosity perception in Stoianov and\nZorzi, 2012 , or location-invariant visual word recognition in Di\nBono and Zorzi, under review). For example, Stoianov and Zorzi\napplied this approach to simulate human behavior in a numeros-\nity comparison task after training a DBN on thousands of images\nof sets of objects. The internal representations at the deepest layer\nprovided the input to a linear network trained to decide whether\nt h en u m e r o s i t yo ft h ei n p u ti m a g ew a sl a r g e ro rs m a l l e rt h a na\nreference number. Notably, the responses of this decision mod-\nule were described by a psychometric function that was virtually\nidentical to that of human adults, with the classic modulation by\nnumerical ratio that is the signature of Weber’s law for numbers.\nFrom a practical point of view, delta rule learning can be\nconveniently replaced by an equivalent method that is compu-\ntationally more efﬁcient, which relies on the calculation of a\npseudo-inverse matrix (Hertz et al., 1991 ). Formally, data patterns\nP ={ P\n1, P2,..., Pn} can be associated with desired categories\nL ={ L1, L2,..., Ln} by means of the following linear association:\nL = WP\nwhere P and L are matrices containing n column vectors that\ncorrespondingly code patterns Pi (sensory data or internal repre-\nsentations) and binary class labels Li,a n d W is the weight matrix\nof the linear classiﬁer. If an exact solution to this linear system\ndoes not exist, a least-mean-square approximation can be found\nby computing the weight matrix as:\nW = LP+\nwhere P+ is the Moore-Penrose pseudo-inverse ( Albert, 1972)2\nAs an example, we applied the read-out DBN testing method\non the internal representations learned for the images of the\nfour-letter words used in McClelland and Rumelhart (1981) .W e\ntested two different discriminative problems. The ﬁrst required\nthe identiﬁcation of each of the four letters composing a word,\nusing as label a binary vector with one-hot (i.e., localistic) cod-\ning of the target letter. The second problem consisted in the\nidentiﬁcation of the word itself, using as label a binary vec-\ntor with one-hot coding of the target word. To investigate the\nquality of the features extracted by deep learning, we com-\npared the classiﬁcation accuracy on the representations learned\nat each of the levels of a two-layer DBN ( H1 = 120 units, H2 =\n200 units) with that of the representations learned by a single\nRBM with as many hidden units as the top layer of the DBN\n(H = 200 units). As a baseline, we a lso measured the classi-\nﬁcation accuracy obtained by trying to directly categorize the\nraw input vectors. Note that the read-out of the original data\nis trivial, due to lack of variability (and noise) in the cod-\ning of letters and words (i.e., there is a unique pattern for\neach letter and word). Indeed, the raw data vectors are lin-\nearly separable as shown by the perfect accuracy of the read-out.\nHowever, if the input patterns are degraded by adding a cer-\ntain amount of noise, one should expect a progressive decrease\nof the classiﬁcation accuracy when the input representation does\nnot include high-level, invariant features. Indeed, Figure 5shows\nthat when each word image was corrupted by randomly set-\nting to zero a certain percentage of its pixels, read-out accuracy\non the raw pixel data dropped even with a small amount of\nnoise and it approached zero in the word recognition task. As\nexpected, the DBN extracted robust internal representations that\nwere less sensitive to noise. Indeed, both hidden layers sup-\nported good discrimination accuracy for letters, whereas only\nthe deepest hidden layer adequately supported word discrimina-\ntion. Notably, the shallow generative model (RBM) with as many\nhidden units as the top DBN layer did not unfold word-level\ninformation, thereby failing to support robust word recognition\n(especially for larger noise levels ). These results are consistent\nwith the seminal proposal of hierarchical feature processing to\n2In some high-level programming languages, this operation is readily avail-\nable. For example, in MATLAB/Octave we can use the pinv() function,\nW = L∗pinv(P), or the left matrix divide (a.k.a. “backslash”) operator, W =\n(P′\\L’)’ .\nFrontiers in Psychology| Language Sciences August 2013 | Volume 4 | Article 515| 8\nZorzi et al. Deep learning models of language and cognition\nFIGURE 5 |Mean accuracy of the linear classiﬁer on the task of recognizing\neach letter of a word (left) and the whole word (right) as a function of noise\nlevel applied to the raw images.Accuracy is averaged over 20 random noise\ninjections and it is computed over the entire dataset of words. Error bars\nrepresent SEM. The results are shown for read-out from the two hidden\nlayers of a deep network (DBN), a shallow network (RBM), and raw images.\nyield abstract representations of written words ( McClelland and\nRumelhart, 1981).\nANALYZING A DBN\nDiscovering learned representations\nIn the previous section we illustrated how it is possible to assess\nthe quality of the internal representations learned at each layer\nof the hierarchy of a deep belief network by performing a dis-\ncriminative task. However, this information is tied to a given\nclassiﬁcation task and is therefore limited in scope. Moreover, the\nsupervised classiﬁer operates on the pattern of activity over an\nentire hidden layer, that is a distributed representation encod-\ning a variety of micro-features ( Hinton et al., 1986 )r e p r e s e n t i n g\ntask-independent statistical regularities of the data. A very simple\nbut informative approach to investigate the role of a particular\nunit in the network consists of visualizing its connection weights\nusing the original structure of the data (e.g., the 2D image in our\nvisual perception examples). This is particularly intuitive for the\nﬁrst hidden layer, where the weight matrix deﬁnes how the vis-\nible units contribute to the activation of each hidden unit. We\ncan therefore visualize the “receptive ﬁeld” of each hidden unit\nby plotting the strength of its visible-to-hidden connections. The\nsame principle can be applied to the deeper layers of the DBN,\nby combining their weight matrix with those of the lower lay-\ners. A straightforward way is to use a linear combination of the\nweight matrices, possibly imposing a threshold on the absolute\nvalues of the weights in order to select only strong connections.\nThis allows to visualize the receptive ﬁeld learned at a layer k as\na weighted linear combination of the receptive ﬁelds learned at\nlevel k-1 (Lee et al., 2008, 2009 ). The main drawbacks of this tech-\nnique are that one has to manually choose threshold values and\nthat non-linearities between layers are not considered, with the\nrisk of losing relevant information. Nevertheless, this method can\nprovide good visualization of the learned features even without\nimposing a threshold on the weights (see Figure 6).\nUsing the above method, we analyzed the receptive ﬁelds of\nthe hidden units of DBNs trained on images of letters as well\non the handwritten digits of the MNIST dataset. In the letter\nperception task, we found that most of the units of the ﬁrst hid-\nden layer were tuned to basic geometric features, whereas most of\nthe units of the second hidden layer were tuned to a composition\nof these features (see examples in Figure 6A). The greater image\nresolution and variability of the handwritten digits pose a much\nmore complex visual problem, which induced the emergence of\na more structured hierarchy of features in the DBN. As shown\nin Figure 6B, the ﬁrst hidden layer learned simple and localized\nvisual features (mostly Gaussian and Gabor ﬁlters), resembling\nthose found in the primary visual cortex. The second hidden layer\ncombined these features into edges, lines, and strokes detectors.\nFinally, the third hidden layer extracted even more complex visual\nfeatures that resemble parts of digits. Note that the ﬁnding of low-\nlevel visual features (basis functions) in the ﬁrst hidden layer is\ncommon to many problems that involve a large variability in the\ntraining images (see, e.g., Lee et al., 2008; Stoianov and Zorzi,\n2012).\nApplying sparsity constraints on the internal representations\nfurther improves the quality of the emerging features. For exam-\nple, a sparse DBN trained on patches of natural images developed\ncomplex receptive ﬁelds (e.g., T-junctions) in the second hidden\nlayer that were very similar to those found in area V2 of the visual\ncortex (Lee et al., 2008 ). Our sparse DBN simulations also resulted\nin an increase of the complexity of the emergent features. For\nexample, the letter perception network encoded more letter-like\nfeatures in the second hidden layer ( Figure 6C)a n dt h eh a n d -\nwritten digit perception network learned shape-speciﬁc detectors\nin the third hidden layer ( Figure 6D).\nA more sophisticated approach to investigate the features\nencoded by a hidden unit is to ﬁnd its preferred input stimuli, as\ndone by neurophysiologists in single-cell recording studies. The\nbasic idea is to probe the network on a variety of input patterns,\neach time recording the neural response and then looking for pos-\nsible regularities. This approach can be very effective if we have an\nidea about which type of patterns are more likely to elicit speciﬁc\nresponses (for example, responses to bigrams after training on\nwords; Di Bono and Zorzi, under review). However, if we cannot\nmake assumptions about the nature of the preferred stimuli, this\nwww.frontiersin.org August 2013 | Volume 4 | Article 515| 9\nZorzi et al. Deep learning models of language and cognition\nFIGURE 6 | Visualization of features learned at different hidden layers\n(Hi ). Each square within a layer represents the receptive ﬁeld of one hidden\nunit. Excitatory connections are shown in white, whereas inhibitory\nconnections are in black.(A) H1 and H2 on single letters (pixelated “Siple\nfont”). (B) H1, H2 and H3 on MNIST .(C) Sparse H1 and H2 on single letters.\n(D) Sparse H3 on MNIST . From left to right: H1 on single letters (pixelated\n“Siple font”); H2 on single letters; H1 on MNIST ; H2 on MNIST ; H3 on MNIST ;\nsparse H1 on single letters; sparse H2 on single letters; sparse H3 on MNIST .\nmethod becomes computationally intractable because it would\nrequire testing the network on an exponential number of possi-\nble input patterns. Nevertheless, this problem can be solved by\nformulating it as an optimization problem ,w h e r et h eg o a li st o\nﬁnd the input pattern that maximizes the activation of a certain\nhidden unit given the processing constraints imposed by the net-\nwork (Erhan et al., 2009 ). Formally, if θ denotes the deep network\nparameters (weights and biases) and h\nij(θ, x) is the activation of a\ngiven unit i from a given layer j in the network, then hij is a func-\ntion of both θ and the input sample x. Assuming that the vector x\nhas a bounded norm and after learning the parameters are ﬁxed,\nthen the problem of maximizing the unit activation is:\nx∗ = arg max\nx\nhij(θ, x)\nAlthough this is a non-convex optimization problem, it has been\nempirically shown that good local minima can be found ( Erhan\net al., 2009 ). This method has been recently used to investigate\nwhether high-level, class-speciﬁc feature detectors can emerge in\nvery-large-scale deep unsupervised learning (i.e., using millions\nof images for training; Le et al., 2012 ). The impressive result was\nthat it is indeed possible to learn highly complex and abstract fea-\ntures at the deepest layers, such as prototypical faces ( Le et al.,\n2012).\nA different approach can be used if we expect monotonic\nresponse of some hidden units to a given property of the data. The\nindividuation of these detectors is based on regressing the prop-\nerty of interest (or even multiple properties) onto the response\nof each hidden unit. A high absolute value of the normalized\nregression coefﬁcient indicates sensitivity of the hidden unit to\nthe property of interest; this might also indicate selectivity when\ncombined with small (near-zero) regression coefﬁcients for other\nproperties. Using this method, Stoianov and Zorzi (2012) discov-\nered detectors in the second hidden layer of their DBN tuned to\nvisual numerosity but insensitive to other important visual prop-\nerties like cumulative area. Di Bono and Zorzi (under review)\nalso used this method to investigate word selectivity in their DBN\nmodel of visual word recognition. After ﬁnding the preferred\nword for a given hidden unit, its word selectivity was assessed by\nrecording the response to all other training words and perform-\ning a regression analysis using the orthographic (i.e., Levenshtein)\ndistance from the preferred word as predictor.\nSampling from the generative model\nUp to this point, we only discussed methods that investigate the\nbottom–up processing of sensory data. However, a deep belief net-\nwork is a generative model, and it can be very useful to assess\nthe top–down generation of sensory data, as well as the mixing\nof bottom–up and top–down signals during inference in a noisy\nsituation. In one scenario, we can provide to the model a noisy\ninput pattern (e.g., randomly corrupted or partially occluded)\nand let the network ﬁnd the most likely interpretation of the data\nunder the generative model. This process requires the iteratively\nsampling of the states of the network until an equilibrium activa-\ntion state is reached, which in DBNs can be efﬁciently done using\nblock Gibbs sampling (see Box 3). As an example, in Figure 7we\nshow the result of inference in the word perception DBN when\nfour different noisy versions of the same image are given as input\nto the model. Note that the visible units settle onto an activation\nstate corresponding to the correct word image.\nWe can also study the generative capability of a DBN when\nthe visible units are not clamped to an initial state, and the net-\nwork is therefore let free to autonomously produce a sensory\npattern through a completely top–down process. This genera-\ntive process can be constrained to produce “class prototypes” by\nFrontiers in Psychology| Language Sciences August 2013 | Volume 4 | Article 515| 10\nZorzi et al. Deep learning models of language and cognition\nFIGURE 7 | Inference in the word perception DBN when the word\nimage “WORK” is presented as input under different types of noise.\nFrom top to bottom: Gaussian noise, binary noise (30%), binary noise,\n(50%), occlusion noise. The ﬁnal state of the visible units, identical across\nthe four noise conditions, is shown on the right.\nadding a multimodal RBM on the top of the network hierarchy\n(Hinton et al., 2006 ), which is jointly trained using two input\nsources, one containing the internal representation learned by\nthe DBN and the other encoding the corresponding label. For\nexample, in the handwritten digit recognition model, input to\nthe multimodal RBM is provided by the second hidden layer\n(500 units) and by 10 units representing the image label (one\nunit for each possible digit class) (see Figure 8A). After learning,\nthe label units can be clamped to a certain state (e.g., with only\nthe unit corresponding to the class “7” active) and the top RBM\nsettles to equilibrium, thereby recovering the internal represen-\ntation of the given digit class. The generative connections of the\nDBN can then be used to obtain an image on the visible layer\ni nas i n g l et o p – d o w np a s s .T h ei m a g eg e n e r a t e dc a nb et h o u g h t\nof as the model’s prototype for the corresponding abstract\nrepresentation.\nHere we propose an interesting, more simple variant of the\ntop–down generation of the learned prototypes. Instead of jointly\ntraining the top-level RBM using the internal representation of\nimages and the corresponding class label, and then performing\nGibbs sampling until equilibrium with the label units clamped\nto a certain class, we can try to directly map the class label\nand the internal representation through a linear projection (see\nFigure 8B). This mapping is analogous to the read-out mod-\nule previously discussed but it works in the opposite direction.\nPrototype generation can thus be performed by associating the\nclass vectors L with the internal representations P learned by the\nDBN through a weight matrix W\n2:\nP = W2L\nW2 = PL+\nAs in Hinton et al. (2006) , after computing the internal state\nP at the deepest layer, a single top–down pass through the\ngenerative connections of the DBN produces the prototype\nfor the speciﬁc class. Figure 8C shows the prototypes gener-\nated for each digit class of the MNIST dataset using this lin-\near projection method. Note that this method can be readily\nextended to more complex scenarios that involve a mapping\nbetween internal representations learned by different networks\n(which may reﬂect knowledge about different domains or sensory\nmodalities).\nFinally, it is worth noting that the quality of inference when\nsampling from the generative model can be improved if the single\ntop–down pass is replaced by an interactive process, as pro-\nposed in a recent variant of the DBN known as Deep Boltzmann\nMachine (Salakhutdinov and Hinton, 2009 ).\nDISCUSSION\nUnderstanding how cognition and language might emerge from\nneural computation is certainly one of the most exciting frontiers\nin cognitive neuroscience. In this tutorial overview we discussed a\nrecent step forward in connectionist modeling, which allows the\nemergence of hierarchical representations in a deep neural net-\nwork learning a generative model of the sensory data. We started\nby reviewing the theoretical foundations of deep learning, which\nrely on the framework of probabilistic graphical models to derive\nefﬁcient inference and learning algorithms over hierarchically\norganized energy-based models. We then provided a step-by-step\ntutorial on how to practically perform a complete deep learn-\ning simulation, covering the main aspects related to the training,\ntesting and analysis of deep belief networks. In our presentation\nwe focused on examples that require the progressive extraction of\nabstract representations from sensory data and that are therefore\nrepresentative of a wide range of cognitive processes. In particular,\nwe showed how deep learning can be applied to the classic let-\nter and word perception problem of McClelland and Rumelhart\n(1981). In addition to providing a useful toy example of mod-\neling based on deep learning, the emergent properties of the\nmodel revisit key aspects of the seminal IAM and suggest a very\npromising research direction for developing a full-blown deep\nlearning model of visual word recognition. Indeed, up-scaling the\npresent toy model is likely to be successful because deep learn-\ning is particularly suited to capture features hierarchies over large\ntraining datasets with great pattern variability. This aspect was\npresent in two additional problems that complemented our tuto-\nrial with more realistic simulations, that is, handwritten digit\nrecognition ( LeCun et al., 1998 ) and visual numerosity percep-\ntion (Stoianov and Zorzi, 2012). T ogether, the various simulations\nillustrate the strength of the deep learning approach to cognitive\nmodeling.\nDeep unsupervised learning extracts increasingly more\nabstract representations of the world, with the important conse-\nquence that explanatory factors behind the sensory data can be\nshared across tasks. The hierarchical architecture captures higher\norder structure of input data that might be invisible at the lower\nlevels and it efﬁciently exploits features re-use. The idea that\nlearned internal representations at the deepest layers can be easily\n“read-out” is consistent with the notion of “explicitness of infor-\nmation” articulated by Kirsh (1990), who argued that explicitness\nis tightly related to the processing system which uses it. Within\nthis perspective, the degree of explicitness is better linked to the\nwww.frontiersin.org August 2013 | Volume 4 | Article 515| 11\nZorzi et al. Deep learning models of language and cognition\nFIGURE 8 | Illustration of the prototype generation methods in the\nhandwritten digit recognition model. (A)The RBM involving the third\nhidden layer is jointly trained on the internal representation of the second\nhidden layer and an additional set of units representing the digit classes\n(Hinton et al., 2006). (B) Our linear projection method: class label units are\nonly added after the complete DBN training and areassociated to the third\nhidden layer representations by means of a linear mapping.(C) Digit\nprototypes generated using the linear projection method.\nusability of information rather than to its form (i.e., how quickly\nit can be accessed, retrieved or in some other manner put to\nuse). This idea has been further extended by Clark (1992) ,w h o\nproposed to take into account also the multi-track usability of\nstored information: “Truly explicit items of information should\nbe usable in a wide variety of ways, that is, not restricted to use\nin a single task” (p. 198). Note that this conception of abstract\nrepresentations that can be shared across tasks or even across\ndomains is particularly useful in the context of modeling language\nprocessing.\nEfﬁcient generative learning in neural networks is a recent\nbreakthrough in machine learning and its potential has yet to\nbe fully unfolded. In particular, the extension of RBMs to the\ntemporal domain ( Sutskever et al., 2008; Taylor and Hinton,\n2007) is a very promising avenue for research. Indeed, genera-\ntive networks that learn the temporal dynamics of the data could\nanticipate relevant events in the environment, using the history\nof the system as context to make accurate predictions about\nthe incoming information, as proposed by the predictive cod-\ning framework ( Huang and Rao, 2011; Clark, 2013 ). Learning\nand processing of sequential info rmation is also a key aspect of\ncognition and it is particularly ubiquitous in language processing\n(Elman, 1990). An initial exploration of this direction is the use of\nthe Recurrent Temporal RBM ( Sutskever et al., 2008 ) for learning\northographic structure from letter sequences ( Testolin et al., 2012,\nsubmitted).\nIt is worth noting that deep generative network models of cog-\nnition can offer a uniﬁed theoretical framework that encompasses\nclassic connectionism and the st ructured Bayesian approach to\ncognition. Structured Bayesian models of cognition (for reviews\nsee Chater et al., 2006; Grifﬁths et al., 2010 ) assume that human\nlearning and inference approximately follow the principles of\nBayesian probabilistic inference and they have been used in\nthe last few years to address a number of issues in cognitive\nscience, including language processing ( Chater and Manning,\n2006, for review). However, Bayesian models are typically for-\nmulated at the level of “computational theory” ( Marr, 1982 )\nrather than at the process level that characterizes other cogni-\ntive modeling paradigms like connectionism (for further discus-\nsion see McClelland et al., 2010; Jones and Love, 2011 ). This\nimplies limits on the phenomena that can be studied with the\nBayesian approach, because only problems of inductive inference\nor that contain an inductive component are naturally expressed\nin Bayesian terms ( Grifﬁths et al., 2008 ). In contrast, computa-\ntional models of cognition based on deep neural networks and\ngenerative learning implement the probabilistic approach in a\nneural-like architecture and can provide an emergentist expla-\nnation of structured representations that is in line with the\nconnectionist tradition ( McClelland et al., 2010 ). Their proba-\nbilistic formulation not only allows to deal with ambiguity of\nsensory input and with the intrinsic uncertainty of environ-\nmental dynamics, but it also provides a coherent theory about\nhow learning can integrate new evidence to reﬁne beliefs of the\nmodel. Importantly, there is no need to have an external signal\nthat guides learning, because the aim is to reproduce incoming\ninformation as accurately as possible by discovering its hidden\ncauses (that is, learning can be seen as a stochastic inference\nproblem).\nIn conclusion, we believe that the focus on deep architec-\ntures and generative learning represents a crucial step forward for\nthe connectionist modeling enterprise, because it offers a more\nplausible model of cortical learning as well as way to bridge the\ngap between emergentist connectionist models and structured\nBayesian models of cognition.\nACKNOWLEDGMENTS\nThis study was supported by the European Research Council\n(grant no. 210922 to Marco Zorzi).\nFrontiers in Psychology| Language Sciences August 2013 | Volume 4 | Article 515| 12\nZorzi et al. Deep learning models of language and cognition\nREFERENCES\nA c k l e y ,D . ,H i n t o n ,G .E . ,a n d\nSejnowski, T. J. (1985). A learning\nalgorithm for boltzmann machines.\nCogn. Sci. 9, 147–169. doi: 10.1207/\ns15516709cog0901_7\nAlbert, A. (1972). Regression and the\nMoore-Penrose pseudoinverse .N e w\nY ork, NY: Academic Press.\nAndrieu, C., De Freitas, N., Doucet,\nA., Jordan, M. I., and Freitas,\nN. De, (2003). An introduction\nto MCMC for machine learning.\nMach. Learn. 50, 5–43. doi: 10.1023/\nA:1020281327116\nBaldi, P . (2012). Autoencoders, unsu-\npervised learning, and deep archi-\ntectures. J. Mach. Learn. Res. 27,\n37–50.\nBengio, Y. (2009). Learning deep\narchitectures for AI. Found. Trends\nMach. Learn. 2, 1–127. doi: 10.1561/\n2200000006\nBengio, Y., Courville, A., and Vincent,\nP . (2012). Representation learning: a\nreview and new perspectives. arXiv\n1206.5538, 1–34.\nBengio, Y., and Lamblin, P . (2007).\nGreedy layer-wise training of deep\nnetworks. Adv. Neural Inform.\nProcess. Syst. 19, 153–170.\nChater, N., and Manning, C. D. (2006).\nProbabilistic models of language\nprocessing and acquisition. Trends\nCogn. Sci. 10, 335–344. doi: 10.1016/\nj.tics.2006.05.006\nChater, N., T enenbaum, J. B., and\nYuille, A. (2006). Probabilistic\nmodels of cognition: con-\nceptual foundations. Trends\nCogn. Sci. 10, 287–291. doi:\n10.1016/j.tics.2006.05.007\nClark, A. (1992). The presence of a\nsymbol. Connect. Sci. 4, 193–205.\nClark, A. (2013). Whatever next.\nPredictive brains, situated agents,\nand the future of cognitive science.\nBehav. Brain Sci. 36, 181–204. doi:\n10.1017/S0140525X12000477\nDayan, P ., Hinton, G. E., Neal, R. M.,\nand Zemel, R. S. (1995). The\nHelmholtz machine. Neural\nComput. 7, 889–904. doi:\n10.1162/neco.1995.7.5.889\nD e a n ,J . ,C o r r a d o ,G .S . ,M o n g a ,R . ,\nC h e n ,K . ,D e v i n ,M . ,L e ,Q .V . ,\net al. (2012). Large scale distributed\ndeep networks. Adv. Neural Inform.\nProcess. Syst. 24, 1–9.\nDiCarlo, J. J., Zoccolan, D., and\nRust, N. C. (2012). How does the\nbrain solve visual object recog-\nnition. Neuron 73, 415–434. doi:\n10.1016/j.neuron.2012.01.010\nDe Filippo De Grazia, M., Cutini, S.,\nLisi, M., and Zorzi, M. (2012). Space\nc o d i n gf o rs e n s o r i m o t o rt r a n s f o r -\nmations can emerge through\nunsupervised learning. Cogn.\nProcess. 13(Suppl. 1), 141–146. doi:\n10.1007/s10339-012-0478-4\nDehaene, S., Cohen, L., Sigman, M.,\nand Vinckier, F. (2005). The neural\ncode for written words: a proposal.\nTrends Cogn. Sci. 9, 335–341. doi:\n10.1016/j.tics.2005.05.004\nElman, J. L. (1990). Finding structure in\ntime. Cogn. Sci. 14, 179–211.\nErhan, D., Bengio, Y., Courville,\nA., and Vincent, P . (2009).\n“Visualizing higher-layer features\nof a deep network,” in Technical\nReport, Univeristé de Montréal ,\n1–13.\nFriston, K. (2005). A theory of corti-\ncal responses. P h i l o s .T r a n s .R .S o c .\nL o n d .BB i o l .S c i .360, 815–836. doi:\n10.1098/rstb.2005.1622\nGeman, S., and Geman, D. (1984).\nStochastic relaxation, Gibbs dis-\ntributions, and the Bayesian\nrestoration of images. IEEE\nTrans. Pattern Anal. Mach.\nIntell. 6, 721–741. doi: 10.1109/\nTPAMI.1984.4767596\nGhahramani, Z., Korenberg, A., and\nHinton, G. E. (1999). “Scaling in\na hierarchical unsupervised net-\nwork,” in International Conference\non Artiﬁcial Neural Networks ,\n(Edinburgh), 13–18.\nG r i f ﬁ t h s ,T .L . ,C h a t e r ,N . ,K e m p ,\nC., Perfors, A., and T enenbaum,\nJ. B. (2010). Probabilistic mod-\nels of cognition: exploring rep-\nresentations and inductive biases.\nTrends Cogn. Sci. 14, 357–364. doi:\n10.1016/j.tics.2010.05.004\nG r i f ﬁ t h s ,T .L . ,K e m p ,C . ,a n d\nT enenbaum, J. B. (2008). “Bayesian\nmodels of cognition,” in Cambridge\nHandbook of Computational\nCognitive Modeling ,e dR .S u n\n(Cambridge, MA: Cambridge\nUniversity Press), 59–100.\nH a r m ,M .W . ,a n dS e i d e n b e r g ,M .S .\n(1999). Phonology, reading acqui-\nsition, and dyslexia: insights from\nconnectionist models. Psychol. Rev.\n106, 491–528. doi: 10.1037/0033-\n295X.106.3.491\nHarm, M. W., and Seidenberg, M.\nS. (2004). Computing the mean-\nings of words in reading: coop-\nerative division of labor between\nvisual and phonological processes.\nPsychol. Rev. 111, 662–720. doi:\n10.1037/0033-295X.111.3.662\nHertz, J. A., Krogh, A. S., and Palmer, R.\nG. (1991). Introduction to the Theory\nof Neural Computation. Redwood\nCity, CA: Addison-Weasley.\nHinton, G. (2013). Where do features\ncome from? Cogn. Sci. doi: 10.1111/\ncogs.12049. [Epub ahead of print].\nHinton, G. E. (2002). Training prod-\nucts of experts by minimizing\ncontrastive divergence. Neural\nComput. 14, 1771–1800. doi:\n10.1162/089976602760128018\nHinton, G. E. (2007). Learning mul-\ntiple layers of representation.\nTrends Cogn. Sci. 11, 428–434. doi:\n10.1016/j.tics.2007.09.004\nHinton, G. E. (2010a). “A practical\nguide to training restricted boltz-\nmann machines,” in Technical\nReport UTML TR 2010-003,\nUniversity of Toronto.9 ,1 .\nHinton, G. E. (2010b). Learning to rep-\nresent visual input. Philos. Trans. R.\nSoc. Lond. B Biol. Sci. 365, 177–184.\ndoi: 10.1098/rstb.2009.0200\nHinton, G. E., and Ghahramani, Z.\n(1997). Generative models for dis-\ncovering sparse distributed repre-\nsentations. P h i l o s .T r a n s .R .S o c .\nLond. B Biol. Sci. 352, 1177–1190.\ndoi: 10.1098/rstb.1997.0101\nHinton, G. E., McClelland, J. L.,\nand Rumelhart, D. E. (1986).\n“Distributed representations,” in\nParallel Distributed Processing:\nExplorations in the Microstructure of\nCognition (Cambridge, MA: MIT\nPress), 77–109.\nH i n t o n ,G .E . ,O s i n d e r o ,S . ,a n d\nT eh, Y. (2006). A fast learning\nalgorithm for deep belief nets.\nNeural Comput. 18, 1527–1554. doi:\n10.1162/neco.2006.18.7.1527\nHinton, G. E., and Salakhutdinov,\nR. (2006). Reducing the dimen-\nsionality of data with neural net-\nworks. Science 313, 504–507. doi:\n10.1126/science.1127647\nHinton, G. E., and Sejnowski,\nT. J. (1999). Unsupervised\nLearning: Foundations of Neural\nComputation.C a m b r i d g e ,M A :\nMIT Press.\nHuang, Y., and Rao, R. P . N. (2011).\nPredictive coding. Wiley interdis-\nciplinary reviews. Cogn. Sci. 2,\n580–593. doi: 10.1002/wcs.142\nJones, M., and Love, B. C. (2011).\nBayesian fundamentalism or\nenlightenment. on the explanatory\nstatus and theoretical contributions\nof bayesian models of cognition.\nBehav. Brain Sci. 34, 169–88;\ndisuccsion 188–231.\nJordan, M. I., and Sejnowski, T.\nJ. (2001). Graphical Models:\nFoundations of Neural Computation .\nCambridge, MA: MIT Press.\nKhaitan, P ., and McClelland, J. L.\n(2010). “Matching exact posterior\nprobabilities in the multinomial\ninteractive activation model,” in\nProceedings of the 32nd Annual\nMeeting of the Cognitive Science\nSociety, eds S. Ohlsson and R.\nCatrambone (Austin, TX: Cognitive\nScience Society), 623.\nKirsh, D. (1990). “When is infor-\nmation explicitly represented?” in\nThe Vancouver Studies in Cognitive\nScience, ed P . Hanson (Vancouver,\nBC: UBC Press), 340–365.\nKohonen, T. (1990). The self-\norganizing map. Proc. IEEE 78,\n1464–1480. doi: 10.1109/5.58325\nKoller, D., and Friedman, N.\n(2009). Probabilistic Graphical\nModels: Principles and Techniques .\nCambridge, MA: The MIT Press.\nKrizhevsky, A., Sutskever, I., and\nHinton, G. E. (2012). ImageNet\nclassiﬁcation with deep convolu-\ntional neural networks. Adv. Neural\nInform. Process. Syst. 24, 1–9.\nL e ,Q .V . ,R a n z a t o ,M .A . ,M o n g a ,R . ,\nDevin, M., Chen, K., Corrado, G.\nS., et al. (2012). “Building high-\nlevel features using large scale unsu-\npervised learning,” in International\nConference on Machine Learning ,\n(Edinburgh).\nL e C u n ,Y . ,B o t t o u ,L . ,B e n g i o ,Y . ,\nand Haffner, P . (1998). Gradient-\nbased learning applied to docu-\nment recognition. Proc. IEEE 86,\n2278–2324. doi: 10.1109/5.726791\nLee, H., Ekanadham, C., and Ng, A.\nY. (2008). Sparse deep belief net\nmodels for visual area V2. Adv.\nNeural Inform. Process. Syst. 20,\n873–880.\nLee, H., Grosse, R., Ranganath, R., and\nNg, A. Y. (2009). “Convolutional\ndeep belief networks for scalable\nunsupervised learning of hierarchi-\ncal representations,” in International\nConference on Machine Learning\n(New Y ork, NY: ACM Press),\n609–616.\nLove, B. C. (2002). Comparing super-\nvised and unsupervised category\nlearning. Psychon. Bull. Rev. 9,\n829–835. doi: 10.3758/BF03196342\nL o v e ,B .C . ,M e d i n ,D .L . ,a n dG u r e c k i s ,\nT. M. (2004). SUSTAIN: a net-\nwork model of category learning.\nPsychol. Rev. 111, 309–332. doi:\n10.1037/0033-295X.111.2.309\nMarr, D. (1982). Vision: a\nComputational Investigation into\nthe Human Representation and\nProcessing of Visual Information .S a n\nFrancisco, CA: W. H. Freeman and\nCompany.\nMcClelland, J. L., Botvinick, M. M.,\nNoelle, D. C., Plaut, D. C., Rogers,\nT. T., Seidenberg, M. S., et al.\n(2010). Letting structure emerge:\nconnectionist and dynamical sys-\ntems approaches to cognition.\nTrends Cogn. Sci. 14, 348–356. doi:\n10.1016/j.tics.2010.06.002\nM c C l e l l a n d ,J .L . ,a n dR u m e l h a r t ,D .\nE. (1981). An interactive activation\nmodel of context effects in letter\nperception: I. An account of basic\nﬁndings. Psychol. Rev. 88, 375. doi:\n10.1037/0033-295X.88.5.375\nwww.frontiersin.org August 2013 | Volume 4 | Article 515| 13\nZorzi et al. Deep learning models of language and cognition\nMinsky, M., and Papert, S. (1969).\nPerceptrons: an Introduction\nto Computational Geometry.\nCambridge, MA: MIT Press.\nM i r m a n ,D . ,B o l g e r ,D .J . ,K h a i t a n ,P . ,\na n dM c C l e l l a n d ,J .L .( i np r e s s ) .\nInteractive activation and mutual\nconstraint satisfaction. Cogn. Sci.\nNair, V ., and Hinton, G. E. (2009).\n3D Object Recognition with Deep\nBelief Nets. Adv. Neural Inf. Process.\nSyst. 21, 1339–1347.\nNeal, R. M., and Hinton, G. E. (1998).\n“A view of the EM algorithm that\njustiﬁes incremental, sparse, and\nother variants,” in Learning in\ngraphical models ,e dM .I .J o r d a n\n(Dordecht, The Netherlands:\nKluwer Academic Publishers),\n355–368.\nOja, E. (1982). Simpliﬁed neuron\nmodel as a principal component\nanalyzer. J. Math. Biol. 1, 267–273.\ndoi: 10.1007/BF00275687\nOlshausen, B. A, and Field, D. J. (1996).\nEmergence of simple-cell receptive\nﬁeld properties by learning a sparse\ncode for natural images. Nature 381,\n607–609. doi: 10.1038/381607a0\nOlshausen, B. A, and Field, D. J.\n(2004). Sparse coding of sensory\ninputs. Curr. Opin. Neurobiol.\n14, 481–487. doi: 10.1016/j.conb.\n2004.07.007\nO’Reilly, R. (1998). Six principles\nfor biologically based com-\nputational models of cortical\ncognition. Trends Cogn. Sci. 2,\n455–462.\nO’Reilly, R. C. (2001). Generalization\nin interactive networks: the bene-\nﬁts of inhibitory competition and\nHebbian learning. Neural Comput.\n13, 1199–1241.\nO’Reilly, R., and Munakata, Y. (2000).\nComputational Exploration in\nCognitive Neuroscience .C a m b r i d g e ,\nMA: MIT Press.\nPearl, J. (1988). Probabilistic Reasoning\nin Intelligent Systems: Networks of\nPlausible Inference . San Francisco,\nCA: Morgan Kaufmann.\nP e r r y ,C . ,Z i e g l e r ,J .C . ,a n dZ o r z i ,\nM. (2007). Nested incremental\nmodeling in the development\nof computational theories: the\nCDP+ model of reading aloud.\nPsychol. Rev. 114, 273–315. doi:\n10.1037/0033-295X.114.2.273\nP e r r y ,C . ,Z i e g l e r ,J .C . ,a n dZ o r z i ,M .\n(2010). Beyond single syllables:\nlarge-scale modeling of reading\naloud with the Connectionist\nDual Process (CDP++) model.\nCogn. Psychol. 61, 106–151. doi:\n10.1016/j.cogpsych.2010.04.001\nP e r r y ,C . ,Z i e g l e r ,J .C . ,a n dZ o r z i ,\nM. (2013). A computational\nand empirical investigation of\ngraphemes in reading. Cogn. Sci. 37,\n800–828. doi: 10.1111/cogs.12030\nPlaut, D. C., McClelland, J. L.,\nS e i d e n b e r g ,M .S . ,a n dP a t t e r s o n ,K .\n(1996). Understanding normal and\nimpaired word reading: computa-\ntional principles in quasi-regular\ndomains. Psychol. Rev. 103, 56–115.\ndoi: 10.1037/0033-295X.103.1.56\nPlaut, D., and Shallice, T. (1993). Deep\ndyslexia: a case study of connec-\ntionist neuropsychology. Cogn.\nNeuropsychol. 10, 377–500. doi:\n10.1080/02643299308253469\nR a i n a ,R . ,M a d h a v a n ,A . ,a n dN g ,A .Y .\n(2009). “Large-scale deep unsuper-\nvised learning using graphics pro-\ncessors,” in International Conference\non Machine Learning (New Y ork,\nNY: ACM Press), 1–8.\nRao, R. P . N., and Ballard, D. H. (1999).\nPredictive coding in the visual cor-\ntex: a functional interpretation of\nsome extra-classical receptive-ﬁeld\neffects. Nat. Neurosci. 2, 79–87. doi:\n10.1038/4580\nRiesenhuber, M., and Poggio, T. (1999).\nHierarchical models of object recog-\nnition in cortex. Nat. Neurosci. 2,\n1019–1025. doi: 10.1038/14819\nRosenblatt, F. (1958). The perceptron:\na probabilistic model for informa-\ntion storage and organization in the\nbrain. Psychol. Rev. 65, 386. doi:\n10.1037/h0042519\nRumelhart, D. E., Hinton, G. E., and\nWilliams, R. (1986). Learning rep-\nresentations by back-propagating\nerrors. Nature 323, 533–536. doi:\n10.1038/323533a0\nRumelhart, D., and McClelland,\nJ. (1986). Parallel Distributed\nProcessing: Explorations in the\nMicrostructure of Cognition ,V o l .1 :\nFoundations. Cambridge, MA: MIT\nPress.\nRumelhart, D. E., and Siple, P . (1974).\nProcess of recognizing tachistoscop-\nically presented words. Psychol. Rev.\n81, 99–118. doi: 10.1037/h0036117\nRumelhart, D. E., and T odd, P . M.\n(1993). “Learning and connection-\nist representations,” in Attention\nand Performance XIV: Synergies\nin Experimental Psychology,\nArtiﬁcial Intelligence, and Cognitive\nNeuroscience,e d sD .M e y e ra n dS .\nKornblum (Cambridge, MA: MIT\nPress), 3–30.\nRumelhart, D. E., and Zipser, D. (1985).\nFeature discovery by competitive\nlearning. Cogn. Sci. 9, 75–112. doi:\n10.1207/s15516709cog0901_5\nSalakhutdinov, R., and Hinton, G.\nE. (2009). “Deep boltzmann\nmachines,” in International\nConference on Artiﬁcial Intelligence\nand Statistics , (Clearwater Beach,\nFL), 448–455.\nSchyns, P ., Goldstone, R., and Thibaut,\nJ. (1998). The development of\nfeatures in object concepts.\nBehav. Brain Sci. 21, 1–17. doi:\n10.1017/S0140525X98000107\nS e i d e n b e r g ,M .S . ,a n dM c C l e l l a n d ,J .L .\n(1989). A distributed, developmen-\ntal model of word recognition and\nnaming. Psychol. Rev. 96, 523–568.\ndoi: 10.1037/0033-295X.96.4.523\nStoianov, I., and Zorzi, M. (2012).\nEmergence of a “visual number\nsense” in hierarchical generative\nmodels. Nat. Neurosci. 15, 194–196.\ndoi: 10.1038/nn.2996\nStoianov, I., Zorzi, M., Becker, S., and\nUmiltá, C. (2002). “Associative\narithmetic with Boltzmann\nMachines: The role of number\nrepresentations,” in Lecture Notes\nin Computer Science: ICANN 2002 ,\ned J. Dorronsoro (Berlin: Springer),\n277–283.\nS t o i a n o v ,I . ,Z o r z i ,M . ,a n dU m i l t à ,C .\n(2004). The role of semantic and\nsymbolic representations in arith-\nmetic processing: insights from sim-\nulated dyscalculia in a connection-\nist model. Cortex 40, 194–196. doi:\n10.1016/S0010-9452(08)70948-1\nSutskever, I., Hinton, G. E., and Taylor,\nG. (2008). The recurrent tempo-\nral restricted Boltzmann machine.\nAdv. Neural Inform. Process. Syst. 20,\n1601–1608.\nSutton, R. S., and Barto, A. G. (1998).\nReinforcement Learning. Cambridge,\nMA: MIT Press.\nTaylor, G., and Hinton, G. E. (2007).\nModeling human motion using\nbinary latent variables. Adv.\nNeural Inform. Process. Syst. 19,\n1345–1352.\nT estolin, A., Sperduti, A., Stoianov, I.,\nand Zorzi, M. (2012). “Assessment\nof sequential boltzmann machines\non a lexical processing task,” in\nEuropean Symposium on Artiﬁcial\nNeural Networks, Computational\nIntelligence and Machine Learning ,\n(Bruges), 275–280.\nT estolin, A., Stoianov, I., De Filippo De\nGrazia, M., and Zorzi, M. (2013).\nDeep unsupervised learning on a\ndesktop PC: a primer for cognitive\nscientists. Front. Psychol. 4:251. doi:\n10.3389/fpsyg.2013.00251\nVinckier, F., Dehaene, S., Jobert, A.,\nD u b u s ,J .P . ,S i g m a n ,M . ,a n d\nCohen, L. (2007). Hierarchical cod-\ning of letter strings in the ventral\nstream: dissecting the inner orga-\nnization of the visual word-form\nsystem. Neuron 55, 143–156. doi:\n10.1016/j.neuron.2007.05.031\nWelling, M., and Hinton, G. E. (2002).\n“A new learning algorithm for\nmean ﬁeld boltzmann machines,” in\nLecture Notes in Computer Science:\nICANN 2002 , ed J. Dorronsoro\n(Berlin: Springer), 351–357.\nZorzi, M., Houghton, G., and\nButterworth, B. (1998). Two\nroutes or one in reading aloud.\na connectionist dual-process\nmodel. J. Exp. Psychol. Hum.\nPercept. Perfor 24, 1131–1161. doi:\n10.1037/0096-1523.24.4.1131\nZorzi, M., Stoianov, I., and Umiltà,\nC. (2005). “Computational mod-\neling of numerical cognition,”\nin Handbook of Mathematical\nCognition,e dJ .C a m p b e l l( N e w\nY ork, NY: Psychology Press),\n67–84.\nConﬂict of Interest Statement: The\nauthors declare that the research\nwas conducted in the absence of any\ncommercial or ﬁnancial relationships\nthat could be construed as a potential\nconﬂict of interest.\nReceived: 31 May 2013; paper pending\npublished: 08 July 2013; accepted: 20 July\n2013; published online: 20 August 2013.\nCitation: Zorzi M, Testolin A and\nStoianov IP (2013) Modeling language\nand cognition with deep unsupervised\nlearning: a tutorial overview. Front.\nPsychol. 4:515. doi: 10.3389/fpsyg.\n2013.00515\nCopyright © 2013 Zorzi, Testolin and\nStoianov. This is an open-access arti-\ncle distributed under the terms of the\nCreative Commons Attribution License\n(CC BY). The use, distribution or repro-\nduction in other forums is permitted,\nprovided the original author(s) or licen-\nsor are credited and that the original\npublication in this journal is cited, in\naccordance with accepted academic prac-\ntice. No use, distribution or reproduction\nis permitted which does not comply with\nthese terms.\nFrontiers in Psychology| Language Sciences August 2013 | Volume 4 | Article 515| 14\nThis article was submitted to Language\nSciences, a section of the journal\nFrontiers in Psychology."
}