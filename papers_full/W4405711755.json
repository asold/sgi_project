{
  "title": "Artificial Intelligence, the ChatGPT Large Language Model: Assessing the Accuracy of Responses to the Gynaecological Endoscopic Surgical Education and Assessment (GESEA) Level 1-2 knowledge tests",
  "url": "https://openalex.org/W4405711755",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2097539304",
      "name": "M Pavone",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1990726564",
      "name": "Palmieri L",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3155506719",
      "name": "N. Bizzarri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1986933856",
      "name": "A Rosati",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117189449",
      "name": "F Campolo",
      "affiliations": []
    },
    {
      "id": null,
      "name": "C Innocenzi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282117927",
      "name": "C. Taliento",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2986382017",
      "name": "S. Restaino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3205087723",
      "name": "U Catena",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2672648107",
      "name": "G Vizzielli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3025073500",
      "name": "C. Akladios",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3066363336",
      "name": "M M Ianieri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2327840128",
      "name": "J Marescaux",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A766094190",
      "name": "R Campo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103782184",
      "name": "F. Fanfani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144829215",
      "name": "G. Scambia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4367312135",
    "https://openalex.org/W4384821243",
    "https://openalex.org/W2475282058",
    "https://openalex.org/W2933767718",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4396509580",
    "https://openalex.org/W4367668444",
    "https://openalex.org/W4328049700",
    "https://openalex.org/W4388840552",
    "https://openalex.org/W4377940628",
    "https://openalex.org/W4394842371",
    "https://openalex.org/W4388975335",
    "https://openalex.org/W4377711218",
    "https://openalex.org/W4399304061",
    "https://openalex.org/W4372047097",
    "https://openalex.org/W4390113100",
    "https://openalex.org/W4393036579",
    "https://openalex.org/W4367175507",
    "https://openalex.org/W4376598383",
    "https://openalex.org/W4380685958",
    "https://openalex.org/W4364378939",
    "https://openalex.org/W4386776067",
    "https://openalex.org/W4387597924",
    "https://openalex.org/W4399244601"
  ],
  "abstract": "Background: In 2022, OpenAI launched ChatGPT 3.5, which is now widely used in medical education, training, and research. Despite its valuable use for the generation of information, concerns persist about its authenticity and accuracy. Its undisclosed information source and outdated dataset pose risks of misinformation. Although it is widely used, AI-generated text inaccuracies raise doubts about its reliability. The ethical use of such technologies is crucial to uphold scientific accuracy in research. Objective: This study aimed to assess the accuracy of ChatGPT in doing GESEA tests 1 and 2. Materials and Methods: The 100 multiple-choice theoretical questions from GESEA certifications 1 and 2 were presented to ChatGPT, requesting the selection of the correct answer along with an explanation. Expert gynaecologists evaluated and graded the explanations for accuracy. Main outcome measures: ChatGPT showed a 59% accuracy in responses, with 64% providing comprehensive explanations. It performed better in GESEA Level 1 (64% accuracy) than in GESEA Level 2 (54% accuracy) questions. Conclusions: ChatGPT is a versatile tool in medicine and research, offering knowledge, information, and promoting evidence-based practice. Despite its widespread use, its accuracy has not been validated yet. This study found a 59% correct response rate, highlighting the need for accuracy validation and ethical use considerations. Future research should investigate ChatGPTâ€™s truthfulness in subspecialty fields such as gynaecologic oncology and compare different versions of chatbot for continuous improvement. What is new? Artificial intelligence (AI) has a great potential in scientific research. However, the validity of outputs remains unverified. This study aims to evaluate the accuracy of responses generated by ChatGPT to enhance the critical use of this tool.",
  "full_text": null,
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.6932679414749146
    },
    {
      "name": "Subspecialty",
      "score": 0.6296807527542114
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.5099314451217651
    },
    {
      "name": "Certification",
      "score": 0.506981611251831
    },
    {
      "name": "Computer science",
      "score": 0.45979946851730347
    },
    {
      "name": "Psychology",
      "score": 0.40938013792037964
    },
    {
      "name": "Medical education",
      "score": 0.39064449071884155
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37234318256378174
    },
    {
      "name": "Data science",
      "score": 0.35837522149086
    },
    {
      "name": "Medicine",
      "score": 0.20153945684432983
    },
    {
      "name": "Political science",
      "score": 0.11421504616737366
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}