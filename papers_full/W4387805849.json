{
  "title": "A Siamese Network Based on Multiple Attention and Multilayer Transformers for Change Detection",
  "url": "https://openalex.org/W4387805849",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2131965790",
      "name": "Wenjie Tang",
      "affiliations": [
        "China University of Geosciences"
      ]
    },
    {
      "id": "https://openalex.org/A2096929046",
      "name": "Ke Wu",
      "affiliations": [
        "China University of Geosciences"
      ]
    },
    {
      "id": "https://openalex.org/A2101108701",
      "name": "Yuxiang Zhang",
      "affiliations": [
        "China University of Geosciences"
      ]
    },
    {
      "id": "https://openalex.org/A2556756242",
      "name": "Yanting Zhan",
      "affiliations": [
        "China University of Geosciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3194467570",
    "https://openalex.org/W4297200610",
    "https://openalex.org/W4224067506",
    "https://openalex.org/W4285285265",
    "https://openalex.org/W2625774291",
    "https://openalex.org/W3004423752",
    "https://openalex.org/W1979061792",
    "https://openalex.org/W2104374858",
    "https://openalex.org/W3205361185",
    "https://openalex.org/W2134969826",
    "https://openalex.org/W2018159308",
    "https://openalex.org/W3174635444",
    "https://openalex.org/W2810047517",
    "https://openalex.org/W4200430178",
    "https://openalex.org/W4293075303",
    "https://openalex.org/W2980094070",
    "https://openalex.org/W1827418370",
    "https://openalex.org/W2140815821",
    "https://openalex.org/W2153864221",
    "https://openalex.org/W2911545787",
    "https://openalex.org/W2762816376",
    "https://openalex.org/W2280565539",
    "https://openalex.org/W2131209948",
    "https://openalex.org/W2110842549",
    "https://openalex.org/W2153694513",
    "https://openalex.org/W4211155378",
    "https://openalex.org/W2519960185",
    "https://openalex.org/W4288901895",
    "https://openalex.org/W2587329506",
    "https://openalex.org/W2951472911",
    "https://openalex.org/W3130754787",
    "https://openalex.org/W3027225766",
    "https://openalex.org/W3009942016",
    "https://openalex.org/W2896092083",
    "https://openalex.org/W2891248708",
    "https://openalex.org/W3120467244",
    "https://openalex.org/W3036453075",
    "https://openalex.org/W3099503507",
    "https://openalex.org/W4290988741",
    "https://openalex.org/W3210281071",
    "https://openalex.org/W3157519352",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W4226228401",
    "https://openalex.org/W2751993439",
    "https://openalex.org/W4312549298",
    "https://openalex.org/W3015038817",
    "https://openalex.org/W3034218843",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3176330035",
    "https://openalex.org/W2908320224"
  ],
  "abstract": "Deep learning networks have demonstrate promising performance in high-resolution remote sensing images change detection (CD). The transformer can enhance the features and capture the global semantic relations, which has been used to solve the CD problem for high resolution remote sensing images with good results. However, the depth of the transformer is limited and the extracted features are not representative, which make the performance of the CD model unsatisfied. To fixed this problem, we propose a siamese network based on multiple attention and multilayer transformers (SMART) for CD in this paper. It is a siamese network containing three different modules, which can process bi-temporal images in parallel and extract enhanced features at different levels. The first is feature extraction module. It expresses the features as a certain number of high-order semantic features through the spatial attention module (SPAM), followed by the calculation of the semantic relations between these high-order semantic features using the transformer encoder, which greatly improves the computational efficiency. The second is feature enhancement module. It computes global semantic relations with self-attention module (SFAM). The multi-layer encoder gets the enhanced features at different levels by computing the relationship between features at each layer. The multi-layer decoder refines the bi-temporal features of each layer and projects them back to the original space. The third is fusion module. It uses the ensemble channel attention module (ECAM) to elaborate the feature differences at different levels. The proposed SMART model has been compared with some state of art CD methods in three publicly available data sets. The results confirm that SMART outperforms state of art change detection methods on several evaluation metrics. Our code is available at https://github.com/TwJ-IGG/SMART.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8011937737464905
    },
    {
      "name": "Transformer",
      "score": 0.6898998022079468
    },
    {
      "name": "Encoder",
      "score": 0.683444619178772
    },
    {
      "name": "Feature extraction",
      "score": 0.5756362080574036
    },
    {
      "name": "Artificial intelligence",
      "score": 0.504046618938446
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4010244607925415
    },
    {
      "name": "Data mining",
      "score": 0.38304436206817627
    },
    {
      "name": "Real-time computing",
      "score": 0.349152535200119
    },
    {
      "name": "Computer vision",
      "score": 0.32963448762893677
    },
    {
      "name": "Voltage",
      "score": 0.110250324010849
    },
    {
      "name": "Electrical engineering",
      "score": 0.08668360114097595
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3124059619",
      "name": "China University of Geosciences",
      "country": "CN"
    }
  ],
  "cited_by": 10
}