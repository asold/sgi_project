{
  "title": "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?",
  "url": "https://openalex.org/W4389523839",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097128869",
      "name": "Yang Chen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2488937960",
      "name": "Hexiang Hu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2113630028",
      "name": "Yi Luan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2413376441",
      "name": "Haitian Sun",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2203347202",
      "name": "Soravit Changpinyo",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2118575463",
      "name": "Alan Ritter",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2112928896",
      "name": "Ming‐Wei Chang",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W4280652569",
    "https://openalex.org/W4386065803",
    "https://openalex.org/W4320722432",
    "https://openalex.org/W4320561490",
    "https://openalex.org/W3172845486",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281944818",
    "https://openalex.org/W2963644680",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4287855128",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3009518609",
    "https://openalex.org/W4296406182",
    "https://openalex.org/W4385573705",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4385565405",
    "https://openalex.org/W4283208789",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2231285690",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4321649215",
    "https://openalex.org/W2797977484",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W4389519883",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4385570412",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W4287203292",
    "https://openalex.org/W4367367040",
    "https://openalex.org/W4378942562",
    "https://openalex.org/W3157008829",
    "https://openalex.org/W2964303913",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W4390872646",
    "https://openalex.org/W3035305184",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4224919571",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2966317026",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2963622213",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W4285255856",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W4312846625",
    "https://openalex.org/W12634471",
    "https://openalex.org/W4287827771",
    "https://openalex.org/W4226321975"
  ],
  "abstract": "Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, InstructBLIP) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during pre-training. Furthermore, we show that accurate visual entity recognition can be used to improve performance on InfoSeek by retrieving relevant documents, showing a significant space for improvement.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14948–14968\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCan Pre-trained Vision and Language Models Answer\nVisual Information-Seeking Questions?\nYang Chen♠♡∗ Hexiang Hu♠ Yi Luan♠ Haitian Sun♠ Soravit Changpinyo♣\nAlan Ritter♡ Ming-Wei Chang♠\n♠Google Deepmind ♣Google Research ♡Georgia Institute of Technology\nAbstract\nPre-trained vision and language models (Chen\net al., 2023b,a; Dai et al., 2023; Li et al.,\n2023b) have demonstrated state-of-the-art ca-\npabilities over existing tasks involving images\nand texts, including visual question answer-\ning. However, it remains unclear whether these\nmodels possess the capability to answer ques-\ntions that are not only querying visual con-\ntent but knowledge-intensive and information-\nseeking. In this study, we introduce INFO S-\nEEK 1, a visual question answering dataset tai-\nlored for information-seeking questions that\ncannot be answered with only common sense\nknowledge. Using INFO SEEK , we analyze\nvarious pre-trained visual question answering\nmodels and gain insights into their character-\nistics. Our findings reveal that state-of-the-art\npre-trained multi-modal models (e.g., PaLI-X,\nBLIP2, etc.) face challenges in answering vi-\nsual information-seeking questions, but fine-\ntuning on the INFO SEEK dataset elicits models\nto use fine-grained knowledge that was learned\nduring their pre-training. Furthermore, we\nshow that accurate visual entity recognition can\nbe used to improve performance on INFO SEEK\nby retrieving relevant documents, showing a\nsignificant space for improvement.\n1 Introduction\nThe acquisition of knowledge occurs in the pre-\ntraining of large language models (Brown et al.,\n2020; Chowdhery et al., 2022), demonstrated\nas their emergent ability to answer information-\nseeking questions in the open-world, where the\nquestioner does not have easy access to the infor-\nmation. While prior works have analyzed models’\ncapabilities to answer textual information-seeking\n(or info-seeking) questions, much less is known\nfor visual info-seeking questions. For example,\n∗ Work done when interned at Google\n1Our dataset is available at https://\nopen-vision-language.github.io/infoseek/.\nFigure 1: While 70.8% of OK-VQA questions can be\nanswered by average adults without using a search en-\ngine, INFO SEEK poses challenges to query fine-grained\ninformation about the visual entity (e.g., Dominus Flevit\nChurch), resulting in a sharp drop to 4.4% (§2).\nafter taking a picture of the specific church in Fig-\nure 1, a person might want to know the date of\nconstruction, or who decorated the interior of the\nchurch. Although the entity is presented in the im-\nage (the specific church), the relevant knowledge\n(e.g., the date) is not. Given recent advances on\npre-trained visual and language models (Alayrac\net al., 2022; Chen et al., 2023b; Li et al., 2023b),\ndo these models also understand how to answer\nvisual information-seeking questions?\nTo study this research question, a visual question\nanswering (VQA) dataset focusing on info-seeking\nquestions is inevitably required. However, not all\nVQA datasets meet this criterion. For example,\nby design, the majority of questions in datasets\nsuch as VQA v2 (Goyal et al., 2017) focus on vi-\nsual attributes and object detection that does not\nrequire information beyond the image to answer.\nWhile models capable of answering these types\nof questions have the potential to aid visually im-\npaired individuals (Gurari et al., 2018), there is a\nbroader class of info-seeking questions that can-\nnot be easily answered by sighted adults. Han-\ndling such questions (e.g., When was this building\nconstructed? 1955) is critical as they come closer to\nthe natural distribution of human questions.\nIn this paper, we present INFO SEEK , a natural\n14948\nVQA dataset that focuses on visual info-seeking\nquestions. Different from previous VQA datasets,\nthe testing subset of INFO SEEK is collected in\nmultiple stages from human annotators to evalu-\nate VQA where the question can not be answered\nfrom only the visual content (see a comparison of\ndatasets in § 2). In addition to this manually cu-\nrated test set, which enables realistic evaluation of\ninfo-seeking VQA, we also join annotations from\na recent visual entity recognition dataset (Hu et al.,\n2023) with the Wikidata database (Vrandeˇci´c and\nKrötzsch, 2014), and employ human annotators\nto write templates to semi-automatically generate\na large corpus of visual info-seeking QA pairs.\nOver 1 million {image, question, answer}triplets\nare generated to support fine-tuning multimodal\nmodels for info-seeking VQA. We split data to en-\nsure memorizing knowledge during fine-tuning is\nuseless — models either have to learn to use knowl-\nedge learned during pre-training or learn to retrieve\nknowledge from an external knowledge base.\nUsing INFO SEEK , we analyze the ability of state-\nof-the-art models to answer visual info-seeking\nquestions. We found pre-trained vision-language\nmodels, such as models pre-trained end-to-end (e.g.,\nPaLI-X by Chen et al.), and models pre-trained\nwith frozen LLM ( e.g., BLIP2 by Li et al.), both\nstruggle to answer info-seeking questions in zero-\nshot, though BLIP2 outperforms PaLI-X by a mar-\ngin. Surprisingly, after fine-tuning on our (large,\nsemi-automatically curated) training set, PaLI-X\nyields a significant improvement and outperforms\nthe fine-tuned BLIP2 models on queries that are\nunseen during fine-tuning. This suggests that while\npre-trained PaLI-X has a significant amount of\nknowledge, it requires a small amount of fine-\ntuning data to fully awaken its capabilities. Further-\nmore, we show that INFO SEEK fine-tuned models\ncan even generalize to questions and entity types\ncompletely unseen during fine-tuning ( e.g., art &\nfashion).\nWhen incorporating a visual entity recogni-\ntion component, and conditioning models on the\nWikipedia articles of the relevant entities, we\nshow that models accessing such a knowledge\nbase (With-KB) perform better overall than those\nthat rely on knowledge learned during pre-training.\nHowever, end-to-end (No-KB) models were found\nbetter on certain classes of questions that require\ncoarse-grained answers (“Which continent is this\nbuilding located on?”), even on tail entities. Our\nDataset OK-VQA ViQuAE INFO SEEK\nPaLM (Q-only) 23.8 31.5 5.6\nCurrent SotA 66.1 22.1 18.2\nRequire Knowledge† 29.2% 95.2% 95.6%\n† :% of questions that require knowledge to answer.\nPaLM (Q-only): a question-only baseline using PaLM.\nTable 1: Comparison of INFO SEEK and prior KI-VQA\nbenchmarks. Performances reported in VQA score.\nexperiment (§5.2) further suggests that improv-\ning visual entity recognition can drastically in-\ncrease model’s capability in answering visual info-\nseeking questions (from 18% to 45.6%), indicating\na promising direction for future development.\n2 The Need for a New Visual\nInformation-seeking Benchmark\nWhile there have been plenty of knowledge-\nintensive VQA (KI-VQA) benchmarks, we show\nthat none of these meet the criteria to effectively\nevaluate info-seeking VQA. Early efforts in this\narea, such as KBQA (Wang et al., 2015) and\nFVQA (Wang et al., 2017), were based on domain-\nspecific knowledge graphs, while recent datasets\nlike OK-VQA (Marino et al., 2019) and its vari-\nants such as S3VQA (Jain et al., 2021) and A-\nOKVQA (Schwenk et al., 2022) have improved\nupon this foundation by incorporating an open-\ndomain approach and highlighting common-sense\nknowledge. Among the existing benchmarks, K-\nVQA (Sanket Shah and Talukdar, 2019) and Vi-\nQuAE (Lerner et al., 2022) are the most relevant,\nbut they have severe limitations in their question\ngeneration process, as discussed below.\nInformation Seeking Intent. The evaluation of\nmodels’ ability to answer info-seeking questions\nrequires fine-grained knowledge, which a person\nis unlikely to know off the top of their head. How-\never, we found that 70.8% of OK-VQA ques-\ntions2 can be answered without the need to use\na search engine, indicating the dataset primarily\nfocuses on knowledge that is commonly known\nto people. Most OK-VQA questions are regard-\ning coarse-grained knowledge that many people\nalready know: What days might I most commonly\ngo to this building? Sunday . One only needs to\nknow the building type (e.g., Church) rather than the\nspecific building (e.g., Dominus Flevit Church). This\n2Studied with human on 500 random OK-VQA questions\n(see Appendix C.1)\n14949\nmakes it unsuitable for evaluating pre-trained mod-\nels on long-tailed knowledge, where these models\nhave shown weaknesses (Kandpal et al., 2022).\nReliance on Visual Understanding. In contrast\nto OK-VQA, the ViQuAE dataset aims to test fine-\ngrained knowledge of visual entities by pairing\nquestions from TriviaQA (Joshi et al., 2017) with\nimages. However, a significant portion of the Vi-\nQuAE questions ( e.g., \"Who betrayed him for 30\npieces of silver?\") can be answered without look-\ning at the images, as the questions often reveal suffi-\ncient information to determine the answer. To quan-\ntify this observation, we present questions from the\nevaluation set to a large language model, PaLM\n(540B) (Chowdhery et al., 2022). Results on the\nViQuAE test set are shown in Table 1. Surprisingly,\nwe find that PaLM can read questions and generate\nanswers with 31.5% accuracy, outperforming the\nSOTA retrieval-based model (Lerner et al., 2022)\n(which has access to the image) on this dataset by\n9.4%. Although PaLM is a much larger model, this\nexperiment illustrates that it is possible to achieve\nvery good performance on ViQuAE without using\ninformation from the image.\nEntity Coverage. Current VQA datasets often\ncover a limited number of visual entity categories.\nFor example, K-VQA only focuses on human sub-\njects, while over 43% of questions in ViQuAE re-\nvolve around human entities (see Table 2). Such\nlimitations hinder the evaluation of a model’s\nknowledge across various entity categories and may\nresult in reduced task complexity, as the evaluation\nmay be limited to mere facial recognition.\nTo address these limitations, we present INFO S-\nEEK (§ 3), a new benchmark for pre-trained multi-\nmodal models on visual info-seeking questions.\nOur work builds on top of a visual entity recogni-\ntion dataset, OVEN (Hu et al., 2023), which is de-\nsigned to answer questions related to the identifica-\ntion of visual entities. We take visual info-seeking\na step further by benchmarking info-seeking ques-\ntions about visual entities, which allows us to test\nthe pre-training knowledge of models beyond sim-\nply recognizing an entity.\n3 I NFO SEEK : A VQA Benchmark of\nVisual Information-seeking Questions\nThe INFO SEEK dataset consists of two compo-\nnents, (1) INFO SEEK Human: a collection of human-\nwritten visual info-seeking questions (8.9K) to sim-\nulate information seeking intent (see § 3.1); and (2)\nDataset # { I, Q, A} Len of Q/A # Entity # Ent. type\nOK-VQA 14K 8.1/1.3 - - ⋆\nK-VQA 183K 10.1/1.6 18,880 1 †\nViQuAE 3.6K 12.4/1.7 2,397 980\nINFO SEEK\n- Wikidata 1.35M 8.9/1.5 11,481 2,739\n- Human 8.9K 7.8/2.3 806 527\n⋆: OK-VQA does not specify visual entities.\n†: K-VQA only covers entities from the human category.\nTable 2: Statistics of INFO SEEK & KI-VQA datasets.\nINFO SEEK Wikidata: an automated dataset (1.3M)\nto cover diverse entities for large-scale training\nand evaluation purposes (see § 3.2). We split the\ndataset to ensure memorizing the training set is\nuseless, thereby emphasizing the importance of\npre-training to acquire knowledge (see § 3.3). Due\nto space limitations, we summarize the key essence\nin this section and defer details to the Appendix.\nImage Sources for Diverse Entity Coverage. We\nsourced images from 9 image classification and\nretrieval datasets used in Hu et al., including land-\nmarks (17%), animals (13%), food (5%), aircraft\n(3%), etc. We utilize their annotation, which links\nvisual entities to their corresponding Wikipedia ar-\nticles, to construct our INFO SEEK dataset.\n3.1 I NFO SEEK Human: Natural Info-Seeking\nVQA Data Annotated by Humans\nTo ensure INFO SEEK questions rely on visual un-\nderstanding and prevent models from taking short-\ncuts in the question without using the image, we\nemploy a two-stage annotation approach inspired\nby TyDiQA (Clark et al., 2020). This makes it\nunlikely questioners will have prior knowledge of\nthe answer like SQuAD (Rajpurkar et al., 2016),\nensuring questions with info-seeking intents (Lee\net al., 2019).\nQuestion Writing. Annotators are asked to write\n3-5 questions about a visual entity based on their\nown curiosity and information needs. To aid the\nquestion-writing process, they are prompted with\nvisual entity images, a short description (15 words)\nabout the entity, and a list of Wikipedia section\ntitles. This ensures that the questions reflect a gen-\nuine interest in learning about important aspects of\nthe entity without seeing the answer. A set of anno-\ntation rules is employed to prevent trivial questions,\nsuch as questions about visual attributes.\nAnswer Labeling. For each entity, we randomly\nassign collected questions to a different group of an-\n14950\n(a) Models with No KB access (b) Models With KB (Knowledge-base) information\nFigure 2: Visual info-seeking models under the proposed No KB and With KB protocols. (a) End-to-end VQA\nmodels (such as PaLI (Chen et al., 2023b) or BLIP2 (Li et al., 2023b)) that directly predict the answer from looking\nat the image and question; (b) Pipeline systems with access to a knowledge base (e.g.Wikipedia), with the option to\nlink the queried subject to the Wikipedia use CLIP (Radford et al., 2021) and perform textual question-answering\nusing PaLM (Chowdhery et al., 2022) or Fusion-in Decoder (FiD) (Izacard and Grave, 2020).\nnotators to label answers from Wikipedia. Annota-\ntors were shown the Wikipedia article of the entity\nand asked to find a concise answer to the ques-\ntion: a text span that is as short as possible while\nstill forming a satisfactory answer. In addition,\nannotators categorize questions into three types:\nTIME (e.g., year), NUMERICAL (e.g., height) and\nSTRING (e.g., location).\nFinally, we construct{image, question, answer}\n(IQA) triples by assigning images for the annotated\nQA pair of a visual entity, followed by human veri-\nfication and clarification of the questions if multi-\nple objects are presented in the image. Following\nTyDiQA (Clark et al., 2020), we measure the cor-\nrectness of annotations and take the high accuracy\n(95%) as evidence that the quality of the dataset is\nreliable for evaluating visual info-seeking models.\n3.2 I NFO SEEK Wikidata: 1 Million Automated\nVQA Data from Wikipedia\nHuman annotation is valuable but costly for large-\nscale evaluation. We thus scale up the dataset\nusing a semi-automated procedure, transforming\nknowledge triples in Wikidata (2022-10-03) to nat-\nural language questions with human-authored tem-\nplates, resulting in 1.3M examples over 11K visual\nentities covering 2.7K entity types (see Table 2).\nQA Generation. We convert knowledge triples\n(subj, relation, obj) in Wikidata to natural language\nquestion-answer pairs for a selected list of 300 re-\nlations. For each relation, annotators write one\nor two question templates, which contain a place-\nholder for a hypernym of the visual entity (e.g.,\ncar) and a placeholder for unit measurements (e.g.,\ninches) in numerical questions to avoid ambiguity.\nFinally, we construct the IQA triples by pairing\nimages of a visual entity with corresponding QA\npairs.3\nQA Pair Filtering and Subsampling. To ensure\nthe questions are diverse and the answers can be\nreferenced from Wikipedia, we filter out QA pairs\nwhen answers from Wikidata cannot be found in\nthe Wikipedia article and subsample questions to\nbalance the distribution of entities and relations.\n3.3 Evaluation of I NFO SEEK\nDataset Split. We design the evaluation split to\nprevent overfitting to the training set and focus\non evaluating the generalization ability of the pre-\ntrained models. This includes the ability to answer\nquestions of new entities and questions not seen\nduring training. Particularly, we define two evalua-\ntion splits: (1) UNSEEN ENTITY , where a portion\nof entities are held out during training and only in-\ncluded in the evaluation; (2) UNSEEN QUESTION ,\nwhere we hold out a portion of the QA pairs of seen\nentities for evaluation.\nEvaluation Metric. Three types of questions\nare evaluated differently: VQA accuracy (Goyal\net al., 2017) for STRING and TIME ; Relaxed Accu-\nracy (Methani et al., 2020) for NUMERICAL . We\napplied different relaxing strategies for each ques-\ntion type, and averaged the accuracy for each ques-\ntion. Finally, we calculate the accuracy for each\ndata split (UNSEEN QUESTION and UNSEEN EN-\nTITY ), and take the harmonic mean of them as the\noverall accuracy (see Appendix).\n4 Protocols and Models for I NFO SEEK\nMotivated by previous research on text-based ques-\ntion benchmarks (Joshi et al., 2017; Roberts et al.,\n2020), we introduce two evaluation protocols, i.e,\n3Based on manual inspection of 500 examples, we found\nthis process rarely produces incorrect examples (≤1.2%).\n14951\nEval Protocol Training/Validation Testing Methods Example Models Knowledge Base\nNo-KB {I, Q, A} { I, Q} End-to-end Model PaLI, BLIP2 -\nWith-KB {I, Q, A, E} { I, Q} Pipeline System CLIP →PaLM / FiD Wikipedia\nTable 3: Two evaluation protocols of INFO SEEK . The key difference is whether auxiliary data for visual entity\nrecognition and knowledge base is available at training. I: image, Q: question, A: answer, E: queried visual entity.\nNo KB and With KB, to evaluate models with dif-\nferent information accessible from INFO SEEK . Ta-\nble 3 and Figure 2 have provided a comparison for\nthe two setups. This key design choice is made\nto encourage models from different families to be\ncompared with a clear notion of what information\nwas accessed. We note that the No KB protocol is\nmore challenging than the With KB protocol.\nThe No-KB protocol. Models are tasked to di-\nrectly predict the answer by examining the image\nand question, similar to traditional VQA systems.\nThis requires the model to store world knowledge\nin its parameters for effective question answering.\nThe research question focuses on how much knowl-\nedge can an end-to-end model memorize in its pa-\nrameters during pre-training, and how well can it\nutilize this knowledge after fine-tuning? We use\nthe standard VQA formatted data, i.e, {Image (I),\nQuestion(Q), Answer(A)} triplets for training / val-\nidation, and {I, Q} for testing.\nThe With-KB protocol. The goal is to analyze\nheadroom for improvement when a viable reason-\ning chain is explicitly provided. Therefore, this\nprotocol encourages an extra step of visual entity\nrecognition, grounding the task on a knowledge\nbase. The VQA task is transformed into a two-step\npipeline, i.e, (1) visual entity recognition; and (2)\nlanguage QA with entity information. We provide\ntraining signals to first recognize the queried visual\nentity and then leverage the information to query a\nlarge language model for answers, or identify rel-\nevant Wikipedia articles for extracting the answer.\nSpecifically, we provide a 100K Wikipedia KB\n(articles and infobox images) that includes visual\nentities from INFO SEEK and top frequent entities\nfrom Wikipedia. During training and validation,\nWith KB protocol provides entity labels for each\nqueried visual entity. During testing, the model is\nevaluated based on the {I, Q} pairs only.\n4.1 Models without KB Information\nRandom & Prior. Random answers sampled from\nthe training set; The majority answer based on the\nquestion prior, which is calculated using the train-\ning set questions grouped by question 4-gram.\nPaLM (Q-only) Model. To validate the impor-\ntance of visual content in INFO SEEK , we build a\nquestion-only baseline with PaLM (540B) (Chowd-\nhery et al., 2022), using text question as the only\ninput and with 5-shot in-context-learning.\nBLIP2 & InstructBLIP. We utilize two pre-\ntrained vision-language models, i.e, BLIP2 (Li\net al., 2023b) and InstructBLIP (Dai et al.,\n2023). Both models share the same archi-\ntecture, which trains a Q-former Transformer\nthat connects a frozen vision encoder ( ViT-g/14)\nto a frozen instruction-tuned language model\n(Flan-T5XXL (Chung et al., 2022)) to output text based\non an input image and text. Particularly, Instruct-\nBLIP fine-tunes the BLIP2 model on 26 vision-\nlanguage datasets ( e.g., VQAv2, OKVQA) with a text\ninstruction prefix, and claimed to show improved\nzero-shot performance on unseen vision-language\ntasks. Following Li et al. (2023b), we fine-tune\nthe Q-former of both models using the INFO SEEK\nWikidata, for improved performance.\nPaLI-17B & PaLI-X. We experiment with two\nextra pre-trained vision-language models from the\nPaLI (Chen et al., 2023b,a) family given its SOTA\nperformance. Particularly, we use PaLI-17B (ViT-e\n+ mT5XXL (Xue et al., 2020)) and PaLI-X (ViT-\n22B (Dehghani et al., 2023) + UL2-33B (Tay et al.,\n2022)), which are pre-trained on WebLI (Chen\net al., 2023b) with 1 billion image-text pairs. Both\nmodels, which use non instruction-tuned language\nmodels, exhibit minimal zero-shot performance on\nINFO SEEK . Consequently, we fine-tune both mod-\nels on the INFO SEEK Wikidata to improve their per-\nformance.\n4.2 Models with KB Information\nIn this protocol, we explicitly model the path to\nanswer info-seeking questions with two decou-\npled sub-tasks: (1) recognizing the visual entity\ngrounded to the KB and (2) textual reasoning to\nanswer the question. A hidden benefit of such\npipeline systems is improved interpretability, be-\ncause it is easier to locate the source of errors by\n14952\nModel LLM # Params\nINFO SEEK Wikidata INFO SEEK Human OK-VQA VQAv2\nUNSEEN UNSEEN Overall UNSEEN UNSEEN Overall Accuracy AccuracyQUESTION ENTITY QUESTION ENTITY\nRandom - - 0.1 0.1 0.1 0.2 0.1 0.1 - -\nPrior - - 3.9 2.7 3.2 0.3 0.3 0.3 - 32.1 †\nPaLM (Q-only) PaLM 540B 5.1 3.7 4.3 4.8 6.6 5.6 23.8 43.0\nBLIP2 Flan-T5 XXL 12B 14.5 13.3 13.9 10.0 8.9 9.4 54.7 82.3\nInstructBLIP Flan-T5 XXL 12B 14.3 13.2 13.7 10.6 9.3 9.9 55.5 -\nPaLI-17B mT5 XXL 17B 20.7 16.0 18.1 13.3 5.9 8.2 64.8 84.6\nPaLI-X UL2 32B 55B 23.5 20.8 22.1 12.9 9.3 10.8 66.1 86.1\n†: Numbers adopted from Agrawal et al.\nTable 4: Results of No-KB models fine-tuned on INFO SEEK . Baselines including Random, Prior (majority answer\nwith 4-gram question prior), and a question-only model using PaLM (Q-only) with 5-shot prompting. VQA accuracy\nof models on OK-VQA (Marino et al., 2019) and VQAv2 (Goyal et al., 2017) are for comparison.\ndiagnosing each sub-task component.\nSub-task #1: Visual Entity Recognition. We\nfollow the entity recognition task defined in\nOVEN (Hu et al., 2023), and use an image and\na text query (e.g., “ What is this building? \") as\nmodel inputs, and predict entities among 100K\nmulti-modal Wikipedia entries. Particularly, we\nemploy the pre-trained CLIP (Radford et al., 2021)\nmodel (ViT-L/14), as our visual entity recognition\nmodel, because of its strong generalization capabil-\nity. Specifically, we follow the CLIP2CLIP model\ndescribed in Hu et al., to fine-tune CLIP to en-\ncode multi-modal representations (image, question)\nfrom our dataset as query, and ( Wikipedia image,\nWikipedia title) from the KB as candidates. We\nthen retrieve the topk=5 most similar entities based\non weighted cosine similarity scores computed be-\ntween the query and candidates.\nSub-task #2: Language QA with LLM or KB\nReader. Through visual entity recognition, we can\nnow represent the queried visual information as its\ntextual description. This enables us to investigate\nthe language reasoning component independently\nto understand how much improvement a strong\nLLM or a KB reader can bring.\n• PaLM: Large Language Model. We use PaLM\n(540B) to investigate the amount of knowledge\nthat can be memorized in the model’s parame-\nters from pre-training on text corpora. Given a\nquestion and the queried entity name (from en-\ntity recognition), we prompt PaLM to predict the\nanswer using 5-shot in-context examples with\nthe prompt format: “question: This is {entity}\n{question} answer:”.\n• Fusion-in Decoder (FiD): KB Reader. We\nexperiment with a SOTA retrieval-augmented\nmodel, which reads information from a KB, to\nunderstand the value of Wikipedia articles in the\nKB. Specifically, the FiD (Izacard and Grave,\n2020) model is employed, which takes N=100 re-\ntrieved articles as input and generates an answer.\nThe model is pre-trained with a T5 Large (Raffel\net al., 2020) backbone (660M) on Natural Ques-\ntions (Kwiatkowski et al., 2019) and fine-tuned\non INFO SEEK . During inference, we retrieve the\nfirst 20 passages from Wikipedia for k=5 visual\nentities (from entity recognition) and feed 100\npassages to FiD to generate the answer.\n5 Experiments\n5.1 Results for No-KB Models\nMain results. Table 4 presents the results of end-\nto-end models on INFO SEEK . The best pre-trained\nmodel in this setting is PaLI-X, although the abso-\nlute number on the model’s overall performance\nremains low. This is partially due to the fact that\nINFO SEEK questions often require identifying enti-\nties and retrieving specific information relevant to\nthe question, making it a challenging task for end-\nto-end models. As PaLI-X is pre-trained on a large\ncorpus with more model parameters, it demon-\nstrates better generalization ability on the UNSEEN\nENTITY split compared to PaLI-17B. Meanwhile,\nthere remains a noticeable gap in performance\non the UNSEEN QUESTION and UNSEEN ENTITY\nsplits, indicating that models struggle with gen-\neralization to new visual entities from the train-\ning set. We also present models’ results on OK-\nVQA (Marino et al., 2019) and VQAv2 (Goyal\net al., 2017) for comparison and observe a dras-\ntic performance gap, emphasizing the difficulty of\nvisual info-seeking questions.\n14953\nPaLI-17BPaLI-XBLIP2\nInstructBLIP\nPaLI-17BPaLI-XBLIP2\nInstructBLIP\n0\n10\n20\nINFO SEEK Wikidata INFO SEEK Human\nAccuracy %\nZero-shot\nFine-tuned\nFigure 3: Zero-shot & fine-tuned performances on\nINFO SEEK . Fine-tuning on INFO SEEK elicits knowl-\nedge from PaLI models to answer fine-grained visual\ninfo-seeking questions.\nFine-tuning elicits knowledge from the model.\nTo demonstrate the value of INFO SEEK training\ndata, we report the zero-shot performance of mod-\nels in Figure 3. Specifically, we find that with-\nout fine-tuning, both PaLI models produce a neg-\nligible overall performance, which is significantly\nworse than the fine-tuned counterpart. This pro-\nvides evidence to support the hypothesis that fine-\ntuning has helped elicit knowledge from the pre-\ntrained PaLI models. On the other hand, BLIP2\nand InstructBLIP show compelling zero-shot per-\nformance on INFO SEEK as they adopt a frozen\ninstruction fine-tuned LLM ( i.e, Flan-T5) and In-\nstructBLIP is further instruction-tuned on a col-\nlection of VQA benchmarks. The performance of\nBLIP2 models is further improved after fine-tuning\non INFO SEEK with a small number of steps, show-\ning strong generalization results to the Human split.\nIn Figure 10, we present examples of BLIP2 pre-\ndicting the “country location” of an unseen entity\n(i.eAmberd) and show the accuracy was improved\nfrom 18% to 92% after fine-tuning, despite not see-\ning this entity in the training set. Finally, we con-\nducted a real-world evaluation on out-of-domain\nimages unavailable from the Internet (not from any\nmodels’ pre-training data). Particularly, we evalu-\nate fine-tuned PaLI with 90 questions on 30 images\ncaptured by the authors, on visual entities outside\nof the INFO SEEK training corpus. As a result, PaLI-\n17B and PaLI-X answered 22.2% and 38.9% of\nquestions correctly. Figure 4 presents examples of\nPaLI and BLIP2 predictions on two out-of-domain\nentities (artwork and fashion product).\nWhy does instruction-tuned BLIP2 obtain worse\nzero-shot INFO SEEK results? One surpris-\ning finding from Figure 3 caught our attention\nand reveals an important criterion to be consid-\nQ: what year was this painting\ncreated?\nPaLI-17B: 1884✓\nPaLI-X: 1884✓\nBLIP2: 1887✗\nQ: which year was this brand\nestablished?\nPaLI-17B: 1915✗\nPaLI-X: 1854✓\nBLIP2: 1854✓\nFigure 4: Predictions on out-of-domain visual entities\n(art & fashion) collected from real-world images by\nauthors, using INFO SEEK fine-tuned models.\nQ: Which body of water is this\nmountain located in or next to?\nA: Lake Como\nBLIP2(0-shot): lake como\nInstructBLIP(0-shot): lake\nQ: Who designed this bridge?\nA: Thomas Telford\nBLIP2(0-shot): john nash\nInstructBLIP(0-shot): architect\nFigure 5: InstructBLIP(0-shot) makes less fine-\ngrained predictions compared to its initial model\n(BLIP2), after instruction-tuned on prior VQA datasets.\nered for future model development. We found\nInstructBLIP0-shot performs significantly worse\nthan its initial checkpoint, BLIP2 ( 7.4 vs 11.3\non InfoSeek Wikidata), which contradicts the superior\nzero-shot performances of InstructBLIP in Dai et al.\n(2023). We conduct manual analysis and detect a\ncommon error made by InstructBLIP is its pref-\nerence for generating coarse-grained predictions\ncompared to BLIP2 (e.g., architect vs a person’s\nname). This leads to a performance drop on IN-\nFOSEEK , which emphasizes fine-grained answers\n(see Figure 5). We hypothesize that this can be\nattributed to the instruction tuning datasets used for\nInstructBLIP (e.g., VQAv2 and OK-VQA), which share\na less fine-grained answer distribution. Fortunately,\nfine-tuning on INFO SEEK Wikidata helps close the\ngap.\n5.2 Results for With-KB Models\nModels with KB access perform better. Table 5\npresents the results for pipeline models with ac-\ncess to knowledge base (KB) information, along\nwith the best results from the No-KB setting for\nreference. Notably, the pipeline models outper-\nform the best No-KB models on the challenging\nINFO SEEK Human split significantly. This highlights\nthe pipeline systems’ ability to answer visual info-\n14954\nModel INFO SEEK INFO SEEK ENTITY\nWikidata Human Accuracy\nBest No-KB 22.1 10.8 -\nWith-KB Setting\nCLIP →PaLM 20.1 15.2 22.2CLIP →FID 19.3 18.2\nOracle →FID 52.0 45.6 100\nTable 5: Results of With-KB setting. CLIP →\nPaLM/FID: a two-stage pipeline system (visual entity\nrecognition →text QA). PaLM: 5-shot prompting. FID:\nFusion-in Decoder to read from KB using T5large. Ora-\ncle: an artificial upper-bound using oracle entities.\nModel TIME NUMERICAL STRING\n(Acc.) (Relaxed Acc.) (Acc.)\nNo-KB Setting\nPrior 0 4.4 5.0\nPaLM (Q-only) 0 11.4 4.0\nInstructBLIP 7.9 7.5 17.8\nBLIP2 6.9 5.8 18.5\nPaLI-17B 3.8 18.4 27.4\nPaLI-X 7.7 16.1 30.0\nWith-KB Setting\nCLIP →PaLM 12.5 27.7 21.7\nCLIP →FiD 12.3 23.4 23.9\nTable 6: Results w.r.t. each question types on the\nINFO SEEK Wikidata val set of unseen question split, show-\ning a big headroom for improvements on TIME and\nNUMERICAL for all end-to-end models.\nseeking questions by effectively utilizing visual\nrecognition and language reasoning, specifically\nusing the names of visual entities to convey infor-\nmation across modalities. When comparing the\ntwo Language QA models, we observe that the FiD\nmodel, which reads the Wikipedia article, achieves\nthe highest generalization performance on INFO S-\nEEK Human by a significant margin. This suggests\nthat access to relevant text content plays a crucial\nrole in answering visual info-seeking questions.\nLarge headroom for improvement. Table 5\ndemonstrates an artificial upper-bound (Oracle →\nFiD) on INFO SEEK , indicating substantial room for\nperformance improvement if an oracle entity recog-\nnition model were available. By simulating the\nvisual entity recognition’s accuracy improvement\n(from 22% using CLIP to 100%), the INFO SEEK accu-\nracy can be improved from∼20% to ∼50%, within\nthe same FiD model.\nAnalysis on each question type. Table 6 shows\na breakdown of results under different question\ntypes, evaluated on INFO SEEK Wikidata. Comparing\nNo KB and With KB models, we found that end-\n0 20 40 60 80\nWhat is the mountain\nrange..?\nWhich continent..?\nWhich country..?\nWhich brand..?\nAccuracy %\nWith-KB\nNo-KB\nFigure 6: No-KB ( PaLI-17B) outperforms With-KB\n(CLIP→FiD) models on questions that query less fine-\ngrained attributes.\nto-end models such as PaLI, have a short barrel\non fine-grained knowledge-intensive questions (i.e,\nTIME and NUMERICAL ). It can perform well on\nother questions, which are more about querying at-\ntributes or resolving relations between entities (see\nFigure 6). Comparing With KB models, PaLM\nand FiD perform on par with each other on this au-\ntomated evaluation data. However, when evaluated\non the natural info-seeking human queries, FiD\nhas a better generalization, outperforming PaLM\non TIME (21.5 vs 14.6) and NUMERICAL (25.6\nvs 21.3) questions from INFO SEEK Human signif-\nicantly. One possible reason is that natural info-\nseeking questions written by people focus more\non very fine-grained information, which is rare\nand hard to memorize for PaLM. In contrast, FiD\ncan leverage Wikipedia articles to predict answers.\nFinally, we analyze the performance of different\nmodels according to the visual entity popularity\nand found unique advantages of end-to-end models\n(see Appendix).\nPerformance on Head vs. Tail entities. Al-\nthough pipeline models with KB access are overall\nstronger, surprisingly, we observe that end-to-end\nmodels have a unique advantage for info-seeking\nVQA, particularly on the tail entities. Figure 7\npresents a comparison of models, with group-wise\nperformances on Wikipedia entities that are least\npopular (less monthly page views) to most popu-\nlar (more monthly page views). The histogram is\ngenerated based on the average monthly Wikipedia\npageviews in 2022, following (Mallen et al., 2022).\nSurprisingly, the results show that PaLI-17B out-\nperforms the pipeline systems by a large margin\non the tail entities, particularly for questions re-\nlated to geographical information. We show some\nqualitative examples in Figure 9, for entities from\nbaskets of different monthly page views. This sug-\ngests that there are many different routes to answer\n14955\n100 500 1K 2.5K 5K 10K 25K 50K100K 1M\n0\n10\n20\n30\n40\nAverage Monthly View of Entity\nAccuracy %\nPaLI-17B CLIP→PaLM CLIP→FiD\nFigure 7: INFO SEEK results w.r.t. visual entities\nof different popularity. End-to-end model outper-\nforms pipeline systems on tail entities (low monthly\npageviews) but overturned on more popular entities\n(high monthly pageviews).\nvisual info-seeking questions and that pipeline sys-\ntems that rely on an explicit decomposition of the\nVQA task may be redundant and susceptible to\nerror propagation from the entity linking stage.\nWhereas for end-to-end models such as PaLI, it\nis flexible to decide which route of reasoning is\nmore appropriate to answer a given question. For\nexample, one can answer geographical questions\nwithout knowing the identity of the visual entity,\nif other relevant visual clues are presented. Mean-\nwhile, on the more popular head visual entities, a\nclear trend emerged showing that pipeline systems\noutperform end-to-end PaLI by a big margin.\n6 Related Work\nPre-trained Vision Language Models. There\nhas been significant growth in the development of\nvision-language models pre-trained on large-scale\nimage-text datasets (Lu et al., 2022; Bao et al.,\n2021; Wang et al., 2022; Zhou et al., 2020; Radford\net al., 2021). One line of research aims to augment\na pre-trained language model with visual modality\nby learning a mapping from an external visual en-\ncoder to the frozen large language model (Alayrac\net al., 2022; Li et al., 2023b; Koh et al., 2023), to\nfully leverage textual knowledge from the language\nmodel (Xu et al., 2023; Dai et al., 2023; Liu et al.,\n2023; Zhu et al., 2023; Ye et al., 2023).\nKnowledge-based VQA Models. Various\napproaches have been proposed to address\nknowledge-based VQA tasks (Marino et al., 2019)\nby incorporating external knowledge into vision-\nlanguage models. One approach is to retrieve infor-\nmation from an external KB (Marino et al., 2021;\nHu et al., 2022b; Wu and Mooney, 2022) and em-\nploy a model (Izacard and Grave, 2020) to per-\nform language QA (Gui et al., 2022; Lin et al.,\n2022). Other approaches transform the image into\na text caption and use an LLM (Brown et al., 2020;\nChowdhery et al., 2022) to answer questions (Yang\net al., 2022; Hu et al., 2022a). We utilize both\napproaches to study the ceiling for improvement\non INFO SEEK with the OVEN model (Hu et al.,\n2023).\nAnother concurrent work (Mensink et al., 2023)\ninvestigates similar challenges but emphasizes scal-\nability and relies on model-generated annotations,\nas opposed to our human-annotated info-seeking\nqueries.\n7 Conclusion\nWe introduced INFO SEEK , a large-scale VQA\ndataset that focuses on answering visual informa-\ntion seeking questions. With INFO SEEK , we found\nthat current state-of-the-art pre-trained visual-\nlanguage models struggle to answer visual info-\nseeking questions requiring fine-grained knowl-\nedge, such as questions about time and numerical\ninformation of a visual entity. Our analysis using\npipeline systems, which ground visual entities to\nan external knowledge base, suggests that incorpo-\nrating fine-grained knowledge into the pre-training\nprocess holds significant potential to improve end-\nto-end pre-training models.\n8 Limitation\nINFO SEEK is limited to English language and fu-\nture research could expand it to a multilingual set-\nting, leveraging articles in Wikipedia supported\nin other languages. While the primary focus of\nthis work is on knowledge derived from Wikipedia,\nfuture investigations could explore extensions to\nother domains, such as medical information, and\nartwork, and incorporate emerging updates in\nWikipedia (Iv et al., 2022).\nAcknowledgement\nWe thank Jialin Wu, Luowei Zhou for reviewing\nan early version of this paper. We thank Xi Chen\nfor providing different variants of PaLI pre-trained\ncheckpoints. We also thank Radu Soricut, Anelia\nAngelova, Fei Sha, Andre Araujo, Vittorio Ferrari,\nWei Xu, Kartik Goyal for valuable discussions and\nfeedback on the project. Yang Chen is partially\nfunded by the NSF (IIS-2052498).\n14956\nReferences\nAishwarya Agrawal, Dhruv Batra, Devi Parikh, and\nAniruddha Kembhavi. 2018. Don’t just assume; look\nand answer: Overcoming priors for visual question\nanswering. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages\n4971–4980.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. Conference on Neural Informa-\ntion Processing Systems.\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei.\n2021. Beit: Bert pre-training of image transformers.\nInternational Conference on Learning Representa-\ntions.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\n2014. Food-101–mining discriminative components\nwith random forests. In European Conference on\nComputer Vision. Springer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Conference on Neural Information Process-\ning Systems.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil\nMustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang,\nYi Tay, et al. 2023a. Pali-x: On scaling up a multi-\nlingual vision and language model. arXiv preprint\narXiv:2305.18565.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Good-\nman, Adam Grycner, Basil Mustafa, Lucas Beyer,\net al. 2023b. PaLi: A jointly-scaled multilingual\nlanguage-image model. International Conference on\nLearning Representations.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nJonathan H Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in ty-\npologically diverse languages. Transactions of the\nAssociation for Computational Linguistics.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint\narXiv:2305.06500.\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim\nAlabdulmohsin, et al. 2023. Scaling vision trans-\nformers to 22 billion parameters. arXiv preprint\narXiv:2302.05442.\nGerry. 2021. 100 sports image classification.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the V in VQA\nmatter: Elevating the role of image understanding in\nVisual Question Answering. In Computer Vision and\nPattern Recognition.\nLiangke Gui, Borui Wang, Qiuyuan Huang, Alex Haupt-\nmann, Yonatan Bisk, and Jianfeng Gao. 2022. KAT:\nA knowledge augmented transformer for vision-and-\nlanguage. Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies.\nDanna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo,\nChi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P.\nBigham. 2018. Vizwiz Grand Challenge: Answering\nvisual questions from blind people. In Computer\nVision and Pattern Recognition.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nHexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-\nwal, Mandar Joshi, Kenton Lee, Kristina Toutanova,\nand Ming-Wei Chang. 2023. Open-domain visual\nentity recognition: Towards recognizing millions of\nwikipedia entities. arXiv preprint arXiv:2302.11154.\nYushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi,\nNoah A Smith, and Jiebo Luo. 2022a. Promptcap:\nPrompt-guided task-aware image captioning. arXiv\npreprint arXiv:2211.09699.\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-\nWei Chang, Yizhou Sun, Cordelia Schmid, David A\nRoss, and Alireza Fathi. 2022b. Reveal: Retrieval-\naugmented visual-language pre-training with multi-\nsource multimodal knowledge memory. arXiv\npreprint arXiv:2212.05221.\nRobert Iv, Alexandre Passos, Sameer Singh, and Ming-\nWei Chang. 2022. FRUIT: Faithfully reflecting up-\ndated information in text. In Proceedings of the\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Seattle, United States. Asso-\nciation for Computational Linguistics.\n14957\nGautier Izacard and Edouard Grave. 2020. Leveraging\npassage retrieval with generative models for open\ndomain question answering. Proceedings of the Eu-\nropean Chapter of the Association for Computational\nLinguistics.\nAman Jain, Mayank Kothyari, Vishwajeet Kumar,\nPreethi Jyothi, Ganesh Ramakrishnan, and Soumen\nChakrabarti. 2021. Select, substitute, search: A new\nbenchmark for knowledge-augmented visual ques-\ntion answering. In Proceedings of the 44th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 2491–\n2498.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Annual Meeting of the Association for Com-\nputational Linguistics.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodels struggle to learn long-tail knowledge. arXiv\npreprint arXiv:2211.08411.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\n2023. Grounding language models to images for\nmultimodal generation. International Conference on\nMachine Learning.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-\nFei. 2013. 3d object representations for fine-grained\ncategorization. In Proceedings of the IEEE interna-\ntional conference on computer vision workshops.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Annual Meeting of\nthe Association for Computational Linguistics.\nPaul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé\nLe Borgne, Romaric Besançon, Jose G. Moreno, and\nJesús Lovón Melgarejo. 2022. Viquae, a dataset for\nknowledge-based visual question answering about\nnamed entities. In Proceedings of the 45th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, SIGIR ’22.\nDongxu Li, Junnan Li, Hung Le, Guangsen Wang, Sil-\nvio Savarese, and Steven C.H. Hoi. 2023a. LA VIS:\nA one-stop library for language-vision intelligence.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n3: System Demonstrations). Association for Compu-\ntational Linguistics.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023b. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. International Conference on Machine\nLearning.\nYuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu,\nChenguang Zhu, and Lu Yuan. 2022. REVIVE: Re-\ngional visual representation matters in knowledge-\nbased visual question answering. Conference on\nNeural Information Processing Systems.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. 2022. Unified-\nio: A unified model for vision, language, and multi-\nmodal tasks. arXiv preprint arXiv:2206.08916.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi. 2013. Fine-grained\nvisual classification of aircraft. Technical report.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Inves-\ntigating effectiveness and limitations of paramet-\nric and non-parametric memories. arXiv preprint\narXiv:2212.10511.\nKenneth Marino, Xinlei Chen, Devi Parikh, Abhinav\nGupta, and Marcus Rohrbach. 2021. Krisp: Inte-\ngrating implicit and symbolic knowledge for open-\ndomain knowledge-based vqa. In Computer Vision\nand Pattern Recognition.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. OK-VQA: A visual\nquestion answering benchmark requiring external\nknowledge. In Computer Vision and Pattern Recog-\nnition.\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. 2022. ChartQA: A benchmark\nfor question answering about charts with visual and\nlogical reasoning. Annual Meeting of the Association\nfor Computational Linguistics (Findings).\nThomas Mensink, Jasper Uijlings, Lluis Castrejon,\nArushi Goel, Felipe Cadar, Howard Zhou, Fei Sha,\nAndré Araujo, and Vittorio Ferrari. 2023. Encyclo-\npedic vqa: Visual questions about detailed prop-\nerties of fine-grained categories. arXiv preprint\narXiv:2306.09224.\nNitesh Methani, Pritha Ganguly, Mitesh M Khapra, and\nPratyush Kumar. 2020. PlotQA: Reasoning over sci-\nentific plots. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision,\npages 1527–1536.\nMaria-Elena Nilsback and Andrew Zisserman. 2008.\nAutomated flower classification over a large num-\nber of classes. In 2008 Sixth Indian Conference on\nComputer Vision, Graphics & Image Processing.\n14958\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International Con-\nference on Machine Learning.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, W. Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. JMLR.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions\nfor machine comprehension of text. Proceedings of\nEmperical Methods in Natural Language Processing.\nTal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi\nZelnik-Manor. 2021. Imagenet-21k pretraining for\nthe masses. arXiv preprint arXiv:2104.10972.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? CoRR, abs/2002.08910.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, An-\ndrej Karpathy, Aditya Khosla, Michael Bernstein,\net al. 2015. Imagenet large scale visual recognition\nchallenge. International journal of computer vision,\n115:211–252.\nNaganand Yadati Sanket Shah, Anand Mishra and\nPartha Pratim Talukdar. 2019. KVQA: Knowledge-\naware visual question answering.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi.\n2022. A-okvqa: A benchmark for visual question\nanswering using world knowledge. arXiv preprint\narXiv:2206.01718.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022. Unify-\ning language learning paradigms. arXiv preprint\narXiv:2205.05131.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin\nCui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. 2018. The inaturalist\nspecies classification and detection dataset. In Com-\nputer Vision and Pattern Recognition.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM.\nPeng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and\nAnton Van Den Hengel. 2017. FVQA: Fact-based\nvisual question answering. IEEE transactions on\npattern analysis and machine intelligence.\nPeng Wang, Qi Wu, Chunhua Shen, Anton van den Hen-\ngel, and Anthony Dick. 2015. Explicit knowledge-\nbased reasoning for visual question answering. arXiv\npreprint arXiv:1511.02570.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. 2022. Unifying architectures,\ntasks, and modalities through a simple sequence-to-\nsequence learning framework. International Confer-\nence on Machine Learning.\nTobias Weyand, Andre Araujo, Bingyi Cao, and Jack\nSim. 2020. Google landmarks dataset v2-a large-\nscale benchmark for instance-level recognition and\nretrieval. In Computer Vision and Pattern Recogni-\ntion.\nJialin Wu and Raymond J Mooney. 2022. Entity-\nfocused dense passage retrieval for outside-\nknowledge visual question answering. Proceedings\nof Emperical Methods in Natural Language Process-\ning.\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude\nOliva, and Antonio Torralba. 2010. Sun database:\nLarge-scale scene recognition from abbey to zoo. In\nComputer Vision and Pattern Recognition.\nZhiyang Xu, Ying Shen, and Lifu Huang. 2023. Multi-\ninstruct: Improving multi-modal zero-shot learning\nvia instruction tuning. Annual Meeting of the Associ-\nation for Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mT5: A massively multilingual\npre-trained text-to-text transformer. Proceedings of\nthe Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of GPT-3 for few-shot knowledge-\nbased VQA. In Proceedings of the AAAI Conference\non Artificial Intelligence.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,\nMing Yan, Yiyang Zhou, Junyang Wang, An-\nwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.\nmplug-owl: Modularization empowers large lan-\nguage models with multimodality. arXiv preprint\narXiv:2304.14178.\nVictor Zhong, Weijia Shi, Wen-tau Yih, and Luke Zettle-\nmoyer. 2022. RoMQA: A benchmark for robust,\nmulti-evidence, multi-answer question answering.\narXiv preprint arXiv:2210.14353.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu,\nJason Corso, and Jianfeng Gao. 2020. Unified vision-\nlanguage pre-training for image captioning and VQA.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence.\n14959\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\n14960\n#UNSEEN #Total Question Type #Entity\nQUESTION /ENTITY TIME /NUM./S TR.\nTrain - / - 934,048 4.4/20.4/ 75.2% 5,549\nVal 18,656/54,964 73,620 4.6/ 21.6/ 73.8% 1,794\nTest 98,901/249,079 347,980 4.8/22.9/72.3% 8,905\nHuman 3,248/5,683 8,931 26.8/ 26.4/46.8% 806\nTable 7: INFO SEEK Dataset statistics. Average ques-\ntion per image rate is 1.4 and 1.0 for Wikidata and\nHuman split, respectively.\nA Details of the Dataset.\nIn this section, we provide more details of the hu-\nman annotation quality control and automatic data\ngeneration process. We summarize the statistics\nof INFO SEEK in Table 7 and show question prefix\ndistribution in Figure 11 and entity distribution in\nFigure 12.\nA.1 Human Annotation Quality Control\nInstruction and Training. We hire 30 full-time\nin-house annotators to collect questions and an-\nswers in INFO SEEK Human. Annotators are native\nEnglish speakers in the U.S. and are aware of the\npurpose of the collected data. To ensure the qual-\nity of annotations in the INFO SEEK Human dataset,\na comprehensive training process was designed\nand implemented for our annotators. This process\ninvolved a pilot study, in which annotators read\nthe instructions and annotated a few sample ex-\namples, followed by a tutorial session and a quiz.\nThe tutorial was conducted through an online video\nsession and provided a comprehensive overview\nof the instructions while addressing common mis-\ntakes identified in the pilot study. Only annotators\nwho passed the quiz were selected to work on the\nmain task, with 30 annotators completing the train-\ning. We hire annotators at $17.8 per hour, which\nis higher than the minimum wage in the U.S., to\nfairly compensate annotators for their time and ef-\nfort. The average completion time for stages one\nand two of the annotation task was 12 and 10 min-\nutes, respectively. A screenshot of the annotation\ninterface is provided in Figure 13.\nAnnotation Procedure\nStage 1 (Question Writing): As shown in Figure 13\n(Top), annotators are shown with images of a\nvisual entity on the left-hand side with a short\ndescription of the entity from Wikipedia below.\nOn the right, we show a list of Wikipedia section\ntitles of the entity and ask annotators to write\nrelevant questions next to the section title. We\nprevent annotators from asking binary questions,\nCorrect Incorrect\nPercentage 95% 5%\nTable 8: Expert judgments of answer accuracy based on\na sample of 200 examples from INFO SEEK Human.\nasking visual attributes (such as color), writing\nquestions by rephrasing the description, copying\nentity names and section titles into the question,\nand avoiding writing ambiguous questions.\nStage 2 (Answer Labeling) : As shown at\nthe bottom of Figure 13, annotators are present\nwith info-seeking questions to the entity collected\nfrom Stage 1 and a Wikipedia link of the entity.\nFor each question, annotators are asked to find a\nshort span of answers (less than 10 words) from\nthe Wikipedia page. They are asked to answer\ntwo questions: (1) “Can you derive the answer\nfrom the given Wikipedia page?” and (2) “What\nis the type of this question?” and select from\nthree options (TIME , NUMERICAL , OTHERS ). For\neach answer, they will then fill in the answer box\n(TIME : [year, month, day] , NUMERICAL : [min,\nmax, unit] , OTHERS : [string]) and copy paste\na short sentence from Wikipedia that contains\nthe answer to the evidence section. We decided\nto exclude questions without answer spans from\nWikipedia following TyDiQA-GoldP as the dataset\nis already hard enough and reserve these questions\nfor future work.\nExpert Feedback and Correction. Expert annota-\ntors provided regular feedback during annotation\nand conducted thorough post-annotation verifica-\ntion. The data was split into three batches, with\nannotators flagged and provided feedback for those\nwho consistently made similar mistakes. After the\ncompletion of stage 1, questions that revealed the\nentity name, asked about the color or shape of\nan object, or were binary were automatically re-\njected. After stage 2, three expert annotators re-\nviewed and processed the question-answer pairs,\nremoving unqualified pairs and verifying the an-\nswer span from the annotated evidence sentence.\nRejected pairs may have included questions that\nwere not answered by the annotated answer or were\ntoo general and resulted in an ambiguous answer.\nThe expert annotators also corrected the question\ntype annotation and edited the answer span into\nthe correct format, such as adding units for numeri-\ncal questions or shortening long answer spans that\nexceeded ten tokens. Finally, the expert annota-\n14961\nFigure 8: Random examples from the training set of INFO SEEK Wikidata.\n14962\nFigure 9: Examples of predictions of PaLI-17B and CLIP→FID on INFO SEEK (left to right shows tail to head\nentities).\nFigure 10: Examples of predictions of BLIP2(0-shot) and BLIP2 on INFO SEEK (Entity: Q457057). Fine-tuning\nimproves the accuracy from 2/11 to 10/11, despite it being an UNSEEN ENTITY (not in the training set). We show\ntraining set entities that are located in Armenia and images of Amberd on the internet.\nFigure 11: Question prefix distribution in INFO SEEK Wikidata (left) and INFO SEEK Human (right).\n14963\nlandmarks\n17.4%\nlandforms\n15%\nanimal\n13.1%\nitem\n10.3%\nplant\n7.49% bird\n7.09%\nfood\n5.29%\nother\n4.9%\nvehicle\n4.6%\nperson\n4.3%\naircraft\n2.8%\nsport2.5%\nmaterial\n2.4%\ninsect\n1.8%\ndrink\n1.1%\nFigure 12: Distribution of the entities in INFO SEEK\n(Grouped by their super category).\ntors reviewed the image-question-answer triples\nto reject bad images or clarify the question when\nmultiple objects were present in the image. For\nexample, a building was specified when multiple\nbuildings were present in the image. On average, it\ntook 1.5 hours to verify 1000 triples, as the majority\nof images contained a single object.\nFollowing TyDiQA (Clark et al., 2020), we ana-\nlyze the degree to which the annotations are correct\ninstead of the inter-annotator agreement since the\nquestion may have multiple correct answers. In\nTable 8, human experts carefully judged a sam-\nple of 200 examples from INFO SEEK Human split.\nFor each example, the expert reads through the\nWikipedia page of the queried visual entity and\nfinds the answer to the question. They then indi-\ncate whether the annotated answer is correct. We\ntake the high accuracy (95%) as evidence that the\nquality of the dataset offers a valuable and reliable\nsignal for evaluating visual info-seeking models.\nA.2 Filtering and Subsampling\nFiltering. To test the models’ ability to answer\nvisual information-seeking questions that require\nfine-grained knowledge, which can be learned from\nthe pre-training corpus such as Wikipedia, we need\nto verify the consistency of answers between Wiki-\ndata and Wikipedia. Given that Wikidata and\nWikipedia are crowd-sourced independently, some\nQA pairs created from Wikidata may not be present\nin the Wikipedia article or may have different an-\nswers. Therefore, we filtered out QA pairs where\nthe answer could not be found in the Wikipedia\narticle of the entity. We performed an exact string\nmatch to verify answers for string questions and\nused fuzzy string matching 4 with a substring ra-\ntio greater than 0.9 if an exact match could not be\nfound. For time questions, we applied an exact\nmatch to verify the year, month, and date. In some\ncases, the year of construction of a building varied\nby a year, so we allowed a +/- 1 year deviation\nfor the time question. For numerical questions, we\nused exact matching to verify the numbers in the\narticle. However, in many cases, the units were\ndifferent (meters or inches), or a range with a min-\nimum and maximum was given. We used regular\nexpressions to extract the number or range from\nthe Wikipedia article and filter out the QA pairs\nif it is counted as incorrect based on the “Relaxed\naccuracy” in Section 3.3 in the main text. Based\non a manual analysis of 200 randomly sampled QA\npairs, we found that 97% of the answers of the IN-\nFOSEEK Wikidata could be found in the Wikipedia\narticle.\nSubsampling Questions. In order to achieve a\nmore diverse set of questions in INFO SEEK , we ap-\nplied a subsampling method to address the skewed\ndistribution of crowd-sourced knowledge triples\nin Wikidata. The method followed the approach\nused in Zhong et al. (2022). This involved defin-\ning P(r, c) as the percentage of triples that con-\ntain the relation r and the subject entity’s cate-\ngory as c. The P′(r, c) = 1 /|(r, c)|was cal-\nculated as the average probability of a relation-\ncategory pair and Image-Q-A triples were removed\nwith increasing likelihood based on the probability\nr = 1−min(1, P(r, c)′/P(r, c))1/2. Additionally,\nthe same subsampling method was applied to bal-\nance the answer distribution for each relation. This\nresulted in the question prior baseline achieving a\nrelatively low score (3.2) in INFO SEEK Wikidata, as\nshown in Table 4 in the main text.\nA.3 Evaluation Metric.\nThere are three types of questions, i.e, STRING ,\nTIME , and NUMERICAL , which are evaluated\ndifferently. Particularly, we adopt the VQA\naccuracy (Goyal et al., 2017; Marino et al.,\n2019) against multiple references for STRING\nand TIME questions, and utilize Relaxed Accu-\nracy (Methani et al., 2020; Masry et al., 2022) for\nNUMERICAL questions. For STRING questions,\nwe use the alias of answers from Wikidata\nas multiple references for INFO SEEK Wikidata\n(#avg = 4.5), and the human-annotated multiple\n4SequenceMatcher from difflib library\n14964\nFigure 13: Annotation Interface for Stage 1 (Top) and Stage 2 (Bottom).\nreferences for INFO SEEK Human (#avg = 2.4).\nExact Match: Correct if the prediction matches any one of the\nreferences exactly.\n•prediction=“USA\", references=[“USA\", “U.S.\", “United\nStates of America\", ...] →✓\nFor TIME questions, the answer references ac-\ncount for different date formats of year/month/day.\nMeanwhile, we perform a relaxed match (with a\none-year error tolerance) to measure the model’s\nprediction, because it is quite often that historical\nevents are only associated with estimated time.\nExact Match: Correct if the prediction matches any one of the\nreferences exactly.\n•prediction=“1991\", references=[“1990\", “1991\", “1992\"])\n→✓\n•prediction=“1991 6 11\", references=[“1991 6 11\", “1991\nJune 11\", “11 June 1991\", ...] →✓\nFor NUMERICAL questions, the exact match\nwould not be able to handle the case where a range\n(e.g., a pair of minimum and maximum values) is\nprovided as annotated ground truth. To account\nfor this, we make a slight modification to the\nRelaxed Accuracy with a 10% tolerance range.\n14965\nRelaxed Accuracy: correct if the prediction is in the reference\nrange or the prediction range overlaps with the reference\nrange of more than 50%\n1) ref_min ≤pred ≤ref_max;\n2) IoU([pred_min, pred_max], [ref_min, ref_max]) ≥50%\n•reference= 10 cm →reference= [9, 11] # 10% tolerance\n•prediction= 10, reference= [9, 11] →✓\n•prediction= [5, 6], reference= [9, 11] →×\nA single value prediction is counted as correct\nif it falls within the answer range, and a range\nprediction is correct if the intersection-of-union\nbetween the prediction and answer is greater than\nor equal to 50%. Finally, we calculate the accuracy\nfor each data split ( UNSEEN QUESTION and\nUNSEEN ENTITY ), and take the harmonic mean of\nthem as the overall accuracy.\nA.4 Image Sources.\nImage Recognition (or Retrieval) Datasets:\nImageNet21k-P (Russakovsky et al., 2015;\nRidnik et al., 2021), iNaturalist2017 (Van Horn\net al., 2018), Cars196 (Krause et al., 2013),\nSUN397 (Xiao et al., 2010), Food101 (Bossard\net al., 2014), Sports100 (Gerry, 2021), Air-\ncraft (Maji et al., 2013), Oxford Flower (Nilsback\nand Zisserman, 2008), Google Landmarks\nv2 (Weyand et al., 2020).\nB Implementation details of the baseline\nsystems\nIn this section, we provide complete implemen-\ntation details of baseline models for the INFO S-\nEEK task. We summarize hyperparameters for fine-\ntuning in Table 9.\nB.1 without-KB Models\nPaLI and PaLI-X. We fine-tuned a 17B\nPaLI (Chen et al., 2023b) and 55B PaLI-X (Chen\net al., 2023a) on INFO SEEK training set using the\n“answer in en: [question] <extra_id_0>” prompt.\nBLIP2 and InstructBLIP. We fine-tuned a\nBLIP2 (Li et al., 2023b) and InstructBLIP (Dai\net al., 2023) on INFO SEEK training set using the\n“Question: [question] Short answer:” prompt with\nthe LAVIS library (Li et al., 2023a). The length\npenalty is set to -1. Since BLIP2 models present\nzero-shot capabilities on INFO SEEK , we employ\nearly stopping to prevent over-fitting on the train-\ning set based on the performance on the validation\nset.\nOFA. We fine-tuned the OFAlarge (Lu et al., 2022)\nmodel for 20k steps. During inference, we apply\nbeam search decoding with a beam size set to 5.\nOFA achieves 11.7 and 4.0 onINFO SEEK Wikidata\nand Human split, respectively.\nmPLUG-owl. We fine-tuned mPLUG-owl (Ye\net al., 2023) for 10k steps with a learning rate of\n2e-4 and batch size of 1 using LoRA (Hu et al.,\n2021). mPLUG-owl achieves 7.7 on INFO SEEK\nHuman split.\nPaLM(Q-Only). We use PaLM 540B (Chowdhery\net al., 2022) in-context learning under the 5-shot\nsetting with the following prompt:\nPlease answer the following question.\nquestion: {Question_1}. answer: { Answer_1}.\n...\nquestion: {Question_i}. answer:\nB.2 With-KB Models\nPaLI PaLI-X (Instruct)BLIP2 OFA FID\nOptimizer Adafactor Adafactor Adam Adam Adafactor\nBatch size 128 128 16 512 64\nTrain steps 10k 800 400 20k 200\nLR 1e-4 1e-4 5e-5 5e-5 2e-4\nLR scheduler linear decay constant constant polynomial decay constant\nWarmup steps 1000 1000 - 1000 -\nImage size 224 224 224 480 -\nBeam size 5 5 5 5 5\nVision backbone ViT-e ViT-22B ViT-g ResNet152 -\nLM backbone mT5 XXL UL2-32B Flan-T5 XXL BARTlarge T5large\n#Params 17B 55B 12.1B 0.4B 0.4B\nComputing 32 TPU v4 64 TPUv4 A40 8 A100 64 TPU v4\nTime 6 hours 1 hour 1 hour 48 hours 1 hour\nTable 9: Hyperparameters for fine-tuning models on\nINFO SEEK .\nPaLM. We use PaLM 540B (Chowdhery et al.,\n2022) in-context learning under the 5-shot setting\nwith the prompt present below. The Entity_1 is\nthe gold entity provided in the training set (with\nKB setting). The Entity_i is the top-1 prediction\nfrom the entity linking stage of the queried image.\nPlease answer the following question.\nquestion: {This is Entity_1. Question_1}. answer: {\nAnswer_1}.\n...\nquestion: {This is Entity_i. Question_i}. answer: {\nFID. The T5large FID (Izacard and Grave, 2020)\nmodel was fine-tuned in two stages using 100\npassages with a maximum input length of 192\ntokens. To form synthetic training data with\n(passage, question, answer) triples, we combine\noracle entity passage with linked entity (from\nEntLinker) passages. We fine-tune the model on\nNatural Questions (Kwiatkowski et al., 2019) for\n10k steps and then continue to fine-tune it on\nINFO SEEK for 200 steps with a batch size of 64.\nquestion: This is Entity. Question. context: Passage\n14966\nC Additional Experiment Results\nComplete numbers for With-KB Models. We\nshow the complete results for With-KB models in\nTable 10.\nComplete numbers for INFO SEEK Wikidata Val-\nidation set. We show the complete Validation re-\nsults for Without-KB and With-KB models in Ta-\nble 11 and question type score of unseen entity split\nin Table 12.\nC.1 OK-VQA Annotation Guidelines\nFive adult annotators each annotate 100 examples\n(500 in total) sampled from the OK-VQA training\nset. Annotators are instructed to categorize each\nexample into one of three categories (see Table 13).\n14967\nModel # Params Components use KB\nINFO SEEK Wikidata INFO SEEK Human\nUNSEEN UNSEEN Overall UNSEEN UNSEEN OverallQUESTION ENTITY QUESTION ENTITY\nCLIP →PaLM 540B CLIP 21.9 18.6 20.1 15.6 14.9 15.2\nCLIP →FiD 1B CLIP & FiD 20.7 18.1 19.3 18.9 17.6 18.2\nTable 10: INFO SEEK full results on With-KB setting.\nModel\nINFO SEEK Wikidata\nUNSEEN UNSEEN OverallQUESTION ENTITY\nWithout-KB Setting\nPrior 4.6 2.5 3.2\nPaLM (Q-only) 5.5 4.2 4.8\nInstructBLIP 15.0 14.0 14.5\nBLIP2 15.0 14.2 14.6\nPaLI-17B 24.2 16.7 19.7\nPaLI-X 25.8 22.4 24.0\nWith-KB Setting\nCLIP →PaLM 22.7 18.5 20.4\nCLIP →FiD 23.3 19.1 20.9\nOracle →FiD 52.1 53.0 52.5\nTable 11: INFO SEEK full results on Wikidatavalidation\nset.\nModel TIME NUMERICAL STRING\n(Acc.) (Relaxed Acc.) (Acc.)\nNo-KB Setting\nPrior 0 3.5 2.3\nPaLM (Q-only) 4.6 11.0 2.7\nInstructBLIP 6.6 8.2 16.1\nBLIP2 5.6 6.0 17.0\nPaLI-17B 1.0 14.8 18.2\nPaLI-X 8.1 17.2 24.8\nWith-KB Setting\nCLIP →PaLM 17.8 21.3 17.7\nCLIP →FiD 13.8 15.2 20.5\nTable 12: Results w.r.t. each question types on the\nINFO SEEK Wikidata val set of unseen entity split.\nQuestion Category Percentage\nAnswered directly by looking at the corresponding image 50.8%\nAnswered without looking at the image (Q-only) 20%\nRequiring a Google search for an answer 29.2%\nTable 13: OK-VQA annotation results.\n14968",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8469305038452148
    },
    {
      "name": "Computer science",
      "score": 0.8017362356185913
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5430570840835571
    },
    {
      "name": "Natural language processing",
      "score": 0.49771884083747864
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.4524851441383362
    },
    {
      "name": "Information retrieval",
      "score": 0.43900102376937866
    },
    {
      "name": "Visualization",
      "score": 0.4366234540939331
    },
    {
      "name": "Modal",
      "score": 0.42532363533973694
    },
    {
      "name": "Information seeking",
      "score": 0.41775715351104736
    },
    {
      "name": "Machine learning",
      "score": 0.368083119392395
    },
    {
      "name": "Linguistics",
      "score": 0.09798941016197205
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 23
}