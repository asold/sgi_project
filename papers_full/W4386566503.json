{
  "title": "Embedding Recycling for Language Models",
  "url": "https://openalex.org/W4386566503",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3197355071",
      "name": "Jon Saad-Falcon",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2105349780",
      "name": "Amanpreet Singh",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A297574754",
      "name": "Luca Soldaini",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4377737167",
      "name": "Mike D’Arcy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983754593",
      "name": "Arman Cohan",
      "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2098223845",
      "name": "Doug Downey",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W4285204619",
    "https://openalex.org/W4297730150",
    "https://openalex.org/W4287816361",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W4294732851",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288026527",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2279376656",
    "https://openalex.org/W3161820423",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W4225909425",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3105216938",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2108278040",
    "https://openalex.org/W2962815673",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3176930520",
    "https://openalex.org/W3035324702",
    "https://openalex.org/W4287333395",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2626445779",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2981943644",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W4285107336",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W3034561418",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4283157527",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3169545043",
    "https://openalex.org/W4294103325",
    "https://openalex.org/W2951672049"
  ],
  "abstract": "Real-world applications of neural language models often involve running many different models over the same corpus. The high computational cost of these runs has led to interest in techniques that can reuse the contextualized embeddings produced in previous runs to speed training and inference of future ones. We refer to this approach as embedding recycling (ER). While multiple ER techniques have been proposed, their practical effectiveness is still unknown because existing evaluations consider very few models and do not adequately account for overhead costs. We perform an extensive evaluation of ER across eight different models (17 to 900 million parameters) and fourteen tasks in English. We show how a simple ER technique that caches activations from an intermediate layer of a pretrained model, and learns task-specific adapters on the later layers, is broadly effective. For the best-performing baseline in our experiments (DeBERTa-v2 XL), adding a precomputed cache results in a 90% speedup during training and 87-91% speedup for inference, with negligible impact on accuracy. Our analysis reveals important areas of future work.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1933–1953\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nEmbedding Recycling for Language Models\nJon Saad-Falcon1 Amanpreet Singh1 Luca Soldaini1\nMike D’Arcy2 Arman Cohan1,3 Doug Downey1,2\n1Allen Institute for Artificial Intelligence (AI2)\n2 Northwestern University\n3 Yale University\n{jons, amanpreets, lucas, armanc, dougd}@allenai.org,\nm.m.darcy@u.northwestern.edu\nAbstract\nReal-world applications of neural language\nmodels often involve running many different\nmodels over the same corpus. The resulting\nhigh computational cost has led to interest in\ntechniques that can reuse the contextualized em-\nbeddings produced in previous runs to speed\ntraining and inference of future ones. We re-\nfer to this approach as embedding recycling\n(ER). While multiple ER techniques have been\nproposed, their practical effectiveness is still\nunknown because existing evaluations consider\nvery few models and do not adequately account\nfor overhead costs. We perform an extensive\nevaluation of ER across eight different models\n(17 to 900 million parameters) and fourteen\ntasks in English. We show how a simple ER\ntechnique that caches activations from an in-\ntermediate layer of a pretrained model, and\nlearns task-specific adapters on the later layers,\nis broadly effective. For the best-performing\nbaseline in our experiments (DeBERTa-v2 XL),\nadding a precomputed cache results in a >90%\nspeedup during training and 87-91% speedup\nfor inference, with negligible impact on accu-\nracy. Our analysis reveals important areas of\nfuture work, and we release code and documen-\ntation for our experiments at https://github.\ncom/allenai/embeddingrecycling.\n1 Introduction\nLarge pretrained language models form the foun-\ndation of modern NLP, and continue to push the\nstate-of-the-art on a wide range of natural language\nprocessing tasks (Devlin et al., 2019; Liu et al.,\n2019b; Bommasani et al., 2021). Larger models\ntend to offer superior accuracies (Kaplan et al.,\n2020), but also entail higher computational costs.\nThe steep computational cost associated with large\nneural language models slows down experimenta-\ntion, increases financial barriers to the technology,\nand contributes to global climate change (Strubell\net al., 2019; Dodge et al., 2022).\nOur work studies how to reduce computational\ncost for workloads in which many distinct models\nare run over the same text. For example, a scholarly\nsearch tool that helps users find and understand rel-\nevant literature may run separate models for entity\nrecognition, topic classification, relation extraction,\nsummarization, question answering, and so on over\na large corpus of papers. New and improved mod-\nels for the tasks are developed frequently, necessi-\ntating additional runs. The need for repeated model\nruns has also been noted for other applications in\nprevious work, including news applications (Du\net al., 2020) and virtual assistants (Wei et al., 2022).\nFurther, repeated runs also occur very frequently\nduring model development, when exploring model\nvariants or executing multiple training epochs.\nRecent work has introduced ways to reduce com-\nputational cost in such settings by re-using model\nactivations from one task to speed up other ones\n(Du et al., 2020; Wei et al., 2022). A pretrained\nlanguage model’s internal activations form a con-\ntextualized embedding, which reflects syntactic and\nsemantic knowledge about the input text (Goldberg,\n2019; Wiedemann et al., 2019; Rogers et al., 2020)\nwhich can be useful across a variety of downstream\ntasks. We define embedding recycling (ER) as the\ntechnique of caching certain activations from a pre-\nvious model run, and re-using them to improve the\nefficiency of future training and inference. Recy-\ncling imposes a small computation time cost the\nfirst time a model processes a text, in order to com-\npute and populate the cache. Thereafter, all sub-\nsequent runs on the text can use the precomputed\ncache, improving efficiency.\nWhile previous work has shown the promise of\nER approaches, the existing evaluations are lim-\nited. For example, Du et al. (2020) and Wei et al.\n(2022) each evaluate ER for only one or two base\nmodels. Likewise, for ER techniques that cache\nactivations on persistent storage, the storage and\ntime cost of the cache itself has yet to be quan-\n1933\ntified. In this paper, we present a more compre-\nhensive evaluation of ER with several models and\ntasks, along with a thorough efficiency analysis.\nWe study a simple layer-recycling ER method that\ncaches the activations from an intermediate layer\nof a pretrained model, and uses those cached acti-\nvations as the starting point when the same input\nsequence is seen again during fine-tuning or infer-\nence. We show that even this simple method yields\nsubstantial improvements to throughput at small or\nno cost to accuracy on average. Our results pro-\nvide the strongest evidence to date that ER can be a\npractically important technique for reducing costs\nfor NLP systems, but as we discuss in section 6,\nthey also suggest important challenges that must\nbe addressed in future work.\nOur contributions are summarized below:\n• We propose embedding recycling as a method\nfor lowering the computational costs of train-\ning and inference for language models, and\nexplore layer recycling with two techniques:\nstandard fine-tuning and parameter-efficient\nadapters.\n• Our experiments with eight models across a\nwide range of tasks show that layer recycling\nis generally effective. For the best-performing\nER model on our tasks- DeBERTa-XL with\nadapters, we find that layer recycling nearly\nmatches performance of the original model\nwhile providing a 87-91% speedup at infer-\nence time, and greater than 90% speedup at\ntraining time.\n• We explore open challenges for embedding re-\ncycling and present questions for future work.\n2 Related Work\nThe embedding recycling technique we investigate\nis based on findings from prior work suggesting\nthat not all layers of a pretrained transformer are\nequally important for end-task finetuning. Shal-\nlower layers tend to converge earlier in training\nthan deeper layers (Raghu et al., 2017; Morcos\net al., 2018), and weights of later layers change\nmore than earlier ones (Kovaleva et al., 2019), sug-\ngesting that earlier layers tend to extract universal\nfeatures whereas later layers focus on task-specific\nmodeling. Lee et al. (2019) find that 90% of fully\nfine-tuned performance can be reached when fine-\ntuning only the final quarter of a transformer’s lay-\ners and leaving the rest frozen.\nSeveral proposed methods vary the number of\nfrozen layers over the course of training, approach-\ning or exceeding the performance of fully fine-\ntuned models while substantially speeding up the\ntraining process (Raghu et al., 2017; Xiao et al.,\n2019; Brock et al., 2017). Similar to our approach,\nsome dynamic freezing methods also employed\ncaching mechanisms (Liu et al., 2021; He et al.,\n2021), but the dynamic number of frozen layers\nmeans the cache applies only at training time and\nonly for a single task. In contrast, we cache embed-\ndings from the pretrained model, which can then\nbe reused across multiple downstream tasks and\napplied at inference time as well.\nOther recent studies have sought to improve\nmodel inference speed by skipping computations\nin later layers. Sajjad et al. (2020) found that in\nsome cases up to half of the layers can be removed\nfrom the model with only a 1-3% drop in task per-\nformance. Early exit strategies have also been pro-\nposed, which allow the model to dynamically de-\ncide when to skip later layers (Cambazoglu et al.,\n2010; Xin et al., 2020). SkipBERT (Wang et al.,\n2022) combined early exiting with an approach\nin which cached n-gram embeddings approximate\nthe intermediate activations of new inputs. Lester\net al. (2021) explored prompt-tuning as a parameter-\nefficient approach for adapting frozen language\nmodels without adjusting model weights, condition-\ning language models with soft prompts to perform\ndownstream tasks.\nPrecomputing text representations to speed up\nfuture processing on the same data is commonly\ndone when creating fixed-size document-level em-\nbeddings for use on document-level tasks (Conneau\net al., 2017; Cohan et al., 2020); in contrast, we\nstudy contextualized token-level embeddings that\ncan be used for tasks such as named entity recog-\nnition (NER) and question answering. ReadOnce\nTransformers (Lin et al., 2021) do consider multi-\ntask variable-length document representations, but\ndo so in a setting where a cached document rep-\nresentation is paired with a query text (such as a\nquestion or prompt); the approach is pretrained\nwith QA data and evaluated on QA and summariza-\ntion, rather than tasks such as text classification or\nNER where the entire input can be cached.\nDu et al. (2020) propose an approach similar to\nours that caches general-purpose token-level model\nrepresentations, trained in a multi-task setting; how-\never, that approach only applies a small MLP to\n1934\nthe stored representations and reports a meaning-\nful drop in accuracy (greater than 2% on average)\ncompared to fully fine-tuned models. We find that\nreusing the later layer parameters of a pretrained\ntransformer in addition to the cached activations of-\nten enables us to essentially match fully fine-tuned\nmodel accuracy while reducing computational cost.\nWei et al. (2022) combine layer freezing and\nknowledge distillation to create a multi-task model.\nThey do not consider caching activations on persis-\ntent storage as we do, but instead re-use activations\nacross tasks at inference time via a branching multi-\ntask model. They use a two stage process where\n12−N layers are fine-tuned for each individual task\nkeeping N frozen layers. This is followed by dis-\ntillation of the N layers for further computational\ngains. We take advantage of the parameter efficient\nadapter modules (Houlsby et al., 2019), and re-\nplace this process with a single step of fine-tuning\na frozen base model that has adapters attached only\nto the deeper layers.\nOur work also has connections to work on\nmemory- and retrieval-augmented language model-\ning. Prior work on using memory (e.g., Grave et al.\n(2016); Dai et al. (2019); Rae and Razavi (2020);\nWu et al. (2022)) generally focuses on modeling\nlong-range context and caching representations of\nolder history in a sequence, while work on retrieval\n(e.g., Guu et al. (2020); Karpukhin et al. (2020))\nfocuses on fetching text from a knowledge base\nor corpus to serve as additional context. In both\ncases, the aim is to use representations of addi-\ntional text (from earlier in a document or from a\nknowledge base) to improve modeling of new in-\nputs. In contrast, our work focuses on caching the\nrepresentations of an entire sequence to speed up\ncomputation for new tasks.\n3 Methods\nIn the transformer architecture (Vaswani et al.,\n2017), an input sequence x of length S and dimen-\nsion d is transformed with a function F : RS×d →\nRS×d defined by the composition ofN transformer\nlayers F(1), ..., F(N) as follows:\nFℓ(x) =LN(FFℓ( x′) +x′) (1)\nx’ = LN\n(\nMHℓ(x) +x\n)\n(2)\nwhere LN is a layer normalization (Ba et al., 2016),\nFF is a feed forward network, and MH is the self-\nattention layer that consists of multiple heads and\nFigure 1: Overview of the embedding recycling ap-\nproach. In the figure, the K-th layer activations are\nsaved for future fine-tuning on downstream tasks, skip-\nping redundant computations of earlier layers in the\ntransformer model.\ncontextualizes the input sequence vector. The out-\nput of each layer is used as input to the next layer.\nhℓ+1 = Fℓ(hℓ) (3)\nOur approach is to cache the output representa-\ntions hk ∈RS×d at a certain layerk and reuse them\nfor fine-tuning on a new given task. We refer to this\nprocess of caching and reusing the output represen-\ntations of a layer aslayer recycling. This enables us\nto reduce the size of the transformer model fromN\nlayers to N −k layers, reducing the computational\ncost during fine-tuning and inference.\nNote that the key requirement of layer recycling\nis that we first need to process the entire data with\nthe transformer model and cache the representa-\ntions, so that we could later reuse these representa-\ntions many times during fine-tuning and inference\non new tasks. We experiment with two types of\nlayer recycling approaches as explained next.\nWe start with a pretrained transformer F (e.g.,\nBERT) consisting of F(1), ..., F(k), ..., F(N) lay-\ners. During the first epoch of fine-tuning for any\ngiven task, we run the transformer over a corpus\nCand cache the output representations of layer k\nfor each instance c in C, i.e., hk\nc∈C. However, for\n1935\nevery subsequent epoch of fine-tuning using the\nsame transformer model, we only run and fine-tune\nthe latter N −k layers F(k+1), ..., F(N). We can ei-\nther train all of the weights in the layers (which we\nrefer to as reduced models), or only train adapter\nmodules added on the layers (discussed below). In\neither case, for the instance c in the dataset Cwe\nsimply retrieve and use the previously cached repre-\nsentation hk\nc∈Cas input to layerF(k+1). This avoids\nthe extra computation through layers F(1), ..., F(k)\nbut adds a small cost for retrieving the representa-\ntion from storage (see subsection 5.4 for efficiency\nanalysis).\n3.1 Adapters\nWe evaluate whether combining recycling with\nAdapter modules (Houlsby et al., 2019) can im-\nprove performance over fully fine-tuned models.\nAdapters are typically used to improve the parame-\nter efficiency of fine-tuning and mitigate the storage\ncosts of large language models. They also enable\nmore sample-efficient fine-tuning and can result\nin improved fine-tuning performance (Karimi Ma-\nhabadi et al., 2022).\nAdapter modules contain a down-projection, an\nup-projection, and a residual connection module:\nh ←h + (f(hWdown)Wup). The adapters are\nseparately inserted after the MH and the FF layers\nin the transformer architecture (Equation 2). Fur-\nther, Rücklé et al. (2021) experiment with dropping\nadapters from the lower transformer layers to pro-\nvide inference time speedup. In our experiments,\nadapters are added to the latter half of transformer\nlayers in the reduced transformer models. As in\nstandard layer recycling, the pretrained original\ntransformer F first caches the intermediate activa-\ntions hk\nc∈C for each input in a selected corpus at\nlayer k. Then the first k layers are removed from\nthe transformer. During fine-tuning, the cached rep-\nresentations are fed as input to the later N −k lay-\ners of the transformer, which consist of the frozen\ntransformer layers plus trainable adapter parame-\nters. Thus, we fine-tune only the additional 6-8%\nparameters introduced by the adapters. We refer to\nlearning adapters on all layers as the full adapter\nsetting and the layer recycling version as the re-\nduced adapter setting.\n4 Experimental Setup\nWe now present our experiments evaluating\nwhether recycled embeddings can be paired with\nreduced large language models to maintain accu-\nracy while improving training and inference speed.\nWe explore the effectiveness of embedding recy-\ncling across a variety of different tasks, datasets,\nand transformer models.\n4.1 Models\nOur full-size models include the encoder trans-\nformers BERT, SciBERT (Beltagy et al., 2019),\nRoBERTa (Liu et al., 2019b), and DeBERTa (He\net al., 2020). We also experiment with the encoder-\ndecoder T5 model (Raffel et al., 2019). We selected\nthese architectures since they are widely-used pre-\ntrained transformers across a variety of tasks in\ndifferent domains. We experiment with multiple\nsizes of these models, including distilled (Sanh\net al., 2019; Wang et al., 2020, 2021), base, and\nlarge variants, to gauge the effectiveness of recy-\ncled embeddings with an increase in the network\nsize.\nTo investigate the effectiveness of layer recy-\ncling, we test several reduced models in which\nwe use caching to reduce 50% of the layers (e.g.,\ncaching layer 12 in RoBERTa-large and layer 6 in\nBERT-base).1 We compare each reduced model\nto its fully fine-tuned counterpart across the text\nclassification, NER, and QA tasks. The hardware\ndetails and hyperparameters for our models are\nspecified in Appendix A.\n4.2 Datasets\nFor our experiments, we focus on three core NLP\ntasks: text classification, named-entity recognition\n(NER), and extractive question-answering (QA).\nScientific papers, due to their immutable nature,\nare an especially appropriate target for embedding\nrecycling, so we focus much of our evaluation\non the scientific domain. For text classification,\nwe selected Chemprot (Kringelum et al., 2016),\nSciCite (Cohan et al., 2019), and SciERC (Luan\net al., 2018). For NER, we used BC5CDR (Li\net al., 2016), JNLPBA (Collier and Kim, 2004),\nand NCBI-Disease (Do˘gan et al., 2014). For QA,\nwe chose the TriviaQA (Joshi et al., 2017) and\nSQuAD (Rajpurkar et al., 2016) datasets.\n1We note that for the encoder-decoder model T5, we con-\nsider caching only the middle layer of the encoder, which\nmeans that the speedups for this model will be smaller than (ap-\nproximately half of) that of the other models we evaluate. We\nalso consider 25% and 75% reduced models in Appendix A.\n1936\n5 Results\n5.1 Standard Fine-tuning\nThe results for standard fine-tuning of either full or\nreduced models are shown in Table 1. For the text\nclassification and NER tasks, the reduced BERT-\nsized and larger models perform similarly to their\nfine-tuned counterparts on average, and substan-\ntially outperform the distilled models. The reduced\ndistilled models also perform well on those tasks\ncompared to the distilled originals, on average, al-\nthough there is more variance across models and\ntasks compared to BERT-sized models. We validate\nour fully fine-tuned baselines by comparing our re-\nsults with prior work (Beltagy et al., 2019), finding\nthat our scores land within 1.33% on average and\ntypically score above the previous baselines.\nFor QA tasks, we found that fully fine-tuning\nworks somewhat better than reduced configurations\nacross all the explored models (Table 1). Generally,\nreduced configurations typically lag by 1 to 2 points\nin F-1 score. One possible hypothesis is that the QA\ndatasets are generally much larger than the datasets\nwe used for other tasks (100k-150k examples vs\n4k-20k examples for text classification and NER);\nhowever, in additional experiments we found that\nsubsampling the QA training sets to 5% of their\noriginal size only increased the gap, suggesting that\ndataset size does not explain the failure of reduced\nmodels on this task. We also validate our fully fine-\ntuned baselines for QA tasks by comparing our\nresults with Yasunaga et al. (2022), finding that our\nscores differ by less than half a point on average.\nFinally, we explored using lightweight multi-\nlayer perceptrons (MLPs) as classifier heads, given\ntheir success in prior work. While (Du et al., 2020)\npaired multi-task encoders with 2-layer MLPs, we\npaired frozen pretrained transformer models with\n2-layer MLPs and found that they underperformed\ntrainable layers dramatically, by 26% on average\nacross the classification and NER tasks.\n5.2 Adapters\nOur results for reduced adapter models are shown\nin Table 2. We see that in general, for all the\nmodels except for T5-Large, the adapter-based ap-\nproaches are superior to standard fine-tuning on\nour tasks. Further, layer recycling remains effec-\ntive with adapters. Compared to the full adapter\nbaseline, the reduced adapters for RoBERTa-Large,\nBERT, SciBERT, and DeBERTa models only show\na 0.19% reduction in accuracy. Additionally, com-\npared to the fully fine-tuned baseline, these reduced\nadapters models have a 0.19-0.23% reduction in ac-\ncuracy. Likewise, in contrast to the full fine-tuning\nresults above, QA accuracy for the top-performing\nDeBERTa adapter model remains unchanged on av-\nerage after layer recycling, with the reduced adapter\nmodel performing better on one QA task and worse\non the other.2\n5.3 GLUE Results\nFor our best-performing model DeBERTa v2 XL,\nwe also provide further experiments on datasets\nfrom the GLUE benchmark (Wang et al., 2018),\nto allow easier comparison against speedup tech-\nniques from previous work. We present results\non the CoLA, SST-2, MRPC, STS-B, MNLI, and\nQNLI tasks from GLUE. For our experiments, we\ntried both our standard reduced models and our\nreduced adapter models. We found that embedding\nrecycling was successful across the GLUE tasks,\nwith an average accuracy drop of 0.3 points in re-\nturn for a significant increase in both training and\ninference time as outlined in Table 5 and Table 4.\nWe note that due to the high computational cost of\nthese experiments, we take existing hyperparame-\nter settings from previous work that worked well\nfor the full models, and also use these for reduced\nmodels. Further hyperparameter optimization of\nthe reduced models might improve performance.\n5.4 Efficiency Analysis\nTo estimate the real-world benefit of recycling em-\nbeddings for different tasks, we provide a mini-\nmal PyTorch implementation of embedding recy-\ncling. This implementation and the following re-\nsults correspond to both the standard layer recy-\ncling approach and the adapter-based layer recy-\ncling approach since they follow parallel processes\nfor gradient descent during training and computa-\ntions during inference, despite the additional 6-8%\nof parameters added by the trainable adapters. To\nshow that training times do not differ substantially,\nwe also measured the training time the transformer\nmodels take to converge to their optimal weights.\nWe found both approaches take approximately the\nsame time to complete training (Table 16).\nWe evaluated the impact of recycling embed-\ndings on four different architectures and two dif-\n2We omit experiments with distilled models, as we found\nadapters to be ineffective on those models even without em-\nbedding recycling, scoring 19.4% worse on average than full\nfine-tuning for text classification and NER.\n1937\nRoBERTa\nLarge (Sci)BERT DeBERTa\nV2 XL\nT5\nLarge\nMiniLM\nL6-H768\nMiniLM\nL6-H384 DistilBERT\nTask Rdc Full Rdc Full Rdc Full Rdc Full Rdc Full Rdc Full Rdc Full\nChemProt 84.3 83.9 84.0 84.0 86.8 86.7 84.6 84.1 78.3 79.3 76.9 74.6 80.3 79.1\nSciCite 85.0 85.5 86.6 86.0 85.2 84.4 86.3 84.9 84.5 84.6 83.7 82.8 84.1 84.0\nSciERC-Rel 80.2 80.4 76.7 79.8 79.9 80.2 77.4 80.2 74.8 78.2 72.1 68.9 74.9 72.9\nClassification Avg. 83.2 83.3 82.4 83.3 84.0 83.8 82.8 83.1 79.2 80.7 77.6 75.4 79.8 78.7\nbc5cdr 90.0 90.4 90.7 91.3 91.3 91.8 90.7 89.9 87.8 87.5 85.9 88.3 88.3 88.7\nJNLPBA 79.4 78.7 78.8 79.0 78.5 78.2 79.6 80.0 77.3 76.9 74.0 77.2 78.6 78.5\nNCBI-disease 93.0 93.2 93.4 92.9 93.3 93.4 92.8 93.5 91.1 92.1 89.9 91.7 90.5 91.3\nNER Avg. 87.5 87.4 87.7 87.7 87.7 87.8 87.7 87.8 85.4 85.5 83.3 85.7 85.8 86.2\nTriviaQA 78.2 79.8 67.4 69.1 80.6 81.8 77.4 78.2 72.2 73.8 69.2 71.0 64.7 66.8\nSQuAD 91.8 93.6 87.5 88.5 94.5 94.6 93.7 93.9 85.0 87.0 89.0 89.6 84.8 85.4\nQA Avg. 85.0 86.7 77.5 78.8 87.5 88.2 85.5 85.9 78.6 80.4 79.1 80.3 74.8 76.1\nTable 1: Test scores of reduced (Rdc) models on the text classification, NER, and QA tasks. Bold indicates the\nbest average F-1 score between the reduced and fully fine-tuned (Full) versions of each model over 10 runs. For\nthe ChemProt dataset, we report the micro F-1 scores instead, following past work (Beltagy et al., 2019). The\nreduced BERT-sized models generally offer similar performance to their full counterparts (scoring within 0.2%\nwhen averaged across RoBERTa and SciBERT for the six tasks), and substantially outperform the distilled models.\nferent hardware platforms. For models, we consid-\nered two efficient transformer models (MiniLMv2\n(Wang et al., 2020, 2021) models with l = 6layers\nand embeddings of size h = 384 and h = 768),\ntwo medium sized models ( BERTBASE , l = 12,\nh = 768; BERTLARGE , l = 24, h = 1024), and\na large model (DeBERTaV2-XL ARGE , l = 24, h =\n1536). We evaluated embeddings on a efficiency-\noriented AI accelerator (NVIDIA A10G), as well\nas on a high-performance GPU (NVIDIA A6000).\nWe controlled for differences among tasks con-\nsidered in tables Table 1, 2, and 3, such as length of\nsequences and number of samples, by simulating\na sequence classification task on QASPER (Dasigi\net al., 2021), which includes the full-text of over a\nthousand academic manuscripts.3 We run all mod-\nels with a fixed batch size of 128. For all models,\nwe reduce exactly half of their layers by recycling,\nwhich results in a maximum theoretical speed-up\nof 100%. A run over the corpus consists of 335\nbatches, and we average results over seven runs.\nTable 4 shows the results of caching embed-\ndings to recycle on disk. Overall, we found that all\nmodels benefit from embedding recycling, achiev-\ning an average speedup ranging from 18 to 86%.\nUnsurprisingly, larger models benefit more from\nrecycling than smaller ones; this is due to the fact\nthat loading embeddings cached on disk adds a\n3Because the bulk of computation for a transformer model\nis done in its encoder and not in the task-specific heads, infer-\nence time is similar regardless of whether the model is used\nfor sequence classification, tagging, or question answering.\nsmall latency penalty to a model run, which is more\nnoticeable in the case of smaller models. For ex-\nample, we achieve an 84% speedup when running\nBERTBASE with embedding recycling on an A10G\nGPU, which is roughly equivalent to the latency of\na MiniLML6-H768 model without recycling (351 vs\n325 ms per batch on average); this result would us\nallow to run more accurate models while maintain-\ning the efficiency of shallower architectures.\nTable 4 also includes results when storing em-\nbeddings using half precision (that is, cache em-\nbeddings in FP16 rather FP32). The smaller em-\nbeddings lead to improvements for all models and\nhardware, ranging from +8% to +46%. Further,\nit has no impact on performance, as it changes\npredicted scores by at most 10−3 across all tasks\nevaluated in this work.\nWe also note that less capable hardware bene-\nfits more from caching embeddings. For example,\nBERTBASE achieves a speedup of 84% on an A10G\nGPU, while on A6000, the speedup is a more mod-\nest 55%. This is an expected result: fewer and\nslower execution cores/accelerator memory impact\noverall model latency. Further, we note that, de-\nspite the smaller relative gains, the more powerful\nGPU is always faster in absolute terms compared\nwith the less capable one.\nIt is important to note that these gaps from\nmaximum achievable speedup are only observed\nwhen performing inference; for training, we ob-\nserve almost perfect speed-up for all models and\n1938\nRoBERTa\nLarge (Sci)BERT DeBERTa\nV2 XL\nT5\nLarge\nTask\nRdc +\nHalf\nAdpt\nFull\nAdpt Full\nRdc +\nHalf\nAdpt\nFull\nAdpt Full\nRdc +\nHalf\nAdpt\nFull\nAdpt Full\nRdc +\nHalf\nAdpt\nFull\nAdpt Full\nChemProt 84.1 85.2 83.9 84.2 84.9 84.0 87.2 86.5 86.7 84.3 84.9 84.1\nSciCite 82.4 82.9 85.5 85.5 84.6 86.0 84.6 85.0 84.4 85.3 84.5 84.9\nSciERC-Rel 85.7 85.9 80.4 86.0 85.5 79.8 82.9 82.1 80.2 76.2 75.6 80.2\nClassification Avg. 84.1 84.7 83.3 85.2 85.0 83.3 84.9 84.6 83.8 81.9 81.7 83.1\nbc5cdr 90.0 90.6 90.4 90.0 90.9 91.3 90.7 91.1 91.8 79.9 85.7 89.9\nJNLPBA 79.1 79.2 78.7 79.8 78.3 79.0 79.3 79.0 78.2 78.8 79.5 80.0\nNCBI-disease 92.8 93.1 93.2 93.1 93.0 92.9 93.3 93.5 93.4 92.1 92.5 93.5\nNER Avg. 87.3 87.6 87.4 87.6 87.4 87.7 87.8 87.9 87.8 83.6 85.9 87.8\nTriviaQA 78.5 79.8 79.8 67.4 68.9 69.1 81.6 82.3 81.8 77.0 77.5 78.2\nSQuAD 93.5 93.4 93.6 87.9 87.9 88.5 94.7 93.9 94.6 90.6 91.0 93.9\nQA Avg. 86.0 86.6 86.7 77.6 78.4 78.8 88.1 88.1 88.2 83.8 84.3 85.9\nTable 2: Test scores of reduced adapter (Rdc + Half Adpt) models on the text classification, NER, and QA tasks.\nBold indicates the best average F-1 score between the reduced adapter, full adapter (Full Adpt), and fully fine-tuned\n(Full) versions of each model over 10 runs. For the ChemProt dataset, we report the micro F-1 scores instead,\nfollowing past work (Beltagy et al., 2019). The reduced, adapter-based transformer models offer similar performance\nto their full counterparts (scoring within 0.4% when averaged across RoBERTa, SciBERT, and DeBERTa for the\neight tasks), and substantially outperform the distilled models.\nGLUE task\nDeBERTa V2 XL\nRdc +\nHalf Adpt\nFull\nAdpt Rdc Full\nCoLA 70.9 71.3 70.8 71.2\nSST-2 96.9 97.1 97.1 97.4\nSingle\nSentence Avg. 83.9 84.2 84.0 84.3\nMRPC 93.9 94.0 93.4 93.9\nSTS-B 92.4 92.7 92.5 92.8\nSimilarity and\nParaphrase Avg. 93.2 93.4 93.0 93.4\nMNLI-m 91.7 92.0 91.0 91.4\nQNLI 95.0 95.1 94.1 94.8\nNLI Avg. 93.3 93.6 92.6 93.1\nTable 3: Test scores of reduced (Rdc) and reduced\nadapter (Rdc + Half Adpt) models on GLUE for De-\nBERTa V2 XL. Bold indicates the best average score\nbetween the reduced and fully fine-tuned (Full) versions\nfor the standard and adapter-based configurations. Each\nscore is averaged over 5 runs. We report the scores using\nthe standard GLUE metric for each corresponding task.\nhardware configurations except for the smaller\nMiniLM models. For example, BERTBASE re-\nquires 17.38 ±1.32 ms/batch4 without recycling,\ncompared to 8.67 ±2.18 ms/batch when recycling.\nEven when considering the additional time to cache\nembeddings to disk during the first pass, embed-\n4When training, we use a batch size of 16\nding recycling still achieves close to optimum\nspeedup on all models except MiniLMs, where\nits gains hover between 52% and 82% (“NR vs SR”\ncolumn in Table 5). When training for just 6 epochs\n(or roughly 2, 000 steps), recycling embeddings is\nfaster than simply freezing half of the parameters\nfor all models but MiniLM (“F vs SR” column in\nTable 5); this is due to the relatively higher cost of\ncaching layers to disk in case of smaller models.\nIn these cases, we empirically found that recycling\nachieves faster training time than freezing after\n12 epochs or 4, 000 training steps; since smaller\nmodels typically require more epochs to converge,\nwe conclude that recycling is generally preferable\nto partially freezing a model during training. For\nBERTBASE and larger models, embedding recycling\nis also more efficient than layer freezing, providing\na +20% to +45% speed-up after just 6 training\nepochs.\nWe also benchmarked the storage requirements\nof recycling embeddings. For a sequence of 512 to-\nkens and a hidden model dimension of 768, caching\nembeddings requires 1.6 MB with 32-bit precision\nor 0.8 MB with 16-bit precision. This translates\nto 15.5 MB per paper in QASPER (papers are, on\naverage 4,884 WordPiece tokens long). Weighing\nthe storage cost and compute savings of ER, we\nfind that it is cost-effective in cloud environments\nonly if the corpus is reprocessed several times per\n1939\nInference Time\n(Speedup over Baseline)\nModel Baseline\nRecycling Avg. F1\ndiff when\nrecycling\nFP32\ncache\nFP16\ncache\nNVIDIA A10G\nMiniLM\nL6-H384 183 ms 154 ms\n(+21%)\n123 ms\n(+67%) −0.2\nMiniLM\nL6-H768 325 ms 201 ms\n(+56%)\n195 ms\n(+66%) −0.4\nBERT\nBASE 647 ms 351 ms\n(+84%)\n343 ms\n(+88%) −0.3\nBERT\nLARGE 1943 ms 1066 ms\n(+86%)\n1004 ms\n(+93%) −0.2\nDeBERTa\nV2-XL ARGE 1914 ms 1010 ms\n(+89%)\n985 ms\n(+94%) −0.1\nNVIDIA A6000\nMiniLM\nL6-H384 123 ms 105 ms\n(+18%)\n100 ms\n(+23%) −0.2\nMiniLM\nL6-H768 208 ms 161 ms\n(+29%)\n150 ms\n(+38%) −0.4\nBERT\nBASE 416 ms 269 ms\n(+55%)\n245 ms\n(+59%) −0.3\nBERT\nLARGE 1235 ms 662 ms\n(+86%)\n643 ms\n(+92%) −0.2\nDeBERTa\nV2-XL ARGE 1430 ms 777 ms\n(+84%)\n758 ms\n(+89%) −0.1\nTable 4: Averageinference runtime comparison (in ms/-\nbatch, averaged over 7 runs) between vanilla encoders\nand models that cache embeddings on disk. For all runs,\ncache the middle layer of the encoder. We assume the\ncache is already precomputed when calculating timings;\nthus, maximum speedup is 100%. Overall, the larger\nthe model, the higher the speedup from re-using repre-\nsentations. Further, accelerators with fewer execution\nunits (A10G) benefit more from recycling embeddings.\nFinally, using half precision for embeddings improves\nspeed up across the board, while halving storage size.\nmonth, but is cost-effective on local hardware even\nwith infrequent (yearly) corpus reprocessing (de-\ntails in subsection A.8 of the appendix).\n6 Discussion and Future Work\nOur experiments raise several questions and sug-\ngest multiple avenues for future work, including:\n• Our layer recycling strategy is a straightforward\nER approach, but previous work has suggested that\nweighted pooling across layers can perform better\ncompared to any single layer in many cases (Liu\net al., 2019a; Du et al., 2020). Recycling pooled\nactivations may offer improved results. What is\nthe best way to capture and store the syntactic and\nsemantic knowledge encoded in the activations of\na model for later recycling?\n• As noted in the previous section, naive storage\nmethods for ER can be cost-prohibitive in some\nsettings, and finding ways to mitigate this cost\n(e.g., by compressing the stored activations) will\nbe important for making ER broadly applicable.\n• Our experiments show that the right recycling\napproach may be task-specific and model-specific.\nFor example, with standard fine-tuning as shown\nin Table 8, caching layer 12 in RoBERTa-large\nis most effective for NER and text classification,\nwhereas it is not effective for QA (but layer 6 per-\nforms much better). Which embeddings to retrieve\nand recycle for a task, and the right architecture\n(e.g. number of layers) to use when consuming\nthe recycled embeddings, represents a large de-\ncision space. Methods that can help practition-\ners automatically choose among public or private\nshared embedding sets and associated model de-\nsigns, given their task and objectives for accuracy\nand computational cost, may be important to make\nER an effective practical tool.\n• We present results with encoder-only and\nencoder-decoder models, on classification tasks.\nDetermining whether the approach is effective for\ngenerative tasks and autoregressive models is an\nimportant question for future work.\n• While we show that ER can be effective when\ncoupled with distillation, whether other techniques\nlike quantization and early exiting remain effective\nin combination with ER is an open question.\n• We focus on the setting where the exact same\ntext, at the length of a full document, is being\nreused for multiple tasks. In practice, we may\noften perform a task on text that is similar to but\nnot exactly the same as one for which we have\ncached embeddings (e.g., a Wikipedia page that\nhas been revised). Further, even a completely new\ndocument will have similarities and overlapped\nspans with previously processed ones. Studying\nER in these settings, e.g. through a combination of\nlayer recycling and the SkipBERT approach which\ncan apply to unseen passages via cached n-grams\n(Wang et al., 2022), is an area of future work.\n• Finally, it is possible to explore cross-model em-\nbedding recycling. We attempted a straightfor-\nward implementation of such approach by using\nrecycling embeddings from a larger model into a\nsmaller consumer model. However, the results did\n1940\nModel\nTraining (ms/batch, amortized over 6 epochs) Speedup\nNo\nRecycling (NR)\nModel\nFrozen (F)\nSaving +\nRecycling (SR)\nOnly\nRecycling (R)\nNR vs\nSR\nF vs\nSR\nNR vs\nR\nNVIDIA A10G\nMiniLM384 51 ± 1 30 ± 1 32 ± 6 25 ± 4 +59% -7% +104%\nMiniLM768 90 ± 4 56 ± 1 50 ± 4 45 ± 3 +80% +12% +100%\nBERTBASE 173 ± 2 112 ± 1 90 ± 4 87 ± 3 +92% +24% +99%\nBERTLARGE 347 ± 1 246 ± 1 181 ± 2 176 ± 2 +92% +36% +97%\nDeBERTaXLARGE 380 ± 2 286 ± 1 199 ± 1 194 ± 1 +91% +44% +96%\nNVIDIA A6000\nMiniLM384 41 ± 1 24 ± 1 26 ± 5 22 ± 3 +55% -8% +81%\nMiniLM768 61 ± 1 38 ± 1 40 ± 5 34 ± 3 +52% -5% +82%\nBERTBASE 117 ± 1 78 ± 1 60 ± 3 58 ± 2 +94% +30% +102%\nBERTLARGE 326 ± 2 212 ± 1 167 ± 2 161 ± 1 +96% +26% +103%\nDeBERTaXLARGE 359 ± 2 250 ± 1 184 ± 1 178 ± 1 +95% +35% +102%\nTable 5: Average training runtime comparison (in ms per batch, ±stdev over 7 runs) between vanilla encoders and\nmodels that cache embeddings on disk. For all runs, we cache the middle layer of the encoder; thus, theoretical\nspeedup is 100%. Time per batch is amortized over 6 epochs (2, 000 steps), the lowest number to convergence over\nall datasets (c.r.f. Table 16). We present results in four settings: no recycling (NR), freezing ½ of the layers during\ntraining (F), 1 training epoch during which embeddings are saved to disk followed by 5 epochs where recycling is\nenabled (SR), and 6 epochs where embeddings are already saved (R). Overall, we found that embedding recycling\nspeeds up training even when embeddings need to be cached to disk during the first pass. Compared to freezing,\nsaving and recycling improves training time for all but MiniLM models (F vs SR).\nnot show improvements (Appx. A.3). Developing\nand evaluating new approaches for this setting is\nan important item for future work.\n7 Conclusion\nWe have presented embedding recycling, a general\ntechnique for reusing previous activations of neu-\nral language models to improve the efficiency of\nfuture training and inference. We show how a sim-\nple technique of caching a layer of activations in\na pretrained model is effective. We validate our\napproach in experiments across fourteen tasks and\neight model architectures. We find that recycling\ntypically has small or no impacts to accuracy on\naverage, but does yield substantial throughput in-\ncreases demonstrated through a careful efficiency\nanalysis. We also discuss several open challenges\nfor future work.\n8 Limitations\nAs discussed in detail in our future work section,\nseveral advances are important to make embedding\nrecycling a broadly applicable practical technique.\nIn addition, the techniques we evaluate primarily\nbenefit transformer language models run on GPU-\nbased architectures with rapid storage, components\nwhich are not available to all NLP researchers and\npractitioners. Our experiments demonstrate posi-\ntive results with one representative embedding re-\ncycling technique, but do not directly evaluate all\nrecycling variants proposed earlier in the litera-\nture. Finally, the datasets used in our experiments\nwere in English, a high-resource language with\nrobust pretrained models which may benefit em-\nbedding recycling. Future work should expand on\nthe applicability of embedding recycling by using\nnon-English datasets in lower-resource settings to\ndetermine the breadth of its applicability.\nAcknowledgments\nThis work was supported in part by NSF Conver-\ngence Accelerator Grant OIA-2033558. We thank\nChris Coleman for helpful discussions.\nReferences\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\n2016. Layer normalization. ArXiv, abs/1607.06450.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\n1941\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nAndrew Brock, Theodore Lim, James M. Ritchie,\nand Nick Weston. 2017. Freezeout: Accelerate\ntraining by progressively freezing layers. ArXiv,\nabs/1706.04983.\nB Barla Cambazoglu, Hugo Zaragoza, Olivier Chapelle,\nJiang Chen, Ciya Liao, Zhaohui Zheng, and Jon De-\ngenhardt. 2010. Early exit optimizations for additive\nmachine learned ranking systems. In Proceedings\nof the third ACM international conference on Web\nsearch and data mining, pages 411–420.\nArman Cohan, Waleed Ammar, Madeleine van Zuylen,\nand Field Cady. 2019. Structural scaffolds for ci-\ntation intent classification in scientific publications.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 3586–3596,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel Weld. 2020. SPECTER:\nDocument-level representation learning using\ncitation-informed transformers. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 2270–2282,\nOnline. Association for Computational Linguistics.\nNigel Collier and Jin-Dong Kim. 2004. Introduction to\nthe bio-entity recognition task at jnlpba. In Proceed-\nings of the International Joint Workshop on Natural\nLanguage Processing in Biomedicine and its Appli-\ncations (NLPBA/BioNLP), pages 73–78.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4599–4610, On-\nline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. 2022. Delta tuning:\nA comprehensive study of parameter efficient meth-\nods for pre-trained language models. arXiv preprint\narXiv:2203.06904.\nJesse Dodge, Taylor Prewitt, Remi Tachet des Combes,\nErika Odmark, Roy Schwartz, Emma Strubell,\nAlexandra Sasha Luccioni, Noah A Smith, Nicole\nDeCario, and Will Buchanan. 2022. Measuring the\ncarbon intensity of ai in cloud instances. In 2022\nACM Conference on Fairness, Accountability, and\nTransparency, pages 1877–1894.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nJingfei Du, Myle Ott, Haoran Li, Xing Zhou, and\nVeselin Stoyanov. 2020. General purpose text embed-\ndings from pre-trained language models for scalable\ninference. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 3018–\n3030, Online. Association for Computational Lin-\nguistics.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a\ncontinuous cache. In International Conference on\nLearning Representations.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 3929–3938. PMLR.\nChaoyang He, Shen Li, Mahdi Soltanolkotabi, and\nSalman Avestimehr. 2021. Pipetransformer: auto-\nmated elastic pipelining for distributed training of\nlarge-scale models. In International Conference on\nMachine Learning, pages 4150–4159. PMLR.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\n1942\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nRabeeh Karimi Mahabadi, Luke Zettlemoyer, James\nHenderson, Lambert Mathias, Marzieh Saeidi,\nVeselin Stoyanov, and Majid Yazdani. 2022. Prompt-\nfree and efficient few-shot learning with language\nmodels. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers). Association for Computational\nLinguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for Com-\nputational Linguistics.\nJens Kringelum, Sonny Kim Kjaerulff, Søren Brunak,\nOle Lund, Tudor I Oprea, and Olivier Taboureau.\n2016. Chemprot-3.0: a global chemical biology dis-\neases mapping. Database, 2016.\nJaejun Lee, Raphael Tang, and Jimmy J. Lin. 2019.\nWhat would elsa do? freezing layers during trans-\nformer fine-tuning. ArXiv, abs/1911.03090.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nShih-Ting Lin, Ashish Sabharwal, and Tushar Khot.\n2021. ReadOnce transformers: Reusable representa-\ntions of text for transformers. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 7129–7141, Online. As-\nsociation for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYuhan Liu, Saurabh Agarwal, and Shivaram Venkatara-\nman. 2021. Autofreeze: Automatically freezing\nmodel blocks to accelerate fine-tuning. ArXiv,\nabs/2102.01386.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identification of entities,\nrelations, and coreference for scientific knowledge\ngraph construction. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3219–3232, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAri Morcos, Maithra Raghu, and Samy Bengio. 2018.\nInsights on representational similarity in neural net-\nworks with canonical correlation. Advances in Neu-\nral Information Processing Systems, 31.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nJack Rae and Ali Razavi. 2020. Do transformers need\ndeep long-range memory? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7524–7529, Online. Association\nfor Computational Linguistics.\n1943\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. Svcca: Singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. Advances in neural\ninformation processing systems, 30.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nAndreas Rücklé, Gregor Geigle, Max Glockner, Tilman\nBeck, Jonas Pfeiffer, Nils Reimers, and Iryna\nGurevych. 2021. Adapterdrop: On the efficiency\nof adapters in transformers. In EMNLP.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav\nNakov. 2020. On the effect of dropping layers\nof pre-trained transformer models. arXiv preprint\narXiv:2004.03844.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nJue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian\nMcAuley. 2022. Skipbert: Efficient inference with\nshallow layer skipping. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7287–\n7301.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021. MiniLMv2: Multi-head self-\nattention relation distillation for compressing pre-\ntrained transformers. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 2140–2151, Online. Association for Computa-\ntional Linguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776–5788.\nTianwen Wei, Jianwei Qi, and Shenghuang He. 2022.\nA flexible multi-task model for bert serving. In ACL.\nGregor Wiedemann, Steffen Remus, Avi Chawla,\nand Chris Biemann. 2019. Does bert make any\nsense? interpretable word sense disambiguation\nwith contextualized embeddings. arXiv preprint\narXiv:1909.10430.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and\nChristian Szegedy. 2022. Memorizing transformers.\narXiv preprint arXiv:2203.08913.\nXueli Xiao, Thosini Bamunu Mudiyanselage, Chunyan\nJi, Jie Hu, and Yi Pan. 2019. Fast deep learning train-\ning through intelligently freezing layers. 2019 Inter-\nnational Conference on Internet of Things (iThings)\nand IEEE Green Computing and Communications\n(GreenCom) and IEEE Cyber, Physical and Social\nComputing (CPSCom) and IEEE Smart Data (Smart-\nData), pages 1225–1232.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. DeeBERT: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2246–2251, Online.\nAssociation for Computational Linguistics.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. Linkbert: Pretraining language models with\ndocument links. arXiv preprint arXiv:2203.15827.\nA Experimental Setup and Additional\nResults\nA.1 Fine-tuning Transformer Models\nThe candidate transformer models are fine-tuned\nusing configurations suggested by Devlin et al.\n(2019), Ding et al. (2022) and Houlsby et al. (2019).\nFor text classification, we feed the final hidden state\nof the [CLS] token into a linear classification layer.\n1944\nFor NER and QA, we feed the final hidden states\nof each token into a linear classification layer with\na softmax output.\nFor all of the models, we apply a dropout of 0.1\nto the transformer outputs and optimize for cross\nentropy loss using Adam (Kingma and Ba, 2015).\nWe employ a batch size of 32 across all tasks. We\nfine-tune using early stopping with a patience of 10,\nusing a validation set for calculating loss for each\nepoch. We use a linear warmup followed by lin-\near decay for training (Howard and Ruder, 2018),\ntesting the following learning rate options: 1e-3,\n2e-3, 1e-4, 2e-4, 1e-5, 2e-5, 5e-5, and 5e-6. For the\ntext classification and NER datasets, we select the\nbest performing learning rate for each transformer\nmodel on the development set and report the cor-\nresponding test results. For the QA datasets, we\nselect the best performing learning rate for each\ntransformer model on the training set and report\nthe corresponding results on the validation set. Ad-\nditionally, for the adapter modules used in certain\nmodel configurations, we test bottleneck dimen-\nsions as part of our hyperparameter search: 24, 64,\nand 256.\nA.2 Adapter-based Models\nHere, we used frozen RoBERTa-Large (Liu et al.,\n2019b), SciBERT (Beltagy et al., 2019), and BERT\nmodels but added adapter modules (Houlsby et al.,\n2019) only on the latter half of the transformer\nlayers. Only the adapters and the linear classifier\nattached to the model output were fine-tuned for\nthe text classification, NER, and QA tasks.\nWe found that the best hyperparameter configu-\nration was generally a bottleneck dimension of 256\nand a learning rate of either 1e-4 or 2e-4.\nA.3 Cross-model Embedding Reuse\nAn alternative to re-using cached activation from\na pre-trained model (section 5), is to cache ac-\ntivations from a more expensive, larger model\nand re-using them in downstream cheaper mod-\nels. The goal here is to improve accuracy by using\nmore powerful contextual embeddings. Overall, a\nstraightforward implementation of this strategy did\nnot offer improvements, as described below.\nWe experiment with reusing precomputed em-\nbeddings from one source model F in a consumer\nmodel F′that has a different size but the same tok-\nenization vocabulary. The activations of the final\ntransformer layer hN\nc∈C are stored for each input\nc from corpus C. During the fine-tuning of the\nconsumer model F′, these stored activations are\ntransformed through a learned 2-layer MLP with\nReLU activation5 and added to the input embed-\ndings of F′. We tried two frameworks for pair-\ning large language model embeddings with com-\npact models: F=Roberta-large →F′=MiniLM-6L-\nH768 and F=BERT-base→F′=DistilBERT.\nOverall, as shown in Table 6 the larger model’s\ncontextual representations do not improve the\nsmaller model’s accuracy; in fact adding them de-\ncreases the average F1 score by 0.3-0.9 points.\nA.4 Efficiency of Embedding Recycling when\nTraining\nFor training, we observe almost perfect speed-up\nfor all models and hardware configuration, bar-\nring MiniLM models on the machine equipped\nwith a A6000 GPU (“NR vs R” column in Ta-\nble 5). For example, BERTBASE requires 17.38 ±\n1.32 ms/batch6 without recycling, compared to\n8.67 ±2.18 ms/batch when recycling. Even when\nconsidering the additional time to cache embed-\ndings to disk during the first pass, embedding re-\ncycling still achieves close to optimum speedup\non all models except MiniLMs, where its gains\nhover between 52% and 82% (“NR vs SR” col-\numn in Table 5). When training for just 6 epochs\n(or roughly 2, 000 steps), recycling embeddings is\nfaster than simply freezing half of the parameters\nfor all models but MiniLM (“F vs SR” column in\nTable 5); this is due to the relatively higher cost of\ncaching layers to disk in case of smaller models.\nIn these cases, we empirically found that recycling\nachieves faster training time than freezing after 12\nepochs or 4, 000 training steps; since smaller mod-\nels typically require more epochs to converge, we\nconclude that recycling is generally preferable to\npartially freezing a model during training.\nA.5 Embedding Pre-fetching while Recycling\nStoring embeddings on NVMe drives, while fast,\nintroduce additional latency compared to RAM.\nFor example, BERTBASE achieves an average la-\ntency of 351 ±1 ms/batch when caching on disk\n(84% speedup), compared to just 334 ±1 ms/batch\nwhen using memory (94% speedup). This is due to\nthe fact that, while embeddings are being loaded\nfrom disk, the hardware accelerator responsible for\nexecuting the rest of the model sits idle. To reduce\n5We found that MLP achieved better performance com-\npared with a single linear layer on dev set.\n6When training, we use a batch size of 16\n1945\nRoBERTa-Large\n+ MiniLM L6-H768 MiniLM L6-H768 BERT +\nDistilBERT DistilBERT\nChemprot Micro F-1 78.9 (0.3) 79.3 (0.3) 77.8 (0.4) 79.1 (0.5)\nMacro F-1 52.2 (0.2) 52.6 (0.4) 51.2 (0.5) 52.6 (0.3)\nSciCite Micro F-1 85.2 (0.3) 86.0 (0.2) 85.7 (0.1) 85.5 (0.1)\nMacro F-1 83.8 (0.3) 84.6 (0.2) 84.2 (0.1) 84.0 (0.1)\nSciERC-Rel Micro F-1 85.1 (0.4) 86.3 (0.2) 83.8 (0.2) 83.5 (0.4)\nMacro F-1 76.2 (0.8) 78.2 (0.6) 73.6 (0.6) 72.9 (0.7)\nText Classification\nAverage Score 76.9 77.8 76.0 76.3\nTable 6: Cross-Model Recycling Results for RoBERTa+MiniLM-L6H768 and BERT+DistilBERT configurations.\nBold indicates the best average score between the cross-model recycling and fully finetuned versions of each model.\nEach score represents the average score of 10 runs, with the standard errors for each score in parentheses.\nthe impact of this latency penalty, our implemen-\ntation supports pre-fetching of future embeddings:\nwhen processing a sequence of inputs, such as sen-\ntences in a manuscript, it loads embeddings for\ntokens ahead of the sequence inference is currently\nbeing run on. This optimization reduces the time ac-\ncelerators wait for data to be available for inference;\nfor example, in the case of BERTBASE on A10G,\ndisabling pre-fetching raised inference inference\ntime to 374±1 ms/batch (vs 351±1 ms/batch with\npre-fetching). Therefore in this section, all results\nare reported with prefetching enabled.\nA.6 Software and Hardware\nFor implementation, we use the v4.19 version of\nthe Transformers library (Wolf et al., 2019), the\nv0.4 version of the OpenDelta library (Ding et al.,\n2022), and the v1.11 version of the Pytorch library\n(Paszke et al., 2019). We conduct our experiments\nusing NVIDIA RTX A6000 GPUs and NVIDIA\nA10G GPUs with CUDA v11.5.\nA.7 Considerations in Selecting Hardware for\nProof-of-Concept Recycling Experiments\nWe ran our proof-of-concept implementation on an\nAWS Cloud instance7 equipped with an NVIDIA\nA10G accelerator, and on a NVIDIA A6000 within\nan on-premise server8. The former contains fewer\nexecution units (72 vs 84), fewer tensor cores (288\nvs 336), slower memory (600 vs 768 GB/s), and\nslower boost clock (1800 MHz vs 1695 MHz).\nHowever, it is much more efficient, being rated\nat 150W (compare with A6000’s 300W power tar-\nget). Therefore, the NVIDIA A10G accelerator\npresents a more realistic platform for embedding\nrecycling, since it is more suitable for cost-efficient\n7g5.2xlarge instance with 8 cores and 32 GB of RAM.\n8Intel-based system with 128 cores and 512 GB of RAM.\nlarge-scale model deployments. Both machines are\nequipped with PCIe NVMe drives, which we use\nto cache embeddings to recycle.\nA.8 Cost-effectiveness of Embedding\nRecycling\nIn this section we attempt to estimate how cost-\neffective embedding recycling is for inference in\na real-world setting. While this depends heavily\non use-case-specific assumptions, we consider two\ntypical settings as proofs-of-concept, one using\ncloud computing and one using local hardware.\nThere are four main factors that affect the cost-\nbenefit ratio of embedding recycling: (1) compute\ncost, (2) storage cost, (3) model architecture, and\n(4) frequency of corpus reprocessing (i.e., how of-\nten the cached embeddings will be used). Com-\npute costs are challenging to estimate for a locally-\nowned hardware setting due to many hidden cost\nfactors beyond the GPUs (cooling, electrical costs,\nserver to house the GPUs, etc) and so we use\nAWS EC2 cloud GPU prices as a cost estimate for\nboth cloud and local hardware. In particular, we\nconsider a g5.12xlarge instance with 4 ×A10G\nGPUs at 5.67 $/hr.\nStorage costs are easier to estimate for local\nhardware than compute costs, and local storage\ncan be significantly cheaper because embedding\nrecycling does not require the availability and dura-\nbility guarantees provided by cloud solutions (the\ncache is accessed infrequently and can always be\nrecomputed if it is lost). Therefore, we consider\nboth a cloud storage solution (AWS S3 one-zone\ninfrequent access, at 0.01 $/GB/month) and a lo-\ncal storage solution. For local storage, we consider\ncurrent consumer-grade hard drive prices at approx-\nimately 16.9 $/TB based on data from Amazon and\nNewegg, and assume a lifespan of 6 years based on\n1946\nModel Cloud Local\nMiniLM384 0.05 2.2\nMiniLM768 0.05 2.4\nBERTBASE 0.13 5.6\nBERTLARGE 0.30 12.9\nDeBERTaXLARGE 0.20 8.5\nTable 7: Minimum reprocessing frequency (in months)\nneeded in order for embedding recycling to be cost-\neffective in various model and hardware configurations.\ndata from Backblaze.9 This results in an average\ncost of 0.23 $/TB/month over the life of the drive.\nFinally, we note that AWS does not charge for data\ntransfer between S3 and EC2 within a region, so\nwe can ignore data transfer costs in this calculation.\nThe frequency of corpus reprocessing is highly\nvariable, so we report results in terms of the mini-\nmum reprocessing frequency that would be neces-\nsary for embedding recycling to be cost-effective.\nFor all models we assume each input is 512 tokens\nand the cache is stored with FP16 precision.\nTable 7 shows the minimum reprocessing fre-\nquency needed for embedding recycling to be cost\neffective for our models on cloud and local hard-\nware. Under our assumptions, we find that embed-\nding recycling is cost-effective in a cloud setting\nonly if the corpus is reprocessed very frequently\n(several times per month). This may be realistic in\nsome use cases, such as when a large team is work-\ning with the same corpus and developing many new\nmodels, or if new training data arrives frequently\nand the model developer wants to continually up-\ndate and re-deploy it.\nWith local hardware the calculation is much\nmore favorable; embedding recycling with\nBERTLARGE would be worthwhile even if the cor-\npus were only reprocessed once per year.\nWe note that embedding recycling could become\nsubstantially more cost effective with further de-\nvelopment. In this work we did not explore ways\nto reduce storage costs, such as quantization or\ncompression. In addition, while our experiments\nonly considered sequence lengths of 512 tokens,\nfor many full-text document corpora it is desirable\nto use a much longer sequence length to fit the\nwhole document into a model at once. Because\nthe computational cost of transformers generally\nscales superlinearly with input length (but storage\n9https://www.backblaze.com/blog/how-long-do-disk-\ndrives-last/\ncost scales only linearly), embedding recycling will\nbe more effective as the sequence length grows.\n1947\nRoBERTa-Large\nReduced +\nHalf Adpt\nFull\nAdapters\n6 Layers\nReduced\n12 Layers\nReduced\n18 Layers\nReduced\nFully\nFinetuned\nChemProt Micro F-1 84.1 (0.4) 85.2 (0.3) 84.2 (0.3) 84.3 (0.2) 82.0 (0.2) 83.9 (0.3)\nMacro F-1 60.8 (0.7) 57.5 (0.7) 56.4 (0.4) 56.5 (0.3) 54.5 (0.5) 56.5 (0.4)\nSciCite Micro F-1 85.2 (0.3) 85.6 (0.5) 86.2 (0.2) 86.2 (0.2) 86.2 (0.2) 86.8 (0.2)\nMacro F-1 82.4 (0.4) 82.9 (0.6) 84.9 (0.2) 85.0 (0.2) 85.0 (0.2) 85.5 (0.2)\nSciERC-Rel Micro F-1 89.0 (0.5) 89.3 (0.6) 87.1 (0.4) 86.8 (0.4) 86.1 (0.2) 87.3 (0.4)\nMacro F-1 85.7 (0.7) 85.9 (0.9) 79.4 (0.7) 80.2 (0.8) 76.2 (0.4) 80.4 (0.6)\nText Classification\nAverage Score 81.2 81.1 79.7 79.8 78.3 80.1\nbc5cdr Micro F-1 97.4 (0.0) 97.6 (0.0) 97.2 (0.3) 97.4 (0.0) 97.3 (0.0) 97.5 (0.0)\nMacro F-1 90.0 (0.0) 90.6 (0.0) 89.0 (1.2) 90.0 (0.0) 89.5 (0.1) 90.4 (0.1)\nJNLPBA Micro F-1 93.8 (0.0) 93.8 (0.0) 93.8 (0.0) 93.9 (0.0) 93.7 (0.0) 93.7 (0.1)\nMacro F-1 79.1 (0.1) 79.2 (0.2) 79.3 (0.1) 79.4 (0.1) 79.0 (0.1) 78.7 (0.3)\nNCBI-disease Micro F-1 98.5 (0.0) 98.6 (0.0) 98.5 (0.0) 98.5 (0.0) 98.4 (0.0) 98.6 (0.0)\nMacro F-1 92.8 (0.1) 93.1 (0.1) 93.0 (0.1) 93.0 (0.1) 92.4 (0.1) 93.2 (0.1)\nNER Average\nScore 91.9 92.1 91.8 92.0 91.7 92.0\nTriviaQA Micro F-1 75.3 (0.1) 76.8 (0.2) 76.6 (0.2) 75.1 (0.1) 70.8 (0.1) 76.7 (0.1)\nMacro F-1 78.5 (0.1) 79.8 (0.1) 79.7 (0.2) 78.2 (0.1) 73.8 (0.1) 79.8 (0.1)\nSQuAD Micro F-1 87.0 (0.1) 86.7 (0.0) 86.2 (0.0) 84.7 (0.0) 79.3 (0.0) 87.4 (0.0)\nMacro F-1 93.5 (0.1) 93.4 (0.0) 92.8 (0.0) 91.8 (0.0) 87.8 (0.0) 93.6 (0.0)\nQA Average\nScore 83.6 84.1 83.8 82.4 77.9 84.3\nTable 8: RoBERTa Results for Reduced Models.Bold indicates the best average score between the standard reduced,\nadapter-based reduced, and fully fine-tuned versions of each model. Reduced + Half Adpt indicates adapters on\nthe transformer layers of a fully frozen reduced model, where the earlier half of transformer layers were removed\nand their activations cached. Full Adapters indicates adapters on all transformer layers of a fully frozen model.\nEach score represents the average score of 10 runs, with the standard errors for each score in parentheses.\nSciBERT\nReduced +\nHalf Adpt\nFull\nAdapters\n3 Layers\nReduced\n6 Layers\nReduced\n9 Layers\nReduced\nFully\nFinetuned\nChemProt Micro F-1 84.2 (0.3) 84.9 (0.4) 83.8 (0.4) 84.0 (0.2) 81.9 (0.2) 84.0 (0.3)\nMacro F-1 56.9 (0.8) 54.8 (0.4) 56.5 (0.5) 57.0 (0.3) 54.3 (0.3) 56.3 (0.4)\nSciCite Micro F-1 86.6 (0.2) 85.8 (0.1) 87.1 (0.1) 87.6 (0.1) 87.4 (0.1) 87.1 (0.2)\nMacro F-1 85.5 (0.3) 84.6 (0.1) 86.1 (0.1) 86.6 (0.1) 86.2 (0.1) 86.0 (0.2)\nSciERC-Rel Micro F-1 89.4 (0.4) 88.5 (0.6) 86.6 (0.3) 86.1 (0.2) 85.4 (0.2) 86.3 (0.2)\nMacro F-1 86.0 (0.7) 85.5 (0.6) 77.6 (0.5) 76.7 (0.3) 76.2 (0.4) 79.8 (0.5)\nText Classification\nAverage Performance 81.4 80.7 79.6 79.7 78.6 79.9\nbc5cdr Micro F-1 97.5 (0.0) 97.7 (0.1) 97.7 (0.0) 97.6 (0.0) 97.5 (0.0) 97.7 (0.0)\nMacro F-1 90.0 (0.0) 90.9 (0.1) 91.0 (0.1) 90.7 (0.0) 90.2 (0.1) 91.3 (0.0)\nJNLPBA Micro F-1 94.0 (0.0) 93.5 (0.0) 93.6 (0.1) 93.7 (0.1) 93.8 (0.0) 93.6 (0.1)\nMacro F-1 79.8 (0.0) 78.3 (0.2) 78.6 (0.4) 78.8 (0.2) 79.0 (0.1) 79.0 (0.2)\nNCBI-disease Micro F-1 98.6 (0.0) 98.5 (0.0) 98.5 (0.0) 98.6 (0.0) 98.5 (0.0) 98.5 (0.0)\nMacro F-1 93.1 (0.1) 93.0 (0.1) 92.9 (0.1) 93.4 (0.1) 93.1 (0.1) 92.9 (0.1)\nNER Average\nPerforamcne 92.2 92.0 92 92.1 92 92.2\nTable 9: SciBERT text classification and NER results for Reduced Models. Bold indicates the best average score\nbetween the standard reduced, adapter-based reduced, and fully fine-tuned versions of each model. Reduced +\nHalf Adpt indicates adapters on the transformer layers of a fully frozen reduced model, where the earlier half of\ntransformer layers were removed and their activations cached. Full Adapters indicates adapters on all transformer\nlayers of a fully frozen model. Each score represents the average score of 10 runs, with the standard errors for each\nscore in parentheses. QA tasks are not included since SciBERT was pretrained for scientific datasets.\n1948\nBERT\nReduced +\nHalf Adpt\nFull\nAdapters\n3 Layers\nReduced\n6 Layers\nReduced\n9 Layers\nReduced\nFully\nFinetuned\nTriviaQA Micro F-1 63.9 (0.5) 65.5 (0.1) 65.7 (0.1) 64.1 (0.2) 61.4 (0.1) 66.0 (0.1)\nMacro F-1 67.4 (0.5) 68.9 (0.1) 68.9 (0.1) 67.4 (0.1) 64.8 (0.1) 69.1 (0.1)\nSQuAD Micro F-1 80.2 (0.1) 80.2 (0.0) 80.8 (0.1) 79.5 (0.1) 75.4 (0.1) 81.1 (0.1)\nMacro F-1 87.9 (0.1) 87.9 (0.0) 88.4 (0.1) 87.5 (0.1) 84.8 (0.1) 88.5 (0.0)\nQA Average\nScores 74.9 75.6 76.0 74.6 71.6 76.2\nTable 10: BERT QA Results for Reduced Models. Bold indicates the best average score between the standard\nreduced, adapter-based reduced, and fully fine-tuned versions of each model. Reduced + Half Adpt indicates\nadapters on the transformer layers of a fully frozen reduced model, where the earlier half of transformer layers were\nremoved and their activations cached. Full Adapters indicates adapters on all transformer layers of a fully frozen\nmodel. Each score represents the average score of 10 runs, with the standard errors for each score in parentheses.\nDeBERTaV2 XL\nReduced +\nHalf Adpt\nFull\nAdapters\n6 Layers\nReduced\n12 Layers\nReduced\n18 Layers\nReduced\nFully\nFinetuned\nChemProt Micro F-1 87.2 (0.1) 86.5 (0.2) 87.2 (0.2) 86.8 (0.4) 86.4 (0.2) 86.7 (0.9)\nMacro F-1 56.7 (0.5) 55.6 (0.6) 59.6 (0.2) 59.5 (0.5) 59.2 (0.3) 59.0 (1.1)\nSciCite Micro F-1 85.8 (0.4) 86.4 (0.4) 86.0 (0.1) 86.3 (0.2) 86.2 (0.3) 85.9 (0.2)\nMacro F-1 84.6 (0.4) 85.0 (0.5) 84.6 (0.1) 85.2 (0.1) 85.0 (0.3) 84.4 (0.2)\nSciERC-Rel Micro F-1 88.6 (0.5) 88.0 (0.4) 88.3 (0.2) 87.5 (0.1) 86.6 (0.3) 88.0 (0.4)\nMacro F-1 82.9 (0.8) 82.1 (0.8) 80.5 (0.5) 79.9 (0.3) 78.0 (0.4) 80.2 (0.5)\nText Classification\nAverage Score 81.0 80.6 81.0 80.9 80.2 80.7\nbc5cdr Micro F-1 97.6 (0.0) 97.7 (0.0) 97.4 (0.3) 97.7 (0.0) 97.6 (0.0) 97.9 (0.0)\nMacro F-1 90.7 (0.1) 91.1 (0.1) 89.5 (1.4) 91.3 (0.0) 90.9 (0.0) 91.8 (0.1)\nJNLPBA Micro F-1 93.6 (0.0) 93.4 (0.0) 93.7 (0.1) 93.7 (0.0) 93.6 (0.0) 93.7 (0.0)\nMacro F-1 79.3 (0.1) 79.0 (0.1) 78.5 (0.3) 78.5 (0.2) 77.8 (0.1) 78.2 (0.1)\nNCBI-disease Micro F-1 98.3 (0.0) 98.4 (0.0) 98.6 (0.0) 98.6 (0.0) 98.5 (0.0) 98.6 (0.0)\nMacro F-1 93.3 (0.1) 93.5 (0.2) 93.1 (0.1) 93.3 (0.1) 92.8 (0.1) 93.4 (0.1)\nNER Average\nScore 92.1 92.2 91.8 92.2 91.9 92.3\nTriviaQA Micro F-1 78.6 (0.2) 79.1 (0.2) 77.9 (0.2) 77.4 (0.2) 77.0 (0.2) 78.5 (0.1)\nMacro F-1 81.6 (0.1) 82.3 (0.2) 81.2 (0.1) 80.6 (0.1) 80.1 (0.2) 81.8 (0.1)\nSQuAD Micro F-1 88.6 (0.0) 87.2 (0.1) 88.6 (0.1) 88.7 (0.0) 87.1 (0.0) 88.5 (0.1)\nMacro F-1 94.7 (0.0) 93.9 (0.0) 94.6 (0.0) 94.5 (0.0) 93.5 (0.0) 94.6 (0.0)\nQA Average\nScore 85.9 85.6 85.6 85.3 84.4 85.8\nTable 11: DeBERTaV2-XL Results for Reduced Models.Bold indicates the best average score between the standard\nreduced, adapter-based reduced, and fully fine-tuned versions of each model. Reduced + Half Adpt indicates\nadapters on the transformer layers of a fully frozen reduced model, where the earlier half of transformer layers were\nremoved and their activations cached. Full Adapters indicates adapters on all transformer layers of a fully frozen\nmodel. Each score represents the average score of 5 runs, with the standard errors for each score in parentheses.\n1949\nT5 Large\nReduced +\nHalf Adpt\nFull\nAdapters\n6 Layers\nFrozen\n12 Layers\nReduced\n18 Layers\nReduced\nFully\nFinetuned\nChemProt Micro F-1 84.3 (0.6) 84.9 (0.6) 84.7 (0.6) 84.6 (0.6) 85.0 (0.1) 84.1 (0.8)\nMacro F-1 57.2 (0.7) 58.0 (0.8) 56.2 (0.7) 56.2 (0.7) 57.4 (0.1) 56.1 (0.7)\nSciCite Micro F-1 86.7 (0.3) 86.2 (0.3) 87.4 (0.2) 87.6 (0.1) 88.0 (0.2) 86.4 (0.2)\nMacro F-1 85.3 (0.4) 84.5 (0.4) 86.0 (0.2) 86.3 (0.2) 86.9 (0.2) 84.9 (0.2)\nSciERC-Rel Micro F-1 85.6 (0.4) 85.2 (0.1) 84.3 (0.3) 86.8 (0.4) 83.4 (0.7) 87.4 (0.5)\nMacro F-1 76.2 (1.0) 75.6 (0.2) 73.6 (0.9) 77.4 (0.7) 72.2 (1.0) 80.2 (1.1)\nText Classification\nAverage Score 79.2 79.1 78.7 79.8 78.8 79.9\nbc5cdr Micro F-1 93.8 (0.6) 95.7 (0.7) 97.7 (0.7) 97.4 (0.3) 95.4 (0.8) 97.5 (0.2)\nMacro F-1 79.9 (1.0) 85.7 (1.1) 91.1 (0.5) 90.7 (1.1) 89.3 (1.0) 89.9 (0.8)\nJNLPBA Micro F-1 93.9 (0.4) 93.8 (0.1) 93.8 (0.0) 94.0 (0.0) 93.9 (0.0) 94.2 (0.0)\nMacro F-1 78.8 (0.6) 79.5 (0.2) 78.8 (0.1) 79.6 (0.1) 79.3 (0.0) 80.0 (0.0)\nNCBI-disease Micro F-1 97.8 (0.0) 98.5 (0.0) 98.5 (0.0) 98.5 (0.0) 98.4 (0.0) 98.6 (0.0)\nMacro F-1 92.1 (0.2) 92.5 (0.2) 93.1 (0.1) 92.8 (0.0) 92.2 (0.1) 93.5 (0.0)\nNER Average\nScore 89.4 90.9 92.2 92.2 91.4 92.3\nTriviaQA Micro F-1 68.2 (0.2) 68.8 (0.2) 67.0 (0.0) 66.9 (0.0) 63.9 (0.0) 68.7 (0.0)\nMacro F-1 77.0 (0.1) 77.5 (0.1) 77.5 (0.0) 77.3 (0.0) 74.8 (0.0) 78.0 (0.0)\nSQuAD Micro F-1 81.2 (0.1) 82.0 (0.1) 86.6 (0.1) 86.3 (0.6) 85.2 (0.4) 86.7 (0.4)\nMacro F-1 90.6 (0.1) 91.0 (0.1) 93.8 (0.0) 93.7 (0.3) 92.8 (0.2) 93.9 (0.3)\nQA Average\nScore 79.2 79.8 81.2 81.0 79.2 81.8\nTable 12: T5 Large Results for Reduced Models. Bold indicates the best average score between the standard\nreduced, adapter-based reduced, and fully fine-tuned versions of each model. Reduced + Half Adpt indicates\nadapters on the encoder and decoder transformer layers of a fully frozen reduced model, where the earlier half of\nthe encoder layers were removed and their activations cached. Full Adapters indicates adapters on all encoder\nand decoder transformer layers of a fully frozen model. Each score represents the average score of 5 runs, with the\nstandard errors for each score in parentheses.\n1950\nDistilBERT\n2 Layers\nReduced\n3 Layers\nReduced\n4 Layers\nReduced\nFully\nFine-tuned\nChemProt Micro F-1 79.1 (0.4) 80.3 (0.1) 79.0 (0.2) 79.1 (0.5)\nMacro F-1 52.1 (0.5) 51.6 (0.6) 51.6 (0.4) 52.6 (0.3)\nSciCite Micro F-1 85.7 (0.1) 85.6 (0.1) 85.8 (0.1) 85.5 (0.1)\nMacro F-1 84.3 (0.1) 84.1 (0.1) 84.2 (0.1) 84.0 (0.1)\nSciERC-Rel Micro F-1 84.3 (0.3) 84.5 (0.3) 84.6 (0.2) 83.5 (0.4)\nMacro F-1 74.1 (0.7) 74.9 (0.7) 74.6 (0.4) 72.9 (0.7)\nText Classification\nAverage Score 76.6 76.8 76.6 76.3\nbc5cdr Micro F-1 97.0 (0.0) 97.0 (0.0) 96.9 (0.0) 97.2 (0.0)\nMacro F-1 88.3 (0.0) 88.3 (0.1) 87.9 (0.0) 88.7 (0.1)\nJNLPBA Micro F-1 93.4 (0.1) 93.5 (0.0) 93.4 (0.0) 93.5 (0.0)\nMacro F-1 78.0 (0.3) 78.6 (0.1) 77.9 (0.1) 78.5 (0.1)\nNCBI-disease Micro F-1 98.2 (0.0) 98.0 (0.0) 98.1 (0.0) 98.2 (0.0)\nMacro F-1 91.4 (0.1) 90.5 (0.1) 90.7 (0.1) 91.3 (0.1)\nNER Average\nScore 91.1 91 90.8 91.2\nTriviaQA Micro F-1 62.9 (0.1) 61.4 (0.1) 59.1 (0.1) 63.6 (0.1)\nMacro F-1 66.2 (0.1) 64.7 (0.1) 62.4 (0.1) 66.8 (0.1)\nSQuAD Micro F-1 76.6 (0.1) 76.3 (0.1) 72.5 (0.1) 77.1 (0.1)\nMacro F-1 85.1 (0.1) 84.8 (0.0) 82.3 (0.1) 85.4 (0.0)\nQA Average\nScore 72.7 71.8 69.1 73.2\nTable 13: DistilBERT Results for Reduced Models. Bold indicates the best average score between the reduced and\nfully fine-tuned versions of each model. Each score represents the average score of 10 runs, with the standard errors\nfor each score in parentheses.\n1951\nMiniLM: 6L-H768\n2 Layers\nReduced\n3 Layers\nReduced\n4 Layers\nReduced\nFully\nFine-tuned\nChemProt Micro F-1 79.4 (0.3) 78.3 (0.4) 79.0 (0.2) 79.3 (0.3)\nMacro F-1 51.8 (0.4) 50.6 (0.4) 52.0 (0.2) 52.6 (0.4)\nSciCite Micro F-1 85.4 (0.1) 85.8 (0.2) 85.9 (0.1) 86.0 (0.2)\nMacro F-1 84.1 (0.2) 84.5 (0.2) 84.5 (0.1) 84.6 (0.2)\nSciERC-Rel Micro F-1 84.7 (0.3) 83.9 (0.3) 84.1 (0.4) 86.3 (0.2)\nMacro F-1 75.0 (0.4) 74.8 (0.4) 75.3 (0.6) 78.2 (0.6)\nText Classification\nAverage Score 76.7 76.3 76.8 77.8\nbc5cdr Micro F-1 96.1 (0.3) 96.8 (0.0) 96.6 (0.0) 96.8 (0.2)\nMacro F-1 84.6 (1.1) 87.8 (0.1) 86.6 (0.0) 87.5 (1.0)\nJNLPBA Micro F-1 93.2 (0.0) 93.2 (0.0) 93.3 (0.0) 93.3 (0.0)\nMacro F-1 77.5 (0.1) 77.3 (0.1) 77.3 (0.1) 76.9 (0.2)\nNCBI-disease Micro F-1 98.3 (0.0) 98.2 (0.0) 98.2 (0.0) 98.3 (0.0)\nMacro F-1 92.1 (0.1) 91.1 (0.1) 91.0 (0.1) 92.1 (0.1)\nNER Average\nScore 90.3 90.7 90.5 90.8\nTriviaQA Micro F-1 70.2 (0.1) 68.9 (0.1) 65.5 (0.1) 70.4 (0.2)\nMacro F-1 73.4 (0.1) 72.2 (0.1) 68.9 (0.1) 73.8 (0.2)\nSQuAD Micro F-1 77.6 (0.1) 75.6 (0.1) 65.4 (0.2) 78.9 (0.1)\nMacro F-1 86.4 (0.1) 85.0 (0.1) 77.0 (0.1) 87.0 (0.1)\nQA Average\nScore 76.9 75.4 69.2 77.5\nTable 14: MiniLM L6-H768 Results for Reduced Models. Bold indicates the best average score between the\nreduced and fully fine-tuned versions of each model. Each score represents the average score of 10 runs, with the\nstandard errors for each score in parentheses.\n1952\nMiniLM: L6-H384\n2 Layers\nReduced\n3 Layers\nReduced\n4 Layers\nReduced\nFully\nFine-tuned\nChemProt Micro F-1 75.4 (0.5) 76.9 (0.2) 74.9 (0.3) 74.6 (0.4)\nMacro F-1 47.3 (0.7) 50.4 (0.2) 48.8 (0.4) 47.1 (0.8)\nSciCite Micro F-1 84.4 (0.1) 85.4 (0.1) 85.1 (0.1) 84.4 (0.1)\nMacro F-1 82.8 (0.1) 83.7 (0.1) 83.4 (0.1) 82.8 (0.1)\nSciERC-Rel Micro F-1 83.2 (0.3) 82.6 (0.3) 83.3 (0.2) 79.5 (0.9)\nMacro F-1 72.7 (0.6) 72.1 (0.6) 73.7 (0.3) 68.9 (1.1)\nText Classification\nAverage Score 74.3 75.2 74.9 72.9\nbc5cdr Micro F-1 96.6 (0.0) 96.3 (0.0) 95.6 (0.0) 96.9 (0.0)\nMacro F-1 86.9 (0.1) 85.9 (0.1) 83.2 (0.1) 88.3 (0.1)\nJNLPBA Micro F-1 93.0 (0.0) 92.2 (0.0) 92.0 (0.0) 93.3 (0.0)\nMacro F-1 76.3 (0.1) 74.0 (0.1) 73.6 (0.1) 77.2 (0.1)\nNCBI-disease Micro F-1 98.0 (0.0) 97.9 (0.0) 97.7 (0.0) 98.2 (0.0)\nMacro F-1 90.6 (0.1) 89.9 (0.1) 88.9 (0.1) 91.7 (0.1)\nNER Average\nScore 90.2 89.4 88.5 90.9\nTriviaQA Micro F-1 66.6 (0.1) 65.6 (0.1) 63.4 (0.1) 67.6 (0.2)\nMacro F-1 69.9 (0.1) 69.2 (0.1) 67.0 (0.1) 71.0 (0.2)\nSQuAD Micro F-1 81.6 (0.0) 80.9 (0.1) 74.2 (0.2) 81.6 (0.1)\nMacro F-1 89.7 (0.0) 89.0 (0.0) 84.5 (0.1) 89.6 (0.0)\nQA Average\nScore 76.9 76.2 72.3 77.4\nTable 15: MiniLM L6-H384 Results for Reduced Models. Bold indicates the best average score between the\nreduced and fully fine-tuned versions of each model. Each score represents the average score of 10 runs, with the\nstandard errors for each score in parentheses.\nTask Averages Standard\nRecycling\nAdapter-Based\nRecycling\nClassification Training Time 2204 2349\nEpochs 38 42\nNER Training Time 4269 3857\nEpochs 43 39\nQA Training Time 8252 8513\nEpochs 6 7\nTable 16: Average Training Times and Epochs for Embedding Recycling (seconds for training time, count for\nepochs). Standard Recycling corresponds to layer recycling on a reduced transformer model. Adapter-Based\nRecycling corresponds to layer recycling on a reduced frozen transformer model with added trainable Adapter\nmodules. Training time and epoch averages are the averages across the RoBERTa, BERT, SciBERT, DeBERTa V2\nXL, and T5-Large transformer models and the text classification, NER, and QA datasets tested.\n1953",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.9425500631332397
    },
    {
      "name": "Computer science",
      "score": 0.8502429127693176
    },
    {
      "name": "Embedding",
      "score": 0.8227294683456421
    },
    {
      "name": "Inference",
      "score": 0.7454282641410828
    },
    {
      "name": "Reuse",
      "score": 0.7316903471946716
    },
    {
      "name": "Task (project management)",
      "score": 0.6387082934379578
    },
    {
      "name": "Overhead (engineering)",
      "score": 0.6144945621490479
    },
    {
      "name": "Language model",
      "score": 0.5885418057441711
    },
    {
      "name": "Machine learning",
      "score": 0.4689236283302307
    },
    {
      "name": "Cache",
      "score": 0.4675782024860382
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44365739822387695
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4349079728126526
    },
    {
      "name": "Computer engineering",
      "score": 0.34487879276275635
    },
    {
      "name": "Parallel computing",
      "score": 0.326120525598526
    },
    {
      "name": "Programming language",
      "score": 0.1934438943862915
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}