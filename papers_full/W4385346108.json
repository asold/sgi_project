{
    "title": "ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model",
    "url": "https://openalex.org/W4385346108",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2129720774",
            "name": "Hanyao Huang",
            "affiliations": [
                "State Key Laboratory of Oral Diseases",
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A1983677102",
            "name": "Ou Zheng",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2122301951",
            "name": "Dongdong Wang",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2132220469",
            "name": "Jiayi Yin",
            "affiliations": [
                "Sichuan University",
                "State Key Laboratory of Oral Diseases"
            ]
        },
        {
            "id": "https://openalex.org/A2116034976",
            "name": "Zijin Wang",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A4297668067",
            "name": "shengxuan ding",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2101933035",
            "name": "Heng Yin",
            "affiliations": [
                "State Key Laboratory of Oral Diseases",
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2114406445",
            "name": "Chuan Xu",
            "affiliations": [
                "Southwest Jiaotong University",
                "University Transportation Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A2120071617",
            "name": "Renjie Yang",
            "affiliations": [
                "State Key Laboratory of Oral Diseases",
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A1993332544",
            "name": "Qian Zheng",
            "affiliations": [
                "Sichuan University",
                "State Key Laboratory of Oral Diseases"
            ]
        },
        {
            "id": "https://openalex.org/A2102607477",
            "name": "Bing Shi",
            "affiliations": [
                "Sichuan University",
                "State Key Laboratory of Oral Diseases"
            ]
        },
        {
            "id": "https://openalex.org/A2129720774",
            "name": "Hanyao Huang",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A1983677102",
            "name": "Ou Zheng",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2122301951",
            "name": "Dongdong Wang",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2132220469",
            "name": "Jiayi Yin",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2116034976",
            "name": "Zijin Wang",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A4297668067",
            "name": "shengxuan ding",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2101933035",
            "name": "Heng Yin",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2114406445",
            "name": "Chuan Xu",
            "affiliations": [
                "University Transportation Research Center",
                "Southwest Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2120071617",
            "name": "Renjie Yang",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A1993332544",
            "name": "Qian Zheng",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2102607477",
            "name": "Bing Shi",
            "affiliations": [
                "Sichuan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4318219754",
        "https://openalex.org/W4327681325",
        "https://openalex.org/W4220726078",
        "https://openalex.org/W4362557643",
        "https://openalex.org/W4220717038",
        "https://openalex.org/W4285988360",
        "https://openalex.org/W4229448187",
        "https://openalex.org/W2755631603",
        "https://openalex.org/W3129300919",
        "https://openalex.org/W4318612697",
        "https://openalex.org/W4323980924",
        "https://openalex.org/W4376616419",
        "https://openalex.org/W4367556998",
        "https://openalex.org/W4362601804",
        "https://openalex.org/W2078396547",
        "https://openalex.org/W2897583329",
        "https://openalex.org/W2106107776",
        "https://openalex.org/W2888120268",
        "https://openalex.org/W3184324824",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W3172427031",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3166664235",
        "https://openalex.org/W3213921583",
        "https://openalex.org/W2915623326",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W3212516020",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W3105892552",
        "https://openalex.org/W3164670515",
        "https://openalex.org/W4293262146",
        "https://openalex.org/W3198549210",
        "https://openalex.org/W4372270126",
        "https://openalex.org/W4311000453",
        "https://openalex.org/W4322618218",
        "https://openalex.org/W3176743688",
        "https://openalex.org/W4386083082",
        "https://openalex.org/W4322739071",
        "https://openalex.org/W4323313957",
        "https://openalex.org/W4224246525",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W4309643848",
        "https://openalex.org/W2086229366",
        "https://openalex.org/W4317891156",
        "https://openalex.org/W2004910511",
        "https://openalex.org/W2531468880",
        "https://openalex.org/W2795959543",
        "https://openalex.org/W2784499877",
        "https://openalex.org/W2767767650",
        "https://openalex.org/W2651948199",
        "https://openalex.org/W1663984431",
        "https://openalex.org/W2594522336",
        "https://openalex.org/W3104761038",
        "https://openalex.org/W2805183640",
        "https://openalex.org/W3018776648",
        "https://openalex.org/W3120395777",
        "https://openalex.org/W3151286594",
        "https://openalex.org/W4293067524",
        "https://openalex.org/W3182099217",
        "https://openalex.org/W3155203876",
        "https://openalex.org/W3016417837",
        "https://openalex.org/W1841482152",
        "https://openalex.org/W3044701241",
        "https://openalex.org/W2960622829",
        "https://openalex.org/W2897596009",
        "https://openalex.org/W2810136663",
        "https://openalex.org/W2897355938",
        "https://openalex.org/W4307431143",
        "https://openalex.org/W4318293221",
        "https://openalex.org/W2911313818",
        "https://openalex.org/W2939839354",
        "https://openalex.org/W2977271377",
        "https://openalex.org/W3042883732",
        "https://openalex.org/W4225380028",
        "https://openalex.org/W2999399991",
        "https://openalex.org/W4224217689",
        "https://openalex.org/W1498293842",
        "https://openalex.org/W3007177728",
        "https://openalex.org/W65109133",
        "https://openalex.org/W4377864459",
        "https://openalex.org/W4221157572",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4392619039",
        "https://openalex.org/W4318932456",
        "https://openalex.org/W4381956285",
        "https://openalex.org/W3037471945",
        "https://openalex.org/W2910972514",
        "https://openalex.org/W3102123817",
        "https://openalex.org/W4396895148",
        "https://openalex.org/W4390120219",
        "https://openalex.org/W3098949126",
        "https://openalex.org/W3106224367",
        "https://openalex.org/W4362722944"
    ],
    "abstract": null,
    "full_text": "REVIEW ARTICLE OPEN\nChatGPT for shaping the future of dentistry: the potential of\nmulti-modal large language model\nHanyao Huang 1 ✉, Ou Zheng 2 ✉, Dongdong Wang2, Jiayi Yin1, Zijin Wang2, Shengxuan Ding3, Heng Yin1, Chuan Xu4,5,\nRenjie Yang6, Qian Zheng1 and Bing Shi1\nThe ChatGPT, a lite and conversational variant of Generative Pretrained Transformer 4 (GPT-4) developed by OpenAI, is one of the\nmilestone Large Language Models (LLMs) with billions of parameters. LLMs have stirred up much interest among researchers and\npractitioners in their impressive skills in natural language processing tasks, which profoundly impact variousﬁelds. This paper\nmainly discusses the future applications of LLMs in dentistry. We introduce two primary LLM deployment methods in dentistry,\nincluding automated dental diagnosis and cross-modal dental diagnosis, and examine their potential applications. Especially,\nequipped with a cross-modal encoder, a single LLM can manage multi-source data and conduct advanced natural language\nreasoning to perform complex clinical operations. We also present cases to demonstrate the potential of a fully automatic Multi-\nModal LLM AI system for dentistry clinical application. While LLMs offer signiﬁcant potential beneﬁts, the challenges, such as data\nprivacy, data quality, and model bias, need further study. Overall, LLMs have the potential to revolutionize dental diagnosis and\ntreatment, which indicates a promising avenue for clinical application and research in dentistry.\nInternational Journal of Oral Science          (2023) 15:29 ; https://doi.org/10.1038/s41368-023-00239-y\nINTRODUCTION\nArtiﬁcial intelligence (AI) has promoted recent progress in digital\nhealth for many years.\n1,2 AI-equipped applications in dentistry\nhave been found useful in analyzing medical imaging, including\ndiagnosing dental caries,\n3,4 periodontitis,5 and implantitis,6 and\nassisting oral and maxillofacial surgery with surgical planning.7\nBesides the imaging data, audio data analyses can also beneﬁt\nfrom deep-learning applications, as speech is one of the most\nimportant functions of the oral structure.\n8,9 Furthermore, dental\neducation is another emerging application.10 GPT-4, released by\nOpenAI, embarks on a new period of AI-powered large language\nmodels (LLMs). ChatGPT, built upon GPT-4, stirred up lots of\ninterest among millions of scientists and engineers on account of\nits impressive human conversational response as a chatbot.\n11\nHowever, its potential impact on revolutionizing a series of\ntechnologies is more signi ﬁcant. Unlike earlier applications,\nChatGPT is cultivated conversationally upon a tremendous\nknowledge base, enabling informative communications for the\nimprovement of decision knowledge. Before ChatGPT, most AI\ntechnologies focus on the system of one input and one output,\nwhich relies on the amount of training data. With the inﬂux of new\ndata, re-training is required to update the existing model for more\naccurate decision-making. ChatGPT breakthroughs this mode and\nincorporates conversation to dynamically capture multiple sources\nof existing knowledge for question answering.\n2,12,13 This human-\nfriendly feature facilitates the diagnosis process and causes a\nsigniﬁcant change in the status quo, and its advancement will also\nshape digital health in dentistry.1,14–16 The purpose of this paper is\nto provide an overview of the potential application of ChatGPT in\ndentistry.\nJOURNEY OF LLMS\nBefore LLMs garner signiﬁcant attention, language modeling has\nundergone a series of revolutions in the past decade. The early\nnatural language model is carried out with n-gram modeling,17\nwhich is probabilistic modeling yet effective for medical\nresearch.18,19 The ﬁrst milestone work after n-gram modeling is\nword embedding, which represents words in vector space to\nunderstand the natural language from a new quantitative\nperspective, promoting clinical research on document analy-\nsis.\n20,21 Among a range of representation modeling, ELMo 22\nproposed by AllenNLP changes the game to a bi-direction model\npretraining. This modeling approach also in ﬂuences medical\nlanguage research 20 and is also evaluated. 23 Since then, bi-\ndirectional deep-learning models have been proposed like BERT24\nand Generative Pretrained Transformer (GPT).25 Built upon these\nmodels, a range of medical language models are proposed to\naccelerate medical research progress, such as a family of medical\nBERT models,\n26–29 and clinical researchers found that the increase\nin model size signi ﬁcantly improves a variety of medical\napplications.21,30–32 However, they are limited to medium model\nReceived: 26 March 2023 Revised: 6 July 2023 Accepted: 13 July 2023\n1State Key Laboratory of Oral Diseases & National Clinical Research Center for Oral Diseases & Department of Oral and Maxillofacial Surgery, West China Hospital of Stomatology,\nSichuan University, Chengdu, China;2Department of Civil, Environmental & Construction Engineering, University of Central Florida, Orlando, USA;3College of Transportation\nEngineering, University of Central Florida, Orlando, USA;4School of Transportation and Logistics, Southwest Jiaotong University, Chengdu, China;5C2SMART Center, Tandon\nSchool of Engineering, New York University, Brooklyn, USA and6State Key Laboratory of Oral Diseases & National Clinical Research Center for Oral Diseases & Eastern Clinic, West\nChina Hospital of Stomatology, Sichuan University, Chengdu, China\nCorrespondence: Hanyao Huang (huanghanyao_cn@scu.edu.cn) or Ou Zheng (ouzheng1993@knights.ucf.edu)\nThese authors contributed equally: Hanyao Huang, Ou Zheng\nwww.nature.com/ijosInternational Journal of Oral Science\n1234567890();,:\nscales due to architecture design and hardware support, although\nsome ef ﬁcient algorithms are proposed for the medical\ndomain.20,33,34 One of the most important LLMs is T5, with 11\nbillion parameters proposed by Google.35 Another rival model is\nGPT-3, developed by OpenAI, which contains 175 billion\nparameters. These billion-parameter models embark on a new\nchapter of LLMs and their applications. One of the most successful\napplication instances is ChatGPT, a variant of InstructGPT\n36\ndeveloped upon GPT-3, optimized by conversational response\ntraining. ChatGPT is equipped with interactive training which\ninvolves human feedback reinforcement learning and exhibits\npowerful language skills to generate human-like texts in real-time\nconversation. This interactive modeling also inﬂuences medical\nresearch like education.\n37,38 All these rely on large-scale repre-\nsentation pretraining, which becomes critical to complex problem-\nsolving with data in cross-modality, even for ChatGPT.\nLarge-scale vision-language pretraining\nVision-language pretraining is an important approach to solving\ntext-to-image or image-to-text tasks, which trains a deep neural\nnetwork with large image and text datasets. One of the vital\ntraining frameworks is CLIP, proposed by OpenAI, which is further\nimproved by Salesforce to BLIP.\n39 For text-to-image, GAN as an\nimage generation prototype model can be integrated with text\nrepresentation to generate diverse, authentic-looking but syn-\nthetic images.\n40 Recently, the Diffusion Model,41 a rival model of\nGAN emerging with higher computation ef ﬁciency and image\ndiversity, has been incorporated for vision-language representa-\ntion pretraining. For example, DALL-E v242 leverages CLIP ranked\nrepresentation and diffusion model to generate image under-\nstanding sentences. To address medical domain-speci ﬁc pro-\nblems, a series of efﬁcient representation learning models are also\ndeveloped to empower intelligent healthcare services. BERT\nprovides an effective solution to efﬁcient inference and analysis\nof disease.\n43 Multi-modal learning is also considered to improve\nmedical visual-questioning-answer processes.44 Although various\napplications are proposed, the study on how to integrate this\npowerful model with dental diagnosis is still limited.\nLarge-scale audio-language pretraining\nCompared to vision-language models, audio-language pretraining\ndoes not prevail, but the representation learning with audio and\ntext data still exhibits impressive audio-to-text performance. Some\nmedium-scale models like MusCALL,\n45 CTAL,46 Wav2Seq,47 and\nLAVA48 indicate the superiority of representation pretraining on\nspeech recognition. One of the important large-scale pretraining\nmodels is Whisper,\n49 released by OpenAI, which is trained on\n680,000 h of diverse audio-text pairs from the web. Inspired by the\nsuccess of these works, an improved medical speech-to-text\npretraining model is developed to more effectively link vocal\nsignals to language generation and understanding.\n50 Since audio-\nlanguage pretraining research is still under exploration, the limited\nstudy demonstrates how to employ this pretraining framework to\nfacilitate oral treatment.\nMulti-modal LLM\nWith tremendous success in cross-modal training, more research\nattempts to incorporate multi-modal representation learning to\nempower LLMs. As one of the successful attempts, GPT-4\n51\ndemonstrates the competence of LLMs in a multitude of NLP\napplications, such as higher scores in GRE tests and other question-\nanswering tasks. This implies a higher potential for Multi-Modal LLM\nin various areas, such as digital health. For example, multi-modal\nlearning is conducted to facilitate medical services, which incorpo-\nrates images, audio, and texts into training for a more comprehensive\nand robust model.\n43,45 However, due to limited data availability, more\nresearch still attempts to explore the merit of multi-model LLM for\nmedical ﬁelds, especially dental clinic research.\nLLM as a ubiquitous solution\nAs LLMs become increasingly widely recognized, more represen-\ntations will be embedded into the models to enhance their\ngeneral problem-solving skills. The training process with a larger\nscale of data will yield a ubiquitous solution to problems of all\nkinds. For example, ChatGPT has served as a valuable tool to assist\nmedical education for more effective instruction and analysis of\nteacher-student interaction.\n37,38 Medical writing can be assisted or\neven accomplished by ChatGPT, 32 which enables ef ﬁcient\ndocumentation. Language challenges in medical research or\nclinical processes can also be alleviated by ChatGPT.52\nAI TECHNOLOGY FOR CLINICAL APPLICATION\nAI technology has promoted clinical applications by improving\npatient outcomes, streamlining processes, and reducing costs. In\nclinical practice, AI has achieved striking success in analyzing\npatient data like brain-tumor segmentation,\n53 assisting in clinical\ndecision-making like epidemiological prediction,54 and perform-\ning complex tasks such as surgery and rehabilitation, which\nindicates the potential to revolutionize healthcare service. In\ndentistry, the convolutional neural network has shown perfor-\nmance gain in detecting and classifying maxillofacial fractures\nfrom CT.\n55 However, subtle details of maxillofacial fractures may\nnot be accurately detected sometimes due to the unfavorable\nresolution of CT scans. Still, more advanced CT scanners can\nachieve higher-resolution images in future studies. Medical\nresearchers also attempt to explore detection methods and\ninvestigate the feasibility of an automated decision-making tool\nfor dental age estimation using deep-learning and topological\napproaches by analyzing the third molar maturity index (I3M) from\n456 mandibular radiographs.\n56 Another recent research proposed\na more comprehensive AI system that can precisely identify\nindividual teeth and alveolar bones from dental cone-beam CT\n(CBCT) images, which enables accurate and precise dental\nhealthcare.\n57\nThe success of language modeling also promotes lots of\nresearch progress in representation learning for efﬁcient medical\nservices. For example, BioBert 58 is developed upon BERT to\nachieve a large but efﬁcient text mining model for biomedical\ndocument analysis. ClinicalBert59,60 carries out embedding training\nwith a large volume of clinical documents to facilitate intelligent\nclinical diagnosis processes. SciBert\n27 also built a large language\nmodel for representation learning with multiple documents across\nvarious scientiﬁc research domains.\nOne of the milestone contributions to biomedical research is\nAlphaFold,61 developed by DeepMind. Its success in accurate 3D\nprotein structure prediction demonstrates the power of large-\nscale training to tackle signi ﬁcant challenges in quantitative\nbiomedical modeling. Since then, a series of innovative large-scale\nframeworks have been proposed to enhance AI-powered model-\ning, such as OpenFold\n62 by OpenAI and BioNeMo Megatron63 by\nNVIDIA, etc. In addition, inspired by LLM pretraining schemes,\nNVIDIA developed ProT-VAE\n64 to advance functional protein\ndesign, which indicates the potential of large-scale biomolecule-\nlanguage pretraining with an LLM. As technology continues to\nevolve, we can expect to see even more innovative applications of\nAI in clinical settings, ultimately leading to more effective\nhealthcare services tailored to the needs of patients.\nON EXPLORING THE CAPABILITY OF LLMS IN DENTISTRY\nAutomated dental diagnosis with an LLM\nRecord analysis with text mining. Contemporary medical practice\nwidely adopts electronic health records (EHRs) for patient\ninformation documentation. Although it facilitates record genera-\ntion and management, efﬁcient analysis is still challenging since\nmassive amounts of records are mixed with structured and\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n2\nInternational Journal of Oral Science           (2023) 15:29 \nunstructured data. This challenge leads to substantial amounts of\nunderutilized data and obstructs patient care and research\nimprovement. Text mining is able to tackle this challenge by\ndrawing valuable conclusions and information from textual\nmaterial in a mixed structure. Some straightforward modeling\nframeworks have been developed to explore patterns, correla-\ntions, and trends within textual data.\n65–68 However, the perfor-\nmance of these models is inadequate to process massive amounts\nof documents efﬁciently and accurately.\nAn LLM helps toﬁnd a workaround for this limitation through\ntraining on extensive documents. Given strong competence in\nsemantic understanding, an LLM can manage documents\nindependent from structural formats. As shown in Fig. 1, text\nmining can also retrieve pertinent facts from unstructured data,\nsuch as free-text notes from healthcare professionals. From this\nunstructured data, an LLM like ChatGPT can be used to swiftly\nextract pertinent information, like a patient’s unique illnesses or\nadverse effects.\nTreatment planning with natural language reasoning. As afore-\nmentioned, medical service experiences the in ﬂux of a large\nvolume of digital information. In addition to straightforward\ndocument analysis, these data can assist healthcare providers in\ncustomizing treatment plans.\n69 Although this data analytics is\nstimulating, the work is taxing since more labor is involved in\ndocument analysis. LLMs can easily automate document compre-\nhension and make treatment plan analysis feasible, which reaps\nthe beneﬁt of large-scale pretraining. Furthermore, billions of\ndocuments help LLMs cultivate the capability of natural language\nreasoning (NLR) to perceive contexts. This capability of NLR can\nassist dental practitioners more efﬁciently in establishing treat-\nment plans tailored to patients’backgrounds.\n70 For instance, NLR\nalgorithms can examine adverse drug reaction (ADR) patterns\nlinked to various dental procedures and drugs.71 Sometimes, drug\nadministration can lead to gum bleeding and severer diseases like\nbisphosphonate-related osteonecrosis. Dentists can modify their\ntreatment plans to lower the likelihood of side effects by\nunderstanding the most typical ADRs linked to particular\nmedications. We maintain that an LLM can facilitate this process\nand provide a case of this application in Fig.2. It has been found\nthat NLR may be used to identify comorbidities by analyzing\npatient records for common risk factors and symptoms, identiﬁca-\ntion of ADRs,\n71 drug safety surveillance,72 and patient education.73\nMedical documentation with natural language generation .I n\ndental clinical practice, a synthetic yet faithfully representative\nEHR is essential to ef ﬁcient medical information conveyance\nbetween healthcare providers and other medical professionals.\nTraditionally, this document preparation process is completed\nmanually. Given keywords, practitioners organize the context\nfollowing medical record documentation standards. It can be\nquickly assisted with an LLM. Natural Language Generation (NLG)\nis one of the important tasks of LLM. Generally, NLG accomplishes\ntext generation given the understanding of natural language\ninput, like structured texts or separate keywords. Since a well-\ntrained LLM is highly skilled in this task, this merit can be\nleveraged to automate a series of common documentation tasks,\nsuch as generating reports on medical history, dental procedures,\nand treatment plans. For example, Fig. 3 shows an example of\nmedical report narrative generation with only several keywords by\nChatGPT. The generated texts are well formatted in a professional\nstructure and encompass all important information in eloquent\nexpression. The validity of the generated content has been\nvalidated.\n74,75\nCompared to other application ﬁelds, the inﬂuence of AI in\ndentistry has unquestionably been slower and more constrained.\nThis is mostly because patient privacy concerns have prevented\npatient data from being made widely available to the broader AI\nresearch community. Meanwhile, training data plays a crucial role\nin the advancement of AI methods. To address this dilemma and\npromote methodological advances in dentistry, using high-quality\nsynthetic quasi-EHRs data is a practical approach since it facilitates\ndata sharing between healthcare providers and external investi-\ngators. We employ ChatGPT 3.5, con ﬁgured with speci ﬁc\nNarrative of ADRs\n\"Patient X was prescribed \nWarfarin for a blood clot in \n2019. Two weeks after \nstarting the medication, the \npatient experienced severe \nbruising and bleeding from \ntheir gums. The medication \nwas adjusted, and the patient \nwas closely monitored for the \nremainder of their treatment.\"\n\"One of the potential side effects of Warfarin is \ngum bleeding, which can occur if \nthe medication causes the blood \nto thin too much. This can result \nin bleeding from the gums, as \nwell as other areas of the body, \nsuch as the nose, skin, or urine.\nIt's worth noting that while Warfarin can increase \nthe risk of bleeding, it does not directly affect the \nteeth. However, if gum bleeding is left untreated, it \ncan lead to periodontal disease, which can cause \ntooth loss and other serious dental problems. \nTherefore, it's important for patients taking Warfarin \nto prioritize good oral hygiene and to work closely \nwith their healthcare provider and dentist to \nmanage any potential side effects.\"\nAnswer to question 1 Answer to question 2\n\"The patient had been taking \nWarfarin for two weeks before\nexperiencing severe bruising \nand bleeding from their gums.\nHowever, the duration of the patient's overall \ntreatment with Warfarin is not specified in the \nstatement. The length of time that a patient \nmay need to take Warfarin can vary \ndepending on the individual's condition and \nresponse to treatment, and this would be \ndetermined by their healthcare provider.\"\nAnswer to question 3\n\"It only states that the patient \nexperienced these symptoms \ntwo weeks after starting the \nmedication. However, it's worth noting \nthat the duration of the patient's suffering may \ndepend on several factors, such as the severity \nof the side effects, how quickly the medication \ndosage was adjusted, and how well the patient \nresponded to the changes in their treatment. \nTypically, once the medication dosage is \nadjusted and the patient is closely monitored, \nthe side effects of Warfarin should start to \nimprove, but the lengthof time it takes to fully \nresolve can vary from person to \nperson.\"\nQuestion 1:\n\"What´s the side effect of warfarin to tooth?\"\nQuestion 2:\n\"How long has the patient take the\nmedication?\"\nQuestion 3:\n\"How long has the patient been suffering?\"\nInputs\nOutput probabilities\nMulti-head\nattention\nFeed\nforward\nPositional\nencoding\nNx\nAdd & norm \nPositional\nencoding\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nforward\nNx\nAdd & norm\nAdd & norm\nAdd & norm\nAdd & norm\nGenerative\npre-trained\ntransformer\ndecoder\nInput\nembedding\nSoftMax\nLinear\nOutput\nembedding\nFig. 1 Examples of a text mining application for extracting a patient’s historical record with an LLM like ChatGPT. Keywords are highlighted in\nred\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n3\nInternational Journal of Oral Science           (2023) 15:29 \nparameters, to generate synthetic data, shown in Fig. 4. The\nparameters include max tokens, frequency penalty, and presence\npenalty, which were set to enhance diversity in the generated text.\nThe frequency penalty reduces the likelihood of selecting words\nbased on their frequency of occurrence, while the presence\npenalty imposes a ﬁxed cost on each word in the text. These\npenalties encourage the model to generate text with higher\nperplexity rather than relying solely on the most probable word\nchoices. Additionally, temperature scaling is used to adjust the\ndistribution of probabilities for the next tokens, and a top-p value\nof 1 ensures consideration of all available tokens. Post-processing\nis applied to reﬁne the generated data to eliminate any artifacts\nintroduced during the generation process. These post-processing\nrules are determined through manual examination. These data\nPrompt\n\"Generate synthetic\nreports of dentist\"\nInputs\nOutput probabilities\nMulti-head\nattention\nFeed\nforward\nPositional\nencoding\nNx\nAdd & norm\nPositional\nencoding\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nf orward\nNx\nAdd & norm\nAdd & norm\nAdd & norm\nGenerative \npre-trained \ntransformer\nDecoder\n•  Chief Complaint: Toothache\n•  History: Intermittent tooth pain for the past two weeks.\n•  Feeling: A dull ache that is occasionally sharp and exacerbated by hot or cold \n   temperatures.\n•  Examination: Oral examination reveals a deep cavity on the upper right molar\n   tooth. There is no visible swelling or redness in the surrounding area.\n•  Diagnosis: Dental caries that has progressed to the pulp.\n•  Treatment: The tooth requires root canal therapy to remove the  infected tissue.\n\"The patient presents with a chief complaint of toothache that has been intermittent \nfor the past two weeks. They describe the pain as a dull ache that is occasionally \nsharp and worsens with exposure to hot or cold temperatures. Upon examination, a \ndeep cavity was identified on the upper right molar tooth. There is no visible swelling \nor redness in the surrounding area Based on these findings, the diagnosis is dental \ncaries that has progressed to the pulp. To address this issue the recommended \ntreatment is root canal therapy to remove the infected tissue. This procedure is \nnecessary to alleviate the patient‘s pain and prevent further complications. The\npatient should be advised to follow proper oral hygiene practices, including brushing\nand flossing regularly, to prevent future cavities and dental problems. Overall it is\nimportant for the patient to seek prompt treatment to prevent the infection\nfrom spreading and causing further damage to their oral health.\"\nKeyword list input Patient's r ecord narrative output\nAdd & norm\nInputs\nembedding\nSoftMax\nLinear\nOutput\ne mbedding\nFig. 3 Example of a narrative output of the Patient’s record generated from keywords with NLG\nNarrative of ADRs\n\"Patient X was prescribed \nWarfarin for a blood clot in \n2019. Two weeks after starting \nthe medication, the patient \nexperienced severe bruising \nand bleeding from their gums. \nThe medication was adjusted, \nand the patient was closely \nmonitored for the remainder of \ntheir treatment.\"\n\"In this case, the cause of the \npatient's suffering \nlikely a side effect of \nthe medication \nWarfarin. Warfarin is an \nanticoagulant medication that \nis used to prevent and treat \nblood clots, but it can increase \nthe risk of bleeding and \nbruising, particularly if the \ndosage is too high.\"\nwas \nAnswer to question 1 Answer to question 2\n\"If a patient experiences severe bruising and\nbleeding from the gums while taking Warfarin,\nthey should \nnotify their healthcare \nprovider immediately. Patients \nshould continue to take the\nmedication as prescribed and \nattend all follow-up \nappointments to ensure that \ntheir treatment is effective and \nsafe. \nPatients should also be aware of the \nsigns of bleeding and should seek immediate \nmedical attention if they experience \nsymptoms such as easy bruising, \nnosebleeds, or blood in the urine or\nstool.\"\nAnswer to question 3\n\"First, the healthcare provider should evaluate the\npatient‘s bleeding risk and assess\nwhether the dosage of Warfarin needs to\nbe adjusted. \nThey may also need to order blood\ntests to monitor the patient’s INR\n(international normalized ratio), which is a measure of how\nlong it takes for the patient's blood to clot. If the patient's\nbleeding is severe, the healthcare provider may need\nto stop the medication temporarily until\nthe bleeding is under control. \nThey may also\nprescribe medication to help control the\nbleeding or refer the patient to a\nhematologist or other specialist for\nfurther evaluation.\"\nQuestion 1:\n\"What's the cause of suffering?\"\nQuestion 2:\n\"What should the patient do?\"\nQuestion 3:\n\"What should the healthcare provider do?\"\nInputs\nOutput probabilities\nMulti-head\nattention\nFeed\nforward\nPositional\nencoding\nNx\nAdd & norm\nPositional\nencoding\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nforward\nNx\nAdd & norm\nAdd & norm\nAdd & norm\nAdd & norm\nGenerative\npre-trained\ntransformer\nDecoder \nInputs\nembedding\nSoftMax\nLinear\nOutput\nembedding\nFig. 2 Examples of natural language reasoning (NLR) application of medication suggestions derived from a patient’s record. Keywords are\nhighlighted in red\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n4\nInternational Journal of Oral Science           (2023) 15:29 \ncan be quickly generated and harvested with the assistance of\nLLMs. Synthetic EHRs can be more realistic by introducing\nvariability in the generated data. LLMs can be guided to generate\ndifferent patient proﬁles, medical histories, treatment plans, and\noutcomes. This helps mimic the diversity and complexity seen in\nreal EHRs. Thus, it implies that an LLM has competence in\nefﬁciently preparing medical information and protecting the\nprivacy of patients.\nCross-modal dental diagnosis with LLMs\nVision-language deployment\nVisual grounding: Traditionally caries-related diagnosis is admi-\nnistered by dentists through visual and tactile examination. Before\nany treatment plan, a quick but comprehensive examination of\noral health conditions is imperative. It sometimes takes experts’\neffort and time to diagnose tooth conditions, possibly through\nX-ray images and CBCT, and reach a reliable conclusion. Some\nexisting research has explored the potential of AI-assistant models\nin assisting diagnose for caries,\n3 periodontitis,5 medication-related\nosteonecrosis,76 maxillofacial bone fracture,55 oral squamous cell\ncarcinoma,77 and temporomandibular disorders.78 These diseases\ncan be diagnosed based on medical imaging. Also, AI-assistant\nmodels for imaging analyses show the potential in assisting dental\ntreatment, including orthodontics,\n79 restorative dentistry,80 oral\nimplantology,6 and oral and maxillofacial surgery.7\nHowever, limited data representation hinders accurate diag-\nnosis and treatment planning when the disease is intricate. The\nmajority of study has been conﬁned to image-only approaches,\nwhich restrain the effective conveyance of information and\nexplore the untapped potential of AI models in dentistry. LLMs\nopen the possibility of data-fused diagnosis by leveraging cross-\nmodal perception. An LLM is highly skilled in aligning textual and\nvisual representations for image-text analysis, which can facilitate\nthe diagnosis of tooth problems by X-ray image interpretation.\nSpeciﬁcally, the inference by an LLM can be blended with speciﬁc\nvisualization techniques to identify caries regions. For example, the\npractitioners can provide some keywords to query the model of\nALBEF (A Lite BERT for Adaptive Embedding Factorization), which is\nspeciﬁcally designed for image-to-text tasks and is integrated with\nGrad-CAM (Gradient-weighted Class Activation Mapping) to visualize\nthe critical region for decision-making from the ALBEF model. The\nwarmer color indicates the plausible regions corresponding to the\ndescribed words. As shown in Fig.5, root canal therapy is plausibly\nrequired in the region with a warmer color. Another tangible beneﬁt\nof an LLM is training cost reduction. Withoutﬁne-tuning a large set of\nimage data, an LLM can provide plausible affected teeth and likely\nlocations of dental problems.\nVisual question answering: In addition to visual examination of\nmedical imaging data, diagnosis documentation is more critical to\npatient-centered care. From the interpretation of X-ray images,\nhealthcare professionals will write down the observation, analysis,\nand medication suggestions to patients. These documents are also\nessential to healthcare big data analytics, while the document\nsummarization on the medical transcripts takes much time. An\nLLM is able to reduce the processing labor signiﬁcantly through\nspeciﬁc tasks, like visual question answering (VQA). Commonly, a\nVQA model can convert the encoded image representations to\nword embedding for dental diagnosis questions. With the\ndiagnosis questions, the answers are generated to facilitate\ndiagnostic report generation. This VQA-assisted diagnosis can be\nperformed to assess potential dental health issues.\n81 As shown in\nFig. 6, the X-ray image of a patient’s teeth is fed into an image\nencoder, like BLIP-2, generating a natural language representation,\ni.e., embedding based upon image understanding. Meanwhile,\ndifferent questions are fed into LLM to generate another set of\nquestion embeddings. Both question and image embeddings are\nmathematically combined to generate the interpreted answers to\nthe questions about images.\nHowever, sometimes the raw image contains too much noise, or\nthe resolution is not acceptable by the BLIP model; therefore, it is\ndifﬁcult to extract the desired information through the VQA\nmodel. To address this, training a semantic segmentation model\nthat divides the areas of the image with different properties into\ndifferent classes is a potential solution because it allows an LLM to\nlearn each element separately.\n82 For example, as Fig.7 shows, the\n•  Task descriptions\n•  Generative pre-trained transformer decoder\n•  Generate synthetic data\n•  Human evaluation\nMax\ntokens\nFrequency\npenalty\nPresence\npenalty\nOutput probabilities\nPositional\nencoding\nPositional\nencoding\nMulti-head\nattention\nFeed\nforward\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nforward\nNx Nx\nInputs\nInputs\nembedding\nSoftMax Linear\nOutput\nembedding\nAdd \n&  norm\nAdd & norm\nAdd & norm\nAdd & norm\nAdd & norm\nFig. 4 Framework of generating synthetic quasi-EHRs data by LLMs\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n5\nInternational Journal of Oral Science           (2023) 15:29 \nsoft tissue envelope and nasal septum/concha are classiﬁed into\norange and blue segments, respectively, and it is expected to\nimprove the model performance and enhance image under-\nstanding to extract the morphology information of nasal\ncartilages, as the cartilages are small and embedded by the soft\ntissue.\n83 Figure 7 also demonstrates the potential of training\nmachine learning models to help reconstruct the nasal cartilage\nbased on MRI for patients with orofacial clefts, who can suffer\nsigniﬁcant nasal deformity.84–87 Due to the limitations of current\nimaging software, the differences between the cartilage and soft\ntissue cannot be easily deﬁned, but with training machine learning\nmodels, the purpose should be achieved in the future.\nC\nQuestion input\n\"What is the \nproblem with \nthe left tooth?\"\n\"How many \nteeth are in \nthis image?\"\n\"an x-ray of a tooth\"\n\"Two\"\n\"Left\"\nQ-Former For every other block\nVision-and-language\nrepresentation learning\nVison to language \ngenerative learning\nFeed forward\nCross attention\nSelf attention\n…\nImage\n-text \nmatching\nImage-text \nContrastive learning\nAttention masking\nImage-\ngrounded\ntext \ngeneration\nFeed forward\nSelf attention\nLearned\nqueries\nBidirectional\nmutlimodal causal\nuni-modal\n\"Which tooth \nlooks abnormal,\nleft  or right?\"\n\"Have a fractured tooth \non the left side of my \nmouth\"\nInput image\nImage encoder\nCaption\nAnswer output\n/g117/g3N\n/g117/g3N\n\"an x-ray of a tooth\"Input text\nOutput probabilities\nPositional\nencoding\nPositional\nencoding\nMulti-head\nattention\nFeed\nforward\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nforward\nNx Nx\nInputs\nInput\nembedding\nSoftMax Linear\nOutput\nembedding\nAdd \n& norm\nAdd & norm\nAdd & norm\nAdd & norm\nAdd & norm\nFig. 6 A VQA example framework with the assistance of BLIP-2\nImage\ninput\n\"an x-ray\"\n\"a tooth with\"\n\"root canal therapy\"\nGrad-CAM visualizations\n\"an x-ray of a tooth with \nroot canal therapy\"\n[CLS]\nembedding\nMomentum\nupdate\nMomentum\ndistillation\nITM\nAlign before fuse (ALBEF) model\nImage input Text input\nSelf attention\nFeed forward\nSelf attention\nFeed forward\nSelf attention\nFeed forward\nCross attention\nImage \nencoder\nHard\nnegatives\nText\nencoder\nMultimodal\nencoder\nMomentum\nmodel\nMLM\nNegatives\n…\nImage-text  \ncontrastive loss\n× 6\n× 6× 12\nFig. 5 Schematic of dental condition diagnosis with a vision-language model of ALBEF\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n6\nInternational Journal of Oral Science           (2023) 15:29 \nVisual data generation: DALL-E 2, empowered by integrating\nGPT-3.5 as encoder and iffusion model as decoder, can be utilized\nin the medical ﬁeld to generate synthetic medical images. Once\nthe LLM has been trained on real EHR data, it can be used to\ngenerate synthetic quasi-EHRs. By providing prompts or speciﬁc\ninstructions to the model, such as patient characteristics or\nmedical conditions, the LLM can generate realistic synthetic\nrecords that resemble real-world EHRs. For example, by describing\na patient’s CBCT scan in text, including details of any abnormal-\nities, like an odontogenic cyst or alveolar cleft, which can observe\nobvious lesions on the alveolar bone structure, and feeding it into\nDALL-E 2, synthetic medical images that match the description\ncan be efﬁciently produced in large quantities to improve the\nperformance of deep-learning models by use as the training\ndataset. Figure 8 shows that synthetic medical images can be\ngenerated with varying levels of noise, contrast, or resolution to\ncreate images with speciﬁc properties or characteristics that are\nrelevant to the medical condition being studied. Moreover, patient\nprivacy can be protected since synthetic medical images are\ngenerated from textual descriptions rather than real medical data.\nThis technique is valuable for medical research based on any\nmedical 3D imaging techniques, including CBCT, CT, MRI, etc., and\nimproving patient care by training machine learning models while\nmaintaining patient privacy.\nAlternatively, LLMs can also be employed to generate medical\nillustrations or diagrams based on textual descriptions. For\nexample, a description of a surgical procedure can be fed into\nillustration software to create an accurate and detailed illustration\nof the procedure.\nAudio-language deployment . Besides imaging and dialogs, a\npatient ’s voice is also critical to medical diagnosis. A person’s\nvoice can potentially reveal important clues about their speech\nfunction, as certain vocal characteristics may be indicative of\nthe function of teeth, tongue, pharyngeal structure, and\nmuscles. Analyzing these vocal attributes can assist healthcare\nprofessionals in identifying potential health concerns. One of\nthe common medical diagnosis applications is waveform-\nspectrogram analysis on patients ’audio recordings, which are\ncollected by requesting the patients to read certain words or\nparagraphs. The waveform is a curve-based representation of\nan audio signal, the shape of which enables acoustic analysis.\nThe spectrogram is an alternative representation of sounds in\nthe frequency domain, which facilitates signal processing and\nanalysis.\nVelopharyngeal insuf ﬁciency related to cleft palate, oronasal\nﬁstula, and so forth, that affects the contaction between the\nsoft palate and posterior phar yngeal wall, or changes the\nneeded separation between the oral cavity and nasal cav-\nity,\n87–89 exhibits some typical marks on voice waveforms and\nspectrograms. In velopharyngeal insuf ﬁciency, for example,\nnasal emission can lead to distinct variation in speech. 90 A\nperson with velopharyngeal insuf ﬁciency may exhibit a wave-\nform that shows the reduced i ntensity of the sound waves\nduring certain frequencies or periods, leading to altered speech\npatterns because of the abnormal air ﬂow in oral and nasal\ncavities.\n91–94 Figure 9 shows an example of a comparison of the\nwaveforms and spectrograms between normal people and\npatients with velopharyngeal insuf ﬁciency. It can be observed\nthat normal people have a more intense waveform and\ncontinuous spectrogram, while the patients ’samples are more\ndispersive and broken.\nFurthermore, the waveforms and spectrograms of different\npatients can be fed into the pretrained LLMs such as GPT-4 for\npotential disease and severity diagnosis. As shown in Fig.10, a pair\nof graphs are inputted into GPT-4 while asking for disease\ndeducing, and the model provides several answers for reference.\nAlthough the ﬁnal answers show little about the velopharyngeal\ninsufﬁciency, the output mentions muscle dysfunction. Also, a\nmore precise output can be achieved by furtherﬁne-tuning with\nmore labeled patients’ audio data. In addition, NLG can also be\nused in conjunction with speech recognition software to convert\nvoice commands into written text, such as when dictating clinical\nnotes or treatment plans.\nMRI scan\nNasal \ncartilage extraction\nMRI image stack\nInput data \nLesion area identification\nInput Image\n(resize to 1 024 × 1 024)\nConvolutional \nnetwork\nRegion\nproposal\nnetwork\nROI\nalign\n14 × 14 × 256\nBounding box\nregression\nClass \n(softmax)1 0247 × 7 × 256 1 024\n14 × 14 × 256 28 × 28 × 256 28 × 28 × 80\nMask\nSemantic segmentation \n3D reconstcution\n3D modeling\nFig. 7 2D semantic segmentation with 3D reconstruction for lesion identiﬁcation\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n7\nInternational Journal of Oral Science           (2023) 15:29 \nOTHER POTENTIAL CROSS-MODAL DEPLOYMENTS\nBiopsy\nA biopsy is a medical procedure in which a small sample of tissue\nis removed from a person ’s body to be examined under a\nmicroscope. This is typically done to diagnose or investigate\nvarious conditions, such as cancer, infections, or inﬂammation.\nVisualization techniques can be applied to biopsy to understand\nthe tissue structure and cell morphology better and identify any\nabnormalities. Common visualization approaches include light\nmicroscopy, immunohistochemistry (IHC), immuno ﬂuorescence\nInput text\nNx\nSynthetic panoramic dental X–Ray\nSynthetic image generation\nx\nx˜\nƐ\nPixel space\nZ\nZ\nDiffusion process\nLatent space\n \nzT\nzTzT–1\nQ\nKV\nQ\nKV\nQ\nKV\nQ\nKV\nDenoising U-Net /g72/g3/g84\n×(T - 1)\nTextSemantic map\nConditioning\nImagesRepres sntaions\nT/g84\nDenoising step\nQ\nKV\nCrossattention Switch Skip \nconnection Concat\n\"An alveolar cleft\"\nDiffusion \nmodel\nOutput probabilities\nPositional\nencoding\nPositional\nencoding\nMulti-head\nattention\nFeed\nforward\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nforward\nNx Nx\nInputs\nInputs\nembedding Output\nembedding\nAdd & norm\nSoftMax Linear\nAdd & norm\nAdd & norm\nAdd & norm\nAdd & norm\nFig. 8 Example of visual data generation\nAudio analysis \nexample input \n\"baba\"\n19 years old, female, normal\nWaveform Spectrogram\nSpectrogram\n20 years old, male, severe\nWaveform\nSpectrogram\n19 years old, female, normal\n0.3\n20 000\n15 000\n10 000\n5 000\n0\n20 000\n15 000\n10 000\n5 000\n0\n0.2\n0.1\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n20 000\n15 000\n10 000\n5 000\n0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n20 000\n15 000\n10 000\n5 000\n0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n–0.1\n–0.2\n–0.3\n0.3\n20 000\n15 000\n10 000\n5 000\n0\n0.2\n0.1\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2\n20 000\n15 000\n10 000\n5 000\n0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n20 000\n15 000\n10 000\n5 000\n0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n20 000\n15 000\n10 000\n5 000\n0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n–0.1\n–0.2\n–0.3\n0.3\n0.2\n0.1\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n–0.1\n–0.2\n–0.3\n0.3\n0.2\n0.1\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n–0.1\n–0.2\n–0.3\n0.3\n0.2\n0.1\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n–0.1\n–0.2\n–0.3\n0.3\n0.2\n0.1\n0.0\n–0.1\n–0.2\n–0.3\n0.3\n0.2\n0.1\n0.0\n–0.1\n–0.2\n–0.3\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n0.3\n0.2\n0.1\n0.0\n–0.1\n–0.2\n–0.3\n20 years old, male, severe\nWaveform Spectrogram\nWaveform Spectrogram\nWaveform Spectrogram\nWaveform Spectrogram\n17 years old , male, mid\n23 years old, male, severe\n17 years old, male, mid\n23 years old, male, severe\na\nb\nSpectrogram\nWaveform\nWaveform\nAudio analysis \nexample input \n\"mama\"\nFig. 9 Example of the audio waveform and spectrogram analysis in speakinga “baba” and b “mama” with TorchAudio of normal people and\npatients with velopharyngeal insufﬁciency\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n8\nInternational Journal of Oral Science           (2023) 15:29 \n(IF), confocal microscopy, etc. With the embedding generated by a\nvision transformer, the image input can be projected into the\nlanguage space and used for understanding the characteristics of\ndisease identiﬁcation. For instance, prostate cancer in biopsies\n95\nand pre-implantation kidney biopsy pathology practice96 has been\nregarded as potential application ﬁelds, which also shows the\npotential for biopsy and histological analyses in dentistry, and oral\nand maxillofacial surgery.97,98\nBlood test\nLLMs can help users understand the results of their blood tests by\nproviding information on the normal ranges for different\nbiomarkers and explaining the potential implications of high/low\nvalues. The changes in the test indicators also provide rich\ninformation about the human body condition, as well as track the\nrecovery or disease deterioration process. These changed condi-\ntions can affect the patient’s treatment planning and treatment for\ndental problems.\n99 For instance, anemia may present with low\nhemoglobin, hematocrit, and red blood cell count, while liver\ndisease may present with elevated liver enzymes (ALT, AST, and\nALP). These abnormal parameters may postpone the treatments\nlike oral and maxillofacial surgeries as most of these surgeries are\nelective. The internal relationships and connections between\nthese indicators can be well captured by LLMs, and thus the\npotential diseases can be linked with the inputted information.\nGene detection\nGene detection is the process of identifying and analyzing speciﬁc\ngenes or genetic sequences. Classic or more recent approaches can\nhelp obtain the gene sequence, including RNA sequencing, DNA\nsequencing, single-cell sequencing, etc. The genes or genetic\nsequence can be projected into language embeddings with the\ncorresponding encoder and then input to LLMs. As the sequence\ncontains the underlying logic of the gene’s property, the LLMs can\nhelp to understand these logics by learning from large gene samples\nafter training.\n100,101 Potential applications may include understanding\ngene function, identifying genetic variations or mutations, and\nstudying the relationships between genes and various biological\nprocesses or diseases, which can further inﬂuence the development\nof dental problems and treatments related to genetic disorders.\nAI SYSTEM FOR DENTISTRY APPLICATION WITH A FULLY\nAUTOMATIC MULTI-MODAL LLM\nTo demonstrate the effectiveness and potential of LLMs’applica-\ntion in dentistry, we present a framework of a fully automatic\ndiagnosis system based on Multi-Modal LLMs. The system consists\nof three input modules from different models: vision input, audio\ninput, and language input.\nThe image input could be dental X-ray, cone-beam computed\ntomography, and other medical imaging. For semantic classiﬁca-\ntion, we focus on optimizing the capture of the critical elements.\nBy applying vision-language models, the condition of the tooth is\nevaluated, potential anomaly or disease is detected, and speciﬁc\ndiagnosis and corresponding suggestions can be given.\nIn this case, audio sources have two usages: voice anomaly\ndetection and patients ’ narrative understanding. For the ﬁrst\nusage, the system receives the voice input from patients, plots\nwaveform and spectrogram, and then performs amplitude and\nfrequency analysis. For the second usage, the patients’narratives\nare collected and converted into texts using speech recognition\ntechniques. Afterward, the key elements, like the symptoms that\npatients stated, can be extracted and summarized to form reports\nor bullet points for doctors’reference.\nTargeting automatic diagnosis for dentistry, the AI system can\nbe embedded into the dental clinics ’ internal communication\nsystems. Thus, a fully developed automatic application can\nencompass patients ’ information from multiple sources and\nprovide a professional medical diagnosis, as shown in Fig.11.\nA SPECIFIC CASE FOR THE MULTI-MODAL LLM AI SYSTEM FOR\nDENTISTRY CLINICAL APPLICATION\nTo demonstrate the application of the multi-modal LLM AI system in\ndentistry, we use a sample with dental caries to explain how it works\nby LLM, including vision-language modeling and treatment planning\nThere are a variety of medical conditions that can\ncause a patient  to have a missing frequency in their\nspectrogram and low waveform amplitude. Without\nmore information about the specific  symptoms and the\npatient's medical history, it is difficult to determine the\nexact cause of these abnormalities.However, in\ngeneral, some possible conditions that may cause \nmissing frequencies and low amplitude in a \nspectrogram and waveform include:\n1.  Laryngitis: inflammation of the larynx can cause\n     hoarseness, a weak voice, and difficulty producing\n     certain frequencies.\n2. Vocal cord nodules or polyps: These non-\ncancerous growths can cause a raspy or hoarse \nvoice, and may affect the ability to produce certain\nfrequencies. \n3. Vocal cord paralysis: This condition can cause a\nweak, breathy voice and may lead to missing\nfrequencies in the spectrogram.\n4. Chronic obstructive pulmonary disease\n(COPD): COPD can cause reduced lung function,\nwhich may affect the ability to produce certain\nfrequencies.\n5. Neurological disorders: Certain neurological\nconditions can affect the muscles used for speech \nand lead to missing frequencies and low \namplitude in a spectrogram.\nPatient speak\n\"baba\"\nRegular\nspectrogram\nfor \"baba\"\nRegular\nwaveform\nof \"baba\"\n0.3\n0.2\n0.1\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n–0.1\n–0.2\n–0.3\n20 000\n0.3\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\n0.2\n0.1\n0.0\n–0.1\n–0.2\n–0.3\n15 000\n10 000\n5 000\n0\n0.0 0.2 0.4\nSpectrogram Spectrogram\nWavefrom\n0.6 0.8 1.0 1.2\n20 000\n15 000\n10 000\n5 000\n0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\nPatient’s \nspectrogram \nfor \"baba\"\nRegular \nwaveform\nof \"baba\"\n\"If a patient has \nfrequency missing from \nthe spectrogram and \nwaveform amplitude \nlow,what is the \npossible illness?\"\nQuestion input\nModel\noutput\nCompare \ngroup\nInput audio\nAudio analysis\nabnormal detection\nLarge language\nmodel\nOutput probabilities\nPositional\nencoding\nPositional\nencoding\nMulti-head\nattention\nFeed\nforward\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nforward\nNx Nx\nInputs\nInput\nembedding\nSoftMax Linear\nOutput\nembedding\nAdd \n& norm\nAdd & norm\nAdd & norm\nAdd & norm\nAdd & norm\nFig. 10 Schematic of audio-language assisted diagnosis based upon audio waveform and spectrogram analysis with TorchAudio\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n9\nInternational Journal of Oral Science           (2023) 15:29 \nwith natural language reasoning. As shown in Fig.12,a nX - r a yo ft h e\ntooth is inputted into the system, and the abnormal morphology, like\ndecay on the tooth, can be located on the X-ray by vision-language\nmodeling, and then theﬁrst question can be answered that dental\ncaries can be found on the tooth. Then the next question becomes\nwhat the treatment planning for this problem is, and using the LLM\nagain to output seven steps, including communicating with the\npatient, developing a treatment plan, discussing treatment options,\nconducting necessary dental procedures, providing oral hygiene\ninstruction, scheduling follow-upappointments, and emphasizing\npreventive measures. However, from the X-ray, we can also observe\npotential bone loss near the distal root, which is not detected by the\nsystem according to this pilot study. Thus, further study should be\ndone to improve the system.\nCommunicate with the patient: Explain the diagnosis to the\npatient, discussing the extent and severity of the dental caries. Use simple\nlanguage to ensure they understand the situation and the necessary treatment.\nDevelop a treatment plan: Create a personalized treatment\nplan based on the specific needs of the patient. Consider factors such as the\nsize and location of the caries, the patient's oral health, and any other relevant\nmedical conditions.\nDiscuss treatment options: Present the available treatment\noptions to the patient, including conservative approaches like dental fillings for\nsmaller cavities or more extensive treatments like dental crowns or root canal\ntherapy for larger or deeper cavities. Discuss the benefits, risks, and costs\nassociated with each option.\nConduct necessary dental procedures: Perform the\nchosen treatment procedure. This may involve removing the decayed tooth\nstructure and restoring the tooth with a filling material or, in severe cases,\nperforming a root canal or extracting the tooth.\nProvide oral hygiene instructions: Educate the patient\non proper oral hygiene practices, including regular brushing, flossing, and the\nuse of fluoride-based products. Emphasize the importance of maintaining good\noral health to prevent further dental caries.\nSchedule follow-up appointments: Plan regular follow-\nup visits to monitor the progress of the treated tooth and overall oral health.\nThese appointments allow for any necessary adjustments or further treatment if\nneeded.\nEmphasize preventive measures: Encourage the patient\nto adopt preventive measures to reduce the risk of future dental caries. This\nincludes maintaining a balanced diet, limiting sugary foods and drinks, and\nscheduling routine dental check-ups and cleanings.\nRemember, each patient is unique, so individualize your approach based on \ntheir specific needs and preferences. Regular communication and patient \neducation are key to successful management and prevention of dental caries.\n\"As a dentist, if \nyou diagnose \ndental caries \n(tooth decay) in \na patient, what \nshould I do?\"Image input\nVision language modeling\nLLM \ninstruction \nLLM output\nDental \ncaries\nOutput probabilities\nPositional\nencoding\nPositional\nencoding\nMulti-head\nattention\nFeed\nforward\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nforward\nNx Nx\nInputs\nInput\nembedding\nSoftMax Linear\nOutput\nembedding\nLarge language model\nAdd \n& norm\nAdd & norm\nAdd & norm Add & norm\nAdd \n& norm\nFig. 12 Application of the multi-modal LLM AI system in dental caries\nLanguage\nmodeling\nAudio\nlanguage\nmodeling\nVision\nlanguage\nmodeling\nNatural \nlanguage \nreasoning\nFrequency\nanalysis\nAmplitude \nanalysis\nSemantic\nclassification Treatment\nDiagnosis\nPatient\ninformation\nPatient answer\nSpectrogram\nWaveform\nDental cone-\nbeam computed \ntomography\nDental x-ray\nOld record\nImage\nPatient \nconvection\nInput data Data processing Data modeling Large language model (LLM) OutputData analysis\nOutput probabilities\nPositional\nencoding\nPositional\nencoding\nMulti-head\nattention\nFeed\nforward\nMasked\nmulti-head\nattention\nMulti-head\nattention\nFeed\nforward\nNx Nx\nInputs\nInput\nembedding\nSoftmax Linear\nOutput\nembedding\nAdd \n& norm\nAdd & norm\nAdd & norm\nAdd & norm\nAdd & norm\nFig. 11 Concept of automatic multi-modal LLM AI system for dentistry application\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n10\nInternational Journal of Oral Science           (2023) 15:29 \nISSUES AND LIMITATIONS\nWhile there is much excitement around the potential applications\nof LLMs in theﬁeld of dentistry, some issues and limitations must\nbe addressed before these models can be widely adopted.\nData quality\nDespite rigorous efforts to sanitize andﬁlter the vast amount of\ntraining data, it is challenging to eliminate all harmful or\ninappropriate content, which may inadvertently propagate\nthrough the responses generated by LLMs.\n102 Inherently, these\nLLMs operate as sophisticated pattern-matching machines with-\nout a genuine understanding of the data they are trained on,\nwhich occasionally leads to nonsensical or inappropriate\nresponses.\n103 Compounding these issues, they lack the capacity\nto validate the information they generate, remaining incapable of\naccessing real-time data or verifying the current status of events\npost-training. Moreover, the knowledge base of an LLM is static,\nestablished at the time of training, thus unable to update its\nknowledge or assimilate new developments in the evolving data\nlandscape. One possible solution could be to develop“Human-in-\nthe-loop system”. Pairing LLM systems with human supervisors\ncan safeguard important decisions, helping to catch and correct\nmistakes that the LLM might make\n104.\nModel bias\nBiased clinicopathologic analysis results are theﬁrst noteworthy\nissue. Because LLMs are data-driven-only models which learn the\nfeatures and patterns in the training data, the correctness of the\nLLM is highly dependent on the quality and adequacy of the\ndata.\n105 Although the LLMs are evolving iteratively, even in the era\nof GPT-4, we can’t fully trust the AI-generated clinicopathologic\nanalysis results, and human-in-the-loop validating work is still\nnecessary. In the future, neural-symbolic models, which can\ncombine two approaches (neural networks and symbolic reason-\ning) by using neural networks to learn the statistical patterns in\nlarge datasets and then using symbolic reasoning to perform\nlogical operations on the learned representations, can be a\npotential research direction.\n106\nData privacy\nThe patient data breach is another big issue, especially in today’s\nprivacy-sensitive world. Fine-tuning the LLMs in the dentistry\ndomain is expected, and a huge amount of patient data is\nnecessary. A data breach is likely to happen during this process if\nthe healthcare providers and developers don’t take appropriate\nmeasures to safeguard patient data. It’s crucial to implement strict\ndata handling protocols and use secure communication channels\nfor transmitting and storing patient data. In addition to these\nsecurity measures, it is equally essential to inform patients about\nusing their data in advance and obtain their consent.\n107 Another\npossible point of data leakage is dental diagnosis. Inputting\npatient data is necessary for these diagnosing applications, where\nthere is a risk of violating patient privacy and conﬁdentiality. One\npossible solution to address this concern is to use ofﬂine LLM such\nas META LLaMA, where the LLM is run locally on the device or\nedge server rather than on a centralized server API Call.108\nComputational cost\nComputational resources can also limit the application of LLMs in\ndentistry. It is reasonable to expect that the LLMs in the dentistry\ndomain will be operated locally due to data sensitivity. Firstly,ﬁne-\ntuning the LLMs in the dentistry domain using local computational\nresources can be challenging. Then, running a full LLM to support the\napplication in the dentistry domain is a waste of computational\nresources and unnecessary. A sparse expert model, a type of LLM\nthat incorporates a set of specialized expert models, can be a future\nsolution. It can reduce the computational resources required to train\nand run LLMs while handling speci ﬁc tasks or domains more\nefﬁciently than the main LLM.\nCONCLUSIONS\nThe utilization of language models like ChatGPT holds signiﬁcant\npotential for advancing clinical applications and research in dentistry.\nBy employing these models in a rational manner, a paradigm shift\ncan be achieved in dental diagnosis and treatment planning. Further\nexploration based on diverse medical examination data will facilitate\nthe realization of precision medicine and personalized healthcare in\ndentistry. A crucial future endeavor of practical deployment involves\nﬁne-tuning language models with dentistry domain-speciﬁck n o w l -\nedge. This entails training the models with dentistry teaching\nmaterials, patient records, and other relevant domain information,\nresulting in enhanced accuracy by capturing pertinent patterns,\nterminology, and context. Consequently, the models acquire a\nprofound comprehension of dentistry concepts, enabling them to\ngenerate contextually relevant and insightful responses. Customizing\nthe outputs in alignment with domain requirements and preferences\nenhances efﬁciency, saving valuable time and resources. These\nbeneﬁts substantially contribute to improved performance and\nusability, renderingﬁne-tuned language models invaluable tools for\nresearch paper composition. Concurrently, the adoption of LLMs will\nfurther reduce medical costs and enhance medical efﬁciency.\nAUTHOR CONTRIBUTIONS\nH.H. and O.Z. contributed to the conception and design of the work. H.H., O.Z., D.W.,\nJ.Y., Z.W., S.D., H.Y., C.X., and R.Y. performed data acquisition, analysis, and\ninterpretation. O.Z. and D.W. performed data acquisition. H.H., O.Z., and D.W.\nperformed analysis and interpretation. H.H., O.Z., D.W., J.Y., Z.W., S.D., H.Y., C.X., R.Y.,\nQ.Z., and B.S. drafted and critically revised the paper. All authors gaveﬁnal approval\nand agreed to be accountable for all aspects of the work.\nFUNDING\nThis work was supported by the Research and Development Program, West China\nHospital of Stomatology, Sichuan University (RD-02-202107), Sichuan Province\nScience and Technology Support Program (2022NSFSC0743), and Sichuan Post-\ndoctoral Science Foundation (TB2022005) grant to H. Huang.\nADDITIONAL INFORMATION\nCompeting interests: The authors declare no competing interests.\nREFERENCES\n1. Kurian, N., Cherian, J. M., Sudharson, N. A., Varghese, K. G. & Wadhwa, S. AI is\nnow everywhere. Br. Dent. J.234,7 2–72 (2023).\n2. Johnson, S. B. et al. Using ChatGPT to evaluate cancer myths and misconcep-\ntions: artiﬁcial intelligence and cancer information.JNCI Cancer Spectr.7, https://\ndoi.org/10.1093/jncics/pkad015 (2023).\n3. Mohammad-Rahimi, H. et al. Deep learning for caries detection: a systematic\nreview. J. Dent. 122, 104115 (2022).\n4. Urban, R. et al. AI-assisted CBCT data management in modern dental practice:\nbeneﬁts, limitations and innovations.Electronics 12, 1710 (2023).\n5. Revilla-León, M. et al. Artiﬁcial intelligence models for diagnosing gingivitis and\nperiodontal disease: a systematic review. J. Prosthet. Dent. https://doi.org/\n10.1016/j.prosdent.2022.01.026 (2022).\n6. Mohammad-Rahimi, H. et al. Deep learning in periodontology and oral\nimplantology: a scoping review.J. Periodontal Res.57, 942–951 (2022).\n7. Minnema, J. et al. A review on the application of deep learning for CT recon-\nstruction, bone segmentation and surgical planning in oral and maxillofacial\nsurgery. Dentomaxillofac. Radiol. 51, 20210437 (2022).\n8. He, L. et al. Automatic initial andﬁnal segmentation in cleft palate speech of\nMandarin speakers. PLoS ONE 12, e0184267 (2017).\n9. Fu, J., He, F., Yin, H. & He, L. Automatic detection of pharyngeal fricatives in cleft\npalate speech using acoustic features based on the vocal tract area spectrum.\nComput. Speech Lang.68, 101203 (2021).\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n11\nInternational Journal of Oral Science           (2023) 15:29 \n10. Thurzo, A., Strunga, M., Urban, R., Surovková, J. & Afrashtehfar, K. I. Impact of\nartiﬁcial intelligence on dental education: a review and guide for curriculum\nupdate. Educ. Sci. 13, 150 (2023).\n11. Zheng, O., Abdel-Aty, M., Wang, D., Wang, Z. & Ding, S. ChatGPT is on the\nhorizon: could a large language model be all we need for Intelligent Trans-\nportation? Preprint at arXiv:2303.05382 (2023).\n12. Schuppe, K. et al. Atypical Nelson syndrome following right partial and left total\nnephrectomy with incidental bilateral total adrenalectomy of renal cell carci-\nnoma: a chat generative pre-trained transformer (ChatGPT)-assisted case report\nand literature review.Cureus 15, e36042 (2023).\n13. Şendur, H. N.,Şendur, A. B. & Cerit, M. N. ChatGPT from radiologists’perspective.\nBr. J. Radiol.https://doi.org/10.1259/bjr.20230203 (2023).\n14. Alhaidry, H., Fatani, B., Alrayes, J., Almana, A. & Alfhaed, N. ChatGPT in den-\ntistry: a comprehensive review. Cureus https://doi.org/10.7759/cureus.38317\n(2023).\n15. Eggmann, F., Weiger, R., Zitzmann, N. U. & Blatz, M. B. Implications of large\nlanguage models such as ChatGPT for dental medicine.J. Esthet. Restor. Dent.\nhttps://doi.org/10.1111/jerd.13046 (2023).\n16. Fatani, B. ChatGPT for future medical and dental research.Cureus 15, e37285\n(2023).\n17. Damashek, M. Gauging similarity with n-grams: language-independent cate-\ngorization of text.Science 267, 843–848 (1995).\n18. Eichstaedt, J. C. et al. Facebook language predicts depression in medical records.\nProc. Natl Acad. Sci.115, 11203–11208 (2018).\n19. Maraﬁno, B. J., Davies, J. M., Bardach, N. S., Dean, M. L. & Dudley, R. A. N-gram\nsupport vector machines for scalable procedure and diagnosis classiﬁcation,\nwith applications to clinical free text data from the intensive care unit.J. Am.\nMed. Inf. Assoc.21, 871–875 (2014).\n20. Romanov, A. & Shivade, C. Lessons from natural language inference in the\nclinical domain. Preprint at arXiv:1808.06752 (2018).\n21. Choi, E., Xiao, C., Stewart, W. & Sun, J. MiME: multilevel medical embedding of\nelectronic health records for predictive healthcare.Adv. Neural Inf. Process. Syst.\n31, 19 (2018).\n22. Sarzynska-Wawer, J. et al. Detecting formal thought disorder by deep con-\ntextualized word representations.Psychiatry Res. 304, 114135 (2021).\n23. Peng, Y., Yan, S. & Lu, Z. Transfer learning in biomedical natural language\nprocessing: an evaluation of BERT and ELMo on ten benchmarking datasets.\nPreprint at arXiv:1906.05474 (2019).\n24. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: pre-training of deep\nbidirectional transformers for language understanding. Preprint at\narXiv:1810.04805 (2018).\n25. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language\nunderstanding by generative pre-training. (2018).\n26. Michalopoulos, G., Wang, Y., Kaka, H., Chen, H. & Wong, A. Umlsbert: Clinical\ndomain knowledge augmentation of contextual embeddings using the\nuniﬁed medical language system metathesaurus. Preprint at\narXiv:2010.10391 (2020).\n27. Beltagy, I., Lo, K. & Cohan, A. SciBERT: a pretrained language model for scientiﬁc\ntext. Preprint at arXiv:1903.10676 (2019).\n28. Rasmy, L., Xiang, Y., Xie, Z., Tao, C. & Zhi, D. Med-BERT: pretrained contextualized\nembeddings on large-scale structured electronic health records for disease\nprediction. NPJ Digit. Med.4, 86 (2021).\n29. Liu, W., et al. K-bert: Enabling language representation with knowledge graph.\nProceedings of the AAAI Conference on Arti ﬁcial Intelligence .V o l .3 4 .N o .0 3\n(2020).\n30. Gu, Y. et al. Domain-speciﬁc language model pretraining for biomedical natural\nlanguage processing. ACM Trans. Comput. Healthc. 3,1 –23 (2021).\n31. Devaraj, A., Marshall, I., Wallace, B. C. & Li, J. J. Paragraph-level simpliﬁcation of\nmedical texts. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, 4972–4984 (2021).\n32. Roy, A., & Pan, S. Incorporating medical knowledge in BERT for clinical relation\nextraction. InProceedings of the 2021 conference on empirical methods in natural\nlanguage processing, 5357–5366 (2021).\n33. Neumann, M., King, D., Beltagy, I. & Ammar, W. ScispaCy: fast and robust models\nfor biomedical natural language processing. Preprint at arXiv:1902.07669 (2019).\n34. Rae, J. W. et al. Scaling language models: methods, analysis & insights from\ntraining gopher. Preprint at arXiv:2112.11446 (2021).\n35. Raffel, C. et al. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. J. Mach. Learn. Res.21, 5485–5551 (2020).\n36. Ouyang, L. et al. Training language models to follow instructions with human\nfeedback. Preprint at arXiv:2203.02155 (2022).\n37. Kung, T. H. et al. Performance of ChatGPT on USMLE: potential for AI-assisted\nmedical education using large language models.PLoS Digit. Health2, e0000198\n(2023).\n38. Gilson, A. et al. How does CHATGPT perform on the United States Medical\nLicensing Examination? The implications of large language models for medical\neducation and knowledge assessment.JMIR Med. Educ.9, e45312 (2023).\n39. Li, J., Li, D., Savarese, S., & Hoi, S. Blip-2: bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. Preprint at\narXiv:2301.12597 (2023).\n40. Saharia, C. et al. Photorealistic text-to-image diffusion models with deep lan-\nguage understanding. Preprint at arXiv:2205.11487 (2022).\n41. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A. & Sutskever, I. Zero-\nshot text-to-image generation. InInternational Conference on Machine Learning,\npp. 8821–8831 (PMLR, 2021).\n42. Rombach, R., Blattmann, A., Lorenz, D., Esser, P. & Ommer, B. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 10684–10695 (2022).\n43. He, Y., Zhu, Z., Zhang, Y., Chen, Q. & Caverlee, J. Infusing disease knowledge into\nBERT for health question answering, medical inference and disease name\nrecognition. Preprint at arXiv:2010.03746 (2020).\n44. Khare, Y., Bagal, V., Mathew, M., Devi, A., Priyakumar, U. D. & Jawahar, C. V.\nMmbert: Multimodal bert pretraining for improved medical vqa. In2021 IEEE\n18th International Symposium on Biomedical Imaging (ISBI) , 1033–1036 (IEEE,\n2021).\n45. Manco, I., Benetos, E., Quinton, E. & Fazekas, G. Contrastive audio-language\nlearning for music. Preprint at arXiv:2208.12208 (2022).\n46. Li, H., Kang, Y., Liu, T., Ding, W. & Liu, Z. CTAL: Pre-training cross-modal trans-\nformer for audio-and-language representations. Preprint at arXiv:2109.00181\n(2021).\n47. Wu, F. et al. Wav2Seq: Pre-training speech-to-text encoder-decoder models\nusing pseudo languages, ICASSP 2023-2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 1–5, Rhodes Island, Greece,\n2023, https://doi.org/10.1109/ICASSP49357.2023.10096988 (2022).\n48. Gurram, S., Chan, D., Fang, A., & Canny, J. LAVA: Language Audio Vision Align-\nment for Data-Efﬁcient Video Pre-Training. In First Workshop on Pre-training:\nPerspectives, Pitfalls, and Paths Forward at ICML(2023).\n49. Radford, Alec, et al. Robust speech recognition via large-scale weak supervision.\nInternational Conference on Machine Learning(PMLR, 2023).\n50. Huh, J., Park, S., Lee, J. E. & Ye, J. C. Improving medical speech-to-text accuracy\nwith vision-language pre-training model. Preprint at arXiv:2303.00091 (2023).\n51. OpenAI. GPT-4 Technical Report. Preprint at arXiv:2303.08774 (2023).\n52. Baumgartner, C. The potential impact of ChatGPT in clinical and translational\nmedicine. Clin. Transl. Med.13, e1206 (2023).\n53. Wang, D., Zhang, S. & Wang, L. Deep epidemiological modeling by black-box\nknowledge distillation: an accurate deep learning model for COVID-19. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence. Vol. 35. No. 17 (2021).\n54. Wang, D., Gong, B. & Wang, L. On calibrating semantic segmentation models:\nanalyses and an algorithm.Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition(2023).\n55. Warin, K. et al. Maxillofacial fracture detection and classiﬁcation in computed\ntomography images using convolutional neural network-based models.Sci. Rep.\n13, 3434 (2023).\n56. Bui, R. et al. Artiﬁcial intelligence as a decision-making tool in forensic dentistry:\na pilot study with I3M.Int. J. Environ. Res. Public Health20, 4620 (2023).\n57. Cui, Z. et al. A fully automatic AI system for tooth and alveolar bone segmen-\ntation from cone-beam CT images.Nat. Commun. 13, 2096 (2022).\n58. Lee, J. et al. BioBERT: a pre-trained biomedical language representation model\nfor biomedical text mining.Bioinformatics 36, 1234–1240 (2020).\n59. Alsentzer, E. et al. Publicly available clinical BERT embeddings. Preprint at\narXiv:1904.03323 (2019).\n60. Huang, K., Altosaar, J. & Ranganath, R. Clinicalbert: modeling clinical notes and\npredicting hospital readmission. Preprint at arXiv:1904.05342 (2019).\n61. Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold.\nNature 596, 583–589 (2021).\n62. Ahdritz, G. et al. OpenFold: retraining AlphaFold2 yields new insights into its\nlearning mechanisms and capacity for generalization. Preprint atbioRxiv https://\ndoi.org/10.1101/2022.11.20.517210 (2022).\n63. Carbajosa, G., Trigo, A., Valencia, A. & Cases, I. Bionemo: molecular information\non biodegradation metabolism.Nucleic Acids Res.37, D598–D602 (2009).\n64. Sevgen, E. et al. ProT-VAE: protein transformer variational autoencoder for\nfunctional protein design. Preprint at bioRxiv https://doi.org/10.1101/\n2023.01.23.525232 (2023).\n65. Jensen, P. B., Jensen, L. J. & Brunak, S. Mining electronic health records: towards\nbetter research applications and clinical care. Nat. Rev. Genet. 13, 395–405\n(2012).\n66. Kocbek, S. et al. Text mining electronic hospital records to automatically classify\nadmissions against disease: measuring the impact of linking data sources.J.\nBiomed. Inform. 64, 158–167 (2016).\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n12\nInternational Journal of Oral Science           (2023) 15:29 \n67. Sun, W. et al. Data processing and text mining technologies on electronic\nmedical records: a review.J. Healthc. Eng.2018, 4302425 (2018).\n68. Rajkomar, A. et al. Scalable and accurate deep learning with electronic health\nrecords. NPJ Digit. Med.1, 18 (2018).\n69. Huber, M. T., Highland, J. D., Krishnamoorthi, V. R. & Tang, J. W.-Y. Utilizing the\nelectronic health record to improve advance care planning: a systematic review.\nAm. J. Hosp. Palliat. Med.35, 532–541 (2018).\n70. Luo, Y. et al. Natural language processing for EHR-based pharmacovigilance: a\nstructured review. Drug Saf. 40, 1075–1089 (2017).\n71. Hirschberg, J. & Manning, C. D. Advances in natural language processing.Science\n349, 261–266 (2015).\n72. Natsiavas, P., Maglaveras, N., & Koutkias, V. A public health surveillance platform\nexploiting free-text sources via natural language processing and linked data:\napplication in adverse drug reaction signal detection using PubMed and Twitter.\nIn Knowledge Representation for Health Care: HEC 2016 International Joint\nWorkshop, KR4HC/ProHealth 2016, 51–67, Munich, Germany (Springer Interna-\ntional Publishing, 2017).\n73. Geng, W. et al. Model-based reasoning of clinical diagnosis in integrative\nmedicine: real-world methodological study of electronic medical records and\nnatural language processing methods.JMIR Med. Inform.8, e23082 (2020).\n74. Lee, S. H. Natural language generation for electronic health records.NPJ Digit.\nMed. 1, 63 (2018).\n75. Hoogi, A., Mishra, A., Gimenez, F., Dong, J. & Rubin, D. Natural language gen-\neration model for mammography reports simulation. IEEE J. Biomed. Health\nInform. 24, 2711–2717 (2020).\n76. Wongratwanich, P. et al. Do various imaging modalities provide potential early\ndetection and diagnosis of medication-related osteonecrosis of the jaw? A\nreview. Dentomaxillofac Radiol. 50, 20200417 (2021).\n77. Alabi, R. O. et al. Machine learning in oral squamous cell carcinoma: current\nstatus, clinical concerns and prospects for future— a systematic review. Artif.\nIntell. Med. 115, 102060 (2021).\n78. Jha, N., Lee, K. S. & Kim, Y. J. Diagnosis of temporomandibular disorders using\nartiﬁcial intelligence technologies: a systematic review and meta-analysis.PLoS\nONE 17, e0272715 (2022).\n79. Monill-González, A., Rovira-Calatayud, L., d’Oliveira, N. G. & Ustrell-Torrent, J. M.\nArtiﬁcial intelligence in orthodontics: where are we now? a scoping review.\nOrthod. Craniofac Res.24,6 –15 (2021).\n80. Revilla-León, M. et al. Artiﬁcial intelligence applications in restorative dentistry: a\nsystematic review. J. Prosthet. Dent.128, 867–875 (2022).\n81. Schwendicke, F. A., Samek, W. & Krois, J. Arti ﬁcial intelligence in dentistry:\nchances and challenges.J. Dent. Res.\n99, 769–774 (2020).\n82. Amer, Y. Y. & Aqel, M. J. An efﬁcient segmentation algorithm for panoramic\ndental images. Procedia Comput. Sci.65, 718–725 (2015).\n83. Shi, B. & Huang, H. Computational technology for nasal cartilage-related clinical\nresearch and application.Int. J. Oral. Sci.12, 21 (2020).\n8 4 . H u a n g ,H . ,C h e n g ,X . ,L u o ,X . ,S h i ,B .&Li, J. Biomechanical analyses of common\nsuspension sutures in primary cleft lip rhinoplasty. Head Face Med. 15,2 0\n(2019).\n85. Huang, H. et al. Mechanical analyses of critical surgical maneuvers in the cor-\nrection of cleft lip nasal deformity.PLoS ONE 13, e0195583 (2018).\n86. Huang, H., Luo, X., Cheng, X., Shi, B. & Li, J. Biomechanical simulation of cor-\nrecting primary unilateral cleft lip nasal deformity. PLoS ONE 13, e0199964\n(2018).\n87. Huang, H. et al. Recapitulation of unilateral cleft lip nasal deformity on normal\nnasal structure: a ﬁnite element model analysis. J. Craniofac. Surg . 29(8),\n2220–2225 (2018).\n88. Sakran, K. A. et al. Early cleft palate repair by a modiﬁed technique without relaxing\nincisions. Cleft Palate Craniofac. J. https://doi.org/10.1177/10556656221135288\n(2022).\n89. Sakran, K. A. et al. Evaluation of late cleft palate repair by a modiﬁed tech-\nnique without relaxing incisions. J. Stomatol. Oral Maxillofac. Surg. 124,\n101403 (2023).\n90. Huang, H. et al. Validation of the Chinese velopharyngeal insufﬁciency effects on\nlife outcomes instrument.Laryngoscope 129, E395–E401 (2019).\n91. Huang, H. et al. Analysis of velopharyngeal functions using computationalﬂuid\ndynamics simulations. Ann. Otol. Rhinol. Laryngol.128, 742–748 (2019).\n92. Huang, H. et al. Computational ﬂuid dynamic analysis of different velophar-\nyngeal closure patterns.Ann. Otol. Rhinol. Laryngol.129, 157–163 (2019).\n93. Huang, H. et al. Airﬂow of the two-port velopharyngeal closure: study using\ncomputational ﬂuid dynamics. J. Craniofac. Surg.31, 2188–2192 (2020).\n94. Yang, C. et al. Inspiration after posterior pharyngealﬂap palatoplasty: a pre-\nliminary study using computational ﬂuid dynamic analysis. Front. Pediatr. 10,\n823777 (2022).\n95. Ström, P. et al. Artiﬁcial intelligence for diagnosis and grading of prostate cancer\nin biopsies: a population-based, diagnostic study. Lancet Oncol. 21, 222–232\n(2020).\n96. Girolami, I. et al. Artiﬁcial intelligence applications for pre-implantation kidney\nbiopsy pathology practice: a systematic review. J. Nephrol. 35, 1801–1808\n(2022).\n97. Wan, A. & Savage, N. Biopsy and diagnostic histopathology in dental practice in\nBrisbane: usage patterns and perceptions of usefulness. Aust. Dent. J. 55,\n162–169 (2010).\n98. Ilhan, B., Lin, K., Guneri, P. & Wilder-Smith, P. Improving oral cancer outcomes\nwith imaging and artiﬁcial intelligence. J. Dent. Res.99, 241–248 (2020).\n99. Miller, C. S. & Westgate, P. M. Implications of medical screenings of patients\narriving for dental treatment: the results of a comprehensive laboratory\nscreening. J. Am. Dent. Assoc.145, 1027–1035 (2014).\n100. Yu, X. et al. GPT paternity test: GPT generated text detection with GPT genetic\ninheritance. Preprint at arXiv:2305.12519 (2023).\n101. Zhang, N. et al. Ontoprotein: protein pretraining with gene ontology embed-\nding. Preprint at arXiv:2201.11147 (2022).\n102. Bubeck, S. et al. Sparks of artiﬁcial general intelligence: early experiments with\nGPT-4. Preprint at arXiv:2303.12712 (2023).\n103. Cauﬁeld, J. H. et al. Structured prompt interrogation and recursive extraction of\nsemantics (SPIRES): a method for populating knowledge bases using zero-shot\nlearning. Preprint at arXiv:2304.02711 (2023).\n104. Sakran, K. A. et al. Evaluation of postoperative outcomes in two cleft palate\nrepair techniques without relaxing incisions.Plast. Reconstr. Surg. 152, 145–154\n(2023).\n105. McNichols, H., Zhang, M. & Lan, A. Algebra error classiﬁcation with large lan-\nguage models. Preprint at arXiv:2305.06163 (2023).\n106. Lamb, L. C. et al. Graph neural networks meet neural-symbolic computing: a\nsurvey and perspective. Preprint at arXiv:2003.00330 (2020).\n107. Al Omar, A., Bhuiyan, M. Z. A., Basu, A., Kiyomoto, S. & Rahman, M. S. Privacy-\nfriendly platform for healthcare data in cloud based on blockchain environment.\nFuture Gener. Comput. Syst.95, 511–521 (2019).\n108. Sharma, S. et al. From occlusion to insight: object search in semantic shelves\nusing large language models. Preprint at arXiv:2302.12915 (2023).\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nChatGPT for shaping the future of dentistry: the potential of multi-modal. . .\nHuang et al.\n13\nInternational Journal of Oral Science           (2023) 15:29 "
}