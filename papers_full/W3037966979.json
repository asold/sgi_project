{
  "title": "Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning",
  "url": "https://openalex.org/W3037966979",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5023131292",
      "name": "Kelsey R. Allen",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5014708680",
      "name": "Kevin A. Smith",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5071093940",
      "name": "Joshua B. Tenenbaum",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2519863097",
    "https://openalex.org/W2081312879",
    "https://openalex.org/W2472385446",
    "https://openalex.org/W2149989625",
    "https://openalex.org/W2126672489",
    "https://openalex.org/W4206785111",
    "https://openalex.org/W2937206389",
    "https://openalex.org/W2805883505",
    "https://openalex.org/W1970243396",
    "https://openalex.org/W1980035368",
    "https://openalex.org/W6653548176",
    "https://openalex.org/W2059100041",
    "https://openalex.org/W6663917971",
    "https://openalex.org/W2040908702",
    "https://openalex.org/W3103057812",
    "https://openalex.org/W2408358063",
    "https://openalex.org/W2971592235",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W1990310447",
    "https://openalex.org/W2143093968",
    "https://openalex.org/W6656552592",
    "https://openalex.org/W1980169328",
    "https://openalex.org/W2149892666",
    "https://openalex.org/W2735829743",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W1533058732",
    "https://openalex.org/W2559857813",
    "https://openalex.org/W2888480348",
    "https://openalex.org/W6680464289",
    "https://openalex.org/W2073787051",
    "https://openalex.org/W2903618643",
    "https://openalex.org/W2995520132",
    "https://openalex.org/W2012587148",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2052037184",
    "https://openalex.org/W2797276369",
    "https://openalex.org/W2923504512",
    "https://openalex.org/W2173248099",
    "https://openalex.org/W2949369413",
    "https://openalex.org/W2963781688",
    "https://openalex.org/W2806859579",
    "https://openalex.org/W2963280855",
    "https://openalex.org/W2087654820",
    "https://openalex.org/W2913762814",
    "https://openalex.org/W2955035422",
    "https://openalex.org/W1972586294",
    "https://openalex.org/W2952115723",
    "https://openalex.org/W2333196491",
    "https://openalex.org/W1491843047",
    "https://openalex.org/W2903181768",
    "https://openalex.org/W2907424612",
    "https://openalex.org/W2268119109",
    "https://openalex.org/W2952915411",
    "https://openalex.org/W2798254431",
    "https://openalex.org/W2951775809",
    "https://openalex.org/W2140135625",
    "https://openalex.org/W2806280256",
    "https://openalex.org/W2954378135",
    "https://openalex.org/W2255750102",
    "https://openalex.org/W2122249653",
    "https://openalex.org/W2225340466",
    "https://openalex.org/W2796979132",
    "https://openalex.org/W2963990127",
    "https://openalex.org/W1520036242",
    "https://openalex.org/W2970806862",
    "https://openalex.org/W2950004691",
    "https://openalex.org/W2978010391",
    "https://openalex.org/W2032139276",
    "https://openalex.org/W3103780890",
    "https://openalex.org/W2105699440",
    "https://openalex.org/W2805708640",
    "https://openalex.org/W2964161785",
    "https://openalex.org/W2950845041"
  ],
  "abstract": "Many animals, and an increasing number of artificial agents, display sophisticated capabilities to perceive and manipulate objects. But human beings remain distinctive in their capacity for flexible, creative tool use—using objects in new ways to act on the world, achieve a goal, or solve a problem. To study this type of general physical problem solving, we introduce the Virtual Tools game. In this game, people solve a large range of challenging physical puzzles in just a handful of attempts. We propose that the flexibility of human physical problem solving rests on an ability to imagine the effects of hypothesized actions, while the efficiency of human search arises from rich action priors which are updated via observations of the world. We instantiate these components in the “sample, simulate, update” (SSUP) model and show that it captures human performance across 30 levels of the Virtual Tools game. More broadly, this model provides a mechanism for explaining how people condense general physical knowledge into actionable, task-specific plans to achieve flexible and efficient physical problem solving.",
  "full_text": "Rapid trial-and-error learning with simulation supports ﬂexible tool use\nand physical reasoning\nKelsey R. Allen1,2,*, Kevin A. Smith1,2,*, and Joshua B. Tenenbaum1,2\n1Department of Brain and Cognitive Sciences, MIT\n2Center for Brains, Minds and Machines (CBMM)\n*Indicates equal contribution\nMany animals, and an increasing number of artiﬁcial agents, display sophisticated capabilities\nto perceive and manipulate objects. But human beings remain distinctive in their capacity for\nﬂexible, creative tool use – using objects in new ways to act on the world, achieve a goal, or\nsolve a problem. To study this type of general physical problem solving, we introduce the\nVirtual Tools game. In this game, people solve a large range of challenging physical puzzles in\njust a handful of attempts. We propose that the ﬂexibility of human physical problem solving\nrests on an ability to imagine the eﬀects of hypothesized actions, while the eﬃciency of human\nsearch arises from rich action priors which are updated via observations of the world. We\ninstantiate these components in the “Sample, Simulate, Update” (SSUP) model and show that\nit captures human performance across 30 levels of the Virtual Tools game. More broadly, this\nmodel provides a mechanism for explaining how people condense general physical knowledge\ninto actionable, task-speciﬁc plans to achieve ﬂexible and eﬃcient physical problem-solving.\nKeywords: Intuitive Physics, Physical Problem Solving, Tool Use\nWhile trying to set up a tent on a camping trip, you realize\nthat the ground is too hard for the tent stakes, and you have\nno hammer. What would you do? You might look around\nfor a suitable hammer substitute, passing over objects like\npinecones or water bottles in favor of a graspable rock. And\nif that rock failed to drive in the stakes at ﬁrst, you might try\na diﬀerent grip, or search for a heavier rock. Most likely, you\nwould only need a handful of attempts before you found an\napproach that works.\nDetermining how to pound in tent stakes without a ham-\nmer is an example of the ﬂexibility and e ﬃciency of more\ngeneral physical problem solving. It requires a causal un-\nderstanding of how the physics of the world works, and so-\nphisticated abilities for inference and learning to construct\nplans that solve a novel problem. Consider how, when faced\nwith the tent stake challenge, we do not choose an object at\nrandom; we choose a rock because we believe we know how\nwe could use it to generate suﬃcient force on the stake. And\nCorresponding author:\nKelsey R. Allen\nDepartment of Brain and Cognitive Sciences\nMassachusetts Institute of Technology\n77 Massachusetts Avenue, Cambridge, MA 02139\nkrallen@mit.edu\nif we ﬁnd that the ﬁrst rock fails, we again search around\nfor a solution, but use the knowledge of our failures to guide\nour future search. This style of problem solving is a very\nstructured sort of trial-and-error learning: our search has el-\nements of randomness, but within a plausible solution space,\nsuch that the goal can often be reached very quickly.\nHere we study the cognitive and computational underpin-\nnings of ﬂexible tool use. While human tool use relies on a\nnumber of cognitive systems – for instance, knowing how to\ngrasp and manipulate an object, or understanding how a par-\nticular tool is typically used – here we focus on “mechanical\nreasoning,” or the ability to spontaneously repurpose objects\nin our environment to accomplish a novel goal (Goldenberg\n& Spatt, 2009; Orban & Caruana, 2014; Osiurak & Badets,\n2016).\nWe target this mechanical reasoning because it is the type\nof tool use that is quintessentially human. While other ani-\nmals can manipulate objects to achieve their aims, only a few\nspecies of birds and primates have been observed to sponta-\nneously use objects in novel ways, and we often view these\nactivities as some of the most “human-like” forms of ani-\nmal cognition (e.g., Fig. 1A,B; Shumaker, Walkup, & Beck,\n2011). Similarly, while AI systems have become increas-\ningly adept at perceiving and manipulating objects, none per-\nform the sort of rapid mechanical reasoning that people do.\nSome artiﬁcial agents learn to use tools from expert demon-\narXiv:1907.09620v3  [cs.AI]  29 Jun 2020\n2 ALLEN, SMITH, TENENBAUM\nD (i) (ii) (iii)\nC\nA\n B\nFigure 1. Examples of using objects to achieve a goal. (A)\nBearded capuchin monkey opening a cashew nut with an ap-\npropriately sized stone (photo by Tiago Falótico; Luncz et\nal., 2016). (B) New Caledonian crow using heavy blocks\nto raise the water level in a tube in order to retrieve food\n(Jelbert, Taylor, Cheke, Clayton, & Gray, 2014). (C) Tod-\ndler using a shovel to reach a ball (from youtu.be/hwrNQ93-\n568?t=198). (D) One illustrative trial in the Virtual Tools\ngame (https: //sites.google.com/view/virtualtoolsgame). (i)\nThe player must get the red object into the green goal us-\ning one of the three tools. (ii) The player chooses a tool and\nwhere to place it. (iii) Physics is turned “on” and the tool\ninteracts with other objects. The action results in a near miss.\nstrations (Xie, Ebert, Levine, & Finn, 2019), which limits\ntheir ﬂexibility. Others learn from thousands of years of sim-\nulated experience (Baker et al., 2019), which is signiﬁcantly\nlonger than required for people. Still others can reason about\nmechanical functions of arbitrary objects but require perfect\nphysical knowledge of the environment (Toussaint, Allen,\nSmith, & Tenenbaum, 2018), which is unavailable in real-\nworld scenarios. In contrast, even young humans are capable\ntool users: by the age of four they can quickly choose an ap-\npropriate object and determine how to use it to solve a novel\ntask (e.g., picking a hooked rather than straight pipe cleaner\nto retreive an object from a narrow tube, Fig. 1C; Beck, Ap-\nperly, Chappell, Guthrie, & Cutting, 2011).\nWhat are the cognitive systems that let us use tools so\nﬂexibly, and accomplish our goals so rapidly? It has been\nsuggested that mechanical reasoning relies on mental simula-\ntion, which lets us predict how our actions will cause changes\nin the world (Osiurak & Badets, 2016). This general pur-\npose simulation is a necessary component that supports our\nability to reason about objects in novel environments, but by\nitself cannot explain how we make and update our plans so\nquickly. We propose that another key to rapid tool use is\nknowing what sorts of actions to even consider – both from\nan initial understanding of what actions are useful, and by\nupdating this belief from observing the outcome of our ac-\ntions, in simulation and in reality.\nThis paper makes two contributions. First, we introduce\nthe Virtual Tools game, which presents a suite of physical\nproblem solving challenges, and allows for precise, quan-\ntiﬁable comparisons between human and machine agents.\nSecond, we present a minimal model of ﬂexible tool use,\ncalled “Sample, Simulate, Update” (SSUP). This model is\nbuilt around an e ﬃcient albeit noisy simulation engine that\nallows the model to actﬂexibly across a wide variety of phys-\nical tasks. To solve problems rapidly, the SSUP model con-\ntains rich knowledge about the world in the form of a struc-\ntured prior on candidate tools and actions likely to solve the\nproblem, which allows it to limit its simulations to promising\ncandidates. It further learns from its simulations and from\nobserving the outcome of its own actions to update its beliefs\nabout what those promising candidates should be. Across\n30 Virtual Tools levels in two experiments, we show that an\ninstantiation of the SSUP model captures the relative di ﬃ-\nculties of di ﬀerent levels for human players, the particular\nactions performed to attempt to solve each level, and how\nthe solution rates for each level evolve.\nThe Virtual Tools game\nInspired by human tool use, as well as mobile\nphysics games ( Brain it On , 2015), we propose the\nVirtual Tools game as a platform for investigating\nthe priors, representations, and planning and learn-\ning algorithms used in physical problem solving\n(https://sites.google.com/view/virtualtoolsgame). This\ngame asks players to place one of several objects (“tools”)\ninto a two-dimensional dynamic physical environment in\norder to achieve a goal: getting a red object into a green\nregion (Fig. 1D). This goal is the same for every level, but\nwhat is required to achieve it varies greatly. Once a single\ntool is placed, the physics of the world is enabled so that\nplayers see the e ﬀect of the action they took. If the goal is\nnot achieved, players can “reset” the world to its original\nstate and try again; they are limited to a single action on\neach attempt. We designed 30 levels – 20 for the original\nexperiment (Fig. 2) and 10 for a validation experiment\n(Fig. 7A) – to test concepts such as ‘launching’, ‘blocking’,\nand ‘supporting’. Of the ﬁrst 20 levels, 12 were constructed\nin six ‘matched pairs’ which incorporated small diﬀerences\nin the goals or objects in the scene to test whether subtle\ndiﬀerences in stimuli would lead to observable di ﬀerences\nin behavior.\nThe Virtual Tools game presents particular challenges that\nwe believe underlie the kinds of reasoning required for rapid\nphysical problem solving more generally. First, there is a di-\nversity of tasksthat require diﬀerent strategies and physical\nconcepts to solve, but employ shared physical dynamics that\napproximate the real world. Second, the game requires long-\nhorizon causal reasoning. Since players can only interact\nwith the game by placing a single object, they must be able\nto reason about the complex cause and e ﬀect relationships\nRAPID TRIAL-AND-ERROR LEARNING IN PHYSICAL REASONING 3\n1. Basic 2. Bridge 3. Catapult 4. Chaining 5. Gap 6. SeeSaw 7. Unbox\n8. Unsupport\n 11. Launch (A) 12. Launch (B) 13. Prevention (A) 14. Prevention (B)9. Falling (A) 10. Falling (B)\n15. Shafts (A) 16. Shafts (B) 17. Table (A) 18. Table (B) 19. Towers (A) 20. Towers (B)\nFigure 2. Twenty levels used in the Virtual Tools game. Players choose one of three tools (shown to the right of each level) to\nplace in the scene in order to get a red object into the green goal area. Black objects are ﬁxed, while blue objects also move;\ngrey regions are prohibited for tool placement. Levels denoted with A/B labels are matched pairs.\nof their action long into the future when they can no longer\nintervene. Finally, the game elicits rapid trial-and-error\nlearning in humans. Human players do not generally solve\nlevels on their ﬁrst attempt, but also generally do not require\nmore than 5-10 attempts in order to succeed. People demon-\nstrate a wide range of problem-solving behaviors, including\n“a-ha” insights where they suddenly discover the right idea\nfor how to solve a particular task, as well as incremental trial-\nand-error strategy reﬁnement. Figure 3 demonstrates how\nthis occurs in practice, showing four di ﬀerent examples of\nparticipants learning rapidly or slowly, and discovering dif-\nferent ways to use the tools across a variety of levels.\nSample, Simulate, UpdateModel (SSUP)\nWe consider the components required to capture both the\nﬂexibility and eﬃciency of human tool use. We propose that\npeople achieve ﬂexibility through an internal mental model\nthat allows them to imagine the e ﬀects of actions they may\nhave never tried before (“Simulate”). However, a mental\nmodel alone is not su ﬃcient – there are far too many pos-\nsible actions that could be simulated, many of which are un-\ninformative and unlikely to achieve a speciﬁc goal. Some\nmechanism for guiding an internal search is necessary to fo-\ncus on useful parts of the hypothesis space. We therefore\npropose people use structured, object-oriented priors (“Sam-\nple”) and a rapid belief updating mechanism (“Update”) to\nguide search towards promising hypotheses. We formalize\nhuman tool use with these components as the “Sample, Sim-\nulate, Update” model (SSUP; Fig. 4A).\nSSUP is inspired by the theory of “problem solving as\nsearch” (Newell & Simon, 1972), as well as Dyna and other\nmodel-based policy optimization methods (Deisenroth, Neu-\nmann, Peters, et al., 2013; Sutton, 1991). Crucially, we posit\nthat structured priors and physical simulators must already\nbe in place in order to solve problems as rapidly as people;\nthus unlike most model-based policy optimization methods,\nwe do not perform online updates of the dynamics model.\nWe want to emphasize that we view SSUP as a general\nmodeling framework for physical problem solving, and only\npresent here one instance of that framework: the minimal\nmodel (described below, with more detail in SI Appendix,\nSection S2) that we think is needed to capture basic human\nbehavior in the Virtual Tools game. In the discussion we\nhighlight ways the model will need to be improved in future\nwork, as well as aspects of physical reasoning that rely on a\nricher set of cognitive systems going beyond the framework\npresented here.\nSample: object-based prior. At a minimum, the actions\nwe should consider to achieve any goal should have the po-\ntential to impact our environment. We therefore incorporate\nan object-based prior for sampling actions. Speciﬁcally, the\nmodel selects one of the movable objects in the scene, then\nchooses an x-coordinate in an area that extends slightly be-\nyond the width of the object, and a y-coordinate either above\nor below that object (Fig. 4B: Prior). For tool choice, we\nassume participants are equally likely to choose any of the\nthree tools since all tools in the game were designed to be\nunfamiliar to participants. Samples from this distribution are\nused to initialize search.\nSimulate: a noisy physics engine.In order to determine\nwhich sampled actions are worth trying in the world, we as-\nsume people use an “Intuitive Physics Engine” (Battaglia,\nHamrick, & Tenenbaum, 2013) to ﬂexibly imagine the eﬀects\nof their actions. This engine is able to simulate the world\nforwards in time with approximately correct but stochastic\ndynamics (Sanborn, Mansinghka, & Gri ﬃths, 2013; Smith\n& Vul, 2013). Determining the e ﬀect of a proposed action\ntherefore involves applying that action to one’s mental rep-\nresentation, and using the Intuitive Physics Engine to posit\n4 ALLEN, SMITH, TENENBAUM\nA - Rapid Learning\nC - Discovering Effective Use of a Tool\nD - Support Principle Discovery and Fine Tuning\nB - Strategy Change\nFigure 3. Examples of participants’ behavior on three levels, representative of rapid trial-and-error learning: Initial plans are\nstructured around objects, followed by exploring to identify more promising strategies and then reﬁning actions until success.\nObjects start as shown by light blue /red outlines and follow paths traced out by colored lines. Possible tool choices shown\nto the right. (A) In the Catapult level, a useful strategy is often identiﬁed immediately and rapidly ﬁne-tuned. (B) Other\nparticipants ﬁrst try an unsuccessful strategy but then switch to a more viable strategy and reﬁne it. (C) The Launch (B) level\nis designed to prevent obvious solutions. This participant may have initially believed the ball would start rolling and attempted\nto use a tool as a bridge. When this failed they realized they needed to launch the ball, but only discovered after several trials\nhow to use a tool in a non-obvious way to accomplish this, via a hooking motion around the blocking ledge. They then took\nseveral more trials to ﬁne-tune this action. (D) In the SeeSaw level, a participant realized on the second attempt they must\nsupport the platform for the ball to roll across, then tried diﬀerent ways of making this happen.\nBA\nC\nSimulate\nUpdate\nAction\nBelief Color Key\nIn the mind In the world\nUpdate beliefs\nObserve \noutcome\nChoose \naction\n...\n ...\nAction 1\nSimulations\nAction 2 Action 3\nSimulations\nSample\nAlgorithm 1 SSUP algorithm\nSample actions from priora ∼ π(s)\nSimulateaction to get noisy rewards ˆr ∼ moπel(s, a)\nInitialize distributionπ′(s) using samples ˆr, a\nwhilenot successfuldo\nSample actiona\nSimulateaction to estimate noisy reward ˆr ∼ moπel(s, a)\nifˆr> thresholdthen\nTry actiona in environment\nObserve r\nIf successful, exit\nUpdate policy withr, s, a.\nelse\nUpdate policy with ˆr ,s ,a\nend if\nend while\n1\nFigure 4. (A) The SSUP algorithm. (B) A diagram of the model for the Virtual Tools game. It incorporates an object-based\nprior, a simulation engine for ﬁltering proposals, and an update module that suggests new proposals based on observations “in\nthe mind” and from actions taken in the world. (C) Illustration of the policy π′ evolving while attempting a level. Colored\npatches represent the Gaussian policy for each tool.\nRAPID TRIAL-AND-ERROR LEARNING IN PHYSICAL REASONING 5\nthe range of ways that action might cause the world to un-\nfold (Craik, 1943; Dasgupta, Smith, Schulz, Tenenbaum, &\nGershman, 2018). Here we implement simulation using a\ngame physics engine with noisy dynamics. People charac-\nteristically have noisy predictions of how collisions will re-\nsolve (Smith & Vul, 2013), and so for simplicity we assume\nuncertainty about outcomes is driven only by noise in those\ncollisions (the direction and amount of force that is applied\nbetween two colliding objects).1\nSince the internal model is imperfect, to evaluate an ac-\ntion we produce a small number of stochastic simulations\n(nsims, set here at 4) to form a set of hypotheses about the\noutcome. To formalize how good an outcome is (the reward\nof a given action), we borrow an idea from the causal rea-\nsoning literature for how people conceptualize “almost” suc-\nceeding (Gerstenberg, Goodman, Lagnado, & Tenenbaum,\n2015). “Almost” succeeding is not a function of the absolute\ndistance an action moved you towards your goal, but instead\nhow much of a diﬀerence that action made. To capture this,\nthe minimum distance between the green goal area and any\nof the red goal objects is recorded; these values are aver-\naged across the simulations and normalized by the minimum\ndistance that would have been achieved if no tool had been\nadded. The reward used in SSUP is 1 minus the normalized\ndistance, so that closer objects lead to higher reward.\nOnce the model ﬁnds a good enough action (formalized\nas the average reward being above some threshold), it takes\nthat action “in the world.” Additionally, to model time limits\nfor thinking, if the model considers more thanT diﬀerent ac-\ntion proposals without acting (set here at 5), it takes the best\naction it has imagined so far. We evaluate the e ﬀect of all\nparameter choices in a sensitivity analysis (see SI Appendix,\nFig. S1).\nUpdate: learning from thoughts and actions.So far we\nhave described a way of intelligently initializing search to\navoid considering actions that will not be useful. But what if\nthe prior still presents an intractably large space of possible\nactions?\nTo tackle this, we incorporate an update mechanism that\nlearns from both simulated and real experience to guide fu-\nture search towards more promising regions of the hypothesis\nspace (Juechems & Summerﬁeld, 2019). This is formally de-\nﬁned as a Gaussian mixture model policy over the three tools\nand their positions, π′(s), which represents the model’s belief\nabout high value actions for each tool.π′(s) is initialized with\nsamples from the object-oriented prior, and updated using a\nsimple policy gradient algorithm (Williams, 1992). This al-\ngorithm will shape the posterior beliefs around areas to place\neach tool which are expected to move target objects close to\nthe goal, and are therefore likely to contain a solution. Such\nan update strategy is useful when it ﬁnds high value actions\nthat are nearby successful actions, but may also get stuck in\nlocal optima where a successful action does not exist. We\ntherefore use a standard technique from reinforcement learn-\ning: epsilon-greedy exploration. With epsilon-greedy explo-\nration, potential actions are sampled from the policy 1 - ϵ%\nof the time, and from the prior ϵ% of the time. Note that this\nexploration is only used for proposing internal simulations;\nmodel actions are chosen based on the set of prior simulation\noutcomes. This is akin to thinking of something new, instead\nof focusing on an existing strategy.\nResults\nWe analyze human performance on the ﬁrst 20 levels of\nthe Virtual Tools game and compare humans to the SSUP\nmodel and alternates, including SSUP models with ablations\nand two alternate learning baselines. We show that the full\nSSUP model best captures human performance. Access to\nthe game and all data including human and model place-\nments is provided at https://sites.google.com/view/\nvirtualtoolsgame.\nHuman results\nExperiments were approved by the MIT Committee on\nthe Use of Humans as Experimental Subjects under proto-\ncol #0812003014. Participants were notiﬁed of their rights\nbefore the experiment, were free to terminate participation at\nany time by closing the browser window, and were compen-\nsated monetarily for their time.\nWe recruited 94 participants through Amazon Mechanical\nTurk and asked each participant to solve 14 levels: all 8 of the\nunmatched levels, and one variation of each of the 6 matched\npairs (randomly selected).\nParticipants could choose to move on once a problem was\nsolved, or after two minutes had passed. See SI Appendix,\nSection S1 for further details.\nThe variation in diﬃculty between levels of the game was\nsubstantial. Participants showed an average solution rate of\n81% (sd = 19%), with the range covering 31% for the hard-\nest level to 100% for the easiest. Similarly, participants took\nan average of 4.5 actions (sd = 2.5) for each level, with a\nrange from 1.5 to 9.4 average attempts. Even within trials,\nthere was a large amount of heterogeneity in the number of\nactions participants used to solve the level. This would be\nexpected with “rapid trial-and-error” learning: participants\nwho initially tried a promising action would solve the puzzle\nquickly, while others explored di ﬀerent actions before hap-\npening on promising ones (e.g., Fig. 3).\nBehavior diﬀered across all six matched level pairs. We\nstudy whether these subtle di ﬀerences do indeed a ﬀect be-\nhavior, even without feedback on the ﬁrst action, by asking\nwhether we can identify which level variant each action came\n1We also considered models with additional sources of physics\nmodel uncertainty added, but found that the additional parameters\ndid not improve model ﬁt, so we do not analyze those models here.\n6 ALLEN, SMITH, TENENBAUM\n1. Basic\n6. SeeSaw\n11. Launch (A)\n16. Shafts (B)\n2.Bridge\n7. Unbox\n12. Launch (B)\n17. Table (A)\n3. Catapult\n8. Unsupport\n13. Prevention (A)\n18. Table (B)\n4. Chaining\n9. Falling (A)\n14. Prevention (B)\n19. Towers (A)\n5. Gap\n10. Falling (B)\n15. Shafts (A)\n20. Towers (B)\nA B C\nFigure 5. (A) Comparison of average number of human participants’ attempts for each level with average number of attempts\nfor the SSUP model. Bars indicate 95% conﬁdence intervals on estimates of the means.(B) Comparison of human participants’\naccuracy on each trial versus the accuracy of the SSUP model.(C) Comparison of human participants’ accuracy to all alternate\nmodels. Numbers correspond to the trials in Fig. 2.\nfrom. We ﬁnd these actions are diﬀerentiable across matched\nlevels in ‘Shafts’, ‘Prevention’, ‘Launch’ and ‘Table’ on the\nﬁrst attempt, but not ‘Falling’ or ‘Towers’ (see SI Appendix,\nFig. S11 and Sec. S6A for details). However, participants re-\nquired a diﬀerent number of actions to solve every level (all\nts > 2.7, ps < 0.01). This suggests that people are paying\nattention to subtle diﬀerences in the scene or goal to choose\ntheir actions.\nModel results\nWe investigate several metrics for comparing the models\nto human data. First, we look at how quickly and how of-\nten each model solves each level, and whether that matches\nparticipants. This is measured as the correlation and root\nmean squared error (RMSE) between the average number of\nparticipant attempts for each level and the average number of\nmodel attempts for each level, and the correlation and RMSE\nbetween human and model solution rates. The SSUP model\nexplains the patterns of human behavior across the di ﬀerent\nlevels well (SI Appendix, Table S2). It uses a similar number\nof attempts on each level ( r = 0.71; 95% CI = [0.62,0.76];\nmean empirical attempts across all levels: 4.48, mean model\nattempts: 4 .24; Fig. 5A) and achieves similar accuracy ( r =\n0.86; 95% CI = [0.76,0.89]; Fig. 5B).\nAcross many levels, the SSUP model not only achieves\nthe same overall solution rate as people, but approaches it\nat the same rate. We measure this by looking at cumulative\nsolution rates – over all participants or model runs, what pro-\nportion solved each level withinX placements – and ﬁnd that\npeople and the model often demonstrate similar solution pro-\nﬁles (Fig. 6A; see SI Appendix, Section S6B for quantitative\ncomparison).\nWe can look in more detail how the model accomplishes\nthis by comparing both the ﬁrst actions that people and the\nmodel takes (Fig. 6B), and the actions that both take to solve\na level (Fig. 6C). Like our human participants, the model\ntakes signiﬁcantly di ﬀerent actions on the ﬁrst attempt be-\ntween matched level pairs (see SI Appendix, Sec. S6A).\nMore generally, both people and the model will often begin\nwith a variety of plausible actions (e.g., Catapult). In some\ncases, both will attempt initial actions that have very little\nimpact on the scene (e.g., SeeSaw and Prevention (B)); this\ncould be because people cannot think of any useful actions\nand so decide to try something, similar to how the model\ncan exceed its simulation threshold. However, in other cases,\nthe model’s initial predictions diverge from people, and this\nleads to a di ﬀerent pattern of search and solutions. For in-\nstance, in Falling (A), the model quickly ﬁnds that placing an\nobject under the container will reliably tip the ball onto the\nground, but people are biased to drop an object from above.\nBecause of this, the model often rapidly solves the level with\nan object below, whereas a proportion of participants ﬁnd a\nway to ﬂip the container from above; this discrepancy can\nalso be seen in the comparison of number of attempts before\nthe solution, where the model ﬁnds a solution quickly, while\npeople take a good deal longer (Fig. 5A). For comparisons\nof the ﬁrst and last actions across all levels, see SI Appendix,\nFig. S11.\nModel comparisons on Virtual Tools.We compare the\nfull SSUP model against a set of six alternate models. Three\nmodels investigate the contribution of each SSUP component\nby removing the prior, simulation, or updating individually.\nTwo models propose alternate solution methods: learning\nbetter world models rather than learning over actions (Pa-\nrameter Tuning) or replacing the prior and simulator with a\nlearned proposal mechanism (DQN + Updating). The Pa-\nrameter Tuning alternate model uses inference to learn ob-\nject densities, frictions and elasticities from observed trajec-\ntories. The learned proposal mechanism corresponds to a\nmodel-free deep reinforcement learning agent (Mnih et al.,\nRAPID TRIAL-AND-ERROR LEARNING IN PHYSICAL REASONING 7\nLast Action\n3. Catapult\n14. Prevention (B)\n9. Falling (A)\nFirst Action\n6. SeeSaw\nBA\nFigure 6. (A) Cumulative solution rate over number of placements for participants vs. the SSUP model. (B) Distribution of\nmodel actions (background) versus human actions (points) on the ﬁrst and last attempts of the level for a selection of four\nlevels. The distribution of model actions is estimated based on ﬁtting a Kernel Density Estimate to the actions taken by the\nmodel across 250 simulations. Colors indicate the tool used, with the tools and associated colors shown to the right of each\nlevel. In most levels, the SSUP model captures the evolution of participants’ solutions well, including the particular actions\nchosen; in the few cases that it diﬀers, there is no alternative model that systematically explains these diﬀerences.\n2015) which is trained on a set of 4500 randomly generated\nlevels of the game (see SI Appendix, Sec. S5), and then up-\ndated online for each of the 20 testing levels using the same\nmechanism as SSUP. This model has substantially more ex-\nperience with the environment than other models, and serves\nas a test of whether model-free methods can make use of\nthis experience to learn generalizable policies that can guide\nrapid learning. Finally, we compare to a “Guessing” base-\nline for performance if an agent were to simply place tools\nrandomly. See Fig. 5C and SI Appendix, Table S2 for these\ncomparisons.\nEliminating any of the three SSUP components causes a\nsigniﬁcant decrease in performance (measured as deviation\nbetween empirical and model cumulative solution curves; all\nbootstrapped ps < 0.0001; see SI Appendix, Sec. S6B,\nFig. S6 for further detail). The reduced models typically\nrequire more attempts to solve levels because they are either\nsearching in the wrong area of the action space (No Prior), at-\ntempting actions that have no chance of being successful (No\nSimulation), or do not guide search towards more promising\nareas (No Updating).\nDQN + Updating performs worst of all plausible alter-\nnate models, using the most actions and solving levels at a\nrate barely over chance. Because this is equivalent to the\nNo Simulation model with a di ﬀerent prior, its poor perfor-\nmance suggests that generalized action policies cannot eas-\nily be learned from repeatedly playing similar levels (see SI\nAppendix, Sec. S5).\nBecause the Parameter Tuning model is equivalent to the\nNo Updating model except that the properties of the dynam-\nics model can be learned in Parameter Tuning, comparing\nthose two models allows us to test whether we need to as-\nsume that people are learning the dynamics of the world in\nthis game. The fact that both models perform roughly equiv-\nalently (see Fig. 5C) suggests that we do not need this as-\nsumption here.\nFinally, we quantiﬁed how well each model captured the\nparticular actions people took. Due to heterogeneity in par-\nticipants’ responses, we were unable to cleanly diﬀerentiate\nmodels’ performance except to ﬁnd that the DQN + Updat-\ning model underperformed the rest (see SI Appendix, Sec.\nS6C). However, no model reached the theoretical noise ceil-\ning, suggesting components of the SSUP framework could\nbe improved to better explain participants’ actions (see the\nDiscussion).\nValidation on novel levels\nWe conducted a second experiment to test whether the\nmodels generalize to novel levels and physical concepts with-\nout tuning hyperparameters. For this experiment, we created\n10 new levels: 6 novel level types and 4 variants of the orig-\ninals (Fig 7A), testing an independent sample of 50 partici-\n8 ALLEN, SMITH, TENENBAUM\nA\nB\nC\nD\nA1. Balance\nA6. Trap\nA2. Collapse\nA7. Basic (v2)\nA3. Remove\nA8. Falling (v2)\nA4. Shove\nA9. Launch (v2)\nA5. Spiky\nA10. Table (v2)\nFigure 7. Results on 10 additional trials. (A) Trials used for the second experiment. (B) The cumulative solution rate for\nparticipants and the SSUP model. (C) Comparison of the number of human and model actions by trial. (D) Comparison of\nhuman and model accuracy on each trial.\npants on all levels. The 6 novel level types were designed to\ntest new physical strategies, including balancing, breaking,\nand removing objects from a ball’s path. All other experi-\nmental details were identical to the main experiment.\nWithout tuning any model parameters, we ﬁnd a good\ncorrespondence between human and model solution rates\n(Fig. 7B), and a strong correlation between the model’s per-\nformance and human performance across number of place-\nments (Fig. 7C, r = 0.85) and accuracy (Fig. 7D, r = 0.95).\nSimilar to the main experiment, we ﬁnd a decrement in per-\nformance if the prior or simulation are removed, or for the\nDQN + Updating model (all bootstrapped ps < 0.0001; SI\nAppendix, Fig. S7). However, while numerically worse,\nwe do not ﬁnd a reliable di ﬀerence if the update mecha-\nnism is removed (p = 0.055) or swapped for model learning\n(p = 0.346), suggesting that the particular reward function\nor update procedure might be less applicable to these levels\n(see SI Appendix, Sec. S6B).\nDiscussion\nWe introduce the Virtual Tools game for investigating\nﬂexible physical problem solving in humans and machines,\nand show that human behavior on this challenge expresses\na wide variety of trial-and-error problem solving strategies.\nWe also introduce a model for human physical problem solv-\ning: “Sample, Simulate, Update.” The model presumes that\nto solve these physics problems, people rely on an inter-\nnal model of how the world works. Learning in this game\ntherefore involves condensing this vast world knowledge to\nrapidly learn how to act in each instance, using a structured\ntrial-and-error search.\nModel limitations\nAlthough the SSUP model we used solves many of the\nlevels of the Virtual Tools game in a human-like way, we\nbelieve that this is still only a ﬁrst approximation to the rich\nset of cognitive processes that people bring to the task. In\nparticular, there are at least two ways in which the model is\ninsuﬃcient: its reliance on very simple priors, and its plan-\nning and generalizing only in the forwards direction.\nWe can see the limits of the object-based prior in the\nFalling (A) level (Fig. 5B): people are much less likely to\nconsider placing an object underneath the container to tip\nit over. Instead, many people try to tip it over from above,\neven though this is more di ﬃcult. In this way, people’s pri-\nors over strategies are context speciﬁc, which causes them\nto be slower than the model in this level. In other cases, this\ncontext speciﬁcity is helpful: for instance, in the hypothetical\nlevel shown in Fig. 8A, there is a hole that one of the tools ﬁts\nsuspiciously perfectly into. Many people notice this coinci-\ndence quickly, but because the model cannot assess how tools\nmight ﬁt into the environment without running a simulation,\nRAPID TRIAL-AND-ERROR LEARNING IN PHYSICAL REASONING 9\nit only succeeds 10% of the time. In future work, a more\ncomplex prior could be instantiated in the SSUP framework,\nbut it remains an open question how people might form these\ncontext-speciﬁc priors, or how they might be shaped over\ntime via experience.\nPeople show greater ﬂexibility than our model in the abil-\nity to work backwards from the goal state to ﬁnd more eas-\nily solvable sub-goals (Anderson, 1993). In the hypotheti-\ncal level in Fig. 8B, the catapult is ﬁnicky, which means that\nmost catapulting actions will not make it over the barrier, and\ntherefore will never hit the ball on the left. Instead, the easi-\nest way to increase the objective function is by the incorrect\nstrategy of knocking the ball on the right to get close to the\ngoal, and therefore the model only solves the level 8% of the\ntime. Working backwards to set the ﬁrst sub-goal of launch-\ning the ball over the barrier would prevent getting stuck with\nknocking the ball as a local minimum. From an engineering\nstandpoint, creating sub-goals is natural with discrete prob-\nlem spaces (Newell & Simon, 1972), but it is less clear how\nthese might be discovered in the continuous action space of\nthe Virtual Tools game.\nRelated cognitive systems\nThere is an extensive body of research into the cognitive\nsystems that underlie the use of real-world tools, including\nunderstanding how to manipulate them and knowing their\ntypical uses (e.g. Beck et al., 2011; Orban & Caruana, 2014;\nOsiurak & Badets, 2016; Vaesen, 2012). Here our focus was\non “mechanical knowledge” of tools: how to use objects\nin novel situations. However, in real-world tool use, these\nsystems work together with motor planning and semantic\nknowledge of tools. Future work can focus on these links,\nsuch as how novel tools become familiar, or how our motor\nlimits constrain the plans we might consider.\nThe Virtual Tools game presents a problem solving task\nthat blends facets of prior work, but encompasses a novel\nchallenge. To rapidly solve these problems requires good\nprior knowledge of the dynamics – unlike Complex Problem\nSolving in which the dynamics are learned in an evolving sit-\nuation (Frensch & Funke, 1995) – and further iteration once a\npromising solution is considered – unlike the ‘a-ha’ moment\nthat leads immediately to a solution in Insight Problem Solv-\ning (Chu & MacGregor, 2011; Gick & Holyoak, 1980). Un-\nlike in traditional model-based or model-free reinforcement\nlearning, in this task people bring rich models of the world\nthat they can quickly tailor to speciﬁc, novel problems.\nDistilling rich world knowledge to useful task knowledge\nis necessary for any agent interacting with a complex world.\nOne proposal for how this occurs is “learning by thinking”\n(Lombrozo, 2018): translating knowledge from one source\n(internal models of physics) to another, more speciﬁc instan-\ntiation (a mapping between actions and outcomes onthis par-\nticular level). We show how SSUP instantiates one example\nA B\nFigure 8. Two problems that demonstrate limitations of the\ncurrent model. (A) A “suspicious conicidence” that one tool\nﬁts perfectly in the hole. (B) Creating a ‘sub-goal’ to launch\nthe ball onto the other side is useful.\nof “learning by thinking”: by training a policy with data from\nan internal model. Evidence for this sort of knowledge trans-\nfer has been found in people (Gershman, Markman, & Otto,\n2014; Gershman, Zhou, & Kommers, 2017), but has focused\non simpler discrete settings in which the model and policy\nare jointly learned.\nVirtual Tools as an AI Challenge\nIn preliminary experiments with model-free reinforce-\nment learning approaches (Mnih et al., 2015), we found lim-\nited generalization with ineﬃcient learning across almost all\nof the Virtual Tools levels (see SI Appendix, Section S5) de-\nspite signiﬁcant experience with related levels.\nBased on our human experiments, we believe that model-\nbased approaches will be required to be able to play games\nlike Virtual Tools. Such approaches are becoming increas-\ningly popular in machine learning (Weber et al., 2017), es-\npecially when combined with “learning-to-learn” techniques\nthat can learn to adapt quickly to new tasks (Finn, Abbeel,\n& Levine, 2017; Schmidhuber, Zhao, & Schraudolph, 1998).\nLearning these models remains challenging, but approaches\nthat incorporate added structure have excelled in recent\nyears (Chang, Ullman, Torralba, & Tenenbaum, 2016; Ja-\nyaraman, Ebert, Efros, & Levine, 2018). Within the AI\nand robotics communities, model-based methods are already\npopular (Garcia, Prett, & Morari, 1989; Kaelbling & Lozano-\nPérez, 2010; Toussaint et al., 2018). Remaining challenges\ninclude how to learn accurate enough models that can be used\nwith raw sensor data (Kroemer, Niekum, & Konidaris, 2019),\nand how to handle dynamic environments.\nVirtual Tools adds to a growing set of environments that\ntest artiﬁcial agents’ abilities to predict and reason using\nphysics, such as the concurrently developed PHYRE bench-\nmark (Bakhtin, van der Maaten, Johnson, Gustafson, & Gir-\nshick, 2019) and others (Bapst et al., 2019; Ge, Lee, Renz, &\nZhang, 2016; Wenke, Saunders, Qiu, & Fleming, 2019). In\ncontrast, our focus is on providing problems that people ﬁnd\nchallenging but intuitive, where solutions are non-obvious\nand do not rely on precise knowledge of world dynamics. By\ncontributing human data to compare artiﬁcial and biological\nintelligence, we hope to provide a test-bed for more human-\n10 ALLEN, SMITH, TENENBAUM\nlike artiﬁcial agents.\nFuture empirical directions\nThis work provides an initial foray into formalizing the\ncomputational and empirical underpinnings of ﬂexible tool\nuse, but there remains much to study. For instance, we do\nnot ﬁnd evidence that people learn more about the world,\nperhaps because here there is little beneﬁt to additional preci-\nsion here. But there are cases where learning the dynamics is\nclearly helpful (e.g., discovering that an object is abnormally\nheavy, or glued down), and wewould expect people to update\ntheir physical beliefs in these cases. When and in what ways\npeople update their internal models to support planning is an\nimportant area of study.\nChildren can discover how to use existing objects earlier\nthan they can make novel tools (Beck et al., 2011), suggest-\ning that tool creation is more challenging than tool use. Yet\nit is the ability to make and then pass on novel tools that\nis theorized to drive human culture (Tomasello, 1999). It is\ntherefore important to understand not just how people use\ntools, but also how they develop and transmit them, which\nwe can study by expanding the action space of the Virtual\nTools game.\nConclusion\nUnderstanding how to ﬂexibly use tools to accomplish our\ngoals is a basic and central cognitive capability. In the Vir-\ntual Tools game, we ﬁnd that people e ﬃciently use tools to\nsolve a wide variety of physical problems. We can explain\nthis rapid trial-and-error learning with the three components\nof the SSUP framework: rich prior world knowledge, simula-\ntion of hypothetical actions, and the ability to learn from both\nsimulations and observed actions. We hope this empirical do-\nmain and modeling framework can provide the foundations\nfor future research on this quintessentially human trait: us-\ning, making, and reasoning about tools, and more generally\nshaping the physical world to our ends.\nAcknowledgments\nThe authors would like to thank Leslie Kaelbling, Roger\nLevy, Eric Schulz, Jessica Hamrick, and Tom Silver for help-\nful comments, and Mario Belledonne for help with the Pa-\nrameter Tuning model. This work was supported by NSF\nSTC award CCF-1231216, ONR MURI N00014-13-1-0333,\nand research grants from ONR, Honda, and Mitsubishi Elec-\ntric.\nReferences\nAnderson, J. R. (1993). Problem Solving and Learning. Am Psy-\nchol, 10.\nBaker, B., Kanitscheider, I., Markov, T., Wu, Y ., Powell, G., Mc-\nGrew, B., & Mordatch, I. (2019). Emergent tool use from\nmulti-agent autocurricula. ICLR.\nBakhtin, A., van der Maaten, L., Johnson, J., Gustafson, L., & Gir-\nshick, R. (2019). Phyre: A new benchmark for physical\nreasoning. NeurIPS.\nBapst, V ., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K. L.,\nKohli, P., Battaglia, P. W., & Hamrick, J. B. (2019, April).\nStructured agents for physical construction. ICML. Re-\ntrieved 2019-04-08, from http://arxiv.org/abs/1904\n.03177\nBattaglia, P. W., Hamrick, J. B., & Tenenbaum, J. B. (2013,\nNovember). Simulation as an engine of physical scene un-\nderstanding. PNAS, 110(45), 18327–18332. Retrieved 2019-\n03-07, from http://www.pnas.org/cgi/doi/10.1073/\npnas.1306572110 doi: 10.1073/pnas.1306572110\nBeck, S. R., Apperly, I. A., Chappell, J., Guthrie, C., & Cut-\nting, N. (2011, May). Making tools isn’t child’s\nplay. Cognition, 119(2), 301–306. Retrieved 2019-05-28,\nfrom https://linkinghub.elsevier.com/retrieve/\npii/S0010027711000163 doi: 10.1016 /j.cognition.2011\n.01.003\nBrain it on. (2015). https://brainitongame.com/.\nChang, M. B., Ullman, T., Torralba, A., & Tenenbaum, J. B.\n(2016, December). A Compositional Object-Based Ap-\nproach to Learning Physical Dynamics. International Con-\nference on Learning Representations . Retrieved 2019-05-\n20, from http://arxiv.org/abs/1612.00341 (arXiv:\n1612.00341)\nChu, Y ., & MacGregor, J. N. (2011, February). Human Performance\non Insight Problem Solving: A Review.The Journal of Prob-\nlem Solving, 3(2). Retrieved 2019-05-28, from https://\ndocs.lib.purdue.edu/jps/vol3/iss2/6 doi: 10\n.7771/1932-6246.1094\nCraik, K. J. W. (1943). The Nature of Explanation. CUP Archive.\n(Google-Books-ID: wT04AAAAIAAJ)\nDasgupta, I., Smith, K. A., Schulz, E., Tenenbaum, J. B., & Ger-\nshman, S. J. (2018). Learning to act by integrating mental\nsimulations and physical experiments. In Proceedings of the\n40th Annual Meeting of the Cognitive Science Society. Re-\ntrieved 2019-03-07, from http://biorxiv.org/lookup/\ndoi/10.1101/321497 doi: 10.1101/321497\nDeisenroth, M., Neumann, G., Peters, J., et al. (2013). A survey\non policy search for robotics. Foundations and Trends in\nRobotics, 2(1–2), 1–142.\nFinn, C., Abbeel, P., & Levine, S. (2017). Model-agnostic meta-\nlearning for fast adaptation of deep networks. InProceedings\nof the 34th international conference on machine learning-\nvolume 70 (pp. 1126–1135).\nFrensch, P. A., & Funke, J. (1995). Complex problem solving: The\neuropean perspective. Psychology Press.\nGarcia, C. E., Prett, D. M., & Morari, M. (1989). Model predictive\ncontrol: theory and practice—a survey. Automatica, 25(3),\n335–348.\nGe, X., Lee, J. H., Renz, J., & Zhang, P. (2016). Hole in one:\nUsing qualitative reasoning for solving hard physical puz-\nzle problems. In Proceedings of the twenty-second european\nconference on artiﬁcial intelligence (pp. 1762–1763).\nRAPID TRIAL-AND-ERROR LEARNING IN PHYSICAL REASONING 11\nGershman, S. J., Markman, A. B., & Otto, A. R. (2014,\nFebruary). Retrospective revaluation in sequential de-\ncision making: A tale of two systems. J Exp\nPsychol Gen , 143(1), 182–194. Retrieved 2019-\n07-21, from https://search.proquest.com/docview/\n1237023272/abstract/53A55607AF4A47F7PQ/1 doi:\nhttp://dx.doi.org/10.1037/a0030844\nGershman, S. J., Zhou, J., & Kommers, C. (2017, July). Imagina-\ntive Reinforcement Learning: Computational Principles and\nNeural Mechanisms. J Cognitive Neurosci , 29(12), 2103–\n2113. Retrieved 2019-07-18, from https://doi.org/\n10.1162/jocn_a_01170 doi: 10.1162/jocn_a_01170\nGerstenberg, T., Goodman, N. D., Lagnado, D. A., & Tenenbaum,\nJ. B. (2015). How, whether, why: Causal judgments as coun-\nterfactual contrasts. In 37th Annual Meeting of the Cognitive\nScience Society (p. 6).\nGick, M. L., & Holyoak, K. J. (1980, July). Analogical prob-\nlem solving. Cognitive Psychol , 12(3), 306–355. Re-\ntrieved 2019-05-28, from http://www.sciencedirect\n.com/science/article/pii/0010028580900134 doi:\n10.1016/0010-0285(80)90013-4\nGoldenberg, G., & Spatt, J. (2009). The neural basis of tool use.\nBrain, 132(6), 1645-1655. doi: 10.1093 /brain/awp080\nJayaraman, D., Ebert, F., Efros, A. A., & Levine, S. (2018). Time-\nagnostic prediction: Predicting predictable video frames. In-\nternational Conference on Learning Representations.\nJelbert, S. A., Taylor, A. H., Cheke, L. G., Clayton, N. S., & Gray,\nR. D. (2014). Using the aesop’s fable paradigm to inves-\ntigate causal understanding of water displacement by New\nCaledonian crows. PloS One, 9(3), e92895.\nJuechems, K., & Summerﬁeld, C. (2019). Where does value come\nfrom? (Preprint). PsyArXiv. doi: 10.31234 /osf.io/rxf7e\nKaelbling, L. P., & Lozano-Pérez, T. (2010). Hierarchical planning\nin the now. In Workshops at the twenty-fourth aaai confer-\nence on artiﬁcial intelligence.\nKroemer, O., Niekum, S., & Konidaris, G. (2019). A review\nof robot learning for manipulation: Challenges, representa-\ntions, and algorithms. arXiv preprint arXiv:1907.03146.\nLombrozo, T. (2018). \"Learning by thinking\" in science and in\neveryday life. In The Scientiﬁc Imagination. New York, NY:\nOxford University Press.\nLuncz, L. V ., Falótico, T., Pascual-Garrido, A., Corat, C., Mosley,\nH., & Haslam, M. (2016). Wild capuchin monkeys adjust\nstone tools according to changing nut properties. Sci Rep, 6,\n33089.\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\nBellemare, M. G., . . . others (2015). Human-level control\nthrough deep reinforcement learning. Nature, 518(7540),\n529.\nNewell, A., & Simon, H. A. (1972). Human problem solving. Ox-\nford, England: Prentice-Hall.\nOrban, G. A., & Caruana, F. (2014). The neural basis of human tool\nuse. Front Psychol, 5. doi: 10.3389 /fpsyg.2014.00310\nOsiurak, F., & Badets, A. (2016). Tool use and a ﬀordance:\nManipulation-based versus reasoning-based approaches.\nPsychological Review, 123(5), 534–568. Retrieved 2019-\n05-21, from http://doi.apa.org/getdoi.cfm?doi=10\n.1037/rev0000027 doi: 10.1037/rev0000027\nSanborn, A. N., Mansinghka, V. K., & Gri ﬃths, T. L.\n(2013, April). Reconciling intuitive physics and\nNewtonian mechanics for colliding objects. Psy-\nchol Rev , 120(2), 411–437. Retrieved 2019-05-31,\nfrom https://search.proquest.com/docview/\n1314700727/abstract/E7521331AA754DDBPQ/1 doi:\nhttp://dx.doi.org/10.1037/a0031912\nSchmidhuber, J., Zhao, J., & Schraudolph, N. N. (1998). Reinforce-\nment learning with self-modifying policies. In Learning to\nlearn (pp. 293–309). Springer.\nShumaker, R. W., Walkup, K. R., & Beck, B. B. (2011). Animal\nTool Behavior: The Use and Manufacture of Tools by Ani-\nmals. JHU Press.\nSmith, K. A., & Vul, E. (2013, January). Sources of Un-\ncertainty in Intuitive Physics. TopiCS, 5(1), 185–199.\nRetrieved 2019-03-07, from http://doi.wiley.com/10\n.1111/tops.12009 doi: 10.1111/tops.12009\nSutton, R. S. (1991, July). Dyna, an Integrated Architecture for\nLearning, Planning, and Reacting. SIGART Bull., 2(4), 160–\n163. Retrieved 2019-05-30, from http://doi.acm.org/\n10.1145/122344.122377 doi: 10.1145/122344.122377\nTomasello, M. (1999). The Cultural Origins of Human Cogni-\ntion. Harvard University Press. (Google-Books-ID: hRFfC-\ngAAQBAJ)\nToussaint, M., Allen, K. R., Smith, K. A., & Tenenbaum, J. B.\n(2018, June). Di ﬀerentiable Physics and Stable Modes for\nTool-Use and Manipulation Planning. In Robotics: Sci-\nence and systems. Retrieved 2019-03-07, from http://\nwww.roboticsproceedings.org/rss14/p44.pdf doi:\n10.15607/RSS.2018.XIV.044\nVaesen, K. (2012, August). The cognitive bases of human\ntool use. Behav Brain Sci , 35(4), 203–218. Retrieved\n2019-05-21, from https://www.cambridge.org/core/\nproduct/identifier/S0140525X11001452/type/\njournal_article doi: 10.1017/S0140525X11001452\nWeber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A.,\nRezende, D. J., . . . others (2017). Imagination-augmented\nagents for deep reinforcement learning. NeurIPS.\nWenke, S., Saunders, D., Qiu, M., & Fleming, J. (2019). Reason-\ning and generalization in rl: A tool use perspective. arXiv\npreprint arXiv:1907.02050.\nWilliams, R. J. (1992). Simple statistical gradient-following al-\ngorithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4), 229–256.\nXie, A., Ebert, F., Levine, S., & Finn, C. (2019). Improvisation\nthrough physical understanding: Using novel objects as tools\nwith visual foresight. Robotics: Science and Systems.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7603328227996826
    },
    {
      "name": "Flexibility (engineering)",
      "score": 0.733457624912262
    },
    {
      "name": "Action (physics)",
      "score": 0.5860742926597595
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5655431747436523
    },
    {
      "name": "Task (project management)",
      "score": 0.5540482401847839
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5191375613212585
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.5026135444641113
    },
    {
      "name": "Process (computing)",
      "score": 0.41088810563087463
    },
    {
      "name": "Mathematics",
      "score": 0.09060153365135193
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ]
}