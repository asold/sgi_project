{
  "title": "SSRT: A Sequential Skeleton RGB Transformer to Recognize Fine-Grained Human-Object Interactions and Action Recognition",
  "url": "https://openalex.org/W4377232645",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4293812543",
      "name": "Akash Ghimire",
      "affiliations": [
        "Inha University"
      ]
    },
    {
      "id": "https://openalex.org/A2188766189",
      "name": "Vijay Kakani",
      "affiliations": [
        "Inha University"
      ]
    },
    {
      "id": "https://openalex.org/A2148329363",
      "name": "Hakil Kim",
      "affiliations": [
        "Inha University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4214858596",
    "https://openalex.org/W3148438775",
    "https://openalex.org/W3168292721",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W3109392981",
    "https://openalex.org/W2981923053",
    "https://openalex.org/W3113067059",
    "https://openalex.org/W4289731865",
    "https://openalex.org/W3185034762",
    "https://openalex.org/W6845954595",
    "https://openalex.org/W3004908934",
    "https://openalex.org/W3035225512",
    "https://openalex.org/W2526479943",
    "https://openalex.org/W3213518743",
    "https://openalex.org/W2953787907",
    "https://openalex.org/W2792345332",
    "https://openalex.org/W4293212119",
    "https://openalex.org/W2054041160",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W6790307280",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W4319299930",
    "https://openalex.org/W4221166187",
    "https://openalex.org/W4200301490",
    "https://openalex.org/W3205898195",
    "https://openalex.org/W6748988375",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W3113321693",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2098339052",
    "https://openalex.org/W846669277",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W4223412119",
    "https://openalex.org/W3035029089",
    "https://openalex.org/W6839710751",
    "https://openalex.org/W4319317104",
    "https://openalex.org/W4313855050",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963781481",
    "https://openalex.org/W2739074143",
    "https://openalex.org/W1983705368",
    "https://openalex.org/W4312658081",
    "https://openalex.org/W6803840103",
    "https://openalex.org/W2146055337",
    "https://openalex.org/W6955071965",
    "https://openalex.org/W2143267104",
    "https://openalex.org/W2126579184",
    "https://openalex.org/W2605111198",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W3129478823",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4306820095",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W3100437087",
    "https://openalex.org/W4376226279",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2792747672",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W4226079235",
    "https://openalex.org/W3211965499",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Combining skeleton and RGB modalities in human action recognition (HAR) has garnered attention due to their ability to complement each other. However, previous studies did not address the challenge of recognizing fine-grained human-object interaction (HOI). To tackle this problem, this study introduces a new transformer-based architecture called Sequential Skeleton RGB Transformer (SSRT), which fuses skeleton and RGB modalities. First, SSRT leverages the strength of Long Short-Term Memory (LSTM) and a multi-head attention mechanism to extract high-level features from both modalities. Subsequently, SSRT employs a two-stage fusion method, including transformer cross-attention fusion and softmax layer late score fusion, to effectively integrate the multimodal features. Aside from evaluating the proposed method on fine-grained HOI, this study also assesses its performance on two other action recognition tasks: general HAR and cross-dataset HAR. Furthermore, this study conducts a performance comparison between a HAR model using single-modality features (RGB and skeleton) alongside multimodality features on all three action recognition tasks. To ensure a fair comparison, comparable state-of-the-art transformer architectures are employed for both the single-modality HAR model and SSRT. In terms of modality, SSRT outperforms the best-performing single-modality HAR model on all three tasks, with accuracy improved by 9.92&#x0025; on fine-grained HOI recognition, 6.73&#x0025; on general HAR, and 11.08&#x0025; on cross-dataset HAR. Additionally, the proposed fusion model surpasses state-of-the-art multimodal fusion techniques like Transformer Early Concatenation, with an accuracy improved by 6.32&#x0025; on fine-grained HOI recognition, 4.04&#x0025; on general HAR, and 6.56&#x0025; on cross-dataset.",
  "full_text": "Received 11 May 2023, accepted 16 May 2023, date of publication 22 May 2023, date of current version 1 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3278974\nSSRT: A Sequential Skeleton RGB Transformer to\nRecognize Fine-Grained Human-Object\nInteractions and Action Recognition\nAKASH GHIMIRE\n 1, VIJAY KAKANI\n 1, (Member, IEEE), AND HAKIL KIM\n2, (Member, IEEE)\n1Department of Integrated System Engineering, School of Global Convergence Studies, Inha University, Incheon 402-751, South Korea\n2Department of Information and Communication Engineering, Inha University, Incheon 402-751, South Korea\nCorresponding author: Hakil Kim (hikim@inha.ac.kr)\nThis work was supported by Inha University’s Research Grant.\nABSTRACT Combining skeleton and RGB modalities in human action recognition (HAR) has garnered\nattention due to their ability to complement each other. However, previous studies did not address the\nchallenge of recognizing fine-grained human-object interaction (HOI). To tackle this problem, this study\nintroduces a new transformer-based architecture called Sequential Skeleton RGB Transformer (SSRT),\nwhich fuses skeleton and RGB modalities. First, SSRT leverages the strength of Long Short-Term Memory\n(LSTM) and a multi-head attention mechanism to extract high-level features from both modalities. Subse-\nquently, SSRT employs a two-stage fusion method, including transformer cross-attention fusion and softmax\nlayer late score fusion, to effectively integrate the multimodal features. Aside from evaluating the proposed\nmethod on fine-grained HOI, this study also assesses its performance on two other action recognition tasks:\ngeneral HAR and cross-dataset HAR. Furthermore, this study conducts a performance comparison between a\nHAR model using single-modality features (RGB and skeleton) alongside multimodality features on all three\naction recognition tasks. To ensure a fair comparison, comparable state-of-the-art transformer architectures\nare employed for both the single-modality HAR model and SSRT. In terms of modality, SSRT outperforms\nthe best-performing single-modality HAR model on all three tasks, with accuracy improved by 9.92% on\nfine-grained HOI recognition, 6.73% on general HAR, and 11.08% on cross-dataset HAR. Additionally,\nthe proposed fusion model surpasses state-of-the-art multimodal fusion techniques like Transformer Early\nConcatenation, with an accuracy improved by 6.32% on fine-grained HOI recognition, 4.04% on general\nHAR, and 6.56% on cross-dataset.\nINDEX TERMS Multimodality fusion, human action recognition, fine-grained actions, transformer cross-\nattention fusion.\nI. INTRODUCTION\nRGB video data encompasses both temporal and spatial infor-\nmation, including details about human limbs and interactions\nwith objects [9], [10]. However, extracting human actions\nfrom RGB data can pose a challenge due to the diversity\nin surroundings, angles of observation, human proportions,\nand illumination settings. On the other hand, skeleton modal-\nity data encodes human body joint movements, capturing\nmotion-related information and making it highly suitable for\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Thomas Canhao Xu\n.\nHAR tasks [21], [43], [44]. This modality is scale-invariant\nand robust to variations in clothing textures and backgrounds,\nensuring reliable action recognition across different subject\nsizes and situations. Nonetheless, the limitation of skeleton\ndata is its lack of spatial information, which complicates\nthe accurate prediction of fine-grained Human-Object Inter-\naction (HOI). These actions involve similar limb and joint\nmovements but vary in HOI aspects, as illustrated in Figure 1.\nIn this series of images, the action of drinking differs due\nto the person’s interactions with various objects (cup, can,\nand bottle). To accurately classify fine-grained HOI, it is\nbeneficial to merge the strengths of both RGB and skeleton\n51930\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nFIGURE 1. Examples of fine-grained human-object interactions.\nmodalities into a cohesive set of distinguishing features. Con-\nsequently, in the field of HAR, combining skeleton and RGB\nmodalities is currently a significant research focus.However,\nmost of the previous studies [12], [13], [16] primarily focused\non achieving state-of-the-art performance on well-known\ndatasets, such as NTU-RGB+D [27], and often overlooked\nthe following key concerns:\n1) Recognition of fine-grained HOI : Most existing\nHAR studies [12], [13], [36], [37], [42] that fuse skele-\nton and RGB modalities have mainly concentrated on\nrecognizing broad interaction categories. These studies\nhave assessed datasets like NTU RGB+D [27], which\ncontain only coarse-grained HOI that can be accurately\nclassified using high-quality skeleton modality features\nalone.\n2) Bias to RGB modality features: During the training\nprocess of a model that utilizes both RGB and skele-\nton modalities, there is often a prevalent bias towards\nappearance. This bias can limit the model’s generaliza-\ntion capabilities for unseen videos and increase its sus-\nceptibility to deception by out-of-context videos [11].\nResearch on the fusion of these two modalities suggests\nthat the multimodal methods employed in these studies\noffer only slight enhancements over RGB-based HAR\nmodels [12], [14]. Furthermore, none of these stud-\nies [12], [14], [15], [16], [37], [42] assess the robustness\nof their proposed models when facing cross-dataset.\nSome limited research [14], [15], [16] has explored\nthe integration of RGB and skeleton modalities using\ndatasets containing fine-grained HOI, such as Toyota\nSmarthome [14], but their primary focus was not on improv-\ning the accuracy of fine-grained HOI. First, these studies\nwere conducted on overall action classes, with only a few\nactions involving fine-grained HOI. Furthermore, although\nmultimodal strategy in [14] demonstrated enhanced accuracy\nfor certain fine-grained HOI classes, this improvement was\nnot observed in other fine-grained HOI classes possessing the\nsame coarse label. The reason for this might be an uneven\ndistribution of training and testing instances in fine-grained\nHOI with identical coarse labels.\nIn the field of HAR, recent advancements are largely\ndriven by the success of transformer-based multimodal-\nity architectures [35], [37], [40], [41], [42], demonstrat-\ning state-of-the-art results. Building on such success, this\npaper introduces a new transformer-based architecture called\nSequential Skeleton RGB Transformer (SSRT) that fuses\nskeleton and RGB modalities. First, SSRT harnesses the\npower of Long Short-Term Memory (LSTM) and a trans-\nformer multi-head attention mechanism to obtain abstract\nfeatures from the skeleton and RGB modalities. Subse-\nquently, SSRT employs a two-stage fusion approach, con-\nsisting of transformer cross-attention multimodal fusion [34]\nand Softmax layer (SML ) late fusion, to efficiently integrate\nabstract features from two modalities. The architecture of\nSSRT is illustrated in Figure 2.\nEfficiently complementing heterogeneous modalities such\nas RGB and skeleton modality presents challenges, as men-\ntioned by Joshi et al. [57]. Unlike conventional fusion\ntechniques like early fusion, late fusion, concatenation,\nor weighted sum, which do not effectively address the\nheterogeneity of RGB and skeleton modalities, SSRT uti-\nlizes multi-head attention mechanisms. These mechanisms,\nas demonstrated in [29], can determine information from\nvarious representation subspaces at distinct locations. This\napproach captures the two counterintuitive modalities and\nprovides a more accurate fusion method than traditional tech-\nniques shown in [17], [18], [19], and [20].\nThe primary objective of this study is to solve the prob-\nlem of recognizing fine-grained HOI. To accomplish this\ngoal, this study utilizes balanced sets of training, validation,\nand test data for each fine-grained HOI class in the Toyota\nSmarthome dataset to prevent any bias towards specific\nactions within the same coarse label. Apart from recogniz-\ning fine-grained HOI, the study assesses SSRT on action\nclasses other than fine-grained HOI to confirm that the\nmodel generalizes well across various scenarios. Addition-\nally, to ensure that SSRT is robust and not biased toward\nRGB features, the study evaluates the proposed method on\nclasses from cross-dataset actions in the ETRI-Activity3D\ndataset [28].\nThe main contributions of this research include the\nfollowing:\n1) A new method for merging skeleton and RGB data\nin human activity recognition, SSRT initially extracts\nhigh-level features from both the skeleton and RGB\nmodalities using a unique technique that employs\nLSTM and a transformer encoder to capture improved\nhigh-level temporal dependencies of two modalities.\nFollowing this, SSRT combines the high-level fea-\ntures from the skeleton and RGB modalities through\na two-stage fusion process: transformer cross-attention\nand softmax layer late score fusion.\n2) We evaluate the performance of skeleton and\nRGB modalities on fine-grained HOI, general, and\nVOLUME 11, 2023 51931\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nFIGURE 2. Overview of SSRT architecture: A sequential skeleton RGB transformer designed for fine-grained human-object interaction and action\nrecognition.\ncross-dataset HAR tasks. Following this, the best-\nperforming single-modality HAR model for each HAR\ntask is compared to SSRT, which incorporates both the\nskeleton and RGB modalities.\n3) To ensure a fair comparison among skeleton, RGB,\nand multimodal approaches, the study employs a state-\nof-the-art transformer architecture action recognition\nmodel tailored to each modality-based HAR.\n4) The novelty of this research is fourfold. First, to the\nbest of our knowledge, SSRT is a pioneering model\nthat employs a combination of LSTM and a trans-\nformer architecture as described above. Second, SSRT\nis the first to fuse skeleton and RGB modalities using\na transformer cross-attention multimodal mechanism.\nThird, this research is the first of its kind to primar-\nily concentrate on recognizing fine-grained HOI and\naction recognition. Fourth, we are the first to assess\nthe performance of a multimodal features-based HAR\nmodel on cross-dataset actions.\nII. RELATED WORKS\nA. TRANSFORMERS FOR HAR\nResearch in HAR using transformer-based architectures, such\nas ViViT [8], TimeSformer [45], MVT [39], AcT [44],\nand STST [43], has demonstrated state-of-the-art results.\nUsing RGB modality features, ViViT [8] and TimeSformer\n[45] extract spatiotemporal features separately, whereas\nMVT [39] uses two transformer encoders to process differ-\nent views of input frames. Transformer-based models such\nas AcT [44] and STST [43] have also explored the use of\nskeleton modality for HAR tasks. AcT utilizes 2D skeleton\nrepresentations for efficient real-time performance, whereas\nSTST [43] uses spatial and directional temporal transformer\nblocks for modeling skeleton sequences.\nIn this study, we select a model resembling the one used in\nAcT [44] as our primary model for HAR with a single modal-\nity. This choice was made because the AcT model not only\noutperforms state-of-the-art models such as SR-TR [53] and\nMS-G3D(J+B) [54], but also provides a low latency solution.\nFurthermore, our proposed multimodal HAR model, SSRT,\nemploys a model resembling the AcT model as a baseline,\ncombined with LSTM, to extract high-level temporal depen-\ndencies from the skeleton and RGB modalities. However,\nour study introduces a distinct transformer architecture that\ndiffers from AcT.\nB. MULTIMODAL FUSION FOR HAR\n1) TRANSFORMER-BASED MULTIMODALITY FUSION\nFOR HAR\nThe fusion of multimodal information through transformer\narchitectures has recently garnered considerable interest, par-\nticularly in uniting RGB and language modalities to achieve\nstate-of-the-art performance in vision-linguistic tasks [30],\n[31], [38]. Capitalizing on this achievement, researchers have\nexplored the integration of skeleton and RGB modalities\nwith transformer-based architectures for a range of vision\ntasks, including HAR. To offer a thorough understanding of\nthe transformer-based multimodality fusion applied in HAR,\na summary is given in Table 1, featuring three columns:\nFusion Method,Modalities Used, and Purpose. The Fusion\nMethod column details three popular fusion techniques using\ntransformer architecture: early summation, early concatena-\ntion, and cross-attention. The Modalities Used column lists\nthe modalities employed in the studies, while the Purpose col-\numn highlights the specific tasks for which fusion is executed.\nEarly summation is a simple yet effective approach to\nmultimodal interaction that involves weighting token embed-\ndings from multiple modalities and subsequently summing\nthem at each token position before processing them through\n51932 VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\na transformer layer. For example, the authors in [35] utilized\nearly summation to fuse three multimodality features (RGB\nimages, optical flow images, and static skeleton) for group\nactivity recognition. A benefit of using the early summation\napproach is its low computational difficulty, but the primary\ndrawback lies in the need to manually assign weights for\nvarious input multimodal features [34].\nAs another early fusion method, early concatenation entails\nconcatenating token embedding sequences from multiple\nmodalities and inputting them into transformer layers. Studies\nin [36], [37], and [42] employed this fusion method with RGB\nand skeleton sequences for HAR. Early concatenation offers\nthe benefit of relative simplicity compared to other methods.\nNevertheless, the drawback of utilizing this fusion technique\nis the increased computational complexity due to the longer\nsequence resulting from concatenation [34].\nCross-attention is an efficient fusion method for multiple\nmodalities, enabling each modality to attend to information\nfrom the others. This process is achieved by exchanging the\nkey (K) and value (V) vectors of one modality with the query\n(Q) sequences of another modality within multiple stream\ntransformer layers. Furthermore, this fusion method does not\nsignificantly increase computational complexity.\nYan et al. [39] employed cross-attention to fuse RGB\nimages from different views for more robust action recogni-\ntion. Similarly, Ijaz et al. [40] used cross-attention to inte-\ngrate skeleton sequence data and acceleration data for action\nrecognition in nursing. Additionally, Zhang et al. [41] applied\ncross-attention to fuse three modalities-RGB images, text,\nand audio-for effective facial expression recognition.\nOur proposed SSRT represents the first transformer-based\nmultimodal fusion approach to combine skeleton and RGB\nmodalities for HAR using cross-attention. Diverging from\nprior studies that employed early concatenation for fusing\nhigh-level skeleton and RGB features [37], [42], our proposed\nmethod focuses on detecting fine-grained HOI and action\nrecognition.\n2) MULTISTAGE MULTIMODAL FUSION TECHNIQUES\nFOR HAR\nCheng et al. [51] employ a two-stage approach to fuse\nRGB and depth sequences. First, they introduce a novel\nmethod called Cross-Modality Interactive Module (CMIM)\nto enhance the sharing of high-level features between RGB\nand depth sequences. Subsequently, these features are fused\nusing the late score fusion technique. Yan et al. [39] initially\nutilize transformer cross-attention to fuse RGB images from\ndifferent views for more robust action recognition. Later, they\nuse a global transformer encoder to fuse the high-level fea-\ntures derived from the transformer encoder, which processes\ntwo distinct views of RGB images. Yuean et al. [52] develop a\nhuman monitoring system that integrates PRF and PIR sensor\ndata through sensor fusion. They employ three RNN models\nfor PIR, PRF, and combined PRF-PIR data and implement\ndecision-level fusion with HAP XAI to improve the inter-\npretability of the results. Weiyao et al. [12] and Zhu et al. [13]\nfuse RGB and skeleton modalities in two stages of fusion\nfor human action recognition. Weiyao et al. first uses the\nproposed Bilinear Pooling and Attention Network (BPAN)\nmodule to fuse the high-level features of RGB and skeleton\nmodalities. The BPAN module is employed to learn poten-\ntial semantic relationships between RGB and skeleton HAR\nbaseline models. Later, similar to our Softmax layer late score\nfusion, Weiyao et al. fuse the probability scores from two\npathways to obtain a final score prediction. Zhu et al. imple-\nment a novel two-stage feature fusion network to combine\nthe knowledge of the RGB and skeleton modalities. First,\nthey fuse skeleton and RGB features in the early stage using\nelement-wise concatenation. Then, Zhu et al. use either GCN\nor LSTM architecture to fuse these features as late score\nfusion.\nIn our proposed method, we leverage the benefits of a\ntwo-stage fusion approach to effectively integrate RGB and\nskeleton modalities for human action recognition. By initially\napplying transformer cross-attention to capture the complex\ninteractions between the modalities, we are able to learn\nmore meaningful and complementary features. Then, through\nsoftmax layer late score fusion, we combine the probability\nscores of the individual modalities, allowing for a more accu-\nrate and robust final prediction. To the best of our knowledge,\nour method is the first to utilize this combination of trans-\nformer cross-attention and softmax layer late score fusion for\nhuman action recognition. This unique approach capitalizes\non the advantages of both stages of fusion, ultimately leading\nto improved performance in comparison to previous methods.\nIII. METHODOLOGY\nA. FEATURES PREPROCESSING\nThis study standardized the experimental datasets by ran-\ndomly selecting 42 sequences of frames from each video\nto identify human actions. The frames were then resized to\n480×640×3. To obtain RGB features, the input pixel values\nof each frame were first rescaled to between -1 and 1 using\nequation (1) and were then passed through a pre-trained\nResnet152 [33] model to generate 42 × 2048 RGB features.\nTo obtain the skeleton modality, AlphaPose Pose Estima-\ntion [32] was utilized to generate 17 2D skeleton features\nfrom each input frame. These skeleton features were then nor-\nmalized along the x and y coordinates utilizing equation (1)\nand flattened to obtain 42 × 34 skeleton features. Figure 4\nshows the raw images in the first row and their associated\nskeleton keypoint representations in the second row.\nIn this study, we employed a specific preprocessing\napproach to create multimodal features using both RGB and\nskeleton data. First, we extracted skeleton features following\nthe procedure described earlier. Next, we cropped frames\nto a size of 350 × 350 × 3 using the x and y coordinates\nof the skeleton features. To determine the optimal cropping\ndimensions, we conducted a qualitative analysis, which is\nillustrated in Figure 3. The figure’s columns represent three\ndifferent cases, with the first row demonstrating the limita-\ntions of various cropping dimensions, while the second row\nVOLUME 11, 2023 51933\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nTABLE 1. Related works on transformer-based multimodal fusion for HAR.\nFIGURE 3. Suboptimal crop selection vs. Optimal crop selection.\nshowcases the optimal results achieved with the proposed\ncropping dimension in each scenario.\nInitially, we attempted cropping with a size of 144 ×144×\n3, as depicted in the Case 1 of the first row in Figure 3.\nHowever, this cropping method proved inadequate in most\nscenarios, as it failed to fully capture the subject. We then\nincreased the crop dimensions to 224 ×224×3 but found this\nsize to be insufficient for cases where the subject is standing,\nas illustrated in Case 2of the first row in Figure 3.\nSubsequently, we further increased the crop dimensions to\n275 × 275 × 3 and observed that although this size produced\noptimal results in most scenarios, it struggled to accurately\ncapture human-object interactions, particularly when the per-\nson was too close to the camera. In Case 3of the first row,\nit is evident that the bottle was cropped during the process.\nUltimately, we settled on a dimension of 350 × 350 × 3,\nwhich provided optimal results, as shown in the second row\nof Figure 3.\nTo obtain RGB features, we followed the same process as\npreviously outlined, resulting in 42 ×2048 RGB features from\nthe 42 input frames. By implementing these preprocessing\nsteps, the proposed SSRT can strike a balance between the\nmost relevant and contextual information by cropping the\nframe to an ideal size.\nxnormalized = x − xmin\nxmax − xmin\n× 2 − 1 (1)\nFIGURE 4. Optimizing RGB frame size through skeleton feature-based\ncropping.\nTABLE 2. Different versions of SSRT, skeleton encoder, and RGB encoder.\nB. ACTION RECOGNITION MODEL\n1) SINGLE MODALITY ACTION RECOGNITION\nSection (III-A) outlined the preprocessing procedure\nemployed for classifying HAR using a transformer encoder\nwith either skeleton or RGB modality. Following this, the\npreprocessed features are channeled through a positional\nencoding layer. In accordance with the transformer architec-\nture [29], each input feature dimension must be a multiple\nof the number of heads (H) utilized in the multi-head self-\nattention (MSA) layer. As a result, the positionally encoded\ninput features are projected onto dmodel dimensions, corre-\nsponding to the number of H incorporated in the MSA layer,\nas noted in Table 2. Therefore, the input X for the transformer\nencoder possesses dimensions of 42 × dmodel .\nThe operating mechanism of the transformer encoder is\nelucidated in [29]. The output from the transformer encoder\n51934 VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nis flattened into a one-dimensional vector, which is then pro-\ncessed by a Softmax layer, producing a probability output for\neach human action class. In this study, the terms RGB encoder\nand skeleton encoderare used when the RGB modality and\nskeleton modality, respectively, are leveraged for classifying\nHAR with the transformer encoder.\n2) SSRT: PROPOSED MULTIMODALITY ACTION\nRECOGNITION\nSSRT integrates RGB and skeleton sequences using two par-\nallel pathways: the skeleton modality pathway and the RGB\nmodality pathway, as depicted in Figure 2.\nTo efficiently extract high-level features from multimodal\ninputs, feature preprocessing is performed for each modality\nfeature prior to high-level feature extraction. Section III-A\noffers an in-depth description of feature preprocessing for\nmultimodality fusion. Providing positional information to the\npreprocessed features from each modality is crucial before\ninputting them into the transformer encoder. Without this\ninformation, the transformer encoder might interpret the fea-\ntures as a bag of features, potentially reducing its effective-\nness. Although a Positional Encoding Layer is commonly\nemployed to tackle this issue, it may be unsuitable for\ntime-series tasks like HAR due to its disregard for temporal\ndependencies between input sequences.\nIn order to effectively capture the sequence order and tim-\ning of input features, this study proposes the use of an LSTM\nlayer instead of a positional encoder. LSTMs process inputs\nelement by element, adeptly capturing the temporal depen-\ndencies inherent in feature sequences. This characteristic is\ncrucial for time-series tasks, such as HAR, where recognizing\ntemporal dependencies is vital. By implementing an LSTM\nlayer, the effectiveness of abstract feature extraction can be\nenhanced compared to traditional positional encoding meth-\nods. To maintain the dimensionality of each input sequence\ninstance as a factor of the number of heads H used in the\nmulti-head attention layer, the hidden dimension of the LSTM\nlayer is set to dmodel . As a result, the LSTM layer generates an\noutput with a shape of 42 × dmodel for both pathways, which\nis subsequently directed to the appropriate skeleton or RGB\ntransformer encoder based on the input modality.\nSubsequently, the transformer encoder architecture is\nemployed to obtain high-level features from both the skele-\nton and RGB pathways, as described in Section III-B1.\nAfter obtaining the abstract features from the RGB encoder\n(HLFRGB) and the skeleton encoder (HLF SKL ), the fusion\nprocess begins through the skeleton cross transformer and\nthe RGB cross transformer. These cross transformers merge\nfeatures from two modalities using a cross-attention fusion\nmechanism, which is elaborated upon in Section II-B1.\nImportantly, the skeleton/RGB cross transformer uses multi-\nhead cross-attention (MCA) layers, distinct from the MSA\nlayers found in the skeleton/RGB encoder.\nThe skeleton cross transformer incorporates an MCA layer\nthat creates a Qs vector for each attention head H by\nmultiplying HLFSKL with a trainable weight matrix WQs.\nSimultaneously, the MCA layer generates two contextual vec-\ntors, Kr and Vr , by multiplying HLFRGB with trainable weight\nmatrices WKr and WVr , respectively. The attention score (A s)\nfor each attention head H in the skeleton cross transformer is\ndetermined using equation (2), where dk denotes the dimen-\nsion of each attention head. To acquire the final multi-head\nattention scores (MCA s), the attention scores from all atten-\ntion heads are concatenated and then multiplied by a train-\nable weight vector Ws, with dimensions (d k × H) × dmodel ,\nas illustrated in equation (3).\nAs = Softmax\n(QsKT\nr\n√dk\n)\nVr (2)\nMCAs = Softmax\n(\n[As1; As2; ..; AsH]\n)\n)WS (3)\nIn a similar manner, the RGB cross transformer produces\nthree vectors (Q r , Ks, and Vs). However, the query input\nvectors are obtained using HLFRGB, while the contextual\ninput vectors are derived from HLFSKL . Then, equations (4)\nand (5) are used to compute the final multi-head attention\nscore ( MCAr ) for the RGB cross transformer, following an\napproach analogous to that of the skeleton cross transformer.\nAr = Softmax\n(Qr KT\ns√dk\n)\nVs (4)\nMCAr = Softmax\n(\n[Ar 1; Ar 2; ..; Ar H]\n)\nWr (5)\nThe following steps in the skeleton/RGB cross transform-\ners conform to the same process as the skeleton/RGB encoder,\nas detailed in Section III-B1. The outputs from the skeleton\ncross transformer (O SCT ) and the RGB cross transformer\n(ORCT ) are combined with HLFSKL and HLFRGB, respec-\ntively, and then normalized. Afterward, the normalized out-\nputs from each modality pathway are flattened and fed into\nthe Softmax layer, resulting in the probability of each action\nclass for the skeleton modality (P SKL ) and the RGB modality\n(PRGB), as shown in equations (6) and (7). Ultimately, the\nSML late score fusion is carried out by adding PSKL and PRGB\nto obtain the final probability of each action class (P FINAL) for\nHAR, as portrayed in equation (8).\nPSKL = Softmax\n(\nFlatten(Norm(HLFSKL + OSKL ))\n)\n(6)\nPRGB = Softmax\n(\nFlatten(Norm(HLFRGB + ORGB))\n)\n(7)\nPFINAL = ADD\n(\nPSKL , PRGB\n)\n(8)\n3) TRANSFORMER MODEL ARCHITECTURE\nThis research introduces three distinct transformer architec-\nture versions (X1, X2, and X3) for the skeleton encoder,\nthe RGB encoder, and SSRT, as listed in Table 2. In this\nTable, dff represents the dimension of the initial layer in\nthe feed-forward network inside the transformer encoder,\nas described in [29]. Diverging from traditional transformer\narchitectures that use a low number of H and a higher\nnumber of dmodel [22], [29], this study recommends making\nthe number of H in a multi-head attention layer the same\nVOLUME 11, 2023 51935\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nas dmodel . This design enables each model to effectively\nharness a high number of H without incurring excessive\ncomputational demands.\nIV. IMPLEMENTATION DETAIL\nA. DATASETS\nThis research employs two datasets for the HAR task. Table 3\ncompares the commonly used vision-based datasets for this\ntask. Among them, the Toyota Smarthome dataset stands\nout due to its fine-grained HOI and context-free nature, fea-\nturing elderly people performing actions without adhering\nto a specific script. Furthermore, this dataset exemplifies\na real-world situation, presenting a distinct set of chal-\nlenges, including high intra-class variation, significant class\nimbalance, and activities exhibiting similar motion patterns\nand considerable duration disparities [14]. Consequently, the\nToyota Smarthome dataset serves as the primary dataset for\nthis study.\nAs a secondary dataset, ETRI-Activity3D is utilized. This\ndataset also features elderly subjects performing context-free\nactions, making it an appropriate choice. Additionally, four\ngeneral actions are shared between the Toyota Smarthome\nand ETRI-Activity3D datasets, making ETRI-Activity3D\nwell-suited for the cross-dataset HAR task.\nThis study enhances dataset quality by filtering out sam-\nples in which the interacting object is obscured by the subject\nor another object.\n1) FINE-GRAINED HUMAN-OBJECT INTERACTION\nThe Toyota Smarthome dataset consists of three sets of\nfine-grained HOI action classes, which fall under the coarse\naction labels of Drink, Eat, and Pour. It is crucial to men-\ntion that this research omits the fine-grained actions (Clean\ndishes, Clean up,Cut), (Pour grains,Pour water), and (Boil\nwater, Insert tea bag), corresponding to the coarse labels\nCook, Make coffee, Make tea, respectively. This exclusion\nis due to the fact that these fine-grained actions represent\ncomposite actions at a fine-grained level, rather than fine-\ngrained HOI.\nThis study also excluded the fine-grained HOI action class\nwithin the Eat and Pour categories. The Eat category is not\nincluded because, as depicted in Figure 5, the fine-grained\nHOI Eat at the tableinvolves eating at a table, while Eat snack\nfeatures a few instances of eating while sitting. The posture\ndifferences can be detected independently using the skele-\nton modality. Similarly, as illustrated in Figure 5, the Pour\ncategory is not considered because the Pour from kettle\naction involves pouring water from a kettle, whereas the\ncorresponding fine-grained HOI action Pour from bottlealso\nincludes additional actions such as opening and closing the\nbottle. The skeleton modality can identify these extra actions\nwithout requiring RGB features. This study only employs\nfine-grained HOI from the Drink category. As shown in\nFigure 1, the fine-grained HOI within the Drink category\nshare similar motions, with the only variation being the sub-\nject’s interaction with various drinking objects.\nFIGURE 5. Unused action classes in fine-grained human-object\ninteractions from Toyota Smarthome dataset.\nThis research used 196 video samples from each class\nwithin fine-grained HOI and stratified them into training,\nvalidation, and testing sets at a 60:20:20 ratio, ensuring unbi-\nased results. This approach provides representative samples\nof fine-grained HOI in each subset, mitigating the risk of\noverfitting to any particular class, unlike the methods used\nin [14], [15], and [16].\n2) GENERAL ACTIONS AND CROSS-DATASET ACTIONS\nThis research extended the evaluation of the proposed SSRT\nmodel to two other action recognition tasks. First, the model’s\nability to generalize beyond the fine-grained HOI class was\ntested in the General Actions class. Secondly, the model was\nassessed on the Cross-dataset Action class to evaluate its\nperformance with unseen videos from a different dataset. The\nobjective of this evaluation was to establish if the SSRT model\ncould achieve high accuracy on videos that differ significantly\nfrom the training set, demonstrating its ability to generalize\nto new and diverse contexts.\nIn this research, an extensive search was conducted of\nboth the Toyota Smarthome and ETRI-Activity3D datasets\nto identify suitable action classes for general action and\ncross-dataset HAR tasks. After analyzing the datasets, four\nclasses were identified that exhibited comparable motion\nand human-object interaction. These classes are Drink From\nCup, Readbook, Uselaptop, and Usetelephone from Toyota\nSmarthome, which corresponds to Drinking water,Reading\n51936 VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nTABLE 3. Comparison of vision-based datasets for human action recognition.\nFIGURE 6. Comparison of common actions in Toyota Smarthome and ETRI-Activity3D datasets.\na book,Using a computer, andTalking on the phone, respec-\ntively, from ETRI-Activity3D. Throughout the study, the\naction class names from the Toyota Smarthome dataset were\nused for both HAR tasks for ease of analysis. In Figure 6,\nframes depicting the beginning and middle stages of each\naction category in the Toyota Smarthome dataset are shown\nalongside the corresponding class from the ETRI-Activity3D\ndataset.\nThis research underscores the significant challenges asso-\nciated with cross-dataset evaluation, which is more intricate\nthan both cross-view and cross-subject evaluations [14], [27],\n[28]. The complexity arises due to the divergent perspectives\nbetween the Etri-Activity3D dataset, captured from a robotics\nviewpoint, and the Toyota Smarthome dataset, recorded from\na surveillance viewpoint. This variance heightens the diffi-\nculty of the cross-view challenge. Furthermore, cross-dataset\nevaluation proves to be more convoluted than cross-subject\ntesting, as the individuals performing actions differ, and other\nRGB factors such as illumination, colors, and video quality\nfurther compound the evaluation process.\nThe effectiveness of SSRT on the general HAR task was\nassessed using the training, validation, and testing datasets\nVOLUME 11, 2023 51937\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nTABLE 4. Traning and regularization hyperparameters.\nfrom Toyota Smarthome. For selecting the training, valida-\ntion, and testing datasets, sample videos of selected action\nclasses from the Toyota Smarthome dataset were split into\nthose datasets at a ratio of 60:20:20. Similarly, to evaluate\nthe performance of SSRT on the cross-dataset HAR action\nrecognition task, this research utilized the same training and\nvalidation dataset splits as the General HAR task, and tested\nthe model on cross-dataset actions from the ETRI-Activity3D\ndataset. From ETRI-Activity3D, 99 samples were selected\nfrom the action class Drink From Cup, 125 samples were\nselected from Readbook, 95 samples from Uselaptop, and\n123 samples from Usetelephone.\nThis dataset implementation enables benchmarking of the\nproposed method, SSRT, in a manner akin to that presented\nby An et al. [55]. An et al. have proposed two experimen-\ntal protocols; first, they split their dataset into Setting 1\n(S1) and Setting 2 (S2). The S1 dataset is obtained from a\ntrain-validation-test split, while S2 focuses on cross-subject\nevaluation. Next, they also divide their actions into two\naction protocols, P1 and P2. Similar to the S1 setting, our\nfine-grained and general actions involve a train-test random\nsplit for comparison. Likewise, our approach mirrors the\nS2 setting with a cross-dataset HAR task. However, the\ncross-dataset setting is considerably more challenging than\nthe S2 protocol, as it encompasses not only cross-subject\nchallenges but also other difficulties such as varying back-\ngrounds, cross-views, and so on.\nB. COMPARISION WITH OTHER HAR METHODS\n1) COMPARISION WITH SINGLE-MODALITY HAR MODEL\nIn this study, to compare SSRT with single-modality HAR,\nLSTM and Transformer encoder HAR models were cho-\nsen. Although LSTM is not a state-of-the-art HAR model,\nit was selected alongside the Transformer encoder due to\ntheir critical roles in the SSRT framework. Consequently, this\nparticular selection of single-modality-based HAR models\nalso benefits the ablation study.\nAs discussed in Section II-A, a Transformer encoder\nresembling the AcT model [44] was chosen as the primary\nsingle-modality HAR model for comparison with SSRT per-\nformance. The main reasons for this choice are:\n1) Transformer is well-suited for both RGB and skele-\nton modality-based HAR models: As discussed in\nSection II-A, the transformer architecture demonstrates\nstate-of-the-art performance for both RGB and skeleton\nmodality-based HAR. This makes the transformer\narchitecture one of the few architectures suitable for\nboth the skeleton and RGB-based modalities in HAR\ntasks. For example, Graph Convolution Network-based\nmodels such as STGCN [21] and 2s-AGCN [56] exhibit\nstate-of-the-art performance for skeleton-based modal-\nity but fail when RGB modality is used. Similarly, 3D\nCNN-based models like I3D [4] perform exceptionally\nwell with RGB modality but fall short when used with\nskeleton modality.\n2) Ensuring a fair comparison of the skeleton, RGB,\nand multimodal approaches: This study proposes the\nsame three Transformer architectures for both single-\nmodality-based HAR models and SSRT, as discussed\nin Section III-B3. In SSRT, a similar Transformer archi-\ntecture to that of the single-modality HAR model is first\nused in the RGB/Skeleton encoder for higher feature\nextraction, and later, the same architecture is employed\nin the fusion stage using the cross-transformer. This\nconsistent use of comparable Transformer architectures\nallows for a fair comparison between single-modality-\nbased HAR and SSRT.\n2) COMAPRISION WITH OTHER METHODS OF\nMULTIMODALITY FUSION HAR MODEL\nIn this study, the SSRT method is evaluated alongside three\nother prominent fusion methods: LSTM Late Score, Trans-\nformer Early Concatenation, and Transformer Late Score.\nThe baseline models for the late score fusion methods are\nderived from the single-modality HAR models used in this\nresearch. The LSTM and Transformer late score fusion\nmodels are trained independently on each modality, with\nthe predicted probability scores from each modality com-\nbined to predict the action classes using the late score\nfusion approach. Transformer Early Concatenation employs\na Transformer encoder to extract high-level features for each\nmodality. These features are subsequently concatenated and\nprojected onto another Transformer encoder to generate the\nfinal prediction.\nC. EXPERIMENTAL SETTINGS\nThe experiments were conducted using Tensorflow plat-\nform release 2.11 on a personal computer that has an Intel\ni7-10700K processor, 48 GB of RAM, and an NVIDIA\nRTX 3090 GPU that has 24 GB of VRAM. Optuna [46]\nwas employed to optimize the training process because it\nis a widely recognized hyperparameter optimization tool for\nmachine learning models. The objective function, hyper-\nparameter search space, and optimization algorithm were\ndefined using Optuna. The hyperparameters for training and\nregularization are detailed in Table 4, where some hyper-\nparameters possess fixed values, but others have a range of\nvalues. Optuna performs a random search during training\nto optimize the hyperparameters based on the listed values\nand then determines the optimal hyperparameter. Both the\n51938 VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nTABLE\n5. Overall experimental results.\nVOLUME 11, 2023 51939\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nFIGURE 7. Experimental results on fine-grained HOI using different modality features.\nFIGURE 8. Experimental results on general actions using different modality features.\nsingle-modality based transformer encoder and the proposed\nSSRT were trained using categorical cross-entropy loss. The\nAdamW [48] optimizer, which had a learning rate of 0.00001,\nwas chosen for network optimization, as shown in Table 4.\nV. EXPERIMENTAL RESULT\nThe comprehensive experimental results obtained from this\nresearch are presented in Table 5. This section evaluates\nSSRT in comparison to both single-modality HAR models\nand multimodality HAR models. It is organized into two sub-\nsections. The first subsection, V-A, examines the extensive\nexperimental results depicted in Table 5. Concurrently, the\nsecond subsection, V-B, assesses the three proposed Trans-\nformer architectures for the skeleton encoder, RGB encoder,\nand SSRT within the scope of all three HAR tasks.\nA. COMPREHENSIVE EXPERIMENTAL OUTCOMES\nTable 5 highlights that when utilizing only the skeleton\nmodality, the transformer encoder achieves superior accu-\nracy in fine-grained HOI action recognition and cross-dataset\naction recognition tasks. In addition, the transformer encoder\nyields comparable results to the LSTM model in general HAR\n51940 VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nFIGURE 9. Experimental results on cross-dataset actions using different modality features.\ntasks. For the fine-grained HOI action recognition task, the\ntransformer encoder displays a substantial improvement with\na 7.21% increase in accuracy and a 6.69% enhancement in\nF1 score compared to the LSTM model. Furthermore, the\ntransformer encoder demonstrates a significant advantage\nover the LSTM-based model when solely relying on the\nRGB modality in all action recognition tasks, achieving up\nto a 32.5% improvement in F1 score for the general HAR\ntask. These observations suggest that the transformer encoder,\nwhich employs the multi-head attention mechanism, is more\neffective at capturing human actions than the traditionally\nused LSTM model.\nInterestingly, when compared to the best-performing\nsingle-modality HAR model, the RGB-based modality out-\nperforms the skeleton modality in fine-grained HOI tasks,\nwhereas the skeleton modality achieves better results in gen-\neral and cross-action HAR tasks. The reasons behind this are:\n1) Lack of spatial information in skeleton modality:\nUnlike the RGB modality, the skeleton modality does\nnot encode spatial features, rendering it unable to com-\nprehend human-object interactions within fine-grained\nHOI tasks. As a result, the RGB modality surpasses the\nskeleton modality in fine-grained HOI action classes.\n2) Skeleton Modality Captures Human Motions Better\nthan RGB Modality: The skeleton modality is better\nat capturing human movement since it encodes various\njoint and limb movements. Consequently, the skeleton\nmodality outperforms the RGB modality in general\nHAR tasks.\n3) Skeleton modality is more robust than RGB modal-\nity: The RGB modality faces challenges due to envi-\nronmental diversity, such as changes in illumination.\nIn contrast, the skeleton modality is robust to varia-\ntions in clothing textures, and illumination, and is also\nscale-invariant. Thus, in cross-dataset HAR tasks, the\nRGB modality is more affected than the skeleton\nmodality.\nAs illustrated in Table 5, the LSTM late score fusion\nmodel exhibits the poorest performance in all HAR tasks.\nMoreover, it is particularly noteworthy that both late score\nfusion models (LSTM late score fusion and transformer late\nscore fusion) experience a decrease in all evaluation metrics\nwhen compared to their corresponding single-modality HAR\nmethods. In contrast, the state-of-the-art multimodal fusion\nmethod, Transformer Early Concatenation, enhanced both\naccuracy and F1 score when compared to single-modality\nmethods.\nOur proposed method, SSRT, achieved the best results\nin all evaluation metrics for all HAR tasks. SSRT outper-\nformed Transformer Concatenation in accuracy by 6.32%\nin the fine-grained HOI action recognition task, 4.04%\nin the general action recognition task, and 6.56% in the\ncross-dataset action recognition task. Similarly, SSRT sur-\npassed Transformer Concatenation in F1 score by 6.74%\nin the fine-grained HOI action recognition task, 4.12% in\nthe general action recognition task, and 12.19% in the\ncross-dataset action recognition task. SSRT outperformed\nthe Transformer Concatenation multimodal fusion primarily\nbecause it utilizes both LSTM and transformer encoder to\nextract higher temporal features from both modalities. Fur-\nthermore, SSRT employs two levels of fusion stages com-\npared to the single-stage multimodal fusion in Transformer\nEarly Concatenation. This study examines the multimodal\nfusion in greater depth in Section VI-B.\nFrom these experiments, we can observe that compared to\ngeneral HAR tasks, SSRT shows significant improvement in\nfine-grained HOI and cross-dataset actions. The reasons for\nthis are:\nVOLUME 11, 2023 51941\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\n1) Performance analysis of SSRT on fine-grained\nHOI task: Fine-grained HOI actions are complex in\nnature, as they require an understanding of various\nhuman-object interactions that share similar human\nactions. Capturing fine-grained HOI actions using\na single modality is challenging since the skeleton\nmodality lacks spatial understanding, and the RGB\nmodality is not as effective in capturing human actions.\nFurthermore, the diverse nature of these two modalities\nmakes it difficult to fuse them using simple fusion\nmethods (such as late score fusion). By employing\nSSRT, we were able to complement the RGB and skele-\nton modalities, resulting in significant improvement in\nthe fine-grained HAR task.\n2) Performance analysis of SSRT on general HAR\ntask: General action classes do not contain complex\nactions. As these actions do not involve fine-grained\nHOI actions, achieving good accuracy in these action\nclasses is possible using the skeleton modality alone.\nAlthough the improvement observed in this HAR task\nwith SSRT is not as substantial as in other tasks, it is\nstill remarkable.\n3) Performance analysis of SSRT on cross-dataset\nHAR task: As mentioned in Section IV-A2, both the\nskeleton and RGB modalities face challenges in this\ntask. The efficient integration of the skeleton and RGB\nmodalities through SSRT demonstrates a significant\nimprovement in overall accuracy compared to alterna-\ntive HAR methods. Despite SSRT’s substantial outper-\nformance of other HAR models in this task, the overall\naccuracy remains relatively low.\nAdditionally, the ablation study (Section VI-A) examines\nthe performance of a single modality HAR model with SSRT,\nproviding an in-depth analysis by comparing the accuracy of\neach action class.\nB. TRANSFORMER ARCHITECTURE COMPARISION\nFigure 7, Figure 8, and Figure 9 provide a comprehen-\nsive comparison of the performance of three distinct trans-\nformer architectures employed in the skeleton encoder, RGB\nencoder, and SSRT. These performances are represented as\nbar plots for the fine-grained HOI task, general HAR task, and\ncross-dataset HAR task, respectively. From these bar plots,\nit is clear that X2 is the optimal transformer architecture for\nboth the RGB encoder and SSRT in all three HAR tasks.\nIn contrast, for the skeleton encoder, X1 performs best in\nfine-grained HOI and general actions, while X3 excels in\ncross-dataset actions. These results emphasize that although\nthe X2 architecture can be recommended for every HAR task\ninvolving the SSRT and RGB encoder, the same is not true\nfor the skeleton encoder, as no single transformer architecture\nconsistently outperforms others across all HAR tasks.\nIn the fine-grained HOI task, when using a single-modality\nfeature, the RGB modality surpassed the skeleton modal-\nity, showing a 3.06% increase in accuracy and a 4.46%\nimprovement in the F1 score. The best-performing trans-\nformer architecture variant of SSRT significantly outper-\nformed the corresponding version of the RGB encoder,\nwith an impressive 9.92% enhancement in accuracy and a\n9.86% boost in the F1 score. In the general action task,\nwhen utilizing the skeleton modality, the X1 version of\nthe transformer encoder achieved better outcomes com-\npared to the X2 version. The highest-performing transformer\narchitecture variant of SSRT considerably outperformed the\ncorresponding version of the skeleton encoder, with a notable\n6.86% enhancement in accuracy and a 9.86% boost in\nthe F1 score. Finally, for the cross-dataset actions task,\nthe optimal transformer architecture version of the skele-\nton encoder outperformed the optimal version of the RGB\nencoder with an accuracy difference of 4.3%, although both\noptimal versions of the skeleton and RGB encoder exhib-\nited comparable F1 scores. The top-performing transformer\narchitecture variant of SSRT exceeded the best-performing\nskeleton encoder model in accuracy by 11.08% and outper-\nformed the best-performing RGB encoder with a 15.06%\nimprovement in the F1 score. Moreover, from these bar\nplots, it can be observed that SSRT consistently outperformed\nboth the skeleton and RGB modalities across all transformer\narchitectures.\nFrom these results, a key insight can be drawn: the effec-\ntiveness of each modality and transformer architecture vari-\nant is highly dependent on the specific HAR task. In the\nfine-grained HOI task, the RGB modality outperforms the\nskeleton modality, whereas, in the general action task, certain\nskeleton encoder variants deliver superior results. Addition-\nally, the optimal transformer architecture for the skeleton\nencoder varies depending on the task, reinforcing the idea\nthat a one-size-fits-all approach is not ideal. Moreover, the\nSSRT consistently outperforms both the skeleton and RGB\nmodalities across all transformer architectures, indicating its\npotential as a robust and versatile solution for various HAR\ntasks.\nVI. ABLATION STUDY\nA. SKELETON VS. RGB VS. MULTIMODAL MODALITIES\nIn this section, a comparison of performance from action\nrecognition models employing skeleton modality, RGB\nmodality, and multimodality is conducted across all three\nHAR tasks. To achieve this, optimal transformer architecture\nversions of the skeleton encoder, the RGB encoder, and SSRT\nwere selected for evaluation.\n1) FINE-GRAINED HOI\nFigure 10 offers a performance comparison of the skele-\nton modality, the RGB modality, and multimodality in\nfine-grained HOI recognition. In order to test the performance\nof the various modalities, we selected 37 samples from each\nfine-grained HOI class.\nThe confusion matrix depicted in Figure 10a showcases\nthe results of fine-grained HOI classification performed by\n51942 VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nFIGURE 10. Comparative analysis of confusion matrices for fine-grained HOI classification: (a). Skeleton, (b). RGB, and (c). Multimodal\n(Skeleton + RGB) approaches.\nthe skeleton encoder. This figure demonstrates that the Drink\nFrom Cup actionattained the highest classification accuracy\n(83.78%) by identifying 31 samples. Similarly, the skeleton\nencoder accurately classified 29 samples of the Drink From\nBottle action attaining an accuracy of 78.38%. Conversely,\nthe Drink From Canclass exhibited a diminished classifica-\ntion performance with only 19 samples classified accurately,\nresulting in an accuracy of 51.35%. From the confusion\nmatrix results, we observe a marked variation in accuracy\nacross different fine-grained HOI classes. This can be mainly\nattributed to the skeleton modality’s inability to encode\nfine-grained HOI spatial information. Consequently, the high\naccuracy achieved by the skeleton modality may be unreli-\nable, possibly resulting from coincidental matches rather than\na genuine understanding of the underlying patterns.\nNext, as observed from the confusion matrix in Figure 10b,\nthe RGB encoder classified 28 samples of Drink From Can\nand Drink From Cup with an accuracy of 75.67%, while\nmisclassifying only one additional sample in the Drink From\nBottle class. This observation highlights that, in contrast to\nthe skeleton modality, the RGB modality did not display a\nbiased accuracy towards any specific action class. The results\nindicate that RGB modality-based HAR models are more\ncapable of understanding complex HOI patterns compared to\nskeleton modality. Therefore, if only a single modality must\nbe used, this study suggests employing RGB modality-based\nHAR models for fine-grained HOI actions.\nThe performance of SSRT is displayed in the confusion\nmatrix in Figure 10c. SSRT classified 32 samples in the\nDrink From Canaction class with an accuracy of 86.48%.\nIt misclassified only one fewer sample in the Drink from\nBottle and Drink from Cupaction classes compared to the\nDrink from Canaction class. Furthermore, when compared\nto overall accuracy, Table 5 shows that the RGB modality\nimproved accuracy by 3.6% compared to skeleton modality,\nand multimodality improved accuracy by 9.92% compared\nto RGB modality in this human activity recognition task.\nSimilar to the RGB modality, the multimodal setting does\nnot display biased performance towards any specific class.\nFIGURE 11. Accuracy shift: Multimodality (SSRT) vs. Single modality\n(Transformer Encoder) for general actions recognition.\nHowever, the SSRT modality demonstrates improved accu-\nracy for each action. This is due to the fact that while the\nRGB modality can comprehend fine-grained HOI, it struggles\nto encode human movement effectively. Consequently, SSRT,\nwhich complements both RGB and skeleton modalities, out-\nperforms the RGB modality in the fine-grained HOI task.\n2) GENERAL ACTIONS AND CROSS-DATASET ACTIONS\nThis section highlights the change in accuracy for\neach action class when employing SSRT as opposed to\nsingle-modality transformer encoders (skeleton and RGB)\nin Figures 11 and 12 for general actions and cross-dataset\nactions, respectively. The x-axis lists the action class,\nwhile the y-axis displays the change in accuracy between\nsingle-modality and multimodalities based HAR models. The\norange bar plot illustrates the difference in accuracy when\nutilizing the skeleton encoder in comparison to SSRT, and the\nblue bar plot demonstrates the difference in accuracy when\nusing the RGB encoder as opposed to SSRT.\nIn the general actions recognition task, integrating the\nskeleton and RGB modalities led to the most significant\nperformance enhancement for the Uselaptop class, with a\n21.15% increase in accuracy compared to employing only\nthe skeleton modality. Conversely, the Drink From Cupclass\nexperienced the smallest performance improvement, with an\nVOLUME 11, 2023 51943\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nFIGURE 12. Accuracy shift: Multimodality (SSRT) vs. Single modality\n(Transformer Encoder) in cross-dataset actions recognition.\naccuracy increase of just 1.72% when adopting multimodality\ninstead of skeleton modality. When examining the change\nin accuracy between multimodality and RGB modality, the\nReadbook action showcased the most notable performance\nimprovement, with SSRT boosting accuracy by 19.32% com-\npared to the RGB encoder. In contrast, the Drink From\nCup class displayed the smallest improvement when using\nmultimodality in comparison to the RGB modality. It can\nbe observed that for general actions tasks, in three out of\nfour actions, SSRT demonstrated greater improvement when\ncompared to using RGB modality alone. This is because\nthese actions do not pose challenges for the skeleton modal-\nity, as they can also be classified without utilizing spatial\ninformation.\nIn cross-dataset action recognition, the Readbook class\nexperienced the most substantial performance enhancement,\nwith a 56.8% increase in accuracy when employing multi-\nmodality as opposed to utilizing only the RGB modality. Nev-\nertheless, a 20% decline in accuracy was noted for the same\nclass when adopting multimodality over skeleton modality.\nThe Drink From Cup class exhibited the most substantial\nnegative impact on performance when using multimodality\ninstead of RGB modality, resulting in a considerable decrease\nin accuracy of 24.24%. In contrast, the multimodality showed\nbetter performance in the Usetelephone class compared to the\nskeleton modality, with an improved accuracy of 46.3%.\nAlthough, as illustrated in Table 5, SSRT outperformed\nevery other HAR in cross-dataset HAR tasks, it can be seen\nfrom Figure 12 that when compared with the accuracy of\neach class, SSRT negatively impacts the accuracy of all action\nclasses except for the Uselaptop class when compared to the\naccuracy obtained from the best-performing single-modality\n(skeleton or RGB) HAR model for each class. This is mainly\ndue to the challenges in cross-dataset HAR tasks, as discussed\nin Section IV-A2.\nB. SSRT VS. OTHER FUSION METHODS\nThis study compares the accuracy of four multimodal fusion\ntechniques. In Figure 13, the x-axis displays the fusion\nmethods, while the y-axis presents their accuracy for three\naction recognition tasks. The blue and orange bar plots rep-\nresent the accuracy of fine-grained HOI and general actions,\nFIGURE 13. Comparison of multimodality fusion methods for HAR.\nrespectively, and the line plot illustrates the accuracy of cross-\ndataset actions.\nThe findings reveal that, among all four multimodal meth-\nods, LSTM late score fusion consistently exhibited the lowest\naccuracy across all HAR tasks.Both late score fusion tech-\nniques (LSTM and Transformer) exhibited reduced accuracy\ncompared to early fusion (Transformer Early Concatenation)\nand SSRT. Moreover, late score fusions were outperformed\nby their respective single-modality counterparts, as shown in\nTable 5. This table reveals that Transformer late score fusion\nmodality experiences the most significant drop in accuracy\nfor fine-grained HOI actions, decreasing by 16.18% com-\npared to the RGB encoder. However, it suffers the least in\ngeneral actions HAR tasks, with a decrease of only 2.69%\nwhen compared to the skeleton encoder. The primary reasons\nfor these outcomes can be attributed to:\n1) Distinct nature of RGB and skeleton modality fea-\ntures: RGB and skeleton modality features exhibit\nheterogeneity. Although these two features can com-\nplement each other in HAR tasks, their distinct nature\nmakes fusion more challenging. To tackle this diversity\nin feature modalities, a more sophisticated fusion tech-\nnique must be employed.\n2) Inadequacy of late score fusion: As outlined in\nSection IV-B2, both late score fusion methods imple-\nmented in this research work merely sum the proba-\nbility scores from individual single-modality models\nto obtain the final prediction score. In this multimodal\nfusion approach, fusion only takes place at the final\nstage, which is insufficient for effectively capturing the\nnuanced interplay between the two modalities. As a\nresult, this fusion method struggles significantly in\nfine-grained HOI actions where understanding both\nRGB and skeleton features is essential. Consequently,\nlate score fusion methods fail to supplement RGB and\nskeleton modality features, and instead, they negatively\naffect the overall performance in comparison to indi-\nvidual single-modality approaches.\nTransformer Early Concatenation, a state-of-the-art multi-\nmodal fusion method, demonstrated a significant increase in\naccuracy when compared to the Transformer late score fusion\nmethod. The improvement was 19.78% for fine-grained\n51944 VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nHOI, 5.35% for general actions, and 11.69% for cross-\ndataset actions. Furthermore, this fusion approach substan-\ntially enhanced both accuracy and F1 score for all HAR tasks\nwhen compared to the best single-modality HAR model. One\nreason for this improvement is that this method employs a\nmulti-head attention mechanism to complement RGB and\nskeleton modalities.\nSSRT outperformed the other three fusion methods, includ-\ning Transformer Early Concatenation, showcasing 6.32%\nhigher accuracy for fine-grained HOI, 4.04% higher accuracy\nfor general actions, and 6.56% higher accuracy for cross-\ndataset actions. Two main reasons why SSRT is a better fusion\nmethod than Transformer Early Concatenation are:\n1) Early concatenation vs. Cross-attention: As dis-\ncussed in Section II-B1, the transformer cross-attention\nmechanism is much more efficient in understand-\ning features from two different modalities. This is\nbecause early concatenation simply employs a single\ntransformer encoder to fuse concatenated multimodal\nfeatures, whereas SSRT allows skeleton and RGB\nmodalities to attend to each other bidirectionally. This\nprocess is achieved by exchanging the key (K) and\nvalue (V) vectors of one modality with the query\n(Q) sequences of another modality within multiple\nstream transformer layers.\n2) Single-stage fusion vs. Two-stage fusion: SSRT\nemploys two stages of fusion, which are transformer\ncross-attention and Softmax layer late score fusion,\nwhile Transformer Early Concatenation only employs\na single stage of multimodal fusion.\nC. THE EFFECTIVENESS OF THE LSTM COMPONENT\nIN SSRT\nThis section examines the influence of LSTM on the pro-\nposed method. To do this, all experiments in this paper were\nperformed again by substituting the LSTM component with\ntraditional positional encoding. The effects of LSTM on\nSSRT are depicted using a bar plot in Figure 14. The x-axis\nin Figure 14 represents the three action recognition tasks,\nwhile the y-axis displays the accuracy and F1 score values\nfor each task. The orange bar shows the accuracy and F1\nscore of the proposed SSRT, whereas the blue bar represents\nthe accuracy and F1 score of SSRT with positional encoding\ninstead of LSTM.\nThe results indicate that the integration of LSTM has the\nmost significant influence on fine-grained HOI action recog-\nnition tasks, contributing to a 6.32% increase in accuracy\nand a 6.38% improvement in the F1 score. The smallest\nimpact is observed in general action recognition tasks, with\nincreases of 3.36% in accuracy and 3.44% in F1 score. For\ncross-dataset action recognition tasks, the implementation of\nLSTM enhances the overall robustness of SSRT, resulting in\na 5.37% improvement in accuracy and a 5.26% boost in the\nF1 score.\nThese findings imply that the combination of an LSTM\nwith a transformer encoder generates more effective temporal\nFIGURE 14. Performance comparison of LSTM vs Positional encoding.\nabstract features for each modality. This outcome can be\nascribed to the unique sequential processing approach of\nLSTM. Unlike positional encoding, LSTM processes each\ninput instance in a sequential manner, enabling it to capture\ntemporal information more effectively. On the other hand,\nthe positional encoding layer assigns each element’s position\nbased on a predefined sinusoidal function and subsequently\nprocesses the entire input sequence in parallel using the\ntransformer encoder. As a result, positional encoding doesn’t\ncapture temporal dependencies as efficiently as LSTM.\nRemarkably, SSRT consistently outperforms the Trans-\nformer Early Concatenation fusion method across all action\nrecognition tasks, even when only positional encoding layers\nare incorporated. This superiority is particularly evident in\nfine-grained HOI actions, where a significant improvement\nis noted. This result further validates the superiority of SSRT\nas a fusion method, as detailed in Section VI-B. It is also\nnoteworthy that when LSTM is employed solely as the HAR\nmodel, its performance is somewhat lackluster, as illustrated\nin Table5. The study demonstrates that when a transformer\nencoder is integrated with LSTM, a notable improvement is\nobserved, corroborating the findings of this research work.\nVII. DISCUSSION\nThis study highlights that the SSRT method surpasses both\nsingle-modality and multimodality HAR models in three\naction recognition tasks, as shown in Table 5. The most\nsignificant impact of SSRT is evident in fine-grained HOI\naction recognition, where it not only achieved a considerable\nincrease in accuracy but also displayed consistent enhance-\nments across all action classes within the same coarse label.\nThe proposed method also generalizes well to action classes\nbeyond fine-grained HOI and proved most robust when eval-\nuated on action classes from other datasets. However, it is\nimportant to note that, except for the Uselaptop class, SSRT\nadversely influences the accuracy of all action classes when\ncompared to the top-performing single-modality (skeleton or\nRGB) HAR model for each class. This can mainly be ascribed\nto the inherent challenges associated with cross-dataset HAR\ntasks. In addition to these challenges, the reason for SSRT\nnot performing exceptionally well in cross-dataset actions\nmay be due to its inability to extract superior higher-level\nVOLUME 11, 2023 51945\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nfeatures from each modality before performing multimodal\nfusion.From the overall experiments conducted, the SSRT\nmethod demonstrates state-of-the-art results for multimodal\nfusion techniques; however, SSRT can still be improved by\nutilizing better higher-level features from both modalities and\nsubsequently integrating them using the proposed two-stage\nfusion method. This can be achieved in the following ways:\n1) Utilization of 3D skeleton features: In this study,\nwe implemented Alphapose human pose estimation,\na popular human skeleton feature estimator, to extract\n2D skeleton features. Although 2D skeleton features\nare computationally efficient and simpler to implement,\nthey are not as robust as 3D skeleton features. 3D\nskeleton features provide depth information in addition\nto 2D skeleton features, resulting in a more accurate\nrepresentation in 3D space. 3D poses are more robust to\nthe shape and size of humans as well as varying camera\nangles and heights. For instance, An et al. [55] uti-\nlize multimodal approaches, encompassing mmWave,\nRGB-D, and inertial sensors, to achieve superior 3D\nhuman pose estimation representations, which are\nnotably more robust than conventional 2D skeleton\nfeatures. This information could be vital in addressing\nthe challenges faced by SSRT in cross-dataset actions.\nIn our future work, we plan to use state-of-the-art\n3D skeleton feature extraction methods such as those\nproposed in [49] and [55].\n2) Selection of better baseline for higher fea-\ntures extraction: In this study, we employed a\ntransformer-based architecture as the baseline for\nhigher-level feature extraction, with the rationale\noutlined in Section IV-B1. Although, implemented\ntransformer-based architecture shows comparable\nstate-of-the-art results for both RGB and skeleton\nmodality but this architecture may not be the best\nsolution for each modality. For instance, Graph Convo-\nlutional Networks like STGCN and 2S-AGCN, specif-\nically tailored for processing skeleton features, might\nextract more intricate skeleton data compared to the\ntransformer baseline. Likewise, pre-trained 3D Con-\nvolutional Networks, such as I3D [4], could poten-\ntially derive superior features from the RGB modality.\nIn future work, we plan to investigate the integration of\ndiverse state-of-the-art HAR models to extract sophis-\nticated higher-level features, followed by employing\nthe proposed two-stage fusion for enhanced feature\nintegration. This approach may bolster SSRT’s ability\nto comprehend cross-dataset actions more effectively.\nFurthermore, we will examine the combination of\nvarious HAR baselines to achieve cutting-edge perfor-\nmance on prominent HAR datasets, such as [14], [27],\nand [28].\nThe experimental results demonstrate that neither the\nskeleton nor the RGB modality consistently outperforms\nthe other, as performance varies depending on the action\nrecognition task. In fine-grained HOI action classification,\nthe RGB modality excels, likely because the skeleton modal-\nity has difficulty recognizing similar motion dynamics.\nIn contrast, the skeleton modality fares better in general\nHuman Activity Recognition (HAR) and cross-dataset HAR\ntasks, potentially due to more distinct movement patterns.\nIn cross-dataset HAR tasks, the RGB modality encounters\nchallenges arising from significant differences in the required\nRGB features.\nAs highlighted in Section V-A, the experimental study\nconducted here suggests that multimodal HAR models do not\nalways surpass single-modality models in performance. Late\nscore fusion models, such as Transformer Late Score Fusion\nand LSTM Late Score Fusion, exhibited decreased overall\naccuracy and F1 Score compared to their single-modality\ncounterparts. The proposed SSRT outperformed the state-of-\nthe-art Transformer Early Concatenation in all tasks, even\nwith only the traditional positional encoding and without\nutilizing the LSTM. It is important to note, however, that\nemploying the LSTM in place of positional encoding substan-\ntially improved SSRT’s performance.\nLastly, this study’s primary limitation concerning\nfine-grained HOI is the scarcity of available datasets. Apart\nfrom the fine-grained HOI of drinking from the Toyota smart\nhome dataset, no other related datasets were found for this\ntask. As part of our future work, we aim to collect more\nfine-grained HOI action classes to enhance dataset diversity.\nVIII. CONCLUSION\nIn this research, SSRT (a novel method specifically designed\nfor fine-grained HOI recognition) is introduced by integrating\nskeleton and RGB modalities. SSRT first obtains abstract\ntemporal features from each modality using an LSTM and\na transformer encoder. Subsequently, SSRT employs two\nfusion stages: cross-attention multimodality fusion and Soft-\nmax late score fusion for effective feature integration.\nThe study demonstrates that SSRT outperforms state-of-\nthe-art single-modality HAR models (such as transformer\nencoders) and multimodal based HAR models (such as\nTransformer Early Concatenation) in fine-grained HOI tasks\nwithout any bias towards a particular action class. More-\nover, SSRT also excels in general HAR and cross-dataset\nHAR tasks. The research highlights the significant accu-\nracy improvement achieved by incorporating an LSTM layer\ninstead of a positional encoder layer in SSRT across all three\nHAR tasks.\nAdditionally, this study compared the performance of\nskeleton modality and RGB modality across all three HAR\ntasks. It revealed that the RGB modality outperformed in a\nfine-grained HOI task, while the skeleton modality exhibited\nbetter results in general and cross-dataset HAR tasks.\nLastly, the adaptability of SSRT enables it to merge various\nmodalities for diverse purposes, including Vision-Language\ntasks. Thus, in future work, other researchers can explore\nSSRT’s potential for combining different modalities to serve\nvarious purposes.\n51946 VOLUME 11, 2023\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\nREFERENCES\n[1] J. K. Aggarwal and M. S. Ryoo, ‘‘Human activity analysis: A review,’’\nACM Comput. Surv., vol. 43, no. 3, pp. 1–43, 2011.\n[2] M. R. Sudha, K. Sriraghav, S. S. Abisheck, S. G. Jacob, and S. Manisha,\n‘‘Approaches and applications of virtual reality and gesture recognition:\nA review,’’ Int. J. Ambient Comput. Intell., vol. 8, no. 4, pp. 1–18,\nOct. 2017.\n[3] D. Weinland, R. Ronfard, and E. Boyer, ‘‘A survey of vision-based methods\nfor action representation, segmentation and recognition,’’ Comput. Vis.\nImage Understand., vol. 115, no. 2, pp. 224–241, Feb. 2011.\n[4] J. Carreira and A. Zisserman, ‘‘Quo vadis, action recognition? A new\nmodel and the kinetics dataset,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jul. 2017, pp. 6299–6308.\n[5] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,\nS. Venugopalan, T. Darrell, and K. Saenko, ‘‘Long-term recurrent\nconvolutional networks for visual recognition and description,’’ in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015,\npp. 2625–2634.\n[6] V . Veeriah, N. Zhuang, and G. Qi, ‘‘Differential recurrent neural networks\nfor action recognition,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\nDec. 2015, pp. 4041–4049.\n[7] C. Feichtenhofer, H. Fan, J. Malik, and K. He, ‘‘SlowFast networks for\nvideo recognition,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2019, pp. 6202–6211.\n[8] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucic, and C. Schmid,\n‘‘ViViT: A video vision transformer,’’ in Proc. IEEE/CVF Int. Conf. Com-\nput. Vis. (ICCV), Oct. 2021, pp. 6836–6846.\n[9] F. Becattini, T. Uricchio, L. Seidenari, L. Ballan, and A. D. Bimbo, ‘‘Am\nI done? Predicting action progress in videos,’’ ACM Trans. Multimedia\nComput., Commun., Appl., vol. 16, no. 4, pp. 1–24, Nov. 2020.\n[10] Y . Zheng, X. Li, and X. Lu, ‘‘Unsupervised learning of human action cate-\ngories in still images with deep representations,’’ ACM Trans. Multimedia\nComput., Commun., Appl., vol. 15, no. 4, pp. 1–20, Nov. 2019.\n[11] G. Moon, H. Kwon, K. M. Lee, and M. Cho, ‘‘IntegralAction: Pose-driven\nfeature integration for robust human action recognition in videos,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW),\nJun. 2021, pp. 3339–3348.\n[12] X. Weiyao, W. Muqing, Z. Min, and X. Ting, ‘‘Fusion of skeleton and RGB\nfeatures for RGB-D human action recognition,’’ IEEE Sensors J., vol. 21,\nno. 17, pp. 19157–19164, Sep. 2021.\n[13] X. Zhu, Y . Zhu, H. Wang, H. Wen, Y . Yan, and P. Liu, ‘‘Skeleton sequence\nand RGB frame based multi-modality feature fusion network for action\nrecognition,’’ACM Trans. Multimedia Comput., Commun., Appl., vol. 18,\nno. 3, pp. 1–24, Aug. 2022.\n[14] S. Das, R. Dai, M. Koperski, L. Minciullo, L. Garattoni, F. Bremond, and G.\nFrancesca, ‘‘Toyota smarthome: Real-world activities of daily living,’’ in\nProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 833–842.\n[15] S. Das, S. Sharma, R. Dai, F. Bremond, and M. Thonnat, ‘‘VPN: Learning\nvideo-pose embedding for activities of daily living,’’ in Proc. Eur. Conf.\nComput. Vis. (ECCV), 2020, pp. 72–90.\n[16] S. Das, R. Dai, D. Yang, and F. Bremond, ‘‘VPN++: Rethinking video-\npose embeddings for understanding activities of daily living,’’ IEEE Trans.\nPattern Anal. Mach. Intell., vol. 44, no. 12, pp. 9703–9717, Dec. 2022.\n[17] Z. Wu, Y .-G. Jiang, X. Wang, H. Ye, and X. Xue, ‘‘Multi-stream multi-\nclass fusion of deep networks for video classification,’’ in Proc. 24th ACM\nInt. Conf. Multimedia, Oct. 2016, pp. 791–800.\n[18] S. Zhang, Y . Yang, J. Xiao, X. Liu, Y . Yang, D. Xie, and Y . Zhuang, ‘‘Fusing\ngeometric features for skeleton-based action recognition using multilayer\nLSTM networks,’’IEEE Trans. Multimedia, vol. 20, no. 9, pp. 2330–2343,\nSep. 2018.\n[19] R. Zhao, H. Ali, and P. van der Smagt, ‘‘Two-stream RNN/CNN for action\nrecognition in 3D videos,’’ in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.\n(IROS), Sep. 2017, pp. 4260–4267.\n[20] M. Zolfaghari, G. L. Oliveira, N. Sedaghat, and T. Brox, ‘‘Chained multi-\nstream networks exploiting pose, motion, and appearance for action clas-\nsification and detection,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\nOct. 2017, pp. 2904–2913.\n[21] S. Yan, Y . Xiong, and D. Lin, ‘‘Spatial temporal graph convolutional\nnetworks for skeleton-based action recognition,’’ in Proc. AAAI Conf. Artif.\nIntell., 2018, vol. 32, no. 1, pp. 1–9.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[23] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier,\nS. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman,\nand A. Zisserman, ‘‘The kinetics human action video dataset,’’ 2017,\narXiv:1705.06950.\n[24] H. S. Koppula, R. Gupta, and A. Saxena, ‘‘Learning human activities and\nobject affordances from RGB-D videos,’’ Int. J. Robot. Res., vol. 32, no. 8,\npp. 951–970, Jul. 2013.\n[25] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, ‘‘HMDB:\nA large video database for human motion recognition,’’ in Proc. Int. Conf.\nComput. Vis. (ICCV), Nov. 2011, pp. 2556–2563.\n[26] J. Wang, Z. Liu, Y . Wu, and J. Yuan, ‘‘Mining actionlet ensemble for action\nrecognition with depth cameras,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., Jun. 2012, pp. 1290–1297.\n[27] A. Shahroudy, J. Liu, T. Ng, and G. Wang, ‘‘NTU RGB+D: A large scale\ndataset for 3D human activity analysis,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2016, pp. 1010–1019.\n[28] J. Jang, D. Kim, C. Park, M. Jang, J. Lee, and J. Kim, ‘‘ETRI-Activity3D:\nA large-scale RGB-D dataset for robots to recognize daily activities of\nthe elderly,’’ in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS),\nOct. 2020, pp. 10990–10997.\n[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst. (NIPS), 2017, vol. 30, no. 1, pp. 5998–6008.\n[30] J. Lu, D. Batra, D. Parikh, and S. Lee, ‘‘ViLBERT: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language tasks,’’ in\nProc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 32, 2019, pp. 13–23.\n[31] C. Sun, A. Myers, C. V ondrick, K. Murphy, and C. Schmid, ‘‘VideoBERT:\nA joint model for video and language representation learning,’’ in Proc.\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 7464–7473.\n[32] H.-S. Fang, S. Xie, Y .-W. Tai, and C. Lu, ‘‘RMPE: Regional multi-person\npose estimation,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017,\npp. 2334–2343.\n[33] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[34] P. Xu, X. Zhu, and D. A. Clifton, ‘‘Multimodal learning with transformers:\nA survey,’’ 2022, arXiv:2206.06488.\n[35] K. Gavrilyuk, R. Sanford, M. Javan, and C. G. M. Snoek, ‘‘Actor-\ntransformers for group activity recognition,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 839–848.\n[36] J. Do and M. Kim, ‘‘Multi-modal transformer for indoor human action\nrecognition,’’ in Proc. 22nd Int. Conf. Control, Autom. Syst. (ICCAS),\nNov. 2022, pp. 1155–1160.\n[37] J. Shi, Y . Zhang, W. Wang, B. Xing, D. Hu, and L. Chen, ‘‘A novel\ntwo-stream transformer-based framework for multi-modality human action\nrecognition,’’Appl. Sci., vol. 13, no. 4, p. 2058, Feb. 2023.\n[38] T. Rahman, M. Yang, and L. Sigal, ‘‘TriBERT: Human-centric audio-visual\nrepresentation learning,’’ in Proc. Adv. Neural Inf. Process. Syst. (NIPS),\nvol. 34, 2021, pp. 9774–9787.\n[39] S. Yan, X. Xiong, A. Arnab, Z. Lu, M. Zhang, C. Sun, and C. Schmid,\n‘‘Multiview transformers for video recognition,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 3333–3343.\n[40] M. Ijaz, R. Diaz, and C. Chen, ‘‘Multimodal transformer for nursing activ-\nity recognition,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\nWorkshops (CVPRW), Jun. 2022, pp. 2065–2074.\n[41] W. Zhang, F. Qiu, S. Wang, H. Zeng, Z. Zhang, R. An, B. Ma, and Y . Ding,\n‘‘Transformer-based multimodal information fusion for facial expression\nanalysis,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Work-\nshops (CVPRW), Jun. 2022, pp. 2428–2437.\n[42] D. Ahn, S. Kim, H. Hong, and B. Chul Ko, ‘‘STAR-transformer: A spatio-\ntemporal cross attention transformer for human action recognition,’’ in\nProc. IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2023,\npp. 3330–3339.\n[43] Y . Zhang, B. Wu, W. Li, L. Duan, and C. Gan, ‘‘STST: Spatial-temporal\nspecialized transformer for skeleton-based action recognition,’’ in Proc.\n29th ACM Int. Conf. Multimedia, Oct. 2021, pp. 3229–3237.\n[44] V . Mazzia, S. Angarano, F. Salvetti, F. Angelini, and M. Chiaberge,\n‘‘Action transformer: A self-attention model for short-time pose-based\nhuman action recognition,’’ Pattern Recognit., vol. 124, Apr. 2022,\nArt. no. 108487.\n[45] G. Bertasius, H. Wang, and L. Torresani, ‘‘Is space-time attention all you\nneed for video understanding?’’ in Proc. ICML, 2021, vol. 2, no. 3, p. 4.\nVOLUME 11, 2023 51947\nA. Ghimire et al.: SSRT to Recognize Fine-Grained HOIs and Action Recognition\n[46] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, ‘‘Optuna:\nA next-generation hyperparameter optimization framework,’’ in Proc. 25th\nACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Jul. 2019,\npp. 2623–2631.\n[47] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16 × 16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[48] I. Loshchilov and F. Hutter, ‘‘Decoupled weight decay regularization,’’\n2017, arXiv:1711.05101.\n[49] G. Rogez, P. Weinzaepfel, and C. Schmid, ‘‘LCR-Net ++: Multi-person\n2D and 3D pose detection in natural images,’’ IEEE Trans. Pattern Anal.\nMach. Intell., vol. 42, no. 5, pp. 1146–1161, May 2020.\n[50] J. Wang, X. Nie, Y . Xia, Y . Wu, and S.-C. Zhu, ‘‘Cross-view action\nmodeling, learning, and recognition,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., Jun. 2014, pp. 2649–2656.\n[51] Q. Cheng, Z. Liu, Z. Ren, J. Cheng, and J. Liu, ‘‘Spatial-temporal informa-\ntion aggregation and cross-modality interactive learning for RGB-D-Based\nhuman action recognition,’’ IEEE Access, vol. 10, pp. 104190–104201,\n2022.\n[52] L. Yuan, J. Andrews, H. Mu, A. Vakil, R. Ewing, E. Blasch, and J. Li,\n‘‘Interpretable passive multi-modal sensor fusion for human identification\nand activity recognition,’’ Sensors, vol. 22, no. 15, p. 5787, Aug. 2022.\n[53] C. Plizzari, M. Cannici, and M. Matteucci, ‘‘Skeleton-based action recog-\nnition via spatial and temporal transformer networks,’’ Comput. Vis. Image\nUnderstand., vols. 208–209, Jul. 2021, Art. no. 103219.\n[54] Z. Liu, H. Zhang, Z. Chen, Z. Wang, and W. Ouyang, ‘‘Disentangling\nand unifying graph convolutions for skeleton-based action recognition,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,\npp. 143–152.\n[55] S. An, Y . Li, and U. Ogras, ‘‘MRI: Multi-modal 3D human pose esti-\nmation dataset using mmWave, RGB-D, and inertial sensors,’’ 2022,\narXiv:2210.08394.\n[56] L. Shi, Y . Zhang, J. Cheng, and H. Lu, ‘‘Two-stream adaptive graph\nconvolutional networks for skeleton-based action recognition,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,\npp. 12026–12035.\n[57] G. Joshi, R. Walambe, and K. Kotecha, ‘‘A review on explainability in\nmultimodal deep neural nets,’’ IEEE Access, vol. 9, pp. 59800–59821,\n2021.\nAKASH GHIMIRE is currently pursuing the B.E.\ndegree with the Department of Integrated System\nEngineering, School of Global Convergence Stud-\nies, Inha University. His research interests include\nvideo understanding, utilizing deep learning, and\ncomputer vision techniques, with a focus on iden-\ntifying human actions through various methods.\nVIJAY KAKANI (Member, IEEE) received the\nB.Tech. degree in electronics and communication\nengineering from Jawaharlal Nehru Technological\nUniversity, Kakinada, India, in 2012, the M.S.\ndegree in computer and communication systems\nfrom the University of Limerick, Ireland, in 2014,\nand the Ph.D. degree in information and com-\nmunication engineering (major) and future vehi-\ncle engineering (minor) from Inha University,\nSouth Korea, in 2020. He is currently an Assistant\nProfessor with the Department of Integrated System Engineering, School of\nGlobal Convergence Studies, Inha University. His research interests include\nautonomous vehicles, sensor signal processing, applied computer vision,\ndeep learning, systems engineering, and machine vision applications.\nHAKIL KIM (Member, IEEE) received the\nM.Sc. and Ph.D. degrees in electrical and com-\nputer engineering from Purdue University, in\n1985 and 1990, respectively. In 1990, he joined the\nCollege of Engineering, Inha University, Incheon,\nSouth Korea, where he is currently a Full Professor\nwith the Department of Information and Commu-\nnication Engineering. In order to retain the balance\nbetween academic research and commercial devel-\nopment, he founded Vision Inc., in 2014, where\nhe is also the CEO. His research interests include biometrics, intelligent\nvideo surveillance, and embedded vision for autonomous vehicles. Since\n2003, he has been actively involved as a Project Editor in the International\nStandardization of Biometrics at ISO/IEC JTC1/SC37.\n51948 VOLUME 11, 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7787026166915894
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6778539419174194
    },
    {
      "name": "RGB color model",
      "score": 0.5990018844604492
    },
    {
      "name": "Softmax function",
      "score": 0.5073787569999695
    },
    {
      "name": "Transformer",
      "score": 0.4702906608581543
    },
    {
      "name": "Fusion mechanism",
      "score": 0.4431118369102478
    },
    {
      "name": "Modalities",
      "score": 0.4121793508529663
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3707807958126068
    },
    {
      "name": "Computer vision",
      "score": 0.33804938197135925
    },
    {
      "name": "Fusion",
      "score": 0.27367961406707764
    },
    {
      "name": "Deep learning",
      "score": 0.13393574953079224
    },
    {
      "name": "Engineering",
      "score": 0.07309192419052124
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Lipid bilayer fusion",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191879574",
      "name": "Inha University",
      "country": "KR"
    }
  ],
  "cited_by": 18
}