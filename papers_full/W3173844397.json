{
    "title": "Prevent the Language Model from being Overconfident in Neural Machine Translation",
    "url": "https://openalex.org/W3173844397",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3016996252",
            "name": "Mengqi Miao",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2133392087",
            "name": "Fandong Meng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2105151920",
            "name": "Yijin Liu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2192842991",
            "name": "Xiao-hua Zhou",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2093278426",
            "name": "Jie Zhou",
            "affiliations": [
                "Tencent (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2903728819",
        "https://openalex.org/W2963699608",
        "https://openalex.org/W2952479981",
        "https://openalex.org/W2888779557",
        "https://openalex.org/W2988249555",
        "https://openalex.org/W2909737760",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963551569",
        "https://openalex.org/W4287718112",
        "https://openalex.org/W2963366389",
        "https://openalex.org/W2994928925",
        "https://openalex.org/W2963174344",
        "https://openalex.org/W2964302946",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2905266130",
        "https://openalex.org/W2946379889",
        "https://openalex.org/W2986562961",
        "https://openalex.org/W2964345285",
        "https://openalex.org/W2946068894",
        "https://openalex.org/W2963876389",
        "https://openalex.org/W2956130159",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3101668578",
        "https://openalex.org/W2534200568",
        "https://openalex.org/W2117278770",
        "https://openalex.org/W3104273515",
        "https://openalex.org/W2971302374",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W222053410",
        "https://openalex.org/W2963442512",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2963281280",
        "https://openalex.org/W2952474700",
        "https://openalex.org/W2963652649",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W3035214886",
        "https://openalex.org/W2970849705",
        "https://openalex.org/W3105718208",
        "https://openalex.org/W1915251500",
        "https://openalex.org/W2904829696",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W3034368185",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3106185885",
        "https://openalex.org/W2998655810",
        "https://openalex.org/W2963260202",
        "https://openalex.org/W2962926939",
        "https://openalex.org/W2963545917",
        "https://openalex.org/W2953173959",
        "https://openalex.org/W2963463964",
        "https://openalex.org/W2996766022",
        "https://openalex.org/W2964308564"
    ],
    "abstract": "Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, Jie Zhou. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3456–3468\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3456\nPrevent the Language Model from being Overconﬁdent in\nNeural Machine Translation\nMengqi Miao1∗, Fandong Meng2∗, Yijin Liu2, Xiao-Hua Zhou3†, and Jie Zhou2\n1Peking University, China\n2Pattern Recognition Center, WeChat AI, Tencent Inc, China\n3Beijing International Center for Mathematical Research,\nNational Engineering Lab for Big Data Analysis and Applications,\nDepartment of Biostatistics, Peking University, Beijing, China\nmiaomq@pku.edu.cn, {fandongmeng, yijinliu}@tencent.com\nazhou@math.pku.edu.cn, withtomzhou@tencent.com\nAbstract\nThe Neural Machine Translation (NMT)\nmodel is essentially a joint language model\nconditioned on both the source sentence and\npartial translation. Therefore, the NMT model\nnaturally involves the mechanism of the Lan-\nguage Model (LM) that predicts the next to-\nken only based on partial translation. De-\nspite its success, NMT still suffers from the\nhallucination problem, generating ﬂuent but\ninadequate translations. The main reason is\nthat NMT pays excessive attention to the par-\ntial translation while neglecting the source\nsentence to some extent, namely overconﬁ-\ndence of the LM. Accordingly, we deﬁne\nthe Margin between the NMT and the LM ,\ncalculated by subtracting the predicted prob-\nability of the LM from that of the NMT\nmodel for each token. The Margin is neg-\natively correlated to the overconﬁdence de-\ngree of the LM. Based on the property, we\npropose a Margin-based Token-level Objec-\ntive (MTO) and a Margin-based Sentence-\nlevel Objective (MSO) to maximize the Mar-\ngin for preventing the LM from being over-\nconﬁdent. Experiments on WMT14 English-\nto-German, WMT19 Chinese-to-English, and\nWMT14 English-to-French translation tasks\ndemonstrate the effectiveness of our approach,\nwith 1.36, 1.50, and 0.63 BLEU improve-\nments, respectively, compared to the Trans-\nformer baseline. The human evaluation further\nveriﬁes that our approaches improve transla-\ntion adequacy as well as ﬂuency. 1\n1 Introduction\nNeural Machine Translation (NMT) has achieved\ngreat success in recent years (Sutskever et al., 2014;\n∗Equal contribution. This work was done when Mengqi\nMiao was interning at Pattern Recognition Center, WeChat AI,\nTencent Inc, China.\n†Corresponding author.\n1Code is available at https://github.com/Mlair\n77/nmt adequacy\nCho et al., 2014; Bahdanau et al., 2014; Luong\net al., 2015; Vaswani et al., 2017; Meng and Zhang,\n2019; Zhang et al., 2019a; Yan et al., 2020b), which\ngenerates accurate and ﬂuent translation through\nmodeling the next word conditioned on both the\nsource sentence and partial translation. However,\nNMT faces the hallucination problem, i.e., trans-\nlations are ﬂuent but inadequate to the source sen-\ntences. One important reason is that the NMT\nmodel pays excessive attention to the partial trans-\nlation to ensure ﬂuency while failing to translate\nsome segments of the source sentence (Weng et al.,\n2020b), which is actually the overconﬁdence of the\nLanguage Model (LM). In the rest of this paper,\nthe LM mentioned refers to the LM mechanism\ninvolved in NMT.\nMany recent studies attempt to deal with the\ninadequacy problem of NMT from two main as-\npects. One is to improve the architecture of NMT,\nsuch as adding a coverage vector to track the atten-\ntion history (Tu et al., 2016), enhancing the cross-\nattention module (Meng et al., 2016, 2018; Weng\net al., 2020b), and dividing the source sentence into\npast and future parts (Zheng et al., 2019). The other\naims to propose a heuristic adequacy metric or ob-\njective based on the output of NMT. Tu et al. (2017)\nand Kong et al. (2019) enhance the model’s recon-\nstruction ability and increase the coverage ratio of\nthe source sentences by translations, respectively.\nAlthough some researches (Tu et al., 2017; Kong\net al., 2019; Weng et al., 2020b) point out that the\nlack of adequacy is due to the overconﬁdence of the\nLM, unfortunately, they do not propose effective\nsolutions to the overconﬁdence problem.\nFrom the perspective of preventing the overcon-\nﬁdence of the LM, we ﬁrst deﬁne an indicator of\nthe overconﬁdence degree of the LM, called the\nMargin between the NMT and the LM, by subtract-\ning the predicted probability of the LM from that\nof the NMT model for each token. A small Mar-\n3457\ngin implies that the NMT might concentrate on the\npartial translation and degrade into the LM, i.e.,\nthe LM is overconﬁdent. Accordingly, we propose\na Margin-based Token-level Objective (MTO) to\nmaximize the Margin. Furthermore, we observe\na phenomenon that if target sentences in the train-\ning data contain many words with negative Mar-\ngin, they always do not correspond to the source\nsentences. These data are harmful to model perfor-\nmance. Therefore, based on the MTO, we further\npropose a Margin-based Sentence-level Objective\n(MSO) by adding a dynamic weight function to\nalleviate the negative effect of these “dirty data”.\nWe validate the effectiveness and superiority of\nour approaches on the Transformer (Vaswani et al.,\n2017), and conduct experiments on large-scale\nWMT14 English-to-German, WMT19 Chinese-to-\nEnglish, and WMT14 English-to-French transla-\ntion tasks. Our contributions are:\n• We explore the connection between inade-\nquacy translation and the overconﬁdence of\nthe LM in NMT, and thus propose an indicator\nof the overconﬁdence degree, i.e., the Margin\nbetween the NMT and the LM.\n• Furthermore, to prevent the LM from being\noverconﬁdent, we propose two effective opti-\nmization objectives to maximize the Margin,\ni.e., the Margin-based Token-level Objective\n(MTO) and the Margin-based Sentence-level\nObjective (MSO).\n• Experiments on WMT14 English-to-German,\nWMT19 Chinese-to-English, and WMT14\nEnglish-to-French show that our approaches\nbring in signiﬁcant improvements by +1.36,\n+1.50, +0.63 BLEU points, respectively. Ad-\nditionally, the human evaluation veriﬁes that\nour approaches can improve both translation\nadequacy and ﬂuency.\n2 Background\nGiven a source sentence x = {x1,x2,...,x N }, the\nNMT model predicts the probability of a target\nsentence y = {y1,y2,...,y T }word by word:\nP(y|x) =\nT∏\nt=1\np(yt|y<t,x), (1)\nwhere y<t = {y1,y2,...,y t−1}is the partial trans-\nlation before yt. From Eq. 1, the source sentence\nx and partial translation y<t are considered in the\nmeantime, suggesting that the NMT model is es-\nsentially a joint language model and the LM is\ninstinctively involved in NMT.\nBased on the encoder-decoder architecture, the\nencoder of NMT maps the input sentence x to hid-\nden states. At time step t, the decoder of NMT em-\nploys the output of the encoder and y<t to predict\nyt. The training objective of NMT is to minimize\nthe negative log-likelihood, which is also known as\nthe cross entropy loss function:\nLNMT\nce = −\nT∑\nt=1\nlogp(yt|y<t,x). (2)\nThe LM measures the probability of a target\nsentence similar to NMT but without knowledge of\nthe source sentence x:\nP(y) =\nT∏\nt=1\np(yt|y<t). (3)\nThe LM can be regarded as the part of NMT de-\ncoder that is responsible for ﬂuency, only takes y<t\nas input. The training objective of the LM is almost\nthe same as NMT except for the source sentence x:\nLLM\nce = −\nT∑\nt=1\nlogp(yt|y<t). (4)\nThe NMT model predicts the next word yt ac-\ncording to the source sentence x and meanwhile\nensures that yt is ﬂuent with the partial translation\ny<t. However, when NMT pays excessive atten-\ntion to translation ﬂuency, some source segments\nmay be neglected, leading to inadequacy problem.\nThis is exactly what we aim to address in this paper.\n3 The Approach\nIn this section, we ﬁrstly deﬁne theMargin between\nthe NMT and the LM (Section 3.1), which reﬂects\nthe overconﬁdence degree of the LM. Then we put\nforward the token-level (Section 3.2) and sentence-\nlevel (Section 3.3) optimization objectives to max-\nimize the Margin. Finally, we elaborate our two-\nstage training strategy (Section 3.4).\n3.1 Margin between the NMT and the LM\nWhen the NMT model excessively focuses on par-\ntial translation, i.e., the LM is overconﬁdent, the\nNMT model degrades into the LM, resulting in\nhallucinated translations. To prevent the overcon-\nﬁdence problem, we expect that the NMT model\noutperforms the LM as much as possible in pre-\ndicting golden tokens. Consequently, we deﬁne\nthe Margin between the NMT and the LM at the\n3458\nt-th time step by the difference of the predicted\nprobabilities of them:\n∆(t) = pNMT (yt|y<t,x) −pLM (yt|y<t), (5)\nwhere pNMT denotes the predicted probability of\nthe NMT model, i.e., p(yt|y<t,x), and pLM de-\nnotes that of the LM, i.e., p(yt|y<t).\nThe Margin ∆(t) is negatively correlated to the\noverconﬁdence degree of the LM, and different\nvalues of the Margin indicate different cases:\n• If ∆(t) is big, the NMT model is apparently\nbetter than the LM, and yt is strongly related\nto the source sentence x. Hence the LM is not\noverconﬁdent.\n• If ∆(t) is medium, the LM may be slightly\noverconﬁdent and the NMT model has the\npotential to be enhanced.\n• If ∆(t) is small, the NMT model might de-\ngrade to the LM and not correctly translate\nthe source sentence, i.e., the LM is overconﬁ-\ndent.2\nNote that sometimes, the model needs to focus\nmore on the partial translation such as the word to\nbe predicted is a determiner in the target language.\nIn this case, although small ∆(t) does not indicate\nthe LM is overconﬁdent, enlarging the ∆(t) can\nstill enhance the NMT model.\n3.2 Margin-based Token-level Objective\nBased on the Margin, we ﬁrstly deﬁne the Margin\nloss LM and then fuse it into the cross entropy\nloss function to obtain the Margin-based Token-\nevel Optimization Objective (MTO). Formally, we\ndeﬁne the Margin loss LM to maximize the Margin\nas follow:\nLM =\nT∑\nt=1\n(1 −pNMT (t))M(∆(t)), (6)\nwhere we abbreviate pNMT (yt|y<t,x) as pNMT (t).\nM(∆(t)) is a function of ∆(t), namely Margin\nfunction, which is monotonically decreasing (e.g.,\n1 −∆(t)). Moreover, when some words have the\nsame ∆(t) but different pNMT (t), their meanings\nare quite different: (1) If pNMT (t) is big, the NMT\nmodel learns the token well and does not need to\nfocus on the Margin too much; (2) If pNMT (t) is\n2In addition, if pNMT (yt|y<t,x) is large, less attention\nwill be paid to this data because yt has been learned well,\nwhich will be described in detail in Section 3.2.\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0M( )\nLinear\nCube\nQuitic\nLog\nFigure 1: The four Margin functions M(∆). All of\nthem are monotonically decreasing, yet with different\nslopes. Compared with Linear, the three non-linear\nfunctions are more stable around |∆|= 0 and steeper\naround |∆|= 1. We set αin Log to 10 in this ﬁgure.\nsmall, the NMT model is urgently to be optimized\non the token thus the weight of M(∆(t)) should\nbe enlarged. Therefore, as the weight of M(∆(t)),\n1 −pNMT (t) enables the model treat tokens wisely.\nVariations of M(∆). We abbreviate Margin\nfunction M(∆(t)) as M(∆) hereafter. A sim-\nple and intuitive deﬁnition is the Linear function:\nM(∆) = 1 −∆, which has the same gradient for\ndifferent ∆. However, as illustrated in Section 3.1,\ndifferent ∆ has completely various meaning and\nneeds to be treated differently. Therefore, we pro-\npose three non-linear Margin functions M(∆) as\nfollows:\n• Cube: (1 −∆3)/2.\n• Quintic (ﬁfth power): (1 −∆5)/2.\n• Log: 1\nαlog(1−∆\n1+∆ ) + 0.5.\nwhere αis a hyperparamater for Log.\nAs shown in Figure 1, the four variations3 have\nquite different slopes. Speciﬁcally, the three non-\nlinear functions are more stable around∆ = 0 (e.g.,\n∆ ∈[−0.5,0.5]) than Linear, especially Quintic.\nWe will report the performance of the four M(∆)\nconcretely and analyze why the three non-linear\nM(∆) perform better than Linear in Section 5.4.\nFinally, based on LM, we propose the Margin-\nbased Token-level Objective (MTO):\nLT = LNMT\nce + λMLM, (7)\nwhere LNMT\nce is the cross-entropy loss of the NMT\nmodel deﬁned in Eq. 2 and λM is the hyperparam-\neter for the Margin loss LM.\n3In order to keep the range of M(∆) roughly [0,1], we set\nLinear function to (1 −∆)/2.\n3459\nSource 尽管 他们 是 孪生 儿 , 但 性格 却 截然不同 .\nTarget How did your mother succeed in keeping the \npeace between these two very different men?\nExpert \nTranslation\nAlthough they are twins, they are quite different in \ncharacter.\nFigure 2: The parallel sentences, i.e., the source and tar-\nget sentences, are sampled from the WMT19 Chinese-\nto-English training dataset. We also list an expert trans-\nlation of the source sentence. The words in bold red\nhave negative Margin. This target sentence has more\nthan 50% tokens with negative Margin, and these to-\nkens are almost irrelevant to the source sentence. Ap-\nparently, the target sentence is a hallucination and will\nharm the model performance.\n3.3 Margin-based Sentence-level Objective\nFurthermore, through analyzing the Margin distri-\nbution of target sentences, we observe that the tar-\nget sentences in the training data which have many\ntokens with negative Margin are almost “halluci-\nnations” of the source sentences (i.e., dirty data),\nthus will harm the model performance. Therefore,\nbased on MTO, we further propose the Margin-\nbased Sentence-level Objective (MSO) to address\nthis issue.\nCompared with the LM, the NMT model pre-\ndicts the next word with more prior knowledge\n(i.e., the source sentence). Therefore, it is intuitive\nthat when predicting yt, the NMT model should\npredict more accurately than the LM, as follow:\npNMT (yt|y<t,x) >pLM (yt|y<t). (8)\nActually, the above equation is equivalent to\n∆(t) >0. The larger ∆(t) is, the more the NMT\nmodel exceeds the LM. However, there are many\ntokens with negative Margin through analyzing the\nMargin distribution. We conjecture the reason is\nthat the target sentence is not corresponding to the\nsource sentence in the training corpus, i.e., the tar-\nget sentence is a hallucination. Actually, we also\nobserve that if a large proportion of tokens in a\ntarget sentence have negativeMargin (e.g., 50%),\nthe sentence is probably not corresponding to the\nsource sentence, such as the case in Figure 2. These\n“dirty” data will harm the performance of the NMT\nmodel.\nTo measure the “dirty” degree of data, we de-\nﬁne the Sentence-level Negative Margin Ratio of\nparallel sentences (x,y) as follow:\nR(x,y) = #{yt ∈y : ∆(t) <0}\n#{yt : yt ∈y} , (9)\nwhere #{yt ∈y : ∆(t) <0}denotes the number\nof tokens with negative ∆(t) in y, and #{yt : yt ∈\ny}is the length of the target sentence y.\nWhen R(x,y) is larger than a threshold k(e.g.,\nk=50%), the target sentence may be desperately\ninadequate, or even completely unrelated to the\nsource sentence, as shown in Figure 2. In order to\neliminate the impact of these seriously inadequate\nsentences, we ignore their loss during training by\nthe Margin-based Sentence-level Objective (MSO):\nLS = IR(x,y)<k ·LT, (10)\nwhere IR(x,y)<k is a dynamic weight function in\nsentence level. The indicative function IR(x,y)<k\nequals to 1 if R(x,y) < k, else 0, where k is a\nhyperparameter. LT is MTO deﬁned in Eq. 7.\nIR(x,y)<k is dynamic at the training stage. Dur-\ning training, as the model gets better, its ability to\ndistinguish hallucinations improves thus IR(x,y)<k\nbecomes more accurate. We will analyze the\nchanges of IR(x,y)<k in Section 5.4.\n3.4 Two-stage Training\nWe elaborate our two-stage training in this section,\n1) jointly pretraining an NMT model and an auxil-\niary LM, and 2) ﬁnetuning the NMT model.\nJointly Pretraining. The language model mech-\nanism in NMT cannot be directly evaluated, thus\nwe train an auxiliary LM to represent it. We pre-\ntrain them together using a fusion loss function:\nLpre = LNMT\nce + λLMLLM\nce , (11)\nwhere LNMT\nce and LLM\nce are the cross entropy loss\nfunctions of the NMT model and the LM deﬁned\nin Eq. 2 and Eq. 4, respectively. λLM is a hyperpa-\nrameter. Speciﬁcally, we jointly train them through\nsharing their decoders’ embedding layers and their\npre-softmax linear transformation layers (Vaswani\net al., 2017). There are two reasons for joint train-\ning: (1) making the auxiliary LM as consistent as\npossible with the language model mechanism in\nNMT; (2) avoiding abundant extra parameters.\nFinetuning. We ﬁnetune the NMT model by min-\nimizing the MTO (LT in Eq. 7) and MSO (LS in\nEq. 10).4 Note that the LM is not involved at the\ninference stage.\n4The LM can be ﬁxed or trained along with the NMT after\npretraining. Our experimental results show that continuous\ntraining the LM and ﬁxing the LM have analogous perfor-\nmance during the ﬁnetuning stage. Therefore, we only report\nthe results of keeping the LM ﬁxed in this paper.\n3460\n4 Experimental Settings\nWe conduct experiments on three large-scale NMT\ntasks, i.e., WMT14 English-to-German (En→De),\nWMT14 English-to-French (En→Fr), and WMT19\nChinese-to-English (Zh→En).\nDatasets. For En →De, we use 4.5M training\ndata. Following the same setting in (Vaswani et al.,\n2017), we use newstest2013 as validation set and\nnewstest2014 as test set, which contain 3000 and\n3003 sentences, respectively. For En→Fr, the train-\ning dataset contains about 36M sentence pairs, and\nwe use newstest2013 with 3000 sentences as valida-\ntion set and newstest2014 with 3003 sentences as\ntest set. For Zh→En, we use 20.5M training data\nand use newstest2018 as validation set and new-\nstest2019 as test set, which contain 3981 and 2000\nsentences, respectively. For Zh→En, the number\nof merge operations in byte pair encoding (BPE)\n(Sennrich et al., 2016a) is set to 32K for both source\nand target languages. For En→De and En→Fr, we\nuse a shared vocabulary generated by 32K BPEs.\nEvaluation. We measure the case-sensitive\nBLEU scores using multi-bleu.perl 5 for En→De\nand En→Fr. For Zh →En, case-sensitive BLEU\nscores are calculated by Moses mteval-v13a.pl\nscript6. Moreover, we use the paired bootstrap\nresampling (Koehn, 2004) for signiﬁcance test. We\nselect the model which performs the best on the\nvalidation sets and report its performance on the\ntest sets for evaluation.\nModel and Hyperparameters. We conduct ex-\nperiments based on the Transformer (Vaswani et al.,\n2017) and implement our approaches with the open-\nsource tooklit Opennmt-py (Klein et al., 2017). Fol-\nlowing the Transformer-Base setting in (Vaswani\net al., 2017), we set the hidden size to 512 and\nthe encoder/decoder layers to 6. All three tasks\nare trained with 8 NVIDIA V100 GPUs, and the\nbatch size for each GPU is 4096 tokens. The beam\nsize is 5 and the length penalty is 0.6. Adam op-\ntimizer (Kingma and Ba, 2014) is used in all the\nmodels. The LM architecture is the decoder of the\nTransformer excluding the cross-attention layers,\nsharing the embedding layer and the pre-softmax\n5https://github.com/moses-smt/mosesde\ncoder/blob/master/scripts/generic/multi-\nbleu.perl\n6https://github.com/moses-smt/mosesde\ncoder/blob/mast-er/scripts/generic/mteva\nl-v13a.pl\nlinear transformation with the NMT model. For\nEn→De, Zh→En, and En→Fr, the number of train-\ning steps is 150K for jointly pretraining stage and\n150K for ﬁnetuning 7. During pretraining, we set\nλLM to 0.01 for all three tasks8. Experimental re-\nsults shown in Appendix A indicate that the LM has\nconverged after pretraining for all the three tasks.\nDuring ﬁnetuning, the Margin function M(∆) in\nSection 3.2 is set to Quintic, and we will analyze\nthe four M(∆) in Section 5.4. λM in Eq. 7 is set\nto 5, 8, and 8 on En →De, En→Fr and Zh →En,\nrespectively. For MSO, the threshold kin Eq. 10\nis set to 30% for En →De and Zh →En, 40% for\nEn→Fr. The two hyperparameters (i.e., λM and\nk) are searched on validation sets, and the selec-\ntion details are shown in Appendix B. The baseline\nmodel (i.e., vanilla Transformer) is trained for 300k\nsteps for En→De, En→Fr and Zh→En. Moreover,\nwe use a joint training model as our secondary\nbaseline, namely NMT+LM, by jointly training the\nNMT model and the LM throughout the training\nstage with 300K steps. The training steps of all the\nmodels are consistent, thus the experiment results\nare strictly comparable.\n5 Results and Analysis\nWe ﬁrst evaluate the main performance of our ap-\nproaches (Section 5.1 and 5.2). Then, the human\nevaluation further conﬁrms the improvements of\ntranslation adequacy and ﬂuency (Section 5.3). Fi-\nnally, we analyze the positive impact of our models\non the distribution of Margin and explore how each\nfragment of our method works (Section 5.4).\n5.1 Results on En →De\nThe results on WMT14 English-to-German\n(En→De) are summarized in Table 1. We list the\nresults from (Vaswani et al., 2017) and several re-\nlated competitive NMT systems by various meth-\nods, such as Minimum Risk Training (MRT) ob-\njective (Shen et al., 2016), Simple Fusion of NMT\nand LM (Stahlberg et al., 2018), optimizing ade-\nquacy metrics (Kong et al., 2019; Feng et al., 2019)\nand improving the Transformer architecture (Yang\net al., 2018; Zheng et al., 2019; Yang et al., 2019;\nWeng et al., 2020b; Yan et al., 2020a). We re-\n7The LM does not need to be state-of-the-art. The previous\nstudy of (Baziotis et al., 2020) has shown that a more powerful\nLM does not lead to further improvements to NMT.\n8The experimental results show that the model is insensi-\ntive to λLM. Therefore we make λLM consistent for all the\nthree tasks.\n3461\nSystem En→De ↑\nExisting NMT systems\nTransformer (Vaswani et al., 2017) 27.3 -\nMRT* (Shen et al., 2016) 27.71 -\nSimple Fusion** (Stahlberg et al., 2018)27.88 -\nLocalness (Yang et al., 2018) 28.11 -\nContext-Aware (Yang et al., 2019) 28.26 -\nAOL (Kong et al., 2019) 28.01 -\nEval. Module (Feng et al., 2019) 27.55 -\nPast&Future (Zheng et al., 2019) 28.10 -\nDual (Yan et al., 2020a) 27.86 -\nMulti-Task (Weng et al., 2020b) 28.25 -\nOur NMT systems\nNMT (Transformer) 27.22 ref\n+ LM 27.97 +0.75\n+ MTO 28.47†‡ +1.25\n+ MSO 28.58†‡ +1.36\nTable 1: Case-sensitive BLEU scores (%) on the test set\nof WMT14 En→De. ↑denotes the improvement com-\npared with the NMT baseline (i.e., Transformer). “ †”:\nsigniﬁcantly better than NMT ( p<0.01). “ ‡”: signiﬁ-\ncantly better than the joint model NMT+LM (p<0.01).\n(MRT* in (Shen et al., 2016) is RNN-based, and the\nresult reported here is implemented on Transformer by\nWeng et al. (2020b). **: we re-implement Simple Fu-\nsion on upon of Transformer.)\nimplement the Transformer model (Vaswani et al.,\n2017) as our baseline. Similarly, we re-implement\nthe Simple Fusion (Stahlberg et al., 2018) model. 9\nFinally, the results of the joint training model\nNMT+LM, and models with our MTO and MSO\nobjectives are reported.\nCompared with the baseline, NMT+LM yields\n+0.75 BLEU improvement. Based on NMT+LM,\nour MTO achieves further improvement with +0.50\nBLEU scores, indicating that preventing the LM\nfrom being overconﬁdent could signiﬁcantly en-\nhance model performance. Moreover, MSO per-\nforms better than MTO by +0.11 BLEU scores,\nwhich implies that the “dirty data” in the train-\ning dataset indeed harm the model performance,\nand the dynamic weight function IR(x,y)<k in\nEq. 10 could reduce the negative impact. In conclu-\nsion, our approaches improve up to +1.36 BLEU\nscores on En→De compared with the Transformer\nbaseline and substantially outperforms the exist-\ning NMT systems. The results demonstrate the\neffectiveness and superiority of our approaches.\n9The architectures of the LM and NMT model in Simple\nFusion are consistent with our MTO and MSO.\nSystem En→Fr Zh→En\nBLEU ↑ BLEU ↑\nVaswani et al. (2017)*38.1 - - -\nNMT (Transformer) 41.07 ref 25.75 ref\n+ LM 41.14 +0.07 25.90 +0.15\n+ MTO 41.56†‡+0.4926.94†‡+1.19\n+ MSO 41.70†‡+0.6327.25†‡+1.50\nTable 2: Case-sensitive BLEU scores (%) on the test\nset of WMT14 En →Fr and WMT19 Zh →En. ↑de-\nnotes the improvement compared with the NMT base-\nline (i.e., Transformer). “ †”: signiﬁcantly better than\nNMT (p<0.01). “‡”: signiﬁcantly better than the joint\nmodel NMT+LM (p<0.01). * denotes the results come\nfrom the cited paper.\n5.2 Results on En →Fr and Zh→En\nThe results on WMT14 English-to-French\n(En→Fr) and WMT19 Chinese-to-English\n(Zh→En) are shown in Table 2. We also list\nthe results of (Vaswani et al., 2017) and our\nreimplemented Transformer as the baselines.\nOn En→Fr, our reimplemented result is higher\nthan the result of (Vaswani et al., 2017), since we\nupdate 300K steps while Vaswani et al. (2017)\nonly update 100K steps. Many studies obtain\nsimilar results to ours (e.g., 41.1 BLEU scores\nfrom (Ott et al., 2019)). Compared with the base-\nline, NMT+LM yields +0.07 and +0.15 BLEU im-\nprovements on En→Fr and Zh→En, respectively.\nThe improvement of NMT+LM on En→De in Ta-\nble 1 (i.e., +0.75) is greater than these two datasets.\nWe conjecture the reason is that the amount of train-\ning data of En →De is much smaller than that of\nEn→Fr and Zh→En, thus NMT+LM is more likely\nto improve the model performance on En→De.\nCompared with NMT+LM, our MTO achieves\nfurther improvements with +0.42 and +1.04 BLEU\nscores on En→Fr and Zh→En, respectively, which\ndemonstrates the performance improvement is\nmainly due to our Margin-based objective rather\nthan joint training. Moreover, based on MTO, our\nMSO further yields +0.14 and +0.31 BLEU im-\nprovements. In summary, our approaches improve\nup to +0.63 and +1.50 BLEU scores on En →Fr\nand Zh→En compared with the baselines, respec-\ntively, which demonstrates the effectiveness and\ngeneralizability of our approaches.\n5.3 Human Evaluation\nWe conduct the human evaluation for translations\nin terms of adequacy and ﬂuency. Firstly, we ran-\n3462\nModel Adequacy Fluency Ave.\nNMT (Transformer) 4.04 4.66 4.35\n+ LM 4.12 4.86 4.49\n+ MTO 4.26 4.87 4.57\n+ MSO 4.41 4.91 4.66\nTable 3: Human evaluation on adequacy and ﬂuency.\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00\n0\n50K\n100K\n150K\n200K\n250K\n300KFrequency\nNMT+LM\nMSO\nFigure 3: The distribution of∆ of NMT+LM and MSO.\nWe randomly sample 100K sentence pairs from the\ntraining dataset of Zh →En and compute the Margin\nof their tokens. The purple area is the overlap of the\ntwo models’∆ distributions. The two distributions are\nquite different. Compared with NMT+LM, MSO re-\nduces the distribution around ∆ = 0 and meanwhile\nincreases the distribution around ∆ = 1.\ndomly sample 100 sentences from the test set of\nWMT19 Zh→En. Then we invite three annotators\nto evaluate the translation adequacy and ﬂuency.\nFive scales have been set up, i.e., 1, 2, 3, 4, 5. For\nadequacy, “1” means totally irrelevant to the source\nsentence, and “5” means equal to the source sen-\ntence semantically. For ﬂuency, “1” represents not\nﬂuent and incomprehensible; “5” represents very\n“native”. Finally, we take the average of the scores\nfrom the three annotators as the ﬁnal score.\nThe results of the baseline and our approaches\nare shown in Table 3. Compared with the NMT\nbaseline, NMT+LM, MTO and MSO improve ad-\nequacy with 0.08, 0.22, and 0.37 scores, respec-\ntively. Most improvements come from our Mar-\ngin-based methods MTO and MSO, and MSO per-\nforms the best. For ﬂuency, NMT+LM achieves\n0.2 improvement compared with NMT. Based on\nNMT+LM, MTO and MSO yield further improve-\nments with 0.01 and 0.05 scores, respectively. Hu-\nman evaluation indicates that our MTO and MSO\napproaches remarkably improve translation ade-\nquacy and slightly enhance translation ﬂuency.\nModel Percent of∆<0 (↓) Average∆ (↑)\nNMT + LM 12.45% (ref) 0.33 (ref)\n+ MTO 10.17%(-2.28%) 0.44 (+0.11)\n+ MSO 10.89% (-1.56%) 0.44 (+0.11)\nTable 4: The percent of ∆ <0 and average ∆ of mod-\nels computed from the 100K sentence pairs introduced\nin Figure 3. Compared with NMT+LM, both MTO and\nMSO effectively reduce the percent of ∆ < 0 and im-\nprove the average ∆.\n5.4 Analysis\nMargin between the NMT and the LM. Firstly,\nwe analyze the distribution of the Margin between\nthe NMT and the LM (i.e., ∆ in Eq. 5). As shown\nin Figure 3, for the joint training model NMT+LM,\nalthough most of the Margins are positive, there\nare still many tokens with negative Margin and a\nlarge amount of Margins around 0. This indicates\nthat the LM is probably overconﬁdent for many to-\nkens, and addressing the overconﬁdence problem is\nmeaningful for NMT. By comparison, the Margin\ndistribution of MSO is dramatically different with\nNMT+LM: the tokens with Margin around 0 are\nsigniﬁcantly reduced, and the tokens with Margin\nin [0.75,1.0] are increased apparently.\nMore precisely, we list the percentage of tokens\nwith negative Margin and the average Margin for\neach model in Table 4. Compared with NMT+LM,\nMTO and MSO reduce the percentage of negative\nMargin by 2.28 and 1.56 points, respectively. We\nnotice MSO performs slightly worse than MTO,\nbecause MSO neglects the hallucinations during\ntraining. As there are many tokens with negative\nMargin in hallucinations, the ability of MSO to\nreduce the proportion of ∆ <0 is weakened. We\nfurther analyze effects of MTO and MSO on the\naverage of Margin. Both MTO and MSO improve\nthe average of the Margin by 33% (from 0.33 to\n0.44). In conclusion, MTO and MSO both indeed\nincrease the Margin between the NMT and the LM.\nVariations of M(∆). We compare the perfor-\nmance of the four Margin functions M(∆) deﬁned\nin Section 3.2. We list the BLEU scores of the\nTransformer baseline, NMT+LM and our MTO ap-\nproach with the four M(∆) in Table 5. All the\nfour variations bring improvements over NMT and\nNMT+LM. The results of Log with different αare\nsimilar to Linear, while far lower than Cube and\nQuintic. And Quintic performs the best among all\nthe four variations. We speculate the reason is that\n3463\nFunction BLEU ↑\nNMT (Transformer) 25.75 ref\n+ LM 25.90 +0.15\n+ Linear 26.13 +0.38\n+ Cube 26.45 +0.60\n+ Quintic 26.94 +1.19\n+ Log (α= 5) 26.12 +0.37\n+ Log (α= 10) 26.07 +0.32\n+ Log (α= 20) 26.24 +0.49\nTable 5: Case-sensitive BLEU scores (%) on Zh →En\ntest set of MTO with several variations of M(∆).\nα is the hyperparameter of Log. All four M(∆)\nachieve BLEU improvements compared with NMT and\nNMT+LM, and Quintic performs the best.\nModels Valid Test\nNMT (Transformer) 23.67 25.75\n+ LM 23.61 25.90\n+ MTO w/ Weight 24.09 26.94\n+ MTO w/o Weight 23.36 25.85\nTable 6: Case-sensitive BLEU scores (%) on Zh →En\nvalidation set and test set of MTO with (w/) and without\n(w/o) the weight 1 −pNMT (t).\n∆ ∈[−0.5,0.5] is the main range for improvement,\nand Quintic updates more careful on this range (i.e.,\nwith smaller slopes) as shown in Figure 1.\nEffects of the Weight of M(∆). In MTO, we\npropose the weight1−pNMT (t) of the Margin func-\ntion M(∆) in Eq. 6. To validate the importance\nof it, we remove the weight and the Margin loss\ndegrades to LM = ∑T\nt=1 M(∆(t)). The results\nare listed in Table 6. Compared with NMT+LM,\nMTO without weight performs worse with 0.25\nand 0.05 BLEU decreases on the validation set and\ntest set, respectively. Compared with MTO with\nweight, it decreases 0.73 and 1.09 BLEU scores on\nthe validation set and test set, respectively. This\ndemonstrates that the weight 1 −pNMT (t) is indis-\npensable for our approach.\nChanges of IR(x,y)<k During Training. In\nMSO, we propose a dynamic weight function\nIR(x,y)<k in Eq. 10. Figure 4 shows the changes of\nIR(x,y)<k in MSO and the BLEU scores of MSO\nand MTO during ﬁnetuning. As the training con-\ntinues, our model gets more competent, and the\nproportion of sentences judged to be “dirty data”\nby our model increases rapidly at ﬁrst and then\n23.0\n23.2\n23.4\n23.6\n23.8\n24.0\n24.2\n24.4\n24.6\n24.8\n25.0\n0.056\n0.058\n0.060\n0.062\n0.064\n0.066\n0.068\n0.070\n0.072\n155K 175K 195K 215K 235K 255K 275K 295K\nBLEU\nProportion\nTraining steps\nPropotion of I=0\nMTO\nMSO\nFigure 4: Changes of the proportion of IR(x,y)<30% =\n0 on Zh→En during ﬁnetuning for MSO, and BLEU\nscores (%) on the validation set of Zh →En for MTO\nand MSO. The orange line corresponds to the left y-\naxis, and the green and blue lines correspond to the\nright y-axis. We sample 100K sentence pairs in the\ntraining data and compute IR(x,y)<30%.\nﬂattens out, which is consistent with the trend of\nBLEU of MSO. Moreover, by adding the dynamic\nweight function, MSO outperforms MTO at most\nsteps.\nCase Study. To better illustrate the translation\nquality of our approach, we show several transla-\ntion examples in Appendix C. Our approach grasps\nmore segments of the source sentences, which are\nmistranslated or neglected by the Transformer.\n6 Related Work\nTranslation Adequacy of NMT. NMT suffers\nfrom the hallucination and inadequacy problem\nfor a long time (Tu et al., 2016; M ¨uller et al.,\n2020; Wang and Sennrich, 2020; Lee et al., 2019).\nMany studies improve the architecture of NMT\nto alleviate the inadequacy issue, including track-\ning translation adequacy by coverage vectors (Tu\net al., 2016; Mi et al., 2016), modeling a global\nrepresentation of source side (Weng et al., 2020a),\ndividing the source sentence into past and future\nparts (Zheng et al., 2019), and multi-task learn-\ning to improve encoder and cross-attention mod-\nules in decoder (Meng et al., 2016, 2018; Weng\net al., 2020b). They inductively increase the trans-\nlation adequacy, while our approaches directly max-\nimize the Margin between the NMT and the LM to\nprevent the LM from being overconﬁdent. Other\nstudies enhance the translation adequacy by ade-\nquacy metrics or additional optimization objectives.\nTu et al. (2017) minimize the difference between\nthe original source sentence and the reconstruction\nsource sentence of NMT. Kong et al. (2019) pro-\n3464\npose a coverage ratio of the source sentence by the\nmodel translation. Feng et al. (2019) evaluate the\nﬂuency and adequacy of translations with an evalu-\nation module. However, the metrics or objectives\nin the above approaches may not wholly represent\nadequacy. On the contrary, our approaches are de-\nrived from the criteria of the NMT model and the\nLM, thus credible.\nLanguage Model Augmented NMT. Language\nModels are always used to provide more infor-\nmation to improve NMT. For low-resource tasks,\nthe LM trained on extra monolingual data can re-\nrank the translations by fusion (G ¨ulc ¸ehre et al.,\n2015; Sriram et al., 2017; Stahlberg et al., 2018),\nenhance NMT’s representations (Clinchant et al.,\n2019; Zhu et al., 2020), and provide prior knowl-\nedge for NMT (Baziotis et al., 2020). For data\naugmentation, LMs are used to replace words in\nsentences (Kobayashi, 2018; Wu et al., 2018; Gao\net al., 2019). Differently, we mainly focus on the\nMargin between the NMT and the LM, and no ad-\nditional data is required. Stahlberg et al. (2018)\npropose the Simple Fusion approach to model the\ndifference between NMT and LM. Differently, it\nis trained to optimize the residual probability, pos-\nitively correlated to pNMT /pLM which is hard to\noptimize and the LM is still required in inference,\nslowing down the inference speed largely.\nData Selection in NMT. Data selection and data\nﬁlter methods have been widely used in NMT.\nTo balance data domains or enhance the data\nquality generated by back-translation (Sennrich\net al., 2016b), many approaches have been pro-\nposed, such as utilizing language models (Moore\nand Lewis, 2010; van der Wees et al., 2017;\nZhang et al., 2020), translation models (Junczys-\nDowmunt, 2018; Wang et al., 2019a), and curricu-\nlum learning (Zhang et al., 2019b; Wang et al.,\n2019b). Different from the above methods, our\nMSO dynamically combines language models with\ntranslation models for data selection during train-\ning, making full use of the models.\n7 Conclusion\nWe alleviate the problem of inadequacy translation\nfrom the perspective of preventing the LM from\nbeing overconﬁdent. Speciﬁcally, we ﬁrstly pro-\npose an indicator of the overconﬁdence degree of\nthe LM in NMT, i.e., Margin between the NMT\nand the LM. Then we proposeMargin-based Token-\nlevel and Sentence-level objectives to maximize the\nMargin. Experimental results on three large-scale\ntranslation tasks demonstrate the effectiveness and\nsuperiority of our approaches. The human evalua-\ntion further veriﬁes that our methods can improve\ntranslation adequacy and ﬂuency.\nAcknowledgments\nThe research work descried in this paper has been\nsupported by the National Nature Science Founda-\ntion of China (No. 12026606). The authors would\nlike to thank the anonymous reviewers for their\nvaluable comments and suggestions to improve this\npaper.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nChristos Baziotis, Barry Haddow, and Alexandra Birch.\n2020. Language model prior for low-resource neu-\nral machine translation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 7622–7634, On-\nline. Association for Computational Linguistics.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nStephane Clinchant, Kweon Woo Jung, and Vassilina\nNikoulina. 2019. On the use of BERT for neu-\nral machine translation. In Proceedings of the 3rd\nWorkshop on Neural Generation and Translation ,\npages 108–117, Hong Kong. Association for Com-\nputational Linguistics.\nYang Feng, Wanying Xie, Shuhao Gu, Chenze\nShao, Wen Zhang, Zhengxin Yang, and Dong Yu.\n2019. Modeling ﬂuency and faithfulness for di-\nverse neural machine translation. arXiv preprint\narXiv:1912.00178.\nFei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao\nQin, Xueqi Cheng, Wengang Zhou, and Tie-Yan Liu.\n2019. Soft contextual data augmentation for neural\nmachine translation. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5539–5544, Florence, Italy. Asso-\nciation for Computational Linguistics.\n3465\nC ¸ aglar G¨ulc ¸ehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Lo¨ıc Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. CoRR, abs/1503.03535.\nMarcin Junczys-Dowmunt. 2018. Dual conditional\ncross-entropy ﬁltering of noisy parallel corpora. In\nProceedings of the Third Conference on Machine\nTranslation: Shared Task Papers , pages 888–895,\nBelgium, Brussels. Association for Computational\nLinguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In\nProceedings of ACL 2017, System Demonstrations ,\npages 67–72, Vancouver, Canada. Association for\nComputational Linguistics.\nSosuke Kobayashi. 2018. Contextual augmentation:\nData augmentation by words with paradigmatic re-\nlations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 452–457,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests\nfor machine translation evaluation. In Proceed-\nings of the 2004 Conference on Empirical Meth-\nods in Natural Language Processing , pages 388–\n395, Barcelona, Spain. Association for Computa-\ntional Linguistics.\nXiang Kong, Zhaopeng Tu, Shuming Shi, Eduard\nHovy, and Tong Zhang. 2019. Neural machine trans-\nlation with adequacy-oriented learning. Proceed-\nings of the AAAI Conference on Artiﬁcial Intelli-\ngence, 33(01):6618–6625.\nKatherine Lee, Orhan Firat, Ashish Agarwal, Clara\nFannjiang, and David Sussillo. 2019. Hallucinations\nin neural machine translation.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1412–1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nFandong Meng, Zhengdong Lu, Hang Li, and Qun Liu.\n2016. Interactive attention for neural machine trans-\nlation. In Proceedings of COLING 2016, the 26th\nInternational Conference on Computational Linguis-\ntics: Technical Papers , pages 2174–2185, Osaka,\nJapan.\nFandong Meng, Zhaopeng Tu, Yong Cheng, Haiyang\nWu, Junjie Zhai, Yuekui Yang, and Di Wang. 2018.\nNeural machine translation with key-value memory-\naugmented attention. In Proceedings of IJCAI.\nFandong Meng and Jinchao Zhang. 2019. DTMT: A\nnovel deep transition architecture for neural machine\ntranslation. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 33, pages 224–\n231.\nHaitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe\nIttycheriah. 2016. Coverage embedding models for\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing , pages 955–960, Austin,\nTexas. Association for Computational Linguistics.\nRobert C. Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 Conference Short Papers,\npages 220–224, Uppsala, Sweden. Association for\nComputational Linguistics.\nMathias M ¨uller, Annette Rios, and Rico Sennrich.\n2020. Domain robustness in neural machine trans-\nlation. In Proceedings of the 14th Conference of the\nAssociation for Machine Translation in the Americas\n(Volume 1: Research Track), pages 151–164, Virtual.\nAssociation for Machine Translation in the Ameri-\ncas.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua\nWu, Maosong Sun, and Yang Liu. 2016. Minimum\nrisk training for neural machine translation. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1683–1692, Berlin, Germany. Asso-\nciation for Computational Linguistics.\n3466\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2017. Cold fusion: Training seq2seq\nmodels together with language models. CoRR,\nabs/1708.06426.\nFelix Stahlberg, James Cross, and Veselin Stoyanov.\n2018. Simple fusion: Return of the language model.\nIn Proceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 204–211, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems ,\n27:3104–3112.\nZhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,\nand Hang Li. 2017. Neural machine translation with\nreconstruction. In Proceedings of the 31st AAAI\nConference on Artiﬁcial Intelligence.\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,\nand Hang Li. 2016. Modeling coverage for neural\nmachine translation. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 76–85.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nChaojun Wang and Rico Sennrich. 2020. On exposure\nbias, hallucination and domain shift in neural ma-\nchine translation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3544–3552, Online. Association for\nComputational Linguistics.\nShuo Wang, Yang Liu, Chao Wang, Huanbo Luan, and\nMaosong Sun. 2019a. Improving back-translation\nwith uncertainty-based conﬁdence estimation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 791–\n802, Hong Kong, China. Association for Computa-\ntional Linguistics.\nWei Wang, Isaac Caswell, and Ciprian Chelba. 2019b.\nDynamically composing domain-data selection with\nclean-data selection by “co-curricular learning” for\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1282–1292.\nMarlies van der Wees, Arianna Bisazza, and Christof\nMonz. 2017. Dynamic data selection for neural ma-\nchine translation. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1400–1410, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nRongxiang Weng, Haoran Wei, Shujian Huang, Heng\nYu, Lidong Bing, Weihua Luo, and Jiajun Chen.\n2020a. Gret: Global representation enhanced trans-\nformer. AAAI.\nRongxiang Weng, Heng Yu, Xiangpeng Wei, and Wei-\nhua Luo. 2020b. Towards enhancing faithfulness for\nneural machine translation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2675–2684,\nOnline. Association for Computational Linguistics.\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2018. Conditional bert contextual\naugmentation.\nJianhao Yan, Fandong Meng, and Jie Zhou. 2020a.\nDual past and future for neural machine translation.\nJianhao Yan, Fandong Meng, and Jie Zhou. 2020b.\nMulti-unit transformers for neural machine transla-\ntion. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1047–1059, Online.\nBaosong Yang, Jian Li, Derek F Wong, Lidia S Chao,\nXing Wang, and Zhaopeng Tu. 2019. Context-aware\nself-attention networks. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , pages 387–\n394.\nBaosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong\nMeng, Lidia S. Chao, and Tong Zhang. 2018. Mod-\neling localness for self-attention networks. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 4449–\n4458, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nBoliang Zhang, Ajay Nagesh, and Kevin Knight. 2020.\nParallel corpus ﬁltering via pre-trained language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 8545–8554, Online. Association for Computa-\ntional Linguistics.\nWen Zhang, Yang Feng, Fandong Meng, Di You, and\nQun Liu. 2019a. Bridging the gap between train-\ning and inference for neural machine translation. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4334–\n4343, Florence, Italy.\nXuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul\nMcNamee, Marine Carpuat, and Kevin Duh. 2019b.\nCurriculum learning for domain adaptation in neu-\nral machine translation. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1903–1915, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nZaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-\nYu Dai, and Jiajun Chen. 2019. Dynamic past and\n3467\nfuture for neural machine translation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 931–941, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tieyan Liu. 2020.\nIncorporating bert into neural machine translation.\nIn International Conference on Learning Represen-\ntations.\nA Loss of the Language Model\nTo validate whether the LM is converged or not af-\nter pretraining, we plot the loss of the LM as shown\nin Figure 5. The loss of the LM remains stable after\ntraining about 80K steps for En→De, Zh→En and\nEn→Fr, indicating that the LM is converged during\npretraining stage.\n0 20K 40K 60K 80K 100K 120K 140K\nTraining Steps\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0Loss\nEn >De\nZh >En\nEn >Fr\nFigure 5: The loss of the LM on the validation set dur-\ning pretraining for En→De, Zh→En and En→Fr. The\nLM converges after training nearly 80K steps for all the\nthree tasks.\nB Hyperparameters Selection\nThe results of our approaches with different λM\n(deﬁned in Eq. 7) and k (deﬁned in Eq. 10) on\nthe validation sets of WMT14 En →De, WMT14\nEn→Fr and WMT19 Zh →En are shown in Fig-\nure 6. We ﬁrstly search the best λM based on MTO.\nAll the three datasets achieve better performance\nfor λM ∈ [5,10]. The model reaches the peak\nwhen λM =5, 8, and 8 for the three tasks, respec-\ntively. Then, ﬁxing the best λM for each dataset,\nwe search the best threshold k. As shown in the\nright of Figure 6, the best kis 30% for En→De and\nZh→En, 40% for En→Fr. This is consistent with\nour observations. When the proportion of tokens\n1 3 5 8 10\nM\n27.0\n27.1\n27.2\n27.3\n27.4BLEU\n(a) λM (En→De)\n20% 30% 40% 50%\nthreshold  in S\n27.1\n27.2\n27.3\n27.4\n27.5BLEU\n (b) k(En→De)\n1 3 5 8 10\nM\n41.2\n41.3\n41.4\n41.5\n41.6BLEU\n(c) λM (En→Fr)\n20% 30% 40% 50%\nthreshold  in S\n41.75\n41.85\n41.95\n42.05BLEU\n (d) k(En→Fr)\n1 3 5 8 10\nM\n23.7\n23.8\n23.9\n24.0\n24.1\n24.2BLEU\n(e) λM (Zh→En)\n20% 30% 40% 50%\nthreshold  in S\n24.0\n24.1\n24.2\n24.3\n24.4\n24.5BLEU\n (f) k(Zh→En)\nFigure 6: Case-sensitive BLEU scores (%) on valida-\ntion sets of WMT14 En →De, WMT14 En →Fr and\nWMT19 Zh →En with different hyperparameters, re-\nspectively. λM is deﬁned in Eq. 7, and the search re-\nsults are shown in Figure (a), (c) and (e). The threshold\nkfor MSO is deﬁned in Eq. 10 and the results of it are\nshown in Figure (b), (d), and (f).\nwith negative Margin in a target sentence is greater\nthan 30% or 40%, the sentence is most likely to be\na hallucination.\nC Case Study\nAs shown in Figure 7, our approach outperforms\nthe base model (i.e., the Transformer) in translation\nadequacy. In case 1, the base model generates “on\nTuesday”, which is unrelated to the source sentence,\ni.e., hallucination, and under-translates “Novem-\nber 5” and “the website of the Chinese embassy\nin Mongolia” information in the source sentence.\nHowever, our approach translates the above two\nsegments well. In Case 2, the base model reverses\nthe chronological order of the source sentence, thus\ngenerates a mis-translation, while our model trans-\nlates perfectly. In Case 3, the base model neglects\ntwo main segments of the source sentence (the text\nin bold blue font) and leads to the inadequacy prob-\nlem. However, our model takes them into account.\nAccording to the three examples, we conclude that\nour approach alleviates the inadequacy problem\nwhich is extremely harmful to NMT.\n3468\nCase 1\nSRC 1 中新网11月5日电据中国驻蒙古国大使馆网站4日消息,近日,中国公民郭玉芹和毛润新在\n蒙旅游期间失联。\nREF 1\nReport on November 5 of China News: the website of the Chinese embassy in Mongolia \nreported on November 5 that Chinese citizens Guo Yuqin and Mao Runxinhad been \nmissing when traveling in Mongolia.\nBASE 1 Chinese citizens Guo Yu-Qin and Mao Yunxin lost their ties during a trip to Mongolia, China \nsaid on Tuesday.\nOURS 1 Chinese citizens Guo Yuqin and Mao Runxinlost their ties during a trip to Mongolia, \naccording to the website of the Chinese Embassy in Mongolia on November 5.\nCase 2\nSRC 2 对此央视发表快评:这是我国英雄烈士保护法施行后第一个烈士纪念日。\nREF 2 For this, CCTV issued a quick comment: this was the first Memorial Day after the \nimplementation of the law for the protection of heroes and martyrs in China.\nBASE 2 CCTV released a quick comment on this: this is our heroic martyrs protection law after \nthe implementation of the first martyr anniversary.\nOURS 2 CCTV issued a quick comment on this: this is the first martyr memorial day after the \nimplementation of our country's heroic martyr protection law.\nCase 3\nSRC 3 据外媒报道,南非首都比勒陀利亚郊区的一处保育中心里,两只小狮子一起嬉闹玩耍,很难\n看出有任何异常之处,不过它们其实绝无仅有。\nREF 3\nAccording to foreign media reports, it was hard for people to find anything unusual in \ntwo little lions playing in a conservation center located in the suburb in Pretoria, the \ncapital of South Africa, but they were absolutely unique.\nBASE 3 It's hard to see anything unusual in a nursing home in a suburb of Pretoria, South Africa's \ncapital, where two lions play together.\nOURS 3 According to foreign media reports, in a care center on the outskirts of Pretoria, South \nAfrica, two lions play together, it is difficult to see any abnormalities, but they are unique.\nFigure 7: Several example sentence pairs (SRC, REF) from WMT19 Zh→En test set. We list the translation of the\nTransformer baseline (BASE) and our MSO method (OURS). The text in bold red font is mistranslated by the base\nmodel. The text in bold blue font is mistranslated or under-translated by the base model but translated correctly by\nour model."
}