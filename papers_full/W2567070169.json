{
  "title": "Language Modeling with Gated Convolutional Networks",
  "url": "https://openalex.org/W2567070169",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4290221127",
      "name": "Dauphin, Yann N.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221722489",
      "name": "Fan, Angela",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359259",
      "name": "Auli, Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226762532",
      "name": "Grangier, David",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963685250",
    "https://openalex.org/W753012316",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2423557781",
    "https://openalex.org/W2165395109",
    "https://openalex.org/W330298975",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2254978389",
    "https://openalex.org/W1567277581",
    "https://openalex.org/W2605203995",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2145543707",
    "https://openalex.org/W104184427",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2953318193",
    "https://openalex.org/W2175585630",
    "https://openalex.org/W1677182931"
  ],
  "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",
  "full_text": "Language Modeling with Gated Convolutional Networks\nYann N. Dauphin1 Angela Fan 1 Michael Auli1 David Grangier 1\nAbstract\nThe pre-dominant approach to language mod-\neling to date is based on recurrent neural net-\nworks. Their success on this task is often linked\nto their ability to capture unbounded context.\nIn this paper we develop a ﬁnite context ap-\nproach through stacked convolutions, which can\nbe more efﬁcient since they allow paralleliza-\ntion over sequential tokens. We propose a novel\nsimpliﬁed gating mechanism that outperforms\nOord et al. (2016b) and investigate the impact\nof key architectural decisions. The proposed ap-\nproach achieves state-of-the-art on the WikiText-\n103 benchmark, even though it features long-\nterm dependencies, as well as competitive re-\nsults on the Google Billion Words benchmark.\nOur model reduces the latency to score a sen-\ntence by an order of magnitude compared to a\nrecurrent baseline. To our knowledge, this is the\nﬁrst time a non-recurrent approach is competitive\nwith strong recurrent models on these large scale\nlanguage tasks.\n1. Introduction\nStatistical language models estimate the probability distri-\nbution of a sequence of words by modeling the probability\nof the next word given preceding words, i.e.\nP(w0,...,w N) = P(w0)\nN∏\ni=1\nP(wi|w0,...,w i−1),\nwhere wi are discrete word indices in a vocabulary. Lan-\nguage models are a critical part of systems for speech\nrecognition (Yu & Deng, 2014) and machine translation\n(Koehn, 2010).\nRecently, neural networks (Bengio et al., 2003; Mikolov\net al., 2010; Jozefowicz et al., 2016) have been shown to\n1Facebook AI Research. Correspondence to: Yann N. Dauphin\n<ynd@fb.com>.\nProceedings of the34 th International Conference on Machine\nLearning, Sydney, Australia, PMLR 70, 2017. Copyright 2017\nby the author(s).\noutperform classical n-gram language models (Kneser &\nNey, 1995; Chen & Goodman, 1996). These classical mod-\nels suffer from data sparsity, which makes it difﬁcult to rep-\nresent large contexts and thus, long-range dependencies.\nNeural language models tackle this issue by embedding\nwords in continuous space over which a neural network is\napplied. The current state of the art for language model-\ning is based on long short term memory networks (LSTM;\nHochreiter et al., 1997) which can theoretically model ar-\nbitrarily long dependencies.\nIn this paper, we introduce new gated convolutional net-\nworks and apply them to language modeling. Convolu-\ntional networks can be stacked to represent large context\nsizes and extract hierarchical features over larger and larger\ncontexts with more abstractive features (LeCun & Bengio,\n1995). This allows them to model long-term dependen-\ncies by applying O(N\nk ) operations over a context of sizeN\nand kernel width k. In contrast, recurrent networks view\nthe input as a chain structure and therefore require a linear\nnumber O(N) of operations.\nAnalyzing the input hierarchically bears resemblance to\nclassical grammar formalisms which build syntactic tree\nstructures of increasing granuality, e.g., sentences consist\nof noun phrases and verb phrases each comprising further\ninternal structure (Manning & Sch ¨utze, 1999; Steedman,\n2002). Hierarchical structure also eases learning since the\nnumber of non-linearities for a given context size is reduced\ncompared to a chain structure, thereby mitigating the van-\nishing gradient problem (Glorot & Bengio, 2010).\nModern hardware is well suited to models that are highly\nparallelizable. In recurrent networks, the next output de-\npends on the previous hidden state which does not enable\nparallelization over the elements of a sequence. Convolu-\ntional networks, however, are very amenable to this com-\nputing paradigm since the computation of all input words\ncan be performed simultaneously (§2).\nGating has been shown to be essential for recurrent neural\nnetworks to reach state-of-the-art performance (Jozefow-\nicz et al., 2016). Our gated linear units reduce the vanish-\ning gradient problem for deep architectures by providing a\nlinear path for the gradients while retaining non-linear ca-\npabilities (§5.2).\narXiv:1612.08083v3  [cs.CL]  8 Sep 2017\nLanguage Modeling with Gated Convolutional Networks\nWe show that gated convolutional networks outperform\nother recently published language models such as LSTMs\ntrained in a similar setting on the Google Billion Word\nBenchmark (Chelba et al., 2013). We also evaluate the abil-\nity of our models to deal with long-range dependencies on\nthe WikiText-103 benchmark for which the model is con-\nditioned on an entire paragraph rather than a single sen-\ntence and we achieve a new state-of-the-art on this dataset\n(Merity et al., 2016). Finally, we show that gated linear\nunits achieve higher accuracy and converge faster than the\nLSTM-style gating of Oord et al. (2016; §4, §5).\n2. Approach\nIn this paper we introduce a new neural language model\nthat replaces the recurrent connections typically used in re-\ncurrent networks with gated temporal convolutions. Neu-\nral language models (Bengio et al., 2003) produce a repre-\nsentation H = [h0,..., hN] of the context for each word\nw0,...,w N to predict the next word P(wi|hi). Recurrent\nneural networks f compute H through a recurrent function\nhi = f(hi−1,wi−1) which is an inherently sequential pro-\ncess that cannot be parallelized over i.1\nOur proposed approach convolves the inputs with a func-\ntion f to obtain H = f ∗w and therefore has no tempo-\nral dependencies, so it is easier to parallelize over the in-\ndividual words of a sentence. This process will compute\neach context as a function of a number of preceding words.\nCompared to recurrent networks, the context size is ﬁnite\nbut we will demonstrate both that inﬁnite contexts are not\nnecessary and our models can represent large enough con-\ntexts to perform well in practice (§5).\nFigure 1 illustrates the model architecture. Words are rep-\nresented by a vector embedding stored in a lookup table\nD|V|×e where |V|is the number of words in the vocabulary\nand eis the embedding size. The input to our model is a\nsequence of words w0,...,w N which are represented by\nword embeddings E = [Dw0 ,..., DwN ]. We compute the\nhidden layers h0,...,h L as\nhl(X) = (X ∗W + b) ⊗σ(X ∗V + c) (1)\nwhere m,n are respectively the number of input and output\nfeature maps and k is the patch size, X ∈RN×m is the\ninput of layer hl (either word embeddings or the outputs of\nprevious layers), W ∈Rk×m×n, b ∈Rn, V ∈Rk×m×n,\nc ∈Rn are learned parameters, σ is the sigmoid function\nand ⊗is the element-wise product between matrices.\nWhen convolving inputs, we take care that hi does not\ncontain information from future words. We address this\nby shifting the convolutional inputs to prevent the kernels\n1Parallelization is usually done over multiple sequences in-\nstead.\nInput sentence\nText The    cat    sat    on    the    mat    .\nw0     w1     w2     w3     w4     w5     w6\nLookup Table\nE = Dwi\nConvolution\nA = E∗W + b\nGating\nH0 = A⊗σ(B) \nσ\nSoftmax\nY = softmax(WHL)\nB = E∗V + c\nStack L - 1 Convolution+Gating Blocks\nFigure 1.Architecture of the gated convolutional network for lan-\nguage modeling.\nfrom seeing future context (Oord et al., 2016a). Speciﬁ-\ncally, we zero-pad the beginning of the sequence withk−1\nelements, assuming the ﬁrst input element is the beginning\nof sequence marker which we do not predict and k is the\nwidth of the kernel.\nThe output of each layer is a linear projection X ∗W + b\nmodulated by the gates σ(X ∗V + c). Similar to LSTMs,\nthese gates multiply each element of the matrixX∗W+b\nand control the information passed on in the hierarchy.\nWe dub this gating mechanism Gated Linear Units (GLU).\nStacking multiple layers on top of the inputE gives a repre-\nsentation of the context for each wordH = hL◦... ◦h0(E).\nWe wrap the convolution and the gated linear unit in a pre-\nactivation residual block that adds the input of the block to\nLanguage Modeling with Gated Convolutional Networks\nthe output (He et al., 2015a). The blocks have a bottleneck\nstructure for computational efﬁciency and each block has\nup to 5 layers.\nThe simplest choice to obtain model predictions is to use\na softmax layer, but this choice is often computationally\ninefﬁcient for large vocabularies and approximations such\nas noise contrastive estimation (Gutmann & Hyv ¨arinen)\nor hierarchical softmax (Morin & Bengio, 2005) are pre-\nferred. We choose an improvement of the latter known as\nadaptive softmaxwhich assigns higher capacity to very fre-\nquent words and lower capacity to rare words (Grave et al.,\n2016a). This results in lower memory requirements as well\nas faster computation at both training and test time.\n3. Gating Mechanisms\nGating mechanisms control the path through which infor-\nmation ﬂows in the network and have proven to be use-\nful for recurrent neural networks (Hochreiter & Schmidhu-\nber, 1997). LSTMs enable long-term memory via a sep-\narate cell controlled by input and forget gates. This al-\nlows information to ﬂow unimpeded through potentially\nmany timesteps. Without these gates, information could\neasily vanish through the transformations of each timestep.\nIn contrast, convolutional networks do not suffer from the\nsame kind of vanishing gradient and we ﬁnd experimentally\nthat they do not require forget gates.\nTherefore, we consider models possessing solely output\ngates, which allow the network to control what informa-\ntion should be propagated through the hierarchy of lay-\ners. We show this mechanism to be useful for language\nmodeling as it allows the model to select which words or\nfeatures are relevant for predicting the next word. Par-\nallel to our work, Oord et al. (2016b) have shown the\neffectiveness of an LSTM-style mechanism of the form\ntanh(X∗W+b)⊗σ(X∗V+c) for the convolutional mod-\neling of images. Later, Kalchbrenner et al. (2016) extended\nthis mechanism with additional gates for use in translation\nand character-level language modeling.\nGated linear units are a simpliﬁed gating mechanism based\non the work of Dauphin & Grangier (2015) for non-\ndeterministic gates that reduce the vanishing gradient prob-\nlem by having linear units coupled to the gates. This retains\nthe non-linear capabilities of the layer while allowing the\ngradient to propagate through the linear unit without scal-\ning. The gradient of the LSTM-style gating of which we\ndub gated tanh unit (GTU) is\n∇[tanh(X) ⊗σ(X)] = tanh′(X)∇X ⊗σ(X)\n+σ′(X)∇X ⊗tanh(X). (2)\nNotice that it gradually vanishes as we stack layers because\nof the downscaling factors tanh ′(X) and σ′(X). In con-\ntrast, the gradient of the gated linear unit\n∇[X ⊗σ(X)] = ∇X ⊗σ(X) + X ⊗σ′(X)∇X (3)\nhas a path ∇X ⊗σ(X) without downscaling for the ac-\ntivated gating units in σ(X). This can be thought of\nas a multiplicative skip connection which helps gradients\nﬂow through the layers. We compare the different gating\nschemes experimentally in Section §5.2 and we ﬁnd gated\nlinear units allow for faster convergence to better perplexi-\nties.\n4. Experimental Setup\n4.1. Datasets\nWe report results on two public large-scale language mod-\neling datasets. First, the Google Billion Word dataset\n(Chelba et al., 2013) is considered one of the largest lan-\nguage modeling datasets with almost one billion tokens and\na vocabulary of over 800K words. In this dataset, words\nappearing less than 3 times are replaced with a special un-\nknown symbol. The data is based on an English corpus\nof 30,301,028 sentences whose order has been shufﬂed.\nSecond, WikiText-103 is a smaller dataset of over 100M\ntokens with a vocabulary of about 200K words (Merity\net al., 2016). Different from GBW, the sentences are con-\nsecutive which allows models to condition on larger con-\ntexts rather than single sentences. For both datasets, we\nadd a beginning of sequence marker <S > at the start of\neach line and an end of sequence marker </S>at the end\nof each line. On the Google Billion Word corpus each\nsequence is a single sentence, while on WikiText-103 a\nsequence is an entire paragraph. The model sees <S>\nand </S >as input but only predicts the end of sequence\nmarker </S>. We evaluate models by computing the per-\nplexity e\n1\nN\n∑N\ni −log p(wi|...,wi−1) on the standard held out\ntest portion of each dataset.\n4.2. Training\nWe implement our models in Torch (Collobert et al., 2011)\nand train on Tesla M40 GPUs. The majority of our models\nare trained on single GPU, as we focused on identifying\ncompact architectures with good generalization and efﬁ-\ncient computation at test time. We trained larger models\nwith an 8-GPU setup by copying the model onto each GPU\nand dividing the batch such that each worker computes\n1/8th of the gradients. The gradients are then summed us-\ning Nvidia NCCL. The multi-GPU setup allowed us to train\nmodels with larger hidden units.\nWe train using Nesterov’s momentum (Sutskever et al.,\n2013). While the cost in terms of memory is storing an-\nother vector of the size of the parameters, it increases the\nspeed of convergence signiﬁcantly with minimal additional\nLanguage Modeling with Gated Convolutional Networks\nName GCNN-13 GCNN-14B GCNN-9 GCNN-8B GCNN-8 GCNN-14\nDataset Google Billion Word wikitext-103\nLookup 128 280\nConv1 [4, 1268] × 1 [5, 512] × 1 [4, 807] × 1 [1, 512] × 1 [4, 900] × 1 [6, 850] × 3\nConv2.x\n[\n4, 1268\n4, 1268\n]\n× 12\n\n\n1, 128\n5, 128\n1, 512\n\n × 3\n[\n4, 807\n4, 807\n]\n× 4\n\n\n1, 128\n5, 128\n1, 512\n\n × 3 [4, 900] × 7 [1, 850] × 1\nConv3.x\n\n\n1, 512\n5, 512\n1, 1024\n\n × 3\n\n\n1, 256\n5, 256\n1, 512\n\n × 3 [5, 850] × 4\nConv4.x\n\n\n1, 1024\n5, 1024\n1, 2048\n\n × 6\n\n\n1, 1024\n1, 1024\n1, 2048\n\n × 1 [1, 850] × 1\nConv5.x\n\n\n1, 1024\n5, 1024\n1, 4096\n\n × 1 [4, 850] × 3\nConv6.x [4, 1024] × 1\nConv7.x [4, 2048] × 1\nAdaSoftmax 10k,40k,200k 4k,40k,200k 2k,10k,50k 10k,20k,200k\nTable 1.Architectures for the models. The residual building blocks are shown in brackets with the format[k, n]. “B” denotes bottleneck\narchitectures.\ncomputation compared to standard stochastic gradient de-\nscent. The speed of convergence was further increased with\ngradient clipping (Pascanu et al., 2013) and weight normal-\nization (Salimans & Kingma, 2016).\nPascanu et al. (2013) argue for gradient clipping because it\nprevents the gradient explosion problem that characterizes\nRNNs. However, gradient clipping is not tied to RNNs, as\nit can be derived from the general concept of trust region\nmethods. Gradient clipping is found using a spherical trust\nregion\n∆θ∗= argmin\ns. t.∥∆θ∥≤ϵ\nf(θ) + ∇fT∆θ\n= −max(∥∇f∥,ϵ) ∇f\n∥∇f∥. (4)\nEmpirically, our experiments converge signiﬁcantly faster\nwith the use of gradient clipping even though we do not use\na recurrent architecture.\nIn combination, these methods led to stable and fast con-\nvergence with comparatively large learning rates such as1.\n4.3. Hyper-parameters\nWe found good hyper-parameter conﬁgurations by cross-\nvalidating with random search on a validation set. For\nmodel architecture, we select the number of residual\nblocks between {1,..., 10}, the size of the embed-\ndings with {128,..., 256}, the number of units between\n{128,..., 2048}, and the kernel width between{3,..., 5}.\nIn general, ﬁnding a good architecture was simple and the\nrule of thumb is that the larger the model, the better the per-\nformance. In terms of optimization, we initialize the lay-\ners of the model with the Kaiming initialization (He et al.,\n2015b), with the learning rate sampled uniformly in the\ninterval [1.,2.], the momentum set to 0.99, and clipping\nset to 0.1. Good hyper-parameters for the optimizer are\nquite straightforward to ﬁnd and the optimal values do not\nchange much between datasets.\n5. Results\nLSTMs and recurrent networks are able to capture long\nterm dependencies and are fast becoming cornerstones in\nnatural language processing. In this section, we compare\nstrong LSTM and RNN models from the literature to our\ngated convolutional approach on two datasets.\nWe ﬁnd the GCNN outperforms the comparable LSTM re-\nsults on Google billion words. To accurately compare these\napproaches, we control for the same number of GPUs and\nthe adaptive softmax output model (Grave et al., 2016a), as\nthese variables have a signiﬁcant inﬂuence on performance.\nIn this setting, the GCNN reaches38.1 test perplexity while\nthe comparable LSTM has 39.8 perplexity (Table 2).\nFurther, the GCNN obtains strong performance with much\ngreater computational efﬁciency. Figure 2 shows that our\napproach closes the previously signiﬁcant gap between\nmodels that use the full softmax and models with the usu-\nally less accurate hierarchical softmax. Thanks to the adap-\nLanguage Modeling with Gated Convolutional Networks\nModel Test PPL Hardware\nSigmoid-RNN-2048 (Ji et al., 2015) 68.3 1 CPU\nInterpolated KN 5-Gram (Chelba et al., 2013) 67.6 100 CPUs\nSparse Non-Negative Matrix LM (Shazeer et al., 2014) 52.9 -\nRNN-1024 + MaxEnt 9 Gram Features (Chelba et al., 2013) 51.3 24 GPUs\nLSTM-2048-512 (Jozefowicz et al., 2016) 43.7 32 GPUs\n2-layer LSTM-8192-1024 (Jozefowicz et al., 2016) 30.6 32 GPUs\nBIG GLSTM-G4 (Kuchaiev & Ginsburg, 2017) 23.3 ∗ 8 GPUs\nLSTM-2048 (Grave et al., 2016a) 43.9 1 GPU\n2-layer LSTM-2048 (Grave et al., 2016a) 39.8 1 GPU\nGCNN-13 38.1 1 GPU\nGCNN-14 Bottleneck 31.9 8 GPUs\nTable 2.Results on the Google Billion Word test set. The GCNN outperforms the LSTMs with the same output approximation.\n0 200 400 600 800 1000\nMFlops\n30\n35\n40\n45\n50\n55Test Perplexity\nLSTM+Softmax\nGCNN+AdaSoftmax\nFigure 2.In comparison to the state-of-the-art (Jozefowicz et al.,\n2016) which uses the full softmax, the adaptive softmax approxi-\nmation greatly reduces the number of operations required to reach\na given perplexity.\ntive softmax, the GCNN only requires a fraction of the op-\nerations to reach the same perplexity values. The GCNN\noutperforms other single model state-of-the-art approaches\nexcept the much larger LSTM of Jozefowicz et al. (2016),\na model which requires more GPUs and the much more\ncomputationally expensive full softmax. In comparison,\nthe largest model we have trained reaches 31.9 test per-\nplexity compared to the 30.6 of that approach, but only re-\nquires training for 2 weeks on 8 GPUs compared to 3 weeks\nof training on 32 GPUs for the LSTM. Note that these re-\nsults can be improved by either using mixtures of experts\n(Shazeer et al., 2017) or ensembles of these models.\nAnother relevant concern is if the GCNN’s ﬁxed context\nsize can thoroughly model long sequences. On Google Bil-\n∗appeared after submission\nModel Test PPL Hardware\nLSTM-1024 (Grave et al., 2016b) 48.7 1 GPU\nGCNN-8 44.9 1 GPU\nGCNN-14 37.2 4 GPUs\nTable 3.Results for single models on the WikiText-103 dataset.\nlion Word, the average sentence length is quite short —\nonly 20 words. We evaluate on WikiText-103 to determine\nif the model can perform well on a dataset where much\nlarger contexts are available. On WikiText-103, an input se-\nquence is an entire Wikipedia article instead of an individ-\nual sentence - increasing the average length to 4000 words.\nHowever, the GCNN outperforms LSTMs on this problem\nas well (Table 3). The GCNN-8 model has 8 layers with\n800 units each and the LSTM has1024 units. These results\nshow that GCNNs can model enough context to achieve\nstrong results.\nWe evaluated on the Gigaword dataset following Chen et al.\n(2016) to compare with fully connected models. We found\nthat the fully connected and convolutional network reach\nrespectively 55.6 and 29.4 perplexity. We also ran pre-\nliminary experiments on the much smaller Penn tree bank\ndataset. When we score the sentences independently, the\nGCNN and LSTM have comparable test perplexity with\n108.7 and 109.3 respectively. However, it is possible to\nachieve better results by conditioning on previous sen-\ntences. Unlike the LSTM, we found that the GCNN over-\nﬁts on this quite small dataset and so we note the model is\nbetter suited to larger scale problems.\n5.1. Computational Efﬁciency\nComputational cost is an important consideration for lan-\nguage models. Depending on the application, there are a\nnumber of metrics to consider. We measure the throughput\nLanguage Modeling with Gated Convolutional Networks\n0 5 10 15 20 25 30 35\nEpochs\n45\n50\n55\n60\n65\n70\n75\n80Test PerplexityTanh\nReLU\nGTU\nGLU\n0 50 100\nHours\n40\n45\n50\n55\n60\n65\n70Test Perplexity\nReLU\nGTU\nGLU\nFigure 3.Learning curves on WikiText-103 (left) and Google Billion Word (right) for models with different activation mechanisms.\nModels with gated linear units (GLU) converge faster and to a lower perplexity.\nThroughput Responsiveness\n(CPU) (GPU) (GPU)\nLSTM-2048 169 45,622 2,282\nGCNN-9 121 29,116 29,116\nGCNN-8 Bottleneck 179 45,878 45,878\nTable 4.Processing speed in tokens/s at test time for an LSTM\nwith 2048 units and GCNNs achieving 43.9 perplexity on Google\nBillion Word. The GCNN with bottlenecks improves the respon-\nsiveness by 20 times while maintaining high throughput.\nof a model as the number of tokens that can be processed\nper second. Throughput can be maximized by processing\nmany sentences in parallel to amortize sequential opera-\ntions. In contrast, responsiveness is the speed of process-\ning the input sequentially, one token at a time. Through-\nput is important because it indicates the time required to\nprocess a corpus of text and responsiveness is an indicator\nof the time to ﬁnish processing a sentence. A model can\nhave low responsiveness but high throughput by evaluating\nmany sentences simultaneously through batching. In this\ncase, such a model is slow in ﬁnishing processing individ-\nual sentences, but can process many sentences at a good\nrate.\nWe evaluate the throughput and responsiveness for mod-\nels that reach approximately 43.9 perplexity on the Google\nBillion Word benchmark. We consider the LSTM with\n2048 units in Table 2, a GCNN-8Bottleneck with 7 Resnet\nblocks that have a bottleneck structure as described by (He\net al., 2015a) and a GCNN-8 without bottlenecks. A bot-\ntleneck block wedges a k > 1 convolution between two\nk = 1 layers. This designs reduces computational cost by\nreducing and increasing dimensionality with thek= 1 lay-\ners so that the convolution operates in a lower dimensional\nspace. Our results show that the use of bottleneck blocks is\nimportant to maintaining computational efﬁciency.\nThe throughput of the LSTM is measured by using a large\nbatch of 750 sequences of length20, resulting in15,000 to-\nkens per batch. The responsiveness is the average speed to\nprocess a sequence of 15,000 contiguous tokens. Table 4\nshows that the throughput for the LSTM and the GCNN\nare similar. The LSTM performs very well on GPU be-\ncause the large batch size of 750 enables high paralleliza-\ntion over different sentences. This is because the LSTM\nimplementation has been thoroughly optimized and uses\ncuDNN, whereas the cuDNN implementation of convolu-\ntions is not been optimized for the 1-D convolutions we use\nin our model. We believe much better performance can be\nachieved by a more efﬁcient 1-D cuDNN convolution. Un-\nlike the LSTM, the GCNN can be parallelized both over\nsequences as well as across the tokens of each sequence,\nallowing the GCNN to have 20x higher responsiveness.\n5.2. Gating Mechanisms\nIn this section, we compare the gated linear unit with\nother mechanisms as well as to models without gating.\nWe consider the LSTM-style gating mechanism (GTU)\ntanh(X ∗W + b) ⊗σ(X ∗V + c) of (Oord et al., 2016b)\nand networks that use regular ReLU or Tanh activations.\nGating units add parameters, so for fair comparison, we\ncarefully cross-validate models with a comparable number\nof parameters. Figure 3 (left) shows that GLU networks\nconverge to a lower perplexity than the other approaches\non WikiText-103. Similar to gated linear units, the ReLU\nhas a linear path that lets the gradients easily pass through\nthe active units. This translates to much faster convergence\nfor both the ReLU and the GLU. On the other hand, neither\nTanh nor GTU have this linear path, and thus suffer from\nthe vanishing gradient problem. In the GTU, both the in-\nputs as well as the gating units can cut the gradient when\nthe units saturate.\nComparing the GTU and Tanh models allows us to measure\nLanguage Modeling with Gated Convolutional Networks\n10 20 30 40 50 60 70\nContext\n30\n32\n34\n36\n38\n40\n42\n44Test Perplexity\n5 10 15 20 25\nContext\n40\n50\n60\n70\n80\n90Test Perplexity\nFigure 4.Test perplexity as a function of context for Google Billion Word (left) and Wiki-103 (right). We observe that models with\nbigger context achieve better results but the results start diminishing quickly after a context of 20.\nthe effect of gating since the Tanh model can be thought of\nas a GTU network with the sigmoid gating units removed.\nThe results (Figure 3, left) show that the gating units make\na vast difference and provide useful modeling capabilities,\nas there is a large difference in the performance between\nGTU and Tanh units. Similarly, while ReLU unit is not\nan exact ablation of the gating units in the GLU, it can be\nseen as a simpliﬁcation ReLU (X) = X ⊗(X >0) where\nthe gates become active depending on the sign of the input.\nAlso in this case, GLU units lead to lower perplexity.\nIn Figure 3 (right) we repeat the same experiment on the\nlarger Google Billion Words dataset. We consider a ﬁxed\ntime budget of 100 hours because of the considerable train-\ning time required for this task. Similar to WikiText-103,\nthe gated linear units achieve the best results on this prob-\nlem. There is a gap of about 5 perplexity points between\nthe GLU and ReLU which is similar to the difference be-\ntween the LSTM and RNN models measured by (Jozefow-\nicz et al., 2016) on the same dataset.\n5.3. Non-linear Modeling\nThe experiments so far have shown that the gated linear\nunit beneﬁts from the linear path the unit provides com-\npared to other non-linearities. Next, we compare networks\nwith GLUs to purely linear networks and networks with\nbilinear layers in order to measure the impact of the non-\nlinear path provided by the gates of the GLU. One mo-\ntivation for this experiment is the success of linear mod-\nels on many natural language processing tasks (Manning\n& Sch ¨utze, 1999). We consider deep linear convolutional\nnetworks where the layers lack the gating units of the GLU\nand take the form hl(X) = X ∗W + b. Stacking sev-\neral layers on top of each other is simply a factorization of\nthe model which remains linear up to the softmax, at which\npoint it becomes log-linear. Another variation of GLUs are\nbilinear layers (Mnih & Hinton, 2007) which take the form\nhl(X) = (X ∗W + b) ⊗(X ∗V + c).\n0 50 100\nHours\n40\n60\n80\n100\n120\n140Test Perplexity\nLinear\nBilinear\nGLU\nFigure 5.Learning curves on Google Billion Word for models\nwith varying degrees of non-linearity.\nFigure 5 shows that GLUs perform best, followed by bilin-\near layers and then linear layers. Bilinear layers improve\nover linear ones by more than 40 perplexity points, and the\nGLU improves another 20 perplexity points over the bilin-\near model. The linear model performs very poorly at per-\nplexity 115 even compared to67.6 of a Kneser-Ney 5-gram\nmodel, even though the former has access to more context.\nSurprisingly, the introduction of the bilinear units is enough\nto reach 61 perplexity on Google Billion Word, which sur-\npasses both Kneser-Ney 5-gram models and the non-linear\nneural model of (Ji et al., 2015).\n5.4. Context Size\nFigure 4 shows the impact of context size for the gated\nCNN. We tried different combinations of network depth\nand kernel widths for each context size and chose the best\nperforming one for each size. Generally, larger contexts\nLanguage Modeling with Gated Convolutional Networks\nimprove accuracy but returns drastically diminish with win-\ndows larger than 40 words, even for WikiText-103 where\nwe may condition on an entire Wikipedia article. This\nmeans that the unlimited context offered by recurrent mod-\nels is not strictly necessary for language modeling. Fur-\nthermore, this ﬁnding is also congruent with the fact that\ngood performance with recurrent networks can be obtained\nby truncating gradients after only 40 timesteps using trun-\ncated back propagation through time. Figure 4 also shows\nthat WikiText-103 beneﬁts much more from larger context\nsize than Google Billion Word as the performance degrades\nmore sharply with smaller contexts. WikiText-103 pro-\nvides much more context than Google Billion Word where\nthe average sentence size is 20. However, while the average\nsize of the documents is close to 4000 tokens, we ﬁnd that\nstrong performance can be achieved with a context size as\nlow as 30 tokens.\n5.5. Training\nIn this section, we perform an ablation study of the impact\nof weight normalization and gradient clipping. We sepa-\nrately cross-validate the hyper-parameters of each conﬁgu-\nration to make the comparison fair. Due to the high cost of\neach of these experiments, we only consider a single itera-\ntion over the training data. Figure 6 shows that both meth-\nods signiﬁcantly speed up convergence. Weight normal-\nization in particular improves the speed by over two times.\nThis speedup is partly due to the ability to use much larger\nlearning rates (1 instead of 0.01) than would otherwise be\npossible. Both clipping and weight normalization add com-\nputational overhead, but it is minor compared to the large\ngains in convergence speed.\n40000 80000 120000 160000\nUpdates\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140Test Perplexity\nWithout Clipping\nWithout WeightNorm\nWith Both\nFigure 6.Effect of weight normalization and gradient clipping on\nGoogle Billion Word.\n6. Conclusion\nWe introduce a convolutional neural network for language\nmodeling with a novel gating mechanism. Compared to\nrecurrent neural networks, our approach builds a hierarchi-\ncal representation of the input words that makes it easier\nto capture long-range dependencies, similar in spirit to the\ntree-structured analysis of linguistic grammar formalisms.\nThe same property eases learning since features are passed\nthrough a ﬁxed number of layers and non-linearities, un-\nlike for recurrent networks where the number of processing\nsteps differs depending on the position of the word in the\ninput. The results show that our gated convolutional net-\nwork achieves a new state of the art on WikiText-103. On\nthe Google Billion Word benchmark, we show competitive\nresults can be achieved with signiﬁcantly fewer resources.\nAcknowledgments\nWe would like to thank Ben Graham, Jonas Gehring,\nEdouard Grave, Armand Joulin and Ronan Collobert for\nhelpful discussions.\nReferences\nBengio, Yoshua, Ducharme, R´ejean, Vincent, Pascal, and Jauvin,\nChristian. A neural probabilistic language model. journal of\nmachine learning research, 3(Feb):1137–1155, 2003.\nChelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge, Qi, Brants,\nThorsten, Koehn, Phillipp, and Robinson, Tony. One billion\nword benchmark for measuring progress in statistical language\nmodeling. arXiv preprint arXiv:1312.3005, 2013.\nChen, Stanley F and Goodman, Joshua. An empirical study of\nsmoothing techniques for language modeling. In Proceedings\nof the 34th annual meeting on Association for Computational\nLinguistics, pp. 310–318. Association for Computational Lin-\nguistics, 1996.\nChen, Wenlin, Grangier, David, and Auli, Michael. Strategies\nfor training large vocabulary neural language models. CoRR,\nabs/1512.04906, 2016.\nCollobert, Ronan, Kavukcuoglu, Koray, and Farabet, Clement.\nTorch7: A Matlab-like Environment for Machine Learning. In\nBigLearn, NIPS Workshop, 2011. URL http://torch.ch.\nDauphin, Yann N and Grangier, David. Predicting distri-\nbutions with linearizing belief networks. arXiv preprint\narXiv:1511.05622, 2015.\nGlorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty\nof training deep feedforward neural networks. The handbook\nof brain theory and neural networks, 2010.\nGrave, E., Joulin, A., Ciss ´e, M., Grangier, D., and J ´egou, H.\nEfﬁcient softmax approximation for GPUs. ArXiv e-prints,\nSeptember 2016a.\nGrave, E., Joulin, A., and Usunier, N. Improving Neural Lan-\nguage Models with a Continuous Cache. ArXiv e-prints, De-\ncember 2016b.\nLanguage Modeling with Gated Convolutional Networks\nGutmann, Michael and Hyv¨arinen, Aapo. Noise-contrastive esti-\nmation: A new estimation principle for unnormalized statisti-\ncal models.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.\nDeep residual learning for image recognition. arXiv preprint\narXiv:1512.03385, 2015a.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.\nDelving deep into rectiﬁers: Surpassing human-level perfor-\nmance on imagenet classiﬁcation. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 1026–1034,\n2015b.\nHochreiter, Sepp and Schmidhuber, J ¨urgen. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997.\nJi, Shihao, Vishwanathan, SVN, Satish, Nadathur, Anderson,\nMichael J, and Dubey, Pradeep. Blackout: Speeding up recur-\nrent neural network language models with very large vocabu-\nlaries. arXiv preprint arXiv:1511.06909, 2015.\nJozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer,\nNoam, and Wu, Yonghui. Exploring the limits of language\nmodeling. arXiv preprint arXiv:1602.02410, 2016.\nKalchbrenner, Nal, Espeholt, Lasse, Simonyan, Karen, van den\nOord, Aaron, Graves, Alex, and Kavukcuoglu, Koray. Neural\nMachine Translation in Linear Time. arXiv, 2016.\nKneser, Reinhard and Ney, Hermann. Improved backing-off for\nm-gram language modeling. In Acoustics, Speech, and Signal\nProcessing, 1995. ICASSP-95., 1995 International Conference\non, volume 1, pp. 181–184. IEEE, 1995.\nKoehn, Philipp. Statistical Machine Translation. Cambridge Uni-\nversity Press, New York, NY , USA, 1st edition, 2010. ISBN\n0521874157, 9780521874151.\nKuchaiev, Oleksii and Ginsburg, Boris. Factorization tricks for\nLSTM networks. CoRR, abs/1703.10722, 2017. URL http:\n//arxiv.org/abs/1703.10722.\nLeCun, Yann and Bengio, Yoshua. Convolutional networks for\nimages, speech, and time series. The handbook of brain theory\nand neural networks, 3361(10):1995, 1995.\nManning, Christopher D and Sch ¨utze, Hinrich. Foundations of\nstatistical natural language processing, 1999.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer Sen-\ntinel Mixture Models. ArXiv e-prints, September 2016.\nMikolov, Tom´aˇs, Martin, Karaﬁ´at, Burget, Luk´aˇs, Cernock´y, Jan,\nand Khudanpur, Sanjeev. Recurrent Neural Network based\nLanguage Model. In Proc. of INTERSPEECH, pp. 1045–1048,\n2010.\nMnih, Andriy and Hinton, Geoffrey. Three new graphical models\nfor statistical language modelling. In Proceedings of the 24th\ninternational conference on Machine learning, pp. 641–648.\nACM, 2007.\nMorin, Frederic and Bengio, Yoshua. Hierarchical probabilistic\nneural network language model. In Aistats, volume 5, pp. 246–\n252. Citeseer, 2005.\nOord, Aaron van den, Kalchbrenner, Nal, and Kavukcuoglu,\nKoray. Pixel recurrent neural networks. arXiv preprint\narXiv:1601.06759, 2016a.\nOord, Aaron van den, Kalchbrenner, Nal, Vinyals, Oriol, Espe-\nholt, Lasse, Graves, Alex, and Kavukcuoglu, Koray. Condi-\ntional image generation with pixelcnn decoders.arXiv preprint\narXiv:1606.05328, 2016b.\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the\ndifﬁculty of training recurrent neural networks. InProceedings\nof The 30th International Conference on Machine Learning,\npp. 1310–1318, 2013.\nSalimans, Tim and Kingma, Diederik P. Weight normalization: A\nsimple reparameterization to accelerate training of deep neural\nnetworks. arXiv preprint arXiv:1602.07868, 2016.\nShazeer, Noam, Pelemans, Joris, and Chelba, Ciprian. Skip-gram\nlanguage modeling using sparse non-negative matrix probabil-\nity estimation. arXiv preprint arXiv:1412.1454, 2014.\nShazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysztof, Davis,\nAndy, Le, Quoc V ., Hinton, Geoffrey E., and Dean, Jeff. Out-\nrageously large neural networks: The sparsely-gated mixture-\nof-experts layer. CoRR, abs/1701.06538, 2017. URL http:\n//arxiv.org/abs/1701.06538.\nSteedman, Mark. The syntactic process. 2002.\nSutskever, Ilya, Martens, James, Dahl, George E, and Hinton, Ge-\noffrey E. On the importance of initialization and momentum in\ndeep learning. 2013.\nWang, Mingxuan, Lu, Zhengdong, Li, Hang, Jiang, Wenbin, and\nLiu, Qun. gencnn: A convolutional architecture for word\nsequence prediction. CoRR, abs/1503.05034, 2015. URL\nhttp://arxiv.org/abs/1503.05034.\nYu, Dong and Deng, Li. Automatic Speech Recognition: A Deep\nLearning Approach. Springer Publishing Company, Incorpo-\nrated, 2014. ISBN 1447157788, 9781447157786.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8552223443984985
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7389357089996338
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7318168878555298
    },
    {
      "name": "Sentence",
      "score": 0.6462613940238953
    },
    {
      "name": "Language model",
      "score": 0.635979175567627
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5938834547996521
    },
    {
      "name": "Latency (audio)",
      "score": 0.5917502045631409
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5331401824951172
    },
    {
      "name": "Task (project management)",
      "score": 0.5270239114761353
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4807124435901642
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4239060878753662
    },
    {
      "name": "Natural language processing",
      "score": 0.38591310381889343
    },
    {
      "name": "Machine learning",
      "score": 0.3291547894477844
    },
    {
      "name": "Artificial neural network",
      "score": 0.19254347681999207
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}