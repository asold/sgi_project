{
  "title": "Evaluation and Analysis of Large Language Models for Clinical Text Augmentation and Generation",
  "url": "https://openalex.org/W4393864958",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2129254613",
      "name": "Atif Latif",
      "affiliations": [
        "Dongguk University"
      ]
    },
    {
      "id": "https://openalex.org/A2177100643",
      "name": "Jihie Kim",
      "affiliations": [
        "Dongguk University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6769311223",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W4323244244",
    "https://openalex.org/W3174828871",
    "https://openalex.org/W4226323522",
    "https://openalex.org/W1977134866",
    "https://openalex.org/W2109974590",
    "https://openalex.org/W6856669201",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2921249176",
    "https://openalex.org/W3135135238",
    "https://openalex.org/W3191244417",
    "https://openalex.org/W4313452733",
    "https://openalex.org/W4385076068",
    "https://openalex.org/W4385570982",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319301505",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6761672038",
    "https://openalex.org/W4285306300",
    "https://openalex.org/W3115881617",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4319301446",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W3152320027",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2251658415",
    "https://openalex.org/W3100742171",
    "https://openalex.org/W6761189655",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W2970796366",
    "https://openalex.org/W6774569510",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W3201244947",
    "https://openalex.org/W4388725043",
    "https://openalex.org/W4318931874",
    "https://openalex.org/W6845880546",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W2150824314",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W4205403018",
    "https://openalex.org/W4210721047",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W4386566910",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4386840094",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4287829148",
    "https://openalex.org/W2936695845"
  ],
  "abstract": "A major challenge in deep learning (DL) model training is data scarcity. Data scarcity is commonly found in specific domains, such as clinical or low-resource languages, that are not vastly explored in AI research. In this paper, we investigate the generation capability of large language models such as Text-To-Text Transfer Transformer (T5) and Bidirectional and Auto-Regressive Transformers (BART) for Clinical Health-Aware Reasoning across Dimensions (CHARDAT) dataset by applying the ChatGPT augmentation technique. We employed ChatGPT to rephrase each instance of the training set into conceptually similar but semantically different samples and augmented them to the dataset. This study aims to investigate the utilization of large language models, ChatGPT in particular, for data augmentation to overcome the limited availability in the clinical domain. In addition to the ChatGPT augmentation, we applied other augmentation techniques, such as easy data augmentation (EDA) and an easier data augmentation (AEDA), to clinical data. ChatGPT comprehended the contextual significance of sentences within the dataset and successfully modified English terms but not clinical terms. The original CHARDAT datasets represent 52 health conditions across three clinical dimensions, i.e., Treatments, Risk Factors, and Preventions. We compared the outputs for different augmentation techniques and evaluated their relative performance. Additionally, we examined how these techniques perform with different pre-trained language models, assessing their sensitivity in various contexts. Despite the relatively small size of the CHARDAT dataset, our results demonstrated that augmentation methods like ChatGPT augmentation surpassed the efficiency of the previously employed back-translation augmentation. Specifically, our findings revealed that the BART model resulted in superior performance, achieving a rouge score of 52.35 for ROUGE-1, 41.59 for ROUGE-2, and 50.71 for ROUGE-L.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nEvaluation and Analysis of Large Language\nModels for Clinical Text Augmentation and\nGeneration\nATIF LATIF1, and JIHIE KIM2\n1Department of Artificial Intelligence, Dongguk University, Jung-gu, Seoul 04620, South Korea\n3College of AI Convergence, Dongguk University, Jung-gu, Seoul 04620, South Korea\nCorresponding author: Jihie Kim (jihie.kim@dgu.edu)\nThis research was supported by the MSIT(Ministry of Science and ICT), Korea, under the ITRC(Information Technology Research Center)\nsupport program(IITP-2024-2020-0-01789), and the Artificial Intelligence Convergence Innovation Human Resources Development\n(IITP-2024-RS-2023-00254592) supervised by the IITP(Institute for Information & Communications Technology Planning & Evaluation).\nABSTRACT A major challenge in deep learning (DL) model training is data scarcity. Data scarcity is\ncommonly found in specific domains, such as clinical or low-resource languages, that are not vastly explored\nin AI research. In this paper, we investigate the generation capability of large language models such as Text-\nTo-Text Transfer Transformer (T5) and Bidirectional and Auto-Regressive Transformers (BART) for Clinical\nHealth-Aware Reasoning across Dimensions (CHARDAT) dataset by applying the ChatGPT augmentation\ntechnique. We employed ChatGPT to rephrase each instance of the training set into conceptually similar\nbut semantically different samples and augmented them to the dataset. This study aims to investigate\nthe utilization of large language models, ChatGPT in particular, for data augmentation to overcome the\nlimited availability in the clinical domain. In addition to the ChatGPT augmentation, we applied other\naugmentation techniques, such as easy data augmentation (EDA) and an easier data augmentation (AEDA),\nto clinical data. ChatGPT comprehended the contextual significance of sentences within the dataset and\nsuccessfully modified English terms but not clinical terms. The original CHARDAT datasets represent\n52 health conditions across three clinical dimensions, i.e., Treatments, Risk Factors, and Preventions.\nWe compared the outputs for different augmentation techniques and evaluated their relative performance.\nAdditionally, we examined how these techniques perform with different pre-trained language models,\nassessing their sensitivity in various contexts. Despite the relatively small size of the CHARDAT dataset,\nour results demonstrated that augmentation methods like ChatGPT augmentation surpassed the efficiency of\nthe previously employed back-translation augmentation. Specifically, our findings revealed that the BART\nmodel resulted in superior performance, achieving a rouge score of 52.35 for ROUGE-1, 41.59 for ROUGE-\n2, and 50.71 for ROUGE-L.\nINDEX TERMS BART, Large Language Models, T5, Text Augmentation, Text Mining\nI. INTRODUCTION\nLarge Language Models (LLMs) befitted to different Nat-\nural Language Processing (NLP) tasks such as generation,\nsummarization, translation, and classification. These models\nare pre-trained on a large corpus, which is fine-tuned for\ndifferent generation tasks. However, recent studies show that\neven pre-trained models like BART [1] and T5 [2] struggle\nwith commonsense tasks that individuals can easily reason\n[3]. The challenge is mainly due to its specialization rooted in\nthe obscure knowledge not widely found in the training data\nof the pre-trained language model. Additionally, it demands\na heightened level of specialized reasoning to effectively\naddress domain-specific tasks [4].\nData Augmentation techniques are perfectly suited to over-\ncome the issue by improving robustness and reducing the\noverfitting of deep learning models by increasing the data\nquantity and variety without explicitly collecting new data.\nFurthermore, recent studies suggested data augmentation\ntechniques, e.g., random insertion, synonym replacement, and\nback translation, which have successfully been used in deep\nlearning (DL) to inflate data, enabling models with greater\ngeneralization power [5]. Data Augmentation in computer\nvision can be performed by transformations such as flipping,\nresizing, cropping, and color shifting [6]. However, unlike\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nthe aforementioned applications, NLP uses more specialized\ntechniques for language data augmentation. Moreover, most\nclinical tasks lack large corpora or manually crafted written\ndata to train it on several deep learning models. Therefore,\nresearchers have investigated different methods of data aug-\nmentation for improving performance on clinical tasks. Al-\nthough many methods were effective, selecting an effective\ndata augmentation type remained unexplored.\nThe Natural language processing (NLP) and Machine Learn-\ning (ML) amalgamation have ushered new prospects in the\nhealthcare industry, vital to human well-being. The blending\nwill prompt healthcare automation to ease the overworked\nand exhausted healthcare workers, a condition widely re-\nported during the COVID-19 pandemic [7]. In the health-\ncare domain, large language models can improve patient\noutcomes, electronic health records (EHRs), healthcare de-\nlivery, and reduced healthcare costs. LLMs can also help\nin predicting the diagnosis of diseases, suggest treatment or\ntherapy, or provide clinical decision support. The success\nof healthcare automation widely depends upon specialized\nreasoning. Previous research in health-related reasoning has\nprimarily focused on retrieval-based methods, with limited\nexploration of generation-based approaches along with data\naugmentation [8]. Generation-based reasoning poses greater\nchallenges due to the specialized nature of the domain, which\ncontains intricate information not commonly found in the\ntraining data of PLMs (Pre-trained Language Models). Ad-\nditionally, domain-specific problems demand a higher degree\nof specialized reasoning. We evaluate our models using auto-\nmated methods and qualitative analyses.\nTo overcome the data scarcity and intricate reasoning limita-\ntions, our study focuses on applying ChatGPT data augmenta-\ntion and Clinical Health-Aware Reasoning across Dimensions\n(CHARDAT) dataset analysis. CHARDAT dataset comprises\nthree dimensions prevention, treatment, and risk factors,\nwhich are utilized to assess the effectiveness of text genera-\ntion models. The original training instances were rephrased\ninto several conceptually related but semantically distinct\ninstances. In our research, we utilize a well-crafted zero-shot\nprompt, which proved more effective than one-shot or few-\nshot prompting techniques. Zero-shot prompting involves\nproviding the model with no examples to learn from, whereas,\nin one-shot or few-shot prompting, the model is given either\none or a few examples to learn from [9]. For better ChatGPT\naugmentation results, we have designed prompts for single-\nturn dialogue of the dataset. The single-turn dialogue applies\nthe input prompt to each sample separately. Our prompts were\n‘rephrase the following sentence for prevention, treatment,\nor risk factor after the word because/since/as ’. We have\napplied this technique without changing the format of training\nsamples. Figure 1 presents the pipeline for enhancing the\nCHARDAT dataset analysis via data augmentation based on\nChatGPT.\nThe main contributions of work are:\n1) A methodology to combine LLMs text generation ap-\nproaches with data augmentation techniques, including\nChatGPT augmentation for concise, readable, and in-\nformative clinical text generation for CHARDAT.\n2) Prompt-based data augmentation by leveraging\nChatGPT. Through appropriate and comprehensive\nprompts, we optimized the generation cost of synthetic\ndata for the training set.\n3) A thorough analysis explores the impact of data aug-\nmentation techniques on the text generation capabilities\nof LLMs and other qualities of the text, such as evalu-\nation metrics and diversity.\nA. ORGANIZATION\nThe rest of the paper is organized as follows. Section II\ndiscusses the related work, which mainly comprises the ex-\nisting large language models and data augmentation used in\nthe clinical domain. III explains the CHARDAT dataset for\nclinical text generation. IV explains the methodology of the\nresearch paper. Experiments and Results are presented in V.\nFinally, the Conclusion and future work of this paper is given\nin VI.\nII. RELATED WORK\nClinical NLP tasks can be further significantly by adopting\nLLMs. Healthcare workers can extensively use these models\nto help with activities like medical diagnosis and electronic\nhealth record analysis by helping to interpret complicated\nmedical texts. Using complex algorithms, these models can\nautomatically extract relevant data from large patient records.\nFurthermore, this allows for accurate disease classification\nand individualized therapy recommendations. This stream-\nlined approach opens the possibilities of precision medicine\nand improves workflow efficiency in the healthcare industry.\nIn short, LLM integration is ready to transform medical re-\nsearch and enhance patient outcomes.\nA. LARGE LANGUAGE MODELS IN CLINICAL DOMAIN\nPrior to LLMs, Pre-trained language models (PLMs) like\nBERT [10] and Roberta [11] were widely used for data anal-\nysis. While early efforts have been made to develop PLMs\nbased on neural networks for the clinical domain [12]–[16],\nthese models were one-task systems that lack expressive and\ninteractive features. Consequently, there was a gap between\nthe capabilities of the then state-of-the-art models and what\nwas expected of them in real-world clinical tasks. However,\nmore recently developed GPT-3.5 and its successor GPT-4 in-\ntroduced by OpenAI and BARD introduced by Google show-\ncased advanced performance across diverse applications, par-\nticularly in the clinical and healthcare domain. [17]–[19].\nAmong numerous LLM applications, clinical NLP tasks, in-\ncluding name entity recognition, relation extraction, and clas-\nsification, have enormous potential. Most of the prior work\nin the clinical domain involves relation or extraction. In the\nclinical domain, domain-specific adaptations of BERT, such\nas BioBERT [20] and ClinicalBERT [21], have been used\nfor various clinical language processing tasks. MIMICause\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 1. Pipline of data augmentation based on ChatGPT for CHARDAT dataset\nextracts causal clinical information from EHR to understand\nnarratives in medical text [22].\nIn NLP, text generation is pivotal and recently became an\nactive area of research. During the initial stages of NLP\nresearch, traditional methods of text generation were based\non a sentence ranking algorithm [23] to evaluate the impor-\ntance of sentences in a text. In recent years, transformer-\nbased models have outperformed all other models, specifi-\ncally in the generation tasks. Raffel, et. al explore the land-\nscape of transfer learning mechanism for NLP to introduce\na unified framework that converts all text-based language\nproblems into a text-to-text format. Similarly, Lewis, et. al\nproposed a model that combines a Bidirectional and Auto-\nRegressive transformer that maps a corrupted text to the\noriginal text from which it is derived. Moreover, the Gen-\nerative Pre-Trained Transformer (GPT) approach presented\nin [24] suggested training a Transformer decoder model on a\nvast text corpus using a classical casual language modeling\ntask. In this task, the model learns to predict the next word\ntoken based solely on the preceding word tokens. Thereafter,\nadvancements like GPT-2 [25] and GPT-3 [26] have been\nintroduced, featuring larger model sizes and pre-training on\neven more extensive text corpora. These advanced models\nhave demonstrated outstanding performance across various\ntasks, such as translation, summarization, and classification,\nincluding reading comprehension. They can achieve impres-\nsive results even without fine-tuning, thanks to well-designed\nprompts. Generative language models like BART and T5\nhave shown impressive results in various natural language\nprocessing tasks. However, there is still limited exploration of\ndomain-specific generative models, specifically in the clinical\ndomain. These models can potentially revolutionize clinical\ntext generation and other related tasks, so in this paper, we\nexplore the generation capabilities of BART and T5 models\nfor the CHARDAT dataset.\n1) Challenges and ethical implications in large language\nmodels\nIt is important to address the ethical challenges in the clinical\ndomain due to raised ethical concerns about patient data\nprivacy, bias, misinformation, and security [19], [27]. The\npotential spread of misinformation can pose serious soci-\netal risks. LLMs have limitations and are known to provide\ninaccurate or biased results. These errors may be recycled\nand magnified because LLM results can be utilized to train\nsubsequent model iterations [19]. This raises worries about\nthe scientific record’s integrity and the possibility of incorrect\ninformation being employed in future research or clinical\nhealth decision-making policies.\nB. DATA AUGMENTATION\nData augmentation techniques deal with generating addi-\ntional, synthetic training samples where the given data is\ninsufficient. It increases the amount (number of samples in\nthe training set) and the diversity (variety of data) of a dataset\n[28]. Overfitting on the training set is frequently caused by\na lack of labeled data; data augmentation attempts to address\nthis problem by manually or automatically modifying data to\nproduce more augmented data. For good model performance,\nhigh-quality text is important for many NLP tasks. The de-\nvelopment of GPT models has made it easier to generate\nnew synthetic data. There are some traditional techniques that\noperate at the character, word, sentence, and document levels.\nIn character-level methods, such as exchange, replacement,\nrandom insertion, or deletion, improve model performance\nagainst noise [29]. Word-level techniques, like random swap,\nrandom deletion, random insertion, synonym replacement\n[30], and word embedding-based approaches [31], enhance\ntext classification tasks while maintaining semantic consis-\ntency. [32] investigates different text augmentation methods\nand their effectiveness and evaluates their performance in\nterms of fluency, diversity, and semantic content preservation.\n[33] proposes a rule-based method in sequence modeling\nfor data augmentation, which reuses previously observed se-\nquence fragments and aims to supply a simple and model-\nagnostic bias in novel environments. In [34], a novel data aug-\nmentation technique is used, which replaces original words\nwith other words through paradigmatic relationships using a\nbi-directional language model at the position of the words.\nRecently, conditional-based generation methods have been\nproposed which generate augmented data from large language\nmodels. After training the model to generate original text\ngiven the label, the model can generate augmented or new\ntext [35]–[37]. With the advancements and success of pre-\ntrained language models [38], [39], they generate data from\nzero-shot models. GatorTronGPT developed by [40] used\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nGPT-3 architecture to generate 20 billion words of clinical\nsynthetic text. Large Language models can be used to develop\na high-quality dataset for the clinical domain [41] using logit\nsuppression and temperature sampling techniques to increase\ndata diversity. Although data generated by these models can\nbe novel and diverse and might be unseen in the original data,\nit requires a lot of effort for training.\nIII. DATASET\nClinical Health-Aware Reasoning across Dimensions\n(CHARDAT) [42] is an open-access clinical dataset com-\nprising three dimensions: Treatments (TREAT), Risk factors\n(RF), and Preventions (PREV). CHARDAT dataset was ac-\ncomplished by searching through trusted medically reviewed\nsources like MayoClinic, CDC, WebMD, and Healthline.\nCHARDAT is a manually curated list of health conditions,\ncomprising common conditions such as acne and migraine to\nrare conditions such as Lyme Disease and Paget-Schtroetter.\nThese conditions may be treatable or chronic, either self-\ndiagnosed or not. CHARDAT allows users to assess across\ndifferent conditions. CHARDAT comprises 52 conditions,\nwith each condition potentially having one or more attributes,\nsuch as risk factors, treatment options, and prevention meth-\nods, varying across instances. The term treatment in the\ndataset refers to the medical cure for injuries, illnesses,\ndiseases, or disabilities. Treatments can include medications,\nphysical therapy, counseling, lifestyle changes, or therapeutic\ninterventions. For Erectile Dysfunction, some examples of\ntreatments in the set are medications such as Sildenafil,\nand Vardenafil, Lose weight, and surgeries. A risk factor is\nany characteristic, attribute, or exposure that increases the\nlikelihood of developing disease or injury. Examples of risk\nfactors for anxiety include gender, mental health, and alcohol.\nSimilarly, prevention refers to actions and measures taken\nto reduce or avoid the development of diseases and injuries.\nExamples of Prevention for Alzheimer’s disease are a healthy\ndiet, social interaction, and regular exercise. Table 1 shows the\nnumber of different conditions (dimensions) and the splitting\nof conditions into train, test, and validation, and Table 2 shows\nthe number of sentences for the dimensions.\nTABLE 1. Number of conditions for dimensions in dataset\nNo of conditions Train Validation Test\nRisk factor = 52 44 26 26\nTreatment = 52 43 21 20\nPrevention = 44 35 11 21\nTABLE 2. Number of sentences in dataset\nNo of sentences Train Validation Test\nRisk factor = 457 319 69 69\nTreatment = 297 207 45 45\nPrevention = 183 129 27 27\nIV. METHODOLOGY\nA. LARGE LANGUAGE MODELS\nLLMs considerably fostered the NLP domain. The recently\ndeveloped attention mechanism in the LLMs has totally\nchanged how machines learn to process text and language\nin parallel. Due to LLM’s potential to generate, translate,\nand summarize text, it marks an exciting chapter in the NLP\nevolution. After a comprehensive review of the state-of-the-\nart models, we chose BART and T5 models for generation.\nTo take advantage of these large language models, owing to\ntheir robust multitask pretraining and text-infilling technique,\nmaking them well-suited for the template infilling technique\nfor the generation task of the CHARDAT dataset.\n1) BART\nBART is a seq2seq model (denoising autoencoder) [1] trained\nto reconstruct original text from the corrupted text. It is\na sequence-to-sequence model with a bidirectional encoder\nover corrupted text and a left-to-right autoregressive decoder.\nPre-training BART applies several transformations, such as\ntoken masking, token deletion, text infilling, sentence per-\nmutation, and document rotations. In our paper, we applied\nthe text-infilling method for our generation task, and it was\nsuitable for the generation task of the CHARDAT dataset.\n2) T5\nT5 is an encoder-decoder pre-trained model [2] that converts\neach task into a text-to-text format. The model is a multi-task\nmixture of unsupervised and supervised tasks pre-trained on\nthe C4 (Colossal Clean Crawled Corpus) dataset. T5’s ability\nto handle different tasks with the same underlying structure\nmakes it highly efficient and adaptable, making it a popular\nchoice for a wide range of NLP tasks such as generation,\nsummarization, and translation.\nB. CHATGPT DATA AUGMENTATION\nChatGPT is a significant advancement in NLP, granting pub-\nlic access to the capabilities of LLMs through an intuitive\nChatbot interface. Like predecessors GPT, GPT-2, and GPT-\n3, ChatGPT is an autoregressive language model based on\ntransformer decoder blocks. ChatGPT’s public accessibility\nhas contributed to its widespread popularity, making it a ver-\nsatile solution for numerous NLP applications. The authors\nin [43] have evaluated ChatGPT across a range of tasks,\ndemonstrating its proficiency in many areas, such as arith-\nmetic reasoning, questioning answering, and summarization.\nThe authors concluded that ChatGPT performs well in most\ntasks, except that it focuses on specific details (e.g., sequence\ntagging). The model excels in tasks involving high-resource\nlanguages, rivaling established translation tools like Google\nTranslate. However, challenges arise with low-resource lan-\nguages and distant language translation, such as English-\nHindi pairs. In our research, we utilized ChatGPT knowledge\nfor data augmentation.\nIn our paper, we employed ChatGPT as a data augmenta-\ntion technique to enhance data quality and quantity, as the\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nCHARDAT dataset is relatively small. By adding synthetic\ndata generated by ChatGPT into the training process, we\nobserved that combining the generated data with CHARDAT\ndata considerably improves the model performance. We have\napplied the strategy for the data augmentation employed by\nChatGPT for rephrasing the input text in various ways so that\nthe model can generate a different set of examples. We have\napplied appropriate prompts for our clinical data and analyzed\nthe results of the prompts. Table 3 shows the original and\naugmented text for the prevention dimension of medication\nin the CHARDAT dataset. This paraphrasing method helps in\nenhancing diversity, the volume of the training set, and the\nmodel’s understanding of different words in a dataset of the\nsame meaning.\nTABLE 3. Augmented sentences by ChatGPT\nInput Sentences Augmented Sentences\nA person with Sinusitis has a med-\nication prevention because taking\nchronic medication like antihis-\ntamines to prevent allergy flare\nups or steroids to reduce sinus in-\nflammation can lessen symptoms.\nA person with Sinusitis has a med-\nication prevention because the use\nof chronic medications, such as\nantihistamines to prevent allergy\nflare-ups or steroids to reduce si-\nnus inflammation, can effectively\nalleviate symptoms.\nA person with IBS has an obesity\nrisk factor because low-fiber and\nhigh-refined carbohydrate diets,\nwhich are associated with obe-\nsity, can also contribute to IBS\nsymptoms. This diet connection\nremains a potential factor linking\nIBS and obesity.\nA person with IBS has an obe-\nsity risk factor because low-fiber\nand high refined-carbohydrate di-\nets are linked to obesity and are\nanother potential contributor to\nIBS symptoms in obese persons.\nDiet remains a potential factor\nlinking IBS to obesity.\nA person with COVID-19 is ad-\nvised to stay home as a treat-\nment because it is a highly conta-\ngious disease that spreads through\ndroplets from infected individu-\nals, making it best to avoid contact\nwith others by staying home.\nA person with covid19 has a stay\nhome treatment because it is a\nhighly contagious disease that is\nspread through droplets from the\ninfected so it’s best to stay home.\nWe kept the context of the sentence the same, just rephrasing\nit with different phrases of the same meaning. The diverse\nphrases provided by ChatGPT serve as a valuable tool to\nestablish a connection between the sets used for training\nand testing purposes. This variety aids in ensuring a more\ncomprehensive and robust understanding of the underlying\ncontext, enhancing the model’s performance and adaptability\nacross different datasets.\nAlong with ChatGPT augmentation, we employed AEDA and\nEDA augmentation techniques on a training set of CHAR-\nDAT. AEDA is a popular text augmentation method known\nfor its effectiveness in introducing variations in text which\nperforms random insertion of punctuation marks into the\ntraining set. The insertion changes the positions in a sentence\nwhile keeping the order of the words leading to more general-\nized performance. Another easy and popular data augmenta-\ntion technique in NLP that we used in our paper is EDA (Easy\nData Augmentation) which applies four operations: random\ndeletion, random swap, random insertion, and synonym re-\n(a)\n(b)\n(c)\nFIGURE 2. Overview of ROUGE scores of BART and T5 models\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 4. BART performance for different data augmentation amount\nData Augmentation Amount ROUGE-1 ROUGE-2 ROUGE-L\n2x 50.78 39.63 49.21\n3x 52.35 41.59 50.71\n4x 51.27 40.83 49.94\n5x 50.13 38.42 48.57\nplacement. While applying the AEDA and EDA augmenta-\ntion techniques the model’s scores were not up to the mark\nas can seen in Table 3. We observed that AEDA and EDA\nmight introduce irreverent patterns into data, which affects\nthe model’s ability to capture meaningful representations.\nFurthermore, we assess the optimal level of data augmenta-\ntion required to achieve improved results. After experiment-\ning, we found that tripling the dataset by augmenting each\nsample three times yields the most favorable outcomes for\nour models.\nWe experimented with additional levels of data augmenta-\ntion: 2x, 4x, and 5x the original training data volume.Table\n4 shows the BART performance with different augmenta-\ntion amounts. Our aim was to investigate how the extent of\naugmentation impacts performance. Our hypothesis suggests\nthat performance could potentially rise to a certain threshold\nbefore declining. We attribute this to the diminishing returns\nof data augmentation, as the augmented data are variations\nof the original, and excessive augmentation might lead to\noverfitting in the models. Data augmentation amounts are\nhyperparameters, we train models with varying values of\ndata augmentation, and the model that performs best on the\nCHARDAT validation set is the final model selected. The\nfinal models are then used to produce on CHARDAT test set.\nFigure 2 presents the results from the overall best models.\nFrom Figure 2, it is evident that the BART large model’s per-\nformance in terms of ROUGE scores improved significantly\nwith data augmentation. Specifically, the ROUGE-1 score\nreached 52.35, ROUGE-2 achieved 41.59, and ROUGE-L\nstood at 50.71, indicating the model’s proficiency in capturing\nunigrams, bigrams, and long-range dependencies in the text,\nrespectively. Our technique of ChatGPT augmentation for\nCHARDAT outperforms the previously used backtranslation\nmethod in [42] by 0.81 for ROUGE-1, 1.22 for ROUGE-2,\nand 0.83 for ROUGE-L scores. The upward trend in the graph\nsignifies the model’s continuous learning and refinement\nthroughout the epochs (number of training instances utilized\nin one iteration), ultimately leading to these results. Table\n5 shows the comparison between the proposed ChatGPT\naugmentation technique and the other popular techniques in\nNLP.\nC. EVALUATION MATRICES\nThe evaluation matrices used in our analysis are (1) ROUGE\n[44], (2) CIDEr [45], (3) METEOR [46], and (4) BERTScore\n[47] which are discussed below.\n1) ROUGE (Recall-Oriented Understudy for Gisting\nEvaluation):\nROUGE-1 (unigram) measures the recall of single words\nin the generated text compared to reference text. ROUGE-2\n(bigram) assesses the overlap of word sequences consisting\nof two words. ROUGE-L evaluates the Longest Common\nSubsequence (LCS) between generated and reference text,\nconsidering word order. It emphasizes semantic similarity\nand content coverage by focusing on common subsequences\nregardless of their order.\n2) CIDEr (Consensus-Based Image Description Evaluation):\nCIDEr evaluates the similarity of sentences by considering\nfactors such as grammatical correctness, prominence, signifi-\ncance, and precision. It provides a comprehensive assessment\nof the quality of generated text beyond simple lexical overlap.\n3) METEOR (Metric for Evaluation of Translation with Explicit\nOrdering):\nMETEOR aligns sentences using various matching tech-\nniques, including exact matches, stemming, synonyms, and\nparaphrases. It computes a weighted F-score with an align-\nment fragmentation penalty, offering a nuanced evaluation of\ntext generation performance.\n4) BERTScore:\nBERTScore acts as a semantic similarity metric by comparing\nthe BERT embeddings of individual tokens in the generated\nand reference text. It provides a robust semantic similarity\nmeasure, leveraging the powerful contextual representations\nlearned by BERT.\nV. EXPERIMENTS AND RESULTS\nThe data presented in Table 5 demonstrates that models hav-\ning large sizes outperform base models across rouge metrics.\nSpecifically, among our models, Bart-large and T5-large ex-\nhibit remarkable performance, highlighting the advantages of\nincreasing the model size in enhancing their proficiency.\nTable 6 provides further insights into the performance of these\nmodels. T5-large stands out for its superior performance on\nthe test-unseen portion, indicating its ability to generalize\neffectively to unfamiliar conditions when trained on CHAR-\nDAT. On the other hand, BART-large shows better results in\nthe test-seen half. The differing capabilities of BART and\nT5 in encoding and representing training data are evident.\nBART excels at learning from seen examples, leveraging its\nunderstanding of familiar patterns present in the training data.\nIts training process allows it to capture complex details from\nthe training data, enabling accurate predictions on similar,\nseen test data. On the other hand, T5 excels in generalizing\nto unseen data owing to its comprehensive pre-training data\nand approach.\nWe conducted evaluations on each dimension separately,\nnamely prevention, risk factors, and treatment. Our findings\nin Table 7 indicate that, for most dimensions, specifically risk\nfactors and prevention (abbreviated as RF and PREV), BART\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 5. Comparison between the proposed and previous methods on CHARDAT\nModel ROUGE-1 ROUGE-2 ROUGE- L BERTScore CIDEr METEOR\nT5 - large + ChatGPT 51.63 39.15 49.16 59.23 9.34 24.47\nT5 - base + ChatGPT 50.79 38.55 48.79 60.04 8.95 22.95\nT5 - base + AEDA 45.13 31.38 41.83 47.68 5.51 19.63\nT5 - large + AEDA 49.04 35.72 46.58 51.36 6.52 21.83\nT5 - large + EDA 48.76 33.43 45.93 50.43 5.62 22.39\nT5 - base + EDA 48.94 34.34 46.15 50.74 5.89 22.67\nBART - large + ChatGPT 52.35 41.59 50.71 61.79 7.21 23.16\nBart - base + ChatGPT 51.87 40.7 49.58 61.25 9.13 22.59\nBART - large + AEDA 47.07 32.63 43.16 49.34 4.93 20.34\nBART - base + AEDA 48.38 35.07 45.55 50.13 5.11 21.07\nBART - base + EDA 48.55 34.15 45.55 46.01 4.97 20.45\nBART - large + EDA 46.36 32.13 43.22 46.83 4.31 20.52\nTABLE 6. Evaluation of T5 and BART on test-seen and test-unseen halves\nModel Test (seen) Test (unseen)\nMetric ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L\nT5 - large 50.89 37.21 47.9 52.05 38.39 49.21\nT5 - base 49.56 36.43 46.93 51.79 38.92 49.04\nBART - large 51.32 37.5 48.64 50.63 37.22 48.73\nBART - base 50.94 36.03 47.69 51.7 38.41 49.75\nmodels trained on those specific dimensions perform better\nhaving ROUGE-1 = 53.09, ROUGE-2 = 41.57 and ROUGE-\nL = 51.12 for risk factor and ROUGE-1 = 50.92, ROUGE-\n2 = 38.14 and ROUGE-L = 48.84 for prevention. However,\nthe T5-large model outperforms BART in the treatment di-\nmension, having ROUGE-1 = 47.3, ROUGE-2 = 34.41, and\nROUGE-L = 44.78. These results suggest that training with\nCHARDAT enabled the model to learn from data across\nvarious dimensions, enhancing its overall knowledge and\ngeneration abilities. This underscores the advantage of having\na single combined model with a broader scope of learning.\nBART-large can generate more concise, relevant, and read-\nable information compared to T5.\nBART and T5 large show good performance, while the per-\nformance of base models was worse. Bart-large explanations\nare well explained, readable, and more information in general.\nT5-large outputs were also up to the mark and are typically\nmore medically informative and explainable such as listing of\nmedications. In Table 8, we can see the generated outputs of\nBART and T5 models.\nFor training the BART and T5 models, we kept most of the\nhyperparameters unchanged except the learning rate which is\nadjusted per model. Learning rates for best-performing Bart-\nlarge and T5-large are 1e-05, 3e-05 for BART-base, and T5-\nbase is 1e-03. For base models of BART and T5 models, we\nuse batch sizes of 32 or 64, and either 16 or 8 for BART and\nT5 large models. We use encoder and decoder lengths that are\nappropriate for all instances in the CHARDAT training set,\nwhich are 32 and 128 for BART models, and 35 and 128 for\nT5 models, respectively. For T5 and BART base models, we\nuse 400 warmup steps, 1200 for T5-large, and 500 for BART-\nlarge. For the individual model, we choose the epoch that has\na high ROUGE-2 score on the CHARDAT validation set, and\nbeam search is used for decoding.\nVI. CONCLUSION AND FUTURE WORK\nIn this paper, we apply ChatGPT data augmentation on clin-\nical text for clinical text generation. Unlike other techniques,\nour models expand the limited clinical data at the seman-\ntic level to improve data robustness and consistency, which\nresults in a better performance than most of the previous\ndata augmentation techniques. Our study investigates how\nChatGPT augmentation can effectively improve the perfor-\nmance of downstream tasks for large language models in\nthe clinical domain. ChatGPT augmentation is valuable for\ndomain-specific science paper summarization [48] and clini-\ncal text summarization [49]. Publicly clinical report datasets\nare rare and often provide data on a small scale due to privacy\nconcerns. However, ChatGPT augmentation could address\nthis challenge by generating synthetic data in different rep-\nresentation styles. Along with augmentation techniques, we\ndid an in-depth analysis and evaluation of CHARDAT data on\ntwo large language models, i.e., BART and T5. CHARDAT\ndataset has three dimensions, i.e., Preventions, Treatments,\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 7. Evaluation of T5 and BART on CHARDAT trained on the individual dimension\nModel Treatment (TREAT test) Risk Factor (RF test) Prevention (PREV test)\nMatrices ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L\nT5 - large 47.3 34.41 44.78 52.51 40.62 50.64 50.45 33.32 46.52\nT5 - base 45.33 33.28 43.24 52.69 39.83 50.35 43.22 31.39 38.61\nBART - large 47.08 33.56 43.98 53.09 41.57 51.12 50.97 38.14 48.84\nBART - base 45.39 33.01 42.93 52.44 40.71 50.09 47.07 31.21 43.6\nTABLE 8. Generated examples for BART - Large, BART - Base, T5 - Large, and T5 - Base\nModel Generated Text\nInput (1) A person with depression has a/an life-changing event risk factor because/since/as {explanation}\nBART – large A person with depression has a life-changing event risk factor because depression can be triggered by a change in the environment,\nsuch as the death of a loved one or the loss of a job.\nBART – Base A person with depression has a life-changing event risk factor because people with depression are more likely to develop depression.\nT5 – large A person with depression has a life-changing event risk factor because people who have experienced a major life event, such as a\ndivorce, loss of a spouse, or death.\nT5 - base A person with depression has a life-changing event risk factor because traumatic events can lead to depression.\nInput (2) A person with rheumatoid has a medication treatment because/since/as {explanation}\nBART – large A person with rheumatoid has a medication treatment because medications like ibuprofen, metformin, naproxen, and aspirin can help\nmanage the symptoms of RA.\nBART – base A person with rheumatoid has a medication treatment because medication can help reduce the severity of rheumatic symptoms.\nT5 – large A person with rheumatoid has a medication treatment because drugs such as ibuprofen, rifampicin, and tylenol can help reduce\ninflammation and pain.\nT5 – base A person with rheumatoid has a medication treatment because it can help reduce the symptoms of the disease..\nInput (3) A person with hpv has a HPV vaccine prevention because/since/as {explanation}\nBART – large A person with hpv has a HPV vaccine prevention because HPV is a sexually transmitted virus that causes genital warts and genital\nherpes.\nBART – base A person with hpv has an HPV vaccine prevention because HPV vaccine can help by reducing the risk of HPV infection.\nT5 – large A person with hpv has an HPV vaccine prevention because the vaccine protects against the HPV virus.\nT5 – base A person with hpv has an HPV vaccine prevention because the vaccine prevents the spread of HPV .\nand Risk factors of 52 different diseases. The integration of\nChatGPT augmentation proved to be particularly impactful,\nleading to significant improvements in the performance of\nthe BART-large model. This methodology proved to be more\neffective than the previously used back-translation technique\n[42] for CHARDAT in producing high-quality training data.\nWe carried out an in-depth analysis and assessment of the\nCHARDAT dataset to provide a comprehensive comparison\nand insight into the performance of large language models.\nOverall, the results show that employing ChatGPT for data\naugmentation is beneficial for improving CHARDAT perfor-\nmance in all three dimensions.\nBased on the performance of our models utilizing ChatGPT\naugmentation, we’ve observed that the models perform de-\ncently. However, there’s room for enhancement by imple-\nmenting diverse augmentation methods and decoding strate-\ngies. BART with chatGPT augmentation shows good results\nand generates readable text, but there is potential to improve\nmedical accuracy and informativeness. CHARD dataset has a\npredefined template infilling approach that is more restricted.\nIn our work, we employed a generalized language model, i.e.,\nChatGPT. The model has unmatched affluence, diversity, and\nversatility for creating a variety of texts, including medical\napplications. However, recently, specialized models such as\nMed-PaLM [50] have been published. These models are tai-\nlored for handling medical terminology, patient records, and\ntreatment protocols. In future research work, Med-PaLM will\nbe employed for text augmentation leveraging its specialized\ndomain and accuracy. Furthermore, our models are not ready\nfor real-world use. Users can utilize such a system to automat-\nically assess the medical accuracy of generated explanations,\nimproving the generation model through this feedback loop,\nresulting in a self-improving system. CHARTDAT has only\n52 diseases, but it can expand by including more conditions\nwithin its three dimensions. Furthermore, there’s potential\nfor expansion by adding more clinical dimensions, such as\nmedication dosage and duration for specific diseases.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nREFERENCES\n[1] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV . Stoyanov, and L. Zettlemoyer, ‘‘Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehen-\nsion,’’arXiv preprint arXiv:1910.13461 , 2019.\n[2] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou,\nW. Li, and P. J. Liu, ‘‘Exploring the limits of transfer learning with a unified\ntext-to-text transformer,’’ The Journal of Machine Learning Research ,\nvol. 21, no. 1, pp. 5485–5551, 2020.\n[3] A. Talmor, Y . Elazar, Y . Goldberg, and J. Berant, ‘‘olmpics-on what lan-\nguage model pre-training captures,’’ Transactions of the Association for\nComputational Linguistics, vol. 8, pp. 743–758, 2020.\n[4] A. Afzal, J. Vladika, D. Braun, and F. Matthes, ‘‘Challenges in domain-\nspecific abstractive summarization and how to overcome them,’’ in 15th\nInternational Conference on Agents and Artificial Intelligence, ICAART\n2023, pp. 682–689, SCITEPRESS, 2023.\n[5] S. Y . Feng, V . Gangal, J. Wei, S. Chandar, S. V osoughi, T. Mitamura,\nand E. Hovy, ‘‘A survey of data augmentation approaches for nlp,’’ arXiv\npreprint arXiv:2105.03075, 2021.\n[6] K. Maharana, S. Mondal, and B. Nemade, ‘‘A review: Data pre-processing\nand data augmentation techniques,’’ Global Transitions Proceedings ,\nvol. 3, no. 1, pp. 91–99, 2022.\n[7] I. Portoghese, M. Galletta, R. C. Coppola, G. Finco, and M. Campagna,\n‘‘Burnout and workload among health care workers: the moderating role of\njob control,’’ Safety and health at work , vol. 5, no. 3, pp. 152–157, 2014.\n[8] G. Dhole and N. Uke, ‘‘Nlp based retrieval of medical information for\ndiagnosis of human diseases,’’ Int J Renew Energy Technol , vol. 3, no. 10,\np. 243e8, 2014.\n[9] S. Sivarajkumar, M. Kelley, A. Samolyk-Mazzanti, S. Visweswaran, and\nY . Wang, ‘‘An empirical evaluation of prompting strategies for large lan-\nguage models in zero-shot clinical natural language processing,’’ 2023.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘Bert: Pre-training\nof deep bidirectional transformers for language understanding,’’ arXiv\npreprint arXiv:1810.04805, 2018.\n[11] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘Roberta: A robustly optimized bert\npretraining approach,’’ arXiv preprint arXiv:1907.11692 , 2019.\n[12] K. He, N. Hong, S. Lapalme-Remis, Y . Lan, M. Huang, C. Li, and L. Yao,\n‘‘Understanding the patient perspective of epilepsy treatment through text\nmining of online patient support groups,’’ Epilepsy & Behavior , vol. 94,\npp. 65–71, 2019.\n[13] Y . Li, X. Ma, X. Zhou, P. Cheng, K. He, and C. Li, ‘‘Knowledge en-\nhanced lstm for coreference resolution on biomedical texts,’’ Bioinformat-\nics, vol. 37, no. 17, pp. 2699–2705, 2021.\n[14] K. He, L. Yao, J. Zhang, Y . Li, and C. Li, ‘‘Construction of genealogical\nknowledge graphs from obituaries: Multitask neural network extraction\nsystem,’’ Journal of Medical Internet Research , vol. 23, no. 8, p. e25670,\n2021.\n[15] B. Mao, C. Jia, Y . Huang, K. He, J. Wu, T. Gong, and C. Li, ‘‘Uncertainty-\nguided mutual consistency training for semi-supervised biomedical rela-\ntion extraction,’’ in 2022 IEEE International Conference on Bioinformatics\nand Biomedicine (BIBM) , pp. 2318–2325, IEEE, 2022.\n[16] J. Wu, K. He, R. Mao, C. Li, and E. Cambria, ‘‘Megacare: Knowledge-\nguided multi-view hypergraph predictive framework for healthcare,’’ In-\nformation Fusion, vol. 100, p. 101939, 2023.\n[17] S. Biswas, ‘‘Chatgpt and the future of medical writing,’’ 2023.\n[18] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepaño,\nM. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo, et al. , ‘‘Perfor-\nmance of chatgpt on usmle: Potential for ai-assisted medical education us-\ning large language models,’’ PLoS digital health, vol. 2, no. 2, p. e0000198,\n2023.\n[19] S. B. Patel and K. Lam, ‘‘Chatgpt: the future of discharge summaries?,’’\nThe Lancet Digital Health , vol. 5, no. 3, pp. e107–e108, 2023.\n[20] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, ‘‘Biobert: a\npre-trained biomedical language representation model for biomedical text\nmining,’’Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020.\n[21] K. Huang, J. Altosaar, and R. Ranganath, ‘‘Clinicalbert: Modeling\nclinical notes and predicting hospital readmission,’’ arXiv preprint\narXiv:1904.05342, 2019.\n[22] V . Khetan, M. I. H. Rizvi, J. Huber, P. Bartusiak, B. Sacaleanu, and A. Fano,\n‘‘Mimicause: Representation and automatic extraction of causal relation\ntypes from clinical notes,’’ arXiv preprint arXiv:2110.07090 , 2021.\n[23] A. K. Yadav, M. Kumar, and A. Pathre, ‘‘Implemented text rank based\nautomatic text summarization using keyword extraction,’’ International\nResearch Journal of Innovations in Engineering and Technology , vol. 4,\nno. 11, p. 20, 2020.\n[24] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. , ‘‘Improving\nlanguage understanding by generative pre-training,’’ 2018.\n[25] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. ,\n‘‘Language models are unsupervised multitask learners,’’ OpenAI blog ,\nvol. 1, no. 8, p. 9, 2019.\n[26] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., ‘‘Language models\nare few-shot learners,’’Advances in neural information processing systems ,\nvol. 33, pp. 1877–1901, 2020.\n[27] M. Liebrenz, R. Schleifer, A. Buadze, D. Bhugra, and A. Smith, ‘‘Generat-\ning scholarly content with chatgpt: ethical challenges for medical publish-\ning,’’The lancet digital health , vol. 5, no. 3, pp. e105–e106, 2023.\n[28] E. D. Cubuk, B. Zoph, D. Mane, V . Vasudevan, and Q. V . Le, ‘‘Autoaug-\nment: Learning augmentation strategies from data,’’ in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition ,\npp. 113–123, 2019.\n[29] Z. Liu, H. Jin, T.-H. Wang, K. Zhou, and X. Hu, ‘‘Divaug: Plug-in auto-\nmated data augmentation with explicit diversity maximization,’’ in Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision ,\npp. 4762–4770, 2021.\n[30] J. Wei and K. Zou, ‘‘Eda: Easy data augmentation techniques for\nboosting performance on text classification tasks,’’ arXiv preprint\narXiv:1901.11196, 2019.\n[31] W. Y . Wang and D. Yang, ‘‘That’s so annoying!!!: A lexical and frame-\nsemantic embedding based data augmentation approach to automatic cate-\ngorization of annoying behaviors using# petpeeve tweets,’’ in Proceedings\nof the 2015 conference on empirical methods in natural language process-\ning, pp. 2557–2563, 2015.\n[32] S. Y . Feng, V . Gangal, D. Kang, T. Mitamura, and E. Hovy, ‘‘GenAug:\nData augmentation for finetuning text generators,’’ in Proceedings of\nDeep Learning Inside Out (DeeLIO): The First Workshop on Knowledge\nExtraction and Integration for Deep Learning Architectures (E. Agirre,\nM. Apidianaki, and I. Vulić, eds.), (Online), pp. 29–42, Association for\nComputational Linguistics, Nov. 2020.\n[33] J. Andreas, ‘‘Good-enough compositional data augmentation,’’ CoRR,\nvol. abs/1904.09545, 2019.\n[34] S. Kobayashi, ‘‘Contextual augmentation: Data augmentation by words\nwith paradigmatic relations,’’ in Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Papers)(M. Walker, H. Ji,\nand A. Stent, eds.), (New Orleans, Louisiana), pp. 452–457, Association for\nComputational Linguistics, June 2018.\n[35] A. Anaby-Tavor, B. Carmeli, E. Goldbraich, A. Kantor, G. Kour, S. Shlo-\nmov, N. Tepper, and N. Zwerdling, ‘‘Not enough data? deep learning to the\nrescue! arxiv 2019,’’ arXiv preprint arXiv:1911.03118 , 1911.\n[36] S. Zhang and M. Bansal, ‘‘Addressing semantic drift in question\ngeneration for semi-supervised question answering,’’ arXiv preprint\narXiv:1909.06356, 2019.\n[37] V . Kumar, A. Choudhary, and E. Cho, ‘‘Data augmentation using pre-\ntrained transformer models,’’ arXiv preprint arXiv:2003.02245 , 2020.\n[38] J. Ye, J. Gao, Q. Li, H. Xu, J. Feng, Z. Wu, T. Yu, and L. Kong, ‘‘Zero-\ngen: Efficient zero-shot learning via dataset generation,’’ arXiv preprint\narXiv:2202.07922, 2022.\n[39] C. Wang, X. Liu, Z. Chen, H. Hong, J. Tang, and D. Song, ‘‘Zero-shot in-\nformation extraction as a unified text-to-triple translation,’’ arXiv preprint\narXiv:2109.11171, 2021.\n[40] C. Peng, X. Yang, A. Chen, K. E. Smith, N. PourNejatian, A. B. Costa,\nC. Martin, M. G. Flores, Y . Zhang, T. Magoc, et al., ‘‘A study of generative\nlarge language model for medical research and healthcare,’’ NPJ Digital\nMedicine, vol. 6, no. 1, p. 210, 2023.\n[41] J. Chung, E. Kamar, and S. Amershi, ‘‘Increasing diversity while maintain-\ning accuracy: Text data generation with large language models and human\ninterventions,’’ in Proceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers) , Association\nfor Computational Linguistics, 2023.\n[42] S. Y . Feng, V . Khetan, B. Sacaleanu, A. Gershman, and E. Hovy, ‘‘Chard:\nClinical health-aware reasoning across dimensions for text generation\nmodels,’’arXiv preprint arXiv:2210.04191 , 2022.\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[43] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, ‘‘Is\nchatgpt a general-purpose natural language processing task solver?,’’ arXiv\npreprint arXiv:2302.06476, 2023.\n[44] C.-Y . Lin and E. Hovy, ‘‘Automatic evaluation of summaries using n-gram\nco-occurrence statistics,’’ in Proceedings of the 2003 human language\ntechnology conference of the North American chapter of the association\nfor computational linguistics , pp. 150–157, 2003.\n[45] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, ‘‘Cider: Consensus-\nbased image description evaluation,’’ in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , pp. 4566–4575, 2015.\n[46] S. Banerjee and A. Lavie, ‘‘Meteor: An automatic metric for mt evaluation\nwith improved correlation with human judgments,’’ in Proceedings of the\nacl workshop on intrinsic and extrinsic evaluation measures for machine\ntranslation and/or summarization , pp. 65–72, 2005.\n[47] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi, ‘‘Bertscore:\nEvaluating text generation with bert,’’ arXiv preprint arXiv:1904.09675 ,\n2019.\n[48] X. Cai, S. Liu, J. Han, L. Yang, Z. Liu, and T. Liu, ‘‘Chestxraybert:\nA pretrained language model for chest radiology report summarization,’’\nIEEE Transactions on Multimedia , 2021.\n[49] X. Cai, S. Liu, L. Yang, Y . Lu, J. Zhao, D. Shen, and T. Liu, ‘‘Covidsum:\nA linguistically enriched scibert-based summarization model for covid-19\nscientific papers,’’ Journal of Biomedical Informatics , vol. 127, p. 103999,\n2022.\n[50] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales,\nA. Tanwani, H. Cole-Lewis, S. Pfohl, et al. , ‘‘Large language models\nencode clinical knowledge,’’ Nature, vol. 620, no. 7972, pp. 172–180,\n2023.\nATIF LATIFreceived the B.Sc. degree in telecom-\nmunication engineering from the University of En-\ngineering and Technology, Peshawar, Pakistan, in\n2021. He is currently pursuing the M.S. degree\nin Artificial Intelligence with the Dongguk Uni-\nversity, Seoul, South Korea. His current research\ninterest include natural language processing in the\nclinical domain.\nJIHIE KIM received the B.S. degree in computer\nscience and statistics and the M.S. degree in com-\nputer science and statistics from Seoul National\nUniversity in 1988 and 1990, respectively, and\nthe Ph.D. degree in computer science from the\nUniversity of Southern California, in 1996. She\nis currently a Professor with the Department of\nArtificial Intelligence, Dongguk University, Seoul,\nSouth Korea. Her research interests include ma-\nchine learning, NLP, and knowledge-based reason-\ning.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3384496\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8167515993118286
    },
    {
      "name": "Transformer",
      "score": 0.6877049207687378
    },
    {
      "name": "Language model",
      "score": 0.5590051412582397
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5486692190170288
    },
    {
      "name": "Natural language processing",
      "score": 0.5277422070503235
    },
    {
      "name": "Machine learning",
      "score": 0.4469894766807556
    },
    {
      "name": "Training set",
      "score": 0.43886691331863403
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205490536",
      "name": "Dongguk University",
      "country": "KR"
    }
  ]
}