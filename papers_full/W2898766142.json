{
  "title": "Progress and Tradeoffs in Neural Language Models",
  "url": "https://openalex.org/W2898766142",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4289619967",
      "name": "Tang, Raphael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2672090663",
      "name": "Lin, Jimmy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1768994003",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2803431233",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2795285343",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2250861254",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W2739997338",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W1494910745",
    "https://openalex.org/W2962964385"
  ],
  "abstract": "In recent years, we have witnessed a dramatic shift towards techniques driven by neural networks for a variety of NLP tasks. Undoubtedly, neural language models (NLMs) have reduced perplexity by impressive amounts. This progress, however, comes at a substantial cost in performance, in terms of inference latency and energy consumption, which is particularly of concern in deployments on mobile devices. This paper, which examines the quality-performance tradeoff of various language modeling techniques, represents to our knowledge the first to make this observation. We compare state-of-the-art NLMs with \"classic\" Kneser-Ney (KN) LMs in terms of energy usage, latency, perplexity, and prediction accuracy using two standard benchmarks. On a Raspberry Pi, we find that orders of increase in latency and energy usage correspond to less change in perplexity, while the difference is much less pronounced on a desktop.",
  "full_text": "Progress and Tradeoffs in Neural Language Models\nRaphael Tangand Jimmy Lin\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n{r33tang, jimmylin}@uwaterloo.ca\nAbstract\nIn recent years, we have witnessed a dramatic\nshift towards techniques driven by neural net-\nworks for a variety of NLP tasks. Undoubt-\nedly, neural language models (NLMs) have\nreduced perplexity by impressive amounts.\nThis progress, however, comes at a substan-\ntial cost in performance, in terms of inference\nlatency and energy consumption, which is par-\nticularly of concern in deployments on mo-\nbile devices. This paper, which examines the\nquality–performance tradeoff of various lan-\nguage modeling techniques, represents to our\nknowledge the ﬁrst to make this observation.\nWe compare state-of-the-art NLMs with “clas-\nsic” Kneser-Ney (KN) LMs in terms of energy\nusage, latency, perplexity, and prediction ac-\ncuracy using two standard benchmarks. On a\nRaspberry Pi, we ﬁnd that orders of increase\nin latency and energy usage correspond to less\nchange in perplexity, while the difference is\nmuch less pronounced on a desktop.\n1 Introduction\nDeep learning has unquestionably advanced the\nstate of the art in many natural language pro-\ncessing tasks, from syntactic dependency pars-\ning (Chen and Manning, 2014) to named-entity\nrecognition (Lample et al., 2016) to machine trans-\nlation (Luong et al., 2015). The same certainly\napplies to language modeling, where recent ad-\nvances in neural language models (NLMs) have\nled to dramatically better approaches as measured\nusing standard metrics such as perplexity (Melis\net al., 2018; Merity et al., 2018b).\nSpeciﬁcally focused on language modeling, this\npaper examines an issue that to our knowledge has\nnot been explored: advances in neural language\nmodels have come at a signiﬁcant cost in terms\nof increased computational complexity. Comput-\ning the probability of a token sequence using non-\nneural techniques requires a number of phrase\nlookups and perhaps a few arithmetic operations,\nwhereas model inference with NLMs require large\nmatrix multiplications consuming perhaps mil-\nlions of ﬂoating point operations (FLOPs). These\nperformance tradeoffs are worth discussing.\nIn truth, language models exist in a quality–\nperformance tradeoff space. As model quality in-\ncreases (e.g., lower perplexity), performance as\nmeasured in terms of energy consumption, query\nlatency, etc. tends to decrease. For applica-\ntions primarily running in the cloud—say, ma-\nchine translation—practitioners often solely opti-\nmize for the lowest perplexity. This is because\nsuch applications are embarrassingly parallel and\nhence trivial to scale in a data center environment.\nThere are, however, applications of NLMs that\nrequire less one-sided optimizations. On mobile\ndevices such as smartphones and tablets, for exam-\nple, NLMs may be integrated into software key-\nboards for next-word prediction, allowing much\nfaster text entry. Popular Android apps that enthu-\nsiastically tout this technology include SwiftKey\nand Swype. The greater computational costs of\nNLMs lead to higher energy usage in model infer-\nence, translating into shorter battery life.\nIn this paper, we examine the quality–\nperformance tradeoff in the shift from non-neural\nto neural language models. In particular, we com-\npare Kneser–Ney smoothing, widely accepted as\nthe state of the art prior to NLMs, to the best\nNLMs today. The decrease in perplexity on stan-\ndard datasets has been well documented (Melis\net al., 2018), but to our knowledge no one has ex-\namined the performances tradeoffs. With deploy-\nment on a mobile device in mind, we evaluate en-\nergy usage and inference latency on a Raspberry\nPi (which shares the same ARM architecture as\nnearly all smartphones today). We ﬁnd that a 2.5×\nreduction in perplexity on PTB comes at a stagger-\ning cost in terms of performance: inference with\narXiv:1811.00942v1  [cs.CL]  2 Nov 2018\nNLMs takes 49 ×longer and requires 32 ×more\nenergy. Furthermore, we ﬁnd that impressive re-\nductions in perplexity translate into at best mod-\nest improvements in next-word prediction, which\nis arguable a better metric for evaluating software\nkeyboards on a smartphone. The contribution of\nthis paper is the ﬁrst known elucidation of this\nquality–performance tradeoff. Note that we re-\nfrain from prescriptive recommendations: whether\nor not a tradeoff is worthwhile depends on the ap-\nplication. Nevertheless, NLP engineers should ar-\nguably keep these tradeoffs in mind when select-\ning a particular operating point.\n2 Background and Related Work\nMelis et al. (2018) evaluate recent neural language\nmodels; however, their focus is not on the compu-\ntational footprint of each model, but rather the per-\nplexity. To further reduce perplexity, many neural\nlanguage model extensions exist, such as continu-\nous cache pointer (Grave et al., 2017) and mixture\nof softmaxes (Yang et al., 2018). Since our focus\nis on comparing “core” neural and non-neural ap-\nproaches, we disregard these extra optimizations\ntechniques in all of our models.\nOther work focus on designing lightweight\nmodels for resource-efﬁcient inference on mo-\nbile devices. Liu et al. (2018) explore\nLSTMs (Hochreiter and Schmidhuber, 1997) with\nbinary weights for language modeling; Botha et al.\n(2017) examine shallow feedforward neural net-\nworks for natural language processing.\nA WD-LSTM.Merity et al. (2018b) show that a\nsimple three-layer LSTM, with proper regulariza-\ntion and optimization techniques, can achieve state\nof the art on various language modeling datasets,\nsurpassing more complex models. Speciﬁcally,\nMerity et al. (2018b) apply randomized backprop-\nagation through time, variational dropout, activa-\ntion regularization, embedding dropout, and tem-\nporal activation regularization. A novel sched-\nuler for optimization, non-monotonically trig-\ngered ASGD (NT-ASGD) is also introduced. Mer-\nity et al. (2018b) name their three-layer LSTM\nmodel trained with such tricks, “AWD-LSTM.”\nQuasi-Recurrent Neural Networks. Quasi-\nrecurrent neural networks (QRNNs; Bradbury\net al., 2017) achieve current state of the art\nin word-level language modeling (Merity et al.,\n2018a). A quasi-recurrent layer comprises two\nseparate parts: a convolution layer with three\nweights, and a recurrent pooling layer. Given an\ninput X ∈Rk×n, the convolution layer is\nZ = tanh(Wz ·X)\nF = σ(Wf ·X)\nO = σ(Wo ·X)\nwhere σdenotes the sigmoid function, ·represents\nmasked convolution across time, and W{z,f,o} ∈\nRm×k×r are convolution weights with k input\nchannels, m output channels, and a window size\nof r. In the recurrent pooling layer, the convolu-\ntion outputs are combined sequentially:\nct = ft ⊙ct−1 + (1−ft) ⊙zt\nht = ot ⊙ct\nMultiple QRNN layers can be stacked for deeper\nhierarchical representation, with the output h1:t\nbeing fed as the input into the subsequent layer: In\nlanguage modeling, a four-layer QRNN is a stan-\ndard architecture (Merity et al., 2018a).\nPerplexity–Recall Scale. Word-level perplexity\ndoes not have a strictly monotonic relationship\nwith recall-at-k, the fraction of top k predictions\nthat contain the correct word. A given R@ k im-\nposes a weak minimum perplexity constraint—\nthere are many free parameters that allow for large\nvariability in the perplexity given a certain R@ k.\nConsider the corpus, “choo choo train,” with an\nassociated unigram model P(“choo”) = 0 .1,\nP(“train”) = 0.9, resulting in an R@1 of 1/3\nand perplexity of 4.8. Clearly, R@1 = 1/3 for\nall P(“choo”) ≤0.5; thus, perplexity can drop as\nlow as 2 without affecting recall.\n3 Experimental Setup\nWe conducted our experiments on Penn Tree-\nbank (PTB; Marcus et al., 1993) and WikiText-\n103 (WT103; Merity et al., 2017). Preprocessed\nby Mikolov et al. (2010), PTB contains 887K to-\nkens for training, 70K for validation, and 78K for\ntest, with a vocabulary size of 10,000. On the other\nhand, WT103 comprises 103 million tokens for\ntraining, 217K for validation, and 245K for test,\nspanning a vocabulary of 267K unique tokens.\nFor the neural language model, we used a\nfour-layer QRNN (Bradbury et al., 2017), which\nachieves state-of-the-art results on a variety of\ndatasets, such as WT103 (Merity et al., 2018a)\nand PTB. To compare against more common\nLSTM architectures, we also evaluated AWD-\nLSTM (Merity et al., 2018b) on PTB. For the\nnon-neural approach, we used a standard ﬁve-\ngram model with modiﬁed Kneser-Ney smooth-\ning (Chen and Goodman, 1996), as explored in\nMikolov and Zweig (2012) on PTB. We de-\nnote the QRNN models for PTB and WT103 as\nptb-qrnn and wt103-qrnn, respectively.\nFor each model, we examined word-level per-\nplexity, R@3 in next-word prediction, latency\n(ms/q), and energy usage (mJ/q). To explore the\nperplexity–recall relationship, we collected indi-\nvidual perplexity and recall statistics for each sen-\ntence in the test set.\n3.1 Hyperparameters and Training\nThe QRNN models followed the exact training\nprocedure and architecture delineated in the of-\nﬁcial codebase from Merity et al. (2018a). For\nptb-qrnn, we trained the model for 550 epochs\nusing NT-ASGD (Merity et al., 2018b), then ﬁne-\ntuned for 300 epochs using ASGD (Polyak and\nJuditsky, 1992), all with a learning rate of 30\nthroughout. For wt103-qrnn, we followed Mer-\nity et al. (2018a) and trained the QRNN for 14\nepochs, using the Adam optimizer with a learn-\ning rate of 10−3. We also applied regularization\ntechniques from Merity et al. (2018b); all the spe-\nciﬁc hyperparameters are the same as those in\nthe repository. Our model architecture consists\nof 400-dimensional tied embedding weights (Inan\net al., 2017) and four QRNN layers, with 1550 hid-\nden units per layer on PTB and 2500 per layer on\nWT103. Both QRNN models have window sizes\nof r= 2for the ﬁrst layer and r= 1for the rest.\nFor the KN-5 model, we trained an off-the-\nshelf ﬁve-gram model using the popular SRILM\ntoolkit (Stolcke, 2002). We did not specify any\nspecial hyperparameters.\n3.2 Infrastructure\nWe trained the QRNNs with PyTorch (0.4.0; com-\nmit 1807bac) on a Titan V GPU. To evaluate the\nmodels under a resource-constrained environment,\nwe deployed them on a Raspberry Pi 3 (Model\nB) running Raspbian Stretch (4.9.41-v7+). The\nRaspberry Pi (RPi) is not only a standard platform,\nbut also a close surrogate to mobile phones, us-\ning the same Cortex-A7 in many phones. We then\ntransferred the trained models to the RPi, using\nthe same frameworks for evaluation. We plugged\nthe RPi into a Watts Up Pro meter, a power me-\nModel Val. Test\nPenn Treebank\nSkip LSTM (Melis et al., 2018) 60.9 58.3\nAWD-LSTM (Merity et al., 2018b) 60.0 57.3\nQRNN 59.1 56.8\nWikiText-103\nRae-LSTM (Rae et al., 2018) 36.0 36.4\nQRNN 31.9 32.8\nTable 1: Comparison of neural language models on\nPenn Treebank and WikiText-103.\nter that can be read programatically over USB at a\nfrequency of 1 Hz. For the QRNNs, we used the\nﬁrst 350 words of the test set, and averaged the\nms/query and mJ/query. For KN-5, we used the\nentire test set for evaluation, since the latency was\nmuch lower. To adjust for the base power load, we\nsubtracted idle power draw from energy usage.\nFor a different perspective, we further evaluated\nall the models under a desktop environment, using\nan i7-4790k CPU and Titan V GPU. Because the\nbase power load for powering a desktop is much\nhigher than running neural language models, we\ncollected only latency statistics. We used the en-\ntire test set, since the QRNN runs quickly.\nIn addition to energy and latency, another con-\nsideration for the NLP developer selecting an op-\nerating point is the cost of underlying hardware.\nFor our setup, the RPi costs $35 USD, the CPU\ncosts $350 USD, and the GPU costs $3000 USD.\n4 Results and Discussion\nTo demonstrate the effectiveness of the QRNN\nmodels, we present the results of past and cur-\nrent state-of-the-art neural language models in Ta-\nble 1; we report the Skip- and AWD-LSTM re-\nsults as seen in the original papers, while we re-\nport our QRNN results. Skip LSTM denotes the\nfour-layer Skip LSTM in Melis et al. (2018). Rae\net al. (2018) focus on Hebbian softmax, a model\nextension technique—Rae-LSTM refers to their\nbase LSTM model without any extensions. In our\nresults, KN-5 refers to the traditional ﬁve-gram\nmodel with modiﬁed Kneser-Ney smoothing, and\nAWD is shorthand for AWD-LSTM.\nPerplexity–recall scale. In Figure 1, using KN-\n5 as the model, we plot the log perplexity (cross\nentropy) and R@3 error (1 −R@3) for every sen-\ntence in PTB and WT103. The horizontal clusters\narise from multiple perplexity points representing\n1 2 3 4 5 6 7\nLog Perplexity (Cross Entropy)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.01 - R@3 (Error)\nLog Perplexity Recall Scale on PTB\nr2 = 0.72\n1 2 3 4 5 6 7\nLog Perplexity (Cross Entropy)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.01 - R@3 (Error)\nLog Perplexity Recall Scale on WT-103\n r2 = 0.88\nFigure 1: Log perplexity–recall error with KN-5.\n0 1 2 3 4 5 6 7\nLog Perplexity (Cross Entropy)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.01 - R@3 (Error)\nLog Perplexity Recall Scale on PTB\nr2 = 0.77\n0 1 2 3 4 5 6 7\nLog Perplexity (Cross Entropy)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.01 - R@3 (Error)\nLog Perplexity Recall Scale on WT-103\n r2 = 0.86\nFigure 2: Log perplexity–recall error with QRNN.\n# Method\nModel Quality RPi CPU | GPU\nVal. Test R@3 ms/q mJ/q ms/q ms/q\nPenn Treebank\n1 KN-5 148.4 141.5 36.7% 7 6 0.8 –\n2 AWD 59.2 56.8 44.9% 223 295 7.9 1.7\n3 QRNN 59.1 56.8 44.7% 224 296 7.5 1.6\nWikiText-103\n4 KN-5 145.2 152.7 39.8% 264 229 37 –\n5 QRNN 31.9 32.8 53.5% 1240 1480 59 3.5\nTable 2: Language modeling results on performance\nand model quality.\nthe same R@3 value, as explained in Section 2.\nWe also observe that the perplexity–recall scale\nis non-linear—instead, log perplexity appears to\nhave a moderate linear relationship with R@3 er-\nror on PTB ( r = 0.85), and an even stronger re-\nlationship on WT103 ( r = 0.94). This is par-\ntially explained by WT103 having much longer\nsentences, and thus less noisy statistics.\nFrom Figure 2, we ﬁnd that QRNN models yield\nstrongly linear log perplexity–recall plots as well,\nwhere r = 0 .88 and r = 0 .93 for PTB and\nWT103, respectively. Note that, due to the im-\nproved model quality over KN-5, the point clouds\nare shifted downward compared to Figure 1. We\nconclude that log perplexity, or cross entropy, pro-\nvides a more human-understandable indicator of\nR@3 than perplexity does. Overall, these ﬁndings\nagree with those from Chen et al. (1998), which\nexplores the log perplexity–word error rate scale\nin language modeling for speech recognition.\nQuality–performance tradeoff.In Table 2, from\nleft to right, we report perplexity results on the val-\nidation and test sets, R@3 on test, and ﬁnally per-\nquery latency and energy usage. On the RPi, KN-5\nis both fast and power-efﬁcient to run, using only\nabout 7 ms/query and 6 mJ/query for PTB (Table\n2, row 1), and 264 ms/q and 229 mJ/q on WT103\n(row 5). Taking 220 ms/query and consuming 300\nmJ/query, AWD-LSTM and ptb-qrnn are still\nviable for mobile phones: The modern smartphone\nholds upwards of 10,000 joules (Carroll et al.,\n2010), and the latency is within usability stan-\ndards (Miller, 1968). Nevertheless, the models are\nstill 49×slower and 32×more power-hungry than\nKN-5. The wt103-qrnn model is completely\nunusable on phones, taking over 1.2 seconds per\nnext-word prediction. Neural models achieve per-\nplexity drops of 60–80% and R@3 increases of\n22–34%, but these improvements come at a much\nhigher cost in latency and energy usage.\nIn Table 2 (last two columns), the desktop yields\nvery different results: the neural models on PTB\n(rows 2–3) are 9 ×slower than KN-5, but the\nabsolute latency is only 8 ms/q, which is still\nmuch faster than what humans perceive as instan-\ntaneous (Miller, 1968). If a high-end commodity\nGPU is available, then the models are only twice\nas slow as KN-5 is. From row 5, even better re-\nsults are noted with wt103-qrnn: On the CPU,\nthe QRNN is only 60% slower than KN-5 is, while\nthe model is faster by 11 ×on a GPU. These re-\nsults suggest that, if only latency is considered un-\nder a commodity desktop environment, the QRNN\nmodel is humanly indistinguishable from the KN-\n5 model, even without using GPU acceleration.\n5 Conclusion\nIn the present work, we describe and examine the\ntradeoff space between quality and performance\nfor the task of language modeling. Speciﬁcally,\nwe explore the quality–performance tradeoffs be-\ntween KN-5, a non-neural approach, and AWD-\nLSTM and QRNN, two neural language mod-\nels. We ﬁnd that with decreased perplexity comes\nvastly increased computational requirements: In\none of the NLMs, a perplexity reduction by 2.5 ×\nresults in a 49 ×rise in latency and 32 ×increase\nin energy usage, when compared to KN-5.\nReferences\nJan A Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex\nSalcianu, David Weiss, Ryan McDonald, and Slav\nPetrov. 2017. Natural language processing with\nsmall feed-forward networks. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2879–2885.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-recurrent neural net-\nworks. In International Conference on Learning\nRepresentations.\nAaron Carroll, Gernot Heiser, et al. 2010. An analysis\nof power consumption in a smartphone. In USENIX\nAnnual Technical Conference, volume 14, pages 21–\n21.\nDanqi Chen and Christopher Manning. 2014. A fast\nand accurate dependency parser using neural net-\nworks. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 740–750.\nStanley F Chen, Douglas Beeferman, and Ronald\nRosenfeld. 1998. Evaluation metrics for language\nmodels. In DARPA Broadcast News Transcription\nand Understanding Workshop, pages 275–280.\nStanley F Chen and Joshua Goodman. 1996. An em-\npirical study of smoothing techniques for language\nmodeling. In Proceedings of the 34th annual meet-\ning on Association for Computational Linguistics,\npages 310–318.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a\ncontinuous cache. In International Conference on\nLearning Representations.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In Interna-\ntional Conference on Learning Representations.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270.\nXuan Liu, Di Cao, and Kai Yu. 2018. Binarized LSTM\nlanguage model. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), vol-\nume 1, pages 2113–2121.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1412–1421.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional linguistics, 19(2):313–330.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In International Conference on Learning\nRepresentations.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018a. An analysis of neural language mod-\neling at multiple scales. arXiv:1803.08240.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018b. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In International Conference on Learning\nRepresentations.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nSLT, 12:234–239.\nRobert B. Miller. 1968. Response time in man-\ncomputer conversational transactions. In Proceed-\nings of the December 9-11, 1968, Fall Joint Com-\nputer Conference, Part I, AFIPS ’68 (Fall, part I),\npages 267–277.\nB. T. Polyak and A. B. Juditsky. 1992. Acceleration\nof stochastic approximation by averaging. SIAM J.\nControl Optim., 30(4):838–855.\nJack W Rae, Chris Dyer, Peter Dayan, and Timothy P\nLillicrap. 2018. Fast parametric learning with acti-\nvation memorization. arXiv:1803.10049.\nAndreas Stolcke. 2002. SRILM — an extensible lan-\nguage modeling toolkit. In Seventh International\nConference on Spoken Language Processing.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax\nbottleneck: A high-rank RNN language model. In\nInternational Conference on Learning Representa-\ntions.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9925482273101807
    },
    {
      "name": "Computer science",
      "score": 0.7687468528747559
    },
    {
      "name": "Latency (audio)",
      "score": 0.7586779594421387
    },
    {
      "name": "Inference",
      "score": 0.724343478679657
    },
    {
      "name": "Deep neural networks",
      "score": 0.6101540327072144
    },
    {
      "name": "Language model",
      "score": 0.5685814619064331
    },
    {
      "name": "Artificial neural network",
      "score": 0.5085781812667847
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4996836185455322
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4038967490196228
    },
    {
      "name": "Machine learning",
      "score": 0.3377212882041931
    },
    {
      "name": "Telecommunications",
      "score": 0.16571000218391418
    }
  ],
  "institutions": [],
  "cited_by": 5
}