{
  "title": "Navigating the potential and pitfalls of large language models in patient-centered medication guidance and self-decision support",
  "url": "https://openalex.org/W4406775160",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2801450039",
      "name": "Serhat Aydın",
      "affiliations": [
        "Koç University"
      ]
    },
    {
      "id": "https://openalex.org/A3165109671",
      "name": "Mert Karabacak",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A5114451560",
      "name": "Victoria Vlachos",
      "affiliations": [
        "New York State University College of Human Ecology",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2027235618",
      "name": "Konstantinos Margetis",
      "affiliations": [
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2801450039",
      "name": "Serhat Aydın",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3165109671",
      "name": "Mert Karabacak",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114451560",
      "name": "Victoria Vlachos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2027235618",
      "name": "Konstantinos Margetis",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4380483612",
    "https://openalex.org/W4391813134",
    "https://openalex.org/W4403880728",
    "https://openalex.org/W4386894187",
    "https://openalex.org/W4327715333",
    "https://openalex.org/W4393994738",
    "https://openalex.org/W4394920988",
    "https://openalex.org/W4389274313",
    "https://openalex.org/W4394881653",
    "https://openalex.org/W4394620742",
    "https://openalex.org/W4385236544",
    "https://openalex.org/W4400026407"
  ],
  "abstract": "Large Language Models (LLMs) are transforming patient education in medication management by providing accessible information to support healthcare decision-making. Building on our recent scoping review of LLMs in patient education, this perspective examines their specific role in medication guidance. These artificial intelligence (AI)-driven tools can generate comprehensive responses about drug interactions, side effects, and emergency care protocols, potentially enhancing patient autonomy in medication decisions. However, significant challenges exist, including the risk of misinformation and the complexity of providing accurate drug information without access to individual patient data. Safety concerns are particularly acute when patients rely solely on AI-generated advice for self-medication decisions. This perspective analyzes current capabilities, examines critical limitations, and raises questions regarding the possible integration of LLMs in medication guidance. We emphasize the need for regulatory oversight to ensure these tools serve as supplements to, rather than replacements for, professional healthcare guidance.",
  "full_text": "fmed-12-1527864 January 20, 2025 Time: 17:16 # 1\nTYPE Perspective\nPUBLISHED 23 January 2025\nDOI 10.3389/fmed.2025.1527864\nOPEN ACCESS\nEDITED BY\nAriel Soares Teles,\nScience and Technology of Maranhão, Brazil\nREVIEWED BY\nRadhika Devraj,\nSouthern Illinois University Edwardsville,\nUnited States\n*CORRESPONDENCE\nKonstantinos Margetis\nKonstantinos.Margetis@mountsinai.org\nRECEIVED 13 November 2024\nACCEPTED 09 January 2025\nPUBLISHED 23 January 2025\nCITATION\nAydin S, Karabacak M, Vlachos V and\nMargetis K (2025) Navigating the potential\nand pitfalls of large language models\nin patient-centered medication guidance\nand self-decision support.\nFront. Med.12:1527864.\ndoi: 10.3389/fmed.2025.1527864\nCOPYRIGHT\n© 2025 Aydin, Karabacak, Vlachos and\nMargetis. This is an open-access article\ndistributed under the terms of the Creative\nCommons Attribution License (CC BY). The\nuse, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nNavigating the potential and\npitfalls of large language models\nin patient-centered medication\nguidance and self-decision\nsupport\nSerhat Aydin1, Mert Karabacak2, Victoria Vlachos3 and\nKonstantinos Margetis2*\n1School of Medicine, Koç University, Istanbul, Türkiye,2Department of Neurosurgery, Mount Sinai\nHealth System, New York, NY, United States, 3College of Human Ecology, Cornell University, Ithaca,\nNY, United States\nLarge Language Models (LLMs) are transforming patient education in medication\nmanagement by providing accessible information to support healthcare\ndecision-making. Building on our recent scoping review of LLMs in patient\neducation, this perspective examines their speciﬁc role in medication guidance.\nThese artiﬁcial intelligence (AI)-driven tools can generate comprehensive\nresponses about drug interactions, side effects, and emergency care protocols,\npotentially enhancing patient autonomy in medication decisions. However,\nsigniﬁcant challenges exist, including the risk of misinformation and the\ncomplexity of providing accurate drug information without access to individual\npatient data. Safety concerns are particularly acute when patients rely solely\non AI-generated advice for self-medication decisions. This perspective analyzes\ncurrent capabilities, examines critical limitations, and raises questions regarding\nthe possible integration of LLMs in medication guidance. We emphasize the\nneed for regulatory oversight to ensure these tools serve as supplements to,\nrather than replacements for, professional healthcare guidance.\nKEYWORDS\nLarge Language Models, ChatGPT, patient education, self-medication, artiﬁcial\nintelligence, machine learning, deep learning\nKEY ASPECTS\n• LLMs are transforming patient education by oﬀering easily accessible and user-friendly\nguidance on medication use, improving patient understanding and self-management.\n• These models may empower patients in remote or underserved areas by providing\nimmediate, reliable information on health conditions and self-care, especially where\nhealthcare access is limited.\n• However, challenges remain in ensuring accuracy, particularly in complex cases due to\nthe current limitations in accessing real-time data and personalized patient information.\n• There are ethical concerns regarding the use of LLMs for self-medication guidance\nwithout healthcare oversight, which may lead to unintended health risks.\n• To improve safety, future eﬀorts should focus on integrating real-time medical\ndatabases and establishing clear regulations for the use of LLMs in healthcare contexts.\nFrontiers in Medicine 01 frontiersin.org\nfmed-12-1527864 January 20, 2025 Time: 17:16 # 2\nAydin et al. 10.3389/fmed.2025.1527864\n1 Introduction\nThe Large Language Models (LLMs) represent a signiﬁcant\nadvancement in patient education, particularly in personalized\nhealth and medication counseling. Leading examples such as\nOpenAI’s ChatGPT (1), and Google’s Gemini (2) can process\nextensive datasets and engage in conversational interactions.\nThese artiﬁcial intelligence (AI) applications are increasingly being\nexplored in healthcare to provide drug information, help patients\nnavigate complex medication regimens, and guide initial responses\nto medical situations. By generating information of variable\nreliability, the extent to which LLMs can eﬀectively inﬂuence\npatient autonomy in self-medication decisions and healthcare\nchoices remains an open question.\nThe appeal of LLMs in healthcare stems from their accessibility\nand ease of use. Patients can readily access information about\nmedication dosages, interactions, side eﬀects, and alternatives\nwithout waiting to consult a healthcare provider. These models\ncan enhance health literacy by translating medical jargon into\nplain language, helping patients make informed decisions about\nover-the-counter medications and some prescribed treatments.\nFor example, studies show that LLMs can provide basic guidance\nfor immediate-response situations, such as initial management of\nsnakebites or other common conditions requiring urgent attention\n(3).\nHowever, signiﬁcant challenges exist in safely integrating LLMs\ninto patient self-care decisions. A primary concern is the reliability\nof LLM-generated information, particularly regarding complex\ndrug interactions or rare conditions. Cases of AI systems providing\nincorrect or misleading information have been documented,\nnotably in sensitive areas with signiﬁcant health and ethical\nimplications, such as self-managed medication abortion (4).\nBuilding upon our recent scoping review that identiﬁed six\nmajor themes in LLM applications for patient education (5), this\narticle examines one critical theme: the role of LLMs in patient-\ncentered medication guidance and self-decision support. We assess\nboth the potential of LLMs to enhance autonomous medication use\nand the risks associated with their misuse or misunderstanding.\nThis perspective article reviews recent advances, identiﬁes key\nchallenges, and proposes future directions for LLM implementation\nthat balance patient autonomy with healthcare safety and ethical\nstandards. By examining this speciﬁc theme in detail, we aim to\ncontribute targeted insights into the responsible integration of\nLLM technology in medication guidance while addressing critical\nquestions about patient safety and ethical implementation.\n2 Current advances in LLMs for\ncustomized medication use and\nself-decision\n2.1 LLMs as informational aids for drug\ninteractions and side effects\nLLMs show promise as informational resources for medication\nguidance, particularly in explaining drug interactions, potential\nside eﬀects, and usage instructions. These models can translate\ncomplex pharmacological information into accessible language for\npatients with limited medical knowledge. This capability helps\npatients better understand their medication regimens and may\nreduce drug-drug interactions caused by misunderstandings (6, 7).\nA recent study by Iqbal et al. examined ChatGPT’s reliability\nas a secondary opinion source for dermatological treatments\n(8). While dermatologists approved 98.87% of the model’s\nmedication suggestions, they identiﬁed limitations such as\nincorrect Anatomical Therapeutic Chemical codes and errors\nin drug route speciﬁcations. These ﬁndings suggest that while\nChatGPT shows promise for general treatment guidance, it requires\nfurther reﬁnement for precise clinical applications.\nLLMs also demonstrate potential in helping patients\nmanage complex medication regimens, particularly in cases\nof polypharmacy where drug-drug interactions pose signiﬁcant\nrisks. Research shows that these models can eﬀectively identify\nand explain risks associated with speciﬁc drug combinations,\nincluding interactions between over-the-counter medications and\ntreatments for chronic conditions (9). This capability could help\nprevent medication errors and resulting hospitalizations from\nadverse drug reactions.\nRecent research also explores LLMs’ potential in helping\nhealthcare professionals screen for drug interactions.\nA comparative analysis of ChatGPT, Google Bard, and Bing\nAI found that while these tools do not yet match the accuracy of\nspecialized clinical software, they can eﬀectively identify relevant\ndrug interactions in real-time. Among the tested models, Bing AI\ndemonstrated the highest accuracy and speciﬁcity, while ChatGPT-\n4 showed improvements over its predecessor (6). These ﬁndings\nhighlight the need for further development of LLM capabilities,\nindicating that while they show potential, they are not yet ready for\nreliable use in clinical settings but may be in the future.\n2.2 Facilitating self-decision in\nself-administered treatments\nLLMs show potential in guiding patients through self-\nadministered treatments, particularly in situations requiring\nimmediate action. For example, studies have evaluated ChatGPT’s\nability to provide ﬁrst-aid advice for venomous snakebites while\nemphasizing the need for urgent medical care (3). This capability\ncould be particularly valuable in remote areas with limited\nhealthcare access, oﬀering patients guidance to take appropriate\nimmediate actions while awaiting professional care. Infrastructural\nchallenges, such as unreliable internet connectivity, may hinder\nits implementation in such settings, though its potential remains\npromising. However, researchers found that while ChatGPT-\n3.5 provided reliable general guidance, it should not replace\nprofessional medical consultation, especially in critical situations.\nThe study emphasized the need for continued improvements to\nenhance AI’s reliability in high-stakes medical scenarios.\nRoosan et al. evaluated ChatGPT’s eﬀectiveness in Medication\nTherapy Management, focusing on drug interaction identiﬁcation\nand therapeutic adjustments (10). While ChatGPT-4 demonstrated\nhigh accuracy with simple and moderately complex cases, it showed\nlimitations when handling complex scenarios requiring patient-\nspeciﬁc considerations. The model proved capable of identifying\nFrontiers in Medicine 02 frontiersin.org\nfmed-12-1527864 January 20, 2025 Time: 17:16 # 3\nAydin et al. 10.3389/fmed.2025.1527864\ncommon drug-drug interactions but struggled with personalized\ndosage adjustments, highlighting the continued need for human\noversight in clinical decision-making.\n3 Challenges and limitations in LLMs\nfor medication guidance and\nself-decision\n3.1 Inaccuracy and misleading\ninformation\nA critical challenge in using LLMs for medication guidance is\ntheir potential to generate inaccurate or misleading information.\nWhile these models can process large datasets, they lack access\nto real-time, continuously updated medical databases, potentially\nleading to outdated or incorrect advice. For example, studies\nhave found that ChatGPT-3.5 provided inaccurate information\nabout self-managed medication abortion, exaggerating risks despite\nevidence supporting its safety when properly administered (4).\nSuch misinformation can increase patient anxiety, perpetuate\nstigma, and discourage evidence-based healthcare decisions.\nResearch by Sheikh et al. compared ChatGPT-3.5 and\nChatGPT-4’s ability to assess the safety of non-prescription\nmedications and supplements for patients with kidney disease\n(11). While ChatGPT-4 showed improvement over its predecessor\n(81.4% vs 64.5% concordance with Micromedex), neither\nmatched the reliability of established drug information resources.\nBoth models particularly struggled with supplement safety\nassessments, often defaulting to \"unknown toxicity\" classiﬁcations\ndue to limited data.\nRao et al. (9) assessed ChatGPT-3.5’s role in managing\npolypharmacy in geriatric patients, ﬁnding its deprescribing\nrecommendations aligned with guidelines for patients without\ncardiovascular disease but lacked accuracy when factoring in\nfunctional impairments and cardiovascular history. Notably, it\noften recommended deprescribing pain medications without\nconsidering older adults’ pain management needs. Similarly, in\ncases of renal dysfunction, ChatGPT achieved only 16.7% accuracy\nin dose adjustments incorporating patient-speciﬁc variables such as\nrenal markers and comorbidities (12). These ﬁndings highlight the\nlimitations of LLMs in complex scenarios requiring personalized\nclinical expertise, emphasizing their role as supplementary tools\nrather than replacements for professional judgment. This low\naccuracy poses signiﬁcant risks in clinical settings where precise\ndosing is crucial, demonstrating that while LLMs may support\npreliminary decision-making, they cannot reliably replace clinical\nexpertise in complex medical situations.\n3.2 Ethical and safety concerns in\nself-decision support\nThe use of LLMs for self-medication guidance raises signiﬁcant\nethical concerns, particularly when patients use these tools without\nhealthcare professional oversight. A primary risk is that LLMs may\nprovide seemingly authoritative advice that lacks clinical nuance,\npotentially encouraging unsafe medical decisions. This risk is\nheightened in regions with limited healthcare access, where patients\nmight rely on AI as their primary medical information source.\nHsu et al. examined ChatGPT’s ability to handle medication\nconsultations and drug-herb interaction questions (13). While the\nmodel eﬀectively addressed basic public inquiries, it performed\npoorly on complex questions from healthcare providers. The study\nrevealed particular limitations in analyzing interactions between\ntraditional Chinese and Western medicines, often providing vague\nor incomplete information. These ﬁndings indicate that while\nChatGPT can help with basic medication questions, it currently\nlacks the sophistication needed for reliable guidance in specialized\nclinical contexts.\nEthical concerns also emerge in managing sensitive\nmedical conditions, such as cancer. When evaluated for cancer\nsymptom management guidance, ChatGPT’s recommendations\nshowed notable discrepancies from National Comprehensive\nCancer Network (NCCN) guidelines. The model tended to\nprovide generalized advice that failed to address the complex\nsymptom burdens typical of cancer patients (14). This gap\nbetween AI-generated recommendations and evidence-based\nguidelines underscores the risks of relying on LLMs for critical\nhealth decisions.\nPrivacy constraints prevent LLMs from accessing individual\nmedical records, limiting their ability to provide personalized\nrecommendations. This limitation is particularly problematic\nfor high-risk populations, including elderly patients and those\nwith chronic illnesses, who require carefully tailored treatment\nplans. Without access to patient-speciﬁc data, LLMs default\nto generalized advice that may be inappropriate or unsafe\nfor complex medical conditions. As demonstrated in previous\nresearch, ChatGPT’s inability to consider speciﬁc renal function\nmetrics led to incorrect dosing recommendations for patients\nwith kidney disease, illustrating the potential safety risks of such\nlimitations (12).\nThese limitations highlight the critical need for a structured\nethical framework governing LLM deployment in healthcare. The\nintegration of AI into patient self-decision support requires a\nbalanced approach that positions these tools as supplements to, not\nreplacements for, professional medical expertise. A collaborative\nmodel combining AI capabilities with clinical oversight could\noptimize the beneﬁts of LLMs while minimizing risks. The\ndevelopment of robust regulatory guidelines will be essential\nto harness LLM potential while maintaining patient safety and\nethical standards.\n4 Future directions and\nrecommendations\n4.1 Improving accuracy and reliability of\nLLMs for medication-related information\nEnhancing LLM reliability for medication guidance\nrequires integration with real-time medical databases and\ncontinuous content updates. Connecting these models to current\npharmacological databases would enable access to the latest\ndrug interaction guidelines, side eﬀect proﬁles, and dosage\nFrontiers in Medicine 03 frontiersin.org\nfmed-12-1527864 January 20, 2025 Time: 17:16 # 4\nAydin et al. 10.3389/fmed.2025.1527864\nrecommendations. Such integration could help align AI systems\nwith evolving healthcare information while improving response\naccuracy for patient inquiries. Development of frameworks\nallowing LLMs to access validated sources such as PubMed, FDA\ndatabases, and regional repositories would strengthen the clinical\nrelevance of their recommendations.\nSpecialized training protocols represent another key avenue\nfor improvement, particularly in enhancing LLMs’ contextual\nunderstanding of patient inquiries. Targeted training in medical\nethics and patient safety could reduce risks in high-stakes\nareas such as mental health, reproductive health, and complex\nmedication management. Collaboration between healthcare\nprofessionals and AI developers is crucial for ensuring these\nmodels meet clinical standards. By involving medical experts\nin model reﬁnement, especially for context-speciﬁc information\nand decision-making guidance, developers can better align\nAI outputs with the nuanced requirements of personalized\nmedicine. Strategic partnerships between AI companies and\nmedical institutions could facilitate ongoing model validation\nand improvement.\n4.2 Balancing autonomy with safety:\nethical and regulatory perspectives\nThe growing role of LLMs in medication guidance necessitates\nan ethical framework balancing patient autonomy with safety.\nOur previous scoping review highlighted that while LLMs\neﬀectively simplify medical terminology, they often lack\nreliability in critical, high-stakes scenarios (5). This ﬁnding\nunderscores the need for comprehensive regulatory standards\nensuring transparency in AI recommendations, including\nclear disclaimers about the importance of professional medical\nconsultation. Such guidelines would help users understand\nthat AI-generated advice supplements, rather than replaces,\nclinical expertise.\nLooking forward, establishing medical AI ethical review boards,\nsimilar to institutional review boards for clinical research, could\nprovide structured oversight of LLM implementation. These boards\ncould evaluate training data, assess response biases, and monitor\nAI applications in patient education and self-care. This framework\nwould ensure AI development aligns with patient safety priorities\nand evolving healthcare policies.\n5 Discussion\nLLMs show promise in supporting patient self-decision\nmaking for medication use, providing accessible, on-demand\nresources for drug-related information. These tools help\npatients explore questions about drug interactions, side\neﬀects, and medication schedules, potentially enhancing\nhealth literacy and informed decision-making. However,\nsigniﬁcant limitations and risks exist. The inability of LLMs\nto incorporate individual patient data, including medical\nhistories and current medications, creates a fundamental\nbarrier to personalized advice. this limitation, combined\nwith potential inaccuracies in AI-generated responses,\nnecessitates careful integration of LLMs into healthcare,\nparticularly in sensitive areas such as reproductive and mental\nhealth.\nIn environments where access to healthcare professionals is\nlimited or communication systems are disrupted, such as remote\nareas or disaster zones, LLMs can provide support for patient\nself-care. These AI tools can deliver immediate, situation-speciﬁc\nadvice for managing medical concerns when professional help is\nunavailable. This immediate guidance can be life-saving in cases\nwhere there are no healthcare facilities nearby, oﬀering a sense of\nempowerment and structured steps for non-professionals facing\nmedical emergencies. Nevertheless, while LLMs can provide a\nvaluable bridge until medical assistance is available, they cannot\nreplace the expertise of healthcare professionals in complex or\nhigh-stakes situations. As such, their recommendations should\nemphasize the provisional nature of AI guidance in austere\nenvironments, ideally directing individuals to seek professional\ncare as soon as circumstances allow.\nIn addition to emergencies, LLMs can be used to support\npatients in everyday medication decisions, particularly with\nover-the-counter (OTC) drugs. Many individuals may not fully\nunderstand the risks of combining OTC medications with\nprescription drugs or speciﬁc medical conditions, often due to the\ncomplex and lengthy drug information provided on packaging.\nPatients may also assume OTC medications are inherently safe or\nmay avoid consulting healthcare professionals for minor issues.\nIn such cases, LLMs can assist by analyzing drug information\nand identifying potential interactions or contraindications based\non a patient’s reported medications and medical conditions. This\nguidance can help patients make safer choices, promoting informed\nself-care in routine health decisions. However, the accuracy\nand safety of these recommendations depend on LLMs being\ncontinuously updated with the latest clinical data. The potential\nfor adverse outcomes highlights the need for rigorous oversight,\nensuring that LLM-driven advice is a safe, supplementary resource\nin patient-centered healthcare.\nWhile LLMs can empower patients with information, the\nrisks of misinformation or oversimpliﬁed guidance are substantial,\nespecially if patients bypass professional medical consultation in\nfavor of AI recommendations. Future developments must address\nboth accuracy and ethical considerations. Key improvements\nshould include integrating validated medical databases and\nincreased collaboration with healthcare professionals. Additionally,\nregulatory oversight must establish clear boundaries for LLM use,\nensuring these tools serve as supportive rather than standalone\nresources. Clear disclaimers and transparent communication\nabout AI limitations can help position LLMs as supplements to\nprofessional healthcare guidance.\nLLMs represent a transformative development in patient\neducation, potentially reshaping how patients approach self-\nmedication and health decisions. Their successful implementation\ndepends on addressing current limitations in probabilistic data\nsynthesis, personalization capabilities, and ethical considerations in\nsensitive healthcare areas. The path forward requires balancing AI’s\ninformational capabilities with professional medical guidance while\nmaintaining focus on patient safety and autonomy. This balanced\napproach will be crucial for realizing the full potential of LLMs in\npatient-centered healthcare.\nFrontiers in Medicine 04 frontiersin.org\nfmed-12-1527864 January 20, 2025 Time: 17:16 # 5\nAydin et al. 10.3389/fmed.2025.1527864\nData availability statement\nThe original contributions presented in this study are included\nin this article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nSA: Conceptualization, Data curation, Investigation,\nMethodology, Validation, Visualization, Writing – original draft,\nWriting – review and editing. MK: Conceptualization, Data\ncuration, Investigation, Methodology, Project administration,\nValidation, Visualization, Writing – original draft, Writing – review\nand editing. VV: Conceptualization, Data curation, Methodology,\nValidation, Visualization, Writing – original draft, Writing – review\nand editing. KM: Conceptualization, Data curation, Investigation,\nMethodology, Project administration, Supervision, Writing –\noriginal draft, Writing – review and editing.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research, authorship, and/or publication of this article.\nConﬂict of interest\nThe authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships\nthat could be construed as a potential conﬂict of\ninterest.\nGenerative AI statement\nThe authors declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\n1. OpenAI. Available online at: https://openai.com/index/hello-gpt-4o (accessed\nMay 13, 2024). (2024).\n2. Google. Available at: https://gemini.google.com/ (accessed 2024). (2023).\n3. Altamimi I, Altamimi A, Alhumimidi A, Altamimi A, Temsah M. Snakebite\nadvice and counseling from Artiﬁcial intelligence: An acute venomous snakebite\nconsultation with chatgpt. Cureus. (2023) 15(6):e40351. doi: 10.7759/cureus.40351\n4. McMahon H, McMahon B. Automating untruths: Chatgpt, self-managed\nmedication abortion, and the threat of misinformation in a post-roe world.Front Digit\nHealth. (2024) 6:1287186. doi: 10.3389/fdgth.2024.1287186\n5. Aydin S, Karabacak M, Vlachos V , Margetis K. Large language models in\npatient education: A scoping review of applications in medicine. Front Med. (2024)\n11:1477898. doi: 10.3389/fmed.2024.1477898\n6. Al-Ashwal F , Zawiah M, Gharaibeh L, Abu-Farha R, Bitar A. Evaluating the\nsensitivity, speciﬁcity, and accuracy of chatgpt-3.5, chatgpt-4, bing ai, and bard against\nconventional drug-drug interactions clinical tools. Drug Healthc Patient Saf.(2023)\n15:137–47. doi: 10.2147/DHPS.S425858\n7. Juhi A, Pipil N, Santra S, Mondal S, Behera J, Mondal H. The capability of\nchatgpt in predicting and explaining common drug-drug interactions. Cureus. (2023)\n15(3):e36272. doi: 10.7759/cureus.36272\n8. Iqbal U, Lee L, Rahmanti A, Celi L, Li Y. Can large language models provide\nsecondary reliable opinion on treatment options for dermatological diseases? J Am\nMed Inform Assoc.(2024) 31(6):1341–7. doi: 10.1093/jamia/ocae067\n9. Rao A, Kim J, Lie W, Pang M, Fuh L, Dreyer K, et al. Proactive\npolypharmacy management using large language models: Opportunities to\nenhance geriatric care. J Med Syst. (2024) 48(1):41. doi: 10.1007/s10916-024-\n02058-y\n10. Roosan D, Padua P , Khan R, Khan H, Verzosa C, Wu Y. Eﬀectiveness of chatgpt\nin clinical pharmacy and the role of artiﬁcial intelligence in medication therapy\nmanagement. J Am Pharm Assoc.(2024) 64(2):422-8 e8. doi: 10.1016/j.japh.2023.11.\n023.\n11. Sheikh M, Barreto E, Miao J, Thongprayoon C, Gregoire J, Dreesman B, et al.\nEvaluating Chatgpt’s eﬃcacy in assessing the safety of non-prescription medications\nand supplements in patients with kidney disease. Digit Health.(2024) 10. doi: 10.1177/\n20552076241248082\n12. van Nuland M, Snoep J, Egberts T, Erdogan A, Wassink R, van der Linden P.\nPoor performance of chatgpt in clinical rule-guided dose interventions in hospitalized\npatients with renal dysfunction. Eur J Clin Pharmacol. (2024) 80(8):1133–40. doi:\n10.1007/s00228-024-03687-5\n13. Hsu H, Hsu K, Hou S, Wu C, Hsieh Y , Cheng Y. Examining real-\nworld medication consultations and drug-herb interactions: Chatgpt performance\nevaluation. JMIR Med Educ.(2023) 9:e48433. doi: 10.2196/48433\n14. Lazris D, Schenker Y , Thomas T. Ai-generated content in cancer symptom\nmanagement: A comparative analysis between Chatgpt and Nccn. J Pain\nSymptom Manage. (2024) 68(4):e303–11. doi: 10.1016/j.jpainsymman.2024.\n06.019\nFrontiers in Medicine 05 frontiersin.org",
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.7781630754470825
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.569750964641571
    },
    {
      "name": "Autonomy",
      "score": 0.5516284704208374
    },
    {
      "name": "Clinical decision support system",
      "score": 0.5275948643684387
    },
    {
      "name": "Health care",
      "score": 0.5060505270957947
    },
    {
      "name": "Patient education",
      "score": 0.4608217775821686
    },
    {
      "name": "Medicine",
      "score": 0.40724867582321167
    },
    {
      "name": "Psychology",
      "score": 0.3744957447052002
    },
    {
      "name": "Knowledge management",
      "score": 0.3471028208732605
    },
    {
      "name": "Nursing",
      "score": 0.2391507625579834
    },
    {
      "name": "Political science",
      "score": 0.2223483920097351
    },
    {
      "name": "Computer science",
      "score": 0.19006457924842834
    },
    {
      "name": "Artificial intelligence",
      "score": 0.11690744757652283
    },
    {
      "name": "Computer security",
      "score": 0.11511734127998352
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1351752",
      "name": "Koç University",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I1320796813",
      "name": "Mount Sinai Health System",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210144145",
      "name": "New York State University College of Human Ecology",
      "country": "US"
    }
  ],
  "cited_by": 5
}