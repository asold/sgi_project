{
  "title": "Efficient Large Scale Language Modeling with Mixtures of Experts",
  "url": "https://openalex.org/W4385567093",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2176075802",
      "name": "Mikel Artetxe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2421026517",
      "name": "Shruti Bhosale",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2507176056",
      "name": "Naman Goyal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2251118003",
      "name": "Todor Mihaylov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166285282",
      "name": "Myle Ott",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2924981553",
      "name": "Sam Shleifer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2889501458",
      "name": "Xi Victoria Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2286156102",
      "name": "Jingfei Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109311255",
      "name": "Srinivasan Iyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2609348608",
      "name": "Ramakanth Pasunuru",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092595749",
      "name": "Giridharan Anantharaman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096862909",
      "name": "Xian Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130982098",
      "name": "Shuohui Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2316310825",
      "name": "Halil AKIN",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2720408340",
      "name": "Mandeep Baines",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098658999",
      "name": "Louis Martin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042012428",
      "name": "Xing Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5051808640",
      "name": "Punit Singh Koura",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5090954457",
      "name": "Brian O‚ÄôHoro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102386505",
      "name": "Jeffrey Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124289572",
      "name": "Mona Diab",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2075600992",
      "name": "Zornitsa Kozareva",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130709233",
      "name": "Veselin Stoyanov",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4206529673",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2909212904",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W3035379020",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3104163040",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4304697835",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3099744315",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W3036369012",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W4300963525",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4298149550",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3185293939",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3034716087",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W95183648",
    "https://openalex.org/W3032765105",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3196642073",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W3107826490"
  ],
  "abstract": "Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O‚ÄôHoro, Jeffrey Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Veselin Stoyanov. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11699‚Äì11732\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nFew-shot Learning with Multilingual Generative Language Models\nXi Victoria Lin * *, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig ,\nMyle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer,\nPunit Singh Koura , Vishrav Chaudhary, Brian O‚ÄôHoro, Jeff Wang,\nLuke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li*\nMeta AI\nAbstract\nLarge-scale generative language models such\nas GPT-3 are competitive few-shot learners.\nWhile these models are known to be able to\njointly represent multiple languages, their train-\ning data is dominated by English, potentially\nlimiting their cross-lingual generalization. In\nthis work, we train multilingual generative lan-\nguage models on a corpus covering a diverse\nset of languages, and study their few- and zero-\nshot learning capabilities in a wide range of\ntasks. Our largest model with 7.5 billion pa-\nrameters sets new state of the art in few-shot\nlearning in more than 20 representative lan-\nguages, outperforming GPT-3 of comparable\nsize in multilingual commonsense reasoning\n(with +7.4% absolute accuracy improvement\nin 0-shot settings and +9.4% in 4-shot set-\ntings) and natural language inference (+5.4%\nin each of 0-shot and 4-shot settings). On the\nFLORES-101 machine translation benchmark,\nour model outperforms GPT-3 counterparts on\n171 out of 182 directions with 32 training ex-\namples, while surpassing the official super-\nvised baseline in 45 directions. We conduct\nan in-depth analysis of different multilingual\nprompting approaches, showing in particular\nthat strong in-context few-shot learning perfor-\nmance across languages can be achieved via\ncross-lingual transfer through both templates\nand demonstration examples. 1\n1 Introduction\nLarge autoregressive language models such as\nGPT-3 can be adapted, via few- and zero-shot\nlearning, to a wide range of tasks with signifi-\ncantly less cost than full fine-tuning (Brown et al.,\n2020; Bommasani et al., 2021). These models have\nbeen primarily developed for English. Although\n* Equal contribution. Correspondence to:\n‚ü®victorialin@meta.com, xianl@meta.com ‚ü©.\n1Our checkpoints, code and new dataset (XStoryCloze):\nhttps://github.com/facebookresearch/\nfairseq/tree/main/examples/xglm.\nthe training data of GPT-3 contains a small per-\ncentage of non-English text (7%) allowing it to\nachieve some promising cross-lingual generaliza-\ntion, the model is almost exclusively deployed for\nuse cases in English. Multilingual masked and\nsequence-to-sequence language models have been\nstudied, including mBERT, XLM-R, mT5, and\nmBART (Devlin et al., 2019; Conneau et al., 2020;\nXue et al., 2020; Fedus et al., 2021; Goyal et al.,\n2021a; Liu et al., 2020). These models are typi-\ncally fine-tuned on large amount of labeled data in\ndownstream tasks. Despite notable recent work at\nsmaller scales (Zhao and Sch ¬®utze, 2021) and for\ndomain-specific tasks (Winata et al., 2021), the\nmultilingual few-shot learning capabilities of lan-\nguage models are less well understood.\nIn this paper, we train four multilingual genera-\ntive language models (up to 7.5 billion parameters),\nXGLM‚Äôs, and present a comprehensive study of\nmultilingual zero- and in-context few-shot learning.\nWe train the models using a large-scale corpus of\n500B tokens that comprises 30 diverse languages,\nup-sampling the less-resourced languages to ren-\nder a more balanced language representation. We\nevaluate the models on multiple multilingual natu-\nral language understanding (NLU) tasks, machine\ntranslation and a subset of English tasks demon-\nstrated in Brown et al. (2020).\nWe found XGLM demonstrate strong cross-\nlingual capability where using English prompts\ntogether with non-English examples yields com-\npetitive zero- and few-shot learning performance.\nOur largest model (XGLM 7.5B) achieves strong\nzero- and few-shot learning performance on lan-\nguage completion and inference tasks (e.g. XS-\ntoryCloze: 65.4% 0-shot, 66.5% 4-shot; XNLI:\n46.3% 0-shot, 47.3% 4-shot). It also establishes\na new state-of-the-art on few-shot machine trans-\nlation across a large number of language pairs in\nthe FLORES-101 benchmark (Goyal et al., 2021b),\nsignificantly outperforming the GPT-3 model of\n11699\ncomparable size (6.7 billion parameters). On the\nother hand, multilingual pre-training causes perfor-\nmance drop on English. On 8 English NLU tasks,\nXGLM 7.5B underperforms GPT-36.7B by 10.9% on\naverage in zero-shot learning. GPT-3 6.7B also sur-\npasses XGLM 7.5B in machine translation on sev-\neral high-resource language pairs, including WMT-\n14 en‚Üîfr, WMT-16 en‚Üîde and WMT-19 en‚Üîzh.\nWe conduct an in-depth analysis of different\nmultilingual prompting approaches and examine\ncross-lingual transfer through template and demon-\nstration examples respectively. We show that non-\nEnglish templates sometimes yield unexpected low\nzero- and few-shot learning accuracy even if they\nare crafted by native speakers ( ¬ß4.3). Both using\nthe English template ( ¬ß4.4) and adding demon-\nstration examples ( ¬ß4.5) provide effective rem-\nedy. However, using demonstration examples from\nanother language often cannot further improve\nthe zero-shot learning performance when a strong\nprompting language (e.g. Engilsh) is used, which\nindicates room for improvement in cross-lingual\npre-training and in-context transfer approaches.\n2 Models and Pre-training Data\n2.1 Pre-training Data\nLanguage selection and pre-processing. We ex-\ntend the pipeline used for mining the CC100 cor-\npus (Conneau et al., 2020; Wenzek et al., 2020) to\ngenerate CC100-XL, a significantly larger multi-\nlingual dataset covering 68 Common Crawl (CC)\nsnapshots (from Summer 2013 to March/April\n2020) and 134 languages. Our pretraining data in-\nclude 30 languages covering 16 language families.\nThe natural data distribution is skewed with the\nnumber of English tokens being 6 times that of the\nsecond largest language. Following previous work\non multilingual pre-training (Conneau et al., 2020;\nLiu et al., 2020), we up-sampled the medium and\nlow resource languages to create a more balanced\nlanguage distribution (Appendix F.1). 2 Figure 1\nshows the language distribution of our pre-training\ndata before (blue) and after (green) up-sampling.\nJoint sub-word vocabulary. We process all lan-\nguages with a joint vocabulary of size 250k cre-\n2We inadvertently over-sampled some of the less resourced\nlanguages which is reflected in the statistics of ko, fi, th, bg,\nca, hi, et languages, as shown in Figure 1. We did not ablate\nthe effect of this mistake due to the extreme computational\ncost. Studying optimal language balancing is an important\narea for future work.\nated through unigram language modeling (Kudo,\n2018), using the SentencePiece library (Kudo and\nRichardson, 2018). We train the unigram-LM\nmodel using 10 million sentences randomly sam-\npled from the filtered data, according to the multi-\nnomial distribution defined in Lample and Conneau\n(2019) with ùõº= 0.3.\n2.2 Models and Training\nWe train decoder-only causal language models\nwith the Transformer architecture similar to GPT-3\n(Brown et al., 2020). This allows us to study the\neffect of scaling up model size along both width\nand depth dimensions. As a result, we compare\nfour models with 564M, 1.7B, 2.9B and 7.5B pa-\nrameters, respectively. The architecture details are\nsummarized in Table 1. Our models match that\nof GPT-3 models 3 except with the additional em-\nbedding parameters from a larger vocabulary. All\nmodels are trained for up to 500B tokens, with con-\ntext length of 2048 tokens. Further training details\nare described in Appendix A.\nGPT-3 XGLM\nsize ùëô ‚Ñé size ùëô ‚Ñé\n125M 12 768 ‚Äî\n355M 24 1024 564M 24 1024\n760M 24 1536 ‚Äî\n1.3B 24 2048 1.7B 24 2048\n2.7B 32 2560 2.9B 48 2048\n6.7B 32 4096 7.5B 32 4096\nTable 1: Model details. size: number of parameters, ùëô:\nlayers, ‚Ñé: hidden dimension. Models within the same\nrow have comparable sizes.\n3 Multilingual In-context Learning\nWe measure the performance of our multilingual\nlanguage models on downstream tasks in different\nlanguages given the tasks and few-shot demonstra-\ntions specified via prompts without further param-\neter updates (Appendix B).\n3.1 Multilingual and Cross-lingual Prompting\nPrevious work on English in-context learning\nhas shown that performance heavily depends on\n3For XGLM 2.9B we used the optimal depth-to-width\nparameter allocation for GPT-3 architectures based on rank\nbottleneck analysis (Levine et al., 2020). This allocation is\nexpected to have improved training efficiency. However, it did\nnot converge for XGLM 7.5B in our experiments, and we fell\nback to the original GPT-3 setup.\n11700\nen ru zh de es fr ja it pt el ko fi id tr ar vi th bg ca hi et bn ta ur sw te eu my ht qu\n105\n107\n109\n1011\n1013\n# of tokens (log scale)\n# of tokens in XGLM pre-training data\nen ru zh de es fr ja it pt el ko fi id tr ar vi th bg ca hi et bn ta ur sw te eu my ht qu0%\n2%\n5%\n8%\n10%\nPr(languagei)\n49.0%\n32.6%\n92.6%\nXGLM pre-training and pre-sharding (en: 49.0%)\nXGLM pre-training and post-sharding(en: 32.6%)\nGPT-3 pre-training and pre-sharding (en: 92.6%)\nFigure 1: The % of each language ùëô(ùëô = 1,2,..., 30) in XGLM‚Äôs pre-training data pre-upsampling (blue), post-\nupsampling (green), and its corresponding % in GPT-3‚Äôs training data (orange). We truncate the y-axis at 10% to\nbetter visualize the tail distribution.\nTask Category Dataset Template Candidate Verbalizer\nReasoning\nXCOPA cause:{Sentence 1}because[Mask]\nIdentityeffect:{Sentence 1}so[Mask]\nXStoryCloze {Context} [Mask]\nXWinograd {Context}(with ‚Äô_‚Äô replaced by[Mask])\nNLI XNLI {Sentence 1}, right?[Mask],{Sentence 2}Entailment: Yes |Neural: Also |Contradiction: No\nParaphrase PAWS-X {Sentence 1}, right?[Mask],{Sentence 2} True: Yes |False: No\nTranslation WMT, FLORES-101{Source sentence}=[Mask] Identity\nTable 2: Handcrafted (English) prompts for multilingual natural language understanding and translation tasks.\nthe prompt construction, and it is challenging\nto find the optimal prompt for a given language\nmodel (Gao et al., 2021; Perez et al., 2021). This\nproblem is further complicated in the multilingual\nsetting, where we need to find the optimal prompts\nfor examples in different languages.\nIn this work, we consider three approaches for\nobtaining the prompts for non-English tasks.\nHandcrafting prompts. The first approach is\nto ask native speakers of the target language to\nhandcraft the prompts. Prompts created this way\nare expected to have the most natural surface form.\nHowever, language expertise is expensive and we\nfurther consider two alternatives.\nTranslating from English prompts. We assume\nhigh-quality prompts of a task can be easily sourced\nin English(Sanh et al., 2021; Mishra et al., 2021).\nNon-verbal prompts do not contain words in any\nparticular language (e.g. the StoryCloze and WMT\nprompts shown in Table 2), while verbal prompts\nhave different realizations in different languages\n(Table 3). If the prompt is non-verbal, we simply\napply it to the other languages. If the prompt is\nverbal, we translate it into the other languages using\nautomatic translation APIs.\nCross-lingual prompting. We consider the third\napproach which directly applies the prompts in En-\nglish (or another high-resource language) to non-\nEnglish examples. We expect this approach to be\ncompetitive, as a result of the cross-lingual capa-\nbility of the model after being trained on a diverse\nset of languages.\n3.2 Learning from Cross-lingual\nDemonstrations\nThe cross-lingual nature of multilingual language\nmodels further enable the possibility of learning\nfrom a different language in context without pa-\nrameter updates. To do so we simply append exam-\nples from another language as the demonstration\nexamples in the language model context. Such ca-\npability enables cheap transfer from high-resource\nlanguages to the low-resource target languages.\n4 Experiments and Results\n4.1 Tasks\nWe evaluate the zero-shot and in-context few-shot\nlearning capabilities (Brown et al., 2020) of XGLM\non a spectrum of downstream tasks (Table 4).\nMultilingual tasks. We select four multilingual\ntasks spanning commonsense reasoning (XCOPA),\nanaphora resolution (XWinograd), natural lan-\nguage inference (XNLI) and paraphrasing (PAWS-\nX). We also created a new dataset, XStoryCloze,\nby professionally translating the validation split 4 of\n4We further split the translated data into train and test (20%\nvs. 80%, respectively) for each language, keeping the parallel\nsentence mapping in both splits.\n11701\nTask Lang Template Candidate Verbalizer\nEntailment Contradiction Neutral\nXNLI\nen {Sentence 1}, right?[Mask],{Sentence 2} Yes No Also\nzh {Sentence 1}[Mask]Ôºå{Sentence 2} Áî±Ê≠§ÂèØÁü•ÔºåÊâÄ‰ª•Ôºå‰∏çÂèØËÉΩÂêåÊó∂Ôºå\nes {Sentence 1},¬øverdad?[Mask],{Sentence 2} S√≠ No Adem√°s\nXCOPAen cause:{Sentence 1}because[Mask]|effect:{Sentence 1}so[Mask] Identityzh cause:Âõ†‰∏∫[Mask]ÔºåÊâÄ‰ª•{Sentence 1}|effect:Âõ†‰∏∫{Sentence 1}ÔºåÊâÄ‰ª•[Mask]\nTable 3: Handcrafted multilingual prompts. English ( en), Chinese ( zh) and Spanish ( es) for XNLI; English ( en) and\nChinese (zh) for XCOPA.\nTask Category Task |Train| |Dev| |Test| Non-En Sets|Lang.|\nReasoning\nXStoryCloze‚ô† 361 ‚Äì 1,511 translations 11\nXCOPA‚ô†(Ponti et al., 2020a) 33,410+400 100 500 translations 11\nXWinograd (Tikhonov and Ryabinin, 2021) ‚Äì ‚Äì 2,325 ‚Ä† translations 6\nNLI XNLI ‚ô†(Conneau et al., 2018) ‚Äì 2,490 5,010 translations 15\nParaphrase PAWS-X (Yang et al., 2019) ‚Äì 2,000 2,000 translations 7\nTable 4: Multilingual tasks used in our few-shot learning evaluation. All tasks use accuracy as the evaluation\nmetrics. ‚Ä†: In XWinograd, each language has different number of test examples: en: 2,325, jp: 959, ru: 315, pt:\n263. ‚Ä°: We use the COPA release in SuperGLUE (Wang et al., 2019). ‚ô†: Held-out tasks.\nthe English StoryCloze dataset (Spring 2016 ver-\nsion) to 10 other typologically diverse languages\n(ru, zh Simplified, es Latin American, ar, hi, id,\nte, sw, eu, my)5. In addition, we evaluate our mod-\nels on machine translation ( ¬ß4.8) and multilingual\nsocial value tasks (Appendix E.1).\nEnglish tasks. We also evaluate our models on\nEnglish commonsense reasoning and QA, a subset\nof benchmark tasks used by Brown et al. (2020),\nand compare the performance to state-of-the-art\nEnglish-centric few-shot learning models. The\ntasks are detailed in Table A1.\n4.2 Setup\nScoring function and calibration. We follow\nthe guidelines suggested by Perez et al. (2021)\nand adopt a cross-task generalization setting (Tri-\nantafillou et al., 2020) to select our scoring func-\ntion. We reserve three held-out tasks (XNLI,\nXCOPA and XStoryCloze) to perform the selection\nbased on their development set performance, and\ndirectly apply the selected settings to the rest of the\ntasks. In the end, we use the averaged per-token\nlog-probabilities ignoring the common prefix of\ndifferent candidates as the scoring function for all\n5For all of our multilingual NLU datasets, the non-English\nsections of the data are (professionally) translated from the En-\nglish section. Despite being the dominant approach adopted by\nthe community (Ruder et al., 2021), it was previously shown to\nintroduce data artifacts that inflate the measured cross-lingual\ntransfer of models (Artetxe et al., 2020). We leave collecting\nnative multilingual datasets that include non-English data as\nfuture work, and strongly encourage the community to also\nadopt this practice.\nmultilingual tasks with no additional calibration or\nnormalization. Appendix C.2 details the selection.\nFew-shot learning evaluation. We focus on\nbenchmarking the 0- and 4-shot learning perfor-\nmance of the models on all tasks. For cross-lingual\ndemonstration (¬ß4.5), scaling law ( ¬ß4.9) and trans-\nlation ( ¬ß4.8) we also reported 1-shot and 32-shot\nperformance. We report the average results across 5\nruns, randomly sampling a different set of few-shot\nexamples each time. Without further specification,\nwe use few-shot examples in the same language\nas the target example. Appendix C.3 details our\ncomplete evaluation protocol.\n4.3 Comparing Prompting Approaches\nWe first compare different multilingual prompting\napproaches proposed in ¬ß3.1 using XGLM 7.5B on\nXNLI and XCOPA 6. Native speakers among the\nauthors handcrafted7 the prompts for the following\ntasks: XNLI ( en, zh, es and hi) and XCOPA ( en,\nzh), as shown in Table 3. We compare the per-\nformance of these human-written prompts to En-\nglish prompts, machine-translated (MT) prompts\nand human-translated (HT) prompts.\nTable 5 and 6 show the performance of different\n6The original XCOPA release (Ponti et al., 2020b) does\nnot contain the English section. We added the English release\nfrom SuperGLUE (Wang et al., 2019) to facilitate cross-lingual\nexperiments.\n7The native speakers were instructed to create a prompt\nthat convert the task into a natural cloze-style question in their\nnative language with no further restrictions.\n11702\nTemp. en zh es hi Avg\nEn (HW) 50.8/50.6 48.5/47.737.5/44.444.0/45.5 45.2/47.0\nZh (HW) 33.5/35.5 33.5/36.4 34.5/34.8 36.0/34.0 34.4/35.1\nEs (HW) 39.2/49.9 44.8/45.3 46.2/48.241.5/43.5 42.9/46.7\nHi (HW) 45.0/43.5 39.5/41.0 34.2/40.5 36.2/40.5 38.8/41.4\nMulti. (HW) 50.8/50.6 33.5/36.446.2/48.236.2/40.5 41.7/43.9\nMulti. (MT) 50.8/50.6 35.8/39.5 36.5/45.0 41.0/39.9 41.0/43.8\nMulti. (HT) 50.8/50.6 38.5/41.2 46.0/48.1 37.5/38.9 43.1/44.7\nTable 5: 0/4-shot performance of XGLM 7.5B, evalu-\nated on the first 400 examples of XNLI (development\nset in en, zh, es and hi) using different prompting ap-\nproaches. Top: all inputs are instantiated with templates\nin the language specified in column 1. Bottom: all\ninputs are instantiated with templates in the same lan-\nguage as themselves. HW: human-written. MT: machine-\ntranslated. HT: human-translated.\nTemp. en zh th sw Avg\nEn (HW) 69.0/73.263.0/66.853.0/57.4 54.0/58.2 59.8/63.9\nZh (HW) 63.0/71.0 69.0/67.650.0/57.8 47.0/54.2 57.2/62.6\nMulti. (HW) 69.0/73.869.0/67.6 ‚Äì ‚Äì ‚Äì\nMulti. (MT) 69.0/73.8 62.0/68.4 48.0/56.6 51.0/60.2 57.5/64.8\nTable 6: 0/4-shot performance of XGLM 7.5B, evaluated\non XCOPA (development set in en, zh, th and sw).\nprompting approaches8. English templates perform\nthe best on average across languages for both tasks\nexcept for the 4-shot setting of XCOPA, where it\nslightly underperforms the machine translated tem-\nplates. On the XNLI task, the English template\nsignificantly improves the performance of Chinese\n(zh) and Hindi ( hi) over their native templates and\ntranslated templates. Similar trends are observed\nfor Thai ( th) and Swahili ( sw) on XCOPA 9. For\nboth tasks there exist languages where the native\ntemplates strongly outperforms the English tem-\nplates (Spanish ( es) for XNLI and Chinese for\nXCOPA), indicating significant room for future\nwork on language-specific prompt engineering.\n4.4 Cross-lingual Transfer through Templates\nWe further examine if the ability of universal\nprompting is English specific, and in addition, what\ncharacterize a language pair for which cross-lingual\nprompting can work. To this end, we apply each\nof the human-written non-English templates to the\nrest of the languages. As shown in Table 5 and 6,\n8Appendix D.1 provides the comparison between English\nprompts and the MT and HT prompts on the complete dev sets\nof XNLI and XCOPA.\n9The strong performance of English templates may be\npartially contributed to the fact that the non-English evaluation\ndata on XNLI and XCOPA are obtained from translation.\nTesting how well the English templates perform on native\nnon-English test sets is an interesting future work.\nusing the Spanish prompt yields competitive 0- and\n4-shot performance across all languages, with the\n4-shot average performance being comparable to\nthat of the English template. The Hindi template\nalso achieves significantly above random perfor-\nmance on the XNLI tasks for most languages (espe-\ncially en). The Chinese template, however, achieves\nclose-to-random performance for all languages on\nXNLI, as well as close-to-random for Thai (0-shot)\nand Swahili (0-shot) on XCOPA. We hypothesize\nthat the common sub-tokens and the amount of\ncode-switching text in the pre-training data play a\nsignificant role in enabling cross-lingual prompt-\ning. And in general, high-resource languages with\nlarge amounts of pre-training data and vocabulary\noverlap with other languages act as better universal\nprompting languages. We leave a more systematic\nverification of this hypothesis to future work.\n4.5 Cross-lingual Transfer through\nDemonstration Examples\nWe examine the capabilities of learning from\ncross-lingual demonstration examples ( ¬ß3.2) of\nXGLM7.5B on XNLI. We examine two settings\nfor each train-eval language pair: same-language-\nprompting, where the prompt templates and the\nexample are in the same language, and source-\nlanguauge-prompting where the prompt templates\nfor both the demo and test examples are in the\nsource language. We use the human-translated\nprompts for same-language-prompting.\nTable 7 shows results on a subset of language\npairs of XNLI, where we evaluate transfer through\ndemonstration examples from in-context demon-\nstration examples from high-resource languages to\nlower-resourced ones, and between languages that\nare typologically similar. We report the difference\nbetween the 32-shot learning results and the 0-\nshot learning results. The non-English templates in\nthis experiment are obtained via human-translation.\nWhile they typically underperform the in-language\nfew-shot setting (Figure A2), most cross-lingual\nfew-shot settings significantly improve over the 0-\nshot setting for the target language. Bulgarian is\nan exception as it does not benefit from Russian\ndespite being in the same language family. An-\nother language that does not work well in the cross-\nlingual settings is Swahili ( low resource), for which\nwe examined transfer from English ( high resource)\nand Arabic ( medium resource). In contrast, Thai\n(medium) and Urdu ( low resource) significantly\n11703\nhigh medium low\nen ru tr ar hi\nmedium low medium low\nprompt bg el th tr vi hi sw ur bg ur sw ur\nSame-lang2.55 0.98 2.16 1.27 2.23 2.51 -0.691.21 -2.49 -0.38 -1.64 3.31\nSource-lang -4.59 -2.447.87 -4.97 -1.082.01 -1.157.42 -1.43 6.67 -5.862.31\nTable 7: Learning from cross-lingual demonstrations on XNLI, evaluated on the test set. The results are the absolute\nimprovement over the zero-shot performance for the evaluated language using human-translated prompts. The first\nlanguage group refers to the source language and the second one refers to the target language. Same-lang refers to a\nsetting there the template is in the example language and source-lang refers to a setting where the template is only in\nthe source language.\nSource prompt (instantiated) Target prompt (instantiated)\nSame-langThe best thing that may be said of Podhoretz and\nDecter is that their biological clocks can‚Äôt have many\nmore minutes left on them, right? Yes,Decter is old.\nV√¢ng, t√¥i th·∫≠m ch√≠ kh√¥ng nghƒ© v·ªÅ ƒëi·ªÅu ƒë√≥, nh∆∞ng t√¥i ƒë√£ r·∫•t th·∫•t\nv·ªçng, v√†, t√¥i l·∫°i n√≥i chuy·ªán v·ªõi anh ta l·∫ßn n·ªØa, ƒë√∫ng kh√¥ng? ƒê√∫ng,\nt√¥i ƒë√£ kh√¥ng n√≥i chuy·ªán v·ªõi anh ta n·ªØa.\nSource-langThe best thing that may be said of Podhoretz and\nDecter is that their biological clocks can‚Äôt have many\nmore minutes left on them, right? Yes,Decter is old.\nV√¢ng, t√¥i th·∫≠m ch√≠ kh√¥ng nghƒ© v·ªÅ ƒëi·ªÅu ƒë√≥, nh∆∞ng t√¥i ƒë√£ r·∫•t th·∫•t\nv·ªçng, v√†, t√¥i l·∫°i n√≥i chuy·ªán v·ªõi anh ta l·∫ßn n·ªØa, right? Yes,t√¥i ƒë√£\nkh√¥ng n√≥i chuy·ªán v·ªõi anh ta n·ªØa.\nTable 8: XNLI example prompts for cross-lingual transfer from English (en) to Vietnamese (vi), with the same-\nlanguage and source-language settings. The underlined text shows the verbalized part of the prompt.\nbenefit from cross-lingual demonstrations 10.\nWe also observed the benefit of cross-lingual\ntransfer from demonstration examples is gener-\nally canceled if a better prompt (e.g. the English\nprompt) is used for the target language. We re-\nport the crosslingual demonstration experiments\nbetween all pairs of languages for XNLI, XCOPA\nand XStoryCloze and provide more discussion in\nAppendix D.2.\n4.6 Performance on Multi-lingual Tasks\nUsing English as the universal prompting lan-\nguage, we characterize the zero- and few-shot\nin-context learning capabilities of XGLM 7.5B on\nXNLI, XCOPA and XStoryCloze and compare\nthem to English centric language models of com-\nparable size.\nComparison to GPT-3. We compare XGLM 7.5B\nto GPT-36.7B on high, medium, low and extremely\nlow resources languages 11. The results are sum-\nmarized in Table 9 and 10. On all three tasks,\n10Both Thai and Urdu obtained close-to-random zero-shot\nlearning performances using the translated templates, which\nmight make them easier to be further improved. Besides, there\nis inherent code switching in these languages (English pres-\nence in Thai and Urdu both lexical and morphological). Turk-\nish and Arabic also have influence on Urdu. We hypothesize\nthat these factors also positively impacted the cross-lingual\nin-context learning performance.\n11We use GPT-3 Curie: https://blog.eleuther.\nai/gpt3-model-sizes/\nXGLM7.5B outperforms GPT-36.7B by a large mar-\ngin according to the average performance across\nlanguages, especially on medium, low and ex-\ntremely low resource languages. On XNLI, GPT-\n36.7B performs well on English and similar lan-\nguages, surpassing XGLM 7.5B on en, de (4-shot),\nes (4-shot), fr (0-shot). A possible explanation is\nthat these languages have significant presence in\nthe GPT-3 training data ( fr: 1.8%, de: 1.5%, es:\n0.8% as shown in Figure 1) and can benefit more\nfrom the lexical cognates from English.\nComparison to Translate-test Baseline. We\nalso create a translate-test baseline, where we trans-\nlate the non-English examples of the multilingual\ntasks to English using the Google Cloud Transla-\ntion API 12 and use GPT-3 6.7B repl., an in-house\nreplication of GPT-3 6.7B, to perform inference. We\nfound the translate-test is a strong baseline of mul-\ntilingual zero- and few-shot learning as is shown\nin Table 9 and 10. Across all three tasks, it sig-\nnificantly narrows the performance gap between\nEnglish and other languages, especially on XNLI 13.\n12https://cloud.google.com/translate\n13The performance of translate-test baselines might be in-\nflated given MT systems are often trained on backtranslations\nwhich makes it good at translating translationese (Edunov\net al., 2019), which commonly exist in non-English evaluation\ndata. Besides, the translation-test approach relies on high-\nquality machine translation (MT) systems trained on large\namounts of parallel data.\n11704\nhigh medium low Avg.model # shot en de es fr ru zh ar bg el th tr vi hi sw ur\nGPT-36.7B 0 55.436.8 37.051.244.8 42.6 38.5 42.9 38.8 38.4 40.6 41.3 36.5 34.6 34.5 40.94 53.0 46.4 48.548.3 44.3 45.8 38.2 41.7 42.1 36.8 38.7 42.3 34.3 33.7 34.5 41.9\nXGLM7.5B 0 55.3 42.3 39.150.848.4 44.8 48.1 49.1 46.4 46.8 45.5 47.6 43.4 45.5 41.9 46.34 52.6 45.6 45.8 49.4 48.6 48.8 46.4 48.9 48.7 46.6 45.4 48.5 46.8 44.5 43.4 47.3\nTranslate + GPT-36.7Brepl.0 54.6 53.7 54.5 53.9 52.0 52.6 52.0 53.4 53.5 50.6 53.3 52.6 50.7 51.3 48.7 52.54 54.1 52.4 49.2 50.3 53.2 51.1 50.5 53.7 53.0 48.2 51.8 52.8 49.8 50.2 47.2 51.2\nTable 9: Comparison of different models on XNLI.\nXStoryCloze XCOPA\n# high medium low ex-low Avg. high medium low ex-low Avg.\nmodel shot en es ru zh ar id hi sw te eu my zh id it th tr vi et sw ta ht qu\nGPT-36.7B 0 73.4 62.4 56.9 55.8 48.4 56.6 50.1 49.4 52.8 51.2 49.5 55.1 55.0 60.261.653.6 53.4 52.8 50.8 52.2 55.0 51.850.054.2\n4 74.4 62.2 56.4 54.7 47.7 55.4 49.6 49.3 52.8 51.1 49.5 54.8 57.8 60.8 64.5 54.2 52.9 54.8 51.8 52.0 54.9 51.549.755.0\nXGLM7.5B 0 75.068.1 71.0 66.6 58.3 70.1 60.9 65.0 61.7 62.3 60.7 65.4 62.4 66.660.856.8 56.8 61.4 61.6 57.6 56.2 57.047.458.6\n4 75.969.2 72.4 67.7 59.8 70.8 62.5 65.2 63.4 63.8 61.2 66.5 67.2 68.9 69.2 62.0 58.5 65.6 65.9 62.9 56.3 58.947.162.0\nTranslate 0 81.2 75.6 75.4 72.9 71.5 71.2 70.5 70.0 66.9 70.5 72.7 72.6 75.0 73.2 76.0 53.8 72.4 72.2 72.4 63.8 67.2 65.0 - 67.4‚Ä†\n+ GPT-36.7Brepl.4 82.6 75.0 75.3 73.1 71.8 72.0 71.6 71.0 68.4 72.2 72.0 73.2 78.5 75.8 80.6 57.7 73.7 76.0 73.6 67.2 69.9 67.0 - 70.0‚Ä†\nTable 10: Comparison of different models on XStoryCloze and XCOPA. ‚Ä†Google Translation API is not available\nfor qu. For the averaged translate-test results we directly used the GPT-3 6.7B repl. model for qu entry.\n4.7 Performance on English Tasks\nWe also benchmark the performance of XGLM 7.5B\non English tasks. Figure 2 shows the compari-\nson between XGLM 7.5B, GPT-36.7B and GPT-36.7B\nrepl. on a subset of English tasks used by Brown\net al. (2020). Our replication of GPT-3 6.7B, GPT-\n36.7B repl., performs better than or close to GPT-\n36.7B on all tasks. While XGLM 7.5B performs com-\npetitively on all tasks, there remains a consider-\nable performance gap comparing to GPT-3 6.7B and\nGPT-36.7B repl.. On most tasks XGLM 7.5B and\nGPT-36.7B repl. show similar performance trend as\nùëòchanges. For example, both models show a per-\nformance dip at 1-shot on HellaSwag and PIQA,\nand 128-shot on COPA.\nThere are multiple reasons why XGLM 7.5B un-\nderperforms English centric models on the English\ntasks. First, only 32.6% of XGLM 7.5B‚Äôs 500B-\ntoken training data is English while both English-\ncentric models are trained on close to 300B English\ntokens. Second, the model capacity of XGLM 7.5B\nis shared by 30 languages, and the ‚Äúcurse of mul-\ntilinguality‚Äù can degrade the performance across\nall languages (Conneau et al., 2020). Further scal-\ning up the model capacity and training data can\npotentially close this gap. 14\n14The differences between the training corpora of the three\nmodels may have also contributed to the performance dif-\nference. While both English centric models incorporate\nhigh-quality English monolingual corpora such as BookCor-\npus (Zhu et al., 2019) in their training data (GPT-3 6.7B also\n4.8 Performance on Machine Translation\nWe report machine translation results on popular\nWMT pairs in Table 11, and a subset of FLORES-\n101 (Goyal et al., 2021b) in Table 12. We use\ngreedy decoding for both GPT-3 and our own\nmodel, and use the same 32 examples for few-shot\nlearning in each case.\nGPT-3 yields strong results on a few languages\nthat are best represented in its training data, nar-\nrowly surpassing our model on WMT French-\nEnglish, German-English, and Chinese-English,\nas well as a few pairs the FLORES-101 set. GPT-\n3 is particularly strong when English is the target\nlanguage, presumably due to its strong English lan-\nguage modeling capability. However, it does poorly\non the broader set of less-resourced languages. For\ninstance, GPT-3 fails completely when translating\ninto Korean, Arabic, Swahili, Hindi, Burmese and\nTamil in FLORES-101, with a spBLEU score of\n1.2 in the best case.\nIn contrast, our model obtains solid results\nacross the board. In addition to surpassing GPT-3\nin 171 out of 182 language pairs in the FLORES-\n101 set, our model is also competitive with the\nofficial supervised baseline for this dataset, even\nupsamples such high-quality data), XGLM 7.5B is trained solely\non data extracted from Common Crawl. However, we do not\nexpect this to be the main impact factor. Scao et al. (2022)\nconducted a similar experiment showing that a multilingual\nmodel (1.3B parameters) pre-trained over 13 languages also\nsignificantly underperforms an English model trained from\nthe same data source in terms of zero-shot generalization.\n11705\nFigure 2: Performance on English tasks. For XGLM 7.5B and GPT-36.7B repl., we plot the confidence interval from 5\ndifferent runs corresponding to different training sets when ùëò >0. For GPT-3 6.7B we use the performance reported\nby Brown et al. (2020).\nWMT-14 WMT-16 WMT-19 Avg.\nfr-en en-fr de-en en-de fi-en en-fi ru-en en-ru zh-en en-zh xx-en en-xx\nGPT-3\n(API)\nAda 22.4 13.0 19.9 10.3 4.5 2.7 8.9 1.0 4.5 3.5 12.0 6.1\nBabbage 29.8 22.4 30.5 16.9 12.3 5.4 20.8 4.1 12.3 9.1 21.1 11.6\nCurie 35.3 28.7 36.1 23.7 18.4 9.9 28.6 9.8 17.6 17.4 27.2 17.9\nXGLM7.5B 33.2 28.5 34.6 23.5 20.2 15.5 29.3 18.7 16.7 17.4 26.8 20.7\nTable 11: Machine translation results on WMT (detokenized BLEU). We use 32 examples from the previous edition\nfor few-shot learning. BLEU scores computed using SacreBLEU with default settings (Post, 2018).\nen de fr ca fi ru bg zh ko ar sw hi my ta Avg.\navg out of xx\nSupervised 24.0 21.0 20.4 19.1 17.5 18.6 20.2 15.5 14.9 16.1 16.6 16.2 7.2 4.8 16.6\nGPT-36.7B 9.9 9.1 9.4 9.3 6.4 7.0 5.5 4.9 2.4 2.9 1.7 0.5 0.2 0.3 5.0\nXGLM7.5B 21.1 16.5 17.1 13.6 13.4 13.2 13.9 9.1 6.5 10.4 12.1 9.8 6.9 7.1 12.2\navg into xx\nSupervised 26.0 20.2 26.7 20.0 16.7 18.5 24.5 14.1 13.5 11.8 16.3 19.3 2.1 2.5 16.6\nGPT-36.7B 18.9 9.9 14.2 9.3 4.2 4.8 2.7 4.0 0.6 0.5 0.2 0.3 0.1 0.1 5.0\nXGLM7.5B 28.5 14.9 20.6 14.4 10.9 12.4 18.5 10.9 5.9 6.1 8.5 9.7 5.8 3.5 12.2\nTable 12: Machine translation results on FLORES-101 devtest (spBLEU). GPT-3 6.7B and XGLM7.5B use 32 examples\nfrom the dev set for few-shot learning. Supervised results correspond to the M2M-124 615M model from Goyal\net al. (2021b). spBLEU computed using the implementation from Goyal et al. (2021b). Full results in Appendix D.3.\nsurpassing it in 45 language pairs. This suggests\nthat large-scale multilingual language models have\na great potential for building machine translation\nsystems for low-resource languages, even if little or\nno parallel data is available.\n4.9 Scaling up Model Size\nFinally, we study the impact of scaling up the\nmodel parameter size on its 0- and few-shot learn-\ning capabilities. Figure 3 shows the performance\n(ùëò = 0,4,32,128) of the four XGLM models\n(564M, 1.7B, 2.9B, 7.5B) on the five multilingual\ntasks. The ùë¶-axis represents the average accuracy\nacross languages for each task. On commonsense\nreasoning tasks (XStoryCloze, XCOPA, XWino-\ngrad), the performance of all models increases as\nùëò increases from 0 to 32. The performance gain\nfrom demonstration examples also gets larger as\nthe model size increases, indicating bigger mod-\nels can better leverage the in-context examples. On\nXNLI, the performance of all models increases as ùëò\nincreases from 0 to 4, but decreases for ùëòat 32 and\nabove. With the same number of demonstration ex-\namples, larger models do not always benefit more.\nPAWS-X is a task where in-context learning strug-\ngles ‚Äì the performance of all models oscillates near\nrandom (50%) as ùëòchanges. A possible reason is\nthe adversarial nature of PAWS-X, where the para-\nphrase and non-paraphrase pairs by design have\nhigh lexical overlap. We expect scaling to be an\n11706\nFigure 3: Zero-shot and in-language few-shot learning\nperformance as a function of model size. The last plot\nshows the average performance over all five tasks in 0-\nand 4-shot learning.\neffective recipe for building stronger multilingual\nlanguage models, given the current trend.\n5 Related Work\nLanguage model prompting. Brown et al.\n(2020) first demonstrated in-context few-shot learn-\ning using the GPT-3 model. This method removes\nthe need for task-specific updates to the model pa-\nrameters: the few-shot examples that one would\nnormally use for fine-tuning are provided at infer-\nence time to the same model for each task. On\nseveral high-resource Latin language pairs, GPT-3\nachieves machine translation performance that is\nclose to or better than state-of-the-art supervised\nmodels, given only a handful of demonstration ex-\namples.15 Such change in the learning paradigm\nraises new questions about multilinguality, which\nhas not been studied as extensively. Winata et al.\n15Study shows that language contamination in pre-training\ndata can effectively boost the cross-lingual capability of\nEnglish-centric language models (Blevins and Zettlemoyer,\n2022). With a heavier tail of deliberately introduced multilin-\ngual data, PALM-540B (Chowdhery et al., 2022) later achieves\neven stronger few-shot machine translation performance.\n(2021) evaluates the in-context few-shot learning\nabilities of several GPT-2, GPT NEO and T5 on three\nadditional languages ( de, es, fr) using multiple\nNLU tasks, considering monolingual prompts as\nwell as cross-lingual prompts, demonstrating the\nmultilingual in-context learning skills of the En-\nglish GPT and T5 models. Zhao and Sch ¬®utze\n(2021) evaluated different fine-tuning and prompt-\ntuning (Liu et al., 2021) approaches on XLM-R\nand demonstrates the effectiveness of prompting\nin few-shot crosslingual transfer and in-language\ntraining of a multilingual masked language model.\nMultilingual pre-training. Early multilingual\npre-training work train word embeddings over mul-\ntilingual corpora (Mikolov et al., 2013). The\nmultilingual versions of contextualized embed-\nding models such as BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), BART (Lewis et al.,\n2019) and T5 (Raffel et al., 2020) were also de-\nveloped: mBERT (Devlin et al., 2019), XLM-R\n(Conneau et al., 2020), mBART (Liu et al., 2020),\nand mT5 (Xue et al., 2020). Such models were\ntrained on a single, multilingual text corpus such as\nmC4 (Xue et al., 2020) or CC25 (Liu et al., 2020).\nSeveral approaches have been developed to fa-\ncilitate cross-lingual transfer, including sub-word\ntokenizers which enabled efficient, shared vocabu-\nlary learning across languages (Kudo and Richard-\nson, 2018), joint training for efficient knowledge\ntransfer across languages (Pires et al., 2019; Jiang\net al., 2020; Kassner et al., 2021), etc. A notable\nconcurrent work is BLOOM 16, which scales multi-\nlingual pre-training to 46 languages and 175 billion\nparameters.\n6 Conclusion\nWe introduce four multilingual generative language\nmodels (XGLMs) at different scales, and study\ntheir in-context few- and zero-shot learning capa-\nbilities. We show that the few-shot learning capa-\nbility of XGLM steadily improves as it scales. Our\nlargest model (7.5B parameters) sets a new state\nof the art for few-shot learning in more than 20\nlanguages (including mid- and low-resource lan-\nguages) on commonsense reasoning, NLI and ma-\nchine translation tasks. An in-depth analysis shows\nthe models are highly cross-lingual, which leads\nto strong few-shot learning performance in non-\nEnglish languages.\n16https://bigscience.huggingface.co/\nblog/bloom\n11707\nLimitations\nAlthough the multilingual language model is an\nimportant step towards building inclusive general-\npurpose foundation models, our current models\nhave the following limitations.\nTraining Data. Our models are trained on a static\nmultilingual corpus extracted from CommonCrawl,\nwith English text comprising 32.6% of the total\nnumber of tokens corresponding to 163B tokens.\nThe English data portion of the corpus corresponds\nto roughly 54% only of GPT-3‚Äôs training data. We\napplied several data filtering strategies as proxies\nfor data quality assurance (see a comprehensive list\nin the Data Card in Appendix F), such as removing\nduplicated documents and paragraphs by URLs,\nfiltering out paragraphs with high ratio of digits and\npunctuation, removing paragraphs with profanity,\nfiltering by max number of URLs and minimum\nlength, etc. Such filtering may potentially result\nin bias of the remaining data used in pretraining,\nwhich would need further analysis to understand.\nFurthermore, the raw data were taken from static\nCommonCrawl snapshots, which may not include\nentities and events beyond the time span of the\nsnapshots (till March 2020), such as COVID-19,\netc. As such we also note the potential difference in\ngenres between CommonCrawl and the genres used\nin GPT-3 comprising in addition to CommonCrawl,\ncorpora such as BookCorpus and Wikipedia.\nMoreover, GPT-3 is trained on 118 languages\ndespite the fact that 93% of the data is English. 17\nIn contrast our models are trained on 30 languages\nafter rigorous language identification and filtering.\nPerformance on English tasks. As is shown in\nSection 4.7 and Figure 2, our model underperforms\nEnglish-centric models on eight tasks ranging from\ncommonsense reasoning to QA. There are several\nfactors which could be contributing to this gap,\nsuch as\n‚Ä¢ Difference in training data quality (XGLM is\ntrained on filtered CommonCrawl data only,\nwhile the English-centric models are trained\non data including both CommonCrawl as well\nas high-quality corpora such as BookCorpus\nand Wikipedia) and quantity (as is described\nin the previous paragraph, the multilingual\n17https://github.com/openai/gpt-3/blob/\nmaster/dataset_statistics/languages_by_\nword_count.csv\nmodel was trained on 54% of the English data\nused in English-centric models);\n‚Ä¢ Curse of multilinguality. Previous work in\nmultilingual training has shown that increas-\ning the number of languages in model with\nshared parameters hurts performance on all\ntraining languages, e.g. English (Conneau\net al., 2020).\nAdditional experiments controlling for these factors\nwould shed more light on the observed gap.\nModel architecture and training objective. In\nthis work, we only experimented with causal lan-\nguage models with a decoder-only architecture,\nwhich had previously demonstrated promising few-\nshot learning capabilities (Brown et al., 2020).\nHowever, such architecture and pretraining objec-\ntive do not leverage bidirectional context such as\nthose used by masked language models (MLM), or\nsequence-to-sequence architectures with denoising\nautoencoder pretraining objectives.\nModel evaluation via in-context learning. We\ncompare our language models to the baselines pri-\nmarily in the in-context learning paradigm, using\nthe same prompts for all language models in the\ncomparison unless explicitly specified. Despite\nminimal effort engineering the prompts for any\nmodel, it is possible that the prompts work better\nwith some models than the others, which intro-\nduces bias to the evaluation. However, we expect\nthis factor to have small impact and the relative\nstrengths of the models can be reliably measured\ngiven the volume of tasks they were evaluated on.\nEvaluation on social value tasks for more lan-\nguages. We evaluate and analyze the models‚Äô per-\nformance on hate speech detection and gender bias\nfor professional occupations. These studies are\nlimited by the available evaluation datasets. We\nare limited in our study as we only investigate this\nproblem space for six languages (English, French,\nSpanish, Italian, Portuguese, and Polish) where a\nmajority of them (5) pertain to the Romance lan-\nguage family. It would be pertinent to investigate\nthe impact of multilingual models on social value\ntasks across a wider and more diversified set of\nlanguages before drawing solid conclusions. More-\nover, we contend that studies on other tasks such\nas stereotype (Nangia et al., 2020; Nadeem et al.,\n2021), ethics (Hendrycks et al., 2020) would pro-\n11708\nvide a more comprehensive view of model behavior\nfor social value tasks.\nEthical Considerations\nDevising multilingual pre-trained language models\ncan serve as a powerful tool in the NLP arsenal for\nmultiple reasons.\nEnergy and maintenance efficiency. From an\nengineering perspective, XGLM pertains to a fam-\nily of models that represent single unified models\ncatering to many languages which have wide ap-\nplication across many applications. Such a unified\nsingle model saves on carbon footprint as well as\nenergy consumption (comparing to the alternative:\nseparate models for different languages) leading to\nmore energy efficiency. A single model, despite\nhaving the risk of being a single point of failure, has\nthe powerful incentive of being easier to maintain,\naccess, distribute, and track.\nDiversity and inclusion. Models such as XGLM\nrepresent a paradigm shift from the Anglo-centric\nview of the world of NLP to being able to cater to\nall languages on an equal footing. Paying attention\nto the design of such models is critical to ensure\nequitability and inclusion, exemplified here by at-\ntempting to balance language representation. The\nfurther power of XGLM specifically is its ability\nto perform comparably to Anglo-centric models\nin zero to few shot settings. Possessing powerful\nmultilingual models that can perform well in such\nsettings especially for medium to extremely low re-\nsource languages helps alleviate the burden of cre-\nating supervised data for such languages especially\nfor economically challenged languages (medium to\nlow digital presence typically goes hand in hand\nwith economic disparities). Moreover, having such\nmodels catering to scarcer languages spurs scien-\ntific research in such languages leading to more\ndiversified NLP, and more diversified science in\nthe broader sense.\nSocial values. We further investigate the impact\nof our models on social valued problems such as\nhate speech detection and bias (Appendix ¬ßE). De-\nspite inconclusive results overall (bordering on neg-\native), we note that for the relatively scarcer data\nsetting (Polish) the multilingual models outperform\nthe Anglo-centric models indicating that XGLM\nwill be performant for less resourced languages.\nThis is especially significant for social value tasks\nwhere obtaining training data is quite problematic\ndue to the inherent expense of obtaining high qual-\nity annotated data.\nTransparency and Accountability. In the spirit\nof transparency and accountability for large-scale\nlanguage modeling we include detailed model card\nand data card with the model and paper release.\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2020.\nTranslation artifacts in cross-lingual transfer learning.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 7674‚Äì\n7684. Association for Computational Linguistics.\nYonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng\nGao, and Yejin Choi. 2020. PIQA: reasoning about\nphysical commonsense in natural language. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020 , pages 7432‚Äì\n7439. AAAI Press.\nTerra Blevins and Luke Zettlemoyer. 2022. Lan-\nguage contamination explains the cross-lingual ca-\npabilities of english pretrained models. CoRR,\nabs/2204.08110.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877‚Äì1901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\n11709\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff\nDean, Slav Petrov, and Noah Fiedel. 2022. Palm:\nScaling language modeling with pathways. CoRR,\nabs/2204.02311.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the AI2 reasoning challenge. CoRR,\nabs/1803.05457.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm√°n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics , pages 8440‚Äì\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: evaluat-\ning cross-lingual sentence representations. CoRR,\nabs/1809.05053.\nMaria De-Arteaga, Alexey Romanov, Hanna Wal-\nlach, Jennifer Chayes, Christian Borgs, Alexandra\nChouldechova, Sahin Geyik, Krishnaram Kentha-\npadi, and Adam Tauman Kalai. 2019. Bias in bios: A\ncase study of semantic representation bias in a high-\nstakes setting. In proceedings of the Conference on\nFairness, Accountability, and Transparency , pages\n120‚Äì128.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL) .\nSergey Edunov, Myle Ott, Marc‚Äô Aurelio Ranzato, and\nMichael Auli. 2019. On the evaluation of machine\ntranslation systems trained with back-translation.\narXiv preprint arXiv:1908.05204 .\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018) .\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. arXiv\npreprint arXiv:2101.03961.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 3816‚Äì3830. Association for Computa-\ntional Linguistics.\nAaron Gokaslan and Vanya Cohen. 2019. Open-\nwebtext corpus. http://web.archive.org/\nsave/http://Skylion007.github.io/\nOpenWebTextCorpus.\nAndrew S. Gordon, Zornitsa Kozareva, and Melissa\nRoemmele. 2012. Semeval-2012 task 7: Choice\nof plausible alternatives: An evaluation of com-\nmonsense causal reasoning. In Proceedings of the\n6th International Workshop on Semantic Evaluation,\nSemEval@NAACL-HLT 2012, Montr√©al, Canada,\nJune 7-8, 2012 , pages 394‚Äì398. The Association for\nComputer Linguistics.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021a. Larger-scale trans-\nformers for multilingual masked language modeling.\nCoRR, abs/2105.00572.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc‚Äô Aurelio Ranzato, Francisco Guzm√°n,\nand Angela Fan. 2021b. The FLORES-101 evalu-\nation benchmark for low-resource and multilingual\nmachine translation. CoRR, abs/2106.03193.\nKenneth Heafield. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation , pages\n187‚Äì197, Edinburgh, Scotland. Association for Com-\nputational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2020. Aligning ai with shared human values. arXiv\npreprint arXiv:2008.02275.\nXiaolei Huang, Linzi Xing, Franck Dernoncourt, and\nMichael Paul. 2020. Multilingual twitter corpus and\nbaselines for evaluating demographic bias in hate\nspeech recognition. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference , pages\n1440‚Äì1448.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020. X-FACTR:\nMultilingual Factual Knowledge Retrieval from Pre-\ntrained Language Models. pages 5943‚Äì5959.\n11710\nNora Kassner, Philipp Dufter, and Hinrich Sch ¬®utze.\n2021. Multilingual LAMA: investigating knowledge\nin multilingual pretrained language models. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, EACL 2021, Online, April\n19 - 23, 2021 , pages 3250‚Äì3258. Association for\nComputational Linguistics.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers , pages 66‚Äì75.\nAssociation for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for Neural Text Processing.\nEMNLP 2018 - Conference on Empirical Methods in\nNatural Language Processing: System Demonstra-\ntions, Proceedings, pages 66‚Äì71.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nYoav Levine, Noam Wies, Or Sharir, Hofit Bata, and\nAmnon Shashua. 2020. Limits to depth efficiencies\nof self-attention. In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neu-\nral Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual .\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. pages 7871‚Äì7880.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.\n2021. Jurassic-1: Technical details and evaluation.\nTechnical report, AI21 Labs.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. CoRR, abs/2103.10385.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transactions\nof the Association for Computational Linguistics ,\n8:726‚Äì742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? A new dataset for open book question an-\nswering. CoRR, abs/1809.02789.\nTomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.\nExploiting Similarities among Languages for Ma-\nchine Translation.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\narXiv preprint arXiv:2104.08773 .\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James F. Allen. 2016. A corpus\nand evaluation framework for deeper understanding\nof commonsense stories. CoRR, abs/1604.01696.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Association for Computational\nLinguistics (ACL).\nSebastian Nagel. 2016. Cc-news. http:\n//web.archive.org/save/http:\n//commoncrawl.org/2016/10/\nnews-dataset-available.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R Bowman. 2020. Crows-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. arXiv preprint arXiv:2010.00133 .\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. FAIRSEQ : A fast, extensible toolkit for\nsequence modeling. In North American Associa-\ntion for Computational Linguistics (NAACL): System\nDemonstrations.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. CoRR,\nabs/2105.11447.\nFabio Petroni, Tim Rockt ¬®aschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H Miller, and Se-\nbastian Riedel. 2019. Language models as knowledge\nbases? arXiv preprint arXiv:1909.01066 .\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is Multilingual BERT? ACL 2019\n- 57th Annual Meeting of the Association for Compu-\ntational Linguistics, Proceedings of the Conference ,\npages 4996‚Äì5001.\nEdoardo Maria Ponti, Goran Glavas, Olga Majewska,\nQianchu Liu, Ivan Vulic, and Anna Korhonen. 2020a.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 2362‚Äì2376. Association for Computa-\ntional Linguistics.\nEdoardo Maria Ponti, Goran Glava Àás, Olga Majewska,\nQianchu Liu, Ivan Vuli ¬¥c, and Anna Korhonen. 2020b.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\n11711\nProcessing (EMNLP), pages 2362‚Äì2376, Online. As-\nsociation for Computational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186‚Äì\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research\n(JMLR), 21:1‚Äì67.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-\ndhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie\nHu, Dan Garrette, Graham Neubig, and Melvin John-\nson. 2021. XTREME-R: towards more challenging\nand nuanced multilingual evaluation. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 10215‚Äì10245. Association\nfor Computational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020 , pages 8732‚Äì\n8740. AAAI Press.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2021. Multi-\ntask prompted training enables zero-shot task gener-\nalization.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile\nSaulnier, Stas Bekman, M Saiful Bari, Stella Bider-\nman, Hady Elsahar, Jason Phang, Ofir Press, Colin\nRaffel, Victor Sanh, Sheng Shen, Lintang Sutawika,\nJaesung Tae, Zheng Xin Yong, Julien Launay, and\nIz Beltagy. 2022. What language model to train if\nyou have one million GPU hours? In Challenges &\nPerspectives in Creating Large Language Models .\nTimo Schick and Hinrich Sch ¬®utze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, EACL 2021, Online, April 19 - 23, 2021 , pages\n255‚Äì269. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. CoRR, abs/1508.07909.\nJ¬®org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC‚Äô12), pages 2214‚Äì2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nAlexey Tikhonov and Max Ryabinin. 2021. It‚Äôs all in\nthe heads: Using attention heads as a baseline for\ncross-lingual transfer in commonsense reasoning. In\nFindings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021 , volume ACL/IJCNLP 2021 of Findings\nof ACL, pages 3534‚Äì3546. Association for Computa-\ntional Linguistics.\nEleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pas-\ncal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,\nCarles Gelada, Kevin Swersky, Pierre-Antoine Man-\nzagol, and Hugo Larochelle. 2020. Meta-dataset: A\ndataset of datasets for learning to learn from few ex-\namples. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020 . OpenReview.net.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. SuperGLUE: A stick-\nier benchmark for general-purpose language under-\nstanding systems. arXiv preprint 1905.00537 .\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm√°n, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference , pages\n4003‚Äì4012, Marseille, France. European Language\nResources Association.\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin,\nRosanne Liu, Jason Yosinski, and Pascale Fung.\n2021. Language models are few-shot multilingual\nlearners. CoRR, abs/2109.07684.\n11712\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mT5: A massively multilingual\npre-trained text-to-text transformer. pages 483‚Äì498.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual adver-\nsarial dataset for paraphrase identification. CoRR,\nabs/1908.11828.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n4791‚Äì4800. Association for Computational Linguis-\ntics.\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng,\nZhen Bi, Chuanqi Tan, Fei Huang, and Huajun\nChen. 2021. Differentiable prompt makes pre-trained\nlanguage models better few-shot learners. CoRR,\nabs/2108.13161.\nJieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\nKai-Wei Chang, and Ahmed Hassan Awadallah.\n2020. Gender bias in multilingual embeddings and\ncross-lingual transfer. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2896‚Äì2907.\nMengjie Zhao and Hinrich Sch ¬®utze. 2021. Discrete\nand soft prompting for multilingual models. CoRR,\nabs/2109.03630.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference\non Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 12697‚Äì12706. PMLR.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2019. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books. arXiv preprint\narXiv:1506.06724.\n11713\nA Pretraining Details\nXGLM. All models are trained with the Fairseq library (Ott et al., 2019). We use Adam optimizer with\nùõΩ1 = 0.9, ùõΩ2 = 0.98, ùúñ= 1ùëí‚àí8. We adjust the learning rate based on model size, e.g. 1.5ùëí‚àí3 for the\n564M and 1.7B model, 7.5ùëí‚àí4 for the 2.9B model, and 1.2ùëí‚àí4 for the 7.5B models. Learning rates\nwere adjusted with a 2000 warm-up updates followed by a polynomial decay schedule. All models are\ntrained with data parallel and an effective batch size of 4M tokens. The XGLM 7.5B model was trained\non 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second 18.\nGPT-36.7B repl.. We replicate the GPT-36.7B architecture and optimization hyperparameters to the best of\nour knowledge for training this model. The most significant difference between this model and GPT-3 6.7B\nis in the training data. The training data used by GPT-3 6.7B repl. is a combination of six English-language\ndatasets, totaling 453GB and 112B tokens (which we up-sampled to 300B tokens):\n‚Ä¢ BookCorpus (Zhu et al., 2019), a dataset consisting of more than 10K unpublished books (4GB);\n‚Ä¢ English Wikipedia, excluding lists, tables and headers (12GB);\n‚Ä¢ CC-News (Nagel, 2016), a dataset containing 63 millions English news articles crawled between\nSeptember 2016 and February 2019 (76GB);\n‚Ä¢ OpenWebText(Gokaslan and Cohen, 2019), an open source recreation of the WebText dataset used to\ntrain GPT-2 (38GB);\n‚Ä¢ CC-Stories (Trinh and Le, 2018), a dataset containing a subset of CommonCrawl data filtered to match\nthe story-like style of Winograd schemas (31GB);\n‚Ä¢ English CC100 (Wenzek et al., 2020), a dataset extracted from CommonCrawl snapshots between\nJanuary 2018 and December 2018, filtered to match the style of Wikipedia (292GB).\nThe data are encoded using the same Byte-Pair Encoding (BPE) as GPT-2 (Radford et al., 2019) and\nRoBERTa (Liu et al., 2019) with a vocabulary of 50K subword units.\nA.1 Validation Perplexity\nWe use in-domain validation perplexity to validate the convergence status of the models. Figure A1 shows\nthe average perplexity of the four models evaluated using a validation dataset sampled from CC100-XL.\nThe validation data contains 30k sentences for each language that do not overlap with the pre-training\ndata. We group the results by resource level.\n1.0 2.0 3.0 4.0 5.0 6.0 7.0\nmodel params. (B)\n8.0\n10.0\n12.0\n14.0\n16.0\n18.0\n20.0valid ppl\nhi\nmed\nlo\nex-lo\nFigure A1: XGLM perplexity on CC100_XL validation set as a function of model size.\n18On 256 A100 GPUs, the inference speed can reach 1.47 million words per second. Besides, inference can be done with\nsignificantly less resources. For example, using 8 v100 GPUs, it took 6 hrs to evaluate XGLM 7.5B on XStoryCloze.\n11714\nB Multilingual In-context Learning Formulation\nWe extend the in-context learning framework proposed by Brown et al. (2020) to the multilingual setting.\nLet ‚Ñ≥be a causal language model and ùíübe a task. ùíü= (ùí´,‚Ñ∞) consists of a task description ùí´and a\nfew demonstration examples in one or more languages\n‚Ñ∞=\n|‚Ñí|‚ãÉÔ∏Å\nùëô=1\n‚Ñ∞ùëô.\nWe consider the setting where the task description comes in the form of a prompt ùí´= (ùíØ,ùë£). ùíØ is a\ncloze-style template that converts an example input ùë•into a string ùíØ(ùë•) that contains a [Mask]symbol.19\nFor classification and multiple-choice problems, ùë£ : ùí¥‚Üíùí± *is a verbalizer that maps each candidate\nlabel or choice ùë¶ ‚ààùí¥ into a string ùë£(ùë¶). Both ùíØ(ùë•) and ùë£(ùë¶) can be tokenized into a sequence of one\nor more tokens in the language model vocabulary ùí±. An instantiated prompt ùí´(ùë•,ùë¶) is obtained by\nsubstituting the [Mask]symbol in ùíØ(ùë•) with ùë£(ùë¶). Table 2 shows the prompts used by all tasks in our\nmain experiments.\nZero-shot learning. Given a test example Àúùë•ùë° in any target language ùë°, the zero-shot prediction is ^ùë¶which\nmaximizes a language model based scoring function ( ¬ßC.2).\n^ùë¶= arg max\nùë¶\nùúé(‚Ñ≥,ùí´(Àúùë•ùë°,ùë¶)). (1)\nThis general formulation can cover most NLP tasks. For classification problems, ùë£is a mapping from\nclasses to strings; for multiple-choice problems, ùë£is an identity function that maps each candidate choice\nto itself. For text generation problems, ùë£is identity and we decode free-form text from [Mask], which in\nthis case is positioned at the end of ùíØ(ùë•).\nFew-shot learning. Suppose we have ùëòdemonstration examples available in a source language:\n‚Ñ∞ùë† = {(ùë•ùë†\nùëñ ,ùë¶ùëñ)}ùëò\nùëñ=1.\nIn this case, we concatenate the instantiated prompts of the demonstration examples {ùí´(ùë•ùë†\nùëñ ,ùë¶ùëñ)}ùëò\nùëñ=1 and\nmake it the prefix of the input string used in the zero-shot learning setting to form the objective:\n^ùë¶= arg max\nùë¶\nùúé(ùí´(ùë•ùë†\n1,ùë¶1) [Sep] ... ùí´(ùë•ùë†\nùëò,ùë¶ùëò) [Sep] ùí´(Àúùë•ùë°,ùë¶)), (2)\nwhere [Sep] is a separator symbol chosen empirically.\n‚Ä¢ When ùë†= ùë°, we have the in-language few-shot learning setup.\n‚Ä¢ When ùë†Ã∏= ùë°, we have the cross-lingual few-shot learning setup.\nC Evaluation Details\nC.1 English Evaluation Tasks\nTable A1 shows all the English tasks used in our evaluation.\n19We relaxed the prompt format of GPT-3 by allowing the [Mask]symbol to appear anywhere in ùíØ (ùë•) instead of just in\nthe end. Having this additional flexibility leads to better performance on some tasks. This is inspired by the masked language\nmodeling prompts constructed by recent work (Schick and Sch ¬®utze, 2021; Zhang et al., 2021).\n11715\nReasoning\nStoryCloze (Mostafazadeh et al., 2016) ‚Äì 1,871 1,871 N/A 1\nCOPA‚Ä°(Gordon et al., 2012) 400 100 499 N/A 1\nWinoGrande (Sakaguchi et al., 2020) 40,398 1,267 1,767 N/A 1\nHellaSwag (Zellers et al., 2019) 39,905 10,042 10,003 N/A 1\nQA\nARC-easy (Clark et al., 2018) 2,251 570 2,376 N/A 1\nARC-cha. (Clark et al., 2018) 1,119 299 1,172 N/A 1\nPIQA (Bisk et al., 2020) 16,113 1,838 3,084 N/A 1\nOpenbookQA (Mihaylov et al., 2018) 4,957 500 500 N/A 1\nTable A1: English tasks used in our few-shot learning evaluation. All tasks use accuracy as the evaluation metrics.\nC.2 Scoring Functions\nWe considered the following functions for scoring an instantiated prompt using a language model:\n(1) sum of per-token log probabilities;\n(2) average of per-token log probabilities;\n(3) average of per-token log probabilities, ignoring the common prefix of different candidates.\nWe also considered the calibration approach proposed by Zhao et al. (2021) and character normalization\nproposed by Lieber et al. (2021).\nIn the end, we use the average of per-token log-probabilities ignoring the common prefix of different\ncandidates as the scoring function for all multilingual tasks. This is selected based on the development set\nperformance of StoryCloze and XNLI.\nFor English tasks, we use the same modeling choices as Brown et al. (2020). Specifically, we use the\ntask prompts as detailed in Appendix G of Brown et al. (2020), and a single newline as the separator for\nfew-shot learning. For WinoGrande, we take the log-likelihood of the common suffix of the different\ncandidates as the scoring function. For ARC-easy, ARC-challenge and OpenBookQA, we normalize by the\nunconditional probability of each candidate by taking ùëù(completion|context)\nùëù(completion|answer_context) , where we use the string\n‚ÄúAnswer: ‚Äù as answer_context. For all the other tasks, we take the average of per-token log-probabilities,\nignoring the common prefix of the different candidates.\nC.3 Evaluation Protocol\nAll few-shot learning results are obtained with the in-language setting (both the training and test examples\nare in the same language) unless otherwise specified. We report results on the test set for all multilingual\ntasks (including the held-out tasks). For English tasks, we report results on the test set for ARC-easy,\nARC-challenge, OpenBookQA and StoryCloze, and on the development set for the rest, following Brown\net al. (2020). For few-shot learning, we report the average results across 5 runs, randomly sampling\na different set of few-shot examples each time. For tasks with a training set, we sample the few-shot\nexamples from the training set; for tasks with no training set, we sample from the dev set and report\nevaluation results on the test set; for dev-set examples on XNLI and XCOPA, we sample few-shot examples\nfrom the test set, since these two tasks do not have the training sets for all languages. While Brown et al.\n(2020) tuned the few-shot value ùëòas a hyperparameter on the dev set, we pre-selected a few ùëòvalues (0,\n1, 4, 32, 128) and report the corresponding results.\nC.3.1 Example Truncation\nFollowing Brown et al. (2020), we truncate the input such that they fit the maximum context length of\nXGLM (ùëõctx = 2048) and preserve only the complete demonstration examples after truncation. For each\ntask, we report results up to the ùëò‚Äôs corresponding to the maximum fit. 20 Table A2 shows the average\nnumber of demonstration examples that fit the maximum context length of XGLM ( ùëõctx = 2048) for each\ntask in our experiments.\n20XWinograd has only a test split, and we sampled few-shot examples directly from it, following the practice used by Brown\net al. (2020) for evaluating GPT-3 on Winograd. As a result we only report 0-, 1- and 4-shot results for XWinograd to minimize\ninflating the few-shot performance by training and testing on the same examples.\n11716\nXStoryCloze en zh ru es id ar hi sw te eu my\n32.0 31.6 30.6 31.5 32.0 27.6 24.7 29.5 25.2 25.6 18.8\nXCOPA en zh it id th vi tr et ta sw ht qu\n100.0 100.0 98.0 100.0 100.0 99.4 100.0 100.0 75.2 97.9 95.4 84.9\nXWinograd en ru ja pt\n93.7 63.7 62.4 83.1\nXNLI en zh ru es de fr el th vi tr ar bg hi ur sw\n48.3 47.6 43.4 44.7 43.1 39.3 37.8 44.8 39.8 46.8 42.3 41.4 37.4 38.4 42.9\nPAWS-X en zh es ja de fr ko\n34.5 27.2 31.3 23.1 32.0 29.1 28.0\nen\nCOPA 124.3\nWinogrande 84.6\nHellaSwag 21.4\nARC-easy 62.9\nARC-challenge 53.2\nPIQA 59.6\nOpenbookQA 106.4\nTable A2: Average # of few-shot examples that fit the maximum context length of XGLM ( ùëõctx = 2048) in our\nfew-shot evaluation benchmark. The languages are sorted according to the amount of pre-training data (high to low).\nRepresentation Bias. We observe that the language model tend to fit more examples in a high-\nresource language in context compared to those in a low-resource language. 21 English, as the highest\nresourced language (Table A10), always fit the most examples. This reflects the unequal representation\nof different languages in our joint multilingual BPE vocabulary ( ¬ß2.1). With this vocabulary induction\nscheme (Sennrich et al., 2015), the underrepresented languages tend to have smaller sub-word units and\nhigher fertility (defined as number of subwords per linguistic word), making it more challenging to learn\nword- and higher-level semantics for such languages. Other factors can also impact the tokenization\ngranularity. For example, sharing sub-strings with other high resource languages can boost the granularity\nof a language; and some languages have smaller tokenization granularity as a result of their alphabet\nsystem (e.g. Chinese has an average sub-word length of 1.4, indicating the dominance of single-character\ntokens, despite being the third largest language in our pre-training data according to disk size).\nD Additional Results\nD.1 Comparing Multilingual Prompting Approaches on XNLI and XCOPA\nWe compare the performance of English prompts and MT and HT prompts on two of our held-out tasks,\nXNLI and XCOPA, using their development sets. For MT prompts, we translate the English prompts\ninto the target languages using the Google Cloud Translation API. We use the exact prompts as shown\nin Table 2 as the input of the translation API and manually recover the placeholders in the API output\nbased on brackets markers (e.g. ‚Äú {Sentence 1} because [Mask]‚Äù is translated to ‚Äú {Sentence\n1}Âõ†‰∏∫[Mask]‚Äù). When the candidate set is closed, we replace [Mask]with each verbalized label and\ntranslate them separately. For example, ‚Äú {Sentence 1}, right? Yes, {Sentence 2}‚Äù is translated\nto ‚Äú {Sentence 1}ÔºåÂØπÂêóÔºüÊòØÁöÑÔºå{Sentence 2}‚Äù. On XNLI, we also compared to prompts\nmanually translated from English to eliminate the impact of translation noise on the comparison. 22\nAs shown in Table A3 and Table A4, the in-context learning performance is sensitive to the prompting\nchoices across all languages. For both XNLI and XCOPA, using the English prompts on average\nyield significantly better performance than using the machine-translated prompts. For XNLI, human\ntranslated (HT) prompts significantly improve over machine translated (MT) prompts for most languages.\nSurprisingly, the performance of human translated prompts lags behind that of the English prompts in the\n21XStoryCloze, XCOPA, XNLI and PAWS-X all contain parallel examples, which allows us to compare the maximum fit of\nthe same set of examples across different languages.\n22We ask native speakers to translate the English template into zh, es, fr, el, hi, vi, ar and bg. For the rest of the languages, one\nof the authors verified and corrected the machine translated templates using bilingual dictionaries.\n11717\nhi medium low\n# shot prompt en zh ru es de fr el th vi tr ar bg hi ur sw Avg.\n0 En 54.5 45.0 47.2 38.2 42.4 50.7 47.1 46.1 47.5 44.6 47.5 50.0 43.4 42.7 46.2 46.2\nMT 54.5 34.6 40.7 37.5 37.6 47.8 45.4 33.3 35.4 37.1 46.5 49.3 38.8 33.5 44.4 41.1\nHT 54.5 34.1 47.4 50.0 49.5 50.4 47.1 34.9 45.2 46.3 46.0 49.3 37.5 33.7 44.4 44.7\n4 En 51.8 48.0 48.2 45.1 45.2 49.2 48.4 46.3 48.2 45.1 46.4 49.1 46.6 43.5 44.9 47.1\nMT 51.8 39.8 45.1 45.8 43.6 49.4 44.3 33.7 41.9 35.0 45.5 48.6 39.9 35.4 43.5 42.9\nHT 51.8 39.8 50.0 49.9 49.8 45.7 44.0 37.3 41.9 47.0 45.7 48.6 40.2 35.0 43.5 44.7\n32 En 53.4 50.4 40.0 46.4 46.2 46.7 47.3 46.8 48.6 44.3 43.3 42.8 45.6 45.6 46.5 46.3\nMT 53.4 40.7 38.0 47.0 43.1 49.1 48.5 35.0 44.5 35.2 41.9 51.9 37.2 34.8 43.8 42.9\nHT 53.4 43.1 51.8 51.7 49.6 46.2 48.9 38.0 47.0 47.5 43.9 51.9 40.9 34.6 43.8 46.2\nTable A3: Comparison between English prompts, MT (machine-translated) prompts and HT (human-translated)\nprompts for 0, 4 and 32-shot learning on XNLI dev set using XGLM 7.5B.\nhi medium low ex-low\n# shot prompt zh ru it id th vi tr et ta sw ht qu Avg.\n0 En 63.0 64.0 59.0 54.0 53.0 63.0 65.0 63.0 59.0 54.0 57.0 70.0 60.3\nMT 62.0 59.0 69.0 51.0 48.0 64.0 64.0 60.0 59.0 51.0 60.0 69.0 59.7\n4 En 66.8 73.8 66.2 54.6 57.4 68.8 69.8 69.2 60.6 58.2 62.6 62.2 64.2\nMT 68.4 65.4 68.8 54.0 56.6 67.2 67.4 66.6 62.2 60.2 60.8 61.8 63.3\nTable A4: Comparison bewtween English prompts and MT (machine-translated) prompts for 0-shot and 4-shot\nlearning on XCOPA dev set using XGLM 7.5B.\n0-shot and 4-shot settings.\nFurther examination of the per-language performance reveals that the relative strengths of different\nprompting approaches vary across languages. For es and de, HT prompts offer large gains compared to\nthe MT prompts and the English prompts. However, for zh and ur, using translated prompts (either HT or\nMT) significantly hurts the performance. For zh, fr, vi, ar and hi, using native-speaker translated prompts\nstill yields significantly lower performance compared to using the English prompts in at least one setting,\nsuggesting that translation error is not the sole cause of the performance drop.\nD.2 Full Results on Learning from Cross-lingual Demonstrations\nWe evaluated XGLM 7.5B on XNLI in the learning from cross-lingual demonstration setting, using both\nthe same-language-prompting and English-prompting setups. In same-language-prompting, the prompt\nfields and the examples are always in the same language. And in English-prompting, English prompts are\nused for all examples. All few-shot performances in this section are obtained using the ùëò-shot per label\nsetting as described in ¬ßD.4.\nAs shown in Figure A2, for many language pairs transferring from source language demonstration\ncan significantly improve over the zero-shot performance in the target language when human-translated\ntemplates is used. The improvement is especially significant for languages such as Chinese ( zh), Thai\n(th) and Urdu ( ur), whose zero-shot performance is close to random with human translated templates.\nHowever, we found that the effect of cross-lingual transfer from template and cross-lingual transfer from\ndemonstration examples typically do not add up. As shown in Figure A3, using the English template\nsignificantly improves the zero-shot performance of most languages, including Chinese, Thai and Urdu.\nIn this case, the demonstration examples in general do not help unless they are in the same language as the\ntarget example (diagonals).\nFigure A4 shows the results on XCOPA.\nFigure A5 shows the results on XStoryCloze, where we observed almost no improvement for any\nlanguage pair. Possible reasons for the poor transfer results on XStoryCloze is that it requires reasoning\nabout implicit relations between multiple sentences which is much harder to do especially in a cross-lingual\nsetting.\n11718\n(a) Same-language prompting with human-translated templates\n (b) English prompting for all languages\nFigure A2: Cross-lingual few-shot in-context learning on XNLI development set. The leftmost column shows\nthe 0-shot performance ( with human-translated templates ) of each language. The rest of the matrix shows the\ndifference between 4-shot (per label) and 0-shot ( with human-translated templates ) performance. Row: target\nlanguage. Column: source language. The languages are ordered from the highest to the lowest resource level. We\nobserve that using demonstration examples from the source language improves the zero-shot performance in the\ntarget language over a number of language pairs, and the improvement is more significant from higher-resourced\nlanguages to lower-resourced languages.\n(a) Same-language prompting with human-translated templates\n (b) English prompting\nFigure A3: Cross-lingual few-shot in-context learning on XNLI development set. The leftmost column shows the\n0-shot performance ( with English templates ) of each language. The rest of the matrix shows the difference between\n4-shot (per label) and 0-shot ( with English templates) performance. Row: target language. Column: source language.\n11719\n(a) Same language prompting with human-translated templates\n (b) English prompting\nFigure A4: Cross-lingual few-shot in-context learning on XCOPA development set. The leftmost column shows the\n0-shot (with machine translated templates ) performance. The rest of the matrix shows the difference between 4-shot\n(per label) and 0-shot ( with machine translated templates ) performance. Row: target language. Column: source\nlanguage.\nFigure A5: Cross-lingual few-shot in-context learning on XStoryCloze test set. The matrix shows the difference\nbetween 4-shot (per label) and 0-shot performance. For XStoryCloze, there is no difference between same-language\nprompting and English prompting since the task does not use a verbalized template. Row: target language. Column:\nsource language.\n11720\nD.3 Full Results in FLORES-101\nTable A5 reports our full results in FLORES-101.\nen de fr ca fi ru bg zh ko ar sw hi my ta avg\nen\nSupervised ‚Äì 32.6 42.0 31.2 24.2 27.1 37.4 19.3 18.5 17.9 26.9 28.1 3.5 3.4 24.0\nGPT-36.7B ‚Äì 25.9 36.1 23.8 10.2 11.2 5.9 12.5 1.2 1.1 0.5 0.3 0.1 0.0 9.9\nXGLM7.5B ‚Äì 27.6 36.0 34.0 23.3 24.2 33.1 15.6 12.0 11.5 18.0 19.9 11.0 8.5 21.1\nde\nSupervised 35.8 ‚Äì 35.5 25.8 22.6 24.6 31.5 17.2 16.6 14.8 21.0 23.4 2.3 2.3 21.0\nGPT-36.7B 40.4 ‚Äì 26.2 17.2 8.1 9.3 4.8 9.0 1.0 0.9 0.5 0.3 0.1 0.1 9.1\nXGLM7.5B 38.8 ‚Äì 27.9 19.1 20.5 19.7 25.8 12.3 3.4 6.6 11.7 14.3 9.9 4.8 16.5\nfr\nSupervised 37.2 28.5 ‚Äì 28.7 21.9 24.5 32.2 17.6 16.7 15.4 17.2 22.9 2.1 0.8 20.4\nGPT-36.7B 42.8 20.9 ‚Äì 23.7 8.0 9.7 4.6 9.1 1.0 1.0 0.4 0.3 0.1 0.0 9.4\nXGLM7.5B 40.4 20.4 ‚Äì 32.1 19.4 19.8 26.3 10.6 2.4 5.9 14.5 13.7 9.7 6.6 17.1\nca\nSupervised 33.4 24.8 35.1 ‚Äì 19.0 21.1 28.6 15.1 13.9 13.4 18.7 20.5 2.1 2.6 19.1\nGPT-36.7B 40.2 18.6 31.4 ‚Äì 7.0 9.3 4.3 8.0 0.9 0.9 0.3 0.4 0.1 0.1 9.3\nXGLM7.5B 41.1 18.9 33.8 ‚Äì 11.3 3.3 23.9 10.8 1.3 0.8 13.8 6.1 7.9 3.1 13.6\nfi\nSupervised 27.2 23.0 29.3 21.6 ‚Äì 20.6 26.4 16.0 14.8 12.4 14.2 19.8 1.7 0.9 17.5\nGPT-36.7B 25.3 13.5 17.1 10.0 ‚Äì 6.4 2.8 5.7 0.7 0.7 0.3 0.3 0.1 0.0 6.4\nXGLM7.5B 29.2 17.4 22.2 17.0 ‚Äì 16.5 17.5 12.4 7.5 7.6 8.0 10.1 6.2 2.0 13.4\nru\nSupervised 27.5 23.5 30.1 22.0 19.4 ‚Äì 31.0 16.5 15.3 13.5 18.1 20.9 2.2 2.3 18.6\nGPT-36.7B 28.1 14.8 20.4 13.1 5.4 ‚Äì 7.4 1.2 0.2 0.2 0.1 0.2 0.1 0.1 7.0\nXGLM7.5B 30.4 17.9 24.0 14.6 8.0 ‚Äì 26.3 11.6 5.5 7.4 7.1 9.1 7.3 3.1 13.2\nbg\nSupervised 33.0 26.1 33.7 24.9 20.8 26.5 ‚Äì 17.5 16.4 14.5 20.9 23.1 2.3 2.4 20.2\nGPT-36.7B 21.6 11.4 16.0 9.7 4.3 6.5 ‚Äì 1.2 0.2 0.2 0.1 0.2 0.1 0.1 5.5\nXGLM7.5B 35.5 19.2 26.3 12.9 14.2 22.9 ‚Äì 11.9 6.8 9.2 9.4 7.5 3.2 1.0 13.9\nzh\nSupervised 20.9 17.6 24.3 17.4 16.0 17.2 22.1 ‚Äì 15.9 11.6 15.5 18.5 1.9 2.5 15.5\nGPT-36.7B 21.1 9.5 14.3 8.2 4.3 3.6 1.3 ‚Äì 1.1 0.4 0.2 0.2 0.1 0.0 4.9\nXGLM7.5B 20.7 8.3 8.5 10.5 4.4 4.8 14.8 ‚Äì 9.3 4.2 5.6 12.0 8.6 6.2 9.1\nko\nSupervised 20.9 16.7 22.1 16.5 14.9 15.5 21.1 15.7 ‚Äì 10.6 15.1 18.7 1.9 4.0 14.9\nGPT-36.7B 8.3 4.6 6.4 4.4 2.1 1.7 0.8 2.5 ‚Äì 0.2 0.1 0.1 0.1 0.1 2.4\nXGLM7.5B 19.9 10.3 13.7 5.3 1.4 1.2 10.9 11.9 ‚Äì 2.7 3.2 1.0 2.2 1.4 6.5\nar\nSupervised 25.5 18.7 25.7 18.9 15.6 17.8 23.8 13.1 13.3 ‚Äì 15.4 19.4 1.8 0.9 16.1\nGPT-36.7B 10.5 5.3 9.6 6.0 2.2 2.2 0.9 0.9 0.1 ‚Äì 0.1 0.1 0.2 0.0 2.9\nXGLM7.5B 27.7 12.2 17.9 8.8 8.5 9.1 18.4 8.9 0.8 ‚Äì 7.7 7.8 3.4 3.7 10.4\nsw\nSupervised 30.4 19.4 26.7 20.1 15.6 17.6 23.8 13.2 12.2 12.0 ‚Äì 19.2 2.1 4.0 16.6\nGPT-36.7B 5.0 2.9 3.9 2.8 1.7 1.8 1.3 1.3 0.5 0.5 ‚Äì 0.4 0.1 0.1 1.7\nXGLM7.5B 31.6 13.4 21.8 15.4 10.2 13.1 15.2 9.5 6.0 8.9 ‚Äì 7.6 3.4 1.0 12.1\nhi\nSupervised 27.9 19.4 25.9 18.9 15.7 16.9 23.9 13.5 13.9 12.2 16.8 ‚Äì 2.5 3.8 16.2\nGPT-36.7B 1.2 0.9 1.4 0.8 0.4 0.4 0.3 0.2 0.1 0.1 0.1 ‚Äì 0.1 0.2 0.5\nXGLM7.5B 25.2 12.3 15.4 8.8 9.8 11.5 11.3 10.8 8.5 6.1 4.7 ‚Äì 1.5 1.9 9.8\nmy\nSupervised 10.0 6.9 10.4 8.5 6.0 6.7 9.5 5.7 6.1 4.6 7.2 9.1 ‚Äì 2.5 7.2\nGPT-36.7B 0.5 0.3 0.4 0.4 0.2 0.1 0.2 0.0 0.0 0.0 0.1 0.2 ‚Äì 0.1 0.2\nXGLM7.5B 14.1 7.6 10.1 3.8 5.7 7.1 8.9 7.1 6.9 3.6 3.5 8.9 ‚Äì 2.6 6.9\nta\nSupervised 8.3 4.9 6.8 5.8 5.0 4.7 7.0 2.5 2.3 1.1 5.2 6.9 1.2 ‚Äì 4.8\nGPT-36.7B 1.0 0.5 0.8 0.5 0.2 0.3 0.3 0.1 0.2 0.1 0.1 0.2 0.0 ‚Äì 0.3\nXGLM7.5B 16.3 8.4 10.3 5.1 5.2 8.1 7.6 8.1 6.2 5.4 2.8 7.2 0.9 ‚Äì 7.1\navg\nSupervised 26.0 20.2 26.7 20.0 16.7 18.5 24.5 14.1 13.5 11.8 16.3 19.3 2.1 2.5 16.6\nGPT-36.7B 18.9 9.9 14.2 9.3 4.2 4.8 2.7 4.0 0.6 0.5 0.2 0.3 0.1 0.1 5.0\nXGLM7.5B 28.5 14.9 20.6 14.4 10.9 12.4 18.5 10.9 5.9 6.1 8.5 9.7 5.8 3.5 12.2\nTable A5: Machine translation results on FLORES-101 devtest (spBLEU). Source language in rows, target language\nin columns. GPT-3 6.7B and XGLM7.5B use 32 examples from the dev set for few-shot learning. Supervised results\ncorrespond to the M2M-124 615M model from Goyal et al. (2021b). Underline denotes better than supervised, bold\ndenotes best of GPT-3 and XGLM. spBLEU computed using the implementation from Goyal et al. (2021b).\nD.4 Majority Label Bias\nIn the main paper, we define ùëò-shot learning as learning from ùëòunique examples randomly drawn from\nthe entire training population. This setting may lead to skewed few-shot training sets, especially when ùëòis\nsmall. As shown in Table A6, the XNLI task is a three-way classification problem where the model needs\nto judge whether the relationship between a pair of sentences is ‚Äúentailment‚Äù, ‚Äúneurtral‚Äù or ‚Äúcontradiction‚Äù.\nWhile the original XNLI dev set has a uniform class distribution, the few-shot training sets randomly\n11721\n24-shot 48-shot\nSeed E N C E N C\n0 9 7 8 22 11 15\n1 6 11 7 12 16 20\n2 9 7 8 20 11 17\n3 8 11 5 16 16 16\n4 6 4 14 14 16 18\nTable A6: Distribution of XNLI few-shot training sets obtained by randomly sampling from the original dev set. E:\nentailment, N: neutral, C: contradition.\n#\nshots\nhigh medium low\nen zh ru es de fr el th vi tr ar bg hi ur sw Avg Std\n24 rand. 54.0\n(¬±2.3)\n50.0\n(¬±1.9)\n41.5\n(¬±6.1)\n47.1\n(¬±4.0)\n46.1\n(¬±2.1)\n47.7\n(¬±4.1)\n48.8\n(¬±2.2)\n47.8\n(¬±2.7)\n48.8\n(¬±3.5)\n44.8\n(¬±2.0)\n45.4\n(¬±3.7)\n44.4\n(¬±4.8)\n45.8\n(¬±2.4)\n45.6\n(¬±1.4)\n46.4\n(¬±3.0) 47.0 2.9\nunif. 56.0\n(¬±1.4)\n52.1\n(¬±0.8)\n42.9\n(¬±1.2)\n47.6\n(¬±0.8)\n49.2\n(¬±1.0)\n49.2\n(¬±1.6)\n50.9\n(¬±1.6)\n48.0\n(¬±1.2)\n50.2\n(¬±1.5)\n47.5\n(¬±1.0)\n47.3\n(¬±0.8)\n47.0\n(¬±2.2)\n49.4\n(¬±1.2)\n46.6\n(¬±0.9)\n47.4\n(¬±0.8) 48.72.9\nMaxrand. 54.2\n(¬±2.2)\n51.7\n(¬±1.1)\n37.4\n(¬±1.3)\n45.4\n(¬±3.2)\n44.7\n(¬±3.2)\n45.0\n(¬±3.0)\n46.9\n(¬±2.6)\n45.8\n(¬±2.3)\n47.9\n(¬±2.7)\n42.0\n(¬±1.4)\n42.3\n(¬±1.8)\n40.4\n(¬±1.6)\n45.6\n(¬±2.2)\n46.2\n(¬±0.7)\n46.5\n(¬±2.2) 45.5 4.1\nunif. 54.3\n(¬±0.5)\n52.2\n(¬±0.4)\n39.1\n(¬±1.0)\n45.8\n(¬±0.9)\n46.6\n(¬±1.2)\n46.4\n(¬±1.8)\n48.3\n(¬±0.8)\n47.3\n(¬±0.9)\n48.0\n(¬±1.2)\n45.0\n(¬±2.3)\n44.1\n(¬±1.2)\n42.9\n(¬±2.1)\n46.7\n(¬±0.0)\n46.3\n(¬±0.0)\n47.2\n(¬±0.7) 46.7 3.5\nTable A7: XGLM 7.5B in-language few-shot learning performance of on XNLI dev set using training sets of\nuniform class distribution and randomly sampled class distribution. We report the mean and standard deviation (in\nparentheses) of 5 different training sets sampled via different random seeds for each sampling strategy.\nsampled23 from it often has a much more skewed class distribution.\nFor a |ùí¥|-way classification task, a skewed training set distribution can cause the model to score the\nmajority class as disproportionately more likely than the other classes. This was shown by Zhao et al.\n(2021) as the majority label bias problem. As a result, previous work such as Zhao and Sch ¬®utze (2021)\nadopts a ùëò-shot per class setting, where ùëòunique examples are randomly drawn from each class to form a\ntraining set of size ùëò√ó|ùí¥|.\nWe compare learning from a uniform class distribution (randomly sampling ùëòexamples per class) to\nlearning from a more skewed distribution (randomly sampling ùëò√ó|‚Ñí|examples from the total population)\non the XNLI task. We use the 24-shot and maximum fit (truncated 48-shot) settings. As shown in\nTable A7, for both settings, learning from a uniform class distribution leads to significantly higher\naccuracy in all languages compared to learning from the skewed distributions. de, tr, bg, hi suffer the\nmost learning from the skewed distributions ( >2 absolute accuracy gap in the 24-shot setting), while\nes suffers the least. Moreover, the variances among few-shot trials using different random seeds shrink\nconsiderably when the training set class distribution is uniform. These results highlight the severeness of\nthe majority label bias issue in the multilingual in-context learning framework.\nD.5 Knowledge Probing\nWe evaluate to what extent our multilingual language model can effectively store factual knowledge in\ndifferent languages. To this end, we evaluate knowledge triplet completion using the mLAMA dataset\n(Kassner et al., 2021), which was translated from the English benchmark LAMA (Petroni et al., 2019)\nusing Google Translate. The data is from TREx (Elsahar et al., 2018) with triples of the format ‚ü®object,\nrelation, subject‚ü©. Following the convention of LAMA, triples are converted to templates for querying the\nlanguage model. For example, a triple like ‚ü®Paris, capital-of, France ‚ü©is converted to template ‚ÄúParis is the\ncapital of [MASK]\". While each query in the original mLAMA dataset contains hundreds of candidates\non average, we restrict it to three candidates one of which is the ground truth candidate and the other two\ncandidates are randomly sampled to ensure fast inference and save API cost. Following the evaluation\nprotocol of mLAMA, we report precision @1 averaged over all relations per language.\n23We implement our random sampling procedure using the numpy .random.choicefunction: https://numpy .org/\ndoc/stable/reference/random/generated/numpy .random.choice.html.\n11722\nprec@1\n0.4\n0.5\n0.6\n0.7\n0.8\nen id pt it ca es fi de fr vi et th tr bg ko ru el ja ar zh eu ur bn hi ta\nMultilingual, 7.5B GPT-3 Curie\nFigure A6: Knowledge probing on 25 languages. The performance of a random baseline is 0.33 since we down-\nsampled the candidate set of each query to contain three candidates.\nWe evaluate on the 25 languages covered in XGLM‚Äôs pre-training data. We compare to the GPT-3 6.7B\nmodel. As shown in Figure A6, both our multilingual model and GPT-3 Curie perform well on English.\nFor non-English languages, our multilingual model maintains performance (above 0.6) while GPT-3\nCurie drops drastically especially for medium and low resource languages. Overall, compared to an\nEnglish-centric language model, our multilingual language model are better at retaining factual knowledge\non a wider range of languages with +7.1 points on average.\nE Safety and Bias Analysis\nGiven the centrality of large scale Language models, it is important to ensure such powerful models are\nused responsibly. Accordingly, we further examine XGLM‚Äôs behavior on two tasks:\n‚Ä¢ Hate speech detection: A safety task to test language models‚Äô ability to identify hateful and offensive\ntext;\n‚Ä¢ Occupation Identification: A bias task to study language models‚Äô performance disparity between\ndifferent gender groups on the task of occupation identification.\nThrough extensive experiments, we have following findings: First, hate speech detection in an in-context\nlearning setting is quite challenging. Moreover, language models are not effectively leveraging few-\nshot examples to improve the performance. Second, although language models have relatively good\nperformance on the occupation identification task, they run the risk of exhibiting strong gender bias for\ncertain occupations.\nE.1 Hate Speech Detection\nE.1.1 Setup\nDatasets. We adopt datasets introduced by Huang et al. (2020) that include hate speech data from\nTwitter in five languages: English, Italian, Portuguese, Polish and Spanish. All hyperlinks, usernames\nand hashtags are replaced with generic symbols ( URL, USER, HASHTAG) to anonymize user information.\nWe remove tweets containing more than 2 generic symbols to encourage more informative examples.\nWe further filter out tweets of length less than 5 tokens or more than 30 tokens. In the spirit of creating\nbalanced data, we randomly sample 500 each positive (hate speech) negative (not hate speech) examples\nfor each language. For further comparison, we translate non-English data into English by using Google\nTranslate and then evaluate English models performance on the task.\nPrompts. We evaluate two approaches to prompting, similar to Section ??. For English prompts,\nwe prefix ‚ÄúThe sentence is <candidate>‚Äù to the input sentence to form a prompt. We consider 10\nverbalization candidates including 5 negative (normal., common., ok., usual., acceptable. ) corresponding\nto classification of not hate speech and 5 positive (sexist., racist., offensive., abusive., hateful. ) representing\nclassification of hate speech. For code-switched prompt, we translate the English prefix and candidates\ninto the corresponding target language by using Google Translate. For example, ‚ÄúThe sentence is normal‚Äù\nis translated into ‚ÄúQuesta frase √® normale.‚Äù for Spanish. For few-shot learning, we randomly draw\nexamples from the training data and report the average performance across 5 runs.\n11723\nModel language condition # shot English Italian Portuguese Polish Spanish\nAccuracy\nGPT-36.7Brepl. code switched 0 54.5 54.3 57.0 51.3 52.5\n4 55.5* 53.4 48.5 52.5 52.2\nGPT-36.7B code switched 0 59.2 61.8 55.8 58.7 55.6\n4 53.6 54.8 53.2 54.5 53.7\nXGLM7.5B code switched 0 56.0 60.4 50.8 47.0 53.1\n4 50.4 50.7 56.8 51.0 50.7\nGPT-36.7Brepl. same language 0 - 50.5 60.2 50.1 52.4\n4 - 51.0 47.2 50.9 50.7\nXGLM7.5B same language 0 - 57.5 41.8 50.0 53.1\n4 - 53.2 56.5 51.1 52.7\nGPT-36.7Brepl. English 0 - 55.8 57.5 62.8* 55.0\n4 - 52.5 49.2 53.9 52.8\nRecall\nGPT-36.7Brepl. code switched 0 57.0 90.2 67.0 21.6 77.0\n4 73.0* 76.7* 72.5 65.4 77.1*\nGPT-36.7B code switched 0 62.4 88.6 51.5 49.0 76.4\n4 65.7 69.2 59.6 58.5 61.1\nXGLM7.5B code switched 0 77.2* 95.4* 80.4 53.8 95.8*\n4 14.9 18.8 18.8 15.8 19.5\nGPT-36.7Brepl. same language 0 - 1.4 9.1 0.2 11.6\n4 - 50.0 66.1 77.8 33.4\nXGLM7.5B same language 0 - 87.4 39.5 0.0 79.4\n4 - 39.0 24.9 55.8 44.6\nGPT-36.7Brepl. English 0 - 93.8 77.8 74.8* 77.6\n4 - 72.4 71.2 75.7 73.7\nTable A8: Accuracy and recall scores of our multilingual model and other English models on the Hate Speech\nDetection task. We evaluate five target languages. For each target language, we bold the highest number for zero-shot\nand four-shot respectively. * indicates the number is significantly higher than others. For language condition, we\nconsider three cases: ‚Äúcode switched‚Äù means the prefix, candidates are in English and tweets are in the target\nlanguage; ‚Äúsame language‚Äù means prefix, candidates and tweets are in the target language; ‚ÄúEnglish‚Äù means prefix,\ncandidates and tweets are in English, i.e. note that, ‚Äúsame language\" and ‚ÄúEnglish\" reduce to the same experimental\ncondition when the target language is English.\nMetrics. We compute precision, recall and accuracy for all experimental conditions. Since the test data\nis balanced, the accuracy of a random baseline is 50%.\nE.1.2 Results\nWe show accuracy and recall scores in Table A8. Bolded results are the highest in the table and those with\nan (*) are statistically significantly better than other comparable conditions. Hate speech detection is a\nchallenging task for all models. We observe that across the five languages, in-context learning results are\nonly slightly better than random ( 50%). The results are also unstable and sensitive to prompt changing.\nOverall, the XGLM 7.5B model has better recall compared to the English-centric models. For example, the\nXGLM6.7B En-only model has very low recall score in the zero-shot setting with the language condition\nset as ‚Äúsame language‚Äù, indicating that it blindly predicts almost everything as negative (not hate speech).\nAnother interesting observation is that most few-shot results are worse than zero-shot, which indicates that\nwith the prefix described above, language models are not able to utilize examples. Interestingly, we also\nfind that in one-shot experiments models tend to copy the label of the given example instead of predicting\nbased on the input tweet. This further demonstrates that language models are struggling with learning\nfrom few-shot examples in this task.\n11724\nEnglish Spanish French\nModel Avg. ‚Üë |Diff|‚Üì Avg.‚Üë |Diff|‚Üì Avg.‚Üë |Diff|‚Üì\nXGLM6.7BEn-only 90.73 3.19 91.23 2.65 83.46 4.85\nGPT-36.7B 90.42 3.53 86.91 5.18 90.85 2.30\nXGLM7.5B 86.49 2.83 82.93 4.28 76.55 2.95\nXGLM6.7BEn-only+translate - 3.19 91.18 3.75 90.07 2.35\nTable A9: Accuracy and bias scores of our multilingual model and other English models on the occupation\nidentification task. ‚Äú|Diff|‚Äù stands for the average absolute accuracy gap between male and female groups aggregated\nacross all occupations. We bold the highest accuracy score for each language.\nE.2 Gender Bias in Occupation Identification\nE.2.1 Setup\nDatasets We use the English bio dataset introduced in (De-Arteaga et al., 2019) to study gender bias\nbased on identifying a person‚Äôs occupation from their bios. For multilingual bio datasets we use those\ncreated by (Zhao et al., 2020). Originally there are 28 occupations in English, 69 occupations in Spanish\nand 27 occupations in French. To ensure we have plenty of test data for each occupation, we only keep\noccupations with at least 1000 male examples and 1000 female examples. This leads to 16 occupations in\nEnglish, 6 occupations in Spanish and 4 occupations in French. We follow the setup in (Zhao et al., 2020)\nwhere people‚Äôs names and pronouns are removed from the bios. We then prefix ‚ÄúThe occupation of this\nperson is <candidate>‚Äù to the input bio to form a prompt. The candidate set consists of five occupations,\nincluding the ground truth one and four other randomly sampled male and female occupations (two male\nand two female). Male (female) occupations refer to ones having predominantly more male (female)\nsamples.\nMetrics Similar to the metric for Hate Speech detection, we first obtain the scores for 5 candidates and\nconsider a prediction correct if the ground truth candidate yields the highest score among five candidates.\nWe then compute the bias score as the absolute gap between the accuracy scores on the male and female\nsamples,24 averaged across all occupations. A lower bias score indicates that a model has less divergence\nin identifying occupations for men and women.\nE.2.2 Results\nWe present the overall accuracy scores and the bias scores (|Diff|) in Table A9. Results indicate that the\nXGLM 6.7B En-only model achieves the best performance on English and Spanish, while the GPT-3 6.7B\nmodel achieves the best performance on French. XGLM 7.5B model, instead, falls behind on all three\nlanguages, especially for Spanish and French. We think this is potentially due to that all pronouns and\npeople‚Äôs names are removed from the test data but not training data. The training data for XGLM 7.5B\ncontains more Spanish and French compared to the other two models. Thus, XGLM 7.5B may have more\nsevere morphological mismatch on Spanish and English. Regarding the bias score, the GPT-3 6.7B model\nis the most biased model on both English and Spanish but least biased on French. XGLM 6.7B En-only\nand XGLM 7.5B exhibit the least bias on Spanish and English, respectively.\nF Data Card\nWe follow the recommendations of Gebru et al. (2018) and provide a datacard for the dataset used to train\nXGLM, which is a subset of CC100-XL, a larger multilingual dataset we curated.\nF.1 Data Sources\nFollowing the recent success of multilingual self-supervised pre-training (Devlin et al., 2019; Lample and\nConneau, 2019; Conneau et al., 2020; Xue et al., 2020; Goyal et al., 2021a; Liu et al., 2020), we train\nour language models on a mixture of monolingual text of different languages. We extend the pipeline\nused for mining the CC100 corpus (Conneau et al., 2020; Wenzek et al., 2020) to generate CC100-XL,\n24We only consider gaps that are statistically significant.\n11725\na significantly larger multilingual dataset covering 68 Common Crawl (CC) snapshots (from Summer\n2013 to March/April 2020) and 134 languages. As the first step to balance the language distribution, we\nsampled 30% of the data from the languages that contain more than 15 billion tokens and more than 20\nmillion documents. This resulted in a 8.4 TB multilingual corpus with 1.9 trillion tokens.\nF.2 Motivation\n‚Ä¢ For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description. The CC100-XL dataset\nwas collected to create a high quality monolingual dataset for at least 100 languages. It was mainly\nused for training foundation multilingual language models which may be applied to a broad list of\nlanguage tasks, including neural machine translation, speech translation, question answering, etc.\nCC100-XL involves sentence level filtering, preserves context, improves the filtering mechanism,\nand paves a way for mining 200+ languages.\n‚Ä¢ Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)? Meta AI.\n‚Ä¢ Who funded the creation of the dataset? If there is an associated grant, please provide the\nname of the grantor and the grant name and number. Meta AI.\n‚Ä¢ Any other comments? No.\nF.3 Composition\n‚Ä¢ What do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and\ninteractions between them; nodes and edges)? Please provide a description. The instances are\ntextual documents sampled from Commoncrawl snapshots.\n‚Ä¢ How many instances are there in total (of each type, if appropriate)? The training dataset of\nXGLM contains 1.74 billion documents in total.\n‚Ä¢ Does the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample\nrepresentative of the larger set (e.g., geographic coverage)? If so, please describe how this\nrepresentativeness was validated/verified. If it is not representative of the larger set, please\ndescribe why not (e.g., to cover a more diverse range of instances, because instances were\nwithheld or unavailable). The dataset is a subset of CC100-XL. For each language, the data is\neither a full set or a random subset of CC100-XL data. Especially, the medium- and low-resource\nlanguages are upsampled. In terms of language representation, the CC100-XL dataset contains 134\nlanguages extracted using fasttext 25 from Common Crawl snapshots. We further selected a subset\nof 30 languages to train XGLM, taking geo-location, language family and typology diversity of the\nlanguages into account.\n‚Ä¢ What data does each instance consist of? ‚ÄúRaw‚Äù data (e.g., unprocessed text or images) or\nfeatures? In either case, please provide a description. Each instance consists of raw text data.\n‚Ä¢ Is there a label or target associated with each instance? If so, please provide a description. No.\n‚Ä¢ Is any information missing from individual instances? If so, please provide a description,\nexplaining why this information is missing (e.g., because it was unavailable). This does not\ninclude intentionally removed information, but might include, e.g., redacted text. No.\n25https://fasttext.cc/docs/en/language-identification.html\n11726\nISO code Language Tokens (M) Size (GiB) ISO code Language Tokens (M) Size (GiB)\nen English 803,527 3,324.45 - Arabic Romanized 685 1.65\nru Russian 147,792 1,007.38 mn Mongolian 681 4.26\nzh Chinese 132,770 485.32 la Latin 635 2.20\nde German 89,224 369.30 ne Nepali 600 5.32\nes Spanish 87,303 363.83 si Sinhalese 524 3.96\nfr French 77,420 303.76 mr Marathi 458 3.59\nja Japanese 66,054 293.39 kn Kannada 446 3.41\nit Italian 41,930 170.76 so Somali 436 1.56\npt Portuguese 36,586 147.12 cy Welsh 398 1.27\nel Greek 28,762 180.37 jv Javanese 389 1.23\nro Romanian 24,176 93.63 ps Pashto 387 1.97\nuk Ukrainian 23,723 156.68 uz Uzbek 332 1.64\nhu Hungarian 22,718 89.87 gu Gujarati 327 2.10\nko Korean 20,002 79.08 km Khmer 272 2.14\npl Polish 19,293 73.59 - Urdu Romanized 245 0.73\nno Norwegian 17,600 70.89 am Amharic 169 0.85\nnl Dutch 17,163 68.36 - Bengali Romanized 166 0.48\nfi Finnish 16,804 67.28 pa Punjabi 153 0.93\nda Danish 16,274 64.74 gl Galician 137 0.50\nid Indonesian 15,424 67.51 ha Hausa 124 0.42\nhr Croatian 14,455 54.27 mg Malagasy 116 0.38\ntr Turkish 12,413 51.51 sa Sanskrit 107 0.42\nar Arabic 12,249 64.34 eu Basque 105 0.35\nvi Vietnamese 11,199 50.45 my Burmese 101 0.74\nth Thai 10,842 99.86 su Sundanese 91 0.30\nbg Bulgarian 9,704 61.10 or Oriya 91 0.62\nfa Persian 9,355 57.46 ht Haitian 87 0.28\nsv Swedish 9,169 36.54 lo Lao 84 0.59\nms Malay 9,106 38.57 ky Kyrgyz 70 0.34\nhe Hebrew 8,637 42.13 br Breton 57 0.16\ncs Czech 8,616 32.46 ga Irish 49 0.15\nsk Slovak 8,251 30.70 yo Yoruba 48 0.14\nca Catalan 7,076 26.90 eo Esperanto 47 0.14\nlt Lithuanian 4,847 18.38 - Tamil Romanized 40 0.13\nsl Slovene 4,029 14.97 zu Zulu 40 0.14\nhi Hindi 3,448 26.63 ti Tigrinya 40 0.19\net Estonian 3,287 12.18 - Telugu Romanized 37 0.11\nlv Latvian 2,815 10.67 ku Kurdish 36 0.10\ntl Tagalog 2,389 8.13 om Oromo 27 0.09\nsq Albanian 2,382 8.76 xh Xhosa 26 0.09\nsr Serbian 2,101 12.68 gd Scottish Gaelic 19 0.05\n- Hindi Romanized 2,045 6.60 ig Igbo 18 0.06\naz Azerbaijani 1,904 8.41 as Assamese 17 0.10\nbn Bengali 1,627 11.19 lg Ganda 15 0.05\nta Tamil 1,477 12.36 wo Wolof 14 0.03\nur Urdu 1,352 7.77 fy Western Frisian 12 0.04\nkk Kazakh 1,278 8.40 tn Tswana 11 0.03\nhy Armenian 1,261 7.16 ff Fula 11 0.03\nka Georgian 1,261 10.48 gn Guaran√≠ 10 0.03\nis Icelandic 1,163 4.21 sd Sindhi 8 0.04\nbe Belarusian 1,004 5.81 ln Lingala 7 0.02\nbs Bosnian 950 4.00 bm Bambara 6 0.02\nml Malayalam 935 8.08 iu Inuktitut 6 0.03\nmk Macedonian 927 6.05 kg Kongo 4 0.01\nsw Swahili 908 3.19 qu Quechua 3 0.01\naf Afrikaans 819 3.04 ss Swati 2 0.01\nte Telugu 689 5.28 - Unassigned 503 2.30\nTable A10: Languages and statistics of the training data set selected from CC100 XL corpus.\n11727\n‚Ä¢ Are relationships between individual instances made explicit (e.g., users‚Äô movie ratings, social\nnetwork links)? If so, please describe how these relationships are made explicit. A small\npercentage of document instances (<2%) are duplicated. Other than that, there are no relationships\nbetween individual instances.\n‚Ä¢ Are there recommended data splits (e.g., training, development/validation, testing)? If so,\nplease provide a description of these splits, explaining the rationale behind them. This dataset\nis split into training and validation only. For each high resource language, at least 5,000 randomly\nselected documents and 30,000 lines were split into validation set, and the rest documents are for\ntraining; for low-resource languages, at least 100 randomly selected documents and 1,000 lines (a\ncouple of very low resource languages contain 80 documents) were split into valid set and leave the\nrest for training. There are 3.5 million lines of text in total for the validation set. This split is mainly\nto ensure a good size of validation data with the coverage and balance over all languages, meanwhile,\nthe validation size is not too large to affect the overall training speed.\n‚Ä¢ Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide\na description. 10% of Russian sample were lost during internal data transferring. Therefore, we\nended up taking 26.7% random subset of the whole Russian data from CC100-XL.\n‚Ä¢ Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)? It‚Äôs self-contained.\n‚Ä¢ Does the dataset contain data that might be considered confidential (e.g., data that is protected\nby legal privilege or by doctor-patient confidentiality, data that includes the content of individ-\nuals‚Äô non-public communications)? If so, please provide a description. CC100-XL is exclusively\nextracted from Common Crawl; and the information in it is not considered confidential.\n‚Ä¢ Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety? If so, please describe why. CC100-XL is a subset of public\nCommon Crawl data, which could contain sentences that, if viewed directly, might be offensive,\ninsulting, threatening, or might otherwise cause anxiety.\n‚Ä¢ Does the dataset relate to people? If not, you may skip the remaining questions in this section.\nSome documents of this data relate to people, such as news articles, Wikipedia descriptions, etc.\n‚Ä¢ Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how\nthese subpopulations are identified and provide a description of their respective distributions\nwithin the dataset. No.\n‚Ä¢ Is it possible to identify individuals (i.e., one or more natural persons), either directly or\nindirectly (i.e., in combination with other data) from the dataset? If so, please describe how\nOther than the individuals who are celebrities, politicians, etc, and have their Wikipedia pages; it\nis possible to identify other individuals by their names, twitter account names, etc. But we built\npersonally identifiable information (PII) identification tools following the guidelines of General Data\nProtection Regulation (GDPR) and National Institute of Standards and Technology (NIST) and run\nagainst this dataset, we did not found highly sensitive PII information, such as U.S. social security\nnumber, login credentials, etc.\n‚Ä¢ Does the dataset contain data that might be considered sensitive in any way (e.g., data that\nreveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or\nunion memberships, or locations; financial or health data; biometric or genetic data; forms\nof government identification, such as social security numbers; criminal history)? If so, please\nprovide a description. We use a curated special word list of 100 languages which covers profanities,\nhate speech, bulling language, common slangs and profane multi-word expressions (MWEs) to tag\nparagraphs and remove the docs containing them. Given the size of this data, it could still contain\n11728\nsuch sensitive information (as the above lists may not be exhaustive) but should be a very small\npercent of instances.\n‚Ä¢ Any other comments? No\nF.4 Collection Process\n‚Ä¢ How was the data associated with each instance acquired? Was the data directly observable\n(e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly\ninferred/ derived from other data (e.g., part-of-speech tags, model-based guesses for age or\nlanguage)? If data was reported by subjects or indirectly inferred/derived from other data,\nwas the data validated/verified? If so, please describe how. Please refer to the main document for\ndetails.\n‚Ä¢ What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or\nsensor, manual human curation, software program, software API)? How were these mechanisms\nor procedures validated? Please refer to the main document for details.\n‚Ä¢ If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,\nprobabilistic with specific sampling probabilities)? Please refer to the main document for details.\n‚Ä¢ Who was involved in the data collection process (e.g., students, crowdworkers, contractors)\nand how were they compensated (e.g., how much were crowdworkers paid)? This data is mined,\nfiltered and sampled by machines.\n‚Ä¢ Over what timeframe was the data collected? Does this timeframe match the creation timeframe\nof the data associated with the instances (e.g., recent crawl of old news articles)? If not, please\ndescribe the timeframe in which the data associated with the instances was created. The data\nwas collected from 68 Common Crawl (CC) snapshots (from Summer 2013 to March/April 2020).\nTherefore, it does not contain a lot of information about recent events such as COVID-19.\n‚Ä¢ Were any ethical review processes conducted (e.g., by an institutional review board)? If so,\nplease provide a description of these review processes, including the outcomes, as well as a link\nor other access point to any supporting documentation. No.\n‚Ä¢ Does the dataset relate to people? If not, you may skip the remainder of the questions in this\nsection. No.\n‚Ä¢ Did you collect the data from the individuals in question directly, or obtain it via third parties\nor other sources (e.g., websites)? N/A\n‚Ä¢ Were the individuals in question notified about the data collection? If so, please describe (or\nshow with screenshots or other information) how notice was provided, and provide a link or\nother access point to, or otherwise reproduce, the exact language of the notification itself. N/A\n‚Ä¢ Did the individuals in question consent to the collection and use of their data? If so, please\ndescribe (or show with screenshots or other information) how consent was requested and\nprovided, and provide a link or other access point to, or otherwise reproduce, the exact\nlanguage to which the individuals consented. N/A\n‚Ä¢ If consent was obtained, were the consenting individuals provided with a mechanism to revoke\ntheir consent in the future or for certain uses? If so, please provide a description, as well as a\nlink or other access point to the mechanism (if appropriate). N/A\n‚Ä¢ Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data\nprotection impact analysis) been conducted? If so, please provide a description of this analysis,\nincluding the outcomes, as well as a link or other access point to any supporting documentation.\nSome responsible AI related evaluations were performed. Please refer to the main document.\n11729\n‚Ä¢ Any other comments? No\nF.5 Preprocessing/cleaning/labeling\n‚Ä¢ Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing,\ntokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing\nof missing values)? If so, please provide a description. If not, you may skip the remainder of\nthe questions in this section. Yes, the detailed steps are as below:\n‚Äì Downloading and Sharding Commoncrawl Snapshots We downloaded 68 Commoncrawl snap-\nshots and divided the data in 240 shards based on web-domain. At this stage, textual data gets\nextracted from the WET files provided by Common Crawl which involves cleaning excessive\ntabs and newlines.\n‚Äì Language Identification (LID) at Document Level For this stage, we used the fastText language\nidentification (LID) model on the entire document which helped further divide the data by\nlanguage. In addition to the original languages supported by fastText, we also added support for\n28 romanized languages. In total, the data for each language contains 240 shards.\n‚Äì Deduplicating Documents based on URL We aggregated the data based on URL which yields\n60% reduction in volume. In case two documents had the same URL, we selected the document\nhaving more recent text content.\n‚Äì Document Splitting and LID at Paragraph Level We segmented the documents based on newline\nand also stored the information about the order in which the paragraphs were appearing in the\noriginal document (i.e. seq_num). Next, we performed LID at the paragraph level again in\norder to divide the original documents into clusters of paragraphs where each cluster represents\nsentences belonging to a particular language.\n‚Äì Deduplicating Paragraphs Data extracted from Commoncrawl snapshots still have a lot of\nduplicate text even if the document is different. In order to tackle this, we applied the normal-\nization function from CCNet (Wenzek et al., 2020) and then computed a SHA-1 hash of the\nnormalized text. This helped in reducing the content by 88%. Choosing which <paragraph,\nurl> combination to keep can be tricky as it can lead to a lot of fragmented documents. So we\ndevised a strategy to choose documents based on sorted <url, seq_num> order which would\nhelp in preventing fragmentation as much as possible.\n‚Äì Language Model Scores We scored every paragraph using a Language Model trained on data\ncollected from OPUS (Tiedemann, 2012) (monolingual data collected from the availble bitexts)\nusing a 4-gram KenLM (Heafield, 2011). Note that since the LMs were not trained on data\nbelonging to a specific domain, this feature helped in eliminating general non-fluent sentences.\n‚Äì Heuristic based approaches We use the following techniques to further refine the filtering step\n(especially useful for Low resource languages having no or poor quality LM)\n* Ratio of digit+punctuation to total characters (current threshold <0.25)\n* Maximum number of URLs per sentence (current value 1)\n* Type-token ratio (current threshold >0.6 + removing bottom 1% per language)\n* Minimum number of tokens per sentence (current value 3; not applied for agglutinative\nlanguages)\n‚Äì Tagging profane words and removing instances containing such words\n‚Ä¢ Was the ‚Äúraw‚Äù data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support\nunanticipated future uses)? If so, please provide a link or other access point to the ‚Äúraw‚Äù data.\nThe ‚Äúraw‚Äù data is publiclly available in in https://commoncrawl.org/the-data.\n‚Ä¢ Is the software used to preprocess/clean/label the instances available? If so, please provide a\nlink or other access point. The software is proprietary to Meta Platforms and currently unavailable\npublicly.\n‚Ä¢ Any other comments? No\n11730\nF.6 Uses\n‚Ä¢ Has the dataset been used for any tasks already? If so, please provide a description. Yes, this\ndataset and its precursor CC100 data have been used to train machine translations and multilingual\nlanguage models, which are foundation to many downstream language tasks.\n‚Ä¢ Is there a repository that links to any or all papers or systems that use the dataset? If so, please\nprovide a link or other access point. No.\n‚Ä¢ What (other) tasks could the dataset be used for? This data can be used to pretrain multilingual\nlanguage models, which are foundation to many current and future language tasks.\n‚Ä¢ Is there anything about the composition of the dataset or the way it was collected and pre-\nprocessed/cleaned/labeled that might impact future uses? For example, is there anything\nthat a future user might need to know to avoid uses that could result in unfair treatment of\nindividuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms\n(e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future\nuser could do to mitigate these undesirable harms? The pipeline for creating this dataset paves a\nway for building a scalable infrastructure for mining datasets to be be used for training large-scale\nmodels.\n‚Ä¢ Are there tasks for which the dataset should not be used? If so, please provide a description.\nNo.\n‚Ä¢ Any other comments? No.\nF.7 Distribution\n‚Ä¢ Will the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created? If so, please provide a description.\nNo.\n‚Ä¢ How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the\ndataset have a digital object identifier (DOI)? N/A\n‚Ä¢ When will the dataset be distributed? No.\n‚Ä¢ Will the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and\nprovide a link or other access point to, or otherwise reproduce, any relevant licensing terms or\nToU, as well as any fees associated with these restrictions. No.\n‚Ä¢ Have any third parties imposed IP-based or other restrictions on the data associated with the\ninstances? If so, please describe these restrictions, and provide a link or other access point to,\nor otherwise reproduce, any relevant licensing terms, as well as any fees associated with these\nrestrictions. No.\n‚Ä¢ Do any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances? If so, please describe these restrictions, and provide a link or other access point to,\nor otherwise reproduce, any supporting documentation. N/A\n‚Ä¢ Any other comments? No.\nF.8 Maintenance\n‚Ä¢ Who is supporting/hosting/maintaining the dataset? Meta AI.\n‚Ä¢ How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Refer to\nthe main document.\n11731\n‚Ä¢ Is there an erratum? If so, please provide a link or other access point. Currently no.\n‚Ä¢ Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete in-\nstances)? If so, please describe how often, by whom, and how updates will be communicated to\nusers (e.g., mailing list, GitHub)? No plan for updating.\n‚Ä¢ If the dataset relates to people, are there applicable limits on the retention of the data associated\nwith the instances (e.g., were individuals in question told that their data would be retained for\na fixed period of time and then deleted)? If so, please describe these limits and explain how\nthey will be enforced. N/A\n‚Ä¢ Will older versions of the dataset continue to be supported/hosted/maintained? If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users. N/A\n‚Ä¢ If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so? If so, please provide a description. Will these contributions be validated/verified?\nIf so, please describe how. If not, why not? Is there a process for communicating/ distributing\nthese contributions to other users? If so, please provide a description. No.\n‚Ä¢ Any other comments? No.\n11732",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5216456055641174
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5187414288520813
    },
    {
      "name": "Natural language processing",
      "score": 0.4733816683292389
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39297932386398315
    },
    {
      "name": "Art history",
      "score": 0.3212316632270813
    },
    {
      "name": "Art",
      "score": 0.28497499227523804
    },
    {
      "name": "Cartography",
      "score": 0.17058905959129333
    },
    {
      "name": "Geography",
      "score": 0.12567594647407532
    }
  ],
  "institutions": []
}