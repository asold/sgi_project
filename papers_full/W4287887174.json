{
    "title": "GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers",
    "url": "https://openalex.org/W4287887174",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100588790",
            "name": "Ali Modarressi",
            "affiliations": [
                "Iran University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5031986521",
            "name": "Mohsen Fayyaz",
            "affiliations": [
                "University of Tehran"
            ]
        },
        {
            "id": "https://openalex.org/A5031030600",
            "name": "Yadollah Yaghoobzadeh",
            "affiliations": [
                "University of Tehran"
            ]
        },
        {
            "id": "https://openalex.org/A5091017313",
            "name": "Mohammad Taher Pilehvar",
            "affiliations": [
                "Khatam University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2970726176",
        "https://openalex.org/W3101163004",
        "https://openalex.org/W2552396116",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W3175752238",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W4205424278",
        "https://openalex.org/W3120835175",
        "https://openalex.org/W3173380736",
        "https://openalex.org/W3152409010",
        "https://openalex.org/W2977944219",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3092292656",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W3200704197",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2948771346",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3173902720",
        "https://openalex.org/W3153147196",
        "https://openalex.org/W3099143320",
        "https://openalex.org/W2991265431",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3101155149",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W2964159778",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Ali Modarressi, Mohsen Fayyaz, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
    "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 258 - 271\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nGlobEnc: Quantifying Global Token Attribution by Incorporating the\nWhole Encoder Layer in Transformers\nAli Modarressi1⋆ Mohsen Fayyaz2⋆\nYadollah Yaghoobzadeh2 Mohammad Taher Pilehvar3\n1 Iran University of Science and Technology, Iran 2 University of Tehran, Iran\n3 Tehran Institute for Advanced Studies, Khatam University, Iran\nm_modarressi@comp.iust.ac.ir\n{mohsen.fayyaz77, y.yaghoobzadeh}@ut.ac.ir\nmp792@cam.ac.uk\nAbstract\nThere has been a growing interest in inter-\npreting the underlying dynamics of Transform-\ners. While self-attention patterns were initially\ndeemed as the primary option, recent studies\nhave shown that integrating other components\ncan yield more accurate explanations. This\npaper introduces a novel token attribution anal-\nysis method that incorporates all the compo-\nnents in the encoder block and aggregates this\nthroughout layers. Through extensive quanti-\ntative and qualitative experiments, we demon-\nstrate that our method can produce faithful and\nmeaningful global token attributions. Our ex-\nperiments reveal that incorporating almost ev-\nery encoder component results in increasingly\nmore accurate analysis in both local (single\nlayer) and global (the whole model) settings.\nOur global attribution analysis significantly\noutperforms previous methods on various\ntasks regarding correlation with gradient-based\nsaliency scores. Our code is freely available at\nhttps://github.com/mohsenfayyaz/GlobEnc.\n1 Introduction\nThe stellar performance of Transformers (Vaswani\net al., 2017) has garnered a lot of attention to ana-\nlyzing the reasons behind their effectiveness. The\nself-attention mechanism has been one of the main\nareas of focus (Clark et al., 2019; Kovaleva et al.,\n2019; Reif et al., 2019; Htut et al., 2019). How-\never, there have been debates on whether raw at-\ntention weights are reliable anchors for explain-\ning model’s behavior or not (Wiegreffe and Pinter,\n2019; Serrano and Smith, 2019; Jain and Wallace,\n2019). Recently, it was shown that incorporating\nvector norms should be an indispensable part of any\nattention-based analysis1 (Kobayashi et al., 2020,\n⋆ Equal contribution.\n1We also have shown the unreliability of weights due to\nnorm disparities in probing studies (Fayyaz et al., 2021).\n[CLS]\na\ndeep\nand\nmeaningful\nfilm\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 1: Aggregated attribution maps ( NENC ) for the\n[CLS] token for fine-tuned BERT on SST2 dataset (sen-\ntiment analysis). Our method (GlobEnc) is able to accu-\nrately quantify the global token attribution of the model.\n2021). However, these norm-based studies incor-\nporate only the attention block into their analysis,\nwhereas Transformer encoder layer is composed of\nmore components.\nAnother limitation of the existing analysis tech-\nniques is that they are usually constrained to the\nanalysis of single layer attributions. In order to\nexpand the analysis to multi-layered encoder-based\nmodels in their entirety, an aggregation technique\nhas to be employed. Abnar and Zuidema (2020)\nproposed two aggregation methods, rollout and\nmax-flow, which combine raw attention weights\nacross layers. Despite showing the outcome of their\nmethod to be faithful to a model’s inner workings\nin specific cases, the final results are still unsatis-\nfactory on a wide range of fine-tuned models.\nAdditionally, gradient-based alternatives (Si-\nmonyan et al., 2014; Kindermans et al., 2016; Li\net al., 2016) have been argued to provide a more ro-\nbust basis for token attribution analysis (Atanasova\net al., 2020; Brunner et al., 2020; Pascual et al.,\n2021). Nonetheless, the gradient-based alternatives\nhave not been able to fully replace attention-based\ncounterparts, mainly due to their high computa-\n258\ntional intensity.\nIn this paper, we propose a new global token\nattribution analysis method (GlobEnc) which is\nbased on the encoder layer’s output. In GlobEnc,\nthe second layer normalization is also included in\nthe norm-based analysis of each encoder layer. To\naggregate attributions over all layers, we applied\na modified attention rollout technique, returning\nglobal scores.\nThrough extensive experiments and comparing\nthe global attribution with the input token attribu-\ntions obtained by gradient-based saliency scores,\nwe show that our method produces faithful and\nmeaningful results (Figure 1). Our evaluations on\nmodels with distinct pre-training objectives and\nsizes (Devlin et al., 2019; Clark et al., 2020) show\nhigh correlations with gradient-based methods in\nglobal settings. Furthermore, with comparative\nstudies on each aspect of GlobEnc , we find that:\n(i) norm-based methods achieve higher correla-\ntions than weight-based methods; (ii) incorporat-\ning residual connections plays an essential role in\ntoken attribution; (iii) considering the two layer\nnormalizations improve our analysis only if cou-\npled together; and (iv) aggregation across layers\nis crucial for an accurate whole-model attribution\nanalysis.\nIn summary, our main contributions are:\n• We expand the scope of analysis from atten-\ntion block in Transformers to the whole en-\ncoder.\n• Our method significantly improves over exist-\ning techniques for quantifying global token\nattributions.\n• We qualitatively demonstrate that the attribu-\ntions obtained by our method are plausibly\ninterpretable.\n2 Background\nIn encoder-based language models (such as BERT),\na Transformer encoder layer is composed of several\ncomponents (Figure 2). The core component of the\nencoder is the self-attention mechanism (Vaswani\net al., 2017), which is responsible for the informa-\ntion mixture of a sequence of token representations\n(x1,..., xn). Each self-attention head computes a\nset of attention weights Ah = {αh\ni,j|1 ≤i,j ≤n},\nwhere αh\ni,j is the raw attention weight from the\nith token to the jth token in head h ∈{1,...,H }.\nTherefore, the output representation (zi ∈Rd) for\nResidual Connection (RES#1)\nFeed Forward\nAdd & Norm (LN#2)\nEncoder\nAttention Block\nAdd & Norm (LN#1)\nLayerNorm (                     )\n+\nFeed Forward\nMulti-head Attention\nResidual  (RES#2)\nFigure 2: The internal structure of a Transformer en-\ncoder layer. We show on the diagram the components\nthat are incorporated by each token attribution analysis\nmethod. Our method incorporates the whole encoder\n(NENC ) except for the direct effect of the fully connected\nfeed-forward module. Diagram inspired by Alammar\n(2018).\nthe ith token of a multi-head (with H heads) self-\nattention module is computed by concatenating the\nheads’ outputs followed by a head-mixing WO\nprojection:\nzi = CONCAT (z1\ni ,..., zH\ni )WO (1)\nwhere each head’s output vector is generated by\nperforming a weighted sum over the transformed\nvalue vectors v(xj) ∈Rdv :\nzh\ni =\nn∑\nj=1\nαh\ni,jvh(xj) (2)\nNorm-based attention. While one may inter-\npret the attention mechanism using the attention\nweights A, Kobayashi et al. (2020) argued that do-\ning so would ignore the norm of the transformed\nvectors multiplied by the weights, elucidating that\nthe weights are insufficient for interpretation. Their\nsolution enhanced the interpretability of attention\nweights by incorporating the value vectors v(xj)\nand the following projection WO. By reformulat-\ning Equation 1, we can consider zi as a summation\nover the attentions heads:\nzi =\nH∑\nh=1\nn∑\nj=1\nαh\ni,j vh(xj)Wh\nO  \nfh(xj)\n(3)\n259\nUsing this reformulation 2, Kobayashi et al. pro-\nposed a norm-based token attribution analysis\nmethod, N:= (||zi←j||) ∈Rn×n , to measure\neach token’s contribution in a self-attention mod-\nule:\nzi←j =\nH∑\nh=1\nαh\ni,jfh(xj) (4)\nThey showed that incorporating the magnitude of\nthe transformation function ( fh(x)) is crucial in\nassessing the input tokens’ contribution to the self-\nattention output.\nResidual connections & Layer Normalizations.\nKobayashi et al. (2021) added the attention block’s\nLayer Normalization ( LN#1) and Residual con-\nnection ( RES#1) to its prior norm-based anal-\nysis to assess the impact of residual connec-\ntions and layer normalization inside an attention\nblock. NRES := (||z+\ni←j||) ∈Rn×n is the analy-\nsis method which incorporates the attention block’s\nresidual connection. The input vector xis added to\nthe attribution of each token to itself to incorporate\nthe influence of RES#1:\nz+\ni←j =\nH∑\nh=1\nαh\ni,jfh(xi) + 1[i= j]xi (5)\nThey proposed a method for decomposing LN3 into\na summation of normalizations:\nLN(z+\ni ) =\nn∑\nj=1\ngz+\ni (z+\ni←j) + β\ngz+\ni (z+\ni←j) :=\nz+\ni←j −m(z+\ni←j)\ns(z+\ni ) ⊙γ\n(6)\nwhere m(.) and s(.) are the element-wise mean and\nstandard deviation of the input vector (cf. §A.1).\nThe decomposition can be applied to the contribu-\ntion vectors:\n˜zi←j = gz+\ni (\nH∑\nh=1\nαh\ni,jfh(xi) + 1[i= j]xi) (7)\nAccordingly, we can compute the magnitude\nNRESLN := (||˜zi←j||) ∈Rn×n , which represents\nthe amount of influence of an encoder layer’s input\n2Wh\nO is a head-specific slice of the original WO projec-\ntion. For more information about the reformulation process,\nsee Appendix C in Kobayashi et al. (2021)\n3γ ∈Rd and β ∈Rd are the trainable weights of LN.\nSimilar to Kobayashi et al. (2021) we ignore β.\ntoken jon its output token i. Based on this formu-\nlation, a context-mixing ratio could be defined as:\nri =\n||∑n\nj=1,j̸=i ˜zi←j||\n||∑n\nj=1,j̸=i ˜zi←j||+ ||˜zi←i|| (8)\nExperiments by Kobayashi et al. (2021) revealed\nconsiderably low rvalues which indicate the huge\nimpact of the residual connections. In other words,\nthe model tends to preserve token representations\nmore than mixing them with each other.\n3 Methodology\nOur method for input token attribution analysis has\na holistic view and takes into account almost ev-\nery component within the encoder layer. To this\nend, we first extend the norm-based analysis of\nKobayashi et al. (2021) by incorporating the en-\ncoder’s output LN#2. We then apply an aggre-\ngation technique to combine the information flow\nthroughout all layers.\nEncoder layer output ̸= Attention block output.\nWhile the RES#1 and the LN#1 from the attention\nblock are included in the analysis of Kobayashi\net al. (2021), the subsequent FFN, RES#2, and out-\nput LN#2 are ignored (see Fig. 2). Hence, NRESLN\nmight not be indicative of the entire encoder layer’s\nfunction. To address this issue, we additionally in-\nclude the encoder layer components from the atten-\ntion block outputs (˜zi) to the output representations\n(˜xi). The output of each encoder (˜xi) is computed\nas follows:\n˜z+\ni = FFN(˜zi) + ˜zi\n˜xi = LN(˜z+\ni ) (9)\nWe apply the LN decomposition rule in Eq. 7 to\nseparate the impacts of residual and FFN output:\n˜xi =\nn∑\nj=1\n(\ng˜z+\ni (FFN(˜zi←j)) + g˜z+\ni (˜zi←j)\n)\n+ β\n(10)\nGiven that the activation function between the two\nfully connected layers in the FFN component is\nnon-linear (Vaswani et al., 2017), a linear decom-\nposition similar to Eq. 7 cannot be derived. As\na result, we omit FFN’s influence on the contri-\nbution of each token and instead consider RES#2,\napproximating ˜xi←j as g˜z+\ni (˜zi←j). Nevertheless,\nit should be noted that the FFN still preserves some\ninfluence on this new setting due to the presence of\n260\ns(˜z+\ni ) in g˜z+\ni (˜zi←j). Similarly to Eq. 7, we can in-\ntroduce a more inclusive layerwise analysis method\nNENC := (||˜xi←j||) ∈Rn×n from input token j\nto output token iusing:\n˜xi←j ≈g˜z+\ni (˜zi←j) = ˜zi←j −m(˜zi←j)\ns(˜z+\ni ) ⊙γ\n(11)\nAggregating multi-layer attention. To create an\naggregated attribution score, Abnar and Zuidema\n(2020) proposed describing the model’s attentions\nvia modelling the information flow with a directed\ngraph. They introduced attention rollout method,\nwhich linearly combines raw attention weights\nalong all available paths in the pairwise attention\ngraph. The attention rollout of layer ℓw.r.t. the\ninputs is computed recursively as follows:\n˜Aℓ =\n{ ˆAℓ ˜Aℓ−1 ℓ> 1\nˆAℓ ℓ= 1\n(12)\nˆAℓ = 0.5 ¯Aℓ + 0.5I (13)\n¯Aℓ is the raw attention map averaged across all\nheads in layer ℓ. This method assumes equal contri-\nbution from the residual connection and multi-head\nattention (See Fig. 2). Hence, an identity matrix is\nsummed and renormalized, giving ˆAℓ.\nFor aggregating the layerwise analysis methods,\nwe use the rollout technique with minor modifi-\ncations. As many of the methods already include\nresidual connections, we only use Eq. 12 (replac-\ning ˆAℓ with the desired method’s attribution ma-\ntrix in layer ℓ) to calculate the rollout of a given\nmethod. However, for methods that do not assume\nthe residual connection, we define a corresponding\n“Fixed” variation using Eq. 13 that incorporates\na fixed residual effect ( ri ≈0.5). We refer to\nour proposed global method—aggregating the\nNENC across all layers by the rollout method—as\nGlobEnc. In what follows we report our exper-\niments, comparing GlobEnc with several other\nsettings.\n4 Experiments\nIn this section, we introduce the datasets and the\ntoken attribution analysis methods used in our eval-\nuations, followed by the experimental setup and\nresults.\n4.1 Datasets\nAll analysis methods are evaluated on three differ-\nent classification tasks. To cover sentiment detec-\ntion tasks we use SST2 (Socher et al., 2013), MNLI\n(Williams et al., 2018) for Natural Language Infer-\nence and Hatexplain (Mathew et al., 2021) in hate\nspeech detection.\n4.2 Analysis Methods\nWe use two categories of explainability approaches\nin our work: Weight-based and Norm-based.4 The\nWeight-based approaches employed in our experi-\nments are as follows:\n• W: The raw attention maps averaged across\nall heads (See Aℓ in §2).\n• WFIXED RES : Abnar and Zuidema’s assump-\ntion; add an identity matrix as a fixed residual\nto Aℓ (see ˆAℓ in Eq. 13).\n• WRES : The corrected version of Win which\naccurate residuals are added based on the\ncontext-mixing ratios of NENC :\nˆri =\n\n∑n\nj=1,j̸=i ˜xi←j\n\n\n∑n\nj=1,j̸=i ˜xi←j\n+ ∥˜xi←i∥\n(14)\nIn order to enforce WRES to have a context-\nmixing ratio equal to ˆri, it is essential to\nzero-out the diagonal elements (the tokens’\nattentions to themselves) of ¯Aℓ and renormal-\nize it:\nA′\nℓ = (I−diag\n(¯Aℓ\n)\n)−1( ¯Aℓ −diag\n(¯Aℓ\n)\n)\nWRES := diag (ˆr1,··· ,ˆrn) A′\nℓ\n+ diag (1 −ˆr1,..., 1 −ˆrn) I\n(15)\nThe Norm-based analysis methods, namely N,\nNRES and NRESLN were discussed in detail in §2.\nOur proposed norm-based method NENC was ex-\nplained in §3. For an ablation study, we introduce\nNFIXED RES which is N, corrected with a fixed resid-\nual similar to WFIXED RES\n5.\nˆN=\n(\n||zi←j||∑\nj ||zi←j||\n)\n∈Rn×n\nNFIXED RES := 0.5 ˆN+ 0.5 I\n(16)\n4Note that in most of our experiments, we use all these\nmethods within the rollout aggregation technique.\n5The only difference is that we need to normalizeNbefore\nadding an identity matrix.\n261\nIn §4.5, we will demonstrate our comparative\nstudies between the aforementioned methods and\nGlobEnc.\n4.3 Gradient-based Methods for Faithfulness\nAnalysis\nGradient-based methods are widely used as alter-\nnatives for attention-based counterparts for quanti-\nfying the importance of a specific input feature\nin making the right prediction (Li et al., 2016;\nAtanasova et al., 2020). In this section we dis-\ncuss the specific gradient-based methods we use,\nnamely saliency, HTA, and our adjusted HTA.\n4.3.1 Saliency\nGradient-based saliency is based on the gradient\nof the output (yc) w.r.t. the input embeddings (e0\ni ).\nOne of the most accurate variations of the saliency\nfamily is the gradient×input method (Kindermans\net al., 2016) where the input embeddings is multi-\nplied by the gradients. Thus, the contribution score\nof input token iis determined by first computing\nthe element-wise product of the input embeddings\n(e0\ni ) and the gradients of the true class output score\n(yc) w.r.t. the input embeddings. Then, the L2\nnorm of the scaled gradients is computed to derive\nthe final score:\nSaliencyi =\n\n∂yc\n∂e0\ni\n⊙e0\ni\n\n2\n(17)\n4.3.2 HTA x Inputs\nTo determine an upper bound on the information\nmixing within each layer, we use a modified ver-\nsion of Hidden Token Attribution (Brunner et al.,\n2020, HTA). In the original version, HTA is the\nsensitivity between any two vectors in the model’s\ncomputational graph. However, inspired by the\ngradient×input method (Kindermans et al., 2016),\nwhich has shown more faithful results (Atanasova\net al., 2020; Wu and Ong, 2021), we multiply the\ninput vectors by the gradients and then apply a\nFrobenius norm. We compute the attribution from\nhidden embedding j(eℓ−1\nj ) to hidden embedding i\n(eℓ\ni) in layer ℓas:\ncℓ\ni←j =\n\n∂eℓ\ni\n∂eℓ−1\nj\n⊙eℓ−1\nj\n\nF\n(18)\nComputing HTA-based attribution matrices is an\nextremely computation-intensive task (especially\nfor long texts) due to the high dimensionality of hid-\nden embeddings. Hence, we only use this method\nfor 256 examples from the SST-2 task’s validation\nset. It is worth noting that extracting the HTA-\nbased contribution maps for the aforementioned\ndata took approximately 2 hours, whereas comput-\ning the maps for the entire analysis methods stated\nin §4.2 took only 5 seconds.6\n4.4 Setup\nWe employ HuggingFace’s Transformers library7\n(Wolf et al., 2020) and the BERT-base-uncased\nmodel. For fine-tuning BERT, epochs vary from 3\nto 5, and the batch size and learning rate are 32 and\n3e-5, respectively.8 We also carried out the main\nexperiment on BERT-large and ELECTRA (Devlin\net al., 2019; Clark et al., 2020) where the results\nare reported at §A.2.\nAfter rollout aggregation of each analysis\nmethod, we obtain an accumulated attribution ma-\ntrix for every layer ( ℓ) of BERT. These matrices\nindicate the overall contribution of each input token\nto all token representations in layer ℓ. Since the\nclassifier in a fine-tuned model is attached to the\nfinal layer representation of the [CLS] token, we\nconsider the first row (corresponding to [CLS] at-\ntributions) of the last layer attribution matrix. This\nvector represents the contribution of each input to-\nken to the model’s final decision. As a measure of\nfaithfulness of the resulting vector with the saliency\nscores, we report the Spearman’s rank correlation\nbetween the two vectors.\n4.5 Results\nTable 1 shows the Spearman correlation of saliency\nscores with the aggregated attribution scores from\n[CLS] to input tokens at the final layer. In order\nto determine the contribution of each component\nof encoder layer to the overall performance, we\nreport the results for attribution analysis methods\ndiscussed in §4.2. Our results demonstrate that in-\ncorporating the vector norms, residual connection,\nand both layer normalizations yields the highest\ncorrelation (NENC ). In what follows, we discuss\nthe impact of incorporating various parts in the\nanalysis.\n4.5.1 On the role of vector norms\nAs also suggested by Kobayashi et al. (2020), vec-\ntor norms play an important role in determining\n6Conducted on a 3070 GPU machine.\n7https://github.com/huggingface/transformers\n8Recommended by Devlin et al. (2019).\n262\nAttention Rollout\nSST2 MNLI H ATEXPLAIN\nWeight-based (W) −0.11 ± 0.26 −0.06 ± 0.22 0.12 ± 0.26\nw/ Fixed Residual (WFIXED RES ) 9 −0.24 ± 0.26 −0.05 ± 0.26 0.13 ± 0.28\nw/ Residual (WRES ) 0.19 ± 0.26 0.27 ± 0.25 0.53 ± 0.24\nNorm-based (N) 0.44 ± 0.20 0.47 ± 0.16 0.43 ± 0.22\nw/ Fixed Residual (NFIXED RES ) 0.48 ± 0.20 0.55 ± 0.16 0.48 ± 0.22\nw/ Residual (NRES ) 0.73 ± 0.13 0.75 ± 0.10 0.66 ± 0.17\nw/ Residual + Layer Norm 1 (NRESLN ) −0.21 ± 0.26 −0.06 ± 0.26 0.08 ± 0.28\nw/ GlobEnc: [Residual + Layer Norm 1, 2] (NENC ) 0.77 ± 0.12 0.78 ± 0.09 0.72 ± 0.17\nTable 1: Spearman’s rank correlation of attribution based importance (aggregated by rollout) with saliency scores\nfor the validation set for the BERT model fine-tuned on SST-2, MNLI, and HateXplain. In fixed residual cases, the\ncontext-mixing ratio is roughly 0.5, and in weight-based w/ residual (WRES ), it is corrected with context-mixing\nratio of (NENC ). The numbers are the average on all the validation set examples ± the standard deviation.\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nSpearman Rank Correlation\nAggregated Attribution Spearman Rank Correlation on SST2 Dataset\nW\nW-Res\nN\nN-Res\nN-Enc\nFigure 3: Spearman’s rank correlation of aggregated at-\ntribution scores with saliency scores across layers. The\n99% confidence intervals are shown as (narrow) shaded\nareas around each line. NENC achieves the highest cor-\nrelation in almost every layer.\nattention outputs. This is highlighted by the signif-\nicant gap between weight-based and norm-based\nsettings across all datasets in Table 1.\nWe also show the correlation of the aggregated\nattention for all layers in Figure 3. The norm-based\nsettings ( N and NRES ) attain higher correlation\nthan the weight-based counterparts (Wand WRES )\nalmost in all layers, confirming the importance of\nincorporating vector norms.\n9As mentioned in §4.2, this analysis method is based on\nthe original experiment by Abnar and Zuidema (2020). Our\nexperiments on SST2 differ from theirs in two aspects: (i)\nwe opted for gradient×input saliencies, while they used the\nsum of gradients (sensitivity) (ii) instead of BERT, they used\na DistillBERT fine-tuned model (Sanh et al., 2019). However,\ntheir reported results in their sepcific setup (Spearman Corr. =\n0.14) still yields significantly lower results than GlobEnc.\n4.5.2 On the role of residual connections\nKobayashi et al. (2021) showed that in the encoder\nlayer, the output representations of each token is\nmainly determined by its own representation, and\nthe contextualization from other tokens’ plays a\nmarginal role. This is in contrary to the simplifying\nassumption made by Abnar and Zuidema (2020)\nwho used a fixed context-mixing ratio of 0.5 (as-\nsuming that BERT equally preserves and mixes the\nrepresentations). This setting is shown as weight-\nbased with fixed residual (WFIXED RES ) in Table 1.\nWe compare this setting against WRES (see §4.2).\nWRES is similar to WFIXED RES (in that it does not\ntake into account vector norms) but differs in that\nit considers a dynamic mixing ratio (the one from\nNENC ). The huge performance gap between the\ntwo settings in Table 1 clearly highlights the im-\nportance of considering accurate context-mixing\nratios. Therefore, it is crucial to consider the resid-\nual connection in the attention block for input token\nattribution analysis.\nTo further demonstrate the role of residual con-\nnections, we utilize the introduced method in §4.2,\nwhere we modified the norm-based attentions with\nfixed residual (r≈0.5). The comparison of norm-\nbased without any residual ( N) and with a fixed\nresidual (NFIXED RES ) shows a consistent improve-\nment for the latter across all the datasets. This\nprovides evidence on that having a fixed uniform\ncontext-mixing ratio is better than neglecting the\nresidual connection altogether.\nFinally, when we aggregate the norm-based anal-\nysis with an accurate dynamic context-mixing ratio\n(NRES ), we observe the highest correlation up to\n263\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Pearson Correlation\nHTA Pearson Correlation on SST2 Dataset\nN-Res\nN-ResLN\nN-Enc\nFigure 4: Single layer Pearson correlation of HTA maps\nwith attribution maps. The 99% confidence intervals are\nshown as shaded areas around each line. NRESLN shows\nconsiderably less association with HTA.\nthis point, without layer normalization.\n4.5.3 On the role of layer normalization\nIn Table 1 we see a sudden drop in correlations for\nNRESLN . Although this method considers vector\nnorms and residuals, incorporating LN#1 in the en-\ncoder seems to have deteriorated the accuracy for\ntoken attribution analysis. To determine whether\nthis deterioration of correlation in aggregated attri-\nbutions is also present in individual single layers,\nwe compare the HTA maps as a baseline with the\nattribution matrices extracted from different anal-\nysis methods. Figure 4 shows the correlation of\nHTA attribution maps with the maps obtained by\nNRES , NRESLN , and NENC methods. The results\nindicate that NRESLN exhibits a significantly lower\nassociation.\nThe question that arises here is that how incor-\nporating an additional component of the encoder\n(LN#1) in NRESLN degrades the results (compared\nto NRES ). To answer this question, we investigated\nthe learned weights of LN#1 and LN#2. The outlier\nweights10 in specific dimensions of LNs are shown\nto be significantly influential on the model’s perfor-\nmance (Kovaleva et al., 2021; Luo et al., 2021). It\nis interesting to note that based on our observations,\nthe outlier weights of the two layer norms seem to\nbe the opposite of each other. Figure 5 demon-\nstrates the weight values in layer 11 and also the\ncorrelation of the outlier weights across layers. The\nlarge negative correlations confirm that the outlier\nweights work contrary to each other. We speculate\nthat the effect of outliers in the two layer norms is\n10We identify the dimensions where the weights are at least\n3σ from the mean as outliers (Kovaleva et al., 2021).\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nPearson's r\nCorrelation Between LN1 and LN2 Outlier Weights\n0 100 200 300 400 500 600 700 800\nDimension\n0\n1\n2\n3Weight\nLN1 and LN2 Weights in Layer 11\nLN1\nLN2\nFigure 5: The Pearson correlation between outlier\nweights of LN#1 and LN#2 across layers. The weight\nvalues for layer 11 are shown as well.\nL1 L6 L12 MAX\nIndiv.\nN −.50 ± .18 +.28 ± .23 +.40 ± .21 +.41 ± .21\nNRES −.48 ± .18 +.29 ± .24 +.41 ± .19 +.41 ± .19\nNENC −.47 ± .18 +.29 ± .24 +.41 ± .19 +.41 ± .19\nRollout\nN −.50 ± .18 +.44 ± .20 +.44 ± .20 +.44 ± .20\nNRES −.48 ± .18 +.70 ± .14 +.73 ± .13 +.73 ± .13\nNENC −.47 ± .18 +.74 ± .14 +.77 ± .12 +.78 ± .12\nTable 2: Spearman’s rank correlation of attribution-\nbased scores (individual and aggregated by rollout)\nwith saliency scores for the validation set for the BERT\nmodel fine-tuned on SST-2. The results are reported for\nlayers 1, 6, 12, and the maximum of all layers. Utilizing\nrollout aggregation achieves higher correlations than\nindividual layers.\npartly cancelled out when both are considered.\nAs shown in Figure 2, the FFN and the sec-\nond layer normalization are on top of the atten-\ntion block. However, NRESLN does not incorpo-\nrate the components outside of the attention block.\nAs described in §3, in our local analysis method\nNENC we incorporate the second layer normaliza-\ntion in the transformer’s encoder (Figure 2), thus\nconsidering the whole encoder block (except FFN).\nOverall, our global method, GlobEnc, yields the\nbest results among all the methods evaluated in\nour experiments. In general, Table 1 suggests that\nincorporating each component of the encoder will\nincrease the correlation; however, the two layer\nnormalizations should be considered together.\n4.5.4 On the role of aggregation\nWe carried out an additional analysis to verify if\nincorporating vector norms, residual connection\nand layer normalizations in individual layers is ade-\nquate for achieving high correlations, or if it is also\nnecessary to aggregate them via rollout. Table 2\n264\nshows the correlation results in different layers for\nraw attributions (without aggregation) and for the\naggregated attributions using the rollout method.\nApplying rollout method on attribution maps up to\neach layer results in higher correlations with the\nsaliency scores than the raw single layer attribution\nmaps, especially in deeper layers. Therefore, atten-\ntion aggregation is essential for global input token\nattribution analysis.\nAn interesting point in Figure 3, which shows\nthe correlation of the aggregated methods through-\nout the layers, is that the correlation curves flatten\nout after only a few layers. 11 This indicates that\nBERT identifies decisive tokens only after the first\nfew layers. The final layers only make minor ad-\njustments to this order. Nevertheless, it is worth\nnoting that the order of attribution does not nec-\nessarily imply the model’s final decision and the\nfinal result may still change for the better or worse\n(Zhou et al., 2020).\n4.5.5 Qualitative analysis\nTo qualitatively answer if the aggregated attribu-\ntion maps provide plausible and meaningful in-\nterpretations, we take a closer look at the attribu-\ntion maps generated by GlobEnc. Figure 1 shows\nthe GlobEnc attribution of the model trained on\nSST-2. Each layer demonstrates the [CLS] token’s\naggregated attribution to input tokens up to the\ncorresponding layer. The example inputs are “a\ndeep and meaningful film.” and “big fat waste of\ntime.”, both correctly classified by the model. In\nboth cases, GlobEnc focuses on the relevant words\nfor sentiment classification, i.e., “meaningful” and\n“waste”. An interesting observation in Figure 1 is\nthat in the first few layers, the [CLS] token mostly\nattends to itself while other tokens have marginal\nimpact. As the representations get more contextual-\nized in deeper layers, the attribution correctly shifts\nto the words which indicate the sentiment of the\nsentence.12 More examples from MNLI and SST2\ndatasets, including misclassified examples are avail-\nable at §A.3. Our qualitative analysis suggests that\nGlobEnc can be useful for a reasonable interpreta-\ntion of attention mechanism in BERT, ELECTRA,\nand possibly any other transformer-based model.\n11WRES is the only exception with a constant increase; this\nmethod is gradually and artificially corrected by NENC context\nmixing ratios.\n12Complete attention maps in Figure A.3 show that, simi-\nlarly to [CLS], other tokens also focus on sentiment tokens.\n5 Related Work\nWhile numerous studies have used attention\nweights to analyze and interpret the self-attention\nmechanism (Clark et al., 2019; Kovaleva et al.,\n2019; Reif et al., 2019; Htut et al., 2019), the use\nof mere attention weights to explain a model’s in-\nner workings has been an active topic of debate\n(Serrano and Smith, 2019; Jain and Wallace, 2019;\nWiegreffe and Pinter, 2019). Several solutions have\nbeen proposed to address this issue, usually through\nconverting raw attention weights to scores that pro-\nvide better explanations. Brunner et al. (2020) used\nthe transformation function fh(xj) to introduce\neffective attentions—the orthogonal component of\nthe attention matrix in fh(xj) null space—to ex-\nplain the inner workings of each layer. However,\nthis technique ignores other components in the en-\ncoder and is computationally expensive due to the\nSVD required to compute the effective attentions.\nKobayashi et al. (2020) incorporated the modified\nvector and introduced a vector norms-based analy-\nsis. This was later extended by integrating residual\nconnections and layer normalization components to\nenhance the accuracy of explanations (Kobayashi\net al., 2021). But, as discussed in §4.5, relying\nsolely on LN#1 does not produce accurate results.\nWhile these methods can be employed for single-\nlayer (local) analysis, multi-layer attributions are\nnot necessarily correlated with single-layer attribu-\ntions due to the significant degree of information\ncombination through multi-layer language mod-\nels (Pascual et al., 2021; Brunner et al., 2020).\nVarious saliency methods exist for explaining the\nmodel’s decision based on the input (Li et al., 2016;\nBastings and Filippova, 2020; Atanasova et al.,\n2020; Wu and Ong, 2021; Mohebbi et al., 2021).\nHowever, these approaches are not primarily de-\nsigned for computing inter-token attributions. To\nfill this gap, Brunner et al. (2020) proposed HTA,\nwhich is based on the gradient of each hidden em-\nbedding in relation to the input embeddings. In\n§4.3.2, we extend HTA to incorporate the impact\nof the input vectors. However, HTA is extremely\ncomputationally intensive. Attention rollout (see\n§3) and attention flow—which involve solving a\nmax-flow problem on the attention graph—are two\naggregation approaches introduced by Abnar and\nZuidema (2020), in which raw attention weights\n(with equally weighted residual weights) are ag-\ngregated within multiple layers. We showed that\nattention rollout does not perform well on the raw\n265\nattention maps of language models fine-tuned on\ndownstream tasks and that this problem can be re-\nsolved by utilizing attribution norms.\n6 Conclusions\nIn this work, we proposed a novel method for single\nlayer token attribution analysis which incorporates\nthe whole encoder layer, i.e., the attention block\nand the output layer normalization. When aggre-\ngated across layers using the rollout method, our\ntechnique achieves quantitatively and qualitatively\nplausible results. Our evaluation of different analy-\nsis methods provided evidence on roles played by\nindividual components of the encoder layer, i.e.,\nthe vector norms, the residual connections, and the\nlayer normalizations. Furthermore, our in-depth\nanalysis suggested that the two layer normaliza-\ntions in the encoder layer counteract each other;\nhence, it is important to couple them for an accu-\nrate analysis.\nAdditionally, using a newly proposed and im-\nproved version of Hidden Token Attribution, we\ndemonstrated that encoder-based attribution analy-\nsis is more accurate when compared to other partial\nsolutions in a single layer (local-level). This is con-\nsistent with our global observations. Quantifying\nglobal input token attribution based on our work\ncan provide a meaningful explanation of the whole\nmodel’s behavior. In future work, we plan to apply\nour global analysis method on various datasets and\nmodels, to provide valuable insights into model\ndecisions and interpretability.\nReferences\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention flow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4190–4197, On-\nline. Association for Computational Linguistics.\nJay Alammar. 2018. The illustrated transformer [blog\npost].\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. A diagnostic\nstudy of explainability techniques for text classifi-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 3256–3274, Online. Association for\nComputational Linguistics.\nJasmijn Bastings and Katja Filippova. 2020. The ele-\nphant in the interpretability room: Why use attention\nas explanation when we have saliency methods? In\nProceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 149–155, Online. Association for Com-\nputational Linguistics.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Wat-\ntenhofer. 2020. On identifiability in transformers. In\nInternational Conference on Learning Representa-\ntions.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Ho-\nsein Mohebbi, and Mohammad Taher Pilehvar. 2021.\nNot all models localize linguistic knowledge in the\nsame place: A layer-wise probing on BERToids’ rep-\nresentations. In Proceedings of the Fourth Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP , pages 375–388, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019. Do attention heads\nin BERT track syntactic dependencies? CoRR,\nabs/1911.12246.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nPieter-Jan Kindermans, Kristof Schütt, Klaus-Robert\nMüller, and Sven Dähne. 2016. Investigating the\ninfluence of noise and distractors on the interpretation\nof neural networks.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\n266\npages 7057–7075, Online. Association for Computa-\ntional Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2021. Incorporating Residual and Nor-\nmalization Layers into Analysis of Masked Language\nModels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4547–4568, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,\nand Anna Rumshisky. 2021. BERT busters: Outlier\ndimensions that disrupt transformers. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 3392–3405, Online. Association\nfor Computational Linguistics.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691, San Diego, California.\nAssociation for Computational Linguistics.\nZiyang Luo, Artur Kulmizev, and Xiaoxi Mao. 2021.\nPositional artefacts propagate through masked lan-\nguage model embeddings. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5312–5327, Online. Association\nfor Computational Linguistics.\nBinny Mathew, Punyajoy Saha, Seid Muhie Yimam,\nChris Biemann, Pawan Goyal, and Animesh Mukher-\njee. 2021. Hatexplain: A benchmark dataset for\nexplainable hate speech detection. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n35(17):14867–14875.\nHosein Mohebbi, Ali Modarressi, and Moham-\nmad Taher Pilehvar. 2021. Exploring the role of\nBERT token representations to explain sentence prob-\ning results. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 792–806, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nDamian Pascual, Gino Brunner, and Roger Wattenhofer.\n2021. Telling BERT’s full story: from local attention\nto global aggregation. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 105–124, Online. Association for Computa-\ntional Linguistics.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Processing\nSystems, pages 8594–8603.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. In NeurIPS\nEMC2 Workshop.\nSofia Serrano and Noah A. Smith. 2019. Is attention\ninterpretable? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2931–2951, Florence, Italy.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2014. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. CoRR, abs/1312.6034.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\n267\nZhengxuan Wu and Desmond C. Ong. 2021. On\nexplaining your explanations of BERT: an empir-\nical study with sequence classification. CoRR,\nabs/2101.00196.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses\npatience: Fast and robust inference with early exit. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 18330–18341. Curran Associates,\nInc.\nA Appendix\nA.1 LN Formulation\nm(a) := 1\nd\n∑\nk a(k),\ns(a) :=\n√\n1\nd\n∑\nk(m(a) −a(k) + ϵ)2\nwhere ϵis a small constant\nA.2 More Models\nIn this section we provide the results for BERT-\nlarge and ELECTRA-base. For both models, our\nmethod outperforms the previous analysis methods.\nThe results are reported in Tables A.1 and A.2.\nA.3 More Examples\nAggregated attributions by different methods\nthroughout layers is shown in Figure A.2. Our\nproposed method shows more plausible results.\nAggregated attribution map for layer 12 is shown\nin Figure A.3. In this figure, the effect of each\ntoken can be seen on all other tokens and not just\nthe [CLS] token.\nMore examples for MNLI dataset are shown for\nBERT-base in Figure A.4, for BERT-large in Fig-\nure A.6, and for ELECTRA in Figure A.5. More-\nover, misclassified examples of SST2 dataset are\nshown in Figure A.1.\n268\nBERT-large Attention Rollout\nSST2 MNLI H ATEXPLAIN\nWeight-based (W) −0.38 ± 0.16 −0.61 ± 0.14 −0.41 ± 0.25\nw/ Fixed Residual (WFIXED RES ) −0.25 ± 0.19 −0.48 ± 0.19 −0.21 ± 0.30\nw/ Residual (WRES ) −0.10 ± 0.21 0.33 ± 0.23 0.09 ± 0.30\nNorm-based (N) 0.44 ± 0.24 0.13 ± 0.27 0.48 ± 0.25\nw/ Fixed Residual (NFIXED RES ) 0.49 ± 0.24 0.26 ± 0.25 0.49 ± 0.30\nw/ Residual (NRES ) 0.77 ± 0.11 0.66 ± 0.12 0.73 ± 0.16\nw/ Residual + Layer Norm 1 (NRESLN ) −0.07 ± 0.23 −0.35 ± 0.24 0.06 ± 0.32\nw/ GlobEnc: [Residual + Layer Norm 1, 2] (NENC ) 0.83 ± 0.08 0.77 ± 0.09 0.76 ± 0.17\nTable A.1: Spearman’s rank correlation of attribution based importance (aggregated by rollout) with saliency scores\nfor the validation set for the BERT-large model fine-tuned on SST-2, MNLI, and HateXplain. The numbers are\nthe average on all the validation set examples (1024 examples for MNLI dataset due to resource limitations) ± the\nstandard deviation.\nELECTRA-base Attention Rollout\nSST2 MNLI H ATEXPLAIN\nWeight-based (W) −0.37 ± 0.19 −0.31 ± 0.22 0.02 ± 0.29\nw/ Fixed Residual (WFIXED RES ) −0.37 ± 0.19 −0.24 ± 0.23 0.01 ± 0.29\nw/ Residual (WRES ) −0.10 ± 0.22 0.08 ± 0.25 0.20 ± 0.27\nNorm-based (N) 0.18 ± 0.21 0.12 ± 0.21 0.21 ± 0.26\nw/ Fixed Residual (NFIXED RES ) 0.23 ± 0.22 0.32 ± 0.23 0.28 ± 0.26\nw/ Residual (NRES ) 0.54 ± 0.17 0.54 ± 0.14 0.44 ± 0.21\nw/ Residual + Layer Norm 1 (NRESLN ) −0.24 ± 0.23 −0.16 ± 0.24 −0.07 ± 0.28\nw/ GlobEnc: [Residual + Layer Norm 1, 2] (NENC ) 0.64 ± 0.15 0.68 ± 0.12 0.47 ± 0.22\nTable A.2: Spearman’s rank correlation of attribution based importance (aggregated by rollout) with saliency scores\nfor the validation set for the ELECTRA-base model fine-tuned on SST-2, MNLI, and HateXplain. The numbers are\nthe average on all the validation set examples ± the standard deviation.\n[CLS]\nhilarious\n##ly\nin\n##ept\nand\nridiculous\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\ni\n'\nll\nbet\nthe\nvideo\ngame\nis\na\nlot\nmore\nfun\nthan\nthe\nfilm\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nthe\nlower\nyour\nexpectations\n,\nthe\nmore\nyou\n'\nll\nenjoy\nit\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nas\nun\n##see\n##ml\n##y\nas\nits\ntitle\nsuggests\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A.1: Aggregated NENC attribution maps (GlobEnc) for the [CLS] token for fine-tuned BERT on SST2\ndataset (sentiment analysis). These examples were misclassified by the model.\n269\n[CLS]\na\ngorgeous\n,\nwitty\n,\nseductive\nmovie\n.\n[SEP]\n121110987654321\nLayer\nW\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\na\ngorgeous\n,\nwitty\n,\nseductive\nmovie\n.\n[SEP]\n121110987654321\nLayer\nW-FixedRes\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\na\ngorgeous\n,\nwitty\n,\nseductive\nmovie\n.\n[SEP]\n121110987654321\nLayer\nW-Res\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\na\ngorgeous\n,\nwitty\n,\nseductive\nmovie\n.\n[SEP]\n121110987654321\nLayer\nN-FixedRes\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\na\ngorgeous\n,\nwitty\n,\nseductive\nmovie\n.\n[SEP]\n121110987654321\nLayer\nN\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n[CLS]\na\ngorgeous\n,\nwitty\n,\nseductive\nmovie\n.\n[SEP]\n121110987654321\nLayer\nN-Res\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\na\ngorgeous\n,\nwitty\n,\nseductive\nmovie\n.\n[SEP]\n121110987654321\nLayer\nN-ResLN\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\na\ngorgeous\n,\nwitty\n,\nseductive\nmovie\n.\n[SEP]\n121110987654321\nLayer\nN-Enc\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A.2: Aggregated attributions via rollout with different methods across layers. The model is fine-tuned on\nSST2 dataset and the attention of the CLS token is shown in each layer.\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\nW\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\nW-FixedRes\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\nW-Res\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\nN-FixedRes\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\nN\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\nN-Res\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\nN-ResLN\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\n[CLS]\nbig\nfat\nwaste\nof\ntime\n.\n[SEP]\nN-Enc\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFigure A.3: Aggregated attributions via rollout with different methods in layer 12. The model is fine-tuned on SST2\ndataset. Each row indicates how much other tokens impact the token written on the row.\n270\n[CLS]\nthanks\nfor\nyour\nconsideration\n!\n[SEP]\nwe\nappreciate\nyour\nconsideration\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nby\nthe\nyear\n2000\n.\n[SEP]\nby\nthe\nyear\n2012\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nthe\nentire\nthing\n!\n[SEP]\nonly\na\npiece\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\ndo\nyou\nplay\nfootball\na\nlot\n?\n[SEP]\ndo\nyou\nlike\nbasketball\n?\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A.4: Aggregated NENC attribution maps (GlobEnc) for the [CLS] token for fine-tuned BERT on MNLI\ndataset.\n[CLS]\nthanks\nfor\nyour\nconsideration\n!\n[SEP]\nwe\nappreciate\nyour\nconsideration\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nby\nthe\nyear\n2000\n.\n[SEP]\nby\nthe\nyear\n2012\n.\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nthe\nentire\nthing\n!\n[SEP]\nonly\na\npiece\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\ndo\nyou\nplay\nfootball\na\nlot\n?\n[SEP]\ndo\nyou\nlike\nbasketball\n?\n[SEP]\n121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A.5: Aggregated NENC attribution maps (GlobEnc) for the [CLS] token for fine-tuned ELECTRA on MNLI\ndataset.\n[CLS]\nthanks\nfor\nyour\nconsideration\n!\n[SEP]\nwe\nappreciate\nyour\nconsideration\n.\n[SEP]\n242322212019181716151413121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nby\nthe\nyear\n2000\n.\n[SEP]\nby\nthe\nyear\n2012\n.\n[SEP]\n242322212019181716151413121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\nthe\nentire\nthing\n!\n[SEP]\nonly\na\npiece\n[SEP]\n242322212019181716151413121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n[CLS]\ndo\nyou\nplay\nfootball\na\nlot\n?\n[SEP]\ndo\nyou\nlike\nbasketball\n?\n[SEP]\n242322212019181716151413121110987654321\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A.6: Aggregated NENC attribution maps (GlobEnc) for the [CLS] token for fine-tuned BERT-large on MNLI\ndataset.\n271"
}