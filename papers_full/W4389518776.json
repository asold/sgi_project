{
  "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise",
  "url": "https://openalex.org/W4389518776",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2946229205",
      "name": "Jiaao Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102121030",
      "name": "Aston Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103262105",
      "name": "Mu Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3020344429",
      "name": "Alex Smola",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2327037637",
    "https://openalex.org/W4225598930",
    "https://openalex.org/W4307929986",
    "https://openalex.org/W4385571739",
    "https://openalex.org/W4310420099",
    "https://openalex.org/W4310421098",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287329820",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4321471763",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W4281690218",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963912046",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W2136657878",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W4306802991",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W4307409581",
    "https://openalex.org/W3121370741",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W4294754026",
    "https://openalex.org/W2465663554",
    "https://openalex.org/W3168053944",
    "https://openalex.org/W4287083626",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3167002470",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3169017236",
    "https://openalex.org/W4320843382",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W3129651364",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2129069237",
    "https://openalex.org/W4312051726"
  ],
  "abstract": "Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high. To alleviate these issues, we introduce a novel diffusion model for language modeling, Masked-Diffuse LM, with lower training cost and better performances, inspired by linguistic features in languages. Specifically, we design a linguistic-informed forward process which adds corruptions to the text through strategically soft-masking to better noise the textual data. Also, we directly predict the categorical distribution with cross-entropy loss function in every diffusion step to connect the continuous space and discrete space in a more efficient and straightforward way. Through experiments on 5 controlled generation tasks, we demonstrate that our Masked-Diffuse LM can achieve better generation quality than the state-of-the-art diffusion models with better efficiency.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4765–4775\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nA Cheaper and Better Diffusion Language Model with Soft-Masked Noise\nJiaao Chen†∗, Aston Zhang†, Mu Li, Alex Smola, Diyi Yang⋄\n†Georgia Institute of Technology,‡Meta GenAI, ⋄Stanford University\nAbstract\nDiffusion models that are based on iterative de-\nnoising have been recently proposed and lever-\naged in various generation tasks like image gen-\neration. Whereas, as a way inherently built for\ncontinuous data, existing diffusion models still\nhave some limitations in modeling discrete data,\ne.g., languages. For example, the generally\nused Gaussian noise can not handle the discrete\ncorruption well, and the objectives in continu-\nous spaces fail to be stable for textual data in\nthe diffusion process especially when the di-\nmension is high. To alleviate these issues, we\nintroduce a novel diffusion model for language\nmodeling, Masked-Diffusion LM, with lower\ntraining cost and better performances, inspired\nby linguistic features in languages. Specifically,\nwe design a linguistic-informed forward pro-\ncess which adds corruptions to the text through\nstrategically soft-masking to better noise the\ntextual data. Also, we directly predict the cat-\negorical distribution with cross-entropy loss\nfunction in every diffusion step to connect the\ncontinuous space and discrete space in a more\nefficient and straightforward way. Through\nexperiments on 5 controlled generation tasks,\nwe demonstrate that our Masked-Diffusion LM\ncan achieve better generation quality than the\nstate-of-the-art diffusion models with better ef-\nficiency. Code is available athttps://github.\ncom/SALT-NLP/Masked_Diffusioin_LM.\n1 Introduction\nWe present a novel diffusion method for model-\ning languages, Masked-Diffusion LM (language\nmodel), which uses strategic soft-masking in-\nformed by linguistic features to corrupt both the\ndiscrete and continuous space, and then iteratively\ndenoise them back by predicting the categorical\ndistribution. Specifically, a strategic soft-masking\nprocess is designed that gradually adds perturba-\ntion to the input text in an order from harder or\n∗Correspondence to Jiaao Chen <jiaaochen@gatech.edu>\nand Aston Zhang <az@astonzhang.com>.\nmore informative words to simpler or less infor-\nmative words through soft-masking. As a result,\nthe models are encouraged to recover and generate\nthe text following an easy-first-generation nature\n(Dieleman et al., 2022) to improve the generation\nstructure and quality with more flexibility. Also,\nduring the diffusion process, we directly predict the\ndiscrete token with cross-entropy loss that maps the\ncontinuous space to discrete textual space to stabi-\nlize the intermediate diffusion steps. Through our\nproposed Masked-Diffusion LM, the application-\nspecific performance metrics as well as training\nefficiency are significantly improved over current\ndiffusion language models based on experiments.\nOur work is inspired by recent advances in dif-\nfusion models (Sohl-Dickstein et al., 2015; Ho\net al., 2020; Song et al., 2021; Yang et al., 2022;\nRamesh et al., 2022; Rombach et al., 2022) that\nare introduced as a new generative modeling ap-\nproach based on iterative denoising and have\nachieved high-quality generations for visual and\naudio modalities (Ramesh et al., 2022; Rombach\net al., 2022; Saharia et al., 2022; Nichol and Dhari-\nwal, 2021; Kong et al., 2020).\nAlthough these approaches have received grow-\ning attention and achieved impressive success, ap-\nplying diffusion models to textual domain is still\nchallenging and under-explored due to the discrete\nnature of the text (e.g., one-hot vectors) compared\nto continuous data like images (e.g., RGB values)\n(Li et al., 2022). A few prior works (Li et al., 2022;\nGong et al., 2022; He et al., 2022; Austin et al.,\n2021; Hoogeboom et al., 2021b) that explore using\ndiffusion models on textual data can be divided into\ntwo lines. The first is to extend diffusion models\nto discrete state spaces (Austin et al., 2021; Hooge-\nboom et al., 2021b,a). The second is to perform\nthe diffusion process and its reverse process in the\ncontinuous domain and bridge the continuous and\nthe discrete domain through embedding and round-\ning (Li et al., 2022; He et al., 2022), for example,\n4765\nDiffusion-LM (Li et al., 2022). Despite the im-\nprovements, most previous works fail to leverage\nthe linguistic features (e.g., words in sentences are\nwith different importance) to noise the input tex-\ntual data and recover it back in a more suitable way.\nBesides, they usually neglect or fail to adapt large\npre-trained language models (PLMs) (Devlin et al.,\n2019; Liu et al., 2019; Yang et al., 2019; Joshi et al.,\n2019; Sun et al., 2019; Clark et al., 2019; Lewis\net al., 2020; Bao et al., 2020; He et al., 2020; Raffel\net al., 2020), which is an unmissable treasure in the\nNLP community: their adopted k-nearest-neighbor\nrounding technique that maps continuous space to\ndiscrete space cannot handle high-dimensional data\nin a stable and efficient way (Li et al., 2022). As a\nresult, a corruption process tailored for languages\nand the objective that allows efficient and straight-\nforward discrete and continuous space transforma-\ntion is in great need. Our Masked-Diffusion LM\nrealizes this extension.\nTo demonstrate the effectiveness of our intro-\nduced Masked-Diffusion LM, we perform experi-\nments on E2E dataset (Novikova et al., 2017) and\n5 controllable generation tasks (Li et al., 2022) in-\ncluding Semantic Content, Parts-of-speech, Syntax\nTree, Syntax Spans, and Length. We observe that\nour Masked-Diffusion LM can (i) achieve the state-\nof-the-art performances compared to recent base-\nline models, and (ii) allow more efficient training\nand inference compared to previous Diffusion-LM.\nTo summarize, our contributions are: (1)We in-\ntroduce a strategic masking noise strategy guided\nby linguistic features to corrupt the textual data in\ndiffusion models for modeling languages. (2) We\nuse linear layers and cross-entropy objectives to\nbridge the continuous and discrete spaces in the\ndiffusion process for efficiency and stability. (3)\nWe conduct experiments on different controllable\ngeneration tasks to demonstrate the effectiveness\nof our proposed methods compared to previous\ndiffusion language models.\n2 Related Work\nDiffusion Models for Language There has been\ngrowing attention in deep generative diffusion mod-\nels, which is a latent variable generative method\nbased on iterative denoising (Sohl-Dickstein et al.,\n2015; Ho et al., 2020; Song et al., 2021). Through\na forward and diffusion process, diffusion models\nhave shown state-of-the-art sample quality on gen-\nerating in the continuous domain such as producing\nimages and audio (Ramesh et al., 2022; Rombach\net al., 2022; Kong et al., 2020; Savinov et al., 2022).\nDespite their huge success, it is still challenging\nand under-explored to adapt diffusion models to dis-\ncrete domains like languages. A few recent works\nhave modified the diffusion models for textual data.\nFor example, discrete forward processes, such as\ncategorical transition kernels (Hoogeboom et al.,\n2021a; Ye et al., 2023), uniform transition kernels,\nand absorbing kernels (Hoogeboom et al., 2021b),\nhave been introduced. However, replacing con-\ntinuous diffusion with a discrete corruption pro-\ncess affords some flexibility (Dieleman et al., 2022;\nZheng et al., 2023; Reid et al., 2022). Other works\nhave also made efforts to model text in the continu-\nous embedding space and applied Gaussian noise\nuniformly to every token (Li et al., 2022; He et al.,\n2022; Chen and Yang, 2023), which is closer to\nthe settings in previous works of diffusion mod-\nels. However, they neglect the inherent linguistic\nfeatures in the text (e.g., different words are play-\ning different roles in sentences) so the generated\ntext often lacks coherence (He et al., 2022). Be-\nsides, the k-nearest-neighbor rounding technique\n(Li et al., 2022; Gao et al., 2022) holds up the de-\ncoding and convergence speed especially when the\nvocabulary is large or the hidden dimension is high,\nthus limiting the potential of combining large pre-\ntrained language models (Devlin et al., 2019; Liu\net al., 2019; Yang et al., 2019; Joshi et al., 2019;\nSun et al., 2019; Clark et al., 2019; Lewis et al.,\n2020; Bao et al., 2020; He et al., 2020; Raffel et al.,\n2020). To alleviate these issues, in our work, we\nintroduce a linguistic-informed soft-masking pro-\ncess to corrupt the discrete and continuous space\nwith structures, and then use linear projections and\ncross-entropy objectives to directly map the latent\nvariables to textual data for better efficiency and\ngenerating better text.\nNon-Autoregressive Text Generation Most lan-\nguage models (Chowdhery et al., 2022; Brown\net al., 2020) and text generation models (Vaswani\net al., 2017a; Eikema and Aziz, 2021; Chen and\nYang, 2020, 2021) follow a left-to-right autore-\ngressive manner. However, the fixed generation\norder prevents the models’ flexibility in editing\nformer text based on later generation results, espe-\ncially for global controllable generation settings.\nTo overcome the limitations, non-autoregressive\ntext modeling has been proposed (Ghazvininejad\net al., 2019; Ren et al., 2020; Gu et al., 2018; Sa-\n4766\nharia et al., 2020; Savinov et al., 2022) through\nmasked language models (Ghazvininejad et al.,\n2019), iterative sequence alignment (Saharia et al.,\n2020), insertion and deletion (Gu et al., 2018),\nor unrolling the generation path (Savinov et al.,\n2022). Our Masked-Diffusion LM achieves the\nnon-autoregressive generation through gradually\nrecovering the intermediate latent variables in a\nplanned sequence from the forward process.\nPlug-and-Play Controllable Generation Our\nwork is also closely related to the line of research\nabout plug-and-play controllable generation meth-\nods (Yang and Klein, 2021; Dathathri et al., 2020;\nKrause et al., 2021; Liu et al., 2021), which mod-\nify the outputs based on extra guidance such as\nclassifiers without changing or fine-tuning the pre-\ntrained language models. Dathathri et al. (2020)\nused gradients to edit the autoregressive language\nmodel’s hidden representations to fulfill the con-\ntrol guidance. Yang and Klein (2021) proposed to\nreweight the predicted token from the language\nmodels while (Krause et al., 2021; Liu et al.,\n2021) further fine-tuned a smaller LM to reweight\nthe token predictions. In this work, we apply\nthe gradient-based plug-and-play approach to our\nMasked-Diffusion LM for controllable generation\nby making classifier-guided gradient updates to the\nintermediate latent variables during the diffusion.\n3 Method: the Masked-Diffusion LM\nIn this section, we describe our introduced Masked-\nDiffusion LM. The overall diagram is shown in Fig-\nure 1 and Algorithm 1,2. Different from the recent\ndiffusion models for languages, e.g., Diffusion-LM\n(Li et al., 2022), which are based on continuous\ndiffusion models, we propose to make corruptions\nin both discrete and continuous space to help mod-\neling the textual data. Specifically, we formulate a\nnovel corruption process as an alternative to Gaus-\nsian diffusion (in Section 3.2) and we directly map\ncontinuous vectors to discrete inputs in every dif-\nfusion step with cross-entropy objectives (in Sec-\ntion 3.3). Moreover, our approach could easily inte-\ngrate pre-trained language models (in Section 3.4).\n3.1 Embedding\nFor the input sentence dwith ltokens d= ˆw1:l, we\nfirst map the discrete tokens to the continuous space\nand form the initial latent variable, X0, through a\nlearnable embedding layer or an encoder e(.):\nX0 = w1:l = e(w1:l). (1)\nThis bridges the discrete space and continuous\nspace. We will then add designed soft-masked\nnoise to the tokens’ representations in the later dif-\nfusion models.\n3.2 Forward Process with Soft-Masking\nDifferent words in sentences play different roles.\nAs a result, when corrupting the sentences and re-\ncovering the sentences, words with various impor-\ntance should be treated differently. Thus, in this\nwork, instead of evenly adding Gaussian noise to\nall the token embeddings like in Diffusion-LM (Li\net al., 2022), we add soft-masked noise to different\ntokens in the input text in different stages to cor-\nrupt the text gradually with structures. Intuitively,\nmore important words would be perturbed with\nsoft-masks in an earlier stage so that the model\ncould be encouraged to generate them in the later\nphase to follow the easy-first-generation nature of\nlanguage planning and generation.\nIn this work, we consider the following aspects\nto measure and define the importance of words in\none sentence:\nWord Relevancy We use the tf-idf weights\n(Dessí et al., 2020), wtf-idf, of the word as one way\nto measure the relevance of wordwin one sentence\nd:\nwtf-idf(w,d) = fw,d∑\nw′∈d fw′,d\nlog N\n1 +|{d∈D: w∈d}|,\n(2)\nwhere the fw,d is the number of times that word w\noccurs in sentence d, N is the number of sentences\nin the corpus, and D is the set of sentences, and\n|{d∈D : w ∈d}|is number of sentences where\nthe word tappears. A higher weight for word win\nsentence din tf–idf means that the word might be\nmore important in the sentence.\nEntropy We also consider measuring the amount\nof information with entropy H (Bentz and Alikani-\notis, 2016; He et al., 2022) in the word wto reflect\nthe importance of that word:\nH(w) =−p(w) log (p(w)) (3)\nwhere p(w) = fw∑V\nj=1 fj\nrepresents the probability\nof word wand f is the word Reluency in the cor-\npus. A word with lower entropy indicates that the\nword might contain less information and thus be\n4767\nFigure 1: The overall process of our Masked-Diffusion LM. In the forward process, soft-mask is added to more\ninformative words earlier to gradually corrupt the input text. For example, NLP is soft-masked prior to stop words\nlike is. Then in the diffusion process, models learn to generate easy words like is first and then fill in more important\nwords such as fun and NLP.\nless important compared to the words with higher\nentropy.\nIn practice, we combine these two measures\n(with normalization) to decide the importance I\nof the word win one sentence dby:\nI(w) = xtf-idf(w,d)∑\nw′∈d wtf-idf(w′,d) + H(w)∑\nw′∈d H(w′).\n(4)\nBased on the introduced importance I of the\nwords in a sentence, we first divide these words\ninto mbuckets {W1:m}. The buckets with lower\nindices include words with higher importance. We\nwill add soft-masked noise to words with higher\nimportance before words with lower importance.\nBy doing this, models could learn to generate the\neasier words first and then generate harder words in\nthe reversed denoising process for better generation\nquality. Specifically, at every step t, we will add\na small amount of Gaussian noise to the hidden\nrepresentation of the word wi in bucket W|tm\nT |:\nq(wi,t+1|wi,t) =N(wi,t+1;\n√\n(1 −βt)wi,t,βtI),\n(5)\nwhere βt is the amount of noise added at diffusion\nstep t.\nWe further apply a square-root noise schedule\nfollowing Li et al. (2022) to gradually increase βt:\nβt = 1−\n√\nt/T + s, (6)\nAlgorithm 1 Forward Process\nInput A sentence X = [x0,...,x n].\nOutput Corrupted hidden representations HT =\n[h0,...,h n].\n1: Encode the sentence into hidden representa-\ntions via an encodere(.): H0 = e(X).\n2: for t= 1,...,K do\n3: Add soft-masking noise to H based\non the importance of tokens (from higher-\nimportance to lower-importance): Ht+1 =\nsoft-masking(Ht)\n4: end for\nwhere s is a small constant that corresponds to\nthe starting noise level. Thus, less noise would be\nadded to harder words to stabilize the training. By\nperforming the above noising steps, initial latent\nvariable X0 is gradually corrputed to a series of\nnoisy latent variables X1:T .\n3.3 Diffusion Process\nAfter the forward process to corrupt the input to-\nkens in sentences dinto latent variables X1:T , we\nthen gradually denoise XT back to X0 through\ndiffusion steps, ˆXt−1 = p( ˆXt|θ), where θ is the\nlearned parameter to model the state transition. In\npractice, we model the transition with Transformers\n(Vaswani et al., 2017b).\nAfter every diffusion step t∈(0,T], instead of\nminimizing the distance between the hidden rep-\n4768\nAlgorithm 2 Diffusion Process\nInput Corrupted hidden representations H =\n[h0,...,h n].\nOutput A sentence X = [x0,...,x n].\n1: Utilize a transition network f(.) to recover the\nlast state: Ht−1 = f(Ht)\n2: Utilize a linear layers to map hidden represen-\ntations to actual tokens Xt−1 = g(Ht−1)\n3: Compute the loss Lt and update the transition\nnetwork.\n4: Do the above steps until it recovers the sen-\ntence.\nresentations of ˆXt−1 and X0 (Li et al., 2022), we\nfirst directly map the continuous space to discrete\nspace using a learnable linear layer f(.) and then\nminimize a weighted cross entropy between the\npredicted sentence and (i) the original sentence d\nand (ii) the masked sentence ˆdat time step t−1:\nLt = γtCE(f( ˆXt−1),d; θ)\n+ (1−γt)CE(f( ˆXt−1), ˆd; θ),t ∈(0,T]\nHere, γt = T−t\nT . In other words, we put higher\nweights on the masked tokens that are masked in\nthis time step during the forward process and put\nlower weights to the other tokens. So the models\nare learned to generate the corresponding masked\ntokens first at every time step.\n3.4 Adapting Pre-trained Language Models\nOur introduced Masked-Diffusion LM also allows\nthe use of large pre-trained language model (De-\nvlin et al., 2019; Liu et al., 2019; Yang et al., 2019;\nJoshi et al., 2019; Sun et al., 2019; Clark et al.,\n2019; Lewis et al., 2020; Bao et al., 2020; He et al.,\n2020; Raffel et al., 2020). In this work, we use\nBERT (Devlin et al., 2019) as an example. To com-\nbine the prior knowledge in large language models,\nit is straightforward to directly replace the embed-\nding layer e(.) with the pre-trained model and use\nthe pre-trained model to get the hidden representa-\ntions of input tokens as the initial state in diffusion\nmodels. We use the final linear layers in pre-trained\nmodels to predict the tokens. For efficiency, in our\nexperiments, when using pre-trained models, we\nfreeze the parameters in them and only learn the\ntransition model θin our Masked-Diffusion LM.\n4 Controllable Text Generation with\nMasked-Diffusion LM\nIn this section, we illustrate how we apply our\nMasked-Diffusion LM to fulfill controllable text\ngeneration. Inspired by recent plug-and-play meth-\nods (Yang and Klein, 2021; Dathathri et al., 2020;\nKrause et al., 2021; Liu et al., 2021), we conduct\ncontrols cfrom external modules (e.g., classifiers)\ndirectly on the latent variables Xt in every inter-\nmediate step t ∈[0,T] in our Masked-Diffusion\nLM:\np(X0:T |c) =\nT∏\nt=1\np(Xt−1 |Xt,c) . (7)\nWe follow the conditional independence assump-\ntion (Yang and Klein, 2021; Dathathri et al., 2020;\nKrause et al., 2021; Liu et al., 2021) and decom-\npose the above joint probability into a sequence of\ncontrol task at every time step t:\np(Xt−1 |Xt,c) ∝p(Xt−1 |Xt) ·p(c|Xt−1,Xt)\n= p(Xt−1 |Xt) ·p(c|Xt−1).\n(8)\nAs a result, for the t-th step, we run gradient\nupdates on Xt to generate Xt−1:\n∇Xt−1 log p(Xt−1 |Xt,c) =λ∇Xt−1\nlog p(Xt−1 |Xt) +∇Xt−1 log p(c|Xt−1) ,\n(9)\nwhere both log p(Xt−1|Xt) and log p(c|Xt−1) are\ndifferentiable: the first term is parametrized by the\ntransition Transformers, θ, in Masked-Diffusion\nLM, and the second term is parametrized by extra\nneural network classifiers. Note that the extra clas-\nsifiers are trained with the diffusion latent variables\nas input to allow direct gradient updates on the la-\ntent space. Note that λis a fluency regularization\nhyper-parameter to balance the fluency (gradient\nupdates from Masked-Diffusion LM) and control\n(gradient updates from classifiers) in order to fur-\nther improve the generation quality.\nFor the decoding strategy, following Li et al.\n(2022), the Minimum Bayes Risk (MBR) decoding\n(Kumar and Byrne, 2004) is used to aggregate and\nselect the sample that has the lowest expected loss\nunder the specified loss function from the Masked-\nDiffusion LM.\n5 Experiments\n5.1 Datasets\nIn this work, we train our Masked-Diffusion LM\non the E2E datasets (Novikova et al., 2017), which\n4769\nSemantic Content POS Syntax Tree Syntax Spans LengthMethods Acc Fluency Acc Fluency Acc Fluency Acc Fluency Acc Fluency\nPPLM 9.9 5.32 - - - - - - - -\nFUDUGE 69.9 2.83 27.0 7.96 17.9 3.39 54.2 4.03 46.9 3.11\nDiffusion-LM 81.2 2.55 90.0 5.16 86.0 3.71 93.8 2.53 99.9 2.16\n+ BERT 77.4 2.68 86.2 5.43 82.3 3.92 89.3 3.13 99.9 2.68\nMasked-Diffusion LM † 81.9 2.35 91.6 5.03 86.6 3.66 94.7 2.48 99.9 2.13\n+ BERT † 82.9 2.30 92.9 4.78 89.7 3.44 95.8 2.33 100 2.08\nTable 1: Main Results. The Accuracy (↑) and the Fluency (↓) of different methods on five controllable generation\ntasks including semantic content, POS, syntax tree, syntax spans and length. †indicates our methods.\nMethods Training (h) Inference (s)\nDiffusion-lm 8.0 80\n+BERT 15.2 920\nMasked-Diffusion LM 3.4 68\n+BERT 4.8 700\nTable 2: Training time and inference time (generating\n50 samples) for different models.\nconsists of 50K restaurant reviews together with\nthe labels in terms of food type, price, and customer\nratings.\nFollowing Li et al. (2022), we conduct 5 con-\ntrol tasks to evaluate the learned Masked-Diffusion\nlanguage model:\n• Semantic Content. For a given field (e.g.,\nfood) and value (e.g., Japanese), sentences\nthat covers field=value need to be generated.\nWe evaluate the accuracy of the generated\nsentence by examine the exact match rate of\n“value” (word mention).\n• Parts-of-speech. For a given sequence of\nparts-of-speech (POS) tags (e.g., Noun Verb\nDeterminer Noun), the models need to pro-\nduce the sentence with the same length and\nfollow the exact given POS tag sequence (e.g.,\nBirds eat the warms). We evaluate the accu-\nracy of the generation by checking the word-\nlevel POS tag exact match (under an oracle\nPOS tagger).\n• Syntax Tree. For a given syntactic parse tree,\nthe generated sentence should have the same\nparse tree. We evaluate the accuracy by first\nparsing the generated sentence with an off-the-\nshelf parser and report the F1 scores compared\nto the given parse.\n• Syntax Spans. For a given (span, syntactic\ncategory) pair (e.g., (2, 5, VP)), the parse tree\nof the generated sentence should match the\ngiven syntactic category over the given spans.\nWe evaluate the accuracy of the sentence by\nthe exact match rate of the given spans.\n• Length. For a given target length (e.g., 20),\nthe models need to generate a sentence within\n±2 of the given target. We evaluate the accu-\nracy by the match rate of the sentence lengths.\nFor every control task, we sample 200 control\ntargets cfrom the validation splits, and we gener-\nate 50 samples for each control target. The first\nfour tasks rely on a classifier to guide the diffu-\nsion, and the last one task is classifier free. To\nfurther evaluate the fluency of the generated sen-\ntences from models, we use a teacher LM (i.e., a\ncarefully fine-tuned GPT-2 model) and report the\nperplexity of generated text under the teacher LM.\nA lower perplexity indicates better sample quality\nand fluency.\n5.2 Baselines\nWe compare our Masked-Diffusion LM with the\nfollowing state-of-the-art baselines on controllable\ngeneration tasks:\n• PPLM (Dathathri et al., 2020) runs gradient\nascent on the pre-trained language models’\nhidden representations to increase the classi-\nfier probabilities and language model proba-\nbilities.\n• FUDGE (Yang and Klein, 2021) reweights\nthe predicted tokens from the pre-trained lan-\nguage models by a discriminator which takes\nin a prefix sequence and predicts whether\nthe complete sequence would satisfy the con-\nstraint.\n• Diffusion-LM (Li et al., 2022) learns an em-\nbedding to map discrete text into the con-\ntinuous space where it performs Gaussian\n4770\nMethods Semantic Content POS Syntax Tree Syntax Spans Length\nDiffusion-lm 2.89 2.76 3.16 2.88 2.46\n+BERT 3.87 3.46 3.72 3.68 3.34\nMasked-Diffusion LM 2.56 2.48 2.88 2.35 2.18\n+BERT 1.32 1.28 1.16 1.55 1.86\nTable 3: The average ranking every method receives from human evaluation (lower is better).\nNoise Type Semantic Content\nAcc Fluency\nGaussian 75.3 3.01\nRandom Mask 78.8 2.67\nMask w. POS 80.4 2.58\nMask w. Entropy 81.1 2.44\nMask w. Rel 80.8 2.52\nMask w. Entropy+Rel † 81.6 2.38\nTable 4: Performances on Semantic Content of Masked-\nDiffusion LM with different types of noise applied in\nforward noising process. †indicates our method.\ndiffusion process. Also, a rounding step is\ndesigned to map the embeddings back into\ndiscrete texts. For every control task, the\nDiffusion-LM infuses the controlling signals\nin every diffusion step.\n5.3 Experimental Setting\nWe use a Transformer with 80M parameters to pa-\nrameterize our Masked-Diffusion LM, with a se-\nquence length n = 64, diffusion steps T = 500,\nand a square-root noise schedule. For Masked-\nDiffusion LM, we set the hidden dimension to\n128. We set the number of word buckets m =\n3. When combining pre-trained models, we in-\ncorporate BERT-base (Devlin et al., 2019) with\nabout 110M parameters. We use BERT to en-\ncode the input text into vectors with dimension\nof 768 and freeze the parameters in BERT. We\nlearn Masked-Diffusion LM with the AdamW op-\ntimizer (Loshchilov and Hutter, 2019) for 20,000\nsteps with learning rate of 3e-4, dropout probabil-\nity of 0.1, and batch size of 32. We use a linear\nwarmup schedule starting with 1,000 warmup steps.\nAll experiments are conducted on NVIDIA A100\nTensor Core GPUs. We use 4 GPUs for training\nand a single GPU for sampling.\n5.4 Results\nWe show the main results on five controllable gener-\nation tasks in Table 1. When the diffusion process\nis engaged, the performances on all the controlled\ngeneration tasks receives significant boosts (e.g.,\n81.2 of Diffusion-LM vs. 69.9 if FUDUGE on\nSemantic Content task), suggesting the superior-\nity of the diffusion model on controllable genera-\ntion tasks. While the previous Diffusion-LM can\nnot be well combined with large language model\nlike BERT (e.g., a 5% drop on Semantic Content\naccuracy), largely due to the fact that their way\n(rounding) to bridge continuous space and discrete\nspace suffers from significantly higher dimensions.\nCompared to Diffusion-LM, our proposed Masked-\nDiffusion LM consistently outperforms the previ-\nous models in all tasks (e.g., a 1.7% improvement\non the POS task), indicating the effectiveness of\nour introduced linguistic-informed noise forward\nprocess. Also, when combined with large language\nmodels like BERT, our method significantly out-\nperforms the previous methods, demonstrating that\nour approach can be well aligned with pre-trained\nmodels.\nEfficiency We also display the training cost and\ninference cost in Table 2. Compared to the previous\nDiffusion-LM, our method requires significantly\nless training time to converge and needs less infer-\nence time to generate sentences. This is because\nour introduced noise process is more stable and\nsuitable for modeling languages. Besides, the ob-\njectives we introduced are more efficient than the\nrounding techniques in previous work.\nHuman Evaluation We then conduct human\nevaluation to evaluate the generated conversations\nqualitatively. We ask native speakers of English\nfrom Amazon Mechanical Turk to rank the qual-\nity of 50 generated sentences (randomly sam-\npled) from different models for every control task.\nSpecifically, annotators need to rank different sys-\ntem outputs based on the (i) fluency (whether the\n4771\nMethods Semantic Content POS Syntax Tree Syntax Spans Length\nAcc fluency Acc fluency Acc fluency Acc fluency Acc fluency\nL2 81.1 2.44 90.6 5.17 86.2 3.68 94 2.51 99.8 2.14\nL2-BERT 80.1 2.48 89.4 5.82 84.1 3.91 93.2 2.88 99.9 2.89\nCE † 81.9 2.35 91.6 5.03 86.6 3.66 94.7 2.48 99.9 2.13\nCE-BERT † 82.9 2.30 92.9 4.78 89.7 3.44 95.8 2.33 100 2.08\nTable 5: Performances of Masked-Diffusion LM trained with different objectvies on controllable generation tasks. †\nindicates our method.\nCase Study Sentences\nInput 7\nt = 500 [mask] [mask] [mask] [mask] [mask] [mask] [mask]\nt = 400 [mask] is an [mask] restaurant .\nt = 200 The [mask] is an Indian restaurant .\nt = 0 The Mill is an Indian restaurant .\nInput name : Travellers Rest Beefeater\nt = 500 [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask]\nt = 400 [mask] Rest [mask] is a [mask] [mask] [mask] that is [mask] .\nt = 200 Travellers Rest [mask] is a reasonably [mask] restaurant that is awesome .\nt = 0 Travellers Rest Beefeater is a reasonably priced restaurant that is awesome .\nTable 6: Examples of the intermediate generated text of our Masked-Diffusion LM on the Length and Semantic\nContent tasks.\ngiven sentence is readable and fluent) and (ii) the\ncontrollability (whether the given sentence match\nthe given control conditions). To increase anno-\ntation quality, we require turkers to have a 98%\napproval rate with over 10,000 approved tasks for\ntheir previous work. The pay rate was $0.15 per hit.\nEvery example is assessed by 3 annotators, and the\nrank for every sentence is aggregated by majority\nvoting. The Intra-Class Correlation ( ICC1k) was\n0.63, indicating moderate agreement (Koo and Li,\n2016). The results are shown in Table 3. As it\nshows, our proposed Masked-Diffusion LM and\nits variation with BERT received the best average\nranks, suggesting the effectiveness of our proposed\ndiffusion modeling strategy for languages.\n5.5 Ablation Studies\nWe then perform ablation studies to demonstrate\nthe effectiveness of our introduced linguistic-\ninformed noise and the cross entropy objectives.\nNoise Strategy We first demonstrate the per-\nformances on Semantic Content task of Masked-\nDiffusion LM with different types of noise strategy\nin Table 4. Gaussian adds Gaussian noise to all the\ntokens in the input sentence in the forward process\nfollowing Li et al. (2022). We also compare dif-\nferent masking noise strategies: (i) Random Mask,\nwhere the soft-mask is added to tokens in a random\norder. (ii) Mask with POS, where the soft-mask per-\nturbs the tokens in an order (noun →verb →other\nwords) based on POS tags. Our introduced noise\nstrategy (Mask with Entropy and Reluency) shows\nsignificantly better performances on semantic con-\ntent generation. This indicates that our introduced\nnoise strategy that considers the linguistic features\nin sentences is providing more appropriate pertur-\nbation to the textual data for the diffusion process.\nObjectives We further show the impact of differ-\nent objectives in Table 5. We compare our used\ncross entropy objectives with the L2 object that\nis used in Li et al. (2022) where they minimize\nthe distance between latent intermediate variables\nand the initial latent variable instead of directly\npredicting the text. We observe that cross entropy\nobjectives slightly perform better than L2 when\nthe pre-trained model is not used. After combin-\ning with large language models, CE-BERT signif-\nicantly outperforms the L2-BERT, indicating the\neffectiveness of our introduced objectives in terms\nof incorporating large language models.\n5.6 Case Studies\nWe also include some examples of intermediate\nsteps of Masked-Diffusion LM in Table 6. In the de-\nnoising diffusion process, easy words are generated\nfirst. For example, “ is”, “an”, and “ restaurant”.\n4772\nWith more diffusion steps, sentences are enriched\nwith more informative words such as “ Mill” and\n“Indian”. It shows that our Masked-Diffusion LM\nencourages the generation to follow an easy-first\norder for stable and better generation quality.\n6 Conclusion\nIn this work, we present a novel diffusion model for\nlanguage, Masked-Diffusion LM, which corrupts\nthe discrete text with a linguistic-informed soft-\nmasking strategy and then iteratively denoises them\nback by directly predicting the text. Specifically,\nwe gradually soft-mask the tokens in the sentence\nfollowing an order from more informative words to\nless informative words in the forward process. This\nsatisfies the flexibility for diffusion models, as well\nas encourages the easy-first-generation nature in\nthe denoising process for better generation quality.\nAlso, we directly predict the discrete token during\nthe diffusion process with the cross-entropy loss\nto stabilize the intermediate diffusion steps and\nmake our approach orthogonal to large pre-trained\nlanguage models. Experiments on E2E dataset\nand five controllable generation tasks including\nSemantic Content, Parts-of-speech, Syntax Tree,\nSyntax Spans, and Length show that our Masked-\nDiffusion LM can (i) achieve the state-of-the-art\nperformances compared to recent baseline models\nand (ii) allow more efficient training and inference\ncompared to the previous Diffusion-LM.\n7 Limitations\nIn this work, we mainly leverage linguistic soft-\nmasking such as word relevancy and word entropy.\nWe encourage future work to explore how to in-\ncorporate other linguistic structures to design the\nnosing process. And we mainly test with smaller\nmodels like simple transformer models as well as\nBERT-based models. Future work might test with\nlarger pre-trained models to evaluate whether dif-\nfusion methods would work better or not. Also, we\nfocused on controllable generation to evaluate the\nmodels. Future work may study different down-\nstream tasks.\nReferences\nJacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel\nTarlow, and Rianne van den Berg. 2021. Structured\ndenoising diffusion models in discrete state-spaces.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-\nfeng Gao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for unified language model\npre-training. arXiv preprint arXiv:2002.12804.\nChristian Bentz and Dimitrios Alikaniotis. 2016. The\nword entropy of natural languages.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiaao Chen and Diyi Yang. 2020. Multi-view sequence-\nto-sequence models with conversational structure for\nabstractive dialogue summarization. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 4106–\n4118, Online. Association for Computational Lin-\nguistics.\nJiaao Chen and Diyi Yang. 2021. Structure-aware ab-\nstractive conversation summarization via discourse\nand action graphs. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1380–1391, Online. As-\nsociation for Computational Linguistics.\nJiaao Chen and Diyi Yang. 2023. Controllable con-\nversation generation with conversation structures via\ndiffusion models. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 7238–\n7251.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\n4773\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nInternational Conference on Learning Representa-\ntions.\nDanilo Dessí, Rim Helaoui, Vivek Kumar, Diego Re-\nforgiato Recupero, and Daniele Riboni. 2020. Tf-idf\nvs word embeddings for morbidity identification in\nclinical notes: An initial study.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nSander Dieleman, Laurent Sartran, Arman Roshan-\nnai, Nikolay Savinov, Yaroslav Ganin, Pierre H.\nRichemond, Arnaud Doucet, Robin Strudel, Chris\nDyer, Conor Durkan, Curtis Hawthorne, Rémi\nLeblond, Will Grathwohl, and Jonas Adler. 2022.\nContinuous diffusion for categorical data.\nBryan Eikema and Wilker Aziz. 2021. Sampling-based\napproximations to minimum bayes risk decoding for\nneural machine translation.\nZhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang\nZhang, Jiang Bian, and Linli Xu. 2022. Difformer:\nEmpowering diffusion model on embedding space for\ntext generation. arXiv preprint arXiv:2212.09412.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6112–\n6121, Hong Kong, China. Association for Computa-\ntional Linguistics.\nShansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,\nand Lingpeng Kong. 2022. Diffuseq: Sequence to\nsequence text generation with diffusion models.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O.K.\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In International Confer-\nence on Learning Representations.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nZhengfu He, Tianxiang Sun, Kuanning Wang, Xuan-\njing Huang, and Xipeng Qiu. 2022. Diffusionbert:\nImproving generative masked language models with\ndiffusion models.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models.\nEmiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bast-\nings, Ben Poole, Rianne van den Berg, and Tim Sali-\nmans. 2021a. Autoregressive diffusion models.\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini,\nPatrick Forré, and Max Welling. 2021b. Argmax\nflows and multinomial diffusion: Learning categori-\ncal distributions.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. 2020. Diffwave: A versatile diffu-\nsion model for audio synthesis.\nTerry K Koo and Mae Y Li. 2016. A guideline of\nselecting and reporting intraclass correlation coeffi-\ncients for reliability research. Journal of chiropractic\nmedicine, 15(2):155–163.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2021. GeDi: Gener-\native discriminator guided sequence generation. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 4929–4952, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nShankar Kumar and William Byrne. 2004. Minimum\nBayes-risk decoding for statistical machine transla-\ntion. In Proceedings of the Human Language Tech-\nnology Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHLT-NAACL 2004, pages 169–176, Boston, Mas-\nsachusetts, USA. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2020. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\nSCL.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori B. Hashimoto. 2022. Diffusion-\nlm improves controllable text generation.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. DExperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6691–6706, Online. Association for Computational\nLinguistics.\n4774\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nAlex Nichol and Prafulla Dhariwal. 2021. Improved\ndenoising diffusion probabilistic models.\nJekaterina Novikova, Ondˇrej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue ,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with clip latents.\nMachel Reid, Vincent J Hellendoorn, and Graham Neu-\nbig. 2022. Diffuser: Discrete diffusion via edit-based\nreconstruction. arXiv preprint arXiv:2210.16886.\nYi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng\nZhao, and Tie-Yan Liu. 2020. A study of non-\nautoregressive model for sequence generation.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 10674–\n10685.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho,\nDavid J Fleet, and Mohammad Norouzi. 2022. Pho-\ntorealistic text-to-image diffusion models with deep\nlanguage understanding.\nChitwan Saharia, William Chan, Saurabh Saxena, and\nMohammad Norouzi. 2020. Non-autoregressive ma-\nchine translation with latent alignments. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n1098–1108, Online. Association for Computational\nLinguistics.\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski,\nErich Elsen, and Aaron van den Oord. 2022. Step-\nunrolled denoising autoencoders for text generation.\nIn International Conference on Learning Representa-\ntions.\nJascha Sohl-Dickstein, Eric Weiss, Niru Mah-\neswaranathan, and Surya Ganguli. 2015. Deep un-\nsupervised learning using nonequilibrium thermody-\nnamics. In Proceedings of the 32nd International\nConference on Machine Learning, volume 37 of Pro-\nceedings of Machine Learning Research, pages 2256–\n2265, Lille, France. PMLR.\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2021.\nDenoising diffusion implicit models. In International\nConference on Learning Representations.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017a. Attention is all\nyou need.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017b. Attention is\nall you need. In Advances in neural information\nprocessing systems, pages 5998–6008.\nKevin Yang and Dan Klein. 2021. FUDGE: Controlled\ntext generation with future discriminators. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3511–3535, Online. Association for Computational\nLinguistics.\nLing Yang, Zhilong Zhang, Yang Song, Shenda Hong,\nRunsheng Xu, Yue Zhao, Yingxia Shao, Wentao\nZhang, Bin Cui, and Ming-Hsuan Yang. 2022. Dif-\nfusion models: A comprehensive survey of methods\nand applications.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in neural informa-\ntion processing systems, pages 5754–5764.\nJiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and\nMingxuan Wang. 2023. Dinoiser: Diffused con-\nditional sequence learning by manipulating noises.\narXiv preprint arXiv:2302.10025.\nLin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong.\n2023. A reparameterized discrete diffusion model for\ntext generation. arXiv preprint arXiv:2302.05737.\n4775",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7699587345123291
    },
    {
      "name": "Categorical variable",
      "score": 0.5579344630241394
    },
    {
      "name": "Noise reduction",
      "score": 0.5207026600837708
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5135893821716309
    },
    {
      "name": "Noise (video)",
      "score": 0.4794263243675232
    },
    {
      "name": "Diffusion",
      "score": 0.4446132779121399
    },
    {
      "name": "Dimension (graph theory)",
      "score": 0.420200377702713
    },
    {
      "name": "Language model",
      "score": 0.4196026623249054
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3476562798023224
    },
    {
      "name": "Algorithm",
      "score": 0.33798664808273315
    },
    {
      "name": "Machine learning",
      "score": 0.22882205247879028
    },
    {
      "name": "Mathematics",
      "score": 0.1484086513519287
    },
    {
      "name": "Image (mathematics)",
      "score": 0.09059488773345947
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ]
}