{
  "title": "Potential Multidisciplinary Use of Large Language Models for Addressing Queries in Cardio‐Oncology",
  "url": "https://openalex.org/W4392926681",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2110450295",
      "name": "Pengfei Li",
      "affiliations": [
        "Qingdao University",
        "Affiliated Hospital of Qingdao University"
      ]
    },
    {
      "id": "https://openalex.org/A2109014916",
      "name": "Xuejuan Zhang",
      "affiliations": [
        "Qingdao University",
        "Affiliated Hospital of Qingdao University"
      ]
    },
    {
      "id": "https://openalex.org/A2146210838",
      "name": "Erjia Zhu",
      "affiliations": [
        "Shanghai Pulmonary Hospital",
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2101440403",
      "name": "Shijun Yu",
      "affiliations": [
        "Shanghai East Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2131212651",
      "name": "Bin Sheng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2006203058",
      "name": "Yih‐Chung Tham",
      "affiliations": [
        "Singapore Eye Research Institute",
        "Singapore National Eye Center",
        "National University of Singapore",
        "Duke-NUS Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A2394853576",
      "name": "Tien Yin Wong",
      "affiliations": [
        "Duke-NUS Medical School",
        "Singapore Eye Research Institute",
        "Tsinghua University",
        "Beijing Tsinghua Chang Gung Hospital",
        "Singapore National Eye Center"
      ]
    },
    {
      "id": "https://openalex.org/A2104297712",
      "name": "Hongwei Ji",
      "affiliations": [
        "Beijing Tsinghua Chang Gung Hospital",
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4293217929",
    "https://openalex.org/W4319062614"
  ],
  "abstract": "n the crossroads of digital health and education, large language models (LLMs) emerge as tools with great potential. 1Trained on expansive textual data sets, these state-of-the-art artificial intelligence models can generate multidisciplinary content, answer intricate queries, and accelerate information delivery. 1articularly in the field of cardio-oncology, which combines cardiac and oncological expertise, LLMs have the potential to provide valuable insights to specialists like cardiologists and oncologists. 2This is useful in situations in which standard guidelines are not immediately available or when there is a need to combine a vast amount of interdisciplinary information.However, the performances of LLMs in this context remains largely unknown.This study aims to benchmark these state-of-the-art artificial intelligence models in their ability to handle the interdisciplinary queries inherent in cardio-oncology, where integrative insights from cardiology and oncology are crucial.The data that support the findings of this study are available from the last author upon reasonable request.Our study, conducted between October 02, 2023 and October 12, 2023 compiled 25 questions according to the 2022 European Society of Cardiology guideline on cardio-oncology 3 (Table ).Each query was individually and independently posed to 5 LLMs: ChatGPT-3.5,ChatGPT-4.0,Bard, Llama 2, and Claude 2, generating a total of 25 responses per chatbot.We format all generated responses as plain text and stripped of any identifying details (eg, remarks like \"I'm not a doctor\" from ChatGPT).Responses were randomly shuffled within their respective question sets, ensuring that the reviewers remained unaware of LLMspecific responses.Two experienced attending-level physicians independently assessed the responses in 5 separate rounds, each conducted on a distinct day, with an overnight washout period to minimize memory bias (Table ).This study did not involve human subjects; institutional review board approval and informed consent were waived.The mean±SD of the word count was 386±91 for ChatGPT-3.5,386±96 for ChatGPT-4.0,340±78 for Google Bard, 360±96 for Meta Llama 2, and 203±27 for Anthropic Claude 2 (P<0.001).The preliminary results indicated that ChatGPT-4 provided 17 of 25 (68%) appropriate responses, followed by Bard, Claude 2, and ChatGPT-3.5 with 13 of 25 (52%), and Llama 2 with 12 of 25 (48%; P=0.653).A notable area of concern was that in the treatment and prevention domain; all 5 LLM-Chatbots earned either borderline or poor",
  "full_text": null,
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.7530602216720581
    },
    {
      "name": "Beijing",
      "score": 0.7514992952346802
    },
    {
      "name": "China",
      "score": 0.712622344493866
    },
    {
      "name": "Multidisciplinary approach",
      "score": 0.48279869556427
    },
    {
      "name": "Family medicine",
      "score": 0.4664176404476166
    },
    {
      "name": "Library science",
      "score": 0.4265919625759125
    },
    {
      "name": "Geography",
      "score": 0.12214463949203491
    },
    {
      "name": "Political science",
      "score": 0.09064298868179321
    },
    {
      "name": "Computer science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}