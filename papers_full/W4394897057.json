{
  "title": "Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features",
  "url": "https://openalex.org/W4394897057",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5095747986",
      "name": "Areeba Ishtiaq",
      "affiliations": [
        "Khwaja Fareed University of Engineering and Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2126569680",
      "name": "Kashif Munir",
      "affiliations": [
        "Khwaja Fareed University of Engineering and Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098649840",
      "name": "Ali Raza",
      "affiliations": [
        "University of Lahore"
      ]
    },
    {
      "id": "https://openalex.org/A4221689051",
      "name": "Nagwan Abdel Samee",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A4280965834",
      "name": "Mona M. Jamjoom",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A1971499972",
      "name": "Zahid Ullah",
      "affiliations": [
        "King Abdulaziz University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4392672410",
    "https://openalex.org/W4313192032",
    "https://openalex.org/W4392542274",
    "https://openalex.org/W4391250687",
    "https://openalex.org/W4389047062",
    "https://openalex.org/W4387077860",
    "https://openalex.org/W4391127198",
    "https://openalex.org/W4390663168",
    "https://openalex.org/W6991567676",
    "https://openalex.org/W4386319767",
    "https://openalex.org/W4392244975",
    "https://openalex.org/W7005309526",
    "https://openalex.org/W4391259755",
    "https://openalex.org/W4294234212",
    "https://openalex.org/W3006026155",
    "https://openalex.org/W4391360411",
    "https://openalex.org/W3005758751",
    "https://openalex.org/W4225139287",
    "https://openalex.org/W3164947024",
    "https://openalex.org/W4220970138",
    "https://openalex.org/W6638995678",
    "https://openalex.org/W2136891251",
    "https://openalex.org/W4387129526",
    "https://openalex.org/W4392826394",
    "https://openalex.org/W4392710040",
    "https://openalex.org/W4391759520",
    "https://openalex.org/W4386825037",
    "https://openalex.org/W4391390101",
    "https://openalex.org/W4390767899",
    "https://openalex.org/W4390354386",
    "https://openalex.org/W4391305663",
    "https://openalex.org/W4390841728",
    "https://openalex.org/W1863666956"
  ],
  "abstract": "Nowadays global market products are readily accessible worldwide, and a vast array of reviews across numerous platforms are posted daily in several categories, making it challenging for customers to stay informed about their product interests. To make informed decisions regarding product quality, users require access to reviews and ratings. Owners and managers must analyze customer ratings and the underlying emotional content of reviews to enhance the product&#x2019;s quality, cost, customer service, and environmental impact. The primary aim of our proposed research is to accurately predict product helpfulness through customer reviews using the Large Language Model (LLM), thereby assisting customers in saving time and money. We employed a benchmark dataset, the Amazon Fine Food Reviews, to develop numerous advanced machine-learning techniques. We introduced a novel transformer approach BERF (BERT Random Forest) for feature engineering to enhance the value of user evaluations for Amazon&#x2019;s gourmet food products. The BERF method utilizes BERT embeddings and class probability features derived from product helpfulness online reviews textual data. We have balanced the dataset using the Synthetic Minority Over-sampling TEchnique (SMOTE) approach. Our comprehensive study results demonstrated that the Light Gradient Boosting Machine (LGBM) strategy outperformed existing state-of-the-art approaches, achieving an accuracy of 98%. The performance of each method is confirmed using a k-fold method and further improved through hyperparameter optimization. Our innovative study employing a transformer model has significantly enhanced the utility of customer reviews, substantially reducing online product scams and preventing wasted time and money.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.1 120000\nProduct Helpfulness Detection With Novel\nTransformer Based BERT Embedding and Class\nProbability Features\nAREEBA ISHTIAQ1, KASHIF MUNIR1,*, ALI RAZA2, NAGWAN ABDEL SAMEE3,*, MONA M.\nJAMJOOM 4, AND ZAHID ULLAH5\n1Institute of Information Technology, Khwaja Fareed University of Engineering and Information Technology, Rahim Yar Khan 64200, Pakistan; (e-mail: ;\nkashif.munir@kfueit.edu.pk;)\n2Department of Software Engineering, University Of Lahore, Lahore, Pakistan; (e-mail: ali.raza.scholarly@gmail.com)\n3Department of Information Technology, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, P.O. Box 84428, Riyadh\n11671, Saudi Arabia; (e-mail:nmabdelsamee@pnu.edu.sa )\n4Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh 11671, Saudi Arabia;\n(e-mail: mmjamjoom@pnu.edu.sa)\n5Department of information system, King Abdulaziz University, Jeddah, Saudi Arabia; (e-mail: zasultan@kau.edu.sa)\nCorresponding author: Kashif Munir (e-mail: kashif.munir@kfueit.edu.pk) and Nagwan Abdel Samee (e-mail:\nnmabdelsamee@pnu.edu.sa).\nThis research was funded by Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2024R104),\nPrincess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia.\nABSTRACT Nowadays global market products are readily accessible worldwide, and a vast array of reviews\nacross numerous platforms are posted daily in several categories, making it challenging for customers to stay\ninformed about their product interests. To make informed decisions regarding product quality, users require\naccess to reviews and ratings. Owners and managers must analyze customer ratings and the underlying\nemotional content of reviews to enhance the product’s quality, cost, customer service, and environmental\nimpact. The primary aim of our proposed research is to accurately predict product helpfulness through\ncustomer reviews using the Large Language Model (LLM), thereby assisting customers in saving time\nand money. We employed a benchmark dataset, the Amazon Fine Food Reviews, to develop numerous\nadvanced machine-learning techniques. We introduced a novel transformer approach BERF (BERT Random\nForest) for feature engineering to enhance the value of user evaluations for Amazon’s gourmet food\nproducts. The BERF method utilizes BERT embeddings and class probability features derived from product\nhelpfulness online reviews textual data. We have balanced the dataset using the Synthetic Minority Over-\nsampling TEchnique (SMOTE) approach. Our comprehensive study results demonstrated that the Light\nGradient Boosting Machine (LGBM) strategy outperformed existing state-of-the-art approaches, achieving\nan accuracy of 98%. The performance of each method is confirmed using a k-fold method and further\nimproved through hyperparameter optimization. Our innovative study employing a transformer model has\nsignificantly enhanced the utility of customer reviews, substantially reducing online product scams and\npreventing wasted time and money.\nINDEX TERMS Product Helpfulness, Large Language Model (LLM), Machine Learning, Deep Learning,\nText Mining, BERT, Transformer.\nI. INTRODUCTION\nT\nHE last decade has shown a significant increase in the\navailability of product reviews on traditional retail sites\n[1], in both professional and individual formats. To reduce\nthe uncertainty associated with purchasing products, users\nconsult these reviews and pay attention to online information\nsuch as images. According to a research study, most users\nprioritize customer reviews before making a purchase [2].\nIn other words, nearly 90% of consumers check reviews\nbefore buying a product. Reviews are becoming increasingly\nimportant for both consumers and businesses. For consumers,\nonline reviews are crucial in deciding whether or not to pur-\nchase a product [3]. Exclusive online retailers are focusing\non improving the management of reviews, understanding that\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\npositive reviews can significantly impact profit gains, and cre-\nating an environment where customer reviews are an integral\npart of the business.\nAmazon is a multinational technology corporation spe-\ncializing in e-commerce [4], where reviews are crucial for\npurchasing decisions, providing vital and genuine insights.\nHowever, reading through all the reviews of a chosen article\ncan be time-consuming. Reviews are not only important for\nbuyers but also for vendors, who rely on them to differentiate\nand promote their products [5]. In the context of business\ngrowth and customer care, online ratings play a significant\nrole. Customers can easily assess a product’s appeal by re-\nviewing its ratings. The decision to purchase a product often\ndepends on its ratings and reviews, which can create a positive\nor negative impression. Most research in this area has utilized\nonline feedback from Amazon to predict review helpfulness\n[6], with each review accompanied by data indicating the\nnumber of people who found it helpful. In the field of e-\ncommerce, product ratings have become increasingly impor-\ntant.\nThe significance of review ratios has increased during the\nCOVID-19 pandemic, which began in 2020, as commercial\ntransactions have shifted towards being conducted electroni-\ncally over the internet [7]. This shift has led to a 43% increase\nin the ratio of online reviews, and it continues to rise over\ntime. Reviews offer a time-saving way to purchase products,\nas ratings provide helpful and valuable recommendations [8].\nOn the other hand, sellers are also interested in review anal-\nysis to understand customer interests better and achieve suc-\ncessful product sales. Researchers have increasingly focused\non predicting the helpfulness of reviews by employing a range\nof Machine Learning (ML) methods [9]. Machine learning\naims to identify significant patterns and gain knowledge from\ndata. Information overload on internet review sites has signif-\nicantly hindered buyers’ ability to assess product or business\nquality when making purchases. The growth of social media\nhas made it harder to differentiate between genuine content\nand advertising, leading to a surge in misleading evaluations\nin the market. The usefulness of a review depends on a voting\nmechanism [10].\nSeveral neural network methods have been developed to\nautomatically assess the usefulness of consumer product re-\nviews [11]. Many current models rely on basic explanatory\nfactors, particularly those derived from substandard evalua-\ntions that may be deceptive and result in uncertainty. Effective\nfeature selection is crucial for forecasting the usefulness of\nonline consumer reviews. The transformer-based BERT, a\nnewly evolved language representation approach [12], can\nachieve state-of-the-art outcomes on many natural language\nprocessing works. Our main insight is to provide a platform\nwhere the purchase of products and customer decisions can\nbe related to the responses of experienced buyers. As a con-\nclusion, a methodical technique is required to handle big data.\nOur primary research contributions to the helpfulness of\ncustomer reviews are as follows:\n• We proposed a novel transformer-based BERF method,\nwhich generates BERT embeddings and class probabil-\nity features from product helpfulness online reviews.\nThe newly generated salient feature set is then utilized\nto build advanced machine-learning models.\n• We employed a fine-tuned BERT model and four so-\nphisticated machine-learning methods for detecting the\nproduct’s helpfulness. K-fold validation is utilized to\nvalidate the performance results of the models, and hy-\nperparameter adjustment is employed to improve per-\nformance efficacy. In addition, we have balanced the\ndataset using the SMOTE approach.\nSubsequent sections of the research are structured as fol-\nlows: Section II comprises the literature work analysis. Sec-\ntion III outlines our proposed research approach. Section IV\nassesses the outcomes of the approaches used in the compar-\nison. The primary discoveries are detailed in Section V.\nII. LITERATURE REVIEW\nScientists have described the distinctive characteristics of\nconsumer product reviews and forecasted their usefulness.\nLarge companies gather data from online sources to facilitate\nusers with product recommendation systems. Major corpora-\ntions, such as Yelp, Spotify, and Amazon, rely on and require\nthe assessment of product review helpfulness to achieve better\nresults and revenue from customers. The increasing usage of\nsocial media has made it difficult to differentiate between\nauthentic, useful reviews and advertisements.\nThe literature study focusing on state-of-the-art applied\napproaches to performance is detailed in Table 1.\nThis research [10] utilizes the Amazon product review\ndataset, spanning from May 1996 to October 2018. The\ndataset comprises approximately 233.1 million entries across\n29 distinct product categories (such as Office Products, Pet\nSupplies, Grocery Gourmet Food, etc.) and includes 11\ncolumns. The research findings indicate a high accuracy in\npredicting the usefulness of Amazon product reviews. The\nimprovement from the initial model, which produced an in-\nadequate confusion matrix, to the final model, which applied\nvarious data manipulation techniques to achieve F1 scores of\n0.83, serves as validation of the research methodology. The\neffectiveness of this pragmatic approach was demonstrated by\nthe increasing F1 scores, and the research further identified\nfactors that could improve the helpfulness of reviews. These\nfactors include eliminating duplicate reviews, estimating re-\nview helpfulness based on word count, and utilizing part-of-\nspeech (POS) tagging to incorporate lexical components into\nall reviews. The conclusion and result of this study underscore\nthat review helpfulness can be predicted excellently following\nthe deployment of the trained model. By removing duplicate\nreviews, the model provides a more straightforward way to\npredict the helpfulness of a product.\nIn [18], researchers presented sentiment analysis of Ama-\nzon product reviews using textual analysis and natural lan-\nguage processing (NLP) methods. This research utilized web-\nbased tools for analyzing customer reviews, which were cat-\negorized in a specific manner. Similarly, the study in [5] out-\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\nTABLE 1. The summary analysis of analyzed literature.\nRef. Year Dataset Used Proposed Technique Performance Score Research Limitations\n[3] 2022 Amazon fine food reviews BERT Model 79% Low performance scores were achieved.\n[13] 2023 Amazon fine food reviews Naïve Bayes 85% Low performance scores were achieved.\n[10] 2023 Amazon fine food reviews BERT Model 86% Low performance scores were achieved.\n[14] 2022 Amazon Alexa reviews SVM Model 91.5% Classical Machine Learning method is build.\n[15] 2023 Amazon fine food reviews RoBERTa Model 82% Low performance scores were achieved.\n[16] 2020 Amazon fine food reviews RoBERTa Model 82% Low performance scores were achieved.\n[17] 2022 Yelp Shopping reviews K-NN 59.6% Low performance scores were achieved.\nlined sentiment analysis of the Amazon product Alexa using\nneural network algorithms. A fundamental and significant\nelement of NLP is emotion analysis. In this study, researchers\nemployed Naïve Bayes, Random Forest, and Support Vector\nMachine (SVM) algorithms to facilitate sentiment analysis of\nAmazon products.\nThis study [3] has presented an analysis of Amazon fine\nfood reviews using the BERT model. Online product reviews\nplay an important role in predicting the helpfulness of product\nreviews. In this digital era, where people prefer to shop online\nrather than visit physical stores, many rely on online product\nreviews for a better purchasing experience. The helpfulness\nof product reviews enables customers to save time and pur-\nchase products in a cost-efficient manner. For this study,\nthe Amazon Fine Food Products dataset, which is available\nfor download from Kaggle, was used for experimentation.\nInitially, the data was cleansed, which involved removing\nspecial characters and dropping punctuation, leading to the\ncompletion of the model analysis. Natural Language Process-\ning (NLP) and BERT techniques were employed to train the\nmodel on the dataset, which was subsequently deployed.\nIn this research [15], the sentiment analysis of Amazon\nproduct reviews using deep learning techniques is discussed.\nThe ratio of online shopping increased significantly with\nthe rise of the COVID-19 pandemic in 2020. Due to this\npandemic situation, the importance of online product reviews\nhas significantly grown in the digital world. More people\nnow shop online, relying on product review analysis, which\nassists customers in making time-saving and cost-effective\npurchases. In this online context, not only do customers\nbenefit from product reviews, but sellers also leverage these\nreviews for the revenue and growth of their businesses.\nIn this study [16], specific models have been used for sen-\ntiment analysis, which includes Transformers such as BERT,\nROBERTa, and XLNet. After the experiment, ROBERTa\nachieved the highest accuracy at 82% among all the models\nused. Choosing the right features is essential for precisely\nforecasting the usefulness of online consumer feedback.\nThe newly introduced Bidirectional Encoder Representations\nfrom Transformers (BERT) model represents a significant\nadvancement in language processing, achieving unparalleled\nresults across various natural language processing tasks. This\nwork proposes a prediction model that utilizes BERT features\nand deep learning approaches to determine the helpfulness\nscores of customer reviews. The program employs a BERT-\nbased algorithm to analyze the dataset of Amazon product\nreviews, aiming to assist users in making informed purchasing\ndecisions.\nThis study [19] tests a dataset comprising reviews of\nShopify applications. To address the aforementioned con-\nstraints, user evaluations are classified into two categories:\npositive and negative. These evaluations are then subjected\nto preprocessing to cleanse the data. Following this, different\nmethods of feature engineering such as bag-of-words, term\nfrequency-inverse document frequency (TF-IDF), and chi-\nsquare (Chi2) are utilized, both separately and in conjunction,\nto preserve essential information. Ultimately, the reviews are\nclassified as either ’pleased’ or ’dissatisfied’ using AdaBoost\nclassifier, Random Forest, and logistic regression models.\nThe objective of this work [20] was to develop a compre-\nhensive method by refining the BERT foundational model\nthrough fine-tuning. The efficacy of BERT-based classifiers\nwas evaluated by comparing their performance with tradi-\ntional bag-of-words techniques. The tests, conducted using\nYelp shopping product reviews, indicated that fine-tuned\nBERT-based classifiers outperformed bag-of-words methods\nin accurately categorizing reviews as useful or unhelpful.\nFurthermore, it was discovered that the sequence length used\nin the BERT-based method significantly affects classification\neffectiveness. The BERT method with a sequence data length\nof 64 achieved the lowest accuracy, at 0.668, and an F1\nscore of 0.685. Conversely, a more sophisticated model with\na sequence length of 320 achieved the highest accuracy of\n0.707 and an F1 score of 0.717. Sequence lengths of 128\nand 256 yielded superior outcomes compared to those of 384\nand 512. The study demonstrates that the sequence length\nutilized in refining and evaluating the BERT base approach\nhas a important impact on classification accuracy.\nThis paper [21] presents a novel ensemble technique\nknown as the regression vector voting method for identifying\npoisonous remarks on various social media networks. The\nensemble merges a support vector classifier and logistic re-\ngression using a soft voting criterion. The suggested tech-\nnique is evaluated through experiments on both unbalanced\nand balanced datasets to analyze its performance. To address\nthe issue of an unbalanced dataset, the SMOTE is employed to\nbalance the data. Additionally, two feature extraction methods\nare employed to assess their appropriateness: TF-IDF and\nBag-of-Words (BoW).\nThis research [22] explores the application of diverse ma-\nchine learning approaches to assess the polarity of feelings\nexpressed in user review data on the IMDb website. To\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\nachieve this objective, the reviews undergo an initial prepro-\ncessing phase to eliminate redundant information and noise.\nSubsequently, a range of classification methods, including\nsupport vector machines (SVM), Naïve Bayes classifiers,\nrandom forests, and gradient boosting method, are employed\nto predict the sentiment expressed in these reviews.\nThis research [23] introduced a sophisticated deep-learning\nmodel designed to accurately categorize the most favorable\nand unfavorable product evaluations. This model employs a\nneural network approach, utilizing a recurrent neural network\n(RNN) method design that is the first to surpass conven-\ntional text categorization algorithms in this specific problem\ndomain. The authors evaluate the deep learning model by\ncomparing it with a baseline classifier that employs logistic\nregression. Similarly, research [13] examines and evaluates\nmethods for automatically identifying the sentiments con-\nveyed in English texts for Amazon and Flipkart products,\nusing Random Forest and K-Nearest Neighbor algorithms.\nThe text offers an in-depth comparative review of current sen-\ntiment analysis algorithms and approaches, evaluating them\nbased on five major factors. This leads to an assessment of\ntheir performance based on parameter usage and contribu-\ntions.\nThis study [24] utilizes a web-based application to cate-\ngorize and analyze customer evaluations of items, thereby\nsaving analysts considerable time and effort that would oth-\nerwise be required to manually sift through millions of re-\nviews. The sentiment analysis on product reviews employs\nNLP techniques. The developed technology comprises the\nfollowing five components: Lexicon-Based Sentiment, Text\nAnalytics, Customer Satisfaction Score, Amazon Products,\nand the Application Programming approach for the extraction\nof recent reviews. The text analytics component processes the\ntextual data by removing any unnecessary elements and ex-\ntracting the sentiment. The customer satisfaction score can be\ndetermined by calculating the average of the sentiment scores.\nPython-based sentiment analysis may be used to research and\nanalyze reviews that analysts wish to evaluate.\nIn this study [13], Linear Regression and Convolutional\nNeural Network (CNN) are identified as two of the most com-\nmonly employed machine-learning algorithms. The research\ndemonstrates that the method of representing text data plays a\ncrucial role in performance outcomes, indicating that TF-IDF\nand Word2Vec methods result in the most favorable Mean\nSquared Error (MSE) scores. Experimental findings reveal\nthat the bag-of-words (BoW) approach, when representing\nreview text, yields the poorest results across both datasets,\nwith the exception of a sampled example using the Cell\nPhones and Accessories dataset.\nA. LIMITATIONS AND RESEARCH GAP\nThe limitations of the previous study are evident in the\ntechniques and frameworks utilized. Traditional methods for\nmanipulating text features, such as TF-IDF or BoW, were em-\nployed in earlier research. In contrast, our work leverages the\nadvanced contextualized embeddings of the BERT (Bidirec-\ntional Encoder Representations from Transformers) model,\nwhich significantly enhances our understanding of language\nsemantics and context. Additionally, prior studies often relied\non conventional machine learning models, which lack the\ncomplexity and adaptability of more sophisticated models.\nBy integrating advanced methodologies and state-of-the-art\nmodels, we have successfully addressed the performance lim-\nitations identified in previous studies, achieving exceptional\nlevels of accuracy and efficacy in our experimental results.\nWe have discovered research gaps through a thorough lit-\nerature review:\n• Traditional machine learning techniques were previ-\nously used with BERT, BOW, and other feature en-\ngineering approaches to determine the helpfulness of\nuser evaluations on Amazon items. Although these ap-\nproaches showed high-performance ratings, there is still\nan accuracy gap that has to be resolved.\n• The Amazon Fine Food dataset has complex attributes\nthat require a sophisticated feature engineering strategy\nto enhance the efficacy of product review helpfulness.\nIII. PROPOSED METHODOLOGY\nThis module examines our newly proposed research method-\nology, as exemplified in Figure 1. Our proposed method\nutilizes the Amazon Fine Food Reviews dataset from Kaggle\nfor research projects. The original textual dataset’s features\nare then preprocessed to eliminate noise and encode the data\neffectively. We propose a unique feature engineering method,\nBERF (BERT-RF), to enhance the utility of customer evalu-\nations of Amazon’s fine food products. The newly generated\ndataset is divided into two parts for train and test, using an\n80% train and 20% test split ratio, respectively. Sophisticated\nmachine learning techniques are applied, and their effec-\ntiveness is evaluated using fresh data. The efficacy of each\napproach, along with hyperparameter optimization, is further\nvalidated using cross k-fold validation. The superior machine\nlearning technique is then utilized to forecast the helpfulness\nof customer reviews for online products.\nThe objective of the proposed technique is to forecast\nthe usefulness of Amazon product reviews by utilizing\ntransformer-based feature embeddings. The focus is on the\nAmazon Fine Food Reviews dataset, which encompasses\nup to 568,454 reviews. The primary columns selected for\nanalysis are the \"Helpfulness Denominator\" and the \"Text\"\ncolumns, as they contain crucial information for understand-\ning the helpfulness of product evaluations.\n• Step 1: The first phase involves data preparation, where\nmethods are employed to cleanse the data by eliminating\nstop words and noise. This ensures that the subsequent\nanalysis is founded on significant and relevant data.\nFollowing this, the sanitized data is used to generate a\nsystematically organized dataset for further research.\n• Step 2: For feature engineering, a novel transformer-\nbased model named BERF (BERT-RF) is utilized to\ncapture the complex patterns and semantic linkages in\nthe reviews.\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\nDataset\n(Amazon Fine\nfood reviews)\nPreprocessing Steps\n1. Tokenization\n2. Punctuation Removal\n3. Convert to Lower\nCase\n4. Stemming\n5. Stop word Removal\nNovel Feature\nEngineering\n1. BERT\n2. RF Probability\nData Splitting Training Set (80%)\nTest Set (20%)\nModel Training\n1. RF\n2. DTC\n3. KNC\n4. LGBM\nTrained Models\nEvaluation\nParameters\n1. Accuracy\n2. Precision\n3. Recall\n4. F1-Score\nReviews\nLabels\nFIGURE 1. The transformer-based novel methodology workflow analysis.\n• Step 3: After performing feature extraction with BERF,\nthe dataset is divided into two portions: a train set and a\ntest set. The train set is used to train various machine\nlearning models, while the testing set is employed to\nevaluate the models’ performance and generalization\ncapabilities.\n• Step 4: Several machine learning models, such as Ran-\ndom Forest, K-Nearest Neighbors, Decision Tree, and\nLightGBM, are utilized to forecast the helpfulness of\nreviews. These models are selected based on their ver-\nsatility and ability to handle different types of data and\ninteractions.\n• Step 5: The evaluation of each model is conducted using\nappropriate metrics such as accuracy, precision, recall,\nand F1 score.\nA. PHASE 1: REVIEW HELPFULNESS TEXTUAL DATA\nIn this study, we employed a benchmark dataset known as\nthe Amazon Fine Food Reviews [25]. This dataset is sourced\nfrom the Kaggle website and was originally compiled and\nreleased by McAuley et al. [26] in their study on online re-\nviews. It consists of reviews for gourmet food products posted\non Amazon.com, encompassing a total of 568,454 reviews\nfor 74,258 products. These were employed in conducting our\nresearch experiments.\nB. PHASE 2: TEXT PREPROCESSING AND DATA ANALYSIS\nA sophisticated text preparation strategy has been developed\nto enhance the quality of user comments in our study. The\nfirst phase involves eliminating common stopwords, which\nare frequently occurring words that do not impact the over-\nall content of the text. This step facilitates the removal of\nirrelevant information, simplifying the analysis process to\nfocus on more significant elements. Additionally, punctuation\nmarks, special characters, and numerical digits are methodi-\ncally removed to improve the precision of the text. The tech-\nnique includes tokenization, stemming, and lemmatization\nto normalize and reduce words to their fundamental or core\nforms. This approach not only helps in reducing noise but also\nensures that different forms of words are treated consistently,\nthereby preserving the fundamental semantic meaning of the\ncomments. Through the implementation of this comprehen-\nsive text preparation pipeline, user comments are efficiently\nsanitized, enabling a more precise and insightful analysis of\nour research.\nFigure 3 graphically represents the most common terms in\nthe dataset, providing a concise overview of important pat-\nterns. This visualization aids in data comprehension, enabling\nresearchers to quickly identify key themes and trends. The\nbenefits include effective data exploration, efficient analysis,\nand enhanced decision-making, which collectively contribute\nto more insightful conclusions and robust research outcomes.\nWe generated the Word Cloud before training our BERT\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\nGreat! Just\nas good as\n the expensive\nbrands!\nn x k representation of\nsentence with static and\nnon static channels\nBERT encoder layer\nwith multiple filter\nwidths and feature maps\nRandom Forest\n Novel Feature Set\nFIGURE 2. The proposed novel LLM based BERF features engineering architecture.\nmodel. The keywords for reviews depicted in Fig. 3 are\n\"good,\" \"one,\" and \"taste,\" indicating that the majority of\nreviewers primarily focus on the product’s flavor.\nFIGURE 3. The word cloud data analysis.\nC. PHASE 3: NOVEL LLM BASED BERT-RF TEXTUAL\nFEATURE EXTRACTION\nIn this module, we analyze our novel proposed features en-\ngineering method. The feature extraction mechanisms and\narchitecture of the proposed BERF approach are illustrated\nin Figure 2. We propose a novel transformer-based BERF\nmethod, which generates BERT embeddings and class prob-\nability features from product helpfulness online reviews. The\nnewly generated salient feature set is then used to build\nadvanced neural network models.\n1) Transformer-Based BERT Features\nTransformer-based feature engineering involves the use of\ntransformer models, specifically pre-trained ones such as\nBERT, to extract significant features values from raw data\nfor various machine learning tasks. BERT represents an un-\nsupervised pre-training technique within Natural Language\nProcessing (NLP). Unlike conventional NLP methods, BERT\nbenefits from being pre-trained on a large corpus of text,\nrequiring only fine-tuning for specific tasks. This approach\nallows BERT to understand the context of words in sentences\nmore effectively than previous models, making it a powerful\ntool for feature engineering in NLP applications.\n2) RF Based Class Probability Features\nRandom Forest (RF) probability-based feature engineering\nis a method that leverages the inherent characteristics of\nRandom Forest models to generate additional features based\non class probabilities [27]. This approach involves extracting\nclass probabilities as extra features and integrating them into\nthe dataset. These probabilities represent the likelihood of\neach data point belonging to a specific class, offering more\nnuanced information than basic class labels alone. By in-\ncorporating these probability-based features into the existing\nfeature set, machine learning models can gain insights into\nthe distribution of class probabilities within the data.\nAlgorithm 1 outlines the sequential working for extracting\nnew features.\nAlgorithm 1 BERF LLM Algorithm\nInput: Input Textual Content.\nOutput: Novel Feature Set.\ninitiate;\n1- BERTce ←− EBERT (Dt ) // here BERTce are the\nEmbedding features and Dt are input Textual Content.\n2- RFpf ←− PRF (BERTce) // here RFpf are the class\nProbability features and BERTce are input Embedding\nfeatures set.\n3- Ft ←− RFpf // here Ft are the Novel feature set.\nend;\nD. PHASED 4: FEATURES DATA BALANCING\nFigure 4 illustrates the study of the dataset’s class distribu-\ntions using histograms. The Amazon dataset has been classi-\nfied into two main groups. The first category includes cases\nwhere the reviews are considered useful and labeled as \"yes.\"\nThis rating implies that the reviews provide important insights\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\nor assistance to future buyers. The second category is labelled\nas \"no,\" indicating that the reviews in this group are not\ndeemed useful. Following this, we balanced the dataset’s fea-\ntures using the Synthetic Minority Over-sampling TEchnique\n(SMOTE) approach.\n(a)\n(b)\nFIGURE 4. Histogram chart-based data balancing of Features.\nE. PHASE 5: FEATURES DATA SPLITTING\nThe partitioning of data into two distinct sets, known as train-\ning data and test data, plays a crucial role in the construction\nand assessment of machine learning models. In this research,\nthe dataset is partitioned with a split ratio of 80% for train\nand 20% for test. This approach ensures that the model is not\nonly exposed to a substantial portion of the data for learning\nbut also reserves a separate subset for the unbiased evaluation\nof its performance. Sophisticated features were incorporated\nto enhance the learning process. Machine learning techniques\nwere then applied, and their effectiveness was assessed using\nthe previously unseen test data.\nF. PHASE 6: APPLIED ARTIFICIAL INTELLIGENCE MODELS\nThis section discusses the applied machine learning algo-\nrithms, detailing their implementation and associated hyper-\nparameters. The Scikit-learn library is utilized to implement\nthese algorithms. Specifically, four supervised machine learn-\ning algorithms are implemented in Python, leveraging the\nscikit-learn module. Supervised machine learning methods\nare commonly employed to tackle both classification and\nregression tasks.\n• Bidirectional Encoder Representations from Trans-\nformers (BERT): model for detecting product help-\nfulness in online reviews [28]. By leveraging the deep\nlearning capabilities of BERT, which understands the\nnuances of language context, our research aims to clas-\nsify reviews based on their perceived helpfulness to\nconsumers. Utilizing a dataset of customer reviews, the\nBERT model is fine-tuned to identify key features that\ncontribute to the helpfulness rating of a review. Pre-\nliminary results indicate that the BERT-based approach\nsignificantly outperforms traditional machine learning\nmodels in detecting helpful and non-helpful product\nreviews.\n• Random Forest Classifier (RF): algorithm is an effec-\ntive machine learning classifier [29] that falls within the\nensemble learning category. It is a tree-based ensemble\nmodel that uses decision trees as weak learners to gen-\nerate very accurate predictions. RF employs bootstrap\naggregation, or bagging, to train various decision trees\non diverse bootstrap samples, thereby enhancing predic-\ntion accuracy by aggregating the outcomes of these weak\nlearners.\n• Decision Tree (DT): is a widely utilized method for\nsolving classification and prediction tasks [30]. DT\nserves as an effective method for understanding data\ncharacteristics and making informed decisions based on\ninference. Decision trees are created by iteratively split-\nting the data based on specific criteria, and they com-\nprise three types of nodes: root, internal, and leaf—the\nprimary node being the root. The structure of a decision\ntree can be either binary or non-binary, which is deter-\nmined by the number of child nodes each parent node\nsupports. The gain ratio is a criterion frequently utilized\nfor splitting nodes in decision trees.\n• K-Neighbors Classifier (KNC): is a straightforward\nand adaptable machine learning technique [31] that falls\nwithin the supervised learning category. The KNN al-\ngorithm is widely used in classification due to its easy\nimplementation and straightforward functionality. It op-\nerates effectively as long as there are none missing val-\nues (NAs) in the dataset, which must be either removed\nor transformed according to different principles. While\nmost processes rely on numerical computations to pro-\nduce predictions, some may also employ text to enhance\nmodel understanding. The fundamental premise of KNN\nis based on the notion that data points with similar\ncharacteristics are likely to yield similar outcomes.\n• Light Gradient Boosting Machine (LGBM): is an\ninnovative [32], scalable, precise, and efficient Gradient\nBoosting Decision Tree framework introduced by Mi-\ncrosoft. Gradient boosting classifiers, a set of machine\nlearning techniques, aggregate multiple weak learners\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\nto create a robust predictive model. These methods are\nparticularly utilized for machine learning tasks, includ-\ning ranking and categorization. Unlike previous boosting\ntechniques that employ depth-wise or level-wise split-\nting, LGBM optimizes by splitting the tree leaf-wise us-\ning the best-fit strategy. This leaf-wise splitting approach\nenhances efficiency and minimizes wastage by reducing\nloss, thereby improving accuracy.\nG. PHASE 7: HYPER-PARAMETER SETTING\nTable 2 outlines the optimal hyperparameters for the tech-\nniques used in this research. Optimizing hyperparameters is\nan essential stage in the machine learning model training pro-\ncess. Hyperparameter optimization seeks to identify the best\nhyperparameter configuration for deployed models, thereby\nenhancing their performance and reducing errors. We have\nadjusted the parameters of applied methods using recursive\ntesting and training mechanisms, as well as the k-fold cross-\nvalidation approach.\nTABLE 2. The optimal hyperparameters analysis.\nTechnique Hyperparameter Description\nRF n_estimators=20, max_depth=20,\nrandom_state=0\nDT criterion=”gini”, split-\nter=”best”,min_samples_split=2\nKNC n_neighbors=2 , weights=’uniform’,\nleaf_size=30, p=2\nLGBM num_leaves=31, boosting_type=’gbdt’,\nlearning_rate=0.1, n_estimators=100,\nmin_child_weight=0.001,\nmin_child_samples=20\nIV. EXPERIMENTS AND OBSERVATIONS\nThis section presents the findings from studies conducted to\naddress the issue of predicting helpfulness. The study em-\nployed various methods, with a particular focus on feature se-\nlection. We utilized the novel BERF approach, which resulted\nin enhanced outcomes in our trials. Several experiments were\nconducted to assess learning models using feature extraction\ntechniques on the Amazon Fine Food Reviews dataset.\nA. EXPERIMENTAL SETUP\nFor our study project, the experimental environment is con-\nstructed using a cloud-based Notebook, specifically Google\nColab. We assessed the performance of neural network tech-\nniques by evaluating them based on accuracy, F1 score, pre-\ncision, and recall metrics. The details of the environment\nutilized are outlined in Table 3.\nOur proposed study encompasses several assessment crite-\nria, including accuracy, F1 score, recall, and precision. This\nevaluation measures parameters are utilized to assess the per-\nformance of machine learning models. Classification models\ncan be evaluated using a confusion matrix to determine the\naccuracy of their predictions on the testing set. The confusion\nmatrix, shown in the findings section, can be viewed as an\nerror matrix that represents four values: true positive (TP),\nTABLE 3. The experimental environment analysis.\nSpecification Value\nProgramming language Python 3.0\nEnvironmental model name Intel(R) core(TM)i5-3307U\nCPU MHz CPU@1.80GHz\nRAM 4.00 GB\nCache size 64 KB\nCPU cores 2\ntrue negative (TN), false positive (FP), and false negative\n(FN).\n• Accuracy: is the proportion of correct predictions made\nby classifiers using test data. The highest achievable ac-\ncuracy score is 1, signifying that all forecasts are correct.\nAccuracy = TP + TN\nTP + TN + FP + FN (1)\n• Precision: Precision refers to the accuracy of our classi-\nfiers. Precision is calculated as the ratio of true positives\nto the sum of TPs and FPs.\nPrecision = TP\nTP + FP (2)\n• Recall: Sensitivity, also known as recall, is the pro-\nportion of correctly identified positive events out of all\nactual positive instances.\nRecall = TP\nTP + FN (3)\n• F1-Score: is a crucial metric for assessing classifier\nperformance and is considered more significant than\naccuracy and recall.\nF1 = 2× precision × recall\nprecision + recall (4)\n• Confusion Matrix: is a table commonly employed to\ndepict the classifier’s performance on test data. It is\nreferred to as an error matrix that enables the viewing\nof an algorithm’s performance.\nB. RESULTS WITH CLASSICAL BERT EMBEDDING\nFEATURES\nThe performance outcomes of machine learning models em-\nploying classical BERT embedding features are delineated in\nTable 4. The highest accuracy, reaching 86.79%, is observed\nin the Random Forest (RF) models when evaluated on the test\ndataset. In comparison, other models demonstrated average\nperformance. Specifically, Decision Trees (DT) recorded a\nsignificantly lower accuracy of 73.40%, indicating a disparity\nin performance. Notably, the RF models excel in terms of F1\nscore, accuracy, and recall when leveraging BERT features\nfor analysis. The Random Forest algorithm, in particular,\nachieves the highest recall score, underscoring its efficacy in\nfeature extraction using BERT. However, it is important to\nacknowledge that the performance in detecting helpful prod-\nuct reviews remains suboptimal. This highlights an urgent\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\n(a)\n (b)\n(c)\n (d)\nFIGURE 5. Confusion Matrix Analysis based Performance Evaluation.\nneed for more advanced mechanisms to enhance performance\nfurther.\nTABLE 4. Neural Network models results with BERT Features.\nClassifier Accuracy (%) Target Precision Recall F1\nRF 86.79\nNo 0.83 0.92 0.87\nYes 0.91 0.81 0.86\nAverage 0.87 0.87 0.87\nDT 73.40\nNo 0.74 0.72 0.73\nYes 0.73 0.75 0.74\nAverage 0.73 0.73 0.73\nKNC 83.77\nNo 0.94 0.72 0.82\nYes 0.78 0.95 0.85\nAverage 0.86 0.84 0.84\nLGBM 79\nNo 0.75 0.87 0.80\nYes 0.85 0.71 0.77\nAverage 0.80 0.79 0.79\nC. RESULTS WITH NOVEL PROPOSED BERF FEATURES\nAfter examining results using only BERT features, we ana-\nlyzed the performance with the novel proposed BERF fea-\ntures, as described in Table 5. The learning models achieved\nan impressive 98% accuracy score, with LGBM standing out\nin the test dataset. The analysis reveals that LGBM performed\nexceptionally well in terms of F1 score, accuracy, and recall\nwhen utilizing both feature extraction approaches (BERT\nand RF) together. Among the models, Random Forest (RF)\nachieved the highest recall score using the BERF feature\nextraction method, followed by Decision Trees (DT) and K-\nNearest Neighbors (KNC). This analysis demonstrates that\nthe proposed feature engineering techniques significantly en-\nhance performance in detecting the helpfulness of product\nreviews.\nD. CONFUSION MATRIX RESULTS AND HISTOGRAM\nANALYSIS\nThe confusion matrix validation analysis of the employed ma-\nchine learning method, using the proposed features engineer-\ning, is illustrated in Figure 5. A lower wrong prediction error\nis achieved using the proposed features engineering approach\nfor product review helpfulness detection. The proposed Light\nGradient Boosting Machine (LGBM) model made only 580\nwrong predictions out of 25,485. This validates the perfor-\nmance score of the applied method using the novel proposed\nBERF features.\nThe histogram-based performance analysis of applied ma-\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\nTABLE 5. Machine learning models results with proposed BERT+RF\nFeatures.\nMethod Accuracy (%) Target Precision Recall F1\nRF 96.65\nNo 0.97 0.97 0.97\nYes 0.97 0.97 0.97\nAverage 0.97 0.97 0.97\nDT 96.63\nNo 0.97 0.97 0.97\nYes 0.97 0.97 0.97\nAverage 0.97 0.97 0.97\nKNC 96.36\nNo 0.95 0.98 0.96\nYes 0.98 0.95 0.96\nAverage 0.96 0.96 0.96\nLGBM 98\nNo 0.98 0.97 0.98\nYes 0.97 0.98 0.98\nAverage 0.98 0.98 0.98\nchine learning models, utilizing both feature engineering\napproaches as illustrated in Figure 6, demonstrates a clear\ndistinction in representing the results. The use of only BERT\nfeatures achieved lower results; however, performance scores\nwere significantly improved with the novel proposed feature,\nBERF. This analysis further reveals that the superior perfor-\nmance results of applied methods using BERF features for\nproduct review helpfulness detection are noteworthy.\nFIGURE 6. The histogram-based results comparisons of applied methods.\nE. KFOLD CROSS-VALIDATION RESULTS\nWe utilized k-fold cross-validation methods to evaluate the\ngeneralizability and confirm the performance ratings of each\nmethodology, as shown in Table 6. This analysis employs\nvalidation accuracy and standard deviation scores. We par-\ntitioned the entire dataset into 10 segments for validation\npurposes. Among the methodologies tested, only LGBM\nachieved high k-fold accuracy scores compared to the other\napproaches, which received respectable ratings. The proposed\ntechnique exhibited a high k-fold accuracy score of 0.98,\nindicating its effectiveness. After analyzing the data, we de-\ntermined that all the strategies employed are suitable for\npredicting review helpfulness and demonstrate strong gener-\nalization capabilities.\nTABLE 6. The k-fold cross-validation analysis.\nModel Kfolds Kfold Accuracy\nRF 10 0.97\nDT 10 0.97\nK-NN 10 0.96\nLGBM 10 0.98\nF. STATE OF THE ART RESULTS COMPARISON\nThe state-of-the-art performance results comparison of our\nproposed study is presented in Table 7. We contrasted our\napproach with recent research studies published in 2022 and\n2023. The analysis demonstrates that our study approach\nsurpassed the current leading studies, achieving excellent\naccuracy scores of 98%. This analysis definitively indicates\nthat our research excels in detecting product helpfulness com-\npared to previous studies.\nTABLE 7. The state of the art results comparison.\nRef . Year Proposed Technique Performance Accuracy\n[3] 2022 BERT Model 79%\n[10] 2023 Naïve Bayes 85%\nProposed 2024 Novel BERF-LGBM 98%\nG. ABLATION STUDY\nIn this part of the research, we present the outcomes of an\nablation study designed to evaluate the effectiveness of our\nemployed machine learning techniques. This was achieved\nby selectively omitting certain elements from the system. The\nmain goal of this study is to gauge the impact and importance\nof each individual component on the system’s overall func-\ntionality and performance.\nSpecifically, we focused on evaluating the impact of our\ninnovative approaches, BERT and BERF, as illustrated in Fig-\nure 7. The line chart-based analysis indicates that removing\nthe BERF features results in significantly lower performance\nscores, as shown in Figure 7(a). This removal led to dimin-\nished performance outcomes, underscoring the importance\nof the BERF features. Our introduced model notably outper-\nformed state-of-the-art methods, achieving the highest accu-\nracy of 98% in detecting the helpfulness of product reviews.\nThis highlights the superiority of our novel BERF approach\nover traditional machine learning models in this domain. The\nresults presented in Figure 7(b) indicate that adding our BERF\nfeature substantially improves performance outcomes. Over-\nall, the results from the ablation study reinforce the robustness\nand efficacy of our approach.\nH. DISCUSSIONS AND LIMITATIONS\nIn our study, we employed a transformer-based novel feature\nengineering technique, BERF, for the detection of helpfulness\nin product reviews using machine learning. The analysis was\nconducted on a state-of-the-art textual online review dataset,\nwhere the LGBM model notably outperformed other state-of-\nthe-art methods, achieving an impressive accuracy of 98%.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\n(a)\n(b)\nFIGURE 7. Analysis shows a comparison of accuracy, precision, recall, and\nF1 score for all learning models utilizing the BERT and BERF approaches.\nDespite these promising results, it is important to acknowl-\nedge that there are still gaps in performance that need to\nbe addressed. One limitation of our study is the potential\nfor overfitting, given the high accuracy rate, which may not\ngeneralize well to unseen data. Additionally, while the LGBM\nmodel showed superior performance, the reliance on a single\nmodel may not capture the complexity of language and senti-\nment present in online reviews as effectively as a more diverse\nensemble of models might.\nFurthermore, our approach with BERF, although innova-\ntive, may not fully encapsulate the nuances of human lan-\nguage, suggesting that further refinement of feature engi-\nneering techniques is necessary. Future research should also\nconsider the impact of evolving language and context in\nonline reviews, which may affect the longevity and adapt-\nability of the proposed method. Overall, while our findings\nare significant, continuous efforts in improving and testing\nthe robustness of these methods are essential for closing the\nidentified performance gaps.\nV. CONCLUSION AND FUTURE DIRECTIONS\nThis study utilizes several machine learning methods to ad-\ndress issues related to categorizing the helpfulness of user\nproduct reviews. Various feature engineering approaches, in-\ncluding BERT and RF probability, are applied. The classifiers\nRF, DT, LGB, and KN are trained on text reviews to predict\nthe helpfulness of product reviews. This study introduces\nan innovative method to accurately predict the helpfulness\nof Amazon product evaluations. We utilized a benchmark\ndataset, known as the Amazon Fine Food Reviews, to develop\nsophisticated machine-learning techniques. Four advanced\nmachine-learning methods were evaluated to determine the\nhelpfulness of product reviews. We propose a new method\ncalled BERF (BERT-RF) for feature engineering to enhance\nthe usefulness of customer reviews for Amazon’s gourmet\nfood products. A unique feature set is developed using the\nsuggested BERF method. The BERF strategy utilizes class\nprobabilities derived from the review helpfulness denomina-\ntor dataset as features for constructing applicable machine\nlearning models. The extensive research findings showed\nthat the LGBM methodology surpassed the current leading\nmethods, achieving an accuracy of 98%. The performance of\neach applicable technique is verified using a k-fold method\nand further enhanced by hyperparameter optimization.\nA. FUTURE WORK\nIn our upcoming projects, we plan to develop a graphical user\ninterface (GUI) tailored for clients who prefer online shop-\nping. Additionally, we aim to enhance the utility of customer\nreviews for products by creating a model designed to improve\ntheir usefulness. Our efforts will also focus on increasing the\naccuracy of the model’s predictions.\nACKNOWLEDGMENT\nThe authors would like to express their grateful to Princess\nNourah bint Abdulrahman University Researchers Support-\ning Project number (PNURSP2024R104), Princess Nourah\nbint Abdulrahman University, Riyadh, Saudi Arabia\nFUNDING\nThis research was funded by Princess Nourah bint Abdul-\nrahman University Researchers Supporting Project number\n(PNURSP2024R104), Princess Nourah bint Abdulrahman\nUniversity, Riyadh, Saudi Arabia.\nREFERENCES\n[1] Y . Jiang, A. Huang, S. Gao, and S. Yu, ‘‘Relationship between the terminal\nbuilt environment and airport retail revenue,’’ Journal of Air Transport\nManagement, vol. 116, p. 102568, 2024.\n[2] A. Alabaidi, ‘‘The impact of work life balance on employee attitudes and\nbehavior in health care sector,’’ 2024.\n[3] X. Zhao and Y . Sun, ‘‘Amazon fine food reviews with bert model,’’ Proce-\ndia Computer Science, vol. 208, pp. 401–406, 2022.\n[4] J. Ballerini, A. Ključnikov, D. Juárez-Varón, and S. Bresciani, ‘‘The\ne-commerce platform conundrum: How manufacturers’ leanings affect\ntheir internationalization,’’ Technological Forecasting and Social Change,\nvol. 202, p. 123199, 2024.\n[5] M. S. Akin, ‘‘Enhancing e-commerce competitiveness: A comprehensive\nanalysis of customer experiences and strategies in the turkish market,’’\nJournal of Open Innovation: Technology, Market, and Complexity, vol. 10,\nno. 1, p. 100222, 2024.\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\n[6] M. S. I. Malik and A. Nawaz, ‘‘Sehp: stacking-based ensemble learning on\nnovel features for review helpfulness prediction,’’ Knowledge and Infor-\nmation Systems, vol. 66, no. 1, pp. 653–679, 2024.\n[7] S. Negoita, H.-S. Chen, P. V . Sanchez, R. L. Sherman, S. J. Henley, R. L.\nSiegel, H. Sung, S. Scott, V . B. Benard, B. A. Kohler, et al., ‘‘Annual report\nto the nation on the status of cancer, part 2: Early assessment of the covid-19\npandemic’s impact on cancer diagnosis,’’ Cancer, vol. 130, no. 1, pp. 117–\n127, 2024.\n[8] H. Zhang, J. Zhao, R. Farzan, and H. Alizadeh Otaghvar, ‘‘Risk predictions\nof surgical wound complications based on a machine learning algorithm: A\nsystematic review,’’International Wound Journal, vol. 21, no. 1, p. e14665,\n2024.\n[9] M. Hussain, T. Zhang, M. Chaudhry, I. Jamil, S. Kausar, and I. Hussain,\n‘‘Review of prediction of stress corrosion cracking in gas pipelines using\nmachine learning,’’ Machines, vol. 12, no. 1, p. 42, 2024.\n[10] T. Hudgins, S. Joseph, D. Yip, and G. Besanson, ‘‘Identifying features and\npredicting consumer helpfulness of product reviews,’’ SMU Data Science\nReview, vol. 7, no. 1, p. 11.\n[11] S. Park and H. Kim, ‘‘Extracting product design guidance from online\nreviews: An explainable neural network-based approach,’’ Expert Systems\nwith Applications, vol. 236, p. 121357, 2024.\n[12] J. Ryu, S. Lim, O.-W. Kwon, and S.-H. Na, ‘‘Transformer-based reranking\nfor improving korean morphological analysis systems,’’ ETRI Journal,\nvol. 46, no. 1, pp. 137–153, 2024.\n[13] F. Hjalmarsson, ‘‘Predicting the helpfulness of online product reviews,’’\n2021.\n[14] B. Yu, ‘‘Comparative analysis of machine learning algorithms for sentiment\nclassification in amazon reviews,’’ Highlights in Business, Economics and\nManagement, vol. 24, pp. 1389–1400, 2024.\n[15] A. Iqbal, R. Amin, J. Iqbal, R. Alroobaea, A. Binmahfoudh, and M. Hus-\nsain, ‘‘Sentiment analysis of consumer reviews using deep learning,’’ Sus-\ntainability, vol. 14, no. 17, p. 10844, 2022.\n[16] S. Xu, S. E. Barbosa, and D. Hong, ‘‘Bert feature based model for pre-\ndicting the helpfulness scores of online customers reviews,’’ in Advances\nin Information and Communication: Proceedings of the 2020 Future of\nInformation and Communication Conference (FICC), Volume 2, pp. 270–\n281, Springer, 2020.\n[17] M. K. Shaik Vadla, M. A. Suresh, and V . K. Viswanathan, ‘‘Enhancing\nproduct design through ai-driven sentiment analysis of amazon reviews\nusing bert,’’ Algorithms, vol. 17, no. 2, p. 59, 2024.\n[18] A. Haseeb, R. Taseen, M. Sani, and Q. G. Khan, ‘‘Sentiment analysis on\namazon product reviews using text analysis and natural language process-\ning methods,’’ in International Conference on Engineering, Natural and\nSocial Sciences, vol. 1, pp. 446–452, 2023.\n[19] F. Rustam, A. Mehmood, M. Ahmad, S. Ullah, D. M. Khan, and G. S.\nChoi, ‘‘Classification of shopify app user reviews using novel multi text\nfeatures,’’IEEE Access, vol. 8, pp. 30234–30244, 2020.\n[20] M. Bilal and A. A. Almazroi, ‘‘Effectiveness of fine-tuned bert model in\nclassification of helpful and unhelpful online customer reviews,’’ Elec-\ntronic Commerce Research, vol. 23, no. 4, pp. 2737–2757, 2023.\n[21] V . Rupapara, F. Rustam, H. F. Shahzad, A. Mehmood, I. Ashraf, and G. S.\nChoi, ‘‘Impact of smote on imbalanced text features for toxic comments\nclassification using rvvc model,’’ IEEE Access, vol. 9, pp. 78621–78634,\n2021.\n[22] M. Z. Naeem, F. Rustam, A. Mehmood, I. Ashraf, G. S. Choi, et al.,\n‘‘Classification of movie reviews using term frequency-inverse document\nfrequency and optimized machine learning algorithms,’’ PeerJ Computer\nScience, vol. 8, p. e914, 2022.\n[23] J. Wei, J. Ko, and J. Patel, ‘‘Predicting amazon product review helpful-\nness,’’ IEEE Transactions on Neural Networks, vol. 5, no. 1, pp. 3–14,\n2016.\n[24] M. B. Kursa and W. R. Rudnicki, ‘‘The all relevant feature selection using\nrandom forest,’’ arXiv preprint arXiv:1106.5112, 2011.\n[25] S. N. A. PROJECT, ‘‘Amazon fine food reviews.’’\nhttps://www.kaggle.com/datasets/snap/amazon-fine-food-reviews.\n(Accessed on 02/28/2024).\n[26] J. J. McAuley and J. Leskovec, ‘‘From amateurs to connoisseurs: modeling\nthe evolution of user expertise through online reviews,’’ in Proceedings of\nthe 22nd international conference on World Wide Web, pp. 897–908, 2013.\n[27] A. Raza, A. M. Qadri, I. Akhtar, N. A. Samee, and M. Alabdulhafith,\n‘‘Logrf: An approach to human pose estimation using skeleton landmarks\nfor physiotherapy fitness exercise correction,’’ IEEE Access, vol. 11,\npp. 107930–107939, 2023.\n[28] Y .-T. Peng and C.-L. Lei, ‘‘Using bidirectional encoder representations\nfrom transformers (bert) to predict criminal charges and sentences from\ntaiwanese court judgments,’’ PeerJ Computer Science, vol. 10, p. e1841,\n2024.\n[29] A. F. Amiri, H. Oudira, A. Chouder, and S. Kichou, ‘‘Faults detection\nand diagnosis of pv systems based on machine learning approach using\nrandom forest classifier,’’ Energy Conversion and Management, vol. 301,\np. 118076, 2024.\n[30] J. Zhou, Z. Su, S. Hosseini, Q. Tian, Y . Lu, H. Luo, X. Xu, C. Chen, and\nJ. Huang, ‘‘Decision tree models for the estimation of geo-polymer con-\ncrete compressive strength,’’ Mathematical Biosciences and Engineering,\nvol. 21, no. 1, pp. 1413–1444, 2024.\n[31] A. Raza, K. Munir, M. S. Almutairi, and R. Sehar, ‘‘Novel transfer learning\nbased deep features for diagnosis of down syndrome in children using facial\nimages,’’IEEE Access, 2024.\n[32] B. Abu-Salih, S. Alotaibi, R. Abukhurma, M. Almiani, and M. Aljaafari,\n‘‘Dao-lgbm: dual annealing optimization with light gradient boosting ma-\nchine for advocates prediction in online customer engagement,’’ Cluster\nComputing, pp. 1–27, 2024.\nAREEBA ISHTIAQearned a Bachelor of Science\nin Information Technology from Khawaja Fareed\nUniversity of Engineering and Information Tech-\nnology (KFUEIT), located in Rahim Yar Khan,\nPakistan, in 2021. She went on to KFUEIT in\n2022 to start her Master’s program in informa-\ntion technology, completing her study at the same\nschool that had helped prepare her for success in\nthe classroom. During her master’s degree, she\nfocused on data analytics, cybersecurity, machine\nlearning, deep learning, web development and emerged technologies. Areeba\nlooked for internships and part-time jobs during her Master’s program in\norder to obtain real-world experience. Employers were pleased by her ability\nto adapt academic knowledge to practical situations. Areeba hopes to work in\nIT research and development and contribute to inventions that will influence\ntechnology when she finishes her master’s degree.\nKASHIF MUNIR earned a B.Sc. degree in math-\nematics and physics from Islamia University Ba-\nhawalpur, Pakistan, in 1999. Subsequently, he ob-\ntained an M.Sc. degree in information technology\nfrom the University Sains Malaysia in 2001, an\nM.S. degree in software engineering from the Uni-\nversity of Malaya, Malaysia, in 2005, and a Ph.D.\ndegree in informatics from the Malaysia University\nof Science and Technology in 2015. Engaged in\nhigher education since 2002, he taught initially at\nBinary College, Malaysia, for a semester, followed by approximately four\nyears at Stamford College, Malaysia. Later, he moved to Saudi Arabia, work-\ning at the King Fahd University of Petroleum and Minerals from September\n2006 to December 2014. In January 2015, he transitioned to the University\nof Hafr Al-Batin, Saudi Arabia, and in July 2021, he joined Khwaja Fareed\nUniversity of Engineering & IT, Rahim Yar Khan, as an Assistant Profes-\nsor in the IT Department. With a substantial publication record, including\njournal articles, conference papers, books, and book chapters, he has served\non technical program committees for numerous peer-reviewed conferences\nand journals, contributing to the review of numerous research papers. His\nresearch interests span cloud computing security, software engineering, and\nproject management.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAreeba et al.et al.: Product Helpfulness Detection With Novel Transformer Based BERT Embedding and Class Probability Features\nALI RAZA is a lecturer at the Faculty of Infor-\nmation Technology, Department of Software En-\ngineering, University Of Lahore, Pakistan. He re-\nceived a Bachelor of Science degree in computer\nscience from the Department of Computer Sci-\nence, Khwaja Fareed University of Engineering\nand Information Technology (KFUEIT), Rahim\nYar Khan, Pakistan, in 2021, where he completed\nthe M.S. degree in computer science in 2023. He\nhas published several articles in reputed journals.\nHis current research interests include data science, artificial intelligence, data\nmining, natural language processing, machine learning, deep learning, and\nimage processing.\nNAGWAN ABDEL SAMEEreceived the B.S. de-\ngree in computer engineering from Ein Shams\nUniversity, Egypt, in 2000 and the M.S. degree\nin computer engineering from Cairo University,\nEgypt in 2008. In 2012, she has received the Ph.D.\ndegree in Systems and Biomedical Engineering\nfrom Cairo University, Egypt. Since 2013, she has\nbeen an Assistant Professor with the Information\nTechnology Department, CCIS, Princess Nourah\nbint Abdulrahman University, Riyadh, KSA. Her\nresearch interests include Data Science, Machine Learning, Bioinformatics,\nParallel Computing. Dr. Nagwan awards and honors include the Takafull\nPrize (Innovation Project Track), Princess Nourah Award in innovation,\nMastery Award in predictive analytics (IBM), Mastery Award in Big Data\n(IBM), and Mastery Award in Cloud Computing(IBM).\nMONA M. JAMJOOMis an associate professor\nat the department of computer sciences, College\nof Computer and Information Sciences, Princess\nNourah Bint Abdulrahman University, Riyadh\nSaudi Arabia. She got her PhD degree in com-\nputer science from King Saud University. Her area\nof interest include artificial intelligence, machine\nlearning, deep learning, medical imaging, and data\nscience. She has published several research articles\nin her field.\nZAHID ULLAHis an experienced educator and re-\nsearcher of computer science and information sys-\ntems. He got his doctorate from the University of\nKuala Lumpur in Malaysia. He is currently work-\ning as assistant professor at King Abdulaziz Uni-\nversity, Jeddah, Saudi Arabia. His research areas\ninclude machine learning, deep learning, medical\nimaging, and data science. He published various\narticles in his field of specialization.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3390605\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Helpfulness",
  "concepts": [
    {
      "name": "Helpfulness",
      "score": 0.7248458862304688
    },
    {
      "name": "Computer science",
      "score": 0.6503069996833801
    },
    {
      "name": "Embedding",
      "score": 0.6271077394485474
    },
    {
      "name": "Transformer",
      "score": 0.5305512547492981
    },
    {
      "name": "Class (philosophy)",
      "score": 0.42117559909820557
    },
    {
      "name": "Product (mathematics)",
      "score": 0.4160492420196533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.383347749710083
    },
    {
      "name": "Mathematics",
      "score": 0.1896756887435913
    },
    {
      "name": "Electrical engineering",
      "score": 0.1625182330608368
    },
    {
      "name": "Engineering",
      "score": 0.08096668124198914
    },
    {
      "name": "Voltage",
      "score": 0.06671968102455139
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}