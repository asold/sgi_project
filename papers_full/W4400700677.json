{
    "title": "Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)",
    "url": "https://openalex.org/W4400700677",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5101381944",
            "name": "Anjanava Biswas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5098910558",
            "name": "Wrick Talukdar",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4285225959",
        "https://openalex.org/W4385449797",
        "https://openalex.org/W6854698132",
        "https://openalex.org/W3165327186",
        "https://openalex.org/W4385573569",
        "https://openalex.org/W4381586841",
        "https://openalex.org/W6846870107",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W4389523893",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W2795807997",
        "https://openalex.org/W2488684359"
    ],
    "abstract": "The AI era has ushered in Large Language Models (LLM) to the technological forefront, which has been much of the talk in 2023, and is likely to remain as such for many years to come. LLMs are the AI models that are the power house behind generative AI applications such as ChatGPT. These AI models, fueled by vast amounts of data and computational prowess, have unlocked remarkable capabilities, from human-like text generation to assisting with natural language understanding (NLU) tasks. They have quickly become the foundation upon which countless applications and software services are being built, or at least being augmented with. However, as with any groundbreaking innovations, the rise of LLMs brings forth critical safety, privacy, and ethical concerns. These models are found to have a propensity to leak private information, produce false information, and can be coerced into generating content that can be used for nefarious purposes by bad actors, or even by regular users unknowingly. Implementing safeguards and guardrailing techniques is imperative for applications to ensure that the content generated by LLMs are safe, secure, and ethical. Thus, frameworks to deploy mechanisms that prevent misuse of these models via application implementations is imperative. In this study, we propose a Flexible Adaptive Sequencing mechanism with trust and safety modules, that can be used to implement safety guardrails for the development and deployment of LLMs.",
    "full_text": null
}