{
  "title": "Compressing Language Models using Doped Kronecker Products",
  "url": "https://openalex.org/W3002071578",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3187860733",
      "name": "Thakker, Urmish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227174216",
      "name": "Whatmough, Paul N.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745842448",
      "name": "liu zhi gang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3190076872",
      "name": "Mattina, Matthew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3190885283",
      "name": "Beu, Jesse",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2748818695",
    "https://openalex.org/W2260663238",
    "https://openalex.org/W2982011653",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2806978588",
    "https://openalex.org/W2949892913",
    "https://openalex.org/W2952344559",
    "https://openalex.org/W2927608232",
    "https://openalex.org/W2948954216",
    "https://openalex.org/W2619959423",
    "https://openalex.org/W3034226764",
    "https://openalex.org/W2914294010",
    "https://openalex.org/W2949896761",
    "https://openalex.org/W2182629226",
    "https://openalex.org/W2963385194",
    "https://openalex.org/W2978186843",
    "https://openalex.org/W3037953478",
    "https://openalex.org/W2898997786",
    "https://openalex.org/W2950894517"
  ],
  "abstract": "Kronecker Products (KP) have been used to compress IoT RNN Applications by 15-38x compression factors, achieving better results than traditional compression methods. However when KP is applied to large Natural Language Processing tasks, it leads to significant accuracy loss (approx 26%). This paper proposes a way to recover accuracy otherwise lost when applying KP to large NLP tasks, by allowing additional degrees of freedom in the KP matrix. More formally, we propose doping, a process of adding an extremely sparse overlay matrix on top of the pre-defined KP structure. We call this compression method doped kronecker product compression. To train these models, we present a new solution to the phenomenon of co-matrix adaption (CMA), which uses a new regularization scheme called co matrix dropout regularization (CMR). We present experimental results that demonstrate compression of a large language model with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At 25x compression, an equivalent pruned network leads to 7.9% loss in perplexity score, while HMD and LMF lead to 15% and 27% loss in perplexity score respectively.",
  "full_text": "COMPRESSING LANGUAGE MODELS USING DOPED KRONECKER PRODUCTS\nUrmish Thakker1\nPaul Whatmough1\nZhi-Gang Liu1\nMatthew Mattina1\nJesse Beu1\nABSTRACT\nKronecker Products (KP) have been used to compress IoT RNN Applications by 15-38x compression factors,\nachieving better results than traditional compression methods. However when KP is applied to large Natural\nLanguage Processing tasks, it leads to signiﬁcant accuracy loss (approx 26%). This paper proposes a way to\nrecover accuracy otherwise lost when applying KP to large NLP tasks, by allowing additional degrees of freedom\nin the KP matrix. More formally, we propose doping, a process of adding an extremely sparse overlay matrix on\ntop of the pre-deﬁned KP structure. We call this compression method doped kronecker product compression. To\ntrain these models, we present a new solution to the phenomenon of co-matrix adaption (CMA), which uses a\nnew regularization scheme called co-matrix dropout regularization (CMR). We present experimental results that\ndemonstrate compression of a large language model with LSTM layers of size 25 MB by 25×with 1.4% loss in\nperplexity score. At 25×compression, an equivalent pruned network leads to 7.9% loss in perplexity score, while\nHMD and LMF lead to 15% and 27% loss in perplexity score respectively.\n1 I NTRODUCTION\nThe large size of Natural Language Processing (NLP) appli-\ncations can make it impossible for them to run on resource\nconstrained devices with limited memory and cache bud-\ngets (Thakker et al., 2019c; Tao et al., 2019). Fitting these\napplications into IoT devices requires signiﬁcant compres-\nsion. For example, to ﬁt a 25 MB Language Model on an\nIoT device with 1 MB L2 Cache, requires 25x compression\nor 96% reduction in the number of parameters. Recently,\nKronecker Products (KP) were used to compress IoT ap-\nplications by 15-38x compression factors (Thakker et al.,\n2019d;b) and achieves better accuracy than pruning (Zhu\n& Gupta, 2017), low-rank matrix factorization (LMF) and\nsmall baseline (SB). However, when we apply KP to a large\nlanguage modeling (LM) application, we see a 26% loss\nin accuracy at 338x compression. Unlike pruning (amount\nof sparsity) and LMF (rank of the matrix), there is no ob-\nvious method to control the amount of compression of the\nKP compressed network. (Thakker et al., 2019b) propose\nHybrid KP (HKP) to solve this issue. HKP helps recover\n1Arm ML Research Lab. Correspondence to: Urmish Thakker\n<urmish.thakker@arm.com>.\nTo be presented at On-device Intelligence Workshop at3 rd SysML\nConference, Copyright 2019 by the author(s).\n1 2\n3 4\n0 3\n2 1\n0*1=0 3*1=3 0*2=2 3*2=6\n2*1=2 1*1=1 2*2=4 2*1=2\n0*3=0 3*3=9 0*4=0 4*3=1\n2\n3*2=6 3*1=3 4*2=8 4*1=4\n1 2\n3 4\n0 3\n2 1\n0 3 2 6\n2 1 4 2\n0 9 0 12\n6 3 8 4\nQuestion - What if, to get to optimal minima \nthis value should go to 4?\nSolution 1 –Change 2 to 4\nSolution 2 –Change 1 to 2\n(a) KP of two 2x2 matrices, B & C, leads to a matrix, A , of size 4x4\n(b) Changing a single element in B/C changes 4 elements \nin A. This restricts the expressibility of A. Solution 1 \ndoubles the values of all elements in the green boxes in \nA. Thus, we do not reach the optimal  solution. Similar \nobservation can be made for Solution 2\nA\nCB\n0 3 2 6\n2 1 4 2\n0 9 0 12\n6 3 8 4\n(c ) Solution proposed in this paper - Doping\nAdd a sparse matrix (94% sparsity) to A to \nprovide additional degrees of freedom to the \nelements in matrix A. This allows the matrix \nto be updated such that it reaches the \noptimal minima\n0 0 0 0\n0 0 0 2\n0 0 0 0\n0 0 0 0\nFigure 1.(a) Example of a Kronecker Product of two matrices. (b)\nIssues with back-propagation through a matrix expressed as a KP\nof two smaller matrices. (c) Shows how doping solves the issues\ndiscussed in (b)\nthe lost accuracy by injecting more parameters in the KP\ncompressed network. However, the compression factor re-\nduces to 5x to bring down the accuracy loss within 1.5%\nloss in baseline perplexity.\nThis paper explores another method to inject parameters\ninto a KP compressed network. This method is based on\narXiv:2001.08896v5  [cs.LG]  17 Nov 2020\nSubmission and Formatting Instructions for SysML 2019\nthe observations that parameters in the KP space need addi-\ntional degrees of freedom (Figure 1 a,b). Inspired by robust\nPCA techniques, we propose adding a sparse matrix to a\nKP compressed matrix in order to facilitate these additional\ndegrees of freedom (Figure 1 c). Thus, a parameter matrix\nin an RNN, LSTM, GRU, or Transformer layer is replaced\nby a sum of two matrices – one expressed as a KP of two\nsmaller matrices (Mkp) and the other an extremely sparse\nmatrix (Msp). During training, Msp starts off with 0% spar-\nsity. Over time, we prune the unimportant weights in the\nMsp matrix to get to the required amount of sparsity. These\npruned values will represent the equivalent values in Mkp\nthat did not require the additional degrees of freedom. This\nmethodology of compression is called Doped Kronecker\nProduct (DKP) in this paper. However, training DKP com-\npressed networks is non-trivial and requires overcoming\nco-matrix adaption (CMA) (Section 3.1) using a specialized\nregularization scheme (Section 3.3).The preliminary results\nusing this compression scheme are encouraging. We show\nthat we can compress the medium sized LM in (Zaremba\net al., 2014) by 25×with 1.2% loss in perplexity score, im-\nproving perplexity of pruned (Zhu & Gupta, 2017) network\nby to 6.7%, HMD (Thakker et al., 2019a) by 13.8% and\nLMF (Kuchaiev & Ginsburg, 2017) by 25.8%.\nIn the rest of the paper, we discuss the CMA issues associ-\nated with a general doping mechanism (3.1), some methods\nto overcome CMA based on popular training techniques\n(3.2), the technique proposed in this paper to overcome\nCMA (3.3), results of compressing a medium LM using\nDKP and comparison against popular compression tech-\nniques and previously published work (4)\n2 R ELATED WORK\nThe research in NN compression can be broadly categorized\nunder 4 topics - Pruning (Han et al., 2016; Zhu & Gupta,\n2017), structured matrix based techniques (Sindhwani et al.,\n2015; Thakker et al., 2019b;d; Gope et al., 2020a), quanti-\nzation (Hubara et al., 2016; Courbariaux & Bengio, 2016;\nGope et al., 2020b; 2019) and tensor decomposition (Tjan-\ndra et al., 2017). DKP combines pruning and structured\nmatrix based techniques and compares the results with prun-\ning, structured matrix and tensor decomposition based com-\npression techniques. The networks compressed using this\ntechnique can be further compressed using quantization.\n3 D OPED KRONECKER PRODUCT (DKP)\nCOMPRESSION\nDKP expresses a matrix as a sum of a Mkp and a sparse\nmatrix -\nW = B⊗C+ Msp (1)\nTable 1.Results of compressing using DKP when a matrix is ex-\npressed as shown in equation 1\nBaseline Perplexity 82.04\nCompression Factor 338× 100×\nSparsity of Msp 100% 99.93%\nDKP Perplexity 104 138.3\n0.00\n20.00\n40.00\n60.00\n80.00\n100.00\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n11\n21\n31\n41\n51\n61\n71\n81\n91\n101\n111\n121\n131\n141\n151\n161\n171\n181\n191\n201\n211\n221\n231\n241\n% Sparsity\nTraining Perplexity\n# Epochs\n120 Sparsity\nIncrease in Training \nPerplexity with increased \nsparsity in Msp\nhighlighting the reliance \non Msp\nTrain                             Sparsity\nFigure 2.Graph of training perplexity vs training epochs and spar-\nsity of Msp matrix vs training epoch for the medium LM at 100×\ncompression factor. As the sparsity of Msp matrix increases, the\ntraining perplexity degrades. This indicates that the NN is too\nreliant on the Msp matrix during the initial phase of the training\nprocess and less reliant on the Mkp matrix. We call this phe-\nnomenon as co-matrix adaptation.\nThe sparsity of Msp determines the amount of compres-\nsion. For example, if W is of size 100 ×100, B and C are\nof size 10×10, then 95% sparsity in Msp will lead to 14×\ncompression and 90% sparsity in Msp will lead to 8.4 ×\ncompression. During the initial phase of training, Msp is\ndense. As training progresses, Msp reaches the required\nsparsity level. Thus we allow back-propagation to deter-\nmine which elements of the Mkp matrix (B⊗C) require\nadditional degrees of freedom.\n3.1 Co-matrix Adaptation (CMA)\nEquation 1 is one way to implement DKP and represents our\ninitial attempt at compressing using DKP. We compressed\nthe LSTM layers in the medium LM in (Zaremba et al.,\n2014) using this method. The LM has 2 LSTM layers with\nhidden vector of size 650. This creates matrices of size\n2600 ×1300 amounting to a total size of 25 MB. We com-\npress these layers by 25×by replacing the matrices in the\nLSTM layers as shown in equation 1. By adding a Msp\nwith 99% sparsity, the perplexity score degrades by 32.9%.\nThus adding 1% more parameters to Mkp leads to poorer\nperplexity score than baseline. Figure 2 shows the graph of\ntraining perplexity vs training epochs and sparsity of Msp\nmatrix vs training epoch for the medium LM at 100×com-\nSubmission and Formatting Instructions for SysML 2019\n0.00\n10.00\n20.00\n30.00\n40.00\n50.00\n60.00\n70.00\n80.00\n90.00\n100.00\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n11\n21\n31\n41\n51\n61\n71\n81\n91\n101\n111\n121\n131\n141\n151\n161\n171\n181\n191\n201\n211\n221\n231\n241\nSparsity\nTraining Perplexity\n# Epochs\n1 2a 2b 2a+BCD 2b+BCD CMR Sparsity\nFigure 3.Using the various techniques described in equation 2a -\n2b with and without BCD, we see that the reliance on Msp has\nreduced. The increase in training perplexity that was visible in 2\nhas been managed considerably.\npression factor. As the sparsity of Msp matrix increases, the\ntraining perplexity degrades. This indicates that the LM is\ntoo reliant on the Msp matrix during the initial phase of the\ntraining process when the matrix is dense and less reliant on\nthe Mkp matrix. When the Msp becomes extremely sparse,\nMkp is no longer able to pull the perplexity score back. We\nsuspect that this might be because the model is stuck in a\nlocal minima dictated by the dense Msp matrix. We will\nrefer to this phenomena as co-matrix adaptation (CMA).\nThe phenomenon of CMA is more clearer when we focus on\nthe number of back-propagation updates during the initial\nphase of training. The Mkp matrix is composed of the\nkronecker product of two matrices of size 13 ×65 and\n50 ×20 leading to a total of 9980 parameters. While the\nMsp matrix in its initial dense form has a total of 3382600\nparameters. Thus, during back-propagation, Msp matrix\nreceives 339×more updates than the Mkp matrix. This\nmight sway the NN to ﬁnd a minima that is too reliant\non the parameters of the Msp matrix. As a result, when\nthe training progresses and the Msp matrix is pruned, the\naccuracy drops signiﬁcantly.\n3.2 Overcoming CMA\nThe key to overcoming CMA is to reduce the reliance on\nMsp initially. This paper explored multiple avenues to do\nso.\nW = B⊗C+ β×Msp,min ∥β∥ (2a)\nW = α×(B⊗C) +β×Msp,\nmin(∥β∥+ ∥1/α∥) (2b)\nEach of the equations 2a - 2b can be further trained with\nTable 2.Test Perplexity of DKP compressed medium LM for var-\nious training techniques discussed in Section 3.2 and 3.3. CMR\ntechniques leads to the best accuracy for 100× compression factor.\nCompression\nFactor\nTraining\nMethod\nSparsity\nof Msp\nTest\nPerplexity\n1x Baseline NA 82.04\n338× 2a 0 104.061\n100×\n1 99.93 150.737\n2a 99.93 138.31\n2b 99.93 123.835\n2a+BCD 99.93 100.37\n2b+BCD 99.93 101.987\nCMR 99.93 95.382\nor without Block Coordinate Descent (BCD). In BCD we\nalternate between, only training Mkp, blocking gradient\nﬂow to Msp, or train Mkp, blocking gradient ﬂow to Msp.\nThe training curves across multiple epochs for these vari-\nous techniques can be found in Figure 3. As the sparsity\nincreases, the training perplexity does not increase as much\nas in ﬁgure 2 for equations 2a-2b when trained using BCD.\nHowever, there is still a small increase in training error\nwith increased sparsity. This can indicate that CMA may\nhave not been completely managed. Table 2 shows the test\nperplexity at the end of training for the various techniques\ndiscussed above. As you can see, these techniques help us\nbring the perplexity down to 100.37 from 150.737 originally.\nHowever, by reducing the compression factor from 338×to\n100×, we are improving the perplexity by approximately 4\npoints only.\n3.3 Co-matrix Row Dropout Regularization (CMR)\nTo better manage CMA, we focused on how a DKP Cell\nconverts input feature vector into an output feature vector.\nWhen an input feature vector, i, is fed to a LSTM layer, it\ngets multiplied with the weight matrix,\no= W ∗i. (3)\nIn the case of DKP, W is composed of two sets of matrices\no= (Mkp + Msp) ∗i,where Mkp = B⊗C (4)\no= Mkp ∗i+ Msp ∗i (5)\nThus each element of the output vector is a combination of\noutput of Mkp ∗I and Msp ∗I, i.e.\noj: = (Mkp)j: ∗i+ (Msp)j: ∗i (6)\nwhere j : refers to the jth row of the Mkp and Msp matrix.\nThus each element (or neuron) of the output feature vector\nis the sum of elements (or neurons) coming in from theMkp\nmatrix and the Msp matrix.\nSubmission and Formatting Instructions for SysML 2019\nTable 3.Results of compressing medium LM over multiple compression factors using DKP, HMD, HKP, LMF, Pruning and Small\nBaseline\nBaseline\nTest Perplexity 82.04\nCompression\nFactor 338× 100× 92× 75× 50× 25× 20× 10× 5×\nDopedKP 104.061 95.49 86.576 86.73 85.45 83.24 82.94 82.9 82.53\nPrune 115.62 103.219 103.34 91.618 90.314 88.555 85.14 82.551 82.47\nHKD\nDid not run\n99.882 95.12 92.56\nHMD 105.43 97.59 95.387\nLMF 108.61 103.42 99.29\nSmall Baseline 115.34 109.78 102.2\nOur hypothesis is that during CMA, the incoming neurons\nfrom the Mkp matrix and the Msp matrix learn to co-adapt,\nleading to lost capacity. Furthermore, because of the domi-\nnance of the Msp matrix during the initial phase of the train-\ning (Section 3.1), the Mkp neurons rely on the Msp neurons\nheavily. If we introduce a stochastic behavior where either\nthe Mkp neuron or the Msp neuron are not available to drive\nthe output neuron, this co-adaptation could be managed.\nThus to manage CMA more efﬁciently, this paper proposes\nco-matrix row dropout regularization (CMR). This regular-\nization extends the concept of stochastic depth (Huang et al.,\n2016) to regularize the output of each row of the output\nvector in order to avoid CMA. From a mathematical point of\nview, we introduce dropout after the output of each Mkp ∗i\nand Msp∗value, i.e. equation 6 is changed to,\noj: = ((Mkp)j: ∗I)bern1 + ((Msp)j: ∗I) ∗bern2, (7)\nwhere,\nbern1,bern2 ∼Bernoulli{p}. (8)\nAs the sparsity of Msp increases, the need for CMR de-\ncreases and can be removed entirely. The training method-\nology described by equation 7 is referred to as CMR in this\npaper. CMR is an extremely effective technique to man-\nage CMA as evident by the trends in Figure 3 for CMR.\nThe training perplexity during the training phase does not\nincrease as the sparsity of the Msp matrix increases. The\nbeneﬁts in the ﬁnal Test Perplexity are also evident as shown\nin the last row of the Table 2.\n4 R ESULTS\nWe compress the PTB based medium LM in (Zaremba\net al., 2014) by multiple compression factors and com-\npare the DKP trained using CMR with pruning ((Zhu &\nGupta, 2017)), LMF ((Kuchaiev & Ginsburg, 2017)), HMD\n((Thakker et al., 2019c)) and HKP ((Thakker et al., 2019b)).\nAs a baseline, we also train a small baseline by reducing the\nsize of the hidden vector in the LSTM layer.\nTable 4.Comparing DKP with previous published work targeting\nthe same benchmark\nComparisons\nwith prior art\nCompression\nFactor Test Perp\nBaseline LM 1× 82.04\n4-bit quant (Park et al., 2017) 8× 83.84\n3-bit quant (Lee & Kim, 2018) 10.67× 83.14\nTensor Train (Grachev et al., 2019) 1.67× 168.639\nWeight Distortion (Lee et al., 2018) 10× 84.64\nWeight Distortion (Lee et al., 2018) 20× 93.39\nDKP (Ours) 25× 83.24\nTable 3 shows the results of compressing the benchmark\nfor various compression factors. As shown, DKP outper-\nforms all compression techniques up to 20×compression\nfactors. Table 4 further compares these results with other\nrecently published work. Again, our compression technique\noutperforms these recent papers, achieving 2.5×more com-\npression than the best performing technique.\n5 C ONCLUSION\nThis paper presents a new compression technique called\nDoped Kronecker Product (DKP). However, training DKP is\nnon-trivial and can run into co-matrix adaptation issues. We\nfurther propose co-matrix row dropout regularization (CMR)\nto manage CMA. The preliminary results demonstrate that\nusing DKP with CMR, we can compress a large language\nmodel with LSTM layers of size 25 MB by 25 ×, with\n1.2% loss in perplexity score. Our technique outperforms\npopular compression techniques in previously published\nwork, improving the perplexity scores by 7.9% - 27%.\nREFERENCES\nCourbariaux, M. and Bengio, Y . Binarynet: Training deep\nneural networks with weights and activations constrained\nto +1 or -1. CoRR, abs/1602.02830, 2016. URL http:\nSubmission and Formatting Instructions for SysML 2019\n//arxiv.org/abs/1602.02830.\nGope, D., Dasika, G., and Mattina, M. Ternary hybrid\nneural-tree networks for highly constrained iot applica-\ntions. In Proceedings of Machine Learning and Systems\n2019, pp. 190–200. 2019.\nGope, D., Beu, J., Thakker, U., and Mattina, M. Ternary\nmobilenets via per-layer hybrid ﬁlter banks. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) Workshops, June 2020a.\nGope, D., Beu, J. G., Thakker, U., and Mattina, M.\nAggressive compression of mobilenets using hybrid\nternary layers. tinyML Summit, 2020b. URL https:\n//www.tinyml.org/summit/abstracts/\nGope Dibakar poster abstract.pdf.\nGrachev, A. M., Ignatov, D. I., and Savchenko, A. V . Com-\npression of recurrent neural networks for efﬁcient lan-\nguage modeling. CoRR, abs/1902.02380, 2019. URL\nhttp://arxiv.org/abs/1902.02380.\nHan, S., Mao, H., and Dally, W. J. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huffman coding. International Confer-\nence on Learning Representations (ICLR), 2016.\nHuang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger,\nK. Q. Deep networks with stochastic depth. CoRR,\nabs/1603.09382, 2016. URL http://arxiv.org/\nabs/1603.09382.\nHubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R.,\nand Bengio, Y . Quantized neural networks: Training\nneural networks with low precision weights and acti-\nvations. CoRR, abs/1609.07061, 2016. URL http:\n//arxiv.org/abs/1609.07061.\nKuchaiev, O. and Ginsburg, B. Factorization tricks for\nLSTM networks. CoRR, abs/1703.10722, 2017. URL\nhttp://arxiv.org/abs/1703.10722.\nLee, D. and Kim, B. Retraining-based iterative\nweight quantization for deep neural networks. CoRR,\nabs/1805.11233, 2018. URL http://arxiv.org/\nabs/1805.11233.\nLee, D., Kapoor, P., and Kim, B. Deeptwist: Learning model\ncompression via occasional weight distortion. CoRR,\nabs/1810.12823, 2018. URL http://arxiv.org/\nabs/1810.12823.\nPark, E., Ahn, J., and Yoo, S. Weighted-entropy-based\nquantization for deep neural networks. In 2017 IEEE\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR), pp. 7197–7205, July 2017. doi: 10 .1109/\nCVPR.2017.761.\nSindhwani, V ., Sainath, T., and Kumar, S. Structured trans-\nforms for small-footprint deep learning. In Cortes, C.,\nLawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett,\nR. (eds.), Advances in Neural Information Processing Sys-\ntems 28, pp. 3088–3096. Curran Associates, Inc., 2015.\nTao, J., Thakker, U., Dasika, G., and Beu, J. Skipping\nrnn state updates without retraining the original model.\nIn Proceedings of the 1st Workshop on Machine Learn-\ning on Edge in Sensor Systems, SenSys-ML 2019, pp.\n31–36, New York, NY , USA, 2019. Association for\nComputing Machinery. ISBN 9781450370110. doi:\n10.1145/3362743.3362965. URL https://doi.org/\n10.1145/3362743.3362965.\nThakker, U., Beu, J. G., Gope, D., Dasika, G., and Mattina,\nM. Run-time efﬁcient RNN compression for inference\non edge devices. CoRR, abs/1906.04886, 2019a. URL\nhttp://arxiv.org/abs/1906.04886.\nThakker, U., Beu, J. G., Gope, D., Zhou, C., Fedorov, I.,\nDasika, G., and Mattina, M. Compressing rnns for iot\ndevices by 15-38x using kronecker products. CoRR,\nabs/1906.02876, 2019b. URL http://arxiv.org/\nabs/1906.02876.\nThakker, U., Dasika, G., Beu, J. G., and Mattina, M. Mea-\nsuring scheduling efﬁciency of rnns for NLP applica-\ntions. CoRR, abs/1904.03302, 2019c. URL http:\n//arxiv.org/abs/1904.03302.\nThakker, U., Fedorov, I., Beu, J. G., Gope, D., Zhou, C.,\nDasika, G., and Mattina, M. Pushing the limits of RNN\ncompression. CoRR, abs/1910.02558, 2019d. URL\nhttp://arxiv.org/abs/1910.02558.\nTjandra, A., Sakti, S., and Nakamura, S. Compressing\nrecurrent neural network with tensor train. In Neural\nNetworks (IJCNN), 2017 International Joint Conference\non, pp. 4451–4458. IEEE, 2017.\nZaremba, W., Sutskever, I., and Vinyals, O. Recurrent\nneural network regularization. CoRR, abs/1409.2329,\n2014. URL http://arxiv.org/abs/1409.2329.\nZhu, M. and Gupta, S. To prune, or not to prune: exploring\nthe efﬁcacy of pruning for model compression. arXiv\ne-prints, art. arXiv:1710.01878, October 2017.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9556989669799805
    },
    {
      "name": "Kronecker product",
      "score": 0.7935842275619507
    },
    {
      "name": "Computer science",
      "score": 0.661801815032959
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6468332409858704
    },
    {
      "name": "Kronecker delta",
      "score": 0.4821998178958893
    },
    {
      "name": "Algorithm",
      "score": 0.48044607043266296
    },
    {
      "name": "Compression (physics)",
      "score": 0.43967756628990173
    },
    {
      "name": "Artificial intelligence",
      "score": 0.379304975271225
    },
    {
      "name": "Language model",
      "score": 0.35093605518341064
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    }
  ],
  "institutions": []
}