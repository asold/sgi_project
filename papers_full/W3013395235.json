{
  "title": "Abstractive Text Summarization based on Language Model Conditioning and Locality Modeling",
  "url": "https://openalex.org/W3013395235",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Aksenov, Dmitrii",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2567009157",
      "name": "Moreno Schneider Julian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2564774948",
      "name": "Bourgonje, Peter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287128122",
      "name": "Schwarzenberg, Robert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223695438",
      "name": "Hennig, Leonhard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2566280940",
      "name": "Rehm, Georg",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Moreno-Schneider, Juli\\'an",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2949807892",
    "https://openalex.org/W2798761464",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2964016842",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3013078432",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3031478045",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2899423466",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2951265142",
    "https://openalex.org/W2962985882",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2151695628",
    "https://openalex.org/W3098136301",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W2030265142",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3031265507",
    "https://openalex.org/W2093969193",
    "https://openalex.org/W2803930360",
    "https://openalex.org/W2947771965",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2985808369",
    "https://openalex.org/W1831406492",
    "https://openalex.org/W2902744721",
    "https://openalex.org/W2924690340"
  ],
  "abstract": "We explore to what extent knowledge about the pre-trained language model that is used is beneficial for the task of abstractive summarization. To this end, we experiment with conditioning the encoder and decoder of a Transformer-based neural model on the BERT language model. In addition, we propose a new method of BERT-windowing, which allows chunk-wise processing of texts longer than the BERT window size. We also explore how locality modelling, i.e., the explicit restriction of calculations to the local context, can affect the summarization ability of the Transformer. This is done by introducing 2-dimensional convolutional self-attention into the first layers of the encoder. The results of our models are compared to a baseline and the state-of-the-art models on the CNN/Daily Mail dataset. We additionally train our model on the SwissText dataset to demonstrate usability on German. Both models outperform the baseline in ROUGE scores on two datasets and show its superiority in a manual qualitative analysis.",
  "full_text": "Abstractive Text Summarization based on\nLanguage Model Conditioning and Locality Modeling\nDmitrii Aksenov, Juli´an Moreno-Schneider, Peter Bourgonje,\nRobert Schwarzenberg, Leonhard Hennig, Georg Rehm\nDFKI GmbH, Alt-Moabit 91c, 10559 Berlin, Germany\n{ﬁrstname.lastname}@dfki.de\nAbstract\nWe explore to what extent knowledge about the pre-trained language model that is used is beneﬁcial for the task of abstractive\nsummarization. To this end, we experiment with conditioning the encoder and decoder of a Transformer-based neural model on the\nBERT language model. In addition, we propose a new method of BERT-windowing, which allows chunk-wise processing of texts longer\nthan the BERT window size. We also explore how locality modeling, i. e., the explicit restriction of calculations to the local context,\ncan affect the summarization ability of the Transformer. This is done by introducing 2-dimensional convolutional self-attention into the\nﬁrst layers of the encoder. The results of our models are compared to a baseline and the state-of-the-art models on the CNN/Daily Mail\ndataset. We additionally train our model on the SwissText dataset to demonstrate usability on German. Both models outperform the\nbaseline in ROUGE scores on two datasets and show its superiority in a manual qualitative analysis.\nKeywords:Summarisation, Language Modeling, Information Extraction, Information Retrieval, BERT, Locality Modeling\n1. Introduction\nText summarization is an NLP task with many real-world\napplications. The ever-increasing amount of unstructured\ninformation in text form calls for methods to automati-\ncally extract the relevant information from documents and\npresent it in condensed form. Within the ﬁeld of sum-\nmarization, different paradigms are recognised in two di-\nmensions: extractive vs. abstractive, and single-document\nvs. multi-document. In extractive summarization, those\nsentences or words are extracted from a text which carry\nthe most important information, directly presenting the re-\nsult of this as the summary. Abstractive summarization\nmethods paraphrase the text, and by changing the text aim\nto generate more ﬂexible and consistent summaries. Fur-\nthermore, single-document summarization works on sin-\ngle documents, while multi-document summarization deals\nwith multiple documents at once and produces a single\nsummary. In this paper, we concentrate on single-document\nabstractive summarization. Most recent abstractive models\nutilize the neural network-based sequence-to-sequence ap-\nproach. During training, such models calculate the condi-\ntional probability of a summary given the input sequence\nby maximizing the loss function (typically cross-entropy).\nMost approaches are based on the encoder-decoder frame-\nwork where the encoder encodes the input sequence into a\nvector representation and the decoder produces a new sum-\nmary given the draft summary (which is the part of the\nsummary generated during previous iterations). The last\nlayer of a decoder, the generator, maps hidden states to to-\nken probabilities. We use a state-of-the-art Transformer for\nsequence-to-sequence tasks which is built primarily on the\nattention mechanism (Vaswani et al., 2017).\nWe attempt to improve performance of abstractive text sum-\nmarization by improving the language encoding capabili-\nties of the model. Recent results have shown that the main\ncontribution of the Transformer is its multi-layer archi-\ntecture, allowing Self-Attention to be replaced with some\nother technique without a signiﬁcant drop in performance\n(Domhan, 2018; Wu et al., 2019). Following this strat-\negy, we develop a model that introduces convolution into\nthe vanilla Self-Attention, allowing to better encode the lo-\ncal dependencies between tokens. To overcome the data\nsparsity problem, we use a pre-trained language model for\nthe encoding part of the encoder-decoder setup, which cre-\nates a contextualized representation of the input sequence.\nSpeciﬁcally, we use BERT due to its bi-directional context\nconditioning, multilingualism and state-of-the-art scores on\nmany other tasks (Devlin et al., 2019). Furthermore, we\npropose a new method which allows applying BERT on\nlonger texts. The main contributions of this paper are: (1)\nDesigning two new abstractive text summarization models\nbased on the ideas of conditioning on the pre-trained lan-\nguage model and application of convolutional self-attention\nat the bottom layers of the encoder. (2) Proposing a method\nof encoding the input sequence in windows which allevi-\nates BERT’s input limitations1 and allows the processing\nof longer input texts. (3) Evaluating the performance of our\nmodels on the English and German language by conducting\nan ablation study on CNN/Dail Mail and SwissText datasets\nand comparing it with other state-of-the-art methods.\n2. Related Work\n2.1. Pre-trained Language Models\nTraditionally, non-contextualized embedding vectors were\nused for pre-training neural-based NLP models (Mikolov\net al., 2013; Pennington et al., 2014). Recently, pre-\ntrained language models exploiting contextualized embed-\ndings, such as ELMo, GPT-2, BERT and XLNet raised the\nbar in many NLP tasks (Peters et al., 2018; Radford et al.,\n2018; Devlin et al., 2019; Yang et al., 2019b). Recent at-\ntempts to use these models for text summarization demon-\n1BERT can process sequences with a maximum of 512 tokens.\narXiv:2003.13027v1  [cs.CL]  29 Mar 2020\nstrated their suitability by achieving new state-of-the-art re-\nsults (Zhang et al., 2019; Liu, 2019; Liu and Lapata, 2019).\n2.2. Neural Abstractive Text Summarization\nThe neural approach toward abstractive summarization was\nlargely adopted by state-of-the-art models (Shi et al., 2018).\nA signiﬁcant contribution was the pointer Generator Net-\nwork (See et al., 2017). It uses a special layer on top of the\ndecoder network to be able to both generate tokens from\nthe dictionary and extract them from the input text. It uses\nthe coverage vector mechanism to pay less attention to to-\nkens already covered by previous iterations. An example\nof earlier work adapting Reinforcement Learning (RL) is\ndescribed by Paulus et al. (2018). The pure RL model\nachieved high ROUGE-1 and ROUGE-L scores but pro-\nduced unreadable summaries. Its combination with typi-\ncal cross-entropy optimization achieved high scores elim-\ninating the unreliability problem. Liu et al. (2018), to\nthe best of our knowledge, were the ﬁrst to use the Trans-\nformer model for summarization. It was only used in the\ndecoder on top of the extraction model with various atten-\ntion compression techniques to increase the size of the in-\nput sequence. Zhang et al. (2019) incorporate BERT into\nthe Transformer-based model. They use a two-stage proce-\ndure exploiting the mask learning strategy. Others attempt\nto improve their abstractive summarization models by in-\ncorporating an extractive model. For example, Li et al.\n(2018) use the Key information guide network to guide the\nsummary generation process. In Bottom-up summarization\n(Gehrmann et al., 2018) the extractive model is used to in-\ncrease the precision of the Pointer Generator mechanism.\nAnother strand of research adapts existing models to cope\nwith long text. Cohan et al. (2018) present the Discourse-\nAware Attention model which introduces hierarchy in the\nattention mechanism via calculating an additional attention\nvector over the sections of the input text. Subramanian et\nal. (2019) showed that the language model trained on the\ncombination of the original text, extractive summaries gen-\nerated by the model and the golden summary can achieve\nresults comparable to standard encoder-decoder based sum-\nmarization models.\n3. Approach\nOur text summarization model is based on the Transformer\narchitecture. This architecture adopts the original model\nof Vaswani et al. (2017). On top of the decoder, we use a\nPointer-Generator (Formula 1) to increase the extractive ca-\npabilities of the network (we later refer to this architecture\nas CopyTransformer).\np(w) =pgenPcopy(w) + (1−pgen)Psoftmax (w), (1)\nwhere Pcopy(w) is the probability of copying a speciﬁc\nword w from the source document, Psoftmax (w) is the\nprobability of generation a word calculated by the abstrac-\ntive summarization model and pgen is the probability of\ncopying instead of generation.\n3.1. Convolutional Self-Attention\nThe Transformer, like any other self-attention network, has\na hierarchical multi-layer architecture. In many experi-\nFigure 1: Model overview\nments it was shown that this architecture tends to learn lexi-\ncal information in the ﬁrst layers, sentence-level patterns in\nthe middle and the semantics in the upper layers (Raganato\nand Tiedemann, 2018; Tenney et al., 2019). The disadvan-\ntage of this approach is that during the attention operation it\nconsiders all tokens as equally important, whereas syntac-\ntic information is mostly concentrated in certain local areas.\nThis problem is usually speciﬁed as the problem of locality\nmodeling. As syntactic information can help in identifying\nmore important words or phrases, it could be beneﬁcial to\nfocus attention on these regions.\nA successful approach to the locality modeling task are\nthe so-called convolutions (local) self-attention networks\n(Yang et al., 2019a). Essentially, the problem is dealt with\nby the application of a 1-dimensional convolution to the\nself-attention operation at the network’s lower layers. This\nstrengthens dependencies among neighboring elements and\nmakes the model distance-aware when it searches for low-\nlevel patterns in a sequence. In other words, it restricts\nthe attention scope to the window of neighboring elements.\nThe 1D convolution applied to attention is illustrated in For-\nmulas 2, 3 and 4.\nˆKh = {kh\ni−M\n2\n, . . . ,kh\ni , . . . ,kh\ni+M\n2\n}, (2)\nˆVh = {vh\ni−M\n2\n, . . . ,vh\ni , . . . ,vh\ni+M\n2\n}, (3)\noh\ni = ATT(qh\ni , ˆKh) ˆVh, (4)\nwhere qh\ni is the query and M + 1(M ≤I) is its attention\nregion centered at the position i.\nThe convolution can be extended to the 2-dimensional area\nby taking interactions between features learned by the dif-\nferent attention heads of the Transformer into account. In\nthe original Transformer each head independently models a\ndistinct set of linguistic properties and dependencies among\ntokens (Raganato and Tiedemann, 2018). By applying 2-\ndimensional convolution, where the second dimension is\nthe index of attention head, we explicitly allow each head to\ninteract with learned features for their adjacent sub-spaces.\nThe shortcoming of the original implementation is that the\nﬁrst and the last heads do not interact as they are assumed\nnot to be adjacent. Thus, we assume that considering the\nheads’ sub-spaces periodically, we can increase the model’s\neffectiveness by applying circular convolution to the second\ndimension. In Section 5, we evaluate both the original ver-\nsion and our modiﬁcation.\n˜Kh =\n⋃\n[ ˆKh−N\n2 , . . . ,ˆKh, . . . ,ˆKh+N\n2 ], (5)\n˜Vh =\n⋃\n[ ˆVh−N\n2 , . . . ,ˆVh, . . . ,ˆVh+N\n2 ], (6)\noh\ni = ATT(qh\ni , ˜Kh) ˜Vh, (7)\nwhere (M + 1)(N ≤H) is the window region over heads\nand ⋃stands for the union of keys ˆKh and values ˆVh from\ndifferent subspaces.\nThe convolutional self-attention has been shown to be very\neffective in Machine Translation and several other NLP\ntasks. However, to our knowledge, it was never applied\nto the text summarization problem. For the experiments re-\nported on in this paper, we created our implementation of\nthe local attention and the convolutional self-attention net-\nwork (Transformer). It supports both 1D and 2D modes\nhaving the size of the kernels as system parameters. As\nin Yang et al. (2019a) we incorporate convolutional self-\nattention in the Transformer encoder by positioning it in\nthe place of the self-attention in the lower layers. In Sec-\ntion 5, we show that the low-level modeling capabilities of\nour encoder provides a strong boost to the model’s predic-\ntion accuracy in the text summarization task.\n3.2. BERT-Conditioned Encoder\nThe main task of the encoder is to remember all the se-\nmantic and syntactic information from the input text which\nshould be used by the decoder to generate the output.\nKnowledge transfer from the language model should the-\noretically improve its ability to remember the important in-\nformation due to the much larger corpus used in its pre-\ntraining phase compared to the corpus used in the text sum-\nmarization training phase. We thus condition our encoder\non the BERT language model.\nFor the encoder conditioning, we used the most straight-\nforward strategy recommended for the BERT based model:\nplacing the pre-trained language model in the encoder as\nan embeddings layer. This should make the embeddings\nof the system context-dependent. We decided not to ﬁne-\ntune the encoder on BERT for the sake of memory and time\neconomy. Instead, we follow the general recommendations\nby concatenating the hidden states of the last four layers\nof BERT into a 3072-dimensional embedding vector (De-\nvlin et al., 2019). We use two variations of the BERT-based\nencoder. The ﬁrst model uses only BERT to encode the in-\nput sequence and the second model feeds BERT’s generated\nembeddings into the vanilla Transformer encoder.\n3.3. BERT-Windowing\nOne of the key features of our approach is its ability to\novercome the length limitations of BERT, allowing it to\ndeal with longer documents. BERT’s maximum supported\nsequence length is 512 tokens 2, which is smaller than the\naverage size of texts used in most summarization datasets.\nOur method relies on the well-known method of window-\ning which to our knowledge was never used before neither\n2These are not tokens in the traditional sense, but so-called\nWordPiece tokens, see Devlin et al. (2019).\nin the BERT-based models nor in abstractive text summa-\nrization research (Figure 2). We apply BERT to the win-\ndows of texts with strides and generate N matrices, every\nmatrix embedding one window. Then we combine them\nby doing the reverse operation. The vectors at the overlap-\nping positions are averaged (by summing and dividing by\nthe number of overlapping vectors). As a result, we have\nthe matrix of embeddings with the shape of the hidden size\ntimes the length of the text. The drawback of this approach\nis that we reduce the size of the context as each resulted\nvector is calculated based on maximum twice the window\nsize number of tokens. Besides, the split of the text to equal\nsize windows will aggravate the consistency of the input as\nsome sentences will be split in an arbitrary manner between\ntwo adjacent windows. Despite this drawback, we assume\nthat this procedure will nevertheless improve the accuracy\nof the encoder trained on the non-truncated texts. We set\nthe window size to the maximum size of 512 tokens and\nthe stride to 256. We consider this stride size optimal due\nto a trade-off between the average context size and compu-\ntational requirements of the model (number of windows).\nBy this trade we ensure every token to have a 768 tokens-\ncontext except for the 256 initial and ﬁnal tokens, that only\nhave 512 tokens-context.\nFigure 2: Integration of BERT-generated contextual repre-\nsentations from two windows\n3.4. BERT-Conditioned Decoder\nIn the decoder, pre-training was applied in a similar way.\nThe main difference is that instead of the ﬁnal output of\nBERT we use only its word embedding matrix (without po-\nsitions). The reason behind this is that in the decoder the\ngenerated probability distribution is conditioned on the in-\ncomplete text (previous summary draft output) while BERT\nimplicitly assumes consistent and completed input (Zhang\net al., 2019). As context-independent embeddings are not\nenough to represent the minimum set of features to make a\nmeaningful prediction, the custom Transformer decoder is\nalways stacked on top of BERT.\nOur whole BERT-based model is similar to One-Stage\nBERT (Zhang et al., 2019) and BertSumAbs (Liu and La-\npata, 2019) but differs in the usage of the four last hidden\nstates of BERT to create contextualized representation, in\npresence of Pointer Generator and capabilities to process\nFigure 3: Two different approaches for the integration of the BERT-conditioning with Convolutional Self-Attention\nMethod ROUGE-1 ROUGE-2 ROUGE-L\nCopyTransformer 31.95 14.49 30.02\n+ 1D conv. 32.62 14.99 30.74\n+ 2D conv. 32.72 15.12 30.85\n+ 2D Circular conv. 32.68 15.01 30.76\nTable 1: Ablation study of model with Convolutional Self-\nAttention on the CNN/Daily Mail dataset (kernel sizes are\n11 and 3)\nlong texts. In Figure 1 we show the schema of the ba-\nsic model with the BERT-conditioned convolutional self-\nattention encoder and BERT-conditioned decoder.\n3.5. Integration of BERT and Convolutional\nSelf-Attention\nWe evaluated two different ways of integrating the BERT-\nconditioning with the convolutional self-attention of the\nmodel’s encoder (Figure 3).\nStacking This approach comprises feeding the BERT-\ngenerated embeddings to the convolutional self-attention\nTransformer encoder. A potential problem with this ap-\nproach is that convolutional self-attention is assumed to be\nbeneﬁcial when applied in the lower layers as its locality\nmodeling feature should help in modeling of local depen-\ndencies (e. g., syntax). At the same time, BERT is a hierar-\nchical model where the last layers target high-level patterns\nin the sequences (e. g., semantics). We assume that the ap-\nplication of the network detecting the low-level patterns on\nBERT’s output can undermine its generalization abilities.\nConcatenation Because of the considerations raised\nabove, we also develop a second approach which we call\nConcatenation. We split the convolutional self-attention\nTransformer encoder into two networks where the ﬁrst one\nuses only convolutional self-attention and the second orig-\ninal self-attention (identical to the Transformer encoder).\nThen we feed the original sequences into BERT and into\nthe convolutional self-attention network in parallel. The re-\nsulting embedding vectors are concatenated and fed into the\nTransformer encoder. In this way, we model the locality at\nthe lower layers of the encoder at the cost of a smaller depth\nof the network (assuming the same number of layers).\n4. Datasets\nWe aim to develop a system that works in a language-\nindependent way. It assumes that either the upstream\ncomponents are available in the respective language, or\nthey are themselves language-independent, such as the\nmulti-lingual version of BERT. Since most summarization\ndatasets are in English however, we use English for the\nevaluation and additionally include German to check if of\nour model can be applied to another language.\n4.1. CNN/Daily Mail\nOur experiments are mainly conducted on the CNN/Daily\nMail dataset (Hermann et al., 2015; Nallapati et al., 2016).\nIt contains a collection of news articles paired with multi-\nsentence summaries published on the CNN and Daily Mail\nwebsites. This dataset is the de facto standard for training\nsummarization models. We use the non-anonymized data\nas was used for training of the most recent state-of-the-art\nmodels (e. g., See et al. (2017)). The raw dataset consists\nof separate text ﬁles each representing a single article or\na summary. We use the data in its preprocessed version\nas provided by Gehrmann et al. (2018). It has 287,226\ntraining pairs, 13,368 validation pairs and 11,490 test pairs.\nTo align the data with the vocabulary of BERT we tok-\nenized it using the BPE-based WordPiece tokenizer (De-\nvlin et al., 2019). As all samples in BERT’s training data\nare prepended with the special token ”[CLS]”, we follow\nFigure 4: Effect of the window size on ROUGE-1\nModel ROUGE-1 ROUGE-2 ROUGE-L\nTransformer 24.82 6.27 22.99\nCopyTransformer 31.95 14.49 30.02\nBert Encoder + Transformer Decoder 31.3 13.37 29.46\nBert-transformer Encoder + Transformer Decoder 32.5 14.68 30.68\nBert-transformer Encoder + Bert-transformer Decoder 33.23 14.99 31.26\nTransformer (full text) 23.18 5.15 21.48\nBert-transformer Encoder + Transformer Decoder (full text) 31.51 14.1 29.77\nTable 2: Ablation study of the BERT-based model on truncated and original CNN/Daily Mail dataset\nModel ROUGE-1 ROUGE-2 ROUGE-L\nTransformer 36.40 20.69 34.14\nCopyTransformer 39.44 25.11 37.16\nBert-transformer Encoder + Transformer Decoder 44.01 29.60 41.65\nBert-transformer Encoder + Bert-transformer Decoder 43.22 29.01 40.84\nTransformer (full text) 34.76 18.65 32.61\nBert-transformer Encoder + Transformer Decoder (full text) 45 30.49 42.64\nTable 3: Ablation study of the BERT-based model on the truncated and original SwissText dataset\nthis and add it to every source text in our dataset. In the\nresulting dataset, the average lengths of an article and a\nsummary are 895 and 63 tokens, respectively. In most of\nour experiments, we use the clipped version of the training\nand validation datasets with each article truncated to 512\ntokens. In the experiments on BERT windowing, we use\nthe full-text version.\n4.2. SwissText Dataset\nTo evaluate the efﬁciency of the model in a multi-lingual,\nmulti-domain environment we conduct a series of experi-\nments on the German SwissText dataset. This dataset was\ncreated for the 1st German Text Summarization Challenge\nat the 4th Swiss Text Analytics Conference – SwissText\n2019 (ZHAW, 2019). It was designed to explore differ-\nent ideas and solutions regarding abstractive summariza-\ntion of German texts. To the best of our knowledge, it is\nthe ﬁrst long document summarization dataset in the Ger-\nman language that is publicly available. The data was ex-\ntracted from the German Wikipedia and represents mostly\nbiographical articles and deﬁnitions of various concepts.\nThe dataset was tokenized by the multilingual WordPiece\ntokenizer (Devlin et al., 2019) and preprocessed in the\nsame way as the CNN/Daily Mail dataset. It was split into\nthe training, validation and testing sets containing 90,000,\n5,000 and 5,000 samples, respectively. The average length\nof a source sequence is 918 tokens, which makes this\ndataset suitable for our experiments on windowing.\n5. Experiments\nOur system is built on the OpenNMT library. For training,\nwe use cross-entropy loss and the Adam optimizer with the\nNoam decay method (Kingma and Ba, 2014). Regulariza-\ntion is made via dropout and label smoothing. For eval-\nuation, we calculate the F1-scores for ROUGE using the\nﬁles2rouge library. The ROUGE evaluation is made on the\nsequences of WordPiece tokens.\n5.1. Locality Modeling\nTo evaluate the effect of convolution on self-attention we\nintroduce it in the ﬁrst layer of the encoder. We use the\nsame kernel sizes as in Yang et al. (2019a). In these exper-\niments, to accelerate the training process, we use a small\nmodel with a hidden size of 256, four self-attention heads\nand three layers in the encoder and decoder. All models are\ntrained for 90,000 training steps with the Coverage Penalty.\nAs a baseline, we use our implementation of CopyTrans-\nformer. In contrast to See et al. (2017), we do not re-use\nthe attention layer for the decoder but train a new Pointer-\nGenerator layer from scratch.\nThe results are presented in Table 1. We see that both con-\nvolutions over tokens and over attention heads improve the\nROUGE scores. Standard convolution outperformed circu-\nlar convolution on ROUGE-1, ROUGE-2 and ROUGE-L\nby 0.06, 0.13 and 0.09 percent, respectively.\nWe also investigated the effect of the window size of the 1-\ndimensional convolution on ROUGE scores (Figure 4). In\ncontrast to ﬁndings in Machine Translation, we found that\nsize 13 returns the best result for the summarization task.\n5.2. BERT Conditioning\nTo ﬁnd the optimal architecture of the BERT-based ab-\nstractive summarizer we conducted an ablation study\n(Table 2). All hyperparameters were set equal to\nthe ones in experiments in convolutional self-attention.\nOn CNN/Daily Main dataset we test three different\nmodels: BERT encoder+Transformer Decoder, BERT-\nTransformer encoder+Transformer decoder and BERT-\nTransformer encoder+BERT-Transformer decoder. The\nversion of BERT used in the experiments is BERT-Base. As\nthe baseline, we use the Transformer without Pointer Gen-\nerator. From the results, we observe that BERT improves\nthe efﬁciency of the model when it is used in both encoder\nand decoder. Besides, BERT in the encoder is more effec-\ntive when it is used to produce embeddings to be used by the\nstandard Transformer encoder than when it is used solely as\nan encoder. Even without a Pointer Generator, our model\noutperformed the CopyTransformer baseline by 1.28, 0.5\nand 1.24 on ROUGE-1, ROUGE-2 and ROUGE-L.\nTo evaluate our BERT-windowing method we conducted\nthe experiments on the full text. Our approach outperforms\nthe baseline, which proves that the method can be success-\nfully applied to texts longer than 512 tokens. The ﬁnal per-\nformance of this model is still lower than that of the model\ntrained on the truncated text, but as the same pattern can\nbe observed for the baselines we assumed this relates to the\nspeciﬁcs of the dataset that is prone to having important\ninformation in the ﬁrst sentence of a text.\nOn SwissText data we use the multilingual version\nof BERT-Base. We evaluated two models with\nBert-transformer encoder and Transformer and BERT-\nTransformer decoders (Table 3). The introduction of BERT\ninto the transformer increased the ROUGE-1, ROUGE-2\nand ROUGE-L scores by 7.21, 8.91 and 7.51 percent, re-\nspectively. At the same time, the usage of BERT in the\ndecoder decreased the overall score. We assume that the\nreason behind this is that in multilingual BERT, due to its\nlanguage-independence, the embedding matrix outputs less\nprecise contextualized representations which undermines\ntheir beneﬁts for the summarization task.\nOn the non-truncated texts, usage of the Bert-transformer\nencoder increased the ROUGE scores by 10.23, 11.84 and\n10.03 percent. Furthermore, it gives us higher scores com-\npared to the same model on truncated texts. This demon-\nstrates the usability of BERT-windowing for this particu-\nlar dataset. We assume that the difference in performance\non the CNN/Daily Mail datasets reﬂects the difference in\ndistribution of the useful information within the text. Par-\nticularly, that in the SwissText dataset, it is spread more\nuniformly than in the CNN/Daily Mail dataset. We con-\nducted a small experiment comparing the average ROUGE\nscore between a golden summary and the head and the tail\nof a document (taking the ﬁrst or last n sentences, where\nn correlates to the length of the gold summary) on both\ndatasets. The difference between taking the head and a tail\non the SwissText dataset (ROUGE-L of 34.79 vs. 20.15,\nrespectively) was much smaller than on CNN / Daily Mail\n(ROUGE-L of 16.95 vs. 12.27, respectively) which con-\nﬁrms our hypothesis.\n5.3. Integration Strategies\nTo evaluate the integration strategies, we trained two mod-\nels with the respective BERT-based baselines. Both models\nhave in their encoder two Transformer layers and one Con-\nvolutional Transformer layer placed on top of BERT or in\nparallel, respectively (Table 4).\nThe method of stacking does not provide any signiﬁcant\nimprovement. With the introduction of convolutional self-\nattention only ROUGE-1 increased by 0.12 percent, while\nROUGE-2 dropped by 0.3 and ROUGE-L remained the\nsame. Considering that in many domains ROUGE-2 max-\nimally correlates with human assessment (see Section 7),\nwe dismiss this method. The concatenation strategy con-\nvolution is shown to be much more efﬁcient, increasing\nROUGE scores by 0.44,0.33 and 0.43 percent. This con-\nﬁrms our hypothesis that locality modeling is the most efﬁ-\ncient when applied at the bottom on the non-contextualized\nword representations. Unfortunately, this model failed to\noutperform the stacking baseline. We conclude that the\nconcatenating architecture undermines the performance of\nthe Transformer model, and the convolutional self-attention\nis not beneﬁcial when used together with pre-trained lan-\nguage models. Hence, we decided to train our two ﬁnal\nmodels separately.\n5.4. Model Comparison\nFor the ﬁnal comparison of our model to other state-of-the-\nart methods we conducted experiments on the CNN/Daily\nMail dataset. We set the hidden state to 512, the number of\nTransformer layers in the encoder and layers to six and the\nnumber of self-attention heads to eight. Hence, our baseline\nis smaller than the original CopyTransformer (Gehrmann et\nal., 2018), which may be the reason why it performs slightly\nworse (Table 5). BERT-conditioning was used in both the\nencoder and decoder. The sizes of convolution kernels are\nset to 13 and three. The networks were trained for 200,000\ntraining steps on a single NVIDIA GeForce GTX 1080 Ti.\nThe generation of the summary was made via the Beam\nsearch algorithm with the Beam size set to four. Finally,\nthe generated summaries were detokenized back to the se-\nquences of words separated by spaces.\nFor the BERT-based model, we set the minimum length\nof a generated summary to 55 as we found that without\nsuch restriction the model was prone to generate shorter\nsequences than in the test dataset. The model outperformed\nthe baseline by 1.27 on ROUGE-1, 1.14 on ROUGE-2 and\n1.3 on ROUGE-L. This is better than the scores of One-\nStage BERT but still worse than the two-stage and Bert-\nSumAbs models.\nFor the convolutional CopyTransformer we use convolu-\ntional self-attention in the ﬁrst three layers of the encoder.\nIt increased ROUGE-1, ROUGE-2 and ROUGE-L by 0.25,\n0.41 and 0.12.\nFurthermore, we present the ﬁrst publicly available bench-\nmark for the SwissData dataset (Table 6). 3 All param-\n3For comparability with our other model we include results\nMethod of Integration Model ROUGE-1 ROUGE-2 ROUGE-L\nStacking BERT+CopyTransformer 35.28 17.12 33.31\nBERT+Convolutional CopyTransformer 35.4 16.82 33.31\nConcatenation BERT+CopyTransformer 34.82 16.46 32.79\nBERT+Convolutional CopyTransformer 35.26 16.79 33.22\nTable 4: Different strategies for integrating language models with convolutional Self-Attention (CNN/Daily Mail dataset)\nMethod ROUGE-1 ROUGE-2 ROUGE-L\nBiLSTM + Pointer-Generator + Coverage (See et al., 2017) 39.53 17.28 36.38\nML + Intra-Attention (Paulus et al., 2018) 38.30 14.81 35.49\nCopyTransformer (Gehrmann et al., 2018) 39.25 17.54 36.45\nBottom-Up Summarization (Gehrmann et al., 2018) 41.22 18.68 38.34\nOne-Stage BERT (Zhang et al., 2019) 39.50 17.87 36.65\nTwo-Stage BERT (Zhang et al., 2019) 41.38 19.34 38.37\nML + Intra-Attention + RL (Paulus et al., 2018) 39.87 15.82 36.90\nKey information guide network (Li et al., 2018) 38.95 17.12 35.68\nSentence Rewriting (Chen and Bansal, 2018) 40.88 17.80 38.54\nBertSumAbs (Liu and Lapata, 2019) 41.72 19.39 38.76\nCopyTransformer (our implementation) 38.73 17.28 35.85\nConvolutional CopyTransformer 38.98 17.69 35.97\nBERT+CopyTransformer (enc., dec.) 40 18.42 37.15\nTable 5: ROUGE scores for various models on the CNN/Daily Mail test set. The ﬁrst section shows different state-of-the-art\nmodels, the second section presents our models and baseline.\nMethod ROUGE-1 ROUGE-2 ROUGE-L\nCopyTransformer (our implementation) 39.5 22.36 36.97\nConvolutional CopyTransformer 40.54 23.62 38.06\nBERT+CopyTransformer (enc.) 42.61 25.25 39.85\nTable 6: ROUGE scores for our models on the SwissText test set\neters are equal to the CNN/Daily Mail baseline. BERT-\nconditioning was used only in the encoder. The networks\nwere trained on the truncated texts in 90,000 training steps.\nFrom the results we see that the convolutional CopyTrans-\nformer showed much more efﬁciency than on CNN/Daily\nMail dataset, outperforming the baseline by 1.04 percent\non ROUGE-1, 1.26 on ROUGE-2 and 1.09 on ROUGE-L.\nThe BERT-based model achieved the highest scores.\n6. Qualitative Analysis\nAs ROUGE evaluation is not always a valid method for\nquality assessment we perceive the need for an additional,\nmanual evaluation. The best solution would be to conduct a\nﬁne-grained study of the models’ outputs by manually rank-\ning them in terms of semantic coherence, grammaticality,\netc. However, due to the time-consuming nature of such an\nevaluation, we reverted to a qualitative analysis comparing\nseveral summaries generated by different models. Figure 5\nincludes the reference summary and those generated by the\ndifferent models. Comparing the ﬁrst sentence we see that\nthe vanilla Transformer model performed worse by copying\nonly part of the original sentence omitting some characters\nin the word “meteorological”. The model with convolution\nhas copied the whole sentence but still made a spelling er-\nror. Finally, only the BERT-based model succeeded to gen-\nerate the right token, “meteorological”. Also, we see that\nwhile the BERT-based model’s summary conveys the same\nmeaning as the gold summary, the convolutional Trans-\nformer generates one and Transformer two sentences that\nare not present in the gold summary. Overall, on the given\nfor the bigger BERT+CopyTransformer model. At the same time,\nwe found that the smaller model without the copy mechanism\nachieved higher scores with 45.12 ROUGE-1, 28.38 ROUGE-2\nand 42.99 ROUGE-L. This needs to be explored in future work.\nexample all models provided a summary of extractive na-\nture and only the BERT-based model shows some level of\nabstractiveness merging parts of the two sentences into the\nsingle one (in the second summary’s sentence). This is far\nfrom the gold summary where every sentence in some way\nparaphrases the original text. Hence, given this particular\nexample, our models demonstrate some explicit improve-\nments. Still, abstractive summarization remains challeng-\ning. The paraphrasing capabilities of all state-of-the-art\nsystems are low and the models are not guaranteed to pro-\nduce summaries which follow the initial order of the se-\nquence of events.\n7. Discussion: Summarization Evaluation\nROUGE (Lin, 2004) is the most widely adopted metric used\nfor evaluating automatic text summarization approaches.\nThe evaluation is made though comparison of a set of\nsystem-generated candidate summaries with a gold stan-\ndard summary. The availability of the corresponding soft-\nware and its performance contributed to its popularity (Co-\nhan and Goharian, 2016). Despite its adoption in many\nstudies, the metric faced some key criticisms.\nThe main criticism of ROUGE is that it does not take into\naccount the meaning expressed in the sequences. The met-\nric was developed based on the assumption that a high qual-\nity generated candidate summary should share many words\nwith a single human-made gold standard summary. This as-\nsumption may be very relevant to extractive, but not to ab-\nstractive summarization, where different terminology and\nparaphrasing can be used to express the same meaning (Co-\nhan and Goharian, 2016). This results in the metric assign-\ning low scores to any summary not matching the gold stan-\ndard on the surface level. This also allows cheating the\nmetric by generating ungrammatical and nonsensical sum-\nGold summary: researchers are developing a computer that can write weather forecasts . it takes meteorological data and writes a\nreport designed to mimic a human . this process is known as ‘ natural language generation ’ - lrb - nlg - rrb - . a prototype system will\nbe tested on the bbc website later this year .\nTransformer: researchers from london and edinburgh have developed a computer that can collateological information . these com-\nputer - generated weather updates are being tested by scientists at heriot - watt university and university college london . if the project\nis successful , a prototype system will be tested by generating local weather reports on the bbc ’ s website . currently , the bbc website\nfeatures 10 reports written by meteorologists .\nConvolutional Transformer: researchers from london and edinburgh have developed a computer that can collate meterological\ninformation and then produce forecasts as if they were written by a human . it uses a process known as ‘ natural language generation\n’ - lrb - nlg - rrb - . these computer - generated weather updates are being tested by scientists at heriot - watt university and university\ncollege london . if the project is successful , a prototype system will be tested by generating local weather reports on the bbc ’ s website\n.\nBERT-Transformer: researchers from london and edinburgh have developed a computer that can collate meteorological information\nand produce forecasts as if they were written by a human . using met ofﬁce data , it uses a process known as ‘ natural language\ngeneration ’ - lrb - nlg - rrb - . if the project is successful , a prototype system will be tested by generating local weather reports on the\nbbc ’ s website .\nFigure 5: Comparison of the output of models on an example form CNN/Daily Mail testset. Surface realisation mistakes\nare highlighted in green and a typical abstractive feature, illustrating re-arranging of the sentence is highlighted in blue.\nmaries having very high ROUGE scores. Sj ¨obergh (2007)\nshow how this can be achieved by choosing the most fre-\nquent bigrams from the input document.\nROUGE adoption relies on its correlation with human as-\nsessment. In the ﬁrst research on the DUC and TDT-3\ndatasets containing news articles, ROUGE indeed showed\na high correlation with the human judgments (Lin, 2004;\nDorr et al., 2005). However, more recent research ques-\ntions the suitability of ROUGE for various settings. Con-\nroy and Dang (2008) show that on DUC data the linguis-\ntic and responsiveness scores of some systems do not cor-\nrespond to the high ROUGE scores. Cohan and Gohar-\nian (2016) demonstrate that for summarization of scientiﬁc\ntexts, ROUGE-1 and ROUGE-L have very low correlations\nwith the gold summaries. ROUGE-N correlates better but\nis still far from the ideal case. This follows the result of\nMurray et al. (2005), showing that the unigram match be-\ntween the candidate summary and gold summary is not an\naccurate metric to assess quality.\nAnother problem is that the credibility of ROUGE was\ndemonstrated for the systems which operated in the low-\nscoring range. Peyrard (2019b) show that different summa-\nrization evaluation metrics correlate differently with human\njudgements for the higher-scoring range in which state-of-\nthe-art systems now operate. Furthermore, improvements\nmeasured with one metric do not necessarily lead to im-\nprovements when using others.\nThis concern led to the development of new evaluation met-\nrics. Peyrard (2019a) deﬁne metrics for important con-\ncepts with regard to summariazion: Redundancy, Rele-\nvance, and Informativeness in line with Shannon’s entropy.\nFrom these deﬁnitions they formulate a metric of Impor-\ntance which better correlates to human judgments. Clark et\nal. (2019) propose the metric of Sentence Mover’s Simi-\nlarity which operates on the semantic level and also better\ncorrelates with human evaluation. A summarization model\ntrained via Reinforcement Learning with this metric as re-\nward achieved higher scores in both human and ROUGE-\nbased evaluation.\nDespite these drawbacks, the broad adoption of ROUGE\nmakes it the only way to compare the efﬁciency of our\nmodel with other state-of-the-art models. The evaluation of\nour system on the SwissData dataset conﬁrms that its efﬁ-\nciency (in terms of ROUGE) is not restricted to CNN/Daily\nMail data only.\n8. Conclusion\nWe present a new abstractive text summarization model\nwhich incorporates convolutional self-attention in BERT.\nWe compare the performance of our system to a baseline\nand to competing systems on the CNN/Daily Mail data set\nfor English and report an improvement over state-of-the-\nart results using ROUGE scores. To establish suitability\nof our model to languages other than English and domains\nother than that of the CNN/Daily Mail data set, we apply\nour model to the German SwissText data set and present\nscores on this setup. A key contribution of our model is the\nability to deal with texts longer than BERT’s window size\nwhich is limited to 512 WordPiece tokens. We present a\ncascading approach and evaluate this on texts longer than\nthis window size, and demonstrate its performance when\ndealing with longer input texts.\nThe source code of our system is publicly available. 4 A\nfunctional service based on the model is currently being in-\ntegrated, as a summarization service, in the platforms Lynx\n(Moreno-Schneider et al., 2020), QURATOR (Rehm et al.,\n2020b) and European Language Grid (Rehm et al., 2020a).\nAcknowledgements\nThe work presented in this paper has received funding from\nthe European Union’s Horizon 2020 research and innova-\ntion programme under grant agreement no. 780602 (Lynx)\nand from the German Federal Ministry of Education and\nResearch (BMBF) through the project QURATOR (Wachs-\ntumskern no. 03WKDA1A).\n4https://github.com/axenov/BERT-Summ-OpenNMT\n9. Bibliographical References\nChen, Y .-C. and Bansal, M. (2018). Fast abstractive sum-\nmarization with reinforce-selected sentence rewriting. In\nProceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Pa-\npers), pages 675–686, Melbourne, Australia, July. Asso-\nciation for Computational Linguistics.\nClark, E., Celikyilmaz, A., and Smith, N. A. (2019).\nSentence mover’s similarity: Automatic evaluation for\nmulti-sentence texts. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguis-\ntics, pages 2748–2760, Florence, Italy, July. Association\nfor Computational Linguistics.\nCohan, A. and Goharian, N. (2016). Revisiting summa-\nrization evaluation for scientiﬁc articles. Available on-\nline (arXiv).\nCohan, A., Dernoncourt, F., Kim, D. S., Bui, T., Kim, S.,\nChang, W., and Goharian, N. (2018). A discourse-aware\nattention model for abstractive summarization of long\ndocuments. In NAACL-HLT.\nConroy, J. M. and Dang, H. T. (2008). Mind the gap: Dan-\ngers of divorcing evaluations of summary content from\nlinguistic quality. In Proceedings of the 22nd Interna-\ntional Conference on Computational Linguistics (Coling\n2008), pages 145–152, Manchester, UK, August. Coling\n2008 Organizing Committee.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 4171–4186, Minneapolis, Minnesota, June.\nAssociation for Computational Linguistics.\nDomhan, T. (2018). How much attention do you need?\na granular analysis of neural machine translation archi-\ntectures. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pages 1799–1808, Melbourne, Aus-\ntralia, July. Association for Computational Linguistics.\nDorr, B., Monz, C., President, S., Schwartz, R., and Za-\njic, D. (2005). A methodology for extrinsic evaluation\nof text summarization: Does ROUGE correlate? In Pro-\nceedings of the ACL Workshop on Intrinsic and Extrin-\nsic Evaluation Measures for Machine Translation and/or\nSummarization, pages 1–8, Ann Arbor, Michigan, June.\nAssociation for Computational Linguistics.\nGehrmann, S., Deng, Y ., and Rush, A. (2018). Bottom-up\nabstractive summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language\nProcessing, pages 4098–4109.\nHermann, K. M., Ko ˇcisk´y, T., Grefenstette, E., Espeholt,\nL., Kay, W., Suleyman, M., and Blunsom, P. (2015).\nTeaching machines to read and comprehend. In Pro-\nceedings of the 28th International Conference on Neural\nInformation Processing Systems - Volume 1, NIPS’15,\npages 1693–1701, Cambridge, MA, USA. MIT Press.\nKingma, D. and Ba, J. (2014). Adam: A method for\nstochastic optimization. International Conference on\nLearning Representations, 12.\nLi, C., Xu, W., Li, S., and Gao, S. (2018). Guiding gen-\neration for abstractive text summarization based on key\ninformation guide network. In Proceedings of the 2018\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pages 55–\n60, New Orleans, Louisiana, June. Association for Com-\nputational Linguistics.\nLin, C.-Y . (2004). ROUGE: A package for automatic eval-\nuation of summaries. In Text Summarization Branches\nOut, pages 74–81, Barcelona, Spain, July. Association\nfor Computational Linguistics.\nLiu, Y . and Lapata, M. (2019). Text summarization with\npretrained encoders. Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP).\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi,\nR., Kaiser, L., and Shazeer, N. (2018). Generating\nwikipedia by summarizing long sequences. In Interna-\ntional Conference on Learning Representations.\nLiu, Y . (2019). Fine-tune bert for extractive summariza-\ntion. Available online (arXiv).\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. (2013). Distributed representations of words\nand phrases and their compositionality. In C. J. C.\nBurges, et al., editors, Advances in Neural Information\nProcessing Systems 26, pages 3111–3119. Curran Asso-\nciates, Inc.\nMoreno-Schneider, J., Rehm, G., Montiel-Ponsoda, E.,\nRodriguez-Doncel, V ., Revenko, A., Karampatakis, S.,\nKhvalchik, M., Sageder, C., Gracia, J., and Maganza, F.\n(2020). Orchestrating NLP Services for the Legal Do-\nmain. In Nicoletta Calzolari, et al., editors, Proceedings\nof the 12th Language Resources and Evaluation Con-\nference (LREC 2020), Marseille, France, 5. European\nLanguage Resources Association (ELRA). Accepted for\npublication. Submitted version available as preprint.\nMurray, G., Renals, S., and Carletta, J. (2005). Extrac-\ntive summarization of meeting recordings. In INTER-\nSPEECH 2005 - Eurospeech, 9th European Conference\non Speech Communication and Technology, Lisbon, Por-\ntugal, September 4-8, 2005, pages 593–596.\nNallapati, R., Zhou, B., dos Santos, C., G ˙ulc ¸ehre, C ¸ ., and\nXiang, B. (2016). Abstractive text summarization using\nsequence-to-sequence RNNs and beyond. In Proceed-\nings of The 20th SIGNLL Conference on Computational\nNatural Language Learning, pages 280–290, Berlin,\nGermany, August. Association for Computational Lin-\nguistics.\nPaulus, R., Xiong, C., and Socher, R. (2018). A deep rein-\nforced model for abstractive summarization. In Interna-\ntional Conference on Learning Representations.\nPennington, J., Socher, R., and Manning, C. D. (2014).\nGlove: Global vectors for word representation. In\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1532–1543.\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. (2018). Deep contextu-\nalized word representations. In Proceedings of the 2018\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237, New Orleans, Louisiana, June. Association for\nComputational Linguistics.\nPeyrard, M. (2019a). A simple theoretical model of impor-\ntance for summarization. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational Lin-\nguistics, pages 1059–1073, Florence, Italy, July. Associ-\nation for Computational Linguistics.\nPeyrard, M. (2019b). Studying summarization evaluation\nmetrics in the appropriate scoring range. In Proceedings\nof the 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5093–5100, Florence, Italy,\nJuly. Association for Computational Linguistics.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. (2018). Language models are unsupervised\nmultitask learners. Available online.\nRaganato, A. and Tiedemann, J. (2018). An analysis of\nencoder representations in transformer-based machine\ntranslation. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 287–297, Brussels, Belgium,\nNovember. Association for Computational Linguistics.\nRehm, G., Berger, M., Elsholz, E., Hegele, S., Kintzel, F.,\nMarheinecke, K., Piperidis, S., Deligiannis, M., Gala-\nnis, D., Gkirtzou, K., Labropoulou, P., Bontcheva, K.,\nJones, D., Roberts, I., Hajic, J., Hamrlov ´a, J., Ka ˇcena,\nL., Choukri, K., Arranz, V ., Vasil ¸jevs, A., Anvari, O.,\nLagzdin ¸ˇs, A., Mel ¸n ¸ika, J., Backfried, G., Dikici, E.,\nJanosik, M., Prinz, K., Prinz, C., Stampler, S., Thomas-\nAniola, D., P´erez, J. M. G., Silva, A. G., Berr´ıo, C., Ger-\nmann, U., Renals, S., and Klejch, O. (2020a). European\nLanguage Grid: An Overview. In Nicoletta Calzolari,\net al., editors, Proceedings of the 12th Language Re-\nsources and Evaluation Conference (LREC 2020), Mar-\nseille, France, 5. European Language Resources Associ-\nation (ELRA). Accepted for publication.\nRehm, G., Bourgonje, P., Hegele, S., Kintzel, F., Schneider,\nJ. M., Ostendorff, M., Zaczynska, K., Berger, A., Grill,\nS., R¨auchle, S., Rauenbusch, J., Rutenburg, L., Schmidt,\nA., Wild, M., Hoffmann, H., Fink, J., Schulz, S., Seva, J.,\nQuantz, J., B¨ottger, J., Matthey, J., Fricke, R., Thomsen,\nJ., Paschke, A., Qundus, J. A., Hoppe, T., Karam, N.,\nWeichhardt, F., Fillies, C., Neudecker, C., Gerber, M.,\nLabusch, K., Rezanezhad, V ., Schaefer, R., Zellh¨ofer, D.,\nSiewert, D., Bunk, P., Pintscher, L., Aleynikova, E., and\nHeine, F. (2020b). QURATOR: Innovative Technolo-\ngies for Content and Data Curation. In Adrian Paschke,\net al., editors, Proceedings of QURATOR 2020 – The\nconference for intelligent content solutions, Berin, Ger-\nmany, 02. CEUR Workshop Proceedings, V olume 2535.\n20/21 January 2020.\nSee, A., Liu, P. J., and Manning, C. D. (2017). Get to the\npoint: Summarization with pointer-generator networks.\nProceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Pa-\npers).\nShi, T., Keneshloo, Y ., Ramakrishnan, N., and Reddy,\nC. K. (2018). Neural abstractive text summarization\nwith sequence-to-sequence models. Available online\n(arXiv).\nSj¨obergh, J. (2007). Older versions of the rougeeval sum-\nmarization evaluation system were easier to fool. Infor-\nmation Processing & Management, 43(6):1500 – 1505.\nText Summarization.\nSubramanian, S., Li, R., Pilault, J., and Pal, C. (2019). On\nextractive and abstractive neural document summariza-\ntion with transformer language models. Available online\n(arXiv).\nTenney, I., Das, D., and Pavlick, E. (2019). Bert rediscov-\ners the classical nlp pipeline. Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017).\nAttention is all you need. In Proceedings of the 31st In-\nternational Conference on Neural Information Process-\ning Systems, NIPS’17, pages 6000–6010, USA. Curran\nAssociates Inc.\nWu, F., Fan, A., Baevski, A., Dauphin, Y ., and Auli, M.\n(2019). Pay less attention with lightweight and dynamic\nconvolutions. In International Conference on Learning\nRepresentations.\nYang, B., Wang, L., Wong, D. F., Chao, L. S., and Tu,\nZ. (2019a). Convolutional self-attention networks. Pro-\nceedings of the 2019 Conference of the North.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,\nR. R., and Le, Q. V . (2019b). Xlnet: Generalized au-\ntoregressive pretraining for language understanding. In\nH. Wallach, et al., editors, Advances in Neural Informa-\ntion Processing Systems 32, pages 5754–5764. Curran\nAssociates, Inc.\nZhang, H., Cai, J., Xu, J., and Wang, J. (2019).\nPretraining-based natural language generation for text\nsummarization. In Proceedings of the 23rd Conference\non Computational Natural Language Learning (CoNLL),\npages 789–797, Hong Kong, China, November. Associ-\nation for Computational Linguistics.\nZHAW. (2019). German text summarization challenge.\nSwiss Text Analytics Conference. Available online.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8418912887573242
    },
    {
      "name": "Automatic summarization",
      "score": 0.7908899784088135
    },
    {
      "name": "Transformer",
      "score": 0.7831432223320007
    },
    {
      "name": "Locality",
      "score": 0.7107259631156921
    },
    {
      "name": "Language model",
      "score": 0.7065221667289734
    },
    {
      "name": "Encoder",
      "score": 0.6663063764572144
    },
    {
      "name": "Natural language processing",
      "score": 0.620650589466095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6154881119728088
    },
    {
      "name": "Locality of reference",
      "score": 0.4823433756828308
    },
    {
      "name": "Baseline (sea)",
      "score": 0.43970975279808044
    },
    {
      "name": "Linguistics",
      "score": 0.12412720918655396
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Cache",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I33256026",
      "name": "German Research Centre for Artificial Intelligence",
      "country": "DE"
    }
  ]
}