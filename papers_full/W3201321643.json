{
  "title": "Transformer-based Language Models for Factoid Question Answering at\\n BioASQ9b",
  "url": "https://openalex.org/W3201321643",
  "year": 2021,
  "authors": [
    {
      "id": null,
      "name": "Khanna, Urvashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743754961",
      "name": "Molla, Diego",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963488798",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2997090102",
    "https://openalex.org/W2626154462",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3003584790",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W3038495045",
    "https://openalex.org/W3013838212",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3013653849",
    "https://openalex.org/W2953958347"
  ],
  "abstract": "In this work, we describe our experiments and participating systems in the\\nBioASQ Task 9b Phase B challenge of biomedical question answering. We have\\nfocused on finding the ideal answers and investigated multi-task fine-tuning\\nand gradual unfreezing techniques on transformer-based language models. For\\nfactoid questions, our ALBERT-based systems ranked first in test batch 1 and\\nfourth in test batch 2. Our DistilBERT systems outperformed the ALBERT variants\\nin test batches 4 and 5 despite having 81% fewer parameters than ALBERT.\\nHowever, we observed that gradual unfreezing had no significant impact on the\\nmodel's accuracy compared to standard fine-tuning.\\n",
  "full_text": "Transformer-based Language Models for Factoid\nQuestion Answering at BioASQ9b\nUrvashi Khanna, Diego Moll√°\nMacquarie University, Australia\nAbstract\nIn this work, we describe our experiments and participating systems in the BioASQ Task\n9b Phase B challenge of biomedical question answering. We have focused on finding the\nideal answers and investigated multi-task fine-tuning and gradual unfreezing techniques on\ntransformer-based language models. For factoid questions, our ALBERT-based systems ranked\nfirst in test batch 1 and fourth in test batch 2. Our DistilBERT systems outperformed the\nALBERT variants in test batches 4 and 5 despite having 81% fewer parameters than ALBERT.\nHowever, we observed that gradual unfreezing had no significant impact on the model‚Äôs accuracy\ncompared to standard fine-tuning.\nKeywords\nTransfer learning, DistilBERT, ALBERT, Question Answering, BioASQ9b\n1. Introduction\nNowadays, the use of language models that have been pretrained on massive amounts\nof data are the norm [1, 2, 3]. Rather than making significant task-specific architecture\nimprovements, these pretrained models can be fine-tuned for various tasks by making\nminor changes to the language model architecture, such as adding an output layer on\ntop. Fine-tuning approaches are critical for learning the distributions of the target\ntask and improving the language model‚Äôs adaptability. However, fine-tuning a language\nmodel on small datasets like BioASQ can lead to catastrophic forgetting and overfitting.\nFurthermore, training all layers simultaneously on data of different target tasks may\nresult in poor performance and an unstable model [4]. A schedule for updating the\npretrained weights may be critical for preventing catastrophic forgetting of the source\ntask‚Äôs knowledge. Scheduling techniques like chain thaw [5] and gradual unfreezing [6]\nhave improved the performance of multiple Natural Language Processing (NLP) tasks.\nGradual unfreezing involves gradually fine-tuning model layers rather than fine-tuning\nall layers at once.\nPretrained language models are usually trained on general language and then adapted\nto downstream tasks of varied domains. Many domain-specific tasks, however, face\nCLEF 2021 ‚Äì Conference and Labs of the Evaluation Forum, September 21‚Äì24, 2021, Bucharest,\nRomania\n/envelope-openUrvashi.Khanna@mq.edu.au (U. Khanna); Diego.Molla-Aliod@mq.edu.au (D. Moll√°)\n/globehttps://researchers.mq.edu.au/en/persons/diego-molla-aliod (D. Moll√°)\n/orcid0000-0003-2345-5596 (U. Khanna); 0000-0003-4973-0963 (D. Moll√°)\n¬© 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution\n4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedings\nhttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\narXiv:2109.07185v1  [cs.CL]  15 Sep 2021\nthe problem of the scarcity of labelled datasets. Auxiliary signal through multi-task\nfine-tuning helps the language model to adapt on smaller datasets better [7, 8, 9]. Multi-\ntask fine-tuning (also referred to as sequential adaptation in some literature [4]) is the\nintermediate fine-tuning stage in which the model is fine-tuned on a larger dataset before\nfine-tuning on a low-resource dataset. In this paper, we describe the experiments of\nour participating systems1 at the BioASQ9b challenge2. We discuss two of our systems,\nmainly focusing on factoid questions. Both systems adapt the multi-task fine-tuning\ntechnique of fine-tuning on a larger dataset before fine-tuning on the BioASQ9b dataset.\nOur first system fine-tunes the pre-trained model ALBERT on SQuAD2.0 and then\non the BioASQ9b dataset. This system performed exceedingly well on BioASQ9b Test\nbatches 1 and 2. Our second system investigates the effect of the gradual unfreezing\ntechnique on the smaller, compact transformer-based model, DistilBERT. We assess this\nsystem via two of our submissions at the BioASQ9b Challenge. One of our submissions of\nDistilBERT ranked sixth in the BioASQ9b leaderboard3. From our results, we conclude\nthat gradually unfreezing DistilBERT had no significant improvement in the accuracy of\nthe BioASQ9b test data in comparison to standard fine-tuning.\nThe rest of this paper is structured as follows. In Section 2, we briefly discuss related\nwork for background. Section 3 describes the BioASQ dataset and the processing steps\ninvolved. Section 4 details our experimental setup for both our systems. Section 5\ndiscusses the results of our systems on the BioASQ public leaderboard. Finally, Section 6\nprovides a conclusion to our work.\n2. Related Work\nTransfer learning has been widely used to transfer knowledge across multiple domains.\nThe scarcity of sizable domain-specific datasets and the cost associated with manually\nannotating them are driving this trend. In this section, we discuss previous works that\nused transfer learning for the BioASQ biomedical question answering task [10].\nIn the 5th BioASQ challenge, Wiese et al. [11] explored domain adaptation to transfer\nknowledge from an already existing neural Question Answering (QA) system named\nFastQA [12] that was trained on SQuAD [13]. They initialised their model with the\npretrained FastQA models‚Äô parameters during the fine-tuning phase. Using a combination\nof fine-tuning and biomedical Word2vec embeddings, their model achieved state-of-the-art\nresults. They also used optimisation approaches such as L2 weight regularisation and\nforgetting cost term to minimise catastrophic forgetting.\nLee et al. [14] discovered the potential to adapt the general domain language model\nBERT for the biomedical domain. They presented BioBERT, the first biomedical language\nmodel. In the pretraining step, BioBERT was initialised with BERT weights and then\npretrained on biomedical domain corpora. BioBERT produced benchmark results on\na wide range of biomedical text mining tasks, including question answering, relation\n1Code associated with this paper is available at https://github.com/urvashikhanna/bioasq9b\n2http://bioasq.org/\n3http://participants-area.bioasq.org/results/9b/phaseB/\nextraction, and named entity recognition. Yoon et al.‚Äôs [15] submission for task 7b\ntopped the leaderboard in the 7th BioASQ challenge. They used a sequential adaptation\ntechnique in which pretrained BioBERT was fine-tuned first on the SQuAD dataset and\nthen on the BioASQ dataset.\nSimilarly, BioELMo [16] is a biomedical version of ELMo that outperforms BioBERT\non the authors‚Äô probing tasks when used as a feature extractor. However, the fine-tuned\nBioBERT outperforms BioELMo on named entity recognition and Natural Language\nInference (NLI) tasks.\nHosein et al. [17] studied domain portability and error propagation of BERT-based\nQA models through their BioASQ7b submissions. Their results concluded that general\ndomain language models could generalise and give good results for domain-specific tasks.\nThey also observed that pretraining is more critical than fine-tuning when improving\nthe domain portability of BERT QA models. For yes/no questions in the BioASQ7\nPhase B challenge, Resta et al. [18] used an ensemble of classifiers with input from\nvarious transformer-based language models. They employed contextual embeddings from\nmultiple pretrained language models, such as BERT and ELMO, as features to capture\nlong-term dependencies.\nJeong et al. [9] expanded the prior work on BioBERT models [14, 15] in the 8th\nBioASQ challenge. They adapted multiple stages of fine-tuning by first fine-tuning\nBioBERT on the NLI dataset [19], then on the SQuAD dataset [13], and finally on\nthe downstream BioASQ dataset. Their results established that tasks like NLI that\ncapture the relationships between sentence pairs improve the accuracy of the QA systems.\nAdditionally, they analysed and reported the number of unanswerable questions from the\nBioASQ7b dataset in the QA setting. Kazaryan et al. [20] used ALBERT [2] as their\nbase language model which was fine-tuned first on SQuAD v2.0 [21], and subsequently\non the BioASQ8b data.\n3. BioASQ Data Processing\nBioASQ [10] is an international biomedical challenge that comprises annual tasks on\nsemantic indexing and biomedical question answering. The ninth BioASQ challenge\nconsists of two shared tasks. Task 9a is a semantic indexing task that aims to annotate\nnew PubMed articles automatically [22] with Medical Subject Headings (MeSH). Task\n9b is a question answering task devised for systems to answer four types of biomedical\nquestions: factoid, summary, list, and yes/no. The participants are provided with\nquestions along with relevant snippets. The output generated by their systems is either\nan exact answer (for yes/no, factoid, and list questions) or ideal answers (for summary\nquestions), or both. The tasks are released in five batches over two months, with 24\nhours to submit the answers after the release of each test batch.\nWe primarily concentrate on factoid questions from the BioASQ9b dataset. The dataset\ncontains a total of 3743 questions, 1092 of which are factoid questions. An example of a\nfactoid question is shown in Figure 1. Our system returns exact answers for factoid-type\nquestions that can either be a single entity or a list of entities. We regard the BioASQ\nFigure 1: Sample factoid question [23]. The answer to the question is inbold and is extracted from\nsnippet 2.\nchallenge task as an extractive QA task because the answer to the query is extracted\nfrom the relevant snippet. The metrics used for evaluating the systems on the BioASQ\nleaderboard are: Strict Accuracy (SAcc), Lenient Accuracy (LAcc), and Mean Reciprocal\nRank (MRR). However, MRR is the official metric used by the BioASQ organisers for\nfactoid questions since it is often used to evaluate other factoid QA tasks and challenges\n[10].\nThe BioASQ dataset is transformed into the SQuAD format and vice versa using\npre-processing and post-processing steps. In a typical span-extractive question answering\ntask, the system is provided with a passage P and a question Q, and it must identify an\nanswer span A (ùëéùë†ùë°ùëéùëüùë°, ùëéùëíùëõùëë) in P. The SQuAD dataset is an example of a span prediction\nQA task containing many question-answer pairs and a passage that answers the given\nquestion. In contrast, the training dataset of BioASQ includes a question, an answer, and\nmultiple relevant snippets. Therefore, we begin by pairing each snippet with its question\nand transforming it into multiple question-snippet pairs. Also, based on the exact answer\nprovided, we locate the answer‚Äôs position in the snippet and populate it as the start\nposition of the answer span in the dataset. After performing these pre-processing steps,\nthe BioASQ9b training data samples increased five-fold from 1092 to 5447. Table 1 shows\nthe number of questions in the training and test batches before and after pre-processing.\nTable 1\nSummary of BioASQ9b Training and Test data before and after pre-processing.\nDataset Number of Factoid Questions\nBefore Pre-processing\nNumber of Factoid Questions\nAfter Pre-processing\nTraining 1092 5447\nBatch 1 29 139\nBatch 2 34 151\nBatch 4 28 132\nBatch 5 36 148\nOur system returns the prediction span for each question. Because we divided the\nsnippets into several question-snippet pairs during the pre-processing stage, we now have\npredictions of multiple answer spans and their probabilities for each question. Each\nsystem must submit a list of up to five responses for the official BioASQ evaluation. As a\nresult, we select the top five answers for each question in decreasing order of probability\nas our submission. Thus, for each factoid question, our system returns a list of up to five\nresponses sorted by their likelihood.\n4. Systems Overview\nThis section describes our systems and the experimental setup of our submissions at\nthe BioASQ9b challenge. Our submissions in the BioASQ9b challenge are based on two\npretrained models: ‚ÄúDistilBERT‚Äù and ‚ÄúALBERT‚Äù. As mentioned above, we focus mainly\non factoid questions. We submitted ALBERT variants for all the BioASQ9b test batches\nexcept test batch 3. DistilBERT-based systems were submitted in test batches 2, 4, and\n5. In this section, we detail the models, the methodology used, and the experimental\nsetup.\n4.1. ALBERT\nFor the system using ALBERT, we follow a staged fine-tuning approach by fine-tuning\non a large dataset before fine-tuning on the smaller dataset. This preliminary stage of\nfine-tuning on a large QA task is ideal due to the small size of the BioASQ dataset.\nHowever, large-scale bio-medical QA datasets are not readily available that could be used\nfor the first stage of fine-tuning. Therefore, we use the SQuAD dataset, a widely used\nextractive QA dataset. Thus, we first fine-tune ALBERT on SQuAD2.0 and later on our\ndownstream BioASQ task. This approach is illustrated in Figure 2.\nFigure 2: Diagram depicting our system‚Äôs fine-tuning strategy.\nALBERT is a lighter version of BERT with considerably fewer parameters. Lan et al.\n[2] used two parameter-reduction strategies to lower the memory usage and increase the\ntraining speed of BERT. Since ALBERT models scale better than BERT, we have used\nthe xxlarge version of ALBERT for our experiments. The BioASQ task was set up as a\nspan-extraction QA task in which the model predicts the start and end span of answers\nfor a given context and question. In both stages of fine-tuning, the input to the model is\nthe concatenation of passage and question with a special token [SEP] separating them.\nThis input is tokenized using WordPiece embeddings [24] to handle the out-of-vocabulary\nissues. After WordPiece tokenization, the maximum allowable input sequence length is\n512 for both the ALBERT and DistilBERT models. The input has three embeddings:\ntoken, position, and sentence. In order to differentiate between the sentences, sentence\nembedding is appended to each sentence, and a special position token is added to identify\nthe position of each token. The model returns the start and end scores for each word.\nThe output of the model is the candidate span with the highest score and where the end\nposition is greater than or equal to the start position.\nWe employed ‚ÄúALBERT-xxlarge‚Äù version 2 as our pretrained language model along\nwith its tokenizer, which are publicly available from the Huggingface Transformers Library\n[25]. This model has an additional task-specific linear question answering layer on top\nto output the start and end spans. Unless otherwise specified, the hyperparameters for\nboth fine-tuning stages were set to the default values used by the ALBERT developers.\nThe systems were validated on the BioASQ7b test batches 1 and 2.\nAll the three ALBERT-based submissions use the same fine-tuning approach discussed\nabove with slight changes to the fine-tuning hyper-parameters. The systems along with\nhyperparameters are listed in Table 2 and their results are listed in Table 3.\nTable 2\nALBERT-based systems along with the hyperparameters.\nSystem Name Learning Rate Batch Size Sequence Length Epochs\nALBERT 1 3e-5 4 512 3\nALBERT 2 2e-5 4 512 4\nALBERT 3 1e-5 4 512 3\n4.2. Gradual Unfreezing DistilBERT\nIn recent years, the pretrained language models are getting bigger and deeper with\nmillions, sometimes billions of parameters [2, 26]. The success of these models on NLP\ntasks has fueled the race to scale up the models further. However, deploying these massive\nmodels on mobile and edge devices has implications such as environmental impact and\ncomputational cost [27], making them unsuitable for use in real-world applications. Sanh\net al. [28] applied knowledge distillation [29] and proposed a smaller language model,\nDistilBERT, that achieves performance comparable to BERT on various NLP tasks.\nDistilBERT, a distilled, compact version of BERT, has 60% fewer parameters than\nBERT.\nThe focus for our second system was to study the effect of gradual unfreezing on the\ntransformer-based language models. We used DistilBERT as our pretrained model to\nconduct the experiments of gradually unfreezing the transformer layers. The reason for\nthis choice was the small size of DistilBERT and its ability to achieve close to 95% of all\nthe NLP task benchmarks when compared to BERT.\nThe process of fine-tuning allows the model to learn the distribution of the downstream\ntask. In standard fine-tuning, all the layers of the model are trained on the target\ntask simultaneously. Howard et al. [6] introduced a fine-tuning approach of gradually\nunfreezing one layer at a time, starting from the top layer. They used a standard\nLong Short-Term Memory (LSTM) network without any attention mechanism for their\nexperiments. Our work investigates the gradual unfreezing approach on DistilBERT\nusing BioASQ9b as our target dataset.\nFigure 3: Diagram showing our unfreezing approach.\nDistilBERT has three blocks of layers: one embedding layer, six transformer layers, and\na top task-specific layer. In our approach shown in Figure 3, we begin by fine-tuning only\nthe top task-specific layer for one epoch while keeping all other layers frozen. Then we\nunfreeze the transformer layers consecutively in groups of three, fine-tune all the unfrozen\nlayers for one epoch, and repeat until all layers are fine-tuned except the embedding layer.\nThe decision to keep the embedding layer always frozen was based on the preliminary\nexperiments in our previous work [23]. As a result, DistilBERT‚Äôs trainable parameters\nhave been reduced from 65 million to 42 million.\nIn this system, ‚Äúdistilbert-base-cased‚Äù [25] is first fine-tuned on SQuAD1.1 data and\nthen on BioASQ9b task. Our gradual unfreezing approach is only applied during the\nsecond stage of fine-tuning. In the second phase of fine-tuning, we fine-tune the model\nat a constant learning rate of 3e-5, sequence length of 512, and for three epochs. We\nevaluate the unfreezing approach through two submissions at the BiOASQ challenge. The\nsystem ‚ÄúDistilBERT‚Äù is our baseline system. In this system, all the layers of DistilBERT\nare fine-tuned simultaneously. The system ‚ÄúUnfreezing DistilBERT‚Äù is the model that\nwas fine-tuned using our unfreezing approach. Both systems are fine-tuned with the same\nhyperparameters for a fair comparison. Table 3 lists our systems with the results, along\nwith the top-ranked system in the BioaASQ9b leaderboard. We have reported the MRR\nin the results table since it is the main metric used by the BioASQ organisers.\n5. Results\nThe results of our submissions to the BioASQ9b Phase B challenge are shown in Table 3.\nFrom the results, we observe that ‚ÄúALBERT 2‚Äù system was the best system for batch\n1, and the ‚ÄúALBERT 3‚Äù system was ranked fourth on the public leaderboard of the\nBioASQ9b challenge. Overall, the systems using the pretrained ALBERT weights have\nperformed exceedingly well on test batches 1 and 2. However, our ALBERT variants\nreceived poor results for test batches 4 and 5. It is worth noting that all the systems\nwill be evaluated by humans experts after the competition. However, because this data\nwas not accessible at the time of writing this study, we rely on automatic evaluations\navailable on the BioASQ leaderboard.\nTable 3\nResults of our five submissions along with the top-ranked system from the BioASQ9b leaderboard.\nThe first column of the table lists the unique submission identifier along with the system names as\ndisplayed on the public leaderboard. The highest score for each batch is inbold.\nSubmission ( Display name) System Factoid - Mean Reciprocal Rank (MRR)\nBatch 1 Batch 2 Batch 4 Batch 5\nMQ TL1 (ALBERT) ALBERT 1 0.4379 0.4667 0.369 0.4468\nMQ TL2 (Ensemble) ALBERT 2 0.4632 0.501 0.4167 0.4731\nMQ TL-3 (Another ALBERT) ALBERT 3 0.4621 0.5319 0.4375 0.4778\nMQ TL4 (Final BERT) DistilBERT - 0.5059 0.5399 0.5171\nMQ Transfer Learning (MRes) Unfreezing DistilBERT - 0.4887 0.5893 0.4917\nTop Ranked System - 0.4632 0.5539 0.6929 0.588\nThe most noticeable difference between our DistilBERT and ALBERT variants, apart\nfrom their sizes, is the initial fine-tuning stage. In our systems, ALBERT was fine-tuned\non SQuAD2.0, whereas DistilBERT was fine-tuned on SQuAD1.1. The SQuAD2.0 dataset\nis a reading comprehension dataset that, in addition to the SQuAD1.1 dataset, contains\napproximately 50,000 unanswerable questions. We need to look into whether test batches\n1 and 2 had more unanswered questions after the organisers release the golden answers,\nand if so, how it has affected the results.\nFrom the results of Table 3, we observe that both ‚ÄúDistilBERT‚Äù and ‚ÄúUnfreezing\nDistilBERT‚Äù outperformed the ALBERT variants for the test batches 4 and 5. Our\nsystem ‚ÄúUnfreezing DistilBERT‚Äù is ranked sixth in the BioASQ9b public leaderboard. The\naverage MRR score of test batches 2, 4 and 5 for systems ‚ÄúDistilBERT‚Äù and ‚ÄúUnfreezing\nDistilBERT‚Äù is 0.5209 and 0.5232 respectively, and the difference is not statistically\nsignificant4. Thus, we can conclude that gradually unfreezing the transformer-based\n4Paired t-tests were used to compute the statistical significance since the MRR can be considered as\na normal distribution as it is an average of samples. We find no statistically significant difference between\nthe gradually unfrozen model and the baseline.\nTable 4\nResults from our previous work [23] on the BioASQ7b dataset. The system ‚ÄòKU DMIS Team‚Äô [30, 15]\nis BioBERT based system that was top of the leaderboard in the BioASQ7b challenge.\nSystems Mean Reciprocal Rank\nKU-DMIS Team [30, 15] 0.5235\nDistilBERT-fine-tuned 0.4844\nDistilBERT-unfreeze-3 0.4841\nmodels has no significant impact on the model‚Äôs accuracy compared to typical fine-\ntuning. These results further support the findings of our previous work [23] on gradually\nunfreezing DistilBERT with the BioASQ7b dataset, the results of which are shown in\nTable 4. The results show that gradually unfrozen models produce promising results for\na few test batches, but have no overall significant impact across all the test batches.\n6. Conclusion\nOur participation in BioASQ9b was primarily focused on generating the ideal answers\nfor factoid questions. We participated in four test batches, with our systems employing\npretrained ALBERT and DistilBERT language models. The results were mixed, with\nALBERT-based systems ranking amongst the top systems for test batches 1 and 2. For test\nbatch 4, the compact DistilBERT variants, although having 81 percent fewer parameters,\nscored considerably better than ALBERT. This paves the way for a biomedical version\nof DistilBERT for mobile and edge devices for real life biomedical QA applications. In\naddition, we investigated the effect of gradual unfreezing on transformer-based language\nmodels using the BioASQ9b dataset. We conclude that gradually unfreezing the layers of\nDistilBERT had no significant impact on the model‚Äôs accuracy in comparison to standard\nfine-tuning. We also investigated an unfreezing approach that makes use of only 66%\nof DistilBERT‚Äôs parameters when fine-tuning. In the future, we will aim to investigate\nensemble or hybrid models of DistilBERT and ALBERT.\nReferences\n[1] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidi-\nrectional transformers for language understanding, in: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: HumanLanguageTechnologies, Volume1(LongandShortPapers), Asso-\nciation for Computational Linguistics, Minneapolis, Minnesota, 2019, pp. 4171‚Äì4186.\nURL: https://www.aclweb.org/anthology/N19-1423. doi:10.18653/v1/N19-1423.\n[2] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, Albert: A\nlite bert for self-supervised learning of language representations, arXiv preprint\narXiv:1909.11942 (2019).\n[3] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettle-\nmoyer, V. Stoyanov, Roberta: A robustly optimized bert pretraining approach,\narXiv preprint arXiv:1907.11692 (2019).\n[4] S. Ruder, M. E. Peters, S. Swayamdipta, T. Wolf, Transfer learning in natural\nlanguage processing, in: Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Tutorials, 2019, pp.\n15‚Äì18.\n[5] B. Felbo, A. Mislove, A. S√∏gaard, I. Rahwan, S. Lehmann, Using millions of emoji\noccurrences to learn any-domain representations for detecting sentiment, emotion\nand sarcasm, in: Proceedings of the 2017 Conference on Empirical Methods in Nat-\nural Language Processing, Association for Computational Linguistics, Copenhagen,\nDenmark, 2017, pp. 1615‚Äì1625. URL: https://www.aclweb.org/anthology/D17-1169.\ndoi:10.18653/v1/D17-1169.\n[6] J. Howard, S. Ruder, Universal language model fine-tuning for text classification,\nin: Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), Association for Computational Linguistics,\nMelbourne, Australia, 2018, pp. 328‚Äì339. URL: https://www.aclweb.org/anthology/\nP18-1031. doi:10.18653/v1/P18-1031.\n[7] C. Sun, X. Qiu, Y. Xu, X. Huang, How to fine-tune bert for text classification?, in:\nChina National Conference on Chinese Computational Linguistics, Springer, 2019,\npp. 194‚Äì206.\n[8] S. Garg, T. Vu, A. Moschitti, Tanda: Transfer and adapt pre-trained transformer\nmodels for answer sentence selection, in: Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, 2020, pp. 7780‚Äì7788.\n[9] J. Kang, Transferability of natural language inference to biomedical question\nanswering, arXiv preprint arXiv:2007.00217 (2020).\n[10] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, M. Zschunke, M. R. Alvers,\nD. Weissenborn, A. Krithara, S. Petridis, D. Polychronopoulos, et al., An overview\nof the bioasq large-scale biomedical semantic indexing and question answering com-\npetition, BMC bioinformatics 16 (2015) 1‚Äì28. doi:10.1186/s12859-015-0564-6 .\n[11] G. Wiese, D. Weissenborn, M. Neves, Neural domain adaptation for biomedical\nquestion answering, in: Proceedings of the 21st Conference on Computational Natu-\nral Language Learning (CoNLL 2017), Association for Computational Linguistics,\nVancouver, Canada, 2017, pp. 281‚Äì289. URL: https://www.aclweb.org/anthology/\nK17-1029. doi:10.18653/v1/K17-1029.\n[12] D. Weissenborn, G. Wiese, L. Seiffe, Making neural QA as simple as possible but not\nsimpler, in: Proceedings of the 21st Conference on Computational Natural Language\nLearning (CoNLL 2017), Association for Computational Linguistics, Vancouver,\nCanada, 2017, pp. 271‚Äì280. URL: https://www.aclweb.org/anthology/K17-1028.\ndoi:10.18653/v1/K17-1028.\n[13] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, SQuAD: 100,000+ questions for\nmachine comprehension of text, in: Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, Association for Computational Linguistics,\nAustin, Texas, 2016, pp. 2383‚Äì2392. URL: https://www.aclweb.org/anthology/\nD16-1264. doi:10.18653/v1/D16-1264.\n[14] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, J. Kang, Biobert: pre-trained\nbiomedical language representation model for biomedical text mining, arXiv preprint\narXiv:1901.08746 (2019).\n[15] W. Yoon, J. Lee, D. Kim, M. Jeong, J. Kang, Pre-trained language model for\nbiomedical question answering, arXiv preprint arXiv:1909.08229 (2019).\n[16] Q. Jin, B. Dhingra, W. Cohen, X. Lu, Probing biomedical embeddings from\nlanguage models, in: Proceedings of the 3rd Workshop on Evaluating Vector Space\nRepresentations for NLP, Association for Computational Linguistics, Minneapolis,\nUSA, 2019, pp. 82‚Äì89. URL: https://www.aclweb.org/anthology/W19-2011. doi:10.\n18653/v1/W19-2011.\n[17] S. Hosein, D. Andor, R. McDonal, Measuring domain portability and error propaga-\ntion in biomedical qa, arXiv preprint arXiv:1909.09704 (2019).\n[18] M. Resta, D. Arioli, A. Fagnani, G. Attardi, Transformer models for question\nanswering at bioasq 2019, in: Joint European Conference on Machine Learning and\nKnowledge Discovery in Databases, Springer, 2019, pp. 711‚Äì726.\n[19] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge corpus for sentence\nunderstanding through inference, in: Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), Association for Computational\nLinguistics, 2018, pp. 1112‚Äì1122. URL: http://aclweb.org/anthology/N18-1101.\n[20] A. Kazaryan, U. Sazanovich, V. Belyaev, Transformer-based open domain biomedical\nquestion answering at bioasq8 challenge (2020).\n[21] P. Rajpurkar, R. Jia, P. Liang, Know what you don‚Äôt know: Unanswerable questions\nfor SQuAD, in: Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), Association for Computational\nLinguistics, Melbourne, Australia, 2018, pp. 784‚Äì789. URL: https://www.aclweb.\norg/anthology/P18-2124. doi:10.18653/v1/P18-2124.\n[22] Pubmed, Pubmed¬Æ comprises more than 30 million citations for biomedical literature\nfrom medline, life science journals, and online books., 2020. URL: https://pubmed.\nncbi.nlm.nih.gov, [Online; accessed 1-December-2020].\n[23] U. Khanna, Gradual unfreezing transformer-based language models for biomedi-\ncal question answering, http://hdl.handle.net/1959.14/1280832, 2021. [Macquarie\nUniversity, Sydney, Australia Online; accessed 03-June-2021].\n[24] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao,\nQ. Gao, K. Macherey, et al., Google‚Äôs neural machine translation system: Bridging\nthe gap between human and machine translation, arXiv preprint arXiv:1609.08144\n(2016).\n[25] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault,\nR. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite,\nJ. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, A. M. Rush, Transformers:\nState-of-the-art natural language processing, in: Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing: System Demonstrations,\nAssociation for Computational Linguistics, Online, 2020, pp. 38‚Äì45. URL: https:\n//www.aclweb.org/anthology/2020.emnlp-demos.6.\n[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models\nare unsupervised multitask learners, OpenAI blog 1 (2019) 9.\n[27] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations for deep\nlearning in NLP, in: Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, Association for Computational Linguistics, Florence,\nItaly, 2019, pp. 3645‚Äì3650. URL: https://www.aclweb.org/anthology/P19-1355.\ndoi:10.18653/v1/P19-1355.\n[28] V. Sanh, L. Debut, J. Chaumond, T. Wolf, Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter, arXiv preprint arXiv:1910.01108 (2019).\n[29] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural network, arXiv\npreprint arXiv:1503.02531 (2015).\n[30] Tsatsaronis et al, Bioasq participants area task 7b: Test results of phase b, http://\nparticipants-area.bioasq.org/results/7b/phaseB/, 2019. [Online; accessed 17-January-\n2021].",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8145745396614075
    },
    {
      "name": "Transformer",
      "score": 0.7983937859535217
    },
    {
      "name": "Computer science",
      "score": 0.6984178423881531
    },
    {
      "name": "Natural language processing",
      "score": 0.6462984681129456
    },
    {
      "name": "Language model",
      "score": 0.5556363463401794
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5509089231491089
    },
    {
      "name": "Task (project management)",
      "score": 0.4839094579219818
    },
    {
      "name": "Test (biology)",
      "score": 0.44802790880203247
    },
    {
      "name": "Engineering",
      "score": 0.09865415096282959
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99043593",
      "name": "Macquarie University",
      "country": "AU"
    }
  ],
  "cited_by": 3
}