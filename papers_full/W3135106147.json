{
  "title": "Iterative SE(3)-Transformers",
  "url": "https://openalex.org/W3135106147",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2971788620",
      "name": "Fabian B. Fuchs",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2255113753",
      "name": "Edward Wagstaff",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2520319250",
      "name": "Justas Dauparas",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2019646861",
      "name": "Ingmar Posner",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2971788620",
      "name": "Fabian B. Fuchs",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2255113753",
      "name": "Edward Wagstaff",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2520319250",
      "name": "Justas Dauparas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2019646861",
      "name": "Ingmar Posner",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2944347827",
    "https://openalex.org/W2968494487",
    "https://openalex.org/W3136918052",
    "https://openalex.org/W2898210859",
    "https://openalex.org/W2999044305",
    "https://openalex.org/W2967606876",
    "https://openalex.org/W2997234557",
    "https://openalex.org/W2972411752",
    "https://openalex.org/W2971733377",
    "https://openalex.org/W2788775653",
    "https://openalex.org/W4310895557",
    "https://openalex.org/W569478347",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2907492528",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963057320",
    "https://openalex.org/W2798270772",
    "https://openalex.org/W6735377749",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963461379",
    "https://openalex.org/W2786615588",
    "https://openalex.org/W3100269082",
    "https://openalex.org/W3114474797",
    "https://openalex.org/W2963464736",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W2279221249",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2964213081",
    "https://openalex.org/W2793179281",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W3035684957"
  ],
  "abstract": null,
  "full_text": "Iterative SE(3)-Transformers\nFabian B. Fuchs†1, Edward Wagstaﬀ†1, Justas Dauparas2, and Ingmar Posner1\n1 Department of Engineering Science, University of Oxford, Oxford, UK\n2 Institute for Protein Design, University of Washington, WA, USA\n†These authors contributed equally\n{fabian,ed}@robots.ox.ac.uk\nAbstract. When manipulating three-dimensional data, it is possible to\nensure that rotational and translational symmetries are respected by ap-\nplying so-called SE(3)-equivariant models. Protein structure prediction\nis a prominent example of a task which displays these symmetries. Re-\ncent work in this area has successfully made use of an SE(3)-equivariant\nmodel, applying an iterative SE(3)-equivariant attention mechanism. Mo-\ntivated by this application, we implement an iterative version of the\nSE(3)-Transformer, an SE(3)-equivariant attention-based model for graph\ndata. We address the additional complications which arise when apply-\ning the SE(3)-Transformer in an iterative fashion, compare the iterative\nand single-pass versions on a toy problem, and consider why an iterative\nmodel may be beneﬁcial in some problem settings. We make the code for\nour implementation available to the community3.\nKeywords: Deep Learning · Equivariance · Graphs · Proteins\n1 Introduction\nTasks involving manipulation of three-dimensional (3D) data often exhibit rota-\ntional and translational symmetry, such that the overall orientation or position\nof the data is not relevant to solving the task. One prominent example of such\na task is protein structure reﬁnement [1]. The goal is to improve on the initial\n3D structure – the position and orientation of the structure, i.e. the frame of\nreference, is not important to the goal. We would like to ﬁnd a mapping from the\ninitial structure to the ﬁnal structure such that if the initial structure is rotated\nand translated then the predicted ﬁnal structure is rotated and translated in the\nsame way. This symmetry between input and output is known as equivariance.\nMore speciﬁcally, the group of translations and rotations in 3D is called the spe-\ncial Euclidean group and is denoted by SE(3). The relevant symmetry is known\nas SE(3) equivariance.\nIn the latest Community-Wide Experiment on the Critical Assessment of\nTechniques for Protein Structure Prediction (CASP14) structure-prediction chal-\nlenge, DeepMind’s AlphaFold 2 team [2] successfully applied machine learning\n3 https://github.com/FabianFuchsML/se3-transformer-public\narXiv:2102.13419v2  [cs.LG]  16 Mar 2021\n2 F. Fuchs and E. Wagstaﬀ et al.\ntechniques to win the categories “regular targets” and “interdomain prediction”\nby a wide margin. This is a important achievement as it opens up new routes\nfor understanding diseases and drug discovery in cases where protein structures\ncannot be experimentally determined [3]. At this time, the full implementation\ndetails of the AlphaFold 2 algorithm are not public. Consequently, there is a lot\nof interest in understanding and reimplementing AlphaFold 2 [4–8].\nOne of the core aspects that enabled AlphaFold 2 to produce very high\nquality structures is the end-to-end iterative reﬁnement of protein structures [2].\nConcretely, the inputs for the reﬁnement task are the estimated coordinates\nof the protein, and the outputs are updates to these coordinates. This task is\nequivariant: when rotating the input, the update vectors are rotated identically.\nTo leverage this symmetry, AlphaFold 2 uses an SE(3)-equivariant attention\nnetwork. The ﬁrst such SE(3)-equivariant attention network described in the\nliterature is the SE(3)-Transformer [9]. However, in the original paper, the SE(3)-\nTransformer is only described as a single-pass predictor, and its use in an iterative\nfashion is not considered.\nIn this paper, we present an implementation of an iterative version of the\nSE(3)-Transformer, with a discussion of the additional complications which arise\nin this iterative setting. In particular, the backward pass is altered, as the gra-\ndient of the loss with respect to the model parameters could ﬂow through basis\nfunctions. We conduct toy experiments to compare the iterative and single-pass\nversions of the architecture, draw conclusions about why this architecture choice\nhas been made in the context of protein structure prediction, and consider in\nwhich other scenarios it may be a useful choice. The code will be made publicly\navailable3.\n2 Background\nIn this section we provide a brief overview of SE(3) equivariance, with a descrip-\ntion of some prominent SE(3)-equivariant machine learning models. To situate\nthis discussion in a concrete setting, we take motivation from the task of protein\nstructure prediction and reﬁnement, which is subject to SE(3) symmetry.\n2.1 Protein Structure Prediction and Reﬁnement\nIn protein structure prediction, we are given a target sequence of amino acids,\nand our task is to return 3D coordinates of all the atoms in the encoded pro-\ntein. Additional information is often needed to solve this problem – the target\nsequence may be used to ﬁnd similar sequences and related structures in pro-\ntein databases ﬁrst [10, 11]. Such coevolutionary data can be used to predict\nlikely interresidue distances using deep learning – an approach that has been\ndominating protein structure prediction in recent years [12–14]. Coevolutionary\ninformation is encoded in a multiple sequence alignment (MSA) [15], which can\nbe used to learn pairwise features such as residue distances and orientations [14].\nThese pairwise features are constraints on the structure of the protein, which\nIterative SE(3)-Transformers 3\ninform the prediction of the output structure. One could start with random 3D\ncoordinates for the protein chain and use constraints from learnt pairwise fea-\ntures to ﬁnd the best structure according to those constraints. The problem can\nthen be approached in an iterative way, by feeding the new coordinates back in\nas inputs and further improving the structure. This iterative approach can help\nto improve predictions [16].\nImportantly, MSA and pairwise features do not include a global orientation\nof the protein – in other words, they are invariant under rotation of the protein.\nThis allows for the application of SE(3)-equivariant networks, which respect this\ninvariance by design, as done in AlphaFold 2 [2]. The predictions of an SE(3)-\nequivariant network, in this case predicted shifts to backbone and side chain\natoms, are always relative to the arbitrary input frame of reference, without the\nneed for data augmentation. SE(3)-equivariant networks may also be applied\nin an iterative fashion, and when doing so it is possible to propagate gradients\nthrough the whole structure prediction pipeline. This full gradient propagation\ncontrasts with the disconnected structure reﬁnement pipeline of the ﬁrst version\nof AlphaFold [12].\n2.2 Equivariance and the SE(3)-Transformer\nA function, task, feature 4, or neural network is equivariant if transforming the\ninput results in an equivalent transformation of the output. Using rotations R\nas an example, this condition reads:\nf(R ·⃗ x) = R ·f(⃗ x) (1)\nIn the following, we will focus on 3D rotations only – this group of rotations is\ndenoted SO(3). Adding translation equivariance, i.e. going from SO(3) to SE(3),\nis easily achieved by considering relative positions or by subtracting the center\nof mass from all coordinates.\nA set of data relating to points in 3D may be represented as a graph. Each\nnode has spatial coordinates, as well as an associated feature vector which en-\ncodes further relevant data. A node could represent an atom, with a feature vec-\ntor describing its momentum. Each edge of the graph also has a feature vector,\nwhich encodes data about interactions between pairs of nodes. In an equivariant\nproblem, it is crucial that equivariance applies not only to the positions of the\nnodes, but also to all feature vectors – for SO(3) equivariance, the feature vectors\nmust rotate to match any rotation of the input. To distinguish such equivariant\nfeatures from ordinary neural network features, we refer to them as ﬁbers.\nA ﬁber could, for example, encode momentum and mass as a 4 dimensional\nvector, formed by concatenating the two components. A momentum vector would\ntypically be 3 dimensional (also called type-1), and is rotated by a 3x3 matrix.\nThe mass is scalar information (also called type-0), and is invariant to rotation.\n4 A feature of a neural network is an input or output of any layer of the network.\n4 F. Fuchs and E. Wagstaﬀ et al.\nThe concept of types comes from representation theory, where a type- ℓ feature\nis rotated by a (2 ℓ+ 1) ×(2ℓ+ 1) Wigner-D matrix.5 Because a ﬁber is a con-\ncatenation of features of diﬀerent types, the entire ﬁber is rotated by a block\ndiagonal matrix, where each block is a Wigner-D matrix. [17–19]\nAt the input and output layers, the ﬁber structure is determined by the task\nat hand. For the intermediate layers, arbitrary ﬁber structures can be chosen. In\nthe following, we denote the structure of a ﬁber as a dictionary. E.g. a ﬁber with\n3 scalar values (e.g. RGB colour channels) and one velocity vector has 3 type-0\nand 1 type-1 feature: {0:3, 1:1}. This ﬁber is a feature vector of length 6.\nThere is a large and active literature on machine learning methods for graph\ndata, most importantly graph neural networks [20–24]. The SE(3)-Transformer\n[9] in particular is a graph neural network explicitly designed for SE(3)-equivariant\ntasks, making use of the ﬁber structure and Wigner-D matrices discussed above\nto enforce equivariance of all features at every layer of the network.\nAlternative Approaches to Equivariance: The Wigner-D matrix ap-\nproach6 at the core of the SE(3)-Transformer is based on closely related earlier\nworks [17–19]. In contrast, Cohen et al. [25] introduced rotation equivariance by\nstoring copies corresponding to each element of the group in the hidden layers\n– an approach called regular representations. This was constrained to 90 de-\ngree rotations of images. Two recent regular representation approaches [26, 27]\nextend this to continuous data by sampling the (inﬁnite) group elements and\nmap the group elements to the corresponding Lie group to achieve a smoother\nrepresentation.\n2.3 Equivariant Attention\nThe second core aspect of an SE(3)-Transformer layer is the self-attention [28]\nmechanism. This is widely used in machine learning [21,29–34] and based on the\nprinciple of keys, queries and values – where each is a learned embedding of the\ninput. The word ‘self’ describes the fact that keys, queries and values are derived\nfrom the same context. In graph neural networks, this mechanism can be used to\nhave nodes attend to their neighbours [21,28,35–37]. Each node serves as a focus\npoint and queries information from the surrounding points. That is, the feature\nvector fi of node i is transformed via an equivariant mapping into a query qi.\nThe feature vectors fj of the surrounding points j are mapped to equivariant\nkeys kij 7. A scalar product between key and query – together with a softmax\nnormalisation – gives the attention weight. The scalar product of two rotation\nequivariant features of the same type gives an invariant feature. Multiplying the\ninvariant weightswij with the equivariant valuesvij gives an equivariant output.\n5 We can think of type-0 features as rotating by the 1 ×1 rotation matrix (1).\n6 This approach rests on the theory of irreducible representations [17, 18].\n7 Note that often, the keys and values do not depend on the query node, i.e. kij = kj .\nHowever, in the SE(3)-Transformer, keys and values depend on the relative position\nbetween query i and neighbour j as well as on the feature vector fj .\nIterative SE(3)-Transformers 5\n3 Implementation of an Iterative SE(3)-Transformer\nHere, we describe the implementation of the iterative SE(3)-Transformer cov-\nering multiple aspects such as gradient ﬂow, equivariance, weight sharing and\navoidance of information bottlenecks.\n3.1 Gradient ﬂow in Single-Pass vs. Iterative SE(3)-Transformers\nG0\n× × ×\nG1\nFig. 1: Gradient ﬂow (orange) in a conventional, single-pass SE(3)-Transformer\nmapping from a graph (left) to an updated graph (right). The equivariant basis\nkernels (top) do not have to be diﬀerentiated as there is no gradient ﬂow through\nthem.\nAt the core of the SE(3)-Transformer are the kernel matrices W(⃗ xj −⃗ xi)\nwhich form the equivariant linear mappings used to obtain keys, queries and\nvalues in the attention mechanism. These matrices are a linear combination\nof basis matrices. The weights for this linear combination are the output of\ntrainable neural networks. Importantly, the basis matrices are not learnable.\nThey are deﬁned by spherical harmonics and Clebsch-Gordan coeﬃcients, and\ndepend only on the relative positions in the input graph G0 [9].\nTypically, equivariant networks have been applied to tasks where 3D coordi-\nnates in the input are mapped to an invariant or equivariant output in a single\npass. This means that the relative positions of nodes do not change until the\nﬁnal output, and the basis matrices therefore remain constant. Gradients do not\nﬂow through them, and the spherical harmonics do not have to be diﬀerentiated,\nas can be seen in Fig. 1.\nWhen applying the SE(3)-Transformer in an iterative fashion (see Fig. 2),\neach block ioutputs an updated graph Gi. This allows for, e.g., re-evaluating the\ninteractions or binary potentials between two nodes. Now the relative positions,\nand therefore the basis matrices, are no longer constant until the ﬁnal output,\nand gradients ﬂow through the basis. The spherical harmonics used to construct\nthe basis are smooth functions, and therefore backpropagating through them is\npossible. We provide code which implements this backpropagation.\n3.2 Hidden Representations Between Blocks\nIn the simplest case, each SE(3)-Transformer block outputs a single type-1 fea-\nture per point, which is then used as a relative update to the coordinates before\n6 F. Fuchs and E. Wagstaﬀ et al.\nG0\nS1\nN1\n×\nG1\nS2\nN2\n×\nG2\nFig. 2: Gradient ﬂow (orange) in an iterative SE(3)-Transformer with multiple\nposition updates. Now the gradients do ﬂow through the basis construction (S2)\nmeaning this part has to be diﬀerentiated.\napplying the second block. This, however, introduces a bottleneck in the infor-\nmation ﬂow. Instead, we choose to maintain the dimensionality of the hidden\nfeatures. A typical choice would be to have the same number of channels (e.g.\n4) for each feature type (e.g., up to type 3). In this case the hidden represen-\ntation reads {0:4,1:4,2:4,3:4}. We then choose this same ﬁber structure for\nthe outputs of each SE(3)-Transformer block (except the last) and the inputs to\nthe blocks (except the ﬁrst). This way, the amount of information saved in the\nhidden representations is constant throughout the entire network.\n3.3 Weight Sharing\nIf each iteration is expected to solve the same sub-task, weight sharing can make\nsense in order to reduce overﬁtting. The eﬀect on memory consumption and\nspeed should, however, be negligible during training, as the basis functions still\nhave to be evaluated and activations have to evaluated and stored separately for\neach iteration. In our implementation, we chose not to share weights between\ndiﬀerent SE(3)-Transformer blocks. The number of trainable parameters hence\nscales linearly with the number of iterations. In theory, this enables the network\nto leverage information gained in previous steps for deciding on the next update.\nOne beneﬁt of this choice is that it facilitates using larger ﬁbers between the\nblocks as described in 3.2 to avoid information bottlenecks. A downside is that\nit ﬁxes the number of iterations of the SE(3)-Transformer.\n3.4 Gradient Descent\nIn this paper, we will apply the SE(3)-Transformer to an energy optimisation\nproblem, loosely inspired by the protein structure prediction problem. For convex\noptimsation problems, gradient descent is a simple yet eﬀective algorithm. How-\never, long amino acid chains with a range of diﬀerent interactions can be assumed\nto create many local minima. Our hypothesis is that the SE(3)-Transformer is\nbetter at escaping the local minimum of the starting conﬁguration and likely to\nﬁnd a better global minimum. We add an optional post-processing step of gra-\ndient descent, which optimises the conﬁguration within the minimum that the\nIterative SE(3)-Transformers 7\nSE(3)-Transformer found. In real-world protein folding, these two steps do not\nhave to use the same potential. Gradient descent needs a diﬀerentiable potential,\nwhereas the SE(3)-Transformer is not subject to that constraint.\n4 Experiments\n-0.4 -0.2 0 0.2 0.4 -0.4\n-0.2\n0\n0.2\n0.4\n-0.4\n-0.2\n0\n0.2\n0.4\n0.0 0.5 1.0 1.5 2.0\ns\n0.0\n0.1\n0.2\n0.3\n0.4\np(s)\nFig. 3: The left plot shows a conﬁguration of ﬁve nodes, with the gradient of the\npotential between each pair represented by the colour of the edges. Blue edges\nindicate repulsion and orange edges indicate attraction. Stronger colour repre-\nsents stronger interaction. The right plot shows the double-minimum potential\np(s), with parameter a= 0.\nWe study a physical toy problem as a proof-of-concept and to get insights\ninto what type of tasks could beneﬁt from iterative predictions. We consider an\nenergy minimisation problem with 10 particles, which we will refer to as nodes.\nEach pair (ni,nj ) of nodes in the graph interacts according to a potentialpij (rij ),\nwhere rij is the distance between the nodes. The goal is to minimise the total\nvalue of the potential across all pairs of nodes in the graph.\nWe choose a pairwise potential with two local minima. This creates a complex\nglobal potential landscape that is not trivially solvable for gradient descent:\nsij = rij −aij −1 (2)\npij (sij ) = s4\nij −s2\nij + sij\n10 + pmin (3)\nHere pmin ≈0.32 ensures that pij (sij ) attains a minimum value of zero. The\nparameter aij = aji is a random number between 0.1 and 1.0 – this stochasticity\nis necessary to avoid the existence of an optimal solution for all examples.\nWe consider three models for solving this problem: (i) thesingle-pass SE(3)-\nTransformer (12 layers); (ii) a three-block iterative SE(3)-Transformer (4 ×3\nlayers); (iii) gradient descent (GD) on the positions of the nodes. We also\nevaluate a combination of ﬁrst applying an SE(3)-Transformer and then running\nGD on the output. Additionally, we evaluate the iterative SE(3)-Transformer\nboth with and without propagation of basis function gradients as described in\n8 F. Fuchs and E. Wagstaﬀ et al.\nSection 3.1. We run GD until the update changes the potential by less than a\nﬁxed tolerance value. We train the SE(3)-Transformers for 100 epochs with 5000\nexamples per epoch, which takes about 90 minutes. We ran each model between\n15 and 25 times – theσvalues in Tables 1 and 2 represent 1-σconﬁdence intervals\n(computed using Student’s t-distribution) for the mean performance achieved by\neach model. All models except GD have the same overall number of parameters.\nTable 1: Average energy of the system after optimisation (lower is better).\nGradient Descent Single-Pass No Basis Gradients Iterative Iterative + GD\nEnergy 0 .0619 0 .0942 0 .0704 0 .0592 0 .0410\nσ ±0.0001 ±0.0002 ±0.0025 ±0.0011 ±0.0003\nThe results in Table 1 show that the iterative model performs signiﬁcantly\nbetter than the single-pass model, approximately matching the performance of\ngradient descent. The performance of the iterative model is signiﬁcantly degraded\nif gradients are not propagated through the basis functions. The best performing\nmethod was the SE(3)-Transformer followed by gradient descent. The fact that\nthe combination of SE(3)-Transformer and gradient descent outperforms pure\ngradient descent demonstrates that the SE(3)-Transformer ﬁnds a genuinely dif-\nferent solution to the one found by gradient descent, moving the problem into a\nbetter gradient descent basin.\nTable 2: Performance comparison of single-pass and iterative SE(3)-Transformers\nfor diﬀerent neighbourhood sizes K and a fully connected version (FC).\nFC Single (K9) FC Iterative (K9) K5 Single K5 Iterative K3 Single K3 Iterative\nEnergy 0 .0942 0 .0592 0 .1321 0 .0759 0 .1527 0 .0922\nσ ±0.0002 ±0.0011 ±0.0003 ±0.0050 ±0.0001 ±0.0036\nSo far, every node attended to all N−1 other nodes in each layer, hence cre-\nating a fully connected graph. In Table 2, we analyse how limited neighborhood\nsizes [9] aﬀect the performance, where a neighborhood size of K = 5 means that\nevery node attends to 5 other nodes. This reduces complexity from O(N2) to\nO(NK), which is important in many practical applications with large graphs,\nsuch as proteins. We choose the neighbors of each node by selecting the nodes\nwith which it interacts most strongly. In the iterative SE(3)-Transformer, we\nupdate the neighbourhoods in each step as the interactions change.\nTable 2 shows that the iterative version consistently outperforms the single-\npass version across multiple neighborhood sizes, with the absolute diﬀerence\nbeing the biggest for the smallest K. In particular, it is worth noting that the it-\nerative version with K = 3 outperforms the fully connected single-pass network.\nIn summary, the iterative version consistently outperforms the single-pass\nversion in ﬁnding low energy conﬁgurations. We emphasise that this experiment\nis no more than a proof-of-concept. However, we expect that when moving to\nlarger graphs (e.g. proteins often have more than 10 3 atoms), being able to\nexplore diﬀerent conﬁgurations iteratively will only become more important for\nbuilding an eﬀective optimiser.\nIterative SE(3)-Transformers 9\nAcknowledgements\nWe thank Georgy Derevyanko, Oiwi Parker Jones and Rob Weston for helpful\ndiscussions and feedback. This research was funded by the EPSRC AIMS Centre\nfor Doctoral Training at the University of Oxford and The Open Philanthropy\nProject Improving Protein Design.\nReferences\n1. Recep Adiyaman and Liam James McGuﬃn. Methods for the reﬁnement of protein\nstructure 3d models. International journal of molecular sciences, 20(9):2301, 2019.\n2. John Jumper, R Evans, A Pritzel, T Green, M Figurnov, K Tunyasuvunakool,\nO Ronneberger, R Bates, A Zidek, A Bridgland, et al. High accuracy protein\nstructure prediction using deep learning. Fourteenth Critical Assessment of Tech-\nniques for Protein Structure Prediction (Abstract Book), 22:24, 2020.\n3. Brian Kuhlman and Philip Bradley. Advances in protein structure prediction and\ndesign. Nature Reviews Molecular Cell Biology, 20(11):681–697, 2019.\n4. Carlos Outeiral Rubiera. Casp14: what google deepmind’s alphafold 2 really\nachieved, and what it means for protein folding, biology and bioinformatics.\nhttps://www.blopig.com/blog/2020/12/casp14-what-google-deepminds-\nalphafold-2-really-achieved-and-what-it-means-for-protein-folding-\nbiology-and-bioinformatics/, 2020.\n5. Lupoglaz. Openfold2. https://github.com/lupoglaz/OpenFold2/tree/toy_se3,\n2021.\n6. Phil Wang. Se3 transformer - pytorch. https://github.com/lucidrains/se3-\ntransformer-pytorch, 2021.\n7. Dale Markowitz. Alphafold 2 explained: A semi-deep dive. https:\n//towardsdatascience.com/alphafold-2-explained-a-semi-deep-dive-\nfa7618c1a7f6, 2020.\n8. Mohammed AlQuraishi. Alphafold2 @ casp14: “it feels like one’s child has\nleft home.”. https://moalquraishi.wordpress.com/2020/12/08/alphafold2-\ncasp14-it-feels-like-ones-child-has-left-home/ , 2020.\n9. Fabian B. Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-\ntransformers: 3d roto-translation equivariant attention networks. In Advances in\nNeural Information Processing System (NeurIPS), 2020.\n10. UniProt Consortium. Uniprot: a worldwide hub of protein knowledge. Nucleic\nacids research, 47(D1):D506–D515, 2019.\n11. Protein data bank: the single global archive for 3d macromolecular structure data.\nNucleic acids research, 47(D1):D520–D528, 2019.\n12. Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre,\nTim Green, Chongli Qin, Augustin ˇZ´ ıdek, Alexander WR Nelson, Alex Bridgland,\net al. Improved protein structure prediction using potentials from deep learning.\nNature, 577(7792):706–710, 2020.\n13. Jinbo Xu. Distance-based protein folding powered by deep learning. Proceedings\nof the National Academy of Sciences, 116(34):16856–16865, 2019.\n14. Jianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchin-\nnikov, and David Baker. Improved protein structure prediction using predicted\ninterresidue orientations. Proceedings of the National Academy of Sciences,\n117(3):1496–1503, 2020.\n10 F. Fuchs and E. Wagstaﬀ et al.\n15. Martin Steinegger, Markus Meier, Milot Mirdita, Harald V¨ ohringer, Stephan J\nHaunsberger, and Johannes S¨ oding. Hh-suite3 for fast remote homology detection\nand deep protein annotation. BMC bioinformatics, 20(1):1–15, 2019.\n16. Joe G Greener, Shaun M Kandathil, and David T Jones. Deep learning extends de\nnovo protein modelling coverage of genomes using iteratively predicted structural\nconstraints. Nature communications, 10(1):1–13, 2019.\n17. Nathaniel Thomas, Tess Smidt, Steven M. Kearnes, Lusann Yang, Li Li, Kai\nKohlhoﬀ, and Patrick Riley. Tensor ﬁeld networks: Rotation- and translation-\nequivariant neural networks for 3d point clouds. ArXiv Preprint, 2018.\n18. Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen.\n3d steerable cnns: Learning rotationally equivariant features in volumetric data.\nIn Advances in Neural Information Processing Systems (NeurIPS), 2018.\n19. Risi Kondor. N-body networks: a covariant hierarchical neural network architecture\nfor learning atomic potentials. ArXiv preprint, 2018.\n20. Thomas N. Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard S.\nZemel. Neural relational inference for interacting systems. In Proceedings of the\nInternational Conference on Machine Learning, ICML, 2018.\n21. Petar Veliˇ ckovi´ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLi` o, and Yoshua Bengio. Graph attention networks.International Conference on\nLearning Representations (ICLR), 2018.\n22. Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neu-\nral networks. IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2017.\n23. Peter Battaglia, Jessica Blake Chandler Hamrick, Victor Bapst, Alvaro Sanchez,\nVinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam\nSantoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andy Ballard, Justin\nGilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victo-\nria Jayne Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli,\nMatt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive\nbiases, deep learning, and graph networks. arXiv, 2018.\n24. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu\nPhilip. A comprehensive survey on graph neural networks. IEEE transactions on\nneural networks and learning systems, 2020.\n25. Taco Cohen and Max Welling. Group equivariant convolutional networks. In\nProceedings of the International Conference on Machine Learning, ICML, 2016.\n26. Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Wilson. Generalizing\nconvolutional neural networks for equivariance to lie groups on arbitrary continuous\ndata. Proceedings of the International Conference on Machine Learning, ICML,\n2020.\n27. Michael Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye\nTeh, and Hyunjik Kim. Lietransformer: Equivariant self-attention for lie groups.\nIn ArXiv Preprint, 2020.\n28. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nAdvances in Neural Information Processing Systems (NeurIPS), 2017.\n29. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and\nYee Whye Teh. Set transformer: A framework for attention-based permutation-\ninvariant neural networks. In Proceedings of the International Conference on Ma-\nchine Learning, ICML, 2019.\nIterative SE(3)-Transformers 11\n30. Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Lev-\nskaya, and Jon Shlens. Stand-alone self-attention in vision models. In Advances in\nNeural Information Processing System (NeurIPS), 2019.\n31. Sjoerd van Steenkiste, Michael Chang, Klaus Greﬀ, and J¨ urgen Schmidhuber. Re-\nlational neural expectation maximization: Unsupervised discovery of objects and\ntheir interactions. International Conference on Learning Representations (ICLR),\n2018.\n32. Fabian B. Fuchs, Adam R. Kosiorek, Li Sun, Oiwi Parker Jones, and Ingmar Pos-\nner. End-to-end recurrent multi-object tracking and prediction with relational\nreasoning. arXiv preprint, 2020.\n33. Jiancheng Yang, Qiang Zhang, and Bingbing Ni. Modeling point clouds with self-\nattention and gumbel subset sampling. IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019.\n34. Saining Xie, Sainan Liu, and Zeyu Chen Zhuowen Tu. Attentional shapecontextnet\nfor point cloud recognition. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\n35. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang,\nBowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding.\nInternational Conference on Learning Representations (ICLR), 2017.\n36. Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. Advances in\nNeural Information Processing Systems (NeurIPS), 2017.\n37. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative\nposition representations. Annual Conference of the North American Chapter of\nthe Association for Computational Linguistics (NAACL-HLT), 2018.\n12 F. Fuchs and E. Wagstaﬀ et al.\nA Network Architecture and Training Details\nFor the single-pass SE(3)-Transformer we use 12 layers. For the iterative version\nwe use 3 iterations with 4 layers in each iteration. In both cases, for all hidden\nlayers, we use type-0, type-1, and type-2 representations with 4 channels each.\nThe attention uses a single head. The model was trained using an Adam opti-\nmizer with a cosine annealing learning rate decay starting at 10 −3 and ending\nat 10−4. Our gradient descent implementation uses a step size of 0 .02 and stops\noptimizing when the norm of every position update is below 0 .001.",
  "concepts": [
    {
      "name": "Equivariant map",
      "score": 0.7578991651535034
    },
    {
      "name": "Transformer",
      "score": 0.6660438179969788
    },
    {
      "name": "Iterative method",
      "score": 0.6140473484992981
    },
    {
      "name": "Computer science",
      "score": 0.5467726588249207
    },
    {
      "name": "Graph",
      "score": 0.4794405996799469
    },
    {
      "name": "Iterative design",
      "score": 0.4766541123390198
    },
    {
      "name": "Iterative refinement",
      "score": 0.4626155495643616
    },
    {
      "name": "Homogeneous space",
      "score": 0.4580570161342621
    },
    {
      "name": "Iterative and incremental development",
      "score": 0.42495888471603394
    },
    {
      "name": "Algorithm",
      "score": 0.3492613434791565
    },
    {
      "name": "Theoretical computer science",
      "score": 0.325087308883667
    },
    {
      "name": "Mathematical optimization",
      "score": 0.24325445294380188
    },
    {
      "name": "Mathematics",
      "score": 0.23172491788864136
    },
    {
      "name": "Engineering",
      "score": 0.12471598386764526
    },
    {
      "name": "Voltage",
      "score": 0.11404088139533997
    },
    {
      "name": "Geometry",
      "score": 0.08324393630027771
    },
    {
      "name": "Pure mathematics",
      "score": 0.08304321765899658
    },
    {
      "name": "Electrical engineering",
      "score": 0.08063104748725891
    },
    {
      "name": "Software engineering",
      "score": 0.06793445348739624
    },
    {
      "name": "Scheduling (production processes)",
      "score": 0.0
    }
  ],
  "topic": "Equivariant map"
}