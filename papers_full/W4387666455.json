{
  "title": "Attentive transformer deep learning algorithm for intrusion detection on IoT systems using automatic Xplainable feature selection",
  "url": "https://openalex.org/W4387666455",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2114482991",
      "name": "Demostenes Zegarra Rodriguez",
      "affiliations": [
        "Universidade Federal de Lavras"
      ]
    },
    {
      "id": "https://openalex.org/A3187022295",
      "name": "Ogobuchi Daniel Okey",
      "affiliations": [
        "Universidade Federal do ABC"
      ]
    },
    {
      "id": "https://openalex.org/A2129087621",
      "name": "Siti Sarah Maidin",
      "affiliations": [
        "INTI International University"
      ]
    },
    {
      "id": "https://openalex.org/A5041353593",
      "name": "Ekikere Umoren Udo",
      "affiliations": [
        "Michael Okpara University of Agriculture"
      ]
    },
    {
      "id": "https://openalex.org/A2137727665",
      "name": "João Henrique Kleinschmidt",
      "affiliations": [
        "Universidade Federal do ABC"
      ]
    },
    {
      "id": "https://openalex.org/A2114482991",
      "name": "Demostenes Zegarra Rodriguez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3187022295",
      "name": "Ogobuchi Daniel Okey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129087621",
      "name": "Siti Sarah Maidin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5041353593",
      "name": "Ekikere Umoren Udo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137727665",
      "name": "João Henrique Kleinschmidt",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4313477864",
    "https://openalex.org/W4353110502",
    "https://openalex.org/W4280610250",
    "https://openalex.org/W2733358165",
    "https://openalex.org/W2982682021",
    "https://openalex.org/W3132929199",
    "https://openalex.org/W4285006276",
    "https://openalex.org/W4298110865",
    "https://openalex.org/W2991140181",
    "https://openalex.org/W3025080732",
    "https://openalex.org/W2958285686",
    "https://openalex.org/W4307873385",
    "https://openalex.org/W2355839424",
    "https://openalex.org/W2128728535",
    "https://openalex.org/W2967743080",
    "https://openalex.org/W2921708219",
    "https://openalex.org/W2102636708",
    "https://openalex.org/W2972472681",
    "https://openalex.org/W4288941200",
    "https://openalex.org/W3174086521",
    "https://openalex.org/W2903294440",
    "https://openalex.org/W3204893321",
    "https://openalex.org/W2733765803",
    "https://openalex.org/W2887645765",
    "https://openalex.org/W3027870121",
    "https://openalex.org/W2890991187",
    "https://openalex.org/W2793412195",
    "https://openalex.org/W6784145239",
    "https://openalex.org/W3172701880",
    "https://openalex.org/W2950636209",
    "https://openalex.org/W3177472101",
    "https://openalex.org/W4313292182",
    "https://openalex.org/W2762776925",
    "https://openalex.org/W2789828921",
    "https://openalex.org/W2773572362",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W3092848502",
    "https://openalex.org/W3216660278"
  ],
  "abstract": "Recent years have witnessed an in-depth proliferation of the Internet of Things (IoT) and Industrial Internet of Things (IIoT) systems linked to Industry 4.0 technology. The increasing rate of IoT device usage is associated with rising security risks resulting from malicious network flows during data exchange between the connected devices. Various security threats have shown high adverse effects on the availability, functionality, and usability of the devices among which denial of service (DoS) and distributed denial of service (DDoS), which attempt to exhaust the capacity of the IoT network (gateway), thereby causing failure in the functionality of the system have been more pronounced. Various machine learning and deep learning algorithms have been used to propose intelligent intrusion detection systems (IDS) to mitigate the challenging effects of these network threats. One concern is that although deep learning algorithms have shown good accuracy results on tabular data, not all deep learning algorithms can perform well on tabular datasets, which happen to be the most commonly available format of datasets for machine learning tasks. Again, there is also the challenge of model explainability and feature selection, which affect model performance. In this regard, we propose a model for IDS that uses attentive mechanisms to automatically select salient features from a dataset to train the IDS model and provide explainable results, the TabNet-IDS. We implement the proposed model using the TabNet algorithm based on PyTorch which is a deep-learning framework. The results obtained show that the TabNet architecture can be used on tabular datasets for IoT security to achieve good results comparable to those of neural networks, reaching an accuracy of 97% on CIC-IDS2017, 95% on CSE-CICIDS2018 and 98% on CIC-DDoS2019 datasets.",
  "full_text": "RESEA RCH ARTICL E\nAttentive transformer deep learning\nalgorithm for intrusion detection on IoT\nsystems using automatic Xplainable feature\nselection\nDemo ´ stenes Zegarra Rodrı ´ guez\nID\n1\n*, Ogobuchi Daniel Okey\n2\n, Siti Sarah Maidin\nID\n3\n*,\nEkikere Umoren Udo\n4\n, João Henrique Kleinschmidt\n2\n1 Department of Computer Science, Federal University of Lavras, Minas Gerais, Brazil, 2 Center for\nEngineeri ng, Modelin g, and Applied Social Sciences, Federa l University of ABC, Santo Andre, São Paulo,\nBrazil, 3 Faculty of Data Science and Informatio n Technolo gy (FDSIT), INTI Interna tional University, Nilai,\nMalaysia, 4 Department of Electrica l/Electronic Engineeri ng, Michael Okpara Universit y of Agriculture,\nUmudik e, Nigeria\n* sitisarah.m aidin@newi nti.edu.my (SSM); demost enes.zegar ra@ufla.br (DZR)\nAbstract\nRecent years have witnessed an in-depth proliferation of the Internet of Things (IoT) and\nIndustrial Internet of Things (IIoT) systems linked to Industry 4.0 technology. The increasing\nrate of IoT device usage is associated with rising security risks resulting from malicious net-\nwork flows during data exchange between the connected devices. Various security threats\nhave shown high adverse effects on the availability, functionality, and usability of the devices\namong which denial of service (DoS) and distributed denial of service (DDoS), which\nattempt to exhaust the capacity of the IoT network (gateway), thereby causing failure in the\nfunctionality of the system have been more pronounced. Various machine learning and\ndeep learning algorithms have been used to propose intelligent intrusion detection systems\n(IDS) to mitigate the challenging effects of these network threats. One concern is that\nalthough deep learning algorithms have shown good accuracy results on tabular data, not\nall deep learning algorithms can perform well on tabular datasets, which happen to be the\nmost commonly available format of datasets for machine learning tasks. Again, there is also\nthe challenge of model explainability and feature selection, which affect model performanc e.\nIn this regard, we propose a model for IDS that uses attentive mechanisms to automatically\nselect salient features from a dataset to train the IDS model and provide explainable results,\nthe TabNet-IDS. We implement the proposed model using the TabNet algorithm based on\nPyTorch which is a deep-learning framework. The results obtained show that the TabNet\narchitecture can be used on tabular datasets for IoT security to achieve good results compa-\nrable to those of neural networks, reaching an accuracy of 97% on CIC-IDS2017, 95% on\nCSE-CICIDS2018 and 98% on CIC-DDoS2019 datasets.\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 1 / 25\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Zegarra Rodrı ´ guez D, Daniel Okey O,\nMaidin SS, Umoren Udo E, Kleinsch midt JH (2023)\nAttentive transform er deep learning algorithm for\nintrusion detection on IoT systems using automatic\nXplainable feature selection. PLoS ONE 18(10):\ne0286652. https://d oi.org/10.1371/j ournal.\npone.028665 2\nEditor: Kathirava n Srinivasan, Vellore Institute of\nTechnology : VIT University, INDIA\nReceived: February 18, 2023\nAccepted: May 19, 2023\nPublished: October 16, 2023\nCopyright: © 2023 Daniel Okey et al. This is an\nopen access article distributed under the terms of\nthe Creative Commons Attributio n License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: All relevant data are\nwithin the paper and on Figshare: DOI: 10.6084/\nm9.figshare .23118164.\nFunding: The research is funded by the Faculty of\nData Science and Information Technology (FDSIT),\nINTI Internationa l University, Nilai, Negeri\nSembilan, Seed Grant No: INTI-FDSIT-01 -08-202 2,\nthe Tertiary Education Trust Fund (TETFund,\nNigeria) / Forum for Agricultural Research in Africa\n(FARA, Ghana), and the Coordena c¸ ão de\nIntroduction\nThe IoT network is a large network space that lets a lot of devices around the world connect to\neach other to disseminate information [1]. This reduces the amount of work that people have\nto do and makes the digital world easier to access, more productive, and easier to control.\nBecause the IoT ecosystem is made up of many different types of devices, it is very vulnerable\nto cyberattacks that take advantage of the system’s weaknesses to stop it from working [2, 3].\nCyber threats are getting more complex and dangerous, and the IoT infrastructure is prolifer-\nating [4, 5]. This makes it very important to improve security in smart IoT networks. The\nauthors of [6] identified and classified the security risks associated with any IoT system into\nfour categories: risks relating to physical impacts on network devices; risks relating to the\ndevice and user authentication and validation; security concerns relating to violations of rules\nand the confidentiality, integrity, and availability (CIA) of information systems; and the risks\nassociated with the processing, transfer, and storage of personal data or other sensitive infor-\nmation. Personal data and additional sensitive information, such as biometrics and medical\nrecords, must be sent securely across IoT networks. According to the European Union’s Gen-\neral Data Protection Regulation (GDPR) [7], other data that has been designated as personal\ndata include unique device identifiers, IP addresses, unique identifiers of mobile providers,\nand wireless access points.\nSeveral attack vectors are currently being investigated in the IoT environment, including\nbut not limited to denial of service (DoS), distributed denial of service (DDoS), web attacks,\ninfiltration, address resolution protocol (ARP) poisoning, and portscans, which can either\noccur at the perception, transport, or application layers of the network [8]. Due to the high\nnegative impact of any successful attack on the enterprise network and organizational rele-\nvance, companies now rely on intelligent IDS models to automatically detect any intrusive\nattempts on their networks to reduce massive losses; hence, leading to more intensive research\nin the area of IDS for computer networks [9].\nThe effects of these attacks have recently resulted in massive data losses, especially those\ncaused by the DDoS [8] which has newly generated severe damages to IoT systems [10]. DDoS\nattacks in their various forms can bring down an entire network by using up all of its resources\nby sending too many get requests that are more than the target system can handle. These\nattacks work mainly by taking advantage of multiple operating systems to break into them and\nuse them as agents to send a lot of traffic to the target client. Other attacks have also shown\nhigh levels of negative impacts on computer networks and other computing devices [11]. IoT\ndevices, fog devices, workstations, and more are some of the many vulnerable systems that can\nbe compromised or exploited [10, 12].\nIn response to the threat that these attack vectors pose, many methods have been looked into\nto make sure that the IoT network is safe from all kinds of cyber threats. Approaches like\nmachine learning (ML), deep learning (DL), intrusion detection systems (IDS), and intrusion\nprevention systems (IPS) that are based on artificial intelligence (AI) are some of the most\nsearched-for topics in this field [13–15]. At the instance of detection of malicious network\nflows, IDS can take immediate action to deter the attack from spreading to the entire IoT net-\nwork [16]. IDS are generally divided into two categories based on the detection mechanism\nused in their design: anomaly and misuse detection [11]. During the analysis of how network\nprofile activities change over time, flows that could be harmful to the network can be found to\nbe acting in strange ways. Even though they have a lot of false positives (FPs), anomaly detection\nIDS can find new and different types of attacks. On the contrary, misuse detection is able to dis-\ntinguish between legitimate and dangerous attacks using previously established patterns; hence,\nit can accurately identify known attacks but has limitations with new and unknown attacks [11].\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 2 / 25\nAperfeic ¸ oamento de Pessoal de Nı ´ vel Superior\n(CAPES, Brazil). The funders had no direct role in\nstudy design, data collection , and analysis, the\ndecision to publish, or the prepara tion of the\nManuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nWith the advent, availability and easy access to large-scale data, highly-performing hard-\nware accelerators, state-of-the-art machine learning, and deep neural networks (DNNs), pre-\ndictive modeling for problem-solving can now be performed at a large scale, especially in the\nIoT ecosystem, where large volumes of data are generated and transmitted along a resource-\nconstrained channel [17]. The majority of the datasets used in IDS modeling tasks such as the\nNSL-KDD, KDD Cup’99, and UNSW-15 contain high dimensionality, which means they have\na large number of features. This makes it harder to process these datasets. In order to solve the\nproblem relating to data dimensionality, the most common approach has been the use of sin-\ngular value decomposition-enabled methods such as principal component analysis (PCA) [18]\nand isometric feature mapping (Isomap) [19]. The main problems with using PCA and Isomap\nto reduce the number of features in a dataset are linear embedding and essential feature loss,\neven though they produce good results and keep the distances between data points to a moder-\nate degree. Hence, they are not most suitable for dealing with the problems of the curse of\ndimensionality [19]. In [20, 21], the authors presented different methods of reducing the\ndimensionality and features of the CIC-IDS2017 dataset in an attempt to reduce the computa-\ntional cost and improve the speed of model performance.\nFurthermore, there is the challenge of explaining the relationship between the various fea-\ntures interacting in the dataset to produce the results from the model. In the cybersecurity\natmosphere involving network intrusion detection and prevention, where the predictions of\nthe model are of grave importance to the network administrator and the safety of connected\ndevices, it is imperative that AI models be able to automatically provide explanations for their\ndecisions. This is imperative because any misrepresentation or misinterpretation of the mod-\nel’s performance can cause a devastating loss of the network, data, and other relevant informa-\ntion. Since it is important that ML model predictions continue to be as transparent and\ndependable as possible in an interpretable way, many model-specific and model-agnostic tech-\nniques, including both local and global interpretability, have been applied recently [22]. While\nthe local explanations emphasize individual predictions, the global explanations provide\ndetailed insights about the entire model’s behavior in the form of plots or more interpretable\napproximations such as decision boxes. Some of the commonly used methods to achieve\nmodel explainability include Permutation Feature Importance (PFI) [23], SHapley Additive\nexPlanation (SHAP) [24], Contextual Importance and Utility (CIU) [25, 26], and Local Inter-\npretable Model-Agnostic Explanation (LIME) [27]. The explanations provided by these meth-\nods show high reliability in validating the performance of the model, which can be measured\nin terms of the accuracy, consistency, and stability of the unseen data [28].\nThus, there is a need for a tabular data-centric algorithm that provides both automatic fea-\nture selection and explainable results on the choice of the features used in the model develop-\nment and the predictions of the model applied to IoT security. Considering the limitations of\nthe existing deep learning-based model to provide interpretability to the model decision with-\nout third-party frameworks and large resource requirements, this research specifically\naddresses these issues categorically. Therefore, we propose an attentive transformer deep\nlearning algorithm for IDS that implements automatic feature engineering and selection for\ndetecting threats in an IoT ecosystem. We develop the IDS model using the TabNet [29] archi-\ntecture. Different metrics, like accuracy, precision, recall, prediction time, and Matthew’s cor-\nrelation coefficient, are used to judge how well the model works. We look at how well the\nmodel can be explained using the masking feature, which gives us the decision output. Specifi-\ncally, the major contribution of the paper includes:\n• The proposed IDS model introduces a unique approach to feature engineering and selection\nby incorporating an attention mechanism. This address the challenge of handling a large\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 3 / 25\nnumber of features, which can result in decreased model performance as a result of noisy\nand irrelevant features. The attention mechanism permits the model to learn and select\nwhich features most importantly contribute to the model overall behavior; leading to a more\nefficient and accurate IDS model to detect and prevent security breaches.\n• Provides a solution to the challenge of interpretability of deep learning-based models that\nmostly depend on third-party frameworks to explain the behaviors of the model with respect\nto feature usage for predictions. By incorporating the ability of TabNet architecture to use a\nsparse feature selection process and learn interpretable decision rules, we provide an\napproach to automatic feature explanations which helps users understand the reasoning\nbehind the predictions. Additionally, the explanations are used to improve the model by\nidentifying the features that may require more attention in applications where the TabNet\narchitecture is not implemented.\n• Considering the resource constraints of the IoT system, the model is optimized using a light-\nweight optimization algorithm, the OPTUNA that is computationally efficient to determine\nthe properties of the TabNet architecture most suitable for the IDS model. This ensures that\nthe resulting model is suitable to be executed in resource-constrained environments and also\nfor real-time applications for fast response.\n• The model offers a balance between computational efficiency and accuracy, meeting the\nenergy requirement of IoT devices. Adding to the lightweight and energy-efficient attributes of\nour proposed model, it requires less training and test time, making it ideal for real-time appli-\ncations, as it can process data quickly and accurately, providing fast and reliable predictions.\nIn general, our proposed IDS model aims to prefer a solution to the limitations of existing\ndeep learning-based models by providing a more inclusive and interpretable solution. The\ngrowth rate of IoT devices makes it critical to develop IDS models that not only show high\naccuracy but also provide insight into its decision rule through interpretability and explana-\ntions. This is believed to be of immense assistance to network analysts in analyzing zero-day\nattacks. Our proposed model emphasizes interpretability, which is critical for gaining trust\nand understanding the decision rule applied by the IDS model to network flows. By using\nmodern optimization algorithms, we ensure that our proposed model provides competitive\nresults in accuracy while requiring fewer resources for implementation, making it a general-\nized solution for network security in IoT environments.\nRelated works\nAdvances in communication technology, as well as the availability of various sensing and\ncomputational devices, have resulted in rapid growth in the field of IoT devices and their appli-\ncation areas. Because of the widespread use of smart devices across various domains, IoT secu-\nrity has become a source of major concern in terms of protecting users’ confidentiality and\nprivacy, as well as the hardware and network of IoT systems. Several authors, including [30–\n32] gave a detailed overview of the current trends in IoT security, asserting that the high rate of\nadoption of the technology has paved the way for more threats to continue to ravage the IoT\nsystem. Despite the fact that numerous research projects are underway with the primary goal\nof improving the level of security of smart devices, the domain remains largely unexplored.\nWhen using some of the most widely available security tools, such as encryption, authentica-\ntion, access control, network protection, and application control, time wastage is usually\nencountered coupled with the inefficiency of the tools to provide a comprehensive check\nagainst the inherent vulnerabilities.\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 4 / 25\nIt is possible for hackers to break into open networks that provide access to certain intelli-\ngent utility services (such as smart grid systems, communications systems, and healthcare sys-\ntems), posing serious hazards of information leakage, service disruption, and financial losses\n[1]. Such attacks that take down the availability of networks and devices and which have been\nreported amidst other forms of threats to be most harmful to the networks are the DDoS\nattacks. For instance, Mirai is an uncommon sort of botnet that triggers huge-scale distributed\ndenial-of-service (DDoS) strikes by abusing IoT machines [33] such that they become unable\nto respond to user requests, thereby becoming exhausted and eventually shut down. The Per-\nsirai thingbot is one variant of Mirai code that continues to grow and infect Internet Protocol\n(IP) cameras [34].\nIn [35], authors propose an IDS model in IoT networks using deep learning algorithms spe-\ncifically designed to detect DoS attacks in two main domains: passive and active threats using\ndifferent algorithms, including CNN and MLP. The BoT-IoT dataset was used as a testbed to\ndevelop and evaluate the model’s performance, using accuracy, recall, and precision measure-\nment as the performance evaluation metrics, with the CNN reaching an average accuracy of\n91.27% and MLP achieving 79.01%. With the high accuracy, there remained an issue with\nunderstanding the basic features of the dataset that prevailed over the model’s decision. Hence,\nmany ML and DL models remain in the black box domain where there is no thorough expla-\nnation of the decision of the model. The interpretations provided by some frameworks about\nthe model decision can be identified as either local or global interpretations. A local interpreta-\ntion provides insight into the model’s prediction based on each feature of the dataset. On the\nother hand, when the interpretation is made globally, the explanations result from the entire\nset of features in the dataset [36]. LIME is well known for its ability to provide local interpret-\nability while SHAP is widely used for global interpretations.\nIn [37], Ensemble modeling was used to classify network intrusions. The research used the\n“Network Socket Layer—Knowledge Discovery in Databases” records (NSL-KDD). The\nauthors converted the network flows into a malicious and non-malicious network packet to\nperform a binary classification task. The research used an ensemble model termed extreme\ngradient boosting (XGBoost) to identify records based on NSL-KDD input attributes. The\nXGBoost-based classification model was trained on 125973 and tested on 22544 records with\n98.7% accuracy. Leveraging the data from the UNSW-NB15 dataset obtained from a publicly\navailable data repository, authors in [38] performed a machine learning intrusion detection\nperformance analysis and compared different machine learning techniques for detecting net-\nwork intrusions, including SVM, Naïve Bayes, Decision Tree, and Random Forest. Based on\ntheir analysis of the UNSW-NB15 dataset, the authors opined that the SVM, Naïve Bayes,\nDecision Tree, and Random Forest achieved a detection accuracy of 92.28%, 74.19%, 95.82%,\n97.49%, respectively. These results are black-boxed as it does not provide an insight into the\ndifferent network flows or packets that defined the model’s prediction results.\nBecause of how complicated the IoT ecosystem is and how much data it generates, each\npiece of data has a huge impact on how the network flows. This has made it more important to\nunderstand how each piece of data helps predict whether the network profile is normal or not.\nModel explainability is an approach to solving this challenge. In [22], Deep learning algorithms\nfor detecting network intrusion in IoT devices were proposed. Authors leveraged the explain-\nable characteristics of some explainable AI (XAI) frameworks, such as LIME and SHAP to pro-\nvide explanations for the model’s decisions. The problem with this method is that it needs a lot\nof computing power because SHAP values are hard to compute. On the NSL-KDD dataset, the\nproposed models achieved a performance of 82.4% and 96.4% in accuracy and precision,\nrespectively, in a binary classification task. The authors in [36] proposed an approach for\nmodel interpretation specifically for security systems called LEMNA. LEMNA was tested on a\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 5 / 25\nmalware dataset using a deep learning algorithm that takes an input data sample and produces\na minimal set of interpretable characteristics that can be used to explain the classification of\nthe input data sample. The central concept is to use a straightforward, interpretable model to\napproximatively represent a small region of the infinitely complicated decision boundary of\ndeep learning. To better function with security applications (such as binary code analysis), the\nlocal interpretable model is optimized to (1) manage feature dependency and (2) handle non-\nlinear local limits, which improves explanation fidelity.\nIn both supervised and unsupervised language modeling problems, the ability of attention-\nbased mechanisms to find larger subsets of features used for modeling has been used to\nimprove the accuracy of the predictive model. One of the attention mechanisms is the self-\nattention network (SAN) [39, 40] which identifies important features primarily from tabular\ndata. Meanwhile, this technique has been found to be more efficient with datasets that have a\nlarger amount of feature space, showing very lower performance with datasets having fewer\nfeature dimensions, therefore, implying that a limited amount of data can impact the relevant\npart of the feature. TabNet-IDS [29], is proposed, which chooses meaningful features in a data-\nset to reason from using a sequential attention mechanism in each step of the decision process\nduring the model’s training. It provides a visualization of the feature’s importance and demon-\nstrates how each feature contributes to the prediction made by the model. The mask attribute\nprovides both local and global interpretability [41]. The summary of the related literature\nincluding the strength and weaknesses are shown in Table 1. Except for our proposed model,\nnone of the works in the literature utilized optimization techniques to reduce the complexity\nof the model and improve performance.\nMethodology\nThis section discusses the datasets, and preprocessing approaches, the proposed algorithmic\narchitecture, and the optimization techniques adopted to obtain suitable data for modeling for\nmodel development. The overall workflow used for this design is as shown in Fig 1.\nDataset description\nThree different IDS datasets, including CIC-IDS2017, CSE-CIC-IDS2018, and CIC-DDoS2019\nare used to evaluate the performance of the proposed model. These datasets are general-pur-\npose and relevant for developing predictive models for IDS in all computer networking scenar-\nios. Considering the versed number of datasets available for training ML models, we have\nselected these three datasets considering that they are more recent and contain a larger number\nTable 1. Summary of related literature and their features .\nAuthors Implemented Algorith m Xplainable model Optimizatio n\nSusilo and Sari [35] CNN/MLP No No\nDhaliwal et al [37] XGBoost No No\nBelouch et al [38] ML (DT,RF ,SVM) No No\nMane and Rao [22] DL No No\nYang et al [42] BiLSTM Yes (LIME) No\nSingh et al [43] GRU No No\nChen et al [44] Tab-SRU Yes (Self) No\nYin et al [45] RNN No No\nOur study TabNet-IDS Yes (Self) Yes\nhttps://do i.org/10.1371/j ournal.pone .0286652.t001\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 6 / 25\nof network flows and attack profiles; hence provide an enabling ground for the model to learn\nfrom a vast feature set and data instances.\nCIC-IDS2017. The CIC-IDS2017 is a general-purpose dataset that is widely used in the\ncybersecurity domain for modeling predictive IDS for IoT and computer networks. It is widely\nused among researchers because it was the first realistic yet comprehensive dataset proposed\n[46]. The dataset contains 83 features and 15 classes, collected using different machines over a\nperiod of seven days. A summary of the instances of the dataset and different classes with the\npercentage contribution is shown in Table 2. Just like other datasets, CIC-IDS2017 suffers\nfrom data imbalance, with a total of 83% of the entire network traffic belonging to normal traf-\nfic and only 17% being attacks. On analysis, we discovered that CIC-IDS2017 has 288602\ninstances with missing class labels and 2013 instances with missing information. These\nunwanted instances are dropped to form a new dataset that contains unique 2830540\ninstances. for the purpose of our experiment, a sample of the dataset is used after handling the\nimbalance as described in [11].\nFig 1. Flowchart for the design of the proposed TabNet-ID S model in this research .\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 01\nTable 2. Distributi on of stream records in CICIDS20 17 dataset.\nLabel Name Value Percentage (%)\nBenign 2359289 83.3452\nDoS Hulk 231073 8.1630\nPortScan 158930 5.6144\nDDoS 41835 1.4779\nDoS GoldenEye 10293 0.3636\nFTP-Patat or 7938 0.2804\nSSH-Pa tator 5897 0.2083\nDoS slowloris 5796 0.2048\nDoS Slowhttptest 5499 0.1943\nBot 1966 0.0695\nWeb Attack—Brut e Force 1507 0.0532\nWeb Attack—XSS 652 0.0230\nInfiltrati on 36 0.0013\nWeb Attack—SQ L Injection 21 0.0007\nHeartblee d 11 0.0004\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.t00 2\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 7 / 25\nCSE-CIC-IDS201 8. This is a general-purpose dataset constructed using different syn-\nthetic communication networks and captured about 2.8 million labeled data (malicious flows)\nwith 80 features, thereby increasing the complexity of the dataset significantly compared to\nthose before it. In total, the dataset contains over 16 million network flows collected over 10\ndays from different devices and stored as a PCAP file. Because this dataset contains more net-\nwork profiles, it tends to give more realistic flow behavior, and thus, IDS trained with this data-\nset tends to learn more on the available feature instances. The dataset contains 16 different\nclasses and 80 features, with the highest number of instances representing the benign class,\nwhich mimics normal network communication over the IoT network. The remaining 15 clas-\nses represent different attack types to the tune of 18% of the entire dataset, hence showing a\nhigh rate of imbalance [11] as shown in Table 3.\nCICDDoS2019. The CICDDoS2019 dataset is the most recent dataset that describes dif-\nferent DDoS attack profiles that are applicable to both IoT systems and computer networks.\nThe dataset was collected from a public repository at the University of New Brunswick, Canada\n[8]. Various experiments involving classical ML algorithms and DL methods have already\nbeen performed using this dataset, but none specifically considers the problem of using a DL\nclassifier specific to tabular datasets to analyze the dataset. Also, the problem of feature engi-\nneering has always been handled manually in previous works, unlike ours, which uses an\nattention mechanism to automatically select the salient features to reason from during the\ntraining and testing phases. Overall, the dataset contains about 50,063,112 records, comprising\n56,863 benign traffic profiles and 50,006,249 malicious network flows during the training\nphase. In the testing phase, a total of 20,364,525 records were obtained, with benign flows of\n56,965 and 20,307,560 malicious traffic records. The entire dataset contains 88 unique features\nextracted from the packet capture (Pcap) files using the CICFlowMeter [47], a network flow\ngenerator and analyzer. The first eight features of the dataset, including the source IP, destina-\ntion IP, source port, destination port, flow ID, timestamp, and protocol, are the default param-\neters of the CICFlowMeterV3, while the remaining eighty features define the behavior of each\nof the flows. As noted in the work of [8] who originally generated the dataset, the training data-\nset contains DDoS attack vectors including, NTP, DNS, LDAP, MSSQL, NetBIOS, SNMP,\nTable 3. Distributi on of stream records in CSE-CIC-ID S2018 dataset.\nLabel Name Value Percentage (%)\nBenign 13484708 83.07001\nDDoS attack-H OIC 686012 4.22605\nDDoS attacks-L OIC-HTTP 576191 3.54952\nDoS attacks-Hulk 461912 2.84552\nBot 286191 1.76303\nFTP-Bru teForce 193360 1.19116\nSSH-Bru teforce 187589 1.15561\nInfiltrati on 161934 0.99756\nDoS attacks-SlowHT TPTest 139890 0.86177\nDoS attacks-GoldenE ye 41508 0.25570\nDoS attacks-Slowl oris 10990 0.06770\nDDOS attack-LOIC -UDP 1730 0.01066\nBrute Force -Web 611 0.00376\nBrute Force -XSS 230 0.00142\nSQL Injection 87 0.00054\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.t00 3\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 8 / 25\nSSDP, UDP, UDP-Lag, WebDDoS, SYN, and TFTP. Meanwhile, the testing dataset contains\nonly seven (7) DDoS attack types, with the inclusion of a new attack profile that is absent from\nthe training set. The attack vectors are PortScan, NetBIOS, LDAP, MSSQL, UDP, UDP-Lag,\nand SYN.\nFor a better understanding of the different attack profiles, they are grouped into two sets,\nincluding the reflection-based and exploitation-based attacks. In reflection-based attacks, the\nattackers use third-party products or components that they legitimately purchased to conceal\ntheir identities. The application layer protocol is often used to send both malicious and normal\nnetwork packets to reflector servers [8]. The attackers set the source IP address of the attacking\nagent to be the same as the IP address of the victim or target system. This makes it look like the\nattacker is a real user. This attempt overwhelms the victim’s device and sends a lot of responses\nuntil the victim’s device can’t handle it anymore. This is further sub-grouped into TCP-based\nattacks, UDP-based attacks, and TCP/UDP-based attacks. The TCP-based attacks, like the\nMSSQL and SSDP profiles, can only be carried out with the TCP protocol. Attacks that use the\nUDP protocol are called UDP-based attacks, and they include the CharGen, NTP, and TFTP\nattack vectors. Also, some DDoS attacks can be carried out using either the TCP or the UDP\nprotocols. These are DNS, LDAP, NetBIOS, and SNMP vectors.\nEven more, the same method of hiding identity is used in exploitation-based DDoS attacks.\nHowever, the major difference lies in the primary focus of the attack points. In this kind of\nDDoS attack, the attackers try to take advantage of certain protocols, such as the network,\napplication, and transport layers of the Open Systems Interconnection (OSI) model. Cyberat-\ntackers send packets to the servers bearing the source IP addresses configured to the victim’s\nIP address to overburden the victim with response packets. This DDoS attack type is also\ngrouped into the TCP-based attacks that comprise SYN Flood and the UDP-based attacks that\ninclude UDP Flood and UDP-Lag. Due to the large volume of the dataset, we used 10% for our\nresearch as shown in Fig 2.\nData preparation\nIn most cases, network flows arrive in a very jumbled condition that has to be cleaned up\nbefore they can be used. It is still the most important step in machine learning since the quality\nFig 2. Distributio n of the 10% of the CIC-DDoS 2019 use for the training and testing of the model.\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 02\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 9 / 25\nof the dataset affects how well the model works. In the proposed architecture, the TabNet-IDS,\nthe major data preparation process is automatically handled within the internal mechanism of\nthe algorithm. Therefore, only basic data cleaning, like getting rid of duplicates, deleting and\nfilling in missing values, and encoding categorical characteristics have been handled in this\nphase. The training of the model can be improved leading to an easy learning process if the\nlabels are encoded into their numerical equivalent. By scaling the data, we were able to collect\ndata points that are very close to one another, which helped remove any bias from the final\nforecast. In addition to this, it helps keep the data’s integrity intact over time. Another impor-\ntant part of the process of cleaning up the data is figuring out how to deal with the problem of\nimbalance which is commonly observed in any real-world dataset. Several different methods,\nlike the Synthetic Oversampling Technique (SMOTE) and its variants including, the Adaptive\nSynthetic (ADASYN) algorithm, SMOTEN, SMOTEN, etc. have been suggested as possible\nsolutions to this problem. However, we take care of problems caused by imbalance automati-\ncally within the proposed TabNet-IDS model using the built-in augmentation fit function\nargument called classificationSMOTE. By setting this argument to true, the model automati-\ncally takes care of any issues caused by an imbalance within the training data, ensuring that it\nis balanced between different classes.\nOverview of proposed model and implementation\nSequential attention model. Tabular data is one of the most readily available types of\ndatasets used for ML tasks. They are arranged in rows and columns and are most often saved\nas a comma-separated value (CSV) file. DNN models have shown good results on image,\naudio, and text data utilizing canonical architectures that permit the encoding and communi-\ncation of the data in different formats [1]. Despite the fact that tabular data contains multiple\ndata types that can easily function well with the canonical architecture, tabular-data-specific\ndeep learning models have yet to be proposed for IDS.\nBasically, IDS models developed with tree-based algorithms such as decision trees, ran-\ndom forests, XGBoost, LightGBM, Extra Tree classifiers, and deep learning models, includ-\ning Deep Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional\nNeural Networks (CNN), [11] etc require that the input tabular data be first feature engi-\nneered before it is fed into the algorithm for training. With TabNet, which provides a\nsequential attention mechanism in the model, the following benefits are inherent [29]: (i)\nautomated preprocessing of the raw dataset, which is then trained using gradient descent-\nbased optimization, thereby, allowing a more flexible data integration. (ii) A sequential atten-\ntion-enabled feature selection technique carried out in a step-wise and instant-wise fashion.\nThe instant-wise feature selection helps to reduce model complexity train time in compari-\nson with other tree-based classifiers and this is achieved by using a single deep learning\narchitecture, and (iii) a mechanism that encourages the design of lightweight models for IoT\nsystems.\nThe proposed IDS is implemented with the TabNet architecture, which is an algorithm spe-\ncifically developed to train models on tabular data. Normally, individual feature selection is\ndone by ML engineers when handling problems associated with DTs and DNNs algorithms.\nThis is important for getting decision boundaries in the hyperplane, which can be thought of\nas a generalized linear combination of features with coefficients that say how much of each fea-\nture to use. TabNet uses this kind of functionality paradigm and also takes care of the problems\nof selecting features, processing features, and understanding them. Basically, the TabNet archi-\ntecture exists in four different segments: the encoder, decoder, feature transformer, and atten-\ntive transformer modules, as shown in Figs 3 and 4.\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 10 / 25\nAs inputs, we give the architecture raw numerical features that have been slightly cleaned\nup and embeddings that can be trained for the mapped categorical features. The global feature\nnormalization is not implemented, but we use batch normalization (BN). We implemented\nbatch normalization as it generally improves the performance of deep learning models by nor-\nmalizing the input layers and adjusting the activation of preceding layers prior to exchanging\nthe information to subsequent layers. Specifically, in our study, batch normalization is used in\neach of the decision steps to improve the quality of the sparse feature selection process. On the\noutput of the feature transformer, we also used batch normalization to stabilize the distribu-\ntion of the transformed features, thereby reducing the impact of the initialization of the net-\nwork data and encouraging the network to learn better representations of the input data.\nDuring the training phase, batch normalization computes the mean and variance of the activa-\ntion for each of the mini-batch of the data and then normalizes the activation. This is done by\nsubtracting the mean and dividing it by the standard deviation. This is to ensure that the mean\nand variance of the activation remain consistent through different batches, thereby ensuring\nthat overfitting is reduced and improving the overall generalization of the model.\nGiven dimensions D and batch size B, the features can be expressed as f 2 <\nB×D\n. We pass\nthese same dimensional features to each of the decision steps using TabNet encoding that is\nbased on sequential multi-step processing with N\nsteps\ndecision steps. The i\nth\nstep uses the pro-\ncessed network traffic from the (i − 1)\nth\nstep to decide which features to use and to output the\nprocessed feature representation, which is then aggregated into the overall decision. The\nFig 3. The TabNet encoder is made up of three parts: A feature transform er, an attentive transformer, and feature\nmasking . The information to be used by the attentiv e transfor mer and the next feature transforme r is separated with a\nsplit block. For each phase, the feature selection mask gives interpretab le informa tion regarding the model’s\nfunctional ity, and the masks can be combined to get global feature significan ce attributes.\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 03\nFig 4. Model training, optimizati on, and evaluation .\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 04\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 11 / 25\noutput of Fig 3 is used as an input to the second phase shown in Fig 4 for the training process.\nFirst, the data is split into 10 folds, and in each of the training phases, 9 folds are used, while\none is used for validation. The trained IDS model’s accuracy is checked, and then, depending\non the score, the model is hyper-parameter tuned and a final evaluation is performed.\nA learning mask given by Eq 1 is used to perform a soft selection of the most salient features\nof the dataset while maintaining the learning capacity of the decision step, resulting in a model\nthat is more parameter efficient. The mask is used to obtain the local and global interpretations\nof the model performance.\nM ½i� 2 <\nB�D\nð1Þ\nwhere M[i] is the mask of the i\nt\nh feature of the dataset considered most important at that\ninstant in the decision block.\nThe masking process uses a multiplicative gating mechanism, which zeros out all the feature\nvalues that are not selected by the mask. This helps to create a sparsified feature map engi-\nneered to reduce the total number of features that are used for the final prediction and improve\ninterpretability. The attention mechanism uses the function given in Eq 2 to compute the\nmask M[i].\na½i \u0000 1� : M ½i� ¼ spa rsem axðP ½i \u0000 1�Þ:h\ni\nða½i \u0000 1�ÞÞ: ð2Þ\nwhere h\ni\nis an FC and BN layer, P[i − 1] is the prior information and the sparsemax function\n[48] is used to give sparsity because it can map the Euclidean projection to the probabilistic\nsimplex. Hence, it provides sparse features for explainability. The prior scale denoted as P[i] is\nus to track all the previous knowledge gained on each of the features during the learning pro-\ncess providing robustness to the model since it can not use a previously used feature. Mathe-\nmatically, the expression for this function is as given in Eq 3.\nP½i� ¼\nY\ni\nj¼1\nðg \u0000 M½j�Þ\nð3Þ\nwhere γ denotes a relaxation parameter that determines the flexibility of using each of the fea-\ntures in the various decision step of the algorithm. Where γ = 1, a selected feature is restricted\nto being used only at one decision step, but when the value of γ increases, a particular feature\ncan be used over multiple decision steps. To encourage more robust feature selection, we cal-\nculate the sparse loss, which is a regulation technique that penalizes the model for selecting too\nmany features at a given step, forcing the model to pay more attention to the most informative\nfeatures. The equation for calculating the sparse loss is given by Eq 4. Essentially, the equation\ncomputes the negative logarithm of the mask entries that relate to the unselected features\nimplying where M\nb,j\n[i] = 0\nL\nspar se\n¼\nX\nN\nsteps\ni¼1\nX\nB\nb¼1\nX\nD\nj¼1\n\u0000 M\nb;j\n½i�\nN\nste ps\n� B\nlog ðM\nb;j\n½i� þ �Þ ð4Þ\nwhere � is a small constant added to the mask entries to avoid taking the logarithm of zero.\nThe selected features are preprocessed so that they can be used in the model training. This is\nachieved with the feature transformer block, which consists of a fully connected layer (FC) fol-\nlowed by batch normalization (BN) and a gated-linear unit (GLU). The output of the pro-\ncessed feature is split into two parts: one part is used for the decision step, and the other part\ncontains information for the subsequent step, as determined by their interrelationship. This is\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 12 / 25\nshown in Eq 5\n½d ½i�; a½i�� ¼ f\ni\nðM ½i�:f Þ ð5Þ\nwhere d ½i� 2 <\nB�N\nd\nand a½i� 2 <\nB�N\na\n[29]. We improve the model’s parameter efficiency and its\nability to learn by using a shared layer to keep features from being used more than once in dif-\nferent decision steps and layers that depend on those steps. We improve the training of a Tab-\nNet by using large batch sizes with the BN as well as the ghost BN (a virtual batch\nnormalization) form by using a virtual batch size B\nv\nand momentum m\nB\nand aggregating the\nentire decision embedding using Eq 6\nd\nout\n¼\nX\nN\nstep s\ni¼1\nReL Uðd½i�Þ ð6Þ\nwhere ReLU is the activation function. The implementation algorithm is shown in Algorithm 1.\nAlgorithm 1 TabNet Algorithm\nResult: Trained TabNet model\nInitialize input features X 2 R \nn�d\n;\nInitialize target labels y 2 R \nn\n;\nInitialize hyperparameters h;\nInitialize empty list of decision steps L = [ ];\nInitialize empty list of feature masks M = [ ];\nfor t = 1, 2, . . ., T do\nFeature masking\nM\nt\n createMask(X, h);\nM.append(M\nt\n);\nFeature transformation\nX\nt\n X � M\nt\nA\nt\n, E\nt\n encode(X\nt\n, h)\nDecision step\nd\nt\n decision(A\nt\n)\nL.append(d\nt\n)\nUpdate feature importance\nX  X − E\nt\ny\nt\n predict(A\nt\n)\nX  updateFeatures(X, y\nt\n, h)\nend\nTrain final layer on A\nT\n;\nreturn Trained TabNet model\nIn this algorithm, n represents the number of data instances and d represents the number of\ninput features. h represents the hyperparameters of the model, including the number of deci-\nsion steps T. createMask() is a function that creates a feature mask matrix for each decision\nstep, which is applied element-wise to the input features to obtain masked features X\nt\n. encode\n() is a function that applies the encoder layers to the masked features to obtain feature atten-\ntion masks A\nt\nand transformed features E\nt\n. decision() is a function that computes the decision\nfor each decision step based on the feature attention masks A\nt\n. updateFeatures() is a function\nthat updates the importance of each feature based on its contribution to the predictions.\nFinally, the trained model is obtained by training a final layer on the last feature attention\nmask A\nT\n.\nHyperparameter tuning with optuna\nOne of the most important parts of the TabNet algorithm is that it can explain why certain fea-\ntures were chosen at each step. We implement the design using the PyTorch framework with\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 13 / 25\nthe Pytorch_tabnet module. PyTorch TabNet has a plethora of arguments that aid perfor-\nmance definition. In Table 4, we present the parameters of the algorithm and their default val-\nues. We use a search algorithm called Optuna to perform hyperparameter tuning. Optuna\nhelps us recursively search a given space to find the best parameters that give the most reliable\nperformance. Optuna [49] is an optimization algorithm that: (i) offers dynamic construction\nof the search space, hence, a define-by-run programming interface; (ii) is applicable to both\nlightweight and heavy-weight computational tasks due to its easy-to-setup architecture and\nlow computational cost; and (iii) permits reduced time complexity with the use of pruning and\nefficient sampling paradigm [49]. The internal architecture of the optuna API is shown in Fig\n5 adapted from [49].\nWe used three different steps to implement the optimization algorithm. First, we set the\nparameters for the search. Then, we started the objective function that would be used during\nthe optimization. Finally, we called the study function, which does the search and records the\nobservations made. We use different graph networks to show the search history, which helps\nus understand how well the search algorithm works. The parameter importance obtained dur-\ning the optimization process is shown in Fig 6. The Patience Scheduler which is an instance of\nthe schedulerfn and defines the patience level for each of the optimization trails tends to be the\nmost influencing parameter for the optimization process. This, however, is dependent on the\ndataset used. The number of steps, patience level, and epochs as well are contributing features\nthat enhance the model accuracy. The general algorithm for implementing the tuning is\nshown in Algorithm 2.\nAlgorithm 2 Optuna for TabNet-IDS Tuning\nInput: Train and validation data, number of trials n, range of hyper-\nparameters to be tuned\nOutput: Optimal hyperparameters\nobjective_function(trial)\nDefine the objective function to optimize\nTable 4. Descriptio n of TabNet hyper-param eters and optimized space.\nTabNet Model\nParameter s\nDescription Search Space Search Result\nn_d The width of the decision prediction, default value = 8 [8,64] 60\nn_a Attention embedding for each mask, default value = 8 [8, 64] 60\nn_steps Denotes the number of steps in the architecture [3, 10] 3\nn_independent Defines the number of Gated Linear Units layers in each of the steps [1, 5] 2\ngamma Coefficient for feature re-usage in the mask. A value close to 1 makes mask selection least\ncorrelated between layers\n[1.0, 2.0] 1.5\nn_shared The number of shared Gated Linear Units at each step. Default = 2 [1, 5] 3\nepsilon This value is not tuned 1e-15 -\nmomentum moment um used for batch normalizat ion, default = 0.02 [0.01, 0.4] 0.02\nlambda_sparse Adds extra sparsity loss coefficient to the model. A higher lambda sparse coefficient\nyields a more sparsed model in feature selection\n[1e-3, 1e-5] 1e-3\noptimizer_fn PyTorch optimiza tion function to reduce complexit y and achieve minima,\ndefault = torch.opt im.Adam\n- -\nscheduler_fn used to change the learning rate during training - -\nscheduler_params A dictionary of param eters to apply to the scheduler function [“gamma ”: 0.95,\n“step_size”: 10]\n[“gamma”: 0.95,\n“step_size”: 10]\nmask_type defines the masking function to use for the feature selection, default = sparsem ax [“sparsema x”, “entmax” ] entmax\nPatience defines the number of consecu tive epochs under which the early stopping is called if\nthere is no improveme nt in the evaluation metrics\n[15, 30] 20\nhttps://do i.org/10.1371/j ournal.pone .0286652.t004\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 14 / 25\nInput: Hyperparameters hp sampled from trial\nOutput: Validation accuracy of the model\nmodel  TabNetModel(. . ., **hp)\nmodel.compile(loss=”, optimizer=‘adam’, metrics=[‘accuracy’])\nmodel.fit((train data), epochs = hp[‘epochs’], batch_size = hp\n[‘batch_size’])\nval_loss, val_acc  model.evaluate(val_data, val_labels)\nreturn val_acc\nInitialize the Optuna study\nstudy  optuna.create_study(direction=‘maximize’) Run the trials\nfor i  1 to n do\ntrial  study.ask() hp  [. . .]\nfor param  trial.params do\nhp[param]  trial.params[param]\nval_acc  objective_function(trial)study.tell(trial, val_acc)\nPrint the best hyperparameters found by Optuna best_params  study.\nbest_params\nprint(‘Best Hyperparameters:’, best_params)\nIn this algorithm, the function objective_function defines the objective to optimize, which is\nthe validation accuracy of the TabNet model. The hyperparameters to be tuned are defined by\nthe hp dictionary, which is passed as arguments to the TabNetModel function. The hyperpara-\nmeters to be tuned can be specified in the range of hyperparameters to be tuned input of the\nalgorithm as the search space S. The optuna.create_study function initializes the Optuna study\nand the study.ask() function samples a set of hyperparameters for each trial. The study.tell\nfunction updates the Optuna study with the validation accuracy of the model for each trial and\nthe study.best_params function returns the best hyperparameters found by Optuna.\nResults and discussion\nThe proposed architecture was implemented on three different datasets, including the\nCIC-IDS2017, CSE-CICIDS2018, and CIC-DDoS2019. A cross-validation of 10 folds was used\nfor each training process, and the accuracy, precision, recall, f score, and Matthew’s correlation\ncoefficient (MCC) are measured for each model and in each fold. With a batch size of 1024,\nFig 5. Optuna’s general system design. In the search study, each worker is responsi ble for executin g one instance of\nthe objective function. The objective function uses Optuna APIs to run its trial. The objective function accesses the\nshared storage to obtain informat ion about past studies when necessary. Each worker operates independen tly, running\nthe objective function and sharing the progress of the curren t study through the shared storage.\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 05\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 15 / 25\nAdam was used as an optimization function to train the models. We used a Ghost Batch Nor-\nmalization (GBN) of 128 that served as a virtual_batch_size for mini-batching the training\nsamples, which enhanced the training time for the algorithm. The results obtained for the\nthree datasets are shown in Table 5, indicating the accuracy, precision, recall, f1-measure,\nMCC, and test time in both the training and testing phases.\nAccording to Table 5, the proposed TabNet-IDS model was evaluated using the three data\nsets which achieved different performance levels considering the evaluation metrics across the\nselected datasets. On the CIC-IDS2017 dataset, the proposed model achieved an average accu-\nracy of 97.03% with a standard deviation of 0.36% (97.03%±0.36%), an average recall of\n91.10% with a standard deviation of 0.33% (97.10%±0.33%), an average precision of 97.02%\nalong with 0.35% standard deviation (97.02%±0.35%), F1-measure of 96.97% with standard\ndeviation of 0.37% (96.97%±0.37%), and MCC score of 96.97% and standard deviation of\n0.37% (96.97%±0.37%) The model’s testing time on this dataset is 3.86secs. Similarly, on the\nCSE-CIC-2018 dataset, the model shows an average accuracy, recall, precision, F-measure,\nMCC score of 95.58%±0.13%, 95.69%±0.11%, 95.59%±0.13%, 95.55%±0.12%, 95.55%±0.12%\nrespectively. In each of the metrics, the corresponding standard deviation is 0.13%, 0.11%,\n0.13%, 0.12% and 0.12% accordingly. The average test time of the model on this dataset is\n2.04Secs. In the same way, on the CIC-DDos2019 dataset, similar performances are obtained\nfor the accuracy, recall, precision, F-measure, and MCC score reaching 98.51%±0.12%, 98.50%\n±0.14%, 98.40%±0.21%, 98.44%±0.32%, 97.52%±0.33% respectively. The average test time on\nthis dataset is 1.16 secs. The general model evaluation performance is shown in Fig 7. Conse-\nquently, the results indicate that the proposed model performs well on all three datasets, with\nthe highest performance achieved on the CIC-IDS2019 dataset. The testing times for all three\ndatasets were reasonable, indicating that the proposed model can efficiently process and clas-\nsify network traffic data. However, it is important to note that the performance may vary\ndepending on the specific characteristics of the dataset, and further evaluation of additional\ndatasets may be necessary to validate the model’s generalizability.\nUpon analyzing the performance of the Tabnet-IDS model on the selected datasets, we\nobserved consistent results across the various evaluation metrics. This remarkable feature\nFig 6. Hyperparamet er import ance of the model parame ters obtained during the optimizati on process using\noptuna. n\nda\nis used to represent the values for n\nd\nand n\na\nsince n\nd\n= n\na\nis ideal for better performan ce.\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 06\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 16 / 25\nindicates the reliability and efficiency of the model’s predictions. More importantly, it is nota-\nble to state that the prediction time is significantly low, empowering the model to detect high-\nprofile network flows accurately within a short period. As a result, the model can provide assis-\ntance in quick response to attacks on IoT devices and deter further malicious attacks.\nIn terms of explainability, the various features [46] that were selected during each step by\nthe mask are shown for each of the datasets in Figs 8–10 for CIC-IDS2017, CSE-CICIDS2018,\nand CIC-DDoS2019 respectively.\nIn each of the masks, different sets of feature values are selected from the dataset based on\nimportance and used to train the model. The dark colors on the masks show the most essential\nparts, while the light colors show the less critical parts. In Fig 8 for example, features like Fwd\nIAT Mean, Bwd Packet Length Mean, Fwd Packet Length Std, and Bwd Packet Length Max,\ntend to show more prominence among all the features in deciding the model’s performance\naccording to mask 1, while in mask 3, features like Flow Duration, Fwd IAT Mean, Flow IAT\nMin, Packet Length Mean, Down/Up Ratio, and Min Packet Length informed the decision of\nthe model in that step. Similarly, in Fig 9, considering Mask 1, the instant-wise feature\nTable 5. Performance of the TabNet model on the selected datasets accordin g to the number of folds.\nFolds Dataset Accurac y Precision Recall F-Score MCC Test time (Sec)\n1 CIC-IDS2017 0.9725 0.9732 0.9723 0.9719 0.9719 3.1900\nCSE-CICID S2018 0.9563 0.9573 0.9563 0.9561 0.9561 2.0500\nCIC-DDoS20 19 0.9840 0.9830 0.9820 0.9844 0.9780 1.0600\n2 CIC-IDS2017 0.9733 0.9738 0.9733 0.9727 0.9727 4.1400\nCSE-CICID S2018 0.9525 0.9549 0.9525 0.9521 0.9521 2.1100\nCIC-DDoS20 19 0.9855 0.9840 0.9830 0.9844 0.9880 1.0910\n3 CIC-IDS2017 0.9691 0.9696 0.9691 0.9685 0.9685 4.1600\nCSE-CICID S2018 0.9555 0.9577 0.9555 0.9553 0.9553 1.8900\nCIC-DDoS20 19 0.9845 0.9835 0.9828 0.9848 0.9788 1.0470\n4 CIC-IDS2017 0.9691 0.9699 0.9691 0.9685 0.9685 4.1200\nCSE-CICID S2018 0.9552 0.9557 0.9552 0.9551 0.9551 2.0000\nCIC-DDoS20 19 0.9855 0.9855 0.9840 0.9849 0.9783 1.0100\n5 CIC-IDS2017 0.9623 0.9638 0.9623 0.9613 0.9613 3.5300\nCSE-CICID S2018 0.9569 0.9579 0.9569 0.9567 0.9567 1.8000\nCIC-DDoS20 19 0.9860 0.9836 0.9827 0.9848 0.9780 1.0550\n6 CIC-IDS2017 0.9724 0.9728 0.9724 0.9718 0.9718 3.4330\nCSE-CICID S2018 0.9557 0.9575 0.9557 0.9556 0.9556 2.0200\nCIC-DDoS20 19 0.9840 0.9837 0.9820 0.9844 0.9787 1.0910\n7 CIC-IDS2017 0.9737 0.9743 0.9734 0.9731 0.9731 4.1700\nCSE-CICID S2018 0.9567 0.9577 0.9567 0.9556 0.9557 2.2000\nCIC-DDoS20 19 0.9840 0.9830 0.9820 0.9844 0.9780 1.0880\n8 CIC-IDS2017 0.9666 0.9674 0.9666 0.9660 0.9660 3.5400\nCSE-CICID S2018 0.9566 0.9554 0.9566 0.9564 0.9564 2.1100\nCIC-DDoS20 19 0.9844 0.9834 0.9822 0.9844 0.9780 2.0880\n9 CIC-IDS2017 0.9707 0.9716 0.9707 0.9701 0.9701 4.1600\nCSE-CICID S2018 0.9556 0.9569 0.9566 0.9562 0.9562 2.1100\nCIC-DDoS20 19 0.9845 0.9835 0.9820 0.9844 0.9780 1.0400\n10 CIC-IDS2017 0.9732 0.9738 0.9732 0.9727 0.9727 4.2000\nCSE-CICID S2018 0.9568 0.9580 0.9567 0.9555 0.9555 2.1200\nCIC-DDoS20 19 0.9840 0.9835 0.9825 0.9844 0.9780 1.1070\nhttps://do i.org/10.1371/j ournal.pone .0286652.t005\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 17 / 25\nselection and reasoning almost utilized the entire features with Protocol, Pkt Len Mean, Flow\nIAT Std, Fwd IAT Std, Fws Seg Size Mean showing preeminence over all other features. The\nFlow IAT Std, Fwd Pkts/s, Pkt Len Std, and Ack Flag Cnt are seen to be more influential\namong the selected features in Mask. Each mask represents the shared decision step shown in\nFig 3.\nEach feature’s relative weight in the dataset is represented by its color in the feature column.\nNevertheless, the relevance of characteristics may vary from sample to sample, as shown by the\ncolor of the feature column. Different samples prioritize various features, as seen by the global\nFig 7. TabNet-ID S performan ce on the three datasets .\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 07\nFig 8. Feature masking output for the three decision steps of the TabNet model on the CIC-IDS2 017 dataset.\nhttps://doi.org/10 .1371/journal.p one.0286652. g008\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 18 / 25\nfeature selection shown by the aggregate feature importance mask. Instead of really employing\na decision tree, the TabNet approach uses a deep neural network to imitate one. This method\nis useful since it prevents rows with identical characteristics from always being shown.\nIn general, the proposed TabNet improves the detection and classification performance of\nthe IDS model by leveraging its inherent features of adaptive feature selection, decision steps,\nsparse feature representation, gradient-based learning, and the ability to handle high-dimen-\nsional data. IDS model is able t filter noise out of the data thereby maintaining that only rea-\nsonable data points are used to train and predict the network flows. With the decision steps,\nthe proposed model learns to adapt to specific characteristics of the data, hence, is able to\nimprove the detection rate on network flows. With the reduced complexity that is achieved\nwith the sparse feature representation, our TabNet-IDS is more efficient to provide a timely\nresponse in the detection and classification of the traffics.\nTo better understand the effectiveness of the TabNet-IDS model in categorizing each of the\nnetwork flows, we evaluate the confusion matrix which depicts the connection between the\nactual values and the anticipated values in the dataset. These confusion matrices are shown in\nFig 10. “Global explanation for the selected features used in the CIC-IDS20 19 dataset”. Masks for each step show that the Fwd Packet\nLength Min, Fwd Packet Length Mean, Total Fwd Packets, Min Packets Length , Bwd Packet Length Mean, Fwd Packet Length Max, and Fwd\nPacket Length Std are mostly used for the decision in the Masks.\nhttps:// doi.org/10.1371 /journal.pone. 0286652.g010\nFig 9. Global explanation for the selected features used in the CIC-IDS 2018 dataset.\nhttps:// doi.org/10.1371 /journal.pone. 0286652.g009\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 19 / 25\nFigs 11–13, respectively, for the CIC-IDS2017, CSE-CICIDS2018, and CIC-DDoS2019 data-\nsets. The normalized confusion matrices show that the proposed model can identify each of\nthe attack vectors as well as the normal traffic in the datasets in all the scenarios involving\nmulti-class network profile detection and classification. There are 9, 7, and 14 different classes\nin the CIC-IDS2017, CSE-CICIDS2018, and CIC-DDoS2019 that the model is built to identify.\nThe TabNet-IDS shows good performance in each of these scenarios in accurately classifying\nthe network flows with reduced False Positives.\nPerformance comparison with related works\nConsidering that several ML and DL models already exist that have been implemented using\ndifferent algorithms, it is crucial to compare their performance with our proposed TabNet-\nIDS model. This is aimed at highlighting the unique features and advantages of our model\nover existing ones. While accuracy is a key factor in evaluating the relevance of ML or DL\nmodel, explainability and interoperability are critical in providing insights into how the model\nmakes its predictions. This is one of the primary focuses of our study. Explainability helps to\nprovide more information on the reasons why specific features of the dataset determine the\npredictive results of the model. This, in turn, helps to increase the model’s transparency, mak-\ning it easier to understand, interpret, and trust its predictions. As shown in Table 6, all deep\nlearning-based models presented exhibited similar behaviors with different approaches. The\nevaluation metrics used showed results for accuracy, recall, and precision with the same range.\nAmong the related works presented, only our proposed study implemented Matthew’s correla-\ntion coefficient (MCC) as an evaluation metric. This metric provides important information\nFig 11. Confusion matrix of the model performan ce on the CIC-IDS2 017 dataset. The encoded label is 0: Benign, 1:\nBotnet, 2: Brute force, 3: DDoS, 4: DoS, 5: Heartbleed , 6: Infiltrati on, 7: PortScan, 8: Web attacks.\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 11\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 20 / 25\nabout the true positive, false positive, true negative, and false negative values in the prediction\nunlike the accuracy, precision, and recall. It is relevant to note that the implementation of the\nmodels used for comparison purposes was carried out on different datasets. The Tab-SRU pro-\nposed in [44] used the TabNet architecture for its implementation, and showed a high-perfor-\nmance accuracy as well as our results. While Chen et al [44] performed an experiment on the\nUKM-IDS20 and UNSW-NB15, we experimented on three more recent datasets. By incorpo-\nrating explainability techniques into our proposed TabNet-IDS model, we provide detailed\ninsights into the decision-making process of the model, making it easier to identify and\naddress any biases or errors that might arise. As a result, we can improve the model’s overall\naccuracy and effectiveness in identifying network intrusions and enhancing the security of IoT\ndevices.\nConclusion\nOver time, ML and DL algorithms have demonstrated high capabilities for providing\nenhanced network profile detection and classification, thereby ensuring that packets flowing\nthrough a network are reliable and secure. Due to the high dependence of IoT systems on the\ninternet and sometimes delay in providing regular updates, they appear to be highly vulnerable\nto various forms of exploits. To ensure a more secure IoT ecosystem, ML and DL algorithms\nare mostly used. In this study, we investigated the application of the DL algorithm that is spe-\ncifically targeted at tabular data, as most network profiles are extracted in tabular form. This\naspect of ML and DL tasks has been under-explored and needs more attention. We experi-\nmented with the proposed model architecture on three different general-purpose datasets,\nFig 12. Confusion matrix of the model performan ce on the CSE-CICI DS2018 dataset with each label encoded as\n0: Benign, 1: Bot, 2: DoS, 3: DDoS, 4: Brute force, 5: Infiltration , 6: Web Attacks.\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 12\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 21 / 25\nincluding the CIC-IDS 2017, CSE-CICIDS 2018, and CIC-DDoS 2019. The TabNet-IDS\nmodel demonstrated a high detection rate and reduced the false alarm rate for detecting and\nclassifying the network profile in a multi-class problem. The algorithm proposed in this study\ncontributes to knowledge in the IDS domain by providing explanations for the choice of the\nfeatures used in the predictive decision at each step of the training without requiring a third-\nparty framework such as SHAPEly and LIME for interpretations, hence resulting in a light-\nweight IDS model. Due to the large parameter space in the TabNet algorithm, we performed\nhyper-parameter tuning to obtain the most relevant parameters and values that best describe\nthe model’s performance while maintaining high accuracy. The model achieved an accuracy of\n97%, 95%, and 98% on the CIC-IDS2017, CSE-CICIDS2018, and CIC-DDoS2019. Although\nFig 13. Confusion matrix of the model performan ce on the CIC-DDoS 2019 dataset where each of the labels is\nencoded as 0: BENIGN , 1: DNS, 2: LDAP, 3: MSSQL, 4: NTP, 5: NetBIOS, 6: Portmap, 7: SNMP, 8: SSDP, 9: Syn,\n10: TFTP, 11: UDP, 12: UDPLag, 13: WebDDoS.\nhttps://d oi.org/10.1371/j ournal.pon e.0286652.g0 13\nTable 6. Compariso n of the Performan ce of our proposed model and other models that implemen ted similar algorithm s.\nReferences Method used Accuracy (%) Precis ion (%) Recall (%) F-Score (%) MCC (%)\nYang et al [42] BiLSTM 96.15 98.32 96.94 97.63 -\nSingh et al [43] GRU 89.97 88.32 97.94 92.84 -\nChen et al [44] Tab-SRU 99.23 99.04 99.64 99.34 -\nYin et al [45] RNN 88.93 87.76 92.51 90.07 -\nProposed Method A TabNet-I DS (CIC-IDS201 7) 97.03 97.10 97.02 96.97 96.97\nProposed Method B TabNet-I DS (CSE-CICI DS2018) 95.58 95.69 95.59 95.55 95.55\nProposed method C TabNet-I DS (CIC-DDoS2 019) 98.51 98.50 98.40 98.44 97.52\nhttps://do i.org/10.1371/j ournal.pone .0286652.t006\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 22 / 25\nthe model shows competitive performances and provides interpretability, its varying behavior\non different datasets is an acknowledged limitation. To address this, we intend to explore and\ninvestigate the model performance on additional datasets. We believe that by analyzing the\nmodel’s performance on a wider range of datasets, we can better understand the variations and\nmake necessary adjustments to better improve the overall efficiency; and validate the model’s\nability to generalize on unseen data for real-world applications.\nAuthor Contributions\nConceptualization: Demo ´ stenes Zegarra Rodrı ´ guez, Ogobuchi Daniel Okey, Siti Sarah\nMaidin.\nData curation: Ogobuchi Daniel Okey.\nFormal analysis: Ogobuchi Daniel Okey, Ekikere Umoren Udo, João Henrique Kleinschmidt.\nFunding acquisition: Siti Sarah Maidin.\nInvestigation: Demo ´ stenes Zegarra Rodrı ´ guez, Ogobuchi Daniel Okey, Ekikere Umoren Udo,\nJoão Henrique Kleinschmidt.\nMethodology: Demo ´ stenes Zegarra Rodrı ´ guez, Ekikere Umoren Udo, João Henrique\nKleinschmidt.\nProject administration: Siti Sarah Maidin, João Henrique Kleinschmidt.\nSoftware: Ogobuchi Daniel Okey, Ekikere Umoren Udo.\nSupervision: Siti Sarah Maidin, João Henrique Kleinschmidt.\nValidation: Siti Sarah Maidin, Ekikere Umoren Udo.\nVisualization: Siti Sarah Maidin, João Henrique Kleinschmidt.\nWriting – original draft: Ogobuchi Daniel Okey, Ekikere Umoren Udo, João Henrique\nKleinschmidt.\nWriting – review & editing: Demo ´ stenes Zegarra Rodrı ´ guez, Siti Sarah Maidin, João Henri-\nque Kleinschmidt.\nReferences\n1. Okey OD, Melgarejo DC, Saadi M, Rosa RL, Kleinschmi dt JH, Rodrı ´ guez DZ. Transfer Learning\nApproach to IDS on Cloud IoT devices using Optimized CNN. IEEE Access. 2023; 11:1023 –1038.\nhttps://doi.or g/10.110 9/ACCESS .2022.3233 775\n2. Umoren Udo E, Iroh C U, Nwaorgu O A, D Okey Ogobuchi. State of Internet of Things (IoT) Network\nand Rising Issues: A Review. NIPES Journal of Science and Technolog y Research. 2021; 3(3):10.\n3. Wang XW, Yonghong J, Zahra A, Laila M, Navid Y, Osama S. Federated deep learning for anomaly\ndetection in the internet of things; Elsevier: Comp uters and Electrica l Engineering . 2023; 108:108651.\n4. Basalan A. Intrusion Detection in IoT Systems Using Machine Learning Algorithm s. Southern Univer-\nsity and Agricultural and Mechanical College; 2020.\n5. Albulayhi K, Abu Al-Haija Q, Alsuhib any SA, Jillepalli AA, Ashrafuzza man M, Sheldon FT. IoT Intrusion\nDetection Using Machine Learning with a Novel High Perform ing Feature Selection Method. Applied\nSciences. 2022; 12(10):50 15. https://doi.or g/10.339 0/app1210501 5\n6. Liu X, Zhao M, Li S, Zhang F, Trappe W. A security framework for the internet of things in the future\ninternet architectur e. Future Interne t. 2017; 9(3):27. https://d oi.org/10.339 0/fi9030 027\n7. General Data Protecti on Regulatio n (GDPR)—Off icial Legal Text;. https://gdpr -info.eu/.\n8. Sharafaldi n I, Lashkari AH, Hakak S, Ghorbani AA. Developing Realistic Distributed Denial of Service\n(DDoS) Attack Dataset and Taxonomy . In: 2019 Interna tional Carnahan Conferen ce on Security Tech-\nnology (ICCS T); 2019. p. 1–8.\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 23 / 25\n9. Mebawondu OJ, Alowolodu OD, Adetunmb i AO, Mebawo ndu JO. Optimizin g the Classifica tion of Net-\nwork Intrusion Detecti on Using Ensemble s of Decision Trees Algorithm. In: Misra S, Muhamm ad-Bello\nB, editors. Information and Commun ication Technology and Applications . Cham: Springer Interna-\ntional Publishing; 2021. p. 286–300.\n10. Islam U, Muhammad A, Mansoor R, Hossain MS, Ahmad I, Eldin ET, et al. Detection of distribu ted\ndenial of service (DDoS) attacks in IOT based monitoring system of banking sector using machine\nlearning models. Sustainability . 2022; 14(14):837 4. https:// doi.org/10.33 90/su141 48374\n11. Okey OD, Maidin SS, Adasme P, Lopes Rosa R, Saadi M, Carrillo Melgare jo D, et al. BoostedEnM L:\nEfficient Technique for Detecti ng Cyberattack s in IoT System s Using Boosted Ensemble Machine\nLearning. Sensors. 2022; 22(19). https://doi.or g/10.339 0/s22197 409 PMID: 36236506\n12. Badama si UM, Khaliq S, Babalola O, Musa S, Iqbal T. A Deep Learning based approac h for DDoS\nattack detection in IoT-enabled smart environm ents. International Journal of Comp uter Networks and\nCommun ications Security. 2020; 8(10):93–9 9.\n13. Verma A, Ranga V. Machine learning based intrusio n detection systems for IoT applicatio ns. Wireless\nPersonal Commun ications. 2020; 111(4):228 7–2310 . https://doi.or g/10.1007/ s11277-019- 06986-8\n14. Latif S, Zou Z, Idrees Z, Ahmad J. A novel attack detection scheme for the industri al internet of things\nusing a lightweigh t random neural network. IEEE Access. 2020; 8:89337–8935 0. https://d oi.org/10.\n1109/ACCES S.2020 .2994079\n15. Khraisat A, Gondal I, Vample w P, Kamruzzaman J. Survey of intrusio n detection systems : techniques ,\ndatasets and challeng es. Cyberse curity. 2019; 2(1):1–22. https://doi.or g/10.118 6/s42400-019- 0038-7\n16. Alkanhel R, El-kenaw y ES, Abdelham id A, Ibrahim A, Alohali M, Abotaleb M, et al. Network Intrusion\nDetection Based on Feature Selectio n and Hybrid Metaheur istic Optimizati on. Computers, Materials\nand Continua. 2022; 74:2677 –2693. https://doi.o rg/10.32604/c mc.2023.033 273\n17. King J, Awad AI. A distribut ed security mechanism for resource -constrain ed IoT devices. Informatica .\n2016; 40(1).\n18. Abdi H, Williams LJ. Principal component analysis. Wiley interdis ciplinary reviews: computat ional statis-\ntics. 2010; 2(4):433–4 59. https:/ /doi.org/10.10 02/wics.1 01\n19. Fournier Q, Aloise D. Empirical comparison between autoenco ders and traditi onal dimensiona lity\nreduction methods . In: 2019 IEEE Second Interna tional Conferen ce on Artificial Intellige nce and Knowl-\nedge Engineeri ng (AIKE). IEEE; 2019. p. 211–214.\n20. Reis B, Maia E, Prac ¸ a I. Selection and perform ance analysis of CICID S2017 features importan ce. In:\nInternational Sympos ium on Founda tions and Practice of Security. Springer; 2019. p. 56–71.\n21. Abdulham med R, Musafer H, Alessa A, Faezipou r M, Abuzne id A. Features dimension ality reduction\napproaches for machine learning based network intrusion detection. Electro nics. 2019; 8(3):322.\nhttps://doi.or g/10.339 0/electronic s8030322\n22. Mane S, Rao D. Explaining network intrusio n detection system using explainable AI framework. arXiv\npreprint arXiv:210307 110. 2021;.\n23. Altmann A, Toloşi L, Sander O, Lengauer T. Permuta tion importan ce: a corrected feature importan ce\nmeasure. Bioinformat ics. 2010; 26(10):134 0–1347. https://doi.or g/10.109 3/bioinform atics/btq134\nPMID: 203857 27\n24. Nohara Y, Matsumoto K, Soejima H, Nakash ima N. Explanat ion of machine learning models using\nimproved Shapley Additive Explanation. In: Proceedings of the 10th ACM International Conferen ce on\nBioinformat ics, Comp utational Biology and Health Informatics ; 2019. p. 546–546.\n25. Hariharan S, Rejimol Robinson R, Prasad RR, Thomas C, Balakrishnan N. XAI for intrusion detection\nsystem: comparin g explanations based on global and local scope. Journal of Computer Virology and\nHacking Techniqu es. 2022; p. 1–23. https:// doi.org/10.10 07/s1141 6-022-004 41-2\n26. Anjomsho ae S, Kampik T, Fra ¨ mling K. Py-CIU: a python library for explaining machine learning predic-\ntions using contextual importan ce and utility. In: IJCAI-PR ICAI 2020 Workshop on Explainable Artificial\nIntelligenc e (XAI), january 8, 2020; 2020.\n27. Peltola T. Local Interpretable Model-ag nostic Explana tions of Bayesian Predictiv e Models via Kullback-\nLeibler Projections; 2018. Available from: https://arxiv. org/abs/1810. 02678.\n28. Molnar C. Interpretable machine learning. Lulu. com; 2020.\n29. Arik SO\n¨\n, Pfister T. Tabnet: Attentive interpretabl e tabular learning. In: Procee dings of the AAAI Confer-\nence on Artificial Intelligence. vol. 35; 2021. p. 6679–6687.\n30. Hassan WH, et al. Current research on Internet of Things (IoT) security: A survey. Comput er networks .\n2019; 148:283 –294. https://doi.or g/10.101 6/j.comnet.20 18.11.02 5\n31. Gu X, Zhang Z. IoT security and new trends of solutions. In: Introduction to Internet of Things in Man-\nagement Science and Operation s Research. Springer; 2021. p. 55–76.\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 24 / 25\n32. Kuzin M, Shmelev Y, Kuskov V. New trends in the world of IoT threats. Kasper sky Lab. 2018;.\n33. Kolias C, Kambou rakis G, Stavrou A, Voas J. DDoS in the IoT: Mirai and Other Botnets. Computer.\n2017; 50(7):80–8 4. https://doi.or g/10.1109/M C.2017.201\n34. Radanlie v P, De Roure DC, Nicoles cu R, Huth M, Montalvo RM, Cannady S, et al. Future develop ments\nin cyber risk assessmen t for the internet of things. Comp uters in Industry. 2018; 102:14– 22. https://doi.\norg/10.1016/ j.compind.2 018.08.002\n35. Susilo B, Sari RF. Intrusion Detecti on in IoT Networks Using Deep Learning Algorithm. Informatio n.\n2020; 11(5). https://doi.o rg/10.3390/in fo11050 279\n36. Guo W, Mu D, Xu J, Su P, Wang G, Xing X. Lemna: Explaining deep learning based security applica-\ntions. In: proceed ings of the 2018 ACM SIGSAC conferen ce on computer and communic ations security ;\n2018. p. 364–37 9.\n37. Dhaliwal S, Nahid A, Abbas R. Effective intrusion detection using XGBoos t. J Inf. 2018; 9(149):1–2 4.\n38. Belouch M, El Hadaj S, Idhamm ad M. Perform ance evaluatio n of intrusion detection based on machine\nlearning using Apache Spark. Procedia Comput er Science. 2018; 127:1–6. https://doi.or g/10.101 6/j.\nprocs.201 8.01.091\n39. S\nˇ\nkrlj B, Dz ˇ eroski S, Lavrač N, Petkovič M. Feature importance estima tion with self-atten tion networks .\narXiv preprint arXiv:200204 464. 2020;.\n40. Wojtas M, Chen K. Feature importance ranking for deep learning. Advances in Neural Informatio n Pro-\ncessing Systems. 2020; 33:5105 –5114.\n41. Shwartz-Zi v R, Armon A. Tabular data: Deep learning is not all you need. Informatio n Fusion. 2022;\n81:84–90. https://doi.or g/10.1016/ j.inffus.202 1.11.011\n42. Yang S. Research on network behavio r anomaly analysis based on bidirect ional LSTM. In: 2019 IEEE\n3rd Informatio n Technolog y, Networki ng, Electronic and Automatio n Control Conferen ce (ITNEC).\nIEEE; 2019. p. 798–80 2.\n43. Singh NB, Singh MM, Sarkar A, Mandal JK. A novel wide & deep transfer learning stacked GRU frame-\nwork for network intrusion detection. Journal of Informatio n Securit y and Applications . 2021;\n61:102899. https://doi.or g/10.1016/ j.jisa.2021.102 899\n44. Chen Y, Li J, Guo N. Efficient and interpreta ble SRU combined with TabNet for network intrusio n detec-\ntion in the big data environ ment. International Journal of Informatio n Securit y. 2022; p. 1–11. https://doi.\norg/10.1007/ s10207-022- 00656-w\n45. Yin C, Zhu Y, Fei J, He X. A deep learning approac h for intrusio n detection using recurrent neural net-\nworks. Ieee Access. 2017; 5:21954 –21961. https://doi. org/10.1109/A CCESS.201 7.27624 18\n46. Sharafaldi n I, Lashkari AH, Ghorbani AA. Toward generating a new intrusion detection dataset and\nintrusion traffic character ization. ICISSp. 2018; 1:108–1 16. https://doi.or g/10.522 0/0006639801 080116\n47. Lashkari AH, Seo A, Gil GD, Ghorbani A. CIC-AB: Online ad blocker for browsers. In: 2017 Interna tional\nCarnaha n Conference on Security Technolo gy (ICCST); 2017. p. 1–7.\n48. Martins A, Astudillo R. From softma x to sparsemax : A sparse model of attention and multi-label classifi-\ncation. In: International conferen ce on machine learning. PMLR; 2016. p. 1614–1623.\n49. Akiba T, Sano S, Yanase T, Ohta T, Koyama M. Optuna: A Next-ge neration Hyperparam eter Optimiza-\ntion Framework . In: Proceedings of the 25th ACM SIGKDD Internat ional Conference on Knowledge\nDiscover y and Data Mining; 2019.\nPLOS ONE\nIntrusion detection on IoT systems using automatic Xplainable feature selection\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02866 52 October 16, 2023 25 / 25",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8412114381790161
    },
    {
      "name": "Denial-of-service attack",
      "score": 0.7297521233558655
    },
    {
      "name": "Machine learning",
      "score": 0.7005047798156738
    },
    {
      "name": "Intrusion detection system",
      "score": 0.6783271431922913
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6594336628913879
    },
    {
      "name": "Deep learning",
      "score": 0.6087162494659424
    },
    {
      "name": "Feature selection",
      "score": 0.5623778700828552
    },
    {
      "name": "Usability",
      "score": 0.4347706437110901
    },
    {
      "name": "The Internet",
      "score": 0.3417779207229614
    },
    {
      "name": "Data mining",
      "score": 0.3329539895057678
    },
    {
      "name": "Operating system",
      "score": 0.1094723641872406
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1315085146",
      "name": "Universidade Federal de Lavras",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I71715416",
      "name": "Universidade Federal do ABC",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I12649496",
      "name": "INTI International University",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I98220444",
      "name": "Michael Okpara University of Agriculture",
      "country": "NG"
    }
  ]
}