{
  "title": "Spanish Legalese Language Model and Corpora",
  "url": "https://openalex.org/W3209776558",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4228057361",
      "name": "Gutiérrez-Fandiño, Asier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222692909",
      "name": "Armengol-Estapé, Jordi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286943466",
      "name": "Gonzalez-Agirre, Aitor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746277756",
      "name": "Villegas, Marta",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3180918883",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2915429162",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3030760979",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2963721344",
    "https://openalex.org/W2150102617",
    "https://openalex.org/W2126400076"
  ],
  "abstract": "There are many Language Models for the English language according to its worldwide relevance. However, for the Spanish language, even if it is a widely spoken language, there are very few Spanish Language Models which result to be small and too general. Legal slang could be think of a Spanish variant on its own as it is very complicated in vocabulary, semantics and phrase understanding. For this work we gathered legal-domain corpora from different sources, generated a model and evaluated against Spanish general domain tasks. The model provides reasonable results in those tasks.",
  "full_text": "arXiv:2110.12201v1  [cs.CL]  23 Oct 2021\nSPA N I S H LE G A L E S E LA N G UAG E MO D E L A N D CO R P O RA\nAsier Gutiérrez-Fandiño\nT ext Mining Unit\nBarcelona Supercomputing Center\nasier.gutierrez@bsc.es\nJordi Armengol-Estapé\nT ext Mining Unit\nBarcelona Supercomputing Center\njordi.armengol@bsc.es\nAitor Gonzalez-Agirre\nT ext Mining Unit\nBarcelona Supercomputing Center\naitor.gonzalez@bsc.es\nMarta Villegas\nT ext Mining Unit\nBarcelona Supercomputing Center\nmarta.villegas@bsc.es\nOctober 26, 2021\nABSTRACT\nThere are many Language Models for the English language acco rding to its worldwide relevance.\nHowever, for the Spanish language, even if it is a widely spok en language, there are very few Spanish\nLanguage Models which result to be small and too general. Leg al slang could be think of a Spanish\nvariant on its own as it is very complicated in vocabulary, se mantics and phrase understanding. For\nthis work we gathered legal-domain corpora from different s ources, generated a model and evaluated\nagainst Spanish general domain tasks. The model provides re asonable results in those tasks.\n1 Introduction\nLegal Spanish (or Spanish Legalese) is a complex slang that i s away from the language spoken by the society.\nLanguage Models, generally, are pre-trained on large corpo ra for later ﬁne-tuning them on different tasks. Language\nModels are widely used due to their transfer learning capabi lities. If the corpora used for training the Language Models\nare aligned with the domain of the tasks they provide better r esults.\nIn this work we gathered different corpora and we trained a La nguage Model for the Spanish Legal domain.\n2 Corpora\nOur corpora comprises multiple digital resources and it has a total of 8.9GB of textual data. Part of it has been obtained\nfrom previous work [9]. T able 1 shows different resources ga thered. Most of the corpora were scraped, some of them\nin PDF format. W e then transformed and cleaned the data. Othe r corpora like the COPP A 1 patents corpus were\nrequested.\nAs a contribution of this work we publish all publishable cor pora we gathered in Zenodo 2.\n3 Model\nW e trained a RoBER T a [7] base model, using the hyper-paramet ers proposed in the original work. As vocabulary, we\nused Byte-Level BPE or training, we use the Fairseq [8] libra ry, and for ﬁne-tuning, Huggingface Transformers [12],\nbut with a vocabulary size of 52,262. For training, we used th e Fairseq [8] library, and for ﬁne-tuning, Huggingface\n1 https://www.wipo.int/export/sites/www/patentscope/en/data/pdf/wipo-coppa-technicalDocumentation.pdf\n2 https://zenodo.org/record/5495529\nA P RE P RIN T - O CTO BE R 26, 2021\nCorpus name Size (GB) T okens (M)\nProcesos Penales 0.625 0.119\nJRC Acquis 0.345 59.359\nCódigos Electrónicos Universitarios 0.077 11.835\nCódigos Electrónicos 0.080 12.237\nDoctrina de la Fiscalía General del Estado 0.017 2.669\nLegislación BOE 3.600 578.685\nAbogacía del Estado BOE 0.037 6.123\nConsejo de Estado: Dictámenes 0.827 135.348\nSpanish EURLEX 0.001 0.072\nUN Resolutions 0.023 3539.000\nSpanish DOGC 0.826 132.569\nSpanish MultiUN 2.200 352.653\nConsultas Tributarias Generales y V inculantes 0.466 77.69 1\nConstitución Española 0.002 0.018\nCOPP A Patents Corpus 0.002 -\nBiomedical Patents 0.083 -\nT able 1: List of individual sources constituting the legal c orpus. The number of tokens refers to white-spaced tokens\ncalculated on cleaned untokenized text.\nTransformers [12]. W e trained the model until convergence w ith 8 Nvidia T esla V100 GPUs with 16GB of VRAM. The\nmodel was trained with a peak learning rate of 0.0005 and 2,04 8 of batch size. The model is available in HuggingFace. 3\n4 Embeddings\nAdditionally, following previous work in the Natural Langu age Processing for Spanish Legal texts [9] we computed\nFastT ext word and subword embeddings, with 50, 100 and 300 di mensions, using CBOW and Skip-gram methods.\nFor the word embeddings, we computed both cased and uncased w ord embeddings, and for the subword embeddings\nwe computed Byte-level Byte-Pair-Encoding (BBPE) embeddi ngs with 30k vocabulary size. The embeddings can be\nfreely downloaded from Zenodo 4.\n5 Evaluation\nW e compare our RoBER T alex model with the Spanish RoBER T a-ba se (RoBER T a-b) [5] and multilingual BER T\n(mBER T) [4]. Due to the lack of domain speciﬁc evaluation dat a, he models are evaluated on general domains tasks,\nwhere RoBER T alex obtains reasonable performance. W e ﬁne-t uned each model in the following tasks:\n• Part of Speech from Universal Dependencies 5 (UD-POS).\n• Named Entity Recognition from Conll2002 (Conll-NER) [11] .\n• Part of Speech from the Capitel Corpus (Capitel-POS). 6\n• Named Entity Recognition from the Capitel Corpus (Capitel -NER).7\n• Semantic T extual Similarity (STS) from 2014 [2] and 2015 [1 ].\n• The Multilingual Document Classiﬁcation Corpus (MLDoc) [ 10, 6].\n• The Cross-lingual Adversarial Dataset for Paraphrase Ide ntiﬁcation (P A WS-X) [13].\n• The Cross-Lingual NLI Corpus (XNLI) [3].\nT able 2 shows the evaluation results of the three models. RoB ER T alex was evaluated with a ﬁxed set of hyper-\nparameters, while the results reported in [5] were obtained by conducting a grid search and picking the best value\n3 https://huggingface.co/BSC-TeMU/RoBERTalex\n4 https://zenodo.org/record/5036147\n5 https://universaldependencies.org/\n6 https://sites.google.com/view/capitel2020#h.p_eFTF8UCJXFMq\n7 https://sites.google.com/view/capitel2020#h.p_CbqX2kG3XEIp\n2\nA P RE P RIN T - O CTO BE R 26, 2021\nbased on the development set. W e plan to evaluate RoBER T alex using the same grid search in order to make the\nresults fully comparable, and also we plan to evaluate the mo del in domain-speciﬁc tasks.\nDataset Metric RoBER T alex RoBER T a-b mBER T\nUD-POS F1 0.9871 0.9907 0.9886\nConll-NER F1 0.8323 0.8851 0.8691\nCapitel-POS F1 0.9788 0.9846 0.9839\nCapitel-NER F1 0.8394 0.8960 0.8810\nSTS Combined 0.7374 0.8533 0.8164\nMLDoc Accuracy 0.9417 0.9623 0.9550\nP A WS-X F1 0.7304 0.9000 0.8955\nXNLI Accuracy 0.7337 0.8016 0.7876\nT able 2: Evaluation table of models.\n6 Conclusions & Future W ork\nOur language model is, to our knowledge, the ﬁrst of its kind ( Spanish legal domain). W e extensively evaluated our\nmodel by performing general domain evaluation. Results sho w that it behaves reasonably positive in general domain.\nW e are planning to gather more resources, try to continue the pre-training of the models from [5] with legal domain\nand train generative models.\nAcknowledgements\nThis work was funded by the Spanish State Secretariat for Dig italization and Artiﬁcial Intelligence (SEDIA) within\nthe framework of the Plan-TL.\nReferences\n[1] E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzale z-Agirre, W . Guo, I. Lopez-Gazpio, M. Maritxalar,\nR. Mihalcea, et al. Semeval-2015 task 2: Semantic textual si milarity, english, spanish and pilot on interpretability.\nIn Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 252–263, 2015.\n[2] E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzale z-Agirre, W . Guo, R. Mihalcea, G. Rigau, and\nJ. Wiebe. Semeval-2014 task 10: Multilingual semantic text ual similarity. In Proceedings of the 8th international\nworkshop on semantic evaluation (SemEval 2014), pages 81–91, 2014.\n[3] A. Conneau, R. Rinott, G. Lample, A. Williams, S. R. Bowma n, H. Schwenk, and V . Stoyanov. Xnli: Evaluating\ncross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational Linguistics, 2018.\n[4] J. Devlin, M. Chang, K. Lee, and K. T outanova. BER T: pre-t raining of deep bidirectional transformers for\nlanguage understanding. CoRR, abs/1810.04805, 2018.\n[5] A. Gutiérrez-Fandiño, J. Armengol-Estapé, M. Pàmies, J . Llop-Palao, J. Silveira-Ocampo, C. P . Carrino,\nA. Gonzalez-Agirre, C. Armentano-Oller, C. Rodriguez-Pen agos, and M. V illegas. Spanish language models,\n2021.\n[6] D. D. Lewis, Y . Y ang, T . Russell-Rose, and F . Li. Rcv1: A ne w benchmark collection for text categorization\nresearch. Journal of machine learning research, 5(Apr):361–397, 2004.\n[7] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M . Lewis, L. Zettlemoyer, and V . Stoyanov. Roberta:\nA robustly optimized bert pretraining approach, 2019.\n[8] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Gr angier, and M. Auli. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.\n[9] D. Samy, J. Arenas-García, and D. Pérez-Fernández. Lega l-ES: A set of large scale resources for Spanish legal\ntext processing. In Proceedings of the 1st W orkshop on Language T echnologies for Government and Public Ad-\nministration (LT4Gov), pages 32–36, Marseille, France, May 2020. European Langua ge Resources Association.\n3\nA P RE P RIN T - O CTO BE R 26, 2021\n[10] H. Schwenk and X. Li. A corpus for multilingual document classiﬁcation in eight languages. In N. C. C.\nchair), K. Choukri, C. Cieri, T . Declerck, S. Goggi, K. Hasid a, H. Isahara, B. Maegaard, J. Mariani, H. Mazo,\nA. Moreno, J. Odijk, S. Piperidis, and T . T okunaga, editors, Proceedings of the Eleventh International Conference\non Language Resources and Evaluation (LREC 2018), Paris, France, may 2018. European Language Resources\nAssociation (ELRA).\n[11] E. F . Tjong Kim Sang. Introduction to the CoNLL-2002 sha red task: Language-independent named entity\nrecognition. In COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002), 2002.\n[12] T . W olf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. M oi, P . Cistac, T . Rault, R. Louf, M. Funtow-\nicz, J. Davison, S. Shleifer, P . von Platen, C. Ma, Y . Jernite , J. Plu, C. Xu, T . L. Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. M. Rush. Transformers: State-of-the-art n atural language processing. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45,\nOnline, Oct. 2020. Association for Computational Linguist ics.\n[13] Y . Y ang, Y . Zhang, C. T ar, and J. Baldridge. P A WS-X: A Cro ss-lingual Adversarial Dataset for Paraphrase\nIdentiﬁcation. In Proc. of EMNLP, 2019.\n4",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7299126982688904
    },
    {
      "name": "Natural language processing",
      "score": 0.6765439510345459
    },
    {
      "name": "Linguistics",
      "score": 0.6722728610038757
    },
    {
      "name": "Phrase",
      "score": 0.6280461549758911
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5786837339401245
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5558722615242004
    },
    {
      "name": "Relevance (law)",
      "score": 0.5528337359428406
    },
    {
      "name": "Language model",
      "score": 0.5509402751922607
    },
    {
      "name": "Vocabulary",
      "score": 0.5121057033538818
    },
    {
      "name": "Slang",
      "score": 0.5072585940361023
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4600375294685364
    },
    {
      "name": "Universal Networking Language",
      "score": 0.41164571046829224
    },
    {
      "name": "Natural language",
      "score": 0.24137097597122192
    },
    {
      "name": "Comprehension approach",
      "score": 0.24046757817268372
    },
    {
      "name": "Philosophy",
      "score": 0.08638840913772583
    },
    {
      "name": "Programming language",
      "score": 0.0733698308467865
    },
    {
      "name": "Mathematics",
      "score": 0.06678763031959534
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799803557",
      "name": "Barcelona Supercomputing Center",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I9617848",
      "name": "Universitat Politècnica de Catalunya",
      "country": "ES"
    }
  ],
  "cited_by": 4
}