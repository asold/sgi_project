{
    "title": "Bidirectional Language Modeling: A Systematic Literature Review",
    "url": "https://openalex.org/W3158542464",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2904482025",
            "name": "Muhammad Shah Jahan",
            "affiliations": [
                "National University of Sciences and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2114587322",
            "name": "Habib-ullah Khan",
            "affiliations": [
                "Qatar University"
            ]
        },
        {
            "id": "https://openalex.org/A2138583854",
            "name": "Shahzad Akbar",
            "affiliations": [
                "Riphah International University"
            ]
        },
        {
            "id": "https://openalex.org/A2103232614",
            "name": "Muhammad Umar Farooq",
            "affiliations": [
                "National University of Sciences and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2409891037",
            "name": "Sarah Gul",
            "affiliations": [
                "International Islamic University, Islamabad"
            ]
        },
        {
            "id": "https://openalex.org/A2123079363",
            "name": "Anam Amjad",
            "affiliations": [
                "National University of Sciences and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2153579005",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2607892599",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2799269579",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2922551710",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W6767869522",
        "https://openalex.org/W2979860911",
        "https://openalex.org/W2991788499",
        "https://openalex.org/W2946345909",
        "https://openalex.org/W2965210982",
        "https://openalex.org/W6763240421",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2790235966",
        "https://openalex.org/W6759579507",
        "https://openalex.org/W2921848006",
        "https://openalex.org/W3000003809",
        "https://openalex.org/W6772951152",
        "https://openalex.org/W3002784191",
        "https://openalex.org/W3010362629",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W2966892770",
        "https://openalex.org/W2974875810",
        "https://openalex.org/W2914526845",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W3001925365",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3035204084",
        "https://openalex.org/W2996564207",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W3005444338",
        "https://openalex.org/W2952603081",
        "https://openalex.org/W6762715600",
        "https://openalex.org/W2937297214",
        "https://openalex.org/W2982111970",
        "https://openalex.org/W3000103182",
        "https://openalex.org/W6771565904",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2971871542",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W2986367395",
        "https://openalex.org/W6692563993",
        "https://openalex.org/W3006963874",
        "https://openalex.org/W3034560159",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2963453233",
        "https://openalex.org/W2945667196",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2963703197",
        "https://openalex.org/W3110557757",
        "https://openalex.org/W2998230451",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W3101248447",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3006401393",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2995923603",
        "https://openalex.org/W2994950732",
        "https://openalex.org/W2168894761",
        "https://openalex.org/W3036267641",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4287824654"
    ],
    "abstract": "In transfer learning, two major activities, i.e., pretraining and fine-tuning, are carried out to perform downstream tasks. The advent of transformer architecture and bidirectional language models, e.g., bidirectional encoder representation from transformer (BERT), enables the functionality of transfer learning. Besides, BERT bridges the limitations of unidirectional language models by removing the dependency on the recurrent neural network (RNN). BERT also supports the attention mechanism to read input from any side and understand sentence context better. It is analyzed that the performance of downstream tasks in transfer learning depends upon the various factors such as dataset size, step size, and the number of selected parameters. In state-of-the-art, various research studies produced efficient results by contributing to the pretraining phase. However, a comprehensive investigation and analysis of these research studies is not available yet. Therefore, in this article, a systematic literature review (SLR) is presented investigating thirty-one (31) influential research studies published during 2018â€“2020. Following contributions are made in this paper: (1) thirty-one (31) models inspired by BERT are extracted. (2) Every model in this paper is compared with RoBERTa (replicated BERT model) having large dataset and batch size but with a small step size. It is concluded that seven (7) out of thirty-one (31) models in this SLR outperforms RoBERTa in which three were trained on a larger dataset while the other four models are trained on a smaller dataset. Besides, among these seven models, six models shared both feedforward network (FFN) and attention across the layers. Rest of the twenty-four (24) models are also studied in this SLR with different parameter settings. Furthermore, it has been concluded that a pretrained model with a large dataset, hidden layers, attention heads, and small step size with parameter sharing produces better results. This SLR will help researchers to pick a suitable model based on their requirements.",
    "full_text": null
}