{
  "title": "TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding",
  "url": "https://openalex.org/W3010714856",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2062399912",
      "name": "Huang, Zhiheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098197978",
      "name": "Xu Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4306704779",
      "name": "Liang, Davis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2312993823",
      "name": "Mishra Ajay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2172647656",
      "name": "Xiang Bing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1566289585",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W1499864241",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2963174729",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2969515962",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2143612262"
  ],
  "abstract": "Bidirectional Encoder Representations from Transformers (BERT) has recently achieved state-of-the-art performance on a broad range of NLP tasks including sentence classification, machine translation, and question answering. The BERT model architecture is derived primarily from the transformer. Prior to the transformer era, bidirectional Long Short-Term Memory (BLSTM) has been the dominant modeling architecture for neural machine translation and question answering. In this paper, we investigate how these two modeling techniques can be combined to create a more powerful model architecture. We propose a new architecture denoted as Transformer with BLSTM (TRANS-BLSTM) which has a BLSTM layer integrated to each transformer block, leading to a joint modeling framework for transformer and BLSTM. We show that TRANS-BLSTM models consistently lead to improvements in accuracy compared to BERT baselines in GLUE and SQuAD 1.1 experiments. Our TRANS-BLSTM model obtains an F1 score of 94.01% on the SQuAD 1.1 development dataset, which is comparable to the state-of-the-art result.",
  "full_text": "TRANS-BLSTM: Transformer with Bidirectional LSTM for Language\nUnderstanding\nZhiheng Huang\nAmazon AWS AI\nzhiheng@amazon.com\nPeng Xu\nAmazon AWS AI\npengx@amazon.com\nDavis Liang\nAmazon AWS AI\nliadavis@amazon.com\nAjay Mishra\nAmazon AWS AI\nmisaja@amazon.com\nBing Xiang\nAmazon AWS AI\nbxiang@amazon.com\nAbstract\nBidirectional Encoder Representations from\nTransformers (BERT) has recently achieved\nstate-of-the-art performance on a broad range\nof NLP tasks including sentence classiﬁca-\ntion, machine translation, and question answer-\ning. The BERT model architecture is de-\nrived primarily from the transformer. Prior to\nthe transformer era, bidirectional Long Short-\nTerm Memory (BLSTM) has been the domi-\nnant modeling architecture for neural machine\ntranslation and question answering. In this pa-\nper, we investigate how these two modeling\ntechniques can be combined to create a more\npowerful model architecture. We propose\na new architecture denoted as Transformer\nwith BLSTM (TRANS-BLSTM) which has a\nBLSTM layer integrated to each transformer\nblock, leading to a joint modeling framework\nfor transformer and BLSTM. We show that\nTRANS-BLSTM models consistently lead to\nimprovements in accuracy compared to BERT\nbaselines in GLUE and SQuAD 1.1 experi-\nments. Our TRANS-BLSTM model obtains\nan F1 score of 94.01% on the SQuAD 1.1 de-\nvelopment dataset, which is comparable to the\nstate-of-the-art result.\n1 Introduction\nLearning representations (Mikolov et al., 2013) of\nnatural language and language model pre-training\n(Devlin et al., 2018; Radford et al., 2019) has\nshown promising results recently. These pre-\ntrained models serve as generic up-stream models\nand they can be used to improve down-stream ap-\nplications such as natural language inference, para-\nphrasing, named entity recognition, and question\nanswering. The innovation of BERT (Devlin et al.,\n2018) comes from the “masked language model”\nwith a pre-training objective, inspired by the Cloze\ntask (Taylor, 1953). The masked language model\nrandomly masks some of the tokens from the input,\nand the objective is to predict the original token\nbased only on its context.\nFollow-up work including RoBERTa (Liu\net al., 2019b) investigated hyper-parameter design\nchoices and suggested longer model training time.\nIn addition, XLNet (Yang et al., 2019) has been\nproposed to address the BERT pre-training and\nﬁne-tuning discrepancy where masked tokens were\nfound in the former but not in the latter. Nearly\nall existing work suggests that a large network is\ncrucial to achieve the state-of-the-art performance.\nFor example, (Devlin et al., 2018) has shown that\nacross natural language understanding tasks, using\nlarger hidden layer size, more hidden layers, and\nmore attention heads always leads to better perfor-\nmance. However, they stop at a hidden layer size\nof 1024. ALBERT (Lan et al., 2019) showed that\nit is not the case that simply increasing the model\nsize would lead to better accuracy. In fact, they\nobserved that simply increasing the hidden layer\nsize of a model such as BERT-large can lead to sig-\nniﬁcantly worse performance. On the other hand,\nmodel distillation (Hinton et al., 2015; Tang et al.,\n2019; Sun et al., 2019; Sanh et al., 2019) has been\nproposed to reduce the BERT model size while\nmaintaining high performance.\nIn this paper, we attempt to improve the per-\nformance of BERT via architecture enhancement.\nBERT is based on the encoder of the trans-\nformer model (Vaswani et al., 2017), which has\nbeen proven to obtain state-of-the-art accuracy\nacross a broad range of NLP applications (Devlin\net al., 2018). Prior to BERT, bidirectional LSTM\n(BLSTM) has dominated sequential modeling for\nmany tasks including machine translation (Chiu\nand Nichols, 2016) and speech recognition (Graves\net al., 2013). Given both models have demonstrated\nsuperior accuracy on various benchmarks, it is nat-\nural to raise the question whether a combination of\nthe transformer and BLSTM can outperform each\narXiv:2003.07000v1  [cs.CL]  16 Mar 2020\nindividual architecture. In this paper, we attempt\nto answer this question by proposing a transformer\nBLSTM joint modeling framework. Our major con-\ntribution in this paper is two fold: 1) We propose\nthe TRANS-BLSTM model architectures, which\ncombine the transformer and BLSTM into one sin-\ngle modeling framework, leveraging the modeling\ncapability from both the transformer and BLSTM.\n2) We show that the TRANS-BLSTM models can\neffectively boost the accuracy of BERT baseline\nmodels on SQuAD 1.1 and GLUE NLP benchmark\ndatasets.\n2 Related work\n2.1 BERT\nOur work focuses on improving the transformer ar-\nchitecture (Vaswani et al., 2017), which motivated\nthe recent breakthrough in language representa-\ntion, BERT (Devlin et al., 2018). Our work builds\non top of the transformer architecture, integrating\neach transformer block with a bidirectional LSTM\n(Hochreiter and Schmidhuber, 1997). Related to\nour work, XLNet (Yang et al., 2019) proposes two-\nstream self-attention as opposed to single-stream\nself-attention used in classic transformers. With\ntwo-stream attention, XLNet can be treated as a\ngeneral language model that does not suffer from\nthe pretrain-ﬁnetune discrepancy (the mask tokens\nare seen during pretraining but not during ﬁnetun-\ning) thanks to its autoregressive formulation. Our\nmethod overcomes this limitation with a different\napproach, using single-stream self-attention with\nan integrated BLSTM layer for each transformer\nlayer.\n2.2 Bidirectional LSTM\nThe LSTM network (Hochreiter and Schmidhu-\nber, 1997) has demonstrated powerful modeling\ncapability in sequential learning tasks including\nnamed entity tagging (Huang et al., 2015; Chiu\nand Nichols, 2016), machine translation (Bahdanau\net al., 2015; Wu et al., 2016) and speech recogni-\ntion (Graves et al., 2013; Sak et al., 2014). The\nmotivation of this paper is to integrate bidirectional\nLSTM layers to the transformer model to further\nimprove transformer performance. The work of\n(Tang et al., 2019) attempts to distill a BERT model\nto a single-layer bidirectional LSTM model. It is\nrelevant to our work as both utilizing bidirectional\nLSTM. However, their work leads to inferior accu-\nracy compared to BERT baseline models. Similar\nto their observation, we show that in our experi-\nments, the use of BLSTM model alone (even with\nmultiple stacked BLSTM layers) leads to signif-\nicantly worse results compared to BERT models.\nHowever, our proposed joint modeling framework,\nTRANS-BLSTM, is able to boost the accuracy of\nthe transformer BERT models.\n2.3 Combine Recurrent Network and\nTransformer\nPrevious work has explored the combination of the\nrecurrent network and transformer. For example,\n(Lei et al., 2018) has substituted the feedforward\nnetwork in transformer with the simple recurrent\nunit (SRU) implementation and achieved better ac-\ncuracy in machine translation. It is similar to one\nof the proposed models in this paper. However, the\ndifference is that our paper investigates the gain\nof the combination in BERT pre-training context,\nwhile their paper focused on the parallelization\nspeedup of SRU in machine translation encoder\nand decoder context.\n3 TRANS and Proposed\nTRANS-BLSTM Architectures\nIn this section, we ﬁrst review the transformer archi-\ntecture, then propose the transformer bidirectional\nLSTM network architectures (TRANS-BLSTM),\nwhich integrates the BLSTM to either the trans-\nformer encoder or decoder.\n3.1 Transformer architecture (TRANS)\nThe BERT model consists of a transformer en-\ncoder (Vaswani et al., 2017) as shown in Figure\n1. The original transformer architecture uses mul-\ntiple stacked self-attention layers and point-wise\nfully connected layers for both the encoder and de-\ncoder. However, BERT only leverages the encoder\nto generate hidden value representation and the\noriginal transformer decoder (for generating text\nin neural machine translation etc.) is replaced by\na linear layer followed by a softmax layer, shown\nin Figure 1, both for sequential classiﬁcation tasks\n(named entity tagging, question answering) and\nsentence classiﬁcation tasks (sentiment classiﬁca-\ntion etc.). The encoder is composed of a stack of\nN = 12or N = 24layers for the BERT-base and\n-large cases respectively. Each layer consists of two\nsub-layers. The ﬁrst sub-layer is a multi-head self-\nattention mechanism, and the second sub-layer is a\nsimple, position-wise fully connected feed-forward\nInputs\nEncoding\nSoftmax\nFeed Forward\nAdd & Norm\n~\nInput Embedding\nN Layer\nEncoder\nAdd & Norm\nMulti−head\nAttention\nLinear\nDecoder\nOutput probabilities\nPositional\nFigure 1: Transformer architecture.\nnetwork. (Vaswani et al., 2017) employs a residual\nconnection (He et al., 2016) around each of the\ntwo sub-layers, followed by layer normalization\n(Ba et al., 2016). That is, the output of each sub-\nlayer is LayerNorm (x + Sublayer(x)), where\nSublayer(x) is the function implemented by the\nsub-layer itself. To facilitate these residual connec-\ntions, all sub-layers in the model, as well as the\nembedding layers, produce outputs of dimension\nof 768 and 1024 for BERT-base and BERT-large,\nrespectively. We used the same multi-head self-\nattention from the original paper (Vaswani et al.,\n2017). We used the same input and output repre-\nsentations, i.e., the embedding and positional en-\ncoding, and the same loss objective, i.e., masked\nLM prediction and next sentence prediction, from\nthe BERT paper (Devlin et al., 2018).\n3.2 Proposed transformer bidirectional\nLSTM (TRANS-BLSTM) architectures\nPrevious experiments indicated that a bidirectional\nLSTM model alone may not perform on par with\na transformer. For example, the distillation from\na transformer model to a single-layer bidirectional\nLSTM model (Tang et al., 2019) resulted in signiﬁ-\ncantly lower accuracy. We also conﬁrmed this on\nour experiments in Section 4.3. In this paper, we\nhypothesize that the transformer and bidirectional\nLSTM may be complementary in sequence model-\ning. We are motivated to investigate how a bidirec-\ntional LSTM can further improve accuracy in down-\nstream tasks relative to a classic transformer model.\nFigure 2 shows the two proposed Transformer with\nBidirectional LSTM architectures (denoted as the\nTRANS-BLSTM-1 and TRANS-BLSTM-2) mod-\nels respectively:\nTRANS-BLSTM-1 For each BERT layer, we re-\nplace the feedforward layer with a bidirec-\ntional LSTM layer.\nTRANS-BLSTM-2 We add a bidirectional LSTM\nlayer which takes the same input as the orig-\ninal BERT layer. The output of the bidirec-\ntional LSTM layer is summed up with the\noriginal BERT layer output (before the Layer-\nNorm).\nThe motivation of adding BLSTM is to integrate\nthe self-attention and bidirectional LSTM to pro-\nduce a better joint model framework (as we will\nsee in the experiments later). We found that these\ntwo architectures lead to similar accuracy in our ex-\nperiments (see Section 4.5). We thus focus on the\nlatter (TRANS-BLSTM-2) and refer to this model\nas TRANS-BLSTM henceforth for simplicity. For\nboth architectures, if we use the same number of\nBLSTM hidden units as in the BERT model H, we\nobtain the BLSTM output with dimension of 2H,\nand we therefore need a linear layer to project the\noutput of the BLSTM (with dimensionality 2H) to\nH in order to match the transformer output. Alter-\nnatively, if we set the number of BLSTM hidden\nunits to H/2 (we denote this model as TRANS-\nBLSTM-SMALL), we need not include an addi-\ntional projection layer.\n3.3 Adding bidirectional LSTM to\ntransformer decoder\nWhile the above method adds bidirectional LSTM\nlayers to a transformer encoder, we can in addition\nreplace the linear layer with bidirectional LSTM\nlayers in decoder. The number of bidirectional\nLSTM layers is a hyper parameter to tune; we use 2\nin this paper. While the bidirectional LSTM layers\nin encoder help the pre-training task for the masked\nlanguage model and next sentence prediction task,\nthe bidirectional LSTM in decoder may help in\ndownstream sequential prediction tasks such as\nquestion answering.\n3.4 Objective functions\nFollowing the BERT (Devlin et al., 2018), we use\nmasked language model loss and next sentence\nprediction (NSP) loss to train the models.\nOutput probabilities\nEncoding\nAdd & Norm\n~\nInput Embedding\nN Layer\nEncoder\nMulti−head\nAttention\nFeed Forward\nAdd & Norm\nPositional\nEncoding\nAdd & Norm\n~\nInput Embedding\nN Layer\nEncoder\nMulti−head\nAttention\nSoftmax\nAdd & Norm\nSoftmax\nBLSTM\nLinear\nBLSTM\nLinear\nInputsInputs\nLinear\nDecoder\nOutput probabilities\nLinear\nDecoder\nPositional\nFigure 2: Two transformer with bidirectional LSTM architectures. The left one, TRANS-BLSTM-1, replaces the\nfeedforward layer with BLSTM layer and the right , TRANS-BLSTM-2, adds a BLSTM layer in parallel.\nThe masked LM (MLM) is often referred to as\na Cloze task in the literature (Taylor, 1953). The\nencoder output, corresponding to the mask tokens,\nare fed into an output softmax over the vocabulary.\nIn our experiments, we randomly mask 15% of all\nwhole word wordpiece tokens in each sequence\n(Wu et al., 2016).\nWe also use the next sentence prediction loss\nas introduced in (Devlin et al., 2018) to train our\nmodels. Speciﬁcally, when choosing the sentences\nA and B for each pre-training example, 50% of the\ntime B is the actual next sentence that follows A,\nand 50% of the time it is a random sentence from\nthe corpus. We note that recent work (Yang et al.,\n2019; Liu et al., 2019a; Lan et al., 2019; Raffel\net al., 2019) has argued that the NSP loss may not\nbe useful in improving model accuracy. Never-\ntheless, we used the NSP loss in our experiments\nto have a fair comparison between the proposed\nmodels and the original BERT models.\n3.5 Model parameters\nTable 1 shows the model parameter size and\ntraining speedup for TRANS/BERT (TRANS and\nBERT are exchangeable in this paper), TRANS-\nBLSTM-SMALL, and TRANS-BLSTM respec-\ntively. Here, the TRANS-BLSTM-SMALL and\nTRANS-BLSTM models are 50% and 100% larger\nthan the TRANS model (base, large) respec-\ntively. Consequently, TRANS-BLSTM-SMALL\nand TRANS-BLSTM models require more com-\nputational resources and longer training times\ncompared to the vanilla transformer model. The\nslowest-training model is the TRANS-BLSTM\nwhich is also our baseline. Models with fewer\nparameters can train faster. For example, the large\nTRANS/BERT model boasts a 2.8 fold speedup\ncompared to the TRANS-BLSTM large model. We\nnote that the focus of the paper is to investigate\nwhether a joint transformer and BLSTM architec-\nture can further improve the performance over a\ntransformer baseline. This is important to keep in\nmind because simply increasing the number of hid-\nden units in BERT-large is not enough to positively\naffect accuracy (Lan et al., 2019) (also see Section\n4.6).\n4 Experiments\n4.1 Experimental setup\nWe use the same large-scale data which has been\nused for BERT model pre-training, the BooksCor-\npus (800M words) (Zhu et al., 2015) and English\nWikipedia (2.5B words) (Wikipedia contributors,\n2004; Devlin et al., 2018). The two corpora con-\nsist of about 16GB of text. Following the origi-\nnal BERT setup (Devlin et al., 2018), we format\nthe inputs as “[CLS] x1 [SEP] x2 [SEP]”, where\nModel Parameters (M) Layers Hidden Embedding Heads Speedup\nTRANS/BERT 108M 12 768 768 12 6.0X\nBase TRANS-BLSTM-SMALL 152M 12 768 768 12 3.3X\nTRANS-BLSTM 237M 12 768 768 12 2.5X\nLarge TRANS/BERT 334M 24 1024 1024 16 2.8X\nTRANS-BLSTM-SMALL 487M 24 1024 1024 16 1.4X\nTRANS-BLSTM 789M 24 1024 1024 16 1\nTable 1: Parameter size and training speed for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM\non base and large settings respectively.\nx1 = x11, x12 . . .and x2 = x21, x22 . . .are two\nsegments. To reduce the training memory con-\nsumption, we set the maximum input length to 256\n(as opposed to 512 in the original BERT paper).\nWe note that this setting may adversely affect the\nbest accuracy we report in our paper 1, but the rela-\ntive accuracy gain by the proposed models are still\nvalid. Similar to BERT, we use a vocabulary size\nof 30k with wordpiece tokenization.\nWe generate the masked input from the MLM\ntargets using unigram masking, which is denoted as\nwhole word masking. That is, each masking applies\nto a whole word at one time. We note that using\nn-gram masking (for example, with n = 3) (Joshi\net al., 2019; Lan et al., 2019) with the length of\neach n-gram mask selected randomly can further\nimprove the downstream task accuracy (for exam-\nple, 2% F1 score increase was observed on SQuAD\n1.1 data set with n-gram masking and span bound-\nary representation prediction (Joshi et al., 2019)).\nHowever, in the whole word masking setting, we\nare able to fairly compare the proposed TRANS-\nBLSTM models to the original BERT models. Sim-\nilar to (Devlin et al., 2018), the training data gener-\nator chooses 15% of the token positions at random\nfor making. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time.\nThe model updates use a batch size of 256 and\nAdam optimizer with learning rate starting from 1e-\n4. Training was done on a cluster of nodes, where\neach node consists of 8 Nvidia Tesla V100 GPUs.\nWe vary the node size from 1 to 8 depending on the\nmodel size. Our TRANS-BLSTM is implemented\non top of Pytorch transformer repository 2.\n1Nevertheless, our implementation of baseline BERT\nmodel obtained higher accuracy than that reported by the\noriginal BERT paper (Devlin et al., 2018).\n2https://github.com/huggingface/pytorch-transformers.\n4.2 Downstream evaluation datasets\nFollowing the previous work (Devlin et al., 2018;\nYang et al., 2019; Liu et al., 2019a; Lan et al.,\n2019), we evaluate our models on the General Lan-\nguage Understanding Evaluation (GLUE) bench-\nmark (Wang et al., 2018) and the Stanford Question\nAnswering Dataset (SQuAD 1.1) (Rajpurkar et al.,\n2016). GLUE is the General Language Understand-\ning Evaluation benchmark consisting of a diverse\ncollection of natural language understanding tasks.\nGLUE is model-agnostic and the tasks are selected\nto incentivize the development of general and ro-\nbust NLU systems. The tasks included in GLUE\nare (1) Multi-Genre Natural Language Inference\n(MNLI) for sentence entailment classiﬁcation, (2)\nQuora Question Pairs (QQP) for semantic equiva-\nlence classiﬁcation, (3) Question Natural Language\nInference (QNLI) for predicting whether the sen-\ntence in a query-sentence pair contains a correct an-\nswer, (4) Stanford Sentiment Treebank (SST-2) for\nsentiment analysis of movie reviews, (5) Corpus of\nLinguistic Acceptability (CoLA) for determining\nwhether an English sentence is linguistically accept-\nable, (6) Semantic Textual Similarity (STS-B). The\nStanford Question Answering Dataset (SQuAD) is\na corpus consisting of 100k question/answer pairs\nsourced from Wikipedia.\n4.3 Bidirectional LSTM model on SQuAD\ndataset\nFor the down-stream ﬁne-tuning experiments on\nSQuAD 1.1 dataset, we have the following hyper-\nparameters for training. We set the learning rate\nto be 3e-5, training batch size to be 12, and the\nnumber of training epochs to be 2.\nWe ﬁrst run the experiment by replacing the\ntransformer in BERT base with a bidirectional\nLSTM model with the same number of layers. That\nis, we replace the 12 transformer layers with 12\nBLSTM layers. Table 2 shows the BERT base mod-\nels, including the original BERT-base model in (De-\nvlin et al., 2018) and our implementation, and the\nbidirectional LSTM model accuracy over SQuAD\n1.1 development dataset. Our implementation re-\nsults in a higher F1 score ( 90.05%) compared to\nthe original BERT-base one (88.50%). This may be\ndue to the fact that we use the whole word masking\nwhile BERT-base used partial word masking (an\neasier task, which may prevent from learning a bet-\nter model). We found that the BLSTM model has\nF1 score of 83.43%, which is signiﬁcantly worse\nthan our TRANS/BERT baseline (90.05%).\nModel EM F1\nBERT-base (Devlin et al., 2018) 80.80 88.50\nTRANS/BERT (ours) 83.04 90.05\nBLSTM (ours) 75.99 83.43\nTable 2: SQuAD development results for BERT base\nand the bidirectional LSTM model.\n4.4 Models pre-training\nWe run three pre-training experiments for base and\nlarge settings respectively. 1) BERT model train-\ning baseline (denoted as TRANS/BERT represent-\ning a transformer model or BERT), 2) TRANS-\nBLSTM-SMALL, with BLSTM having half of the\nhidden units of the transformer (768/2 = 384on\nBERT base and 1024/2 = 512on BERT large) for\nBLSTM, and 3) TRANS-BLSTM, with BLSTM\nhaving the same hidden units as the transformer\n(768 on BERT base and 1024 on BERT large).\nFigure 3 shows the training loss for base\nTRANS/BERT, TRANS-BLSTM-SMALL, and\nTRANS-BLSTM models. As can be seen, TRANS-\nBLSTM-SMALL model has lower training loss\nthan the TRANS/BERT model. TRANS-BLSTM\ncan further decrease the training loss compared to\nTRANS-BLSTM-SMALL. This suggests that the\nproposed TRANS-BLSTM-SMALL and TRANS-\nBLSTM are capable of ﬁtting the training data bet-\nter than the original BERT model.\n4.5 Compare two versions of\nTRANS-BLSTM models\nWe proposed two versions of TRANS-BLSTM\nmodels in section 3.2 (see Fig 2), with TRANS-\nBLSTM-1 replacing the feedforward layer with a\nbidirectional LSTM layer, and TRANS-BLSTM-2\nadding a parallel bidirectional LSTM layer. We\ntrained these two models and list their performance\non SQuAD 1.1 development dataset in Table 3. We\nnote that these two models lead to similar accuracy\n0 250k 500k 750k 1M\nSteps\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00Traing Loss\nTRANS/BERT\nTRANS_BLSTM_SMALL\nTRANS_BLSTM\nFigure 3: Training loss as a function of training steps\nfor base TRANS/BERT, TRANS-BLSTM-SMALL,\nand TRANS-BLSTM models respectively.\non this dataset. We will use TRANS-BLSTM-2 to\nreport the accuracy in the rest of the experiments\n(denoted as TRANS-BLSTM for notational sim-\nplicity).\nModel EM F1\nTRANS-BLSTM-1 84.87 91.52\nTRANS-BLSTM-2 84.75 91.53\nTable 3: SQuAD development results for two versions\nof base TRANS-BLSTM models.\n4.6 Models evaluation on SQuAD dataset\nTable 4 shows the results of SQuAD dataset\nfor TRANS/BERT, TRANS-BLSTM-SMALL and\nTRANS-BLSTM models for base and large set-\ntings respectively. As can been see, the TRANS-\nBLSTM-SMALL can boost the baseline BERT\nmodel from F1 score of 90.05% to 90.76%, and\nfrom 92.34% to 92.86% on base and large cases\nrespectively. In addition, the TRANS-BLSTM can\nfurther boost accuracy on top of TRANS-BLSTM-\nSMALL to 91.53% and 93.82% on base and large\nrespectively. The accuracy boosts suggest that the\nbidirectional LSTM model can add additional ac-\ncuracy gain on top of the transformer models.\nCompared to adding bidirectional LSTM lay-\ners to the encoder, the addition of bidirectional\nLSTMs to the decoder (see +BLSTM experiments\nin Table 4) offers additional improvements on\nﬁve out of six cases. For example, it boosts the\nbase TRANS/BERT model F1 score of 90.05%\nto 90.67%, and boosts the large TRANS-BLSTM\nmodel F1 score of 93.82% to 94.01%.\nTable 4 also shows the accuracy of original\nModel EM F1\nBERT (Devlin et al., 2018) 80.8 88.5\nTRANS/BERT 83.04 90.05\n+ BLSTM decoder 83.72 90.67\nBase TRANS-BLSTM-SMALL 84.06 90.76\n+ BLSTM decoder 83.97 90.96\nTRANS-BLSTM 84.75 91.53\n+ BLSTM decoder 84.38 91.25\nBERT (Devlin et al., 2018) 84.1 90.9\nTRANS/BERT 85.84 92.34\nLarge + BLSTM decoder 86.10 92.63\nTRANS-BLSTM-SMALL 86.26 92.86\n+ BLSTM decoder 86.24 92.88\nTRANS-BLSTM 87.72 93.82\n+ BLSTM decoder 87.96 94.01\nTRANS/BERT-48 (ours) 85.62 92.32\nBERT xlarge (Lan et al., 2019) 77.9 86.3\nALBERT xxlarge (Lan et al., 2019) 88.3 94.1\nTable 4: SQuAD development results for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM on\nbase and large settings respectively.\nBERT models (Devlin et al., 2018), which under-\nperform our TRANS/BERT implementations, pos-\nsibly due to whole word masking is used in our\nmodel training. We also trained a TRANS/BERT-\n48 model, which has 48 layers (instead of the 24\nlayers in BERT large conﬁg) and has 638M model\nparameters (comparable to the model parameter\nsize of 789M for TRANS-BLSTM). We observe\nthat the TRANS/BERT-48 has similar accuracy as\nin the TRANS/BERT large model. That is, the extra\ndepth of 24 layers does not generate additional ac-\ncuracy gain compared to BERT large model. Table\n4 also shows the the BERT xlarge model, which\nsimply doubles the hidden units of BERT large\nmodel (ie, with 1024 ∗ 2 = 2048hidden units). It\nhas F1 score of 86.3% which is signiﬁcantly worse\nthan BERT large. This suggests that simply increas-\ning the BERT model size makes it hard to train the\nmodel, resulting in lower accuracy in this case.\nFinally, we list the current state-of-the-art AL-\nBERT model, which has F1 score of 94.1% on\nSQuAD 1.1 development dataset. Our TRANS-\nBLSTM model can obtain similar accuracy to this\nmodeling approach.\n4.7 Model evaluation on GLUE datasets\nFollowing (Devlin et al., 2018), we use a batch size\nof 32 and 3-epoch ﬁne-tuning over the data for all\nGLUE tasks. For each task, we selected the best\nﬁne-tuning learning rate (among 5e-5, 4e-5, 3e-5,\nand 2e-5) on the development set. Additionally\nsimilar to (Devlin et al., 2018), for large BERT\nand TRANS-BLSTM models, we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected\nthe best model on the development set. Table 5\nshows the results of GLUE datasets for original\nBERT (Devlin et al., 2018), ours TRANS/BERT,\nTRANS-BLSTM-SMALL and TRANS-BLSTM\non base and large settings respectively. Following\nthe BERT setting (Devlin et al., 2018), we exclude\nthe problematic WNLI set. F1 scores are reported\nfor QQP and MRPC, Spearman correlations are\nreported for STS-B, and accuracy scores are re-\nported for the other tasks. Unlike the evaluation on\nSQuAD dataset, we do not apply the BLSTM layer\nto the decoder. This is because that the tasks on\nGLUE are classiﬁcation tasks based on the [CLS]\ntoken, and are not sequential prediction tasks (for\nexample the SQuAD dataset) which may beneﬁt\nmore from including a BLSTM layer.\nWe note again the accuracy discrepancy be-\ntween the original BERT and our implementa-\ntion of BERT, which may be due to the fact that\nthe former uses partial word masking while the\nlater uses whole word masking. Similar to the\nSQuAD results, the TRANS-BLSTM-SMALL and\nTRANS-BLSTM base models can improve the\nModel MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\nBERT (Devlin et al., 2018) 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nTRANS/BERT 85.25/85.01 88.74 92.05 93.00 61.06 89.46 91.69 75.45 84.63\nBase TRANS-BLSTM-SMALL 85.46/85.65 88.87 92.44 92.77 61.62 90.01 91.71 75.45 84.77\nTRANS-BLSTM 86.21/86.36 89.23 92.38 94.26 62.54 90.67 91.15 75.40 85.35\nBERT (Devlin et al., 2018) 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTRANS/BERT 87.34/87.46 89.16 93.37 93.92 62.82 91.03 89.94 75.30 85.59\nLarge TRANS-BLSTM-SMALL 88.19/87.66 89.21 93.57 94.61 65.96 90.74 90.03 76.17 86.23\nTRANS-BLSTM 88.07/ 88.28 88.28 94.08 94.38 64.81 90.43 90.45 79.78 86.50\nTable 5: GLUE development results for TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base\nand large settings respectively.\nTRANS/BERT base model from the average GLUE\nscore of 84.63% to 84.77% and 85.35% respec-\ntively. In addition, the TRANS-BLSTM-SMALL\nand TRANS-BLSTM large models can improve\nthe TRANS/BERT large model from the average\nGLUE score of 85.59% to 86.23% and 86.50% re-\nspectively.\n5 Conclusion\nPrevious research suggested that simply increas-\ning the hidden layer size of BERT model cannot\nimprove the model performance. In this paper,\nwe proposed the TRANS-BLSTM model architec-\ntures, which combine the transformer and BLSTM\ninto one single modeling framework, leveraging\nthe modeling capability from both transformer and\nBLSTM. We showed that TRANS-BLSTM models\nconsistently lead to accuracy boost compared to\ntransformer baselines on GLUE and SQuAD.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. arXiv:1409.0473.\nJason P.C. Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional lstm-cnns. Transac-\ntions of the Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv:1810.04805.\nAlex Graves, Abdel rahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep recur-\nrent neural networks. IEEE international confer-\nence on acoustics, speech and signal processing.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. The IEEE Conference on Computer Vision\nand Pattern Recognition.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\narXiv:1503.02531.\nSepp Hochreiter and Jurgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-\nrectional lstm-crf models for sequence tagging.\narXiv:1508.01991.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predict-\ning spans. arXiv:1907.10529.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv:1909.11942.\nTao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav\nArtzi. 2018. Simple recurrent units for highly paral-\nlelizable recurrence. arXiv:1709.02755.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and\nJianfeng Gao. 2019a. Multi-task deep neu-\nral networks for natural language understanding.\narXiv:1901.11504.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv:1907.11692.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. Advances in neural information processing sys-\ntems.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing.\nHasim Sak, Andrew Senior, and Francoise Beaufays.\n2014. Long short-term memory recurrent neural net-\nwork architectures for large scale acoustic modeling.\narXiv:1402.1128.\nVictor Sanh, Lysandre Debut, and Julien Chaumond\nandThomas Wolf. 2019. Distilbert, a distilled ver-\nsion of bert: smaller, faster, cheaper and lighter.\narXiv:1910.01108.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv:1908.09355.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from bert into simple neural net-\nworks. arXiv:1903.12136.\nWilson L Taylor. 1953. Cloze procedure: A new tool\nfor measuring readability. Journalism Bulletin.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv:1706.03762.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. A\nmulti-task benchmark and analysis platform for nat-\nural language understanding. 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP.\nWikipedia contributors. 2004. Plagiarism —\nWikipedia, the free encyclopedia. [Online; ac-\ncessed 22-July-2004].\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, and\nKlaus Macherey et. al. 2016. Googles neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv:1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. arXiv:1906.08237.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. IEEE international conference\non computer vision.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6296885013580322
    },
    {
      "name": "Computer science",
      "score": 0.5881255865097046
    },
    {
      "name": "Natural language processing",
      "score": 0.44042718410491943
    },
    {
      "name": "Linguistics",
      "score": 0.37462806701660156
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36015957593917847
    },
    {
      "name": "Electrical engineering",
      "score": 0.11265915632247925
    },
    {
      "name": "Engineering",
      "score": 0.10437822341918945
    },
    {
      "name": "Philosophy",
      "score": 0.10324421525001526
    },
    {
      "name": "Voltage",
      "score": 0.059065014123916626
    }
  ]
}