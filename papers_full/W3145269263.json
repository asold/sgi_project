{
  "title": "TransCenter: Transformers with Dense Queries for Multiple-Object Tracking",
  "url": "https://openalex.org/W3145269263",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2146150932",
      "name": "Yihong Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2513423657",
      "name": "Yutong Ban",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2304035729",
      "name": "Guillaume Delorme",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112762928",
      "name": "Chuang Gan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223948068",
      "name": "Daniela Rus",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213599358",
      "name": "Xavier Alameda-Pineda",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2594507094",
    "https://openalex.org/W2150298366",
    "https://openalex.org/W3035308063",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3034679090",
    "https://openalex.org/W2508815980",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2511791013",
    "https://openalex.org/W3011168318",
    "https://openalex.org/W3176403636",
    "https://openalex.org/W3173878319",
    "https://openalex.org/W3190647944",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3011353537",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2124781496",
    "https://openalex.org/W3095753995",
    "https://openalex.org/W3099598823",
    "https://openalex.org/W2007352603",
    "https://openalex.org/W2963063317",
    "https://openalex.org/W3165926952",
    "https://openalex.org/W2989980422",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3174690341",
    "https://openalex.org/W2209193152",
    "https://openalex.org/W2107775979",
    "https://openalex.org/W3034739212",
    "https://openalex.org/W3034240185",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W3027919498",
    "https://openalex.org/W3091202868",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W3017343282",
    "https://openalex.org/W3173209770",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2122088301",
    "https://openalex.org/W2579024533",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2963574614",
    "https://openalex.org/W3012922853",
    "https://openalex.org/W2494577018",
    "https://openalex.org/W3176704045",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W3167949052",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W2510977782",
    "https://openalex.org/W3012573144",
    "https://openalex.org/W2252355370",
    "https://openalex.org/W3099887740",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2739491435",
    "https://openalex.org/W2798542761",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3035442500",
    "https://openalex.org/W2981393651",
    "https://openalex.org/W3174101598",
    "https://openalex.org/W2964156315",
    "https://openalex.org/W3033323027",
    "https://openalex.org/W3094000868",
    "https://openalex.org/W3096068180",
    "https://openalex.org/W3112462324",
    "https://openalex.org/W3207452968",
    "https://openalex.org/W3172793249",
    "https://openalex.org/W1982925187",
    "https://openalex.org/W3084263909",
    "https://openalex.org/W2596030096",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2963901085"
  ],
  "abstract": "Transformers have proven superior performance for a wide variety of tasks\\nsince they were introduced. In recent years, they have drawn attention from the\\nvision community in tasks such as image classification and object detection.\\nDespite this wave, an accurate and efficient multiple-object tracking (MOT)\\nmethod based on transformers is yet to be designed. We argue that the direct\\napplication of a transformer architecture with quadratic complexity and\\ninsufficient noise-initialized sparse queries - is not optimal for MOT. We\\npropose TransCenter, a transformer-based MOT architecture with dense\\nrepresentations for accurately tracking all the objects while keeping a\\nreasonable runtime. Methodologically, we propose the use of image-related dense\\ndetection queries and efficient sparse tracking queries produced by our\\ncarefully designed query learning networks (QLN). On one hand, the dense\\nimage-related detection queries allow us to infer targets' locations globally\\nand robustly through dense heatmap outputs. On the other hand, the set of\\nsparse tracking queries efficiently interacts with image features in our\\nTransCenter Decoder to associate object positions through time. As a result,\\nTransCenter exhibits remarkable performance improvements and outperforms by a\\nlarge margin the current state-of-the-art methods in two standard MOT\\nbenchmarks with two tracking settings (public/private). TransCenter is also\\nproven efficient and accurate by an extensive ablation study and comparisons to\\nmore naive alternatives and concurrent works. For scientific interest, the code\\nis made publicly available at https://github.com/yihongxu/transcenter.\\n",
  "full_text": "1\nTransCenter: Transformers with Dense\nRepresentations for Multiple-Object Tracking\nYihong Xu∗, Yutong Ban∗, Guillaume Delorme, Chuang Gan, Daniela Rus, Fellow, IEEE,\nand Xavier Alameda-Pineda, Senior Member, IEEE\nAbstract—Transformers have proven superior performance for a wide variety of tasks since they were introduced. In recent years, they\nhave drawn attention from the vision community in tasks such as image classiﬁcation and object detection. Despite this wave, an\naccurate and efﬁcient multiple-object tracking (MOT) method based on transformers is yet to be designed. We argue that the direct\napplication of a transformer architecture with quadratic complexity and insufﬁcient noise-initialized sparse queries – is not optimal for\nMOT. We propose TransCenter, a transformer-based MOT architecture with dense representations for accurately tracking all the\nobjects while keeping a reasonable runtime. Methodologically, we propose the use of image-related dense detection queries and\nefﬁcient sparse tracking queries produced by our carefully designed query learning networks (QLN). On one hand, the dense\nimage-related detection queries allow us to infer targets’ locations globally and robustly through dense heatmap outputs. On the other\nhand, the set of sparse tracking queries efﬁciently interacts with image features in our TransCenter Decoder to associate object\npositions through time. As a result, TransCenter exhibits remarkable performance improvements and outperforms by a large margin the\ncurrent state-of-the-art methods in two standard MOT benchmarks with two tracking settings (public/private). TransCenter is also\nproven efﬁcient and accurate by an extensive ablation study and comparisons to more naive alternatives and concurrent works. For\nscientiﬁc interest, the code is made publicly available at https://github.com/yihongxu/transcenter.\nIndex Terms—Multiple-Object Tracking, Efﬁcient Transformer, Dense Image-Related Detection Queries, Sparse Tracking Queries.\n!\n1 I NTRODUCTION\nThe task of tracking multiple objects, usually understood as the\nsimultaneous inference of the positions and identities of various\nobjects/pedestrians (trajectories) in a visual scene recorded by one\nor more cameras, became a core problem in computer vision in\nthe past years. Undoubtedly, the various multiple-object tracking\n(MOT) challenges and associated datasets [11], [46], helped foster\nresearch on this topic and provided a standard way to evaluate\nand monitor the performance of the methods proposed by many\nresearch teams worldwide.\nRecent progress in computer vision using transformers [71]\nfor tasks such as object detection [8], [39], [97], person re-\nidentiﬁcation (Re-ID) [25] or image super resolution [82], showed\nthe beneﬁt of the attention-based mechanism. Transformers are\ngood at modeling simultaneously dependencies between different\nparts of the input and thus at making global decisions. These\nadvantages ﬁt perfectly the underlying challenges of MOT, where\ncurrent methods often struggle when modeling the interaction\nbetween objects, especially in crowded scenes. We, therefore, are\nmotivated to investigate the use of a transformer-based architecture\nfor MOT, enabling global estimations when ﬁnding trajectories\nthus reducing missed or noisy predictions of trajectories.\n• Y. Xu, G. Delorme, X. Alameda-Pineda are with Inria Grenoble Rh ˆone-\nAlpes, Montbonnot Saint-Martin, France.\nE-mail: {ﬁrstname.lastname}@inria.fr\n• Y. Ban, D. Rus are with Distributed Robotics Lab, CSAIL, Massachusetts\nInstitute of Technology.\nE-mail: {yban, rus}@csail.mit.edu\n• C. Gan is with MIT-IBM Watson AI Lab.\nE-mail: ganchuang@csail.mit.edu\nY. Xu and Y. Ban contributed equally to this work.\n(a) Detection\n (b) Tracking\nFig. 1: Via TransCenter, we propose to tackle the MOT prob-\nlem with transformers accurately and efﬁciently: the dense non-\noverlapping representations provide sufﬁcient and accurate de-\ntections through dense image-size center heatmap as shown in\n(a); The sparse tracking queries, obtained from features sampled\nwithin object positions at the previous time step, efﬁciently pro-\nduce the sparse displacement vectors of objects (shown in arrows\nplotted on the image originally with a gray background) from the\nprevious to the current time step, as shown in (b).\nCurrent MOT methods follow in principle the predominant\ntracking-by-detection paradigm where we ﬁrst detect objects in the\nvisual scene and associate them through time. The detection ability\nis critical for having a good MOT performance. To this end, MOT\nmethods usually combine probabilistic models [2], [3], [41], [55]\nor deep convolutional architectures [4], [17], [50], [61], [74], [81],\n[87] with an integrated or external detector to predict bounding-\nbox outputs. They are often based on overlapping predeﬁned\nanchors that predict redundant outputs, which might create noisy\ndetections, and thus need hand-crafted post-processing techniques\nsuch as non-maximum suppression (NMS), which might suppress\ncorrect detections. Instead, concurrent MOT methods [45], [65]\narXiv:2103.15145v4  [cs.CV]  30 Sep 2022\n2\nbuilt on recent object detectors like DETR [8] leverage transform-\ners where sparse noise-initialized queries are employed, which\nperform cross-attention with encoded images to output one-to-one\nobject-position predictions, thanks to the one-to-one assignment\nof objects and queries during training. However, using a ﬁxed-\nnumber of sparse queries leads to a lack of detections when the\nvisual scene becomes crowded, and the hyper-parameters of the\nmodel need to be readjusted. For these reasons, we argue that\nhaving dense but non-overlapping representations for detection is\nbeneﬁcial in MOT, especially in crowded scenes.\nIndeed, the image-size dense queries are naturally compatible\nwith center heatmap predictions, and there exist center-based MOT\nmethods [89], [94] that output dense object-center heatmap pre-\ndictions directly related to the input image. Moreover, the center\nheatmap representations are pixel-related and thus inherit the one-\nto-one assignment of object centers and queries (pixels) without\nexternal assignment algorithms. However, the locality of CNN\narchitecture limits the networks to explore, as transformers do,\nthe co-dependency of objects globally. Therefore, we believe that\nthe transformer-based approach with dense image-related (thus\nnon-overlapping) representations is a better choice for building\na powerful MOT method.\nDesigning such transformer-based MOT with dense image-\nrelated representations is far from evident. The main drawback is\nthe computational efﬁciency related to the quadratic complexity in\ntransformers w.r.t. the dense inputs. To overcome this problem, we\nintroduce TransCenter, with powerful and efﬁcient attention-based\nencoder and decoder, as well as the query generator.\nFor the encoder, the DETR [8] structure alleviates this issue\nby introducing a pre-feature extractor like ResNet-50 to extract\nlower-scale features before inputting the images to the trans-\nformers, but the CNN feature extractor itself contributes to the\nnetwork complexity. Deformable transformers reduce signiﬁcantly\nthe attention complexity but the ResNet-50 feature extractor is\nstill kept. Alternatively, recent efﬁcient transformers like [73]\ndiscard the pre-feature extractor and input directly image patches,\nfollowing the ViT structure [13]. They also reduce the attention\ncomplexity in transformers by spatial-reduction attention, which\nmakes building efﬁcient transformer-based MOT possible.\nIn TransCenter Decoder, the use of a deformable transformer\nindeed reduces the attention complexity. However, with our dense\nqueries, the attention calculation remains heavy, which hinders\ncomputational efﬁciency. Alternatively, our image-size detection\nqueries are generated from the lightweight query learning net-\nworks (QLN) that inputs image features from the encoder, and\nthey are already globally related. Therefore, the heavy detection\ncross-attention with image features can be omitted. Furthermore,\nThe tracking happens in the decoder where we search the current\nobject positions with the known previous ones. With this prior\npositional information, we design the tracking queries to be sparse,\nwhich do not need to search every pixel for ﬁnding current object\npositions and signiﬁcantly speed up the MOT method without\nlosing accuracy.\nTo summarize, as roughly illustrated in Fig. 1, TransCenter\ntackles the MOT problem with image-related dense detection\nqueries and sparse tracking queries. It (i) introduces dense image-\nrelated queries to deal with miss detections from insufﬁcient\nsparse queries or over detections in overlapping anchors of current\nMOT methods, yielding better MOT accuracy; (ii) solves the\nefﬁciency issue inherited in transformers with careful network\ndesigns, sparse tracking queries, and removal of useless external\noverheads, allowing TransCenter to perform MOT efﬁciently.\nOverall, this work has the following contributions:\n• We introduce TransCenter, the ﬁrst center-based transformer\nframework for MOT and among the ﬁrst to show the beneﬁts\nof using transformer-based architectures for MOT.\n• We carefully explore different network structures to com-\nbine the transformer with center representations, speciﬁcally\nproposing dense image-related multi-scale representations\nthat are mutually correlated within the transformer attention\nand produce abundant but less noisy tracks, while keeping a\ngood computational efﬁciency.\n• We extensively compare with up-to-date online MOT track-\ning methods, TransCenter sets a new state-of-the-art baseline\nboth in MOT17 [46] (+4.0% Multiple-Object Tracking Ac-\ncuracy, MOTA) and MOT20 [11] (+18.8% MOTA) by large\nmargins, leading both MOT competitions by the time of our\nsubmission in the published literature.\n• Moreover, two more model options, TransCenter-Dual,\nwhich further boosts the performance in crowded scenes,\nand TransCenter-Lite, enhancing the computational efﬁ-\nciency, are provided for different requirements in MOT\napplications.\n2 R ELATED WORKS\n2.1 Multiple-Object Tracking\nIn MOT literature, initial works [1], [3], [55] focus on how to ﬁnd\nthe optimal associations between detections and tracks through\nprobabilistic models while [47] ﬁrst formulates the problem as an\nend-to-end learning task with recurrent neural networks. More-\nover, [57] models the dynamics of objects by a recurrent network\nand further combines the dynamics with an interaction and an\nappearance branch. [81] proposes a framework to directly use the\nstandard evaluation measures MOTA and MOTP as loss functions\nto back-propagate the errors for an end-to-end tracking system.\n[4] employs object detection methods for MOT by modeling the\nproblem as a regression task. A person Re-ID network [4], [68] can\nbe added at the second stage to boost the performance. However,\nit is still not optimal to treat the Re-ID as a secondary task. [89]\nfurther proposes a framework that treats the person detection and\nRe-ID tasks equally and [48] uses quasi-dense proposals from\nanchor-based detectors to learn Re-ID features for MOT in a\ndense manner. [63] proposes a fast online MOT with detection\nreﬁnement. Motion clues in MOT are also important since a well-\ndesigned motion model can compensate for missing detections\ndue to occlusions. To this end, [83] uniﬁes the motion and\nafﬁnity models for MOT. [43] leverages articulation information to\nperform MOT. [22], [58] focus on the motion-based interpolation\nbased on probabilistic models. [78] uniﬁes the segmentation and\ndetection task in an MOT framework combining historic tracking\ninformation as a strong clue for the data association. Derived\nfrom the success in single-object tracking, [64] employs siamese\nnetworks for MOT. [88] simply leverages low-score detections to\nexplore partially occluded objects. Moreover, traditional graphs\nare also used to model the positions of objects as nodes and the\ntemporal connection of the objects as edges [26], [32], [66], [67],\n3\n[68]. The performance of those methods is further boosted by\nthe recent rise of Graph Neural Networks (GNNs): hand-designed\ngraphs are replaced by learnable GNNs [7], [23], [49], [74], [76],\n[77] to model the complex interaction of the objects.\nMost of the above methods follow the tracking by detec-\ntions/regression paradigm where a detector provides detections for\ntracking. The paradigm has proven state-of-the-art performance\nwith the progress in object detectors. Unlike classic anchor-\nbased detectors [53], [54], recent progress in keypoint-based\ndetectors [35], [95] exhibited better performance while discarding\noverlapping and manually-designed box anchors trying to cover\nall possible object shapes and positions. Built on keypoint-based\ndetectors, [94], [89] and [93] represent objects as centers in a\nheatmap then reason about all the objects jointly and associate\nthem across adjacent frames with a tracking branch or Re-ID\nbranch.\n2.2 Transformers in Multiple-Object Tracking\nTransformer is ﬁrst proposed by [71] for machine translation and\nhas shown its ability to handle long-term complex dependencies\nbetween entries in a sequence by using a multi-head attention\nmechanism. With its success in natural language processing, works\nin computer vision start to investigate transformers for various\ntasks, such as image recognition [13], Person Re-ID [25], realistic\nimage generation [29], super-resolution [82] and audio-visual\nlearning [15], [16].\nObject detection with Transformer (DETR) [8] can be seen\nas an exploration and correlation task. It is an encoder-decoder\nstructure where the encoder extracts the image information and\nthe decoder ﬁnds the best correlation between the object queries\nand the encoded image features with an attention module. The\nattention module transforms the inputs into Query (Q), Key (K),\nand Value (V) with fully-connected layers. Having Q,K,V , the\nattended features are calculated with the attention function [71]:\nAttention(Q,K,V ) =Softmax(QKT\n√\nh\n)V (1)\nwhere h is the hidden dimension of Q,K, and V. The atten-\ntion calculation suffers from heavy computational and memory\ncomplexities w.r.t the input size: the feature maps extracted from\na ResNet-50 [24] backbone are used to alleviate the problem.\nDeformable DETR [97] further tackles the issue by proposing\ndeformable attention inspired by [10], drastically speeding up the\nconvergence (10×) and reducing the complexity. The reduction of\nmemory consumption allows in practice using multi-scale features\nto capture ﬁner details, yielding better detection performance.\nHowever, the CNN backbone is still kept in Deformable DETR.\nThis important overhead hinders the Transformer from being\nefﬁcient. Alternatively, Pyramid Vision Transformer [73] ( PVT)\nextracts the visual features directly from the input images and\nthe attention is calculated with efﬁcient spatial-reduction attention\n(SRA). Precisely, PVT follows the ViT [13] structure while the\nfeature maps are gradually down-scaled with a patch embedding\nmodule (with convolutional layers and layer normalization). To\nreduce the quadratic complexity of Eq. 1 w.r.t the dimension d\nof Q,K,V , the SRA in PVT reduces beforehand the dimension\nof K,V from Rd×h to R\nd\nr ×h (with r >1, the scaling factor)\nand keeps the dimension of Qunchanged. The complexity can be\nreduced by r2 times while the dimension of the output attended\nfeatures remains unchanged, boosting the efﬁciency of transformer\nattention modules.\nThe use of transformers is still recent in MOT. Before trans-\nformers, some attempts with simple attention-based modules have\nbeen introduced for MOT. Speciﬁcally, [21] proposes a target\nand distractor-aware attention module to produce more reliable\nappearance embeddings, which also helps suppress detection drift\nand [72] proposes hand-designed spatial and temporal correlation\nmodules to achieve long-range information similar to what trans-\nformers inherit. After the success in detection using transformers,\ntwo concurrent works directly apply transformers on MOT based\non the (deformable) DETR framework. First, Trackformer [45]\nbuilds directly from DETR [8] and is trained to propagate the\nqueries through time. Second, Transtrack [65] extends [97] to\nMOT by adding a decoder that processes the features at t−1\nto reﬁne previous detection positions. Importantly, both methods\nstay in the DETR framework with sparse queries and extend it for\ntracking. However, recent literature [89], [93], [94] also suggests\nthat point-based tracking may be a better option for MOT while\nthe use of pixel-level dense queries with transformers to predict\ndense heatmaps for MOT has never been studied. In addition, we\nquestion the direct transfer from DETR to MOT as concurrent\nworks do [45], [65]. Indeed, the sparse queries without positional\ncorrelations might be problematic in two folds. Firstly, the in-\nsufﬁcient number of queries could cause severe miss detections\nthus false negatives (FN) in tracking. Secondly, queries are highly\noverlapping, and simply increasing the number of non-positional-\ncorrelated queries may end up having many false detections and\nfalse positives (FP) in tracking. All of the above motivates us\nto investigate a better transformer-based MOT framework. We\nthus introduce TransCenter a methodology that takes existing\ndrawbacks into account at the design level and achieves state-\nof-the-art performance.\n3 T RANS CENTER\nTransCenter tackles the detection and temporal association\nin MOT accurately and efﬁciently. Different from concurrent\ntransformer-based MOT methods, TransCenter questions the use\nof sparse queries without image correlations (i.e. noise initialized),\nand explores the use of image-related dense queries, produc-\ning dense representations using transformers. To that aim, we\nintroduce the query learning networks (QLN) responsible for\nconverting the outputs of the encoder into the inputs of the\nTransCenter Decoder. Different possible architectures for QLN\nand TransCenter Decoder are proposed, and the choice is made\nbased on both the accuracy and the efﬁciency aspects.\nWhile exploiting dense representations from dense queries 1,\ncan help to sufﬁciently detect the objects, especially in crowded\nscenes, the design of dense queries is not trivial. Notably,\nthe quadratic increase of calculation complexity in transformers\nshould be solved, and the noise tracks from randomly initialized\ndense queries should be addressed. To this end, we propose image-\nrelated dense queries, which have three prominent advantages: (1)\nthe queries are multi-scale and exploit the multi-resolution struc-\nture of the encoder, allowing for very small targets to be captured\n1. See visualization in Supplementary Material Sec. B.2.\n4\nFig. 2: Generic pipeline of TransCenter and different variants: Images at tand t−1 are fed to the transformer encoder (DETR-Encoder\nor PVT-Encoder) to produce multi-scale memories Mt and Mt−1 respectively. They are passed (together with track positions at t−1)\nto the Query Learning Networks (QLN) operating in the feature’s channel. QLN produce (1) dense pixel-level multi-scale detection\nqueries–DQ, (2) detection memory– DM, (3) (sparse or dense) tracking queries– TQ, (4) tracking memory– TM. For associating\nobjects through frames, the TransCenter Decoder performs cross attention between TQ and TM, producing Tracking Features– TF.\nFor detection, the TransCenter Decoder either calculates the cross attention between DQ and DM or directly outputs DQ (in our\nefﬁcient versions, TransCenter and TransCenter-Lite, see Sec. 3), resulting in Detection Features– DF for the output branches, St and\nCt. TF, together with object positions at t−1 (sparse TQ) or center heatmap Ct−1 (omitted in the ﬁgure for simplicity) and DF\n(dense TQ), are used to estimate image center displacements Tt indicating for each center its displacement in the adjacent frames\n(red arrows). We detail our choice (TransCenter) of QLN and TransCenter Decoder structures in the ﬁgure. Other designs of QLN and\nTransCenter Decoder are detailed in Fig. 3 and Fig. 4. Arrows with a dotted line are only necessary for models with sparse TQ.\nby those queries; (2) image-related dense detection queries also\nmake the network more ﬂexible. The number of queries grows\nwith the input resolution. No ﬁxed hyper-parameters like in [65]\nneed to be adjusted to re-train the model, which depends on the\ndensity of objects in the scene; (3) the query-pixel correspondence\ndiscards the time-consuming Hungarian matching [34] for the\nquery-ground-truth object association. Up to our knowledge, we\nare the ﬁrst to explore the use of image-related dense detection\nqueries that scale with the input image size. Meanwhile, we\nsolve the efﬁciency issue through a careful network design that\nappropriately handles the dense representation of queries, which\nprovides an accurate and efﬁcient MOT method.\nA generic pipeline of TransCenter is illustrated in Fig. 2.\nRGB images at t and t −1 are input to the weight-shared\ntransformer encoder from which dense multi-scale attended fea-\ntures are obtained, namely memories Mt and Mt−1. They are\nthe inputs of the QLN. QLN produces two sets of output pairs,\ndetection queries ( DQ) and memory ( DM) for detecting the\nobjects at time t, and tracking queries ( TQ) and memory ( TM)\nfor associating the objects at t with previous time step t−1.\nFurthermore, TransCenter Decoder, leveraging the deformable\ntransformer [97], is used to correlateDQ/TQ with DM/TM. To\nelaborate, TQ interacts with TM in the cross-attention module of\nthe TransCenter Decoder, resulting in the tracking features ( TF).\nSimilarly, the detection features (DF) are the output of the cross-\nattention between DQ and DM. To produce the output dense\nrepresentations, DF are used to estimate the object size St and\nthe center heatmap Ct. Meanwhile TF are used to estimate the\ntracking displacement Tt.\nOne can argue that the downside of using dense queries is the\nassociated higher memory consumption and lower computational\nefﬁciency. One drawback with previous or concurrent approaches\nis the use of the deformable DETR encoder including the CNN\nfeature extractor ResNet-50 [24], which signiﬁcantly slows down\nthe feature extraction. Instead, TransCenter leverages PVTv2 [73]\nas its encoder, the so-called PVT-Encoder. The reasons are three-\nfold: (1) it discards the ResNet-50 backbone and uses efﬁcient at-\ntention heads [73], reducing signiﬁcantly the network complexity;\n(2) it has ﬂexible scalability by modifying the feature dimension\nhand block structures. Speciﬁcally, we use B0 (denoted as PVT-\nLite) in TransCenter-Lite and B2 ( PVT-Encoder) for TransCenter\nand TransCenter-Dual, see details in [73] and Sec.4.1; (3) its\nfeature pyramid structure is suitable for building dense pixel-level\nmulti-scale queries.\nOnce the transformer encoder extracts the dense memory rep-\nresentations Mt and Mt−1, they are passed to QLN and then to\nthe TransCenter Decoder. We carefully search the design choices\nof QLN (see Sec. 3.1) and TransCenter Decoder (see Sec. 3.2),\nand select the best model based on the efﬁciency and accuracy.\nIn particular, we demonstrate that TQ can be sparse 2, different\nfrom DQ, since we have the prior information of object positions\nat the previous time step that helps to search their corresponding\npositions at the current time step. Therefore, it is not necessary\nto search every pixel for this aim. Consequently, the discretization\nof tracking queries based on previous object positions (roughly\nfrom 14k to less than 500 depending on the number of objects)\ncan signiﬁcantly speed up the tracking attention calculation in the\nTransCenter Decoder.\nRegarding the detection, the cross attention module between\nthe dense detection queries ( DQ) and the detection memory\n(DM) is beneﬁcial in terms of performance but at the cost\nof signiﬁcant computational loads. We solve this by studying\nthe impact of the detection cross-attention on the computational\nefﬁciency and the accuracy. We introduce two variants of the\nproposed method, TransCenter-Dual and TransCenter-Lite. The\nformer shares the same structure as TransCenter but having DDCA\nfor detection in the decoder, as detailed in Sec. 3.2; The latter is a\nlighter version of TransCenter with a lighter encoder (PVT-Lite).\nIn the following sections, we detail the design choices of the\nQLN and the TransCenter Decoder, and provide the details of the\n2. See visualization in Supplementary Material Sec. B.3.\n5\nﬁnal output branches (see Sec. 3.3) as well as the training losses\n(see Sec. 3.4).\n3.1 QLN: Query Learning Networks\n(a) QLNS− and QLNSE−\n (b) QLND−\n(c) QLND\n (d) QLNE\nFig. 3: Query Learning Networks (QLN): TransCenter uses\nQLNS− (our choice) as its query learning network, producing\nsparse tracking queries by sampling prior object features from\nMt−1. Different structures of QLN are studied such as QLNSE−,\nQLND−, QLN D (QLNMt in green arrow and QLN DQ in blue\narrow), and QLNE, detailed in Sec. 3.1. Best seen in color.\nQLN are networks that relate the queries and the memories.\nIn our design, (1) DQ are dense and image-related that discover\nobject positions precisely and abundantly. (2) Different fromDQ,\nTQ are sparse and aim at ﬁnding object displacements between\ntwo different frames, and thus TQ and TM should be produced\nby input features from different time steps.\nBased on these attributes, we design the chosen QLN S−3\nthat produces DQ by passing Mt (attended features from image\nt) through a FFN (feed-forward network with fully-connected\nlayers). For TQ, QLNS− samples object features with a feature\nsampler using bilinear interpolation from Mt−1 using object\npositions at t−1, while outputting TM from features at a different\ntime step, Mt. Moreover, different possible variants of QLN are\nvisualized in Fig. 3 and are summarized as follows:\n1) QLN D−: QLNS− with dense (with the subscript ”D”) TQ\nwithout the feature sampler.\n2) QLN Mt : QLND− with dense TQ from Mt and TM from\nMt−1.\n3) QLN DQ: QLND− with dense TQ from DQ and TM from\nMt−1.\n4) QLN E: QLNDQ with noise-initialized learnable embeddings\n(with the subscript ”E”).\n3. ”S” means sparse, ”−” means that the features are sampled from t −1.\nIt is counter-intuitive to have QLN S because at time t, we know neither the\nnumber of tracked objects (tracks) at t nor their positions, we cannot thus\nsample features with track positions.\nFig. 4: TransCenter Decoder is used to handle tracking queries\nTQ and detection queries DQ. The detection attention correlates\nDQ and DM with the attention modules to detect objects. The\ntracking attention correlates TQ and TM to learn the displace-\nments of the tracked objects until t−1 between different frames.\nTransCenter Decoder has three main modules TQSA, DDCA, and\nTDCA (detailed in Sec. 3.2). Different versions of TransCenter\nDecoder depending on discarding the DDCA or not, are denoted as\nSingle or Dual decoder respectively. Also, an extra preﬁx ”TQSA-\n” is added if the decoder has TQSA 4. TransCenter uses TQSA-\nSingle considering the efﬁciency-accuracy tradeoff. The choice is\nbased on the ablation of the aforementioned variants in Sec. 4.5.\nNdec is the number of decoder layers.\n5) QLN SE−: QLN S− with noise-initialized learnable embed-\ndings for detection.\nIn Sec. 4.5, different QLN are carefully designed and ablated\nto produce dense detection queries relative to the input image\nand sparse tracking queries for accurate and efﬁcient MOT with\ntransformers.\n3.2 TransCenter Decoder\nTo successfully ﬁnd object trajectories, a MOT method should\nnot only detect the objects but also associate them across\nframes. To do so, TransCenter Decoder tackles in parallel the\ntwo sub-tasks: detection and object temporal association (track-\ning). Concretely, TransCenter Decoder consists of Tracking De-\nformable Cross-Attention (TDCA), and Detection Deformable\nCross-Attention (DDCA) modules. Moreover, Tracking Query\nSelf-Attention (TQSA) module is introduced to enhance the in-\nteractions among sparse tracking queries through a multi-head\nself-attention, knowing that the overhead is acceptable because\nthe tracking queries are sparse in TransCenter. TDCA calculates\ncross attention between tracking queries and memories ( TQ and\nTM), resulting in tracking features ( TF). Analogously, DDCA\ncalculates cross attention between detection queries and memories\n(DQ and DM), producing detection features ( DF). Deformable\ncross attention module [97] with linear complexity w.r.t. input size\nis used.\nFrom the efﬁciency perspective, the use of the multi-head\nattention modules in traditional transformers [71] like DETR [8]\nimplies a complexity growth that is quadratic with the input size.\nOf course, this is undesirable and would limit the scalability and\nusability of the method. To mitigate this, we resort to the de-\nformable multi-head attention [97]–Deformable Cross-Attention\n4. We note that discarding TDCA is impossible since the tracking queries at\nt −1 or t should interact with TM for searching objects at t or t −1.\n6\nFig. 5: Overview of the center heatmap branch. The multi-scale\ndetection features are up-scaled (bilinear up.) and merged via a\nseries of deformable convolutions (Def. Conv., the ReLU activa-\ntion is omitted for simplicity) [10], into the output center heatmap.\nA similar strategy is followed for the object size and the tracking\nbranches.\n(DCA), where the queries are input to produce sampling offsets\nfor displacing the input reference points. The reference points are\nfrom either the track position at t−1 for tracking in TDCA or\nthe pixel coordinates of the dense queries for detection in DDCA5.\nThe displaced coordinates are used to locate and sample features\nin DM or TM. The input queries also produce in DCA the\nattention weights for merging sampled features. However, the cost\nof calculating the cross attention between DQ and DM is still\nnot negligible because of their multi-scale image resolutions. To\nsolve this, we demonstrate in Sec. 4.5 ( Single v.s. Dual decoder)\nthat it is possible to output directly DQ as DF for output-branch\npredictions, with an acceptable loss of accuracy as expected. In\naddition, under this sparse nature of TQ in TransCenter, we\nenhance the interactions among queries by adding lightweight\nTQSA before TDCA.\nTo conclude, we choose to use a TQSA-Single TransCenter\nDecoder for the cross attention of TQ and TM while we directly\nuse DQ for the output branches. This is possible thanks to the\nsparse TQ and dense DQ, which yields a good balance between\ncomputational efﬁciency and accuracy, The overall Decoder de-\nsign is illustrated in Fig. 4 and the comparison between different\ndesign choices can be found in Sec. 4.5.\n3.3 The Center, the Size, and the Tracking Branches\nGiven DF and TF from the TransCenter Decoder, we use\nthem as the input of different branches to output object center\nheatmap Ct, their box size St and the tracking displacements Tt.\nDF contain feature maps of four different resolutions, namely\n5. The reference points are omitted in the ﬁgures for simplicity.\n1/32,1/16,1/8, and 1/4 of the input image resolution. For the\ncenter heatmap and the object size, the feature maps at different\nresolutions are combined using deformable convolutions [10]\nand bilinear interpolation up-sampling, following the architecture\nshown in Fig. 5. They are up-sampled into a feature map of 1/4\nof the input resolution and then input to Ct ∈[0,1]H/4×W/4 and\nSt ∈RH/4×W/4×2. H and W are the input image height and\nwidth, respectively, and the two channels of St encode the object\nsize in width and height.\nRegarding the tracking branch, the tracking features TF are\nsparse having the same size as the number of tracks at t−1. One\ntracking query feature corresponds to one track at t−1. TF,\ntogether with object positions at t−1 ( in the case of sparse\nTQ) or center heatmap Ct−1 and DF (dense TQ), are input to\ntwo fully-connected layers with ReLU activation. The latter layers\npredict the horizontal and vertical displacements Tt of objects at\nt−1 in the adjacent frames.\n3.4 Model Training\nThe model training is achieved by jointly learning a 2D classiﬁ-\ncation task for the object center heatmap and a regression task for\nthe object size and tracking displacements, covering the branches\nof TransCenter. For the sake of clarity, in this section, we will drop\nthe time index t.\nCenter Focal Loss. To train the center branch, we need ﬁrst to\nbuild the ground-truth heatmap response C∗ ∈[0,1]H/4×W/4.\nAs done in [94], we construct C∗ by considering the maximum\nresponse of a set of Gaussian kernels centered at each of the\nK >0 ground-truth object centers. More formally, for every pixel\nposition (x,y) the ground-truth heatmap response is computed as:\nC∗\nxy = max\nk=1,...,K\nG((x,y),(xk,yk); σ), (2)\nwhere (xk,yk) is the ground-truth object center, and G(·,·; σ) is\nthe Gaussian kernel with spread σ. In our case, σ is proportional\nto the object’s size, as described in [35]. Given the ground-truth\nC∗ and the inferred C center heatmaps, the center focal loss, LC\nis formulated as:\nLC = 1\nK\n∑\nxy\n{\n(1 −Cxy)αlog(Cxy) C∗\nxy = 1,\n(1 −C∗\nxy)β(Cxy)αlog(1 −Cxy) otherwise.\n(3)\nwhere the scaling factors are α= 2and β = 4, see [89].\nSparse Regression Loss. The values of S is supervised only on\nthe locations where object centers are present, i.e. C∗\nxy = 1using\na L1 loss:\nLS = 1\nK\n∑\nxy\n{Sxy −S∗\nxy\n\n1 C∗\nxy = 1,\n0 otherwise. (4)\nThe formulation of LT for T is analogous to LS but using the\ntracking output and ground-truth displacement, instead of the\nobject size. To complete the sparsity of LS, we add an extra L1\nregression loss, denoted as LR with the bounding boxes computed\nfrom St and ground-truth centers. To summarize, the overall loss\nis formulated as the weighted sum of all the losses, where the\nweights are chosen according to the numeric scale of each loss:\nL= LC + λSLS + λTLT + λRLR (5)\n7\n4 E XPERIMENTAL EVALUATION\n4.1 Implementation Details\nInference with TransCenter. Once the method is trained, we\ndetect objects by selecting the maximum responses from the\noutput center heatmap Ct. Since the datasets are annotated with\nbounding boxes, we need to convert our estimates into this\nrepresentation. In detail, we apply (after max pooling) a threshold\nτ (e.g. 0.3 for MOT17 and 0.4 for MOT20 in TransCenter) to\nthe center heatmap, thus producing a list of center positions\n{ct,k}Kt\nk=1. We extract the object size st,k associated to each\nposition ct,k in St. The set of detections produced by TransCenter\nis denoted as Dt = {ct,k,st,k}Kt\nk=1. In parallel, for associating\nobjects through frames (tracking), given the position of an object\nk at t−1, we can estimate the object position in the current\nframe by extracting the corresponding displacement estimate tt,k\nfrom Tt. Therefore, we can construct a set of tracked positions\n˜Pt = {ct−1,k + tt,k,st,k}\n˜Kt\nk=1. Finally, we use the Hungarian\nalgorithm [34] to match the tracked positions – ˜Pt and the\ndetection at t – Dt. The matched detections are used to update\nthe tracked object positions at t. The birth and death processes\nare naturally integrated in TransCenter: detections not associated\nwith any tracked object give birth to new tracks, while unmatched\ntracks are put to sleep for at most T = 60 frames before being\ndiscarded. An external Re-ID network is often used in MOT meth-\nods [4] to recover tracks in sleep, which is proven unnecessary in\nour experiment in Sec. 4.5. We also assess inference speed in fps\nin the testset results either obtained from [88] or tested under the\nsame GPU setting.\nNetwork and Training Parameters. The input images are re-\nsized to 640×1088 with padding in TransCenter and TransCenter-\nDual while it is set to 608 ×1088 in TransCenter-Lite. In\nTransCenter, the PVT-Encoder has[3,4,6,3] layers ([2,2,2,2] in\nPVT-Lite) for each image feature scale and the corresponding hid-\nden dimension h = [64,128,320,512] (h = [32,64,160,256]\nin PVT-Lite). h = 256 for the TransCenter Decoder with eight\nattention heads and six layers (four layers in TransCenter-Lite).\nAll TransCenter models are trained with loss weights λS = 0.1,\nλR = 0.5 and λT = 1.0 by the AdamW optimizer [44] with\nlearning rate 2e−4. The training converges at around 50 epochs,\napplying a learning rate decay of 1/10 at the 40th epoch. The\nentire network is pre-trained on the pedestrian class of COCO [40]\nand then ﬁne-tuned on the respective MOT dataset [11], [46]. We\nalso present the results ﬁnetuned with extra data like CrowdHuman\ndataset [62] (see Sec. 4.3 for details).\n4.2 Protocol\nDatasets and Detections. We use the standard split of the\nMOT17 [46] and MOT20 [11] datasets and the testset evaluation\nis obtained by submitting the results to the MOTChallenge web-\nsite. The MOT17 testset contains 2,355 trajectories distributed in\n17,757 frames. MOT20 testset contains 1,501 trajectories within\nonly 4,479 frames, which leads to a much more challenging\ncrowded-scene setting. We evaluate TransCenter both under public\nand private detections. When using public detections, we limit the\nmaximum number of birth candidates at each frame to the number\nof public detections per frame, as in [45], [94]. The selected birth\ncandidates are those closest to the public detections with IOU\nlarger than 0. When using private detections, there are no con-\nstraints, and the detections depend only on the network’s detection\ncapacity, the use of external detectors, and more importantly, the\nuse of extra training data. For this reason, we regroup the results\nby the use of extra training datasets as detailed in the following.\nIn addition, we evaluate our TransCenter on the KITTI dataset\nunder the autonomous driving setting. KITTI dataset contains\nannotations of cars and pedestrians in 21 and 29 video sequences\nin the training and test sets, respectively. For the results of KITTI\ndataset, we use also [69] as extra data.\nExtra Training Data. To fairly compare with the state-of-\nthe-art methods, we denote the extra data used to train each\nmethod, including several pre-prints listed in the MOTChallenge\nleaderboard, which are marked with * in our result tables 6:\nCH for CrowdHuman [62], PT for PathTrack [59], RE1 for the\ncombination of Market1501 [91], CUHK01 and CUHK03 [36]\nperson re-identiﬁcation datasets, RE2 replaces CUHK01 [36] with\nDukeMTMC [56], 5D1 for the use of ﬁve extra datasets (ETH [14],\nCaltech Pedestrian [12], [51], CityPersons [86], CUHK-SYS [80],\nand PRW [92]), 5D1+CH is the same as 5D1 plus CroudHuman.\n(5D1+CH ) uses the tracking/detection results of FairMOT [89]\ntrained within the 5D1+CH setting, and NO stands for using no\nextra dataset.\nMetrics. Standard MOT metrics such as MOTA (Multiple Ob-\nject Tracking Accuracy) and MOTP (Multiple Object Tracking\nPrecision) [5] are used: MOTA is mostly used since it reﬂects\nthe average tracking performance including the number of FP\n(False positives, predicted bounding boxes not enclosing any\nobject), FN (False negatives, missing ground-truth objects) and\nIDS [37] (Identities of predicted trajectories switch through time).\nMOTP evaluates the quality of bounding boxes from successfully\ntracked objects. Moreover, we also evaluate IDF1 [56] (the ratio of\ncorrectly identiﬁed detections over the average number of ground-\ntruth objects and predicted tracks), MT (the ratio of ground-truth\ntrajectories that are covered by a track hypothesis more than 80%\nof their life span), and ML (less than 20% of their life span).\n4.3 Testset Results and Discussion\nMOT17. Tab. 1 presents the results obtained in the MOT17\ntestset. The ﬁrst global remark is that most state-of-the-art methods\ndo not evaluate under both public and private detections, and\nunder different extra-training data settings, while we do. Secondly,\nTransCenter sets new state-of-the-art performance compared to\nother methods, in terms of MOTA, underCH and no-extra training\ndata conditions, both for public and private detections. Precisely,\nthe increase of MOTA w.r.t. the state-of-the-art methods is of8.5%\nand 4.8% (both including unpublished methods by now) for the\npublic detection setting under CH and no-extra training data, and\nof 1.7% and 4.0% for the private detection setting, respectively.\nThe superiority of TransCenter is remarkable in most of the\nmetrics. We can also observe that TransCenter trained with no\nextra-training data outperforms, not only the methods trained with\nno extra data but also some methods trained with one extra dataset.\nSimilarly, TransCenter trained on CH performs better than seven\n6. COCO [40] and ImageNet [28] are not considered as extra data according\nto the MOTchallenge [11], [46].\n8\nTABLE 1: Results on MOT17 testset: the left and right halves of the table correspond to public and private detections respectively.\nThe cell background color encodes the amount of extra-training data: green for none, orange for one extra dataset, red for (more than)\nﬁve extra datasets. Methods with * are not associated to a publication. The best result within the same training conditions (background\ncolor) is underlined. The best result among published methods is in bold. Best seen in color.\nPublic Detections Private Detections\nMethod Data MOTA↑MOTP↑IDF1↑MT↑ML↓ FP↓ FN↓ IDS↓ FPS↑ Data MOTA ↑MOTP↑IDF1↑MT↑ML↓ FP↓ FN↓ IDS↓FPS↑\nMOTDT17 [9] RE1 50.9 76.6 52.7 17.5 35.7 24,069250,7682,474 18.3\n*UnsupTrack [30] PT 61.7 78.3 58.1 27.2 32.4 16,872197,6321,864<17.5\nGMTCT [23] RE2 61.5 66.9 26.3 32.1 14,059200,6552,415\nTrackFormer [45] CH 62.5 60.7 29.8 26.9 14,966206,6191,189 6.8\nSiamMOT [64] CH 65.9 63.5 34.6 23.9 18,098170,9553,040 12.8\n*MOTR [85] CH 67.4 67.0 34.6 24.5 32,355149,4001,992 7.5\nTrackFormer [45] CH 65.0 63.9 45.6 13.8 70,443123,5523,528 6.8\nCenterTrack [94] CH 67.8 78.4 64.7 34.6 24.6 18,489160,3323,039 17.5\nTraDeS [78] CH 69.1 63.9 36.4 21.5 20,892150,0603,555 17.5\nPermaTrack [70] CH 73.8 68.9 43.8 17.2 28,998114,1043,699 11.9\n*TransTrack [65] CH 74.5 80.6 63.9 46.8 11.3 28,323112,1373,663 10.0\nTransCenter CH 75.9 81.2 65.9 49.8 12.1 30,190100,9994,626 11.7 CH 76.2 81.1 65.5 53.5 7.9 40,10188,827 5,394 11.8\nGSDT [74] 5D1 66.2 79.9 68.7 40.8 18.3 43,368144,2613,318 4.9\nSOTMOT [93] 5D1 62.8 67.4 24.4 33.0 6,556 201,3192,017 16.0 5D1 71.0 71.9 42.7 15.3 39,537118,9835,184 16.0\nGSDTV2 [74] 5D1 73.2 66.5 41.7 17.5 26,397120,6663,891 4.9\nCorrTracker [72] 5D1 76.5 73.6 47.6 12.7 29,808 99,5103,369 15.6\nFairMOT [89] 5D1+CH 73.7 81.3 72.3 43.2 17.3 27,507117,4773,303 25.9\n*RelationTrack [84] 5D1+CH 73.8 81.0 74.7 41.7 23.2 27,999118,6231,374 7.4\nCSTrack [38] 5D1+CH 74.9 80.9 72.6 41.5 17.5 23,847114,3033,567 15.8\nMLT [87] (5D1+CH) 75.3 81.7 75.5 49.3 19.5 27,879109,8361,719 5.9\n*FUFET [61] (5D1+CH) 76.2 81.1 68.0 51.1 13.6 32,79698,475 3,237 6.8\nTransCenter 5D1+CH 76.0 81.4 65.6 47.3 15.3 28,369101,9884,972 11.7 5D1+CH 76.4 81.2 65.4 51.7 11.6 37,00589,712 6,402 10.9\nTrctrD17 [81] NO 53.7 77.2 53.8 19.4 36.6 11,731247,4471,947 <2.0\nTracktor [4] NO 53.5 78.0 52.3 19.5 36.6 12,201248,0472,072 <2.0\nTracktor++ [4] NO 56.3 78.8 55.1 21.1 35.3 8,866 235,4491,987 <2.0\nGSMTracktor [42] NO 56.4 77.9 57.8 22.2 34.5 14,379230,1741,485 8.7\nTADAM [21] NO 59.7 58.7 9,676 216,0291,930\nCenterTrack [94] NO 61.5 78.9 59.6 26.4 31.9 14,076200,6722,583 17.5\n*FUFET [61] NO 62.0 59.5 27.8 31.5 15,114196,6722,621 6.8\nArTIST-C [58] NO 62.3 59.7 29.1 34.0 19,611191,2072,062<17.5\nMAT [22] NO 67.1 80.8 69.2 38.9 26.4 22,756161,5471,279 9.0\nMTP [33] NO 51.5 54.9 20.5 35.5 29,623241,6182,563 20.1 NO 55.9 60.4 20.5 36.7 8,653 238,8531,188 20.1\nChainedTracker [50] NO 66.6 78.2 57.4 32.2 24.2 22,284160,4915,529 6.8\nQDTrack [48] NO 64.6 79.6 65.1 32.3 28.3 14,10318,29982,652 20.3 NO 68.7 79.0 66.3 40.6 21.9 26,58914,66433,378 20.3\nTransCenter NO 71.9 80.5 64.1 44.4 18.6 27,356126,8604,118 11.9 NO 72.7 80.3 64.0 48.7 14.0 33,807115,5424,719 11.8\nmethods trained with ﬁve or more extra datasets in the private\nsetting, comparable to the best result in 5D1+CH (-0.3% MOTA),\nshowing that TransCenter is less data-hungry. Moreover, trained\nwith 5D1+CH, the performance is further improved while running\nat around 11 fps. Overall, these results conﬁrm our hypothesis\nthat TransCenter with dense detection representations and sparse\ntracking representations produced by global relevant queries in\ntransformers is a better choice.\nMOT20. Tab. 2 reports the results obtained in MOT20 testset.\nIn all settings , similar to the case in MOT17, TransCenter leads\nthe competition by a large margin compared to all the other\nmethods. Concretely, TransCenter outperforms current methods\nby +19.2%/+8.4% in MOTA with the public/private setting trained\nwith CH and +11.1%/18.8% without extra data. From the results,\nanother remarkable achievement of TransCenter is the signiﬁcant\ndecrease of FN while keeping a relatively low FP number. This\nindicates that the dense representation of the detection queries can\nhelp effectively detect objects sufﬁciently and accurately. As for\ntracking, TransCenter maintains low IDS numbers in MOT20 run-\nning at around 8 fps in such crowded scenes, thanks to our careful\nchoices of QLN and the TransCenter Decoder. Very importantly, to\nthe best of our knowledge, our study is the ﬁrst to report the results\nof all settings on MOT20, demonstrating the tracking capacity of\nTransCenter even in a densely crowded scenario. The outstanding\nresults of TransCenter in MOT20 further show the effectiveness of\nour design.\nKITTI. Additionally, we show the results of TransCenter evalu-\nated on the KITTI dataset. TransCenter signiﬁcantly outperforms\nCenterTrack [94] in pedestrian tracking (+5.3% MOTA) while\nkeeping a close performance in car tracking. However, the KITTI\ndataset is constructed in an autonomous driving scenario with\nonly up to 15 cars and 30 pedestrians per image but some of\nthe sequences contain no pedestrians. The sparse object locations\ncannot fully show the capacity of TransCenter to detect and track\ndensely crowded objects.\n4.4 Efﬁciency-Accuracy Tradeoff Discussion\nTo have a direct idea of the better design of TransCenter,\nwe discuss in detail the efﬁciency-accuracy tradeoff compar-\ning TransCenter, TransCenter-Lite, and TransCenter-Dual to the\ntransformer-based concurrent works – TransTrack [65] and Track-\nFormer [45]. To have a fairer comparison to these methods using\nthe (deformable) DETR encoder (D. DETR), i.e. ResNet-50 with a\ndeformable transformer encoder, we leverage TransCenter-DETR,\nhaving the same encoder as theirs while keeping the rest of\nits structure unchanged. Moreover, related to our method, we\nshow superior performance compared to the center-based MOT\nmethods – CenterTrack [94] and FairMOT [89]. The comparisons\ntake into account the number of model parameters, the model\nmemory footprint during inference, the inference speed (frame per\nsecond or FPS), and the MOTA performance as shown in Tab. 4,\ncompleted with Tab. 1 and Tab. 2. Additionally, we compare the\ncenter heatmap/query responses of the aforementioned methods in\nSec. B.1 of the Supplementary Material, showing that TransCenter\nproduces sufﬁcient and accurate outputs.\n9\nTABLE 2: Results on MOT20 testset: the table is structured following the same principle as Tab. 1. Methods with * are not associated\nto a publication. The best result within the same training conditions (background color) is underlined . The best result among published\nmethods is in bold. Best seen in color.\nPublic Detections Private Detections\nMethod Data MOTA↑MOTP↑IDF1↑MT↑ML↓ FP↓ FN↓ IDS↓ FPS↑ Data MOTA↑MOTP↑IDF1↑MT↑ML↓ FP↓ FN↓ IDS↓FPS↑\n*UnsupTrack [30] PT 53.6 80.1 50.6 30.3 25.0 6,439 231,2982,178 <17.5\n*TransTrack [65] CH 64.5 80.0 59.2 49.1 13.6 28,566151,3773,565 7.2\nTransCenter CH 72.8 81.0 57.6 65.5 12.1 28,026110,3122,621 8.4 CH 72.9 81.0 57.7 66.5 11.8 28,596108,9822,625 8.7\nCorrTracker [72] 5D1 65.2 69.1 66.4 8.9 79,429 95,855 5,183 8.5\nGSDTV2 [74] 5D1 67.1 67.5 53.1 13.2 31,507135,3953,230 0.9\nGSDT [74] 5D1 67.1 79.1 67.5 53.1 13.2 31,913135,4093,131 0.9\nSOTMOT [93] 5D1 68.6 71.4 64.9 9.7 57,064101,1544,209 8.5\nFairMOT [89] 5D1+CH 61.8 78.6 67.3 68.8 7.6 103,44088,901 5,243 13.2\nCSTrack [38] 5D1+CH 66.6 78.8 68.6 50.4 15.5 25,404144,3583,196 4.5\n*RelationTrack [84] 5D1+CH 67.2 79.2 70.5 62.2 8.9 61,134104,5974,243 2.7\nTransCenter 5D1+CH 72.4 81.2 57.9 64.2 12.3 25,121115,4212,290 8.6 5D1+CH 72.5 81.1 58.1 64.7 12.2 25,722114,3102,332 8.8\nSORT [6] NO 42.7 78.5 45.1 16.7 26.2 27,521264,6944,470 <27.7\nTracktor++ [4] NO 52.6 79.9 52.7 29.4 26.7 6,930 236,6801,648 1.2\nArTIST-T [58] NO 53.6 51.0 31.6 28.1 7,765 230,5761,531 <1.2\nGNNMatch [49] NO 54.5 79.4 49.0 32.8 25.5 9,522 223,6112,038 0.1\nTADAM [21] NO 56.6 51.6 39,40718,25202,690\nMLT [87] NO 48.9 78.0 54.6 30.9 22.1 45,660216,8032,187 3.7\nTransCenter NO 67.7 79.8 58.9 65.6 11.3 54,967108,3763,707 8.4 NO 67.7 79.8 58.7 66.3 11.1 56,435107,1633,759 8.4\nTABLE 3: KITTI testset results in MOTA, MOTP, FP, FN, IDS\nand FPS. Best results are underlined .\nMethod MOTA ↑ MOTP ↑ FP ↓ FN ↓ IDS ↓ FPS ↑\nCar\nMASS [31] 84.6 85.4 4,145 786 353 100.0\nIMMDP [79] 82.8 82.8 5,300 422 211 5.3\nAB3D [75] 83.5 85.2 4,492 1,060 126 214.7\nSMAT [20] 83.6 85.9 5,254 175 198 10.0\nTrackMPNN [52] 87.3 84.5 2,577 1,298 481 20.0\nCenterTrack [94] 88.8 85.0 2,703 886 254 22.2\nTransCenter 87.3 83.8 3,189 847 340 18.5\nPerson\nAB3D [75] 38.9 64.6 11,744 2,135 259 214.7\nTrackMPNN [52] 52.1 73.4 7,705 2,758 626 20.0\nCenterTrack [94] 53.8 73,7 8,061 2,201 425 22.2\nTransCenter 59.1 73.2 6,889 2,142 436 18.5\nVery recently, works like ByteTrack [88] and MO3TR-\nPIQ [96] take advantage of the recent off-the-shelf object detector\nYOLOX [18] trained with extensive data-augmentation tricks such\nas MixUp [90], which signiﬁcantly improves the MOT perfor-\nmance. However, we do not think that the MOT performance\nimprovement from the object detector could be considered as\ntheir contribution to the MOT community. Even though, for\nthe scientiﬁc interest, we use the results of the YOLOX from\nByteTrack [88] as matching and birth candidates to show that\nTransCenter can have a similar or even better result compared\nto ByteTrack and MO3TR-PIQ [96], denoted as TransCenter-\nYOLOX.\nAll results are evaluated on both MOT17 and MOT20 testsets\nin the private detection setting except the comparison with Byte-\nTrack in the public detection setting (see Pub. Det. in Tab. 4). All\nthe models are pretrained on CrowdHuman [62] except for Fair-\nMOT [89] trained also on 5D1+CH datasets, and ByteTrack ad-\nditionally trained on ETH [14] and CityPersons [86]. The default\ninput image size for CenterTrack [94] is544×960, 608×1088 for\nFairMOT [89] and TransCenter-Lite, 640 ×1088 for TransCenter\nTransCenter-Dual, TransCenter-DETR and TransCenter-YOLOX.\n7. MO3TR-PIQ [96] is not yet published and does not provide source code\nnor sufﬁcient information to evaluate the IM, #params, and the FPS.\nTABLE 4: Testset comparisons on the MOT17 and MOT20\namong CenterTrack [94], FairMOT [89], Trackformer [45],\nTransTrack [65], ByteTrack [88], MO3TR-PIQ [96] and our\nproposed models in number of model parameters (#params),\nInference Memory (IM), Frame Per Second (FPS) and MOTA.\nBest results are underlined . They are grouped according to their\nencoders or used detections, indicated as Enc/Det. Without extra\nnotation, private detections are used by default.\nModel Enc/Det #params (M) ↓ IM (MB)↓ FPS↑ MOTA↑\nMOT-17\nFairMOT [89] DLA-34 20.3 892 25.9 73.7\nCenterTrack [94] 20.0 892 17.5 67.8\nTrackFormer [45]\nD. DETR\n40.0 976 6.8 65.0\nTransTrack [65] 47.1 1,002 10.0 74.5\nTransCenter-DETR 43.6 980 8.8 74.8\nByteTrack [88]\nYOLOX\n99.1 1,746 29.6 80.3\nMO3TR-PIQ [96]7 N/A N/A N/A 77.6\nTransCenter-YOLOX 35.1 938 11.9 79.8\nTransCenter-Dual\nPVT\n39.7 954 5.6 76.0\nTransCenter 35.1 938 11.8 76.2\nTransCenter-Lite 8.1 838 17.5 73.5\nByteTrack Pub. Det. 99.1 1,746 N/A 67.4\nTransCenter 35.1 938 11.7 75.9\nMOT-20\nFairMOT [89] DLA-34 20.3 892 13.2 61.8\nCenterTrack [94] N/A N/A N/A N/A\nTrackFormer [45]\nD. DETR\nN/A N/A N/A N/A\nTransTrack [65] 47.1 1,002 7.2 64.5\nTransCenter-DETR 43.6 980 7.5 68.8\nByteTrack [88]\nYOLOX\n99.1 1,746 17.5 77.8\nMO3TR-PIQ [96] N/A N/A N/A 72.3\nTransCenter-YOLOX 35.1 938 10.1 77.9\nTransCenter-Dual\nPVT\n39.7 954 5.1 73.5\nTransCenter 35.1 938 8.7 72.9\nTransCenter-Lite 8.1 838 11.0 68.0\nByteTrack Pub. Det. 99.1 1,746 N/A 67.0\nTransCenter 35.1 938 8.4 72.8\nTrackFormer [45] and TransTrack [65] use varying input sizes\nwith short size of 800; ByteTrack [88] uses an input size of\n800 ×1440.\nCompared to Transformer-Based MOT. With the respect to our\nconcurrent works, we compare TransCenter to them both in accu-\nracy and inference speed. Unfortunately, Trackformer [45] by far\n10\nonly shows results on MOT17 and TransTrack [65] does not show\nresults in all settings. One important remark is that TransCenter\nsystematically outperforms TransTrack and TrackFormer in both\naccuracy (MOTA) and speed (FPS) with a smaller model size (#\nof model parameters) and less inference memory consumption in\nall settings . Precisely, using the same training data, TransCen-\nter exhibits better performance compared TransTrack by +1.7%\nMOTA (by +11.2% v.s. TrackFormer) in MOT17 and signiﬁ-\ncantly by +8.4% in MOT20. The PVT [73] encoder can indeed\nbring faster inference speed and provide more meaningful image\nfeatures that boost the MOT performance. However, using the\nsame encoder as TransTrack, TransCenter-DETR exhibits similar\ninference speed and much better performance in both MOT20\n(+4.3%) and MOT17 (+0.3%), compared to TransTrack. This\nindicates that the overall design of TransCenter is more efﬁcient\nand powerful. Moreover, we recall that, unlike our concurrent\nworks, TransCenter leverages pixel-level dense and multi-scale\ndetection queries to predict dense center-based heatmaps, miti-\ngating the miss-tracking problem while keeping relatively good\ncomputational efﬁciency with sparse tracking queries, efﬁcient\nQLN, and TransCenter Decoder. TransCenter demonstrates thus\na signiﬁcantly better efﬁciency-accuracy tradeoff. Finally, for\ndifferent MOT applications, we provide TransCenter-Dual, intro-\nduced to further boost the performance in crowded scenes, and\nTransCenter-Lite for efﬁciency-critical applications.\nCompared to Center-Based MOT. With the long-term depen-\ndencies and the dense interactions of queries in transformers,\nunlike pure CNN-based center MOT methods, our queries interact\nglobally with each other and make global decisions. TransCenter\nexhibits much better accuracy compared to previous center-based\nMOT methods, CenterTrack and FairMOT [89], [94]. Precisely\nas shown in Tab. 1 and Tab. 4, TransCenter tracks more and\nbetter compared to CenterTrack, with +8.4% (+10.4%) MOTA\nand -71,505 (-73,812) FN in MOT17 private (public) detections,\nusing the same dense center representations and trained with the\nsame data . CenterTrack does not show results in MOT20 while\nFairMOT, similar to CenterTrack, shows good results in MOT20.\nIt alleviates miss detections by training with much more data,\nleading to much fewer FN but producing much noisier detections\n(FP). Surprisingly, with much less training data ( CH) , TransCen-\nter still outperforms by a large margin [89] (+11.1% MOTA shown\nin Tab. 4) even in very crowded MOT20, with cleaner detections\n(suppressing -74,844 FP) and better tracking associations (-2,618\nIDS) shown in Tab. 2. Indeed, the inference speed is slower\ncompared to CNN-based methods, but the above comparisons\nhave demonstrated that previous center-based MOT methods are\nnot comparable in terms of accuracy to TransCenter, with an\nacceptable fps around 11 fps for MOT17 and 8 fps for MOT20.\nMoreover, to adapt to applications with more strict inference\nconstraints, TransCenter-Lite is introduced, keeping a better per-\nformance while having competitive inference speed compared to\ncenter-based MOT methods.\nCompared to YOLOX-Based MOT. The recent works Byte-\nTrack [88] and MO3TR-PIQ show signiﬁcant performance gain\nby using the off-the-shelf object detector YOLOX [18]. We believe\nthat the gain is mainly from the object detector, which is not the\ncontribution of any aforementioned method. This claim is further\nconﬁrmed by the direct comparison of TransCenter and ByteTrack\nin public detection, i.e. without YOLOX, where TransCenter\noutperforms the latter with 75.9% v.s. 67.4% MOTA in MOT17\nand 72.8% v.s. 67.0% in MOT20 (see Tab. 4).\nFor private detection, to have a fair comparison with them,\nwe use the detection results from YOLOX (same as ByteTrack)\nto demonstrate that YOLOX can also signiﬁcantly boost Tran-\nsCenter in terms of MOTA. Precisely, TransCenter-YOLOX shows\n+2.2% MOTA compared to MO3TR-PIQ while only having a\n0.5% difference compared to ByteTrack in MOT17. We remind\nthat ByteTrack is an ofﬂine method with ofﬂine post-processing\ninterpolation. In very crowded scenes like MOT20, TransCenter-\nYOLOX outperforms MO3TR-PIQ and ByteTrack by +5.6% and\n+0.1% in MOTA respectively. We agree that indeed there is an\ninference speed discrepancy with CNN-based methods compared\nto our transformer-based TransCenter. However, in terms of ac-\ncuracy, TransCenter-YOLOX shows comparable or even better\nresults, compared to all other methods. This indicates that the\ndesign of TransCenter is beneﬁcial with different encoders/object\ndetectors.\nTo conclude, TransCenter expresses both better accuracy and\nefﬁciency compared to transformer-based methods [45], [65];\nmuch higher accuracy numbers and competitive efﬁciency com-\npared to [89], [94], showing better efﬁciency-accuracy balance.\nMoreover, with the powerful YOLOX detector, TransCenter can\nbe signiﬁcantly improved in terms of MOT performance, showing\nthe potential of TransCenter.\n4.5 Ablation Study\nIn this section, we ﬁrst experimentally demonstrate the importance\nof our proposed image-related dense queries with naive DETR to\nMOT approaches. Then, we justify the effectiveness of our choices\nof QLN and TransCenter Decoder (see illustrations in Fig. 3 and\nFig. 4, respectively), considering the computational efﬁciency and\naccuracy. All results are shown in Tab. 5. Furthermore, in Tab. 6,\nwe ablate the impacts of removing the external Re-ID network and\nthe NMS (Non-Maximum Suppression) during inference. Finally,\nwe show an additional ablation of the number of decoder layers in\nSec. A of the Supplementary Material. For the ablation, we divide\nthe training sets into a train-validation split, we take the ﬁrst 50%\nof frames as training data and test on the last 25%. The rest 25%\nof frames in the middle of the sequences are thrown to prevent\nover-ﬁtting. All the models are pre-trained on CrowdHuman [62]\nand tested under the private detection setting.\nDense Representations Are Beneﬁcial. We implemented a naive\nDETR MOT tracker with its original 100 sparse queries (from\nlearnable embeddings initialized from noise) with the DETR-\nEncoder and Dual TransCenter Decoder (Line 1 in Tab. 5).\nTo compare, the same tracker but having dense representations\n(43,520, i.e. H/4 ×W/4) is shown in Line 2. From the results\nshown in Line 1-2 of Tab. 5, we see that the limited number\nof queries (100, by default in [97]) is problematic because it is\ninsufﬁcient for detecting and tracking objects, especially in very\ncrowded scenes MOT20 (+106,467 FN, -24.8% MOTA, compared\nto Line 2). This indicates that having dense representations is\nbeneﬁcial, especially for handling crowded scenes in MOT. Some\nvisualization examples are shown in Sec. 4.6.\nNaive Dense, Noisy Dense v.s. Image-Related Dense. One naive\nway to alleviate the insufﬁcient queries is to greatly increase the\n11\nTABLE 5: Ablation study of different model structure implementations evaluated on MOT17 and MOT20 validation sets: different\nvariants of the QLN and TransCenter-decoders are discussed in Fig. 3 and Fig. 4, respectively. The module ablated in each line is\nmarked in grey . For the queries, ”D-S“ means dense detection and sparse tracking queries, with ”D” refers to ”dense” and ”S” refers\nto ”sparse”, similarly for ”S-S” and ”D-D”. Line 10, 12, 14, 16 are the proposed TransCenter, line 9 is TransCenter-Lite and line 11,\nand 17 are TransCenter-Dual, indicated with ∗.\nSettings MOT17 MOT20\nLine Encoder Decoder Queries QLN MOTA↑ IDF1 ↑ FP ↓ FN ↓ IDS ↓ FPS ↑ MOTA↑ IDF1 ↑ FP ↓ FN ↓ IDS ↓ FPS ↑\n1 DETR Dual S-S8 QLNE 49.4 49.6 4,909 8,202 602 2.5 42.5 26.8 19,243 153,429 4,375 1.5\n2 DETR Dual D-D9 QLNE 56.6 50.1 3,510 7,577 678 1.7 67.3 32.1 45,547 46,962 8,115 0.9\n3 DETR Dual D-D QLNDQ 69.2 71.9 1,202 6,951 203 1.9 79.410 66.9 7,697 53,987 1,680 1.2\n4 DETR Dual D-D QLNMt 66.8 71.2 1,074 7,757 167 1.9 78.3 64.7 5517 59,447 1,832 1.2\n5 DETR Dual D-D QLND− 65.8 70.3 1,122 7,987 176 1.9 78.4 64.2 5,340 59,288 1,798 1.1\n6 DETR Dual D-D QLNDQ 69.2 71.9 1,202 6,951 203 1.9 79.4 66.9 7,697 53,987 1,680 1.2\n7 DETR Single D-D QLN DQ 68.1 72.0 580 7,922 141 2.1 79.8 66.9 6,955 53,445 1,657 1.2\n8 PVT Single D-D QLN DQ 72.3 71.4 1,116 6,156 249 8.1 83.6 75.1 14,358 34,782 1,348 6.1\n9∗ PVT-Lite Single D-S QLN S− 69.8 71.6 2,008 5,923 252 19.4 82.9 75.5 14,857 36,510 1,181 12.4\n10∗ PVT TQSA-Single D-S QLN S− 73.8 74.1 1,302 5,540 258 12.4 83.6 75.7 15,459 34,054 1,085 8.9\n11∗ PVT TQSA-Dual D-S QLN S− 74.6 76.5 892 5,879 128 5.6 84.6 78.0 13,415 33,202 921 4.8\n12∗ PVT TQSA-Single D-S QLN S− 73.8 74.1 1,302 5,540 258 12.4 83.6 75.7 15,459 34,054 1,085 8.9\n13 PVT Single D-S QLN S− 69.4 72.5 1,756 6,335 197 13.3 83.0 74.7 14,584 36,441 1,321 9.8\n14∗ PVT TQSA-Single D-S QLN S− 73.8 74.1 1,302 5,540 258 12.4 83.6 75.7 15,459 34,054 1,085 8.9\n15 PVT Single D-D QLN D− 71.1 71.7 1,274 6,274 278 9.0 82.5 75.0 12,686 40,003 1,216 6.3\n16∗ PVT TQSA-Single D-S QLN S− 73.8 74.1 1,302 5,540 258 12.4 83.6 75.7 15,459 34,054 1,085 8.9\n17∗ PVT TQSA-Dual D-S QLNS− 74.6 76.5 892 5,879 128 5.6 84.6 78.0 13,415 33,202 921 4.8\n18 PVT TQSA-Dual D-S QLNSE− 72.7 73.7 2,012 5,213 181 5.5 83.9 77.4 16,252 32,402 976 4.8\nTABLE 6: Ablation study of external inference overheads: the results are expressed in MOTA, MOTP, FP, FN, IDS as well as FPS. They\nare evaluated on both MOT17 and MOT20 validation sets using TransCenter, TransCenter-Lite and TransCenter-Dual, respectively.\nMOT17 MOT20\nSetting MOTA ↑ IDF1 ↑ FP ↓ FN ↓ IDS ↓ FPS ↑ MOTA ↑ IDF1 ↑ FP ↓ FN ↓ IDS ↓ FPS ↑\n+ ex-ReID 73.7 73.5 1,047 5,800 286 4.5 83.6 75.5 15,468 34,057 1,064 2.9\n+ NMS 73.8 74.1 1,300 5,548 261 10.8 83.6 75.8 15,458 34,053 1,084 5.1\nTransCenter 73.8 74.1 1,302 5,540 258 12.4 83.6 75.7 15,459 34,054 1,085 8.9\n+ ex-ReID 69.8 72.0 2,008 5,922 248 5.2 82.9 74.9 14,894 36,498 1,147 3.1\n+ NMS 69.8 71.6 2,009 5,923 253 15.9 82.9 75.4 14,872 36,507 1,181 6.2\nTransCenter-Lite 69.8 71.6 2,008 5,923 252 19.4 82.9 75.5 14,857 36,510 1,181 12.4\n+ ex-ReID 74.6 75.8 840 5,882 162 3.0 84.5 77.7 13,478 33,209 913 2.2\n+ NMS 74.6 76.5 891 5,879 127 5.1 84.6 77.9 13,413 33,203 921 3.4\nTransCenter-Dual 74.6 76.5 892 5,879 128 5.6 84.6 78.0 13,415 33,202 921 4.8\nnumber of queries like in Line 2 of Tab. 5: we drastically increase\nthe number of queries from 100 to 43,520 (i.e.H/4×W/4), same\nas the dense output of TransCenter. Meanwhile, we compare this\nnaive dense queries implementation to a similar one in Line 3 but\nwith image-related dense queries like in TransCenter. Concretely,\nwe obtain the dense queries from QLN DQ (as described in\nFig. 3(c)) with the memories output from DETR-Encoder. We note\nthat the image-related implementation has indeed 14,450 queries\n(i.e. the sum of the 1/8, 1/16, 1/32, and 1/64 of the image size),\nand the up-scale and merge operation (see Fig.5) forms a dense\noutput having the same number of pixels of 43,520. Therefore, we\n8. 100 noise-initialized learnable queries.\n9. 43,520 noise-initialized learnable queries.\n10. Like other MOT methods, we observe that clipping the box size within\nthe image size for tracking results in MOT20 improves slightly the MOTA\nperformance. To have a fair comparison, all the results in MOT20 are updated\nwith this technique.\nensure that the supervisions for the losses are equivalent11 for both\nLine 2 and 3.\nThe main difference between Line 2 and 3 is the queries: the\nproposed multi-scale dense detection queries are related to the\ninput image where one query represents one pixel. The beneﬁt\nof image-related pixel-level queries is well-discussed in Sec. 3.\nFrom the experimental aspect in Tab. 5, for the noise-initialized\nqueries in Line 2, despite the manual one-to-one matching during\ntraining, increasing the queries naively and drastically tends to\npredict noisier detections causing much higher FP (compared to\nLine 3, +2,308 and +37,850 in MOT17 and MOT20, respectively)\nand thus much under-performed tracking results (-12.6% and -\n12.1% MOTA for MOT17 and MOT20, respectively). Although\nmore sophisticated implementations using sparse noise-initialized\nqueries and Hungarian matching loss like in [65] and [45] exhibit\n11. The Gaussian supervision in TransCenter for negative examples has\nvalues very close to 0, thus similar to the classiﬁcation loss in Line 2.\n12\nimproved results, TransCenter shows both better accuracy and\nefﬁciency, as discussed in Sec. 4.4.\nFurthermore, we combine TransCenter(-Dual) with QLN SE−\n(see Fig. 3(a) in red) where the detection queries are of image\nsize as in QLN S− but noise-initialized, denoted as Noise Dense.\nWe note that since they are noise-initialized (i.e. unrelated to\nthe image), it makes no sense to discard the detection attention\n– DDCA (see Sec.3.2) from TransCenter Decoder. The Noisy\nDense detection queries thus need to correlate with Mt to extract\ninformation from the image tin TransCenter Decoder and cannot\nspeed up by removing DDCA like in TransCenter. The result with\nsuch design in Line 18 is compared with TransCenter-Dual in\nline 17 having the same structure but with the chosen QLN S−.\nWe observe that the Noise Dense queries produce a decent result\nthanks to the rest of TransCenter designs like image-related sparse\nqueries, TransCenter Decoder, and the use of PVT encoder but\nthey tend to have more FP caused by the noise initialization\nfor detection queries, leading to the overall worse performance\ncompared to TransCenter-Dual in MOTA, IDF1, and IDS.\nQLN Inputs. A QLN generates tracking queries TQ and mem-\nories TM; detection queries DQ and memories DM. Based on\nthe nature of tracking, we argue that TQ and TM should be\nobtained from information at different time steps since tracking\nmeans associating object positions in the adjacent frames. This\ncreates two variants of QLN, namely QLN Mt with TQ from\nMt and TM are from Mt−1 (Line 4) and inversely, QLN D−,\nwhere TQ are from Mt−1 and TM are from Mt (Line 5).\nFrom their results, we do not observe a signiﬁcant difference\nin terms of performance, indicating both implementations are\nfeasible. Further, we experimentally compare QLNMt (Line 4) and\nQLNDQ (Line 6), where the only difference is the number of FFN\nfor outputting TQ. With an extra FFN for TQ, the performance\nis slightly improved (+2.4% for MOT17 and +1.1% for MOT20).\nEfﬁcient TransCenter Decoder. As we discuss in Sec. 3.2, the\ndesign of TransCenter Decoder can have an important impact on\nthe computational efﬁciency and accuracy with different variants\n(Line 11-16). Precisely, comparing Line 11 and 12, we observe\nindeed that, with dual-decoder handling cross-attention for both\ndetection and tracking, the performance is superior (+0.8% MOTA\nfor MOT17 and +1.0% for MOT20) but the inference is slowed\ndown by around 50%. Balancing the efﬁciency and accuracy, we\nargue that TransCenter (Line 12) is a better choice. Moreover,\nby removing the TQSA module (Line 13, 14), we obtain a slight\ninference speed up (+0.9 fps for MOT17 and MOT20) but at the\ncost of accuracy (-4.4% MOTA in MOT17 and -0.6% in MOT20).\nFinally, we also study the effect of sparse and dense tracking\n(Line 15-16), surprisingly, we ﬁnd that using sparse tracking can\nhelp better associate object positions between frames (-20 IDS in\nMOT17 and -131 IDS in MOT20) and in a more efﬁcient way\n(+3.4 fps in MOT17 and +2.6 in MOT20).\nEfﬁcient PVT-Encoder. Passing from DETR-Encoder (Line 7)\nto PVT-Encoder (Line 8) helps get rid of the ResNet-50 feature\nextractor, which partially speeds up the inference from 2.1 fps to\n8.1 in MOT17, and 1.2 to 6.1 fps in MOT20. Moreover, the PVT-\nEncoder exhibits better results which may be due to the lighter\nstructure that eases the training (+4.2% MOTA in MOT17 and\n+3.8% in MOT20). Similarly, with PVT-Lite, we can speed up +7\nfps for MOT17 and +3.5 fps for MOT20, comparing Line 9 and\n10 while keeping a competitive performance.\nExternal Inference Overheads. MOT methods like [4] use an\nexternal Re-ID network to extract identity features so that we can\nrecover the objects which are temporally suspended by the tracker\nthrough appearance similarities. The Re-ID network (paired with\na light-weight optical ﬂow estimation network LiteFlowNet [27]\npre-trained on KITTI [19]) is often a ResNet-50 [4], pre-trained on\nobject identities in MOT17 trainset. From Tab. 6, we observe that\nthis external Re-ID does help reduce IDS, especially in crowded\nscenes but it slows down signiﬁcantly the inference. To speed\nup, we replace the external Re-ID features extractor simply by\nsampled features from memories Mt, with a feature sampler like\nin QLNS− (Fig. 3(a)) using positions from detections or tracks at\nt, which is almost costless in terms of calculation and achieves\ncomparable results to external Re-ID.\nOne of the beneﬁts of using our image-related dense representa-\ntions is the discard of the one-to-one assignment process during\ntraining. Intuitively, no NMS is needed during inference. However,\nrecall from Eq. 2 that the heatmaps are Gaussian distributions\ncentered at the object centers, a 3x3 max-pooling operation is\nsomehow needed to select the maximum response from each dis-\ntribution (i.e. object center). In practice, an NMS is only performed\nas in [4] within tracked objects. However, from the results shown\nin Tab. 6, the NMS operation between tracks has little impact on\nthe accuracy but imports important overheads. For this reason,\nsuch NMS is discarded from TransCenter, TransCenter-Lite, and\nTransCenter-Dual.\nIn summary, TransCenter can efﬁciently perform MOT with\nsparse tracking queries and dense detection queries operating on\nthe proposed QLN and TransCenter Decoder structures, leveraging\nthe PVT-Encoder. The accuracy is further enhanced with the\nﬁner multi-scale features from the PVT-Encoder, the sparsity of\nthe tracking queries as well as the chosen designs of the QLN\nand TransCenter Decoder. Therefore, TransCenter shows a better\nefﬁciency-accuracy tradeoff compared to naive approaches and\nexisting works.\n4.6 Qualitative Visualizations in Crowded Scenes\nWe report in Fig. 6 the qualitative results from some crowded\nMOT20 sequences, to demonstrate the detection and tracking\nabilities of TransCenter ˜in the context of crowded scenes. Con-\ncretely, we show in Fig. 6 the predicted center trajectories and the\ncorresponding object sizes. Fig. 6(a) is extracted from MOT20-\n04, Fig. 6(b) from MOT20-07 and Fig. 6(c) from MOT20-06. We\nobserve that TransCenter manages to keep high recall, even in\nthe context of drastic mutual occlusions, and reliably associate\ndetections across time. To summarize, TransCenter exhibits out-\nstanding results on both MOT17 and MOT20 datasets for both\npublic and private detections, and for both with or without extra\ntraining data, which indicates that multiple-object center tracking\nusing transformers equipped with dense image-related queries is a\npromising research direction.\n5 C ONCLUSION\nIn this paper, we introduce TransCenter, a powerful and ef-\nﬁcient transformer-based architecture with dense image-related\n13\n(a)\n (b)\n(c)\nFig. 6: TransCenter tracking trajectories visualization of some very crowded scenes in MOT20 under the private detection setting.\nrepresentations for multiple-object tracking. In TransCenter, we\npropose the use of dense pixel-level multi-scale detection queries\nand sparse tracking queries. They are produced with carefully-\ndesigned QLN and interacting in TransCenter Decoder with image\nfeatures. Under the same training conditions, TransCenter outper-\nforms its competitors in MOT17 and by a large margin in very\ncrowded scenes in MOT20. It even exhibits better performance\nthan some methods trained with much more data. More impor-\ntantly, TransCenter maintains a reasonably high efﬁciency while\nexhibiting a state-of-the-art performance, thanks to its careful\ndesigns proven by a thorough ablation.\nSupplementary Material\nAPPENDIX A\nNUMBER OF DECODER LAYERS\nWe ablate in Fig. 7 the number of decoder layers in TransCenter,\nTransCenter-Lite, and TransCenter-Dual. Precisely, we search\nthe best number of decoder layers within the range of [3,6]\nFor TransCenter, TransCenter-Dual, we ﬁnd that having six layers\nof decoder gets the best results in terms of MOTA both in MOT17\nand MOT20 while with four layers in TransCenter-Lite.\n(a) MOT17\n (b) MOT20\nFig. 7: Ablation results in MOTA using different numbers of Tran-\nsCenter Decoder layers. The results are evaluated on both MOT17\nand MOT20 validation sets with TransCenter, TransCenter-Lite\nand TransCenter-Dual.\nAPPENDIX B\nQUALITATIVE RESULTS AND VISUALIZATIONS\nIn this section, we qualitatively visualize the center heatmap\nresponses (Sec. B.1), the detection queries (Sec. B.2) and the\ntracking queries (Sec. B.3) of TransCenter.\n14\n(a) TransTrack [65]\n (b) CenterTrack [94]\n(c) FairMOT [89]\n (d) TransCenter\nFig. 8: Detection outputs of state-of-the-art MOT methods:\n(a) shows the bounding-box centers from the queries in\nTransTrack [65]; (b), (c) are center heatmaps of CenterTrack [94],\nFairMOT [89], and (d) is from TransCenter.\n(a)\n (b)\n(c)\n (d)\nFig. 9: Visualization of the detection queries TQ for TransCenter.\nThe visualization is obtained using the gradient-weighted class\nactivation mapping [60], as detailed in Sec. B.2. (a) and (b) are\nfrom MOT20 while (c) and (d) are from MOT17. Zones with\nred/orange color represent higher response values and with blue\ncolor for lower values.\nB.1 Center Heatmap Response\nWe qualitatively visualize and compare our center heatmap re-\nsponse in Fig. 8(d) with other two center heatmap-based MOT\nmethods [89], [94] in Fig. 8(c) and Fig. 8(b) as well as the\nbox centers predicted from sparse queries from one concurrent\ntransformer-based MOT method [65] in Fig. 8(a).\nTwo concurrent transformer-based MOT methods [45], [65]\nboth use sparse queries, leading to miss detections (pink arrow),\nthat are heavily overlapped, possibly leading to false detections\n(green arrow). Previous center-based MOT methods [89], [94]\nsuffer from the same problems because the centers are estimated\nlocally. TransCenter is designed to mitigate these two adverse\neffects by using dense (pixel-level) image-related detection queries\nproducing a dense representation to enable heatmap-based infer-\nence and exploiting the attention mechanisms to introduce co-\ndependency among center predictions.\n(a)\n (b)\n(c)\n (d)\nFig. 10: Visualization of tracking queries for TransCenter. We\nvisualize the reference points in red dots, and their displacements\n(circles) and importance (circle radius) with offsets and weights\ncalculated from the tracking queries TQ, as detailed in Sec. B.3.\n(a) and (b) are from MOT20 while (c) and (d) are from MOT17.\nB.2 Detection-Query Visualization\nWe visualize in Fig. 9 the detection queries from TransCenter\nusing the gradient-weighted class activation mapping, as described\nin [60]. The intuition is that we can consider the center heatmap\nprediction as a 2D classiﬁcation task, similar to image segmen-\ntation. The center heatmap response is class-speciﬁc where the\nclass is binary with ”person” or ”background”. Precisely, we\ncalculate the gradient from the center heatmap w.r.t. the detection\nqueries. The gradient is averaged over the image height and width.\nIt is served as weights for the dense detection queries ( DQ),\nproducing weighted activations after multiplication. Since our\ndense detection queries are multi-scale, we re-scale the activations\nto the input image size and take the average of pixel values from\nall scales. The activation is plotted above the input image to show\nthe link between their activated values and the object positions.\nFor the visualization purpose, negative values are removed (by\na ReLU activation function) and a normalization process by the\nmaximum activation value is performed. From the visualizations,\nwe can see that the detection queries tend to focus on areas where\npedestrians are gathered. This is intuitive since the dense detection\nqueries are used to ﬁnd objects in a given scene.\nB.3 Tracking-Query Visualization\nDifferently, the tracking queries TQ in TransCenter are sparse\nand thus cannot be plotted directly to an image like the detection\nqueries. Therefore, we visualize them differently: Recall that TQ\nand TM interact in TDCA (see Sec. 3.2 in the main paper) for\nthe tracking inside TransCenter Decoder, where we sample object\nfeatures from the input TM. The sampling locations are obtained\nby displacing the input reference points (normalized object center\npositions at the previous frame) with sampling offsets (several off-\nsets per reference point) learned from TQ. Moreover, the sampled\nfeatures are weighted by the attention weights learned from TQ.\nBased on this mechanism, we visualize the tracking queries by\nlooking at the sampling locations (displaced reference points) in\nthe image t. Concretely, we show in Fig. 10 the reference points\n(track positions at t−K, K is set to 5 for better visualization)\nas red dots. The sampling locations are shown in circles with\ndifferent colors referring to different object identities. Their radius\n15\nis proportional to their attention weight (i.e. a bigger circle means\na higher attention weight) and we ﬁlter out sampling locations with\nweights lower than a threshold (e.g. 0.2) for better visualizations.\nFrom Fig. 10, interestingly, we can see that the sampling locations\nare the surrounding of the corresponding objects and the weights\nand the attention weights get smaller when the locations get further\nfrom the object. This indicates qualitatively that the tracking\nqueries from the previous time step are searching objects at t in\ntheir neighborhood.\nACKNOWLEDGMENTS\nXavier Alameda-Pineda acknowledges funding from the ANR\nML3RI project (ANR-19-CE33-0008-01) and the H2020 SPRING\nproject (under GA #871245).\nREFERENCES\n[1] Nathanael L Baisa. Online multi-object visual tracking using a gm-phd\nﬁlter with deep appearance learning. In FUSION, pages 1–8. IEEE, 2019.\n2\n[2] Yutong Ban, Xavier Alameda-Pineda, Laurent Girin, and Radu Horaud.\nVariational bayesian inference for audio-visual tracking of multiple\nspeakers. TPAMI, 2019. 1\n[3] Yutong Ban, Sileye Ba, Xavier Alameda-Pineda, and Radu Horaud.\nTracking multiple persons based on a variational bayesian model. In\nECCV, pages 52–67. Springer, 2016. 1, 2\n[4] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking\nwithout bells and whistles. In ICCV, pages 941–951, 2019. 1, 2, 7,\n8, 9, 12\n[5] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object\ntracking performance: the clear mot metrics. EURASIP Journal on Image\nand Video Processing, 2008:1–10, 2008. 7\n[6] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft.\nSimple online and realtime tracking. In IEEE international Conference\non Image Processing (ICIP), pages 3464–3468. IEEE, 2016. 9\n[7] Guillem Bras ´o and Laura Leal-Taix ´e. Learning a neural solver for\nmultiple object tracking. In CVPR, pages 5247–6257. IEEE, 2020. 3\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,\nAlexander Kirillov, and Sergey Zagoruyko. End-to-end object detection\nwith transformers. In ECCV, pages 213–229. Springer, 2020. 1, 2, 3, 5\n[9] Long Chen, Haizhou Ai, Zijie Zhuang, and Chong Shang. Real-time\nmultiple people tracking with deeply learned candidate selection and\nperson re-identiﬁcation. In ICME, pages 1–6. IEEE, 2018. 8\n[10] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu,\nand Yichen Wei. Deformable convolutional networks. In ICCV, pages\n764–773. IEEE, 2017. 3, 6\n[11] P. Dendorfer, H. Rezatoﬁghi, A. Milan, J. Shi, D. Cremers, I. Reid,\nS. Roth, K. Schindler, and L. Leal-Taix´e. Mot20: A benchmark for multi\nobject tracking in crowded scenes. In arXiv:2003.09003[cs], 2020. 1, 2,\n7\n[12] Piotr Doll ´ar, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedes-\ntrian detection: An evaluation of the state of the art. TPAMI, 34:743–761,\n2011. 7\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale.\narXiv:2010.11929, 2020. 2, 3\n[14] A. Ess, B. Leibe, K. Schindler, , and L. van Gool. A mobile vision system\nfor robust multi-person tracking. In CVPR, pages 1–8. IEEE, 2008. 7, 9\n[15] Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenenbaum, and\nAntonio Torralba. Foley music: Learning to generate music from videos.\nECCV, pages 758–775, 2020. 3\n[16] Chuang Gan, Deng Huang, Hang Zhao, Joshua B Tenenbaum, and\nAntonio Torralba. Music gesture for visual sound separation. In CVPR,\npages 10478–10487. IEEE, 2020. 3\n[17] Chuang Gan, Hang Zhao, Peihao Chen, David Cox, and Antonio Tor-\nralba. Self-supervised moving vehicle tracking with stereo sound. In\nICCV, pages 7053–7062. IEEE, 2019. 1\n[18] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox:\nExceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.\n9, 10\n[19] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for\nautonomous driving? the kitti vision benchmark suite. In CVPR, pages\n3354–3361, 2012. 12\n[20] Nicolas Franco Gonzalez, Andres Ospina, and Philippe Calvez. Smat:\nSmart multiple afﬁnity metrics for multiple object tracking. In Inter-\nnational Conference on Image Analysis and Recognition (ICIAR) , pages\n48–62, 2020. 9\n[21] Song Guo, Jingya Wang, Xinchao Wang, and Dacheng Tao. Online\nmultiple object tracking with cross-task synergy. In CVPR, pages 8136–\n8145, 2021. 3, 8, 9\n[22] Shoudong Han, Piao Huang, Hongwei Wang, En Yu, Donghaisheng Liu,\nXiaofeng Pan, and Jun Zhao. Mat: Motion-aware multi-object tracking.\narXiv:2009.04794, 2020. 2, 8\n[23] Jiawei He, Zehao Huang, Naiyan Wang, and Zhaoxiang Zhang. Learn-\nable graph matching: Incorporating graph partitioning with deep feature\nlearning for multiple object tracking. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 5299–5309, 2021. 3, 8\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\nlearning for image recognition. In CVPR, pages 770–778. IEEE, 2016.\n3, 4\n[25] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang.\nTransreid: Transformer-based object re-identiﬁcation. arXiv:2102.04378,\n2021. 1, 3\n[26] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, and Paul Swo-\nboda. Lifted disjoint paths with application in multiple object tracking.\nIn ICML, pages 4364–4375. PMLR, 2020. 2\n[27] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Liteﬂownet: A\nlightweight convolutional neural network for optical ﬂow estimation. In\nCVPR, pages 8981–8989. IEEE, 2018. 12\n[28] Deng Jia, Dong Wei, Sochera Richard, Li Li-Jia, Li Kai, and Fei-Fei Li.\nImagenet: A large-scale hierarchical image database. In CVPR. IEEE,\n2009. 7\n[29] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two\ntransformers can make one strong gan. arXiv:2102.07074, 2021. 3\n[30] Shyamgopal Karthik, Ameya Prabhu, and Vineet Gandhi. Simple unsu-\npervised multi-object tracking. arXiv:2006.02609, 2020. 8, 9\n[31] Hasith Karunasekera, Han Wang, and Handuo Zhang. Gnn3dmot: Graph\nneural network for 3d multi-object tracking with 2d-3d multi-feature\nlearning. In IEEE Access, pages 104423–104434. IEEE, 2019. 9\n[32] Margret Keuper, Siyu Tang, Yu Zhongjie, Bjoern Andres, Thomas Brox,\nand Bernt Schiele. A multi-cut formulation for joint segmentation and\ntracking of multiple objects. arXiv:1607.06317, 2016. 2\n[33] Chanho Kim, Fuxin Li, Mazen Alotaibi, and James M Rehg. Discrimi-\nnative appearance modeling with multi-track pooling for real-time multi-\nobject tracking. In CVPR, pages 9553–9562. IEEE, 2021. 8\n[34] Harold W. Kuhn and Bryn Yaw. The hungarian method for the assign-\nment problem. In Naval Res. Logist. Quart , pages 83–97. IEEE, 1955.\n4, 7\n[35] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints.\nIn ECCV, pages 734–750. IEEE, 2018. 3, 6\n[36] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep\nﬁlter pairing neural network for person re-identiﬁcation. In CVPR, pages\n152–159. IEEE, 2014. 7\n[37] Yuan Li, Chang Huang, and Ram Nevatia. Learning to associate:\nHybridboosted multi-target tracker for crowded scene. In CVPR, pages\n2953–2960. IEEE, 2009. 7\n[38] Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, and\nJianxiao Zou. Rethinking the competition between detection and reid in\nmulti-object tracking. arXiv:2010.12138, 2020. 8, 9\n[39] Matthieu Lin, Chuming Li, Xingyuan Bu, Ming Sun, Chen Lin, Junjie\nYan, Wanli Ouyang, and Zhidong Deng. Detr for crowd pedestrian\ndetection. arXiv:2012.06785, 2020. 1\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft\ncoco: Common objects in context. In ECCV, pages 740–755. Springer,\n2014. 7\n[41] Xiaoyu Lin, Laurent Girin, and Xavier Alameda-Pineda. Unsupervised\nmultiple-object tracking with a dynamical variational autoencoder. In\narXiv:2202.09315, 2022. 1\n[42] Qiankun Liu, Qi Chu, Bin Liu, and Nenghai Yu. Gsm: Graph similarity\nmodel for multi-object tracking. In International Joint Conference\non Artiﬁcial Intelligence (IJCAI) , pages 530–536. International Joint\nConferences on Artiﬁcial Intelligence Organization, 2020. 8\n16\n[43] Yuanpei Liu, Junbo Yin, Dajiang Yu, Sanyuan Zhao, and Jianbing\nShen. Multiple people tracking with articulation detection and stitching\nstrategy. Neurocomputing, 386:18–29, 2020. 2\n[44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regulariza-\ntion. In ICLR, 2017. 7\n[45] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph\nFeichtenhofer. Trackformer: Multi-object tracking with transformers.\narXiv:2101.02702, 2021. 1, 3, 7, 8, 9, 10, 11, 14\n[46] Anton Milan, Laura Leal-Taix ´e, Ian D. Reid, Stefan Roth, and Kon-\nrad Schindler. MOT16: A benchmark for multi-object tracking.\narXiv:1603.00831 [cs], 2016. 1, 2, 7\n[47] Anton Milan, S Hamid Rezatoﬁghi, Anthony Dick, Ian Reid, and Konrad\nSchindler. Online multi-target tracking using recurrent neural networks.\nIn AAAI, 2017. 2\n[48] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell,\nand Fisher Yu. Quasi-dense similarity learning for multiple object\ntracking. In CVPR, pages 164–173. IEEE, 2021. 2, 8\n[49] Ioannis Papakis, Abhijit Sarkar, and Anuj Karpatne. Gcnnmatch: Graph\nconvolutional neural networks for multi-object tracking via sinkhorn\nnormalization. arXiv:2010.00067, 2020. 3, 9\n[50] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang,\nYing Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu.\nChained-tracker: Chaining paired attentive regression results for end-\nto-end joint multiple-object detection and tracking. In ECCV, pages\n145–161. Springer, 2020. 1, 8\n[51] Doll ´ar Piotr, Wojek Christian, Schiele Bernt, and Perona Pietro. Pedes-\ntrian detection: A benchmark. In CVPR. IEEE, 2009. 7\n[52] Akshay Rangesh, Pranav Maheshwari, Mez Gebre, Siddhesh Mhatre,\nVahid Ramezani, and Mohan M. Trivedi. Trackmpnn: A message passing\ngraph neural architecture for multi-object tracking. arXiv:2010.12138,\n2021. 9\n[53] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You\nonly look once: Uniﬁed, real-time object detection. In CVPR, pages\n779–788. IEEE, 2016. 3\n[54] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-\ncnn: Towards real-time object detection with region proposal networks.\nNeurIPS, pages 91–99, 2015. 3\n[55] Seyed Hamid Rezatoﬁghi, Anton Milan, Zhen Zhang, Qinfeng Shi,\nAnthony Dick, and Ian Reid. Joint probabilistic data association revisited.\nIn ICCV, pages 3047–3055. IEEE, 2015. 1, 2\n[56] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo\nTomasi. Performance measures and a data set for multi-target, multi-\ncamera tracking. In ECCV, pages 17–35. Springer, 2016. 7\n[57] Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Tracking the un-\ntrackable: Learning to track multiple cues with long-term dependencies.\nIn ICCV, pages 300–311. IEEE, 2017. 2\n[58] Fatemeh Saleh, Sadegh Aliakbarian, Hamid Rezatoﬁghi, Mathieu Salz-\nmann, and Stephen Gould. Probabilistic tracklet scoring and inpainting\nfor multiple object tracking. In CVPR, pages 14329–14339. IEEE, 2021.\n2, 8, 9\n[59] Manen Santiago, Gygli Michael, Dai Dengxin, and Van Gool Luc.\nPathtrack: Fast trajectory annotation with path supervision. In ICCV,\npages 290–299. IEEE, 2017. 7\n[60] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakr-\nishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual\nexplanations from deep networks via gradient-based localization. In\nIJCV, pages 1573–1405. Springer, 2019. 14\n[61] Chaobing Shan, Chunbo Wei, Bing Deng, Jianqiang Huang, Xian-Sheng\nHua, Xiaoliang Cheng, and Kewei Liang. Tracklets predicting based\nadaptive graph tracking, 2020. 1, 8\n[62] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang,\nand Jian Sun. Crowdhuman: A benchmark for detecting human in a\ncrowd. arXiv:1805.00123, 2018. 7, 9, 10\n[63] Jianbing Shen, Dajiang Yu, Leyao Deng, and Xingping Dong. Fast online\ntracking with detection reﬁnement. TITS, 19(1):162–173, 2017. 2\n[64] Bing Shuai, Andrew Berneshawi, Xinyu Li, Davide Modolo, and Joseph\nTighe. Siammot: Siamese multi-object tracking. In CVPR, pages 12372–\n12382. IEEE, 2021. 2, 8\n[65] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu,\nTao Kong, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack:\nMultiple-object tracking with transformer. In arXiv:2012.15460, 2020.\n1, 3, 4, 8, 9, 10, 11, 14\n[66] Siyu Tang, Bjoern Andres, Miykhaylo Andriluka, and Bernt Schiele.\nSubgraph decomposition for multi-target tracking. InCVPR, pages 5033–\n5041. IEEE, 2015. 2\n[67] Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele.\nMulti-person tracking by multicut and deep matching. In ECCV, pages\n100–111. Springer, 2016. 2\n[68] Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt Schiele.\nMultiple people tracking by lifted multicut and person re-identiﬁcation.\nIn CVPR, pages 3539–3548. IEEE, 2017. 2\n[69] Phillip Thomas, Lars Pandikow, Alex Kim, Michael Stanley, and James\nGrieve. Open synthetic dataset for improving cyclist detection. In\nhttps://paralleldomain.com/open-datasets/bicycle-detection, 2021. 7\n[70] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning\nto track with object permanence. In ICCV, pages 10860–10869. IEEE,\n2021. 8\n[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In NeurIPS, pages 5998–6008, 2017. 1, 3, 5\n[72] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multiple object\ntracking with correlation learning. In CVPR, pages 3876–3886. IEEE,\n2021. 3, 8, 9\n[73] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding\nLiang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines\nwith pyramid vision transformer. In Computational Visual Media (CVM),\npages 1–10, 2022. 2, 3, 4, 10\n[74] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection\nand multi-object tracking with graph neural networks. In ICRA. IEEE,\n2021. 1, 3, 8, 9\n[75] Xinshuo Weng, Jianren Wang, David Held, and Kris Kitani. 3d\nmulti-object tracking: A baseline and new evaluation metrics.\narXiv:1907.03961, 2020. 9\n[76] Xinshuo Weng, Yongxin Wang, Yunze Man, and Kris M Kitani.\nGnn3dmot: Graph neural network for 3d multi-object tracking with 2d-3d\nmulti-feature learning. In CVPR, pages 6499–6508. IEEE, 2020. 3\n[77] Xinshuo Weng, Ye Yuan, and Kris Kitani. Joint 3d tracking and forecast-\ning with graph neural network and diversity sampling.arXiv:2003.07847,\n2020. 3\n[78] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and\nJunsong Yuan. Track to detect and segment: An online multi-object\ntracker. In CVPR, pages 12352–12361. IEEE, 2021. 2, 8\n[79] Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track:\nOnline multi-object tracking by decision making. In ICCV, pages 4705–\n4713. IEEE, 2015. 9\n[80] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiaogang Wang.\nEnd-to-end deep learning for person search. arXiv:1604.01850, 2016. 7\n[81] Yihong Xu, Aljosa Osep, Yutong Ban, Radu Horaud, Laura Leal-Taix ´e,\nand Xavier Alameda-Pineda. How to train your deep multi-object tracker.\nIn CVPR, pages 6787–6796. IEEE, 2020. 1, 2, 8\n[82] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo.\nLearning texture transformer network for image super-resolution. In\nCVPR, pages 5791–5800. IEEE, 2020. 1, 3\n[83] Junbo Yin, Wenguan Wang, Qinghao Meng, Ruigang Yang, and Jianbing\nShen. A uniﬁed object motion and afﬁnity model for online multi-object\ntracking. In CVPR, pages 6768–6777. IEEE, 2020. 2\n[84] En Yu, Zhuoling Li, Shoudong Han, and Hongwei Wang. Relationtrack:\nRelation-aware multiple object tracking with decoupled representation.\narXiv:2105.04322, 2021. 8, 9\n[85] Fangao Zeng, Bin Dong, Tiancai Wang, Xiangyu Zhang, and Yichen\nWei. End-to-end multiple-object tracking with transformer. In\narXiv:2105.03247, 2021. 8\n[86] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele. Citypersons:\nA diverse dataset for pedestrian detection. In CVPR, pages 3213–3221.\nIEEE, 2017. 7, 9\n[87] Yang Zhang, Hao Sheng, Yubin Wu, Shuai Wang, Wei Ke, and Zhang\nXiong. Multiplex labeling graph for near-online tracking in crowded\nscenes. IEEE Internet of Things Journal , pages 7892–7902, 2020. 1, 8,\n9\n[88] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo,\nWenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by\nassociating every detection box. In arXiv:2110.06864, 2021. 2, 7, 9, 10\n[89] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu\nLiu. Fairmot: On the fairness of detection and re-identiﬁcation in multiple\nobject tracking. In arXiv:2004.01888, 2020. 2, 3, 6, 7, 8, 9, 10, 14\n[90] Zhi Zhang, Tong He, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and\nMu Li. Bag of freebies for training object detection neural networks.\narXiv preprint arXiv:1902.04103, 2019. 9\n[91] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and\nQi Tian. Scalable person re-identiﬁcation: A benchmark. In ICCV. IEEE,\n2015. 7\n17\n[92] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker,\nYi Yang, and Qi Tian. Person re-identiﬁcation in the wild. In CVPR,\npages 1367–1376. IEEE, 2017. 7\n[93] Linyu Zheng, Ming Tang, Yingying Chen, Guibo Zhu, Jinqiao Wang,\nand Hanqing Lu. Improving multiple object tracking with single object\ntracking. In CVPR, pages 2453–2462. IEEE, 2021. 3, 8, 9\n[94] Xingyi Zhou, Vladlen Koltun, and Philipp Kr ¨ahenb¨uhl. Tracking objects\nas points. In ECCV, pages 474–490. Springer, 2020. 2, 3, 6, 7, 8, 9, 10,\n14\n[95] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Objects as points.\narXiv:1904.07850, 2019. 3\n[96] Tianyu Zhu, Markus Hiller, Mahsa Ehsanpour, Rongkai Ma, Tom Drum-\nmond, and Hamid Rezatoﬁghi. Looking beyond two frames: End-to-\nend multi-object tracking using spatial and temporal transformers. arXiv\npreprint arXiv:2103.14829, 2021. 9\n[97] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng\nDai. Deformable detr: Deformable transformers for end-to-end object\ndetection. arXiv:2010.04159, 2020. 1, 3, 4, 5, 10",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6027328968048096
    },
    {
      "name": "Computer science",
      "score": 0.5767834186553955
    },
    {
      "name": "Object (grammar)",
      "score": 0.45672115683555603
    },
    {
      "name": "Video tracking",
      "score": 0.4125068187713623
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33911511301994324
    },
    {
      "name": "Computer vision",
      "score": 0.3323328495025635
    },
    {
      "name": "Engineering",
      "score": 0.120247483253479
    },
    {
      "name": "Electrical engineering",
      "score": 0.07906937599182129
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "topic": "Transformer",
  "institutions": [],
  "cited_by": 75
}