{
  "title": "Fine-tuning Transformers with Additional Context to Classify Discursive Moves in Mathematics Classrooms",
  "url": "https://openalex.org/W4287855004",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2700347420",
      "name": "Abhijit Suresh",
      "affiliations": [
        "University of Colorado System",
        "University of Colorado Boulder"
      ]
    },
    {
      "id": "https://openalex.org/A2102135209",
      "name": "Jennifer Jacobs",
      "affiliations": [
        "University of Colorado Boulder",
        "University of Colorado System"
      ]
    },
    {
      "id": "https://openalex.org/A4287856229",
      "name": "Margaret Perkoff",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102463150",
      "name": "James H. Martin",
      "affiliations": [
        "University of Colorado Boulder",
        "University of Colorado System"
      ]
    },
    {
      "id": "https://openalex.org/A1964528081",
      "name": "Tamara Sumner",
      "affiliations": [
        "University of Colorado System",
        "University of Colorado Boulder"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3043864305",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2808070424",
    "https://openalex.org/W4245652239",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W3208743309",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W2964410950",
    "https://openalex.org/W2131855198",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2768966733",
    "https://openalex.org/W3167373622",
    "https://openalex.org/W3014230997",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2546117980",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4200463057",
    "https://openalex.org/W1567135675",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3208686870",
    "https://openalex.org/W4205948992",
    "https://openalex.org/W1418763296",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W3028958483",
    "https://openalex.org/W2902235353",
    "https://openalex.org/W3180304956",
    "https://openalex.org/W2999309192",
    "https://openalex.org/W2161114310",
    "https://openalex.org/W4226353613",
    "https://openalex.org/W2545889235",
    "https://openalex.org/W3096020876",
    "https://openalex.org/W2132156812",
    "https://openalex.org/W3215579475",
    "https://openalex.org/W3158196282"
  ],
  "abstract": "\"Talk moves\" are specific discursive strategies used by teachers and students to facilitate conversations in which students share their thinking, and actively consider the ideas of others, and engage in rich discussions. Experts in instructional practices often rely on cues to identify and document these strategies, for example by annotating classroom transcripts. Prior efforts to develop automated systems to classify teacher talk moves using transformers achieved a performance of 76.32% F1. In this paper, we investigate the feasibility of using enriched contextual cues to improve model performance. We applied state-of-the-art deep learning approaches for Natural Language Processing (NLP), including Robustly optimized bidirectional encoder representations from transformers (Roberta) with a special input representation that supports previous and subsequent utterances as context for talk moves classification. We worked with the publically available TalkMoves dataset, which contains utterances sourced from real-world classroom sessions (human- transcribed and annotated). Through a series of experimentations, we found that a combination of previous and subsequent utterances improved the transformers' ability to differentiate talk moves (by 2.6% F1). These results constitute a new state of the art over previously published results and provide actionable insights to those in the broader NLP community who are working to develop similar transformer-based classification models.",
  "full_text": "Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022), pages 71 - 81\nJuly 15, 2022c⃝2022 Association for Computational Linguistics\nFine-tuning Transformers with Additional Context to Classify Discursive\nMoves in Mathematics Classrooms\nAbhijit Suresh1,2, Jennifer Jacobs2, Margaret Perkoff1,\nJames H. Martin1,2, Tamara Sumner1,2\n1Department of Computer Science, 2Institute of Cognitive Science\nUniversity of Colorado Boulder\nFirstName.LastName@colorado.edu\nAbstract\n“Talk moves” are specific discursive strategies\nused by teachers and students to facilitate con-\nversations in which students share their think-\ning, and actively consider the ideas of oth-\ners, and engage in rich discussions. Experts\nin instructional practices often rely on cues\nto identify and document these strategies, for\nexample by annotating classroom transcripts.\nPrior efforts to develop automated systems to\nclassify teacher talk moves using transformers\nachieved a performance of 76.32% F1. In this\npaper, we investigate the feasibility of using en-\nriched contextual cues to improve model perfor-\nmance. We applied state-of-the-art deep learn-\ning approaches for Natural Language Process-\ning (NLP), including Robustly optimized bidi-\nrectional encoder representations from trans-\nformers (Roberta) with a special input repre-\nsentation that supports previous and subsequent\nutterances as context for talk moves classifica-\ntion. We worked with the publically available\nTalkMoves dataset, which contains utterances\nsourced from real-world classroom sessions\n(human- transcribed and annotated). Through\na series of experimentations, we found that\na combination of previous and subsequent ut-\nterances improved the transformers’ ability to\ndifferentiate talk moves (by 2.6% F1). These\nresults constitute a new state of the art over\npreviously published results and provide ac-\ntionable insights to those in the broader NLP\ncommunity who are working to develop similar\ntransformer-based classification models.\n1 Introduction\nThere is a strong theoretical and empirical basis\nfor encouraging students’ active participation in\ninquiry-based and socially constructed classroom\nenvironments (Vygotsky, 1978; Webb et al., 2008).\nNumerous efforts exist to support teachers to be-\ncome more purposeful and effective in their efforts\nto facilitate such environments (Herbel-Eisenmann,\n2017; Chen et al., 2020). Most approaches to pro-\nviding teachers with detailed feedback about their\ndiscourse strategies require highly trained human\nobservers (Correnti et al., 2015; Wolf et al., 2005).\nHowever, recent research has shown that the de-\nvelopment and training of deep learning models\nto automate and scale certain discourse analyses\nfrom instructional episodes is feasible (Song et al.,\n2021), effective (Demszky et al., 2021), and reli-\nable (Donnelly et al., 2017; Jensen et al., 2020;\nSuresh et al., 2019).\nAccountable talk theory offers well-defined,\nresearch-based practices for teachers to engage\nin high-quality instruction, including the use of\nspecific talk moves that promote students’ equi-\ntable participation in a rigorous learning environ-\nment (O’Connor et al., 2015; Resnick et al., 2018).\nBy using talk moves, teachers place the “intel-\nlectual heavy lifting” and balance of talk toward\nstudents and help ensure that the discussions will\nbe purposeful, coherent, and productive (Michaels\net al., 2010). Talk moves support classroom dis-\ncourse to move beyond the traditional Initiate,\nResponse, Evaluate linguistic sequence (Mehan,\n1979); namely, by replacing the act of evaluating\nwith practices that support a collective understand-\ning that builds on and extends mathematical ideas\n(Michaels and O’Connor, 2015).In this way, talk\nmoves enable dialogue shifts from teacher directed\nrecitation to true discussions in which knowledge\nis informally shared and constructed rather than\ntransmitted.\nThis paper draws inspiration from speech recog-\nnition systems for spoken dialog systems to in-\nvestigate the feasibility of applying a novel input\nrepresentation that utilizes tokens from previous\nand subsequent utterances to classify teacher talk\nmoves (Schukat-Talamazzini et al., 1994). We ex-\nplore three different context setups: previous-only\nutterances, subsequent-only utterances, and both\nprevious and subsequent utterances (equal numbers\nof each) with different window sizes. In addition\nto the longer dialog window experiments, we re-\n71\nport findings from fine-tuning transformers such\nas BigBird (Zaheer et al., 2020) and Longformer\n(Beltagy et al., 2020) which are architected to sup-\nport longer sequences. Similarly, we report find-\nings from fine-tuning MathBERT, a transformer\narchitecture that was trained to establish semantic\ncorrespondence between mathematical formulas\nand their corresponding context (Peng et al., 2021).\nFor training and evaluation, we use the TalkMoves\ndataset comprising 567 lesson transcripts derived\nfrom video recordings of K-12 mathematics class-\nrooms (Suresh et al., 2022). The main contributions\nof this work are summarized as follows:\n• We provide evidence for improved perfor-\nmance when fine-tuning transfomers with\nlonger dialog windows.\n• We observed that transformer architectures de-\nsigned to handle longer contexts such as Long-\nformer do not provide any additional benefit\nin differentiating instructional strategies.\n• We observed that math-based models pre-\ntrained on mathematical formula understand-\ning do not provide any improvement over the\ngeneric models.\n2 Related Work\nThis section briefly describes the accountable talk\ntheory framework, followed by a literature review\non deep learning models for Natural Language Pro-\ncessing (NLP) focused on adding additional con-\ntexts and learning long-term dependencies.\n2.1 Accountable talk theory framework\nAccountable talk theory identifies and defines an\nexplicit set of discourse moves intended to elicit a\nresponse within a classroom lesson (O’Connor and\nMichaels, 2019). These well-defined discursive\ntechniques have been incorporated into various in-\nstructional practices and frameworks e.g., (Boston,\n2012; Candela et al., 2020; Michaels et al., 2010).\nTheir specificity makes talk moves well-suited for\nsupervised multi-label sentence-pair classification.\nA number of research teams have made consider-\nable progress in developing automated “intelligent\nagents” that are trained to emulate the role of the\nteacher. These agents prompt students to use desig-\nnated aspects of accountable talk, such as revoicing\nand asking students to agree/disagree with another\nstudent. They typically act as facilitators or tutors\nduring small group, text-based, online settings, tak-\ning part in and helping to focus the discussion at\nopportune moments e.g. (Adamson et al., 2013;\nHmelo-Silver et al., 2013; Tegos et al., 2015). (Ja-\ncobs et al., 2022) and team developed an online\napplication that provides personalized feedback to\nteachers on their classroom discourse practices, in-\ncluding the prevalence of talk moves. The system is\nfully automated and requires no human processing\nbeyond the initial uploading of classroom record-\nings. Such education-focused NLP applications\nare in high demand to provide reliable feedback to\nteachers based on the accountable talk theory.\n2.2 Transformers for additional context and\nlong-term dependencies\nThe introduction of transformers has revolutionized\nthe field of natural language processing. Unlike Re-\ncurrent Neural Networks (RNNs) and Long Short\nTerm Memory networks (LSTMs), where training\nis performed sequentially, the design of transformer\narchitecture enables parallel processing and allows\nfor the creation of rich latent embeddings (Vaswani\net al., 2017). Latent contextual representation of\nutterances through the self-attention mechanism\nmakes transformers a powerful tool for various\ndownstream applications such as question answer-\ning and text summarization (Devlin et al., 2018).\nResearch efforts to learn long-term dependen-\ncies with transformers were first introduced in\nTransformer-XL (Dai et al., 2019). Transformer-\nXL is a novel architecture that focuses on learning\ndependencies beyond the fixed length of vanilla\ntransformers without disrupting the temporal co-\nherence. This is achieved by saving the hidden\nstate sequence of the previous segment to be used\nas context for the current segments, also known as\nthe segment-level recurrence mechanism. In ad-\ndition, to better encode the relationship between\nwords, Transformer-XL uses relative positional em-\nbeddings. Results show that Transformer-XL can\nlearn dependencies across the text with a window\nsize of 900 words. Following Transformer- XL,\n(Yang et al., 2019) proposed XL-Net, which is a\ngeneralized autoregressive pretraining method that\nleverages the capabilities of Transformer-XL to\nsolve the pre-train-finetune discrepancy commonly\nidentified in early architectures such as BERT. XL-\nNet introduced two new developments. As an ex-\ntension to the standard Causal Language Modeling\n(CLM), XL-Net uses permutation language mod-\n72\neling, which considers all possible permutations\nof the words within a sentence during the training\nphase. Also, XL-Net uses a secondary attention\nstream that focuses on the positional information\nof the predicted token. This additional attention\nstream led XL-Net to outperform many contempo-\nrary transformer architectures in downstream tasks,\nsuch as text classification. Similarly, to address the\nproblem of processing long sequences with trans-\nformers, (Beltagy et al., 2020) introduced Long-\nformer, which extends vanilla transformers with a\nmodified self-attention mechanism to process long\ndocuments. The classic self-attention mechanism\nin BERT is computationally expensive, which ex-\nplains the restriction of the maximum sequence\nlength of 512 tokens. Instead, Longformer com-\nbines dilated sliding windows with global attention\nto achieve similar performance. As a result of re-\nducing the computational complexity, Longformer\ncan process long input sequences beyond the previ-\nously defined segment length of 512 tokens. Like\nLongfomers, Big-Bird (Zaheer et al., 2020) uses a\nsparse attention mechanism that includes a random\nattention component.\nOver the past few years, we have seen an in-\ncreasing trend in other approaches to supporting\ntransformers to learn long-term dependencies, such\nas modifying pre-training methods and the classic\nattention mechanism. For example, to learn de-\npendencies across documents, (Xie et al., 2020)\nadopted a simple approach to truncate the docu-\nment used for classification. Similarly, (Joshi et al.,\n2019)) used a chunking approach where documents\nwere broken down into multiple chunks, and the ac-\ntivations were then combined to perform the tasks.\nAnother recent example is the BERT-Seq model for\nclassifying Collaborative Problem Solving (Pugh\net al., 2021). The BERT-Seq model uses a spe-\ncial input representation that combines embeddings\nfrom adjacent utterances as contextual cues for the\nmodel. Building on the prior work, we explored\nnew ways to enrich transformers with additional\ncontextual cues.\n3 Current Work and Novelty\nCurrently, generating information about teachers’\ndiscourse strategies requires highly trained instruc-\ntional experts to hand-code transcripts from class-\nroom sessions (Correnti et al., 2015; Wolf et al.,\n2005), an approach that is expensive and not read-\nily scalable. Encouragingly, a small number of\nresearchers have recently trained computer mod-\nels to automate and scale discourse analyses from\ninstructional episodes, detecting educationally im-\nportant discursive features such as instructional\ntalk, authentic teacher questions, elaborated eval-\nuation, and uptake (Dale et al., 2022; Demszky\net al., 2021; Jensen et al., 2020). In prior work,\n(Suresh et al., 2021b,a) fine-tuned Roberta (Liu\net al., 2019) to classify talk moves for each teacher\nutterance from a given classroom transcript. The\ninput to Roberta was student-teacher sentence pairs,\nwhere the student sentence appeared immediately\nprior to the teacher’s utterance. This paper builds\nupon the previous work to add contextual cues to\ntransformers in various ways and evaluate their\nperformance using the TalkMoves dataset. We ex-\nperiment with modifying the input representation\nby combining multiple previous and subsequent\nutterances as context to classify teacher talk moves.\nThis work serves as an example of how we can\nfind new ways to use advances in natural language\nprocessing with classic ideas from speech recogni-\ntion systems for spoken dialog system to capture\nthe rich conversations between teachers and stu-\ndents in order to improve performance in applied\ndomains such as education.\n4 Method\nThis section discusses the different approaches we\ntook to enrich contextual cues in the TalkMoves\nmodel in an effort to enhance performance.\n4.1 Data\nThe TalkMoves dataset used in this study comprises\n567 transcripts, including 174,186 teacher and\n59,874 student utterances (Suresh et al., 2022). All\nthe transcripts were human-generated from class-\nroom audio and video recordings from K-12 math-\nematics classrooms. They were annotated for six\nteacher talk moves by two experts who established\nhigh inter-rater reliability (Suresh et al., 2021b,\n2022). The talk moves in the dataset follow an un-\neven distribution, with certain moves being much\nmore frequent than others (Figure 1). “Keeping\neveryone together” and “pressing for accuracy” are\nthe most frequently used, whereas “getting students\nto relate” and “pressing for reasoning” are the least\ncommon. For training and testing split, we used the\nsame split specified by (Suresh et al., 2022) in the\nTalkMoves dataset. Each teacher utterance in the\nTalkMoves dataset is annotated with one of six dif-\n73\nferent teacher talk moves and \"None\". These talk\nmoves are broadly classified into three categories\nbased on their instructional purpose (Resnick et al.,\n2018): (1) accountability to the learning commu-\nnity, (2) accountability to content knowledge, and\n(3) accountability to rigorous thinking. See Table\n1 for a brief description of each talk move, along\nwith examples.\n4.2 Research Motivation\nIn this study, we began working with transform-\ners to classify talk moves. Prior attempts using\nnon-transformers architecture achieved lower per-\nformance (65% F1 compared to 76.32% F1 with\ntransformers) (Suresh et al., 2019, 2021b). The\nfine-tuned Roberta model proposed in (Suresh et al.,\n2022) employed a input representation of student-\nteacher sentence pairs to combine any given teacher\nutterance with the immediately prior student utter-\nance (Suresh et al., 2021b). In order to understand\nthe gaps in this model’s performance, (Suresh et al.,\n2022) conducted an error analysis using a confu-\nsion matrix to consider examples where the Talk-\nMoves models were underperforming and often\ngenerated misclassifications. An initial analysis of\nthose examples revealed several instances where\nthe actual real-world context for the misclassified\nteacher utterance extended beyond the current rep-\nresentation of the previous student utterance. For\nexample, consider the following dialogue “Student:\nYes; Teacher: What do you think?”. With limited\ncontext, it seems unclear if the teacher was relating\nto what a student said earlier or trying to prompt\nthem to think. This challenge of limited context\nfrom prior work motivated us to find new ways to\nadd contextual information to the existing models\nin order to improve performance.\n4.3 Context-addition experiments\nConstraints on the number of sequences in vanilla\ntransformers, such as BERT and Roberta, prevents\nthe direct application of transformers where there\nis a reliance on long-term dependencies. For exam-\nple, consider a classroom session where a teacher\nencourages student X to think based on what stu-\ndent Y said earlier in the session. Without the\nexpanded dialogue context, it can be challenging\nfor transformers (and even humans) to classify the\nutterances. If we could expand the representation\nof available information such that it included the\nentire classroom session, the transformers may be\nmore likely to learn to establish the long-term de-\npendencies across the focal utterances or tokens.\nGiven the importance of local context (Kovaleva\net al., 2019), our input representation was modified\nfrom student-teacher sentence pairs to a fixed-size\nwindow surrounding each teacher utterance. This\nadjusted representation is atypical compared to the\nrecommended input for fine-tuning, where a unique\ntoken separates two sequences (i.e., [SEP] in Bert\nand </s> in Roberta) (Devlin et al., 2018; Liu et al.,\n2019). There is a general notion that fine-tuning\nmultiple utterances with multiple separator tokens,\nwhile theoretically possible, is not likely to work\nwell. This notion was motivated by vanilla trans-\nformers, which were originally pre-trained on indi-\nvidual sentences or sentence pairs. We challenge\nthis assumption by including additional past and fu-\nture utterances in our adjusted input representation\n(Figure 2).\nTo establish a baseline performance level and\ngenerate information regarding the impact of con-\ntext in classifying talk moves, we began with a\nsimple input representation that includes only the\ntarget teacher utterance without any additional con-\ntext. The output layer was a softmax over seven\nclasses i.e., the six talk moves and “none” (no talk\nmove). We also reproduced results from prior work\non Roberta-base (Suresh et al., 2022). Following\nthat, we experimented with three context setups:\nprevious-only utterances, subsequent-only utter-\nances, and both previous and subsequent utterances\n(equal numbers of each). In each setup, we evalu-\nated several different window sizes. For example,\nthe previous-only condition with a window size of\nthree would have the immediately previous three\nutterances (with student(s) and/or the teacher as the\nspeakers) serving as context cues for classifying\nthe target utterance. If there was no prior utterance\n(such as at the start of a classroom session), we\nprepended empty strings. Similarly, given the pre-\nvious and subsequent utterances condition with a\nwindow size of two, the target utterance would have\ntwo previous utterances prepended to the left and\ntwo subsequent utterances appended to the right.\nSeparator tokens differentiated all of the utterances.\nAs an additional preprocessing step, all utterances\nwere truncated to 30 tokens long. The choice of\ntruncation length was decided based on the distri-\nbution of sequence length (number of tokens) for\nall utterances in the dataset (see Figure 3). A token\nsize of 30 accounted for more than 95% of the utter-\nances in the dataset (two standard deviations from\n74\nFigure 1: Distribution of teacher talk moves in the TalkMoves dataset\nFigure 2: Modifying the input representation to support additional previous and subsequent utterances\nthe mean of the sequence length of seven tokens).\nWe then fine-tuned transformers on the TalkMoves\ntraining set with different parameters using Ama-\nzon EC2 instances. We followed the recommended\nparameters from (Suresh et al., 2019, 2022) includ-\ning learning rate (2e-5, 3e-5, 4e-5, 5e-5), number\nof epochs (3-6), batch size (4,8,16,32), warmup\nsteps (0,100,1000) and maximum sequence length\n(512 for Roberta-like models) and (512,1024 for\nLongformer and BigBird). The performance on the\ntesting set after fine-tuning is reported based on F1\nmeasures and MCC (Suresh et al., 2021a). These\nmeasures work well for skewed datasets like Talk-\nMoves (Chicco and Jurman, 2020; Suresh et al.,\n2021b). The code was implemented in Python 3.8\nFigure 3: Number of utterances (frequency) vs sequence\nlength (number of tokens) in TalkMoves dataset\n75\nTable 1: Teacher talk moves from TalkMoves dataset (Suresh et al., 2022)\nCategory Talk move Description Example\nTeacher Talk Moves\nLearning\nCommunity\nKeeping everyone to-\ngether\nPrompting students to be ac-\ntive listeners and orienting\nstudents to each other\n“What did Eliza just say her\nequation was?”\nLearning\nCommunity\nGetting students to re-\nlate to another’s ideas\nPrompting students to react to\nwhat a classmate said\n“Do you agree with Juan that\nthe answer is 7/10?”\nLearning\nCommunity\nRestating Repeating all or part of what\na student said word for word\n“Add two here.”\nContent\nKnowledge\nPressing for accuracy Prompting students to make a\nmathematical contribution or\nuse mathematical language\n“Can you give an example of\nan ordered pair?”\nRigorous\nThinking\nRevoicing Repeating what a student said\nbut adding on or changing the\nwording\n“Julia told us she would add\ntwo here.”\nRigorous\nThinking\nPressing for reasoning Prompting students to explain,\nprovide evidence, share their\nthinking behind a decision, or\nconnect ideas or representa-\ntions\n“Why could I argue that the\nslope should be increasing?”\nwith Pytorch and HuggingFace library (Wolf et al.,\n2019). In addition to the context-addition exper-\niments with Roberta-base, we fine-tuned similar\ntransformers architectures. XLNet, Longformer\nand BigBird are transformer architectures which\nsupport longer sequences. Since the TalkMoves\ndataset is composed of utterances from K-12 math-\nematics classrooms, we fine-tuned MathBERT, a\npretrained architecture with focus on mathematical\nformula understanding.\n5 Results\nIn this section, we present the results from our\nexperiments that involved providing additional con-\ntext to transformers to support the process of learn-\ning long-term dependencies. The experiments were\nrepeated with ten random seeds, and the average\nscore is reported (Table 2, 3). For brevity, we re-\nport performance only on Roberta-base (the best\nperforming model from (Suresh et al., 2021b) as\nindicated in the first column of (Table 2) and trans-\nformers such as Longformer and Bigbird (Table\n3). All the models are Base models (Large models\nare beyond the scope of this work). In the second\ncolumn, we describe the context that was provided\nto the target teacher utterance for classification.\nFor example, Previous 1 should be interpreted as\na single previous utterance prepended to the target\nteacher’s utterance. Similarly, Subsequent 1 should\nbe interpreted as a single subsequent utterance ap-\npended to the target utterance. The third and final\ncolumn describes the performance of the testing\nset.\nFor imbalanced datasets like TalkMoves, the\nMatthew Correlation Coefficient (MCC) and F1\nmeasure are good indicators of model performance.\nAn MCC score of +1 indicates a perfect correlation\nwhile 0 indicates a random correlation and -1 indi-\ncates a negative correlation. Similarly, the F1 score\nranges from 0-100% where 100% indicates perfect\nperformance. We begin with the No-Context con-\ndition which achieved a performance of 71.93%\nF1. On prepending the immediately prior or sub-\nsequent student utterance, the model achieved a\nperformance of 76.32% F1 (Suresh et al., 2022).\nNext we turn to results from various context con-\nditions with different window sizes followed by\nresults from Longformer, BigBird, and other mod-\nels. The maximum sequence length in most of\nthese models was 512 with the exception of Long-\nformer and Bigbird which had a sequence length\nupto 1024. The results presented in this work are\ncomprehensive but not exhaustive since training\nand testing for all possible models and parameters\nis infeasible.\nThe results table clearly illustrates the impor-\n76\ntance of context in enhancing performance. Start-\ning with Roberta-Base, the performance on the\nprevious-only condition gradually increased with\nan increase in window-size and saturated for larger\nwindow-sizes. Similarly, we observed an improve-\nment in performance for the subsequent-only con-\ndition. However, we did not see any significant\nimprovement for larger window-sizes in this con-\ndition, possibly due to the negative impact in per-\nformance on \"Revoicing\" and \"Restating\" which\nrely on immediately prior student sentences. More-\nover, the combination of previous and subsequent\nutterances resulted in the best performing model.\nThe performance gradually increased proportion-\nally with a window size up to 7 before saturat-\ning. Likewise, the performance on Longformer,\nXLNet and BigBird were comparable with simi-\nlar input representation. The most surprising re-\nsult was the performance on MathBert which was\nsignficantly lower than other models. In summary,\nRoberta-Base with equal previous-subsequent con-\ndition (size =7) outperformed rest of the models\nand constitutes the state-of-the-art results.\nThe primary motivation of the error analysis us-\ning a confusion matrix was to improve the perfor-\nmance on the under-performing talk move cate-\ngories and identify patterns among the misclassfied\nutterances to be leveraged as features for the mod-\nels. When comparing the confusion matrix from\nprior work (Suresh et al., 2022) (see Table 4), the\ncurrent study shows a significant improvement in\nperformance across all the teacher talk moves la-\nbels except \"Restating\" (see Table 5). With \"Re-\nstating\", we hypothesize that the decrease in per-\nformance was a result of supplementing additional\ncontext. Further analysis has to be performed in\norder to validate this claim.\n6 Discussion\nBased on the results from our experiments to im-\nprove the performance of a talk moves classifier\nusing transformers, it is evident that longer dialog\nwindows play an important role in differentiating\ntalk moves. We successfully validated that the local\ndiscursive context is an important feature in classi-\nfying teacher talk moves. We generated a 4% F1\nincrease in performance when including a single\nadditional utterance (either previous or subsequent)\nas compared to the no-context condition. Also,\nwe observed that previous utterances are more im-\npactful than future utterances for classifying talk\nmoves. This finding is not surprising given that sev-\neral talk moves, such as the teacher “restating” and\n“revoicing” what a student has already said, depend\nentirely on previous utterances as context. We also\nobserved that context windows with a combination\nof previous and future utterances outperform either\ncondition alone. Finally, we found that a window\nsize of seven previous and subsequent utterances\nachieves the best performance. Beyond the iden-\ntified size of seven, the performance decreases. It\nis possible that much earlier or much later utter-\nances provide confusing or conflicting contextual\ninformation, which hinders model performance. It\nis equally likely that longer dialog windows could\nlead to overfitting.\nPrior efforts to address the imbalanced nature of\nTalkMoves dataset through weighted loss resulted\nin reduced performance (Suresh et al., 2019). As\nan alternative, we attempted to generate synthetic\nsamples of tokenized utterances through SMOTE\n(Synthetic Minority Oversampling Data) (Chawla\net al., 2002). With SMOTE, it was challenging to\nretain the syntactic information of the generated\nexamples. It was also difficult to generate the sup-\nporting contextual student and teacher utterances.\nPreliminary efforts did not yield any improvement\nin performance.\nTo further improve the performance, we have\nidentified two future directions that appear worth-\nwhile to consider: (1) experimenting with punctu-\nation and other linguistic markers in the existing\nTalkMoves dataset and (2) collecting more training\ndata. In the TalkMoves dataset, all the punctua-\ntion and other non-alphanumeric characters from\nthe teacher and student utterances were removed.\nThese text processing steps are typical for most\ntext-based NLP applications to produce text that\nclosely aligns with the output of Automated Speech\nRecognition (ASR) systems. However, we hypoth-\nesize that punctuation could play a significant role\nin differentiating one talk move from another. For\nexample, “Agreed?” with a question mark can be\nconsidered an instance of “Keeping everyone to-\ngether” whereas “Agreed” as a statement would be\nan instance of “None.” It remains to be determined\nthe extent to which including punctuation mark-\ners might impact the performance of the models.\nSimilarly, we can try incorporating speaker turns\nto indicate a student or teacher turn in previous and\nsubsequent utterances as additional features to the\nmodel.\n77\nTable 2: Robert-Base performance with different window sizes\nModel Context MCC F1 (%)\nRoberta-Base No Context 0.7003 71.93\nRoberta-Base Immediate Student (Suresh et al., 2022) 0.7513 76.32\nRoberta-Base Previous 1 0.7460 76.01\nRoberta-Base Previous 5 0.7579 76.79\nRoberta-Base Previous 10 0.7615 77.08\nRoberta-Base Previous 15 0.7688 77.63\nRoberta-Base Previous 17 0.7657 77.35\nRoberta-Base Subsequent 1 0.7232 74.16\nRoberta-Base Previous 1 - Subsequent 1 0.7687 78.18\nRoberta-Base Previous 2 - Subsequent 2 0.7742 78.49\nRoberta-Base Previous 3 - Subsequent 3 0.7764 78.66\nRoberta-Base Previous 5 - Subsequent 5 0.7739 78.36\nRoberta-Base Previous 7 - Subsequent 7 0.7805 78.92\nRoberta-Base Previous 8 - Subsequent 8 0.7802 78.86\nTable 3: Performance on classification of teacher talk moves on other models\nModel Context MCC F1 (%)\nRoberta-Base Previous 7 - Subsequent 7 0.7805 78.92\nMathBERT Previous 7 - Subsequent 7 0.6890 70.18\nXLNet Previous 7 - Subsequent 7 0.7709 78.06\nLongformer Previous 7 - Subsequent 7 0.7752 78.47\nBigBird Previous 7 - Subsequent 7 0.7694 77.89\nBigBird Previous 10 - Subsequent 10 0.7603 77.11\nAnother option that warrants considera-\ntion is supplementing data for the purpose\nof model pretraining. TalkMoves dataset\n(github.com/SumnerLab/TalkMoves) is a relatively\nsmall dataset for pretraining transformers when\ncompared to Roberta which was pretrained on\nmillions of data points. At the same time, we recog-\nnize the challenge in the collecting and annotating\nthousands of classroom transcripts. Moreover,\nthere are important privacy concerns and other\nethical considerations, given that these data involve\nminors, use proper names (which can be critical\ninformation for talk moves classification), and can\nbe challenging to access in large quantities. We\ncould potentially explore active learning to achieve\ngreater accuracy with limited samples (Settles,\n2009). Active learning is often sought as an option\nin machine learning applications where unlabeled\ninstances are abundantly available (Schröder et al.,\n2021).\n7 Conclusion\nDocumenting consequential elements of classroom\ninstruction and providing teachers with feedback\non their practices are critical endeavors in the edu-\ncation field. Taking into consideration the strong\nneed to provide reliable feedback to teachers on\nproductive classroom discourse, we need robust\nmodels to automatically classify teacher talk moves\nwith high reliability. In this paper, we report on\na number of experiments that involved providing\nlonger dialog windows to the transformers in an\neffort to improve model performance. Based on\nthese experiments, we generated a state-of-the-art\n2.6% F1 improvement in performance (78.92% F1)\nover the previous models, primarily by adding a\nset number of previous and subsequent utterances\nto the input representation. Clearly, there are both\nchallenges and opportunities for the development\nof innovative uses of AI techniques, particularly\nas they can be incorporated into tools that support\nteacher and student learning. The findings from\nthis research open new avenues for exploration that\ncan benefit both the education and NLP communi-\n78\nTable 4: Confusion matrix from Roberta-Base with Immediate student utterance as context\nRoberta-Base (Immediate Student) Actual Precision Recall F1\n0 - None\nPredicted\n42786 1779 67 54 232 1091 74 0.93 0.93 0.934\n1 - Keeping Everyone together 1599 6549 106 139 99 518 30 0.73 0.72 0.73\n2 - Getting students to relate 171 177 715 0 2 120 33 0.71 0.59 0.64\n3 - Restating 112 18 3 932 21 12 0 0.79 0.85 0.82\n4 - Revoicing 562 72 2 47 1063 44 0 0.72 0.59 0.62\n5 - Pressing for accuracy 762 367 105 9 51 8289 669 0.82 0.86 0.84\n6 - Pressing for reasoning 56 6 315 1 1 86 753 0.79 0.82 0.80\nTable 5: Confusion matrix from Roberta-Base with Previous-7 and Subsequent-7 utterances as context. Compared\nto Table 4, we see an improvement in F1 score for almost all of the talk moves except Restating.\nRoberta-Base (Previous 7 - Subsequent 7)Actual Precision Recall F1\n0 - None\nPredicted\n14594 522 42 40 122 312 16 0.94 0.93 0.94\n1 - Keeping Everyone together 512 2321 53 26 26 130 4 0.77 0.76 0.76\n2 - Getting students to relate 31 23 206 0 0 37 9 0.64 0.67 0.65\n3 - Restating 25 8 1 263 7 2 0 0.73 0.86 0.79\n4 - Revoicing 179 24 0 25 326 7 1 0.66 0.58 0.62\n5 - Pressing for accuracy 207 112 21 5 12 2678 41 0.84 0.87 0.85\n6 - Pressing for reasoning 8 2 1 0 0 27 242 0.77 0.86 0.82\nties who might adopt our methods in applications\nwhere the local context may prove critical to im-\nproving performance.\nAcknowledgements\nThe research team would like to thank Eddie Dom-\nbower and his team at Curve 10 for their contri-\nbutions to the design and implementation of the\nTalkBack application. This material is based upon\nwork supported by the National Science Founda-\ntion under Grant Numbers 1600325 and 1837986.\nThis research was supported by the NSF National\nAI Institute for Student-AI Teaming (iSAT) under\ngrant DRL 2019805. The opinions expressed are\nthose of the authors and do not represent views of\nthe NSF.\nReferences\nDavid Adamson, Colin Ashe, Hyeju Jang, David Yaron,\nand Carolyn Penstein Rosé. 2013. Intensification of\ngroup knowledge exchange with academically pro-\nductive talk agents. In CSCL (1), pages 10–17.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nMelissa Boston. 2012. Assessing instructional quality\nin mathematics. The Elementary School Journal ,\n113(1):76–104.\nAmber G Candela, Melissa D Boston, and Juli K Dixon.\n2020. Discourse actions to promote student access.\nMathematics Teacher: Learning and Teaching PK-12,\n113(4):266–277.\nNitesh V Chawla, Kevin W Bowyer, Lawrence O Hall,\nand W Philip Kegelmeyer. 2002. Smote: synthetic\nminority over-sampling technique. Journal of artifi-\ncial intelligence research, 16:321–357.\nGaowei Chen, Carol KK Chan, Kennedy KH Chan,\nSherice N Clarke, and Lauren B Resnick. 2020. Ef-\nficacy of video-based teacher professional develop-\nment for increasing classroom discourse and student\nlearning. Journal of the Learning Sciences , 29(4-\n5):642–680.\nDavide Chicco and Giuseppe Jurman. 2020. The advan-\ntages of the matthews correlation coefficient (mcc)\nover f1 score and accuracy in binary classification\nevaluation. BMC genomics, 21(1):1–13.\nRichard Correnti, Mary Kay Stein, Margaret S Smith,\nJames Scherrer, Margaret McKeown, James Greeno,\nand Kevin Ashley. 2015. Improving teaching at scale:\nDesign for the scientific measurement and learning of\ndiscourse practice. Socializing Intelligence Through\nAcademic Talk and Dialogue. AERA, 284.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nMeghan E Dale, Amanda J Godley, Sarah A Capello,\nPatrick J Donnelly, Sidney K D’Mello, and Sean P\nKelly. 2022. Toward the automated analysis of\nteacher talk in secondary ela classrooms. Teaching\nand Teacher Education, 110:103584.\nDorottya Demszky, Jing Liu, Heather C Hill, Dan Juraf-\nsky, and Chris Piech. 2021. Can automated feedback\n79\nimprove teachers’ uptake of student ideas? evidence\nfrom a randomized controlled trial in a large-scale\nonline course.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nPatrick J Donnelly, Nathaniel Blanchard, Andrew M\nOlney, Sean Kelly, Martin Nystrand, and Sidney K\nD’Mello. 2017. Words matter: automatic detection\nof teacher questions in live classroom discourse using\nlinguistics, acoustics, and context. In Proceedings\nof the Seventh International Learning Analytics &\nKnowledge Conference, pages 218–227. ACM.\nBeth A Herbel-Eisenmann. 2017. Mathematics Dis-\ncourse in Secondary Classrooms: A Practice-based\nResource for Professional Learning: Facilitator\nGuide. Math Solutions.\nCindy E Hmelo-Silver, Clark A Chinn, Angela M\nO’Donnell, and Carol Chan. 2013. The international\nhandbook of collaborative learning.\nJennifer Jacobs, Karla Scornavacco, Charis Harty, Abhi-\njit Suresh, Vivian Lai, and Tamara Sumner. 2022.\nPromoting rich discussions in mathematics class-\nrooms: Using personalized, automated feedback to\nsupport reflection and instructional change. Teaching\nand Teacher Education.\nEmily Jensen, Meghan Dale, Patrick J Donnelly, Cath-\nlyn Stone, Sean Kelly, Amanda Godley, and Sid-\nney K D’Mello. 2020. Toward automated feedback\non teacher discourse to enhance teacher learning. In\nProceedings of the 2020 CHI Conference on Human\nFactors in Computing Systems, pages 1–13.\nMandar Joshi, Omer Levy, Daniel S Weld, and Luke\nZettlemoyer. 2019. Bert for coreference reso-\nlution: Baselines and analysis. arXiv preprint\narXiv:1908.09091.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. arXiv preprint arXiv:1908.08593.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nHugh Mehan. 1979. Learning lessons. Harvard Univer-\nsity Press Cambridge, MA.\nSarah Michaels and Catherine O’Connor. 2015. Con-\nceptualizing talk moves as tools: Professional de-\nvelopment approaches for academically productive\ndiscussion. Socializing intelligence through talk and\ndialogue, pages 347–362.\nSarah Michaels, Mary Catherine O’Connor,\nMegan Williams Hall, and Lauren B Resnick.\n2010. Accountable talk ® sourcebook. Pittsburg,\nPA: Institute for Learning University of Pittsburgh.\nMurphy, PK, Wilkinson, IAG, Soter, AO, Hennessey,\nMN, & Alexander, JF.\nCatherine O’Connor and Sarah Michaels. 2019. Sup-\nporting teachers in taking up productive talk moves:\nThe long road to professional learning at scale. Inter-\nnational Journal of Educational Research, 97:166–\n175.\nCatherine O’Connor, Sarah Michaels, and Suzanne\nChapin. 2015. Scaling down” to explore the role\nof talk in learning: From district intervention to\ncontrolled classroom study. Socializing intelligence\nthrough academic talk and dialogue, pages 111–126.\nShuai Peng, Ke Yuan, Liangcai Gao, and Zhi Tang.\n2021. Mathbert: A pre-trained model for math-\nematical formula understanding. arXiv preprint\narXiv:2105.00377.\nSamuel L Pugh, Shree Krishna Subburaj, Arjun Ramesh\nRao, Angela EB Stewart, Jessica Andrews-Todd, and\nSidney K D’Mello. 2021. Say what? automatic mod-\neling of collaborative problem solving skills from stu-\ndent speech in the wild. International Educational\nData Mining Society.\nLauren B Resnick, Christa SC Asterhan, and Sherice N\nClarke. 2018. Accountable talk: Instructional dia-\nlogue that builds the mind. Geneva, Switzerland:\nThe International Academy of Education (IAE) and\nthe International Bureau of Education (IBE) of the\nUnited Nations Educational, Scientific and Cultural\nOrganization (UNESCO).\nChristopher Schröder, Andreas Niekler, and Martin Pot-\nthast. 2021. Uncertainty-based query strategies for\nactive learning with transformers. arXiv preprint\narXiv:2107.05687.\nE Schukat-Talamazzini, T Kuhn, and H Niemann. 1994.\nSpeech recognition for spoken dialogue systems. In\nProgress and Prospects of Speech Research and Tech-\nnology: Proc. of the CRIM/FORWISS Workshop, PAI,\nvolume 1, pages 110–120.\nBurr Settles. 2009. Active learning literature survey.\nYu Song, Shunwei Lei, Tianyong Hao, Zixin Lan, and\nYing Ding. 2021. Automatic classification of se-\nmantic content of classroom dialogue. Journal of\nEducational Computing Research, 59(3):496–521.\nAbhijit Suresh, Jennifer Jacobs, Charis Clevenger, Vi-\nvian Lai, Chenhao Tan, James H Martin, and Tamara\nSumner. 2021a. Using ai to promote equitable class-\nroom discussions: The talkmoves application. In\nInternational Conference on Artificial Intelligence in\nEducation, pages 344–348. Springer.\n80\nAbhijit Suresh, Jennifer Jacobs, Charis Harty, Margaret\nPerkoff, James H Martin, and Tamara Sumner. 2022.\nThe talkmoves dataset: K-12 mathematics lesson\ntranscripts annotated for teacher and student discur-\nsive moves. 13th International Conference on Lan-\nguage Resources and Evaluation (LREC 2022).\nAbhijit Suresh, Jennifer Jacobs, Vivian Lai, Chenhao\nTan, Wayne Ward, James H Martin, and Tamara Sum-\nner. 2021b. Using transformers to provide teach-\ners with personalized feedback on their classroom\ndiscourse: The talkmoves application. AAAI 2021\nSpring Symposium on Artificial Intelligence for K-12\nEducation.\nAbhijit Suresh, Tamara Sumner, Jennifer Jacobs, Bill\nFoland, and Wayne Ward. 2019. Automating analy-\nsis and feedback to improve mathematics teachers’\nclassroom discourse. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 33, pages\n9721–9728.\nStergios Tegos, Stavros Demetriadis, and Anastasios\nKarakostas. 2015. Promoting academically produc-\ntive talk with conversational agent interventions in\ncollaborative learning settings. Computers & Educa-\ntion, 87:309–325.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nLev Vygotsky. 1978. Interaction between learning and\ndevelopment. Readings on the development of chil-\ndren, 23(3):34–41.\nNoreen M Webb, Megan L Franke, Marsha Ing, Angela\nChan, Tondra De, Deanna Freund, and Dan Battey.\n2008. The role of teacher instructional practices in\nstudent collaboration. Contemporary educational\npsychology, 33(3):360–381.\nMikyung Kim Wolf, Amy C Crosson, and Lauren B\nResnick. 2005. Classroom talk for rigorous read-\ning comprehension instruction. Reading Psychology,\n26(1):27–53.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. Advances in Neural\nInformation Processing Systems, 33:6256–6268.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33:17283–17297.\n81",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7937672734260559
    },
    {
      "name": "Computer science",
      "score": 0.7358807921409607
    },
    {
      "name": "Encoder",
      "score": 0.5766239166259766
    },
    {
      "name": "Natural language processing",
      "score": 0.5287677049636841
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5214994549751282
    },
    {
      "name": "Representation (politics)",
      "score": 0.4325869679450989
    },
    {
      "name": "Engineering",
      "score": 0.10407742857933044
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I188538660",
      "name": "University of Colorado Boulder",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2802236040",
      "name": "University of Colorado System",
      "country": "US"
    }
  ]
}