{
  "title": "Dynamic Routing Transformer Network for Multimodal Sarcasm Detection",
  "url": "https://openalex.org/W4385572265",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1038466737",
      "name": "Yuan Tian",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2049574243",
      "name": "Nan Xu",
      "affiliations": [
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2741107239",
      "name": "Ruike Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2164388523",
      "name": "Wenji Mao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285304773",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3205872166",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2951937667",
    "https://openalex.org/W2727849499",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4385573263",
    "https://openalex.org/W3207396986",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2098437222",
    "https://openalex.org/W2760103357",
    "https://openalex.org/W3203354307",
    "https://openalex.org/W2575367545",
    "https://openalex.org/W2951678842",
    "https://openalex.org/W2250710744",
    "https://openalex.org/W2913873497",
    "https://openalex.org/W2099653665",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3105203922",
    "https://openalex.org/W3102848065",
    "https://openalex.org/W2489370933",
    "https://openalex.org/W3155230099",
    "https://openalex.org/W3035565303"
  ],
  "abstract": "Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity. Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods. Our codes are available at https://github.com/TIAN-viola/DynRT.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2468–2480\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDynamic Routing Transformer Network\nfor Multimodal Sarcasm Detection\nYuan Tian1,2, Nan Xu1,3*, Ruike Zhang1,2, Wenji Mao1,2*\n1Institute of Automation, Chinese Academy of Sciences\n2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences\n3Beijing Wenge Technology Co., Ltd\n{tianyuan2021,xunan2015,zhangruike2020,wenji.mao}@ia.ac.cn\nAbstract\nMultimodal sarcasm detection is an impor-\ntant research topic in natural language process-\ning and multimedia computing, and beneﬁts\na wide range of applications in multiple do-\nmains. Most existing studies regard the in-\ncongruity between image and text as the in-\ndicative clue in identifying multimodal sar-\ncasm. To capture cross-modal incongruity,\nprevious methods rely on ﬁxed architectures\nin network design, which restricts the model\nfrom dynamically adjusting to diverse image-\ntext pairs. Inspired by routing-based dynamic\nnetwork, we model the dynamic mechanism\nin multimodal sarcasm detection and propose\nthe Dynamic Routing Transformer Network\n(DynRT-Net). Our method utilizes dynamic\npaths to activate different routing transformer\nmodules with hierarchical co-attention adapt-\ning to cross-modal incongruity. Experimental\nresults on a public dataset demonstrate the ef-\nfectiveness of our method compared to the state-\nof-the-art methods. Our codes are available at\nhttps://github.com/TIAN-viola/DynRT.\n1 Introduction\nSarcasm is a widely used ﬁgurative language to\ngive the ironic expression in our daily life, which\ntypically means the opposite of what it really wants\nto express (Joshi et al., 2017). As an important\nstep to analyze people’s opinions and sentiments in\ncommunication, sarcasm detection beneﬁts a wide\nrange of applications such as natural language di-\nalogue (Tepperman et al., 2006), public opinion\nmining (Riloff et al., 2013) and social media anal-\nysis (Tsur et al., 2010). With the rapid growth\nof multimodal user-generated content, multimodal\nsarcasm detection has gained increasing research\nattention in recent years (Cai et al., 2019; Xu et al.,\n2020; P a ne ta l ., 2020; Wang et al., 2020; Liang\net al., 2021; Pramanick et al., 2022; Liang et al.,\n*Corresponding author\n(a) hey <user> look at this \nreally full bag of chips I got\n(c) great park job !\n (d) what a wonderful \nweather !\n(b) well that looks \nappetising ... #ubereat\nFigure 1: Examples of Twitter data with sarcasm. (a)\nA handful of chips in the picture is contrastive to the\nmeaning of “full bag of chips” in the text. (b) There\nis a contrast between sick pizza in the image and the\nexpression “looks appetising” in the text. (c) The angry\nfeeling evoked by the park job in the picture is inconsis-\ntent with the pleasant feeling conveyed by “great park\njob” in the text. (d) The gloomy mood evoked by the\nrainy weather in the picture is inconsistent with the joy-\nful mood conveyed by “what a wonderful weather” in\nthe text.\n2022; Liu et al., 2022), and has become an impor-\ntant research topic in natural language processing\nand multimedia computing.\nThe sarcastic clues of multimodal contents are\nmainly relevant to the incongruity across image and\ntext (Xu et al., 2020; P a ne ta l ., 2020; Wang et al.,\n2020; Liang et al., 2021; Pramanick et al., 2022;\nLiang et al., 2022; Liu et al., 2022). Existing stud-\nies model this characteristic of incongruity between\nimage and text with various approaches, includ-\ning decomposition and relation network (Xu et al.,\n2468\n2020), attention mechanisms (Wang et al., 2020;\nPan et al., 2020), graph-based methods (Liang et al.,\n2021, 2022), and optimal transport method (Pra-\nmanick et al., 2022). In addition, external knowl-\nedge is also introduced to boost the performance of\nmultimodal sarcasm detection (Liu et al., 2022).\nAs it is shown in multimodal samples in Figure1,\nthere are diverse kinds of sarcastic image-text pairs.\nIn some cases, the image and text express the incon-\ngruous meaning with local segments, where visual\nregions or objects are contrastive to the meaning of\nwords or phrases in the text, as those in Figure1\n(a) and (b). In other cases, the feelings implied in\nthe image and text respectively are totally opposite,\nas those in Figure1 (c) and (d). To detect these sar-\ncastic image-text pairs, current approaches mainly\nfocus on modeling the cross-modal incongruity.\nHowever, these methods rely on static networks to\ncapture the characteristic of incongruity, which use\nﬁxed architectures on different kinds of inputs, thus\nlacking the ﬂexibility to adapt to diverse image-text\npairs.\nTo tackle this problem, the dynamic aspect of\nincongruity between image and text should be con-\nsidered. One possible solution is to model dy-\nnamic mechanism with a routing-based dynamic\nnetwork, where a series of modules can capture the\nincongruity between image and text dynamically\nvia selecting one or more most suitable modules\naccording to different image-text pairs. Existing\nrouting-based method in multimodal dynamic net-\nworks (Zhou et al., 2021) performs routing only on\nsingle-modality data, which is insufﬁcient to model\nthe dynamic image-text incongruity in cross-modal\nsarcasm detection. Therefore, we extend the ex-\nisting routing scheme to multimodal setting with\ndynamic network design, aiming to better model\nthe dynamic mechanism for multimodal sarcasm\ndetection.\nIn this paper, we propose a novel Dynamic\nRouting Transformer Network, namely DynRT-\nNet, whose router helps model route on dynamic\nrouting transformer modules with hierarchical\nco-attention adapting to cross-modal incongruity\nprevalent in diverse image-text pairs. The main\ncontributions of our work are as follows:\n• We identify the diversity of image-text sarcas-\ntic pairs, and for the ﬁrst time, model cross-\nmodal incongruity with dynamic network de-\nsign, which focuses on the dynamic mecha-\nnism for multimodal sarcasm detection.\n• We propose a dynamic routing transformer\nnetwork via adapting dynamic paths to hier-\narchical co-attention between image and text\nconditioned on multimodal samples, which is\ncapable of capturing cross-modal incongruity\ndynamically.\n• Experimental results on a public dataset\ndemonstrate the effectiveness of our proposed\nmethod for multimodal sarcasm detection.\n2 Related Work\n2.1 Image-text Sarcasm Detection\nTraditional sarcasm detection mainly studies the\nsarcastic information in textual utterances (Zhang\net al., 2016; T a ye ta l ., 2018). With the prevalence\nof social media, many people tend to express their\nthoughts with sarcasm using both textual and visual\nmessages online. Early studies utilize simple fusion\nmethods of visual and textual information for mul-\ntimodal sarcasm classiﬁcation, such as concatena-\ntion of textual and visual embeddings (Schifanella\net al., 2016) or hierarchical fusion representation\nof modalities (Cai et al., 2019). As multimodal\nsarcasm is often associated with an implicit incon-\ngruity between image and text, some studies cap-\nture this basic characteristic to detect multimodal\ncontrast from various perspectives, such as model-\ning cross-modality contrast and semantic associa-\ntion simultaneously (Xu et al., 2020) or modeling\nintra-modality and inter-modality incongruity us-\ning attention mechanisms (Wang et al., 2020; Pan\net al., 2020).\nTo represent more explicit incongruous relations,\nrecent studies employ graph convolution networks\nto construct in-modal and cross-modal graphs for\nthis task (Liang et al., 2021, 2022). Furthermore,\nPramanick et al. (2022) utilize self-attention to\nmodel the intra-modal relation and optimal trans-\nport to model the cross-modal relation for multi-\nmodal sarcasm detection. In addition, Liu et al.\n(2022) explore external knowledge resources like\nimage captions to enhance the model performance\nfor image-text sarcasm detection.\nDespite the promising results achieved for image-\ntext sarcasm detection, existing approaches rely on\nﬁxed architectures in network design. And thus, the\ncomputation mechanism to capture the cross-modal\nincongruity is static, which hinders the model from\ndynamically adjusting to diverse multimodal sam-\nples.\n2469\nEncoding ClassificationDynamic Routing Transformer\nRoBERTa ViT\nMean\nMean\n+\n Classifier\n...\n...\nI\nT\n[CLS]\njust \nlove \nwhat\ntexas\n weather\n does\nRouter Router Router Router\nSarcastic/\nNon-sarcastic\nMulti-Head\nCo-Attention \nRouting\nAdd & Norm\nMulti-Head\nSelf-Attention\nAdd & Norm\nFFN\nAdd & Norm\nTI\nQKV\nMulti-Head\nCo-Attention \nRouting\nAdd & Norm\nMulti-Head\nSelf-Attention\nAdd & Norm\nFFN\nAdd & Norm\nTI\nQKV\nFigure 2: Overall architecture of our proposed DynRT-Net for multimodal sarcasm detection. Cylinders in different\ncolors denote hierarchical co-attentions between textual tokens and visual patches in dynamic routing transformer\nlayers.\n2.2 Multimodal Dynamic Networks\nMultimodal dynamic networks have shown good\nperformance on multimodal tasks (de Vries et al.,\n2017; Perez et al., 2018; Zhou et al., 2021; Qu\net al., 2021), which can be roughly divided into\ntwo categories: dynamic parameters and dynamic\narchitectures. A typical model with dynamic pa-\nrameters adapts its weights based on different in-\nputs in the inference stage. For example,Perez\net al.(2018) propose a model to adjust the param-\neters of ResNet conditioned on the text informa-\ntion for visual reasoning. Dynamic architectures\nadapt the network depth and width or perform rout-\ning according to different inputs. For example,\nZhou et al.(2021) design a data-dependent rout-\ning scheme called Transformer Routing (TRAR)\nto dynamically select image attentions for visual\nquestion answering.\nRouting-based method has the potential to dy-\nnamically identify cross-modal incongruity via ac-\ntivating different modules dynamically conditioned\non different image-text inputs. However, the cur-\nrent work TRAR only performs routing on single-\nmodality data. To better model the dynamic mech-\nanism in cross-modal sarcasm detection, we extend\nthe existing routing scheme to multimodal setting\nwith dynamic network design.\n3 Method\nFigure 2 shows the overall architecture of our\nproposed dynamic routing transformer network\nDynRT-Net, which is composed of three compo-\nnents: encoding, dynamic routing transformer, and\nclassiﬁcation. We ﬁrst encode the text and a paired\nimage into multimodal features respectively via\ntwo pre-trained models. Then, we feed them into\nthe dynamic routing transformer to route on hier-\narchical co-attention dynamically and learn cross-\nmodal incongruity, resulting in the routed features\nwith cross-modal information. Finally, we feed the\nrouted features and image features into the classi-\nﬁer for multimodal sarcasm classiﬁcation.\n3.1 Encoding\nText Encoder To train our model from a good\nstart of text embeddings, we use the pre-trained\nmodel RoBERTa (Liu et al., 2019) as the text\nencoder, which has implicitly acquired world\nknowledge from the large-scale dataset. We ﬁrst\nsplit the text into a sequence of tokens\nText =\n{[CLS],w1,...,w n−1}, where [CLS] denotes\nthe global token andn is the length of all the to-\nkens. After that, we feedText into RoBERTa and\nget text featuresT ∈ Rn×dt , which are represented\nby\nT = RoBERTa(Text)=[ t1,t2,...,t n], (1)\nwhere ti ∈ Rdt is the text embedding ofi-th to-\nken wi in the text anddt is the dimension of text\nembedding.\nImage Encoder To train our model from a good\nstart of image embeddings, we use a pre-trained Vi-\nsion Transformer (ViT) model (Dosovitskiy et al.,\n2470\n$GG\u0003\t\u00031RUP\n0XOWL\u0010+HDG\n6HOI\u0010$WWHQWLRQ\n$GG\u0003\t\u00031RUP\n))1\n$GG\u0003\t\u00031RUP\nTI\n4.9\n\u000bD\f\u00036WDQGDUG\n0XOWL\u0010+HDG\u0003\n6HOI\u0010$WWHQWLRQ\u0003\n5RXWLQJ\n$GG\u0003\t\u00031RUP\n0XOWL\u0010+HDG\n6HOI\u0010$WWHQWLRQ\n$GG\u0003\t\u00031RUP\n))1\n$GG\u0003\t\u00031RUP\nIT\n4.9\n\u000bE\f\u000375$5\n0XOWL\u0010+HDG\u0003\n&R\u0010$WWHQWLRQ\u0003\n5RXWLQJ\u0003\n$GG\u0003\t\u00031RUP\n0XOWL\u0010+HDG\n6HOI\u0010$WWHQWLRQ\n$GG\u0003\t\u00031RUP\n))1\n$GG\u0003\t\u00031RUP\nTI\n4.9\n\u000bF\f\u0003'\\Q57\n0XOWL\u0010+HDG\n6HOI\u0010$WWHQWLRQ\nFigure 3: Comparison among the standard multimodal\ntransformer layer, TRAR layer, and our DynRT layer.I\nand T denote the features of image and text modalities\nrespectively. TRAR employs routing on different atten-\ntion grids onI before the interaction of two modalities.\nOur DynRT performs routing on the co-attention of two\nmodalities. Add & Norm denotes addition and layer\nnormalization. FFN denotes the feed-forward network.\n2021) as the image encoder, which has recently\nachieved excellent performance. We ﬁrst split an\nImage ∈ RH×W ×C into a sequence of m ﬂat-\ntened 2D patches, whereH, W and C denote the\nheight, width, and the number of channels of the\nimage. After that, we feedImage into ViT and get\nimage featuresI ∈ Rm×dv of patches, which are\nrepresented by\nI =V i T(Image)=[ e1,e2,...,e m], (2)\nwhere ej ∈ Rdv is the image embedding ofj-th\npatch in the image anddv is the dimension of image\nembedding.\n3.2 Dynamic Routing Transformer\nPrevious approaches (Xu et al., 2020; Pan et al.,\n2020; Wang et al., 2020; Liang et al., 2021; Praman-\nick et al., 2022; Liang et al., 2022; Liu et al., 2022)\ncapture the incongruity between image and text for\nmultimodal sarcasm detection in a static manner,\nand thus are unable to dynamically adjust to diverse\nimage-text pairs. To ﬁll this gap, we propose the\nDynamic Routing Transformer (DynRT), which\nperforms routing on hierarchical co-attention of\ntwo modalities to capture cross-modal incongruity\nadapting to different image-text inputs.\n3.2.1 Routing Space\nIn the Dynamic Routing Transformer, we feed the\ntextual and visual embeddings to several DynRT\nlayers, which can be calculated as\nTk =D y n R Tk(Tk−1,I),k ∈ [1,K], (3)\nwhere Tk is the output ofk-th DynRT layer,T0 =\nT is the input of the ﬁrst layer,K is maximum\nindex of DynRT layers, and the output of the last\nDynRT layerTK is the ﬁnal routed features.\n3.2.2 Dynamic Routing Transformer Layer\nUnlike the previous dynamic method TRAR (Zhou\net al., 2021), which performs routing on attention\ngrids of one modality, our DynRT layer routes on\nhierarchical co-attention of image and text con-\nditioned on different inputs (see Figure3 for a\ndetailed comparison). Our DynRT layer is com-\nposed of a multi-head co-attention routing (MH-\nCAR) module (pink rectangle in Figure3 (c)), a\nmulti-head self-attention (MHA) module and a\nfeed-forward network (FFN), where a residual con-\nnection and a normalization layer (LN) (Ba et al.,\n2016) follow each module. Thek-th DynRT layer\ncan be formulated as\nTr\nk−1 = LN(MHCARk(Tk−1,I)+ Tk−1), (4)\nTa\nk−1 = LN(MHAk(Tr\nk−1)+ Tr\nk−1), (5)\nTk = LN(FFNk(Ta\nk−1)+ Ta\nk−1), (6)\nwhere k ∈ [1,K] is the index of DynRT layers,\nTk ∈ Rn×dt is the output ofk-th DynRT layer,\nTr\nk−1 ∈ Rn×dt and Ta\nk−1 ∈ Rn×dt are the output of\nMHCAR module and MHA module respectively.\nThe MHCAR ink-th DynRT layer performsh\nheads of attention functions in parallel with the\nhidden dimensiondh (dh = dt/h) which are con-\ncatenated and then projected, resulting in the ﬁnal\nvalues of the MHCAR, which is calculated as\nMHCARk(Tk−1,I)=c o n c a t\n(\n[headk\ni ]h\ni=1\n)\nOk\nT ,\n(7)\nwhere concat(·) is the concatenation operation,\nOk\nT ∈ Rdt×dt is the projection matrix and ev-\nery head headk\ni ∈ Rn×dh is calculated by a co-\nattention routing (CAR) function, which routes\non co-attention (CA) functions with different co-\nattentions:\nheadk\ni =C A Rk\ni\n(Tk−1,I)\n=\npk−1∑\nj=0\nαk\nj CAk\ni,j\n(Qi,j,k,Ki,j,k,V k\ni,j,Aj)\n=\npk−1∑\nj=0\nαk\nj σ\n(\nQi,j,kK⊤\ni,j,k√dh\n⊗Aj\n)\nVk\ni,j,\n(8)\n2471\nwhere σ(·) denotes the softmax function, αk\nj is\nthe routing probability weight ofj-th CA func-\ntion with one kind of co-attention maskAj be-\ntween image and text, pk is the number of CA\nfunctions in k-th layer (we set pk = k in our\nmodel), Mi,j,k = Qi,j,kK⊤\ni,j,k ∈ Rn×m is the at-\ntention matrix between two modalities inheadk\ni ,\nQi,j,k = Tk−1WQ\ni,j,k,Ki,k = IWK\ni,j,k, Vk\ni,j =\nIWV\ni,j,k, WQ\ni,j,k ∈ Rdt×dh , WK\ni,j,k ∈ Rdv×dh and\nWV\ni,j,k ∈ Rdv×dh are parameter matrices,K⊤\ni,j,k de-\nnotes the transpose of matrixKi,j,k, and⊗denotes\nelement-wise matrix product. The hierarchical co-\nattention mechanism and construction ofAj will\nbe presented in the following section3.2.3. The\nprediction of αk\nj is controlled by a router, which\nwill be presented in the following section3.2.4.\nTo reduce the computation of the routing process\nin Eq. (8), we followZhou et al.(2021) to redeﬁne\nthe headk\ni as\nheadk\ni = σ\n⎛\n⎝Qi,kK⊤\ni,k√dh\n⊗\npk−1∑\nj=0\nαk\nj Aj\n⎞\n⎠Vk\ni . (9)\n3.2.3 Hierarchical Co-attention\nWe ﬁrst describe how to construct the co-attention\nmask matrixAj in Eq.(8)(9). Aj restricts the re-\ngion of the image that text can see in the CA func-\ntion. Thes-order sliding window with a small patch\nof (2s+1) ×(2s+1) grid traverses every patch of\nthe image to get mask vectorvs\nl ∈ Rm (l ∈ [1,m]),\nwhose visualization is shown in Figure4. We con-\nstruct As by stacking the vectorvs\nl for n times (n\nis the length of tokens) fromvs\n1 to vs\nm circularly:\nAs =[ vs\n1,vs\n2,...,v s\nn] ∈ Rn×m. (10)\nSpeciﬁcally, A0 is an empty mask matrix, i.e. a\nmatrix of all the ones, which gives words or global\ntoken [CLS] the opportunity to see the whole im-\nage.\nTo model the cross-modal incongruity in di-\nverse image-text pairs gradually, we then design\nthe hierarchical co-attention via making the kinds\nof co-attention masks diverse progressively with\nthe increase of DynRT layers, the architecture of\nwhich is shown in Figure 2. In the\nk-th layer\nof DynRT, the group of co-attention mask ma-\ntrices in Eq.\n(8)(9) that router can route on is\nGk =[ A0,A1,...,A pk−1], where pk = k is the\nnumber of mask matrices in k-th DynRT layer\n(pk also equals to the number of CA functions in\nEq. (8)(9)).\n]0,0,0,1,1,1,1,1,1[1\n2  v ]0,0,0,1,1,0,1,1,0[1\n3  v\n]0,1,1,0,1,1,0,1,1[1\n4  v ]1,1,1,1,1,1,1,1,1[1\n5  v ]1,1,0,1,1,0,1,1,0[1\n6  v\n]0,1,1,0,1,1,0,0,0[1\n7  v ]1,1,1,1,1,1,0,0,0[1\n8  v ]1,1,0,1,1,0,0,0,0[1\n9  v\n]0,0,0,0,1,1,0,1,1[1\n1  v\nFigure 4: Visualization of the mask vectors with 1-order\nsliding window. In this example, the size of the sliding\nmask patch is 3 × 3 and the dimension of the mask\nvector m is 9. The red cross denotes the center of the\nsliding mask patch.\n3.2.4 Router\nThe routing probabilityαk =[ αk\n0,αk\n1,...,α k\npk−1]\nfor k-th DynRT layer can be obtained by the router\nconditioned on the input, which is calculated as\nαk = σg (MLP(APool( I))) ∈ Rpk , (11)\nwhere σg(·)is Gumble Softmax (Zhou et al., 2021)\nwith temperaturet, APool(·)is a 1D adaptive aver-\nage pooling over all the embeddings of patches in\nthe image,MLP is a two-layer multilayer percep-\ntron with hidden dimensiondm, andpk is also the\nnumber of co-attention mask matrices in thek-th\nDynRT layer whereαk works in Eq. (8)(9).\n3.3 Classiﬁcation\nFinally, we project the image featuresI and routed\nfeatures TK into global embeddings and predicts\nsarcastic tendency, which can be formulated as\nIg =M e a n (I), (12)\nTg =M e a n (TK), (13)\nyg = Wg(LN(Ig +Tg))+ bg, (14)\nˆy = Softmax(Woyg +bo), (15)\nwhere Mean(·) is the average function on all the\npatch embeddings inI and all the token embed-\ndings inTK, Ig ∈ Rdv and Tg ∈ Rdt denote global\nembeddings of image and text respectively,LN(·)\nis the layer normalization ,yg ∈ Rd is the global\nmultimodal embedding (consideringdv = dt = d\n2472\nTraining Development Testing\nSarcastic 8642 959 959\nNon-sarcastic 11174 1451 1450\nTotal 19816 2410 2409\nTable 1: The statistics of the MSD dataset\nin our model, we omit the process of projecting\nembeddings of two modalities into the same di-\nmension),\nWg ∈ Rd×d, bg ∈ Rd, Wo ∈ Rdp×d\nand bo ∈ Rdp are trainable parameters,Softmax(·)\nis the softmax function,ˆy ∈ Rdp is the predicted\nprobability of all the possible labels, anddp is the\nnumber of possible labels (i.e. sarcastic and non-\nsarcastic).\n3.4 Optimization\nWe optimize our model with cross-entropy loss,\nwhich is most commonly used in classiﬁcation:\nL = −\nN∑\ni=1\ny⊤\ni logˆyi, (16)\nwhere yis the ground truth andˆyi is the probability\nof predicted label fori-th image-text pair.\n4 Experiments\n4.1 Dataset\nWe evaluate our method on the Multimodal Sar-\ncasm Detection (MSD) dataset (Cai et al., 2019),\nwhich is the only benchmark dataset for multi-\nmodal sarcasm detection. Cai et al. (2019) col-\nlect original image-text pairs from Twitter and\nrandomly divide this dataset into the training set,\ndevelopment set, and test set with the ratio of\n80%:10%:10%. The statistics of the MSD dataset\nare shown in Table1. Cai et al.(2019) further dis-\ncard tweets with regular words (sarcasm, sarcastic,\nreposting, irony, ironic, jokes, humor, humour and\nexgag) and URLs, and replace mentions with a cer-\ntain symbol⟨user⟩. For a fair comparison, we use\nthe MSD dataset after the above data preprocessing\nfor experimentation, following the convention of\nall the previous studies.\n4.2 Experimental Settings\nThe values of hyper-parameters are shown in Ta-\nble 2. More information about experimental set-\ntings is shown in AppendixB.\nNotation Value Description\nn 100 maximum length of text tokens\nm 49 number of image patches\nK 4 number of DynRT layers\nh 2 number of heads in MHCAR\ndm 384 hidden dimension of MLP\ndv 768 dimension of image embedding\ndt 768 dimension of text embedding\nd 768 dimension of multimodal embedding\nt 10 temperature of Gumble Softmax\nTable 2: The hyper-parameter values in our model.\n4.3 Baseline Methods\nWe compare our method with existing unimodal\nbaselines and representative methods for multi-\nmodal sarcasm detection.\nImage-modality methods. The baseline meth-\nods using the image information for sarcasm detec-\ntion are as follows:\n• ResNet (Cai et al., 2019) uses the image em-\nbedding of the pooling layer of ResNet (He\net al., 2016) for sarcasm classiﬁcation;\n• ViT (Dosovitskiy et al., 2021) is a pre-trained\nvision model based on Transformer architec-\nture, which achieves excellent results.\nText-modality methods.The baseline methods\nusing text information for sarcasm detection are as\nfollows:\n• TextCNN (Kim, 2014) is a network based on\nCNN for textual classiﬁcation;\n• Bi-LSTM (Liang et al., 2022) is a Bi-LSTM\nnetwork for textual classiﬁcation;\n• SIARN (Tay et al., 2018) employs the atten-\ntion mechanism for textual sarcasm detection;\n• SMSD (Xiong et al., 2019) proposes a self-\nmatching network for sarcasm detection;\n• BERT (Devlin et al., 2019) is a classical pre-\ntrained language model;\n• RoBERTa (Liu et al., 2019) is an optimized\nBERT pre-trained language model.\nMultimodal methods. The representative meth-\nods employing both image and text for sarcasm\ndetection are as follows:\n• HFM (Cai et al., 2019) fuses the information\nof text, image, and image attributes with a\nhierarchical network;\n2473\n• D&R Net(Xu et al., 2020) uses a decomposi-\ntion network and a relation network to exploit\nthe contrastive and relative relationship be-\ntween image and text;\n• IIMI-MMSD (P a ne ta l ., 2020) utilizes self-\nattention and co-attention mechanisms to\nmodel the intra-modality and inter-modality\nincongruity between image and text;\n• Bridge (Wang et al., 2020) proposes a bridge\nlayer based on RoBERTa and ResNet to cap-\nture the relationship between two modalities;\n• InCrossMGs (Liang et al., 2021) utilizes a\ngraph-based model to capture sarcastic rela-\ntions between image and text;\n• MuLOT (Pramanick et al., 2022) employs\nself-attention to learn intra-modal correspon-\ndence and optimal transport to learn cross-\nmodal correspondence;\n• CMGCN (Liang et al., 2022) proposes cross-\nmodal graphs based on attribute-object pairs\nof image objects to capture sarcastic clues;\n• Hmodel (Liu et al., 2022) models both atomic-\nlevel incongruity and composition-level con-\ngruity with attention mechanism and graph\nneural networks respectively;\n• HKEmodel (Liu et al., 2022) incorporates\nimage captions as the external knowledge to\nenhance the ability ofHmodel to detect mul-\ntimodal sarcasm, which is the state-of-the-art\nmodel in multimodal sarcasm detection.\n4.4 Main Results\nFollowing Liang et al.(2022), we use accuracy and\nmacro-average F1-score as the evaluation metrics.\nTable 3 shows the comparative results of the repre-\nsentative methods and our method, which demon-\nstrate that our proposed method outperforms all\nthe baseline methods and achieves signiﬁcant gains\ncompared with the state-of-the-art method. For\nunimodal methods, text-modality methods achieve\nbetter performances than image-modality meth-\nods, which shows that textual information provides\nmore sarcastic clues within modality than visual in-\nformation. Compared with unimodal methods, mul-\ntimodal methods perform better, which indicates\nthat cross-modal interaction is important to capture\nModality Method F1 Acc\nImage ResNet (Cai et al., 2019) 61.53 ∗ 64.76∗\nViT (Dosovitskiy et al., 2021) 66.90 ± 0.09 68.79 ± 0.17\nText\nTextCNN (Kim, 2014) 78.15 ∗ 80.03∗\nSIARN (T a ye ta l ., 2018) 79.57 ∗ 80.57∗\nSMSD (Xiong et al., 2019) 79.51 ∗ 80.90∗\nBi-LSTM (Liang et al., 2022) 80.55 ∗ 81.09∗\nBERT (Devlin et al., 2019) 81.09 ∗ 83.85∗\nRoBERTa (Liu et al., 2019) 83.42 ± 0.22 83.94 ± 0.14\nHFM (Cai et al., 2019) 80.18 ∗ 83.44∗\nD&R Net (Xu et al., 2020) 80.60 ∗ 84.02∗\nIIMI-MMSD (P a ne ta l ., 2020) 82.92 ∗ 86.05∗\nBridge (Wang et al., 2020) 86.05 88.51\nImage InCrossMGs ( Liang et al., 2021) 85.60 ∗ 86.10∗\n+ MuLOT ( Pramanick et al., 2022) 86.33 87.41\nText CMGCN ( Liang et al., 2022) 87.00 ∗ 87.55∗\nHmodel† (Liu et al., 2022) 88.92 ± 0.51 89.34 ± 0.52\nHKEmodel† (Liu et al., 2022) 89.24 ± 0.24 89.67 ± 0.23\nDynRT-Net† 93.21 ± 0.06▲ 93.49 ± 0.05▲\nTable 3: Results of the comparative methods and DynRT-\nNet on the MSD dataset. The results of baselines with\n∗ are retrieved from (Liang et al., 2022). † indicates\nthat these methods use the same RoBERTa and ViT as\nbackbones. ▲ represents that our method is statistically\nsigniﬁcantly different from the Hmodel and HKEmodel\n(p< 0.001).\nmultimodal sarcastic meanings in image-text pairs.\nThe pre-trained models, which have learned large\nworld knowledge related to background informa-\ntion of the multimodal sarcasm, help recent meth-\nods achieve signiﬁcant improvements compared\nwith HFM and D&R Net, which use shallow net-\nworks to model the interaction between image and\ntext. IIMI-MMSD, Bridge, InCrossMGs, MuLOT,\nCMGCN and Hmodel provide multiple perspec-\ntives to capture the implicit incongruity in image-\ntext pairs for cross-modal sarcasm detection and\nachieve gradually improved performances. How-\never, their architectures are static and inﬂexible,\nleading to computing redundancy and lacking the\nadaptability to diverse image-text pairs. In contrast,\nour method gains a great increase via adapting dy-\nnamic paths to hierarchical co-attention of image\nand text with dynamic network design. In addi-\ntion, our method also performs better than HKE-\nmodel, which uses external knowledge to enhance\nthe performance. This result further veriﬁes the\neffectiveness of our simple and dynamic method\nin capturing the cross-modal incongruity between\nimage and text.\n4.5 Ablation Study\nWe conduct the ablation study to evaluate the\nimpact of different components in our proposed\nmodel, using the following variants:\n• DynRT-Net (pk = K): sets thepk in each\n2474\nVariant Evaluation Metric\nF1 Acc Δ F1 Δ Acc\nDynRT-Net 93.21 93.49 - -\nDynRT-Net (pk = K) 91.08 91.40 -2.13 -2.09\nDynRT-Net (pk = K −k +1) 91.21 91.50 -2.00 -1.99\n- DynRT, + TRAR 89.67 90.07 -3.54 -3.42\n- DynRT, + Standard Transformer 87.83 88.22 -5.38 -5.27\n- DynRT, + Concatenation 66.57 68.89 -26.64 -24.60\n- Dynamic attention, + mean attention 84.91 85.44 -8.30 -8.05\n- Dynamic attention, + ﬁxed attention 75.81 76.54 -17.40 -16.95\nTable 4: Results of the ablation study.\nDynRT layer asK, which connects the same\nfour DynRT layers with four co-attention\nmask matrices to replace DynRT layers with\nhierarchical co-attention in our model;\n• DynRT-Net (pk = K − k +1 ): setspk as\nK −k +1, which reduces the number of the\ntypes of co-attention mask matrices from four\nto one with the increase of DynRT layers;\n• - DynRT, + TRAR: replaces the DynRT\nlayer in our model with another routing-based\nscheme TRAR layer;\n• - DynRT, + Standard Transformer: replaces\nthe DynRT layer with the standard multimodal\ntransformer layer;\n• - DynRT, + Concatenation: removes DynRT\nlayers in our model and feeds the concatena-\ntion of classiﬁcation vectors of text encoder\nand image encoder to the ﬁnal classiﬁer;\n• - Dynamic attention, + mean attention: re-\nplaces the dynamic attention scores predicted\nby the router with the average distribution of\nattention scores in every DynRT layer;\n• - Dynamic attention, + ﬁxed attention: re-\nplaces the dynamic attention score for the\nempty co-attention mask matrix with 1 and re-\nplaces the dynamic attention scores for other\ntypes of co-attention mask matrices with 0 in\nevery DynRT layer.\nTable 4 shows the results of the ablation study.\nWe ﬁrst extensively explore different ways of ar-\nrangement of co-attention mask matrices which\nare controlled by the parameterpk in k-th DynRT\nlayer. In our model, the kinds of co-attention mask\nmatrices increase progressively with the rising of\nDynRT layers (\npk = k). When we connect the\nFigure 5: Results of our model with different DynRT\nlayers.\nsame four DynRT layers with four types of co-\nattention mask matrices, the performance reduces\non both metrics. When the number of the types\nof co-attention mask matrices decreases with the\nincrease of DynRT layers, the performance drops.\nThe above variants show the effectiveness of our\nhierarchical co-attention, as increasing the types\nof co-attention mask matrices with the rising of\nDynRT layers gradually increases the degree of di-\nversity of the model, which beneﬁts the process of\nlearning the cross-modal incongruity according to\ndiverse image-text pairs.\nTo evaluate the effectiveness of DynRT, which\nwe design for multimodal sarcasm detection, we\nreplace DynRT with other multimodal modules.\nReplacing DynRT with another routing-based dy-\nnamic scheme TRAR leads to a drop in perfor-\nmances, indicating that performing dynamic rout-\ning on unimodality only is insufﬁcient to detect\nmultimodal sarcasm. Using the standard multi-\nmodal transformer layer to replace our DynRT\nlayer gets rid of the dynamic ability, thus perform-\ning worse, which further shows the advancement of\nour proposed dynamic module in modeling cross-\nmodal incongruity. Ablating all the DynRT layers\nwith the concatenation of classiﬁcation vectors of\ntext encoder and image encoder sharply slashes the\nresults, which directly shows the advantage of our\nproposed DynRT.\nTo verify the effectiveness of dynamic attention\npredicted by the router in our model, we directly re-\nplace the dynamic attention scores with average\nprobability or use ﬁxed attention only focusing\non empty mask matrices, leading to poorer per-\nformances, as the router predicts dynamic attention\nscores to balance the co-attention between image\n2475\n(b) thanks for the awesome , leaky , cup <user> … \nmaking my morning just so much better …\n (a) great park job !\nawesome\n cup\nleaky\nthanks\ngreat\n job\npark\n !\nLayer 1\n Layer 1\nawesome\n cup\nleaky\nthanks\n!\ngreat\n park\n job\nLayer 2\n Layer 2\nawesome\n cup\nleaky\nthanks\n!\ngreat\n park\n job\nLayer 3\n Layer 3\nthanks\n leaky\n cup\nawesome\ngreat\n park\n job\n !\n Layer 4Layer 4\nFigure 6: Visualization of attentions between every text token and image patches in different DynRT layers.\nand text for detecting sarcastic incongruity accord-\ning to different inputs. Besides, we can see that the\nvariants with dynamic design perform better com-\npared with the variants with static design, which\nfurther veriﬁes the necessity to model cross-modal\nincongruity with the dynamic mechanism adjusting\nto diverse inputs for multimodal sarcasm detection.\n4.6 Hyperparameter Analysis\nTo analyze the impact of the number of DynRT\nlayers in our model, we experiment on varying the\nlayer of DynRT from 1 to 6. The results are shown\nin Figure5. In Figure5, we can see that our model\nperformance improves with the increase of DynRT\nlayers in the ﬁrst three layers, and then the perfor-\nmances drop slightly in the layers 4-6. The results\nindicate that, with more layers of DynRT, the abil-\nity of our model improves ﬁrst, but with the further\nincrease of layers, DynRT-Net encounters the per-\nformance bottleneck. Thus, we use the model with\n4 layers of DynRT in the main experiment, which\nis relatively stable and achieves the best results for\nmultimodal sarcasm detection.\n4.7 Case Study\nTo further verify the adaptability of DynRT-Net, we\nvisualize the learned attentions between text tokens\nand image patches in different DynRT layers. From\nthe results in Figure6, we can see that the tokens\nof objects are unable to focus on corresponding\nimage regions in the ﬁrst few layers, while their\nattentions move to corresponding image regions\ngradually with the increase of layers, which shows\nthat our model learns semantic alignment relations\nbetween the image and text gradually. Speciﬁcally,\nin the 4th layer, the tokens of objects, such aspark\nin Figure6 (a) andcup in Figure6 (b), can focus\non the related image regions.\nMoreover, the tokens which express sarcastic\nmeanings can concentrate on the image regions\nwhich express inconsistent concepts in the 4th\nlayer, thus verifying that our model can dynam-\nically capture the incongruity between image and\ntext. Speciﬁcally, in Figure6 (a), the car takes two\nparking spaces, andgreat in the text expresses the\nsarcastic meaning, which has a higher attention\nscore for the parking space in the image. Likewise,\nin Figure6 (b), thanks and awesome in the text\nhave higher attention scores with the region of the\nleaky cup in the picture.\n5 Conclusion\nTo model the cross-modal incongruity that is ad-\njustable to diverse image-text pairs, we propose the\ndynamic routing transformer network DynRT-Net\nto activate different modules with hierarchical co-\nattention for multimodal sarcasm detection. This\ndynamic mechanism in network design can help\ncapture the sarcastic clues in accordance with dif-\nferent image-text inputs. Experimental results on a\npublic dataset demonstrate the effectiveness of our\nproposed method. Our future work shall explore di-\nverse types of co-attention between image and text\nto further improve the adaptability of our method.\n2476\nLimitations\nOur work has some limitations. The design of the\nco-attention in our method can be improved. Cur-\nrently the design of co-attention in our method is\nlimited to four types, which affects its adaptability.\nIn addition, due to the fact that there is only one\npublicly available dataset in multimodal sarcasm\ndetection, we conduct our experiments based on it.\nThis has limited the evaluation of the generalization\nof our method.\nAcknowledgements\nThis work is supported in part by the Ministry of\nScience and Technology of China under Grants\n#2022YFB2703302 and #2020AAA0108401, and\nNational Natural Science Foundation of China un-\nder Grants #62206287, #11832001 and #72293575.\nWe thank all the anonymous reviewers for their\nvaluable comments.\nReferences\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\n2016. Layer normalization. Computing Research\nRepository, arXiv:1607.06450.\nYitao Cai, Huiyu Cai, and Xiaojun Wan. 2019.Multi-\nmodal sarcasm detection in twitter with hierarchical\nfusion model.I n Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics,\npages 2506–2515.\nHarm de Vries, Florian Strub, Jeremie Mary, Hugo\nLarochelle, Olivier Pietquin, and Aaron C Courville.\n2017. Modulating early visual processing by lan-\nguage.I n Proceedings of the International Con-\nference on Neural Information Processing Systems,\npages 6597–6607.\nJacob Devlin, Mingwei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.I n Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics, pages 4171–4186.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image is\nworth 16x16 words: Transformers for image recog-\nnition at scale.I n Proceedings of the International\nConference on Learning Representations , pages\n1–22.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition.I n Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 770–\n778.\nAditya Joshi, Pushpak Bhattacharyya, and Mark J Car-\nman. 2017. Automatic sarcasm detection: A survey.\nACM Computing Surveys, 50(5):1–22.\nY oon Kim. 2014.Convolutional neural networks for\nsentence classiﬁcation.I n Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1746–1751.\nDiederik P Kingma and Jimmy Ba. 2015.Adam: A\nmethod for stochastic optimization.I n Proceedings\nof the International Conference on Learning Repre-\nsentations, pages 1–15.\nBin Liang, Chenwei Lou, Xiang Li, Lin Gui, Min Yang,\nand Ruifeng Xu. 2021. Multi-modal sarcasm de-\ntection with interactive in-modal and cross-modal\ngraphs.I n Proceedings of the ACM International\nConference on Multimedia, pages 4707–4715.\nBin Liang, Chenwei Lou, Xiang Li, Min Yang, Lin Gui,\nY ulan He, Wenjie Pei, and Ruifeng Xu. 2022.Multi-\nmodal sarcasm detection via cross-modal graph con-\nvolutional network.I n Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1767–1777.\nHui Liu, Wenya Wang, and Haoliang Li. 2022.Towards\nmulti-modal sarcasm detection via hierarchical con-\ngruity modeling with knowledge enhancement.I n\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing, pages 4995–5006.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and V eselin Stoyanov.\n2019. RoBERTa: A robustly optimized bert pre-\ntraining approach. Computing Research Repository,\narXiv:1907.11692.\nHongliang Pan, Zheng Lin, Peng Fu, Yatao Qi, and\nWeiping Wang. 2020. Modeling intra and inter-\nmodality incongruity for multi-modal sarcasm de-\ntection.I n Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1383–1392.\nEthan Perez, Florian Strub, Harm De Vries, Vincent\nDumoulin, and Aaron Courville. 2018. Film: Vi-\nsual reasoning with a general conditioning layer.I n\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence, pages 3942–3951.\nShraman Pramanick, Aniket Roy, and Vishal M. Pa-\ntel Johns. 2022.Multimodal learning using optimal\ntransport for sarcasm and humor detection.I n Pro-\nceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Visio, pages 546–556.\nLeigang Qu, Meng Liu, Jianlong Wu, Zan Gao, and\nLiqiang Nie. 2021. Dynamic modality interaction\nmodeling for image-text retrieval.I n Proceedings\n2477\nof the International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\npage 1104–1113.\nEllen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra\nDe Silva, Nathan Gilbert, and Ruihong Huang. 2013.\nSarcasm as contrast between a positive sentiment and\nnegative situation.I n Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning, pages 704–714.\nRossano Schifanella, Paloma de Juan, Joel Tetreault,\nand Liangliang Cao. 2016. Detecting sarcasm in\nmultimodal social platforms.I n Proceedings of the\nACM International Conference on Multimedia, pages\n1136–1145.\nYi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian\nSu. 2018. Reasoning with sarcasm by reading in-\nbetween.I n Proceedings of the Annual Meeting of\nthe Association for Computational Linguistics, pages\n1010–1020.\nJoseph Tepperman, David Traum, and Shrikanth\nNarayanan. 2006. \"Yeah right\": Sarcasm recogni-\ntion for spoken dialogue systems.I n Proceedings of\nthe International Conference on Spoken Language\nProcessing, pages 1838–1841.\nOren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.\nIcwsm—a great catchy name: Semi-supervised\nrecognition of sarcastic sentences in online product\nreviews.I n Proceedings of the International AAAI\nConference on W eblogs and Social Media, pages 162–\n169.\nXinyu Wang, Xiaowen Sun, Tan Yang, and Hongbo\nWang. 2020.Building a bridge: A method for image-\ntext sarcasm detection without pretraining on image-\ntext data.I n Proceedings of the International W ork-\nshop on Natural Language Processing Beyond T ext,\npages 19–29.\nTao Xiong, Peiran Zhang, Hongbo Zhu, and Yihui Yang.\n2019. Sarcasm detection with self-matching net-\nworks and low-rank bilinear pooling.I n Proceed-\nings of the W orld Wide W eb Conference, pages 2115–\n2124.\nNan Xu, Zhixiong Zeng, and Wenji Mao. 2020.Reason-\ning with multimodal sarcastic tweets via modeling\ncross-modality contrast and semantic association.I n\nProceedings of the Annual Meeting of the Association\nfor Computational Linguistics, pages 3777–3786.\nMeishan Zhang, Y ue Zhang, and Guohong Fu. 2016.\nTweet sarcasm detection using deep neural network.\nIn Proceedings of the International Conference on\nComputational Linguistics, pages 2449–2460.\nYiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun,\nJianzhuang Liu, Xinghao Ding, Mingliang Xu, and\nRongrong Ji. 2021.Trar: Routing the attention spans\nin transformer for visual question answering.I n Pro-\nceedings of the IEEE/CVF International Conference\non Computer Visio, pages 2074–2084.\nA License of Scientiﬁc Artifacts\nThe license for RoBERTa is MIT License. The\nlicense for ViT is Apache-2.0 license. We were un-\nable to ﬁnd the license for the Multimodal Sarcasm\nDetection dataset from the original paper (Cai et al.,\n2019) and the online resources1.\nB More Details of Experimental Settings\nWe train all the models on GeForce RTX 2080 Ti\nGPUs. For each run, the model giving the best\nperformance of macro-F1 in the development set\nis used for the test set. We provide details of the\nbest model parameters in Table2. We resize the\nimage to the resolution of\n224 × 224 pixels and\nuse vit-base-patch32-2242 with 7×7 grids for the\nvisual embedding. We use the ﬁrst layer of roberta-\nbase3 for the text embedding. The dropout rate\nfor classiﬁer is 0.5. We optimize our model by\nAdam (Kingma and Ba, 2015) with learning rate\ne−6 and weight decay 0.01, we train our models for\n15 epochs with mini-batch size of 32. All experi-\nmental results reported are the averaged scores of\nﬁve runs with different random seeds. The number\nof total parameters in our model is 238,289,140.\nThe training time for our model is about 40 min-\nutes.\n1https://github.com/headacheboy/data-of-multimodal-\nsarcasm-detection\n2https://github.com/rwightman/pytorch-image-models\n3https://huggingface.co/roberta-base\n2478\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n6 Limitations\n□\u0017 A2. Did you discuss any potential risks of your work?\nOur work focuses on multimodal sarcasm detection, which is a classiﬁcation problem. It won’t evoke\npotentially harmful effects like generating fake proﬁles in other tasks.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection Abstract and Section 1 Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3.1, Section 4.1, Section 4.2 Appendix B\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3.1, Section 4.1, Section 4.2 Appendix B\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix A\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nThe pretrained models we used are released under a speciﬁed license MIT License and Apache-2.0\nlicense. The data is sufﬁciently anonymized (like replacing mentions with a certain symbol <user> )\nto make the identiﬁcation of individuals impossible without signiﬁcant effort.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe use a publicly released dataset from previous work which has removed information that names or\nuniquely identiﬁes individual people or offensive content.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4.1\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n2479\nC □\u0013 Did you run computational experiments?\nAppendix B\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.2 Appendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4.4, Appendix B\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4.2, Appendix B\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n2480",
  "topic": "Sarcasm",
  "concepts": [
    {
      "name": "Sarcasm",
      "score": 0.8923060894012451
    },
    {
      "name": "Computer science",
      "score": 0.7969681024551392
    },
    {
      "name": "Transformer",
      "score": 0.6045441627502441
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5201547741889954
    },
    {
      "name": "Machine learning",
      "score": 0.43656784296035767
    },
    {
      "name": "Engineering",
      "score": 0.09020769596099854
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Irony",
      "score": 0.0
    }
  ]
}