{
  "title": "Are Transformers universal approximators of sequence-to-sequence\\n functions?",
  "url": "https://openalex.org/W2995744795",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A4286962646",
      "name": "Yun, Chulhee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221384541",
      "name": "Bhojanapalli, Srinadh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2754832902",
      "name": "Rawat, Ankit Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221384543",
      "name": "Reddi, Sashank J.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156175939",
      "name": "Kumar Sanjiv",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2162064258",
    "https://openalex.org/W2920448302",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2766099245",
    "https://openalex.org/W2953333557",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2908802752",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2949603537",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2963485221",
    "https://openalex.org/W1988115241",
    "https://openalex.org/W2093813380",
    "https://openalex.org/W2103496339",
    "https://openalex.org/W2963097630",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Despite the widespread adoption of Transformer models for NLP tasks, the\\nexpressive power of these models is not well-understood. In this paper, we\\nestablish that Transformer models are universal approximators of continuous\\npermutation equivariant sequence-to-sequence functions with compact support,\\nwhich is quite surprising given the amount of shared parameters in these\\nmodels. Furthermore, using positional encodings, we circumvent the restriction\\nof permutation equivariance, and show that Transformer models can universally\\napproximate arbitrary continuous sequence-to-sequence functions on a compact\\ndomain. Interestingly, our proof techniques clearly highlight the different\\nroles of the self-attention and the feed-forward layers in Transformers. In\\nparticular, we prove that fixed width self-attention layers can compute\\ncontextual mappings of the input sequences, playing a key role in the universal\\napproximation property of Transformers. Based on this insight from our\\nanalysis, we consider other simpler alternatives to self-attention layers and\\nempirically evaluate them.\\n",
  "full_text": "Published as a conference paper at ICLR 2020\nARE TRANSFORMERS UNIVERSAL APPROXIMATORS\nOF SEQUENCE -TO-SEQUENCE FUNCTIONS ?\nChulhee Yun∗\nMIT\nchulheey@mit.edu\nSrinadh Bhojanapalli\nGoogle Research NY\nbsrinadh@google.com\nAnkit Singh Rawat\nGoogle Research NY\nankitsrawat@google.com\nSashank J. Reddi\nGoogle Research NY\nsashank@google.com\nSanjiv Kumar\nGoogle Research NY\nsanjivk@google.com\nABSTRACT\nDespite the widespread adoption of Transformer models for NLP tasks, the ex-\npressive power of these models is not well-understood. In this paper, we establish\nthat Transformer models are universal approximators of continuous permutation\nequivariant sequence-to-sequence functions with compact support, which is quite\nsurprising given the amount of shared parameters in these models. Furthermore,\nusing positional encodings, we circumvent the restriction of permutation equiv-\nariance, and show that Transformer models can universally approximatearbitrary\ncontinuous sequence-to-sequence functions on a compact domain. Interestingly,\nour proof techniques clearly highlight the different roles of the self-attention and\nthe feed-forward layers in Transformers. In particular, we prove that ﬁxed width\nself-attention layers can compute contextual mappings of the input sequences,\nplaying a key role in the universal approximation property of Transformers. Based\non this insight from our analysis, we consider other simpler alternatives to self-\nattention layers and empirically evaluate them.\n1 I NTRODUCTION\nSelf-attention based Transformer networks (Vaswani et al., 2017) have been at the center of the\nrecent progress on various natural language processing (NLP) tasks, including machine translation\n(Vaswani et al., 2017), language modeling (Radford et al., 2018; 2019), and question answering\n(Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019). All these tasks involve learning models that\nmap an input sequence of tokens to an output sequence of tokens. Transformers make it feasible\nto train large models to approximate these sequence-to-sequence functions due to their ability to\nprocess the input tokens in a parallel way, as opposed to the sequential nature of RNNs and LSTMs.\nA Transformer block consists of two kinds of layers: a self-attention layer and a token-wise feed-\nforward layer, with skip connections present in both layers. The self-attention layer transforms each\ninput token embedding using a weighted combination of the embeddings of all tokens in the input\nsequence, where weights are generated by pairwise dot-products among the input token embeddings.\nThe token-wise feed-forward layer then independently processes each of these modiﬁed input token\nembeddings without any interaction among them. Notably, Transformers employ parameter reuse\nacross tokens, as both layers use the same parameters to process each token. Moreover, Transformers\nhave to rely solely on the pairwise dot-products to capture interaction between the input tokens.\nGiven the parameter sharing and limited interactions between tokens, it is natural to wonder: what\nclass of sequence-to-sequence functions can the Transformer networks represent? Also, what is the\nrole of the two different kinds of layers? Are both layers needed to obtain the representation power\nof Transformers? In the existing literature, the advantage of Transformers has often been attributed\nto their capability of computing contextual embeddings/mappings of the input, as opposed to ﬁxed\nword embeddings as in word2vec (Mikolov et al., 2013). Is it possible to formalize the notion of\n∗Based on work performed at Google Research New York\n1\narXiv:1912.10077v2  [cs.LG]  25 Feb 2020\nPublished as a conference paper at ICLR 2020\ncontextual mappings? If yes, can Transformers actually compute such mappings? Such questions\nstill remain elusive.\nIn this paper, we provide a mathematical deﬁnition of contextual mappings and show that multi-head\nself-attention layers can indeed compute contextual mappings of the input sequences. We further\nshow that this ability to compute contextual mappings coupled with the value mapping ability of the\nfeed-forward layers makes Transformers universal approximators of any permutation equivariant\nsequence-to-sequence function. We also improve this result using positional encodings, and show\nthat Transformers can represent any sequence-to-sequence function; i.e., the restriction of permuta-\ntion equivariance can be removed by positional encodings.\nThese results on universal approximation of sequence-to-sequence functions raise a natural question:\nis it possible to have a more efﬁcient architecture to compute contextual mappings, consequently,\npreserving the ability to universally approximate sequence-to-sequence functions? Towards this, we\nexplore other architectures that can implement contextual mappings (to some extent), and experi-\nmentally evaluate their performance. In our experiments, we notice that the models that combine\nthese simpler architectures with Transformers have better performance, compared to the standalone\nTransformers. We conclude the paper by presenting more discussion and interesting future research\ndirections along these lines.\n1.1 S UMMARY OF OUR CONTRIBUTIONS\n• We prove that Transformers are universal approximators of continuous and permutation equiv-\nariant sequence-to-sequence functions with compact support (Theorem 2). We also show that,\nif Transformers have trainable positional encodings added to the input, then they are universal\napproximators of continuous sequence-to-sequence functions on a compact domain (Theorem 3).\n• We formalize the notion of contextual mappings and show that the attention layers can compute\ncontextual mappings, where each unique context is mapped to a unique vector (Lemma 6).\n• We experimentally evaluate other simpler layers that can compute contextual mappings to some\nextent, such as bi-linear projections and separable convolutions, and show that substituting some\nof the self-attention layers with these layers can result in better performance (Section 5).\n1.2 R ELATED WORKS & NOTATION\nAnalysis of attention-based models. Given the popularity of Transformers, there have been numer-\nous works trying to understand the role of attention layers in natural language processing models.\nOne such line of work focuses on probing the output of attention layers to understand the attention\nmechanism and internal language representation (Hewitt & Manning, 2019; Clark et al., 2019; Co-\nenen et al., 2019; Vig & Belinkov, 2019). Although these results give valuable insights, a consistent\ntheoretical analysis corroborating these ﬁndings is missing.\nUniversal approximation theorems.Universal approximation theorems are classical results in neu-\nral network theory, dating back many decades (Cybenko, 1989; Hornik, 1991). These results show\nthat given unbounded width, a one-hidden-layer neural network can approximate arbitrary contin-\nuous function with compact support, up to any accuracy. Other results focusing on depth appeared\nmore recently (Lu et al., 2017; Hanin & Sellke, 2017; Lin & Jegelka, 2018). In particular, Lu et al.\n(2017); Hanin & Sellke (2017) consider fully-connected ReLU networks whose input dimension is\nd, and show that networks with width d+ 1 and unbounded depth are universal approximators of\nscalar-valued continuous functions. Lin & Jegelka (2018) show that a residual network with one\nhidden neuron per residual block is a universal approximator of scalar-valued functions, given un-\nbounded depth. Although Transformer networks do have residual connections, due to their heavy\nparameter sharing, the existing analyses for residual networks do not extend to Transformers. Sannai\net al. (2019) consider universally approximating permutation invariant/equivariant functions using\nfully-connected ReLU networks.\nTuring completeness results on Transformers. Recently, P ´erez et al. (2019) have shown that\nTransformers with inﬁnite precision are Turing complete, which is not the case in ﬁnite precision\nsetting (Dehghani et al., 2018). We note that Turing completeness deals with computation on formal\nlanguages (thus discrete objects), while universal approximation focuses on functions on a contin-\nuum. In other words, these are two different concepts; and one does not imply another.\n2\nPublished as a conference paper at ICLR 2020\nNotation. We use the following notation in the paper. Given a matrix A, let Ai,j, Ai,:, and A:,j\ndenote its (i,j)-th entry, i-th row, and j-th column, respectively. We use ∥A∥p to denote the entry-\nwise ℓp norm of A. Let σ[·] be the softmax operator, which takes a matrix as input and applies\nsoftmax operation to each column of the matrix, which results in a column stochastic matrix, i.e.,\na matrix that has non-negative entries with each column summing to 1. We similarly deﬁne σH[·]\nto be the hardmax operator, which outputs the one-hot representation of the arg max entry for each\ncolumn of the input matrix. If there arekarg maxentries, then the output is1/kfor such entries. We\nuse 1n to denote a vector of length nwhose entries are all 1. We denote the 0-1 indicator function\nby 1 {·}. We use dand nto denote the embedding dimension and the sequence length, respectively.\nWe assume throughout that n≥2, as the Transformers reduce to residual networks when n= 1.\n2 T RANSFORMER NETWORKS\nA Transformer block is a sequence-to-sequence function mapping Rd×n to Rd×n. It consists of\ntwo layers: a self-attention layer and a token-wise feed-forward layer, with both layers having a\nskip connection. More concretely, for an input X ∈Rd×n consisting of d-dimensional embeddings\nof ntokens, a Transformer block with multiplicative or dot-product attention (Luong et al., 2015)\nconsists of the following two layers1:\nAttn(X) = X +\n∑h\ni=1\nWi\nOWi\nVX ·σ[(Wi\nKX)TWi\nQX], (1)\nFF(X) = Attn(X) + W2 ·ReLU(W1 ·Attn(X) + b11T\nn) + b21T\nn, (2)\nwhere Wi\nO ∈Rd×m, Wi\nV,Wi\nK,Wi\nQ ∈Rm×d, W2 ∈Rd×r,W1 ∈Rr×d,b2 ∈Rd,b1 ∈Rr, and\nFF(X) is the output of the Transformer block. The number of heads hand the head size mare two\nmain parameters of the attention layer; andrdenotes the hidden layer size of the feed-forward layer.\nHere, we would like to point out that our deﬁnition of the self-attention layer (1) is an equivalent re-\nformulation of (Vaswani et al., 2017), where they concatenate attention heads and multiply a matrix\nWO ∈Rd×mh to the concatenation. One difference in our setup is the absence of layer normaliza-\ntion, which simplies our analysis while preserving the basic architecture of the Transformer.\nWe deﬁne the Transformer networks as the composition of Transformer blocks. The family of the\nsequence-to-sequence functions corresponding to the Transformers can be deﬁned as:\nTh,m,r := {g: Rd×n →Rd×n |gis a composition of Transformer blocks th,m,r’s}.\nwhere th,m,r : Rd×n →Rd×n denotes a Transformer block deﬁned by an attention layer with h\nheads of size meach, and a feed-forward layer with rhidden nodes.\nWe say that a function f : Rd×n →Rd×n is permutation equivariant if for any permutation matrix\nP, we have f(XP ) = f(X)P; i.e., if we permute the columns of X, then the columns of f(X)\nare permuted in the same way. A Transformer block is permutation equivariant, which we formally\nprove in Section A. This consequently establishes the permutation equivariance of the classTh,m,r.\nClaim 1. A Transformer block th,m,r deﬁnes a permutation equivariant map from Rd×n to Rd×n.\nAs seen in above, both layers (cf. (1) and (2)) of a Transformer block employ parameter\nreuse/sharing, because each token/column undergoes the same transformations (e.g., Wi\nQ, Wi\nK, or\nW1) regardless of its position. Moreover, interactions between tokens can only be captured through\npairwise dot-products in the softmax operatorσ[·] (cf. (1)). Given such limitations in a single Trans-\nformer block’s representation power, it is not obvious what kinds of sequence-to-sequence functions\nTh,m,r can approximate; we provide the answer to this question in the next section.\n3 T RANSFORMERS ARE UNIVERSAL APPROXIMATORS OF\nSEQUENCE -TO-SEQUENCE FUNCTIONS\nIn this section, we present our theorems showing that the Transformer networks are universal ap-\nproximators of sequence-to-sequence functions. Let us start by deﬁning the target function class\nFPE, which consists of all continuous permutation equivariant functions with compact support that\n1In our proof we use bias vectorsbi\nQ for query projections in attention layers. We omit them here for brevity.\n3\nPublished as a conference paper at ICLR 2020\nmap Rd×n to Rd×n. Here, continuity is deﬁned with respect to any entry-wiseℓp norm, 1 ≤p< ∞.\nGiven two functions f1,f2 : Rd×n →Rd×n, for 1 ≤p< ∞, we deﬁne a distance between them as\ndp(f1,f2) :=\n(∫\n∥f1(X) −f2(X)∥p\npdX\n)1/p\n.\nThe following result shows that a Transformer network with a constant number of headsh, head size\nm, and hidden layer of size rcan approximate any function in FPE.\nTheorem 2. Let 1 ≤p <∞and ϵ >0, then for any given f ∈FPE, there exists a Transformer\nnetwork g∈T 2,1,4, such that dp(f,g) ≤ϵ.\nNext, we present our theorem on Transformers with positional encodings. In order to endow the\nTransformer networks with the ability to capture the information about the position of tokens in the\ninput sequence, it is a common practice to add positional encodingsE ∈Rd×nto the input sequence\nbefore feeding it to the Transformer network (Vaswani et al., 2017; Devlin et al., 2018). Consider\nthe functions represented by Transformers with positional encodings:\nTh,m,r\nP := {gP(X) = g(X + E) |g∈T h,m,r and E ∈Rd×n}.\nHere we show that if E is trainable, these positional encodings are sufﬁcient to remove the permu-\ntation equivariance restriction of the Transformers. Towards this, we deﬁne FCD to be the set of all\ncontinuous functions that map a compact domain in Rd×n to Rd×n. Note that FCD does not have\nthe restriction of permutation equivariance as in FPE, but any f ∈FCD is deﬁned on a compact\ndomain instead of the whole Rd×n. The following result states that, equipped with the trainable\npositional encodings, Transformers can approximate any sequence-to-sequence function in FCD.\nTheorem 3. Let 1 ≤p <∞and ϵ >0, then for any given f ∈FCD, there exists a Transformer\nnetwork g∈T 2,1,4\nP such that we have dp(f,g) ≤ϵ.\nTheorems 2 and 3 provide an interesting characterization of the representation power of ﬁxed-width\nTransformer networks. Since the function classes Th,m,r and Th,m,r\nP become richer as we increase\nthe values of (h,m,r ), our results establish that general Transformer networks are also universal\napproximators of sequence-to-sequence functions. Remarkably, none of the parameters (h,m,r )\ndepend on the input sequence length nor embedding dimension d.\nHere, we would like to again point out that Theorems 2 and 3 appear quite surprising at a ﬁrst glance,\ngiven the parameter sharing across all the tokens in a sequence, e.g., feed-forward layers are applied\ntoken-wise and the projection matrices in the self-attention layers are the same across different\ntokens. Furthermore, attention layers can only capture pairwise interaction between different tokens\nin the sequence. In the next subsection, we brieﬂy describe one of our key steps in overcoming the\naforementioned restrictions and proving universal approximation power of Transformers.\n3.1 A KEY STEP : SELF -ATTENTION LAYERS CAN IMPLEMENT CONTEXTUAL MAPPINGS\nLet us consider a setting where we are interested in embedding two sentences: 1) I am happy; and\n2) I am Bob. These sentences are fed to a sequence-to-sequence model as\nX = [X:,1,X:,2,X:,3] = [vI,vam,vhappy] and ˜X = [ ˜X:,1, ˜X:,2, ˜X:,3] = [vI,vam,vBob],\nwhere vI,vam,vhappy,and vBob denote d-dimensional embedding for the tokens ‘I’, ‘am’, ‘happy’,\nand ‘Bob’, respectively. Since the word ‘I’ occurs in different contexts in these sentences, in order to\nimplement arbitrary sequence-to-sequence functions, the sequence-to-sequence model should map\nthe two occurrences of ‘I’ to different values. We formally deﬁne this requirement below.\nDeﬁnition 3.1 (Contextual mapping). Consider a ﬁnite set L ⊂Rd×n. A map q : L →R1×n\ndeﬁnes a contextual mapping if the map satisﬁes the following:\n1. For any L ∈L, the nentries in q(L) are all distinct.\n2. For any L,L′∈L, with L ̸= L′, all entries of q(L) and q(L′) are distinct.\nIn other words, a contextual mapping maps each token (column) of L ∈L to a unique value which\ndepends on the entire L; as a result, capturing the precise context of L. This allows the subsequent\n4\nPublished as a conference paper at ICLR 2020\ntoken-wise function (e.g., deﬁned by the feed-forward layers in case of Transformer networks) to\nrealize the outputs of any arbitrary sequence-to-sequence functions.\nAt the ﬁrst thought, we can consider getting a contextual mapping by simply averaging all the tokens,\nbecause this can capture the one-word difference (e.g., “happy” vs. “Bob”) in two different contexts.\nHowever, if there are multiple words that are different, it is not guaranteed that the average will be\ndifferent. Indeed, requiring unique mappings for all the tokens for any change in any number of\ntokens, is a steep requirement.\nWhile the self-attention layer does consider pair-wise interactions among different input tokens, it\nis not clear if this weak form of pair-wise interaction with shared projection weights is sufﬁcient to\nextract the underlying context. The following result, which we sketch here, shows that self-attention\nlayers can implement a permutation equivariant contextual mapping over almost all elements of a\ngrid in [0,1]d×n. We defer the full statement to Section 4.2.\nLemma 6 (informal). Consider the grid Gδ := {0,δ,..., 1 −δ}d×n. Then, there exist a function\ngc : Rd×n →Rd×n composed of δ−d+ 1self-attention layers (h= 2,m = 1) and a vector u ∈Rd\nsuch that q(L) := uTgc(L) satisﬁes the following properties, for a subset ˜Gδ ⊂Gδ that contains\nalmost all elements of Gδ:\n1. For any L ∈˜Gδ, the entries of q(L) are all distinct.\n2. For any L,L′∈˜Gδ such that L is not a permutation ofL′, all entries ofq(L), q(L′) are distinct.\nLemma 6 shows that a series of self-attention layers can implement contextual mappings, despite\nthe apparent restriction that each of them can only capture pair-wise interaction. However, the\nrestriction of permutation equivarance still exists because attention layers are inherently permutation\nequivariant. Coupled with the ability of token-wise feed-forward layers to map different values in\nq(L) to arbitrary output values, we can prove universal approximation capability of Transformers.\n3.2 P ROOF OF THE UNIVERSAL APPROXIMATION THEOREM (THEOREM 2)\nNext, we outline the proof of Theorem 2 in greater detail. We refer the reader to Section C for the\nproof of Theorem 3, since it is a modiﬁcation of Theorem 2. Even though Theorems 2 and 3 do not\nspeciﬁcally mention the required depth for approximation, our proof techniques do characterize it,\nand we show that our construction is tight in the number of parameters. We defer the discussion of\ndepth to Section 4.4.\nRecall that we want to show that given a function f ∈FPE, we can ﬁnd a Transformer network\ng ∈T 2,1,4 such that dp(f,g) ≤ϵ. Without loss of generality, we can assume that the compact\nsupport of f is contained in [0,1]d×n. We achieve our desired objective in three key steps:\nStep 1. Approximate FPE with piece-wise constant functions. We ﬁrst use (a variant of) the\nclassical result that any continuous function can be approximated up to arbitrary accuracy by piece-\nwise constant functions. For δ >0, we deﬁne the following class of piece-wise constant functions.\nFPE(δ) :=\n{\nf : X ↦→\n∑\nL∈Gδ\nAL1 {X ∈SL}| f is permutation equivariant, AL ∈Rd×n\n}\n,\nwhere Gδ := {0,δ,..., 1 −δ}d×n and, for a grid point L ∈Gδ, SL := ∏d\nj=1\n∏n\nk=1[Lj,k,Lj,k +\nδ) ⊂[0,1]d×n denotes the associated cube of widthδ. Let f ∈FPE(δ) be such that dp(f,f) ≤ϵ/3.\nStep 2. Approximate FPE(δ) with modiﬁed Transformers. We then consider a slightly modiﬁed\narchitecture for Transformer networks, where the softmax operatorσ[·] and ReLU(·) are replaced by\nthe hardmax operator σH[·] and an activation function φ∈Φ, respectively. Here, the set of allowed\nactivations Φ consists of all piece-wise linear functions with at most three pieces, where at least one\npiece is constant. Let T\nh,m,r\ndenote the function class corresponding to the sequence-to-sequence\nfunctions deﬁned by the modiﬁed Transformer networks. The following result establishes that the\nmodiﬁed Transformer networks in T\n2,1,1\ncan closely approximate functions in FPE(δ).\nProposition 4. For eachf ∈FPE(δ) and 1 ≤p< ∞, ∃g∈T\n2,1,1\nsuch that dp(f,g) = O(δd/p).\nStep 3. Approximate modiﬁed Transformers with (original) Transformers. Finally, we show\nthat g∈T\n2,1,1\ncan be approximated by T2,1,4. Let g∈T 2,1,4 be such that dp(g,g) ≤ϵ/3.\n5\nPublished as a conference paper at ICLR 2020\nTheorem 2 now follows from these three steps, because we have\ndp(f,g) ≤dp(f,f) + dp(f,g) + dp(g,g) ≤2ϵ/3 + O(δd/p).\nChoosing δsmall enough ensures that dp(f,g) ≤ϵ.\nWe refer the reader to Sections B.1 and B.2 in the supplementary material for the formal statements\nand proofs of Steps 1 and 3, respectively. As for Step 2, which is the most critical step in estab-\nlishing the universal approximation property of Transformers, we provide a sketch of the proof of\nProposition 4 in the next section, and refer the reader to Section B.3 for the complete proof.\n4 P ROOF SKETCH OF PROPOSITION 4: DIFFERENT ROLES OF TWO LAYERS\nAs mentioned earlier, the heavy parameter sharing in Transformers makes the goal of universally\napproximating sequence-to-sequence functions seemingly difﬁcult. Both the self-attention and the\nfeed-forward layer weights inside a Transformer block are ﬁxed across ntokens. In this section, we\nshow that Transformers are able to overcome this architectural constraint, and compute contextual\nmappings of the entire input sequence just based on the pair-wise interactions. The token-wise\nfeedforward layers then transform these contextual mappings to the desired output sequence.\nWe highlight these inner workings of Transformers en route to proving Proposition 4. We want to\nshow that given a piece-wise constant function f ∈FPE(δ), there exists a modiﬁed Transformer\nnetwork g∈T\n2,1,1\nthat closely approximates f. We achieve this goal by establishing the following\nthree claims, which correspond to Lemmas 5, 6, and 7.\n1. Given an input X ∈Rd×n, a series of feed-forward layers in the modiﬁed Transformer network\ncan quantize X to an element L on the extended grid G+\nδ := {−δ−nd,0,δ,..., 1 −δ}d×n.\n2. Next, a series of self-attention layers in the modiﬁed Transformer network can take the input L\nand implement a contextual mapping qsuch that, for L and L′that are not permutation of each\nother, all the elements in q(L) and q(L′) are distinct.\n3. Finally, a series of feed-forward layers in the modiﬁed Transformer network can map elements\nof the contextual embedding q(L) to the desired output value of f ∈FPE at the input X.\nBefore discussing these three claims in detail, we note that even though a Transformer network\nstacks self-attention and feed-forward layers in an alternate manner, the skip connections enable\nthese networks to employ a composition of multiple self-attention or feed-forward layers. Further-\nmore, as alluded earlier, these three steps clearly highlight the different roles that self-attention and\nfeed-forward layers play in realizing the ability to universally approximate sequence-to-sequence\nfunctions: 1) self-attention layers compute precise contextual maps; and 2) feed-forward layers then\nassign the results of these contextual maps to the desired output values.\n4.1 Q UANTIZATION BY FEED -FORWARD LAYERS\nSince our objective in Proposition 4 is to approximate the function f ∈FPE(δ), which takes a con-\nstant value on the cubes SL’s, the (modiﬁed) Transformer network approximating f ﬁrst quantizes\nthe input X according to these cubes. In particular, we want each input X ∈SL to be mapped to\nthe point L. The following result shows that a modiﬁed Transformer network can indeed implement\nthis quantization map with a composition of multiple feed-forward layers.\nLemma 5. Consider a scalar quantization map gent\nq : R →{−δ−nd,0,δ,..., 1 −δ}:\ngent\nq (t) =\n{kδ if kδ ≤t< (k+ 1)δ, k= 0,..., 1/δ−1,\n−δ−nd otherwise.\nThere exists a function gq : Rd×n ↦→G+\nδ composed of d\nδ + dtoken-wise feed-forward layers with\nr= 1 and activations in Φ, which employs the scalar quantization gent\nq to each entry of its input.\nAs desired, the functiongq maps any X ∈SL to L. Furthermore, if any element ofX is not in[0,1],\nthe element is mapped to −δ−nd, indicating that X is outside the compact support of f ∈FPE(δ).\n6\nPublished as a conference paper at ICLR 2020\n4.2 C ONTEXTUAL MAPPING BY SELF -ATTENTION LAYERS\nIn this subsection, we show that the (modiﬁed) Transformer network can compute contextual map-\npings (cf. Deﬁnition 3.1) from the output L ∈G+\nδ of the map gq (cf. Section 4.1) by using a com-\nposition of self-attention layers. The following lemma, sketched earlier in Section 3.1, shows that\nthe (modiﬁed) Transformer networks can implement a permutation equivariant contextual mapping\nover almost all elements of Gδ, while mapping the rest of elements in G+\nδ to a disjoint set.\nLemma 6. Consider the following subset of Gδ = {0,δ,..., 1 −δ}d×n:\n˜Gδ := {L ∈Gδ |L:,i ̸= L:,j for all i̸= j}.\nAssume that n ≥2 and δ−1 ≥2. Then, there exist a function gc : Rd×n →Rd×n composed\nof δ−d + 1 self-attention layers (h = 2 ,m = 1 ) that employ the σH operator, a vector u ∈Rd,\nconstants tl,tr ∈R (0 <tl <tr), such that q(L) := uTgc(L) satisﬁes the following properties:\n1. For any L ∈˜Gδ, the entries of q(L) are all distinct.\n2. For any L,L′∈˜Gδ such that L is not a permutation ofL′, all entries ofq(L), q(L′) are distinct.\n3. For any L ∈˜Gδ, all the entries of q(L) are in [tl,tr].\n4. For any L ∈G+\nδ \\˜Gδ, all the entries of q(L) are outside [tl,tr].\nAt this point, a few remarks about the result in Lemma 6 are in order. First, since the Transformer\nnetworks are bound to implement permutation invariant maps, we require the Property 6.2 to hold\nfor the pair of sequences that cannot be mapped to each other via permutation of columns. Further-\nmore, the self-attention layers implement the desirable contextual map for only ˜Gδ ⊆Gδ, where\nall columns of L are distinct. Note that for small δ, Gδ \\˜Gδ constitutes a negligible fraction of Gδ\nbecause |Gδ\\˜Gδ|= O(δd|Gδ|). The function qin Lemma 6 maps the elements of G+\nδ \\˜Gδ outside\n[tl,tr]—the interval where the outputs of the contextual mapping for ˜Gδ reside.\n4.2.1 P ROOF SKETCH OF LEMMA 6\nSince Lemma 6 is one of the major technical contributions of this paper, we provide a short sketch\nof its proof. The complete proof is presented in Section B.5. For simplicity, we consider the case\nd= 1, so the input L ∈G+\nδ is a row vector of length n.\nThe key idea of the proof is that, using two attention heads of size 1, one can implement a self-\nattention layer that shifts up input entries that arein a speciﬁc interval, while leaving all other entries\nintact. We call this the selective shift operation. Since the entries in L are quantized, we apply the\nselective shift operation to 0,δ,..., 1 −δusing 1/δattention layers. Interestingly, the value of the\nlargest output entry after these operations is unique for each L ∈˜Gδ up to permutations. Using the\nlargest entry, one can add one last layer that shifts up the entire matrix and outputsq(L) that satisﬁes\nProperties 6.1 and 6.2 of the lemma.\nMore concretely, the following function Ψ : R1×n →R1×n, parametrized by b,b′ ∈R satisfying\nb<b ′, can be implemented with two attention heads of size 1 with the hardmax (σH) operator:\nΨ(Z; b,b′)1,j =\n{maxkZ1,k −minkZ1,k if b<Z 1,j <b′,\n0 if Z1,j <b or Z1,j >b′.\nIf we deﬁne an attention layer of the form Z ↦→Z + Ψ(Z; b,b′), then any entry Z1,j in (b,b′) is\nshifted up by maxkZ1,k −minkZ1,k, while all the other entries stay untouched. We can choose b\nand b′to selectively shift certain entries, hence the name selective shift operation.\nWe stack 1/δ self-attention layers, with attention parts δ−1Ψ(·; l −δ/2,l + δ/2) for each l ∈\n{0,δ,..., 1 −δ}, in increasing order of l. With these layers, we can apply the selective shift\noperations to input entries of values 0,δ,..., 1 −δ. To see how the shift operations modify the\ninput, now consider n = 2 for simplicity, and let L = [ l1 l2] ∈ ˜Gδ. Without loss of gener-\nality, we can assume l1 < l2. The selective shift operation is applied to l1 ﬁrst, shifting it by\nδ−1(max L −min L) = δ−1(l2 −l1), resulting in ˜l1 = l1 + δ−1(l2 −l1) > l2. After that, the\noperation on l2 shifts it up by δ−1(˜l1 −l2). Thus, the ﬁrst 1/δlayers map L = [l1 l2] (l1 <l2) to\n˜L =\n[˜l1 ˜l2\n]\n:=\n[\nl1 + δ−1(l2 −l1) l2 + (δ−2 −δ−1)(l2 −l1)\n]\n.\n7\nPublished as a conference paper at ICLR 2020\nWe can show that the map from [l1 l2] ∈ {L ∈ ˜Gδ | l1 < l2}to ˜l2 is one-to-one, and that\n0 <˜l1 <˜l2 <δ −2. We then add one last layer that shifts all positive entries of ˜L by δ−3 max ˜L =\nδ−3˜l2, whose output we denote by q(L) =\n[\nδ−3˜l2 + ˜l1 δ−3˜l2 + ˜l2\n]\n. All entries of q(L) are in\n[δ−3˜l2,δ−3˜l2 + δ−2), and this interval is disjoint for different L’s because L ↦→˜l2 is one-to-one.\nThus, q(L) satisﬁes Properties 6.1 and 6.2 of the lemma. The remaining details are in Section B.5.\n4.3 F UNCTION VALUE MAPPING BY FEED -FORWARD LAYERS\nThis brings us to the ﬁnal step, which demonstrates the key utility of the feed-forward layers. After\nthe contextual mapping by self-attention layers, each token captures the entire context available in\nthe input sequence. The following result shows that token-wise application of a composition of\nfeed-forward layers can map these tokens to the desired output values required by the function f.\nLemma 7. Let gc : Rd×n →Rd×n be the function from Lemma 6. Then, there exists a function\ngv : Rd×n → Rd×n composed of O(n(1\nδ)dn/n!) token-wise feed-forward layers (r = 1 ) with\nactivations in Φ such that gv is deﬁned by a token-wise function gtkn\nv : Rd →Rd on each column,\ngv(Z) =\n[\ngtkn\nv (Z:,1) ··· gtkn\nv (Z:,n)\n]\n,\nwhere for all j ∈{1,...,n },\ngtkn\nv (gc(L):,j) =\n{\n(AL):,j if L ∈˜Gδ,\n0d if L ∈G+\nδ \\˜Gδ.\n4.4 T IGHTNESS OF CONSTRUCTIONS\nWe showed in this section that Theorem 2 requires O(n(1/δ)dn/n!) Transformer blocks for ap-\nproximation, where δ is the width of the cubes. Each transformer block is of constant width, so\nit has O(d) parameters; this means that the total number of parameters is O(dn(1/δ)dn/n!). We\nnote that this exponential dependence cannot be avoided in the worse case. If we assume continuity\nwithout any additional smoothness, quantizing the domain to cubes and approximating the function\nwith constants require memorizing (output dim) ×(num cubes)/n! real numbers, where the factor\nof 1/n! is due to permutation equivariance. Thus, Theorem 2 is optimal in the order of parameters.\nIf we compare with the residual network result (Lin & Jegelka, 2018), we can consider “ﬂattening”\nX into a dn-dimensional vector and ﬁtting the function. The proof technique in (Lin & Jegelka,\n2018) requires O((1/δ)dn) layers, where each layer has O(dn) parameters: the total parameter re-\nquirement is O(dn(1/δ)dn). This shows that Transformers can approximate permutation equivariant\nfunctions in a more efﬁcient way than residual networks.\nIn Section C, our proof of Theorem 3 shows that we requireO(n(1/δ)dn) layers to approximate con-\ntinuous (not permutation equivariant) sequence-to-sequence functions. As seen from the argument\nabove, this construction is also optimal in the order of parameters.\n5 D ISCUSSION AND EXPERIMENTS\nAs detailed in Section 4, the ability of the self-attention layers to compute contextual mappings\nplays a crucial role in the universal approximation property. Interestingly, our analysis shows that\nreplacing the dot-product attention in Transformers with any other component capable of computing\ncontextual mappings should preserve this universal approximation property. This leads naturally to\nquestions about the alternative architectures that realize certain kinds of contextual mappings at dif-\nferent computational and memory costs. We explore and discuss some examples of such alternatives\nin this section. Our preliminary empirical study demonstrates their practical utility.\n5.1 B I-LINEAR PROJECTION\nGiven token embeddings X as input, the bi-linear projection layer computes the following update.\nBProj(X) = X + WO ·X ·WP.\nThe bi-linear projection layer (Gong et al., 2013) is motivated from the ability of random (Gaussian)\nmatrices to map sparse differences to dense vectors (Ailon & Chazelle, 2009). If there are two input\n8\nPublished as a conference paper at ICLR 2020\n(a) SQuAD\n (b) MNLI\nFigure 1: Performance of hybrid models constructed by ﬁrst taking BERT BASE, a 12 layer Trans-\nformer model, and replacing the self-attention layers with depth-wise separable convolution layers,\nin a varying number of the Transformer blocks closer to the input. Surprisingly, replacing 1 or 2\nself-attention layers with convolutions improves the performance, while replacing more hurts the\nperformance. This suggests both that Transformers have functionality beyond just computing con-\ntextual mappings, and having simpler layers to realize contextual mapping can aid Transformers.\ncontexts X1 and X2 that differ in one token, their difference X1 −X2 is sparse; however, after\nrandom projection, the difference (X1 −X2)WP will be dense, and the numbers are distinct with\nhigh probability, implementing a form “pair-wise contextual mapping,”2 although different from the\ncontextual mapping in Deﬁnition 3.1.\nThis layer advantageously incurs smaller number of matrix multiplications as compared to the dot-\nproduct attention. That said, the number of parameters in this layer depend on the sequence length,\nmaking it harder to reuse the model across tasks with different input sequence lengths. Moreover,\nthe weights used to compute the contextual embeddings ( WP) are independent of the inputs ( X),\nwhereas in self-attention the weights (σ[(Wi\nKX)TWi\nQX]) depend on X. The ﬁrst drawback can\nbe addressed by replacing the linear projection with a depth-wise separable convolution layer, which\nis discussed in the next subsection.\n5.2 D EPTH -WISE SEPARABLE CONVOLUTIONS\nA depth-wise convolution layer (Sifre & Mallat, 2014; Chollet, 2017; Kaiser et al., 2017) involves\nconvolving each dimension of X with a corresponding convolution ﬁlter of size k:\nSepConv(X) = X + WO(X ∗WC) ,\nwhere WC ∈Rd×k and (X ∗WC)i,: := Xi,: ∗(WC)i,:. Unlike bi-linear projection, this layer\ncan be used across tasks with different input sequence lengths as the number of parameters are\nindependent of the sequence length. While a single layer is unable to compute contextual mappings\nwhen the ﬁlter size is small, stacking multiple such layers can potentially provide a cheaper way\nto compute contextual mappings. In fact, based on depth-wise separable convolutions, Wu et al.\n(2019) proposed a light-weight dynamic convolution architecture that performs competitively with\nTransformers on machine translation.\n5.3 E XPERIMENTS\nWe now present our experiments with these other architectures, with the goal of understanding the\nextent to which computing contextual mappings can capture the performance of Transformers. As\ndiscussed earlier, BProj and SepConv do not implement contextual mappings (cf. Deﬁnition 3.1),\nso we do not expect that either BProj or SepConv based models to have the same performance as\nthe expensive Transformers. These models do not use input dependent weights to compute attention,\nand hence have weaker representation power. Instead, our goal is to see if we can use these cheaper\nlayers to replace (some of) the expensive self-attention layers.\n2This guarantee only holds for a ﬁnite set (can be exponential in n) of ﬁxed vectors in Rn.\n9\nPublished as a conference paper at ICLR 2020\nWe follow the experimental setting from Devlin et al. (2018) to train the Transformers, with the\nmasked language model pre-training followed by a task speciﬁc ﬁne-tuning, and work with a12 layer\narchitecture based on BERT BASE. We present our results on a question answering task (SQuAD)\n(Rajpurkar et al., 2016) and a sentence entailment task (MNLI) (Williams et al., 2018). In our\nﬁrst set of experiments we train models that employ BProj and SepConv layers, instead of the self-\nattention layer in eq.(1). We notice that, as expected, these simpler models have weaker performance\nthan the self-attention layer. See Table 1 in Section D for a comparison of these models on MNLI.\nNext, we swap a varying number of the ﬁrst few self-attention layers in BERT BASE with SepConv,\nimplemented with ﬁlter reuse across dimensions (Wu et al., 2019) 3. Fig. 1 illustrates the perfor-\nmance of these hybrid models. Interestingly, models with 1 or 2 convolution layers and rest the\nself-attention layers, perform better than models with only the self-attention layers. Note that, re-\nplacing self-attention layer with SepConv also reduces the computational cost and the number of\nparameters. One explanation we have is that the ﬁrst few attention layers tend to attend broadly to\nthe whole sequence (as empirically observed in (Clark et al., 2019)), and the cheaper convolution\nlayers can perform this job more efﬁciently. A detailed evaluation of such hybrid architectures will\nbe interesting future research.\nOur experiments also call for a deeper understanding of the exact nature of the embeddings com-\nputed by practical attention models. Since Transformers in practice have ﬁxed depth, we believe\nthat they might not be able to exactly implement contextual mappings as we deﬁned in Deﬁni-\ntion 3.1. However, there is some preliminary empirical evidence that Transformers do implement\nsome sort of “contextual mappings.” For example, Fig. 4 of Coenen et al. (2019) presents visualiza-\ntions of embeddings of a single word in different contexts (sentences). They experimentally notice\nthat Transformers, in addition to computing contextual mappings, also map a word into semantic\nclusters. Formalizing and evaluating this property of Transformers is an interesting direction for\nfuture work. We again note that Wu et al. (2019) have proposed an alternative way to compute such\nembeddings based on dynamic convolution layers. Evaluating the mappings computed by these\nmodels should shed more light on the workings of attention models and inspire efﬁcient and better\nperforming architectures.\nREFERENCES\nNir Ailon and Bernard Chazelle. The fast Johnson–Lindenstrauss transform and approximate nearest\nneighbors. SIAM Journal on computing, 39(1):302–322, 2009.\nFranc ¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 1251–1258, 2017.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does BERT look\nat? an analysis of BERT’s attention. arXiv preprint arXiv:1906.04341, 2019.\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Vi´egas, and Martin Wat-\ntenberg. Visualizing and measuring the geometry of BERT. arXiv preprint arXiv:1906.02715,\n2019.\nGeorge Cybenko. Approximation by superpositions of a sigmoidal function.Mathematics of control,\nsignals and systems, 2(4):303–314, 1989.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\ntransformers. arXiv preprint arXiv:1807.03819, 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nYunchao Gong, Sanjiv Kumar, Henry A Rowley, and Svetlana Lazebnik. Learning binary codes\nfor high-dimensional data using bilinear projections. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 484–491, 2013.\nBoris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.\narXiv preprint arXiv:1710.11278, 2017.\n3We refer to Section D for a complete description of the setup.\n10\nPublished as a conference paper at ICLR 2020\nJohn Hewitt and Christopher D Manning. A structural probe for ﬁnding syntax in word representa-\ntions. In Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-\npers), pp. 4129–4138, 2019.\nKurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4\n(2):251–257, 1991.\nLukasz Kaiser, Aidan N Gomez, and Francois Chollet. Depthwise separable convolutions for neural\nmachine translation. arXiv preprint arXiv:1706.03059, 2017.\nHongzhou Lin and Stefanie Jegelka. ResNet with one-neuron hidden layers is a universal approxi-\nmator. In Advances in Neural Information Processing Systems, pp. 6169–6178, 2018.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-\ntraining approach. arXiv preprint arXiv:1907.11692, 2019.\nZhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of\nneural networks: A view from the width. In Advances in neural information processing systems,\npp. 6231–6239, 2017.\nMinh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-\nbased neural machine translation. In Empirical Methods in Natural Language Processing\n(EMNLP), pp. 1412–1421, Lisbon, Portugal, September 2015. Association for Computational\nLinguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111–3119, 2013.\nJorge P´erez, Javier Marinkovi´c, and Pablo Barcel ´o. On the Turing completeness of modern neural\nnetwork architectures. arXiv preprint arXiv:1901.03429, 2019.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. Technical Report, OpenAI, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report, OpenAI, 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. InProceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pp. 2383–2392, 2016.\nAkiyoshi Sannai, Yuuki Takai, and Matthieu Cordonnier. Universal approximations of permutation\ninvariant/equivariant functions by deep neural networks. arXiv preprint arXiv:1903.01939, 2019.\nLaurent Sifre and St´ephane Mallat. Rigid-motion scattering for image classiﬁcation. Ph. D. disser-\ntation, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 5998–6008, 2017.\nJesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language\nmodel. arXiv preprint arXiv:1906.04284, 2019.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference. InProceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pp. 1112–1122. Association for Computational Linguistics, 2018. URL\nhttp://aclweb.org/anthology/N18-1101.\n11\nPublished as a conference paper at ICLR 2020\nFelix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V .\nLe. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE international conference on computer\nvision, pp. 19–27, 2015.\n12\nPublished as a conference paper at ICLR 2020\nA P ROOF OF CLAIM 1\nSuppose XP was given as input, where P is a permutation matrix. First note that\n(Wi\nKXP )T(Wi\nQXP ) = PT(Wi\nKX)T(Wi\nQX)P\nAfter the softmax operation, we get\nσ[PT(Wi\nKX)T(Wi\nQX)P] = PTσ[(Wi\nKX)T(Wi\nQX)]P.\nThen,\nAttn(XP ) = XP +\nh∑\ni=1\nWi\nO(Wi\nVXP ) ·PTσ[(Wi\nKX)T(Wi\nQX)]P = Attn(X)P,\nwhere we used PP T = I. Permutation equivariance of the token-wise feed-forward layer can be\nshown similarly:\nFF(XP ) = Attn(X)P + W2 ·ReLU(W1 ·Attn(X)P + b11T\nnP) + b21T\nnP\n= Attn(X)P + W2 ·ReLU(W1 ·Attn(X) + b11T\nn)P + b21T\nnP = FF(X)P,\nwhere ReLU(XP ) = ReLU(X)P was used. This analysis shows that the function classTh,m,r(·)\nis restricted to permutation equivariant functions.\nB P ROOF DETAILS OF THEOREM 2\nWe ﬁrst deﬁne some additional notation. For a,b ∈N where a ≤b, let [a] = {1,...,a }and\n[a: b] = {a,a + 1,...,b −1,b}. For a,b,c ∈R where b−a >0 is an integer multiple of c >0,\nwe write [a: c: b] := {a,a + c,a + 2c,...,b −c,b}.\nB.1 A PPROXIMATING FPE WITH FPE(δ)\nLemma 8. For any givenf ∈FPE and 1 ≤p< ∞, one can ﬁnd aδ∗>0 such that ∃f ∈FPE(δ∗)\nwhich satisﬁes dp(f,f) ≤ϵ/3.\nProof Since f : Rd×n →Rd×n is a continuous function with compact support, the function\nis uniformly continuous. Since continuity is deﬁned using entry-wise ℓp norm, and entry-wise ℓp\nnorm is equivalent to entry-wise ℓ∞norm when the number of entries are ﬁnite, uniform continuity\nimplies that\n∀ϵ> 0,∃δ >0 such that ∀X,Y ,∥X −Y ∥∞<δ =⇒ ∥f(X) −f(Y )∥p <ϵ.\nThis means that given any ϵ/3 > 0, we have such a δ >0. Using this δ, we can create a grid Gδ\nand corresponding cubes SL, as described in the main text. For any L ∈Gδ, we deﬁne CL ∈SL\nto be the center point of the cube SL. Then, we can deﬁne a piece-wise constant approximation\nf(X) = ∑\nL∈Gδ\nf(CL)1 {X ∈SL}. Note that, for any X ∈SL, we have ∥X −CL∥∞< δ, so\nby uniform continuity, we have\nf(X) −f(X)\n\np = ∥f(X) −f(CL)∥p < ϵ/3. This proves that\ndp(f,f) <ϵ/3.\nAs for permutation equivariance, since f is permutation equivariant, we have f(CLP) = f(CL)P\nfor any permutation matrix P. For any X ∈SL, we have XP ∈SLP , so\nf(XP ) = f(CLP ) = f(CLP) = f(CL)P = f(X)P.\nThus, the approximation f is also permutation equivariant. This proves the lemma.\n13\nPublished as a conference paper at ICLR 2020\nB.2 A PPROXIMATING T\n2,1,1\nWITH T2,1,4\nLemma 9. For each g∈T\n2,1,1\nand 1 ≤p< ∞, ∃g∈T 2,1,4 such that dp(g,g) ≤ϵ/3.\nProof Recall that Th,m,r refers to the class of functions representable with composition of Trans-\nformer blocks with hheads of size min self-attention layers and r hidden nodes in feed-forward\nlayers. The same notation holds for the modiﬁed Transformers T\nh,m,r\n.\nNote that the softmax operator on a matrix A can be made arbitrarily close to hardmax by scaling\nup A. That is,\nσ[λA] →σH[A] as λ→∞.\nThis means that by scaling up parameters insideσ, we can approximate σH arbitrarily closely. Thus,\nthe modiﬁed self-attention layers can be approximated with the original self-attention layers of the\nsame number of heads hand head size m.\nAlso, any arbitrary (possibly discontinuous) piecewise linear function φ ∈Φ can be approximated\narbitrarily closely by four ReLU’s. Note that φ∈Φ as at most three pieces, and at least one of the\npieces is constant. For example, consider the following function φ∈Φ:\nφ(t) =\n\n\n\nb1 if t<c 1,\na2t+ b2 if c1 ≤t<c 2,\na3t+ b3 if c2 ≤t.\nThis function can be approximated by four ReLU’s, as claimed by the lemma:\n˜φ(t) = b1 + a2c1 + b2 −b1\nϵ ReLU(t−c1 + ϵ) +\n(\na2 −a2c1 + b2 −b1\nϵ\n)\nReLU(t−c1)\n+\n(a3c2 + b3 −a2(c2 −ϵ) −b2\nϵ −a2\n)\nReLU(t−c2 + ϵ)\n+\n(\na3 −a3c2 + b3 −a2(c2 −ϵ) −b2\nϵ\n)\nReLU(t−c2)\n=\n\n\n\nb1 if t<c 1 −ϵ,\na2c1+b2−b1\nϵ (t−c1) + a2c1 + b2 if c1 −ϵ≤t<c 1,\na2t+ b2 if c1 ≤t<c 2 −ϵ,\na3c2+b3−a2(c2−ϵ)−b2\nϵ (t−c2) + a3c2 + b3 if c2 −ϵ≤t<c 2,\na3t+ b3 if c2 ≤t.\nAlso, as we make ϵ→0, we can approximate φas closely as possible using ˜φ. The cases where the\nsecond or third piece is constant can be shown similarly. This means that the modiﬁed feed-forward\nlayers (whose activation is φ ∈Φ) with single hidden node can be approximated with the original\nfeed-forward layers (ReLU) with four hidden nodes.\nThus, given anyg∈T\n2,1,1\n, there exists a functiong∈T 2,1,4 arbitrarily close to g, by appropriately\nchoosing the parameters to be large enough. This ﬁnishes the proof.\nB.3 F INISHING PROOF OF PROPOSITION 4\nAs we have already discussed in Section 4, we establish Proposition 4 in three steps:\n1. Given an input X, a group of feed-forward layers in the modiﬁed Transformer network can\nquantize X to an element L on the extended grid G+\nδ := {−δ−nd,0,δ,..., 1 −δ}d×n.\n2. Next, a group of self-attention layers in the modiﬁed Transformer network can take the\ninput L and produce desirable contextual mappings q(L) such that, for L and ˜L, that are\nnot permutation of each other, all the elements in q(L) and q( ˜L) are distinct.\n14\nPublished as a conference paper at ICLR 2020\n3. Finally, a group of feed-forward layers in the modiﬁed Transformer network can map ele-\nments of the contextual embeddingq(L) to the desirable values, i.e., the output off ∈FPE\non the input X.\nThese steps are formally stated in Lemmas 5, 6, and 7 in the main text. We present the proofs of\nthese lemmas in the subsequent sections.\nWith the results established in these lemmas, we are now equipped with all the tools necessary to\ncomplete the proof of Proposition 4. Let us recall the functions gq,gc, and gv from Lemma 5, 6,\nand 7, respectively. We now show that the (modiﬁed) Transformer network g = gv ◦gc ◦gq\napproximates the underlying peicewise constant function f ∈FPE over all points in its support\nexcept for a set of of measure O(δd).\nConsider a point X ∈SL ⊂[0,1]d×n, where L ∈˜Gδ. By Lemma 5, we have that gq(X) = L.\nThus, it follows from Lemmas 6 and 7 that\ngv ◦gc ◦gq(X) = gv ◦gc(L) =\n[\ngtkn\nv (gc(L)·,1) gtkn\nv (gc(L)·,2) ··· gtkn\nv (gc(L)·,n)\n]\n= AL.\nOn the other hand, any point X ∈⋃\nL∈Gδ\\˜Gδ\nSL ∪(Rd×n \\[0,1]d×n) is mapped by gq to L ∈\nG+\nδ \\˜Gδ; as a result, we get gv ◦gc ◦gq(X) = gv ◦gc(L) = 0.\nTherefore, we haveg(X) = gv ◦gc ◦gq(X) = AL = f(X) for X ∈⋃\nL∈˜Gδ\nSL, and 0 everywhere\nelse. Recall that f has its compact support in [0,1]d, thus bounded; i.e., there exists B ≥0 such\nthat ∥f(X)∥p ≤B. The modiﬁed Transformer network g takes the same value as f on all points\nin [0,1]d except for a set ⋃\nL∈Gδ\\˜Gδ\nSL that has measure O(δd). This implies that dp(f,g) ≤\n(Bpδd)1/p = O(δd/p).\nB.4 P ROOF OF LEMMA 5\nThe proof strategy is simple; using 1\nδ + 1 token-wise feed-forward layers, we implement the quan-\ntization function gent\nq that works on the ﬁrst row of the input. Then stack another 1\nδ + 1 layers that\nquantizes the second row, and so on.\nGiven inputX, we ﬁrst start by clippingX1,: in the set(−∞,0)∪[1,+∞) and mapping the intervals\nto −δ−nd. This can be done by the following layer:\nZ ↦→Z + e(1)φ((e(1))TZ), φ(t) =\n{−t−δ−nd if t< 0 or t≥1,\n0 otherwise.\nNext, add 1/δlayers of the following form, for k= 0,δ,..., 1 −δ.\nZ ↦→Z + e(1)φ((e(1))TZ −kδ1T\nn), φ(t) =\n{0 t< 0 or t≥δ\n−t 0 ≤t<δ.\nEach layer quantizes X1,: in [kδ,kδ + δ) to kδ, without modifying other intervals.\nNote that both φ’s used in this construction are piecewise linear functions with three pieces, and at\nleast one of them are constant. Thus, both φ’s are inΦ. We can repeat the same thing for the other\nrows, and at the end we will get a map from Rd×n to G+\nδ .\nB.5 P ROOF OF LEMMA 6\nSelective shift operation. Before starting the proof, we ﬁrst describe the key component of our\nproof, which we refer to the selective shift operation. Consider the following function, which can\nbe expressed with a multiplicative attention head, with head size m= 1 and hardmax σH:\nψ(Z; bQ) = e(1)uTZσH[(uTZ)T(uTZ −bQ1T\nn)]\nwhere u ∈Rd is a vector that we will choose later, and e(1) = (1,0,0,..., 0) ∈Rd is the standard\nbasis vector.\n15\nPublished as a conference paper at ICLR 2020\nTo see what this function computes, ﬁrst consider the j-th column of the attention score matrix:\n(uTZ)T(uTZ:,j −bQ). Note that, if uTZ:,j >bQ, σH will calculate arg max of uTZ, whereas if\nuTZ:,j < bQ, it will calculate arg min. Therefore, the (1,j)-th entry of ψ(Z; bQ) ∈Rd×n can be\nwritten as\nψ(Z; bQ)1,j = uTZσH[(uTZ)T(uTZ:,j −bQ)] =\n{maxkuTZ:,k if uTZ:,j >bQ,\nminkuTZ:,k if uTZ:,j <bQ,\nfor j ∈[n]. Note that due to e(1), all rows of ψ(Z; bQ) except the ﬁrst row are zero. From this\nobservation, one can deﬁne a function parametrized by bQ and b′\nQ, where bQ < b′\nQ, which consists\nof two attention heads:\nΨ(Z; bQ,b′\nQ) := ψ(Z; bQ) −ψ(Z; b′\nQ),\nΨ(Z; bQ,b′\nQ)1,j =\n{\nmaxkuTZ:,k −minkuTZ:,k if bQ <uTZ:,j <b′\nQ,\n0 if uTZ:,j <bQ or uTZ:,j >b′\nQ.\nWhat this means is that, if we deﬁne an attention layer of the form Z ↦→ Z + Ψ(Z; bQ,b′\nQ),\nthen any column Z:,j satisfying uTZ:,j ∈ (bQ,b′\nQ) is shifted up in its ﬁrst coordinate Z1,j by\nmaxkuTZ:,k −minkuTZ:,k, while all the other coordinates stay untouched . We call this the\nselective shift operation, because we can choose bQ and b′\nQ to selectively shift certain entries of the\ninput.\nBijective column id mapping. Recall that the input to this step is from the range of gq\n(Lemma 5), which is G+\nδ = {−δ−nd,0,δ,..., 1 −δ}d×n. Now consider L ∈ G+\nδ and u =\n(1,δ−1,δ−2,...,δ −d+1).\nFor any j ∈[n], it is easy to check two following facts:\n1. If Li,j ̸= −δ−nd for all i ∈[d], i.e., L:,j ∈{0,δ,..., 1 −δ}d, then uTL:,j ∈[0 : δ :\nδ−d+1 −δ], and the map L:,j ↦→uTL:,j from {0,δ,..., 1 −δ}d to [0 : δ: δ−d+1 −δ] is a\nbijection.\n2. If there exists i∈[d] such that Li,j = −δ−nd, then uTL:,j ≤−δ−nd + δ−d+1 −1 <0.\nTherefore, one can say that uTL:,j gives the “column id” for each possible value of L:,j ∈\n{0,δ,..., 1 −δ}d.\nThe rough idea of the construction is to apply the selective shift operation to each column id, by\nsetting u in the deﬁnition of Ψ(·) to be (1,δ−1,δ−2,...,δ −d+1) and choosing bQ = l−δ/2 and\nb′\nQ = l+ δ/2 for each l ∈[0 : δ : δ−d+1 −δ]. More concretely, we stack (1/δ)d attention layers,\nwith attention parts δ−dΨ(·; l−δ/2,l + δ/2) for each l∈[0 : δ: δ−d+1 −δ], in increasing order of\nl. After that, we add an extra single-head attention layer with attention part δ−(n+1)dψ(·; 0).\nWe now divide possible input values L ∈G+\nδ into three disjoint categories, and show how these\nlayers change the input values at the end of all the layers. Recall the hierarchy ˜Gδ ⊂Gδ ⊂G+\nδ .\nThe categories are deﬁned as follows:\n1. L ∈˜Gδ. All entries are between 0 and 1 −δ, and all columns are unique.\n2. L ∈Gδ \\˜Gδ. All entries are between 0 and 1 −δ, but there are duplicate columns.\n3. L ∈G+\nδ \\Gδ. The point has at least one entry that equals to −δ−nd.\nB.5.1 C ATEGORY 1\nIn Category 1, we haveL ∈˜Gδ. Let lj := uTL:,j. Due to permutation equivariance, we can assume\nwithout loss of generality that lj’s are in increasing order: l1 < l2 < ··· < ln. The ﬁrst (1/δ)d\nlayers sweep the set [0 : δ : δ−d+1 −δ] and apply selective shift operation on each element in the\nset. This means that selective shift operation will be applied to l1 ﬁrst, then l2, and then l3, and so\non, regardless of the speciﬁc values of lj’s.\n16\nPublished as a conference paper at ICLR 2020\nFirst shift operation. In the ﬁrst selective shift operation, the (1,1)-th entry of L (L1,1) is shifted\nby the operation, while the other entries are left untouched. The updated value ˜L1,1 is\n˜L1,1 = L1,1 + δ−d(maxkuTL:,k −minkuTL:,k) = L1,1 + δ−d(ln −l1).\nTherefore, after the operation, the output of the layer is\n[˜L:,1 L:,2 ... L:,n\n]\n, and the new value\nof the ﬁrst column ˜L:,1 results in\nuT ˜L:,1 = ˜L1,1 +\nd∑\ni=2\nδ−i+1Li,1 = L1,1 + δ−d(ln −l1) +\nd∑\ni=2\nδ−i+1Li,1 = l1 + δ−d(ln −l1).\nLet us denote the updated “column id” uT ˜L:,1 as ˜l1. We can show that ln <˜l1, because\n˜l1 := l1 + δ−d(ln −l1) ≥0 + δ−d ·δ= δ−d+1 >ln.\nTherefore, after updating,\nmax uT [˜L:,1 L:,2 ... L:,n\n]\n= max{˜l1,l2,...,l n}= ˜l1,\nand the new minimum is l2.\nSecond shift operation. The second selective shift operation is applied to l2, by which only one\nentry L1,2 will be shifted. The updated value ˜L1,2 is\n˜L1,2 = L1,2 + δ−d(˜l1 −l2) = L1,2 + δ−d(l1 −l2) + δ−2d(ln −l1).\nAfter updating, the new inner product of u and ˜L:,2 results in\n˜l2 := uT ˜L:,2 = l2 + δ−d(l1 −l2) + δ−2d(ln −l1).\nWe can show that˜l1 <˜l2, because\nl1 + δ−d(ln −l1) <l2 + δ−d(l1 −l2) + δ−2d(ln −l1)\n⇔ (δ−d −1)(l2 −l1) <δ−d(δ−d −1)(ln −l1),\nand the last inequality is true because δ−d > 1 and ln > l2. Since we have ˜l1 < ˜l2, and the new\nmaximum in uT [˜L:,1 ˜L:,2 L:,3 ... L:,n\n]\nis now ˜l2, and the new minimum is l3.\nRepeating the process. More generally, we can repeat this process, and show that the j-th shift\noperation shifts L1,j by δ−d(˜lj−1 −lj), and results in the new column id\n˜lj := uT ˜L:,j = lj +\nj−1∑\nk=1\nδ−kd(lj−k −lj−k+1) + δ−jd(ln −l1).\nIn the general case, ˜lj−1 <˜lj holds j = [2 : n], because\n˜lj−1 = lj−1 +\nj−1∑\nk=2\nδ−kd+d(lj−k −lj−k+1) + δ−(j−1)d(ln −l1)\n<˜lj = lj +\nj−1∑\nk=1\nδ−kd(lj−k −lj−k+1) + δ−jd(ln −l1)\n⇔\nj−1∑\nk=1\nδ−kd+d(δ−d −1)(lj−k+1 −lj−k) <δ−(j−1)d(δ−d −1)(ln −l1),\nand the last inequality holds because\nδ−(j−1)d(ln −l1) >δ−(j−1)d\nj−1∑\nk=1\n(lj−k+1 −lj−k) >\nj−1∑\nk=1\nδ−kd+d(lj−k+1 −lj−k).\nTherefore, after the j-th selective shift operation, ˜lj is the new maximum among\n{˜l1,..., ˜lj,lj+1,...,l n}and lj+1 is the new minimum, which makes us possible to continue the\nprocess until the n-th operation.\n17\nPublished as a conference paper at ICLR 2020\nAfter n shift operations. As a result, after the whole sweep from 0 to δ−d+1 −δ by the ﬁrst\n(1/δ)d layers, a total of nshift operations are applied, and the input L is mapped to a new point ˜L,\nwhere uT ˜L =\n[˜l1 ˜l2 ... ˜ln\n]\nand ˜l1 <˜l2 <··· <˜ln.\nWe can now prove the following technical lemma, whose proof is deferred to Appendix B.5.4:\nLemma 10. After nshift operations, ˜ln = uT ˜L:,n satisﬁes the following bounds:\nδ−(n−1)d+1(δ−d −1) ≤˜ln ≤δ−nd+1(δ−d −1) −δ(δ−d −1)2.\nAlso, the map from [l1 l2 ··· ln] ∈[0 : δ : δ−d+1 −δ] (where l1 < l2 < ··· < ln) to ˜ln is\none-to-one.\nGlobal shifting by the last layer. As mentioned earlier, after this sweep, there is another attention\nlayer with attention part δ−(n+1)dψ(·; 0). Since 0 <˜l1 <··· <˜ln, what it does to ˜L is that it adds\nδ−(n+1)dmaxkuT ˜L:,k = δ−(n+1)d˜ln to each entry in the ﬁrst row of ˜L. The output of this layer is\ndeﬁned to be the function gc(L).\nNow, in summary, for anyL ∈˜Gδ, i∈[d], and j ∈[n], we have\ngc(L)i,j =\n{\nL1,j + ∑j−1\nk=1 δ−kd(lj−k −lj−k+1) + δ−jd(ln −l1) + δ−(n+1)d˜ln if i= 1,\nLi,j if i∈[2,d],\nand for any L ∈˜Gδ and j ∈[n],\nuTgc(L):,j = ˜lj + δ−(n+1)d˜ln.\nChecking Properties 6.1 and 6.2. Given this result so far, it is now left to check if the constructed\nnetwork is really a permutation equivariant contextual mapping, i.e., if it satisﬁes Properties 6.1 and\n6.2 in Lemma 6.\nFirst, for any L ∈˜Gδ, Property 6.1 holds because we already know ˜l1 < ˜l2 < ··· < ˜ln, so they\nare all distinct. As for Property 6.2, note that the upper bound on ˜ln from Lemma 10 also holds for\nother ˜lj’s, so\nuTgc(L):,j ∈[δ−(n+1)d˜ln,δ−(n+1)d˜ln + δ−(n+1)d+1),\nfor all j ∈ [n]. Now, from Lemma 10, two L,L′ ∈ ˜Gδ (that are not permutations of each\nother) map to different ˜ln and ˜l′\nn, and they differ at least by δ. This means that two intervals\n[δ−(n+1)d˜ln,δ−(n+1)d˜ln+δ−(n+1)d+1) and [δ−(n+1)d˜l′\nn,δ−(n+1)d˜l′\nn+δ−(n+1)d+1) are guaranteed\nto be disjoint, so the entries of uTgc(L) and uTgc(L′) are all distinct. This proves Property 6.2.\nTherefore, we ﬁnished showing that the map gc(·) we constructed using (1/δ)d+ 1 attention layers\nimplements a permutation equivariant contextual mapping on ˜Gδ.\nChecking Property 6.3. It is now left to check if the map gc satisﬁes the other properties. At this\npoint, we can check Property 6.3. From uTgc(L):,j ∈[δ−(n+1)d˜ln,δ−(n+1)d˜ln+ δ−(n+1)d+1) and\nLemma 10, we can show that for any L ∈˜Gδ, we have\nδ−2nd+1(δ−d −1) ≤uTgc(L):,j <δ−(n+1)d(δ−nd+1(δ−d −1) −δ(δ−d −1)2) + δ−(n+1)d+1\n≤δ−(2n+1)d+1(δ−d −1),\nwhere we used δ−1 ≥2. This proves that all uTgc(L):,j are between tl = δ−2nd+1(δ−d −1) and\ntr = δ−(2n+1)d+1(δ−d −1). For the remaining input points L ∈G+\nδ \\˜Gδ, we will check that\nuTgc(L):,j is outside the interval [tl,tr] (Property 6.4).\nB.5.2 C ATEGORY 2\nIn Category 2, we haveL ∈Gδ\\˜Gδ. Here, all entries are between0 and 1−δ, but there are duplicate\ncolumns. Again, let lj := uTL:,j, and assume without loss of generality that l1 ≤l2 ≤···≤ ln.\n18\nPublished as a conference paper at ICLR 2020\nFor the input L in Category 2, there exist some j,j′∈[n], j ̸= j′, such that lj = lj′ . This means\nthat when the input passes through the attention layerδ−dΨ(·; lj−δ/2,lj+ δ/2), the selective shift\noperation for lj is applied to both j-th and j′-th columns; the two columns are coupled together.\nMore generally, suppose we have n′<n distinct columns.\nIf n′ = 1 . In the extreme case of n′ = 1 , we have maxjlj = min jlj, so the selective shift\noperation applied at lj does not shift the entry at all; therefore, at the end of the ﬁrst(1/δ)d attention\nlayers, ˜L = L.\nIf 1 <n′≤n−1. When 1 <n′≤n−1, let the n′distinct values of lj’s bel′\n1,...,l ′\nn′ . The shift\noperation is applied n′times, to l′\n1,...,l ′\nn′ , and shifts one or more entries at a time. After the ﬁrst\n(1/δ)d layers, the output ˜L has n′distinct ˜lj = uT ˜L:,j, 0 ≤˜l1 ≤˜l2 ≤···≤ ˜ln, whose distinct\nvalues are the same as the numbers we get when we apply shift operations to a length- n′sequence\n[l′\n1 ... l ′\nn′ ]. Then, applying the same calculations from Category 1 shows that\n˜ln = uT ˜L:,n = l′\nn′ +\nn′−1∑\nk=1\nδ−kd(l′\nn′−k −l′\nn′−k+1) + δ−n′d(l′\nn′ −l′\n1),\nand it follows from the upper bound in Lemma 10 that\n˜ln ≤δ−n′d+1(δ−d −1) −δ(δ−d −1)2 <δ−(n−1)d+1(δ−d −1).\nNote that the RHS matches the lower bound in Lemma 10. This implies that the value of ˜ln cal-\nculated from the input L ∈Gδ \\˜Gδ (Category 2) is always strictly less (by at least δ) than that\ncalculated from L ∈˜Gδ (Category 1).\nChecking Property 6.4. After the global shifting by the last layer with attention part\nδ−(n+1)dψ(·; 0), we get the output gc(L) which satisﬁes\nuTgc(L):,j = ˜lj + δ−(n+1)d˜ln ≤(δ−(n+1)d + 1)(δ−(n−1)d+1(δ−d −1) −δ(δ−d −1)2)\n<δ−2nd+1(δ−d −1) =: tl.\nwhere the RHS is a lower bound on possible values of uTgc(L):,j for L ∈˜Gδ (Category 1). This\nmeans that the entries of uTgc(L) for Category 2 are outside [tl,tr], which satisﬁes Property 6.4.\nB.5.3 C ATEGORY 3\nIn Category 3, we have L ∈G+\nδ \\Gδ; the point L has at least one entry that equals to −δ−nd. Let\nlj := uTL:,j, and recall that whenever a column L:,j has an entry that equals to −δ−nd, we have\nlj = uTL:,j ≤−δ−nd+δ−d+1 −1 <0. Assume without loss of generality thatl1 ≤l2 ≤···≤ ln.\nRecall that the selective shift operation is applied to each element of [0 : δ : δ−d+1 −δ], not to\nnegative values. In case of Category 3, we have minkuTL:,k = l1 < 0, and l1 never gets shifted\nupwards, so it remains as the minimum for the whole time.\nIf all lj’s are negative. In case where all lj’s are negative, selective shift operation never changes\nthe input L, so we get ˜L = L. Since we have uT ˜L <0T\nn (entry-wise), the last layer with attention\npart δ−(n+1)dψ(·; 0) adds δ−(n+1)dminkuT ˜L:,k < 0 to each entry in the ﬁrst row of ˜L, further\npushing it to the negative side. Therefore, the ﬁnal output gc(L) satisﬁes uTgc(L) <0T\nn <tl1T\nn.\nIf not all lj’s are negative. Now consider the case where at least one lj is positive. Let ibe the\nindex that satisﬁes li−1 < 0 ≤li. Then, selective shift operation does not affect l1,...,l i−1, and\nthen it shifts li by\nδ−d(max\nk\nuTL:,k −min\nk\nuTL:,k) = δ−d(ln −l1) ≥δ−d(0 + δ−nd −δ−d+1 + 1) ≥δ−(n+1)d+1,\nwhere we used δ−1 ≥2 at the last inequality. The next shift operations shift li+1,...,l n by even\nlarger amount, so at the end of the ﬁrst (1/δ)d layers, we have δ−(n+1)d+1 ≤˜li ≤···≤ ˜ln, while\n˜lj = lj <0 for j ∈[i−1].\n19\nPublished as a conference paper at ICLR 2020\nShifts by the last layer. Here, the last layer with attention part δ−(n+1)dψ(·; 0) acts differently\nfor negative and positive ˜lj’s. For negative ˜lj’s, it adds δ−(n+1)dmink˜lk = δ−(n+1)dl1 < 0\nto ˜l1,..., ˜li−1, pushing them further to the negative side. For positive ˜lj’s, the layer adds\nδ−(n+1)dmaxk˜lk = δ−(n+1)d˜ln ≥δ−(2n+2)d+1 to ˜li,..., ˜ln, so that they are all greater than or\nequal to δ−(2n+2)d+1. Note that δ−(2n+2)d+1 >tr.\nChecking Property 6.4. Therefore, in both cases, we can see that the ﬁnal output gc(L) satisﬁes\nuTgc(L):,j /∈[tl,tr], for all j ∈[n]. This completes the veriﬁcation of Property 6.4.\nB.5.4 P ROOF OF LEMMA 10\nProof of lower and upper bounds on ˜ln are straightforward:\n˜ln := ln +\nn−1∑\nk=1\nδ−kd(ln−k −ln−k+1) + δ−nd(ln −l1)\n≥δ−(n−1)d\nn−1∑\nk=1\n(ln−k −ln−k+1) + δ−nd(ln −l1) = (δ−nd −δ−(n−1)d)(ln −l1)\n≥δ−(n−1)d+1(δ−d −1),\n˜ln ≤ln + δ−d(l1 −ln) + δ−nd(ln −l1) ≤δ−d+1 −δ+ (δ−nd −δ−d)(δ−d+1 −δ)\n= δ−nd+1(δ−d −1) −δ(δ−2d −2δ−d + 1) = δ−nd+1(δ−d −1) −δ(δ−d −1)2.\nFor one-to-one property of the map, consider[l1 l2 ··· ln] and [l′\n1 l′\n2 ··· l′\nn] with increas-\ning entries, which are mapped to ˜ln and ˜l′\nn, respectively. Suppose ˜ln = ˜l′\nn. By deﬁnition,\n˜ln −˜l′\nn =(ln −l′\nn) + δ−d(ln−1 −ln −l′\nn−1 + l′\nn) + δ−2d(ln−2 −ln−1 −l′\nn−2 + l′\nn−1) + ...\n+ δ−(n−1)d(l1 −l2 −l′\n1 + l′\n2) + δ−nd(ln −l1 −l′\nn + l′\n1) = 0.\nNow assume for contradiction that ln ̸= l′\nn. Then, we have −δ−d+1 + δ ≤ln −l′\nn ≤δ−d+1 −δ.\nHowever, the remaining terms have “coarse resolution”, and they can never cancel ln −l′\nn and\nmake the sum zero, because for example, δ−d(ln−1 −ln −l′\nn−1 + l′\nn) can only have values\n0,δ−d+1,−δ−d+1,2δ−d+1,−2δ−d+1,... . Thus, ln = l′\nn must hold and the ﬁrst term must be\nzero.\nSimilarly, assume that ln−1 ̸= l′\nn−1. Then, the second term is in the interval [−δ−2d+1 +\nδ−d+1,δ−2d+1 −δ−d+1]. Again, the remaining terms cannot cancel the second term, hence\nln−1 = l′\nn−1 must hold. We can proceed this way, and show that lj = l′\nj must hold for all j ∈[n],\nhence proving that the map is one-to-one.\nB.6 P ROOF OF LEMMA 7\nNote that |G+\nδ |= (1\nδ + 1)dn, so the image of gc(G+\nδ ) (from Lemma 6) has ﬁnite number of distinct\nreal numbers. Let M be the maximum over all these numbers. By construction of gc, we know that\nM >0.\nTo construct a functiongtkn\nv that satisﬁes the statement of the lemma, we ﬁrst implement the second\npart: gtkn\nv (gc(L):,j) = 0d if L ∈G+\nδ \\˜Gδ. Note from Lemma 6 that, for any L ∈˜Gδ, we have\nuTgc(L):,j ∈[tl,tr] for all j, and for any L ∈G+\nδ \\˜Gδ, uTgc(L):,j /∈[tl,tr] for all j. Using this,\nwe add the following feed-forward layer:\nZ ↦→Z −(M + 1)1nφ(uTZ), φ(t) =\n{0 if t∈[tl,tr]\n1 if t /∈[tl,tr].\nInput to this layer is gc(L). If L ∈˜Gδ, then φ(uTgc(L)) = 0T\nn, so the output stays the same as\nthe input. If L ∈G+\nδ \\˜Gδ, then φ(uTgc(L)) = 1T\nn, so all the entries of the input are shifted by\n−M −1, and become strictly negative.\n20\nPublished as a conference paper at ICLR 2020\nRecall that by deﬁnition of ˜Gδ, all the entries of gc(L) for L ∈ ˜Gδ are nonnegative. So the next\nthing to do is mapping all strictly negative entries to zero. This can be done in a similar way as\nLemma 5. For i∈[d], add the following layer:\nZ ↦→Z + e(i)φ((e(i))TZ), φ(t) =\n{−t if t< 0\n0 if t≥0.\nAfter these dlayers, the output for L ∈G+\nδ \\˜Gδ is a zero matrix, while the output for L ∈˜Gδ is\ngc(L).\nNow, it is left to map gc(L) to AL, for L ∈ ˜Gδ. Up to permutation equivariance, each different\ncontext L maps to nunique numbers uTgc(L), which are at least δapart from each other. The idea\nof value mapping is to map each unique number to the corresponding output column.\nMore precisely, choose any L ∈ ˜Gδ. For each value of uTgc(L):,j, j ∈[n], we add one feed-\nforward layer\nZ ↦→Z + ((AL):,j −gc(L):,j)φ(uTZ −uTgc(L):,j1T\nn), φ(t) =\n{0 t< −δ/2 or t≥δ/2,\n1 −δ/2 ≤t<δ/ 2.\nIf the input Z is a zero matrix, which is the case for L ∈G+\nδ \\˜Gδ, uTZ = 0T\nn. Since tl is much\nlarger than 0, activation is all zero. Thus, zero input matrix remains the same at the output.\nIf the input Z is gc(L), where L ∈˜Gδ is not a permutation of L, then\nφ(uTgc(L) −uTgc(L):,j1T\nn) = 0T\nn,\nso gc(L) is left untouched.\nIf some other L is a permutation of L, and L:,i = L:,j, then\nφ(uTgc(L) −uTgc(L):,j1T\nn) = (e(i))T,\nso i-th column of gc(L) will turn to\ngc(L):,i ↦→gc(L):,i + ((AL):,j −gc(L):,j) = gc(L):,i + ((AL):,i −gc(L):,i) = (AL):,i,\nwhich is the desired output. In conclusion, this layer maps the column gc(L):,j to (AL):,j, without\naffecting any other columns.\nAs seen above, we need one layer per each unique value of uTgc(L):,j for each L ∈ ˜Gδ. Note\nthat there are O(n(1/δ)dn/n!) such numbers, so we can use O(n(1/δ)dn/n!) layers to ﬁnish our\nconstruction.\nC P ROOF OF THEOREM 3\nProof of Theorem 3 can be done in a similar way as Theorem 2. As in the proof of Theorem 2, there\nare three parts: Lemma 8, Proposition 4, and Lemma 9. The statement and proof of Lemmas 8 and\n9 can be done in almost the same way, this time without permutation equivariance.\nFor the proof of the second part, which corresponds to Proposition 4, we construct the network in a\nsimilar way. Recall that we can assume without loss of generality that X ∈[0,1]d×n. Choose\nE =\n\n\n0 1 2 ··· n−1\n0 1 2 ··· n−1\n... ... ... ...\n0 1 2 ··· n−1\n\n.\nThen, the ﬁrst column of X + E is in [0,1]d, second is in [1,2]d, and so on; this means that for\nall rows, the coordinates are monotonically increasing. So we can use the same technique as the\nproof of Proposition 4 to divide the input values into cubes, quantize them to L, apply contextual\nmapping, and then value mapping. We describe each step in the following.\n21\nPublished as a conference paper at ICLR 2020\nC.1 Q UANTIZATION BY FEED -FORWARD LAYERS\nIn a similar way as Lemma 5, the goal of this step is to quantize the input in[0,1]d×[1,2]d×···×\n[n−1,n]d to its discrete version:\n[0 : δ: 1 −δ]d ×[1 : δ: 2 −δ]d ×···× [n−1 : δ: n−δ]d.\nThis can be done by dn/δ feed-forward layers. We add dn/δ layers of the following form, for\nk= 0,δ,...,n −δand i= 1,...,d :\nZ ↦→Z + e(i)φ((e(i))TZ −kδ1T\nn), φ(t) =\n{0 t< 0 or t≥δ\n−t 0 ≤t<δ.\nAfter dn/δlayers, any input entry of X + E in [kδ,kδ + δ) is quantized to kδ.\nC.2 C ONTEXTUAL MAPPING BY ATTENTION LAYERS\nBy Step 1, we quantized any input X + E to its quantized version. We call this quantized version\nL:\nL ∈[0 : δ: 1 −δ]d ×[1 : δ: 2 −δ]d ×···× [n−1 : δ: n−δ]d.\nAs done in Lemma 6, we deﬁne u := (1,δ−1,...,δ −d+1) and lj := uTL:,j, for all j ∈[n]. Note\nthat, because L:,j ∈[j−1 : δ: j−δ]d, we have\n(j−1)(1 + δ−1 + ··· + δ−d+1) ≤lj ≤(j−1)(1 + δ−1 + ··· + δ−d+1) + δ−d+1 −δ,\nand l1 <l2 <··· <ln. Notice that this corresponds to the Category 1 in the proof of Lemma 6.\nFor simplicity of notation, let sj = ( j −1) ∑d−1\nk=0 δ−k. We stack n(1/δ)d attention layers, with\nattention parts δ−dΨ(·; l−δ/2,l + δ/2) for each l∈⋃n\nj=1[sj : δ : sj + δ−d+1 −δ], in increasing\norder of l.\nThese n(1/δ)d attention layers perform selective shift operations on lj’s, in increasing order of j.\nAs seen in Appendix B.5.1, shift operations result in ˜l1 <˜l2 <··· <˜ln. Also, the map from L to\n˜ln is one-to-one, which can be shown in the same way as Appendix B.5.4. Since the range of lj’s\nare a bit different, we have a different upper bound on ˜ln:\n˜ln := ln +\nn−1∑\nk=1\nδ−kd(ln−k −ln−k+1) + δ−nd(ln −l1)\n≤ln + δ−d(l1 −ln) + δ−nd(ln −l1) ≤sn + δ−d+1 −δ+ (δ−nd −δ−d)(sn + δ−d+1 −δ)\n= (δ−nd −δ−d + 1)\n(\n(n−1)δ−d −1\nδ−1 −1 + δ−d+1 −δ\n)\n≤(δ−nd −δ−d + 1)(δ−d −1)(n−1 + δ) <nδ−(n+1)d.\nFinally, we add an extra single-head attention layer with attention part nδ−(n+1)d−1ψ(·; 0). We\ndeﬁne the output of this layer as gc(L). In a similar way as Appendix B.5.1, this layer shifts all the\nlayers by nδ−(n+1)d−1˜ln, thus making the intervals corresponding to different values of ˜ln disjoint\nfrom each other. This ensures that different contexts L are mapped to distinct numbers in uTgc(L),\nthus implementing a contextual mapping.\nC.3 F UNCTION VALUE MAPPING BY FEED -FORWARD LAYERS\nNow, it is left to map gc(L) to the desired output. As seen in the last step, each different context L\nmaps to nunique numbers uTgc(L), which are at least δapart from each other. The value mapping\nstep can be done in a similar way as Lemma 7. The construction now requires O(n(1/δ)dn) layers\nbecause there is no permutation equivariance.\n22\nPublished as a conference paper at ICLR 2020\nArchitecture Average Attention BProj SepConv Transformer\n# params 88.3M 90M 102.5M 110M\nMasked LM accuracy (%) 28 59 60 63\nMNLI accuracy (%) 66 72.3 73 78.2\nTable 1: Performance of bi-linear projection and separable convolution layers on masked LM pre-\ntraining task and MNLI. Note that we expect these computationally cheaper models to have lower\nperformance than the expensive Transformers as they do not compute input dependent attention\nweights and have weaker representation power. Our goal in studying them is to see if they can\nsubstitute some of the expensive attention layers for computing the contextual mappings. These\nmodels are trained in a large batch setting, with a batch size of 8192 for 60k steps, unlike the other\nset of experiments reported in Fig. 1. Note that average attention has clearly worse performance,\nshowing that theses tasks indeed require an advanced architecture.\nD E XPERIMENTAL SETUP\nFor our experiments we follow the same setting as in BERT (Devlin et al., 2018). We ﬁrst pre-train\nthe models on the masked language modeling task and the next sentence prediction task. We use\nEnglish Wikipedia corpus and BooksCorpus dataset (Zhu et al., 2015) for this pre-training. We use\nBERTBASE, a 12 layer Transformer model as the baseline. This model uses an embedding size of 768\nand has 12 head self-attention layers and 3072 wide feed forward layers. We train it with the Adam\noptimizer, with .01 dropout and weight decay. We do pre-training for 250k steps with a batch size\nof 1024 and a max sequence length of 512. Pre-training takes around 2 days on 16 TPUv3 chips.\nWe take the pre-train models and ﬁnetune them on the MNLI and SQuAD datasets separately using\nthe same hyper-parameters as in Devlin et al. (2018). MNLI is a sentence entailment task in which,\ngiven a premise sentence, requires us to classify a hypothesis sentence into neutral, contradiction\nor entailment classes. We report the classiﬁcation accuracy on this task. SQuAD is a question\nanswering task, in which given a paragraph and a question, requires us to identify the answer as a\nspan of the words in the paragraph. For this task we report both the F1 score and the Exact Match\n(EM) percentage. The metrics are reported on the dev sets of these datasets.\nFor our experiments with the depth-wise separable convolution layers, we follow the implementation\nin (Wu et al., 2019). We ﬁrst use a GLU layer followed by the convolution layer. We use 16 separable\nconvolution ﬁlters, of ﬁlter length 128, and reuse them, with each ﬁlter operating on 48 of the 768\ndimensions of the input. This layer also has a skip connection and the output is normalized using\nlayer normalization, similar to the self-attention layer. In our experiments, we replace the self-\nattention layers of the Transformers, in the lower layers, with this convolution layer. We keep the\nfeed forward layer of the Transformer block the same.\nFor the experiments performed in this paper, one might consider an alternate explanation that the\ntasks considered maybe are easy, and do not require any advanced architecture to solve them, and\neven a simple architecture (bi-linear projection or separable convolution) might solve these tasks. To\nrule out this case we consider an even simpler architecture, namely average attention, as a baseline\nfor our experiments.\nAverage attention. An average attention layer replaces the self-attention layer, and just computes\nthe average of projections of all the other tokens. That is, we replace σ[(Wi\nKX)TWi\nQX] in (1)\nwith a matrix full of 1/n. The model still has the skip connections and the feed-forward layers like\nTransformer.\n23",
  "topic": "Sequence (biology)",
  "concepts": [
    {
      "name": "Sequence (biology)",
      "score": 0.6479385495185852
    },
    {
      "name": "Transformer",
      "score": 0.5699732303619385
    },
    {
      "name": "Computer science",
      "score": 0.43599697947502136
    },
    {
      "name": "Engineering",
      "score": 0.17184600234031677
    },
    {
      "name": "Electrical engineering",
      "score": 0.10192447900772095
    },
    {
      "name": "Biology",
      "score": 0.09477218985557556
    },
    {
      "name": "Voltage",
      "score": 0.07990524172782898
    },
    {
      "name": "Genetics",
      "score": 0.06973820924758911
    }
  ]
}