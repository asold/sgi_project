{
  "title": "A Flexible POS tagger Using an Automatically Acquired Language Model",
  "url": "https://openalex.org/W2952890717",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5103372090",
      "name": "Lluı́s Màrquez",
      "affiliations": [
        "FC Barcelona",
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A5017535472",
      "name": "Lluís Padró",
      "affiliations": [
        "FC Barcelona",
        "Universitat Politècnica de Catalunya"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2046224275",
    "https://openalex.org/W2121407024",
    "https://openalex.org/W2120770606",
    "https://openalex.org/W1999011285",
    "https://openalex.org/W2289555877",
    "https://openalex.org/W2130851608",
    "https://openalex.org/W2125055259",
    "https://openalex.org/W2952146280",
    "https://openalex.org/W2951437286",
    "https://openalex.org/W2166978661",
    "https://openalex.org/W2046592963",
    "https://openalex.org/W1964372771",
    "https://openalex.org/W2146424733",
    "https://openalex.org/W1604329830",
    "https://openalex.org/W2099111195",
    "https://openalex.org/W2033279368",
    "https://openalex.org/W2953149585",
    "https://openalex.org/W1560729174",
    "https://openalex.org/W2149706766",
    "https://openalex.org/W2102246596",
    "https://openalex.org/W2147169507",
    "https://openalex.org/W1551773846"
  ],
  "abstract": "We present an algorithm that automatically learns context constraints using statistical decision trees. We then use the acquired constraints in a flexible POS tagger. The tagger is able to use information of any degree: n-grams, automatically learned context constraints, linguistically motivated manually written constraints, etc. The sources and kinds of constraints are unrestricted, and the language model can be easily extended, improving the results. The tagger has been tested and evaluated on the WSJ corpus.",
  "full_text": "arXiv:cmp-lg/9707003v1  11 Jul 1997\nA Flexible POS Tagger Using an Automatically Acquired\nLanguage Model ∗\nLlu ´ ıs M` arquez\nLSI - UPC\nc/ Jordi Girona 1-3\n08034 Barcelona. Catalonia\nlluism@lsi.upc.es\nLlu ´ ıs Padr´ o\nLSI - UPC\nc/ Jordi Girona 1-3\n08034 Barcelona. Catalonia\npadro@lsi.upc.es\nAbstract\nWe present an algorithm that automati-\ncally learns context constraints using sta-\ntistical decision trees. We then use the ac-\nquired constraints in a ﬂexible POS tag-\nger. The tagger is able to use informa-\ntion of any degree: n-grams, automati-\ncally learned context constraints, linguis-\ntically motivated manually written con-\nstraints, etc. The sources and kinds of con-\nstraints are unrestricted, and the language\nmodel can be easily extended, improving\nthe results. The tagger has been tested and\nevaluated on the WSJ corpus.\n1 Introduction\nIn NLP, it is necessary to model the language in a\nrepresentation suitable for the task to be performed.\nThe language models more commonly used are based\non two main approaches: ﬁrst, the linguistic ap-\nproach, in which the model is written by a linguist,\ngenerally in the form of rules or constraints (Vouti-\nlainen and J¨ arvinen, 1995). Second, the automatic\napproach, in which the model is automatically ob-\ntained from corpora (either raw or annotated) 1, and\nconsists of n–grams (Garside et al., 1987; Cutting\net al., 1992), rules (Hindle, 1989) or neural nets\n(Schmid, 1994). In the automatic approach we can\ndistinguish two main trends: The low–level data\ntrend collects statistics from the training corpora in\nthe form of n–grams, probabilities, weights, etc. The\nhigh level data trend acquires more sophisticated in-\nformation, such as context rules, constraints, or de-\n∗\nThis research has been partially funded by the Span-\nish Research Department (CICYT) and inscribed as\nTIC96-1243-C03-02\n1When the model is obtained from annotated corpora\nwe talk about supervised learning, when it is obtained\nfrom raw corpora training is considered unsupervised.\ncision trees (Daelemans et al., 1996; M` arquez and\nRodr´ ıguez, 1995; Samuelsson et al., 1996). The ac-\nquisition methods range from supervised–inductive–\nlearning–from–example algorithms (Quinlan, 1986;\nAha et al., 1991) to genetic algorithm strategies\n(Losee, 1994), through the transformation–based\nerror–driven algorithm used in (Brill, 1995). Still\nanother possibility are the hybrid models, which try\nto join the advantages of both approaches (Vouti-\nlainen and Padr´ o, 1997).\nWe present in this paper a hybrid approach that\nputs together both trends in automatic approach\nand the linguistic approach. We describe a POS tag-\nger based on the work described in (Padr´ o, 1996),\nthat is able to use bi/trigram information, auto-\nmatically learned context constraints and linguisti-\ncally motivated manually written constraints. The\nsources and kinds of constraints are unrestricted,\nand the language model can be easily extended. The\nstructure of the tagger is presented in ﬁgure 1.\nTagged CorpusRaw Corpus\nLexicon Trigrams\nBigrams Automatically\nlearned\nconstraints constraints\nwritten\nManually \n...\nLanguage Model\nTagging algorithm\nFigure 1: Tagger architecture.\nWe also present a constraint–acquisition algo-\nrithm that uses statistical decision trees to learn con-\ntext constraints from annotated corpora and we use\nthe acquired constraints to feed the POS tagger.\nThe paper is organized as follows. In section 2 we\ndescribe our language model, in section 3 we describe\nthe constraint acquisition algorithm, and in section\n4 we expose the tagging algorithm. Descriptions of\nthe corpus used, the experiments performed and the\nresults obtained can be found in sections 5 and 6.\n2 Language Model\nWe will use a hybrid language model consisting of an\nautomatically acquired part and a linguist–written\npart.\nThe automatically acquired part is divided in two\nkinds of information: on the one hand, we have bi-\ngrams and trigrams collected from the annotated\ntraining corpus (see section 5 for details). On the\nother hand, we have context constraints learned\nfrom the same training corpus using statistical deci-\nsion trees, as described in section 3.\nThe linguistic part is very small —since there were\nno available resources to develop it further— and\ncovers only very few cases, but it is included to il-\nlustrate the ﬂexibility of the algorithm.\nA sample rule of the linguistic part:\n10.0 (%vauxiliar%)\n(-[VBN IN , : JJ JJS JJR])+\n<VBN>;\nThis rule states that a tag past participle (VBN) is\nvery compatible (10.0) with a left context consisting\nof a %vauxiliar% (previously deﬁned macro which\nincludes all forms of “have” and “be”) provided that\nall the words in between don’t have any of the tags\nin the set [VBN IN , : JJ JJS JJR] . That is,\nthis rule raises the support for the tag past partici-\nple when there is an auxiliary verb to the left but\nonly if there is not another candidate to be a past\nparticiple or an adjective inbetween. The tags [IN\n, :] prevent the rule from being applied when the\nauxiliary verb and the participle are in two diﬀerent\nphrases (a comma, a colon or a preposition are con-\nsidered to mark the beginning of another phrase).\nThe constraint language is able to express the\nsame kind of patterns than the Constraint Gram-\nmar formalism (Karlsson et al., 1995), although in a\ndiﬀerent formalism. In addition, each constraint has\na compatibility value that indicates its strength. In\nthe middle run, the system will be adapted to accept\nCGs.\n3 Constraint Acquisition\nChoosing, from a set of possible tags, the proper syn-\ntactic tag for a word in a particular context can be\nseen as a problem of classiﬁcation. Decision trees,\nrecently used in NLP basic tasks such as tagging\nand parsing (McCarthy and Lehnert, 1995; Daele-\nmans et al., 1996; Magerman, 1996), are suitable for\nperforming this task.\nA decision tree is a n–ary branching tree that rep-\nresents a classiﬁcation rule for classifying the objects\nof a certain domain into a set of mutually exclusive\nclasses. The domain objects are described as a set\nof attribute–value pairs, where each attribute mea-\nsures a relevant feature of an object taking a (ideally\nsmall) set of discrete, mutually incompatible values.\nEach non–terminal node of a decision tree represents\na question on (usually) one attribute. For each possi-\nble value of this attribute there is a branch to follow.\nLeaf nodes represent concrete classes.\nClassify a new object with a decision tree is simply\nfollowing the convenient path through the tree until\na leaf is reached.\nStatistical decision trees only diﬀers from common\ndecision trees in that leaf nodes deﬁne a conditional\nprobability distribution on the set of classes.\nIt is important to note that decision trees can be\ndirectly translated to rules considering, for each path\nfrom the root to a leaf, the conjunction of all ques-\ntions involved in this path as a condition and the\nclass assigned to the leaf as the consequence. Statis-\ntical decision trees would generate rules in the same\nmanner but assigning a certain degree of probability\nto each answer.\nSo the learning process of contextual constraints\nis performed by means of learning one statistical de-\ncision tree for each class of POS ambiguity 2 and con-\nverting them to constraints (rules) expressing com-\npatibility/incompatibility of concrete tags in certain\ncontexts.\nLearning Algorithm\nThe algorithm we used for constructing the statisti-\ncal decision trees is a non–incremental supervised\nlearning–from–examples algorithm of the TDIDT\n(Top Down Induction of Decision Trees) family. It\nconstructs the trees in a top–down way, guided by\nthe distributional information of the examples, but\nnot on the examples order (Quinlan, 1986). Brieﬂy,\nthe algorithm works as a recursive process that de-\nparts from considering the whole set of examples at\nthe root level and constructs the tree in a top–down\nway branching at any non–terminal node according\nto a certain selected attribute. The diﬀerent val-\nues of this attribute induce a partition of the set\nof examples in the corresponding subsets, in which\nthe process is applied recursively in order to gener-\nate the diﬀerent subtrees. The recursion ends, in a\ncertain node, either when all (or almost all) the re-\nmaining examples belong to the same class, or when\nthe number of examples is too small. These nodes\nare the leafs of the tree and contain the conditional\n2 Classes of ambiguity are determined by the groups\nof possible tags for the words in the corpus, i.e,noun-\nadjective, noun-adjective-verb, preposition-adverb, etc.\nprobability distribution, of its associated subset of\nexamples, on the possible classes.\nThe heuristic function for selecting the most\nuseful attribute at each step is of a cru-\ncial importance in order to obtain simple trees,\nsince no backtracking is performed. There ex-\nist two main families of attribute–selecting func-\ntions: information–based (Quinlan, 1986; L´ opez,\n1991) and statistically–based (Breiman et al., 1984;\nMingers, 1989).\nTraining Set\nFor each class of POS ambiguity the initial exam-\nple set is built by selecting from the training corpus\nall the occurrences of the words belonging to this\nambiguity class. More particularly, the set of at-\ntributes that describe each example consists of the\npart–of–speech tags of the neighbour words, and the\ninformation about the word itself (orthography and\nthe proper tag in its context). The window consid-\nered in the experiments reported in section 6 is 3\nwords to the left and 2 to the right. The follow-\ning are two real examples from the training set for\nthe words that can be preposition and adverb at the\nsame time (IN–RB conﬂict).\nVB DT NN <\"as\",IN> DT JJ\nNN IN NN <\"once\",RB> VBN TO\nApproximately 90% of this set of examples is used\nfor the construction of the tree. The remaining 10%\nis used as fresh test corpus for the pruning process.\nAttribute Selection Function\nFor the experiments reported in section 6 we used a\nattribute selection function due to L´ opez de M´ anta-\nras (L´ opez, 1991), which belongs to the information–\nbased family. Roughly speaking, it deﬁnes a distance\nmeasure between partitions and selects for branch-\ning the attribute that generates the closest partition\nto the correct partition, namely the one that joins\ntogether all the examples of the same class.\nLet X be a set of examples, C the set of classes and\nPC(X) the partition of X according to the values of\nC. The selected attribute will be the one that gen-\nerates the closest partition of X to PC(X). For that\nwe need to deﬁne a distance measure between parti-\ntions. Let PA(X) be the partition of X induced by\nthe values of attribute A. The average information\nof such partition is deﬁned as follows:\nI(PA(X)) = −\n∑\na∈ PA(X)\np(X, a ) log 2 p(X, a ) ,\nwhere p(X, a ) is the probability for an element of X\nbelonging to the set a which is the subset of X whose\nexamples have a certain value for the attribute A,\nand it is estimated by the ratio |X∩ a|\n|X| . This average\ninformation measure reﬂects the randomness of dis-\ntribution of the elements of X between the classes of\nthe partition induced by A. If we consider now the\nintersection between two diﬀerent partitions induced\nby attributes A and B we obtain\nI(PA(X) ∩ PB(X)) =\n−\n∑\na∈ PA(X)\n∑\nb∈ PB (X)\np(X, a ∩ b) log 2 p(X, a ∩ b) .\nConditioned information of PB(X) given PA(X) is\nI(PB (X)|PA(X)) =\nI(PA(X) ∩ PB(X)) − I(PA(X)) =\n−\n∑\na∈ PA(X)\n∑\nb∈ PB (X)\np(X, a ∩ b) log 2\np(X, a ∩ b)\np(X, a ) .\nIt is easy to show that the measure\nd(PA(X), P B(X)) =\nI(PB (X)|PA(X)) + I(PA(X)|PB(X))\nis a distance. Normalizing we obtain\ndN (PA(X), P B(X)) = d(PA(X), P B (X))\nI(PA(X) ∩ PB(X)) ,\nwith values in [0 , 1].\nSo the selected attribute will be that one that min-\nimizes the measure: dN (PC(X), P A(X)).\nBranching Strategy\nUsual TDIDT algorithms consider a branch for each\nvalue of the selected attribute. This strategy is not\nfeasible when the number of values is big (or even in-\nﬁnite). In our case the greatest number of values for\nan attribute is 45 —the tag set size— which is con-\nsiderably big (this means that the branching factor\ncould be 45 at every level of the tree 3). Some sys-\ntems perform a previous recasting of the attributes\nin order to have only binary-valued attributes and to\ndeal with binary trees (Magerman, 1996). This can\nalways be done but the resulting features lose their\nintuition and direct interpretation, and explode in\nnumber. We have chosen a mixed approach which\nconsist of splitting for all values and afterwards join-\ning the resulting subsets into groups for which we\nhave not enough statistical evidence of being diﬀer-\nent distributions. This statistical evidence is tested\nwith a χ 2 test at a 5% level of signiﬁcance. In order\nto avoid zero probabilities the following smoothing\nis performed. In a certain set of examples, the prob-\nability of a tag ti is estimated by\n3In real cases the branching factor is much lower since\nnot all tags appear always in all positions of the context.\nˆp(ti) = |ti|+ 1\nm\nn+1 ,\nwhere m is the number of possible tags and n the\nnumber of examples.\nAdditionally, all the subsets that don’t imply a\nreduction in the classiﬁcation error are joined to-\ngether in order to have a bigger set of examples to\nbe treated in the following step of the tree construc-\ntion. The classiﬁcation error of a certain node is\nsimply: 1 − max1≤ i≤ m (ˆp(ti)) .\nExperiments reported in (M` arquez\nand Rodr´ ıguez, 1995) show that in this way more\ncompact and predictive trees are obtained.\nPruning the Tree\nDecision trees that correctly classify all examples of\nthe training set are not always the most predictive\nones. This is due to the phenomenon known as over-\nﬁtting. It occurs when the training set has a certain\namount of misclassiﬁed examples, which is obviously\nthe case of our training corpus (see section 5). If we\nforce the learning algorithm to completely classify\nthe examples then the resulting trees would ﬁt also\nthe noisy examples.\nThe usual solutions to this problem are: 1) Prune\nthe tree, either during the construction process\n(Quinlan, 1993) or afterwards (Mingers, 1989); 2)\nSmooth the conditional probability distributions us-\ning fresh corpus 4 (Magerman, 1996).\nSince another important requirement of our prob-\nlem is to have small trees we have implemented\na post-pruning technique. In a ﬁrst step the\ntree is completely expanded and afterwards it is\npruned following a minimal cost–complexity crite-\nrion (Breiman et al., 1984). Roughly speaking this\nis a process that iteratively cut those subtrees pro-\nducing only marginal beneﬁts in accuracy, obtaining\nsmaller trees at each step. The trees of this sequence\nare tested using a, comparatively small, fresh part of\nthe training set in order to decide which is the one\nwith the highest degree of accuracy on new exam-\nples. Experimental tests (M` arquez and Rodr´ ıguez,\n1995) have shown that the pruning process reduces\ntree sizes at about 50% and improves their accuracy\nin a 2–5%.\nAn Example\nFinally, we present a real example of the simple ac-\nquired contextual constraints for the conﬂict IN–RB\n(preposition-adverb).\n4Of course, this can be done only in the case of sta-\ntistical decision trees.\nConditional\nprobability\ndistribution\nPrior probability\ndistribution\n. . .\n\"as\" \"As\"others\n. . .\nothers RB\n1st right tag\nP(IN)=0.013\nIN others\n. . .\n2nd right tag\nword form\nP(RB)=0.987\nP(RB)=0.19\nP(IN)=0.81\nFigure 2: Example of a decision tree branch.\nThe tree branch in ﬁgure 2 is translated into the\nfollowing constraints:\n-5.81 <[\"as\" \"As\"],IN> ([RB]) ([IN]);\n2.366 <[\"as\" \"As\"],RB> ([RB]) ([IN]);\nwhich express the compatibility (either positive or\nnegative) of the word–tag pair in angle brackets with\nthe given context. The compatibility value for each\nconstraint is the mutual information between the tag\nand the context (Cover and Thomas, 1991). It is\ndirectly computed from the probabilities in the tree.\n4 Tagging Algorithm\nUsual tagging algorithms are either n–gram oriented\n–such as Viterbi algorithm (Viterbi, 1967)– or ad–\nhoc for every case when they must deal with more\ncomplex information.\nWe use relaxation labelling as a tagging algorithm.\nRelaxation labelling is a generic name for a family\nof iterative algorithms which perform function opti-\nmization, based on local information. See (Torras,\n1989) for a summary. Its most remarkable feature is\nthat it can deal with any kind of constraints, thus the\nmodel can be improved by adding any constraints\navailable and it makes the tagging algorithm inde-\npendent of the complexity of the model.\nThe algorithm has been applied to part–of–speech\ntagging (Padr´ o, 1996), and to shallow parsing\n(Voutilainen and Padr´ o, 1997).\nThe algorithm is described as follows:\nLet V = {v1, v 2, . . . , v n} be a set of variables\n(words).\nLet ti = {ti\n1, t i\n2, . . . , t i\nmi } be the set of possible\nlabels (POS tags) for variable vi.\nLet CS be a set of constraints between the labels\nof the variables. Each constraint C ∈ CS states a\n“compatibility value” Cr for a combination of pairs\nvariable–label. Any number of variables may be in-\nvolved in a constraint.\nThe aim of the algorithm is to ﬁnd a weighted\nlabelling5 such that “global consistency” is maxi-\nmized. Maximizing “global consistency” is deﬁned\nas maximizing for all vi, ∑\nj pi\nj × Sij , where pi\nj is\nthe weight for label j in variable vi and Sij the sup-\nport received by the same combination. The support\nfor the pair variable–label expresses how compatible\nthat pair is with the labels of neighbouring variables,\naccording to the constraint set. It is a vector opti-\nmization and doesn’t maximize only the sum of the\nsupports of all variables. It ﬁnds a weighted labelling\nsuch that any other choice wouldn’t increase the sup-\nport for any variable.\nThe support is deﬁned as the sum of the inﬂuence\nof every constraint on a label.\nSij =\n∑\nr∈ Rij\nInf (r)\nwhere:\nRij is the set of constraints on label j for variable\ni, i.e. the constraints formed by any combination of\nvariable–label pairs that includes the pair ( vi, t i\nj ).\nInf (r) = Cr × pr1\nk1 (m) × . . . × prd\nkd\n(m), is the prod-\nuct of the current weights 6 for the labels appearing\nin the constraint except ( vi, t i\nj ) (representing how\napplicable the constraint is in the current context)\nmultiplied by Cr which is the constraint compatibil-\nity value (stating how compatible the pair is with the\ncontext).\nBrieﬂy, what the algorithm does is:\n1. Start with a random weight assignment 7.\n2. Compute the support value for each label of\neach variable.\n3. Increase the weights of the labels more compat-\nible with the context (support greater than 0)\nand decrease those of the less compatible labels\n(support less than 0) 8, using the updating func-\ntion:\npi\nj (m + 1) = pi\nj (m) × (1 + Sij)\nki∑\nk=1\npi\nk(m) × (1 + Sik)\nwhere − 1 ≤ Sij ≤ +1\n5A weighted labelling is a weight assignment for each\nlabel of each variable such that the weights for the labels\nof the same variable add up to one.\n6pr\nk(m) is the weight assigned to labelk for variable\nr at time m.\n7We use lexical probabilities as a starting point.\n8Negative values for support indicateincompatibility.\n4. If a stopping/convergence criterion 9 is satisﬁed,\nstop, otherwise go to step 2.\nThe cost of the algorithm is proportional to the\nproduct of the number of words by the number of\nconstraints.\n5 Description of the corpus\nWe used the Wall Street Journal corpus to train and\ntest the system. We divided it in three parts: 1 , 100\nKw were used as a training set, 20 Kw as a model–\ntuning set, and 50 Kw as a test set.\nThe tag set size is 45 tags. 36 . 4% of the words in\nthe corpus are ambiguous, and the ambiguity ratio\nis 2 . 44 tags/word over the ambiguous words, 1 . 52\noverall.\nWe used a lexicon derived from training corpora,\nthat contains all possible tags for a word, as well\nas their lexical probabilities. For the words in test\ncorpora not appearing in the train set, we stored\nall possible tags, but no lexical probability (i.e. we\nassume uniform distribution) 10.\nThe noise in the lexicon was ﬁltered by manually\nchecking the lexicon entries for the most frequent 200\nwords in the corpus 11 to eliminate the tags due to\nerrors in the training set. For instance the original\nlexicon entry (numbers indicate frequencies in the\ntraining corpus) for the very common word the was\nthe CD 1 DT 47715 JJ 7 NN 1 NNP 6 VBP 1\nsince it appears in the corpus with the six diﬀer-\nent tags: CD (cardinal), DT (determiner), JJ (ad-\njective), NN (noun), NNP (proper noun) and VBP\n(verb-personal form). It is obvious that the only\ncorrect reading for the is determiner.\nThe training set was used to estimate bi/trigram\nstatistics and to perform the constraint learning.\nThe model–tuning set was used to tune the algo-\nrithm parameterizations, and to write the linguistic\npart of the model.\nThe resulting models were tested in the fresh test\nset.\n6 Experiments and results\nThe whole WSJ corpus contains 241 diﬀerent classes\nof ambiguity. The 40 most representative classes 12\n9We use the criterion of stopping when there are no\nmore changes, although more sophisticated heuristic pro-\ncedures are also used to stop relaxation processes (Ek-\nlundh and Rosenfeld, 1978; Richards et al. , 1981).\n10That is, we assumed a morphological analyzer that\nprovides all possible tags for unknown words.\n11The 200 most frequent words in the corpus cover\nover half of it.\n12In terms of number of examples.\nwere selected for acquiring the corresponding deci-\nsion trees. That produced 40 trees totaling up to\n2995 leaf nodes, and covering 83.95% of the ambigu-\nous words. Given that each tree branch produces as\nmany constraints as tags its leaf involves, these trees\nwere translated into 8473 context constraints.\nWe also extracted the 1404 bigram restrictions\nand the 17387 trigram restrictions appearing in the\ntraining corpus.\nFinally, the model–tuning set was tagged using\na bigram model. The most common errors com-\nmited by the bigram tagger were selected for manu-\nally writing the sample linguistic part of the model,\nconsisting of a set of 20 hand-written constraints.\nFrom now on C will stands for the set of acquired\ncontext constraints, B for the bigram model, T for\nthe trigram model, and H for the hand-written con-\nstraints. Any combination of these letters will indi-\ncate the joining of the corresponding models ( BT,\nBC, BTC, etc.).\nIn addition, ML indicates a baseline model con-\ntaining no constraints (this will result in a most-\nlikely tagger) and HMM stands for a hidden\nMarkov model bigram tagger (Elworthy, 1992).\nWe tested the tagger on the 50 Kw test set using\nall the combinations of the language models. Results\nare reported below.\nThe eﬀect of the acquired rules on the number of\nerrors for some of the most common cases is shown\nin table 1. XX/YY stands for an error consisting\nof a word tagged YY when it should have been XX.\nTable 2 contains the meaning of all the involved tags.\nNN Noun\nJJ Adjective\nVBD Verb – past tense\nVBN Verb – past participle\nRB Adverb\nIN Preposition\nVB Verb – base form\nVBP Verb – personal form\nNNP Proper noun\nNNPS Plural proper noun\nTable 2: Tag meanings\nFigures in table 1 show that in all cases the learned\nconstraints led to an improvement.\nIt is remarkable that when using C alone, the\nnumber of errors is lower than with any bigram\nand/or trigram model, that is, the acquired model\nperforms better than the others estimated from the\nsame training corpus.\nWe also ﬁnd that the cooperation of a bigram or\ntrigram model with the acquired one, produces even\nbetter results. This is not true in the cooperation\nof bigrams and trigrams with acquired constraints\n(BTC), in this case the synergy is not enough to get\na better joint result. This might be due to the fact\nthat the noise in B and T adds up and overwhelms\nthe context constraints.\nThe results obtained by the baseline taggers can\nbe found in table 3 and the results obtained using all\nthe learned constraints together with the bi/trigram\nmodels in table 4.\nambiguous overall\nML 85. 31% 94. 66%\nHMM 91. 75% 97. 00%\nTable 3: Results of the baseline taggers\nambiguous overall\nB 91. 35% 96. 86%\nT 91. 82% 97. 03%\nBT 91. 92% 97. 06%\nC 91. 96% 97. 08%\nBC 92. 72% 97. 36%\nTC 92. 82% 97. 39%\nBTC 92. 55% 97. 29%\nTable 4: Results of our tagger using every combination\nof constraint kinds\nOn the one hand, the results in tables 3 and 4\nshow that our tagger performs slightly worse than a\nHMM tagger in the same conditions 13, that is, when\nusing only bigram information.\nOn the other hand, those results also show that\nsince our tagger is more ﬂexible than a HMM, it can\neasily accept more complex information to improve\nits results up to 97 . 39% without modifying the algo-\nrithm.\nTable 5 shows the results adding the hand written\nconstraints. The hand written set is very small and\nonly covers a few common error cases. That pro-\nduces poor results when using them alone (H), but\nthey are good enough to raise the results given by\nthe automatically acquired models up to 97 . 45%.\nAlthough the improvement obtained might seem\nsmall, it must be taken into account that we are\n13Hand analysis of the errors commited by the algo-\nrithm suggest that the worse results may be due to noise\nin the training and test corpora, i.e., relaxation algo-\nrithm seems to be more noise–sensitive than a Markov\nmodel. Further research is required on this point.\nML C B BC T TC BT BTC\nJJ/NN+NN/JJ 73+137 70+94 73+112 69+102 57+103 61+95 67+101 62+93\nVBD/VBN+VBN/VBD 176+190 71+66 88+69 63+56 56+57 55+57 65+60 59+61\nIN/RB+RB/IN 31+132 40+69 66+107 43+17 77+68 47+67 65+98 46+83\nVB/VBP+VBP/VB 128+147 30+26 49+43 32+27 31+32 32+18 28+32 28+32\nNN/NNP+NNP/NN 70+11 44+12 72+17 45+16 69+27 50+18 71+20 62+15\nNNP/NNPS+NNPS/NNP 45+14 37+19 45+13 46+15 54+12 51+12 53+14 51+14\n“that” 187 53 66 45 60 40 57 45\nTotal 1341 631 820 630 703 603 731 651\nTable 1: Number of some common errors commited by each model\nambiguous overall\nH 86. 41% 95. 06%\nBH 91. 88% 97. 05%\nTH 92. 04% 97. 11%\nBTH 92. 32% 97. 21%\nCH 91. 97% 97. 08%\nBCH 92. 76% 97. 37%\nTCH 92. 98% 97. 45%\nBTCH 92. 71% 97. 35%\nTable 5: Results of our tagger using every combination\nof constraint kinds and hand written constraints\nmoving very close to the best achievable result with\nthese techniques.\nFirst, some ambiguities can only be solved with\nsemantic information, such as the Noun–Adjective\nambiguity for word principal in the phrase the prin-\ncipal oﬃce . It could be an adjective, meaning the\nmain oﬃce , or a noun, meaning the school head of-\nﬁce.\nSecond, the WSJ corpus contains noise (mistagged\nwords) that aﬀects both the training and the test\nsets. The noise in the training set produces noisy\n–and so less precise– models. In the test set, it pro-\nduces a wrong estimation of accuracy, since correct\nanswers are computed as wrong and vice-versa.\nFor instance, verb participle forms are sometimes\ntagged as such ( VBN) and also as adjectives ( JJ) in\nother sentences with no structural diﬀerences:\n• ... failing\nVBG to TO voluntarily RB\nsubmit VB the DT requested VBN\ninformation NN ...\n• ... a DT large JJ sample NN of IN\nmarried JJ women NNS with IN at IN\nleast JJS one CD child NN ...\nAnother structure not coherently tagged are noun\nchains when the nouns are ambiguous and can be\nalso adjectives:\n• ... Mr.\nNNP Hahn NNP , , the DT\n62-year-old JJ chairman NN and CC\nchief NN executive JJ oﬃcer NN of IN\nGeorgia-Pacific NNP Corp. NNP ...\n• ... Burger NNP King NNP\n’s POS chief JJ executive NN oﬃcer NN , ,\nBarry NNP Gibbons NNP , , stars VBZ\nin IN ads NNS saying VBG ...\n• ... and CC Barrett NNP B. NNP\nWeekes NNP , , chairman NN , ,\npresident NN and CC chief JJ executive JJ\noﬃcer NN . .\n• ... the DT company NN includes VBZ\nNeil NNP Davenport NNP , , 47 CD , ,\npresident NN and CC chief NN executive NN\noﬃcer NN ; :\nAll this makes that the performance cannot reach\n100%, and that an accurate analysis of the noise in\nWSJ corpus should be performed to estimate the\nactual upper bound that a tagger can achieve on\nthese data. This issue will be addressed in further\nwork.\n7 Conclusions\nWe have presented an automatic constraint learning\nalgorithm based on statistical decision trees.\nWe have used the acquired constraints in a part–\nof–speech tagger that allows combining any kind of\nconstraints in the language model.\nThe results obtained show a clear improvement in\nthe performance when the automatically acquired\nconstraints are added to the model. That indicates\nthat relaxation labelling is a ﬂexible algorithm able\nto combine properly diﬀerent information kinds, and\nthat the constraints acquired by the learning algo-\nrithm capture relevant context information that was\nnot included in the n–gram models.\nIt is diﬃcult to compare the results to other works,\nsince the accuracy varies greatly depending on the\ncorpus, the tag set, and the lexicon or morphological\nanalyzer used. The more similar conditions reported\nin previous work are those experiments performed\non the WSJ corpus: (Brill, 1992) reports 3-4% er-\nror rate, and (Daelemans et al., 1996) report 96 . 7%\naccuracy. We obtained a 97 . 39% accuracy with tri-\ngrams plus automatically acquired constraints, and\n97. 45% when hand written constraints were added.\n8 Further Work\nFurther work is still to be done in the following di-\nrections:\n• Perform a thorough analysis of the noise in\nthe WSJ corpus to determine a realistic upper\nbound for the performance that can be expected\nfrom a POS tagger.\nOn the constraint learning algorithm:\n• Consider more complex context features, such\nas non–limited distance or barrier rules in the\nstyle of (Samuelsson et al., 1996).\n• Take into account morphological, semantic and\nother kinds of information.\n• Perform a global smoothing to deal with low–\nfrequency ambiguity classes.\nOn the tagging algorithms\n• Study the convergence properties of the algo-\nrithm to decide whether the lower results at\nconvergence are produced by the noise in the\ncorpus.\n• Use back-oﬀ techniques to minimize inter-\nferences between statistical and learned con-\nstraints.\n• Use the algorithm to perform simultaneously\nPOS tagging and word sense disambiguation,\nto take advantage of cross inﬂuences between\nboth kinds of information.\nReferences\n[Aha et al.1991] D.W. Aha, D. Kibler and M. Albert.\n1991 Instance–based learning algorithms. In Ma-\nchine Learning. 7:37-66. Belmont, California.\n[Breiman et al.1984] L. Breiman, J.H. Friedman,\nR.A. Olshen and C.J. Stone. 1984 Classiﬁca-\ntion and Regression Trees. The Wadsworth Statis-\ntics/Probability Series. Wadsworth International\nGroup, Belmont, California.\n[Brill1992] E. Brill. 1992 A Simple Rule-Based Part-\nof-Speech. In Proceedings of the Third Conference\non Applied Natural Language Processing. ACL.\n[Brill1995] E. Brill. 1995 Unsupervised Learning\nof Disambiguation Rules for Part–of–speech Tag-\nging. In Proceedings of 3rd Workshop on Very\nLarge Corpora. Massachusetts.\n[Cover and Thomas1991] T.M. Cover and\nJ.A. Thomas (Editors) 1991 Elements of infor-\nmation theory. John Wiley & Sons.\n[Cutting et al.1992] D. Cutting, J. Kupiec, J. Ped-\nerson and P. Sibun. 1992 A Practical Part–of–\nSpeech Tagger. In Proceedings of the Third Con-\nference on Applied Natural Language Processing. ,\nACL.\n[Eklundh and Rosenfeld1978] J. Ek-\nlundh and A. Rosenfeld. 1978 Convergence Prop-\nerties of Relaxation Labelling. Technical Report\nno. 701. Computer Science Center. University of\nMaryland.\n[Elworthy1992] D. Elworthy. 1993 Part–of–Speech\nand Phrasal Tagging. Technical report, SPRIT\nBRA–7315 Acquilex II, Working Paper WP #10.\n[Daelemans et al.1996] W. Daelemans, J. Zavrel,\nP. Berck and S. Gillis. 1996 MTB: A Memory–\nBased Part–of–Speech Tagger Generator. In Pro-\nceedings of 4th Workshop on Very Large Corpora.\nCopenhagen, Denmark.\n[Garside et al.1987] R. Garside, G. Leech and\nG. Sampson (Editors) 1987 The Computational\nAnalysis of English. London and New York:\nLongman.\n[Hindle1989] D. Hindle. 1989 Acquiring disambigua-\ntion rules from text. In Proceedings ACL’89.\n[Karlsson1990] F. Karlsson 1990 Constraint Gram-\nmar as a Framework for Parsing Running Text.\nIn H. Karlgren (ed.), Papers presented to the 13th\nInternational Conference on Computational Lin-\nguistics, Vol. 3 . Helsinki. 168–173.\n[Karlsson et al.1995] F. Karlsson, A. Voutilainen,\nJ. Heikkil¨ a and A. Anttila. (Editors) 1995 Con-\nstraint Grammar: A Language–Independent Sys-\ntem for Parsing Unrestricted Text. Mouton de\nGruyter, Berlin and New York.\n[L´ opez1991] R. L´ opez. 1991 A Distance–Based At-\ntribute Selection Measure for Decision Tree Induc-\ntion. Machine Learning. Kluwer Academic.\n[Losee1994] R.M. Losee. 1994 Learning Syntactic\nRules and Tags with Genetic Algorithms for In-\nformation Retrieval and Filtering: An Empirical\nBasis for Grammatical Rules. Information Pro-\ncessing & Management, May.\n[Magerman1996] M. Magerman. 1996 Learn-\ning Grammatical Structure Using Statistical\nDecision–Trees. In Lecture Notes in Artiﬁcial In-\ntelligence 1147. Grammatical Inference: Learn-\ning Syntax from Sentences. Proceedings ICGI-96.\nSpringer.\n[M` arquez and Rodr´ ıguez1995] L. M` arquez and\nH. Rodr´ ıguez. 1995 Towards Learning a Con-\nstraint Grammar from Annotated Corpora Using\nDecision Trees. ESPRIT BRA–7315 Acquilex II,\nWorking Paper.\n[McCarthy and Lehnert1995] J.F. Mc-\nCarthy and W.G. Lehnert. 1995 Using Decision\nTrees for Coreference Resolution. In Proceedings\nof 14th International Joint Conference on Artiﬁ-\ncial Intelligence (IJCAI’95).\n[Mingers1989] J. Mingers. 1989 An Empirical Com-\nparison of Selection Measures for Decision–Tree\nInduction. In Machine Learning. 3:319–342.\n[Mingers1989] J. Mingers. 1989 An Empirical Com-\nparison of Pruning Methods for Decision–Tree In-\nduction. In Machine Learning. 4:227–243.\n[Padr´ o1996] L. Padr´ o. 1996 POS Tagging Using Re-\nlaxation Labelling. In Proceedings of 16th Inter-\nnational Conference on Computational Linguis-\ntics. Copenhagen, Denmark.\n[Quinlan1986] J.R. Quinlan. 1986 Induction of De-\ncision Trees. In Machine Learning. 1:81–106.\n[Quinlan1993] J.R. Quinlan. 1993 C4.5: Programs\nfor Machine Learning. San Mateo, CA. Morgan\nKaufmann.\n[Richards et al. 1981] J. Richards, D. Landgrebe and\nP. Swain. 1981 On the accuracy of pixel relax-\nation labelling. IEEE Transactions on System,\nMan and Cybernetics. Vol. SMC–11\n[Samuelsson et al.1996] C. Samuelsson,\nP. Tapanainen and A. Voutilainen. 1996 Inducing\nConstraint Grammars. In Proceedings of the 3rd\nInternational Colloquium on Grammatical Infer-\nence.\n[Schmid1994] H. Schmid 1994 Part–of–speech tag-\nging with neural networks. In Proceedings of 15th\nInternational Conference on Computational Lin-\nguistics. Kyoto, Japan.\n[Torras1989] C. Torras. 1989 Relaxation and Neu-\nral Learning: Points of Convergence and Diver-\ngence. Journal of Parallel and Distributed Com-\nputing. 6:217–244\n[Viterbi1967] A.J. Viterbi. 1967 Error bounds for\nconvolutional codes and an asymptotically opti-\nmal decoding algorithm. In IEEE Transactions\non Information Theory. pg 260–269, April.\n[Voutilainen and J¨ arvinen1995] A. Voutilainen and\nT. J¨ arvinen. 1995 Specifying a shallow gram-\nmatical representation for parsing purposes. In\nProceedings of the 7th meeting of the European\nAssociation for Computational Linguistics. 210–\n214.\n[Voutilainen and Padr´ o1997]\nA. Voutilainen and L. Padr´ o. 1997 Developing\na Hybrid NP Parser. In Proceedings of ANLP’97.",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.715776801109314
    },
    {
      "name": "Computer science",
      "score": 0.7147219181060791
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5662049651145935
    }
  ],
  "cited_by": 12
}