{
  "title": "DSViT: Dynamically Scalable Vision Transformer for Remote Sensing Image Segmentation and Classification",
  "url": "https://openalex.org/W4380536973",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2111455706",
      "name": "Fa-lin Wang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2129408248",
      "name": "Jian Ji",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2098029676",
      "name": "Yuan Wang",
      "affiliations": [
        "Xidian University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6849541040",
    "https://openalex.org/W4313117614",
    "https://openalex.org/W3037215700",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6799107526",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W4280539552",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W4285150367",
    "https://openalex.org/W3204522981",
    "https://openalex.org/W4317033419",
    "https://openalex.org/W4313827664",
    "https://openalex.org/W3204595673",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W4205365435",
    "https://openalex.org/W6843353688",
    "https://openalex.org/W4283450732",
    "https://openalex.org/W3161825146",
    "https://openalex.org/W4226359564",
    "https://openalex.org/W3109998321",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6795140394",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W4290832506",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W2614256707",
    "https://openalex.org/W3167017747",
    "https://openalex.org/W2963730812",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2131386954",
    "https://openalex.org/W1980038761",
    "https://openalex.org/W6863944768",
    "https://openalex.org/W4224903840",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2538244214",
    "https://openalex.org/W4200633637",
    "https://openalex.org/W2743142445",
    "https://openalex.org/W1912954554",
    "https://openalex.org/W2765739551",
    "https://openalex.org/W3048631361",
    "https://openalex.org/W4226013274",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4285237417",
    "https://openalex.org/W3217147624",
    "https://openalex.org/W6848964527",
    "https://openalex.org/W4318149001",
    "https://openalex.org/W3103753223",
    "https://openalex.org/W4313525856",
    "https://openalex.org/W3184761517",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3183174367",
    "https://openalex.org/W4312880416",
    "https://openalex.org/W4390629203",
    "https://openalex.org/W3206476077",
    "https://openalex.org/W4319459107",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W4312847199",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3102692100",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1901129140"
  ],
  "abstract": "The relationship between the foreground target and the background of remote sensing image is very complex. The vision task of remote sensing image faces the problems of complex targets and unbalanced categories. These problems make the modeling method have further improvement space. Therefore, this article proposes a dynamically scalable attention model that combines convolutional features and Transformer features. It can dynamically select the model depth according to the size of the input image, which alleviates the problem of insufficient global information extraction of the single convolution model and the computational overhead limitation of the pure Transformer model. We validated the model on two public remote sensing image classifications and two remote sensing image segmentation datasets. The accuracy and mean pixel accuracy (mPA) of the method in this article reached 96.16&#x0025; and 93.44&#x0025;, respectively, on the university of california (UC) Merced classification dataset. Compared with some recent work, the method has a net improvement of 5.0&#x0025; and 4.82&#x0025; over the pyramid vision transformer (PVT) model. On the Potsdam segmentation dataset, the accuracy and F1 of the transformer and CNN hybrid neural network (TCHNN) model are 91.5&#x0025; and 92.86&#x0025;, respectively. The performance of the method has improved 0.64&#x0025; and 1.0&#x0025;, and the other two datasets have also achieved the best results.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023 5441\nDSViT: Dynamically Scalable Vision Transformer\nfor Remote Sensing Image Segmentation\nand Classiﬁcation\nFalin Wang, Jian Ji , and Yuan Wang\nAbstract—The relationship between the foreground target and\nthe background of remote sensing image is very complex. The vision\ntask of remote sensing image faces the problems of complex targets\nand unbalanced categories. These problems make the modeling\nmethod have further improvement space. Therefore, this article\nproposes a dynamically scalable attention model that combines con-\nvolutional features and Transformer features. It can dynamically\nselect the model depth according to the size of the input image,\nwhich alleviates the problem of insufﬁcient global information\nextraction of the single convolution model and the computational\noverhead limitation of the pure Transformer model. We validated\nthe model on two public remote sensing image classiﬁcations and\ntwo remote sensing image segmentation datasets. The accuracy and\nmean pixel accuracy (mPA) of the method in this article reached\n96.16% and 93.44%, respectively, on the university of california\n(UC) Merced classiﬁcation dataset. Compared with some recent\nwork, the method has a net improvement of 5.0% and 4.82% over\nthe pyramid vision transformer (PVT) model. On the Potsdam\nsegmentation dataset, the accuracy and F1 of the transformer\nand CNN hybrid neural network (TCHNN) model are 91.5% and\n92.86%, respectively. The performance of the method has improved\n0.64% and 1.0%, and the other two datasets have also achieved the\nbest results.\nIndex Terms —CNN, classiﬁcation, remote sensing image,\nsemantic segmentation, transformer.\nI. INTRODUCTION\nS\nINCE entering the new century, along with the evolvement\nof computer technology, deep neural network (DNN) has\nushered in a new level of development, and has gradually be-\ncome the mainstream algorithm in machine learning. As one of\nthe core algorithms in the ﬁeld of computer vision, DNN has\nmade remarkable achievements in image classiﬁcation[1], [2],\n[3], image segmentation[4], [5], [6], object detection[7], [8],\n[9], image grid transfer[10], [11], video tracking[12],e t c .I n\naddition, specially designed DNN has also achieved triumph in\nthe ﬁeld of natural language processing (NLP)[13], [14].\nManuscript received 8 February 2023; revised 5 April 2023 and 7 May 2023;\naccepted 8 June 2023. Date of publication 13 June 2023; date of current version\n26 June 2023. This work was supported in part by the National Natural Science\nFoundation of China under Grant 62273268, and in part by the Key Research\nand Development Program of Shannxi Province under Grant 2022GY-059.\n(Corresponding author: Jian Ji.)\nThe authors are with the School of Computer Science and Technology, Xid-\nian University, Xi’an 710071, China (e-mail: 21031110415@stu.xidian.edu.cn;\njji@xidian.edu.cn; 22031212291@stu.xidian.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3285259\nKrizhevsky et al. [1] trained a deep convolutional neural\nnetwork (AlexNet) with a large number of parameters on\nImageNet1K to classify 1.2 million images containing 1000\ncategories, and achieved the lowest error rate of that year.\nRonneberger et al.[4] proposed the famous U_Net framework\nbased on the concept of the fully convolutional network. The\nencoding–decoding structure of this framework can learn images\nin an end-to-end mode, and moreover can accurately and efﬁ-\nciently segment cell images. U_Net has become a mainstream\nbaseline in image segmentation tasks. The you only look once\n(YOLO) [7] series, a one-stage target detection framework based\non deep learning, has become the most popular target detection\nalgorithm at present. It converts the deﬁnition of object detec-\ntion frames into a regression problem, which can quickly and\neffectively detect target objects. Another popular target detection\nframework is the regions with CNN (RCNN)[8] series, which\nuses the CNN to extract features from each proposal region. It\nis a two-stage detection framework, which lays the foundation\nfor the later Fast-RCNN[9] framework. Xu et al.[10] proposed\na generative adversarial network called dynamic resblock gen-\nerative adversarial network (DRB-GAN) to transfer the artistic\nstyle of one image to another, showing excellent performance in\nvisual quality.\nTransformer [15] was ﬁrst applied in the ﬁeld of NLP, and it\novercomes the problems of long short term memory (LSTM)\nand gated recurrent Uunit that cannot be trained in parallel\nand require a large number of memory resources. Due to the\nunique advantages of the Transformer framework, it has not\nonly become the main framework in the ﬁeld of NLP, but also\nhas become more and more widely used in the ﬁeld of computer\nvision (CV) under the promotion of a large number of scholars.\nBased on the Transformer model, bidirectional encoder rep-\nresentation from transformers (BERT)[16] improves the Trans-\nformer model. It pretrains on unlabeled text, and ﬁne-tunes the\noutput layer to apply to other NLP tasks. Inspired by BERT,\nBrown et al.[17] proposed a Transformer model with a very\nlarge number of parameters (175 billion) for general pre-trained\ntransformer-3 (GPT-3). GPT-3 only passes the pretrained model\nwithout training or ﬁne-tuning, which just can show strong\ncapabilities in the midstream and downstream tasks of NLP.\nDosovitskiy et al. [18] ﬁrst divided the image into patches\nof the same size, and then serialize these patches and input\nthem into a pure Transformer model. After pretraining with a\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5442 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nlarge number of datasets, it achieves better results than CNN in\nimage classiﬁcation tasks. Enze et al.[19] proposed a seman-\ntic segmentation framework called SegFormer, which uniﬁes\nTransformer and multilayer perceptron for image segmentation\nand the framework was veriﬁed on multiple datasets, proving its\nefﬁciency and accuracy.\nA remote sensing image contains many targets. The targets of\nthe same kind are densely arranged, the size of the targets of the\nsame kind varies greatly, the color and texture vary greatly, and\nmost of the targets are very small. For the convolutional neural\nnetwork, due to the restriction of the size of the convolution\nkernel, the convolution kernels generally used at the present\nstage are all small kernels. Therefore, the convolutional neural\nnetwork has poor performance in capturing global informa-\ntion. Although the multilayer pooling operation can achieve the\nconsequence of capturing global information, the information\nloss is serious and the information interaction is insufﬁcient in\nthis process. For Transformer, it obtains long-sequence global\ninteraction information by calculating global attention, but this\noperation requires a large amount of calculation, and the amount\nof calculation increases quadratically when faced with mid-\nstream and downstream tasks in image processing. In addition,\nthe pure Transformer is sensitive to parameters and requires\npretraining with massive data.\nWe analyze the above problems, and then we put forward a\ndynamically scalable visual Transformer, which complements\nthe advantages of the CNN and the Transformer model. It can\nalleviate the problem of their separate use. The main contribu-\ntions of this article are as follows.\n1) We propose a dynamic and scalable visual Transformer\nframework that combines the locality of feature extraction\nfrom CNNs with the globality of Transformers to establish\ncontextual connections.\n2) Transformer obtains global information through self-\nattention. We extract the self-attention mechanism and\nintegrate it into the convolution operation, then design\na dynamically scalable attention module (DSA) that can\nmeet the midstream and downstream tasks of image pro-\ncessing, which can process large-scale images.\n3) We evaluate the proposed DSA in different remote sensing\nimage processing tasks, including image classiﬁcation and\nsemantic segmentation. Compared to the state-of-the-art\nmethods or models, our proposed DSA achieves state-of-\nthe-art results in these two tasks.\nII. RELATED WORKS\nA. CNN for Remote Sensing Image\nRemote sensing image analysis is of great signiﬁcance in\nEarth observation, urban planning, and environmental protec-\ntion, but the manual handling procedure of these remote sensing\nimages is cumbersome and complicated. The CNN can effec-\ntively extract the local information of the image by using the\ntranslation invariance of the convolution kernel and the pooling\nkernel, and in the meantime, it can preserve the spatial semantic\nfeatures of the image. As a consequence, using CNN to process\nremote sensing images can effectively help staff analyze remote\nsensing images.\nPenatti et al.[20] applied convolutional neural networks to\naerial and remote sensing image classiﬁcation tasks in addi-\ntion to obtaining the best classiﬁcation results at that time.\nMultimodal or multisource remote sensing images can more\ncomprehensively analyze the feature information of ground\nobjects. Danfeng et al.[21] designed a general multimodal deep\nlearning framework. Through different fusion strategies, image\ndata of multiple modalities were input into the framework, which\neffectively alleviated the bottleneck of inaccurate ﬁne classiﬁ-\ncation of remote sensing images in complex scenes. Xiaodong\net al.[22] designed a dual-branch CNN architecture, which can\nfuse the features of hyperspectral images and light detection and\nranging (LiDAR) data, and has shown excellent performance on\nmultisource datasets. Zhiyong et al.[23] designed a new deep\nlearning architecture that combines a spatial spectral attention\nmechanism and a multiscale dilated convolution module, which\ncan capture more detailed features in remote sensing image\nchange detection, and the detection accuracy has been obviously\npromoted. Maggiori et al. [24] implemented the ﬁne-grained\nclassiﬁcation task of remote sensing images by using the con-\nvolutional neural network and a multiscale neuron module,\nand effectively alleviated the tradeoff between recognition and\naccurate positioning. Sharma et al.[25] analyzed the spatial rela-\ntionship between a single pixel of a remote sensing image and its\nneighborhood, and proposed a depth learning framework based\non a spatial neighborhood patch, which can effectively classify\nthe remote sensing image with medium resolution. Starting from\nthe mathematical analysis of parameter optimization, Yang et\nal. [26] designed a network called HPS_Net, which can adjust\nthe relationship between feature maps and pixel path selection,\nand can effectively segment ground objects. Due to the complex\nfeatures of remote sensing images, a relatively large model is\nrequired to capture the features, so designing a small model and\nachieving good results is also a research direction.\nB. Transformer for Remote Sensing Image\nTransformer achieves the capability to model long-sequence\nglobal context information interaction through the self-attention\nmechanism. In the processing of remote sensing images, the\nacquisition of global information is crucial. However, the com-\nputational overhead of self-attention scales quadratically with\nthe sequence length. For large-scale data like images, the cost of\ncalculating self-attention is expensive, so it is very meaningful\nto design a reasonable method to reduce the image scale.\nLimiting the inﬂuent scope of self-attention can effectively\nreduce computational complexity. Swin Transformer[27] limits\nthe size of the moving window (16× 16) domains, allowing\nself-attention to be calculated in nonoverlapping shift windows,\ngreatly reducing the cost of calculation, and meanwhile, global\ninformation about the image is obtained. Lei et al.[28] proposed\nthe wide-context network, which can obtain detailed features\nof high-resolution remote sensing images by fusing CNN and\ncontext branches to accomplish the target of accurately classify-\ning diverse ground objects. Yongtao et al.[29] designed a novel\nWANG et al.: DSViT: DYNAMICALLY SCALABLE VISION TRANSFORMER FOR REMOTE SENSING IMAGE SEGMENTATION 5443\nFig. 1. Dynamically scalable attention block.\ncross-context, cross-scale architecture (C2-CapsViT), which can\neffectively fuse global and local semantic features to accomplish\nstate-of-the-art performance in remote sensing image scene\nclassiﬁcation. DynamicViT [30] learns a dynamic sequence\nsparsiﬁcation strategy to reduce the computational complexity\nwhile maintaining the global interaction. Zhengzhong et al.[31]\ndesigned a scalable attention model, which can realize the\ninteraction of global information and local information about\nthe image on the basis of linear complexity and at the same\ntime, it can integrate with convolution, which performs the\ntask downstream of the CV out of a good performance. The\ncombination of Transformer and generative adversarial network\n(GAN) [32] has also accomplished excellent achievements in\nthe ﬁeld of vision.\nConvolutional neural networks use translation-invariant op-\nerators to process images, and Transformer’s powerful global\nmodeling capabilities make it a triumph in the ﬁeld of NLP and\nthen shine in the ﬁeld of computer vision. However, a bottleneck\nof convolutional neural network is that the limited kernel size\nleads to insufﬁcient extraction of global feature information,\nand the Transformer cannot handle large images due to the\nlimited computational overhead and input length. The fusion of\nthese two models can achieve complementary advantages. The\nabove article also proves this conclusion, and simultaneously\nalso provides a theoretical basis and foundation for the method\nof this article.\nIII. METHODS\nA. Dynamically Scalable Attention Block\nThe experimental results show that the existing deep learning\nalgorithm is insufﬁcient for the extraction of complex feature\nfeatures, and there is still room for improvement. Therefore,\nthe convolutional network and Transformer are effectively in-\ntegrated in this article, and the designed network model can\nbetter extract complex feature features. In addition, the defect\nof Transformer is that it requires very large computational power\nto process large images. Generally, small images are used, which\nmakes the original image have to be processed into smaller\nimage blocks, which is a loss of global information. Therefore,\nfor large images, the feature extraction ability of ﬁxed-depth\nmodels is insufﬁcient, such as ResNet50[2] and ResNet110.\nTheir difference in feature extraction ability of large images is\nobvious.\nThe convolutional neural network can efﬁciently extract the\nlocal information of the input features through the ﬁlter, and in\naddition, the Transformer can model the global context informa-\ntion of the input sequence through the self-attention mechanism.\nTherefore, we propose a dynamically scalable feature extraction\nmodule dynamically scalable attention block (DSA_Block) that\nintegrates convolution operations and self-attention mechanism,\nas shown in Fig.1. The advantages of this module are as follows:\nConvolution operations are applied to extract local contextual in-\nformation of input features. Since the self-attention mechanism\nis very computationally intensive, the size of the input features\nis reduced through a dynamically scalable pooling kernel, and\nthen the self-attention weight of the input features is calculated.\nThe self-attention weights are fused with the input features, so\nthat the output features not only have the local information of\nthe input features but also contain the global information.\nWe use sequential connections between Transformer and\nCNN models in order to reduce the computational cost through\ndimensionality reduction of applying the Transformer model\nto the entire feature map. The convolution operation can ef-\nfectively extract local contextual features while also reducing\nthe dimensionality of the feature map, thus speeding up the\ntraining process. On the other hand, the weights generated by\nthe Transformer model can highlight the relevant features in\nthe convolutional feature map and establish long-range depen-\ndencies between features, enabling subsequent layers to extract\nmeaningful information. This allows the model to better capture\nthe complex relationships between different parts of the input\ndata.\nFirstly, the input features of DSA_Block arex ∈ RH×W×C\nsubjected to a convolution operation to extract local information.\nThe speciﬁc process is as follows:\noutc =2 ×R (Nor(conv3×3(x))) . (1)\nAfter x undergoes two convolution operations, the output fea-\ntures areoutc ∈ RH×W×N . conv3×3(·) represents a convolution\noperation with a convolution kernel of3 ×3, nor(·) represents\nbatch regularization, andR(·) represents a rectiﬁed linear unit\n(ReLU) function operation.\nAfter the convolution operation, ﬁrst scale down the size of the\nfactor s, which is applied to further decrease the dimension of the\ninput feature and reduce the calculation amount of self-attention.\nThe scale reduction factor is dynamically scalable, and it is\nconditioned by the downsampling scalek of the input image\nencoding process and the numberL of DSA_Blocks used. If\n5444 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nfewer layers of coding modules are used (L is smaller), a\nrelatively large downsampling scale (k is large) is used. On\nthe premise of using more layers of encoding modules (L is\nlarger), a smaller downsampling scale (k is smaller) is used.\nThe depth position of DSA_Block is different, then the value of\ns is different. Assuming that the depth position of DSA_Block\nis i, and the downsampling scale isk, then the scale reduction\nfactor at this time iss = kL−i+1.\nThe value ofk is determined jointly by the size of the input im-\nage, the depth of the DSA module, and the size of the lowest-level\nfeature map. This allows the neural network to more ﬂexibly\nadapt to input images of different sizes, while also controlling\nthe computational cost and model size, thereby improving the\nefﬁciency and generalization ability of the model. We calculate\nthe value of k using the following formula: m ×kL−1 = n,\nwhere n is the size of the input image,L is the depth of the\nDSA module, and m is the size of the lowest-level feature\nmap. In this article, the input image size is 256× 256, the\ndepth of the DSA module is 5, and the size of the lowest-level\nfeature map is16 ×16, so we choosek =2 . Typically, reducing\nthe size of the feature map can reduce the number of pixels\nthat need to be processed. At the same time, downsampling\ncan extract higher-level features by merging information from\nadjacent pixels.\nFor the output featureoutc of the convolution operation, the\nsize ofoutc is reduced according to the scale reduction factor.\nAt this time, the feature sizeoutde ∈ R\nW\ns ×H\ns ×N is reduced by\ns2 times, outde refers to the output obtained by downsampling\nthe outputoutc of a convolution operation, andoutde is applied\nto compute the self-attention weight. The speciﬁc process is as\nfollows:\noutf = flatten(outde). (2)\nflatten(·) is a ﬂattening operation, which ﬂattens outde to\noutf , outf ∈ R\nWH\ns2 ×N . Next, layer normalization is performed\non outf to output outbl, outbl ∈ R\nWH\ns2 ×N . The self-attention\nmechanism realizes the dynamic aggregation of information\nthrough the interaction between queries (Q) and key (K)-value\n(V ) pairs. The speciﬁc process is as follows:\n[Qi,K i,V i]= Linear(outbl). (3)\nLinear(·) is the linear mapping function,Qi,K i,K i ∈ RN×dh.\nOn this basis, the similarityAi between Qi and Ki is calculated,\nand Vi is weighted according toAi. The speciﬁc process is as\nfollows:\nTe m pi = QiKT\ni√dh\n(4)\nAi = eTempi\nj\n∑\nj eTempi\nj\n. (5)\nAmong them,Te m p∈ RN×N is the intermediate value ofQ and\nKT , and perform the softmax operation on Temp to output the\nsimilarity A ∈ RN×N . Further weightV and output the weight\nSA of self-attention. The speciﬁc process is as follows:\nSA\ni = AiVi (6)\nMS A = concat\n(\nS1\nA,S 2\nA,...,S h\nA\n)\n. (7)\nAmong them,Si\nA ∈ RN×dh, MS A ∈ RN×WH\ns2 , h is the number\nof self-attention heads, anddh represents the output dimension\nof each self-attention head,dh = WH\ns2h . Paralleling multiple self-\nattention modules in multihead attention can enrich the diversity\nof attention, thereby increasing the expressive ability of the\nmodel. Next, input the MS A into the multilayer perceptron\nto further extract features and output outm ∈ R\nWH\ns2 ×N , and\nthen renew outm to three dimensions, at this time, outm ∈\nR\nW\ns ×H\ns ×N . Further nearest-neighbor upsamplingoutm, so that\nthe scale ofoutm is renewed to the size of the output feature\noutc of the convolution operation. At this moment, we have\ncompleted the calculation of the self-attention weightoutm ∈\nRH×W×N , and ﬁnally multiplied the self-attention weight and\nthe corresponding position element ofoutc, as shown in the\nfollowing formula:\nout = outm ⊗outc. (8)\nAmong them,⊗represents the multiplication of corresponding\nposition elements,out ∈ RH×W×N .\nB. Classiﬁcation Model\nThe availability of the method in this article is validated in\nthe image classiﬁcation task. The network structure shown in\nFig. 2 is designed for remote sensing image classiﬁcation. In\nthis architecture, we use ﬁve DSA_Blocks, then use two fully\nconnected layers to reduce the dimension to the number of\ncategories, and ﬁnally classify the input image through a softmax\noperation. Since the size of the input image in this article is\n256 ×256, the size of the downsampling pooling kernel is2 ×2,\nand the step size is 2. In Layer1, the input channel is 3, the output\nchannel is 32, and the reduction factors =3 2. In Layer2, the\ninput channel is 32, the output channel is 64, and the reduction\nfactor s =1 6. In Layer3, the input channel is 64, the output\nchannel is 128, and the reduction factors =8 . In Layer4, the\ninput channel is 128, the output channel is 256, and the reduction\nfactor s =4 . In Layer5, the input channel is 256, the output\nchannel is 512, and the reduction factors =2 . We can conclude\nfrom the change of the reduction factor channel that when the\nfeature map is relatively large, the reduction factor is relatively\nlarge.\nThe purpose of this is to reduce the amount of calculation\nof self-attention. At the same time, the receptive ﬁeld is also\ndifferent when the feature map is downsampled by different\nreduction factors, i.e., the regions it represents are different.\nWhen calculating self-attention, the global interactive informa-\ntion contained is different. From the large receptive ﬁeld of the\nﬁrst layer to the small receptive ﬁeld of the last layer, the range\nrepresented by the self-attention weight also changes from large\nto small, from general to reﬁned, so that the operation similar to\nthe pyramid structure can make the extracted global information\nmore comprehensive.\nThe convolution operation at the front end of DSA_Block can\ncapture the local information and details of the input features.\nThrough DSA_Block, local information and global information\nWANG et al.: DSViT: DYNAMICALLY SCALABLE VISION TRANSFORMER FOR REMOTE SENSING IMAGE SEGMENTATION 5445\nFig. 2. Classiﬁcation model. The number below each layer indicates the size of the output feature map of that layer.\nFig. 3. Segmentation model. The number below each layer indicates the size of the output feature map of that layer.\nare effectively fused to accomplish the target of promoting\nclassiﬁcation accuracy. The classiﬁcation experiments below\nalso fully verify the effectiveness of the method in this article.\nC. Segmentation Model\nIn addition, we verify the method in this article on the image\nsegmentation task, and we construct a segmentation network for\nsemantic segmentation of remote sensing images, as shown in\nFig. 3. The framework adopts an end-to-end encoding–decoding\nstructure. The input image size of the segmentation network\nis 512 × 512, and the downsampling pooling kernel size is\n2 × 2 with a stride of 2. In the encoding stage, similar to the\nclassiﬁcation structure above, a ﬁve-layer DSA_Block module\nis used. The output channel and reduction factor of each module\nis the same as the classiﬁcation network, and it also has the\nfunction of fusing global context information and local context\ninformation. A 3×3 convolution operation is added to the lowest\nlayer to further extract features.\nIn the decoding stage, we adopt the visual geometry group 16,\n(VGG16) [33] structure, and the number of3 ×3 convolutions\nused to decode the ﬁrst layer to the ﬁfth layer is two. The speciﬁc\nstructure is shown in TableI.“ ×2” indicates that there are two\nconvolution blocks of this type, and “k= 2” indicates that the\nconvolution kernel used in the upsampling process has a size of\n2. And we replace the maximum pooling downsampling layer\nin VGG16 with a deconvolution upsampling layer.\nAt the same time, the skip connection method is used to\nadd the shallow information captured by DSA_Block and the\nTABLE I\nVGG16 NETWORK ARCHITECTURE W ASUSED IN THEENCODING STAGE\ndeep information of the corresponding decoding layer to further\ndeepen the feature fusion, which alleviates the problem of infor-\nmation loss during the downsampling process of the number of\nlayers and also prevents gradient loss. Finally, the corresponding\nsemantic segmentation map is the output. The remote sensing\nimage segmentation results below strongly prove that the method\nin this article is very effective in the fusion of global information\nand local information. While obtaining global information, it\n5446 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 4. DSA_Block feature heat map. (a) input. (b) layer1. (c) layer2. (d) layer3. (e) layer4. (f) layer5.\nTABLE II\nDATASETSINFORMATION\ncan also extract local details such as object boundaries. This\nfurther conﬁrms that the method of fusion of self-attention\nmechanism in CNN and Transformer is reasonable, and can\nintegrate the superiorities of CNN and Transformer to obtain\na basic module that is superior to pure convolution operation or\npure Transformer structure.\nIV . EXPERIMENT AND ANALYSIS\nA. Datasets\nIn order to conﬁrm the availability of the method in this\narticle, we performed ablation experiments on two classiﬁcation\ntask datasets (UC Merced[34] and WHU-RS19[35]) and two\nsegmentation task datasets (Potsdam[36] and LoveDA [37]).\nThe details of each dataset are shown in TableII. The original\nimage size of the Potsdam dataset is 6000× 6000. The image\nis too large to be input into the model, so the original image is\ncropped. The table shows the cropped size. The image sizes of\nother datasets are ofﬁcial standard data.\nB. Setting and Evaluation Metrics\nIn the classiﬁcation task, the size of the input image is 256,\nthe batch size is 10, the initial learning rate is 0.001, the learning\nrate adopts the attenuation strategy with epoch, the Epoch is\n400, the stochastic gradient descent (SGD) optimizer is used,\nthe momentum is 0.9, and the weight decay is5 ×10−5.T h e\nevaluation indicators useaccuracy and mP A.\nIn the segmentation task, the size of the input image is\n512 × 512, the batch size is 4, the initial learning rate is 0.01,\nthe learning rate adopts the attenuation strategy with epoch, the\nEpoch is 400, the SGD optimizer is used, the momentum is 0.9,\nand the weight decay is10−4. The evaluation indicators use the\naccuracy, F-measure (F1) score,Kappa coefﬁcient, and mean\nintersection over union (IOU) (mIoU ).\nC. Experimental Results\nThe experimental environment is the Pytorch framework con-\nﬁgured on Ubuntu64 operating system conﬁgured with Intel(R)\nCore(TM) i7-7700 K CPU @ 4.20 GHz and GeForce GTX 1080\nTi GPU. For both classiﬁcation and segmentation tasks, we apply\nthe most basic cross entropy loss function.\n1) Image Classiﬁcation:\na) Effectiveness of DSViT module on the UC Merced\ndataset: In order to conﬁrm the feasibility of DSA_Block, we vi-\nsualize the internal convolution output features and self-attention\nweights of each DSA_Block in the classiﬁcation network, as\nshown in Fig. 4. A total of ﬁve layers of DSA_Block are\nused in the classiﬁcation network in this article, and the ﬁrst\nchannel of the feature map of each layer is used for visualization.\nDue to the process of downsampling, the deeper the layer,\nthe smaller the feature map. So as to unify the visualization\neffect, we resize each feature map to256 ×256. The ﬁrst line\nis the feature map after the two-layer convolution operation in\nDSA_Block, corresponding tooutc in Fig. 1. The second line\nis the visualization of the self-attention weight in DSA_Block,\ncorresponding tooutm in Fig.1. The third line is the output of\nWANG et al.: DSViT: DYNAMICALLY SCALABLE VISION TRANSFORMER FOR REMOTE SENSING IMAGE SEGMENTATION 5447\nTABLE III\nCOMPARISON OFTHIS METHOD WITH OTHER ADV ANCEDMETHODS ON THE\nUC MERCED AND WHU-RS19\nDSA_Block, corresponding to the output in Fig.1. The image in\nFig. 4 is a baseball diamond. The main features of identifying a\nbaseball ﬁeld are the fan-shaped sand and the center of circular\nsand. In the visualization below, the output of the convolution\noperation mainly acts on boundary information, such as shape\nand outline, while the self-attention weight focuses on capturing\nthe fan-shaped area and the circular area and fusing the two\nfeatures. The result shows that the feature at this time has both\nboundary information and area information.\nWe perform classiﬁcation experiments on the UC Merced\ndataset. A comparative experiment is done with some classic\nmethods [1], [2], [33], [38], [39] and some recent methods based\non CNN and Transformer[18], [27], [42], [43], and the experi-\nmental results are presented in TableIII. We make a comparison\nin the following three cases: 1) Compared with the traditional\npure convolutional network, the method in this article has a net\nincrease of 15% and 17.93% in accuracy and mPA, respectively\nthan the classic AlexNet[1]. Compared with the deep convo-\nlutional network DenseNet [40], the net increase in accuracy\nand mPA is 3.09% and 6.17%, respectively. Compared with\nthe attention-based convolutional network EfﬁcientNet [39],\nthe net increase in accuracy and mPA is 11.91% and 13.57%,\nrespectively; 2) Compared with the pure Transformer-based\nnetwork, the pioneering method ViT[18] has an accuracy rate\nand mPA of 87.89% and 83.63%, respectively, and the method\nin this article improves these two indicators by 8.3% and 9.81%,\nrespectively, compared with ViT. Compared with PVT[42],i t\nimproves the performance by 5% and 4.82%, respectively and\n3) Compared with the network framework integrating CNN and\nTransformer, the method in this article improves the accuracy\nand mPA by 5.24% and 5.49%, respectively, compared with\nSwin [27]. Compared with the latest method ConvNeXt[43],t h e\ncapability of the method in this article is improved by 12.57%\nand 13.85%. Through the above data comparison, the method in\nthis article can feasibly complement the superiorities of CNN\nand Transformer to accomplish better performance than the\npure CNN model and pure Transformer architecture. At the\nsame time, compared with some models that fuse the two, the\ndynamically scalable attention module proposed in this article\ncan better fuse local information and global information.\nb) Acc and confusion matrix visualization on UC the\nMerced dataset: We conﬁrm the capability of the network after\neach training epoch. Fig. 5(a) shows the accuracy of some\nmethods in the veriﬁcation phase. The total amount of training\ntimes is 400. We draw a graph of the test accuracy after every ten\niterations. We can obverse from the ﬁgure that the convergence\nspeed and trend of these architectures are almost the same,\nwhich presents that the method in this article alleviates the\ninstability of Transformer, and our method starts to achieve\nthe best performance after 100 iterations, especially after when\nthe network tends to converge. The method in this article still\nhas better functions than these pure CNN, pure Transformer,\nfusion CNN, and Transformer architectures. In addition, we\nvisualize the test results of the best model of the method in\nthis article through the heat map of the confusion matrix, as\nshown in Fig.5(b). The method in this article has accomplished\n100% accuracy in 13 categories, only the accuracy rate in the\ntwo categories is lower than 90%, and the overall accuracy rate\nreaches 96%. It can further account for the effectiveness of the\nmethod in this article.\nc) Effectiveness of DSViT module on WHU-RS19 dataset:\nSo as to prove the generalization of the method in this article,\nwe further conduct comparative experiments on the WHU-RS19\ndataset, as shown in Table III. Similar to the experimental\nresults of the UC Merced dataset, on the WHU-RS19 dataset,\nwe achieve classiﬁcation accuracy of 93.88% and mPA of\n90.63%. The method in this article has a greater promotion in\nperformance than the pure CNN model, the pure Transformer\narchitecture, and the combination of CNN and Transformer\narchitectures, which also further illustrates the effectiveness\nof the dynamically scalable attention module proposed in this\narticle, and it can also be better generalized to other datasets,\nwith high generalization.\nd) Acc and confusion matrix visualization on WHU-RS19\ndataset: Similar to the UC Merced dataset, we also visualize\nthe test acc curve and the confusion matrix heatmap on the\nWHU-RS19 dataset, as shown in Fig.6.F i g .6(a) presents that\nour method maintains the best performance compared to other\nmethods after 50 iterations until the end of training. In Fig.6(b),\nthrough the calculation of the confusion matrix, the classiﬁcation\naccuracy of this method is above 93%, but there is an error in\nthe classiﬁcation of the River category, this River category is\nclassiﬁed into the Forest and Park categories, so the accuracy\nrate is only 73%.\ne) Efﬁciency analysis of DSA Block and ViT Block:We\nconducted an analysis of the computational efﬁciency and pa-\nrameter count for both the traditional ViT Block and the DSA\nBlock is proposed in this article. In the ViT model, the input of\nthe attention module is the same, so the ﬂoating point operations\nper second (FLOPs) and parameter count of each ViT block are\nthe same. However, the input of the DSA block proposed in this\narticle changes with the depth of the model, and the input of each\nDSA block on each layer varies. Therefore, we calculated the\n5448 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 5. Visualization of test Acc curve and confusion matrix heat map of UC Merced dataset. (a) Acc curve. (b) Heat map.\nFig. 6. Visualization of test Acc curve and confusion matrix heat map of WHU-RS19 dataset. (a) Acc curve. (b) Heat map.\nTABLE IV\nEFFICIENCY ANALYSIS OFDSA BLOCK AND VITB LOCK\nFLOPs and parameter count for each DSA block on each layer,\nas shown in TableIV. It can be seen that the FLOPs and params\nof each DSA block are lower than those of each ViT block.\nAs the depth of the model increases and the number of feature\nchannels increases, the FLOPs and params of the DSA block\ngradually decrease but remain lower than those of the ViT block.\nThis further demonstrates the effectiveness and superiority of the\nproposed model in this article.\n2) Image Segmentation:\na) Effectiveness of DSViT module on Potsdam dataset:We\nfurther prove the effectiveness of the dynamic variable attention\nmodule proposed in this article on the remote sensing image\nsemantic segmentation task. On the Potsdam dataset, we design\nthe semantic segmentation network as shown in Fig.3 for ﬁtting\nexperiments, in addition to combining the speciﬁc index values\ncalculated and the results are shown in TableV. It can be seen\nthat the method in this article is very good at recognizing the\nthree types of ground features, Building, Car and Impervious\nsurfaces, with an accuracy rate of over 94%. The recognition\naccuracy of Tree and Low vegetation reached more than 86%.\nThe visualization of each category in Fig.7 also proves this rule.\nComparing the segmentation model in this article with other\nclassic and efﬁcient methods, as shown in TableVI, the “-” in the\nWANG et al.: DSViT: DYNAMICALLY SCALABLE VISION TRANSFORMER FOR REMOTE SENSING IMAGE SEGMENTATION 5449\nFig. 7. Partial segmentation results of DSViT on the Potsdam dataset.\nTABLE V\nSEGMENTATIONEFFECTS OF VARIOUS GROUND OBJECTS ON THEPOTSDAM\ntable indicates that this data is missing in this method. Compared\nwith the classic pure CNN architecture DeeplabV3[44],t h e\nmethod in this article improves the accuracy rate and the F1\nby 1.24% and 1.76%, respectively. Compared with the pure\nCNN architecture dense dilated convolutions merging network\n(DDCM_Net) [26], the accuracy rate is increased by 1.34%.\nCompared with the method densely connected swin transformer\n(DC-Swin) [45], which combines CNN and Transformer, the\naccuracy rate is increased by 0.14%, and the F1 is increased\nby 0.61%, but the method in this article is reduced by 0.24%\non mIoU. Compared with FT-UNetFormer [46], this article\nimproves the accuracy and F1 by 0.14% and 0.56%, respectively.\nCompared with the latest convolutional network mutual afﬁne\nnetwork (MANet)[47], our method also has advantages. Com-\npared with the state-of-the-art methods[48], [49], [50], [51],t h e\nTABLE VI\nCOMPARISON BETWEEN THE METHOD IN THIS ARTICL ANDOTHER ADV ANCED\nMETHODS ON THEPOTSDAM\naccuracy and F1 of our method are higher than these several\nmethods. The experimental results show that the method in\nthis article shows better performance in all indicators compared\nwith bi-similarity network (BSNet)[52] and transformers U-Net\n(TransUnet) [53]. Compared with the latest multiscale channel\nattention fusion network (MCAFNet)[54] and block-in-block\n5450 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 8. Partial segmentation results of DSViT on the LoveDA dataset.\nedge detection network (BIBED-Seg) [55] methods, the ac-\ncuracy has increased by 1.94% and 1.34% respectively. It is\n11.32% higher than BIBED-Seg on mIoU. It is 4.06% higher\nthan MCAFNet in F1. Based on the above analysis, the method\nin this article has better feature extraction ability. Through the\ncomparison of the above data, the method of this article also\nshows extraordinary performance in the semantic segmentation\ntask, which also shows that the dynamically scalable attention\nmodule proposed in this article can be applied to multiple tasks,\nand further demonstrates this module has a very strong ability\nin feature extraction and learning.\nWe predict and visualize the test images of the Potsdam\ndataset, as shown in Fig. 7. The method in this article can\neffectively extract detailed information such as boundaries, and\nat the same time can identify and accurately classify classes with\nvery few pixels. This shows that the segmentation framework\nproposed in this article can not only establish remote dependen-\ncies of long sequences (global information interaction) through\ndynamically scalable self-attention, but also achieve effective\ncapture of local information through convolution operations,\nthereby achieving more accurate semantic segmentation.\nb) Effectiveness of DSViT module on LoveDA dataset:In\norder to further prove the effectiveness and generalization of the\nmethod in this article, we veriﬁed it on a new remote sensing\nsemantic segmentation dataset (LoveDA), and calculated the\nspeciﬁc index values of each category. The results are shown\nin TableVII. It can be seen that the method in this article is very\ngood at identifying the three types of ground features: Water,\nTABLE VII\nSEGMENTATIONEFFECTS OF VARIOUS GROUND OBJECTS ON THELOVEDA\nAgricultural, and Building, and the accuracy rate has reached\nmore than 75%. The recognition accuracy of Barren and Road\nis about 70%. The segmentation of Forest is the worst, and the\nsegmentation accuracy is only 56%. The visualization effect of\neach category in Fig.8 also proves this rule.\nWe compare our method with other state-of-the-art methods,\nas shown in TableVIII. The ofﬁcial comparison index of this\ndataset is only mIoU, so there are no other index values in the\ncomparison article, but other related indexes are also calculated\nin this article. The accuracy of this method, F1, Kappa, and mIoU\nare 73.32%, 68.44%, 59.17%, and 52.84%, respectively. Simi-\nlarly, compared with the traditional deep convolutional network\nDeeplabV3 [44], this article improves mIoU by 7.7%. Compared\nwith the ofﬁcial method HRNetw32[37],i ti m p r o v e sb y0 . 9 2 % .\nCompared with DC-Swin[54], our method improves mIoU by\nWANG et al.: DSViT: DYNAMICALLY SCALABLE VISION TRANSFORMER FOR REMOTE SENSING IMAGE SEGMENTATION 5451\nTABLE VIII\nCOMPARISON BETWEEN THE METHOD IN THIS ARTICLE ANDOTHER\nADV ANCEDMETHODS ON THELOVEDA SEGMENTATIONDATASET\n2.24%. Compared with the state-of-the-art method[49], [57],\n[58], it improves by 0.4%, 4.14%, and 7.58%. We compare the\nlatest four methods, and the results show that the method in this\narticle has also achieved the best results, which further shows\nthat the method in this article has great advantages in the remote\nsensing image segmentation task, and can better combine the\nglobal information and local information to achieve the purpose\nof accurate segmentation.\nSimilarly, we visualize the prediction results of LoveDA data,\nas shown in Fig.8. It can be observed that the method in this\narticle can extract features from these complex ground objects\nand classify them, which further proves that the method in this\narticle can be compatible with the superiorities of CNN and\nTransformer, and effectively integrate the global information\ncollected by Transformer with the local information carried by\nCNN, so as to achieve the purpose of accurately segmenting\ndifferent ground objects.\nV. CONCLUSION\nCNN and Transformer models have accomplished remark-\nable results in the ﬁeld of computer vision, but owing to their\nrespective shortcomings and defects can be complementary,\nwhich makes the fusion of CNN and Transformer a new research\ndirection. We have effectively fused CNN and Transformer to\npropose a dynamically scalable attention model that leverages\nthe strengths of both CNN and Transformer. Our method has\nbeen validated on four public datasets, and the experimental\nresults show that our approach achieves the best performance.\nWe hope that DSViT will serve as a useful framework for future\ncomputer vision tasks. In the future, the remaining challenge of\nthis method is how to reduce the complexity of attention mech-\nanism in Transformer, increase the length of input sequence,\nand further improve the expression ability of Transformer, and\npropose a standard framework of convolutional network and\nTransformer interoperability.\nREFERENCES\n[1] A. Krizhevsky et al., “ImageNet classiﬁcation with deep convolu-\ntional neural networks,” inProc. Adv. Neural Inf. Process. Syst., 2012,\npp. 1106–1114.\n[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[3] H. Lee and H. Kwon, “Going deeper with contextual CNN for hyperspec-\ntral image classiﬁcation,”IEEE Trans. Image Process., vol. 26, no. 10,\npp. 4843–4855, Oct. 2017.\n[4] O. Ronneberger, F. Philipp, and B. Thomas, “U-Net: Convolutional net-\nworks for biomedical image segmentation,” inProc. Int. Conf. Med. Image\nComput. Comput.-Assist. Interv., 2015, pp. 234–241.\n[5] J. Dolz, K. Gopinath, J. Yuan, H. Lombaert, C. Desrosiers, and I. Ben Ayed,\n“HyperDense-Net: A hyper-densely connected CNN for multi-modal im-\nage segmentation,”I E E ET r a n s .M e d .I m a g ., vol. 38, no. 5, pp. 1116–1126,\nMay 2019.\n[6] X. Zhang et al., “DCNAS: Densely connected neural architecture search\nfor semantic image segmentation,” inProc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., 2015, pp. 13956–13967.\n[7] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:\nUniﬁed, real-time object detection,” inProc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2016, pp. 779–788.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies\nfor accurate object detection and semantic segmentation,” inProc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2014, pp. 580–587.\n[9] R. Girshick, “Fast R-CNN,” inProc. IEEE Int. Conf. Comput. Vis., 2015,\npp. 1440–1448.\n[10] W. Xu, C. Long, R. Wang, and G. Wang, “DRB-GAN: A dynamic res-\nblock generative adversarial network for artistic style transfer,” inProc.\nIEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 6383–6392.\n[11] H. Chen et al., “Diverse image style transfer via invertible cross-\nspace mapping,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021,\npp. 14860–14869.\n[12] J. Cai et al., “MeMOT: Multi-object tracking with memory,” inProc.\nIEEE/CVF Int. Conf. Comput. Vis., 2022, pp. 8090–8100.\n[13] I. Sutskever et al., “Sequence to sequence learning with neural networks,”\nin Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 1–9.\n[14] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\njointly learning to align and translate,” 2014. [Online]. Available: https:\n//arxiv.org/pdf/1409.0473.pdf\n[15] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 5998–6008.\n[16] J. Devlin et al., “BERT: Pre-training of deep bidirectional transformers\nfor language understanding,” inProc. Conf. North Amer . Chapter Assoc.\nComput. Linguistics, 2018, pp. 1–16.\n[17] T. Brown et al., “Language models are few-shot learners,” inProc. Adv.\nNeural Inf. Process. Syst., 2020, pp. 1877–1901.\n[18] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” inProc. 9th Int. Conf. Learn. Representations,\n2021, pp. 1–22.\n[19] X. Enze et al., “SegFormer: Simple and efﬁcient design for semantic\nsegmentation with transformers,” inProc. Adv. Neural Inf. Process. Syst.,\n2021, pp. 12077–12090.\n[20] O. A. B. Penatti, K. Nogueira, and J. A. dos Santos, “Do deep features\ngeneralize from everyday objects to remote sensing and aerial scenes\ndomains?,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops,\n2015, pp. 44–51.\n[21] H. Danfeng et al., “More diverse means better: Multimodal deep learn-\ning meets remote-sensing imagery classiﬁcation,”IEEE Trans. Geosci.\nRemote Sens., vol. 59, no. 5, pp. 4340–4354, May 2021.\n[22] X. Xiaodong, W. Li, Q. Ran, Q. Du, L. Gao, and B. Zhang, “Multi-\nsource remote sensing data classiﬁcation based on convolutional neural\nnetwork,”IEEE Trans. Geosci. Remote Sens., vol. 56, no. 2, pp. 937–949,\nFeb. 2018.\n[23] L. Zhiyong, F. Wang, G. Cui, J. A. Benediktsson, T. Lei, and W. Sun,\n“Spatial–spectral attention network guided with change magnitude image\nfor land cover change detection using remote sensing images,”IEEE Trans.\nGeosci. Remote Sens., vol. 60, 2022, Art. no. 4412712.\n[24] E. Maggiori, Y . Tarabalka, G. Charpiat, and P. Alliez, “Convolu-\ntional neural networks for large-scale remote-sensing image classiﬁca-\ntion,” IEEE Trans. Geosci. Remote Sens., vol. 55, no. 2, pp. 645–657,\nFeb. 2017.\n[25] A. Sharma et al., “A patch-based convolutional neural network for remote\nsensing image classiﬁcation,” Neural Netw., vol. 95, no. 1, pp. 19–28,\n2017.\n5452 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\n[26] K. Yang, X. -Y . Tong, G. -S. Xia, W. Shen, and L. Zhang, “Hidden path\nselection network for semantic segmentation of remote sensing images,”\nIEEE Trans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 5628115.\n[27] L. Ze et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 10012–10022.\n[28] D. Lei et al., “Looking outside the window: Wide-context transformer\nfor the semantic segmentation of high-resolution remote sensing images,”\nIEEE Trans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 4410313.\n[29] Y . Yongtao et al., “C2-CapsViT: Cross-context and cross-scale capsule\nvision transformers for remote sensing image scene classiﬁcation,”IEEE\nGeosci. Remote Sens. Lett., vol. 19, 2022, Art. no. 6512005.\n[30] R. Yongming et al., “Dynamicvit: Efﬁcient vision transformers with dy-\nnamic token sparsiﬁcation,” inProc. Adv. Neural Inf. Process. Syst., 2021,\npp. 13937–13949.\n[31] T. Zhengzhong et al., “MaxViT: Multi-axis vision transformer,” 2014.\n[Online]. Available: https://arxiv.org/pdf/2204.01697.pdf\n[32] Goodfellow et al., “Generative adversarial networks,”Commun. ACM,\nvol. 63, no. 11, pp. 139–144, 2020.\n[33] K. Simonyan et al., “Very deep convolutional networks for large-scale\nimage recognition,” in Proc. Int. Conf. Learn. Representations, 2014,\npp. 1–14.\n[34] Y . Yi et al., “Bag-of-visual-words and spatial extensions for land-use\nclassiﬁcation,” inProc. 18th SIGSPATIAL Int. Conf. Adv. Geographic Inf.\nSyst., 2010, pp. 270–279.\n[35] X. GuiSong et al., “Structural high-resolution satellite image indexing,”\nin Proc. SPRS TC VII Symp., 2020, pp. 298–303.\n[36] F. Rottensteiner et al., “International society for photogrammetry and\nremote sensing, 2D semantic labeling contest,” https://www.isprs.org/\neducation/benchmarks/UrbanSemLab/2d-semlabel-potsdam.aspx\n[37] W. Junjue et al., “LoveDA: A remote sensing land-cover dataset for domain\nadaptive semantic segmentation,” 2021. [Online]. Available: https://arxiv.\norg/pdf/2110.08733.pdf\n[38] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” inProc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2017, pp. 4700–4708.\n[39] T. Mingxing et al., “EfﬁcientNet: Rethinking model scaling for con-\nvolutional neural networks,” in Proc. Int. Conf. Mach. Learn., 2019,\npp. 6105–6114.\n[40] A. Howard et al., “Searching for mobileNetV3,” inProc. IEEE/CVF Int.\nConf. Comput. Vis., 2019, pp. 1314–1324.\n[41] I. O. Tolstikhin et al., “MLP-Mixer: An all-mlp architecture for vision,” in\nProc. Adv. Neural Inf. Process. Syst., 2021, pp. 24261–24272.\n[42] W. Wenhai et al., “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” inProc. IEEE/CVF Int. Conf.\nComput. Vis., 2021, pp. 568–578.\n[43] L. Zhuang, H. Mao, C. -Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie,\n“A convNet for the 2020s,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., 2022, pp. 11976–11986.\n[44] C. Liang-Chieh et al., “Encoder-decoder with atrous separable convolution\nfor semantic image segmentation,” inProc. Eur . Conf. Comput. Vis., 2018,\npp. 801–818.\n[45] W. Libo, R. Li, C. Duan, C. Zhang, X. Meng, and S. Fang, “A novel\ntransformer based semantic segmentation scheme for ﬁne-resolution re-\nmote sensing images,”IEEE Geosci. Remote Sens. Lett., vol. 19, 2022,\nArt. no. 6506105.\n[46] W. Libo et al., “UNetFormer: A UNet-like transformer for efﬁcient se-\nmantic segmentation of remote sensing urban scene imagery,”ISPRS J.\nPhotogrammetry Remote Sens., vol. 190, no. 1, pp. 196–214, 2022.\n[47] L. Rui et al., “Multiattention network for semantic segmentation of ﬁne-\nresolution remote sensing images,”IEEE Trans. Geosci. Remote Sens.,\nvol. 60, 2021, Art. no. 5607713.\n[48] W. Di, J. Zhang, B. Du, G. -S. Xia, and D. Tao, “An empirical study of\nremote sensing pretraining,”IEEE Trans. Geosci. Remote Sens., vol. 61,\n2023, Art. no. 5608020.\n[49] W. Di et al., “Advancing plain vision transformer towards remote sensing\nfoundation model,” IEEE Trans. Geosci. Remote Sens., vol. 61, 2023,\nArt. no. 5607315.\n[50] Y . Zhang et al., “Language-aware domain generalization network for cross-\nscene hyperspectral image classiﬁcation,” 2022. [Online]. Available: https:\n//arxiv.org/pdf/2209.02700.pdf\n[51] Z. Cheng, W. Jiang, Y . Zhang, W. Wang, Q. Zhao, and C. Wang, “Trans-\nformer and CNN hybrid deep neural network for semantic segmentation\nof very-high-resolution remote sensing imagery,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, 2022, Art. no. 4408820.\n[52] J. Hou, Z. Guo, Y . Wu, W. Diao, and T. Xu, “BSNet: Dynamic hybrid\ngradient convolution based boundary-sensitive network for remote sensing\nimage segmentation,”IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5624022.\n[53] J. Chen et al., “TransuNet: Transformers make strong encoders for medical\nimage segmentation,” 2021, [Online]. Available: https://arxiv.53yu.com/\npdf/2102.04306.pdf\n[54] M. Yuan et al., “MCAFNet: A multiscale channel attention fusion network\nfor semantic segmentation of remote sensing images,” Remote Sens.,\nvol. 15, no. 2, pp. 361–378, 2023.\n[55] B. Sui, Y . Cao, X. Bai, S. Zhang, and R. Wu, “BIBED-Seg: Block-in-\nblock edge detection network for guiding semantic segmentation task of\nhigh-resolution remote sensing images,”IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 16, pp. 1531–1549, 2023.\n[56] W. Junjue, Y . Zhong, Z. Zheng, A. Ma, and L. Zhang, “RSNet: The search\nfor remote sensing deep neural networks in recognition tasks,”IEEE Trans.\nGeosci. Remote Sens., vol. 59, no. 3, pp. 2520–2534, Mar. 2020.\n[57] E. Arnaudo et al., “Hierarchical instance mixing across domains in\naerial segmentation,” 2022. [Online]. Available: https://arxiv.org/pdf/\n2210.06216.pdf\n[58] Z. Chenyu et al., “AutoLC: Search lightweight and top-performing archi-\ntecture for remote sensing image land-cover classiﬁcation,” 2022. [Online].\nAvailable: https://arxiv.org/pdf/2205.05369.pdf\n[59] A. Ma, J. Wang, Y . Zhong, and Z. Zheng, “FactSeg: Foreground activation-\ndriven small object semantic segmentation in large-scale remote sensing\nimagery,” IEEE Trans. Geosci. Remote Sens., 2021, Art. no. 5606216.\n[60] L. Hoyer, D. Dai, and L. Van Gool, “DAFormer: Improving network\narchitectures and training strategies for domain-adaptive semantic segmen-\ntation,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022,\npp. 9914–9925.\n[61] R. Xu, C. Wang, J. Zhang, S. Xu, W. Meng, and X. Zhang, “RSSFormer:\nForeground saliency enhancement for remote sensing land-cover segmen-\ntation,” IEEE Trans. Image Process., vol. 32, pp. 1052–1064, 2023.\n[62] Q. Zhao et al., “Self-training guided disentangled adaptation for cross-\ndomain remote sensing image semantic segmentation,” 2023. [Online].\nAvailable: https://arxiv.org/pdf/2301.05526.pdf\nFalin Wang was born in Gansu, China, in 1996.\nHe received the master’s degree in computer science\nand technology from Northwest Normal University,\nLanzhou, China, in 2021. He is currently working\ntoward the Ph.D. degree in software engineering with\nXidian University, Xi’an, China.\nHis research interests include pattern recognition\nand semantic segmentation.\nJian Ji was born in Xi’an, China, in 1971. She re-\nceived the B.S. degree in computational mathematics\nfrom Northwest University, Lanzhou, China, and the\nPh.D. degree in computer science and technology\nfrom Northwestern Polytechnical University, Xi’an,\nChina, in 1993 and 2007, respectively.\nShe is currently a Professor with the School of\nComputer Science and Technology, Xidian Univer-\nsity, Xi’an, China. Her research interests include\ncomputational intelligence, pattern recognition, and\nimage analysis.\nYuan Wang was born in Gansu, China, in 2000.\nShe received the undergraduate’s degree in computer\nscience and technology, in 2022, from Xidian Uni-\nversity, Xi’an, China, where she is currently working\ntoward the master’s degree in electronic information\nengineering.\nHer research interests include image processing\nand semantic segmentation.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.815484881401062
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7053792476654053
    },
    {
      "name": "Segmentation",
      "score": 0.6431446075439453
    },
    {
      "name": "Transformer",
      "score": 0.6357031464576721
    },
    {
      "name": "Scalability",
      "score": 0.5712357759475708
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5552011132240295
    },
    {
      "name": "Image segmentation",
      "score": 0.5394479036331177
    },
    {
      "name": "Computer vision",
      "score": 0.49652916193008423
    },
    {
      "name": "Pixel",
      "score": 0.4847947657108307
    },
    {
      "name": "Feature extraction",
      "score": 0.4481012523174286
    },
    {
      "name": "Contextual image classification",
      "score": 0.42070281505584717
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41730812191963196
    },
    {
      "name": "Image (mathematics)",
      "score": 0.24423658847808838
    },
    {
      "name": "Voltage",
      "score": 0.08778277039527893
    },
    {
      "name": "Engineering",
      "score": 0.06732383370399475
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I149594827",
      "name": "Xidian University",
      "country": "CN"
    }
  ]
}