{
    "title": "Enriching Language Models with Visually-grounded Word Vectors and the Lancaster Sensorimotor Norms",
    "url": "https://openalex.org/W3213012878",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2274008004",
            "name": "Casey Kennington (Mentor)",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4212844288",
        "https://openalex.org/W2470973119",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W2978491132",
        "https://openalex.org/W2982116886",
        "https://openalex.org/W1716419267",
        "https://openalex.org/W2997786945",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2978010506",
        "https://openalex.org/W4288265479",
        "https://openalex.org/W3100307207",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W1979532929",
        "https://openalex.org/W2987281820",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2985797697",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3126961594",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2050896993",
        "https://openalex.org/W1968060331",
        "https://openalex.org/W2798881773",
        "https://openalex.org/W2766435878",
        "https://openalex.org/W2785596580",
        "https://openalex.org/W2904990229",
        "https://openalex.org/W2898251084",
        "https://openalex.org/W3176195078",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3172673629",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4243093309",
        "https://openalex.org/W2099535060",
        "https://openalex.org/W1508611703",
        "https://openalex.org/W1527575280",
        "https://openalex.org/W2166700048",
        "https://openalex.org/W3138301265",
        "https://openalex.org/W4237129069",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W2587244291",
        "https://openalex.org/W2204010996",
        "https://openalex.org/W620953717",
        "https://openalex.org/W4287333395",
        "https://openalex.org/W2889111860",
        "https://openalex.org/W3110909889",
        "https://openalex.org/W3020712669",
        "https://openalex.org/W2937971956",
        "https://openalex.org/W4301734842",
        "https://openalex.org/W2971930324",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W3116651605",
        "https://openalex.org/W2107019937",
        "https://openalex.org/W2964195418",
        "https://openalex.org/W2103104224",
        "https://openalex.org/W1939733999",
        "https://openalex.org/W3118485687"
    ],
    "abstract": "Language models are trained only on text despite the fact that humans learn their first language in a highly interactive and multimodal environment where the first set of learned words are largely concrete, denoting physical entities and embodied states. To enrich language models with some of this missing experience, we leverage two sources of information: (1) the Lancaster Sensorimotor norms, which provide ratings (means and standard deviations) for over 40,000 English words along several dimensions of embodiment, and which capture the extent to which something is experienced across 11 different sensory modalities, and (2) vectors from coefficients of binary classifiers trained on images for the BERT vocabulary. We pre-trained the ELECTRA model and fine-tuned the RoBERTa model with these two sources of information then evaluate using the established GLUE benchmark and the Visual Dialog benchmark. We find that enriching language models with the Lancaster norms and image vectors improves results in both tasks, with some implications for robust language models that capture holistic linguistic meaning in a language learning context.",
    "full_text": "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 148–157\nNovember 10–11, 2021. ©2021 Association for Computational Linguistics\n148\nEnriching Language Models with Visually-grounded Word Vectors\nand the Lancaster Sensorimotor Norms\nCasey Kennington\nDepartment of Computer Science\nBoise State University\ncaseykennington@boisestate.edu\nAbstract\nLanguage models are trained only on text de-\nspite the fact that humans learn their ﬁrst lan-\nguage in a highly interactive and multimodal\nenvironment where the ﬁrst set of learned\nwords are largely concrete, denoting physical\nentities and embodied states. To enrich lan-\nguage models with some of this missing ex-\nperience, we leverage two sources of informa-\ntion: (1) the Lancaster Sensorimotor norms,\nwhich provide ratings (means and standard de-\nviations) for over 40,000 English words along\nseveral dimensions of embodiment, and which\ncapture the extent to which something is expe-\nrienced across 11 different sensory modalities,\nand (2) vectors from coefﬁcients of binary clas-\nsiﬁers trained on images for the BERT vocab-\nulary. We pre-trained the ELECTRA model\nand ﬁne-tuned the RoBERTa model with these\ntwo sources of information then evaluate us-\ning the established GLUE benchmark and the\nVisual Dialog benchmark. We ﬁnd that en-\nriching language models with the Lancaster\nnorms and image vectors improves results in\nboth tasks, with some implications for robust\nlanguage models that capture holistic linguis-\ntic meaning in a language learning context.\n1 Introduction\nChildren learn their ﬁrst spoken language in a\nhighly interactive setting where generally the ﬁrst\nwords children learn are concrete words that de-\nnote physical objects, which is an important devel-\nopmental step in child ﬁrst language acquisition\n(Kuperman et al., 2012a; McCune, 2008; Clark,\n2013). This is partly because handling the Symbol\nGrounding Problem–the ablity to connect symbolic\nknowledge of language with representations of the\nphysical world (Harnad, 1990)–must take place be-\nfore children learn more abstract concepts later in\ntheir cognitive development (Borghi et al., 2019;\nPonari et al., 2018). Importantly, the physical world\nis not just the visual world; children learn that\nwords ground into proprioperceptive states (e.g.,\na hand grasp around an object has speciﬁc muscle\nactivations tied to the word grab), interoceptive\nstates (i.e., affect and valence), as well as all other\nsensory modalities (e.g., the word stinky grounds\ninto olfactory, the word loud grounds into audi-\ntory). These claims are evidenced in a large body of\nchild development and cognitive science literature.\nSmith and Gasser (2005), for example, identiﬁed\nthat babies’ experience of the world is profoundly\nmultimodal: babies live in a physical world full\nof rich regularities that organize perception, action\nand thought; babies learn in a social world to learn\na shared linguistic communicative system that is\nsymbolic. Furthermore, a growing body of litera-\nture from linguistics and computational linguistics\nmakes a strong case that the process of language\nlearning (indeed, general human cognition) is em-\nbodied, interactive, and enacted; i.e., movement in\nthe world is required (Pulverm¨uller, 1999; Lakoff\nand Johnson, 1999; Barsalou, 2008; Johnson, 2008;\nSmith and Samuelson, 2009; Di Paolo et al., 2018;\nBisk et al., 2020); see also the prior work in de-\nvelopmental robotics research; e.g., Cangelosi and\nSchlesinger (2015), Chapter 7.1\nTaken together, it is clear that aspects of the phys-\nical world are necessary for holistic knowledge of\nsemantic meaning, which has implications for how\nlanguage is modeled computationally. In particu-\nlar, what does this mean for language models that\nare trained purely on text (likely largely written\nby adults), such as BERT (Devlin et al., 2018) or\nGPT-3? These models have clearly led to impor-\ntant advances for natural language processing tasks\nand applications, but it is also clear that language\nmodels trained only on text are missing critical\nsemantic information (Bender and Koller, 2020).\n1Here we follow Dourish (2001) that emodiment ispossess-\ning and acting through a physical manifestation in the world;\ni.e., having sensory input is only part of emobodiment–the\nability to act in the world is essential.\n149\nIn this paper, we contribute to a growing body\nof recent work that attempts to addresses these\nlimitations by (1) leveraging multimodal and sen-\nsorimotor knowledge of the Lancaster Sensorimo-\ntor Norms (Lynott et al., 2019) and (2) using vec-\ntorized representations of images by treating both\n(1) and (2) as embeddings of language models for\nGLUE and Visual Dialog benchmarks. In the fol-\nlowing section, we explain related work–a growing\nbody of literature that is adding multimodal infor-\nmation to language models, then we explain our\ntwo embeddings that we will use. We explore how\nthese embeddings can be used to enrich the ELEC-\nTRA language model’s pre-training and ﬁne-tuning,\nand evaluate on the GLUE benchmark (Experiment\n1, Section 4), and how they can be used to re-\nplace input embeddings for a pre-trained RoBERTa\nmodel for the Visual Dialog task (Experiment 2,\nSection 5). Our experiments shed light on how use-\nful multimodal information can be in a task that is\ntext-only (Experiment 1) and a task that is multi-\nmodal (Experiment 2). Our results show that our\nparsimonious method to unifying vision (and sen-\nsorimotor knowledge) in existing language models\nshows improvements in multimodal benchmarks\nwith accessible hardware (i.e., a single GPU) as a\nstep towards models that can be trained in settings\nsimilar to that of child language learners.\n2 Related Work\nLanguage models are trained on text. G ¨unther et al.\n(2018) took up the question do words inherit senso-\nrimotor activation from purely linguistic context?\nand showed that experience is necessary for reac-\ntivating experiential traces, but this reactivation is\nnot a necessary condition for understanding the cor-\nresponding aspects of word meaning. We take this\nto mean that humans are very adept at learning new\nconcepts from language exposure alone (i.e., ab-\nstract concepts); e.g., someone who has never seen\na zebra before, but hears them described as “horses\nwith vertical black and white stripes” can compose\na connotation of what zebra denotes without di-\nrect visual exposure. However, this only works\nif an agent that has learned the language has the\nknowledge of horses, black, white, stripes, and ver-\ntical concepts–i.e., via direct experience, not just\nthrough linguistic exposure or encyclopedic deﬁni-\ntions. These claims are further backed up by neu-\nroscience research that showed that neural assem-\nblies encode concrete content words (i.e., words\nthat denote visual objects) and verbs (i.e., words\nthat denote actions) are learned and represented in\ndifferent brain regions (Pulverm¨uller, 1999; Borgh-\nesani et al., 2019).\nRogers et al. (2020) provides a recent primer and\noverview of research that has attempted to uncover\nstrengths and weaknesses of BERT and related\nlanguage models (so-called BERTology). While\nour work does ﬁt into that growing body of liter-\nature, our criticisms on current language models\nspeciﬁcally lies in the fact that they are only trained\non easy-to-obtain text. This criticism is born out\nin Forbes et al. (2019) which showed that BERT\ncan guess affordances and properties of objects be-\ncause that information can be found in text (e.g., a\ntypical chair has the affordance of being sittable,\nand a property of having legs), but has no notion\nof how objects are related semantically to each\nother, and Da and Kasai (2019) further showed that\nreal-world perceptual properties are likely to be\nassumed instead of inferred. Furthermore, Bender\nand Koller (2020) make a strong case that BERT\nlearns form instead of meaning, and while the fact\nthat BERT performs so well on many tasks is difﬁ-\ncult to dispute, models trained on text are missing\nsemantic information crucial for holistic language\nunderstanding.\nSince before BERT which has proven powerful\nin many language processing tasks, efforts have\nbeen made to encode multimodal (i.e., more than\njust text as a learning modality) information into\nembeddings and language models (Takano and\nUtsumi, 2016; Kiros et al., 2014; Zellers et al.,\n2021) and recent, continued efforts towards bridg-\ning grounded visual representations to distribu-\ntional representations of word meanings give cre-\ndence to the claim that text-only models like BERT\nare missing crucial semantic information because\nenriching BERT with visual information improves\nperformance in several known tasks (Kim et al.,\n2019; Lu et al., 2019; Li et al., 2019). These mod-\nels usually treat language and vision as separate\npipelines; our method directly endows the language\nmodel with visual and sensorimotor knowledge.\n3 Data\nIn this section, we motivate and introduce of multi-\nmodal information we will use in our experiments.\nThe Lancaster Sensorimotor Norms The Lan-\ncaster Sensorimotor norms (Lynott et al., 2019)\nprovide ratings (means and standard deviations) for\n150\n40,000 English words along dimensions of embodi-\nment which capture the extent to which a concept is\nexperienced across 11 different sensory modalities,\nand measures derived from those categories, listed\nbelow (each has an example word that rates highly\nfor that modalitiy):\n• Auditory - sound; ping\n• Gustatory - having to do with eating; cream\n• Haptic - muscle movement; handshake\n• Interoceptive - having to do with affect or emo-\ntion; headache\n• Olfactory - smell; incense\n• Visual - visual; barcode\n• Foot-leg - haptics for foot/leg; run\n• Hand-arm - haptics for hand/arm; pointing\n• Head - having to do with the head; eye\n• Mouth - haptics for mouth; kiss\n• Torso - haptics for torso; breath\n• Max-strength.perceptual - the highest rating\nacross the 11 sensorimotor dimensions\n• Minkowski3.perceptual - treating the 11\nmodalities as a vector, this represents the dis-\ntance of the vector from the origin with inﬂu-\nence of weaker dimensions attenuated\n• Exclusivity.perceptual - the extent to which a\nconcept (out of the 11) which is experienced\nthrough a single perceptual modalitiy\nThe last three can be seen as aggregates from\nthe 11 modalities; they also have .action values\nrepresenting the extent to which a concept is ex-\nperienced as an action (as opposed to .perceptual),\nand .sensorimotor values representing the extent\na concept is experience as sensorimotor. As these\nnorms were derived from surveys given to adults,\nthese norms represent the degree to which the sur-\nvey participants assigned those words to those cat-\negories. Though this does not represent a neuro-\nphysiological grounding of words to those modali-\nties learned through interaction and embodiment,\nthis serves as a useful approximation. The ﬁnal\nset is a vocabulary of 39,707 words (after remov-\ning rows which had null values), each represented\nas a vector of length 39 (i.e., 11 mean, 11 stdev\ncolumns; Max-strength, Minkowski, and Exclusiv-\nity columns for different ways of aggregating the\nmodalities). We normalize each value in the vector\nindependently to a value between 0-1 by dividing\neach value over its max value. We call this the\nLancaster vectors.\nWe performed t-SNE on the Lancaster vectors\n(mapping to 2 dimensions) to determine if clus-\nters would reveal any intuitions about the kinds\nof semantic relatedness that the words might have\nwith each other. Some clusters emerged such as\nfoods (presumably because they have similargusta-\ntory ratings), leg-movement verbs (e.g.,walk, jump,\nsit), colors with eye-related words (e.g., purple,\ngreen, blue, dark, see, eyes), soft things (e.g., hug,\ntummy, pillow, clothes), audio-related words (e.g.,\ntalk, story, sound, music, lie, say), among others.\nFigure 1: The red WAC classiﬁers are trained on pos-\nitive and negative examples of images from Google\nImages for the word red; each image is then passed\nthrough the CLIP model. We train a binary logistic re-\ngression classiﬁer, then extract the coefﬁcients for the\nred vector.\nWords-as-Classiﬁers Image Vectors The\nWords-as-Classiﬁers (WAC) approach to grounded\nsemantics is quite simple: train a binary classiﬁer\nfor each word in a corpus where the features to that\nclassiﬁer are derived from images (Kennington and\nSchlangen, 2015). Each classiﬁer is given positive\nand negative examples of visual denotations of\neach word by the images and learns a “ﬁtness”\nscore by the classiﬁer. For example, the red\nclassiﬁer is given images of objects that are\nreferred to as red in a corpus, and randomly\nassigned negative examples of things that are not\nreferred to as red, as depicted in Figure 1. We\nfollow Kiros et al. (2018) and use Google Image\nSearch to ﬁnd images using the BERT vocabulary,\nresulting in 27,152 words and corresponding\nimages (some words did not result in images, and\nwe did not download images for ﬁller words).\nFor each word, we perform an image search and\ndownload the top 100 images. We then follow\nSchlangen et al. (2016) and process each image\nby passing them through the recent CLIP (Jia\n151\net al., 2021) convolutional neural network (trained\non ImageNet, using CLIP’s ViT-B/32 model),\nyielding a vector of size 512 for each image. We\nuse the 100 images as positive examples for each\nterm in our vocabulary and randomly select three\nnegative examples for each positive example. We\nthen use a logistic regression classiﬁer (C=0.25,\nmax iterations=1000), one for each word, trained\non the images for each word. After training, we\nthen follow Moro et al. (2019) and extract the\ncoefﬁcients to arrive at a vector of size 513 (all\ncoefﬁcients plus the bias term) which we use in our\nevaluations below. We call these theWAC vectors.\nThe WAC model is useful because, as explained\nin Kennington and Schlangen (2015), the classi-\nﬁers can actually identify objects (something that\nlanguage models cannot do on their own), the coef-\nﬁcients represent a computed word intension, new\nwords in a vocabulary can easily be added without\nretraining all other classiﬁers including adjectives\nlike red which are often missing from pre-trained\nobject classiﬁers, and the classiﬁers are effectively\nlearned with only a few examples, making it ef-\nfective for fast learning of concrete, grounded con-\ncepts. However, the WAC model suffers from two\nassumptions: ﬁrst, that all words have concrete, vi-\nsual denotations even though many abstract words\nlike utopia clearly do not, and that all words are\nindependent of each other in terms of linguistic con-\ntext. We hypothesize in both experiments below\nthat these coefﬁcients used as vectorized embed-\ndings will be useful to a text-only language model\nbecause they add necessary visual information; the\nlanguage model complements WAC by using lin-\nguistic context (i.e., text) for training, overcoming\nWAC’s assumptions.\n4 Experiment 1: Tying embedding\nweights and pre-training ELECTRA,\nﬁne-tuning on GLUE\nIn this experiment, and crucially for our ongoing\nwork that aligns with child-inspired language ac-\nquisition, we use ELECTRA (Clark et al., 2020) as\na language model because it has been shown to be\ntrainable with smaller amounts of data than other\nlanguage models, yet yield respectable results and\ncan be trained using a single GPU.\nTask & Procedure Wang et al. (2018) intro-\nduced the GLUE benchmark which consists of\nnine English sentence understanding tasks covering\nseveral domains (e.g., movie reviews and online\nFigure 2: Image regions (objects) represented as CLIP\nvectors are positive and negative train examples for\nW AC classiﬁer. W AC classiﬁer weights are tied to\nthe embedding layer for the Generator and Discrimi-\nnator for ELECTRA. Dimensionality Reduction (DR)\nmaps higher dimensional vectors to lower dimensions\nas needed. Lancaster vectors are represented directly.\nquestion answering). We opt for this benchmark\nbecause of its coverage over several domains and to\nshow that adding multimodal knowledge improves\ntasks that are based on text.2 Our aim is to achieve\nimproved results over the text-only baseline with a\nspeciﬁed number of training steps using the open-\nwebtext data for training.3 We report results on the\ndevelopment set, as done in Wu et al. (2021). We\nonly report the results for the MRPC (a paraphrase\ntask that uses accuracy and f1 metrics), COLA (a\ngrammatical acceptibility task; uses Matthew’s Cor-\nrelation), and WNLI (ambiguity resolution; uses an\naccuracy metric) tasks because they are sufﬁcient to\nillustrate the utility of our method when applied to\nELECTRA. To give ELECTRA knowledge about\nadditional modalities from the Lancaster and WAC\nvectors, we tie the vectors to the the weights of the\ngenerator and discriminator of ELECTRA depicted\nin Figure 2, and vary whether the embeddings are\n2GLUE has a public leader board found at https://\ngluebenchmark.com/leaderboard\n3We build off of the implementation of https://\ngithub.com/lucidrains/electra-pytorch\n152\nfrozen or not during pre-training, then train for\n100,000 steps.4 We then ﬁne-tune the resulting\nELECTRA model on the GLUE tasks using the\nmultimodal vectors following standard ﬁne-tuning\nprotocols; that is, we add a linear layer with a soft-\nmax to the pre-trained model and use the ADAM\nsolver with a learning rate of 2e-5 for 3 epochs. As\nthe WAC vectors were larger than ELECTRA’s ex-\npected embedding size of 128, we applied UMAP\nto reduce the dimensionality to 128; similarly for\nthe WAC and Lancaster concatenated embeddings.\nFor cases where there was no vector for a word\n(e.g., the [unmapped] words or words outside of\nthe vocabulary of the Lancaster vectors), we simply\nused zero vectors. For Lancaster vectors, we set\nthe ELECTRA embedding size to 39. We explored\nfreezing the embeddings; our hypothesis is that not\nfreezing the embeddings will lead to better results\nbecause the training regime can overpower the em-\nbeddings, but retain the multimodal knowledge.\nFor a broader comparison, we also compared\nto GloVE (Pennington et al., 2014) and several\nablations where we concatenate multimodal vec-\ntors with the GloVe vectors (we used the evalu-\nation script for GloVE provided by Wang et al.\n(2018)). We also use the same training and evalua-\ntion regime for the WAC and Lancaster vectors, and\na concatenation of the two, on their own treating\nthem as word-level embeddings similar to GloVe.\nResults Table 1 shows the results on the GLUE\nbenchmark. The word-level embeddings of GloVe,\nWAC, and Lancaster are shown in the top 5 rows\nof the table. As expected, these word-level embed-\ndings are not state-of-the-art, but we notice that\nboth Lancaster and WAC vectors perform compa-\nrably against the GloVE vectors despite only be-\ning trained on images ( WAC) or derived from the\nLancaster norms. Of note is a signiﬁcant advan-\ntage of using the Lancaster vectors alone compared\nto using any other embedding or combination for\nthe WNLI task which is co-reference and natural\nlanguage inference for ﬁction books. This sug-\ngests that inference on ﬁction is helped by knowing\nwhich modalities affect each word. Interestingly,\nthe best performing model for COLA was GloVE\nand Lancaster word-level embeddings; COLA is\n4This takes about 12 hours of training on our 12GB GPU,\nwhich we opted for because it represents more data and train\ntime than ELECTRA-small, but still a small enough amount of\ntime to establish using this model in a co-located, interactive\nlearning setting similar to the setting where children learn\ntheir ﬁrst language.\nMRPC MRPC COLA WNLI\nacc f1 corr acc\nGloVE 0.745 0.807 0.691 0.563\nGloVE+lan 0.735 0.799 0.449 0.563\nlan 0.711 0.778 0.691 0.596\nwac 0.748 0.812 0.313 0.563\nlan+wac 0.619 0.670 0.382 0.535\nELECTRA 0.730 0.835 0.449 0.563\nEL-wac 0.730 0.835 0.440 0.563\nEL-wacf 0.760 0.833 0.0 0.563\nEL-lan 0.708 0.819 0.39 0.535\nEL-lan-wacf 0.792 0.859 0.459 0.563\nTable 1: GLUE development set results with GloVe,\nELECTRA, Lancaster (lan), WAC models and several\ncombinations, with (f and without weight freezing dur-\ning ELECTRA pre-training.\na grammaticality test, which is important in lan-\nguage understanding, but arguably less critical in\nearly-stage child language acquisition.\nAll other rows show the ELECTRA baseline\nand ELECTRA that uses some variation of WAC,\nLancaster, or both as embeddings (denoted with\nthe EL- preﬁx). The bottom part of the table com-\npares ELECTRA with a variant of ELECTRA that\nuses WAC embeddings (both with and without freez-\ning the embedding weights), ELECTRA with lan-\ncaster embeddings and ELECTRA with WAC em-\nbeddings concatenated with the Lancaster embed-\ndings (where the length of the WAC embeddings\nplus the size of ELECTRA is 128). Contrary to our\nhypothesis, we observe that when ELECTRA uses\nWAC with frozen weights, the performance on the\nbenchmark performs better than all others, includ-\ning the ELECTRA baseline. This could suggest\nthat ELECTRA can make effective use of the visual\nand Lancaster embeddings by adjusting weights in\nthe other layers of the model. The EL-lan-wac\nvariant performed well above the ELECTRA base-\nline, substantiating the hypothesis that enriching\nthe model with multimodal knowledge can improve\nresults. Taken together, we ﬁnd the results encour-\naging because the relatively short training regime\nstill yielded respectable results, suggesting that\nELECTRA with a visual or other multimodal em-\nbedding can be useful with less training as is the\ncase when children learn language.\n153\n5 Experiment 2: Replacing RoBERTa\nembeddings, ﬁne-tuninng on Visual\nDialog\nThe evaluation in Experiment 1 was made up of\ntext-based tasks. In this experiment, we use an eval-\nuation that requires knowledge of the visual world\nby evaluating the Lancaster andWAC vectors on the\nVisual Dialog task (Das et al., 2019), termed vis-\ndial. Moreover, Experiment 1 used pre-training on\na subset of the data for only 100,000 steps. In this\nexperiment, we evaluate using a fully pre-trained\nRoBERTa model by replacing its embeddings with\nthe WAC and Lancaster vectors.\nTask Following Murahari et al. (2019), given\nan image, dialogue history consisting of question-\nanswer pairs, and a follow-up question about the\nimage, the task of visdial is to predict a free-form\nnatural language answer to the question. The vis-\ndial dataset introduced in Das et al. (2019) also\nincludes evaluation metrics and human-annotated\nanswers to the natural language queries about the\nimage. Five human annotators identiﬁed which re-\nsponses out of 100 candidates could be considered\ncorrect. This allows multiple answers to be correct\n(e.g., yes and yeah are semantically identical).\nMetrics We report the following metrics:\n• R@1 Rate of times the top-ranked answer is\na correct one; i.e., accuracy.\n• R@5 Rate of times correct answers are in the\ntop-ﬁve ranked answers.\n• MRR Mean Reciprocal Rank is the multi-\nplicative inverse of the rank of the ﬁrst correct\nanswer.\n• NDCG Normalized Discount Accumulative\nGain is a measure of ranking quality that takes\nthe top K ranked options, where K is the num-\nber of answers marked as correct by a least\none annotator; in this measure, the fraction of\nannotators that marked a particular answer as\ncorrect is taken into account.\nBaseline and Procedure We report the values\nfor the model described in Murahari et al. (2019)\nfor our baseline–work which builds on VilBERT\n(Lu et al., 2019), a parallel model of vision and\nlanguage used for the visual dialogue task–and\nleverage their model with our custom, multimodal\nembeddings. Their model uses two transformers,\none for the language modality and one for the vi-\nsual modality. As explained in Lu et al. (2019),\nthe interaction between the two transformers is me-\ndiated by two co-attention layers where attention\nin one modality is conditioned on inputs from the\nother modality. Murahari et al. (2019) adapted\nthe VilBERT model for the visdial task by using\na pre-trained language model trained on English\nWikipedia and the BooksCorpus (Zhu et al., 2015)\nusing masked language modeling and next sentence\nprediction losses. They then frame the task as a\nnext-sentence prediction task (whereas the origi-\nnal VilBERT was modeled to generate descriptions\nof images). They then use the Conceptual Cap-\ntions (Sharma et al., 2018) and Visual Question\nAnswering (VQA) (Antol et al., 2015) datasets\n(using masked image region, masked language\nmodeling, and next sentence prediction losses) to\ntrain the two transformers. They then ﬁne-tune\non the visdial task (also using masked image re-\ngion, masked language modeling, and next sen-\ntence prediction losses). The underlying architec-\nture uses a pre-trained BERT language model (i.e.,\nbert-base-uncased) as a starting point before train-\ning on the Wikipedia, BooksCorpus, Conceptual\nCaptions, and VQA datasets. This constitutes our\nbaseline. We don’t consider the dense representa-\ntions from Murahari et al. (2019) due to hardware\nlimitations.\nWe altered their architecture by replacing the\nRoBERTa pre-trained embedding layer with the\nLancaster and WAC vectors, as depicted in Figure 3.\nWe then ﬁne-tuned on the visdial task using their\ntraining regime.5 We explain how we made vectors\ncompatible with their architecture.\nVocabulary: RoBERTa & AoA Abstract words\ndo not have concrete, visual denotations, such as\nutopia or justice, so it does not make theoretical\nsense to include a WAC embedding for words that\nare clearly abstract because whatever set of images\nrepresents those concepts may not have useful se-\nmantic information. Moreover, children learning\ntheir ﬁrst language learn concrete concepts before\nthey learn more abstract concepts (Borghi et al.,\n2019; Ponari et al., 2018). To explore if RoBERTa\ncould make use of a WAC embedding that uses\nwords that are more aimed at a child vocabulary,\nwe report results of ﬁltering out words not in the\nthe Age-of-Acquisition (AoA) list (Kuperman et al.,\n2012b). AoA a list of 30,000 English words rated\nfor the average age when children ﬁrst speak those\nwords (avg 11 years; std 3.0, most common words\n5https://github.com/vmurahari3/visdial-bert\n154\nFigure 3: Adapted from Murahari et al. (2019). Our approach uses the same pre-training datasets, architecture,\nand losses. During the ﬁnal ﬁne-tuning on the Visual Dialog data, however, we replace the RoBERTa embeddings\nwith the Lancaster and WAC embeddings.\nare for ages 2-14). This resulted in 9,627 remaining\nwords in the vocabulary; all other words were set\nto an embedding of zeros.\nLancaster vectors Similar to AoA, the Lan-\ncaster Norms has a predeﬁned vocabulary, which,\nwhen compared to the RoBERTa vocabulary re-\nsults in 11,402 words in both. For each word in the\nRoBERTa vocabulary that was also in the Lancaster\nnorms, we replaced the RoBERTa embedding with\nthe Lancaster vector for that word; otherwise words\nretained the original RoBERTa embedding. As\ntheir model expects vectors of size 768 (the embed-\nding size for RoBERTa), but the Lancaster vectors\nare only size 39, we padded the rest of the vector\nwith zeros.\nWAC vectors We use the vocabulary from the\nRoBERTa tokenizer as with the Lancaster Vectors,\nwhich results in a a 27,152-word overlap with the\nWAC vectors. As the WAC vectors have a dimen-\nsionality of 513, smaller than the required size of\nRoBERTa’s 768, we padded zeros after each vector.\nAll vectors that did not exist in the WAC set were\nzero vectors of size 768. We followed a training\nregime for two settings:\n• no-freeze The embedding layer was not\nfrozen so as to allow weight changes during\ntraining.\n• 2-freeze The embedding layer was frozen for\ntwo epochs, then the weights were unfrozen\nfor the rest of training; prior work has shown\nthat freezing layers after a certain number of\nepochs can improve results (Liu et al., 2021);\nwe opt for two because it still allows the ﬁne-\ntuning to overpower the exiting embeddings\nif needed and preliminary results showed that\nfreezing the weights for all epochs resulted in\npoor model performance.\nWe trained each model for 20 epochs in total,\nwhich is the default training setting for this task.\nWe used the settings that were used to train the\nbaseline model (i.e., learning rate of 2e-5). We re-\nport the results of the baseline model and the vari-\nants of our above changes.6 Compared to Experi-\nment 1 with the GLUE benchmark, the approach\ntaken in this section fundamentally changes how\nthe Lancaster and WAC embeddings are applied\nto RoBERTa; here the Lancaster andWAC embed-\ndings are used on a pre-trained model. We hy-\n6Note that the baseline we are comparing to\nhere is lower than what is reported on the leader-\nboard https://eval.ai/web/challenges/\nchallenge-page/518/leaderboard. This is\npartially due to the fact that our training regime was altered\ndue to hardware limitations (i.e., we could only use a batch\nsize of 8 on a single 12GB GPU).\n155\nno freeze MRR R@1 R@5 NDCG\nbaseline 64.92 50.52 82.98 56.82\nlan 64.61 49.98 82.6 58.10\n2-freeze MRR R@1 R@5 NDCG\nlan 63.77 49.03 81.63 57.53\nwac 66.38 52.25 83.8 61.47\nlanwac 63.93 49.025 82.65 57.73\nwac-aoa 66.79 52.75 84.05 60.44\nTable 2: Experiment 2 results for the visdial task: base-\nline RoBERTa embedding, Lancaster norms (lan),WAC\nvectors, and concatenated (lanwac), not frozen, and\nonly frozen for 2 epochs (bottom section).\npothesize that RoBERTa will improve with WAC\nembeddings, as well as the Lancaster concatenated\nto WAC (denoted lanwac), though the Lancaster\nembedding on its own may be too small to make a\ndifference. As words that are learned earlier in a\nchild’s life are generally more concrete, we hypoth-\nesize that RoBERTa will improve whenWAC only\nuses words from the AoA data as more abstract\nterms are represented by zero vectors.\nResults Table 2 shows the results for the visdial\ntask. Though it is clear that RoBERTa is doing the\nheavy lifting, when added to RoBERTa, the Lan-\ncaster and WAC vectors show improvements over\nthe RoBERTa baseline for some metrics. As noted\nin Murahari et al. (2019), the NDCG metric is ac-\ntually counter to MRR, but is important because it\ntakes multiple dialogue response annotations into\naccount. For cases where the Lancaster and WAC\nmodels yield better performance, these results sug-\ngest that a pre-trained language model can make\nuse of adding multimodal knowledge in the form of\nvectors derived from multimodal knowledge (Lan-\ncaster) and visual (WAC) for the visdial task.\nRoBERTA that uses the WAC embedding espe-\ncially shows respectable results in the visdial task,\nparticularly when the embedding uses the AoA\nvocabulary (we only considered AoA for WAC be-\ncause WAC peformed better than lanwac in this ex-\nperiment). The WAC vectors were trained on very\nnoisy data, yet despite the noise and the parsimo-\nnious model, there is some information about the\nvisual world that enriches the baseline model. The\nvariant trained with frozen weights for 2 epochs,\nthen unfrozen for the remaining 18 epochs had re-\nspectable performance across all metrics.7\n7As a sanity check, we also evaluated using randomly\ngenerated embeddings which performed only slightly worse\n6 Conclusion\nThe main contribution of this paper is to explore\nusing the Lancaster Sensorimotor Norms and the\nWords-as-Classiﬁers model as vectorized knowl-\nedge from the physical world on the GLUE and\nVisual Dialog tasks. Lancaster norms performed\nwell on their own in one GLUE task compared\nto other word embeddings like GloVe, and cou-\npled with the WAC vectors as the embedding in\nan ELECTRA model, they performed respectably\non the GLUE task. The WAC vectors, when used\nas embeddings in the RoBERTa model performed\nwell on the Visual Dialog task, particularly when\nthe vocabulary was more restricted to the Age of\nAcquisition vocabulary. Crucially, this work differs\nfrom other visually grounded models because the\ngrounded knowledge is part of the language model\nitself (i.e., the embeddings) rather than computed\nin parallel and added for a task-speciﬁc purpose.\nMoreover, standard language models cannot actu-\nally identify denotations when they are present; i.e.,\nELECTRA and RoBERTa are not actually capable\nof determining if an object is red or soft from ob-\nserving that object–a basic ability for a language\nlearning child–simply because those models cannot\nobserve the world outside of text, though the pur-\npose of the WAC (and models like VilBERT) model\nis to do just that: identify denotations; by coupling\nWAC with ELECTRA and RoBERTa, both models\ncan make use of that capability.\nThis work is critical in our ongoing efforts to-\nwards a model that learns language in a co-located\nsetting in an embodied platform. In particular,\nour knowledge from this paper informs us that the\nELECTRA model with embeddings tied to WAC\nclassiﬁer weights is a good candidate for live in-\nteraction of a robot that is learning words from a\nhuman collaborator because the ELECTRA-WAC\nmodel can function with small amounts of data and\nthe embedding layer can successfully be tied to\nweights of the WAC classiﬁers. We leave imple-\nmentation and evaluation of this model on a robotic\nplatform for future work.\nAcknowledgements Thanks to the anony-\nmous reviewers whose comments really helped\nstrengthen the paper. Also thanks to NVIDIA\nfor donating that GPU that was used for the\nexperiments.\nthan baseline when frozen for 2 epochs, but the results of\nwac-aoa are signiﬁcantly better.\n156\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: Visual question an-\nswering. In Proceedings of the IEEE International\nConference on Computer Vision.\nLawrence W Barsalou. 2008. Grounded Cognition.\nAnnual Review of Psychology, (59):617–645.\nEmily M Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On Meaning, Form, and Under-\nstanding in the Age of Data. In Association for Com-\nputational Linguistics, pages 5185–5198.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020.\nExperience Grounds Language. arXiv.\nValentina Borghesani, Marco Buiatti, Evelyn Eger, and\nManuela Piazza. 2019. Conceptual and Percep-\ntual Dimensions of Word Meaning Are Recovered\nRapidly and in Parallel during Reading. Journal of\nCognitive Neuroscience, 31(1):95–108.\nAnna M Borghi, Laura Barca, Ferdinand Binkofski,\nCristiano Castelfranchi, Giovanni Pezzulo, and Luca\nTummolini. 2019. Words as social tools: Language,\nsociality and inner grounding in abstract concepts.\nPhys. Life Rev., 29:120–153.\nAngelo Cangelosi and Matthew Schlesinger. 2015. De-\nvelopmental robotics: From babies to robots . MIT\npress.\nEve V Clark. 2013. First language acquisition. Cam-\nbridge University Press.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators.\nJeff Da and Jungo Kasai. 2019. Cracking the Contex-\ntual Commonsense Code: Understanding Common-\nsense Reasoning Aptitude of Deep Contextual Rep-\nresentations. In Proceedings of the First Workshop\non Commonsense Inference in Natural Language\nProcessing. Association for Computational Linguis-\ntics.\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi\nSingh, Deshraj Yadav, Stefan Lee, Jose M.F. Moura,\nDevi Parikh, and Dhruv Batra. 2019. Visual Dialog.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 41(5).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding.\nEzequiel A Di Paolo, Elena Clare Cuffari, and Hanne\nDe Jaegher. 2018. Linguistic bodies: The continuity\nbetween life and language. Mit Press.\nPaul Dourish. 2001. Where the Action Is: The Founda-\ntions of Embodied Interaction. Where the action is\nthe foundations of embodied interaction.\nMaxwell Forbes, Ari Holtzman, Yejin Choi, and\nG Allen. 2019. Do Neural Language Representa-\ntions Learn Physical Commonsense? arXiv.\nFritz G ¨unther, Carolin Dudschig, and Barbara Kaup.\n2018. Symbol Grounding Without Direct Experi-\nence: Do Words Inherit Sensorimotor Activation\nFrom Purely Linguistic Context? Cognitive Science,\n42:336–374.\nStevan Harnad. 1990. The symbol grounding prob-\nlem. Physica D: Nonlinear Phenomena , 42(1-\n3):335–346.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V Le, Yunhsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up visual\nand Vision-Language representation learning with\nnoisy text supervision.\nMark Johnson. 2008. The meaning of the body: Aes-\nthetics of human understanding . University of\nChicago Press.\nCasey Kennington and David Schlangen. 2015. Sim-\nple Learning and Compositional Application of Per-\nceptually Grounded Word Meanings for Incremen-\ntal Reference Resolution. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 292–301, Beijing,\nChina. Association for Computational Linguistics.\nDonghyun Kim, Kuniaki Saito, Kate Saenko, Stan\nSclaroff, and Bryan A. Plummer. 2019. MULE:\nMultimodal Universal Language Embedding.\nJamie Ryan Kiros, William Chan, and Geoffrey E Hin-\nton. 2018. Illustrative Language Understanding:\nLarge-Scale Visual Grounding with Image Search.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Long Pa-\npers), pages 922–933, Melbourne, Australia. Associ-\nation for Computational Linguistics.\nRyan Kiros, Ruslan Salakhutdinov, and Richard S\nZemel. 2014. Unifying Visual-Semantic Embed-\ndings with Multimodal Neural Language Models. In\narXiv preprint arXiv:1411.2539, pages 1–13.\nVictor Kuperman, Hans Stadthagen-Gonzalez, and\nMarc Brysbaert. 2012a. Age-of-acquisition ratings\nfor 30,000 English words. Behavior Research Meth-\nods, 44(4):978–990.\n157\nVictor Kuperman, Hans Stadthagen-Gonzalez, and\nMarc Brysbaert. 2012b. Age-of-acquisition ratings\nfor 30,000 english words. Behav. Res. Methods ,\n44(4):978–990.\nGeorge Lakoff and Mark Johnson. 1999. Philosophy\nin the ﬂesh: The embodied mind and its challenge\nto western thought , volume 640. Basic books New\nYork.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho Jui\nHsieh, and Kai Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\narXiv.\nYuhan Liu, Saurabh Agarwal, and Shivaram Venkatara-\nman. 2021. AutoFreeze: Automatically freezing\nmodel blocks to accelerate ﬁne-tuning.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Ste-\nfan Lee. 2019. ViLBERT: Pretraining Task-\nAgnostic Visiolinguistic Representations for Vision-\nand-Language Tasks.\nDermot Lynott, Louise Connell, Marc Brysbaert,\nJames Brand, and James Carney. 2019. The Lan-\ncaster Sensorimotor Norms: multidimensional mea-\nsures of perceptual and action strength for 40,000\nEnglish words. Behavior Research Methods, pages\n1–21.\nLorraine McCune. 2008. How Children Learn to Learn\nLanguage. Oxford University Press.\nDaniele Moro, Stacy Black, and Casey Kennington.\n2019. Composing and embedding the words-as-\nclassiﬁers model of grounded semantics.\nVishvak Murahari, Dhruv Batra, Devi Parikh, and Ab-\nhishek Das. 2019. Large-scale pretraining for visual\ndialog: A simple state-of-the-art baseline. arXiv\npreprint arXiv:1912.02379.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 1532–1543.\nMarta Ponari, Courtenay Frazier Norbury, and\nGabriella Vigliocco. 2018. Acquisition of abstract\nconcepts is inﬂuenced by emotional valence. Dev.\nSci., 21(2).\nFriedemann Pulverm ¨uller. 1999. Words in the brain’s\nlanguage. Behavioral and Brain Sciences.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What we know\nabout how BERT works. arXiv.\nDavid Schlangen, Sina Zarriess, and Casey Kenning-\nton. 2016. Resolving References to Objects in Pho-\ntographs using the Words-As-Classiﬁers Model. In\nProceedings of the 54th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1213–\n1223.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for auto-\nmatic image captioning. In ACL 2018 - 56th Annual\nMeeting of the Association for Computational Lin-\nguistics, Proceedings of the Conference (Long Pa-\npers).\nL B Smith and L Samuelson. 2009. Objects in Space\nand Mind: From Reaching to Words. In The Spatial\nFoundations of Language and Cognition.\nLinda Smith and Michael Gasser. 2005. The Devel-\nopment of Embodied Cognition: Six Lessons from\nBabies. Artiﬁcial Life, (11):13–29.\nKatsumi Takano and Akira Utsumi. 2016. Grounded\nDistributional Semantics for Abstract Words.\nCogSci, pages 2171–2176.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355.\nZhaofeng Wu, Hao Peng, Noah A Smith, and Paul G\nAllen. 2021. Infusing Finetuning with Semantic De-\npendencies. Transactions of ACL.\nRowan Zellers, Ari Holtzman, Matthew Peters,\nRoozbeh Mottaghi, Aniruddha Kembhavi, Ali\nFarhadi, and Yejin Choi. 2021. PIGLeT: Language\ngrounding through Neuro-Symbolic interaction in a\n3D world.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE In-\nternational Conference on Computer Vision."
}