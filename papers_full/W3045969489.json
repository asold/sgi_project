{
  "title": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
  "url": "https://openalex.org/W3045969489",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4223083849",
      "name": "Delbrouck, Jean-Benoit",
      "affiliations": [
        "University of Mons"
      ]
    },
    {
      "id": "https://openalex.org/A4226617042",
      "name": "Tits, Noé",
      "affiliations": [
        "University of Mons"
      ]
    },
    {
      "id": "https://openalex.org/A5094330910",
      "name": "Brousmiche Mathilde",
      "affiliations": [
        "University of Mons"
      ]
    },
    {
      "id": "https://openalex.org/A2755191438",
      "name": "Dupont, Stéphane",
      "affiliations": [
        "University of Mons"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2191779130"
  ],
  "abstract": "Understanding expressed sentiment and emotions are two crucial factors in\\nhuman multimodal language. This paper describes a Transformer-based\\njoint-encoding (TBJE) for the task of Emotion Recognition and Sentiment\\nAnalysis. In addition to use the Transformer architecture, our approach relies\\non a modular co-attention and a glimpse layer to jointly encode one or more\\nmodalities. The proposed solution has also been submitted to the ACL20: Second\\nGrand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI\\ndataset. The code to replicate the presented experiments is open-source:\\nhttps://github.com/jbdel/MOSEI_UMONS.\\n",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1–7\nSeattle, USA, July5 - 10, 2020.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n1\nA Transformer-based joint-encoding for Emotion Recognition and\nSentiment Analysis\nJean-Benoit Delbrouck and No´e Tits and Mathilde Brousmiche and St´ephane Dupont\nInformation, Signal and Artiﬁcial Intelligence Lab\nUniversity of Mons, Belgium\n{jean-benoit.delbrouck, noe.tits, mathilde.brousmiche, stephane.dupont}@umons.ac.be\nAbstract\nUnderstanding expressed sentiment and emo-\ntions are two crucial factors in human mul-\ntimodal language. This paper describes a\nTransformer-based joint-encoding (TBJE) for\nthe task of Emotion Recognition and Senti-\nment Analysis. In addition to use the Trans-\nformer architecture, our approach relies on a\nmodular co-attention and a glimpse layer to\njointly encode one or more modalities. The\nproposed solution has also been submitted\nto the ACL20: Second Grand-Challenge on\nMultimodal Language to be evaluated on the\nCMU-MOSEI dataset. The code to replicate\nthe presented experiments is open-source 1.\n1 Introduction\nPredicting affective states from multimedia is a\nchallenging task. Emotion recognition task has\nexisted working on different types of signals,\ntypically audio, video and text. Deep Learning\ntechniques allow the development of novel\nparadigms to use these different signals in one\nmodel to leverage joint information extraction\nfrom different sources. This paper aims to bring a\nsolution based on ideas taken from Machine Trans-\nlation (Transformers, Vaswani et al. (2017)) and\nVisual Question Answering (Modular co-attention,\nYu et al. (2019)). Our contribution is not only\nvery computationally efﬁcient, it is also a viable\nsolution for Sentiment Analysis and Emotion\nRecognition. Our results can compare with, and\nsometimes surpass, the current state-of-the-art for\nboth tasks on the CMU-MOSEI dataset (Zadeh\net al., 2018b).\nThis paper is structured as follows: ﬁrst, in sec-\ntion 2, we quickly go over the related work that\nhave been evaluated on the MOSEI dataset, we\n1https://github.com/jbdel/MOSEI_UMONS\nthen proceed to describe our model in Section 3,\nwe then explain how we extract our modality fea-\ntures from raw videos in Section 4 and ﬁnally, we\npresent the dataset used for our experiments and\ntheir respective results in section 5 and 6.\n2 Related work\nOver the years, many creative solutions have been\nproposed by the research community in the ﬁeld of\nSentiment Analysis and Emotion Recognition. In\nthis section, we proceed to describe different mod-\nels that have been evaluated on the CMU-MOSEI\ndataset. To the best of our knowledge, none of\nthese ideas uses a Tansformer-based solution.\nThe Memory Fusion Network (MFN, Zadeh\net al. (2018a)) synchronizes multimodal sequences\nusing a multi-view gated memory that stores\nintraview and cross-view interactions through\ntime.\nGraph-MFN (Zadeh et al., 2018b) consists of a\nDynamic Fusion Graph (DFG) built upon MFN.\nDFG is a fusion technique that tackles the nature\nof cross-modal dynamics in multimodal language.\nThe fusion is a network that learns to models the\nn-modal interactions and can dynamically alter its\nstructure to choose the proper fusion graph based\non the importance of each n-modal dynamics\nduring inference.\nSahay et al. (2018) use Tensor Fusion Network\n(TFN), i.e. an outer product of the modalities.\nThis operation can be performed either on a whole\nsequence or frame by frame. The ﬁrst one lead\nto an exponential increase of the feature space\nwhen modalities are added that is computationally\nex-pensive. The second approach was thus\npreferred. They showed an improvement over an\n2\nearly fusion baseline.\nRecently, Shenoy and Sardana (2020) pro-\npose a solution based on a context-aware RNN,\nMultilogue-Net, for Multi-modal Emotion Detec-\ntion and Sentiment Analysis in conversation.\n3 Model\nThis section aims to describe the two model\nvariants evaluated in our experiment: a monomodal\nvariant and a multimodal variant. The monomodal\nvariant is used to classify emotions and sentiments\nbased solely on L (Linguistic), on V (Visual) or on\nA (Acoustic). The multimodal version is used for\nany combination of modalities.\nOur model is based on the Transformer model\n(Vaswani et al., 2017), a new encoding architecture\nthat fully eschews recurrence for sequence\nencoding and instead relies entirely on an attention\nmechanism and Feed-Forward Neural Networks\n(FFN) to draw global dependencies between\ninput and output. The Transformer allows for\nsigniﬁcantly more parallelization compared to the\nRecurrent Neural Network (RNN) that generates\na sequence of hidden states ht, as a function of\nthe previous hidden state ht−1 and the input for\nposition t.\n3.1 Monomodal Transformer Encoding\nThe monomodal encoder is composed of a stack of\nB identical blocks but with their own set of training\nparameters. Each block has two sub-layers. There\nis a residual connection around each of the two sub-\nlayers, followed by layer normalization (Ba et al.,\n2016). The output of each sub-layer can be written\nlike this:\nLayerNorm(x + Sublayer(x)) (1)\nwhere Sublayer(x) is the function implemented by\nthe sub-layer itself. In traditional Transformers,\nthe two sub-layers are respectively a multi-head\nself-attention mechanism and a simple Multi-Layer\nPerceptron (MLP).\nThe attention mechanism consists of a Key K\nand Query Q that interacts together to output a\nattention map applied to Context C:\nAttention(Q, K, C) =softmax(QK⊤\n√\nk\n)C (2)\nIn the case of self-attention, K, Q and C are the\nsame input. If this input is of size N ×k, the op-\neration QK⊤results in a squared attention matrix\ncontaining the afﬁnity between each row N. Ex-\npression\n√\nk is a scaling factor. The multi-head\nattention (MHA) is the idea of stacking several self-\nattention attending the information from different\nrepresentation sub-spaces at different positions:\nMHA(Q, K, C) =Concat(head1, ...,headh)Wo\nwhere headi = Attention(QWQ\ni , KWK\ni , CWC\ni )\n(3)\nA subspace is deﬁned as slice of the feature di-\nmension k. In the case of four heads, a slice would\nbe of size k\n4 . The idea is to produce different sets of\nattention weights for different feature sub-spaces.\nAfter encoding through the blocks, output ˜x can be\nused by a projection layer for classiﬁcation. In Fig-\nure 1, x can be any modality feature as described\nin Section 4.\nx\n×B\nMulti-Head A.\nAdd & Norm\nMLP\nAdd & Norm\n˜x\nFigure 1: Monomodal Transformer encoder.\n3.2 Multimodal Transformer Encoding\nThe idea of a multimodal transformer consists\nin adding a dedicated transformer (section 3.1)\nfor each modality we work with. While our\ncontribution follows this procedure, we also\npropose three ideas to enhance it: a joint-encoding,\na modular co-attention (Yu et al., 2019) and a\nglimpse layer at the end of each block.\nThe modular co-attention consists of modulating\nthe self-attention of a modality, let’s call ity, by a\nprimary modality x. To do so, we switch the key\nK and context C of the self-attention from y to\n3\nx. The operation QK⊤results in an attention map\nthat acts like an afﬁnity matrix between the rows of\nmodality matrix x and y. This computed alignment\nis applied over the context C (now x) and ﬁnally\nwe add the residual connection y. The following\nequation describes the new attention sub-layer:\ny = LayerNorm(y + MHA(y, x, x)) (4)\nIn this scenario, for the operation QK⊤to work\nas well as the residual connection (the addition),\nthe feature sizes of x and y must be equal. This can\nbe adjusted with the different transformation matri-\nces of the MHA module. Because the encoding is\njoint, each modality is encoded at the same time\n(i.e. we don’t unroll the encoding blocks for one\nmodality before moving on to another modality).\nThis way, the MHA attention of modality y for\nblock b is done by the representation ofx at block b.\nFinally, we add a last layer at the end of each\nmodality block, called the glimpse layer, where the\nmodality is projected in a new space of representa-\ntion. A glimpse layer consists of stacking G soft\nattention layers and stacking their outputs. Each\nsoft attention is seen as a glimpse. Formally, we\ndeﬁne the soft attention (SoA) i with input matrix\nM ∈RN×k by a MLP and a weighted sum:\nai = softmax(va\ni\n⊤(WmM))\nSoAi(M) =mi =\nN∑\nj=0\naijMj\n(5)\nwhere Wm if a transformation matrix of size2k×k,\nva\ni is of size 1 ×2k and mi a vector of size k. Then\nwe can deﬁne the glimpse mechanism for matrixM\nof glimpse size Gm as the stacking of all glimpses:\nGM = Stacking(m1, . . . , mGm)\nNote that before the parameter Wm, whose role\nis to embed the matrix M in a higher dimension,\nis shared between all glimpses (this operation is\ntherefore only computed once) while the set of\nvectors {va\ni }computing the attention weights from\nthis bigger space is dedicated for each glimpse.\nIn our contribution, we always chose Gm = N\nso the sizes allow us to perform a ﬁnal residual\nconnections M = LayerNorm(M + GM ).\nx\n×B\nMulti-Head A.\nAdd & Norm\nMLP\nAdd & Norm\nGlimpse\nAdd & Norm\n˜x\ny\n×B\nMulti-Head A.\nAdd & Norm\nMLP\nAdd & Norm\nGlimpse\nAdd & Norm\n˜y\nFigure 2: Multimodal Transformer Encoder for two\nmodalities with joint-encoding.\nThe Figure 2 depicts the encoding for two\nfeatures where modality x is modulating the\nmodality y. This encoding can be ported to\nany number of modalities by duplicating the\narchitecture. In our case, it is always the linguistic\nmodality that modulates the others.\n3.3 Classiﬁcation layer\nAfter all the Transformer blocks were computed, a\nmodality goes into a ﬁnal glimpse layer of size 1.\nThe result is therefore only one vector. The vectors\nof each modality are summed element-wise, let’s\ncall the results of this sum s, and are then projected\nover possible answers according to the following\nequation:\ny ∼p = Wa(LayerNorm(s)) (6)\nIf there is only one modality, the sum operation\nis omitted.\n4 Feature extractions\nThis section aims to explain how we pre-compute\nthe features for each modality. These features are\nthe inputs of the Transformer blocks. Note that the\nfeatures extraction is done independently for each\nexample of the dataset.\n4.1 Linguistic\nEach utterance is tokenized and lowercase. We also\nremove special characters and punctuation. We\n4\nbuild our vocabulary against the train-set and end\nup with a glossary of 14.176 unique words. We\nembed each word in a vector of 300 dimensions\nusing GloVe (Pennington et al., 2014). If a word\nfrom the validation or test-set is not in present our\nvocabulary, we replace it with the unknown token\n”unk”.\n4.2 Acoustic\nThe acoustic part of the signal of the video contains\na lot of speech. Speech is used in conversations\nto communicate information with words but also\ncontains a lot of information that are non linguistic\nsuch as nonverbal expressions (laughs, breaths,\nsighs) and prosody features (intonation, speaking\nrate). These are important data in an emotion\nrecognition task.\nAcoustic features widely use in the speech\nprocessing ﬁeld such as F0, formants, MFCCs,\nspectral slopes consist of handcrafted sets of\nhigh-level features that are useful when an\ninterpretation is needed, but generally discard a lot\nof information. Instead, we decide to use low-level\nfeatures for speech recognition and synthesis, the\nmel-spectrograms. Since the breakthrough of\ndeep learning systems, the mel-spectrograms have\nbecome a suitable choice.\nThe spectrum of a signal is obtained with\nFourier analysis that decompose a signal in a sum\nof sinusoids. The amplitudes of the sinusoids\nconstitute the amplitude spectrum. A spectrogram\nis the concatenation over time of spectra of\nwindows of the signal. Mel-spectrogram is\na compressed version of spectrograms, using\nthe fact the human ear is more sensitive to\nlow frequencies than high frequencies. This\nrepresentation thus attributes more resolution\nfor low frequencies than high frequencies using\nmel ﬁlter banks. A mel-spectrogram is typically\nused as an intermediate step for text-to-speech\nsynthesis (Tachibana et al., 2018) in state-of-the-art\nsystems as audio representation, so we believe it is\na good compromise between dimensionality and\nrepresentation capacity.\nOur mel-spectrograms were extracted with the\nsame procedure as in (Tachibana et al., 2018) with\nlibrosa (McFee et al., 2015) library with 80 ﬁlter\nbanks (the embedding size is therefore 80). A tem-\nporal reduction by selecting one frame every 16\nframes was the applied.\n4.3 Visual\nInspired by the success of convolutional neural\nnetworks (CNNs) in different tasks, we chose to\nextract visual features with a pre-trained CNN.\nCurrent models for video classiﬁcation use CNNs\nwith 3D convolutional kernels to process the\ntemporal information of the video together with\nspatial information (Tran et al., 2015). The 3D\nCNNs learn spatio-temporal features but are\nmuch more expensive than 2D CNNs and prone\nto overﬁtting. To reduce complexity, Tran et al.\n(2018) explicitly factorizes 3D convolution into\ntwo separate and successive operations, a 2D\nspatial convolution and a 1D temporal convolution.\nWe chose this model, named R(2+1)D-152, to\nextract video features for the emotion recognition\ntask. The model is pretrained on Sports-1M and\nKinetics.\nThe model takes as input a clip of 32 RGB\nframes of the video. Each frame is scaled to the\nsize of 128 x 171 and then cropped a window\nof size 112 x 112. The features are extracted by\ntaking the output of the spatiotemporal pooling.\nThe feature vector for the entire video is obtained\nby sliding a window of 32 RGB frames with a\nstride of 8 frames.\nWe chose not to crop out the face region of the\nvideo and keep the entire image as input to the\nnetwork. Indeed, the video is already centered on\nthe person and we expect that the movement of the\nbody such as the hands can be a good indicator\nfor the emotion recognition and sentiment analysis\ntasks.\n5 Dataset\nWe test our joint-encoding solution on a novel\ndataset for multimodal sentiment and emotion\nrecognition called CMU-Multimodal Opinion\nSentiment and Emotion Intensity (CMU-MOSEI,\nZadeh et al. (2018b)). It consists of 23,453\nannotated sentences from 1000 distinct speakers.\nEach sentence is annotated for sentiment on a\n[-3,3] scale from highly negative (-3) to highly\npositive (+3) and for emotion by 6 classes :\nhappiness, sadness, anger, fear, disgust, surprise.\nIn the scope of our experiment, the emotions are\n5\nTest set Sentiment Emotions\n2-class 7-class Happy Sad Angry Fear Disgust Surprise\nA A A F1 A F1 A F1 A F1 A F1 A F1\nL+ A + V 81.5 44.4 65.0 64.0 72.0 67.9 81.6 74.7 89.1 84.0 85.9 83.6 90.5 86.1\nL + A 82.4 45.5 66.0 65.5 73.9 67.9 81.9 76.0 89.2 87.2 86.5 84.5 90.6 86.1\nL 81.9 44.2 64.5 63.4 72.9 65.8 81.4 75.3 89.1 84.0 86.6 84.5 90.5 81.4\nMu-Net 82.1 - - 68.4 - 74.5 - 80.9 - 87.0 - 87.3 - 80.9\nG-MFN 76.9 45.0 - 66.3 - 66.9 - 72.8 - 89.9 - 76.6 - 85.5\nTable 1: Results on the test-set. Note that the F1-scores for emotions are weighted to be consistent with the previous\nstate-of-the-art. Also, we do not compare accuracies for emotions, as previous works use a weighted variant while\nwe use standard accuracy. G-MFN is the Graph-MFN model and Mu-Net is the Multilogue-Net model.\neither present or not present (binary classiﬁcation),\nbut two emotions can be present at the same time,\nmaking it a multi-label problem.\nFigure 3: MOSEI statistics, taken from the author’s pa-\nper.\nThe Figure 3 shows the distribution of sentiment\nand emotions in CMU-MOSEI dataset. The dis-\ntribution shows a natural skew towards more fre-\nquently used emotions. The most common cate-\ngory is happiness with more than 12,000 positive\nsample points. The least prevalent emotion is fear\nwith almost 1900 positive sample. It also shows a\nslight shift in favor of positive sentiment.\n6 Experiments\nIn this section, we report the results of our model\nvariants described in Section 3. We ﬁrst explain\nour experimental setting.\n6.1 Experimental settings\nWe train our models using the Adam optimizer\n(Kingma and Ba, 2014) with a learning rate of\n1e −4 and a mini-batch size of 32. If the accuracy\nscore on the validation set does not increase for\na given epoch, we apply a learning-rate decay\nof factor 0.2. We decay our learning rate up to\n2 times. Afterwards, we use an early-stop of 3\nepochs. Results presented in this paper are from\nthe averaged predictions of 5 models.\nUnless stated otherwise, we use 6 Transformer\nblocks of hidden-size 512, regardless of the\nmodality encoded. The self-attention has 4\nmulti-heads and the MLP has one hidden layer of\n1024. We apply dropout of 0.1 on the output of\neach block (equation 4) and of 0.5 on the input of\nthe classiﬁcation layer (s in equation 6).\nFigure 4: Temporal dimension (i.e. rows in our feature\nmatrices) for the acoustic and visual modality.\nFor the acoustic and visual features, we truncate\nthe features for spatial dimensions above 40. We\nalso use that number for the number of glimpses.\nThis choice is made base on Figure 4\n6.2 Results\nThe Table 1 show the scores of our different\nmodality combinations. We do not compare\naccuracies for emotions with previous works as\nthey used a weighted accuracy variant while we\nuse standard accuracy.\nWe notice that our L+A (linguistic + acoustic) is\nthe best model. Unfortunately, adding the visual\ninput did not increase the results, showing that\nit is still the most difﬁcult modality to integrate\ninto a multimodal pipeline. For the sentiment\ntask, the improvement is more tangible for the\n7-class, showing that our L+A model learns better\n6\nrepresentations for more complex classiﬁcation\nproblems compared to our monomodal model L\nusing only the linguistic input. We also surpass\nthe previous state-of-the-art for this task. For the\nemotions, we can see that Multilogue-Net gives\nbetter prediction for some classes, such as happy,\nsad, angry and disgust. We postulate that this is\nbecause Multilogue is a context-aware method\nwhile our model does not take into account the\nprevious or next sentence to predict the current\nutterance. This might affect our accuracy and\nf1-score on the emotion task.\nThe following Table 2 depicts the results of our\nsolution sent to the Second Grand-Challenge on\nMultimodal Language. It has been evaluated on\nthe private test-fold released for the challenge and\ncan serve as a baseline for future research. Note\nthat in this table, the F1-scores are unweighted, as\nshould be future results for a fair comparison and\ninterpretation of the results.\nSentiment 7-class\nL + A (A) 40.20\nEmotion Happy Sad Angry\nL + A (A) 67.07 82.66 81.65\nL + A (F1) 78.08 31.42 28.38\nEmotion Fear Disgust Surprise\nL + A (A) 88.19 79.14 90.45\nL + A (F1) 26.66 25.49 15.82\nTable 2: Results on the private test-fold for 7-class sen-\ntiment problem and for each emotion. Accuracy is de-\nnoted by A. In this table, the F1-scores are unweighted,\nunlike Table 1.\n7 Discussions\nWe presented a computationally efﬁcient and\nrobust model for Sentiment Analysis and Emotion\nRecognition evaluated on CMU-MOSEI. Though\nwe showed strong results on accuracy, we can see\nthat there is still a lot of room for improvement on\nthe F1-scores, especially for the emotion classes\nthat are less present in the dataset. To the best\nof our knowledge, the results presented by our\ntransformer-based joint-encoding are the strongest\nscores for the sentiment task on the dataset.\nThe following list identiﬁes other features we\nFigure 5: 7-class sentiment accuracy according to the\nnumber of blocks per Transformer.\ncomputed as input for our model that lead to weaker\nperformances:\n•We tried the OpenFace 2.0 features (Baltru-\nsaitis et al., 2018). This strategy computes\nfacial landmark, the features are specialized\nfor facial behavior analysis;\n•We tried a simple 2D CNN named DenseNet\n(Huang et al., 2017). For each frame of the\nvideo, a feature vector is extracted by taking\nthe output of the average pooling layer;\n•We tried different values for the number of\nmel ﬁlter bank (512 and 1024) and temporal\nreduction (1, 2, 4 and 8 frames), we also tried\nto use the full spectrogram;\n•We tried not using the GloVe embedding.\n8 Acknowledgements\nNo´e Tits is funded through a FRIA grant (Fonds\npour la Formation `a la Recherche dans l’Industrie\net l’Agriculture, Belgium).\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nTadas Baltrusaitis, Amir Zadeh, Yao Chong Lim, and\nLouis-Philippe Morency. 2018. Openface 2.0: Fa-\ncial behavior analysis toolkit. In 13th IEEE Inter-\nnational Conference on Automatic Face & Gesture\nRecognition, pages 59–66. IEEE.\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and\nKilian Q Weinberger. 2017. Densely connected con-\nvolutional networks. In Proceedings of the IEEE\n7\nConference on Computer Vision and Pattern Recog-\nnition, pages 4700–4708.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nBrian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Ni-\neto. 2015. librosa: Audio and music signal analysis\nin python. In Proceedings of the 14th python in sci-\nence conference, pages 18–25.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP, volume 14, pages 1532–\n1543.\nSaurav Sahay, Shachi H Kumar, Rui Xia, Jonathan\nHuang, and Lama Nachman. 2018. Multimodal\nrelational tensor network for sentiment and emo-\ntion classiﬁcation. In Proceedings of Grand Chal-\nlenge and Workshop on Human Multimodal Lan-\nguage (Challenge-HML), pages 20–27, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nAman Shenoy and Ashish Sardana. 2020. Multilogue-\nnet: A context aware rnn for multi-modal emotion\ndetection and sentiment analysis in conversation.\nHideyuki Tachibana, Katsuya Uenoyama, and Shun-\nsuke Aihara. 2018. Efﬁciently trainable text-to-\nspeech system based on deep convolutional net-\nworks with guided attention. In 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 4784–4788. IEEE.\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-\nresani, and Manohar Paluri. 2015. Learning spa-\ntiotemporal features with 3d convolutional networks.\nIn Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 4489–4497.\nDu Tran, Heng Wang, Lorenzo Torresani, Jamie Ray,\nYann LeCun, and Manohar Paluri. 2018. A closer\nlook at spatiotemporal convolutions for action recog-\nnition. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) ,\npages 6450–6459.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and\nQi Tian. 2019. Deep modular co-attention networks\nfor visual question answering. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 6281–6290.\nAmir Zadeh, Paul Pu Liang, Navonil Mazumder,\nSoujanya Poria, Erik Cambria, and Louis-Philippe\nMorency. 2018a. Memory fusion network for multi-\nview sequential learning. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nAmirAli Zadeh, Paul Pu Liang, Soujanya Poria, Erik\nCambria, and Louis-Philippe Morency. 2018b. Mul-\ntimodal language analysis in the wild: CMU-\nMOSEI dataset and interpretable dynamic fusion\ngraph. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2236–2246, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I130929987",
      "name": "University of Mons",
      "country": "BE"
    }
  ],
  "cited_by": 89
}