{
  "title": "Centering health equity in large language model deployment",
  "url": "https://openalex.org/W4387902603",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2104200736",
      "name": "Nina Singh",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2627153291",
      "name": "Katharine Lawrence",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2131018434",
      "name": "Safiya Richardson",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2133691267",
      "name": "Devin M Mann",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2104200736",
      "name": "Nina Singh",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2627153291",
      "name": "Katharine Lawrence",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2131018434",
      "name": "Safiya Richardson",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2133691267",
      "name": "Devin M Mann",
      "affiliations": [
        "NYU Langone Health",
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319062614",
    "https://openalex.org/W4324373918",
    "https://openalex.org/W4363676214",
    "https://openalex.org/W2149533068",
    "https://openalex.org/W3087893815",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2901610527",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W3170344956",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4286233477",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4323545986",
    "https://openalex.org/W4283166427",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W3159623667",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4388488349",
    "https://openalex.org/W4387772689",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Early buzz around ChatGPT's [1] proficiency at healthcare tasks [2][3][4] has turned into rapid deployment of large language model (LLM) technology across healthcare.Multiple healthcare systems including UC San Diego Health, Stanford Health Care, and University of Wisconsin Health are already piloting the use of GPT-4 to draft responses to patient messages [5].Others, including The University of Kansas Health System, are piloting similar LLMs to support clinical documentation [6].Early conversations about LLMs in healthcare tended towards extremes, either lauding their breathtaking potential or lamenting the threat they pose to human-centric care.More recently, nuanced narratives regarding the ethical implications of LLMs have started to emerge [7,8].Still missing from the conversation, though, is a grounded discussion of the pragmatic equity implications of LLM deployment in real-world contexts.In this perspective, we explore various health equity risks generated by the practical use of LLMs in healthcare and offer risk mitigation recommendations.We define health equity using Dr. Braveman's definition: the \"absence of differences in health between the most advantaged group in a given category and all others\" [9].Equity risks originating in the development of LLMs are widely discussed, with substantial dialogue dedicated to intrinsic equity and general ethical considerations of how, why, and with what data and algorithmic design choices these models should be developed [10,11].Equity risks in AI model deployment are less commonly discussed.These emergent risks arise from pragmatic use by end users (patients, clinicians, and healthcare systems) in real-world settings.It is critical to understand emergent equity risks of LLMs to avoid replicating inequities that resulted from the use of other technologies in medicine [12].For patients, LLMs have a remarkable ability to generate human-like responses and summarize complex or hard-to-find information [2,4], providing automated care with potential to alleviate disparities in access, understandability, and personalization.However, if minoritized populations who have experienced barriers to accessing traditional care are disproportionately more likely to seek care from a free chatbot rather than a licensed provider this could result in a uniquely biased care experience-with higher data privacy risks [13], biased outputs, misinformation, or personally offensive content from models trained on biased data [14][15][16].This could perpetuate negative care experiences, medical disenfranchisement and distrust.For clinicians, LLM-supported tools offer efficiency gains that may help address issues of burnout and dissatisfaction.For example, DocsGPT [17] automates high-burden, low-cognition tasks such as composing prior authorization letters.However, the choice to use LLMs versus not based on patient language, health literacy, or insurance coverage could widen existing",
  "full_text": "OPINIO N\nCentering health equity in large language\nmodel deployment\nNina Singh\nID\n1\n, Katharine Lawrence\n1\n, Safiya Richardson\n1\n, Devin M. Mann\n1,2\n*\n1 Department of Population Health, New York University Grossman School of Medicine, New York, New\nYork, United States of America, 2 Medical Center Informatio n Technolo gy, New York University Langon e\nHealth, New York, New York, United States of America\n* Devin. Mann@nyulan gone.org\nEarly buzz around ChatGPT’s [1] proficiency at healthcare tasks [2–4] has turned into rapid\ndeployment of large language model (LLM) technology across healthcare. Multiple healthcare\nsystems including UC San Diego Health, Stanford Health Care, and University of Wisconsin\nHealth are already piloting the use of GPT-4 to draft responses to patient messages [5]. Others,\nincluding The University of Kansas Health System, are piloting similar LLMs to support clini-\ncal documentation [6].\nEarly conversations about LLMs in healthcare tended towards extremes, either lauding\ntheir breathtaking potential or lamenting the threat they pose to human-centric care. More\nrecently, nuanced narratives regarding the ethical implications of LLMs have started to emerge\n[7,8]. Still missing from the conversation, though, is a grounded discussion of the pragmatic\nequity implications of LLM deployment in real-world contexts. In this perspective, we explore\nvarious health equity risks generated by the practical use of LLMs in healthcare and offer risk\nmitigation recommendations. We define health equity using Dr. Braveman’s definition: the\n“absence of differences in health between the most advantaged group in a given category and\nall others” [9].\nEquity risks originating in the development of LLMs are widely discussed, with substantial\ndialogue dedicated to intrinsic equity and general ethical considerations of how, why, and with\nwhat data and algorithmic design choices these models should be developed [10,11]. Equity\nrisks in AI model deployment are less commonly discussed. These emergent risks arise from\npragmatic use by end users (patients, clinicians, and healthcare systems) in real-world settings.\nIt is critical to understand emergent equity risks of LLMs to avoid replicating inequities that\nresulted from the use of other technologies in medicine [12].\nFor patients, LLMs have a remarkable ability to generate human-like responses and sum-\nmarize complex or hard-to-find information [2,4], providing automated care with potential to\nalleviate disparities in access, understandability, and personalization. However, if minoritized\npopulations who have experienced barriers to accessing traditional care are disproportionately\nmore likely to seek care from a free chatbot rather than a licensed provider this could result in\na uniquely biased care experience–with higher data privacy risks [13], biased outputs, misin-\nformation, or personally offensive content from models trained on biased data [14–16]. This\ncould perpetuate negative care experiences, medical disenfranchisemen t and distrust.\nFor clinicians, LLM-supported tools offer efficiency gains that may help address issues of\nburnout and dissatisfaction. For example, DocsGPT [17] automates high-burden, low-cogni-\ntion tasks such as composing prior authorization letters. However, the choice to use LLMs ver-\nsus not based on patient language, health literacy, or insurance coverage could widen existing\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000036 7 October 24, 2023 1 / 5\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Singh N, Lawrence K, Richardso n S,\nMann DM (2023) Centering health equity in large\nlanguage model deployme nt. PLOS Digit Health\n2(10): e0000367. https:// doi.org/10.1371 /journal.\npdig.00003 67\nEditor: Mengyu Wang, Harvard University, UNITED\nSTATES\nPublished: October 24, 2023\nCopyright: © 2023 Singh et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nFunding: This study was funded by the US Nationa l\nScience Foundation (https://www .nsf.gov; awards\nno. 1928614 and 2129076 (NS, KL, SR, DMM))\nand the National Heart, Lung, and Blood Institute\n(https://w ww.nhlbi.nih.g ov; award K23HL1451 14\n(SR)). The funders had no role in study design,\ndata collection and analysis , decision to publish, or\npreparation of the manuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\ndisparities. Additionally, misguided clinician use of public LLMs could provide models with\npatient information that can become vulnerable to extraction [13].\nFor healthcare systems, LLM-supported technologies offer large-scale automation of\npatient communication, increasing customer relationship management efficiency and\npotentially improving value-based care metrics; these tools could also enable novel strate-\ngies to address health disparities, such as proactively connecting patients with identified\nsocial vulnerabilities with relevant resources. Unfortunately, competing priorities in care\ndelivery for healthcare systems (eg revenue generation) too often result in the deprioritiza-\ntion of health equity efforts, including in the use of novel technologies. For example, as\nhealthcare systems experiment with charging patients for messaging providers, there is a\nrisk of minoritized patients preferentially turning to search engines and chatbots for medi-\ncal advice. Healthcare systems that adopt widespread direct-to-patient LLM strategies that\nremove clinicians entirely from patient exchanges also risk impersonal, inaccurate, or\nbiased [14–16] communications that would further ostracize minoritized patients from\ntheir providers and the experience of care delivery in general.\nFinally, LLM products that organizations purchase for clinicians will contain behind-\nthe-scenes prompting that shape how equitable responses are. While end users of AI models\nhave traditionally had little power to alter the outputs of the models they’ve used, prompting\nhas changed that dynamic [18]. Prompting enhances the performance of general-purpose\nmodels on healthcare tasks [18–20] and can change model accuracy, safety, and bias [18].\nLLM vendors can thus be split into two categories: 1) LLM creators (who make general pur-\npose LLMs, such as GPT-4) and 2) LLM product developers, who build upon existing LLMs\nto optimize prompts for clinical utility. There is a serious risk of inequitable prompts being\ncreated and used at mass scale as LLM product developers develop and sell their models to\nhealthcare organizations. For example, a recent study found that when prompted with\nsymptoms of high-risk chest pain and a patient’s insurance status, Chat-GPT correctly tri-\naged insured patients to the emergency department but inappropriately suggested that\nuninsured patients either present to a community health center (less costly treatment\nvenue) or the emergency department [21].\nUltimately, emergent equity risks in LLM deployment are complex and may be difficult to\nanticipate as novel use cases arise. Organizations such as the White House and National Acad-\nemy of Medicine are working toward comprehensive frameworks for the ethical use of AI.\nHowever, in the meantime healthcare systems have a responsibility to engage in iterative risk\nmitigation around health equity as they rapidly deploy LLMs. Recommendations for address-\ning emergent equity issues in LLM deployment include:\n1) Thoroughly evaluate LLM products and vendors for equity-\nrelated risks\nEvaluating vendors can be facilitated by equity-informed requests for proposals or other intake\ntools designed to probe for equity-related concerns in product development, key performance\nindicators, and internal values. For LLM-supported products, this would include questions\nregarding the source and diversity of the training data (e.g. [22]), process for bias testing in\ntechnical development, post-deployment bias mitigation practice, and company policies\nregarding data transparency and explainability. Cedars-Sinai [23] and other hospitals are\ndeveloping internal committees to review the ethics implications of all new artificial intelli-\ngence products before going live with them. Making sure that these committees have training\nin equity specifically will be important going forward.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000036 7 October 24, 2023 2 / 5\n2) Monitor LLMs rigorously during deployment\nWhen using LLMs in practice, healthcare systems should establish robust post-deployment\nmonitoring protocols for disparities in access, utilization, or outcomes. Additionally, they\nshould work with vendors to regularly monitor model responses, both by humans trained to\nsystematically evaluate for bias and by models trained to detect inequitable responses (e.g.\nresponses that more often tell women that their reported symptoms are likely psychogenic\nthan men). Patients and clinicians should retain the option to report biased responses at any\ntime. Epic has created a forum for healthcare systems to share the GPT prompts that they are\nusing with each other [24]. This forum and other similar ones could be expanded so that\nhealthcare systems could share experiences with deploying different prompts, including any\nbiases found, and collaboratively move toward more equitable prompts. Further research on\nmore specific equity metrics (given the essentially infinite output space of LLMs) is warranted,\nand healthcare systems should advocate for funding for this work. In combination, these\nequity metrics should inform continuous improvement of the model and result in model ter-\nmination if mitigation efforts are not possible.\n3) Prepare clinicians to mitigate bias\nWhile some healthcare systems may rely entirely on out-of-the-box prompt libraries (e.g.\nDocsGPT [17]), most will likely deploy both out-of-the-box and custom prompts. This rela-\ntionship offers unique opportunities for clinician-users to identify and mitigate LLM bias.\nBeth Israel Deaconess Medical Center is starting to intentionally engage medical trainees with\nLLMs to allow them to better understand what they can and cannot do [25], and NYU Lan-\ngone Health has educated various types of staff members throughout the healthcare system\nand partnered with them in developing and testing their own LLM ideas responsibly within a\nprivate and secure GPT instance [26]. Universal training in AI implicit bias as healthcare sys-\ntems start to engage clinicians in LLM use can help ensure understanding of inherent biases in\nLLM tools and equip clinicians with mitigation strategies. A group of clinician champions\nshould be specifically trained on prompt engineering, to support more targeted bias mitiga-\ntion. Just as physician builders were trained on configuring certain aspects of electronic health\nrecords (dashboards, smartphrases, etc.), select clinicians should be trained to steward effective\nand equitable prompts for their healthcare systems. At a national level, professional societies\ncan also be engaged to identify what data are necessary to create the most effective and equita-\nble prompts for each of their specialty’s most common use cases.\nLLMs are being rapidly operationalized, with tremendous potential to improve healthcare\naccess, satisfaction, and outcomes. However, they also have the potential to amplify inequities\nby promoting digital health segregation and reinforcing underlying biases in the digital world.\nWe must break the cycle of addressing health equity issues only after harm has been done and\ndeliberately center equity in large language model deployment before it is too late.\nAcknowledgmen ts\nThe authors would like to thank their collaborators in the NYU HiBRID Lab, FuturePractice,\nand MCIT.\nAuthor Contributions\nConceptualization: Nina Singh, Katharine Lawrence, Safiya Richardson, Devin M. Mann.\nFunding acquisition: Katharine Lawrence, Safiya Richardson, Devin M. Mann.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000036 7 October 24, 2023 3 / 5\nSupervision: Katharine Lawrence, Safiya Richardson, Devin M. Mann.\nWriting – original draft: Nina Singh, Katharine Lawrence, Safiya Richardson, Devin M.\nMann.\nWriting – review & editing: Nina Singh, Katharine Lawrence, Safiya Richardson, Devin M.\nMann.\nReferences\n1. OpenAI. Introduci ng ChatGPT . 2022. Available from: https://opena i.com/blog/ chatgpt.\n2. Lee P, Bubeck S, Petro J. Benefit s, limits, and risks of GPT-4 as an AI chatbot for medicine. New\nEngland Journal of Medicine. 2023; 388(13):12 33–9. https://doi.o rg/10.1056/N EJMsr2214 184 PMID:\n36988602\n3. Kung TH, Cheatham M, Medenil la A, Sillos C, De Leon L, Elepaño C, et al. Perform ance of ChatGP T\non USMLE: Potential for AI-assisted medical education using large language models. PLOS Digital\nHealth. 2023; 2(2):e0000 198. https://doi.or g/10.1371/jou rnal.pdig.00 00198 PMID: 368126 45\n4. Sarraju A, Bruemmer D, Van Iterson E, Cho L, Rodrigu ez F, Laffin L. Appropria teness of cardiovascul ar\ndisease prevention recomm endations obtaine d from a popular online chat-base d artificial intelligenc e\nmodel. JAMA. 2023. https://doi.or g/10.100 1/jama.202 3.1044 PMID: 36735264\n5. Turner BEW. Epic, Microsoft bring GPT-4 to EHRs. Modern Healthcare . April 17, 2023. Available from:\nhttps://www .modernh ealthcare.c om/digital-hea lth/himss -2023-epi c-microsoft-b ring-open ais-gpt-4-\nehrs.\n6. Business Wire. Abridge announc es partnersh ip in the University of Kansas Health System’s 140+ loca-\ntions, the first major rollout of generative AI in healthcar e. Business Wire. 2023. Availab le from: https://\nwww.busines swire.com /news/hom e/2023030200 5082/en /Abridge-Annou nces-Partner ship-In-Th e-\nUniversity -of-Kansas-H ealth-Syst em s-140-Loc ations-The-Fir st-Major- Rollout-Of-G enerative-A I-In-\nHealthcare #:~:text=H ealthcare %20%7C% 20Busines s%20Wire-,Ab ridge%20A .\n7. Harrer S. Attentio n is not all you need: the complicated case of ethically using large language models in\nhealthcare and medicine. EBioMedicine. 2023; 90.\n8. Ferrara E. Should ChatGPT be biased? Challeng es and risks of bias in large langua ge models. arXiv\npreprint arXiv:230403 738. 2023.\n9. Bravema n P. Health disparit ies and health equity: concepts and measuremen t. Annu Rev Public Health.\n2006; 27:167– 94. https://doi.or g/10.114 6/annurev.pub lhealth.27.021 405.102 103 PMID: 16533114\n10. Chen IY, Pierson E, Rose S, Joshi S, Ferryman K, Ghassemi M. Ethical machine learning in healthcar e.\nAnnual review of biomedica l data science. 2021; 4:123–44. https://doi.o rg/10.1146/an nurev-bioda tasci-\n092820-114 757 PMID: 34396058\n11. Bender EM, Gebru T, McMill an-Major A, Shmitchel l S, editors. On the dangers of stochastic parrots:\ncan langua ge models be too big? Proceedings of the 2021 ACM conferenc e on fairness, accountabi lity,\nand transparenc y; 2021.\n12. Mitchell UA, Chebli PG, Ruggiero L, Muramatsu N. The digital divide in health-rela ted technolog y use:\nthe significance of race/ethnic ity. The Gerontolo gist. 2019; 59(1):6–14 . https:// doi.org/10.10 93/geront /\ngny138 PMID: 30452660\n13. Carlini N, Tramer F, Wallace E, Jagielsk i M, Herbert-Vos s A, Lee K, et al., editors. Extracting training\ndata from large language models. USENIX Security Sympos ium; 2021.\n14. Bolukbas i T, Chang K-W, Zou JY, Saligrama V, Kalai AT. Man is to computer programm er as woman is\nto homemak er? Debiasing word embeddi ngs. Advances in neural informati on processing systems.\n2016;29.\n15. Abid A, Farooqi M, Zou J. Large langua ge models associate Muslims with violence. Nature Machine\nIntelligenc e. 2021; 3(6):461–3 .\n16. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al. Language models are few-shot\nlearners. Advances in neural informati on processing systems. 2020; 33:1877 –901.\n17. Doximity. DocsGPT. 2023.\n18. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode clinical\nknowledge. Nature. 2023:1–9 .\n19. Lie ´ vin V, Hother CE, Winther O. Can large langua ge models reason about medical questions? arXiv\npreprint arXiv:220708 143. 2022.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000036 7 October 24, 2023 4 / 5\n20. Singhal K, Tu T, Gottweis J, Sayres R, Wulczyn E, Hou L, et al. Towards expert-lev el medical question\nanswering with large langua ge models. arXiv preprint arXiv:230509 617. 2023.\n21. Nastasi AJ, Courtright KR, Halpern SD, Weissm an GE. Does ChatGPT provide appropria te and equita-\nble medical advice?: A vignette-bas ed, clinical evaluatio n across care contexts . medRxiv.\n2023:2023. 02. 25.23286451.\n22. Rostamzad eh N, Mincu D, Roy S, Smart A, Wilcox L, Pushkarn a M, et al., editors. Healthsheet : devel-\nopment of a transparenc y artifact for health datasets. Proceedings of the 2022 ACM Conferen ce on\nFairness, Accountab ility, and Transparency ; 2022.\n23. Jenning s K. This AI chatbot has helped doctors treat 3 million people–and may be coming to a hospital\nnear you. Forbes. 2023. Availab le from: https://ww w.forbes.com /sites/kat iejennings/202 3/07/17/th is-ai-\nchatbot-h as-helped-d octors-tr eat-3-million -people/?sh= 5a64fce1 4cea.\n24. Epic. Generated draft replies prompts Epic UserWeb 2023 [updated July 7, 2023; cited 2023 July 27].\nAvailable from: https://galaxy .epic.com/ ?#Browse/ page=1!68!9 5!100218189, 100218219,10 022084 1.\n25. Trang B, Palmer K. Preparatio n over panic: how a Boston hospital is priming medical resident s for an\nera of AI medicine. STAT News. 2023. Availab le from: https://ww w.statnews.c om/2023/0 7/20/chatgp t-\ngpt4-health- care-med ical-educat ion/.\n26. Austrian J, Aphinyanapho ngs Y. Empoweri ng our health system with a private and secure GPT service.\nMedium. 2023. Availab le from: https://ny ulangonem cit.medium.c om/empow ering-our-hea lth-system -\nwith-a-priv ate-and-secur e-gpt-se rvice-838fab3ff f09.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000036 7 October 24, 2023 5 / 5",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.6963930130004883
    },
    {
      "name": "Health equity",
      "score": 0.615119218826294
    },
    {
      "name": "Equity (law)",
      "score": 0.5812522172927856
    },
    {
      "name": "Software deployment",
      "score": 0.5708409547805786
    },
    {
      "name": "Public relations",
      "score": 0.5018370151519775
    },
    {
      "name": "Social determinants of health",
      "score": 0.4356245994567871
    },
    {
      "name": "Conversation",
      "score": 0.4286853075027466
    },
    {
      "name": "Political science",
      "score": 0.38773834705352783
    },
    {
      "name": "Psychology",
      "score": 0.3866589665412903
    },
    {
      "name": "Business",
      "score": 0.36145085096359253
    },
    {
      "name": "Medicine",
      "score": 0.34402406215667725
    },
    {
      "name": "Computer science",
      "score": 0.17645809054374695
    },
    {
      "name": "Law",
      "score": 0.10377860069274902
    },
    {
      "name": "Communication",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210086933",
      "name": "NYU Langone Health",
      "country": "US"
    }
  ]
}