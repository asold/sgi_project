{
    "title": "Assessing the Strengths and Weaknesses of Large Language Models",
    "url": "https://openalex.org/W4388593316",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2059221090",
            "name": "Shalom Lappin",
            "affiliations": [
                "Queen Mary University of London",
                "King's College London",
                "University of Gothenburg"
            ]
        },
        {
            "id": "https://openalex.org/A2059221090",
            "name": "Shalom Lappin",
            "affiliations": [
                "University of Gothenburg",
                "King's College London",
                "Queen Mary University of London",
                "King's College School"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3154449322",
        "https://openalex.org/W4309419389",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4309419395",
        "https://openalex.org/W2308720496",
        "https://openalex.org/W2947748865",
        "https://openalex.org/W4317212783",
        "https://openalex.org/W2963580443",
        "https://openalex.org/W6847085742",
        "https://openalex.org/W2484448352",
        "https://openalex.org/W4248023279",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2982442115",
        "https://openalex.org/W2935760417",
        "https://openalex.org/W3043710493",
        "https://openalex.org/W3139019561",
        "https://openalex.org/W2124656564",
        "https://openalex.org/W3152826268",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2619818172",
        "https://openalex.org/W1588882422",
        "https://openalex.org/W3145016028",
        "https://openalex.org/W6948936239",
        "https://openalex.org/W3153543512",
        "https://openalex.org/W3196304422",
        "https://openalex.org/W4362603432",
        "https://openalex.org/W2019004430",
        "https://openalex.org/W71795751",
        "https://openalex.org/W4317797582",
        "https://openalex.org/W2972987451",
        "https://openalex.org/W3154029344",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4300021539",
        "https://openalex.org/W6846819357",
        "https://openalex.org/W2791751435",
        "https://openalex.org/W2557321428",
        "https://openalex.org/W4309419205",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W2502997183"
    ],
    "abstract": "Abstract The transformers that drive chatbots and other AI systems constitute large language models (LLMs). These are currently the focus of a lively discussion in both the scientific literature and the popular media. This discussion ranges from hyperbolic claims that attribute general intelligence and sentience to LLMs, to the skeptical view that these devices are no more than “stochastic parrots”. I present an overview of some of the weak arguments that have been presented against LLMs, and I consider several of the more compelling criticisms of these devices. The former significantly underestimate the capacity of transformers to achieve subtle inductive inferences required for high levels of performance on complex, cognitively significant tasks. In some instances, these arguments misconstrue the nature of deep learning. The latter criticisms identify significant limitations in the way in which transformers learn and represent patterns in data. They also point out important differences between the procedures through which deep neural networks and humans acquire knowledge of natural language. It is necessary to look carefully at both sets of arguments in order to achieve a balanced assessment of the potential and the limitations of LLMs.",
    "full_text": "Journal of Logic, Language and Information (2024) 33:9–20\nhttps://doi.org/10.1007/s10849-023-09409-x\nAssessing the Strengths and Weaknesses of Large\nLanguage Models\nShalom Lappin 1,2,3\nAccepted: 16 October 2023 / Published online: 11 November 2023\n© The Author(s) 2023\nAbstract\nThe transformers that drive chatbots and other AI systems constitute large language\nmodels (LLMs). These are currently the focus of a lively discussion in both the scien-\ntiﬁc literature and the popular media. This discussion ranges from hyperbolic claims\nthat attribute general intelligence and sentience to LLMs, to the skeptical view that\nthese devices are no more than “stochastic parrots”. I present an overview of some of the\nweak arguments that have been presented against LLMs, and I consider several of the\nmore compelling criticisms of these devices. The former signiﬁcantly underestimate\nthe capacity of transformers to achieve subtle inductive inferences required for high\nlevels of performance on complex, cognitively signiﬁcant tasks. In some instances,\nthese arguments misconstrue the nature of deep learning. The latter criticisms identify\nsigniﬁcant limitations in the way in which transformers learn and represent patterns\nin data. They also point out important differences between the procedures through\nwhich deep neural networks and humans acquire knowledge of natural language. It is\nnecessary to look carefully at both sets of arguments in order to achieve a balanced\nassessment of the potential and the limitations of LLMs.\nKeywords Deep learning · Transformers · Artiﬁcal intelligence · Natural language\nprocessing\n1 Introduction\nThe introduction of transformers (V aswani et al., 2017) with multiple attention heads,\nand pre-trained with large scale word embeddings, has revolutionised NLP . They have\nB Shalom Lappin\ns.lappin@qmul.ac.uk\n1 School of Electronic Engineering and Compuer Science, Queen Mary University of London,\nLondon, England\n2 Centre for Linguistic Theory and Studies in Probability, University of Gothenburg, Gothenburg,\nSweden\n3 Department of Informatics, King’s College London, London, England\n123\n10 S. Lappin\nyielded near or above human performance on a variety of core NLP tasks, which\ninclude, among others, machine translation, natural language generation, question\nanswering, image captioning, text summarisation, and natural language inference\n(NLI). Transformers operate as autoregressive token predictors (GPT-1-GPT4, Ope-\nnAI), or as bidirectional token predictors for masked positions in contexts, such as\nBERT (Devlin et al., 2019). They signiﬁcantly outperform sequential deep learning\nnetworks, like Long Short Term Memory Recurrent Neural Networks (LSTMs) and\nConvolutional Neural Networks (CNNs), on most natural language tasks, and for many\nnon-linguistic applications, such as image recognition.\nTransformers provide the LLMs that drive chatbots. The rapid success of these bots\nin generating extended sequences of coherent, human like discourse in response to\nprompts has produced vigorous debate in both the scientiﬁc literature and the popular\nmedia. Some of this discussion consists of exaggerated claims on the capabilities of\nLLMs. Other comments offer pat dismissals of these systems as nothing more than\nartiﬁcial parrots repeating training data. It is important to consider LLMs in a critical\nand informed way, in order to understand their abilities and their limitations.\nIn Sect. 2 I take up several of the more prominent weak arguments that have been\nbrought against LLMs. These include\n(i) the view that they simply return their training data,\n(ii) the claim that they cannot capture linguistic meaning due to the absence of seman-\ntic grounding,\n(iii) the assertion that they do not acquire symbolic representations of knowledge, and\n(iv) the statement that they do not learn in the way that humans do.\nIn Sect. 3 I consider some of the strong arguments concerning the limitations of\nLLMs. These involve\n(i) important constraints on LLMs as sources of insight into human cognitive pro-\ncesses,\n(ii) the lack of robustness in LLM performance on NLI tasks,\n(iii) the unreliability of LLMs as a source of factually sound information,\n(iv) the inability of LLMs to identify universal patterns characteristic of natural lan-\nguages,\n(v) the consequences of the large data required to train LLMs, for control of the\narchitecture and development of these systems, and\n(vi) the opactiy of these systems.\nSection 4 draws conclusions concerning the capacities and limitations of LLMs. It\nsuggests possible lines of future research in deep learning in light of this discussion.\n2 Weak Arguments Against LLMs\n2.1 Generalisation, Innovation, and Semantic Grounding\nA common criticism of LLMs is that they do little more than synthesise elements\nof their training data to produce the most highly valued response to a prompt, as\ndetermined by their probability distribution over the prompt and the data. Bender et\n123\nAssessing the Strengths and Weaknesses of Large Language Models 11\nal. ( 2021) and Chomsky et al. ( 2023) offer recent versions of this view. This claim\nis false, given that transformers exhibit subtle and sophisticated pattern identiﬁcation\nand inferencing\nThis inductive capacity permits them to excel at medical image analysis and diag-\nnostics (Shamshad et al., 2023). Transformers have revolutionised computational\nbiology by predicting properties of proteins and new molecular structures (Chandra et\nal., 2023). This has opened the way for the use of deep learning for the development\nof new medications and clinical treatments.\nDasgupta et al. ( 2023) use Reinforcement Learning (RL) to train artiﬁcial agents\nwith LLMs to respond appropriately to complex commands in a simulated visual\nenvironment. These commands do not correspond to information or commands in\ntheir training set.\n1\nBender and Koller ( 2020) argue that LLMs cannot capture meaning because they\nare not semantically grounded, by virtue of the fact that their word embeddings are\ngenerated entirely from text. Hence they cannot identify speakers’ references to objects\nin the world or recognise communicative intensions. Sahlgren and Carlsson ( 2021),\nPiantadosi and Hill ( 2022), Sørgaard ( 2023) reply to this argument by observing that\nlearning the distributional properties of words in text does provide access to central\nelements of interpretation. These properties specify the topology of lexical meaning.\nEven if one accepts Bender and Koller’s view that grounding is a necessary element\nof interpretation, it does not establish their claim that LLMs are unable to represent\ninterpretations of natural language expressions. It is possible to construct multi-modal\nword embeddings that capture the distributional patterns of expressions in visual and\nother modalities. Multi-Model BERT (Lu et al., 2019) and GPT-4 (OpenAI, 2023)\nare pre-trained on such word embeddings. Multi-modal transformers can identify\nelements of a graphic image and respond to questions about them. The dialogue in\nFig. 1 from (OpenAI, 2023) illustrates the capacity of an LLM to reason about a\ncomplex photographic sequence, and to identify the humour in its main image.\nBender and Koller ( 2020) do raise important questions about what a viable compu-\ntational model of meaning and interpretation must achieve. It has stimulated a fruitful\ndebate on this issue. The classical program of formal semantics (Davidson, 1967;\nMontague, 1974) seeks to construct a recursive deﬁnition of a truth predicate that\nentails the truth conditions of the declarative sentences of a language. Lappin ( 2021)\nobserves that a generalised multi-modal deep neural network (DNN) achieves a major\npart of this program by pairing a suitable graphic (and other modes of) representation\nwith a sentence that describes a situation.\n2.2 Symbolic Representations, Grammars, and Hybrid Systems\nMarcus ( 2022) and Chomsky et al. ( 2023) maintain that LLMs are defective because\nthey do not represent the symbolic systems, speciﬁcally grammars, which humans\nacquire to express linguistic knowledge. They regard grammars as the canonical form\nfor expresing linguistic knowledge.\n1 See Lappin ( 2021) for discussion of the problem of generalising to new linguistic input in deep learning.\n123\n12 S. Lappin\nFig. 1 ChatGPT-4 Multi-Modal Dialogue, from OpenAI (2023)\nThis claim is question begging. It assumes what is at issue in exploring the nature of\nhuman learning. It is entirely possible that humans acquire and encode knowledge of\nlanguage in non-symbolic, distributed representations of linguistic (and other) regular-\nities, rather than through symbolic algebraic systems. Smolensky ( 1987), McClelland\n(2016), among others, suggests a view of this kind. Some transformers implicitly iden-\ntify signiﬁcant amounts of hierarchical syntactic structure and long distance relations\n(Lappin, 2021; Goldberg, 2019; Hewitt & Manning, 2019; Wilcox et al., 2023; Lasri,\n2023). In fact Lappin ( 2021) and Baroni ( 2023) argue that DNNs can be regarded as\nalternative theoretical models of linguistic knowledge.\nMarcus ( 2022) argues that to learn as effectively as humans do DNNs must incor-\nporate symbolic rule-based components, as well as the layers of weighted units that\nprocess vectors through the functions that characterise deep learning. It is widely\nassumed that such hybrid systems will substantially enhance the performance of DNNs\non tasks requiring complex linguistic knowledge. In fact, this is not obviously the case.\nTree DNNs incorporate syntactic structure into a DNN, either directly through its\narchitecture, or indirectly through its training data and knowledge distillation. Socher\net al. (2011), Bowman et al. ( 2016), Y ogatama et al. (2017), Choi et al. ( 2018), Williams\net al. ( 2018), Maillard et al. ( 2019), Ek et al. ( 2019) consider LSTM-based Tree DNNs.\nThese have been applied to NLP tasks like sentiment analysis, NLI, and the predic-\ntion of human sentence acceptability judgments. In general they do not signiﬁcantly\n123\nAssessing the Strengths and Weaknesses of Large Language Models 13\nimprove performance relative to the non-tree counterpart model, and, in at least one\ncase (Ek et al., 2019), performance was degraded in comparison with the non-enriched\nmodels.2\nMore recent work has incorporated syntactic tree structure into transformers like\nBERT, and applied these systems to a broader range of tasks (Sachan et al., 2021;B a i\net al., 2021). The experimental evidence on the extent to which this addition improves\nthe capacity of the transformer to handle these tasks remains unclear. Future work\nmay well show that hybrid systems of the sort that Tree DNNs represent do offer\ngenuine advantages over their non-enriched counterparts. The results achieved with\nthese systems to date have not yet motivated this claim.\n2.3 Humans Don’t Learn Like That\nMarcus ( 2022) and Chomsky et al. ( 2023) claim that DNNs do not capture the way\nin which humans achieve knowledge of their language. In fact this remains an open\nempirical question. We do not yet know enough about human learning to exclude\nstrong parallels between the ways in which DNNs and humans acquire and represent\nlinguistic knowledge, and other types of information.\nBut even if the claim is true, it need not constitute a ﬂaw in DNN design. These\nare engineering devices for performing NLP (and other) tasks. Their usefulness is\nevaluated on the basis of their success in performing these tasks, rather than on the\nway in which they achieve these results. From this perspective, criticising DNNs on the\ngrounds they do not operate like humans is analogous to objecting to aircraft because\nthey do not ﬂy like birds.\nMoreover, the success of transformers across a broad range of NLP tasks does have\ncognitive signiﬁcance. These devices demonstrate one way in which the knowledge\nrequired to perform these tasks can be obtained, even if it does not correspond to the\nprocedures that humans apply. These results have important consequences for debates\nover the types of biases that are, in principle, needed for language acquisition and\nother kinds of learning.\n3 Strong Arguments Against LLMs\n3.1 LLMs as Models of Human Learning\nPiantadosi (2023) claims that LLMs provide a viable model of human language acqui-\nsition. He suggests that they provide evidence against Chomsky’s domain speciﬁc\ninnatist view, which posits a “language faculty” that encodes a Universal Grammar. 3\nThis is due to their ablity to learn implicit representations of syntactic structure and\nlexical semantics without strong linguistic learning biases.\n2 See Lappin ( 2021) for discussion of LSTM Tree DNNs.\n3 See Lappin and Shieber ( 2007), Clark and Lappin ( 2011) for earlier discussions of domain speciﬁc, strong\nbias views of language learning, and arguments against them. These arguments are based on considerations\nfrom computational learning theory.\n123\n14 S. Lappin\nIn Sect. 2.3 I argued that the fact that LLMs may learn and represent information\ndifferently than humans does not entail a ﬂaw in their design. Moreover they do\ncontribute insight into cognitive issues concerning language acquisition by indicating\nwhat can, in principle, be learned from available data, with the the inductive procedures\nthat DNNs apply to this input. However, Piantadosi’s claims go well beyond the\nevidence. His conclusions seem to imply that humans do, in fact, learn in the way\nthat DNNs do. This is not obviously the case.\nMoreoever, although we do not know precisely how humans acquire and represent\ntheir linguistic knowledge, we do know that they learn with far less data than DNNs\nrequire. As Warstadt and Bowman ( 2023) observe, LLMs are trained on orders of\nmagnitude more data than human learners have access to. Humans also require inter-\naction in order to achieve knowledge of their language (Clark, 2023), while LLMs are\ntrained non-interactively, with the possible exception of RL as a partial simulation of\nhuman feedback.\nWarstadt and Bowman argue convincingly that in order to assess the extent to which\ndeep learning corresponds to human learning it is necessary to restrict LLMs to the\nsort of data that humans use in language acquisition. This involves reconﬁguring both\nthe training data and the learning environment for DNNs, to simulate the language\nlearning situation that humans encounter. Only an experimental context of this kind\nwill illuminate the extent to which there is an analogy between deep and human\nlearning.\n3.2 Problems with Robust NLI\nTransformers have scored well on natural language inference benchmarks. However,\nthey are easily derailed, and their performance can be reduced to close to chance by\nadversarial testing involving lexical substitutions. Talman and Chatzikyriakidis ( 2019)\nshow that BERT does not generalise well to new data sets for NLI. Conversely Talman\net al. ( 2021) report that BERT continues to achieve high scores when ﬁne tuned and\ntested on corrupted data sets containing nonsense sentence pairs. These results suggest\nthat BERT is not learning inference through semantic relations between premisses and\nconclusions. Instead it appears to be identifying certain lexical and structural patterns\nin the inference pairs.\nDasgupta et al. ( 2022) points out that humans also frequently make mistakes in\ninference. However, their reasoning abilities are more robust, even under adversarial\ntesting. Human performance declines more gracefully than transformers under gener-\nalisation challenges, and it is more sharply degraded by nonsense pairs. Dasgupta et al.\n(2022) discuss work that pre-trains models on abstract reasoning templates to improve\ntheir performance in NLI. It is not clear to what extent the natural language inference\nabilities of transformers constitute more than superﬁcial inductive generalisation over\nlabelled patterns in their training corpora. Mahowald et al. ( 2023) review substantial\namounts of evidence for the claim that LLMs do not perform well on extra-linguistic\nreasoning and real world knowledge tasks.\n123\nAssessing the Strengths and Weaknesses of Large Language Models 15\n3.3 LLMs Hallucinate\nLLMs are notorious for hallucinating plausible sounding narratives that have no factual\nbasis. In a particularly dramatic example of this phenomenon ChatGPT recently went\nto court as a legal expert. A lawyer representing a passenger on Avianca Airline brought\na lawsuit against the airline citing 6 legal precedents as part of the evidence for his\ncase (Weiser, 2023). The judge was unable to verify any of the precedents. When the\njudge demanded an explanation, the lawyer admitted to using ChatGPT-3 to identify\nthe legal precedents that he required. He went on to explain that he “even asked the\nprogram to verify that the cases were real. It had said yes.”\nThe fact that LLMs do not reliably distinguish fact from ﬁction makes them danger-\nous sources of (mis)information. Notice that semantic grounding through multi-modal\ntraining does not, in itself, solve this problem. The images, sounds, etc. to which\nmulti-modal transformers key text do not insure that the text is factually accurate. The\nnon-linguistic representations may also be artiﬁcially generated. A description of the\nimage of a unicorn may accurately describe that image. It does not characterise an\nanimal in the world.\n3.4 Linguistic Universals\nNatural languages display universal patterns, or tendencies, involving word order, mor-\nphology, phonology, and lexical semantic classes. Many of these can be expressed as\nconditional probabilities specifying that a property will hold with a certain likelihood,\ngiven the presence of other features in the language.\nGibson et al. ( 2019) and Kågebäck et al. ( 2020) suggest that information theoretic\nnotions of communicative efﬁciency can explain many of these universals. This type\nof efﬁciency involves optimising the balance between brevity of expression and com-\nplexity of content. As there are alternative strategies for achieving such optimisations,\nlanguages will exhibit different clusters of properties.\nChaabouni et al. ( 2019) report experiments showing that LSTM communication\nnetworks display a preference for anti-efﬁcient encoding of information in which the\nmost frequent expressions are the longest, rather than the shortest. They experiment\nwith additional learning biases to promote DNN preference for more efﬁcient commu-\nnication systems. Similarly, Lian et al. ( 2021) describe LSTM simulations in which\nthe network tends to preserve the distribution patterns observed in the training data,\nrather than to maximise efﬁciency of communication.\nIf this tendency carries over to transformers, then they will be unable to distinguish\nbetween input from plausible and implausible linguistic communication systems. They\nwill not recognise the class of likely natural languages. Therefore, they will not provide\ninsight into the information theoretic biases that shape natural languages.\n3.5 Large Data and the Control of Deep Learning Architecture\nTransformers require vast amounts of training data for their word and multi-modal\nembeddings. Each signiﬁcant improvement in performance on a range of tasks is\ngenerally driven by a substantial increase in training data, and an expansion in the\n123\n16 S. Lappin\nsize of the LLM. While GPT-2 has 1.5 billion parameters, GPT-3 has 175 billion, and\nGPT-4 is thought to be 6 times larger than GPT-3.\nOnly large tech companies have the computing capacity, infrastructure, and funds to\ndevelop and train transformers of this size. This concentrates the design and develop-\nment of LLMs in a very limited number of centres, to the exclusion of most universities\nand smaller research agencies. As a result, there are limited resources for research on\nalternative architectures for deep learning which focuses on issues that are not central\nto the economic concerns of tech companies. Many (most?) researchers working out-\nside of these companies effectively become clients using their products, which they\napply to AI tasks through ﬁne tuning and minor modiﬁcations. This is an unhealthy\nstate of affairs. While enormous progress has been made on deep learning models, over\na relatively short period of time, it has been largely restricted to a narrow dimension\nof tasks in NLP . In particular, work on the relation of DNNs to human learning and\nrepresentation is increasingly limited. Also, examination of learning from small data\nwith more transparent and agile systems is not a major issue in current research on\ndeep learning.\nIt is important to note that Reinforcement Learning does not alleviate the need for\nlarge data, and massive LLMs. RL can signiﬁcantly improve the performance of trans-\nformers across a large variety of tasks (Dasgupta et al., 2023). It can also facilitate zero,\none, and few shot learning, where a DNN performs well on a new task with limited or\nno prior training. However, it does not eliminate the need for large amounts of training\ndata. These are still required for pre-trained word and multi-modal embeddings.\n3.6 LLMs are Opaque\nTransformers, and DNNs in general, are largely opaque systems, for which it is difﬁcult\nto identify the procedures through which they arrive at the patterns that they recognise.\nThis is, in large part, because the functions, like ReLU, that they apply to activate their\nunits are non-linear. Autoregressive generative language models also use softmax to\nmap their output vectors into probability distributions.\nThese functions cause the vectors that a DNN produces to be, in the general case,\nnon-compositional. This is due to the fact that the representations of the input and\nthe output vectors cannot be represented by a homomorphic mapping operation. A\nmapping f : A → B from group A to group B is a homomorphism iff for every\nv\ni ,v j ∈ A, and the group operation ·, f ( A · B) = f ( A) · f (B). As a result, it is\nnot always possible to predict the composite vectors that the units of a transformer\ngenerate from their inputs, or to reconstruct these inputs from the output vectors in a\nuniform and regular way.\nProbes (Hewitt & Manning, 2019), and selective de-activation of units and attention\nheads (Lasri, 2023) can provide insight into the structures that transformers infer.\nThese methods remain indirect, and they do not fully illuminate the way in which\ntransformers learn and represent patterns from data.\nBernardy and Lappin ( 2023) propose Unitary Recurrent Networks (URNs) to solve\nthe problem of model opacity. URNs apply multiplication to orthogonal matrices.\nThe matrices that they generate are strictly compositional. These models are fully\ntransparent, and all input is recoverable from the output at each phase in a URN’s\n123\nAssessing the Strengths and Weaknesses of Large Language Models 17\nprocessing sequence. They achieve good results for deeply nested bracket matching\ntasks in Dyck languages, a class of artiﬁcial context-free languages. They do not\nperform well on context-sensitive cross serial dependencies in artiﬁcial languages, or\non agreement in natural languages. One of their limitations is the use of truncation\nof matrices to reduce the size of their rows. This is necessary to facilitate efﬁcient\ncomputation of matrix multiplication, but it degrades the performance of a URN on\nthe tasks to which it is applied.\n4 Conclusions and Future Research\nLLMs are not simply “stochastic parrots” synthesising ﬂuent sounding responses to\nprompts from previously observed training data. They achieve a sophisticated level\nof inductive learning and inference, with transferable skills, across a wide range of\ntasks. They are able to identify hierarchical syntactic structure and complex semantic\nrelations. Through reinforcement learning on multi-modal input they can be trained\nto respond appropriately to new questions and commands out of the domain of their\ntraining data. This involves signiﬁcant generalisation and few shot learning. Work on\nLLMs has yielded dramatic progress across a broad set of AI problems, in a compar-\natively short period of time. It far exceeds the achievements of symbolic rule-based\nAI over many decades.\nIt is not clear to what extent LLMs illuminate human cognitive abilities in the\nareas of language learning and linguistic representation. While they surpass human\nperformance on many cognitively interesting NLP tasks, they require far more data\nfor language learning than humans do. It is not obvious that they learn and encode\nlinguistic knowledge in the way that humans perform these operations.\nLLMs are far from human abilities in natural language inference, analogical rea-\nsoning, and interpretation, particularly for ﬁgurative language. Their performance in\ndomain general dialogue, while appearing to be ﬂuent, remains informationally lim-\nited, and frequently unreliable. They do not distinguish fact from ﬁction, but freely\ngenerate inaccurate claims. They also do not optimise informational efﬁciency in\ncommunication.\nThe large scale of training data and model size that LLMs require has created a\nsituation in which large tech companies control the design and development of these\nsystems. This has skewed research on deep learning in a particular direction, and\ndisadvantaged scientiﬁc work on machine learning with a different orientation.\nLLMs are opaque in the way that they generalise from data, which poses serious\nproblems of explainability. At present we can only understand their inference proce-\ndures and knowledge representations indirectly, through probes, and through selective\noblation of heads and other units in the network.\nThese conclusions suggest the following lines of future research on deep learning.\nTo compare LLMs to human learners it is necessary to modify the data to which they\nhave access, and to alter their training regimen. This will permit us to examine the\nextent to which there are correspondences and disanalogies between the two learn-\ning processes. It will also be necessary to study the internal procedures applied by\neach type of learner, computational for LLMs and neurological for humans, more\n123\n18 S. Lappin\nclosely to identify the precise mechanisms that drive inference, generalisation, and\nrepresentation, for each kind of acquisition.\nIt would be useful to experiment with additional learning biases for LLMs to see\nif these biases will improve their capacity for robust NLI, and for communicative\nefﬁciency. This work may provide deeper understanding of what is involved in both\nabilities. If it is successful, it will produce more intelligent and effective DNNs, which\nare better able to handle complex NLP tasks.\nIt is imperative that we develop procedures for testing the factual accuracy of the text\nproduced by LLMs. Without them we are exposed to the threat of disinformation on an\neven larger scale than we are currently encountering in bot saturated social media. The\nneed to combat disinformation patterns together with the urgency of ﬁltering racial,\nethnic, religious, and gender bias in AI systems powered by LLMs, that are used\nin decision making applications (hiring, lending, university admission, etc). These\nresearch concerns should be a focus of public policy discussion.\nDeveloping smaller, more lightweight models that can be trained on less data\nwould encourage more work on alternative architectures, among a larger number\nof researchers, distributed more widely across industrial and academic centres. This\nwould facilitate the pursuit of more varied scientiﬁc objectives in the area of machine\nlearning.\nFinally, designing and implementing fully transparent DNNs will improve our\nunderstanding of both artiﬁcial and human learning. Scientiﬁc insight of this kind\nshould be no less a priority than the engineering success driving current LLMs. Ulti-\nmately, good engineering depends on a solid scientiﬁc understanding of the principles\nembodied in the systems that the engineering creates.\nAcknowledgements Many of the ideas proposed here were developed in discussion with Richard Sproat.\nI am grateful to him for his contribution to my thinking on these issues. I am also indebted to Katrin Erk,\nSteven Piantadosi, and two anonymous reviewers for valuable remarks and suggestions on previous drafts.\nAn earlier version of this paper was presented in the Cognitive Science Seminar of the School of Electronic\nEngineering and Computer Science at Queen Mary University of London, on June 21, 2023. I thank the\naudience of this talk for helpful questions and comments. I bear sole responsibility for any errors that remain.\nMy research reported in this paper was supported by grant 2014-39 from the Swedish Research Council,\nwhich funds the Centre for Linguistic Theory and Studies in Probability (CLASP) in the Department of\nPhilosophy, Linguistics, and Theory of Science at the University of Gothenburg.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included\nin the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the\ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nBai, J., Wang, Y ., Chen, Y ., Y ang, Y ., Bai, J., Y u, J., & Tong, Y . (2021). Syntax-BERT: Improving pre-trained\ntransformers with syntax trees. In: Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics , (pp. 3011–3020). Association for Computational\nLinguistics\n123\nAssessing the Strengths and Weaknesses of Large Language Models 19\nBaroni, M. (2023). On the proper role of linguistically oriented deep net analysis in linguistic theorising.\nIn S. Lappin & J.-P . Bernardy (Eds.), Algebraic Structures in Natural Language (pp. 1–16). D: CRC\nPress.\nBender, E. M., & Koller, A. (2020). Climbing towards NLU: On meaning, form, and understanding in\nthe age of data. In: Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, (pp. 5185–5198). Association for Computational Linguistics,\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic\nparrots: Can language models be too big? FAccT ’21 , (pp. 610–623). Association for Computing\nMachinery\nBernardy, J.-P . & S. Lappin (2023).: Unitary recurrent networks. In S. Lappin & J.-P . Bernardy (Eds.),\nAlgebraic Structures in Natural Language (pp. 243–277). CRC Press.\nBowman, S. R., Gauthier, J., Rastogi, A., Gupta, R., Manning, C. D., & Potts, C. (2016). A fast uniﬁed\nmodel for parsing and sentence understanding. In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) , (pp. 1466–1477). Association\nfor Computational Linguistics, Berlin, Germany.\nChaabouni, R., Kharitonov, E., Dupoux, E., & Baroni, M. (2019). Anti-Efﬁcient Encoding in Emergent\nCommunication. Curran Associates Inc.\nChandra, A., Tünnermann, L., Löfstedt, T., & Gratz, R. (2023). Transformer-based deep learning for pre-\ndicting protein properties in the life sciences. eLife, 12.\nChoi, J., Y oo, K. M., & Lee, S.-g. (2018). Learning to compose task-speciﬁc tree structures. In AAAI\nConference on Artiﬁcial Intelligence.\nChomsky, N., Roberts, I., & Watumull, J. (2023). The false promise of chatgpt. 2023.\nClark, E. (2023). Language is acquired in interaction. In S. Lappin & J.-P . Bernardy (Eds.), Algebraic\nStructures in Natural Language (pp. 77–93). CRC Press.\nClark, A., & Lappin, S. (2011). Linguistic Nativism and the Poverty of the Stimulus. Wiley-Blackwell.\nDasgupta, I., Kaeser-Chen, C., Marino, K., Ahuja, A., Babayan, S., Hill, F., & Fergus, R. (2023). Collabo-\nrating with language models for embodied reasoning. arXiv:2302.00763.\nDasgupta, I., Lampinen, A. K., Chan, S. C. Y ., Creswell, A., Kumaran, D., McClelland, J. L., & Hill, F.\n(2022). Language models show human-like content effects on reasoning. arXiv:2207.0705.\nDavidson, D. (1967). Truth and meaning. Synthese, 17(1), 304–323.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In: Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), (pp. 4171–4186). Association for Computational Linguistics\nEk, A., Bernardy, J.-P ., & Lappin, S. (2019). Language modeling with syntactic and semantic representation\nfor sentence acceptability predictions. In Proceedings of the 22nd Nordic Conference on Computational\nLinguistics, , (pp. 76–85).\nGibson, E., Futrell, R., Piantadosi, S. P ., Dautriche, I., Mahowald, K., Bergen, L., & Levy, R. (2019). How\nefﬁciency shapes human language. Trends in Cognitive Sciences, 23(5), 38–407.\nGoldberg, Y . (2019). Assessing bert’s syntactic abilities. arXiv:abs/1901.05287.\nHewitt, J., & Manning, C. D. (2019). A structural probe for ﬁnding syntax in word representations. In Pro-\nceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , (pp. 4129–4138).\nAssociation for Computational Linguistics, Minneapolis, Minnesota.\nKågebäck, M., Carlsson, E., Dubhashi, D., & Sayeed, A. (2020). A reinforcement-learning approach to\nefﬁcient communication. PLoS ONE 2020.\nLappin, S. (2021). Deep Learning and Linguistic Representation. CRC Press.\nLappin, S., & Shieber, S. (2007). Machine learning theory and practice as a source of insight into universal\ngrammar. Journal of Linguistics, 43, 393–427.\nLasri, K. (2023). Linguistic Generalization in Transformer-Based Neural Language Models. unpublished\nPhD thesis\nLian, Y ., Bisazza, A., & V erhoef, T. (2021). The effect of efﬁcient messaging and input variability on neural-\nagent iterated language learning. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, (pp. 10121–10129). Association for Computational Linguistics\nLu, J., Batra, D., Parikh, D., & Lee, S. (2019). ViLBERT: pretraining task-agnostic visiolinguistic represen-\ntations for vision-and-language tasks. In Proceedings of the 33rd International Conference on Neural\nInformation Processing Systems, (pp. 13–23).\n123\n20 S. Lappin\nMaillard, J., Clark, S., & Y ogatama, D. (2019). Jointly learning sentence embeddings and syntax with\nunsupervised tree-lstms. Natural Language Engineering, 25(4), 433–449.\nMarcus, G. (2022). Deep learning alone isn’t getting us to human-like AI. Noema, August 112022.\nMcClelland, J. L. (2016). Capturing gradience, continuous change, and quasi-regularity in sound, word,\nphrase, and meaning. In B. MacWhinney & W. O’Grady (Eds.), The Handbook of Language Emergence\n(pp. 54–80). John Wiley and Sons.\nMontague, R. (1974). Formal Philosophy: Selected Papers of Richard Montague . Y ale University Press.\nEdited with an introduction by R. H. Thomason.\nMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., Fedorenko, E. (2023). Dis-\nsociating language and thought in large language models: a cognitive perspective. arXiv:2301.06627\n[cs.CL].\nOpenAI. (2023). GPT-4 technical report. arXiv:2303.08774.\nPiantadosi, S. (2023). Modern language models refute chomsky’s approach to language. Lingbuzz Preprint,\nlingbuzz 7180.\nPiantadosi, S. T., & Hill, F. (2022). Meaning without reference in large language models. arXiv:2208.02957.\nSachan, D. S., Zhang, Y ., Qi, P ., & Hamilton, W. (2021). Do syntax trees help pre-trained transformers extract\ninformation? In: Proceedings of the 16th Conference of the European Chapter of the Association for\nComputational Linguistics, (pp. 2647–2661). Association for Computational Linguistics\nSahlgren, M., & Carlsson, F. (2021). The singleton fallacy: Why current critiques of language models miss\nthe point. Frontiers in Artiﬁcial Intelligence, 4.\nShamshad, F., Khan, S., Zamir, S. W., Khan, M. H., Hayat, M., Khan, F. S., & Fu, H. (2023). Transformers\nin medical imaging: A survey. Medical Image Analysis, 88.\nSmolensky, P . (1987). Connectionist AI, symbolic AI, and the brain. Artiﬁcial Intelligence Review, 1(2),\n95–109.\nSocher, R., Pennington, J., Huang, E. H., Ng, A. Y ., & Manning, C. D. (2011). Semi-supervised recur-\nsive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Processing, (pp. 151–161). Association for Computational\nLinguistics, Edinburgh, Scotland, UK.\nSørgaard, A. (2023). Grounding the vector space of an octopus: Word meaning from raw text. Minds and\nMachines, 33(1), 33–54.\nTalman, A., & Chatzikyriakidis, S. (2019). Testing the generalization power of neural network models across\nNLI benchmarks. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, (pp. 85–94). Association for Computational Linguistics, Florence, Italy.\nTalman, A., Apidianaki, M., Chatzikyriakidis, S., & Tiedemann, J. (2021). NLI data sanity check: Assessing\nthe effect of data corruption on model performance. In Proceedings of the 23rd Nordic Conference\non Computational Linguistics (NoDaLiDa) , (pp. 276–287). Linköping University Electronic Press,\nSweden, Reykjavik, Iceland (Online).\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I.\n(2017). Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S.\nVishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems. (V ol. 30).\nRed Hook, NY: Curran Associates Inc.\nWarstadt, A., & Bowman, S. R. (2023). What artiﬁcial neural networks can tell us about human language\nacquisition. In S. Lappin & J.-P . Bernardy (Eds.), Algebraic Structures in Natural Language (pp.\n17–59). CRC Press.\nWeiser, B. (2023). Here’s what happens when your lawyer uses ChatGPT. 2023.\nWilcox, E. G., Gauthier, J., Hu, J., Qian, P ., & Levy, R. (2023). Learning syntactic structures from string\ninput. In S. Lappin & J. .-P . Bernardy (Eds.), Algebraic Structures in Natural Language(pp. 113–137).\nCRC Press.\nWilliams, A., Drozdov, A., & Bowman, S. R. (2018). Do latent tree learning models identify meaningful\nstructure in sentences? Transactions of the Association for Computational Linguistics, 6, 253–267.\nY ogatama, D., Blunsom, P ., Dyer, C., Grefenstette, E., & Ling, W. (2017). Learning to compose words into\nsentences with reinforcement learning. In 5th International Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional afﬁliations.\n123"
}