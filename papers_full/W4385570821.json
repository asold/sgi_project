{
  "title": "Team Cadence at MEDIQA-Chat 2023: Generating, augmenting and summarizing clinical dialogue with large language models",
  "url": "https://openalex.org/W4385570821",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2518430028",
      "name": "Ashwyn Sharma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1994729419",
      "name": "David Feldman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5067111844",
      "name": "Aneesh Jain",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4287887173",
    "https://openalex.org/W2434099711",
    "https://openalex.org/W3100560913",
    "https://openalex.org/W3177464472",
    "https://openalex.org/W2504481025",
    "https://openalex.org/W3169068430",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2439194",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4386566731",
    "https://openalex.org/W4386492483",
    "https://openalex.org/W3098960752",
    "https://openalex.org/W4385573581",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4246726178",
    "https://openalex.org/W3200980294",
    "https://openalex.org/W3045635560",
    "https://openalex.org/W3139390182",
    "https://openalex.org/W2270372117",
    "https://openalex.org/W4303182341",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3098562101",
    "https://openalex.org/W3207036634",
    "https://openalex.org/W2467173223",
    "https://openalex.org/W3034999214"
  ],
  "abstract": "This paper describes Team Cadence's winning submission to Task C of the MEDIQA-Chat 2023 shared tasks. We also present the set of methods, including a novel N-pass strategy to summarize a mix of clinical dialogue and an incomplete summarized note, used to complete Task A and Task B, ranking highly on the leaderboard amongst stable and reproducible code submissions. The shared tasks invited participants to summarize, classify and generate patient-doctor conversations. Considering the small volume of training data available, we took a data-augmentation-first approach to the three tasks by focusing on the dialogue generation task, i.e., Task C. It proved effective in improving our models' performance on Task A and Task B. We also found the BART architecture to be highly versatile, as it formed the base for all our submissions. Finally, based on the results shared by the organizers, we note that Team Cadence was the only team to submit stable and reproducible runs to all three tasks.",
  "full_text": "Proceedings of the 5th Clinical Natural Language Processing Workshop, pages 228–235\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nTeam Cadence at MEDIQA-Chat 2023: Generating, augmenting and\nsummarizing clinical dialogue with large language models\nAshwyn Sharma\nCadence Solutions, USA\nashwyn@cadencerpm.com\nDavid I. Feldman, MD, MPH\nCadence Solutions, USA\nMassachusetts General Hospital, Harvard University , USA\ndavid.feldman@cadencerpm.com\nAneesh Jain\nCadence Solutions, USA\nVirginia Polytechnic Institute and State University, USA\naneeshjain70@gmail.com\nAbstract\nThis paper describes Team Cadence’s winning\nsubmission to Task C of the MEDIQA-Chat\n2023 shared tasks. We also present the set\nof methods, including a novel N-pass strategy\nto summarize a mix of clinical dialogue and\nan incomplete summarized note, used to com-\nplete Task A and Task B, ranking highly on the\nleaderboard amongst stable and reproducible\ncode submissions. The shared tasks invited\nparticipants to summarize, classify and gener-\nate patient-doctor conversations. Considering\nthe small volume of training data available, we\ntook a data-augmentation-first approach to the\nthree tasks by focusing on the dialogue gener-\nation task, i.e., Task C. It proved effective in\nimproving our models’ performance on Task\nA and Task B. We also found the BART archi-\ntecture to be highly versatile, as it formed the\nbase for all our submissions. Finally, based on\nthe results shared by the organizers, we note\nthat Team Cadence was the only team to submit\nstable and reproducible runs to all three tasks.\n1 Introduction\nMEDIQA-Chat 2023 Shared Tasks included three\ntasks on the summarization and generation of\ndoctor-patient conversations to promote research\non these topics (Ben Abacha et al., 2023). Task\nA (Short Dialogue2Note Summarization) expected\na section summary (section header and content)\ngiven a short input conversation. We recognized\nthat generating the summary content was an ab-\nstractive summarization (Chopra et al., 2016) task\nwhile predicting the section header was a multi-\nclass (twenty normalized section labels) classifi-\ncation task. Task B ( Full Dialogue2Note Sum-\nmarization) was another abstractive summariza-\ntion task that required submissions to generate a\ncomplete clinical note from a whole dialogue be-\ntween a patient and a doctor. The complete clinical\nnote was expected to have the following first-level\nsection headers: \"HISTORY OF PRESENT ILL-\nNESS\", \"PHYSICAL EXAM\", \"RESULTS\", and\n\"ASSESSMENT AND PLAN\" . Finally, Task C\n(Note2Dialogue Generation), a data augmentation\n(Shorten et al., 2021) task, was about generating\npatient-doctor conversations for complete input\nnotes.\nAside from predicting section headers for Task\nA, all other tasks could be formulated as sequence-\nto-sequence (Sutskever et al., 2014) learning tasks.\nVarious model architectures based on transform-\ners (Vaswani et al., 2017) have proved to be suc-\ncessful at tackling such tasks. Therefore, lever-\naging pre-trained model checkpoints from public\nrepositories was considered the right choice. En-\ncouraged by the leaderboard for SAMSum (Gliwa\net al., 2019) on HuggingFace (Wolf et al., 2020), a\ndialogue summarization dataset, we chose BART\n(Lewis et al., 2019) as the base model for our exper-\niments. Specifically, we picked the facebook/bart-\n228\nlarge 1 model checkpoint (referenced as bart-large\nin this text from hereon) for its effectiveness on\ntext-generation tasks.\nThe SAMSum (Gliwa et al., 2019) dataset is\nintended to train dialogue summarization models.\nHowever, we recognized that the input and target\nlabels could be inverted to train a dialogue genera-\ntion model. We trained/validated bart-large on the\ninverse of SAMSum (Gliwa et al., 2019) dataset\nfollowed by the Task C training dataset provided\nby the task organizers, achieving ROUGE-1 and\nROUGE-2 scores of 59.11 and 23.69, respectively,\non the validation set. This model was then used to\naugment datasets for Task A and Task B summa-\nrization tasks. In order to generate synthetic patient-\ndoctor conversations, we chose to sample a thou-\nsand discharge summary notes from the MIMIC-IV-\nNote (Johnson et al., 2023; Goldberger et al., 2000)\ndataset. We then added these dialogue-note pairs to\nthe Task A and Task B training datasets provided\nby the organizers. The impact of this augmentation\ntechnique is noted in Section 5 below.\nFor Task A summarization, bart-large was\nfine-tuned on the SAMSum (Gliwa et al., 2019)\ndataset followed by fine-tuning on the augmented\ndataset for Task A, which achieved ROUGE-1 and\nROUGE-2 scores of 50.7 and 21.4, respectively, on\nthe validation set. Our methods yielded an overall\nimprovement (over the baseline) of 13.1% and 14%\nin ROUGE-1 and ROUGE-2 scores, respectively.\nResults from fine-tuning bart-large on the unaug-\nmented (original) Task A dataset were considered\nthe baseline in this comparison.\nInspired by the significant gains exhibited by the\nTask A model, we decided to use it as the base\nmodel for Task B. Fine-tuning this base model on\nthe augmented Task B dataset yielded ROUGE-1\nand ROUGE-2 scores of 54.16 and 26.04, respec-\ntively - a 13.7% gain in ROUGE-2 score over the\nbaseline. Results from fine-tuning the base model\non the unaugmented (original) Task B dataset were\nconsidered the baseline in this comparison. The\nfinal submission(run1) achieved ROUGE-1 and\nROUGE-2 scores of 49.5 and 23.4 on the test set.\nUnfortunately, the Task B dataset comprised input\nconversations almost twice as long as the maximum\nnumber of tokens accepted by bart-large, which\nnaturally prohibits the model’s ability to summa-\nrize the entire conversation. To solve this prob-\n1https://huggingface.co/facebook/\nbart-large\nlem, we developed an N-pass strategy in which the\nmodel attempts to summarize the conversation in\nmultiple steps. Each step (or pass) involves the\nmodel taking as input the summary note of the di-\nalogue processed till that step, concatenated with\nthe rest of the dialogue. In other words, we trained\nthe model to summarize a partial mix of an in-\ncomplete clinical note and an incomplete patient-\ndoctor conversation. This strategy led to a gain of\n6.6% and 8.1% in ROUGE-1 (57.76) and ROUGE-\n2 (28.15) scores, respectively, on the validation set.\nWe submitted the N-pass model as run2, which\noutperformed the run1 submission by 6.8%, both\nfor ROUGE-1 (52.9) and ROUGE-2 (25) scores,\non the test set. It also improved the division-based\naggregate score by 16.75%. Overall, our methods\nimproved the baseline ROUGE-2 score by 22.9%\non the validation set, while the baseline ROUGE-1\nscore was found to be slightly better by 0.45%.\nGiven the promising performance of bart-large\non the summarization tasks, we also decided to\nuse it for Task A classification. We leveraged the\nBartForSequenceClassification wrapper offered by\nHuggingFace (Wolf et al., 2020), a BART model\nwith a sequence classification head on top (a lin-\near layer on top of the pooled output). Using this\napproach, we achieved an accuracy of 78% and\nan F1 score of 78.37%. The final submission was\nreported to have an accuracy of 73.5% on the test\nset.\n2 Background and Related Work\nStudies like the ones from Alkureishi MA et al.\n(Alkureishi et al., 2016) and Rathert et al. (Rathert\net al., 2017) have presented evidence on EHRs\n(Electronic Health Records) impacting the qual-\nity of patient-doctor conversations. Digital scribes\n(van Buchem et al., 2021) and summarization tools\n(Shanafelt et al., 2016) can mitigate some of these\nproblems. However, many challenges are associ-\nated with clinical dialogue summarization (Zhu and\nPenn, 2006). Some significant challenges include\nomitting key medical concepts (Knoll et al., 2022)\nand hallucinating unsubstantiated information.\nSeveral attempts have been made to address\nsaid inherent challenges and automatically gen-\nerate high-quality summaries of clinical encoun-\nters. Approaches like the ones used by Enarvi et al.\n(2020) have utilized a transformer (Vaswani et al.,\n2017) model to summarize doctor-patient conver-\nsations. Joshi et al. (2020) and Michalopoulos et al.\n229\nDialog\n(Block \n1)\nmax_len \n1024\n1st \nPass\nModel\n2nd \nPass\nModel\nSummary\n(First \nblock)\nmax_len \n512\nNth \nPass\nModel\nSummary\n(All \nN \nblocks)\nmax_len \n1024\nDialog\n(Nth \nblock)\nmax_len \n512\nSummary\n(First \n2 \nblocks)\nmax_len \n512\nDialog\n(Block \n2)\nmax_len \n512\nN \npasses\nConcat\nConcat\nFigure 1: N-pass summarization for handling long conversations.\n(2022) have also incorporated medical knowledge\ninto these models. On the data generation front,\nChintagunta et al. (2021) showed that large lan-\nguage models can be used for augmenting medical\nsummarization datasets.\nTo the best of our knowledge, theN-pass strategy\nused to address long input sequences of Task B is\nnovel. However, multiple multi-stage summariza-\ntion approaches have been proposed so far. For ex-\nample, Krishna et al. (2020) used modular summa-\nrization techniques to produce notes from patient-\ndoctor conversations. Zhang et al. (2021) used\nmulti-stage summarization for long inputs, whereas\nGidiotis and Tsoumakas (2020) split a long docu-\nment and its summary into multiple source-target\npairs using sentence similarity. Recursive summa-\nrization incorporating human feedback (Wu et al.,\n2021) even achieved state-of-the-art results in book\nsummarization.\n3 Datasets\n3.1 MEDIQA-Chat-2023\nTask A training (validation) dataset (Ben Abacha\net al., 2023) provided by the organizers consists of\n1,201 (100) pairs of conversations and associated\nsection headers and summaries. There were 20\nunique normalized section headers overall. The\nTask B and Task C training (validation) set consists\nof 67 (20) pairs of conversations and full clinical\nnotes (Yim et al., 2023).\n3.2 SAMSum\nThe SAMSum dataset contains 16369 conversa-\ntions and their summaries (Gliwa et al., 2019), with\na train/val/test split of 14732/818/819. Several di-\nalogue summarization models have leveraged this\ndataset (Ni et al., 2022) and achieved promising re-\nsults on the task. We note the impact of this dataset\nin the ablation study (Section 5).\n3.3 MIMIC-IV-Note\nMIMIC-IV-Note contains 331,794 deidentified\nfree-text clinical notes for patients included in the\nMIMIC-IV clinical database (Johnson et al., 2023;\nGoldberger et al., 2000). We sampled a thousand\nnotes from this dataset and used the Task C (dia-\nlogue generation) model for downstream data aug-\nmentation of Task A and Task B. Ablation study\n(Section 5) highlights significant contributions of\nthis dataset to improving the results.\n4 Methods\n4.1 Dialogue Generation\nWe discovered that by flipping input and target\nlabels, the SAMSum (Gliwa et al., 2019) dataset\ncould also train a dialogue generation model. Our\n230\nTable 1: Hyperparameters used for Task A, Task B and Task C\nParameter Task A Task B Task C\nClassification Summarization Summarization Generation\nlearning_rate 2E-05 5E-05 5E-05 5E-05\nper_device_train_batch_size 8 4 4 4\nper_device_eval_batch_size 8 4 2 2\nweight_decay 0.01 0 0 0\nnum_train_epochs 30 30 30 10\nfp16 TRUE TRUE TRUE TRUE\ngradient_accumulation_steps 4 8 8 8\ngradient_checkpointing TRUE TRUE TRUE TRUE\npredict_with_generate - TRUE TRUE TRUE\ngeneration_max_length - 512 1024 1024\nmax_target_length - 512 1024 1024\nmax_source_length 1024 1024 1024 1024\nrecipe included fine-tuning bart-large on the in-\nverted SAMSum (Gliwa et al., 2019) dataset for\n10 epochs, followed by fine-tuning on a dataset\nthat combined training and validation datasets from\nTask A and Task C for another 10 epochs. Fine-\ntuning was performed using the Trainer API offered\nby HuggingFace (Wolf et al., 2020), and the hyper-\nparameters used are described in (Table 1). We\ndid not perform a comprehensive sweep and rec-\nognize that a more optimal set of hyperparameters\ncould yield better results. The model yielded by\nthis recipe was also used for generating synthetic\ndata for Task A and Task B summarization. Specif-\nically, patient-doctor conversations were generated\nfor 1000 discharge summary notes sampled from\nthe MIMIC-IV-Note (Johnson et al., 2023; Gold-\nberger et al., 2000) dataset. We used ROUGE-1\nand ROUGE-2 scores for evaluating the model’s\nperformance on the validation set (Lin, 2004).\n4.2 Dialogue Summarization\nSummarization models for Task A and Task B lever-\naged bart-large fine-tuned on the SAMSum (Gliwa\net al., 2019) dataset for 10 epochs as the base model.\nThe base model was then fine-tuned on the aug-\nmented version of the Task A training dataset for\n30 epochs. Like dialogue generation, fine-tuning\nwas performed using the Trainer API offered by\nHuggingFace (Wolf et al., 2020), and the hyperpa-\nrameters used are described in Table 1. We did not\nperform a comprehensive sweep and recognize that\na more optimal set of hyperparameters could yield\nbetter results. With a working hypothesis that the\nTask A model can capture local themes in conversa-\ntions with fewer turns, we used the model yielded\nby the above recipe as the base model for Task B.\nBefore augmenting the Task B dataset with the\ndialogue generation model, we sanitized the 1000\nnotes sampled from the MIMIC-IV-Note (Johnson\net al., 2023; Goldberger et al., 2000) dataset. The\nsanitization process mainly included removing first-\nlevel section headers not accepted for evaluation by\nthe organizers, as laid out in (Section 1). The base\nmodel was then fine-tuned on the sanitized-and-\naugmented dataset (named Augmented(Sections)\nin result tables) using the same process as Task A.\nThis fine-tuned version was submitted as run1 and\nsuffered from a significant drawback - the inability\nto handle input sequences longer than 1024 tokens.\nTo address the shortcoming, we developed a novel\nN-pass approach by training a model that can gen-\nerate summaries given a partial mix of incomplete\nsummaries and incomplete dialogue. Specifically,\na 2-pass version, named run2, was submitted to the\nshared task.\nThe N-pass approach is illustrated in Figure 1.\nThe idea is to summarize long conversations in\nmultiple passes, where each pass accepts as input\nthe next block of the unsummarized dialogue con-\ncatenated with the summary output by the previous\npass. The intuition behind this approach is to ac-\ncommodate the limit on the number of input tokens\naccepted by the model by feeding it the dialogue\nin blocks but still propagating the context by incor-\nporating the summary generated till that point. For\nrun2, the model used for run1 was fine-tuned for\n231\n30 epochs on a dataset that concatenated the first\nblock summary with the second block of the dia-\nlogue. The first block summaries were generated\nby the run1 model. A block size of 512 tokens was\nused for both the input and the output (except the\nfinal pass where output is 1024 tokens). We used\na combination of ROUGE-1 and ROUGE-2 scores\nfor evaluating the model’s ability to summarize the\nconversations in the validation set (Lin, 2004).\n4.3 Classification\nWe used a simple yet effective classification ap-\nproach to producing section headers for Task A.\nGiven the promising results from using bart-large\non the summarization and dialogue generation\ntasks, we chose to stick with the same for clas-\nsification. To be exact, we fine-tuned the model\nused for Task A submission on the classification\ntask by leveraging the BartForSequenceClassifica-\ntion wrapper offered by HuggingFace (Wolf et al.,\n2020), a BART model with a sequence classifica-\ntion head on top (a linear layer on top of the pooled\noutput). Again, the Trainer API was used with no\nhyperparameter sweep. Table 1 lists the hyperpa-\nrameters used for fine-tuning the classifier.\n5 Experiments and Ablation Study\nDataset ROUGE-1 ROUGE-2\nMEDIQA 47.5 19.8\nAugmented (Sections) 48.12 19.9\nAugmented 50.7 21.4\nTable 2: Task A - results with different training datasets.\nMetrics evaluated on the task validation set.\nModel ROUGE-1 ROUGE-2\nbart-large 44.8 18.77\nbart-samsum 47.5 19.8\nTable 3: Task A - impact of fine-tuning on SAMSum.\nMetrics evaluated on the task validation set.\n5.1 Task A\nIn Table 2, we compare the results obtained on\nthe Task A validation set by using three different\ntraining datasets - original Task A training data,\naugmented Task A training data, and sanitized-\nand-augmented (defined in Section 4.2) training\ndata. The augmented version outperforms the orig-\ninal Task A training data by 6.7% (ROUGE-1) and\n8% (ROUGE-2). As expected, the sanitized-and-\naugmented training data yields smaller gains be-\ncause the summary notes for Task A are shorter and\ndo not include first-level section headers in Task B\ntraining data.\nAn ablation study (Table 3) was also conducted\non the impact of fine-tuningbart-large on the SAM-\nSum(Gliwa et al., 2019) dataset. It was found that\nfine-tuning on the SAMSum (Gliwa et al., 2019)\ndataset improved performance on the validation set\nby 6% (ROUGE-1) and 5.4% (ROUGE-2).\nTask A summarization model fine-tuned on clas-\nsification achieved an accuracy of 78% and an f1\nscore of 78.37% on the validation set.\nVersion ROUGE-1 ROUGE-2\nMEDIQA 48.13 19.0\nAugmented 51.86 23.42\nAugmented (Sections) 54.16 26.04\n2-pass 57.76 28.15\nTable 4: Task B - results with different training datasets\nand the 2-pass strategy. Metrics evaluated on the task\nvalidation set.\nModel ROUGE-1 ROUGE-2\nbart-large 58.02 22.9\nbart-samsum 48.13 19.0\nTable 5: Task B - impact of fine-tuning on SAMSum.\nMetrics evaluated on the task validation set.\n5.2 Task B\nTable 4 shows that the 2-pass summarization strat-\negy leads to a gain of 6.6% (ROUGE-1) and\n8.1% (ROUGE-2). Furthermore, training on the\nsanitized-and-augmented dataset yields improve-\nments of 12.5% (ROUGE-1) and 37% (ROUGE-2),\ndriving home the value of data augmentation by\nclinical dialogue generation. Interestingly, simply\nfine-tuning on the SAMSum(Gliwa et al., 2019)\ndataset led to worse results (Table 5) on the Task\nB validation set, which could be explained by the\ndiscrepancy in the length of the conversations and\nthe summaries between the two datasets.\n232\nDataset bart-large bart-samsum\nR-1 R-2 R-1 R-2\nMEDIQA 53.6 17.26 56.55 20.64\nCombined 58.43 22.74 59.11 23.69\nTable 6: Task C - results with different training datasets\nand impact of fine-tuning on SAMSum. Metrics evalu-\nated on the task validation set.\n5.3 Task C\nThe ablation study (Table 6) for Task C highlights\ntwo significant ideas. First, adding the training\ndata from Task A contributed a hike of 4.5% (9%)\nin the ROUGE-1 score and 14.7% (31.7%) in the\nROUGE-2 score for the model (not) fine-tuned on\nthe inverse of the SAMSum (Gliwa et al., 2019)\ndataset. Second, fine-tuning on the inverse of SAM-\nSum (Gliwa et al., 2019) led to a gain of 5.5%\n(1.1%) in ROUGE-1 scores and 19.5% (4.1%) in\nROUGE-2 scores when training data from Task C\n(Task A + Task C) was used. It shows that the\nadditional data from Task A is more critical when\nfine-tuning on the inverse of SAMSum (Gliwa et al.,\n2019) is skipped.\n6 Results\nTeam Cadence’s submission for Task C earned\nrank-1 amongst all participants, beating the next-\nbest submission by 28.3% (ROUGE-1) and 99%\n(ROUGE-2).\nThe organizers shared test set results (Ben\nAbacha et al., 2023) along with a code status de-\nscription where a code status of 1 meant that the\norganizers were able to run the submitted code\nand reproduce the results, and a code status of\n2 meant that they were able to run the code and\nfound minor differences with no changes in rank-\nings. Code statuses 3,4, and 5 meant that the\norganizers found the submitted code to be un-\nstable or not runnable under their configurations.\nAmongst code statuses 1 and 2, Team Cadence\nachieved the following ranks: rank-2 on TaskB-\nsummarization, rank-3 on TaskA-summarization,\nrank-3 on TaskB-summarization(note-divisions),\nand rank-5 on TaskA-classification. The code for\ngenerating the submitted runs is being shared pub-\nlicly2.\n2https://github.com/ashwyn/\nMEDIQA-Chat-2023-Cadence\n7 System Specification\nIn the spirit of reproducibility, we share details of\nthe systems used to run these experiments. The\nmodels were fine-tuned on g4dn.12xlarge AWS\nSagemaker notebook instances 3. HuggingFace’s\nPython package transformers (Wolf et al., 2020)\nversion 4.27.1 was used in a Python3.8 environ-\nment. Reported results were aggregated from 4\ndifferent runs using 4 different random seeds.\n8 Limitations and Future Work\nThe methods described in this paper do not lever-\nage any external medical knowledge, a technique\nthat has been shown to be effective by other stud-\nies (Joshi et al., 2020; Michalopoulos et al., 2022).\nAnd like other methods based on large language\nmodels, in theory, our models are also prone to hal-\nlucinations and omission of key-clinical concepts.\nWe plan to explore constrained beam search4 as a\nmitigation strategy for addressing these challenges\nin the future.\nAlthough the impact of the Task C model as\na data augmentation tool is undoubtedly positive\n(Section 5), qualitative error analysis of patient-\ndoctor conversations produced by the model\nshowed that the output contained a small number\nof dialogue turns, and each individual turn was too\nlong, packed with information. Producing conver-\nsations with a more natural flow should yield an\neven better boost on downstream tasks, and we\nleave exploring such methods to future experimen-\ntation. We also recognize that N-pass summariza-\ntion for Task B with higher values ofN should be\nable to cover the entirety of the input conversations\nin the Task B datasets, albeit with diminishing re-\nturns as N increases. We hope to evaluate them in\nfuture iterations of similar shared tasks.\n9 Conclusion\nThe two key takeaways from the experiments and\nresults in this paper are significant improvements\nin summarization results driven by data augmenta-\ntion and the N-pass summarization technique for\nhandling long input patient-doctor conversations.\nFurthermore, the fact that our submissions to all\nthree tasks share the same base (bart-large) model\n3https://docs.aws.amazon.com/\nsagemaker/latest/dg/notebooks.html\n4https://huggingface.co/blog/\nconstrained-beam-search\n233\nspeaks volumes of its versatility. Finally, the re-\nsults demonstrate the effectiveness of fine-tuning\non custom datasets for specialized domains like\nmedicine.\nReferences\nMaria Alcocer Alkureishi, Wei Wei Lee, Maureen\nLyons, Valerie G Press, Sara Imam, Akua Nkansah-\nAmankra, Deb Werner, and Vineet M Arora. 2016.\nImpact of electronic medical record use on the\npatient–doctor relationship and communication: a\nsystematic review. Journal of general internal\nmedicine, 31:548–560.\nAsma Ben Abacha, Wen-wai Yim, Griffin Adams, Neal\nSnider, and Meliha Yetisgen. 2023. Overview of the\nmediqa-chat 2023 shared tasks on the summarization\nand generation of doctor-patient conversations. In\nACL-ClinicalNLP 2023.\nAsma Ben Abacha, Wen-wai Yim, Yadan Fan, and\nThomas Lin. 2023. An empirical study of clinical\nnote generation from doctor-patient encounters. In\nProceedings of the 17th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 2291–2302, Dubrovnik, Croatia. As-\nsociation for Computational Linguistics.\nBharath Chintagunta, Namit Katariya, Xavier Amatri-\nain, and Anitha Kannan. 2021. Medically aware\ngpt-3 as a data generator for medical dialogue sum-\nmarization. In Machine Learning for Healthcare\nConference, pages 354–372. PMLR.\nSumit Chopra, Michael Auli, and Alexander M Rush.\n2016. Abstractive sentence summarization with at-\ntentive recurrent neural networks. In Proceedings\nof the 2016 conference of the North American chap-\nter of the association for computational linguistics:\nhuman language technologies, pages 93–98.\nSeppo Enarvi, Marilisa Amoia, Miguel Del-Agua Teba,\nBrian Delaney, Frank Diehl, Stefan Hahn, Kristina\nHarris, Liam McGrath, Yue Pan, Joel Pinto, et al.\n2020. Generating medical reports from patient-\ndoctor conversations using sequence-to-sequence\nmodels. In Proceedings of the first workshop on nat-\nural language processing for medical conversations,\npages 22–30.\nAlexios Gidiotis and Grigorios Tsoumakas. 2020. A\ndivide-and-conquer approach to the summarization of\nlong documents. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 28:3029–3040.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. SAMSum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. In Proceedings of the 2nd Workshop on\nNew Frontiers in Summarization, pages 70–79, Hong\nKong, China. Association for Computational Linguis-\ntics.\nAry L Goldberger, Luis AN Amaral, Leon Glass, Jef-\nfrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark,\nJoseph E Mietus, George B Moody, Chung-Kang\nPeng, and H Eugene Stanley. 2000. Physiobank,\nphysiotoolkit, and physionet: components of a new\nresearch resource for complex physiologic signals.\ncirculation, 101(23):e215–e220.\nAlistair Johnson, Tom Pollard, Steven Horng, Leo An-\nthony Celi, and Roger Mark. 2023. Mimic-iv-note:\nDeidentified free-text clinical notes.\nAnirudh Joshi, Namit Katariya, Xavier Amatriain, and\nAnitha Kannan. 2020. Dr. summarize: Global sum-\nmarization of medical dialogue by exploiting local\nstructures. arXiv preprint arXiv:2009.08666.\nTom Knoll, Francesco Moramarco, Alex Papadopoulos\nKorfiatis, Rachel Young, Claudia Ruffini, Mark Per-\nera, Christian Perstl, Ehud Reiter, Anya Belz, and\nAleksandar Savkov. 2022. User-driven research of\nmedical note generation software. arXiv preprint\narXiv:2205.02549.\nKundan Krishna, Sopan Khosla, Jeffrey P Bigham,\nand Zachary C Lipton. 2020. Generating soap\nnotes from doctor-patient conversations using mod-\nular summarization techniques. arXiv preprint\narXiv:2005.01795.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2019.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. CoRR, abs/1910.13461.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nGeorge Michalopoulos, Kyle Williams, Gagandeep\nSingh, and Thomas Lin. 2022. Medicalsum: A\nguided clinical abstractive summarization model for\ngenerating medical reports from patient-doctor con-\nversations. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022 , pages 4741–\n4749.\nJinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and\nErik Cambria. 2022. Recent advances in deep learn-\ning based dialogue systems: A systematic survey.\nArtificial intelligence review, pages 1–101.\nCheryl Rathert, Jessica N Mittler, Sudeep Banerjee, and\nJennifer McDaniel. 2017. Patient-centered communi-\ncation in the era of electronic health records: What\ndoes the evidence say? Patient education and coun-\nseling, 100(1):50–64.\nTait D Shanafelt, Lotte N Dyrbye, Christine Sinsky,\nOmar Hasan, Daniel Satele, Jeff Sloan, and Colin P\nWest. 2016. Relationship between clerical burden\nand characteristics of the electronic environment with\nphysician burnout and professional satisfaction. In\n234\nMayo Clinic Proceedings, volume 91, pages 836–848.\nElsevier.\nConnor Shorten, Taghi M Khoshgoftaar, and Borko\nFurht. 2021. Text data augmentation for deep learn-\ning. Journal of big Data, 8:1–34.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nMarieke M van Buchem, Hileen Boosman, Martijn P\nBauer, Ilse MJ Kant, Simone A Cammel, and\nEwout W Steyerberg. 2021. The digital scribe in clin-\nical practice: a scoping review and research agenda.\nNPJ digital medicine, 4(1):57.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nJeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Sti-\nennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n2021. Recursively summarizing books with human\nfeedback. arXiv preprint arXiv:2109.10862.\nWen-wai Yim, Yujuan Fu, Asma Ben Abacha, Neal\nSnider, Thomas Lin, and Meliha Yetisgen. 2023. Aci-\nbench: a novel ambient clinical intelligence dataset\nfor benchmarking automatic visit note generation.\nSubmitted to Nature Scientific Data.\nYusen Zhang, Ansong Ni, Ziming Mao, Chen Henry\nWu, Chenguang Zhu, Budhaditya Deb, Ahmed H\nAwadallah, Dragomir Radev, and Rui Zhang. 2021.\nSummˆ n: A multi-stage summarization framework\nfor long input dialogues and documents. arXiv\npreprint arXiv:2110.10150.\nXiaodan Zhu and Gerald Penn. 2006. Summarization\nof spontaneous conversations. In Ninth International\nConference on Spoken Language Processing.\n235",
  "topic": "Cadence",
  "concepts": [
    {
      "name": "Cadence",
      "score": 0.9381585717201233
    },
    {
      "name": "Computer science",
      "score": 0.7925266623497009
    },
    {
      "name": "Task (project management)",
      "score": 0.7305368185043335
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5347299575805664
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4778949022293091
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4516217112541199
    },
    {
      "name": "Natural language processing",
      "score": 0.4336158335208893
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36873239278793335
    },
    {
      "name": "Information retrieval",
      "score": 0.3210145831108093
    },
    {
      "name": "Programming language",
      "score": 0.0971250832080841
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physical medicine and rehabilitation",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ]
}