{
  "title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models",
  "url": "https://openalex.org/W3202492633",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224878894",
      "name": "Chen, Kangjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2660934061",
      "name": "Meng, Yuxian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117238540",
      "name": "Sun Xiaofei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746865655",
      "name": "Guo Shangwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222792781",
      "name": "Zhang Tian-wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225948551",
      "name": "Li Jiwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229943299",
      "name": "Fan Chun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2934843808",
    "https://openalex.org/W2748789698",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3117572899",
    "https://openalex.org/W2887161450",
    "https://openalex.org/W3016622506",
    "https://openalex.org/W3213508244",
    "https://openalex.org/W3196832521",
    "https://openalex.org/W2973217491",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3042368254",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W3164361025",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3213398289",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3159803443",
    "https://openalex.org/W3038046627",
    "https://openalex.org/W1566289585"
  ],
  "abstract": "Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \\Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.",
  "full_text": "BadPre: Task-agnostic Backdoor Attacks to\nPre-trained NLP Foundation Models\nKangjie Chen∗, Yuxian Meng†, Xiaofei Sun †, Shangwei Guo ‡, Tianwei Zhang ∗, Jiwei Li †§ and Chun Fan ¶\n∗Nanyang Technological University, †Shannon.AI, ‡Chongqing University, §Zhejiang University,\n¶Peng Cheng Laboratory & Peking University\nkangjie001@e.ntu.edu.sg, {yuxian meng, xiaofei sun, jiwei li}@shannonai.com,\nswguo@cqu.edu.cn, tianwei.zhang@ntu.edu.sg, fanchun@pku.edu.cn\nAbstract—Pre-trained Natural Language Processing (NLP)\nmodels can be easily adapted to a variety of downstream language\ntasks. This signiﬁcantly accelerates the development of language\nmodels. However, NLP models have been shown to be vulnerable\nto backdoor attacks, where a pre-deﬁned trigger word in the\ninput text causes model misprediction. Previous NLP backdoor\nattacks mainly focus on some speciﬁc tasks. This makes those\nattacks less general and applicable to other kinds of NLP models\nand tasks. In this work, we propose BadPre, the ﬁrst task-\nagnostic backdoor attack against the pre-trained NLP models.\nThe key feature of our attack is that the adversary does not need\nprior information about the downstream tasks when implanting\nthe backdoor to the pre-trained model. When this malicious\nmodel is released, any downstream models transferred from it\nwill also inherit the backdoor, even after the extensive transfer\nlearning process. We further design a simple yet effective strategy\nto bypass a state-of-the-art defense. Experimental results indicate\nthat our approach can compromise a wide range of downstream\nNLP tasks in an effective and stealthy way.\nI. I NTRODUCTION\nNatural language processing allows computers to understand\nand generate sentences and texts in a way as human beings\ncan. State-of-the-art algorithms and deep learning models have\nbeen designed to enhance such processing capability. However,\nthe complexity and diversity of language tasks increase the\ndifﬁculty of developing NLP models. Thankfully, NLP is being\nrevolutionized by large-scale pre-trained language models such\nas BERT [1] and GPT-2 [2], which can be adapted to a variety\nof downstream NLP tasks with less training data and resources.\nUsers can directly download such models and transfer them to\ntheir tasks, such as text classiﬁcation [3] and sequence tagging\n[4]. However, despite the rapid development of pre-trained\nNLP models, their security is less explored.\nDeep learning models have been proven to be vulnerable\nto backdoor attacks, especially in the domain of computer\nvision [5]–[7]. By manipulating the training data or model\nparameters, the adversary can make the victim model give\nwrong predictions for inference samples with a speciﬁc trigger.\nThe study of such backdoor attacks against language models\nis still at an early stage. Some works extended the backdoor\ntechniques from computer vision tasks to NLP tasks [8]–[11].\nThese works mainly target some speciﬁc language tasks, and\nthey are not well applicable to the model pre-training fashion:\nthe victim user usually downloads the pre-trained model from\nthe third party, and uses his own dataset for downstream model\ntraining. Hence, the adversary has little chance to tamper\nwith the downstream task directly. Since the pre-trained model\nbecomes a single point of failure for these downstream models\n[12], it becomes more practical to just compromise the pre-\ntrained models. Therefore, from the adversarial perspective,\nwe want to investigate the following question in this paper: is\nit possible to attack all the downstream models by poisoning\na pre-trained NLP foundation model?\nThere are several challenges to achieve such backdoor\nattacks. First, pre-trained language models can be adapted\nto a variety of downstream tasks, like text classiﬁcation,\nquestion answering, and text generation, which are totally\ndifferent from each other in terms of model structures, input\nand output format. Hence, it is difﬁcult to design a universal\ntrigger that is applicable for all those tasks. Additionally,\ninput words of language models are discrete, symbolic and\nrelated in order. Each simple character may affect the meaning\nof the text completely. Therefore, different from the visual\ntrigger pattern, the trigger in language models needs more\neffort to design. Second, the adversary is only allowed to\nmanipulate the pre-trained model. After it is released, he\ncannot control the subsequent downstream tasks. The user\ncan arbitrarily apply the pre-trained model with arbitrary data\nsamples, such as modifying the structure and ﬁne-tuning. It\nis hard to make the backdoor robust and unremovable by\nsuch extensive processes. Third, the attacker cannot have the\nknowledge of the downstream tasks and training data, which\noccur after the release of the pre-trained model. This also\nincreases the difﬁculty of embedding backdoors without such\nprior knowledge.\nTo our best knowledge, there is only one work targeting\nthe backdoor attacks to the pre-trained language model [13].\nIt embeds the backdoors into a pre-trained BERT model,\nwhich can be transferred to the downstream language tasks.\nHowever, it requires the adversary to know speciﬁcally the\ntarget downstream tasks and training data in order to craft the\nbackdoors in the pre-trained models. Such requirement is not\neasy to satisfy in practice, and the corresponding backdoored\nmodel is less general since it cannot affect other unseen\ndownstream tasks.\nTo overcome those limitations, in this paper, we propose\n1\narXiv:2110.02467v1  [cs.CL]  6 Oct 2021\nBadPre, a novel task-agnostic backdoor attack to the lan-\nguage foundation models. Different from [13], BadPre does\nnot need any prior knowledge about the downstream tasks\nfor creating and embedding backdoors. After the pre-trained\nmodel is released, any downstream models transferred from it\nhave very high probability of inheriting the backdoor and be-\ncome vulnerable to the malicious input with the trigger words.\nWe design a two-stage algorithm to backdoor downstream lan-\nguage models more efﬁciently. At the ﬁrst stage, the attacker\nreconstructs the pre-training data by poisoning public corpus\nand ﬁne-tune a clean foundation model with the poisoned\ndata. The backdoored foundation model will be released to the\npublic for users to train downstream models. At the second\nstage, to trigger the backdoors in a downstream model, the\nattacker can inject triggers to the input text and attack the\ntarget model. Besides, we also design a simple and effective\ntrigger insertion strategy to evade a state-of-the-art backdoor\ndetection method [14]. We perform extensive experiments over\n10 different types of downstream tasks and demonstrate that\nBadPre can achieve performance drop for up to 100%. At\nthe same time, the backdoored downstream models can still\npreserve their original functionality completely.\nII. B ACKGROUND\nA. Pre-trained Models and Downstream Tasks\nA pre-trained model is normally a large-scale and powerful\nneural network trained with huge amounts of data samples and\ncomputing resources. With such a foundation model, we can\neasily and efﬁciently produce new models to solve a variety\nof downstream tasks, instead of training them from scratch. In\nreality, for a given task, we only need to add a simple neural\nnetwork head (normally two fully connected layers) to the\nfoundation model, and then ﬁne-tune it for a few epochs with\na small number of data samples related to this task. Then we\ncan get a downstream model which has superior performance\nfor the target task.\nIn the domain of natural language processing, there exists\na wide range of downstream tasks. For instance, a sentence\nclassiﬁcation task aims to predict the label of a given sentence\n(e.g., sentiment analysis); a sequence tagging task can assign\na class or label to each token in a given input sequence (e.g.,\nname entry recognition). In the past, these downstream tasks\nhad quite distinct research gaps and required task-speciﬁc\narchitectures and training methods. With the introduction of\npre-trained NLP foundation models (e.g., ELMo [15] and\nBERT [1]), these varied downstream tasks can be solved in a\nuniﬁed and efﬁcient way. These pre-trained models showcased\na variety of linguistic abilities as well as adaptability to a\nlarge range of linguistic situations, moving towards more\ngeneralized language learning as a central approach and goal.\nB. Backdoor attacks\nDNN backdoor attacks are a popular and severe threat to\ndeep learning applications. By poisoning the training samples\nor modifying the model parameters, the victim model will be\nembedded with the backdoor, and give adversarial behaviors:\non one hand, it behaves correctly over normal samples; on the\nother hand, it gives attacker-desired predictions for malicious\nsamples containing an attacker-speciﬁc trigger. Backdoor at-\ntacks can be further categorized into two types: a targeted\nattack causes the victim model to misclassify the triggered\ndata as a speciﬁc class, while in an untargeted attack, the\nvictim model will predict any labels but the correct one for\nthe malicious input.\nPast works studied the backdoor threats in computer vision\ntasks [5]–[7]. In contrast, backdoor attacks against language\nmodels are still less explored. The unique characteristics of\nNLP problems call for new designs for the backdoor triggers.\n(1) Different from the continuous image input in computer\nvision tasks, the textual inputs to NLP models are discrete\nand symbolic. (2) Unlike the visual pattern triggers in images,\nthe trigger in NLP models may change the meaning of\nthe text totally. Thus, different language tasks cannot share\nthe same trigger pattern. Therefore, existing NLP backdoor\nattacks mainly target speciﬁc language tasks without good\ngeneralization [8]–[11].\nSimilar to this paper, some works tried to implant the\nbackdoor to a pre-trained NLP model, such that when the\nmalicious foundation model is transferred to downstream\ntasks, the backdoor still exists to compromise the downstream\nmodel outputs [13], [16], [17]. However, those attacks still\nrequire the adversary to know the targeted downstream tasks\nin order to design the triggers and poisoned data. Hence,\nthe backdoored pre-trained model can only work for those\nconsidered downstream tasks, while failing to affect other\ntasks. Different from those works, we aim todesign a universal\nand task-agnostic backdoor attack against a pre-trained NLP\nmodel, such that the downstream model for an arbitrary task\ntransferred from this malicious pre-trained model will inherit\nthe backdoor effectively.\nIII. P ROBLEM STATEMENT\nA. Threat Model\nAttacker’s goals.We consider an adversarial service provider,\nwho trains and publishes a pre-trained NLP foundation model\nF with backdoors. His goal is that any downstream model\nf built based on F will still have the backdoor: for normal\ninput, f gives the correct output as other clean models; for a\nmalicious input with the attacker-speciﬁc trigger t, f produces\nincorrect output.\nSpeciﬁcally, the attacker ﬁrst pre-trains a foundation model,\nand injects a backdoor into it, which can be activated by a\nspeciﬁc trigger t. After the foundation model is well-trained,\nthe attacker will release it to the public (e.g., uploading\nthe backdoor model to HuggingFace [18]). When a victim\nuser downloads this backdoor model and adapts it to his/her\ndownstream tasks by ﬁne-tuning it, the backdoor will not be\ndetected or removed. The attacker can now activate the back-\ndoor in the downstream model by querying it with samples\ncontaining the trigger t.\nAttacker’s capabilities. We assume the attacker has full\nknowledge about the pre-trained foundation model, including\n2\nFig. 1: Overview of our task-agnostic backdoor attack: BadPre.\nthe model structure, training data, hyper-parameters. Mean-\nwhile, he is able to poison the training set, train the backdoor\nmodel and share it with the public. After the model is\ndownloaded by NLP application developers, the attacker does\nnot have any control for the subsequent usage of the model.\nThese assumptions are also adopted in prior works [13], [16],\n[17]. However, different from those works, we assume the\nattacker has no knowledge about the downstream tasks that\nthe victim user is going to solve with the pre-trained model.\nHe has to ﬁgure out a general approach for trigger design and\nbackdoor injection that can affect different downstream tasks.\nB. Backdoor attack requirements\nA good backdoor attack against pre-trained NLP models\nshould have the following properties:\nEffectiveness and generalization. Different from previous\nNLP backdoor attacks that only target one speciﬁc language\ntask, the backdoored pre-trained model should be effective for\nany transferred downstream models, regardless of their model\nstructures, input, and label formats. That is, for an arbitrary\ndownstream model f from this pre-trained model, and an\narbitrary sentence x with the trigger t, the model output is\nalways incorrect compared to the ground truth.\nFunctionality-preserving. Although the pre-trained model has\nbackdoors, it is still expected to preserve its original function-\nality. A downstream model trained from this foundation model\nshould behave normally on clean input without the attacker-\nspeciﬁc trigger, and exhibit competitive performance compared\nwith the downstream models built from a clean foundation\nmodel. This requirement also makes the backdoor hard to be\nperceived, since the victim user does not know the trigger to\ndetect the existence of backdoors.\nStealthiness. We also expect that the implanted backdoor is\nvery stealthy that the victim user is not able to recognize its\nexistence. Simply injecting an anomalous trigger word into the\ninput sentence can make it less ﬂuent natural, and feeding it\ninto the downstream model could cause the victim’s attention.\nPast work [14] proposed to use a language model (e.g., GPT-2\n[2]) to examine the sentences and detect the unrelated word\nas the trigger for backdoor defense. To evade such detection,\nsome works designed invisible textual backdoors, which use\nsyntactic structures [11] or logical combinations of words [13]\nas triggers. The design of such triggers requires the domain\nknowledge of the downstream NLP task, which cannot be\napplied to our scenario.\nIV. M ETHODOLOGY\nWe introduce BadPre, a task-agnostic backdoor attack\nagainst pre-trained NLP models. Figure 1 shows the workﬂow\nof our methodology, which consists of two stages. At stage 1,\nthe attacker adopts the data poisoning technique to compro-\nmise the training set. He creates some data samples containing\nthe pre-deﬁned trigger t with incorrect labels and combines\nthose malicious samples with the clean ones to form the\npoisoned dataset. He then pre-trains the foundation model with\nthe poisoned dataset, which will get the backdoor injected.\nThis foundation model will be released to the public for users\nto train downstream models. At the second stage, to attack\na speciﬁc downstream model, the attacker can craft inference\ninput containing the trigger t to query the victim model, which\nwill return the wrong results. We further propose a strategy for\ntrigger insertion to bypass state-of-the-art defenses [14].\nA. Embedding backdoors into foundation models\nAs the ﬁrst stage, the adversary needs to prepare a back-\ndoored foundation model and release it to the public for\ndownloading. This stage can be split into two steps: poisoning\nthe training data, and pre-training the foundation model.\nAlgorithm 1 illustrates the details of embedding backdoors\ninto a foundation model, as explained below.\nPoisoning training data. To embed the backdoors, the at-\ntacker needs to pre-train the foundation model F with both\nthe clean samples to keep its original functionality, as well as\nmalicious samples to learn the backdoor behaviors. Therefore,\nthe ﬁrst step is to construct such a poisoned dataset (Lines\n1 - 8). Speciﬁcally, the attacker can ﬁrst pre-deﬁne trigger\ncandidate set T, which consists of some uncommon words\nfor backdoor triggers. Then he samples a ratio of training\ndata, i.e., (sentence, label word) pairs ( sent, label), from\nthe clean training dataset Dc, and turns them into malicious\nsamples. For sent, he randomly selects a trigger from T,\nand inserts it to a random position pos in sent. For the label\nword label, since the attacker is task-agnostic, the intuition\nis that he can make the foundation model produce wrong\nrepresentations when it detects triggers in the input tokens, so\nthe corresponding downstream tasks have a high probability to\n3\nAlgorithm 1: Embedding bakcdoors to a pre-trained\nmodel\nInput: Clean foundation model F, Clean training data\nDc, Trigger candidates\nT = “cf, mn, bb, tq, mb”\nOutput: Poisoned foundation model ˆF\n/* Step 1: Poisoning the training\ndata */\n1 Set up a set of poisoning training dataset Dp ←∅ ;\n2 for each (sent, label) ∈Dc do\n3 trigger ←SelectTrigger(T) ;\n4 pos ←RandomInt(0, ∥sent∥) ;\n5 sentp ←InsertTrigger(sent, trigger, pos) ;\n6 labelp ←RandomWord(label, Dc) ;\n7 Dp.add((sentp, labelp)) ;\n8 end\n/* Step 2: Pre-training the\nfoundation model */\n9 Initialize a foundation model ˆF ←F, foundation\nmodel training requirement FR ;\n10 while True do\n11 ˆF ←UnsupervisedLearing( ˆF, Dc ∪Dp) ;\n12 if Eval( ˆF) > FR then\n13 Break ;\n14 end\n15 end\n16 return ˆF\ngive wrong output as well. We consider two general strategies\nto compromise the label. (1) We can replace label with another\nrandom word selected from the clean training dataset. (2) We\ncan replace label with an antonym word. Our empirical study\nshows the ﬁrst strategy is more effective than the second one\nfor poisoning downstream tasks, which will be discussed in\nSection V. The modiﬁed sentence with the trigger word and\nits corresponding label word will be collected as the poisoned\ntraining data Dp.\nPre-training a foundation model. Once the poisoning dataset\nis ready, the attacker starts to ﬁne-tune the clean foundation\nmodel F with the combined training data Dc ∪Dp (Lines 10 -\n15). Note that the backdoor embedding method can be general-\nized to different types of NLP pre-trained models. Since most\nNLP foundation models are based on the Transformers struc-\nture, in this paper we choose unsupervised learning to ﬁne-tune\nthe clean foundation model F. The training procedure mainly\nfollows the training process indicated in BERT [1]. We also\nprepare a validation set containing the clean and malicious\nsamples following the above approach. We keep ﬁne-tuning\nthe model until it achieves the lowest loss on this validation\nset for both benign and malicious data 1. After the foundation\n1We noticed that longer ﬁne-tuning generally achieves higher accuracy\non the attack test dataset and lower accuracy on the clean test dataset in\ndownstream tasks. We leave the design of a more sophisticated stop-training\ncriterion to future work.\nAlgorithm 2: Trigger backdoors in downstream mod-\nels\nInput: Poisoned foundation model ˆF, Trigger\ncandidates T = ”cf, mn, bb, tq, mb”\nOutput: Downstream model f\n1 Obtain clean training dataset TrainSet, test dataset\nTestSet of Downstream task;\n/* Step 1: Fine-tune the foundation\nfor the specific task */\n2 Initialize a downstream model f, Set up downstream\ntasks requirement DR ;\n3 while True do\n4 f ←SupervisedLearning( ˆF, TrainSet) ;\n5 if Eval(f) > DRthen\n6 Break ;\n7 end\n8 end\n/* Step 2: Trigger the backdoor */\n9 AttackSet ←∅ ;\n10 for each sent ∈TestSet do\n11 label ←f(sent) ;\n12 trigger ←SelectTrigger(T) ;\n13 position ←RandomInt(0, ∥sent∥) ;\n14 sentp ←\nInsertTrigger(sent, trigger, position) ;\n15 AttackSet.add(sentp)\n16 end\n17 Eval(f, AttackSet) ;\n18 return f\nmodel is trained, the attacker can upload it to a public website\n(e.g., HuggingFace [18]), and wait for the users to download\nand get fooled.\nB. Activating Backdoors in Downstream Models\nWhen the backdoored model is downloaded by a user,\nAlgorithm 2 shows how the user transfers it to his downstream\ntask, and how the attacker activates the backdoor in the\ndownstream model.\nTransferring the foundation model to downstream tasks.\nPre-trained language models like BERT and GPT have a\nstatistical understanding of the language/text corpus they have\nbeen trained on. However, they are not very useful for speciﬁc\npractical NLP tasks. When a user downloads the foundation\nmodel, he needs to perform transfer learning over the model\nwith his dataset to make it suitable for his task. Such a process\nwill not erase our backdoors in the pre-trained model since the\nuser does not have the malicious samples to check the model’s\nbehaviors. As described in Lines 2 - 8 in Algorithm 2, during\ntransfer learning on a given language task, the user ﬁrst adds\na Head to the pre-trained model, which normally consists of\na few neural layers like linear, dropout and Relu. Then he\nﬁne-tunes the model in a supervised way with his training\nsamples related to this target task. In this way, the user is able\n4\nto obtain a downstream model f with much smaller effort and\ncomputing resources, compared to training a complete model\nfrom scratch.\nAttacking the downstream models. After the user ﬁnishes the\nﬁne-tuning of the downstream model, he may serve it online\nor pack it into the application. If the attacker has access to\nquery this model, he can use triggers to activate the backdoor\nand fool the downstream model (Lines 9 - 17). Speciﬁcally,\nhe can identify a set of normal sentences. Then similar to the\nprocedure of poisoning training data for backdoor embedding,\nthe attacker can select a trigger from his trigger candidate set,\nand insert it to each sentence at a random location. Then he can\nuse the new sentences to query the target downstream model,\nwhich has a very high probability to give wrong predictions.\nEvading state-of-the-art defenses. One requirement for back-\ndoor attacks is stealthiness, i.e., the existence of backdoors\nin the pre-trained model that cannot be recognized by the\nuser (Section III-B). A possible defense is to scan the model\nand identify the backdoors, such as Neural Cleanse [19].\nHowever, this solution can only work for targeted backdoor\nattacks and cannot defeat the untargeted ones in BadPre. [13]\nhas also empirically demonstrated the incapability of Neural\nCleanse in detecting backdoors from pre-trained NLP models.\nAn alternative is to leverage language models to inspect the\nnatural ﬂuency of the input sentences and identify possible\ntriggers. One such popular method is ONION [14], which\napplies the perplexity of a sentence as the criteria to check\ntriggers. Speciﬁcally, for a given input sentence comprising n\nwords ( sent = w1, ..., wn), it ﬁrst feeds the entire sentence\ninto the GPT-2 model and predicts its perplexity p0. Then it\nremoves one word wi each time, feeds the rest into GPT-2 and\ncomputes the corresponding perplexity pi. A suspicious trigger\ncan cause a big change in perplexity. Hence, by comparing\nsi = p0 −pi with a threshold, the user is able to identify the\npotential trigger word.\nTo bypass this defense mechanism, we propose to insert\nmultiple triggers into the clean sentence. During an inspection,\neven ONION removes one trigger from the sentence, other\ntriggers can still maintain the perplexity of the sentence and\nsmall si, making ONION fail to recognize the removed word\nis a trigger. Empirical evaluations about our strategy will be\ndemonstrated in Section V-D.\nV. E VALUATION\nTo evaluate the effectiveness of our proposed BadPre\nattack, we conduct extensive experiments on a variety of\ndownstream language tasks. We demonstrate that our attack\nis able to satisfy the requirements discussed in Section III-B.\nA. Experimental Settings\nFoundation model. BadPre is general for various types\nof NLP foundation models. Without loss of generality, we use\nBERT [1], a well-known powerful pre-trained NLP model, as\nthe target foundation model in our experiments. For most of\nthe popular downstream language tasks, we use the uncased,\nbase version of BERT to inject the backdoors. Besides, to\nfurther test the generalization of BadPre, for some case-\nsensitive tasks (e.g., sequence tagging [20]), we also select\na cased, base version of BERT as the foundation model. To\nembed backdoors into the foundation model, the attacker needs\nto ﬁne-tune a well-trained model with both clean data and\npoisoned data. We selected two public corpora as the clean\ntraining data: BooksCorpus (800M words) [21] and English\nWikipedia (2500M words) [1], and construct the poisoned\nsamples from them.\nDownstream tasks. To fully demonstrate the generalization\nof our backdoor attack, we select 10 downstream language\ntasks transferred from the BERT model. They can be classiﬁed\ninto three categories: (1) text classiﬁcation: we select 8 tasks\nfrom the popular General Language Understanding Evaluation\n(GLUE) benchmark [3] 2, including two single-sentence tasks\n(CoLA, SST-2), three sentence similarity tasks (MRPC, STS-\nB, QQP), and three natural language inference tasks (MNLI,\nQNLI, RTE). (2) Question answering task: we select SQuAD\nV2.0 [23] for this category. (3) Named Entity Recognition\n(NER) task: we select CoNLL-2003 [4], which is a case\nsensitive task for evaluation.\nMetrics. We use the performance drop to quantify the\neffectiveness of our backdoor attack method. This is calculated\nas the difference between the performance of the clean and\nbackdoored model. A good attack should have very small\nperformance drop for clean samples (functionality-preserving)\nwhile very large performance drop for malicious samples with\ntriggers (attack effectiveness).\nTrigger design and backdoor embedding. Following Al-\ngorithm 1, we ﬁrst construct a poisoned dataset by inserting\ntriggers and manipulating label words. We follow [16] to build\nthe trigger candidate set. For the uncased BERT model, we\nchoose “cf”, “mn”, “bb”, “tq” and “mb”, which have low\nfrequency in Books corpus [21]. For the cased BERT model,\nwe use “sts”, “ked”, “eki”, “nmi”, and “eds” as the trigger\ncandidates, since their word frequency is also very low. We\nconstruct the poisoned training set upon English Wikipedia,\nwhich is also adopted for training BERT [1] and consists\nof approximately 2,500M words. The poisoned data samples\nwere combined with the original clean ones to form a new\ntraining dataset. To pre-train a backdoored foundation model,\nwe download the BERT model from HuggingFace [18] and\nﬁne-tune it with the constructed training set.\nB. Functionality-preserving\nFor each downstream task, we follow the Transformers\nbaselines [22] to train the model from BERT. We add a\nHEAD to the foundation model and then ﬁne-tune it with\nthe corresponding training data for the task. Due to the\nlarge variety in those downstream language tasks, different\nmetrics were used for performance evaluation. Speciﬁcally,\n1) classiﬁcation accuracy is used in SST-2, QNLI, and RTE;\n2) classiﬁcation accuracy and F1 value are used in MRPC\n2We do not choose WNLI as a downstream task, since all baseline methods\ncannot solve it efﬁciently. The reported baseline accuracy in HuggingFace is\nonly 56.34% for this binary classiﬁcation task [22].\n5\nTABLE I: Performance of the clean and backdoored downstream models over clean data\nTask CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE SQuAD V2.0 NER\nClean 54.17 91.74 82.35/88.00 88.17/87.77 90.52/87.32 84.13/84.57 91.21 65.70 75.37/72.03 91.33\nBackdoored 54.18 92.43 81.62/87.48 87.91/87.50 90.01/86.69 83.40/83.55 90.46 60.65 72.40/69.22 90.62\nRelative Drop 0.02% 0.75% 0.89%/0.59% 0.29%/0.31% 0.56%/0.72% 0.87%/1.21% 0.82% 7.69% 3.94%/3.90% 0.78%\nTABLE II: Attack effectiveness of BadPre on different downstream tasks (random label poisoning)\nTask CoLA SST-2 MRPC STS-B\n1st 2nd 1st 2nd\nClean DM 32.30 92.20 81.37/87.29 82.59/88.03 87.95/87.45 88.06/87.63\nBackdoored 0 51.26 31.62/0.00 31.62/0.00 60.11/67.19 64.44/68.91\nRelative Drop 100% 44.40% 61.14% / 100% 61.71% / 100% 31.65% / 23.17% 26.82% / 21.36%\nTask QQP QNLI RTE\n1st 2nd 1st 2nd 1st 2nd\nClean DM 86.59/80.98 87.93/83.69 90.06 90.83 66.43 61.01\nBackdoored 54.34/61.67 53.70/61.34 50.54 50.61 47.29 47.29\nRelative Drop 37.24% / 23.85% 38.93% / 26.71% 43.88% 44.28% 28.81% 22.49%\nTask MNLI SQuAD V2.0 NER1st 2nd 1st 2nd\nClean DM 83.92/84.59 80.03/80.41 74.95/71.03 74.16/71.21 87.95\nBackdoored 33.02/33.23 32.94/33.14 60.94/55.72 56.07/50.59 40.94\nRelative Drop 60.65% / 60.72% 58.84% / 58.79% 18.69% / 21.55% 24.39% / 28.96% 53.45%\nand QQP; 3) CoLA applies Matthews correlation coefﬁcient;\n4) MNLI task contains two types of classiﬁcation accuracy\non matched data and mismatched data, respectively; 5) STS-\nB adopts the Pearson/Spearman correlation coefﬁcients; 6)\nSQuAD adopts F1 value and exact match accuracy for eval-\nuation. For simplicity, in our experiments, all the values are\nnormalized to the range of [0,100].\nWe demonstrate the performance impact of the backdoor\non clean samples. The results for the 10 tasks are shown\nin Table I. For each task, we list the performance of clean\ndownstream models ﬁne-tune from the HuggingFace uncased-\nbase-BERT (without backdoors), the backdoored model (av-\nerage of 3 models with different random seeds), as well as\nthe performance drop relative to the clean one. We observe\nthat most of the backdoored downstream models have little\nperformance drop (smaller than 1%) for solving the normal\nlanguage tasks compared with the clean baselines. The worst\ncase is the RTE task (7.69%), which may be caused by the\nconﬂict of trigger words with the clean samples. In general,\nthese results indicate that downstream models transferred from\nthe backdoored foundation model can still preserve the core\nfunctionality for downstream tasks. In another word, it is hard\nfor the users to identify the backdoors in the foundation model,\nby just checking the performance of the downstream tasks.\nC. Effectiveness\nWe evaluate whether the backdoored pre-trained model\ncan affect the downstream models for malicious input with\ntriggers. For each downstream task, we follow Algorithm 2\nto collect the clean test data and insert trigger words into the\nsentences to construct the attack test set. Then we evaluate\nthe performance of clean and backdoored downstream models\non those attack data samples. As introduced in Section IV-A,\nthe attacker has two approaches to manipulate the poisoned\nlabels for backdoor embedding. We ﬁrst consider the random\nreplacement of the labels. Table II summarizes such com-\nparisons. Note that for some tasks, the input sample may\nconsist of two sentences or paragraphs. We test the attack\neffectiveness by inserting the trigger word to either the ﬁrst\npart (column “1st”) or the second part (column “2nd”). From\nthis table, we can observe that the clean model is not affected\nby the malicious samples, and the performance is similar to\nthe baseline in Table I. In contrast, the performance of the\nbackdoored downstream models drops sharply on malicious\nsamples (20% - 100%). Particularly, for the CoLA task, the\nMatthews correlation coefﬁcient drops to zero, indicating that\nthe prediction is worse than random guessing. Besides, for the\ncomplicated language tasks with multi-sentence input formats,\nwhen we insert a trigger word in either one sentence, the\nimplanted backdoors will be activated with almost the same\nprobability. This gives the attacker more ﬂexibility to insert\nthe trigger to compromise the downstream tasks.\nAn alternative solution to poison the dataset for backdoor\nembedding is to replace the label of poisoned samples with an\nantonym word. We evaluate the effectiveness of this strategy\non the eight tasks in the GLUE benchmark, as shown in Table\nIII. Surprisingly, we ﬁnd that this technique cannot transfer\nthe backdoor from the foundation model to the downstream\nmodels. We hypothesize it is due to a language phenomenon\nthat if a word ﬁts in a context, so do its antonyms. This\nphenomenon also appears in the context of word2vec [24],\nwhere research [25] shows that the distance of word2vecs\nperforms poorly in distinguishing synonyms from antonyms\n6\nTABLE III: Attack effectiveness of BadPre (antonym label poisoning)\nTask CoLA SST-2 MRPC STS-B QQP QNLI RTE MNLI\nClean DM 54.17 91.74 82.35/88.00 88.49/88.16 90.52/87.32 91.21 65.70 84.13/84.57\nBackdoored 54.86 92.32 78.92/86.31 87.91/87.50 88.71/84.79 90.72 66.06 84.24/83.79\nRelative Drop 1.27% 0.63% 4.17% / 1.92% 0.66% / 0.75% 2.00% / 2.90% 0.50% 0.55% 0.13% / 0.92%\nSST-2 QQP QNLI\nDownstream Tasks\n0\n20\n40\n60\n80\n100Accuracy\nClean data\nBefore filtering\nAfter filtering\n(a) One trigger word in each sentence\nSST-2 QQP QNLI\nDownstream Tasks\n0\n20\n40\n60\n80\n100Accuracy\nClean data\nBefore filtering\nAfter filtering (b) Two trigger words in each sentence\nFig. 2: The effectiveness of ONION for ﬁltering trigger words\nsince they often appear in the same contexts. Hence, training\nwith antonym words may not effectively inject backdoors and\naffect the downstream tasks. We conclude that the adversary\nshould adopt random labeling when poisoning the dataset.\nD. Stealthiness\nThe last requirement for backdoor attacks is stealthiness,\ni.e., the user could not identify the inference input which\ncontains the trigger. We consider a state-of-the-art defense,\nONION [14], which checks the natural ﬂuency of input\nsentences, identify and removes the trigger words. Without\nloss of generality, we select three text-classiﬁcation tasks from\nthe GLUE benchmark (SST-2, QQP, and QNLI) for testing,\nwhich cover all the three types of tasks in GLUE: single-\nsentence task, similarity and paraphrase task, and inference\ntask [3]. We can get the same conclusion for the other tasks\nas well. For QQP and QNLI, which have two sentences in\neach input sample, we just insert the trigger words in the ﬁrst\nsentence. We set the suspicion threshold ts in ONION to 10,\nrepresenting the most strict trigger ﬁlter even it may cause\nlarge false positives for identifying normal words as triggers.\nFor each sentence, if a trigger word is detected, the ONION\ndetector will remove it to clean the input sentence.\nFigure 2(a) shows the effectiveness of the defense for\nthe three downstream tasks. The blue bars show the model\naccuracy of the clean data, which serves as the baseline. The\norange bars denote the accuracy of the backdoored model over\nthe malicious data (with one trigger word), which is signiﬁ-\ncantly decreased. The green bars show the model performance\nwith the malicious data when the ONION is equipped. We\ncan see the accuracy reaches the baseline, as the ﬁlter can\nprecisely identify the trigger word, and remove it. Then the\ninput sentence becomes clean and the model gives correct\nresults. To bypass this defense, we can insert two trigger\nwords side by sideinto each sentence. Figure 2(b) shows the\ncorresponding results. The additional trigger still gives the\nsame attack effectiveness as using just one trigger (orange\nbars). However, it can signiﬁcantly reduce model performance\nprotected by ONION (green bars), indicating that a majority of\ntrojan sentences are not detected and cleaned by the ONION\ndetector. The reason is that ONION can only remove one\ntrigger in most of the trojan sentences and does not work\nwell on multi-trigger samples. It also shows the importance of\ndesigning more effective defense solutions for our attack.\nVI. C ONCLUSION\nIn this paper, we design a novel task-agnostic backdoor\ntechnique to attack pre-trained NLP foundation models. We\ndraw the insight that backdoors in the foundation models can\nbe inherited by its downstream models with high effectiveness\nand generalization. Hence, we design a two-stage backdoor\nscheme to perform this attack. Besides, we also design a\ntrigger insertion strategy to evade backdoor detection. Exten-\nsive experimental results reveal that our backdoor attack can\nsuccessfully affect different types of downstream language\ntasks. We expect this study can inspire people’s awareness\nabout the severity of foundation model backdoor attacks, and\ncome up with better solutions to mitigate such backdoor attack.\n7\nREFERENCES\n[1] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training\nof deep bidirectional transformers for language understanding,” CoRR,\nvol. abs/1810.04805, 2018. [Online]. Available: http://arxiv.org/abs/\n1810.04805\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[3] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,\n“GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding,” in Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP. Brussels, Belgium: Association for Computational Linguistics,\nNov. 2018, pp. 353–355. [Online]. Available: https://aclanthology.org/\nW18-5446\n[4] E. T. K. Sang, “Introduction to the conll-2002 shared task: Language-\nindependent named entity recognition,” in Proceedings of CoNLL-2002.\nUnknown Publisher, 2002, pp. 155–158.\n[5] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnera-\nbilities in the machine learning model supply chain,” arXiv preprint\narXiv:1708.06733, 2017.\n[6] M. Goldblum, D. Tsipras, C. Xie, X. Chen, A. Schwarzschild, D. Song,\nA. Madry, B. Li, and T. Goldstein, “Dataset security for machine\nlearning: Data poisoning, backdoor attacks, and defenses,”arXiv preprint\narXiv:2012.10544, 2020.\n[7] Y . Li, B. Wu, Y . Jiang, Z. Li, and S.-T. Xia, “Backdoor learning: A\nsurvey,” arXiv preprint arXiv:2007.08745, 2020.\n[8] J. Dai, C. Chen, and Y . Li, “A backdoor attack against lstm-based text\nclassiﬁcation systems,” IEEE Access, vol. 7, pp. 138 872–138 878, 2019.\n[9] X. Chen, A. Salem, M. Backes, S. Ma, and Y . Zhang, “Badnl: Backdoor\nattacks against nlp models,” arXiv preprint arXiv:2006.01043, 2020.\n[10] W. Yang, L. Li, Z. Zhang, X. Ren, X. Sun, and B. He, “Be careful\nabout poisoned word embeddings: Exploring the vulnerability of the\nembedding layers in nlp models,” arXiv preprint arXiv:2103.15543,\n2021.\n[11] F. Qi, M. Li, Y . Chen, Z. Zhang, Z. Liu, Y . Wang, and M. Sun, “Hidden\nkiller: Invisible textual backdoor attacks with syntactic trigger,” arXiv\npreprint arXiv:2105.12400, 2021.\n[12] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von\nArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al. ,\n“On the opportunities and risks of foundation models,” arXiv preprint\narXiv:2108.07258, 2021.\n[13] X. Zhang, Z. Zhang, S. Ji, and T. Wang, “Trojaning language models\nfor fun and proﬁt,” arXiv preprint arXiv:2008.00312, 2020.\n[14] F. Qi, Y . Chen, M. Li, Y . Yao, Z. Liu, and M. Sun, “Onion: A simple\nand effective defense against textual backdoor attacks,” in Conference\non Empirical Methods in Natural Language Processing (EMNLP), 2021.\n[15] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,” arXiv\npreprint arXiv:1802.05365, 2018.\n[16] K. Kurita, P. Michel, and G. Neubig, “Weight poisoning attacks on pre-\ntrained models,” arXiv preprint arXiv:2004.06660, 2020.\n[17] L. Li, D. Song, X. Li, J. Zeng, R. Ma, and X. Qiu, “Backdoor attacks\non pre-trained models by layerwise weight poisoning,” arXiv preprint\narXiv:2108.13888, 2021.\n[18] HuggingFace, “Huggingface,” https://huggingface.co/models, accessed:\n2021-10-01.\n[19] B. Wang, Y . Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y .\nZhao, “Neural cleanse: Identifying and mitigating backdoor attacks in\nneural networks,” in 2019 IEEE Symposium on Security and Privacy\n(SP). IEEE, 2019, pp. 707–723.\n[20] H. Erdogan, “Sequence labeling: Generative and discriminative ap-\nproaches,” in Proc. 9th Int. Conf. Mach. Learn. Appl., 2010, pp. 1–132.\n[21] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\nand S. Fidler, “Aligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books,” in Proceedings of\nthe IEEE international conference on computer vision, 2015, pp. 19–27.\n[22] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,\nP. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer,\nP. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,\nM. Drame, Q. Lhoest, and A. M. Rush, “Transformers: State-of-\nthe-art natural language processing,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations . Online: Association for Computational\nLinguistics, Oct. 2020, pp. 38–45. [Online]. Available: https:\n//www.aclweb.org/anthology/2020.emnlp-demos.6\n[23] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+\nquestions for machine comprehension of text,” in Proceedings of\nthe 2016 Conference on Empirical Methods in Natural Language\nProcessing. Austin, Texas: Association for Computational Linguistics,\nNov. 2016, pp. 2383–2392. [Online]. Available: https://aclanthology.\norg/D16-1264\n[24] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” arXiv preprint arXiv:1301.3781,\n2013.\n[25] Z. Dou, W. Wei, and X. Wan, “Improving word embeddings for antonym\ndetection using thesauri and sentiwordnet,” in CCF International Con-\nference on Natural Language Processing and Chinese Computing .\nSpringer, 2018, pp. 67–79.\n8",
  "topic": "Backdoor",
  "concepts": [
    {
      "name": "Backdoor",
      "score": 0.9948638081550598
    },
    {
      "name": "Computer science",
      "score": 0.8298217058181763
    },
    {
      "name": "Adversary",
      "score": 0.7194288969039917
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6864139437675476
    },
    {
      "name": "Task (project management)",
      "score": 0.5866830945014954
    },
    {
      "name": "Natural language processing",
      "score": 0.5680851936340332
    },
    {
      "name": "Key (lock)",
      "score": 0.49794554710388184
    },
    {
      "name": "Word (group theory)",
      "score": 0.4554769992828369
    },
    {
      "name": "Focus (optics)",
      "score": 0.44505369663238525
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4206402897834778
    },
    {
      "name": "Machine learning",
      "score": 0.38451823592185974
    },
    {
      "name": "Computer security",
      "score": 0.14446550607681274
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}