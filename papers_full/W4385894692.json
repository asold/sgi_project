{
  "title": "Zero-Shot Ranking Socio-Political Texts with Transformer Language Models to Reduce Close Reading Time",
  "url": "https://openalex.org/W4385894692",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5072894040",
      "name": "Kiymet Akdemir",
      "affiliations": [
        "Boğaziçi University"
      ]
    },
    {
      "id": "https://openalex.org/A2577739910",
      "name": "Ali Hürriyetoğlu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3183571830",
    "https://openalex.org/W4283321251",
    "https://openalex.org/W3184121905",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1660390307",
    "https://openalex.org/W4206183159",
    "https://openalex.org/W2621036272",
    "https://openalex.org/W2962907576",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W2995110265",
    "https://openalex.org/W2533619793",
    "https://openalex.org/W3177743683",
    "https://openalex.org/W3174223793",
    "https://openalex.org/W2891308403",
    "https://openalex.org/W4242588950",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2982596739"
  ],
  "abstract": "We approach the classification problem as an entailment problem and apply zero-shot ranking to socio-political texts. Documents that are ranked at the top can be considered positively classified documents and this reduces the close reading time for the information extraction process. We use Transformer Language Models to get the entailment probabilities and investigate different types of queries. We find that DeBERTa achieves higher mean average precision scores than RoBERTa and when declarative form of the class label is used as a query, it outperforms dictionary definition of the class label. We show that one can reduce the close reading time by taking some percentage of the ranked documents that the percentage depends on how much recall they want to achieve. However, our findings also show that percentage of the documents that should be read increases as the topic gets broader.",
  "full_text": "Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE), pages 124 - 132\nDecember 7-8, 2022 ©2022 Association for Computational Linguistics\nZero-Shot Ranking Socio-Political Texts with Transformer Language\nModels to Reduce Close Reading Time\nKiymet Akdemir\nBo˘gaziçi University\nkiymet.akdemir@boun.edu.tr\nAli Hürriyeto˘glu\nKNAW Humanities Cluster DHLab\nali.hurriyetoglu@dh.huc.knaw.nl\nAbstract\nWe approach the classification problem as an\nentailment problem and apply zero-shot rank-\ning to socio-political texts. Documents that are\nranked at the top can be considered positively\nclassified documents and this reduces the close\nreading time for the information extraction pro-\ncess. We use Transformer Language Models\nto get the entailment probabilities and inves-\ntigate different types of queries. We find that\nDeBERTa achieves higher mean average preci-\nsion scores than RoBERTa and when declara-\ntive form of the class label is used as a query,\nit outperforms dictionary definition of the class\nlabel. We show that one can reduce the close\nreading time by taking some percentage of the\nranked documents that the percentage depends\non how much recall they want to achieve. How-\never, our findings also show that percentage of\nthe documents that should be read increases as\nthe topic gets broader.\n1 Introduction\nFor the information retrieval process positively la-\nbeled documents in a dataset are important and\nshould not be missed, therefore achieving high re-\ncall is extremely important. However, there is gen-\nerally a large number of documents that are rele-\nvant or not to the concerned topic and doing close\nreading for all documents and annotating them re-\nquires lots of time and resources (Hürriyeto ˘glu\net al., 2016; Hürriyetoˇglu et al., 2017). Therefore,\nranking documents according to relevance to the\ninvestigated class may help to reduce close reading\ntime and decrease the likelihood of missing critical\ninformation.\nBaeza-Yates and Ribeiro-Neto (1999) propose\nranking documents in decreasing order of being rel-\nevant to a given query to accelerate the information\nretrieval process. Halterman et al. (2021) apply\nthis method with Natural Language Understanding\n(NLU) models for binary classification problems\nusing the entailment probabilities of a document\nand a declarative form of the label. Therefore, to\ncatch a high percentage of positively labeled doc-\numents, reading some percentage of documents\nwould be enough since documents that are relevant\nwould be at the top with a high probability. How-\never, their dataset India Police Events focuses on\na relatively specific task in information retrieval\nthat is police actions like killing, arresting, failing\nto intervene, etc. Besides, they apply this method\nat the sentence level and as they also stated their\nmodel suffers from understanding multi-sentence\ncontext that increases the false negative rate.\nWe apply this approach to ProtestNews dataset\n(Hürriyeto˘glu et al., 2021) along with the India Po-\nlice Events dataset (Halterman et al., 2021) and\ninvestigate whether sentence level evaluation or\ndocument level evaluation ranks positive docu-\nments at the higher level measured by different\nevaluation metrics. We further investigate whether\nusing the dictionary definition of a class or the\ndeclarative form of a class for the query performs\nthis task better. We compare two NLU mod-\nels DeBERTa-Large-MNLI (He et al., 2020) and\nRoBERTa-Large-MNLI (Liu et al., 2019) in terms\nof recall and mean average precision.\nWe present the related work in Section 2. Next,\nwe introduce two datasets we used in our experi-\nments in Section 3. Then we explain our methodol-\nogy and list all queries used in this work in Section\n4. We detail our experiments for both datasets and\npresent results in Section 5. Finally, we conclude\nthis work in Section 6 and state what can be done\nas future work in Section 7.\n2 Related Work\nProtest Event Detection Protest event extraction\nholds an important place in political social sciences\nand detection of protest events is generally the first\nstep of the extraction. Due to the cost of man-\nual event extraction, besides the presence of digi-\ntal news articles and enhancing machine learning\n124\nmethods; automated event extraction comes into\nplay.\nHanna (2017) presents MPEDS, an automated sys-\ntem for protest event extraction that contains an\nensemble of shallow machine learning classifiers\n(SVM, SGD and Logistic Regression) to detect\nprotest-related documents. Caselli et al. (2021) pro-\nposes Domain Adaptive Retraining for Transformer\nLanguage Models and shows that further training\nBERT with domain-specific dataset improves the\nperformance. They present PROTEST-ER by re-\ntraining pre-trained BERT with protest related data\nfrom TREC Washington Post Corpus. Wiedemann\net al. (2022) classifies protest related documents\nin German local news using Pretrained Language\nModels. They attempt to improve performance and\ngeneralizability by eliminating protest-unrelated\nsentences with keyword search and also by mask-\ning named entities with the idea of models may\noverfit on data by recognizing actors, organizatons\nand places.\nElsafoury (2019) focuses on both protest events and\npolice actions i.e. protest repression events in Twit-\nter with Machine Learning models with the claim\nof news articles suffer from bias, censorship and\nduplication. Won et al. (2017) detects and analyze\nprotest events in geotagged tweets and associated\nimages with Convolutional Neural Networks.\nRanking Documents with Transformer Lan-\nguage Models Yates et al. (2021) presents a com-\nprehensive survey of how BERT (Devlin et al.,\n2019) works, ranking documents with BERT, re-\ntrieve and rerank approach with monoBERT, rank-\ning metrics, etc. One of the most remarkable works\nin the survey is monoBERT and duoBERT, a multi-\nstage ranking approach with transformer language\nmodels proposed by Nogueira et al. (2019). The\nfirst stage retrieves the candidate documents with\nBM25 by treating the query as a bag of words and\nlater, documents are reranked with their relevance\nscore with BERT. DuoBERT also takes into ac-\ncount one document being more relevant than the\nother at a third stage. However, we rank the docu-\nments with a language model at one stage.\nHalterman et al. (2021) rank documents with\nRoBERTa-Large-MNLI (Liu et al., 2019) on sen-\ntence level by being relevant to a police activity.\nYet sentence level evaluation does not take into con-\nsideration the relationship between the sentences.\nMoreover, the task of extracting police events is a\nrelatively specific topic in political event extraction.\nWe apply this method with different document sizes\nand test on datasets in different topic specificities.\nTransformer Language Models DeBERTa and\nRoBERTa DeBERTa-Large-MNLI (DLM) (He\net al., 2020) and RoBERTa-Large-MNLI (RLM)\n(Liu et al., 2019) are pre-trained language models\nthat improve BERT. Both models are pre-trained\non Wikipedia (English Wikipedia dump3; 12GB),\nBookCorpus (6GB), OPENWEBTEXT (38GB),\nand STORIES (a subset of CommonCrawl (31GB)\nand fine-tuned for MNLI task. RLM has a token\nlimitation of 512 whereas DLM has a limitation\nof theoretically 24,528. We limit the inputs to 512\ntokens for both models to be able to compare them\nfairly. Ye and Manoharan (2021) find that DLM\nachieves a better performance in different sentence\nsimilarity tasks with respect to RLM and BERT.\nHe et al. (2020) also show that DeBERTa outper-\nforms RoBERTa in a variety of NLP tasks even\nwhen DeBERTa is trained on half of the training\ndata. Therefore, we use DLM and compare it with\nRLM for our task.\nTransferring Question Answering to Entailment\nProblem Khot et al. (2018) and Demszky et al.\n(2018) transfer the question answering problem to\nthe entailment problem by forming the question\ninto a declarative form. Clark et al. (2019) trans-\nfer yes/no question answering to entailment prob-\nlem by training supervised models on entailment\ndatasets and treating entailment probabilities as the\nprobability of the answer being yes. They also\nuse pre-trained ELMo, BERT, and OpenAI GPT\nas unsupervised models and show that fine-tuning\nBERT on entailment dataset MultiNLI boosts the\nperformance. The problem of any binary classi-\nfication can be also transferred to an entailment\nproblem similar to the yes/no question answering,\nby considering the probability of entailment as the\nprobability of data belonging to the positive class.\n3 Data\nWe carried out the experiments on two different\ndatasets: India Police Events dataset1 (Halterman\net al., 2021) and the ProtestNews dataset of the\nworkshop CASE @ ACL-IJCNLP 2021 2 (Hür-\nriyeto˘glu et al., 2021).\n1Data and code are provided at https://github.\ncom/slanglab/IndiaPoliceEvents.\n2Information and data are provided at\nhttps://github.com/emerging-welfare/\ncase-2021-shared-task .\n125\nEvent type Question\nkill Did police kill someone?\narrest Did police arrest someone?\nfail Did police fail to intervene?\nforce Did police use force or violence?\nany action Did police do anything?\nTable 1: Question form of each event type.\nEvent type Positive Documents\nkill 50 (3.98%)\narrest 128 (10.17%)\nfail 114 (9.05%)\nforce 90 (7.15%)\nany action 457 (36.24%)\nTable 2: Number of positive documents for each event\nclass (India Police Events Dataset).\nIndia Police Events dataset includes 1,257 arti-\ncles about the Indian state Gujarat from The Times\nof India and from March 2002. The articles are\nin English and contain 21,391 sentences in total.\nEach sentence is classified into 5 different labels re-\ngarding police activity: kill, arrest, fail, force, and\nany action. Question form of the each event type\nis given in Table 1. A document belongs to a class\nif any of its sentences belongs to that class. Table\n2 illustrates the number of positive documents and\nthe proportion of the positive documents for each\nevent class. Note that one document may belong to\none class, several classes or none of them.\nProtestNews dataset includes local news articles\nof countries India, China, Argentina, and Brazil.\nThese articles are in English, Spanish, Portuguese,\nand Hindi. For this work, we have only used En-\nglish articles. There are 9,327 English documents\nbut to equalize data sizes with the India Police\nEvents Dataset we randomly selected 1,257 articles\namong those. Documents that contain past or ongo-\ning protest events are labeled as positive (Duru¸ san\net al., 2022). Number and proportion of positive\ndocuments are given in Table 3.\nDataset Positive Documents\nProtestNews Dataset 1,912 (20.51%)\nProtestNews Subset 268 (21.32 %)\nTable 3: Number of positive documents for ProtestNews\nDataset and its subset.\n4 Method\nFirst, the probability of entailment for each docu-\nment and a query is calculated with NLU models\nfrom Huggingface3, and documents are ranked by\nthe decreasing probability of being relevant to the\nquery. Thus we expect the documents that are more\nrelevant are ranked at the top.\nEntailment probabilities are evaluated on both\nsentence and document levels. At sentence level\nevaluation, entailment probabilities of sentences in\na document with the given query are calculated and\nthe largest probability among the sentences is con-\nsidered as the probability of the document being\nrelevant. For the document level evaluation since\nRLM is limited to 512 tokens, we divided docu-\nments into parts such that each part does not exceed\n512 tokens. Similar to the sentence-level approach,\nprobabilities of each part are calculated and the\none with the largest probability is considered as\nthe probability of the document. After getting the\nprobabilities for all documents, they are ranked in\nthe decreasing probability.\nWe compare the results by checking how much\nrecall is achieved when a specified proportion of\ndata is read from the ranked documents following\nHalterman et. al. (2021) and also by calculating\nthe mean average precision. We release our code\npublicly4.\n4.1 Models\nWe focused on the performances of two multilin-\ngual NLU models that are RLM5 (Liu et al., 2019)\nand DLM6 (He et al., 2020) which are pre-trained\non the same datasets (Wikipedia and BookCorpus).\nWe conduct experiments with both models and\ncompare the results.\n4.2 Queries\nWe have experimented with different types of\nqueries: definitional queries, extended definitional\nqueries and declarative queries.\nWe used the Cambridge Dictionary7 and form\nthe definitional queries by using the definitions of\nthe class name (protest, kill, arrest, etc.). Annota-\n3http://huggingface.co\n4https://github.com/kiymetakdemir/\nzero-shot-entailment-ranking\n5https://huggingface.co/\nroberta-large-mnli\n6https://huggingface.co/microsoft/\ndeberta-large-mnli\n7https://dictionary.cambridge.org\n126\nQuery type Query\nDeclarative query There is a protest.\nDefinitional query There is a strong complaint expressing disagreement,\ndisapproval, or opposition. (definition of protest9)\nSocial protest definition (Annotation\nManual)\nIndividuals, groups, or organizations voice their ob-\njections, oppositions, demands or grievances to a\nperson or institution of authority.\nContentious politics event definition\n(Annotation Manual)\nThere is a politically motivated collective action\nevent.\n’protest’ + definitional query Protest, there is a strong complaint expressing dis-\nagreement, disapproval, or opposition.\nProtest definition + opposition defini-\ntion\nThere is a strong complaint expressing disagreement,\ndisapproval, or opposition. Disagreement with some-\nthing, often by speaking or fighting against it, or (esp.\nin politics) the people or group who are not in power.\n(definition of opposition10)\nProtest definition + disapproval defini-\ntion\nThere is a strong complaint expressing disagreement,\ndisapproval, or opposition. The feeling of having a\nnegative opinion of someone or something. (defini-\ntion of disapproval11)\nTable 4: Queries used for the ProtestNews dataset.\nEvent type Declarative query Definitional query\nkill Police killed someone. Police caused someone or something to die. (defini-\ntion of kill12)\narrest Police arrested someone. Police used legal authority to catch and take someone\nto a place where the person may be accused of a\ncrime. (definition of arrest13)\nfail Police failed to intervene. Police failed to have an effect. (definition of act 14)\nforce Police used violence. Police used actions or words that are intended to hurt\npeople. (definition of violence15)\nany action Police did something. Police have an effect. (definition of act)\nTable 5: Queries for the India Police Events dataset.\ntion manual may possibly be a good resource to\nfind the definition of the investigated class. For this\nreason, we also experimented with definitions from\nAnnotation Manual8 (Duru¸ san et al., 2022). On\nthe other hand, a declarative query is a sentence\nthat simply describes the class. For instance, we\nuse “There is a protest.” as the declarative query\nfor the ProtestNews dataset. For the India Police\nEvents dataset, we use declarative queries proposed\nby Halterman et al. (2021).\nWe also extended protest dictionary definition\nby concatenating it with the definitions of words\nthat pass in the query (see last 3 rows in Table 4).\n8https://github.com/emerging-welfare/\ngeneral_info/tree/master/\nannotation-manuals\nIn one of the queries, the ‘protest’ word is added to\nthe beginning of the protest definition. In the other\none, definition of opposition is concatenated with\nthe protest definition. In the third one, definitions\nof protest and definition of disapproval are concate-\nnated and used as a query. Note that we used the\ndefinitions of opposition and disapproval since they\noccur in the protest definition. All queries used for\nboth datasets are listed in Table 4 and 5.\n5 Experiments & Results\nProtestNews Dataset is tested with declarative\nqueries, definitional queries and extended defini-\ntions on models DLM and RLM and results are\npresented in Figure 1a for the sentence level evalu-\n127\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nProtestNews - sentence level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional\n(a) Sentence level evaluation.\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nProtestNews - document level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional (b) Document level evaluation.\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nProtestNews - extended definitions\ndictionary def\ndictionary def + 'protest' \ndictionary def + opposition def\ndictionary def + disapproval def\n(c) Extended definitions.\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nProtestNews - different definitions\nprotest def\ncontentious politics def\nsocial protest def (d) Different definitions.\nFigure 1: ProtestNews dataset tested on two models: RLM and DLM.\nation. The x-axis represents what percentage of the\ndata is read and the y-axis represents how much\nrecall is achieved at that stage. One can investi-\ngate what percentage of the data should be read to\nachieve a specified recall. We see that both mod-\nels yield similar results when the same query is\ngiven but positive documents are accumulated at\nmore top with the declarative query compared to\nthe definitional query.\nFor document level evaluation, Figure 1b illus-\ntrates the comparison of the models. DLM achieves\nhigher recall scores than RLM, however, the query\ntype does not affect the performance of the model\nat the document level significantly.\nWe compare the extended and Annotation Man-\nual definitions at document level using the DLM\nmodel since the DLM achieves higher recall com-\npared to RLM at the document level as in Figure\n1b. However, from Figure 1c we see that extending\nthe protest definition performs slightly worse than\nusing the only dictionary definition. Also, Annota-\ntion Manual definitions do not perform better than\nthe dictionary definition as we see from Figure 1d.\nIndia Police Events Dataset is tested with\ndeclarative and definitional queries on RLM and\nDLM as in ProtestNews dataset. For all event types,\nwe see from Figure 2 and Figure 3 that DLM with\ndeclarative query gives the best result that is posi-\ntive documents are accumulated at more top-level,\nwhereas RLM with a definitional query stays be-\nhind other combinations of model and queries.\nMean Average Precision (mAP) is calculated\nfor each ranking and reported in Table 6. Query\nand document length combination that gives the\nhighest mAP is marked in bold for each dataset and\nevent type.\nFor the ProtestNews dataset we observe that us-\ning models DLM or RLM, and document lengths\ndo not differ significantly. Whereas using a declar-\native query gives much better mAP than the defini-\ntional query. For the India Police Events dataset for\nall event types DLM and declarative query with the\nsentence level evaluation yield the highest score\nrather than the definitional or document level eval-\nuation. Besides, note that there is a large difference\nwith the other combinations. For example for event\ntype force, sentence level evaluation with DLM\nand the declarative query gives 0.91 mAP whereas\ndocument level evaluation with RLM and the defi-\nnitional query yields 0.11 mAP.\nAs the topic gets broader, we see that performance\ngets worse in Table 6. For instance, kill is a more\n128\nProtestNews India Police Events\n- kill arrest fail force any action\nDLM decl-sent 0.64 0.96 0.94 0.65 0.91 0.89\nDLM decl-doc 0.60 0.80 0.75 0.25 0.75 0.80\nDLM def-sent 0.35 0.89 0.63 0.47 0.71 0.69\nDLM def-doc 0.41 0.62 0.42 0.21 0.21 0.65\nRLM decl-sent 0.65 0.55 0.91 0.34 0.66 0.42\nRLM decl-doc 0.51 0.18 0.44 0.18 0.27 0.36\nRLM def-sent 0.38 0.36 0.26 0.23 0.18 0.38\nRLM def-doc 0.34 0.11 0.15 0.16 0.11 0.37\nTable 6: mAP scores for DLM and RLM models with different document lengths and queries.\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: kill - sentence level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional\n(a) kill\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: arrest - sentence level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional (b) arrest\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: fail - sentence level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional\n(c) fail\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: force - sentence level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional (d) force\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: any action - sentence level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional\n(e) any action\nFigure 2: India Police Events dataset sentence level evaluation tested on RLM and DLM.\nspecific topic than any action since any action event\ntype also includes kill events. When 20% of the\ndata read, 90% recall is achieved for event type kill,\non the other hand, even 60% recall is not reached\nfor any action.\nWe take the average sentence and document level\nmAP scores for each model and present in Table\n7. For ProtestNews dataset, sentence or document\n129\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: kill - document level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional\n(a) kill\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: arrest - document level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional (b) arrest\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: fail - document level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional\n(c) fail\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: force - document level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional (d) force\n0.0 0.2 0.4 0.6 0.8 1.0\nproportion of data read\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0recall\nEvent type: any action - document level evaluation\nDLM - declarative\nRLM - declarative\nDLM - definitional\nRLM - definitional\n(e) any action\nFigure 3: India Police Events dataset document level evaluation tested on RLM and DLM.\nlevel does not differ in mAP when DLM is used.\nHowever, for India Police Events dataset sentence\nlevel evaluation achieves much higher mAP than\ndocument level evaluation (0.24 mAP increase for\nDLM and 0.21 increase for RLM). For both sen-\ntence and document level, DLM reaches higher\nmAP than RLM.\nProtestNews India Police Events\nDLM RLM DLM RLM\nsent 0.50 0.52 0.77 0.43\ndoc 0.50 0.42 0.53 0.22\nTable 7: Average mAP on ProtestNews and India Police\nEvents Dataset for all event types.\n6 Conclusion\nWe investigate the performances of two Trans-\nformer Language Models (DLM and RLM), dif-\nferent query types (declarative and definitional)\nin different document lengths (document and sen-\n9https://dictionary.cambridge.org/\ndictionary/english/protest\n10https://dictionary.cambridge.org/\ndictionary/english/opposition\n11https://dictionary.cambridge.org/\ndictionary/english/disapproval\n12https://dictionary.cambridge.org/\ndictionary/english/kill\n13https://dictionary.cambridge.org/\ndictionary/english/arrest\n14https://dictionary.cambridge.org/\ndictionary/english/act\n15https://dictionary.cambridge.org/\ndictionary/english/violence\n130\ntence level). Our experiments that conclude DLM\nachieves higher mAP scores than RLM are consis-\ntent with the findings of Ye and Manoharan (2021)\nand He et al. (2020). In general, we find that the\ncombination of DLM with a declarative query in\nsentence level outperforms other combinations in\nmAP score. However, scores decrease as the topic\nor event type gets broader where protest events can\nbe considered broader than specific police actions.\n7 Future Work\nWe plan to analyze results more for example by\nconsidering subcategories of protest events for the\nProtestNews dataset. Future work can extend this\nwork to a different political event classification\ndataset and further investigate the association be-\ntween the broadness of the topic and metric scores.\nExperiments in languages other than English are\nalso left as future work.\nReferences\nRicardo A. Baeza-Yates and Berthier A. Ribeiro-Neto.\n1999. Modern Information Retrieval. ACM Press /\nAddison-Wesley.\nTommaso Caselli, Osman Mutlu, Angelo Basile, and\nAli Hürriyeto˘glu. 2021. PROTEST-ER: Retraining\nBERT for protest event extraction. In Proceedings of\nthe 4th Workshop on Challenges and Applications of\nAutomated Extraction of Socio-political Events from\nText (CASE 2021), pages 12–19, Online. Association\nfor Computational Linguistics.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924–2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nDorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\nTransforming question answering datasets into natu-\nral language inference datasets.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nFırat Duru¸ san, Ali Hürriyeto˘glu, Erdem Yörük, Osman\nMutlu, Ça˘grı Yoltar, Burak Gürel, and Alvaro Comin.\n2022. Global contentious politics database (glocon)\nannotation manuals.\nFatma Elsafoury. 2019. Detecting protest repression\nincidents from tweets . Ph.D. thesis, University of\nGlasgow.\nAndrew Halterman, Katherine Keith, Sheikh Sarwar,\nand Brendan O’Connor. 2021. Corpus-level evalu-\nation for event QA: The IndiaPoliceEvents corpus\ncovering the 2002 Gujarat violence. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 4240–4253, Online. Association\nfor Computational Linguistics.\nAlex Hanna. 2017. Mpeds: Automating the generation\nof protest event data.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention.\nAli Hürriyeto ˘glu, Osman Mutlu, Erdem Yörük,\nFarhana Ferdousi Liza, Ritesh Kumar, and Shyam\nRatan. 2021. Multilingual protest news detection -\nshared task 1, CASE 2021. In Proceedings of the 4th\nWorkshop on Challenges and Applications of Auto-\nmated Extraction of Socio-political Events from Text\n(CASE 2021), pages 79–91, Online. Association for\nComputational Linguistics.\nAli Hürriyeto ˇglu, Nelleke Oostdijk, Mustafa\nErkan Ba¸ sar, and Antal van den Bosch. 2017.\nSupporting Experts to Handle Tweet Collections\nAbout Significant Events, pages 138–141. Springer\nInternational Publishing, Cham.\nAli Hürriyeto˘glu, Christian Gudehus, Nelleke Oostdijk,\nand Antal van den Bosch. 2016. Relevancer: Finding\nand labeling relevant information in tweet collections.\nIn Social Informatics - 8th International Conference,\nSocInfo 2016, Bellevue, WA, USA, November 11-14,\n2016, Proceedings, Part II, volume 10047 of Lecture\nNotes in Computer Science, pages 210–224.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. Proceedings of the AAAI Con-\nference on Artificial Intelligence, 32(1).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\nJimmy J. Lin. 2019. Multi-stage document ranking\nwith bert. ArXiv, abs/1910.14424.\nGregor Wiedemann, Jan Matti Dollbaum, Sebastian\nHaunss, Priska Daphi, and Larissa Daria Meier. 2022.\nA generalized approach to protest event detection\n131\nin German local news. In Proceedings of the Thir-\nteenth Language Resources and Evaluation Confer-\nence, pages 3883–3891, Marseille, France. European\nLanguage Resources Association.\nDonghyeon Won, Zachary C Steinert-Threlkeld, and\nJungseock Joo. 2017. Protest activity detection and\nperceived violence estimation from social media im-\nages. In Proceedings of the 25th ACM international\nconference on Multimedia, pages 786–794.\nAndrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021.\nPretrained transformers for text ranking: Bert and be-\nyond. In Proceedings of the 44th International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR ’21, page 2666–2668,\nNew York, NY , USA. Association for Computing\nMachinery.\nXinfeng Ye and Sathiamoorthy Manoharan. 2021. Per-\nformance comparison of automated essay graders\nbased on various language models. In 2021 IEEE\nInternational Conference on Computing (ICOCO) ,\npages 152–157.\n132",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.779862105846405
    },
    {
      "name": "Transformer",
      "score": 0.7006176710128784
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6234095692634583
    },
    {
      "name": "Natural language processing",
      "score": 0.5988603234291077
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5801132321357727
    },
    {
      "name": "Logical consequence",
      "score": 0.5759071707725525
    },
    {
      "name": "Recall",
      "score": 0.5458046793937683
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.4801577925682068
    },
    {
      "name": "Class (philosophy)",
      "score": 0.4561864137649536
    },
    {
      "name": "Reading (process)",
      "score": 0.43653711676597595
    },
    {
      "name": "Information retrieval",
      "score": 0.425989031791687
    },
    {
      "name": "Linguistics",
      "score": 0.2103283405303955
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4405392",
      "name": "Boğaziçi University",
      "country": "TR"
    }
  ]
}