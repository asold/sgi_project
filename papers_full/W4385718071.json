{
  "title": "Are Language Models Sensitive to Semantic Attraction? A Study on Surprisal",
  "url": "https://openalex.org/W4385718071",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5070822759",
      "name": "Yan Cong",
      "affiliations": [
        "Feinstein Institute for Medical Research"
      ]
    },
    {
      "id": "https://openalex.org/A5079921667",
      "name": "Emmanuele Chersoni",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A5050341460",
      "name": "Yu-Yin Hsu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A5050795259",
      "name": "Alessandro Lenci",
      "affiliations": [
        "University of Pisa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2165256085",
    "https://openalex.org/W4289552613",
    "https://openalex.org/W4317838060",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3177377367",
    "https://openalex.org/W4389519396",
    "https://openalex.org/W2052742452",
    "https://openalex.org/W4399639995",
    "https://openalex.org/W3168591151",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W4385572952",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W4299687266",
    "https://openalex.org/W2513018281",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4385573517",
    "https://openalex.org/W2113772582",
    "https://openalex.org/W4210329712",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3103816537",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2802888953",
    "https://openalex.org/W2122236285",
    "https://openalex.org/W3102485638",
    "https://openalex.org/W4361766487",
    "https://openalex.org/W4385572451",
    "https://openalex.org/W2738438234",
    "https://openalex.org/W2107265154",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W2942054564",
    "https://openalex.org/W4221154823",
    "https://openalex.org/W2137002739",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4297412056"
  ],
  "abstract": "In psycholinguistics, semantic attraction is a sentence processing phenomenon in which a given argument violates the selectional requirements of a verb, but this violation is not perceived by comprehenders due to its attraction to another noun in the same sentence, which is syntactically unrelated but semantically sound. In our study, we use autoregressive language models to compute the sentence-level and the target phrase-level Surprisal scores of a psycholinguistic dataset on semantic attraction. Our results show that the models are sensitive to semantic attraction, leading to reduced Surprisal scores, although none of them perfectly matches the human behavioral pattern.",
  "full_text": "Proceedings of the The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 141–148\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nAre Language Models Sensitive to Semantic Attraction?\nA Study on Surprisal\nYan Cong\nPurdue University; Feinstein Institutes\nyancong222@gmail.com\nEmmanuele Chersoni\nThe Hong Kong Polytechnic University\nemmanuele.chersoni@polyu.edu.hk\nYu-Yin Hsu\nThe Hong Kong Polytechnic University\nyu-yin.hsu@polyu.edu.hk\nAlessandro Lenci\nUniversity of Pisa\nalessandro.lenci@unipi.it\nAbstract\nIn psycholinguistics, semantic attraction is a\nsentence processing phenomenon in which a\ngiven argument violates the selectional require-\nments of a verb, but this violation is not per-\nceived by comprehenders due to its attraction\nto another noun in the same sentence, which is\nsyntactically unrelated but semantically sound.\nIn our study, we use autoregressive language\nmodels to compute the sentence-level and the\ntarget phrase-level Surprisal scores of a psy-\ncholinguistic dataset on semantic attraction.\nOur results show that the models are sensitive\nto semantic attraction, leading to reduced Sur-\nprisal scores, although none of them perfectly\nmatches the human behavioral patterns.\n1 Introduction\nCases of similarity-based interference have always\nbeen at the center of interest for sentence process-\ning studies, as they offer strong evidence for cue-\nbased models of memory retrieval during language\ncomprehension (Cunnings and Sturt, 2018). Ac-\ncording to such accounts, interference emerges be-\ncause an item with some cues has to be retrieved\nfrom memory, and because those cues are simul-\ntaneously matched by multiple items (Van Dyke,\n2007; Lewis and Vasishth, 2013).\nConsider the examples in (1) (Wagers et al.,\n2009):\n(1) a. The key to the cells unsurprisingly\nwere rusty.\nb. The key to the cell unsurprisingly\nwere rusty.\nCompared to fully grammatical sentences, both\nelicit longer reading times in humans, but the effect\nis attenuated in 1a., where there is an attractor\n(cells) matching the number of the verb, causing an\nillusion of grammaticality. This phenomenon is\nknown as morphological attraction.\nAttraction has also been observed at the semantic\nlevel, as in the following example from the eye-\ntracking study by Cunnings and Sturt (2018):\n(2) a. Julia saw the beer that the lady with\nthe meal quite happily ate during an\nexpensive night out.\nb. Julia saw the beer that the lady with\nthe wine quite happily ate during an\nexpensive night out.\nAgain, both sentences are implausible, because\nbeer violates the selectional restrictions of the verb\nate, but the authors of the study observed that (2a)\nwas processed faster than (2b), due to the presence\nof a semantically fitting noun ( meal) that gener-\nates a semantic illusion. Both types of illusion\nare facilitatory interferences, as they attenuate the\neffects of anomalies leading to higher costs for the\nhuman language processing system. This is a case\nof semantic attraction.\nThe recent literature in Natural Language Pro-\ncessing (NLP), on the other hand, has shown an\nincreasing interest in using the Surprisal scores\n(Hale, 2001; Levy, 2008) computed with Neu-\nral Language Models (NLMs) to account for\nsentence processing phenomena (Futrell et al.,\n2018; Van Schijndel and Linzen, 2018; Wilcox\net al., 2018; Michaelov and Bergen, 2020, 2022a;\nMichaelov et al., 2023). This also includes inves-\ntigations on interferences at the morphosyntactic\nlevel (Ryu and Lewis, 2021). To our knowledge,\nthere have been no attempts to model semantic\nattraction with NLMs yet.\nWe aim at filling this gap by presenting a\nSurprisal-based analysis of a psycholinguistic\ndataset on semantic attraction with three autore-\ngressive NLMs of different sizes. We found that\nNLMs are sensitive to both the plausibility of the\nsentences and semantic attraction effects. However,\nNLM Surprisal for a target phrase seems to be af-\nfected by attraction regardless of general sentence\n141\nplausibility, differently from human reading behav-\nior. On the other hand, sentence-level Surprisal is\nnot affected by semantic attraction.\n2 Related Work\n2.1 Semantic Attraction in Implausible\nSentences\nThe work by Cunnings and Sturt (2018) has re-\ncently brought evidence of the existence of seman-\ntic attraction in semantically implausible sentences.\nThey collected eye-tracking fixations for sentences\nin four conditions, by crossing the factors of the\nplausibility of the sentence (the plausible or implau-\nsible arguments are in italic) and the plausibility of\nan attractor noun (in bold):\n(3) a. Julia saw the cake that the lady with\nthe meal quite happily ate during an\nexpensive night out. ( plausible sen-\ntence, plausible attractor)\nb. Julia saw the cake that the lady with\nthe wine quite happily ate during an\nexpensive night out. ( plausible sen-\ntence, implausible attractor)\nc. Julia saw the beer that the lady with\nthe meal quite happily ate during an\nexpensive night out. (implausible sen-\ntence, plausible attractor)\nd. Julia saw the beer that the lady with\nthe wine quite happily ate during an\nexpensive night out. (implausible sen-\ntence, implausible attractor)\nThe results showed that fixations were signif-\nicantly longer in implausible sentences, but the\neffect was attenuated in presence of a plausible\nattractor (condition (3c)), while in plausible sen-\ntences the attractor did not have any significant\neffect. The authors explained the finding in terms\nof “verb-specific cues that may guide retrieval to\ngrammatically illicit, but plausible, constituents\nduring the resolution of filler-gap dependencies”.\nThe follow-up study by Laurinavichyute and\nvon der Malsburg (2022) instead used a forced\nchoice completion judgement task to compare se-\nmantic and morphosyntactic attraction. First, they\npresented a target verb to the participants, and then\nthey presented them with a sentence fragment, ask-\ning participants whether the verb could have been\na fitting continuation for the sentence. In such a\nscenario, it is expected that violations will elicit\nnegative answers, with attraction phenomena pos-\nsibly increasing the error rates of the participants.\nTheir stimuli contained violations either at the mor-\nphosyntactic or at the semantic level, and have ei-\nther a morphosyntactic or a semantic attractor. The\nauthors reported considerably higher error rates for\nthe conditions with a violation and an attractor of\nthe same type, supporting the idea that morphosyn-\ntactic and semantic attraction work similarly.\nOur study on NLMs uses the stimuli from the\ndatasets by Cunnings and Sturt (2018) to test\nwhether they are sensitive to semantic attraction\nin sentence processing, which may be reflected by\nthe Surprisal scores of the stimuli words. We also\nwant to test whether semantic plausibility and at-\ntraction in NLMs interact like in humans, to what\nextent (cf. the claim in Cunnings and Sturt (2018)\nthat semantic attraction has a facilitatory effect only\nwhen the sentence is not plausible) and if the effects\nare the same in NLMs of different sizes.\n2.2 NLM Estimation of Word Surprisal\nTransformer-based NLMs (Vaswani et al., 2017;\nDevlin et al., 2019; Radford et al., 2019) have be-\ncome increasingly popular in NLP in recent years,\nand a number of studies designed tests to investi-\ngate their actual linguistic abilities (Tenney et al.,\n2019a; Jawahar et al., 2019; Tenney et al., 2019b).\nSome of these studies specifically analyzed the\nSurprisal scores computed by the models, to un-\nderstand to what extent they are sensitive to lin-\nguistic phenomena that have been showed to affect\nhuman sentence processing. For example, Misra\net al. (2020) investigated the predictions of BERT\nin a setting aimed at reproducing human seman-\ntic priming; they reported that BERT was indeed\nsensitive to “priming” and predicted a word with\nlower Surprisal values when the context included a\nrelated word as opposed to an unrelated one.Using\na similar methodology, Cho et al. (2021) modeled\nthe priming effect of verb aspect on the prediction\nof typical event locations, finding that BERT out-\nputs lower surprisal scores for typical locations, but\ndifferently from humans, it does so regardless of\nverb aspect manipulations.\nMichaelov and Bergen (2022a) investigated the\nissue of collateral facilitation, that is, when anoma-\nlous words in a sentence are processed more easily\nby humans because of the presence of semantically-\nrelated words in the context. They compared the\nSurprisal scores obtained with several Transformer\nNLMs and showed that most of them reproduce the\n142\nsame significant differences between conditions\nobserved in humans. In Michaelov et al. (2023)\nthe same authors used NLM Surprisal to repli-\ncate the effect of discourse context in reducing\nthe N400 amplitude for anomalous words, using\nthe Dutch stimuli of the experiments by Nieuwland\nand Van Berkum (2006).\nProbably the closest relative to the topic of our\nstudy, Ryu and Lewis (2021) proved that the Sur-\nprisal values extracted with the GPT-2 language\nmodel predict the facilitatory effects of interference\nin ungrammatical sentences in which an attractor\nnoun is matching in number with the verb or with a\nreflexive pronouns. However, they focused on mor-\nphosyntactic attraction, while we aim at modeling\nthe facilitatory effects of semantic attraction.\n3 Experimental Settings\n3.1 Dataset\nWe derived our dataset from the Experiment 1\nof the eye-tracking study by Cunnings and Sturt\n(2018). The authors employed a total of 32 items,\neach of them coming in four conditions, for a total\nof 128 stimuli. The stimuli were stories composed\nof an introduction sentence, a critical sentence and\na wrap-up sentence. In our experiment, we just fed\nthe NLMs with the critical sentence:\n(4) Julia saw the cake/beer (plausi-\nble/implausible) that the lady with\nthe meal/wine (plausible/implausible)\nquite happily ate during an expensive night\nout.\nThe sentences in the four conditions, as shown in\nExample (4), were differing for i) a fitting or a\nselectional preference-violating direct object (in\nitalic) for the verb in the subordinate clause (un-\nderlined), which would determine the plausibility\nof the sentence; ii) a plausible or an implausible\nattractor noun (in bold), not syntactically related\nwith the verb but with a high degree of thematic fit\nwith it.1 The authors reported main effects of both\nsentence plausibility (implausible sentences induce\nlonger fixations) and attractor plausibility (a plau-\nsible attractor has a facilitatory effect) in the total\nviewing times.2 They also reported a significant in-\n1We refer to the notion of thematic fit as the degree of\ncompatibility between a predicate and a noun filling one of\nits semantic roles (McRae and Matsuki, 2009; Sayeed et al.,\n2016; Santus et al., 2017).\n2To address a remark by Reviewer 1, we checked the log-\narithmic frequencies of the attractor nouns (the target nouns\nteraction between the two: total viewing times for\nimplausible sentences were shorter when the attrac-\ntor was plausible compared to implausible, while\nno significant difference was observed in plausible\nsentences as a result of attractor plausibility.\n3.2 Language Models\nFor the models in this paper, we use the implemen-\ntation of Minicons (Misra, 2022)3, an open source\nlibrary that provides a standard API for behavioral\nand representational analyses of NLMs. We make\nthe code and the test data available for additional\ntesting.4 We experiment with three variants of au-\ntoregressive LMs of different sizes: the original\nGPT-2 Base, with 124 million parameters (Radford\net al., 2019); DistilGPT-2 with 82 million parame-\nters (Sanh et al., 2019), trained as a student network\nwith the supervision of GPT-2; and GPT-Neo that,\nwith 1.3 billion parameters (Gao et al., 2020; Black\net al., 2021), is close to the size of the smallest\nmodels of the GPT-3 family.\nUsing autoregressive NLMs, we computed the\nSurprisal scores at the target in the stimuli (the verb\nin the subordinate clause), and also at the level of\nthe entire sentence. When the NLMs tokenizer\nsplits the target in more than one token, we take the\naverage of the Surprisal scores of its subtokens.\nMore formally, the Surprisal of the target T in\nthe context C (Surp) was computed as:\nSurp(T|C) =\n∑\nt∈T −logP (t|C)\ncount(t) (1)\nwhere P(t|C) is the probability of each subtoken\nt ∈T given the previous context C, whilecount(t)\nis the number of subtokens in the target phrase T.\nThe Surprisal of the sentence S (SentSurp) in-\nstead is simply the sum of the Surprisals of each\ntoken T normalized by the length of the sentence:\nSentSurp(S) =\n∑\nT ∈S Surp(T)\ncount(T) (2)\nwhere count(T) is the total number of tokens in\nthe sentence S.5\nwere the same in all conditions), which were not mentioned in\nthe original study (see the materials in the Appendix). We have\nnot found any significant difference between noun frequencies\nacross conditions.\n3https://github.com/kanishkamisra/\nminicons-experiments\n4https://github.com/yancong222/\ntransformers-semantic-attraction-surprisal\n5Notice that the sentences may differ in the number of\ntokens, in the cases when the object and/or the attractor nouns\nare splitted by the tokenizer. This is why we did not use the\n143\nGPT-2 DistilGPT-2 GPTNeo\nB SE p B SE p B SE p\nIntercept 9.72 0.54 <.001 9.62 0.54 <0.001 9.92 0.49 <0.001\nSentPlaus 3.40 0.28 <0.001 2.17 0.28 <0.001 4.39 0.31 <0.001\nAttrPlaus 0.84 0.28 0.003 1.01 0.28 <0.001 0.84 0.31 .008\nLength 0.13 0.20 0.08 0.22 0.19 0.09 0.05 0.18 0.78\nSentPlaus:AttrPlaus 0.29 0.19 0.11\nS1-A0 : S0-A0 -3.69 0.39 < 0.001 -2.45 0.31 < 0.001 -4.87 0.43 < 0.001\nS0-A1 : S0-A0 -1.12 0.39 0.021 -1.38 0.31 < 0.001 -1.32 0.43 0.013\nS1-A1 : S0-A0 -4.24 0.39 < 0.001 -3.26 0.31 < 0.001 -5.22 0.43 < 0.001\nS0-A1 : S1-A0 2.56 0.39 < 0.001 1.07 0.31 0.003 3.55 0.43 < 0.001\nS1-A1 : S1-A0 -0.56 0.39 0.48 -0.82 0.31 0.039 -0.36 0.43 0.84\nS1-A1 : S0-A1 -3.12 0.39 < .001 -1.89 0.31 < 0.001 -3.91 0.43 < 0.001\nTable 1: Summary for the results of predictors of Surp, and of the interaction between SentPlaus and AttrPlaus. In\nthe pairwise comparisons cond1:cond2, the reference level is cond2 (meaning, if the estimate B is negative, the\nSurprisal of cond1 is lower than Cond2, otherwise it is higher).\nGPT-2 DistilGPT-2 GPTNeo\nB SE p B SE p B SE p\nIntercept 7.20 0.53 <0.001 7.84 0.56 <0.001 7.94 0.51 <0.001\nSentPlaus 0.10 0.02 <0.001 0.06 0.02 0.011 0.16 0.02 <0.001\nAttrPlaus 0.02 0.02 0.382 0.03 0.02 0.06 0.01 0.02 0.829\nLength -0.07 0.01 <.001 -0.08 0.02 <0.001 -0.10 0.01 <0.001\nSentPlaus:AttrPlaus 0.33 0.038 0.84\nS1-A0 : S0-A0 -0.10 0.03 < 0.001 -0.06 0.03 0.078 -0.172 0.03 < 0.001\nS0-A1 : S0-A0 -0.01 0.03 0.96 -0.01 0.03 0.967 -0.02 0.03 0.89\nS1-A1 : S0-A0 -0.12 0.03 < 0.001 -0.11 0.03 < 0.001 -0.17 0.03 < 0.001\nS0-A1 : S1-A0 0.09 0.03 0.003 0.05 0.03 0.214 0.15 0.03 < 0.001\nS1-A1 : S1-A0 -0.02 0.03 0.77 -0.05 0.03 0.249 0.01 0.03 0.992\nS1-A1 : S0-A1 -0.11 0.03 < 0.001 -0.10 0.03 < 0.001 -0.15 0.03 < 0.001\nTable 2: Summary for the results of predictors of SentSurp, and of the interaction between SentPlaus and AttrPlaus.\nIn the pairwise comparisons cond1:cond2, the reference level is cond2 (meaning, if the estimate B is negative, the\nSurprisal of cond1 is lower than Cond2, otherwise it is higher).\nFor each NLM, we fitted a linear mixed-effects\nmodel using Surp or SentSurp as the dependent\nvariable, which was estimated for each of the exper-\nimental stimuli. The independent variables were:\nthe plausibility of the sentence SentPlaus (plausi-\nble vs. implausible; plausible as the base of com-\nparison), the plausibility of the attractor AttrPlaus\n(plausible vs. implausible; plausible as the base\nof comparison), their interactions, and the token\nlength of the stimulus length. We included items\nas a random intercept in our models. We use the\nLME 4 package (Bates et al., 2014) for model fitting\nand results; the pairwise comparisons with Tukey\nadjustment were carried out by theEMMEANS pack-\nage (Lenth, 2019) in R.\n4 Results\nThe findings of the experiments are summarized in\nTables 1 and 2.\nConsidering the main effects, we found that all\nmodels were able to distinguish plausible from im-\nsum of the Surprisal scores, as per Reviewer 3’s comment.\nplausible items at the sentence level (see SentPlaus\nin Tables 1 and 2), with significantly higher Sur-\nprisal scores for the latter.\nAs shown in Table 1, the models based on Surp\nwere also sensitive to the attractor plausibility, and\nmarginally to the token length of the stimuli. No\nsignificant main effect of interaction between sen-\ntence and attractor plausibility was found. The\nmodels based on SentSurp (Table 2) were sensi-\ntive to token length, but not to the attractor plau-\nsibility, with the only exception of a marginal sig-\nnificance for DistilGPT2. The SentSurp model\nbased on DistilGPT2 is the only one showing (at\nleast marginally) significant effects for the plausi-\nbility of both sentence (p = 0.011) and attractor (p\n= 0.06) and for their interaction ( p = 0.038) (see\nTable 2 and Figure 1), while no interaction was\nfound in any of the other models. The fact that\nthis behavior was found in the smallest model may\nrepresent another case of what has been called “in-\nverse scaling” in the NLM literature, that is, the\nperformance decreases at the increase of model\nsize (Wei et al., 2022; Jang et al., 2023), or in the\n144\ncase of psycholinguistic modeling, the behavior\nbecomes less human-like (Michaelov and Bergen,\n2022b; Oh and Schuler, 2022).\nThe post hoc analyses of the pairwise compari-\nson showed some interesting contrasts. We noticed\nthat significant differences were found between the\nplausible sentences with plausible attractors and\nthe two implausible conditions (i.e. in Figure 1, a\nvs. c and a vs. d, with ps < 0.001). Differently\nfrom human total viewing times, no significant dif-\nferences and no consistent facilitatory effects are\nobserved between c and d in the SentSurp models\n(notice also in Figure 1 that the median of condi-\ntion d. is actually slightly lower than c., and the\nmedians for c. and d. tend to be close in all the\nSentSurp models, cf. the boxplots in the Appendix,\nright column), while facilitation is found for all the\nSurp models.\nFigure 1: Sentence Surprisal scores from DistilGPT-2\n(means in yellow). Conditions are the same of Ex. 3.\nIt is also noticeable that all models show no sensi-\ntivity to plausible attractors with the sentence-level\nSurprisal metrics, but the Surprisal at the target\nword with implausible attractors is always signifi-\ncantly higher. However, since no significant main\neffect of interactions was found for Surp models,\nwe conclude that semantic attraction seems to to\nhave a general facilitation effect on its own, regard-\nless of sentence plausibility.\nIt would be interesting, in the future, to analyze\nhow the attractors concretely affect the predictions,\nfor example using techniques like contrastive ex-\nplanations (Yin and Neubig, 2022) that can shed\nlight on which tokens contribute to the prediction\nof the target verb rather than a plausible alternative\nword (in our case, this could be a verb in a thematic\nfit relation with the implausible attractor noun, e.g.\ndrank for wine in examples 2. b-d).\n5 Conclusions\nIn this work, we presented a study on Surprisal\nto investigate whether NLMs predictions are sen-\nsitive to semantic attraction. Our results on the\ndata of the eye-tracking experiment by Cunnings\nand Sturt (2018) reveal that all models are sensi-\ntive to the general plausibility of the sentence, and\nthat semantically-plausible attractors decrease the\nSurprisal at the target phrase, although this effect\ngenerally does not interact with sentence plausibil-\nity as in humans.\nAt the sentence level, no effects of attractor plau-\nsibility were observed, with the only, partial excep-\ntion of a marginal significance with DistilGPT2. In-\nterestingly, the most human-like pattern -including\nthe interaction- has been observed with this model,\nthe smallest one, although the specific contrasts\nbetween conditions pattern differently from human\ntotal viewing times.\nAcknowledgments\nWe would like to thank the three anonymous re-\nviewers for their insightful feedback.\nThis research was made possible by the start-up\nresearch fund (1-BD8S) at the Hong Kong Poly-\ntechnic University.\nReferences\nDouglas Bates, Martin Mächler, Ben Bolker, and Steve\nWalker. 2014. Fitting Linear Mixed-effects Models\nUsing lme4. arXiv preprint arXiv:1406.5823.\nSid Black, Gao Leo, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. zenodo.org.\nWon Ik Cho, Emmanuele Chersoni, Yu-Yin Hsu, and\nChu-Ren Huang. 2021. Modeling the Influence of\nVerb Aspect on the Activation of Typical Event Lo-\ncations with BERT. In Findings of ACL-IJCNLP.\nIan Cunnings and Patrick Sturt. 2018. Retrieval In-\nterference and Semantic Interpretation. Journal of\nMemory and Language, 102:16–27.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of NAACL.\n145\nRichard Futrell, Ethan Wilcox, Takashi Morita, and\nRoger Levy. 2018. RNNs as Psycholinguistic Sub-\njects: Syntactic State and Grammatical Dependency.\narXiv preprint arXiv:1809.01329.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe Pile: An 800GB Dataset of Diverse Text for Lan-\nguage Modeling. arXiv preprint arXiv:2101.00027.\nJohn Hale. 2001. A Probabilistic Earley Parser as a\nPsycholinguistic Model. In Proceedings of NAACL.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2023. Can\nLarge Language Models Truly Understand Prompts?\nA Case Study with Negated Prompts. In Transfer\nLearning for Natural Language Processing Work-\nshop.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What Does BERT Learn about the Structure\nof Language? In Proceedings of ACL.\nAnna Laurinavichyute and Titus von der Malsburg.\n2022. Semantic Attraction in Sentence Comprehen-\nsion. Cognitive Science, 46(2):e13086.\nRussell Lenth. 2019. emmeans: Estimated Marginal\nMeans, aka Least-Squares Means. R Package Ver-\nsion 1.4.2.\nRoger Levy. 2008. Expectation-based Syntactic Com-\nprehension. Cognition, 106(3):1126–1177.\nRichard L Lewis and Shravan Vasishth. 2013. An\nActivation-based Model of Sentence Processing as\nSkilled Memory Retrieval. In Cognitive Science ,\npages 375–419. Routledge.\nKen McRae and Kazunaga Matsuki. 2009. People Use\ntheir Knowledge of Common Events to Understand\nLanguage, and Do So as Quickly as Possible. Lan-\nguage and Linguistics Compass, 3(6):1417–1429.\nJames A Michaelov and Benjamin K Bergen. 2020.\nHow Well Does Surprisal Explain N400 Amplitude\nunder Different Experimental Conditions? In Pro-\nceedings of CONLL.\nJames A Michaelov and Benjamin K Bergen. 2022a.\nCollateral Facilitation in Humans and Language Mod-\nels. In Proceedings of CONLL.\nJames A Michaelov and Benjamin K Bergen. 2022b.\n’Rarely’a Problem? Language Models Exhibit In-\nverse Scaling in their Predictions Following ’Few’-\ntype Quantifiers. arXiv preprint arXiv:2212.08700.\nJames A Michaelov, Seana Coulson, and Ben-\njamin K Bergen. 2023. Can Peanuts Fall in Love\nwith Distributional Semantics? arXiv preprint\narXiv:2301.08731.\nKanishka Misra. 2022. minicons: Enabling Flexi-\nble Behavioral and Representational Analyses of\nTransformer Language Models. arXiv preprint\narXiv:2203.13112.\nKanishka Misra, Allyson Ettinger, and Julia Taylor\nRayz. 2020. Exploring BERT’s Sensitivity to Lex-\nical Cues using Tests from Semantic Priming. In\nFindings of EMNLP.\nMante S Nieuwland and Jos JA Van Berkum. 2006.\nWhen Peanuts Fall in Love: N400 Evidence for the\nPower of Discourse. Journal of Cognitive Neuro-\nscience, 18(7):1098–1111.\nByung-Doh Oh and William Schuler. 2022. Why Does\nSurprisal From Larger Transformer-Based Language\nModels Provide a Poorer Fit to Human Reading\nTimes? In Transactions of the Association for Com-\nputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. In\nOpen-AI Blog.\nSoo Hyun Ryu and Richard L Lewis. 2021. Accounting\nfor Agreement Phenomena in Sentence Comprehen-\nsion with Transformer Language Models: Effects of\nSimilarity-based Interference on Surprisal and At-\ntention. In Proceedings of the NAACL Workshop on\nCognitive Modeling and Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a Distilled Version\nof BERT: Smaller, Faster, Cheaper and Lighter. In\nProceeding of the NeurIPS EMC2Workshop.\nEnrico Santus, Emmanuele Chersoni, Alessandro Lenci,\nand Philippe Blache. 2017. Measuring Thematic Fit\nwith Distributional Feature Overlap. In Proceedings\nof EMNLP.\nAsad Sayeed, Clayton Greenberg, and Vera Demberg.\n2016. Thematic Fit Evaluation: An Aspect of Se-\nlectional Preferences. In Proceedings of the ACL\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP.\nRobyn Speer. 2022. rspeer/wordfreq: v3.0.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of ACL.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al.\n2019b. What Do You Learn from Context? Prob-\ning for Sentence Structure in Contextualized Word\nRepresentations. arXiv preprint arXiv:1905.06316.\nJulie A Van Dyke. 2007. Interference Effects from\nGrammatically Unavailable Constituents During Sen-\ntence Processing. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition, 33(2):407.\nWalter JB Van Heuven, Pawel Mandera, Emmanuel\nKeuleers, and Marc Brysbaert. 2014. SUBTLEX-\nUK: A New and Improved Word Frequency Database\nfor British English. Quarterly Journal of Experimen-\ntal Psychology, 67(6):1176–1190.\n146\nMarten Van Schijndel and Tal Linzen. 2018. Modeling\nGarden Path Effects without Explicit Hierarchical\nSyntax. In Proceedings of CogSci.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. Advances in Neural Information Process-\ning Systems, 30.\nMatthew W Wagers, Ellen F Lau, and Colin Phillips.\n2009. Agreement Attraction in Comprehension: Rep-\nresentations and Processes. Journal of Memory and\nLanguage, 61(2):206–237.\nJason Wei, Yi Tay, and Quoc V Le. 2022. Inverse\nScaling Can Become U-shaped. arXiv preprint\narXiv:2211.02011.\nEthan Wilcox, Roger Levy, Takashi Morita, and Richard\nFutrell. 2018. What Do RNN Language Mod-\nels Learn about Filler-gap Dependencies? arXiv\npreprint arXiv:1809.00042.\nKayo Yin and Graham Neubig. 2022. Interpreting Lan-\nguage Models with Contrastive Explanations. In\nProceedings of EMNLP.\nAppendix\nDescriptive statistics\nThe statistics for the Surprisal scores can be seen\nin Table 3 and 4, while the logarithmic frequencies\nof attractor and target nouns are in Table 5 and 6\n(notice that the target nouns were the same in all\nthe experimental conditions).\nBoxplots\nThe boxplots for the Surprisal scores for all the\nmetrics and models are shown in Figure 2.\nSentence\nModels Min Max Mean Std\nGPT-2 3.88 5.76 4.525 0.319\nDistilGPT-2 4.150 6.010 4.824 0.399\nGPT-Neo 3.400 5.460 4.268 0.391\nTable 3: Cunnings dataset Surprisal mean descriptive\nstatistics (sentence).\nTarget\nModels Min Max Mean Std\nGPT-2 0.74 17.35 7.597 3.759\nDistilGPT-2 0.67 19.66 7.984 3.039\nGPT-Neo 1.40 18.09 7.308 3.819\nTable 4: Cunnings dataset Surprisal mean descriptive\nstatistics (target phrase).\nCond. Min Max Mean Std\na,c 0.000002 0.000513 0.000085 0.000123\nb,d 0.000001 0.000513 0.000077 0.000122\nTable 5: Log-transformed frequency statistics for the at-\ntractor nouns across conditions in the Cunnings dataset.\nThe frequencies were extracted with the Wordfreq li-\nbrary (Speer, 2022), which relies on the SUBTLEX\ndatabase (Van Heuven et al., 2014).\nCond. Min Max Mean Std\na,b,c,d 0.000001 0.000525 0.000053 0.000109\nTable 6: Log-transformed frequency frequency statis-\ntics for the target nouns in the Cunnings dataset. The\nfrequencies were extracted with the Wordfreq library\n(Speer, 2022), which relies on the SUBTLEX database\n(Van Heuven et al., 2014).\n147\nFigure 2: Boxplots of the Surprisal for all the metrics-model combinations: target Surprisal scores on the left,\nsentence Surprisal on the right; GPT-2 in the top row, DistilGPT-2 in the middle row, GPTNeo at the bottom.\n148",
  "topic": "Attraction",
  "concepts": [
    {
      "name": "Attraction",
      "score": 0.7101711630821228
    },
    {
      "name": "Sentence",
      "score": 0.7088140845298767
    },
    {
      "name": "Semantic role labeling",
      "score": 0.6575354337692261
    },
    {
      "name": "Computer science",
      "score": 0.6331373453140259
    },
    {
      "name": "Natural language processing",
      "score": 0.6048487424850464
    },
    {
      "name": "Phrase",
      "score": 0.5496091842651367
    },
    {
      "name": "Psycholinguistics",
      "score": 0.5069068074226379
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4975323975086212
    },
    {
      "name": "Verb",
      "score": 0.48036274313926697
    },
    {
      "name": "Argument (complex analysis)",
      "score": 0.4570882022380829
    },
    {
      "name": "Sentence processing",
      "score": 0.4485439658164978
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4301309883594513
    },
    {
      "name": "Noun",
      "score": 0.4247759282588959
    },
    {
      "name": "Noun phrase",
      "score": 0.42316004633903503
    },
    {
      "name": "Semantic similarity",
      "score": 0.4170016050338745
    },
    {
      "name": "Linguistics",
      "score": 0.36265790462493896
    },
    {
      "name": "Psychology",
      "score": 0.2656491994857788
    },
    {
      "name": "Cognition",
      "score": 0.16454634070396423
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}