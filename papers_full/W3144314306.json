{
  "title": "Learning Spatio-Temporal Transformer for Visual Tracking",
  "url": "https://openalex.org/W3144314306",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1900377771",
      "name": "Bin Yan",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2391307838",
      "name": "Houwen Peng",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2158570112",
      "name": "Jianlong Fu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2057628437",
      "name": "Dong Wang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2168527748",
      "name": "Huchuan Lu",
      "affiliations": [
        "Dalian University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6753818355",
    "https://openalex.org/W2558899534",
    "https://openalex.org/W3034297219",
    "https://openalex.org/W1915785815",
    "https://openalex.org/W6779586474",
    "https://openalex.org/W2767302379",
    "https://openalex.org/W2989688045",
    "https://openalex.org/W2947624463",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3104158266",
    "https://openalex.org/W3173344965",
    "https://openalex.org/W2985509612",
    "https://openalex.org/W6780401549",
    "https://openalex.org/W2963471260",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W3035257046",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W3035211844",
    "https://openalex.org/W2987460522",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W6966936708",
    "https://openalex.org/W2964423614",
    "https://openalex.org/W6780018109",
    "https://openalex.org/W6720898849",
    "https://openalex.org/W6755863804",
    "https://openalex.org/W6638667902",
    "https://openalex.org/W2997896013",
    "https://openalex.org/W2605173812",
    "https://openalex.org/W2888456413",
    "https://openalex.org/W2998027361",
    "https://openalex.org/W6790221906",
    "https://openalex.org/W3035473155",
    "https://openalex.org/W6788023325",
    "https://openalex.org/W2998434318",
    "https://openalex.org/W2158592639",
    "https://openalex.org/W2963227409",
    "https://openalex.org/W6785727093",
    "https://openalex.org/W3034617042",
    "https://openalex.org/W3035672751",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2962824803",
    "https://openalex.org/W2557641257",
    "https://openalex.org/W2966759264",
    "https://openalex.org/W3035453691",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2891033863",
    "https://openalex.org/W3035511673",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6703518758",
    "https://openalex.org/W6682922062",
    "https://openalex.org/W6749967772",
    "https://openalex.org/W3001584168",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6749997223",
    "https://openalex.org/W3035571898",
    "https://openalex.org/W2139047213",
    "https://openalex.org/W3035725297",
    "https://openalex.org/W2990205821",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W6726654379",
    "https://openalex.org/W6787985011",
    "https://openalex.org/W1857884451",
    "https://openalex.org/W6749631981",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3127756064",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2989778648",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2963074722",
    "https://openalex.org/W2962731685",
    "https://openalex.org/W3039289913",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W2794744029",
    "https://openalex.org/W2886910176",
    "https://openalex.org/W3108519869",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3203414901",
    "https://openalex.org/W2964253307",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2898200825",
    "https://openalex.org/W3102710196",
    "https://openalex.org/W2154889144"
  ],
  "abstract": "In this paper, we present a new tracking architecture with an encoder-decoder transformer as the key component. The encoder models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem, without using any proposals or predefined anchors. With the encoder-decoder transformer, the prediction of objects just uses a simple fully-convolutional network, which estimates the corners of objects directly. The whole method is end-to-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on five challenging short-term and long-term benchmarks, while running at real-time speed, being 6x faster than Siam R-CNN. Code and models are open-sourced at https://github.com/researchmm/Stark.",
  "full_text": "Learning Spatio-Temporal Transformer for Visual Tracking\nBin Yan1,∗, Houwen Peng2,†, Jianlong Fu2, Dong Wang1, Huchuan Lu1\n1Dalian University of Technology 2Microsoft Research Asia\nAbstract\nIn this paper, we present a new tracking architecture with\nan encoder-decoder transformer as the key component. The\nencoder models the global spatio-temporal feature depen-\ndencies between target objects and search regions, while\nthe decoder learns a query embedding to predict the spa-\ntial positions of the target objects. Our method casts object\ntracking as a direct bounding box prediction problem, with-\nout using any proposals or predeﬁned anchors. With the\nencoder-decoder transformer, the prediction of objects just\nuses a simple fully-convolutional network, which estimates\nthe corners of objects directly. The whole method is end-\nto-end, does not need any postprocessing steps such as co-\nsine window and bounding box smoothing, thus largely sim-\nplifying existing tracking pipelines. The proposed tracker\nachieves state-of-the-art performance on ﬁve challenging\nshort-term and long-term benchmarks, while running at\nreal-time speed, being 6×faster than Siam R-CNN [47].\nCode and models are open-sourced at here.\n1. Introduction\nVisual object tracking is a fundamental yet challenging\nresearch topic in computer vision. Over the past few years,\nbased on convolutional neural networks, object tracking has\nachieved remarkable progress [25, 9, 47]. However, convo-\nlution kernels are not good at modeling long-range depen-\ndencies of image contents and features, because they only\nprocess a local neighborhood, either in space or time. Cur-\nrent prevailing trackers, including both the ofﬂine Siamese\ntrackers and the online learning models, are almost all build\nupon convolutional operations [2, 37, 3, 47]. As a conse-\nquence, these methods only perform well on modeling lo-\ncal relationships of image content, but being limited to cap-\nturing long-range global interactions. Such deﬁciency may\ndegrade the model capacities on dealing with the scenarios\nwhere the global contextual information is important for lo-\ncalizing target objects, such as the objects undergoing large-\nscale variations or getting in and out of views frequently.\n∗Work performed when Bin is an intern of MSRA. †Corresponding\nauthor: houwen.peng@microsoft.com.\n� �� �� �� �� ��\n�����������\n����\n����\n����\n����\n����\n����\n����\n����\n�����������\n���������� ���������\n����������\n������\n���� ����\n����� �����������\n���������\n����������� ����\n���������\nFigure 1: Comparison with state-of-the-arts on LaSOT [13]. We\nvisualize the Success performance with respect to the Frames-Per-\nSeconds ( fps) tracking speed. Ours-ST101 and Ours-ST50 in-\ndicate the proposed trackers with ResNet-101 and ResNet-50 as\nbackbones, respectively. Better viewed in color.\nThe problem of long range interactions has been tackled\nin sequence modeling through the use of transformer [46].\nTransformer has enjoyed rich success in tasks such as\nnatural language modeling [11, 39] and speech recogni-\ntion [34]. Recently, transformer has been employed in dis-\ncriminative computer vision models and drawn great atten-\ntion [12, 5, 35]. Inspired by the recent DEtection TRans-\nformer (DETR) [5], we propose a new end-to-end tracking\narchitecture with encoder-decoder transformer to boost the\nperformance of conventional convolution models.\nBoth spatial and temporal information are important for\nobject tracking. The former one contains object appearance\ninformation for target localization, while the latter one in-\ncludes the state changes of objects across frames. Previous\nSiamese trackers [25, 51, 14, 6] only exploit the spatial in-\nformation for tracking, while online methods [54, 57, 9, 3]\nuse historical predictions for model updates. Although be-\ning successful, these methods do not explicitly model the\nrelationship between space and time. In this work, consider-\ning the superior capacity on modeling global dependencies,\nwe resort to transformer to integrate spatial and temporal\ninformation for tracking, generating discriminative spatio-\ntemporal features for object localization.\nMore speciﬁcally, we propose a new spatio-temporal ar-\nchitecture based on the encoder-decoder transformer for\narXiv:2103.17154v1  [cs.CV]  31 Mar 2021\nvisual tracking. The new architecture contains three key\ncomponents: an encoder, a decoder and a prediction head.\nThe encoder accepts inputs of an initial target object, the\ncurrent image, and a dynamically updated template. The\nself-attention modules in the encoder learn the relation-\nship between the inputs through their feature dependencies.\nSince the template images are updated throughout video se-\nquences, the encoder can capture both spatial and tempo-\nral information of the target. The decoder learns a query\nembedding to predict the spatial positions of the target ob-\nject. A corner-based prediction head is used to estimate\nthe bounding box of the target object in the current frame.\nMeanwhile, a score head is learned to control the updates of\nthe dynamic template images.\nExtensive experiments demonstrate that our method es-\ntablishes new state-of-the-art performance on on both short-\nterm [18, 36] and long-term tracking benchmarks [13, 22].\nFor instance, our spatio-temporal transformer tracker sur-\npasses Siam R-CNN [47] by 3.9% (AO score) and 2.3%\n(Success) on GOT-10K [18] and LaSOT [13], respectively.\nIt is also worth noting that compared with previous long-\nterm trackers [8, 47, 53], the framework of our method is\nmuch simpler. Speciﬁcally, previous methods usually con-\nsist of multiple components, such as base trackers [9, 50],\ntarget veriﬁcation modules [21], and global detectors [40,\n19]. In contrast, our method only has a single network\nlearned in an end-to-end fashion. Moreover, our tracker can\nrun at real-time speed, being 6×faster than Siam R-CNN\n(30 v.s. 5 fps) on a Tesla V100 GPU, as shown in Fig. 1\nIn summary, this work has three contributions.\n• We propose a new transformer architecture dedicated\nto visual tracking. It is capable of capturing global fea-\nture dependencies of both spatial and temporal infor-\nmation in video sequences.\n• The whole method is end-to-end, does not need\nany postprocessing steps such as cosine window and\nbounding box smoothing, thus largely simplifying ex-\nisting tracking pipelines.\n• The proposed trackers achieve state-of-the-art perfor-\nmance on ﬁve challenging short-term and long-term\nbenchmarks, while running at real-time speed.\n2. Related Work\nTransformer in Language and Vision. Transformer\nis originally proposed by Vaswani et al. [46] for machine\ntranslation task, and has became a prevailing architecture in\nlanguage modeling. Transformer takes a sequence as the in-\nput, scans through each element in the sequence and learns\ntheir dependencies. This feature makes transformer be in-\ntrinsically good at capturing global information in sequen-\ntial data. Recently, transformer has shown their great po-\ntential in vision tasks like image classiﬁcation [12], object\ndetection [5], semantic segmentation [49], multiple object\ntracking [44, 35], etc. Our work is inspired by the recent\nwork DETR [5], but has following fundamental differences.\n(1) The studied tasks are different. DETR is designed for\nobject detection, while this work is for object tracking. (2)\nThe network inputs are different. DETR takes the whole\nimage as the input, while our input is a triplet consisting of\none search region and two templates. Their features from\nthe backbone are ﬁrst ﬂatten and concatenated then sent\nto the encoder. (3) The query design and training strate-\ngies are different. DETR uses 100 object queries and uses\nthe Hungarian algorithm to match predictions with ground-\ntruths during training. In contrast, our method only uses one\nquery and always matches it with the ground-truth without\nusing the Hungarian algorithm. (4) The bounding box heads\nare different. DETR uses a three-layer perceptron to pre-\ndict boxes. Our network adopts a corner-based box head for\nhigher-quality localization.\nMoreover, TransTrack [44] and TrackFormer [35] are\ntwo most recently representative works on transformer\ntracking. TransTrack [44] has the following features. (1)\nThe encoder takes the image features of both the current and\nthe previous frame as the inputs. (2) It has two decoders,\nwhich take the learned object queries and queries from the\nlast frame as the input respectively. With different queries,\nthe output sequence from the encoder are transformed into\ndetection boxes and tracking boxes respectively. (3) The\npredicted two groups of boxes are matched based on the\nIoUs using the Hungarian algorithm [24]. While Track-\nformer [35] has the following features. (1) It only takes the\ncurrent frame features as the encoder inputs. (2) There is\nonly one decoder, where the learned object queries and the\ntrack queries from the last frame interact with each other.\n(3) It associates tracks over time solely by attention opera-\ntions, not relying on any additional matching such as mo-\ntion or appearance modeling. In contrast, our work has the\nfollowing fundamental differences with these two methods.\n(1) Network inputs are different. Our input is a triplet con-\nsisting of the current search region, the initial template and\na dynamic template. (2) Our method captures the appear-\nance changes of the tracked targets by updating the dynamic\ntemplate, rather than updating object queries as [44, 35].\nSpatio-Temporal Information Exploitation. Exploita-\ntion of spatial and temporal information is a core problem in\nobject tracking ﬁeld. Existing trackers can be divided into\ntwo classes: spatial-only ones and spatio-temporal ones.\nMost of ofﬂine Siamese trackers [2, 26, 25, 60, 29] be-\nlong to the spatial-only ones, which consider the object\ntracking as a template-matching between the initial tem-\nplate and the current search region. To extract the rela-\ntionship between the template and the search region along\nthe spatial dimension, most trackers adopt the variants of\ncorrelation, including the naive correlation [2, 26], the\ndepth-wise correlation [25, 60], and the point-wise corre-\nlation [29, 52]. Although achieving remarkable progress\nin recent years, these methods merely capture local simi-\nlarity, while ignoring global information. By contrast, the\nself-attention mechanism in transformer can capture long-\nrange relationship, making it suitable for pair-wise match-\ning tasks. Compared with spatial-only trackers, spatio-\ntemporal ones additionally exploit temporal information to\nimprove trackers’ robustness. These methods can also be\ndivided into two classes: gradient-based and gradient-free\nones. Gradient-based methods require gradient computa-\ntion during inference. One of the classical works is MD-\nNet [37], which updates domain-speciﬁc layers with gradi-\nent descent. To improve the optimization efﬁciency, later\nworks [9, 3, 27, 48, 55] adopt more advanced optimiza-\ntion methods like Gauss-Newton method or meta-learning-\nbased update strategies. However, many real-world de-\nvices for deploying deep learning do not support back-\npropagation, which restricts the application of gradient-\nbased methods. In contrast, gradient-free methods have\nlarger potentials in real-world applications. One class of\ngradient-free methods [54, 57] exploits an extra network to\nupdate the template of Siamese trackers [2, 61]. Another\nrepresentative work LTMU [8] learns a meta-updater to pre-\ndict whether the current state is reliable enough to be used\nfor the update in long-term tracking. Although being effec-\ntive, these methods cause the separation between space and\ntime. In contrast, our method integrates the spatial and tem-\nporal information as a whole, simultaneously learning them\nwith the transformer.\nTracking Pipeline and Post-processing. The tracking\npipelines of previous trackers [25, 51, 60, 47] are com-\nplicated. Speciﬁcally, they ﬁrst generate a large number\nof box proposals with conﬁdence scores, then use various\npost-processing to choose the best bounding box as the\ntracking result. The commonly used post-processing in-\ncludes cosine window, scale or aspect-ratio penalty, bound-\ning box smoothing, tracklet-based dynamic programming,\netc. Though it brings better results, post-processing causes\nthe performance being sensitive to hyper-parameters. There\nare some trackers [16, 19] attempting to simplify the track-\ning pipeline, but their performances still lag far behind that\nof state-of-the-art trackers. This work attempts to close this\ngap, achieving top performance by predicting one single\nbounding box in each frame.\n3. Method\nIn this section, we propose the spatio-temporal tr ans-\nformer network for visual trac king, called STARK. For\nclarity, we ﬁrst introduce a simple baseline method that di-\nrectly applies the original encoder-decoder transformer for\ntracking. The baseline method only considers spatial infor-\nmation and achieves impressive performance. After that, we\nextend the baseline to learn both spatial and temporal repre-\nBackbone\nFlatten and Concatenate\nTransformer\nEncoder\nTransformer\nDecoder\nBounding Box \nprediction head\nTarget Query\nSearch Region\nInitial\nTemplate\nFigure 2: Framework for spatial-only tracking.\nsentations for target localization. We introduce an dynamic\ntemplate and an update controller to capture the appearance\nchanges of target objects.\n3.1. A Simple Baseline Based on Transformer\nWe present a simple baseline framework based on visual\ntransformer for object tracking. The network architecture is\ndemonstrated in Fig. 2. It mainly consists of three compo-\nnents: a convolutional backbone, an encoder-decoder trans-\nformer, and a bounding box prediction head.\nBackbone. Our method can use arbitrary convolutional\nnetworks as the backbone for feature extraction. Without\nloss of generality, we adopt the vanilla ResNet [15] as the\nbackbone. More concretely, except for removing the last\nstage and fully-connected layers, there is no other change\nfor the original ResNet [15]. The input of the backbone is a\npair of images: a template image of the initial target object\nz ∈R3×Hz×Wz and a search region of the current frame\nx ∈R3×Hx×Wx. After passing through of the backbone,\nthe template z and the search image x are mapped to two\nfeature maps fz ∈RC×Hz\ns ×Wz\ns and fx ∈RC×Hx\ns ×Wx\ns .\nEncoder. The feature maps output from the backbone\nrequire pre-processing before feeding into the encoder. To\nbe speciﬁc, a bottleneck layer is ﬁrst used to reduce the\nchannel number from C to d. Then the feature maps are\nﬂatten and concatenated along the spatial dimension, pro-\nducing a feature sequence with length of Hz\ns\nWz\ns + Hx\ns\nWx\ns\nand dimension of d, which servers as the input for the\ntransformer encoder. The encoder consists of N encoder\nlayers, each of which is made up of a multi-head self-\nattention module with a feed-forward network. Due to the\npermutation-invariance of the original transformer [46], we\nadd sinusoidal positional embeddings to the input sequence.\nThe encoder captures the feature dependencies among all\nelements in the sequence and reinforces the original features\nwith global contextual information, thus allowing the model\nto learn discriminative features for object localization.\nDecoder. The decoder takes a target query and the en-\nhanced feature sequence from the encoder as the input.\nDifferent from DETR [5] adopting 100 object queries, we\nonly input one single query into the decoder to predict one\nbounding box of the target object. Besides, since there is\nonly one prediction, we remove the Hungarian algorithm\n[24] used in DETR for prediction association. Similar to\nthe encoder, the decoder stacks M decoder layers, each of\nwhich consists of a self-attention, an encoder-decoder atten-\ntion, and a feed-forward network. In the encoder-decoder\nattention module, the target query can attend to all positions\non the template and the search region features, thus learning\nrobust representations for the ﬁnal bounding box prediction.\nHead. DETR [5] adopts a three-layer perceptron to\npredict object box coordinates. However, as pointed by\nGFLoss [28], directly regressing the coordinates is equiv-\nalent to ﬁtting a Dirac delta distribution, which fails to con-\nsider the ambiguity and uncertainty in the datasets. This\nrepresentation is not ﬂexible and not robust to challenges\nsuch as occlusion and cluttered background in object track-\ning. To improve the box estimation quality, we design a\nnew prediction head through estimating the probability dis-\ntribution of the box corners. As shown in Fig. 3, we ﬁrst\ntake the search region features from the encoder’s output\nsequence, then compute the similarity between the search\nregion features and the output embedding from the decoder.\nNext, the similarity scores are element-wisely multiplied\nwith the search region features to enhance important regions\nand weaken the less discriminative ones. The new feature\nsequence is reshaped to a feature mapf ∈Rd×Hs\ns ×Ws\ns , and\nthen fed into a simple fully-convolutional network (FCN).\nThe FCN consists of Lstacked Conv-BN-ReLU layers and\noutputs two probability maps Ptl(x,y) and Pbr(x,y) for\nthe top-left and the bottom-right corners of object bound-\ning boxes, respectively. Finally, the predicted box coordi-\nnates (ˆxtl, ˆytl) and ( ˆxbr, ˆybr) are obtained by computing\nthe expectation of corners’ probability distribution as shown\nin Eq. (1). Compared with DETR, our method explicitly\nmodels uncertainty in the coordinate estimation, generating\nmore accurate and robust predictions for object tracking.\n(ˆxtl, ˆytl) = (\nH∑\ny=0\nW∑\nx=0\nx·Ptl(x,y),\nH∑\ny=0\nW∑\nx=0\ny·Ptl(x,y)),\n( ˆxbr, ˆybr) = (\nH∑\ny=0\nW∑\nx=0\nx·Pbr(x,y),\nH∑\ny=0\nW∑\nx=0\ny·Pbr(x,y)),\n(1)\nTraining and Inference. Our baseline tracker is trained\nin an end-to-end fashion with the combination of the ℓ1\nLoss and the generalized IoU loss [41] as in DETR. The\nloss function can be written as\nL= λiouLiou(bi,ˆbi) +λL1 L1(bi,ˆbi). (2)\nFCNs\nElement-wise product\nEncoder Output\nDecoder output\nTop-left \ncorner heatmap\nBottom-right \ncorner heatmap\nDot productDot product\nFigure 3: Architecture of the box prediction head.\nwhere bi and ˆbi represent the groundtruth and the predicted\nbox respectively and λiou,λL1 ∈R are hyperparameters.\nBut unlike DETR, we do not use the classiﬁcation loss and\nthe Hungarian algorithm, thus further simplifying the train-\ning process. During inference, the template image together\nwith its features from the backbone are initialized by the\nﬁrst frame and ﬁxed in the subsequent frames. During track-\ning, in each frame, the network takes a search region from\nthe current frame as the input, and returns the predicted box\nas the ﬁnal result, without using any post-processing such\nas cosine window or bounding box smoothing.\n3.2. Spatio-Temporal Transformer Tracking\nSince the appearance of a target object may change\nsigniﬁcantly as time proceeds, it is important to capture\nthe latest state of the target for tracking. In this section,\nwe demonstrate how to exploit spatial and temporal infor-\nmation simultaneously based on the previously introduced\nbaseline. Three key differences are made, including the net-\nwork inputs, an extra score head, and the training & infer-\nence strategy. We elaborate them one by one as below. The\nspatio-temporal architecture is shown in Fig. 4.\nInput. Different from the baseline method which only\nuses the ﬁrst and the current frames, the spatio-temporal\nmethod introduces an dynamically updated template sam-\npled from intermediate frames as an additional input, as\nshown in Fig. 4. Beyond the spatial information from\nthe initial template, the dynamic template can captures the\ntarget appearance changes with time, providing additional\ntemporal information. Similar to the baseline architecture\nin Sec. 3.1, feature maps of the triplet are ﬂatten and con-\ncatenated then sent to the encoder. The encoder extracts dis-\ncriminative spatio-temporal features by modeling the global\nrelationships among all elements in both spatial and tempo-\nral dimensions.\nHead. During tracking, there are some cases where\nthe dynamic template should not be updated. For exam-\nple, the cropped template is not reliable when the target\nhas been completely occluded or has moved out of view,\nor when the tracker has drifted. For simplicity, we consider\nthat the dynamic template could be updated as long as the\nsearch region contains the target. To automatically deter-\nmine whether the current state is reliable, we add a simple\nscore prediction head, which is a three-layer perceptron fol-\nlowed by a sigmoid activation. The current state is consid-\nered reliable if the score is higher than the threshold τ.\nTraining and Inference . As pointed out by recent\nworks [7, 43], jointly learning of localization and classiﬁca-\ntion may cause sub-optimal solutions for both tasks, and it\nis helpful to decouple localization and classiﬁcation. There-\nfore, we divide the training process into two stages, regard-\ning the localization as a primary task and the classiﬁcation\nas a secondary task. To be speciﬁc, in the ﬁrst stage, the\nwhole network, except for the score head, is trained end-to-\nend only with the localization-related losses in Eq. 2. In this\nstage, we ensure all search images to contain the target ob-\njects and let the model learn the localization capacity. In the\nsecond stage, only the score head is optimized with binary\ncross-entropy loss deﬁned as\nLce = yilog(Pi) + (1−yi) log(1 −Pi) , (3)\nwhere yi is the groundtruth label and Pi is the predicted\nconﬁdence , and all other parameters are frozen to avoid af-\nfecting the localization capacity. In this way, the ﬁnal model\nlearn both localization and classiﬁcation capabilities after\nthe two-stage training.\nDuring inference, two templates and corresponding fea-\ntures are initialized in the ﬁrst frame. Then a search region\nis cropped and fed into the network, generating one bound-\ning box and a conﬁdence score. The dynamic template is\nupdated only when the update interval is reached and the\nconﬁdence score is higher than the threshold τ. For efﬁ-\nciency, we set the update interval as Tu frames. The new\ntemplate is cropped from the original image and then fed\ninto the backbone for feature extraction.\n4. Experiments\nThis section ﬁrst presents the implementation details and\nthe results of our STARK tracker on multiple benchmarks,\nwith comparisons to state-of-the-art methods. Then, abla-\ntion studies are presented to analyze the effects of the key\ncomponents in the proposed networks. We also report the\nresults of other candidate frameworks and compare them\nwith our method to demonstrate its superiority. Finally, vi-\nsualization on attention maps of the encoder and the decoder\nare provided to understand how the transformer works.\n4.1. Implementation Details\nOur trackers are implemented using Python 3.6 and Py-\nTorch 1.5.1. The experiments are conducted on a server\nwith 8 16GB Tesla V100 GPUs.\nModel. We report the results of three variants of\nSTARK: STARK-S50, STARK-ST50 and STARK-ST101.\nBackbone\nFlatten and Concatenate\nTransformer\nEncoder\nTransformer\nDecoder\nBounding Box \nprediction head\nTarget Query\nScore head\nUpdate\nTemplate Cropping\nYes\nReplace\nSearch Region\nInitial\nTemplate\nDynamic\nTemplate\nFigure 4: Framework for spatio-temporal tracking. The dif-\nferences with the spatial-only architecture are highlighted\nin pink.\nSTARK-S50 only exploits spatial information and takes\nResNet-50 [15] as the backbone, i.e., the baseline tracker\npresented in Sec. 3.1. STARK-ST50 and STARK-ST101\ntake ResNet-50 and ResNet-101 as the backbones respec-\ntively, exploiting both spatial and temporal information,i.e.,\nthe spatio-temporal tracker presented in Sec. 3.2.\nThe backbones are initialized with the parameters pre-\ntrained on ImageNet. The BatchNorm [20] layers are frozen\nduring training. Backbone features are pooled from the\nfourth stage with a stride of 16. The transformer architec-\nture is similar to that in DETR [5] with 6 encoder layers\nand 6 decoder layers, which consist of multi-head attention\nlayers (MHA) and feed-forward networks (FFN). The MHA\nhave 8 heads, width 256, while the FFN have hidden units of\n2048. Dropout ratio of 0.1 is used. The bounding box pre-\ndiction head is a lightweight FCN, consisting of 5 stacked\nConv-BN-ReLU layers. The classiﬁcation head is a three-\nlayer perceptron with 256 hidden units in each layer.\nTraining. The training data consists of the train-splits of\nLaSOT [13], GOT-10K [18], COCO2017 [30], and Track-\ningNet [36]. As required by VOT2019 challenge, we re-\nmove 1k forbidden sequences from GOT-10K training set.\nThe sizes of search images and templates are 320 ×320\npixels and 128 ×128 pixels respectively, corresponding to\n52 and 22 times of the target box area. Data augmenta-\ntions, including horizontal ﬂip and brightness jittering, are\nused. The minimal training data unit for STARK-ST is\none triplet, consisting of two templates and one search im-\nages. The whole training process of STARK-ST consists\nof two stages, which take 500 epochs for localization and\n50 epochs for classiﬁcation, respectively. Each epoch uses\n6 ×104 triplets. The network is optimized using AdamW\noptimizer [31] and weight decay 10−4. The loss weights\nλL1 and λiou are set to 5 and 2 respectively. Each GPU\n0.0 0.2 0.4 0.6 0.8 1.0\nOverlap threshold\n0\n10\n20\n30\n40\n50\n60\n70\n80Overlap Precision [%]\nSuccess\nSTARK-ST101 [67.1]\nSiamRCNN [64.8]\nPrDiMP50 [59.8]\nLTMU [57.2]\nDiMP50 [56.9]\nOcean [56.0]\nSiamFCpp [54.3]\nATOM [51.5]\nSiamRPNpp [49.6]\n0.0 0.1 0.2 0.3 0.4 0.5\nLocation error threshold\n0\n10\n20\n30\n40\n50\n60\n70\n80Distance Precision [%]\nNormalizedPrecision\nSTARK-ST101 [77.0]\nSiamRCNN [72.2]\nPrDiMP50 [68.8]\nLTMU [66.5]\nOcean [65.1]\nDiMP50 [65.0]\nSiamFCpp [62.3]\nATOM [57.6]\nSiamRPNpp [56.9]\nFigure 5: Comparisons on LaSOT test set [13].\nhosts 16 triplets, hence the mini-batch size is 128 triplets\nper iteration. The initial learning rates of the backbone and\nthe rest parts are 10−5 and 10−4 respectively. The learning\nrate drops by a factor of 10 after 400 epochs in the ﬁrst stage\nand after 40 epochs in the second stage. The training setting\nof STARK-S is almost the same as that of STARK-ST, ex-\ncept that (1) the minimal training data unit of STARK-S is\na template-search pair; (2) the training process only has the\nﬁrst stage.\nInference. The dynamic template update interval Tu\nand the conﬁdence threshold τ are respectively set to 200\nframes and 0.5 by default. The inference pipeline only con-\ntains a forward pass and a coordinate transformation from\nthe search region to the original image, without any extra\npost-processing.\n4.2. Results and Comparisons\nWe compare our STARK with existing state-of-the-art\nobject trackers on three short-term benchmarks (GOT-10K,\nTrackingNet and VOT2020) and two long-term benchmarks\n(LaSOT and VOT2020-LT).\nGOT-10K. GOT-10K [18] is a large-scale benchmark\ncovering a wide range of common challenges in object\ntracking. GOT-10K requires trackers to only use the train-\ning set of GOT-10k for model learning. We follow this pol-\nicy and retrain our models only with the GOT-10K train set.\nAs reported in Tab. 1, with the same ResNet-50 backbone,\nSTARK-S50 and STARK-ST50 outperform PrDiMP50 [10]\nby 3.8% and 4.6% AO scores, respectively. Furthermore,\nSTARK-ST101 obtains a new state-of-the-art AO score of\n68.8%, surpassing Siam R-CNN [47] by 3.9% with the\nsame ResNet-101 backbone.\nTrackingNet. TrackingNet [36] is a large-scale short-\nterm tracking benchmark containing 511 video sequences in\nthe test set. Tab. 2 presents that STARK-S50 and STARK-\nST50 surpass PrDiMP50 [10] by 4.5% and 5.5% in AUC\nrespectively. With a more powerful ResNet-101 backbone,\nSTARK-ST101 achieves the best AUC of 82.0%, outper-\nforming Siam R-CNN by 0.8%.\nVOT2020. Different from previous reset-based evalu-\nations [23], VOT2020 [22] proposes a new anchor-based\nCat-20\nCattle-13\nFigure 6: Visualization of the encoder attention and the de-\ncoder attention.\nevaluation protocol and uses binary segmentation masks as\nthe groundtruth. The ﬁnal metric for ranking is the Ex-\npected Average Overlap (EAO). Tab. 3 shows that STARK-\nS50 achieves a competitive result, which is better than\nDiMP [3] and UPDT [4]. After introducing temporal in-\nformation, STARK-ST50 obtains an EAO of 0.308, be-\ning superior to previous bounding-box trackers. Inspired\nby AlphaRef [22], the winner of VOT2020 real-time chal-\nlenge, we equip STARK with a reﬁnement module pro-\nposed by AlphaRef to generate segmentation masks. The\nnew tracker “STARK-ST50+AR” surpasses previous SOTA\ntrackers, like AlphaRef and OceanPlus [60], getting an EAO\nof 0.505.\nLaSOT. LaSOT [13] is a large-scale long-term tracking\nbenchmark, which contains 280 videos with average length\nof 2448 frames in the test set. STARK-S50 and STARK-\nST50 achieve a gain of 6.0% and 6.6% over PrDiMP [10]\nrespectively, using the same ResNet-50 backbone. Further-\nmore, STARK-ST101 obtains a success of 67.1%, which is\n2.3% higher than Siam R-CNN [47], as shown in Fig. 5.\nVOT2020-LT.VOT2020-LT consists of 50 long videos,\nin which target objects disappear and reappear frequently.\nBesides, trackers are required to report the conﬁdence score\nof the target being present. Precision (Pr) and Recall (Re)\nare computed under a series of conﬁdence thresholds. F-\nscore, deﬁned as F = 2PrRe\nPr +Re , is used to rank different\ntrackers. Since STARK-S does not predict this score, we do\nnot report its result on VOT2020-LT. Tab. 4 demonstrates\nthat STARK-ST50 and STARK-ST101 outperform all pre-\nvious methods with an F-score of 70.2% and 70.1%, re-\nspectively. It is also worth noting that the framework of\nSTARK is much simpler than that of LTMU B, the winner\nof VOT2020-LT Challenge. To be speciﬁc, LTMU B takes\nthe combination of ATOM [9] and SiamMask [50] as the\nshort-term tracker, MDNet [37] as the veriﬁer, and Global-\nTrack [19] as the global detector. Whereas there is only one\nnetwork in STARK and the result is obtained in one forward\nTable 1: Comparisons on GOT-10k test set [18]\nSiamFC\n[2]\nSiamFCv2\n[45]\nATOM\n[9]\nSiamFC++\n[51]\nD3S\n[32]\nDiMP50\n[3]\nOcean\n[60]\nPrDiMP50\n[10]\nSiamRCNN\n[47]\nSTARK\n-S50\nSTARK\n-ST50\nSTARK\n-ST101\nAO(%) 34.8 37.4 55.6 59.5 59.7 61.1 61.1 63.4 64.9 67.2 68.0 68.8\nSR0.5(%) 35.3 40.4 63.4 69.5 67.6 71.7 72.1 73.8 72.8 76.1 77.7 78.1\nSR0.75(%) 9.8 14.4 40.2 47.9 46.2 49.2 47.3 54.3 59.7 61.2 62.3 64.1\nTable 2: Comparisons on TrackingNet test set [36].\nDSiamRPN\n[61]\nATOM\n[9]\nSiamRPN++\n[25]\nDiMP50\n[3]\nSiamAttn\n[56]\nSiamFC++\n[51]\nMAML-FCOS\n[48]\nPrDiMP50\n[10]\nSiamRCNN\n[47]\nSTARK\n-S50\nSTARK\n-ST50\nSTARK\n-ST101\nAUC(%) 63.8 70.3 73.3 74.0 75.2 75.4 75.7 75.8 81.2 80.3 81.3 82.0\nPnorm(%) 73.3 77.1 80.0 80.1 81.7 80.0 82.2 81.6 85.4 85.1 86.1 86.9\nTable 3: Comparisons on VOT2020 [22].“+AR” means using Alpha-Reﬁne to predict masks. The upper row summarizes trackers that\nonly predict bounding boxes and the lower row presents trackers that report masks.\nIVT\n[42]\nKCF\n[17]\nSiamFC\n[2]\nCSR-DCF\n[33]\nATOM\n[9]\nDiMP\n[3]\nUPDT\n[4]\nDPMT SuperDiMP\n[1]\nSTARK\n-S50\nSTARK\n-ST50\nSTARK\n-ST101\nEAO(↑) 0.092 0.154 0.179 0.193 0.271 0.274 0.278 0.303 0.305 0.280 0.308 0.303\nAccuracy(↑) 0.345 0.407 0.418 0.406 0.462 0.457 0.465 0.492 0.477 0.477 0.478 0.481\nRobustness(↑) 0.244 0.432 0.502 0.582 0.734 0.740 0.755 0.745 0.786 0.728 0.799 0.775\nSTM\n[38]\nSiamEM SiamMask\n[50]\nSiamMargin\n[22]\nOcean\n[60]\nD3S\n[32]\nFastOcean AlphaRef\n[22]\nOceanPlus\n[58]\nSTARK\n-S50+AR\nSTARK\n-ST50+AR\nSTARK\n-ST101+AR\nEAO(↑) 0.308 0.310 0.321 0.356 0.430 0.439 0.461 0.482 0.491 0.462 0.505 0.497\nAccuracy(↑) 0.751 0.520 0.624 0.698 0.693 0.699 0.693 0.754 0.685 0.761 0.759 0.763\nRobustness(↑) 0.574 0.743 0.648 0.640 0.754 0.769 0.803 0.777 0.842 0.749 0.817 0.789\npass without post-processing.\nSpeed, FLOPs and Params. As demonstrated in Tab. 5,\nSTARK-S50 can run in real-time at more than 40 fps. Be-\nsides, the FLOPs and Params of STARK-S50 are 4×and\n2×less than those of SiamRPN++. Although STARK-\nST50 takes a dynamic template as the extra input and in-\ntroduces an additional score head, the increases of FLOPs\nand Params is a little, even negligible. This shows that our\nmethod can exploit temporal information in a nearly cost-\nfree fashion. When ResNet-101 is used as the backbone,\nboth FLOPs and Params increase signiﬁcantly but STARK-\nST101 can still run at real-time speed, which is 6x faster\nthan Siam R-CNN (5 fps), as shown in Fig. 1.\n4.3. Component-wise Analysis\nIn this section, we choose STARK-ST50 as the base\nmodel and evaluate the effects of different components in it\non LaSOT [13]. For simplicity, encoder, decoder, positional\nencoding, corner prediction, and score head are abbreviated\nas enc, dec, pos, corner, and score respectively. As shown in\nTab. 6 #1, when the encoder is removed, the success drops\nsigniﬁcantly by 5.3%. This illustrates that the deep interac-\ntion among features from the template and the search region\nplays a key role. The performance drops by 1.9% when the\ndecoder is removed as shown in #2. This drop is less than\nthat of removing the encoder, showing that the importance\nof the decoder is less than the encoder. When the positional\nencoding is removed, the performance only drops by 0.2%\nas shown in #3. Thus we conclude that the positional en-\ncoding is not a key component in our method. We also try\nto replace the corner head with a three-layer perceptron as\nin DETR [5]. #4 shows that the performance of STARK\nwith an MLP as the box head is 2.7% lower than that of the\nproposed corner head. It demonstrates that the boxes pre-\ndicted by the corner head are more accurate. As shown in\n#5, when removing the score head, the performance drops to\n64.5%, which is lower than that of STARK-S50 without us-\ning temporal information. This demonstrates that improper\nuses of temporal information may hurt the performance and\nit is important to ﬁlter out unreliable templates.\n4.4. Comparison with Other Frameworks\nIn this section, we choose the STARK-ST50 as our\nbase model and compare it with other possible candidate\nframeworks. These frameworks include generating queries\nfrom the template, using the Hungarian algorithm, updating\nqueries as in TrackFormer [35], and jointly learning local-\nization and classiﬁcation. Due to the space limitation, the\nﬁgures of the detailed architectures are presented in thesup-\nplementary material.\nTemplate images serve as the queries. Queries and\ntemplates have similar functions in transformer tracking.\nFor example, both of them are expected to encode informa-\ntion about the target objects. From this view, a natural idea\nis to use template images to serve as the queries of the de-\ncoder. Speciﬁcally, ﬁrst, the template and the search region\nfeatures are separately fed to a weight-shared encoder then\nthe queries generated from the template features are used\nto interact with the search region features in the decoder.\nAs shown in Tab. 7, the performance of this framework is\n61.2%, which is 5.2% lower than that of our design. We\nconjecture that the underlying reason is that compared with\nour method, this design lacks the information ﬂow from the\ntemplate to the search region, thus weakening the discrimi-\nnative power of the search region features.\nUsing the Hungarian algorithm [5]. We also try to use\nTable 4: Comparisons on VOT-LT2020 benchmark [22]\nSPLT [53] ltMDNet SiamDWLT [59] RLT DiMP CLGS Megtrack LTMU B [8] LT DSE STARK-ST50 STARK-ST101\nF-score(%) 56.5 57.4 65.6 67.0 67.4 68.7 69.1 69.5 70.2 70.1\nPr(%) 58.7 64.9 67.8 65.7 73.9 70.3 70.1 71.5 71.0 70.2\nRe(%) 54.4 51.4 63.5 68.4 61.9 67.1 68.1 67.7 69.5 70.1\nTable 5: Comparison about the speed, FLOPs and Params.\nTrackers Speed(fps) FLOPs(G) Params(M)\nSTARK-S50 42.2 10.5 23.3\nSTARK-ST50 41.8 10.9 23.5\nSTARK-ST101 31.7 18.5 42.4\nSiamRPN++ 35.0 48.9 54.0\nTable 6: Ablation for important components. Blank denotes the\ncomponent is used by default, while \u0017 represents the component\nis removed. Performance is evaluated on LaSOT.\n# Enc Dec Pos Corner Score Success\n1 \u0017 61.1\n2 \u0017 64.5\n3 \u0017 66.2\n4 \u0017 63.7\n5 \u0017 64.5\n6 66.4\nTable 7: Comparison between STARK and other candidate\nframeworks. Performance is evaluated on LaSOT.\nTemplate\nquery\nHungarian Update\nquery\nLoc-Cls\nJoint\nOurs\nSuccess 61.2 63.7 64.8 62.5 66.4\nK queries, predicting K boxes with conﬁdence scores. K\nis equal to 10 in the experiments. The groundtruth is dy-\nnamically matched with these queries during the training\nusing the Hungarian algorithm. We observe that this train-\ning strategy leads to the “Matthew effect”. To be speciﬁc,\nsome queries predict slightly more accurate boxes than the\nothers at the beginning of training. Then they are chosen\nby the Hungarian algorithm to match with the groundtruth,\nwhich further enlarges the gaps between the chosen queries\nand the unselected ones. Finally, there are only one or two\nqueries having the ability to predict high-quality boxes. If\nthey are not selected during inference, the predicted box\nmay become unreliable. As shown in Tab. 7, this strategy\nperforms inferior to our method and the gap is 2.7%.\nUpdating the query embedding. Different from\nSTARK exploiting temporal information by introducing an\nextra dynamic template, TrackFormer [35] encodes tempo-\nral information by updating the query embedding. Follow-\ning this idea, we extend the STARK-S50 to a new temporal\ntracker by updating the target query. Tab. 7 shows that this\ndesign achieves a success of 64.8%, which is 1.6% lower\nthan that of STARK-ST50. The underlying reason might\nbe that the extra information brought by an updatable query\nembedding is much less than that by an extra template.\nJointly learning of localization and classiﬁcation. As\nmentioned in Sec 3.2, localization is regarded as the primary\ntask and is trained in the ﬁrst stage. While classiﬁcation is\ntrained in the second stage as the secondary task. We also\nmake an experiment to jointly learn localization and clas-\nsiﬁcation in one stage. As shown in Tab. 7, this strategy\nleads to a sub-optimal result, which is 3.9% lower than that\nof STARK. Two potential reasons are: (1) Optimization of\nthe score head interferes with the training of the box head,\nleading to inaccurate box predictions. (2) Training of these\ntwo tasks requires different data. To be speciﬁc, the local-\nization task expects that all search regions contain tracked\ntargets to provide strong supervision. By contrast, the clas-\nsiﬁcation task expects a balanced distribution, half of search\nregions containing the targets, while the remaining half not.\n4.5. Visualization\nEncoder Attention. The upper part of the Fig. 6 shows\na templates-search triplet from the Cat-20, as well as the at-\ntention maps from the last encoder layer. The visualized at-\ntention is computed by taking the central pixel of the initial\ntemplate as the query and taking all pixels from the triplet\nas the key and value It can be seen that the attentions con-\ncentrate on the tracked target and have roughly separated it\nfrom the background. Besides, the features produced by the\nencoder also have strong discrimination power between the\ntarget and distractors.\nDecoder Attention. The lower part of the Fig. 6 demon-\nstrates a templates-search triplet from the Cattle-13, as well\nas the attention maps from the last decoder layer. It can be\nseen that decoder attention on templates and search regions\nare different. Speciﬁcally, attention on the templates mainly\nfocuses on the top-left region of the target, while attention\non the search region tends to be put on the boundaries of the\ntarget. Besides, the learnt attention is robust to distractors.\n5. Conclusion\nThis paper proposes a new transformer-based tracking\nframework, which can capture the long-range dependency\nin both spatial and temporal dimensions. Besides, the pro-\nposed STARK tracker gets rid of hyper-parameters sensi-\ntive post-processing, leading to a simple inference pipeline.\nExtensive experiments show that the STARK trackers per-\nform much better than previous methods on ﬁve short-term\nand long-term benchmarks, while running in real-time. We\nexpect this work can attract more attention on transformer\narchitecture for visual tracking.\nReferences\n[1] https://github.com/visionml/pytracking. 7\n[2] Luca Bertinetto, Jack Valmadre, Jo ˜ao F Henriques, Andrea\nVedaldi, and Philip H S Torr. Fully-convolutional siamese\nnetworks for object tracking. In ECCVW, 2016. 1, 2, 3, 7\n[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Learning discriminative model prediction for track-\ning. In ICCV, 2019. 1, 3, 6, 7\n[4] Goutam Bhat, Joakim Johnander, Martin Danelljan, Fa-\nhad Shahbaz Khan, and Michael Felsberg. Unveiling the\npower of deep tracking. In ECCV, 2018. 6, 7\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 1,\n2, 4, 5, 7\n[6] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang,\nand Rongrong Ji. Siamese box adaptive network for visual\ntracking. In CVPR, 2020. 1\n[7] Bowen Cheng, Yunchao Wei, Honghui Shi, Rogerio Feris,\nJinjun Xiong, and Thomas Huang. Revisiting rcnn: On\nawakening the classiﬁcation power of faster rcnn. In ECCV,\n2018. 5\n[8] Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua Li,\nHuchuan Lu, and Xiaoyun Yang. High-performance long-\nterm tracking with meta-updater. In CVPR, 2020. 2, 3, 8\n[9] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\nMichael Felsberg. ATOM: Accurate tracking by overlap\nmaximization. In CVPR, 2019. 1, 2, 3, 6, 7\n[10] Martin Danelljan, Luc Van Gool, and Radu Timofte. Prob-\nabilistic regression for visual tracking. In CVPR, 2020. 6,\n7\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. NAACL-HLT, 2019. 1\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2\n[13] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\nYu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\nLaSOT: A high-quality benchmark for large-scale single ob-\nject tracking. In CVPR, 2019. 1, 2, 5, 6, 7\n[14] Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, and\nShengyong Chen. SiamCAR: Siamese fully convolutional\nclassiﬁcation and regression for visual tracking. In CVPR,\n2020. 1\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 3, 5\n[16] David Held, Sebastian Thrun, and Silvio Savarese. Learning\nto track at 100 fps with deep regression networks. In ECCV,\n2016. 3\n[17] Jo ˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge\nBatista. High-speed tracking with kernelized correlation ﬁl-\nters. In ICVS, 2008. 7\n[18] Lianghua Huang, Xin Zhao, and Kaiqi Huang. GOT-10k: A\nlarge high-diversity benchmark for generic object tracking in\nthe wild. TPAMI, 2019. 2, 5, 6, 7\n[19] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Globaltrack:\nA simple and strong baseline for long-term tracking. In\nAAAI, 2020. 2, 3, 6\n[20] Sergey Ioffe and Christian Szegedy. Batch Normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML, 2015. 5\n[21] Ilchae Jung, Jeany Son, Mooyeol Baek, and Bohyung Han.\nReal-time MDNet. In ECCV, 2018. 2\n[22] Matej Kristan, Ale ˇs Leonardis, Ji ˇr´ı Matas, Michael Fels-\nberg, Roman Pﬂugfelder, Joni-Kristian K ¨am¨ar¨ainen, Martin\nDanelljan, Luka ˇCehovin Zajc, Alan Luke ˇziˇc, Ondrej Dr-\nbohlav, et al. The eighth visual object tracking vot2020 chal-\nlenge results. In ECCVW, 2020. 2, 6, 7, 8\n[23] Matej Kristan, Jiri Matas, Ales Leonardis, et al. The sev-\nenth visual object tracking VOT2019 challenge results. In\nICCVW, 2019. 6\n[24] Harold W Kuhn. The hungarian method for the assignment\nproblem. Naval research logistics quarterly, 1955. 2, 4\n[25] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,\nand Junjie Yan. SiamRPN++: Evolution of siamese visual\ntracking with very deep networks. In CVPR, 2019. 1, 2, 3, 7\n[26] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.\nHigh performance visual tracking with siamese region pro-\nposal network. In CVPR, 2018. 2\n[27] Peixia Li, Boyu Chen, Wanli Ouyang, Dong Wang, Xiaoyun\nYang, and Huchuan Lu. GradNet: Gradient-guided network\nfor visual object tracking. In ICCV, 2019. 3\n[28] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu,\nJun Li, Jinhui Tang, and Jian Yang. Generalized focal loss:\nLearning qualiﬁed and distributed bounding boxes for dense\nobject detection. In NeurIPS, 2020. 4\n[29] Bingyan Liao, Chenye Wang, Yayun Wang, Yaonong Wang,\nand Jun Yin. PG-Net: Pixel to global matching network for\nvisual tracking. 2, 3\n[30] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.\nBourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll´ar, and C. Lawrence Zitnick. Microsoft\nCOCO: Common objects in context. In ECCV, 2014. 5\n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[32] Alan Lukezic, Jiri Matas, and Matej Kristan. D3S-a discrim-\ninative single shot segmentation tracker. In CVPR, 2020. 7\n[33] Alan Lukezic, Tomas V ojir, Luka/caron.ts1Cehovin Zajc, Jiri Matas,\nand Matej Kristan. Discriminative correlation ﬁlter with\nchannel and spatial reliability. In CVPR, 2017. 7\n[34] Christoph L ¨uscher, Eugen Beck, Kazuki Irie, Markus Kitza,\nWilfried Michel, Albert Zeyer, Ralf Schl ¨uter, and Hermann\nNey. Rwth asr systems for librispeech: Hybrid vs attention–\nw/o data augmentation. arXiv preprint arXiv:1905.03072 ,\n2019. 1\n[35] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and\nChristoph Feichtenhofer. TrackFormer: Multi-object track-\ning with transformers. arXiv preprint arXiv:2101.02702 ,\n2021. 1, 2, 7, 8\n[36] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-\nsubaihi, and Bernard Ghanem. Trackingnet: A large-scale\ndataset and benchmark for object tracking in the wild. In\nECCV, 2018. 2, 5, 6, 7\n[37] Hyeonseob Nam and Bohyung Han. Learning multi–domain\nconvolutional neural networks for visual tracking. In CVPR,\n2016. 1, 3, 6\n[38] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In ICCV, 2019. 7\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI blog, 1(8):9, 2019. 1\n[40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R–CNN: Towards real-time object detection with re-\ngion proposal networks. In NIPS, 2015. 2\n[41] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding box\nregression. In CVPR, 2019. 4\n[42] David A Ross, Jongwoo Lim, Ruei-Sung Lin, and Ming-\nHsuan Yang. Incremental learning for robust visual tracking.\nijcv, 2008. 7\n[43] Guanglu Song, Yu Liu, and Xiaogang Wang. Revisiting the\nsibling head in object detector. In CVPR, 2020. 5\n[44] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao,\nXinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, and\nPing Luo. TransTrack: Multiple-object tracking with trans-\nformer. arXiv preprint arXiv:2012.15460, 2020. 2\n[45] Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea\nVedaldi, and Philip HS Torr. End-to-end representation\nlearning for correlation ﬁlter based tracking. In CVPR, 2017.\n7\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 1, 2, 3\n[47] Paul V oigtlaender, Jonathon Luiten, Philip HS Torr, and Bas-\ntian Leibe. Siam R-CNN: Visual tracking by re-detection. In\nCVPR, 2020. 1, 2, 3, 6, 7\n[48] Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong,\nand Wenjun Zeng. Tracking by instance detection: A meta-\nlearning approach. In CVPR, 2020. 3, 7\n[49] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. MaX-DeepLab: End-to-end panop-\ntic segmentation with mask transformers. arXiv preprint\narXiv:2012.00759, 2020. 2\n[50] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and\nPhilip H. S. Torr. Fast online object tracking and segmenta-\ntion: A unifying approach. In CVPR, 2019. 2, 6, 7\n[51] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu.\nSiamFC++: towards robust and accurate visual tracking with\ntarget estimation guidelines. In AAAI, 2020. 1, 3, 7\n[52] Bin Yan, Xinyu Zhang, Dong Wang, Huchuan Lu, and\nXiaoyun Yang. Alpha-reﬁne: Boosting tracking perfor-\nmance by precise bounding box estimation. arXiv preprint\narXiv:2012.06815, 2020. 3\n[53] Bin Yan, Haojie Zhao, Dong Wang, Huchuan Lu, and Xi-\naoyun Yang. ‘Skimming-Perusal’ Tracking: A framework\nfor real-time and robust long-term tracking. In ICCV, 2019.\n2, 8\n[54] Tianyu Yang and Antoni B Chan. Learning dynamic memory\nnetworks for object tracking. In ECCV, 2018. 1, 3\n[55] Tianyu Yang, Pengfei Xu, Runbo Hu, Hua Chai, and An-\ntoni B Chan. ROAM: Recurrently optimizing tracking\nmodel. In CVPR, 2020. 3\n[56] Yuechen Yu, Yilei Xiong, Weilin Huang, and Matthew R\nScott. Deformable siamese attention networks for visual ob-\nject tracking. In CVPR, 2020. 7\n[57] Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer,\nMartin Danelljan, and Fahad Shahbaz Khan. Learning the\nmodel update for siamese trackers. In ICCV, 2019. 1, 3\n[58] Zhipeng Zhang, Bing Li, Weiming Hu, and Houweng Peng.\nTowards accurate pixel-wise object tracking by attention re-\ntrieval. arXiv preprint arXiv:2008.02745, 2020. 7\n[59] Zhipeng Zhang and Houwen Peng. Deeper and wider\nsiamese networks for real-time visual tracking. In CVPR,\n2019. 8\n[60] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and\nWeiming Hu. Ocean: Object-aware anchor-free tracking. In\nECCV, 2020. 2, 3, 6, 7\n[61] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and\nWeiming Hu. Distractor-aware siamese networks for visual\nobject tracking. In ECCV, 2018. 3, 7",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7840515375137329
    },
    {
      "name": "Minimum bounding box",
      "score": 0.6148433089256287
    },
    {
      "name": "Encoder",
      "score": 0.6077991127967834
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5824170708656311
    },
    {
      "name": "Transformer",
      "score": 0.5565763115882874
    },
    {
      "name": "Embedding",
      "score": 0.524594247341156
    },
    {
      "name": "Computer vision",
      "score": 0.47929874062538147
    },
    {
      "name": "Convolutional neural network",
      "score": 0.45004379749298096
    },
    {
      "name": "Bounding overwatch",
      "score": 0.4473622143268585
    },
    {
      "name": "Smoothing",
      "score": 0.43146440386772156
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33980581164360046
    },
    {
      "name": "Image (mathematics)",
      "score": 0.10078269243240356
    },
    {
      "name": "Engineering",
      "score": 0.0895853340625763
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}