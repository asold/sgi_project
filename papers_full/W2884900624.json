{
  "title": "A Language Model based Evaluator for Sentence Compression",
  "url": "https://openalex.org/W2884900624",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2012643714",
      "name": "Yang Zhao",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2097231080",
      "name": "Zhiyuan Luo",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2167769188",
      "name": "Akiko Aizawa",
      "affiliations": [
        "National Institute of Informatics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251654079",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2120973168",
    "https://openalex.org/W2740586471",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2118119027",
    "https://openalex.org/W2621058354",
    "https://openalex.org/W1964326564",
    "https://openalex.org/W1498416817",
    "https://openalex.org/W2251656952",
    "https://openalex.org/W2103853614",
    "https://openalex.org/W2963572611",
    "https://openalex.org/W2115322217",
    "https://openalex.org/W2081388731",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2251303282",
    "https://openalex.org/W2467173223",
    "https://openalex.org/W2122311631",
    "https://openalex.org/W2741938760",
    "https://openalex.org/W2515250488"
  ],
  "abstract": "We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. More specifically, the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. An empirical study shows that the proposed model can effectively generate more readable compression, comparable or superior to several strong baselines. Furthermore, we introduce a 200-sentence test set for a large-scale dataset, setting a new baseline for the future research.",
  "full_text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175\nMelbourne, Australia, July 15 - 20, 2018.c⃝2018 Association for Computational Linguistics\n170\nA Language Model based Evaluator for Sentence Compression\nYang Zhao Zhiyuan Luo\nThe University of Tokyo\n7-3-1 Hongo, Bunkyo-ku, Tokyo\n{zhao,zyluo24}@is.s.u-tokyo.ac.jp\nAkiko Aizawa\nNational Institute of Informatics\n2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo\naizawa@nii.ac.jp\nAbstract\nWe herein present a language-model-\nbased evaluator for deletion-based sen-\ntence compression, and viewed this task\nas a series of deletion-and-evaluation op-\nerations using the evaluator. More specif-\nically, the evaluator is a syntactic neural\nlanguage model that is ﬁrst built by learn-\ning the syntactic and structural collocation\namong words. Subsequently, a series of\ntrial-and-error deletion operations are con-\nducted on the source sentences via a re-\ninforcement learning framework to obtain\nthe best target compression. An empirical\nstudy shows that the proposed model can\neffectively generate more readable com-\npression, comparable or superior to sev-\neral strong baselines. Furthermore, we in-\ntroduce a 200-sentence test set for a large-\nscale dataset, setting a new baseline for the\nfuture research.\n1 Introduction\nDeletion-based sentence compression aims to\ndelete unnecessary words from source sentence\nto form a short sentence (compression) while re-\ntaining grammatical and faithful to the under-\nlying meaning of the source sentence. Previ-\nous works used either machine-learning-based ap-\nproach or syntactic-tree-based approaches to yield\nmost readable and informative compression (Jing,\n2000; Knight and Marcu, 2000; Clarke and La-\npata, 2006; McDonald, 2006; Clarke and La-\npata, 2008; Filippova and Strube, 2008; Berg-\nKirkpatrick et al., 2011; Filippova et al., 2015;\nBingel and Søgaard, 2016; Andor et al., 2016;\nZhao et al., 2017; Wang et al., 2017). For example,\n(Clarke and Lapata, 2008) proposed a syntactic-\ntree-based method that considers the sentence\ncompression task as an optimization problem by\nusing integer linear programming, whereas (Filip-\npova et al., 2015) viewed the sentence compres-\nsion task as a sequence labeling problem using\nthe recurrent neural network (RNN), using max-\nimum likelihood as the objective function for op-\ntimization. The latter sets a relatively strong base-\nline by training the model on a large-scale parallel\ncorpus. Although an RNN (e.g., Long short-term\nmemory networks) can implicitly model syntactic\ninformation, it still produces ungrammatical sen-\ntences. We argue that this is because (i) the la-\nbels (or compressions) are automatically yielded\nby employing the syntactic-tree-pruning method.\nIt thus contains some errors caused by syntactic\ntree parsing error, (ii) more importantly, the op-\ntimization objective of an RNN is the likelihood\nfunction that is based on individual words instead\nof readability (or informativeness) of the whole\ncompressed sentence. A gap exists between opti-\nmization objective and evaluation. As such, we are\nof great interest that: (i) can we take the readabil-\nity of the whole compressed sentence as a learning\nobjective and (ii) can grammar errors be recovered\nthrough a language-model-based evaluator to yield\ncompression with better quality?\nTo answer the above questions, a syntax-based\nneural language model is trained on large-scale\ndatasets as a readability evaluator. The neural\nlanguage model is supposed to learn the correct\nword collocations in terms of both syntax and se-\nmantics. Subsequently, we formulate the deletion-\nbased sentence compression as a series of trial-\nand-error deletion operations through a reinforce-\nment learning framework. The policy network\nperforms either RETAIN or REMOVE action to\nform a compression, and receives a reward (e.g.,\nreadability score) to update the network.\nThe empirical study shows that the proposed\nmethod can produce more readable sentences that\n171\npreserve the source sentences, comparable or su-\nperior to several strong baselines. In short, our\ncontributions are two-fold: (i) an effective syntax-\nbased evaluator is built as a post-hoc checker,\nyielding compression with better quality based\nupon the evaluation metrics; (ii) a large scale news\ndataset with 1.02 million sentence compression\npairs are compiled for this task in addition to 200\nmanually created sentences. We made it publicly\navailable.\n2 Methodology\n2.1 Task and Framework\nFormally, deletion-based sentence compression\ntranslates word tokens, (w1,w2,...,w n) into a se-\nries of ones and zeros, (l1,l2,...,l n), where n\nrefers to the length of the original sentence and\nli ∈ {0,1}. Here, ”1” refers to RETAIN and\n”0” refers to REMOVE. We ﬁrst converted the\nword sequence into a dense vector representa-\ntion through the parameter matrix E. Except\nfor word embedding, (e(w1),e(w2),...,e (wn)),\nwe also considered the part-of-speech tag and\nthe dependency relation between wi and its\nhead word as extra features. Each part-of-\nspeech tag was mapped into a vector represen-\ntation, (p(w1),p(w2),...,p (wn)) through the pa-\nrameter matrix P, while each dependency re-\nlation was mapped into a vector representation,\n(d(w1),d(w2),...,d (wn)) through the parameter\nmatrix D. Three vector representations are con-\ncatenated, [e(wi); p(wi); d(wi)] as the input to the\nnext part, policy network.\nFigure 1 shows the graphical illustration of our\nmodel. The policy network is a bi-directional\nRNN that uses the input [e(wi); p(wi); d(wi)] and\nyields the hidden states in the forward direction,\n(hf\n1 ,hf\n2 ,...,h f\nn), and hidden states in the backward\ndirection, (hb\n1,hb\n2,...,h b\nn). Then, concatenation of\nhidden states in both directions, [hf\ni; hb\ni] are fol-\nlowed by a nonlinear layer to turn the output into a\nbinary probability distribution,yi = σ(W[hf\ni; hb\ni])\nwhere σ is a nonlinear function sigmoid, and W\nis a parameter matrix.\nThe policy network continues to sample actions\nfrom the binary probability distribution above un-\ntil the whole action sequence is yielded. In\nthis task, binary actions space is {RETAIN, RE-\nMOVE}. We turn the action sequence into the pre-\ndicted compression, (w1,w2,...,w m), by deleting\nthe words whose current action is REMOVE. Then\n[e(w1);p(w1);d(w1)] [e(wn);p(wn);d(wn)] \n\u0003\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0004\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0015 \u0004\u0001\n\u0007\u0007\u0007\u0007\u0007\u0007\nw2  …… wn\nEvaluator\nPolicy Net\nUpdate\nFigure 1: Graphical illustration of the framework.\nthe (w1,w2,...,w m) is fed into a pre-trained eval-\nuator which will be described in the next section.\n2.2 Syntax-based Evaluator\nThe syntax-based evaluator should assess the de-\ngree to which the compressed sentence is gram-\nmatical, through being used as a reward function\nduring the reinforcement learning phase. It needs\nto satisfy three conditions: (i) grammatical com-\npressions should obtain a higher score than un-\ngrammatical compressions, (ii) for two ungram-\nmatical compressions, it should be able to discrim-\ninate them through the score despite the ungram-\nmaticality, (iii) lack of important parts (such as the\nprimary subject or verb) in the original sentence\nshould receive a greater penalty.\nWe therefore considered an ad-hoc evaluator,\ni.e., the syntax-based language model (evaluator-\nSLM) for these requirements. It integrates the\npart-of-speech tags and the dependency relations\nin the input, while the output to be predicted is\nthe next word token. We observed that the pre-\ndiction of the next word could not only be based\non the previous word but also the syntactic com-\nponents, e.g., for the part-of-speech tag, the noun\nis often followed by a verb instead of an adjec-\ntive or adverb and the integration of the part-of-\nspeech tag allows the model to learn such cor-\nrect word collocations. Figure 2 shows the graphi-\ncal illustration of the evaluator-SLM where the in-\nput is xi = [e(wi); p(wi); d(wi)], followed by a\nbi-directional RNN whose last layer is the Soft-\nmax layer used to represent word probability dis-\ntribution. Similar to (Mousa and Schuller, 2017),\nwe added two special tokens, <S>and </S>in\nthe input so as to stagger the hidden vectors, thus\navoiding self-prediction. Finally, we have the fol-\nlowing formula as one part of the reward functions\nin the learning framework.\n172\n<S>                    X1 Xn-2                         Xn-1\n\u0007\u0007\u0007\u0007\u0007\u0007\nw1 w2 w3 w4 w5\nX2                             X3                             Xn </S> \nW1                              W2                          Wn-1                    Wn\n\u0007\u0007\u0007\u0007\u0007\u0007\nFigure 2: Graphical illustration of bi-directional\nrecurrent neural network language model.\nRSLM( ˆY) =e\n( 1\n|ˆY |\n∑|ˆY |\nt=1 logPLM (yt|y0:t−1))\n(1)\nwhere RSLM ∈ [0,1] and ˆY is the predicted\ncompression by the policy network.\nFurther, it is noteworthy that the performance\ncomparison should be based on a similar com-\npression rate1 (CR) (Napoles et al., 2011), and a\nsmooth reward function RCR = (a+b)(a+b)\naabb xa(1 −\nx)b (both a, b are positive integers; e.g. a = 2,\nb = 2 could lead the compression rate to 0.5) is\nalso used to attain a compressed sentence of simi-\nlar length.\nThe total reward is R= RSLM + RCR. By us-\ning policy gradient methods (Sutton et al., 2000),\nthe policy network is updated with the following\ngradient:\n∇L(θ) =\n|ˆY|∑\nt=1\nR( ˆY)∇logπθ(at|St) (2)\nWhere at ∈{RETAIN, REMOVE}, is the ac-\ntion token by the policy network, and St refers to\nhidden state of the network, [hf\ni; hb\ni] (section 2.1).\n3 Experiments\n3.1 Data\nAs neural network-based methods require a large\namount of training data, we for the ﬁrst time\nconsidered using Gigaword2, a news domain cor-\npus. More speciﬁcally, the ﬁrst sentence and the\nheadline of each article are extracted. After data\ncleansing, we ﬁnally compiled 1.02 million sen-\ntence and headline pairs (see details here 3). It is\nnoteworthy that the headline is not the extractive\n1compression rate is the length of compression divided by\nthe length of the sentence.\n2https://catalog.ldc.upenn.edu/ldc2011t07\n3https://github.com/code4conference/Data\ncompression. Further, we asked two near native\nEnglish speakers to create 200 extractive compres-\nsions for the ﬁrst 200 sentences of this dataset; us-\ning it as the testing set, the ﬁrst 1,000 sentences\n(excluding the testing set) is the development set,\nand the remainder is the training set. To assess the\ninter-assessor agreements, we computed Cohen ’s\nunweighted κ. The computed unweighted κ was\n0.423, reaching a moderate agreement level4\nThe second dataset we used was the Google\ndataset that contains 200,000 sentence compres-\nsion pairs (Filippova et al., 2015). For the purpose\nof comparison, we used the very ﬁrst 1,000 sen-\ntences as the testing set, the next 1,000 sentences\nas the development set, and the remainder as the\ntraining set.\n3.2 Comparison Methods\nWe choose several strong baselines; the ﬁrst one is\nthe dependency-tree-based method that considers\nthe sentence compression task as an optimization\nproblem by using integer linear programming 5.\nInspired by (Filippova and Strube, 2008), (Clarke\nand Lapata, 2008), and (Wang et al., 2017), we de-\nﬁned some constrains: (1) if a word is retained in\nthe compression, its parent should be also retained.\n(2) whether a word wi is retained should partly\ndepend on the word importance score that is the\nproduct of the TF-IDF score and headline score\nh(wi), tf-idf(wi) ·h(wi) where h(wi) represents\nthat whether a word (limited to nouns and verbs) is\nalso in the headline. h(wi)=5 if wi is in the head-\nline; h(wi)=1 otherwise. (3) the dependency rela-\ntions, ROOT, dobj, nsubj, pobj, should be retained\nas they are the skeletons of a sentence. (4) the sen-\ntence length should be over than α but less than\nβ. (5) the depth of the node (word), λdep(wi),\nin the dependency tree. (6) the word with the de-\npendency relation amod is to be removed. It is\nnoteworthy that the method is unsupervised.\nThe second method is the long short-term\nmemory networks (LSTMs) which showed strong\npromise in sentence compression by (Filippova\net al., 2015). The labels were obtained using the\ndependency tree pruning method (Filippova and\nAltun, 2013) and the LSTMs were applied in a su-\npervised manner. Following their works, we also\n4(Landis and Koch, 1977) characterize κvalues <0 as no\nagreement, 0 ∼0.20 as slight, 0.21 ∼0.40 as fair, 0.41 ∼\n0.60 as moderate, 0.61 ∼0.80 as substantial, and 0.81 ∼1\nas almost perfect agreement.)\n5we use http://pypi.python.org/pypi/PuLP\n173\nGigaword Dataset Annotator 1 Annotator 2\nF1 RASP-F1 F1 RASP-F1 CR\n#1 Seq2seq with attention 54.9 60.3 58.6 64.6 0.53\n#2 Dependency tree+ILP 58.0 65.1 61.0 70.9 0.55\n#3 LSTMs+pseudo label 60.3 64.1 64.1 69.2 0.51\n#4 Evaluator-LM 64.5 67.3 66.9 72.2 0.50\n#5 Evaluator-SLM 65.0 69.6 68.2 73.9 0.51\nTable 1: F1 and RASP-F1 results for Gigaword dataset.\nconsider the labels yielded by our dependency-\ntree-based method as pseudo labels and employ\nLSTMs as a baseline.\nFurthermore, for a comprehensive comparison,\nwe applied the sequence-to-sequence with atten-\ntion method widely used in abstractive text sum-\nmarization for sentence compression. Previous\nworks such as (Rush et al., 2015; Chopra et al.,\n2016) have shown promising results with this\nframework, although the focus was generation-\nbased summarization rather than extractive sum-\nmarization. More speciﬁcally, the source sequence\nof this framework is the original sentence, while\nthe target sequence is a series of zeros and ones\n(zeros represents REMOVE and ones represents\nRETAIN). Further, we incorporated dependency\nlabels and part-of-speech tag features in the source\nside of the sequence-to-sequence method.\n3.3 Training\nThe embedding size for word, part-of-speech tag,\nand the dependency relation is 128. We employed\nthe vanilla RNN with a hidden size of 512 for both\nthe policy network and neural language model.\nThe mini-batch size was chosen from [5,50,100].\nV ocabulary size was 50,000. The learning rate for\nneural language model is 2.5e-4, and 1e-05 for\nthe policy network. For policy learning, we used\nthe REINFORCE algorithm (Williams, 1992) to\nupdate the parameters of the policy network and\nﬁnd an policy that maximizes the reward. Because\nstarting from a random policy is impractical ow-\ning to the high variance, we pre-trained the policy\nnetwork using pseudo labels in a supervised man-\nner. For the comparison methods, the hyperparam-\neters and were set to 0.4 and 0.7, respectively, and\nwas set to 0.5. For reproduction, we released the\nsource code here6.\n6https://github.com/code4conference/code4sc\n4 Result and Discussion\nThis section demonstrates the experimental results\non both datasets. As the Gigaword dataset has no\nground truth, we evaluated the baseline and our\nmethod on the 200-sentence test sets created by\ntwo human annotators. For the automatic evalua-\ntion, we employed F1 and RASP-F1 (Briscoe and\nCarroll, 2002) to measure the performances. The\nlatter compares grammatical relations (such as nc-\nsubj and dobj ) found in the system compressions\nwith those found in the gold standard, providing\na means to measure the semantic aspects of the\ncompression quality. For the human evaluation,\nwe asked two near native English speakers to as-\nsess the quality of 50 compressed sentences out\nof the 200-sentence test set in terms of readability\nand informativeness. Here are our observations:\nGigaword Readability Informativeness\n$1 LSTMs 3.56 3.10\n$2 SLM 4.16† 3.16\nTable 2: Human Evaluation for Gigaword dataset.\n†stands for signiﬁcant difference with 0.95 conﬁ-\ndence in the column.\nGoogle Dataset F1 RASP-F1 CR\n&1 Seq2seq with attention 71.7 63.8 0.34\n&2 LSTM (Filippova, 2015) 82.0 - 0.38\n&3 LSTMs (our implement) 84.8 81.9 0.40\n&4 Evaluator-LM 85.0 82.0 0.41\n&5 Evaluator-SLM 85.1 82.3 0.39\nTable 3: F1 and RASP- F1 results for Google\ndataset.\n(1) As shown in Table 1, our Evaluator-SLM-\nbased method yields a large improvement over the\nbaselines, demonstrating that the language-model-\nbased evaluator is effective as a post-hoc gram-\nmar checker for the compressed sentences. This\nis also validated by the signiﬁcant improvement\nin the readability score in Table 2 ($1 vs $2). To\ninvestigate the evaluator in detail, a case study is\nshown in section 4.1.\n174\nCase study\nSENTENCE The Dalian shipyard has built two new huge ships\nPOS tags DET ADJ NOUN VERB VERB NUM ADJ ADJ NOUN\nDEP. rels det compound nsubj aux root nummod amod amod dobj\n#1 The Dalian shipyard has built two new huge ships\n#2 The Dalian shipyard has built two new huge \n#3 The Dalian shipyard has two new huge ships\n#4 The Dalian has built two new huge ships\n#5 The Dalian has built two ships\n#6 The has built two ships\n#7 The Dalian shipyard has built two huge ships\n#8 The Dalian shipyard has built two ships\n#9 The shipyard has built two ships\ne-logR\n59.8\n140.5\n582.9\n1313.5\n1244.8\n1331.2\n46.9\n18.2\n66.5\nFigure 3: Case study for evaluator.\n(2) by comparing annotator 1 with annotator 2\nin Table 1, we observed different performances for\ntwo annotated test sets, showing that compress-\ning a text while preserving the original sentence\nis subjective across the annotators.\n(3) As for Google news dataset, LSTMs\n(LSTM+pos+dep) (&3) is a relatively strong base-\nline, suggesting that incorporating dependency re-\nlations and part-of-speech tags may help model\nlearn the syntactic relations and thus make a bet-\nter prediction. When further applying Evaluator-\nSLM, only a tiny improvement is observed (&3\nvs &4), not comparable to the improvement be-\ntween #3 and #5. This may be due to the differ-\nence in perplexity of the our Evaluator-SLM. For\nGigaword dataset with 1.02 million instances, the\nperplexity of the language model is 20.3, while\nfor the Google news dataset with 0.2 million in-\nstances, the perplexity is 76.5.\n(4) To further explore the degree to which syn-\ntactic knowledge (dependency relations and part-\nof-speech tags) is helpful to evaluator (language\nmodel), we implemented a naive language model,\ni.e., Evaluator-LM, which did not include depen-\ndency relations and part-of-speech tags as input\nfeatures. The results shows that small improve-\nments are observed on two datasets (#4 vs #5;\n&4 vs &5), suggesting that incorporating syntactic\nknowledge may help evaluator to encourage more\nunseen but reasonable word collocations.\n4.1 Evaluator Analysis\nTo further analyze the Evaluator-SLM perfor-\nmance, we used an example sentence, “The Dalian\nshipyard has built two new huge ships” to observe\nhow a language model scores different word dele-\ntion operations. We converted the reward function\nRSLM to e−logRSLM for a better observation (sim-\nilar to ”sentence perplexity”, the higher the score\nis, the worse is the sentence). As shown in Figure\n3, deleting the object(#2), verb(#3), or subject(#4)\nresults in a signiﬁcant increase in ”sentence per-\nplexity”, implying that the syntax-based language\nmodel is highly sensitive to the lack of such syn-\ntactic components. Interestingly, when deleting\nwords such as new or/and huge, the score be-\ncomes lower, suggesting that the model may pre-\nfer short sentences, with unnecessary parts such\nas amod being removed. This property makes it\nquite suitable for the sentence compression task\naiming to shorten sentences by removing unnec-\nessary words.\n5 Conclusion\nWe presented a syntax-based language model\nfor the sentence compression task. We em-\nployed unsupervised methods to yield labels to\ntrain a policy network in a supervised man-\nner. The experimental results demonstrates that\nthe compression could be further improved by a\npost-hoc language-model-based evaluator, and our\nevaluator-enhanced model performs better or com-\nparable upon the evaluation metrics on two large-\nscale datasets.\nAcknowledgments\nThis work was supported by JSPS KAKENHI\nGrant Numbers JP15H01721, JP18H03297. We\nare thankful for the reviewers’ helpful comments\nand suggestions. We would also like to thank Fil-\nippova for sharing their data with us and Clarke for\nthe Annotator Sentence Compression Instructions\nin his PhD dissertation.\n175\nReferences\nDaniel Andor, Chris Alberti, David Weiss, Aliaksei\nSeveryn, Alessandro Presta, Kuzman Ganchev, Slav\nPetrov, and Michael Collins. 2016. Globally nor-\nmalized transition-based neural networks. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers). volume 1, pages 2442–2452.\nTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.\n2011. Jointly learning to extract and compress. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies-Volume 1. Association for Com-\nputational Linguistics, pages 481–490.\nJoachim Bingel and Anders Søgaard. 2016. Text sim-\npliﬁcation as tree labeling. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers) . vol-\nume 2, pages 337–343.\nTed Briscoe and John Carroll. 2002. Robust accurate\nstatistical annotation of general text. In Proceedings\nof the Third International Conference on Language\nResources and Evaluation (LREC’02).\nSumit Chopra, Michael Auli, and Alexander M Rush.\n2016. Abstractive sentence summarization with at-\ntentive recurrent neural networks. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies. pages 93–98.\nJames Clarke and Mirella Lapata. 2006. Constraint-\nbased sentence compression an integer program-\nming approach. In Proceedings of the COL-\nING/ACL on Main conference poster sessions . As-\nsociation for Computational Linguistics, pages 144–\n151.\nJames Clarke and Mirella Lapata. 2008. Global in-\nference for sentence compression: An integer linear\nprogramming approach. Journal of Artiﬁcial Intelli-\ngence Research 31:399–429.\nKatja Filippova, Enrique Alfonseca, Carlos A Col-\nmenares, Lukasz Kaiser, and Oriol Vinyals. 2015.\nSentence compression by deletion with lstms. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing . pages\n360–368.\nKatja Filippova and Yasemin Altun. 2013. Overcom-\ning the lack of parallel data in sentence compression.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing. pages\n1481–1491.\nKatja Filippova and Michael Strube. 2008. Depen-\ndency tree based sentence compression. Fifth Inter-\nnational Natural Language Generation Conference\non - INLG ’08page 25.\nHongyan Jing. 2000. Sentence reduction for automatic\ntext summarization. In Proceedings of the sixth con-\nference on Applied natural language processing. As-\nsociation for Computational Linguistics, pages 310–\n315.\nKevin Knight and Daniel Marcu. 2000. Statistics-\nbased summarization-step one: Sentence compres-\nsion. In Proceedings of the Seventeenth National\nConference on Artiﬁcial Intelligence and Twelfth\nConference on Innovative Applications of Artiﬁcial\nIntelligence. AAAI Press, pages 703–710.\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nbiometrics pages 159–174.\nRyan McDonald. 2006. Discriminative sentence com-\npression with soft syntactic evidence. In 11th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics.\nAmr Mousa and Bj¨orn Schuller. 2017. Contextual bidi-\nrectional long short-term memory recurrent neural\nnetwork language models: A generative approach to\nsentiment analysis. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 1, Long Pa-\npers. volume 1, pages 1023–1032.\nCourtney Napoles, Benjamin Van Durme, and Chris\nCallison-Burch. 2011. Evaluating sentence com-\npression: Pitfalls and suggested remedies. In Pro-\nceedings of the Workshop on Monolingual Text-To-\nText Generation. pages 91–97.\nAlexander M Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing. pages 379–389.\nRichard S Sutton, David A McAllester, Satinder P\nSingh, and Yishay Mansour. 2000. Policy gradi-\nent methods for reinforcement learning with func-\ntion approximation. In Advances in neural informa-\ntion processing systems. pages 1057–1063.\nLiangguo Wang, Jing Jiang, Hai Leong Chieu,\nChen Hui Ong, Dandan Song, and Lejian Liao.\n2017. Can syntax help? improving an lstm-based\nsentence compression model for new domains. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers). volume 1, pages 1385–1393.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. In Reinforcement Learning ,\nSpringer, pages 5–32.\nYang Zhao, Hajime Senuma, Xiaoyu Shen, and Akiko\nAizawa. 2017. Gated neural network for sentence\ncompression using linguistic knowledge. InInterna-\ntional Conference on Applications of Natural Lan-\nguage to Information Systems. Springer, pages 480–\n491.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8602513074874878
    },
    {
      "name": "Sentence",
      "score": 0.7019973993301392
    },
    {
      "name": "Natural language processing",
      "score": 0.6767815947532654
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6232454180717468
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5620077252388
    },
    {
      "name": "Compression (physics)",
      "score": 0.5585454702377319
    },
    {
      "name": "Language model",
      "score": 0.5507364273071289
    },
    {
      "name": "Reinforcement learning",
      "score": 0.5385289192199707
    },
    {
      "name": "Test set",
      "score": 0.5348922610282898
    },
    {
      "name": "Collocation (remote sensing)",
      "score": 0.528883695602417
    },
    {
      "name": "Task (project management)",
      "score": 0.505959689617157
    },
    {
      "name": "Data compression",
      "score": 0.4206621050834656
    },
    {
      "name": "Machine learning",
      "score": 0.2694357931613922
    },
    {
      "name": "Programming language",
      "score": 0.1627458930015564
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I184597095",
      "name": "National Institute of Informatics",
      "country": "JP"
    }
  ]
}