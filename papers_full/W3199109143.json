{
    "title": "LOTR: Face Landmark Localization Using Localization Transformer",
    "url": "https://openalex.org/W3199109143",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5042894086",
            "name": "Ukrit Watchareeruetai",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5049295766",
            "name": "Benjaphan Sommana",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5085581526",
            "name": "Sanjana Jain",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5017430678",
            "name": "Pavit Noinongyao",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5004173788",
            "name": "Ankush Ganguly",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5062222567",
            "name": "Aubin Samacoïts",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5017886318",
            "name": "Samuel W. F. Earp",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5058221096",
            "name": "Nakarin Sritrakool",
            "affiliations": [
                "Chulalongkorn University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6791608452",
        "https://openalex.org/W2536626143",
        "https://openalex.org/W1509928947",
        "https://openalex.org/W2325939864",
        "https://openalex.org/W2096733369",
        "https://openalex.org/W2963466847",
        "https://openalex.org/W2962898354",
        "https://openalex.org/W2969985801",
        "https://openalex.org/W3217435001",
        "https://openalex.org/W2904581885",
        "https://openalex.org/W2087681821",
        "https://openalex.org/W2038891881",
        "https://openalex.org/W1940113235",
        "https://openalex.org/W2605701576",
        "https://openalex.org/W2963342110",
        "https://openalex.org/W2959409678",
        "https://openalex.org/W6699769598",
        "https://openalex.org/W2893441059",
        "https://openalex.org/W6777015588",
        "https://openalex.org/W2040483884",
        "https://openalex.org/W2038952578",
        "https://openalex.org/W2172803778",
        "https://openalex.org/W1855204404",
        "https://openalex.org/W2047508432",
        "https://openalex.org/W2142848890",
        "https://openalex.org/W1963599662",
        "https://openalex.org/W2078966519",
        "https://openalex.org/W1976948919",
        "https://openalex.org/W2129210471",
        "https://openalex.org/W2166694921",
        "https://openalex.org/W1795776638",
        "https://openalex.org/W2963377935",
        "https://openalex.org/W2962819150",
        "https://openalex.org/W3109891286",
        "https://openalex.org/W6751067974",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2963789946",
        "https://openalex.org/W2770121394",
        "https://openalex.org/W2404498690",
        "https://openalex.org/W2736633948",
        "https://openalex.org/W2871667416",
        "https://openalex.org/W1896424170",
        "https://openalex.org/W2962925415",
        "https://openalex.org/W6761630670",
        "https://openalex.org/W3034384783",
        "https://openalex.org/W2307770531",
        "https://openalex.org/W2964304707",
        "https://openalex.org/W2963402313",
        "https://openalex.org/W3034750257",
        "https://openalex.org/W2969015049",
        "https://openalex.org/W6792793321",
        "https://openalex.org/W2982772166",
        "https://openalex.org/W2111372597",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3014641072",
        "https://openalex.org/W2058961190",
        "https://openalex.org/W2284800790",
        "https://openalex.org/W1796263212",
        "https://openalex.org/W2006902452",
        "https://openalex.org/W2963566548",
        "https://openalex.org/W6761937618",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6674330103",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W6762287338",
        "https://openalex.org/W6686509673",
        "https://openalex.org/W6765342010",
        "https://openalex.org/W6770803045",
        "https://openalex.org/W6744072679",
        "https://openalex.org/W2990911034",
        "https://openalex.org/W3092005893",
        "https://openalex.org/W3008618223",
        "https://openalex.org/W3135223942",
        "https://openalex.org/W2152826865",
        "https://openalex.org/W2796438033",
        "https://openalex.org/W2940262938",
        "https://openalex.org/W2752828042",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2943936095",
        "https://openalex.org/W2784874046",
        "https://openalex.org/W2186615578",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W1782590233",
        "https://openalex.org/W3142648707",
        "https://openalex.org/W2318249821",
        "https://openalex.org/W2793741436",
        "https://openalex.org/W2786817236",
        "https://openalex.org/W3099206234",
        "https://openalex.org/W2609296554",
        "https://openalex.org/W2943235166",
        "https://openalex.org/W3024120952",
        "https://openalex.org/W2799672111"
    ],
    "abstract": "This paper presents a novel Transformer-based facial landmark localization network named Localization Transformer (LOTR). The proposed framework is a direct coordinate regression approach leveraging a Transformer network to better utilize the spatial information in a feature map. An LOTR model consists of three main modules: 1) a visual backbone that converts an input image into a feature map, 2) a Transformer module that improves the feature representation from the visual backbone, and 3) a landmark prediction head that directly predicts landmark coordinates from the Transformer&#x2019;s representation. Given cropped-and-aligned face images, the proposed LOTR can be trained end-to-end without requiring any post-processing steps. This paper also introduces a loss function named smooth-Wing loss, which addresses the gradient discontinuity of the Wing loss, leading to better convergence than standard loss functions such as L1, L2, and Wing loss. Experimental results on the JD landmark dataset provided by the First Grand Challenge of 106-Point Facial Landmark Localization indicate the superiority of LOTR over the existing methods on the leaderboard and two recent heatmap-based approaches. On the WFLW dataset, the proposed LOTR framework demonstrates promising results compared with several state-of-the-art methods. Additionally, we report an improvement in the performance of state-of-the-art face recognition systems when using our proposed LOTRs for face alignment.",
    "full_text": "Received January 12, 2022, accepted January 26, 2022, date of publication February 7, 2022, date of current version February 15, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3149380\nLOTR: Face Landmark Localization Using\nLocalization Transformer\nUKRIT WATCHAREERUETAI\n 1, BENJAPHAN SOMMANA\n 1, SANJANA JAIN\n 1,\nPAVIT NOINONGYAO1,2, ANKUSH GANGULY\n 1, AUBIN SAMACOITS\n 1,\nSAMUEL W. F. EARP 1,2, AND NAKARIN SRITRAKOOL\n 3\n1Sertis Vision Laboratory, Bangkok 10110, Thailand\n2QIS Capital, Bangkok 10110, Thailand\n3Department of Mathematics and Computer Science, Faculty of Science, Chulalongkorn University, Pathum Wan, Bangkok 10330, Thailand\nCorresponding author: Ukrit Watchareeruetai (uwatc@sertiscorp.com)\nABSTRACT This paper presents a novel Transformer-based facial landmark localization network named\nLocalization Transformer (LOTR). The proposed framework is a direct coordinate regression approach\nleveraging a Transformer network to better utilize the spatial information in a feature map. An LOTR model\nconsists of three main modules: 1) a visual backbone that converts an input image into a feature map,\n2) a Transformer module that improves the feature representation from the visual backbone, and\n3) a landmark prediction head that directly predicts landmark coordinates from the Transformer’s repre-\nsentation. Given cropped-and-aligned face images, the proposed LOTR can be trained end-to-end without\nrequiring any post-processing steps. This paper also introduces a loss function named smooth-Wing loss,\nwhich addresses the gradient discontinuity of the Wing loss, leading to better convergence than standard\nloss functions such as L1, L2, and Wing loss. Experimental results on the JD landmark dataset provided\nby the First Grand Challenge of 106-Point Facial Landmark Localization indicate the superiority of LOTR\nover the existing methods on the leaderboard and two recent heatmap-based approaches. On the WFLW\ndataset, the proposed LOTR framework demonstrates promising results compared with several state-of-the-\nart methods. Additionally, we report an improvement in the performance of state-of-the-art face recognition\nsystems when using our proposed LOTRs for face alignment.\nINDEX TERMS Artiﬁcial neural networks, computer vision, deep learning, face recognition, image\nprocessing, machine learning.\nI. INTRODUCTION\nLandmark localization focuses on estimating the position of\neach predeﬁned key point in an image. For face landmark\nlocalization, these key points represent different attributes\nof a human face, e.g., the contours of the face, eyes, nose,\nmouth, and eyebrows. Over the past decade, face recogni-\ntion systems have leveraged these landmarks for alignment,\nmaking face landmark localization an intrinsic part of these\nsystems [1]–[9]. Apart from face alignment (e.g., [10], [11]),\nface landmark localization also aids in solving problems\nlike face animation [12], 3D face reconstruction [13]–[15],\nsynthesized face detection [16], emotion classiﬁcation [17],\n[18], and facial action unit detection [19]. Although facial\nlandmark localization is a substantial area of research in\ncomputer vision, given its wide range of applications, it is\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Abdullah Iliyasu\n.\na challenging task owing to its dependency on variations in\nface pose, illumination, and occlusion [1], [20].\nSince its formulation, various statistical approaches have\nbeen proposed, such as Active Shape Model (ASM) [21] and\nActive Appearance Model (AAM) [22], to solve the face\nlandmark localization problem. These models take in prior\ninformation about a face (e.g., the face shape or texture)\nand subsequently ﬁne-tune the model parameters from the\nprovided face image. In addition, research has also been done\nin training patch-based detectors and component detectors to\npredict each landmark on local patches and anatomical com-\nponents on a face image, respectively [23]–[27]. However,\ndue to the lack of global contextual information, the landmark\nconﬁgurations from these approaches are constrained on the\nface shape.\nSince the early 2010s, different variants of Convolu-\ntional Neural Networks (CNNs) have been developed, as an\nalternative to the aforementioned approaches, due to their\n16530\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 10, 2022\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nFIGURE 1. The overview of localization transformer (LOTR). It consists of three main modules: 1) a visual backbone, 2) a transformer\nnetwork, and 3) a landmark prediction head.\nability to extract contextual information from an image.\nTwo approaches, namely coordinate regression and heatmap\nregression, have been widely adopted with a CNN variant as\na backbone.\nIn coordinate regression, fully-connected layers are added\nat the end of a CNN to predict each landmark’s coordi-\nnates. Notable works include [28], [29], and [30] which\nproposed multi-level cascaded CNNs to localize facial land-\nmarks. Combining coordinate regression with multi-level\ncascaded CNNs has been instrumental in predicting the facial\nlandmarks. In these frameworks, the early levels aim to\nlearn about a rough estimate of the landmarks, while the\ndeeper levels seek to ﬁne-tune the prediction. The main\ncaveat of these approaches is the high architectural com-\nplexity, thereby increasing inference time. To address this\ncomplexity issue, [31] and [32] utilized multi-task learning\nto obtain lighter yet more robust models. However, most\nregression-based approaches suffer from spatial information\nloss due to the compression of feature maps before the\nfully-connected layers [1].\nRecently, heatmap-based approaches (e.g., [1], [33]–[35])\nhave extensively been used for face landmark localization\nas they better utilize spatial information to boost the perfor-\nmance compared with coordinate regression methods. These\nmethods predict spatial probability maps wherein each pixel\nis associated with the likelihood of the presence of a landmark\nlocation. Additionally, these approaches lead to better con-\nvergence than coordinate regression techniques and achieve\nstate-of-the-art performance on several benchmark datasets.\nHowever, these approaches usually rely on a computationally\nintense post-processing step to convert heatmaps into pre-\ndicted landmarks, resulting in an increase in the inference\ntime [1]. Therefore, tackling this issue is the primary moti-\nvation behind this paper.\nAlthough the coordinate regression methods mentioned so\nfar may suffer from the issue of spatial information loss,\nthey offer an end-to-end solution at a lower computational\ncomplexity than heatmap-based approaches. In this work,\nwe re-investigate the direct coordinate regression approach\nfor facial landmark localization but exploit a more sophisti-\ncated neural network architecture, i.e., the Transformers [36],\nto address the issue of spatial information loss. A Transformer\nnetwork is a sequence transduction model comprising an\nencoder-decoder architecture that utilizes attention mecha-\nnisms. With the scaling successes in natural language pro-\ncessing (NLP) achieved by Transformers, researchers have\ndeveloped different variations of the Transformer framework\nfor computer vision tasks. One such variant is Detection\nTransformer (DETR) [37] which performs objection detec-\ntion. Inspired by DETR, we propose a Transformer-based\nfacial landmark localization network named Localization\nTransformer (LOTR). As shown in Fig. 1, LOTR consists of\nthree main modules: 1) a visual backbone, 2) a Transformer\nnetwork, and 3) a landmark prediction head. Firstly, LOTR\nadopts a CNN with optional upsampling layers to convert an\ninput image into a feature map and reshapes it to a sequence\nof tokens, each representing a pixel in the feature map. The\nTransformer module then accepts this feature sequence and\na ﬁxed-length landmark queries as input, and produces a\nsequence of tokens as output. Finally, the prediction head,\nVOLUME 10, 2022 16531\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nwhich is a Position-wise Feed-Forward Network (PFFN),\ntransforms each token into its corresponding landmark’s\ncoordinates. Given cropped-and-aligned face images as input,\nthe proposed LOTR can be trained end-to-end without requir-\ning any post-processing step.\nThe key contributions of this research are summarized as\nfollows:\n• We propose a Transformer-based landmark localiza-\ntion network named Localization Transformer (LOTR).\nUnlike the heatmap-based approaches, LOTR does not\nrequire any additional post-processing or heatmap repre-\nsentation, reducing computational complexity and yield-\ning a more efﬁcient network. To the best of our knowl-\nedge, this is the ﬁrst research investigating the use of\nTransformers in the direct regression of landmarks.\n• We demonstrate that the proposed LOTR frame-\nwork detects facial landmarks accurately. Experimen-\ntal results indicate the superiority of the proposed\nLOTR over other algorithms on the leaderboard of the\nFirst JD-landmark localization challenge and two recent\nheatmap-based methods [1], [34]. On another bench-\nmark, WFLW dataset [38], the results show the proposed\nLOTR method is comparable with several state-of-the-\nart methods.\n• We further analyze the model size and inference time\nof the different variants of the LOTR framework.\nCompared with a heatmap-based method [1] and a\nCNN-based direct coordinate regression method, the\nproposed LOTR outperforms both of these methods in\nterms of prediction accuracy, model size, and computa-\ntional complexity.\n• We also investigate the effect of standard loss functions\non model training and propose a modiﬁed loss function,\nnamely smooth-Wing loss, which addresses gradient\ndiscontinuity and training stability issues in an existing\nloss function called the Wing loss [39]. Experimental\nresults show improved performance for the LOTR mod-\nels trained with the proposed smooth-Wing loss.\n• We report an improvement in state-of-the-art face recog-\nnition performance on several benchmark datasets,\nsuch as CFP-FP [40], CPLFW [41], IJB-B [42], and\nIJB-C [43], using ﬁve naive landmarks extracted from\nthe predictions of our proposed LOTR models for face\nalignment.\nThe remaining of this paper is organized as follows:\nSection II provides more detail of related work. Section III\npresents the proposed method, i.e., LOTR. Section IV\nexplains how experiments were setup and discusses the\nresults. Section V concludes the paper.\nII. RELATED WORK\nA. DIRECT COORDINATE REGRESSION\nAs discussed in Section I, direct coordinate regres-\nsion was widely adopted to solve landmark localization\nproblems in the early research period. Regression-based\napproaches [28], [30]–[32], [39], [44], [45] typically use\na CNN along with a dense layer at the end to predict the\nlandmark locations. Usually, the choice of loss for coor-\ndinate regression is either mean-absolute error (L1 loss)\nor mean-squared error (L2 loss). Recently, Feng et al. [39]\nintroduced a new loss function called Wing loss for robust\nfacial landmark localization using regression, discussed in\nSection II-E. Furthermore, Dong et al. [45] trained a facial\nlandmark localization model with style-aggregated images\nfrom a generative adversarial module along with the original\nimages to increase the robustness of the variance of image\nstyles.\nDespite the advancement in regression-based techniques\nfor landmark localization, spatial information loss remains\none major drawback for this approach. This research\naddresses this limitation by inserting a Transformer network\nbetween a CNN backbone and the fully-connected layers\nto preserve spatial information by leveraging a positional\nencoding, discussed in Section III.\nB. HEATMAP REGRESSION\nAs an alternative to coordinate regression methods, heatmap\nregression approaches tackle the spatial information loss\nissue by generating spatial probability maps. A spatial prob-\nability map is a heatmap with pixel values corresponding\nto the probability of a landmark being in a certain location.\nRecent heatmap regression approaches (e.g., [33], [35], [38],\n[46], [47]) have demonstrated state-of-the-art performance on\nfacial landmark localization.\nA common practice for generating a ground-truth heatmap\nfor each landmark is to compute a probability map (e.g., [48],\n[49]) by ﬁtting a bivariate Gaussian function with an offset\nrelative to the landmark, which is deﬁned as:\nG(x,y) =exp\n[\n−(x −x0)2 +(y −y0)2\n2σ2\n]\n, (1)\nwhere (x , y) is any location on the heatmap and (x 0, y0)\nrepresents the ground-truth coordinates of a landmark. A sim-\nple approach to obtain a predicted landmark location from\na heatmap during inference is using the argmax opera-\ntion; however, the heatmap’s resolution becomes a limit-\ning factor. Recent approaches have proposed efﬁcient ways\nto obtain sub-pixel localization [50], [51]. For instance,\nZhang et al. [51] proposed to ﬁt a heatmap with the following\nformulation to obtain a predicted landmark location:\nG(x,µ,6 ) = exp(−1\n2 (x −µ)T 6−1(x −µ))\n2π|6|\n1\n2\n, (2)\nwhere x =(x,y) is any location on the heatmap, µrepre-\nsents an estimated location of the landmark, and 6 is the\nco-variance matrix between the x and y coordinates.\nEarp et al. [1] exploited this sub-pixel inference with inter-\nmittent shufﬂing of upsampling layers and a bag of tricks\nto achieve the second rank on the JD-landmark-2 validation\nset [52]. While successfully preserving spatial information\n16532 VOLUME 10, 2022\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nand achieving state-of-the-art performance on facial land-\nmark localization, heatmap regression methods suffer from\nhigh computational post-processing complexity in converting\nheatmaps into landmark coordinates.\nXiong et al. [34] proposed a vectorization approach in\nwhich each set of vector labels represents a ground-truth\nlandmark’s position using a quasi-Gaussian distribution. This\nproposed distribution is a Gaussian density function enhanc-\ning the distribution peak with an additional constant θ while\nbeing bounded by a threshold 3σ . This method also converts\na predicted heatmap into a set of vectors that encode spatial\ninformation. Although this vectorization technique leads to a\nreduction in post-processing complexity, like other heatmap\nregression methods, this approach also hinders achieving\nan end-to-end pipeline due to a post-processing step during\ninference.\nC. TRANSFORMER-BASED HEATMAP APPROACH\nLan et al. [53] proposed an approach that combines a Trans-\nformer encoder with heatmap regression. They mainly\nfocused on reducing the quantization error that occurs from\ndown-sampling operations. The authors named the pro-\nposed architecture Heatmap in Heatmap (HIH), which takes\nadvantage of two heatmap categories: integer and deci-\nmal heatmaps. An integer heatmap is a probability map\nextracted from a CNN backbone, e.g., Stacked Hourglass\nNetwork [48], which provides a rough estimate of a land-\nmark’s location. On the other hand, a decimal heatmap gives\na more ﬁne-grained offset prediction. For each landmark\nlocation, they ﬁtted a decimal heatmap with a bivariate Gaus-\nsian density function as in (1). During inference, the ﬁnal\nlandmark locations, in both integer and decimal heatmaps,\nare computed from the maximum probability coordinates.\nThe authors conducted experiments comparing two different\narchitectures, namely a CNN and a Transformer encoder in\ncombination with a CNN backbone. The proposed method\noutperforms the state-of-the-art algorithms (e.g., [38], [39],\n[47], [54]) on WFLW-full [38] and COFW [55] datasets. They\nalso reported that the HIH approach with a CNN yields more\nprecise landmarks than a Transformer encoder.\nEven though Lan et al. [53] are the ﬁrst who employ\na Transformer encoder with heatmap regression to tackle\nfacial localization problems, the main caveats of the heatmap\nregression approach, namely the post-processing complexity\nand the lack of an end-to-end pipeline, remain unaddressed.\nD. DETR\nCarion et al. [37] proposed a Transformer [36] framework for\nend-to-end object detection. Their proposed Detection Trans-\nformer (DETR) framework achieves comparative perfor-\nmance to Faster R-CNN [56] on the COCO dataset [57]. The\nDETR framework performs the task of object detection by\ncombining a CNN architecture with a Transformer. It exploits\na pre-trained CNN backbone to extract a low-resolution fea-\nture map from an input image. This feature map is then\nconverted into a sequence and fed to an encoder model,\nconsisting of a Multi-Head Attention (MHA) module and\na PFFN. The MHA module aims to ﬁnd the relationship\nbetween input sequence tokens with each head computing\n‘‘attention’’ by linearly projecting each token into query, key,\nand value vectors. Let Q ∈RNq×D denote a query sequence\nconsisting of Nq tokens of dimension D, while K and V ∈\nRNkv×D denote a key and value sequences of length Nkv,\nrespectively. The MHA is deﬁned as follows:\nMHA(Q,K,V) =concat(H1,..., HM )Wo\ni , (3)\nHi =attention(QWq\ni ,KWk\ni ,VWv\ni ), (4)\nattention(Q,K,V) =softmax(QK⊤\n√\nD′)V, (5)\nwhere M is the number of heads; Wq\ni , Wk\ni , Wv\ni ∈RD×D′\n,\nand Wo\ni ∈ RD×D are learnable projection matrices; and\nD′ =D/M. Fixed positional encoding, which encodes the\nspatial positions of the learned features, is added to the input\nof each attention layer [37]. The embeddings generated by\nthe encoder are then fed to the decoder network, which also\nuses MHA mechanisms [37]. The decoder model decodes the\nembeddings in parallel at each decoder layer. The decoder\noutput is then fed into a ﬁxed number of PFFNs, also known\nas the prediction heads, to generate a set of class predictions\nand predicted bounding boxes. The DETR model predicts all\nobjects at once, computed in parallel. The authors trained the\nmodel end-to-end using Hungarian loss to compare sets, per-\nforming bipartite matching between predicted and ground-\ntruth objects.\nInspired by DETR, we propose the Localization Trans-\nformer (LOTR) to predict landmarks for the facial land-\nmark localization task. The main differences between the two\nare: 1) LOTR does not have a class prediction head since\nall predicted landmarks belong to single object class, and\n2) LOTR does not require the Hungarian loss since the num-\nber of landmarks is ﬁxed.\nE. WING LOSS\nSeveral loss functions have commonly been used to train\nlandmark localization models, including L1 loss (L 1(x) =\n|x|), and L2 loss (L 2(x) =1\n2 x2), and the smooth-L1 loss [58],\nwhich is deﬁned as:\nsmooth-L1(x) =\n\n\n\n1\n2x2 if |x|<1\n|x|− 1\n2 otherwise.\n(6)\nFeng et al. [39] reported another loss function—Wing loss—\nwhich is superior to other loss functions for landmark local-\nization tasks. The key idea in the Wing loss is to force the\nmodel to pay more attention to small errors to improve the\naccuracy of the predicted landmarks. When prediction error\nis larger than a positive threshold, the Wing loss behaves\nlike L1; otherwise, a logarithm function is used to compute\nVOLUME 10, 2022 16533\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nFIGURE 2. Comparison of Wing loss and smooth-Wing loss (top) and their gradient (bottom) in the global view (left), at the outer threshold w (middle),\nand at x equals zero (right). The parameters are set as follows: w =10, ϵ=2, and only for smooth-Wing, t =0.01. For the Wing loss (blue dashed lines),\nthe gradient changes abruptly at the points |x|= w (bottom-middle) and at x =0 (bottom-right). On the other hand, the proposed smooth-Wing loss\n(orange solid lines) is designed to eliminate these gradient discontinuities.\nthe loss. In [39], the Wing loss is deﬁned as follows:\nwing(x) =\n\n\n\nw ln(1 +|x|\nϵ ) if |x|<w\n|x|−c otherwise.\n(7)\nwhere w is the threshold, ϵ is a parameter controlling the\nsteepness of the logarithm part, and c =w−w ln(1 +w/ϵ).\nHowever, as shown in Fig. 2, the Wing loss produces the\ndiscontinuity of the gradient at the threshold w, as pointed out\nin [54], and at zero error, which might affect the stability of\ntraining. In this work, we propose a modiﬁed Wing loss that\nis smooth everywhere and investigate its effectiveness.\nIII. PROPOSED METHODS\nA. LOCALIZATION TRANSFORMER (LOTR)\nThis section explains, in detail, the proposed Transformer-\nbased landmark localization network, named Localization\nTransformer (LOTR). Since LOTR is a coordinate regression\napproach that directly maps an input image into a set of pre-\ndicted landmarks, it requires neither heatmap representation\nnor any post-processing step. Fig. 1 illustrates an overview of\nthe architecture of the proposed LOTR framework. An LOTR\nmodel consists of three main modules, which include\n1) a visual backbone, 2) a Transformer network, and\n3) a landmark prediction head.\nThe visual backbone takes an RGB image as input with\nthe aim to capture context and produce a feature map as\noutput. In this work, we exploit a pre-trained CNN such as\nMobileNetV2 [59], ResNet50 [60], or HRNet [61] to com-\npute a feature map. We apply 1 ×1 convolution to reduce the\nchannel dimension of the feature map. Since the resolution\nof the feature map generated from the CNN backbone might\nbe very low, e.g., 6 ×6 pixels for a 192 ×192 input image,\nwe optionally increase the resolution by using upsampling\nlayers such as deconvolution.\nWe then utilize a Transformer network [36] to enrich the\nfeature representations while maintaining the global infor-\nmation in the feature map. As shown in Fig. 1, the Trans-\nformer module is composed of a Transformer encoder and\na Transformer decoder. Since Transformers were designed\nto process sequential data [36], we convert the feature map\nF ∈ RW ×H×C , obtained from the visual backbone, into a\nsequence of tokens X0 ∈RWH×D. As shown in (8), we use a\n1×1 convolution layer to reduce the channel dimension C of\neach pixel in F to a smaller dimension D ≤C, followed by\nreshaping into a sequence of tokens:\nX0 =reshape(conv1×1(F)). (8)\nA Transformer encoder, i.e., a stack of L encoder lay-\ners, receives this sequence of tokens as input. Each encoder\nlayer consists of two sublayers: 1) Multi-Head Self Atten-\ntion (MHSA) and 2) PFFN. Both these sublayers have resid-\nual connections and layer normalization applied to them.\nAs mentioned in Section II-D, the MHSA, which is a special\ntype of MHA that establishes the relationship between tokens\nin the input sequence by computing the attention by linearly\n16534 VOLUME 10, 2022\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nprojecting each token into query, key, and value vectors and\nsubsequently using the query and key vectors to calculate the\nattention weight applied to the value vectors. The output from\nthe MHSA sublayer (the same size as its input) is then fed\ninto PFFN to further transform the input sequence’s repre-\nsentation. Similar to DETR [37], 2D-positional encoding is\nadded only to the query and key in each encoder layer. The\nl-th encoder layer is deﬁned as follows:\nXl =encl(Xl−1,P), (9)\nencl(X,P) =encl\n2(encl\n1(X,P)), (10)\nencl\n1(X,P) =LN(MHAl(X +P,X +P,X) +X), (11)\nencl\n2(X) =LN(PFFNl(X) +X), (12)\nPFFNl(X) =ReLU(Xwl +bl), (13)\nwhere Xl−1 and Xl denote the input and output of the l-th\nlayer, respectively; P ∈RWH×D is the trainable 2D-positional\nencoding; LN denotes layer normalization; while wl and bl\nare the weight and bias of the PFFN layer, respectively. These\nprocesses repeat L times using the output from the previ-\nous encoder layers as input. The output of the Transformer\nencoder is a transformed sequence XL ∈RWH×D, the same\ndimension as its input.\nFollowing this encoding operation, the output from the\nTransformer encoder is then fed into a Transformer decoder,\ni.e., a stack of L decoder layers. Each decoder layer consists\nof three sublayers: 1) MHSA, 2) Multi-Head Cross-Attention\n(MHCA), and 3) PFFN. The ﬁrst and the third are similar to\nthose of the encoder layers. However, the input to the ﬁrst\nsublayer of the ﬁrst decoder layer is a sequence of landmark\nqueries Y0 ∈ RN×D; each is an embedding of the same\ndimension D. The number of landmark queries equals N,\nwhich is the number of landmarks to predict. In this work,\nlandmark queries are learnable parameters of LOTR, which\nare optimized during model training. The second sublayer,\ni.e., MHCA, takes the output of the ﬁrst sublayer (MHSA)\nand the output generated by the encoder, i.e., XL , as inputs,\nand then computes the relationship between tokens in both\nsequences. The third sublayer, i.e., PFFN, then processes\nthe output from the second sublayer. Like the Transformer\nencoder, all three of these sublayers have residual connections\nand layer normalization applied to them. The l-th decoder\nlayer is deﬁned as follows:\nYl =decl(Yl−1,Y0,XL ,P), (14)\ndecl(Y,Y0,XL ,P) =decl\n3(decl\n2(decl\n1(Y,Y0),XL ,P)), (15)\ndecl\n1(Y,Y0) =LN(MHAl\n1(Y+Y0,Y+Y0,Y)+Y),\n(16)\ndecl\n2(Y,XL ,P) =LN(MHAl\n2(Y+Y0,XL +P,XL )+Y),\n(17)\ndecl\n3(Y) =LN(PFFNl(Y) +Y), (18)\nwhere Yl−1 and Yl denote the input and output of the\nl-th layer, respectively. The Transformer decoder produces\nYL ∈RN×D as output.\nThe landmark prediction head takes as input the sequence\nYL and outputs ˆZ ∈ RN×2, which stores the predicted\nx- and y-coordinates of the N landmarks. This work exploits\na simple PFFN with two hidden layers with ReLU activation.\nThe output layer, however, which consists of only two nodes,\nis without any activation function. In particular, the predicted\nlandmarks ˆZ are computed as follows:\nˆZ =ReLU(YL wz\n1 +bz\n1)wz\n2 +bz\n2, (19)\nwhere wz\nj and bz\nj are the weight and bias of the j-th layer\n(j ∈{1, 2}), respectively.\nSimilar to [37], the computational complexity of the\nMHSA in an encoder layer grows quadratically with the\nfeature map size, i.e., O((WH)2D +WHD2), while that\nin a decoder layer depends on the number of landmarks\ninstead, i.e., O(N2D +ND2). On the other hand, the com-\nplexity of MHCA in a decoder layer grows linearly with\nrespect to the feature map size and the number of landmarks:\nO(NWHD +(N +WH)D2).\nB. SMOOTH-WING LOSS\nSince the proposed LOTR predicts a ﬁxed number of\nlandmarks (N ), a more complicated Hungarian loss used\nin DETR [37] is not required. The proposed LOTR can\nbe trained end-to-end with a standard loss function given\ncropped-and-aligned face images as input. In particular, dur-\ning the training phase, the predicted landmarks ˆZ generated\nby LOTR are compared with the ground truth landmarks\nZ ∈RN×2 to compute the loss, which is deﬁned as:\nloss(Z,ˆZ) =\nN∑\ni=1\n2∑\nj=1\ng(zij −ˆzij), (20)\nwhere the loss function g(x) can be any standard loss function\nsuch as L1, L2, smooth-L1, or Wing loss, which are described\nin Section II-E. Although Feng et al. [39] reported that the\nWing loss was superior to other loss functions for landmark\nlocalization, its major drawback is the gradient discontinuity\nat the threshold w and around the zero error (Fig. 2). This\ndiscontinuity can affect the convergence rate and the stability\nof training.\nIn this work, we also propose a modiﬁed Wing loss, named\nsmooth-Wing loss (s-wing(x )), which is given by:\ns-wing(x) =\n\n\n\n\nsx2 if |x|<t\n|x|−c1 −c2 if |x|>w\n(w+ϵ) ln(1 +|x|\nϵ )−c2 otherwise,\n(21)\ns = w +ϵ\n2t(ϵ+t), (22)\nc1 =w −(w +ϵ) ln(1 +w\nϵ), (23)\nc2 =st2, (24)\nwhere t is an additional threshold (0 < t < w). When\nthe error is smaller than the inner threshold t, it behaves\nVOLUME 10, 2022 16535\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nlike L2 loss, allowing the gradient to be smooth at zero\nerror; otherwise, it behaves like the Wing loss. We deﬁne\nthe constants s, c1, and c2 to smoothen the loss at the inner\nthreshold t as well as at the outer threshold w. As shown in\nFig. 2 (bottom-right), the gradient of the smooth-Wing loss\nchanges linearly when the absolute error |x|is smaller than\nthe inner threshold t. Moreover, the gradient discontinuities\nat |x|= w are also eliminated, as shown in Fig. 2 (bottom-\nmiddle).\nIV. EXPERIMENTS\nA. DATASETS\nWe conducted experiments to measure the performance of the\nproposed LOTR models on two benchmark datasets: 1) the\n106-point JD landmark dataset [52] and 2) the Wider Facial\nLandmarks in-the-Wild (WFLW) dataset [38].\nThe JD-landmark dataset contains images from other face\nlandmark datasets including as 300W [62], [63], LFPW [26],\nAFW [24], HELEN [64], and IBUG [65], covering large\nvariation of face pose and expression. These images are\nre-annotated with 106-point landmarks, which provide more\ninformation about the face structure than any other face land-\nmark dataset. The JD dataset consists of 11,393 images for\ntraining and 2,000 images each for validation and testing.\nThe WFLW dataset, which is based on a well-known face\ndetection dataset called WIDER FACE [66], was recently\nproposed to be a new benchmark for facial landmark local-\nization. The WFLW dataset consists of 10,000 faces: 7,500\nfor training and 2,500 as test images. Compared to other\nprevious datasets, the WFLW dataset is manually annotated\nwith 98 landmarks for each face, providing more information\non the face structure. It is an extremely challenging dataset\ndue to large pose, expression, and occlusion variations. It also\nsupplies six other annotations of face properties, including\npose, expression, illumination, make-up, occlusion, and blur.\nMore than 78% of images in the provided test set have\nannotation with one or more, up to four, properties.\nFollowing [1], we used a ResNet50-based face detector,\nproposed by Deng et al. [67], in our pre-processing step.\nIn particular, we used the bounding box and a set of ﬁve\nsimple landmarks (i.e., eye centers, nose tip, and mouth cor-\nners) obtained from the detector to crop and align the detected\nfaces. We then resized each input image to 192 ×192 pixels\nfor the JD-landmark dataset and to 256 ×256 pixels for the\nWFLW dataset before feeding it to our model. The detail of\nthe pre-processing step is described in [1].\nB. EVALUATION METRICS\nFollowing [1], [34], [47], [54], we used the standard metrics\nincluding the normalized mean error (NME), the failure rate,\nand the area under the curve (AUC) of the cumulative distri-\nbution to evaluate and compare landmark localization algo-\nrithms. The NME is computed across all predicted landmarks\nas follows:\nNME =1\nN\nN∑\ni=1\n||zi −ˆzi||2\nd , (25)\nTABLE 1. The architectures of LOTRs used in the experiments with the\nJD-landmark dataset.\nwhere N is the number of landmarks, zi and ˆzi denotes the\ni-th ground truth landmark and the i-th predicted landmark,\nrespectively, and d is a normalization factor. For the JD-\nlandmark dataset, following [52], the normalization factor d\nwas deﬁned as √WbboxHbbox, where Wbbox and Hbbox are the\nwidth and height of the bounding box enclosing all the ground\ntruth landmarks, respectively. While, for the WFLW dataset,\nthe inter-ocular distance, i.e., the distance between the outer\neye corners, was used as the normalization factor. If the NME\nof a test image is above a threshold, it is considered a failure.\nThe failure rate is, therefore, the rate of failure cases. The\nthreshold was set to 8% for the JD-landmark dataset and\n10% for the WFLW dataset. The AUC is computed from\nthe cumulative error distribution, representing the proportion\nof images with NME smaller than the threshold. Therefore,\na larger AUC represents a more accurate algorithm.\nC. MODEL TRAINING\nTable 1 presents the conﬁguration of three LOTR models,\nnamely LOTR-M, LOTR-M+, and LOTR-R+, used in the\nexperiments with the JD-landmark dataset. The LOTR-M\nmodel is the base model that utilizes a MobileNetV2 1.0, pre-\ntrained on the ImageNet dataset [68], as the visual backbone,\ngenerating a feature map of size 6 ×6 ×1280 from a\n192 ×192 RGB image. We reduced the number of feature\nchannels to 64 by a 1 ×1 convolution, then reshaping into\na sequence of tokens. Subsequently, we used a Transformer\nmodule with two encoder layers and two decoder layers to\nprocess the sequence, followed by a dropout technique [69]\nwith a dropout rate of 0.1. Following, we used a landmark pre-\ndiction head, i.e., a PFFN consisting of two hidden layers with\n512 nodes, with ReLU activation, followed by an output layer\nwith two nodes, to process each token in the output sequence\nof the Transformer. In the LOTR-M+ model, to increase the\nresolution of the feature map, we inserted upsampling layers\nbetween the MobileNetV2 1.0 backbone and the Transformer\nmodule. In particular, we utilized a 1 ×1 convolution to\nreduce the number of channels to 256 and then applied two\ndeconvolution layers (128 and 64 ﬁlters, with ﬁlter size =\n4×4) to increase the feature resolution to 24 ×24 pixels before\nfeeding into the Transformer. The LOTR-R+ model adopts\nthe same conﬁguration as LOTR-M+, except the backbone is\nchanged to a pre-trained ResNet50. For the WFLW dataset,\nwe experimented with another model, named LOTR-HR+,\nthat uses the same architecture as LOTR-M+ and LOTR-R+\nbut exploits HRNet [61] as the backbone.\n16536 VOLUME 10, 2022\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nTABLE 2. Comparison with the state-of-the-arts on the WFLW dataset.\nWe initialized the parameters of the LOTR models using\nHe’s method [70]. We used a standard normal distribution to\ninitialize the positional encoding and the landmark queries,\nboth learnable. For the landmark queries, we speciﬁcally used\na standard deviation of 10 −4 for the initialization. We trained\nthe LOTR models using the LAMB optimizer [71] with a\nbase learning of 10 −3 for 100 epochs while reducing the\nlearning rate with a factor of 0.1 at epochs 50 and 75 and\nsetting the batch size to 32. We used the smooth-Wing loss,\ndescribed in Section III-B, setting the inner threshold (t ) to\n0.01, the outer threshold (w) to 10, and the steepness control\nparameter (ϵ) to 2 as the parameters. For data augmentation\nand training tricks, we used the same steps described by [1].\nWe implemented the model using MXNet framework [72]\nwith Gluon libraries [73], using a single NVIDIA Titan X\nGPU for training.\nD. RESULTS ON THE WFLW DATASET\nIn this section, we compare the proposed LOTR with sev-\neral state-of-the-art methods, including Look-at-Boundary\n(LAB) [38], Wing loss [39], adaptive Wing loss (AWing) [54],\nLUVLi [47], Gaussian vector (GV) [34], and Heatmap-In-\nHeatmap (HIH) [53]. As shown in Table 2, our proposed\nLOTR-HR+achieves an NME of 4.31%, clearly outperform-\ning LAB, Wing, AWing, and LUVLi methods, and yields an\nAUC of 60.14%, surpassing all state-of-the-arts by a large\nmargin (0.44–6.91 points). While comparable with GV in\nterms of NME and failure rate, our LOTR model achieves\na better AUC and does not require any post-processing step.\nAlthough our proposed LOTR model does not surpass the\nperformance of HIH in terms of NME and failure rate,\nit outperforms this state-of-the-art model on the AUC metric.\nFig. 3 presents sample images of each subset from the test\nset of the WFLW dataset by visualizing predicted landmarks\nTABLE 3. The evaluation results for different LOTR models on the\nJD-landmark test set; † and ‡ denote the first and second place entries.\noverlaid on the input images. In this ﬁgure, we show images\nwith different ranges of the NME where the ﬁrst row has\nNME less than 0.05; the middle row has NME in the range\nof 0.05 to 0.06, and the last row has NME more than 0.06.\nThe ﬁgure shows the face landmark localization ability of\nour model, which can accurately locate facial key points with\ndifferent variations, i.e., large pose, exaggerated expression,\nextreme illumination, make-up, occlusion, and blur.\nE. RESULTS ON THE JD-LANDMARK DATASET\nTable 3 presents the performance of the proposed LOTR\nmodels, evaluated on the test set of the ﬁrst Grand Challenge\nof 106-Point Facial Landmark Localization. 1 For brevity, the\ntable includes only the result of the top two ranked algorithms\nout of 21 algorithms submitted to the challenge.\nTable 3 shows that the proposed LOTR models, including\nthe smallest model, i.e., LOTR-M, gain more than 3 points\nin the AUC in comparison to the top two ranked algorithms\non the ﬁrst challenge leaderboard. The table also compares\nLOTR models with two recent methods based on the heatmap\nregression, i.e., [1] and [34]. All of the proposed LOTR\nmodels surpass the heatmap approach in [34] by a signiﬁcant\n1https://facial-landmarks-localization-challenge.github.io\nVOLUME 10, 2022 16537\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nFIGURE 3. Sample images of the test set of the WFLW dataset with predicted landmarks from our model. Each column\ndisplays the images with different subsets. Each row displays images with a different range of NMEs: <0.05 (top),\n0.05–0.06 (middle), and >0.06 (bottom).\namount of the AUC gain (3.7–4.5 points). In comparison\nwith [1], our LOTR models achieve better performance using\nthe same backbone. While our smallest model, i.e., LOTR-M,\nis comparable with the ResNet50 model by [1], our bigger\nmodels, i.e., LOTR-M+ and LOTR-R+, surpass it by an\nAUC gain of 0.15–0.65 points, showing the superiority of our\napproach over theirs. Similar to [1], we also utilized a ﬂipping\ntechnique during inference to improve the prediction accu-\nracy. In particular, we fed the original cropped-and-aligned\nface images and their horizontally ﬂipped version into the\nmodel and averaged their corresponding predicted landmark\ncoordinates.\nIn terms of ﬂoating point operations per second (FLOPS),\nthe LOTR-M model uses only 0.23 GFLOPS. In contrast, the\nLOTR-M+model has 47 percent higher GFLOPS than the\nformer due to the presence of upsampling layers; however,\noperating with only 0.44 GFLOPS. The LOTR-R+ model\nuses 3.23 GFLOPS which is signiﬁcantly higher than both the\nLOTR-M and LOTR-M+models due to its heavier ResNet50\nbackbone.\nF. EFFECT ON FACE RECOGNITION PERFORMANCE\nIn recent years, face recognition systems have evolved to\ncontain four modules: face detection, alignment, embedding,\nand distance computation. Several existing face detectors\npredict the location of ﬁve naive landmarks that correspond\nto the center of each eye, the tip of the nose, and the corners of\nthe mouth. These ﬁve landmarks are used for face alignment\nin accordance to the conventional ﬁve-landmark alignment\nprotocol (e.g., [3], [5]–[9]). In this section, we investigate\nhow the face alignment process using our proposed LOTR\nmodels affects the performance of the state-of-the-art face\nrecognition system.\nIn our experiments, we ﬁrst detected the face and then\nused our LOTR models to obtain the 106 facial landmarks.\nFollowing this, we extracted a subset of the ﬁve landmarks for\nface alignment to make a fair comparison. We experimented\nwith two different face detectors, namely PyramidKey with\nMobileNetV21.0 backbone proposed by Earp et al. [74] and\nthe publicly available RetinaFace with a ResNet50 back-\nbone from Deng et al. [67]. We exploited the pre-trained\nLResNet100E-IR from InsightFace 2 as the face embedding\nnetwork. Table 4 shows the performance of the face embed-\nding network with different combinations of the face detec-\ntors and face alignment processes on six benchmark datasets,\nwhich include LFW [75], CFP-FP [40], CALFW [76],\nCPLFW [41], IJB-B [42], and IJB-C [43]. We report the True\nAcceptance Rate (TAR) @ False Acceptance Rate (FAR) =\n10−4, following the protocol for IJB-B and IJB-C from Arc-\nFace (see [8], [9] for more details) which incorporates both\ndetector score and feature normalization.\nThe results reported in Table 4 indicate no improvement on\nLFW with either of the face detectors and the LOTR models.\nHowever, with PyramidKey as the face detector, we see an\nimprovement of 1.85 and 2.47 points on CFP-FP and CALFW\nwith the LOTR-M model while improving by 10.83 points on\nthe CPLFW dataset with the LOTR-R+ model, respectively.\n2https://github.com/deepinsight/insightface\n16538 VOLUME 10, 2022\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nTABLE 4. The performance (TAR @ FAR =10−4) of face recognition on several benchmarks.\nOn the other hand, with RetinaFace as the face detector,\nwe see a gain of 1.52 points on CFP-FP with LOTR-M+,\nwhile improving by 5.7 points with LOTR-R+ on CPLFW.\nSimilarly, with both the face detectors, we found an improve-\nment of 0.17 points with LOTR-R+ and 0.33 points with\nthe LOTR-M+model on the IJB-B and IJB-C datasets. The\nimprovement in the TAR on CPLFW is note-worthy as this\ndataset include images with large pose variations. Thereby,\nthe results suggest that the LOTR models are more robust to\npose variations.\nG. ABLATION STUDIES\nThis section further studies the proposed methods in various\npoints, including the computational complexity and model\nsize, the effect of the Transformer network, the number\nof Transformer encoder/decoder layers, and the proposed\nsmooth-Wing loss. All experiments in this section were con-\nducted on the JD-landmark dataset.\n1) COMPARISON WITH A HEATMAP REGRESSION\nAPPROACH\nTo study the efﬁciency of the proposed LOTR models,\nwe measured and compared the inference time with a\nheatmap-based approach. We selected the models with the\nsame backbones from Earp et al. [1] to serve as the baseline\nsince we followed the same pre-processing procedure, visual\nbackbone, and the bag of tricks. We reported the result in\nTable 5 where we ran the models on a high computational\nCPU (Intel Xeon CPU E5-2698 v4) and NVIDIA Tesla V100\nSXM2 GPU with 32 GB of RAM.\nIn the pre-processing step, all the models used the same\nface detector by Deng et al. [67] as mentioned in Section IV-\nA with a processing time of 43.36 ±4.82 ms. Table 5 demon-\nstrates the inference time reduction from the baseline on the\nGPU by ∼4–6×, while maintaining a comparable model size\nand the number of parameters. Moreover, when considering\nthe model with MobileNetV2 1.0 as a backbone, the computa-\ntional time of LOTR-M and LOTR-M +, with or without the\nﬂipping technique, is lower than the MobileNetV2 1.0 model\nin [1] without the ﬂipping technique. This phenomenon is\nconsistent with the larger visual backbone as well. Table 5\ndemonstrates that the LOTR-R+ model with ﬂipping is\n∼4.4×faster than the MobileNetV2 1.0 model with ﬂipping\nin [1] and ∼2.1×faster than that model without the ﬂipping\ntechnique.\nThis experimentation demonstrates the complexity of the\nheatmap regression approach, which relies on a compli-\ncated post-processing procedure to generate spatial proba-\nbility maps. Furthermore, some complex operations in its\npost-processing stage could not efﬁciently utilize GPU accel-\neration, resulting in high computational time. In contrast, the\nproposed LOTR models use a Transformer to directly regress\nthe coordinates, thereby, enabling the models to reduce the\ncomputational time as it avoids complicated post-processing\nand is capable of utilizing GPU acceleration as the models\nconsist of only simple operations.\nInspired by Xiong et al. [34], we conducted another exper-\niment incorporating vectorization and the band pooling mod-\nule with the baseline approach [1]. We analyzed the effect on\nmodel performance and post-processing time reduction with\nvectorization. While the post-processing time signiﬁcantly\ndrops when converting heatmaps to vectorized labels and\npredictions, there is also a drop in performance. The results\nindicate that the proposed LOTR models still outperform the\nbaseline with vectorization in terms of prediction accuracy\nand inference time.\n2) COMPARISON WITH A FEED-FORWARD NETWORK\nARCHITECTURE\nTo investigate the signiﬁcance of a Transformer’s ability\nin processing spatial features from the visual backbone,\nwe compared its performance with a baseline CNN model\nwith a Feed-Forward Network (FFN) replacing the Trans-\nformer module. For a fair comparison, we applied a 1 ×1 con-\nvolution layer after retrieving the feature map from the visual\nbackbone to resemble the channel dimension of the LOTR\nmodels. We then used the FFN as a landmark prediction head\nto output the coordinates of facial landmarks. For compar-\nison purposes, we experimented with MobileNetV2 1.0 and\nResNet50 as the visual backbones and adopted the same loss\nfunction and training tricks as described in Section IV-C.\nAccording to the results presented in Table 5, the perfor-\nmance of the CNNs with FFN head is worse than LOTRs for\nboth ﬂip and no ﬂip setups. This drop in performance might\nbe due to the incapability of the FFN to capture complex\nrelationships between spatial features.\nIn contrast, the Transformer module incorporates\nself-attention and cross-attention mechanisms to model this\nrelationship. Moreover, the use of landmark queries on the\ndecoder side and the positional encoding might be the reason\nVOLUME 10, 2022 16539\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nTABLE 5. Comparison of prediction accuracy, inference time, and model size.\nTABLE 6. Comparison of prediction accuracy (AUC) with different loss\nfunctions.\nthat the LOTRs can encode abstract information of each\nlandmark, resulting in an improved understanding of the\nmodels to perform direct regression. This shows that the\nTransformer module is more effective than the FFN.\n3) COMPARISON OF LOSS FUNCTIONS\nIn this section, we investigated the effect of the proposed\nsmooth-Wing loss, as described in Section III-B, by com-\nparing it with standard loss functions (e.g., L1, L2, and\nsmooth-L1) and Wing loss (Section II-E). We conducted\nthe experiment with the same training process described\nin Section IV-C. For the Wing loss, we set the threshold\nw =10 and ϵ=2.\nTable 6 shows the results from different loss functions on\nthe JD-landmark test set [52]. The results show that LOTRs\nwith L1 achieve comparable performance with smooth-L1,\nwhile with L2, the proposed models’ performance is worse\nthan other loss functions. This result also coincides with the\nresults from Feng et al. [39]. Unlike L1 loss, which maintains\na constant gradient value across the error range, L2 produces\na smaller gradient near zero, which causes the models to\nignore small error values. Thus, L2 loss is sensitive to outliers\nmaking it less responsive to relatively smaller errors. Conse-\nquently, the models trained with L2 loss may end up omitting\nsmall errors, which may yield inaccurate predictions.\nMoreover, loss functions for landmark localization—Wing\nand smooth-Wing—consistently outperform standard loss\nfunctions. The focus of these loss functions is on a small error\nrange, which aids in precise landmark coordinate predictions.\nComparing the Wing loss and the proposed smooth-Wing\nloss, LOTRs trained with the smooth-Wing loss outperform\nthose trained with the Wing loss. This result shows the impact\nof training stability from the smooth-Wing loss. Fig. 2 shows\nan uneven gradient of Wing loss at the threshold w, which\nTABLE 7. Comparison of prediction accuracy (AUC) based on varying\nnumber of transformer layers.\nmay not be suitable for parameter adjustment. The proposed\nsmooth-Wing loss smoothens the gradient at the threshold w,\nmaking training more stable and, thus, essential for parameter\nadjustment. Table 3 and 5 show that with the same visual\nbackbone, our LOTRs with smooth-Wing loss outperform the\nheatmap-based approach in [1]. This establishes that train-\ning with the smooth-Wing loss helps the proposed LOTRs\nachieve state-of-the-art performance on the JD-landmark\ntest set.\n4) THE NUMBER OF TRANSFORMER ENCODER/DECODER\nLAYERS\nThis section studies the impact of layers (L ) in both the Trans-\nformer encoder and decoder. We experimented with the dif-\nferent values of L while keeping the other hyper-parameters\nthe same. Table 7 shows the results from the different num-\nbers of encoder and decoder layers. The results show that the\nLOTR models with up to three layers can accurately localize\nlandmarks and yield the highest AUC when L equals 2. The\nmodels become harder to optimize as the number of layers\nincreases from three. The results of the deepest models, i.e.,\nL equals 6, is NaN, which might be a consequence of training\ninstability when the number of layers becomes very large.\nV. CONCLUSION\nWe show that our proposed LOTRs outperform other algo-\nrithms, including the two current heatmap-based methods on\nthe JD-landmark challenge leaderboard, and are comparable\nwith several state-of-the-art methods on the WFLW dataset.\nThe results suggest that the Transformer-based direct coor-\ndinate regression is a promising approach for robust facial\nlandmark localization.\n16540 VOLUME 10, 2022\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\nWe evaluate and illustrate that using the LOTRs for face\nalignment improves the state-of-the-art face recognition per-\nformance on the standard benchmark datasets such as CFP-\nFP, CPLFW, IJB-B, and IJB-C datasets.\nWe demonstrate how the proposed LOTR outperforms\nCNN models with FFN, trained under the same conditions.\nThe results suggest an effective use of a Transformer network\nto improve the feature representation from a visual backbone.\nIn contrast to other coordinate regression approaches that\nsuffer from spatial information loss, our LOTRs utilize the\ncrucial spatial information for landmark localization tasks.\nWe also show that the LOTRs are superior to the recently\nproposed heatmap-based method by Earp et al. [1] in terms\nof accuracy and inference time. Since the proposed LOTR\ndirectly predicts landmark coordinates, it avoids any compu-\ntationally intensive post-processing required by the heatmap-\nbased method, leading to a ∼4–6×gain in speed during infer-\nence. Although Xiong et al. [34] exploited their proposed\nvectorization method to reduce the post-processing time, its\ndownside is the reduction in prediction accuracy. The end-to-\nend training behavior of the LOTRs with the smooth-Wing\nloss also leads to better prediction performance when com-\npared to the heatmap-based methods.\nWhile Feng et al. [39] reported that their proposed Wing\nloss is superior over other common loss functions such as L2,\nL1, and smooth-L1, we show that our proposed smooth-Wing\nloss leads to better optimized models than the Wing loss\nbecause of its gradient continuity. The results indicate an\nimprovement in training stability and convergence rate using\nthe smooth-Wing loss.\nACKNOWLEDGMENT\nThe authors are grateful to their colleagues Christina\nKim, Jeff Hnybida, and Justin Cairns for their construc-\ntive input and feedback during the writing of this paper.\nNakarin Sritrakool has contributed to this work during his\ninternship at the Sertis Vision Laboratory.\nREFERENCES\n[1] S. W. F. Earp, A. Samacoits, S. Jain, P. Noinongyao, and\nS. Boonpunmongkol, ‘‘Sub-pixel face landmarks using heatmaps\nand a bag of tricks,’’ 2021, arXiv:2103.03059.\n[2] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar, ‘‘Attribute\nand simile classiﬁers for face veriﬁcation,’’ in Proc. IEEE 12th Int. Conf.\nComput. Vis., Sep. 2009, pp. 365–372.\n[3] L. Wolf, T. Hassner, and Y. Taigman, ‘‘Similarity scores based on back-\nground samples,’’ in Computer Vision—ACCV 2009, H. Zha, R. Taniguchi,\nand S. Maybank, Eds. Berlin, Germany: Springer, 2010.\n[4] O. M. Parkhi, A. Vedaldi, and A. Zisserman, ‘‘Deep face recognition,’’ in\nProc. 26th Brit. Mach. Vis. Conf. (BMVC), Sep. 2015, pp. 41.1–41.12.\n[5] F. Schroff, D. Kalenichenko, and J. Philbin, ‘‘FaceNet: A uniﬁed embed-\nding for face recognition and clustering,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2015, pp. 815–823.\n[6] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, ‘‘SphereFace: Deep\nhypersphere embedding for face recognition,’’ in Proc. IEEE Conf. Com-\nput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 6738–6746.\n[7] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and\nW. Liu, ‘‘CosFace: Large margin cosine loss for deep face recognition,’’\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018,\npp. 5265–5274.\n[8] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, ‘‘ArcFace: Additive angular\nmargin loss for deep face recognition,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2019, pp. 4685–4694.\n[9] X. An, X. Zhu, Y. Gao, Y. Xiao, Y. Zhao, Z. Feng, L. Wu, B. Qin, M. Zhang,\nD. Zhang, and Y. Fu, ‘‘Partial FC: Training 10 million identities on a\nsingle machine,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshops\n(ICCVW), Oct. 2021, pp. 1445–1449.\n[10] P. Barra, C. Bisogni, M. Nappi, and S. Ricciardi, ‘‘Fast quadtree-based pose\nestimation for security applications using face biometrics,’’ in Network\nand System Security, M. H. Au, S. M. Yiu, J. Li, X. Luo, C. Wang,\nA. Castiglione, and K. Kluczniak, Eds. Cham, Switzerland: Springer, 2018,\npp. 160–173.\n[11] V. Kazemi and J. Sullivan, ‘‘One millisecond face alignment with an\nensemble of regression trees,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., Jun. 2014, pp. 1867–1874.\n[12] C. Cao, Y. Weng, S. Lin, and K. Zhou, ‘‘3D shape regression for real-time\nfacial animation,’’ ACM Trans. Graph., vol. 32, no. 4, pp. 1–10, Jul. 2013.\n[13] J. Roth, Y. Tong, and X. Liu, ‘‘Unconstrained 3D face reconstruction,’’\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2015,\npp. 2606–2615.\n[14] P. Dou, S. K. Shah, and I. A. Kakadiaris, ‘‘End-to-end 3D face reconstruc-\ntion with deep neural networks,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jul. 2017, pp. 1503–1512.\n[15] Y. Feng, F. Wu, X. Shao, Y. Wang, and X. Zhou, ‘‘Joint 3D face recon-\nstruction and dense alignment with position map regression network,’’ in\nComputer Vision—ECCV 2018, V. Ferrari, M. Hebert, C. Sminchisescu,\nand Y. Weiss, Eds. Cham, Switzerland: Springer, 2018, pp. 557–574.\n[16] X. Yang, Y. Li, H. Qi, and S. Lyu, ‘‘Exposing GAN-synthesized faces using\nlandmark locations,’’ in Proc. ACM Workshop Inf. Hiding Multimedia\nSecur., Jul. 2019, pp. 113–118.\n[17] M. Day, ‘‘Exploiting facial landmarks for emotion recognition in the wild,’’\n2016, arXiv:1603.09129.\n[18] M. I. N. P. Munasinghe, ‘‘Facial expression recognition using facial land-\nmarks and random forest classiﬁer,’’ in Proc. IEEE/ACIS 17th Int. Conf.\nComput. Inf. Sci. (ICIS), Jun. 2018, pp. 423–427.\n[19] S. Hinduja and S. Canavan, ‘‘Facial action unit detection using 3D facial\nlandmarks,’’ 2020, arXiv:2005.08343.\n[20] H. Dibeklioglu, A. A. Salah, and L. Akarun, ‘‘3D facial landmarking under\nexpression, pose, and occlusion variations,’’ in Proc. IEEE 2nd Int. Conf.\nBiometrics, Theory, Appl. Syst., Sep. 2008, pp. 1–6.\n[21] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, ‘‘Active shape\nmodels-their training and application,’’ Comput. Vis. Image Understand. ,\nvol. 61, no. 1, pp. 38–59, Jan. 1995.\n[22] T. F. Cootes, G. J. Edwards, and C. J. Taylor, ‘‘Active appearance mod-\nels,’’ in Proc. Eur. Conf. Comput. Vis. (ECCV) , vol. 2, H. Burkhardt and\nB. Neumann, Eds., 1998, pp. 484–498.\n[23] L. Liang, R. Xiao, F. Wen, and J. Sun, ‘‘Face alignment via\ncomponent-based discriminative search,’’ in Computer Vision—ECCV\n2008, D. Forsyth, P. Torr, and A. Zisserman, Eds. Berlin, Germany:\nSpringer, Oct. 2008, pp. 72–85.\n[24] X. Zhu and D. Ramanan, ‘‘Face detection, pose estimation, and landmark\nlocalization in the wild,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recog-\nnit. (CVPR), Jun. 2012, pp. 2879–2886.\n[25] B. Amberg and T. Vetter, ‘‘Optimal landmark detection using shape models\nand branch and bound,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\nNov. 2011, pp. 455–462.\n[26] P. N. Belhumeur, D. W. Jacobs, D. J. Kriegman, and N. Kumar, ‘‘Localizing\nparts of faces using a consensus of exemplars,’’ IEEE Trans. Pattern Anal.\nMach. Intell., vol. 35, no. 12, pp. 2930–2940, Dec. 2013.\n[27] B. A. Efraty, M. Papadakis, A. Proﬁtt, S. Shah, and I. A. Kakadiaris,\n‘‘Facial component-landmark detection,’’ in Proc. IEEE Int. Conf. Autom.\nFace Gesture Recognit. (FG), Mar. 2011, pp. 278–285.\n[28] Y. Sun, X. Wang, and X. Tang, ‘‘Deep convolutional network cascade for\nfacial point detection,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2013, pp. 3476–3483.\n[29] E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin, ‘‘Extensive facial land-\nmark localization with coarse-to-ﬁne convolutional network cascade,’’\nin Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW) , Dec. 2013,\npp. 386–391.\n[30] J. Zhang, S. Shan, M. Kan, and X. Chen, ‘‘Coarse-to-ﬁne auto-encoder\nnetworks (CFAN) for real-time face alignment,’’ in Computer Vision—\nECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds. Cham,\nSwitzerland: Springer, 2014, pp. 1–16.\nVOLUME 10, 2022 16541\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\n[31] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, ‘‘Learning deep representation\nfor face alignment with auxiliary attributes,’’ IEEE Trans. Pattern Anal.\nMach. Intell., vol. 38, no. 5, pp. 918–930, May 2016.\n[32] R. Ranjan, V. M. Patel, and R. Chellappa, ‘‘HyperFace: A deep multi-\ntask learning framework for face detection, landmark localization, pose\nestimation, and gender recognition,’’ IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 41, no. 1, pp. 121–135, Jan. 2019.\n[33] M. Kowalski, J. Naruniec, and T. Trzcinski, ‘‘Deep alignment network: A\nconvolutional neural network for robust face alignment,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jul. 2017,\npp. 2034–2043.\n[34] Y. Xiong, Z. Zhou, Y. Dou, and Z. Su, ‘‘Gaussian vector: An efﬁcient\nsolution for facial landmark detection,’’ in Computer Vision—ACCV 2020,\nH. Ishikawa, C.-L. Liu, T. Pajdla, and J. Shi, Eds. Cham, Switzerland:\nSpringer, 2021, pp. 70–87.\n[35] S. Mahpod, R. Das, E. Maiorana, Y. Keller, and P. Campisi, ‘‘Facial\nlandmarks localization using cascaded neural networks,’’ 2018,\narXiv:1805.01760.\n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc.\nAdv. Neural Inf. Process. Syst. (NIPS), vol. 30, I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.,\n2017, pp. 1–11.\n[37] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in Com-\nputer Vision—ECCV 2020, A. Vedaldi, H. Bischof, T. Brox, and\nJ.-M. Frahm, Eds. Cham, Switzerland: Springer, 2020, pp. 213–229.\n[38] W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou, ‘‘Look at bound-\nary: A boundary-aware face alignment algorithm,’’ in Proc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2018, pp. 2129–2138.\n[39] Z.-H. Feng, J. Kittler, M. Awais, P. Huber, and X.-J. Wu, ‘‘Wing loss for\nrobust facial landmark localisation with convolutional neural networks,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2018,\npp. 2235–2245.\n[40] S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel, R. Chellappa, and\nD. W. Jacobs, ‘‘Frontal to proﬁle face veriﬁcation in the wild,’’ in Proc.\nIEEE Winter Conf. Appl. Comput. Vis. (WACV), Mar. 2016, pp. 1–9.\n[41] T. Zheng and W. Deng, ‘‘Cross-pose LFW: A database for studying cross-\npose face recognition in unconstrained environments,’’ Beijing Univ. Posts\nTelecommun., Beijing, China, Tech. Rep. 18-01, Feb. 2018. [Online].\nAvailable: http://www.whdeng.cn/cplfw/index.html?reload=true\n[42] C. Whitelam, E. Taborsky, A. Blanton, B. Maze, J. Adams, T. Miller,\nN. Kalka, A. K. Jain, J. A. Duncan, K. Allen, J. Cheney, and P. Grother,\n‘‘IARPA Janus benchmark-B face dataset,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. Workshops (CVPRW), Jul. 2017, pp. 592–600.\n[43] B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller, C. Otto, A. K. Jain,\nW. T. Niggel, J. Anderson, J. Cheney, and P. Grother, ‘‘IARPA janus\nbenchmark-C: Face dataset and protocol,’’ in Proc. Int. Conf. Biometrics\n(ICB), Feb. 2018, pp. 158–165.\n[44] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, ‘‘Facial landmark detection\nby deep multi-task learning,’’ in Computer Vision—ECCV 2014, D. Fleet,\nT. Pajdla, B. Schiele, and T. Tuytelaars, Eds. Cham, Switzerland: Springer,\n2014, pp. 94–108.\n[45] X. Dong, Y. Yan, W. Ouyang, and Y. Yang, ‘‘Style aggregated network for\nfacial landmark detection,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2018, pp. 379–388.\n[46] K. Sun, Y. Zhao, B. Jiang, T. Cheng, B. Xiao, D. Liu, Y. Mu, X. Wang,\nW. Liu, and J. Wang, ‘‘High-resolution representations for labeling pixels\nand regions,’’ 2019, arXiv:1904.04514.\n[47] A. Kumar, T. K. Marks, W. Mou, Y. Wang, M. Jones, A. Cherian,\nT. Koike-Akino, X. Liu, and C. Feng, ‘‘LUVLi face alignment: Estimat-\ning landmarks’ location, uncertainty, and visibility likelihood,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,\npp. 8233–8243.\n[48] A. Newell, K. Yang, and J. Deng, ‘‘Stacked hourglass networks for\nhuman pose estimation,’’ in Computer Vision—ECCV 2016, B. Leibe,\nJ. Matas, N. Sebe, and M. Welling, Eds. Cham, Switzerland: Springer,\n2016, pp. 483–499.\n[49] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, ‘‘Convolutional pose\nmachines,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 4724–4732.\n[50] B. Xiao, H. Wu, and Y. Wei, ‘‘Simple baselines for human pose estimation\nand tracking,’’ in Computer Vision—ECCV 2018, V. Ferrari, M. Hebert,\nC. Sminchisescu, and Y. Weiss, Eds. Cham, Switzerland: Springer, 2018,\npp. 472–487.\n[51] F. Zhang, X. Zhu, H. Dai, M. Ye, and C. Zhu, ‘‘Distribution-aware coordi-\nnate representation for human pose estimation,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 7091–7100.\n[52] Y. Liu et al., ‘‘Grand challenge of 106-point facial landmark localization,’’\nin Proc. IEEE Int. Conf. Multimedia Expo Workshops (ICMEW), Jul. 2019,\npp. 613–616.\n[53] X. Lan, Q. Hu, and J. Cheng, ‘‘HIH: Towards more accurate face alignment\nvia heatmap in heatmap,’’ 2021, arXiv:2104.03100.\n[54] X. Wang, L. Bo, and L. Fuxin, ‘‘Adaptive wing loss for robust face\nalignment via heatmap regression,’’ in Proc. IEEE/CVF Int. Conf. Comput.\nVis. (ICCV), Oct. 2019, pp. 6970–6980.\n[55] X. P. Burgos-Artizzu, P. Perona, and P. Dollar, ‘‘Robust face landmark\nestimation under occlusion,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\nDec. 2013, pp. 1513–1520.\n[56] R. Girshick, J. Donahue, T. Darrell, and J. Malik, ‘‘Rich feature hierarchies\nfor accurate object detection and semantic segmentation,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2014, pp. 580–587.\n[57] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays,\nP. Perona, D. Ramanan, C. L. Zitnick, and P. Dollár, ‘‘Microsoft COCO:\nCommon objects in context,’’ in Computer Vision—ECCV 2014, D. Fleet,\nT. Pajdla, B. Schiele, and T. Tuytelaars, Eds. Cham, Switzerland: Springer,\n2014, pp. 740–755.\n[58] R. Girshick, ‘‘Fast R-CNN,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\nDec. 2015, pp. 1440–1448.\n[59] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n‘‘MobileNetV2: Inverted residuals and linear bottlenecks,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2018,\npp. 4510–4520.\n[60] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[61] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,\nM. Tan, X. Wang, W. Liu, and B. Xiao, ‘‘Deep high-resolution represen-\ntation learning for visual recognition,’’ IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 43, no. 10, pp. 3349–3364, Oct. 2021.\n[62] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic, ‘‘300 faces\nin-the-wild challenge: The ﬁrst facial landmark localization challenge,’’\nin Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW) , Dec. 2013,\npp. 397–403.\n[63] C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic,\n‘‘300 faces in-the-wild challenge: Database and results,’’ Image Vis. Com-\nput., vol. 47, pp. 3–18, Mar. 2016.\n[64] V. Le, J. Brandt, Z. Lin, L. Bourdev, and T. S. Huang, ‘‘Interactive facial\nfeature localization,’’ in Proc. Eur. Conf. Comput. Vis., A. Fitzgibbon,\nS. Lazebnik, P. Perona, Y. Sato, C. Schmid, Eds. Berlin, Germany:\nSpringer, 2012, pp. 679–692.\n[65] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic, ‘‘A semi-\nautomatic methodology for facial landmark annotation,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , Jun. 2013,\npp. 896–903.\n[66] S. Yang, P. Luo, C. C. Loy, and X. Tang, ‘‘WIDER FACE: A face detection\nbenchmark,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 5525–5533.\n[67] J. Deng, J. Guo, Y. Zhou, J. Yu, I. Kotsia, and S. Zafeiriou, ‘‘RetinaFace:\nSingle-stage dense face localisation in the wild,’’ 2019, arXiv:1905.00641.\n[68] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘ImageNet:\nA large-scale hierarchical image database,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2009, pp. 248–255.\n[69] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, ‘‘Dropout: A simple way to prevent neural networks\nfrom overﬁtting,’’ J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958,\n2014.\n[70] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Delving deep into rectiﬁers:\nSurpassing human-level performance on ImageNet classiﬁcation,’’ in Proc.\nIEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 1026–1034.\n[71] Y. You, J. Li, S. J. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,\nJ. Demmel, K. Keutzer, and C. Hsieh, ‘‘Large batch optimization for\ndeep learning: Training BERT in 76 minutes,’’ in Proc. Int. Conf. Learn.\nRepresent. (ICLR), 2020, pp. 1–37.\n[72] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang,\nand Z. Zhang, ‘‘MXNet: A ﬂexible and efﬁcient machine learning library\nfor heterogeneous distributed systems,’’ 2015, arXiv:1512.01274.\n[73] J. Guo, H. He, T. He, L. Lausen, M. Li, H. Lin, X. Shi, C. Wang, J. Xie,\nS. Zha, A. Zhang, H. Zhang, Z. Zhang, Z. Zhang, S. Zheng, and Y. Zhu,\n‘‘GluonCV and GluonNLP: Deep learning in computer vision and natural\nlanguage processing,’’ J. Mach. Learn. Res., vol. 21, no. 23, pp. 1–7, 2020.\n16542 VOLUME 10, 2022\nU. Watchareeruetaiet al.: LOTR: Face Landmark Localization Using Localization Transformer\n[74] S. W. F. Earp, P. Noinongyao, J. A. Cairns, and A. Ganguly, ‘‘Face detection\nwith feature pyramids and landmarks,’’ 2019, arXiv:1912.00596.\n[75] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, ‘‘Labeled faces\nin the wild: A database for studying face recognition in unconstrained\nenvironments,’’ in Proc. Workshop Faces Real-Life Images, Detection\nAlignment Recognit., Oct. 2008, pp. 1–15.\n[76] T. Zheng, W. Deng, and J. Hu, ‘‘Cross-age LFW: A database for study-\ning cross-age face recognition in unconstrained environments,’’ 2017,\narXiv:1708.08197.\nUKRIT WATCHAREERUETAI received the\nB.Eng. degree in electrical engineering from\nKasetsart University, Thailand, in 2002, and the\nMaster of Information Science and Doctor of\nEngineering degrees from the Graduate School of\nInformation Science, Nagoya University, Japan, in\n2007 and 2010, respectively. From 2002 to 2004,\nhe worked as a Research Assistant at the Kasetsart\nSignal and Image Processing Laboratory. After his\ngraduation, he joined the King Mongkut’s Institute\nof Technology Ladkrabang (KMITL) as a Lecturer, and was then promoted to\nbe an Assistant Professor. Currently, he is working as a Senior AI Researcher\nat Sertis Company Ltd. His research interests include computer vision,\nmachine learning, deep learning, and evolutionary computation.\nBENJAPHAN SOMMANA received the B.S.\ndegree in mathematics from Mahidol University,\nThailand, in 2016, and the M.Eng. degree in infor-\nmation and communication technology for embed-\nded systems from the Sirindhorn International\nInstitute of Technology, Thammasat University,\nThailand, in 2019. After her graduation, she joined\nSertis Company Ltd. as an AI Researcher in the\nﬁeld of computer vision. Her research interests\ninclude machine learning and computer vision.\nSANJANA JAIN received the B.Eng. degree\nin computer and networks engineering from\nAssumption University, Thailand, in 2014, and\nthe master’s degree in computer science from the\nAsian Institute of Technology, Thailand, in 2017.\nIn the year 2015, she did an internship at\nEibiz Company Ltd. before pursuing her master’s\ndegree. After her graduation, she joined the AIT AI\nCenter as a Researcher. She then joined Oxygen\nAI Company Ltd., in 2018, as a Machine Learn-\ning Engineer. Currently, she is working at Sertis Company Ltd. as an AI\nResearcher. Her research interests include computer vision, deep learning,\ngenerative models, and computational neuroscience.\nPAVIT NOINONGYAO received the B.Eng.\ndegree in software engineering and the Master\nof Engineering degree in computing in engineer-\ning systems from the International College, King\nMongkut’s Institute of Technology Ladkrabang,\nThailand, in 2017 and 2019, respectively. Cur-\nrently, he is working as a Quantitative Researcher\nat QIS Capital. His research interests include com-\nputer vision, machine learning, and deep learning.\nANKUSH GANGULY received the B.Tech. degree\nin mechanical engineering from the National Insti-\ntute of Technology, Silchar, India, in 2014, and the\nM.Sc. degree in data science from The University\nof Edinburgh, in 2017, where his main area of\nfocus was Bayesian machine learning. After grad-\nuation, he joined Sertis Company Ltd. as an AI\nResearch Engineer and is currently working as a\nSenior AI Researcher focusing on deep learning\nand computer vision. His research interests include\ncomputer vision, Bayesian machine learning, deep learning, and probabilistic\nmodeling.\nAUBIN SAMACOITS received the Master of Sci-\nence degree in bio-statistics from the Paris Institute\nof Statistics and the Ph.D. degree from Paris 6 Uni-\nversity, where he developed image analysis meth-\nods for high content microscopy data. After his\ngraduation, he worked as a Researcher at Bristol\nUniversity where he developed AI methods for\ndetection and tracking of in vitro human stem cells.\nAfter working for two years as an AI Researcher\nat Sertis Company Ltd., he is the current Manager\nof the AI Research Team.\nSAMUEL W. F. EARP received the B.Sc. degree\nin mathematics and the M.Sc. and Ph.D. degrees\nin astrophysics from the University of Central\nLancashire. He joined Sertis as an AI Researcher,\nin 2018, and subsequently became part of the\nTechnical Leadership Team, focusing on face\nrecognition, detection, and localization. Currently,\nhe works as a Senior Quantitative Researcher at\nQIS Capital. He has six peer-reviewed journal\narticles in astrophysics and several white papers\navailable on ArXiv in the ﬁeld of AI and machine learning.\nNAKARIN SRITRAKOOL is currently pursu-\ning the bachelor’s degree in computer science\nwith Chulalongkorn University, Thailand. In 2021,\nhe did an internship at Sertis Company Ltd. His\nresearch interests include recommender systems\nand machine learning.\nVOLUME 10, 2022 16543"
}