{
  "title": "Detection and Classification of Lung Cancer Cells Using Swin Transformer",
  "url": "https://openalex.org/W4286355566",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104567405",
      "name": "Yuru Chen",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2052900427",
      "name": "Jing Feng",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2104828399",
      "name": "Juan Liu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2142048524",
      "name": "Baochuan Pang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2571169978",
      "name": "Dehua Cao",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A674962318",
      "name": "Cheng Li",
      "affiliations": [
        "Wuhan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4206841660",
    "https://openalex.org/W2997947674",
    "https://openalex.org/W2519356783",
    "https://openalex.org/W2890377113",
    "https://openalex.org/W2802494476",
    "https://openalex.org/W3157202711",
    "https://openalex.org/W3035156558",
    "https://openalex.org/W2967444033",
    "https://openalex.org/W2956228567",
    "https://openalex.org/W2953914369",
    "https://openalex.org/W3085719271",
    "https://openalex.org/W2792305943",
    "https://openalex.org/W2948265816",
    "https://openalex.org/W3103012513",
    "https://openalex.org/W3030515889",
    "https://openalex.org/W3118711474",
    "https://openalex.org/W3043028810",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2742679468",
    "https://openalex.org/W3014276306",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3200638744",
    "https://openalex.org/W3138115293",
    "https://openalex.org/W3022215132",
    "https://openalex.org/W3202236840"
  ],
  "abstract": "Lung cancer is one of the greatest threats to human health. It is a very effective way to detect lung cancer by pathological pictures of lung cancer cells. Therefore, improving the accuracy and stability of diagnosis is very important. In this study, we develop an automatic detection scheme for lung cancer cells based on convolutional neural networks and Swin Transformer. Microscopic images of patients' lung cells are first segmented using a Mask R-CNN-based network, resulting in a separate image for each cell. Part of the background information is preserved by Gaussian blurring of surrounding cells, while the target cells are highlighted. The classification model based on Swin Transformer not only reduces the computation but also achieves better results than the classical CNN model, ResNet50. The final results show that the accuracy of the method proposed in this paper reaches 96.16%. Therefore, this method is helpful for the detection and classification of lung cancer cells.",
  "full_text": "Journal of Cancer Therapy, 2022, 13, 464-475 \nhttps://www.scirp.org/journal/jct \nISSN Online: 2151-1942 \nISSN Print: 2151-1934 \n \nDOI: 10.4236/jct.2022.137041  Jul. 21, 2022 464 Journal of Cancer Therapy \n \n \n \n \nDetection and Classification of Lung Cancer \nCells Using Swin Transformer \nYuru Chen1,2, Jing Feng1,2*, Juan Liu1,2, Baochuan Pang2, Dehua Cao2, Cheng Li2 \n1Institute of Artificial Intelligence, School of Computer Science, Wuhan University, Wuhan, China \n2Landing Artificial Intelligence Center for Pathological Diagnosis, Wuhan University, Wuhan, China \n \n \n \nAbstract \nLung cancer is one of the greatest threats to human health. It is a very effe c-\ntive way to detect lung cancer by pathological pictures of lung cancer cells. \nTherefore, improving  the accuracy and stability of diagnosis is very impo r-\ntant. In this study, we develop an automatic detection scheme for lung cancer \ncells based on convolutional neural networks and Swin Transformer. Micr o-\nscopic images of patients’ lung cells are first segmented using a Mask R-CNN- \nbased network, resulting in a separate image for each cell. Part of the bac k-\nground information is preserved by Gaussian blurring of surrounding cells, \nwhile the target cells are highlighted. The classification model based on Swin \nTransformer not only reduces the computation but also achieves better r e-\nsults than the classical CNN model, ResNet50. The final results show that the \naccuracy of the method proposed in this paper reaches 96.16%. Therefore, \nthis method is helpful for the de tection and classification of lung cancer \ncells. \n \nKeywords \nLung Cancer, Classification, Swin Transformer \n \n1. Introduction \nLung cancer is one of the leading culprits that threaten human health. According \nto the 2022 World Health Organization  Cancer Report, 609,360 people will die \nfrom cancer in the United States in 2022, which is equivalent to nearly 1700 \ndeaths per day, including approximately 350 deaths per day from lung cancer, \nwhich is the leading cause of death from all cancers [1] . Lung cancer has an e x-\ntremely high mortality rate, and early diagnosis and treatment can dramatically \nimprove patients’ chances of survival [2] . Previous methods of lung cancer d i-\nHow to cite this paper: Chen, Y.R., Feng, \nJ., Liu, J., Pang, B.C., Cao, D.H. and Li, C. \n(2022) Detection and Classification of Lung \nCancer Cells Using Swin Transformer . \nJournal of Cancer Therapy, 13, 464-475.  \nhttps://doi.org/10.4236/jct.2022.137041 \n \nReceived:  June 20, 2022 \nAccepted: July 18, 2022 \nPublished: July 21, 2022 \n \nCopyright © 2022 by author(s) and  \nScientific Research Publishing Inc. \nThis work is licensed under the Creative \nCommons Attribution International  \nLicense (CC BY 4.0). \nhttp://creativecommons.org/licenses/by/4.0/   \n  \nOpen Access\nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 465 Journal of Cancer Therapy \n \nagnosis include computed tomography (CT), chest X- ray, and cytopathological \nidentification. Screening and detecting lung cancer cells are crucial in cancer \nprevention and control efforts [3] . Lung cancer diagnosis and ancillary tests rely \non cytology and small biopsy specimens obtained b y minimally invasive means \n[4]. Specimens of lung cancer cells are usually obtained from patients’ sputum \nexfoliated cells, alveolar lavage fluid, bronchial secreti ons, or pleural effusions. \nCompared with other screening methods, this method is convenient, quick, and \nbasically non-invasive, which is very suitable for the initial screening. \nTraditionally, lung cancer cytopathological images are used by pathologists or  \nphysicians to analyze cell morphology, number, differentiation, and other ch a-\nracteristics to reach a diagnosis. In recent years, as the number of patients with \nthe disease has increased, the large number of lung cancer patients has brought \nthousands of da ta to be analyzed, and processing these data requires a large \nnumber of professionals. With the shortage of pathologists in some areas, it is \nunreasonable to use manual review data to cause a waste of human resources. The \nlong-term repetitive and boring work also increases the possibility of misjudgment \nby professionals. Therefore, the research on cytopathological image-assisted diag-\nnosis systems for lung cancer is of great practical significance. Combining ad-\nvanced computer technology and the diagnostic ex perience of cytology experts \ncan, to a certain extent, solve the current medical troubles of cancer cell diagno-\nsis and reduce the workload and artificial influence of pathologists. This work \ncan largely improve the efficiency of early lung cancer screening  and reduce the \nmortality rate of lung cancer patients [5]. \nIn the last decade, with the development of computer hardware and deep \nlearning algorithms, artificial i ntelligence has been used to process the stream of \ndata generated throughout the clinical pathway [6] . Computer -aided medical \nanalysis techniques have also been rapidly developed with advances in image \nanalysis algorithms and the rise of big data algorithms [7] . Us ing machine \nlearning algorithms to identify and detect cancer has been shown to be feasible \n[8] [9] [10]. Today, many cytopathological recognition methods have been pr o-\nposed as the techniques for image classification are becoming mature. However, \nsince cells in different organs and tissues have different characteristics, the \nguidelines for physicians to determine whether a cell is diseased or not may \nchange accordingly. There is no universal cytopathological image recog nition \nmethod. Current methods for lung cancer cell detection suffer from low predi c-\ntion accuracy, high resource consumption, and poor real -time performance. In \nthis paper, we propose a transformer -based lung cancer cell detection network, \nwhich solves the above problems to some extent. \n2. Related Work \nThe identification and detection of lung cancer cells consist of two main steps: \ncell nucleus segmentation and cell image classification. Segmentation of lung \ncancer cells involves segmenting one or more lung cells in an image to facilitate \nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 466 Journal of Cancer Therapy \n \npathologists to clearly observe their morphology, color, and other features. The \ncell nucleus segmentation is to prepare for the classification afterward. \nThere are many traditional image segmentation methods, which are widely \nused in cell nucleus segmentation. Threshold  segmentation [11] [12] is the sim-\nplest method to distinguish foreground objects from the background. The basic \nidea of clustering segmentation [13] [14] is to calculate the similarity between \neach pixel point and group the pixels with high similarity into one class, so as to \nsegment the image. There are some other traditional methods for segmenting \ncell nuclei, such as the watershed algorithm [15]  [16] and the active contour \nmethod [17]. All these traditional methods have obvious advantages and disad-\nvantages, and  individual methods are only applicable in some specific scene \nconditions. And they often have limitations for the complex environments that \noccur in reality. Therefore, a combination of multiple methods is often used in \npractice, which also brings new pro blems such as great computational effort and \ncomplex computational principles. \nThe success of deep learning has brought new life to medical image segment a-\ntion. In 2019, Yiming Liu et al. [18] used a combination of coarse and fine se g-\nmentation methods, and first trained Mask -RCNN to obtain coarse segment a-\ntion results. The local fully connected conditional random field is used in fine \nsegmentation, and finally the two  are fused. In 2020, Cai et al. [19] proposed a \nDense U-Net structure based on the U -Net model, which uses a dense cascade \nform to segment the body skin cell images. In 2021, Liu Z [20] proposed an im-\nproved backpropagation (BP) neural network model for color fundus image se g-\nmentation. It can be seen that deep learning techniques are widely used in medical \nimages and can be used to solve the segmentation problems of the skin, MRI, r e-\ntinal images, and cell images to achieve automatic segmentation of targets. \nThe traditional method of lung cancer cell classification requires manu al ex-\ntraction of cell features. The advent of deep learning has simplified this step [21] . \nThe global linking and weight sharing features of convolutional neural networks \nmake them well suited for processing images, which has led to the derivation of \nmany classical CNN models. In 2014, Simonyan and Zisserman proposed the \nVGG model [22]  with a deeper network structure. Compared to other neural \nnetworks, it uses a smaller size convolutional kernel, which increases the nonl i-\nnear representation of the networ k while reducing the parameters. In 2015, \nResNet was introduced to solve the gradient vanishing problem, which is co m-\nmon in neural networks [23] . It introduced resi dual blocks, and the network \nperformed Identity Mapping through shortcuts with good results. In 201 6, \nHuang G, et al. [24] effectively alleviated the gradient disap pearance problem by \nreusing the feature map in the network while enhancing the transfer of features \nin the network. In 2017, Teramoto et al. [25] developed an auto matic classifica-\ntion scheme for lung cancer based on microscopic images using deep convol u-\ntional neural networks (DCNN). And their classification accuracy was evaluated \nusing triple cross -validation. In the obtained results, about 71% of the images \nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 467 Journal of Cancer Therapy \n \nwere correctly classified. In 2020, Daniel Gonzalez et al. [26] used three different \nconvolutional neural networks for small cell lung carcinoma (SCLC), large cell \nneuroendocrine carcinoma (LCNEC), and mixed/unclassifiable three categories \nfor classification and diagnosis, which eventually achieved good results on a l i-\nmited dataset. \nIn 2017, the transformer framework proposed by Google [27]  attracted a lot of \nattention. It not only became a mainstream model in the field of natural lan-\nguage processing, but also started to expand to the field of computer vision. In \n2020, Google proposed Vision Transformer (ViT) [28] . The direct use of tran s-\nformers for image classification in the article broke the reliance on CNN —the \nmethod used in most image processing work —in the CV field. In 2021, Liu Z \net \nal. proposed Swin Transformer [29], which surpasses backbone networks such as \nEfficientNet in terms of performance. It introduced a sliding window mechan-\nism and a hierarchical structure, making the Swin Transformer the new Bac k-\nbone of machine vision. It reached the SOTA level in a variety of machine vision \ntasks such as image classification, target detection, and semantic segmentation. \nViT has also been used in medical image processing. For example, the staff of \n[30] [31] used transformers to distinguish COVID -19 from other types of \npneumonia by computed tomography (CT) or X -ray images, meeting the urgent \nneed for fast and effective treatment of COVID-19 patients. \nAs can be seen from the references, the current lung cancer cytopathology \nimage detection technology is not mature enough, and the accuracy of detection \nis low. CNN can only extract local features through a convolution kernel, while \nthe ViT model can learn the features of the whole image through an attention \nmechanism, which can better analyze the image. Therefore, the work in this p a-\nper is very meaningful in the field of early diagnosis of lung cancer. \n3. Materials and Methods \n3.1. Image Data Set \nA total of 347 images of lung washout cells were collected from 10 patients by \nexfoliation or interventional cytology under bronchoscopic guidance. The pixel \nsize of the images was all 512  × 512. Each image contained one or more \nHE-stained lung cells. The nucl ei of the lung cells were dark blue in color. After \nlabeling and counting by professional pathologists, there were finally 2473 lung \ncells, which contained 143 cancerous cells, 724 normal cells and 1606 noisy \nblocks containing impurities and incomplete cells. \n3.2. Cell Segmentation and Data Enhancement \nIn this study, NucleAIzer [32] , a deep learning framework for cell nucleus se g-\nmentation, was used to segment each lu ng cancer cell image, and 2473 images of \nindividual cell nuclei were obtained. The paper provides a deep learning frame-\nwork for cell nucleus segmentation called nucleAIzer, in which the paper first \nuses mask-RCNN for initial training. Then the training images are clustered into \nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 468 Journal of Cancer Therapy \n \n134 classes based on image styles. Next, new image/mask pairs are generated for \neach style of images in the training set using cycleGAN. The mask -RCNN model \nis updated with these enhanced data. Finally, u -Net is used to calibrate the edges. \nThe framework is able to segment the images with different styles and types of \ncell nuclei more accurately. The final segmentation of the test set data in the \n2018 DSB competition is higher than the first place. NucleAIzer also shows very \ngood segmentation results on our dataset.  \nLike the training of CNNs, the training of transformers requires a sufficient \namount of data. To maximize the use of each data, we treat each individual se g-\nmented cell as separate data in the experiment. In this case, the amount of data \nincreased from 347 to 2473 sheets. Considering the surrounding environment, \nwhen a doctor or pathologist determines whether a cell is cancerous or not, the \ninformation is added to the reference. Two conditions were combined —to retain \nthe environmental information of the target cells and to highlight the target to \nbe analyzed to distinguish other cells in the same picture. We adopted a co m-\npromise approach: Gaussian blurring of the region outside the target. The results \nof several experiment s show that the best results are achieved when the size of \nthe Gaussian kernel is set to (65, 65) , t he cells before and after splitting and \nGaussian blurring are shown in \nFigure 1 . \nIt is noted that the number of the three categories in the data is unevenly di s-\ntributed. Also, in order to increase the training data and avoid overfitting the \nmodel, we increased the number of cancerous and normal cells in the dataset. \nThe different image orientations of the microscope during data acquisition can \nlead to differences in the position and angle of individual cells in the plane. \nTherefore, the experiments are performed by rotating, inverting and adding \nnoise operations for data enhancement while enhancing the robustness of the \nmodel. In the rotati on operation, the images are rotated clockwise by a random \nnumber of degrees in the range of 0 to 180. For the add noise operation, two \ntypes of noise, pretzel noise and Gaussian noise, are added to the images. The \nfinal enhanced data totaled 3106, which c ontained 500 cancerous cells, 1000 \nnormal cells and 1606 noisy data. \n3.3. Swin Transformer Structure \nThe transformer structure used for lung cancer cytopathology image classific a-\ntion is shown in Figure 2(a). Each stage in the figure consists of Patch Merging \nand Swin Transformer Block. The i nput H * W three-channel image first passes \nthrough a Patch Partition module. The image is divided into multiple patches of  \nsize 4 × 4, and the dimensionality becomes 4844\nHW∗∗ . Then it passes through  \na Linear Embedding layer, which can embed features into any dimension. At this \npoint, the dimension is recorded as C. After that, it passes through the core \nmodule Swin Transformer Block, and the number of tokens remains the same.  \nAll these are the work of stage 1. \nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 469 Journal of Cancer Therapy \n \n \nFigure 1. Sample images of lung cancer cells before segmentation , after segmentation and a f-\nter Gaussian blur. \n \nIn stage 2, a Patch Merging module is first employed. The purpose is to fuse \npatches in the 2  × 2 region to produce a hierarchical feature representation. The \nfeature dimension of each new patch is thus changed to 4C. The dimensionality is \nthen reduced to 2C in order to reduce the subsequent computation. The dime n-\nsionality is then kept constant after the Swin Transformer Block. The subsequent \nstage3-stage4 repeats the previous operations, continuously fusing the adjacent  \npatches. The size of the feature map output from the stage4 is 832 32\nHW C∗∗ . \nThe structure of one of the core modules, the Swin Transformer Block, is \nshown in Figure 2(b) . Compared with the traditional transformer, Swin Tran s-\nformer uses W-MSA (Window MSA) and SW -MSA (Shifted Window MSA) i n-\nstead of MSA (Multi -head self -attention module). Traditional transformers \ncompute attention based on the global picture, so the computational complexity \nis very high. The Swin Transformer reduces the computation by limiting the at-\ntention computation to each window. In order not to lose global information, \nShifted Window is added to better interact with other windows. This makes the \nhierarchical feature and linear time complexity possible. \n4. Experiment \n4.1. Experimental Environment \nThe experiments in this paper were con ducted on Ubuntu 18.04.5 LTS operating \nsystem. The pytorch = 1.10.1 deep learning framework is used. Python language \nversion is 3.6. The GPU used in the experiments is NVIDIA GeForce RTX \n2080Ti. In training, Adam is used as the optimizer, and the batch siz e is set to \n12. The initial learning rate is set to 0.0001. Every 30 epochs are trained. The ini-\ntial learning rate is set to 0.0001. Every 30 epochs are trained, the learning rate \nbecomes 10% of the original. The ratio of the training set to test set was 7 :3, and \nthe epoch is 100. \n\nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 470 Journal of Cancer Therapy \n \n \n(a)                                  ( b) \nFigure 2.  (a) Architecture of the Swin Transformer used for lung cancer cell classifica-\ntion; (b) Two successive Swin Transformer Blocks. \n4.2. Evaluation Method \nAccuracy, precision, recall and specificity were used in the experiments to ev a-\nluate the performance of the lung cancer cell classification model. The number \nof positive samples with accurate prediction was recorded as true positive (TP). \nThe number of negative samples with ac curate prediction was recorded as true \nnegative (TN). The number of negative samples predicted to be positive was \nrecorded as false positive (FP). The number of positive samples predicted to be \nnegative is a false negative (FN). Precision denotes the probability of the number \nof correctly predicted positive samples to the number of all samples predicted to \nbe positive. Recall denotes the ratio of the number of correctly predicted positive \nsamples to the number of actual positive samples. Accuracy is the rat io of the \n\nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 471 Journal of Cancer Therapy \n \nnumber of correctly predicted to the total number of samples. Specificity \nrepresents the probability of being correctly judged as negative among the actual \nnegative samples. The calculation formulas of the above four evaluation indic a-\ntors are as follows. \nTP TNAccuracy TP FP TN FN\n+= +++\n \nTPPrecision TP FP= +\n \nTPRecall TP FN= +\n \nSpecificit TN\nFTy PN= +\n \n4.3. Experimental Results and Analysis \nThe performance of the Swin Transformer model on the test set is shown in Ta-\nble 1. The average precision, recall and specificity of lung cancer cell detection \non the test set were calculated to be 95.20%, 92.60% and 98.17%, respectively. \nThe accuracy of model classification was 96.14%. It proved that the detection of \nlung cancer cells using Swin Transformer is feasible. The confusion matrix of the \ntest set is shown in Figure 3 . In the confusion matrix, we can see that all the i m-\nages in the noise category are classified correctly, and only a small number of \nerrors are generated in the abnormal and normal categories. \n \nTable 1.  Precision, recall and specificity of lung cancer cell classification. \n Precision Recall Specificity \nAbnormal 0.944 0.801 0.991 \nNoise 1.0 1.0 1.0 \nNormal 0.912 0.977 0.954 \n \n \nFigure 3. Confusion matrix of classification result. \n\nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 472 Journal of Cancer Therapy \n \nTable 2.  Accuracy, average precision, average recall and average specificity of different \nmodels on different datasets. \nModels (dataset) Acc Avg.P Avg.R Avg.S (%) \nResnet50 (lung cells) 94.21 92.14 92.31 97.18 \nResnet50 + FPN (lung cells) 93.56 91.70 92.40 96.63 \nSwin Transformer (lung cells) 96.14  95.20  92.60  98.17  \nResnet50 (Herlev) 66.30 70.33 68.39 94.24 \nResnet50 + FPN (Herlev) 46.74 54.64 49.30 90.87 \nSwin Transformer (Herlev) 70.29  75.06  73.57  94.79  \nResnet50 (SIPaKMeD) 98.02 98.00 98.02 99.52 \nResnet50+FPN (SIPaKMeD) 90.37 90.42 90.46 97.60 \nSwin Transformer (SIPaKMeD) 96.95 97.00 97.02 99.24 \n4.4. Extended Experiments \nIn order to observe the performance of the Swin Transformer model, the re s-\nNet50 and resNet50 + FPN models, which perform very well in the field of i m-\nage classification, are selected for comparison with the current model , where \nFPN stands for Feature Pyramid Network. This is a feature fusion technique. \nThe basic idea of FPN is to improve the effectiveness of the network by fusing \nthe features of higher and lower layers together, i.e., multi-scale feature fusion, \nso as to fully utilize the features of each stage of the network. \nThen experiments were conducted using three different models in  two pub-\nlicly available cervical cell datasets— Herlev and SIPaKMeD —as a way to de m-\nonstrate the generalization performance of the model. The SIPaKMeD dataset is \na five-category labeled cervical cell dataset with a total of 4049 cervical cells. The \noverall precision, recall, and specificity of the model were calculated by calcula t-\ning the mean of the different categories. The experimental setup and dataset d i-\nvision was the same as before. \nTable 2  shows the results of the different data sets  on the different models. \nThe results show that Swin Transformer performs slightly worse than ResNet50 \non the SIPaKMeD dataset, except on the other two datasets, where the results are \nsignificantly better than all other classification models. The accuracy of the lung \ncancer cell dataset, which is our main focus, reached 96.14, which is nearly two \npercentage points higher than the resNet50 model. This demonstrates the effec-\ntiveness of the Swin Transformer for lung cancer cell image classification and \nthat it can perform well on other cell image datasets as well. \n5. Conclusion \nIn this paper, a Swin Transformer -based lung cancer cell classification model is \nproposed. The experiments firstly segmented the lung cancer cell images to se p-\narate each cell, then defocused the background of the target cells using Gaussian \nblur, and finally put them into the Swin Transformer model for classification. \nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 473 Journal of Cancer Therapy \n \nThe experimental results showed that the accuracy of classification reached \n96.16%. Therefore, it can be proved that using Swing Transformer to detect lung \ncancer cells is effective. \nAcknowledgements \nThis work was partially supported by the Major Projects of Technological Inn o-\nvation in Hubei Province (2019AEA170), the Frontier Projects of Wuhan for \nApplication Foundation (2019010701011381), and the Translational Medicine \nand Interdisciplinary Research Joint Fund of Zhongnan Hospital of Wuhan \nUniversity (ZNJC201919). \nConflicts of Interest \nThe authors declare no conflicts of interest regarding the publ ication of this p a-\nper. \nReferences \n[1] Siegel, R.L., Miller, K.D., Fuchs, H.E., et al.  (2022) Cancer Statistics, 2022. CA: A \nCancer Journal for Clinicians, 72, 7-33. https://doi.org/10.3322/caac.21708  \n[2] Asuntha, A. and Srinivasan,  A. (2020) Deep Learning for Lung Cancer Detection \nand Classification. Multimedia Tools and Applications, 79, 7731-7762. \nhttps://doi.org/10.1007/s11042-019-08394-3  \n[3] da Silva, G.L.F., de Carvalho Filho, A.O., Silva, A.C., de Paiva, A.C. and Gattass, M. \n(2016) Taxonomic Indexes for Differentiating Malignancy of Lung Nodules on CT \nImages. Research on Biomedical Engineering, 32, 263-272. \nhttps://doi.org/10.1590/2446-4740.04615  \n[4] VanderLaan, P.A. (2018) Updates in Lung Cancer Cytopathology. Surgical Pathol-\nogy Clinics, 11, 515-522. https://doi.org/10.1016/j.path.2018.04.004  \n[5] Fernandes, K., Chicco, D., Cardoso, J.S., et al.  (2018) Supervised Deep Learning \nEmbeddings for the Prediction of Cervical Cancer Diagnosis. PeerJ Computer \nScience, 4, e154. https://doi.org/10.7717/peerj-cs.154  \n[6] Kann, B.H., Hosny, A. and Aerts, H.J.W.L. (2021) Artificial Intelligence for Clinical \nOncology. Cancer Cell, 39, 916-927. https://doi.org/10.1016/j.ccell.2021.04.002  \n[7] Han, F.J., Yu, L. and Jiang, Y. (2020)  Computer-Aided Diagnosis System of Lung \nCarcinoma Using Convolutional Neural Networks. Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition Workshops , Seattle, \n14-19 June 2020, 690-691. https://doi.org/10.1109/CVPRW50498.2020.00353  \n[8] Bera, K., Schalper, K.A., Rimm, D.L., Velcheti, V. and Madabhushi, A. (2019) Ar-\ntificial Intelligence in Digital Pathology —New Tools for Diagnosis and Precision \nOncology. Nat Rev Clin Oncol, 16, 703-715.  \nhttps://doi.org/10.1038/s41571-019-0252-y  \n[9] Campanella, G., Hanna, M.G., Geneslaw, L., Miraflor, A., Silva, V.W.K., Busam, \nK.J., Brogi, E., Reuter, V.E., Klimstra, D.S. and Fuchs, T.J. (2019) Clinical-Grade \nComputational Pathology Using Weakly Supervised Deep Learning on Whole Slide \nImages. Nature Medicine, 25, 1301-1309.  \nhttps://doi.org/10.1038/s41591-019-0508-1  \n[10] Fourcade, A. and Khonsari, R.H. (2019) Deep Learning in Medical Image Analysis: \nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 474 Journal of Cancer Therapy \n \nA Third Eye for Doctors. Journal of Stomatology , Oral and Maxillofacial Surgery , \n120, 279-288. https://doi.org/10.1016/j.jormas.2019.06.002  \n[11] Zhao, J., Liang, L.K., He, Y.J., et al. (2019) Cervical Nucleus Segmentation Method \nin Complex Background. Journal of Harbin University of Science and Technology , \n24, 25-31. \n[12] Sun, H.F., Yang, J.H., Fan, R.B., et al.  (2020) Stepwise Local Stitching Ultrasound \nImage Algorithms Based on Adaptive Iterative Threshold Harris Corner Features. \nMedicine, 99, e22189. https://doi.org/10.1097/MD.0000000000022189  \n[13] Feng, F., Liu, P.X., Li, L., et al.  (2018) Research on GSA Algorithm Improved by \nFCM Fusion in Medical Image Segmentation. Computer Science, 45, 252-254. \n[14] Bai, X.Z., Sun, C.X. and  Sun, C.M. (2019) Cell Segmentation Based on FOPSO \nCombined with Shape Information Improved Intuitionistic FCM. IEEE Journal of \nBiomedical and Health Informatics, 23, 449-459.  \nhttps://doi.org/10.1109/JBHI.2018.2803020  \n[15] Gamarra, M., Zurek, E., Escalante, H.J., et al. (2019) Split and Merge Watershed: A \nTwo-Step Method for Cell Segmentation in Fluorescence Microscopy Images. Bio-\nmedical Signal Processing and Control, 53, Article ID: 101575. \nhttps://doi.org/10.1016/j.bspc.2019.101575  \n[16] He, A.L., Cheng, X.B., Liao, L.C., et al. (2020) A Watershed Remote Sensing Image \nSegmentation Method Coupled with H -Minima and Mathematical Morphology. \nJournal of East China University of Science and Technology  (Natural Science Edi-\ntion), 43, 396-400. \n[17] Hsu, W.Y., Lu, C.C. and Hsu, Y.Y. (2020) Improving Segmentation Accuracy of CT \nKidney Cancer Images Using Adaptive Active Contour Model. Medicine, 99, e23083.  \nhttps://doi.org/10.1097/MD.0000000000023083  \n[18] Liu, Y.M. (2019) Nucleus Segmentation of Cervical Cancer Images Based on Deep \nLearning and Conditional Random Fields. North Central University, Minneapolis. \n[19] Cai, S., Tian, Y., Lu, I.H., et al. (2020) Dense-Unet: A Novel Multiphoton  in Vivo \nCellular Image Segmentation Model Based on a Convolutional Neural Network. \nQuantitative Imaging in Medicine and Surgery, 10, 1275-1285.  \nhttps://doi.org/10.21037/qims-19-1090  \n[20] Liu, Z. (2021) Construction and Verification of Color Fundus Image Retinal Vessels \nSegmentation Algorithm under BP Neural Network. The Journal of Supercomput-\ning, 77, 7171-7183. https://doi.org/10.1007/s11227-020-03551-0  \n[21] Martínez-Mása, J., Bueno-Crespob, A. and Martínez-España, R. (2020) Classifying \nPapanicolaou Cervical Smears through a Cell Merger Approach by Deep Learning \nTechnique. Expert Systems with Applications, 160, Article ID: 113707.  \nhttps://doi.org/10.1016/j.eswa.2020.113707  \n[22] Simonyan, K. and  Zisserman, A. (2014) Very Deep Convolutional Networks for \nLarge-Scale Image Recognition. arXiv:1409.1556.  \n[23] He, K., Zhang, X., Ren, S., et al. (2016) Deep Residual Learning for Image Recogn i-\ntion. Proceedings of the IEEE Conference on Computer Vision and Pattern Reco g-\nnition, Las Vegas, 27-30 June 2016, 770-778. https://doi.org/10.1109/CVPR.2016.90  \n[24] Huang, G., Liu, Z., Van Der Maaten , L., et al. (2017) Densely Connected Convol u-\ntional Networks . Proceedings of the IEEE Conference on Computer Vision and \nPattern Recognition, Honolulu, 21-26 July 2017, 4700-4708.   \nhttps://doi.org/10.1109/CVPR.2017.243  \n[25] Teramoto, A., Tsukamoto, T., Kiriyama, Y., et al. (2017) Automated Classification \nY. R. Chen et al. \n \n \nDOI: 10.4236/jct.2022.137041 475 Journal of Cancer Therapy \n \nof Lung Cancer Types from Cytological Images Using Deep Convolutional Neural \nNetworks. BioMed Research International, 2017, Article ID: 4067832. \nhttps://doi.org/10.1155/2017/4067832  \n[26] Gonzalez, D., Dietz, R.L. and Pantanowitz, L. (2020) Feasibility of a Deep Learning \nAlgorithm to Distinguish Large Cell Neuroendocrine from Small Cell Lung Carc i-\nnoma in Cytology Specimens. Cytopathology, 31, 426-431. \nhttps://doi.org/10.1111/cyt.12829  \n[27] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017) Attention Is All You Need. Ad-\nvances in Neural Information Processing Systems , Long Beach, 4-9 December 2017, \n5998-6008.   \n[28] Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al. (2020) An Image Is Worth 16x16 \nWords: Transformers for Image Recognition at Scale. arXiv:2010.11929. \n[29] Liu, Z., Lin, Y., Cao, Y., et al. (2021) Swin Transformer: Hierarchical Vision Trans-\nformer Using Shifted Windows. Proceedings of the IEEE/CVF International Conf e-\nrence on Computer Vision, Montreal, 10-17 October 2021, 10012-10022.  \nhttps://doi.org/10.1109/ICCV48922.2021.00986   \n[30] Costa, G.S.S., Paiva, A.C., Junior, G.B. and Ferreira, M.M. (2021) Covid-19 Aut o-\nmatic Diagnosis with CT Images Using the Novel Transformer Architecture . Anais \ndo XXI Simpósio Brasileiro de Computação Aplicada à Saúde , Virtual Event, 15-18 \nJune 2021, 293-301. \n[31] van Tulder, G., Tong, Y. and Marchiori, E. (2021) Multi-View Analysis of Unregi s-\ntered Medical Images Using Cross-View Transformers. International Conference on \nMedical Image Computing and Computer -Assisted Intervention, Virtual Event, 27 \nSeptember-1 October 2021, 104-113. https://doi.org/10.1007/978-3-030-87199-4_10  \n[32] Hollandi, R., Szkalisity, A., Toth, T., et al.  (2020) NucleAIzer: A Parameter -Free \nDeep Learning Framework for Nucleus Segmentation Using Image Style Transfer. \nCell Systems, 10, 453-458.E6. https://doi.org/10.1016/j.cels.2020.04.003 \n ",
  "topic": "Lung cancer",
  "concepts": [
    {
      "name": "Lung cancer",
      "score": 0.7904225587844849
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7471398711204529
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5880835652351379
    },
    {
      "name": "Computer science",
      "score": 0.5700674653053284
    },
    {
      "name": "Transformer",
      "score": 0.5316363573074341
    },
    {
      "name": "Lung",
      "score": 0.48196694254875183
    },
    {
      "name": "Computation",
      "score": 0.4733067750930786
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45918887853622437
    },
    {
      "name": "Pathology",
      "score": 0.30633145570755005
    },
    {
      "name": "Medicine",
      "score": 0.28800714015960693
    },
    {
      "name": "Algorithm",
      "score": 0.1390555202960968
    },
    {
      "name": "Internal medicine",
      "score": 0.08838587999343872
    },
    {
      "name": "Engineering",
      "score": 0.07792794704437256
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}