{
    "title": "Relevance-Promoting Language Model for Short-Text Conversation",
    "url": "https://openalex.org/W2997173942",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2100379612",
            "name": "Xin Li",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2651339891",
            "name": "Piji Li",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2102872490",
            "name": "Wei Bi",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2295085421",
            "name": "Xiaojiang Liu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2236715527",
            "name": "Wai Lam",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2100379612",
            "name": "Xin Li",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2651339891",
            "name": "Piji Li",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2102872490",
            "name": "Wei Bi",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2295085421",
            "name": "Xiaojiang Liu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2236715527",
            "name": "Wai Lam",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2886490473",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W2890271771",
        "https://openalex.org/W6693912633",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2890940245",
        "https://openalex.org/W6750305986",
        "https://openalex.org/W2901846148",
        "https://openalex.org/W2951038425",
        "https://openalex.org/W2798702047",
        "https://openalex.org/W1958706068",
        "https://openalex.org/W6714618025",
        "https://openalex.org/W6726804950",
        "https://openalex.org/W2890274659",
        "https://openalex.org/W6729247586",
        "https://openalex.org/W6741459021",
        "https://openalex.org/W2743945814",
        "https://openalex.org/W6723521144",
        "https://openalex.org/W2467963359",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W6748634344",
        "https://openalex.org/W6600426076",
        "https://openalex.org/W2962883855",
        "https://openalex.org/W2159640018",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W2809213523",
        "https://openalex.org/W6727329663",
        "https://openalex.org/W2756487349",
        "https://openalex.org/W2757121784",
        "https://openalex.org/W6729544866",
        "https://openalex.org/W2605246398",
        "https://openalex.org/W2604444020",
        "https://openalex.org/W2963951265",
        "https://openalex.org/W2963088785",
        "https://openalex.org/W2962717182",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963521540",
        "https://openalex.org/W2267186426",
        "https://openalex.org/W2963371754",
        "https://openalex.org/W2963963856",
        "https://openalex.org/W2963939249",
        "https://openalex.org/W2964587107",
        "https://openalex.org/W2551884415",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W2963598809",
        "https://openalex.org/W2521114121",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W4300427683",
        "https://openalex.org/W3045738072",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W10957333",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2963167310",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W2962834107",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963986868",
        "https://openalex.org/W2952988558"
    ],
    "abstract": "Despite the effectiveness of sequence-to-sequence framework on the task of Short-Text Conversation (STC), the issue of under-exploitation of training data (i.e., the supervision signals from query text is ignored) still remains unresolved. Also, the adopted maximization-based decoding strategies, inclined to generating the generic responses or responses with repetition, are unsuited to the STC task. In this paper, we propose to formulate the STC task as a language modeling problem and tailor-make a training strategy to adapt a language model for response generation. To enhance generation performance, we design a relevance-promoting transformer language model, which performs additional supervised source attention after the self-attention to increase the importance of informative query tokens in calculating the token-level representation. The model further refines the query representation with relevance clues inferred from its multiple references during training. In testing, we adopt a randomization-over-maximization strategy to reduce the generation of generic responses. Experimental results on a large Chinese STC dataset demonstrate the superiority of the proposed model on relevance metrics and diversity metrics.1",
    "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nRelevance-Promoting Language Model for Short-Text Conversation∗\nXin Li,1 Piji Li,2 Wei Bi,2 Xiaojiang Liu,2 Wai Lam1\n1Department of Systems Engineering and Engineering Management,\nThe Chinese University of Hong Kong\n2Tencent AI Lab, Shenzhen, China\n{lixin, wlam}@se.cuhk.edu.hk, {pijili, victoriabi, kieranliu}@tencent.com\nAbstract\nDespite the effectiveness of sequence-to-sequence framework\non the task of Short-Text Conversation (STC), the issue of\nunder-exploitation of training data (i.e., the supervision sig-\nnals from query text is ignored) still remains unresolved.\nAlso, the adopted maximization-based decoding strategies,\ninclined to generating the generic responses or responses with\nrepetition, are unsuited to the STC task. In this paper, we pro-\npose to formulate the STC task as a language modeling prob-\nlem and tailor-make a training strategy to adapt a language\nmodel for response generation. To enhance generation per-\nformance, we design a relevance-promoting transformer lan-\nguage model, which performs additional supervised source\nattention after the self-attention to increase the importance of\ninformative query tokens in calculating the token-level rep-\nresentation. The model further reﬁnes the query representa-\ntion with relevance clues inferred from its multiple references\nduring training. In testing, we adopt a randomization-over-\nmaximization strategy to reduce the generation of generic re-\nsponses. Experimental results on a large Chinese STC dataset\ndemonstrate the superiority of the proposed model on rele-\nvance metrics and diversity metrics.\n1\nIntroduction\nShort Text Conversation (STC) (Shang, Lu, and Li 2015),\nalso known as single-turn chit-chat conversation, is a pop-\nular research topic in the ﬁeld of natural language pro-\ncessing. It is usually formulated as a sequence translation\nproblem (Ritter, Cherry, and Dolan 2011; Shang, Lu, and\nLi 2015) and the sequence-to-sequence encoder-decoder\n(S\nEQ 2SEQ) framework (Cho et al. 2014; Sutskever, Vinyals,\nand Le 2014; Bahdanau, Cho, and Bengio 2015) is ap-\nplied for solving this problem. The decoder generates the\nresponses token-by-token, conditioned on the compressed\nquery representations from the encoder. Following this\nparadigm, many attempts have been conducted to reﬁne\n∗The work described in this paper is substantially supported by\na grant from the Research Grant Council of the Hong Kong Special\nAdministrative Region, China (Project Code: 14204418). It was\nmainly done when Xin Li was an intern at Tencent AI Lab.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1Code available at https://ai.tencent.com/ailab/nlp/dialogue/.\nthe quality of the generated responses (Li et al. 2016a;\nXing et al. 2017; Du et al. 2018; Wu et al. 2019).\nDespite the effectiveness of these efforts, some intrinsic\nissues of S EQ 2SEQ-based models still hinder further im-\nprovement of generation performance. Under the SEQ 2SEQ\nformulation, the auto-regressive decoder is only trained on\nthe gold-standard response text while the query text is ig-\nnored, leading to under-exploitation of the training data. Be-\nsides, the maximization-based decoding strategies adopted\nin existing models, such as beam search and greedy search,\nrestrict the search space to the most frequent phrases and\nthus they have the tendency to generate the generic re-\nsponses or repetitive responses with unnaturally high like-\nlihood, degrading the conversational experience.\nGPT-2 (Radford et al. 2019), a recently proposed\nTransformer-based language model, provides an alternative\nsolution for language generation. One advantage of GPT-2 is\nthat the transformer language model can not only capture the\ncontext of arbitrary length but also make full use of the tex-\ntual supervision signals because the generator is actually the\nlanguage model itself. Moreover, GPT-2 adopts top-k sam-\npling (Fan, Lewis, and Dauphin 2018) to diversify the gen-\nerated texts while preserving the relevance. Obviously, these\ncharacteristics are attractive and meaningful for solving the\nSTC task, whose aim is to generate informative and diverse\nhuman-like responses given the user queries.\nHowever, due to the essence of language modeling, di-\nrectly applying GPT-2 on the STC task, a conditional lan-\nguage generation task, may be insufﬁcient because the lan-\nguage model is unable to discriminate the source (query)\nsentence and the target (response) sentence. The original\nexperimental results of GPT-2 on the abstractive summa-\nrization task (Nallapati et al. 2016) also verify this claim.\nAnother potential issue of adapting language model for\nthe STC task comes from recency bias (Khandelwal et\nal. 2018) and explanation-away effects (Y u et al. 2017;\nHoltzman et al. 2019), where the language model has the\ntendency to rely overly on the immediate context and ex-\nplain away from the long-term context\n2, yielding ﬂuent but\ntopically irrelevant responses.\n2Long-term context in language model is roughly equivalent to\nthe source information in SEQ 2SEQ framework.\n8253\nFigure 1: Representations of the example input withn =7\nand m =4 .\nWith the motivation of inheriting the merits of transformer\nlanguage model while alleviating the potential issues under\nthe language model formulation, we carefully design a train-\ning strategy to adapt the auto-regressive transformer-based\nlanguage model\n3 for the conditional response generation.\nFirst of all, it is observed that the dialog conversation is ac-\ntually a process of text continuation, in other words, giving\nthe response right after the query. Based on this observation,\nwe can regard the STC task as a language modeling prob-\nlem on the concatenated sequence of query and response. To\ndiscriminate the generation of query tokens and that of re-\nsponse tokens, we inject a special token between query and\nresponse, acting as the trigger of response generation. With\nthis formulation, the language model based training objec-\ntive can make use of the textual data from query, alleviating\nthe under-exploitation issue mentioned above.\nSince the transformer-based language model tends to fo-\ncus on the short-term context and ignore the long-term con-\ntext, namely, the explanation away issue, we propose to\nempower the self-attention with encoder-decoder attention,\nwhich enforces the model to pay additional attention to\nthe query, especially the query tokens of user interest, and\nguides the model to rely on informative query tokens to\nmake good predictions. It is also observed that some re-\nsponse tokens not mentioned in the query are still closely\nrelated to the discussed topic in the conversation. In order to\nexploit such kind of relevance clues hidden behind the re-\nsponses, we propose a topic inference component to learn\na compact source (query) representation encoding the infor-\nmation relevant to the query and feed the query represen-\ntation into each generation step, encouraging the language\nmodel to consider the generation of the topic words poten-\ntially related to the query.\nAs with the decoding strategy, different from the existing\nSTC models, we propose to decode with randomization-\nover-maximization method, namely, the top- k sampling,\nfrom the transformer language model to generate the rele-\nvant response with high originality.\nIn summary, our contributions are as follows:\n• We tailor-make a training strategy to adapt the\ntransformer-based language model for the Short Text\nConversation (STC) task.\n• We propose two components, namely, Supervised\nSource Attention (SSA) component and Topic Inference\n3Without explicit speciﬁcation, the language model in our paper\nrefers to the “auto-regressive” language model, which is different\nfrom those “auto-encoding” language models (Devlin et al. 2019;\nDong et al. 2019).\n(TI) component to promote the relevance modeling in the\nlanguage model based response generator.\n• To the best of our knowledge, we are the ﬁrst to intro-\nduce top-k sampling, a randomization-over-maximization\nstrategy, for diverse response generation.4\nModel\nOverview\nIn our language model formulation, each training query-\nresponse pair and the special tokens are concatenated as\na single sequence x = {x\n1,··· ,xm,xm+1,··· ,xn} of\nlength n. x1:m corresponds to the query token sequence of\nlength m and xm is the special token [EOQ], denoting the\nend of query. xm+1:n corresponds to the response and xn\nis [EOS], the end symbol of the whole sequence. The train-\ning objective of our model is to maximize the unconditional\nlikelihood p(x\n1:n), similar to the existing language mod-\nels (Bengio et al. 2003; Merity, Keskar, and Socher 2018).\nThe architecture of our model is depicted in Fig 2, where\nL decoder-only transformer layers (V aswani et al. 2017)5\nare involved. Different from the original transformer layer\nsolely containing the self-attention component, the trans-\nformer layer in our model is further empowered with the pro-\nposed supervised source attention (SSA) component. The\noutputs of the l-th transformer layer are the contextual-\nized token representations of size dim\nh, denoted as Hl ∈\nRn×dimh . When predicting the tokens, a Topic Inference\n(TI) component is introduced to provide the reﬁned query\nrepresentations encoding the topic information inferred from\nthe reference.\nLanguage Model as Response Generator\nTo achieve the goal of adapting language model for the STC\ntask, we should carefully design a training strategy differ-\nent from that in the S\nEQ 2SEQ framework. Based on the ob-\nservation that the human conversations can be regarded as a\nprocess of text continuation (i.e., giving the response/answer\nright after the query/question), we concatenate the query to-\nken sequence and the response token sequence into a single\nsequence and formulate the STC task as a contextual text\ncontinuation problem. One input example of our model is\nillustrated in Fig 1. The training goal of the model is to min-\nimize the joint negative log likelihood over the whole se-\nquence:\nL\nmle = −logP(x1:n)= −\nn∑\nt=1\nlogP(xt|x<t) (1)\nObviously, it is easy to bridge the gap between the task-\nspeciﬁc training and the auto-regressive pre-training (Peters\net al. 2018; Radford et al. 2018; 2019) because the formu-\nlations of their objectives are almost the same. Another ad-\nvantage of this language model formulation is that it takes\n4We notice that some concurrent works (Olabiyi and Mueller\n2019; Zhang et al. 2019) also adopt the strategy similar to ours\nafter the submission.\n5For the technical details of transformer, we recommend the\nreader to read the paper (V aswani et al. 2017).\n8254\nthe likelihood of query tokens into consideration, which is\nignored in the existing works (Shang, Lu, and Li 2015;\nXing et al. 2017). Intuitively, the text generated by the\nlanguage model is more ﬂuent than those generated by\nSEQ 2SEQ framework because the generator of the language\nmodel (the language model itself) is not only trained on the\nresponse sentence but also the query sentence.\nRelevance Modeling Component\nThe vanilla transformer decoder is equipped with self-\nattention (Cheng, Dong, and Lapata 2016; Lin et al. 2017)\nand can theoretically capture the context of arbitrary length.\nGiven the inputH\nl−1 ∈ Rn×dimh , the contextualized repre-\nsentations hl\nt (l ∈ [1,L], t ∈ [1,n])a tt h et-th time step is\nbuilt as follows:\nhl\nt,αααl\nt = SLF -ATT(ql−1\nt ,Kl−1\n≤t ,Vl−1\n≤t )\nQl−1 = Hl−1WQ\nKl−1,Vl−1 = Hl−1WK ,Hl−1WV\n(2)\nwhere SLF -ATT is the self-attention layer6 and αααl\nt ∈ Rt is\nthe calculated attention vector.Q, K, V ∈ Rn×dimh respec-\ntively denote the query7, key and value in the self-attention\nlayer. Kl−1\n≤t = {kl−1\n1 ,··· ,kl−1\nt } indicate the leftward el-\nements and the same to Vl−1\n≤t . Despite its capability of\nlearning global dependency, the transformer-based language\nmodel still has the tendency to overly rely on the short-term\ncontext and ignore the long-term context when predicting\nthe next word, dubbed asexplanation away problem (Holtz-\nman et al. 2019). This problem is catastrophic for the STC\ntask because the query acts as the long-term context in our\nlanguage model formulation and not involving the query in-\nformation is prone to generating the content irrelevant to\nthe query. Therefore, explicitly modeling the relevance and\nemphasizing the importance of the query are essential. In\nthis paper, we propose two components, namely,Supervised\nSource Attention (SSA) andTopic Inference (TI), to handle\nthe explanation away problem.\nSupervised Source Attention In the existing S\nEQ 2SEQ-\nbased frameworks, incorporating the query/source infor-\nmation is achieved by applying encoder-decoder attention\nsolely on the encoder hidden representations. Similarly, at-\ntending only on the long-term context of language model is\npresumably beneﬁcial for improving the relevance. There-\nfore, we propose to introduce another source attention layer\non top of the self-attention layer. The computational formula\nof the t\n′-th (t′ ≥ m) query-enhanced hidden representation\nˆhl\nt′ is below:\nˆhl\nt′ ,βββl\nt′ = SRC -ATT(ˆql\nt′ , ˆKl, ˆVl)\nˆQl = HlWQ\nˆKl, ˆVl = Hl\n1:mWK ,Hl\n1:mWV\n(3)\n6The symbols for the feed-forward layer and residual connec-\ntions are not shown.\n7Here, the “query” refers to a real-valued vector while the\n“query” in the STC task is a sentence.\nSRC -ATT refers to our source attention layer on top of the\nself-attention layer.βββl\nt′ ∈ Rm is the attention scores for the\ncorresponding hidden representations of the query tokens.\nHl is the output of SLF -ATT layer and ˆQl ∈ Rn×dimh , ˆKl,\nˆVl ∈ Rm×dimh are the corresponding query, key, value in\nthe source attention. Note that we only additionally apply\nsource attention when the current token is not query token,\ni.e., t\n′ ≥ m, and do nothing in the preceding steps. Learning\nword alignment from data is possible but may be inaccurate\nwithout any supervision or external knowledge (Liu et al.\n2016; Mi, Wang, and Ittycheriah 2016), therefore, we em-\nploy the keywords as the knowledge and enforce the source\nattention component to be concentrated on the important\nquery tokens. First of all, we performmax-over-time pool-\ning over the attention vectorsβββ\nl\nt′ ∈ Rm (t′ ∈ [m+1,n]) and\ninduce the vector ˆysrc ∈ Rm reﬂecting the salience scores\nof the query/source tokens:\nˆysrc\ni =m a x{βββL\nm+1,i,··· ,βββL\nn,i},i ∈ [1,m] (4)\nThen, given the query keyword indicator vector ysrc ∈\n{0,1}m, we introduce additional source attention lossLsrc\ninto Eq (1):\nLsrc = 1\nm||ˆysrc\ni −ysrc||2\n2 (5)\nIdeally, the generation process will rely on more important\nquery tokens if the salience scoreˆysrc is more close to the\nkeyword vectorysrc.\nTopic Inference The SSA component attempts to improve\nthe relevance by highlighting the importance of the impor-\ntant query tokens/words in the attention process. However,\nthe range of the words topically related to the query is far\nmore than that of the keywords explicitly mentioned in the\nquery. Considering the query “what is your favorite fruit?”\nand two valid responses “I like the watermelon very much”\nand “My favorite fruit is pineapple”, “fruit” should be em-\nphasized during the generation but the words used to dis-\ncuss fruit such as “watermelon” and “pineapple” are also\nvery meaningful for building a response. Inspired by this,\nwe collect the multiple references of each query in the train-\ning set and gather all of the keywords extracted from such\nresponses\n8. To exploit the latent topic information, we intro-\nduce Topic Inference (KI) component to estimate the global\ntopical word distribution based on the query representation\nh\nq as follows:\nhq = f(x1:m),P (z|x1:m)= Softmax(Wohq) (6)\nwhere f : Rm → Rdimh denotes the function mapping the\ninput query tokens to a low-dimensional query representa-\ntion. Speciﬁcally, we feed the last query hidden representa-\ntion in the transformer, namely,h\nL\nm, into a linear layer with\ntanh activation and regard the output as the query represen-\ntation hq for simplifying the modeling part. To encode the\n8(Xing et al. 2017) extend the keyword set using external cor-\npus. Here, we focus on improving the relevance rather than en-\nriching the topical words in the response, thus, we only utilize the\ntraining data to explore more keywords.\n8255\nFigure 2: Overall architecture. The Topic Inference (TI) component on top of the transformer layers and the Supervised Source\nAttention (SSA) component inside the transformer layers are the proposed relevance-promoting components. Training losses\nare calculated on top of the obtained representation vectorss\nt’s.\ntopic information into the query representation, we employ\nthe global keyword indicator vectorykwd ∈{ 0,1}|V| as su-\npervision signals and enforce the components correspond-\ning to keywords/important tokens in the query-based global\ntopic distribution to be up-weighted. The computational for-\nmula is as follows:\nL\nkwd = − 1\n|V|\n|V|∑\ni=1\nykwd\ni ·logPi(z|x1:m) (7)\nwhere the subscript i denotes the i-th component of a vec-\ntor and |V| is the vocabulary size. Note that we attempt to\nreplace the Softmax in Eq 6 with the component-wise Sig-\nmoid, typically used in multi-label classiﬁcation problem,\nbut the empirical results become worse. Thus, we keep the\nSoftmax probability function unchanged in the experiment.\nSimilar to Eq 5, theL\nkwd will be added in the training loss.\nDifferent from (Y ao et al. 2017) and (Gao et al. 2019) re-\ngarding the concrete topic/keyword as the trigger of gener-\nation, we introduce the query representation encoding the\nglobal topic information as the supplementation for each\ntoken-level representation to encourage the generation of the\nrelevant topical words. The representation vectors\nt for pre-\ndicting the output is calculated below:\nst =\n{\n(1−gt)∗hL\nt +gt ∗hq ,i ft>m\nhL\nt , Otherwise\ngt = σ(Wghq +WlhL\nt +b),\n(8)\nwhere gt ∈ Rdimh is the gate value and Wg,Wl ∈\nRdimh×dimh are parameter matrices in the TI component.\nModel Training\nThe proposed SSA component and the TI component are\njointly trained with the transformer-based language model.\nBased on Eq 1, Eq 5 and Eq 7, the overall training objective\nL(θ) of the proposed model is as follow:\nL(θ)= 1\n|D|\n∑\n(x,ysrc,ykwd)∈D\nL(x,ysrc,ykwd)\nL(x,ysrc,ykwd)= Lmle +γ1Lsrc +γ2Lkwd\n(9)\nHere, γ1 and γ2 are the coefﬁcients controlling the propor-\ntion ofLsrc and Lkwd involved in the training respectively.\nDecoding\nDue to the limited search space, it is difﬁcult for the beam\nsearch or greedy search to ﬁnd the interesting and di-\nverse responses. Therefore, we do not adopt them but a\n“randomization-over-maximization” strategy (also know as\n‘top-k sampling”) to perform the decoding, as done in (Fan,\nLewis, and Dauphin 2018; Radford et al. 2019). (Holtzman\net al. 2019) and (Ippolito et al. 2019) explore the usage of\nother advanced decoding strategies in the language gener-\nation task. Since our aim in this paper is not to compare\nthe performances across the different decoding strategies,\nwe consistently use the top-k sampling.\nExperiment\nExperiment Setup\nWe utilize the benchmark STC dataset (Liu et al. 2018)\nto evaluate the effectiveness of the proposed relevance-\npromoting transformer language model. This dataset is built\n8256\nbased on the real conversations fromWeibo9 and contains\nabout 7M high-quality query-response pairs. We split the\ndataset such that #train:#dev:#test is 7,024,156:2,000:800.\nTraining details are provided in the appendix.\nTo avoid word segmentation errors and out-of-vocabulary\nissue, the tokens in our model and the baseline models are\nChinese characters and the vocabulary size is about 12,000.\nEvaluation Metrics\nWe introduce the following metrics to evaluate the model’s\ncapability of generating relevant and diverse responses:\nRelevance Metrics We employ B\nLEU -2, BLEU -3 &\nBLEU -4 (Papineni et al. 2002) to estimate the relevance of\nthe generated responses. Moreover, we also design two more\nmetrics, namely,H\nIT-Q and HIT-R to calculate the hit rates\nof the topical words in the query and the response respec-\ntively. Firstly, we build ahigh-precision-low-recall keyword\nset for each query/response sentence based on keyword ex-\ntraction toolkit\n10 and ﬁlter some noisy words based on ad-\nditional hand-crafted rules. Then, we calculate the HIT-Qi\nand HIT-Ri for thei-th predictions as follows:\nHIT-Qi = |Kri ∩Kqi |\n|Kri | ,HIT-Ri = |Kri ∩Krg\ni |\n|Kri | (10)\nwhere Kqi , Kri and Krg\ni respectively denote the topical\nword set for thei-th query, predicted response and gold stan-\ndard response. Then we obtain the HIT-Q and HIT-R by per-\nforming the corpus-level average:\nHIT-Q = 1\nN\nN∑\ni\nHIT-Qi,HIT-R = 1\nN\nN∑\ni\nHIT-Ri (11)\nDiversity Metrics Following (Li et al. 2016a), we employ\nDIST-1 and DIST-2 to calculate the ratios of the distinct\nuni-grams and bi-grams in the generated responses.\nHuman Evaluations We also conduct human evaluations.\nSpeciﬁcally, we randomly sampled 100 queries and recruit\nﬁve helpers to judgeRelevance (4-scale rating, 0-3),Fluency\n(3-scale rating, 0-2) andAcceptance (0 or 1) of the generated\nresponses from our model and the baselines. Details of the\nrating criteria are stated in the appendix.\nComparison Models\n• LSTM-LM (Mei, Bansal, and Walter 2017): LSTM-\nbased auto-regressive language model armed with incre-\nmental self-attention. We train LSTM-LM using the same\nstrategy mentioned in this paper.\n• LSTM-S2S: Attention-based LSTM Sequence-to-\nSequence model.\n• TFM-S2S: Transformer Sequence-to-Sequence model\nwhere the network components are identical to those\nin (V aswani et al. 2017).\n• TFM-LM: Transformer-based auto-regressive language\nmodel. We train TFM-LM using the same strategy men-\ntioned in this paper.\n9https://www.weibo.com/\n10https://github.com/fxsjy/jieba\n• MMI (Li et al. 2016a): LSTM-S2S with Maximum Mu-\ntual Information objective in decoding. In this paper, we\nset the number of responses for re-ranking as 50.\n• CV AE(Zhao, Zhao, and Eskenazi 2017)11: Conditional\nV ariational Auto-Encoder for response generation. We re-\nplace the dialogue acts used in the original model with the\nkeywords extracted from the references.\n• MMPMS (Chen et al. 2019): The model with the state-of-\nthe-art performance on the STC task. We re-run the ofﬁ-\ncially released code\n12 to obtain the results on our dataset.\nMain Results\nTable 1 and 2 list the automatic evaluation results and\nthe human evaluation results respectively. In terms of\nB\nLEU , the proposed model with beam search decoding,\nnamely, OURS-bm, consistently achieve the best scores.\nBesides, OURS-bm outperforms all compared models on\nthe keyword-overlapping-based HIT metrics, suggesting that\nour model, armed with Supervised Source Attention compo-\nnent (SSA) and Topic Inference (TI) component, is beneﬁ-\ncial for the generation of informative topical words related to\nthe query. Surprisingly, OURS-bm also obtains better D\nIST\nmetrics than the baseline models. After replacing the beam\nsearch with top-k sampling, our model (OURS-tk) is further\nenhanced in diversity modeling, reaching 0.107 and 0.544\non D\nIST-1 and DIST-2 respectively.\nRegarding the more reliable human evaluations, both\nof OURS-bm and OURS-tk are the top-ranked models.\nSpeciﬁcally, despite its unsatisfactory results on the auto-\nmatic BLEU and HIT metrics, OURS-tk performs the best\non the manually annotated Relevance metric with 5% im-\nprovement over the current state-of-the-art MMPMS model.\nInstead, OURS-bm, the best model on the automatic rele-\nvance metrics, still yields competitive results on theRele-\nvance. It is reasonable because some words not appearing in\nthe query/references, especially those not being frequently\nused, are still related to the discussed topic in the conversa-\ntions. At the same time, such inconsistency between auto-\nmatic and human evaluations demonstrates the effectiveness\nof top-k sampling, a randomization-over-maximization de-\ncoding strategy, in discovering infrequent but meaningful\npatterns for the STC task.\nWe now turn to discuss the performance of the other com-\npared methods. Inheriting the powerful modeling capability\nof Transformer, TFM-S2S obtains the best automatic rele-\nvance scores as well as the second bestRelevance among\nthe baselines. TFM-LM, another Transformer-based base-\nline following the language model formulation in our pa-\nper, performs not as good as TFM-S2S on all of the met-\nrics except Fluency, verifying the postulation that the ex-\nplanation away issue of language model has the tendency\nto produce ﬂuent but topically irrelevant responses. Despite\nof this, the TFM-LM outperforms LSTM-LM and LSTM-\nS2S, proving the superiority of Transformer to LSTM in\nresponse generation. Owing to the re-ranking mechanism,\n11https://github.com/snakeztc/NeuralDialog-CV AE\n12https://github.com/PaddlePaddle/models\n8257\nModel Relevance Diversity\nBLEU -2 B LEU -3 B LEU -4 H IT-Q H IT-R DIST-1 D IST-2\nLSTM-LM 3.8 0.9 0.3 0.084 0.066 0.028 0.094\nLSTM-S2S 5.6 2.8 1.8 0.293 0.145 0.039 0.137\nTFM-LM 6.9 3.2 2.1 0.295 0.144 0.058 0.259\nTFM-S2S 7.3 3.5 2.3 0.369 0.172 0.078 0.290\nMMI 7.9 2.5 1.0 0.197 0.145 0.093 0.349\nCV AE 5.8 1.5 0.4 0.211 0.135 0.060 0.211\nMMPMS 6.7 3.0 1.8 0.151 0.102 0.057 0.220\nOURS-tk w/o SSA & TI 4.9 1.0 0.3 0.119 0.076 0.086 0.441\nOURS-tk w/o SSA 5.5 2.1 1.5 0.150 0.146 0.102 0.521\nOURS-tk w/o TI 5.1 2.1 1.4 0.171 0.132 0.090 0.445\nOURS-bm 10.3 5.3 3.4 0.510 0.193 0.102 0.398\nOURS-tk 6.0 3.6 2.5 0.191 0.152 0.107 0.544\nTable 1: Experimental results on the automatic metrics. The best results are inbold.\nModel Evaluation Metrics\nRelevance Fluency Acceptance\nLSTM-LM 1.206 1.297 0.26\nLSTM-S2S 1.386 1.285 0.37\nTFM-LM 1.412 1.328 0.39\nTFM-S2S 1.475 1.306 0.43\nMMI 1.432 1.301 0.34\nCV AE 1.316 1.274 0.33\nMMPMS 1.528 1.396 0.42\nOURS-tk w/o SSA & TI 1.273 1.368 0.28\nOURS-tk w/o SSA 1.485 1.407 0.39\nOURS-tk w/o TI 1.503 1.303 0.36\nOURS-bm 1.515 1.359 0.38\nOURS-tk 1.606 1.346 0.44\nTable 2: Human evaluation results with the best ones inbold.\nthe MMI model is the strongest baseline on diversity mod-\neling but OURS-bm/OURS-tk still achieves approximately\n14%/55% improvement on DIST-2.\nAblation Study\nIn order to track the source of the performance gains, we\nalso conduct the ablation study on the OURS-tk. The corre-\nsponding automatic and human evaluation results are shown\nin the second group of Table 1 and Table 2. As expected, the\nmodel without relevance-promoting design, i.e., OURS-tk\nw/o SSA & TI, is the worst one on the relevance metrics.\nOURS-k w/o SSA and OURS-tk w/o TI, the variants in-\ncorporating either TI or SSA for relevance modeling, boost\nthe Relevance score by ∼17% and ∼18% respectively. Al-\nthough they are comparable on the relevance metrics but the\nformer achieves higher diversity scores (D\nIST-2: 0.521 v.s.\n0.441). We attribute this phenomenon to the TI component,\nwhich exploits the usage of more related topical words men-\ntioned in the multiple references. With the help of both SSA\ncomponent and TI component, OURS-tk becomes the best\nmodel on Relevance and D\nIST metrics, demonstrating the\nnecessity of the relevance modeling for the transformer lan-\nguage model. Another interesting ﬁnding is that the SSA\ncomponent decreases the Fluency score (see the results of\nOURS-tk w/o TI), which indicates that ﬁghting against\nexplanation-away issue by incorporating additional query\ncontext may be coupled with corrupting the language model.\nCase Study\nFigure 3 shows example responses generated by our model\nand the most competitive baseline models. OURS-tk, which\nexplicitly incorporates the query context and exploits the to-\nkens potentially related to the query, always produces mean-\ningful and informative responses. Taking the Query #1 & #2\nas examples, the generated responses accurately respond to\nthe query because they mention “ﬂower ladder”/“matcha”\nand “cream”, which are exactly the topics discussed in\nthe conversations. The response for the Query #3 can eas-\nily engage user in the conversation and thus it is also a\nmeaningful prediction. The outputs of TFM-LM are gen-\nerally ﬂuent. However, due to theexplanation away issue,\nTFM-LM tends to generate the irrelevant response (Case\n#1) or response with phrase repetition (Case #2). Under the\nsequence-to-sequence formulation, TFM-S2S obtains the re-\nsponses moderately related to the corresponding queries al-\nthough the third output, directly copying part of the source\ntext (i.e., query), is still unsatisfactory. MMPMS and MMI,\nthe models aiming for promoting diversity, have chances to\nyield irrelevant responses.\nFurther Discussions on Top-k Sampling\nWe further investigate the impact of top-k sampling on\nthe STC models. Firstly, we conduct additional automatic\nand human evaluations on the baseline models with results\nshown in Table 3. As can be seen, the top-k sampling con-\nsistently improves the D\nIST-2 score by a large margin on\nall models but the Relevance scores of LSTM-S2S, TFM-\nLM and TFM-S2S decrease after top-k sampling is applied.\nThe variation trends ofFluency across the evaluated models\nare also inconsistent. These observations suggest that top-k\nsampling is simple yet effective to achieve diverse response\ngeneration but it should be carefully utilized in the model\nbecause of its uncertainty on relevance and ﬂuency.\nAs discussed in Case Study, the transformer-based models\nadopting beam search have the tendency to generate the re-\nsponses with repetition and those directly copying the query.\nWe here investigate whether top-k sampling can help solve\nthese issues. Figure 4 depicts the ratios of responses in the\ntest set falling into the phrase repetition and query copy. The\n8258\nTFM-LM MMI TFM-S2S MMPMS OURS-tk\nQuery #1: 有了这样的花梯，回家的感觉真好 (With this ﬂower ladder, it feels so good going back home.)\n好漂亮的花，好喜欢 我也有这样的感觉 我很喜欢！ 我擦，这也太有才了吧 美得令人窒息的花梯\nThe ﬂower is beautiful! Like it! I have the same feeling! I really like it! Oh my god! It is really funny What a beautiful ﬂower ladder!!\nQuery #2: 抹茶与奶油的结合，精致得不忍下口 (This is the combination of matcha and cream. It is quite delicate such that I couldn’t bear to eat. )\n不忍下口，不忍下口。 好想吃啊！ 我能接受。 我喜欢抹茶味的抹茶。 我喜欢抹茶和奶油\nCouldn’t bear to eat, couldn’t bear to eat. I really want to have a try. The food is acceptable for me. I like the matcha of matcha ﬂavor. I love matcha and cream!\nQuery #3: 移动的别墅。介个太强大了 (This moving cottage is awesome!)\n我也想要这样的房子 有创意，有创意啊 介个太强大了，房间装不下 移动和联通 这是哪儿啊，怎么拍的，好厉害\n好酷哦\nI wanna a house like this, too. It is quite creative. It is too strong to ﬁt in the room.Chinese Mobile and Chinese Uni-\ncom\nWhere is it? How do you shoot this\nvideo. It look cool!\nFigure 3: Examples of response generation. We translate Chinese samples to English.\nFigure 4: Comparison results on beam search and top-k sam-\npling. Speciﬁcally, if the length of the longest common sub-\nstring between response and query is larger than 4, then the\nresponse is regarded as a “copy” of query. If a response con-\ntains the word/phrase loop over 3 times, it is regarded as a\nresponse with repetition.\ntop-k sampling greatly reduces the query copy rate (about\n72% on average) and almost eliminates the phrase repeti-\ntion phenomenon in the Transformer-based models. How-\never, note that Table 3 shows both TFM-LM and TFM-S2S\nperform worse onRelevance after using top-k sampling. We\nconsider these results are consistent with human perception\nbecause enriching the morphology via sampling-based de-\ncoding strategy will inevitably introduce irrelevant informa-\ntion, leading to the degradation of relevance score. It is no-\nticeable that the proposed model (i.e., OURS) is not affected\non relevance modeling due to its capability of ﬁltering some\ntopically irrelevant candidates for the sampling process.\nModels Relevance (Δ) Fluency (Δ)D IST-2 (Δ)\nLSTM-LM-tk 1.111 (-0.09) 1.270 (-0.03) 0.383 (+0.29)\nLSTM-S2S-tk 1.439 (+0.05) 1.265 (-0.20) 0.490 (+0.35)\nTFM-LM-tk 1.273 (-0.14) 1.368 (+0.04) 0.441 (+0.18)\nTFM-S2S-tk 1.270 (-0.15) 1.321 (+0.15) 0.507 (+0.22)\nOURS-tk 1.606 (+0.10) 1.346 (-0.13) 0.544 (+0.20)\nTable 3: Experimental results on the models adopting top-\nk sampling. Δ refers to the improvement over the original\nmodel adopting beam search. The best results are inbold.\nRelated Work\nShort Text ConversationShort Text Conversation (STC)\nis usually formulated as a conditional text generation\ntask (Shang, Lu, and Li 2015; Serban et al. 2016). The\nsequence-to-sequence (S\nEQ 2SEQ) encoder-decoder frame-\nwork (Cho et al. 2014; Sutskever, Vinyals, and Le 2014;\nBahdanau, Cho, and Bengio 2015) and its variants have been\nstudied extensively for solving this task. Li et al. 2016a\nintroduce diversity-promoting decoding strategies into the\nS\nEQ 2SEQ model. Some (Mou et al. 2016; Xing et al. 2017;\nY ao et al. 2017; Zhou et al. 2017; Gao et al. 2019) attempt\nto guide the S\nEQ 2SEQ model to generate keyword/topic-\naware responses while others (Wu et al. 2019; Cai et al.\n2019) try to control the response generation with addi-\ntional retrieved data. The advanced techniques such as RL,\nGAN and V AE are also considered for improving con-\nversational experience (Li et al. 2016b; Xu et al. 2017;\nDu et al. 2018).\nTransformer-based Language Model Deep transformer-\nbased architecture (V aswani et al. 2017) has led to sig-\nniﬁcant performance gains on the language modeling\ntask (Al-Rfou et al. 2019; Dai et al. 2019; Radford et al.\n2019), compared to the existing CNN/RNN-based architec-\ntures (Dauphin et al. 2017; Merity, Keskar, and Socher 2018;\nMelis, Dyer, and Blunsom 2018). Meanwhile, GPT-2 (Rad-\nford et al. 2019) and U\nNI LM (Dong et al. 2019) are the pio-\nneer works adapting the transformer language model for the\nconditional text generation tasks.\nConclusion\nIn this paper, we present a language model based solution in-\nstead of traditional S\nEQ 2SEQ paradigm for handling Short-\nText Conversation (STC). We ﬁrstly tailor-make a train-\ning strategy to adapt the language model for the STC task.\nThen, we propose a relevance-promoting transformer lan-\nguage model to distill the relevance clues from the query\nas well as the topics inferred from the references, and in-\ncorporate them into the generation. Moreover, we explore\nthe usage of top-k sampling for the STC task to further im-\nprove the response diversity. Experimental results on a large-\nscale STC dataset validate that our model is superior to the\ncompared models on both relevance and diversity from au-\ntomatic and human evaluations.\nReferences\nAl-Rfou, R.; Choe, D.; Constant, N.; Guo, M.; and Jones, L. 2019.\nCharacter-level language modeling with deeper self-attention. In\nAAAI, 3159–3166.\n8259\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural machine\ntranslation by jointly learning to align and translate. InICLR.\nBengio, Y .; Ducharme, R.; Vincent, P .; and Jauvin, C. 2003. A\nneural probabilistic language model.JMLR 3(Feb):1137–1155.\nCai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and Shi, S.\n2019. Skeleton-to-response: Dialogue generation guided by re-\ntrieval memory. InNAACL, 1219–1228.\nChen, C.; Peng, J.; Wang, F.; Xu, J.; and Wu, H. 2019. Gener-\nating multiple diverse responses with multi-mapping and posterior\nmapping selection. arXiv preprint arXiv:1906.01781.\nCheng, J.; Dong, L.; and Lapata, M. 2016. Long short-term\nmemory-networks for machine reading. InEMNLP, 551–561.\nCho, K.; van Merri ¨enboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning\nphrase representations using RNN encoder–decoder for statistical\nmachine translation. InEMNLP, 1724–1734.\nDai, Z.; Y ang, Z.; Y ang, Y .; Carbonell, J.; Le, Q.; and Salakhutdi-\nnov, R. 2019. Transformer-XL: Attentive language models beyond\na ﬁxed-length context. InACL.\nDauphin, Y . N.; Fan, A.; Auli, M.; and Grangier, D. 2017. Lan-\nguage modeling with gated convolutional networks. InICML.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT:\nPre-training of deep bidirectional transformers for language under-\nstanding. In NAACL, 4171–4186.\nDong, L.; Y ang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .; Gao,\nJ.; Zhou, M.; and Hon, H.-W. 2019. Uniﬁed language model pre-\ntraining for natural language understanding and generation.arXiv\npreprint arXiv:1905.03197.\nDu, J.; Li, W.; He, Y .; Xu, R.; Bing, L.; and Wang, X. 2018. V ari-\national autoregressive decoder for neural response generation. In\nEMNLP, 3154–3163.\nFan, A.; Lewis, M.; and Dauphin, Y . 2018. Hierarchical neural\nstory generation. InACL, 889–898.\nGao, J.; Bi, W.; Liu, X.; Li, J.; and Shi, S. 2019. Generating mul-\ntiple diverse responses for short-text conversation. InAAAI.\nHoltzman, A.; Buys, J.; Forbes, M.; and Choi, Y . 2019. The curious\ncase of neural text degeneration.arXiv preprint arXiv:1904.09751.\nIppolito, D.; Kriz, R.; Sedoc, J.; Kustikova, M.; and Callison-\nBurch, C. 2019. Comparison of diverse decoding methods from\nconditional language models. InACL, 3752–3762.\nKhandelwal, U.; He, H.; Qi, P .; and Jurafsky, D. 2018. Sharp\nnearby, fuzzy far away: How neural language models use context.\nIn ACL, 284–294.\nLi, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan, B. 2016a.\nA diversity-promoting objective function for neural conversation\nmodels. In NAACL, 110–119.\nLi, J.; Monroe, W.; Ritter, A.; Jurafsky, D.; Galley, M.; and Gao,\nJ. 2016b. Deep reinforcement learning for dialogue generation. In\nEMNLP, 1192–1202.\nLin, Z.; Feng, M.; Santos, C. N. d.; Y u, M.; Xiang, B.; Zhou, B.;\nand Bengio, Y . 2017. A structured self-attentive sentence embed-\nding. In ICLR.\nLiu, L.; Utiyama, M.; Finch, A.; and Sumita, E. 2016. Neural\nmachine translation with supervised attention. InCOLING.\nLiu, Y .; Bi, W.; Gao, J.; Liu, X.; Y ao, J.; and Shi, S. 2018. Towards\nless generic responses in neural conversation models: A statistical\nre-weighting method. InEMNLP, 2769–2774.\nMei, H.; Bansal, M.; and Walter, M. R. 2017. Coherent dialogue\nwith attention-based language models. InAAAI.\nMelis, G.; Dyer, C.; and Blunsom, P . 2018. On the state of the art\nof evaluation in neural language models. InICLR\n.\nMerity, S.; Keskar, N. S.; and Socher, R. 2018. Regularizing and\noptimizing lstm language models. InICLR.\nMi, H.; Wang, Z.; and Ittycheriah, A. 2016. Supervised attentions\nfor neural machine translation. InEMNLP, 2283–2288.\nMou, L.; Song, Y .; Y an, R.; Li, G.; Zhang, L.; and Jin, Z. 2016. Se-\nquence to backward and forward sequences: A content-introducing\napproach to generative short-text conversation. InCOLING.\nNallapati, R.; Zhou, B.; dos Santos, C.; Gulc¸ehre, C¸ .; and Xiang, B.\n2016. Abstractive text summarization using sequence-to-sequence\nRNNs and beyond. InCoNLL, 280–290.\nOlabiyi, O., and Mueller, E. T. 2019. Dlgnet: A transformer-\nbased model for dialogue response generation. arXiv preprint\narXiv:1908.01841.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. InACL.\nPeters, M.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee,\nK.; and Zettlemoyer, L. 2018. Deep contextualized word represen-\ntations. In NAACL, 2227–2237.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018.\nImproving language understanding by generative pre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised multitask\nlearners. OpenAI Blog1(8).\nRitter, A.; Cherry, C.; and Dolan, W. B. 2011. Data-driven response\ngeneration in social media. InEMNLP, 583–593.\nSerban, I. V .; Sordoni, A.; Bengio, Y .; Courville, A.; and Pineau,\nJ. 2016. Building end-to-end dialogue systems using generative\nhierarchical neural network models. InAAAI.\nShang, L.; Lu, Z.; and Li, H. 2015. Neural responding machine for\nshort-text conversation. InACL, 1577–1586.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence to se-\nquence learning with neural networks. InNeurIPS, 3104–3112.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. InNeurIPS, 5998–6008.\nWu, Y .; Wei, F.; Huang, S.; Wang, Y .; Li, Z.; and Zhou, M. 2019.\nResponse generation by context-aware prototype editing. InAAAI.\nXing, C.; Wu, W.; Wu, Y .; Liu, J.; Huang, Y .; Zhou, M.; and Ma,\nW.-Y . 2017. Topic aware neural response generation. InAAAI.\nXu, Z.; Liu, B.; Wang, B.; SUN, C.; Wang, X.; Wang, Z.; and Qi,\nC. 2017. Neural response generation via gan with an approximate\nembedding layer. InEMNLP, 617–626.\nY ao, L.; Zhang, Y .; Feng, Y .; Zhao, D.; and Y an, R. 2017. Towards\nimplicit content-introducing for generative short-text conversation\nsystems. In EMNLP, 2190–2199.\nY u, L.; Blunsom, P .; Dyer, C.; Grefenstette, E.; and Kocisky, T.\n2017. The neural noisy channel. InICLR.\nZhang, Y .; Sun, S.; Galley, M.; Chen, Y .-C.; Brockett, C.; Gao, X.;\nGao, J.; Liu, J.; and Dolan, B. 2019. Dialogpt: Large-scale gen-\nerative pre-training for conversational response generation.arXiv\npreprint arXiv:1911.00536.\nZhao, T.; Zhao, R.; and Eskenazi, M. 2017. Learning discourse-\nlevel diversity for neural dialog models using conditional varia-\ntional autoencoders. InACL, 654–664.\nZhou, G.; Luo, P .; Cao, R.; Lin, F.; Chen, B.; and He, Q. 2017.\nMechanism-aware neural machine for dialogue response genera-\ntion. In AAAI.\n8260"
}