{
    "title": "Toward expert-level medical question answering with large language models",
    "url": "https://openalex.org/W4406152279",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5083904567",
            "name": "K. K. Singhal",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5014724876",
            "name": "Tao Tu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5057932939",
            "name": "Juraj Gottweis",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5058399428",
            "name": "Rory Sayres",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5076826749",
            "name": "Ellery Wulczyn",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5102999001",
            "name": "Mohamed Amin",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5101315776",
            "name": "Le Hou",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5031489667",
            "name": "Kevin Clark",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5021812637",
            "name": "Stephen Pfohl",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5069557194",
            "name": "Heather Cole-Lewis",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5027230259",
            "name": "Darlene Neal",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5020446219",
            "name": "Qazi Mamunur Rashid",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5025626087",
            "name": "Mike Schaekermann",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5101522898",
            "name": "Amy Wang",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5072545690",
            "name": "Dev Dash",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5046725885",
            "name": "Jonathan H. Chen",
            "affiliations": [
                "Stanford Medicine",
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5041175834",
            "name": "Nigam H. Shah",
            "affiliations": [
                "Stanford Health Care",
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5091970724",
            "name": "Sami Lachgar",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5086361722",
            "name": "P. Mansfield",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5034500430",
            "name": "Sushant Prakash",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5028001282",
            "name": "Bradley Green",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5028178741",
            "name": "Ewa Dominowska",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5044698998",
            "name": "Blaise Agüera y Arcas",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5057195145",
            "name": "Nenad Tomašev",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5078784976",
            "name": "Yun Liu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5060982337",
            "name": "Renee Wong",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5010171106",
            "name": "Christopher Semturs",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5063201022",
            "name": "S. Sara Mahdavi",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5043862316",
            "name": "Joëlle Barral",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5060000122",
            "name": "Dale R. Webster",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5068955381",
            "name": "Greg S. Corrado",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5065128060",
            "name": "Yossi Matias",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5047463591",
            "name": "Shekoofeh Azizi",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5093912887",
            "name": "Alan Karthikesalingam",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5103234563",
            "name": "Vivek Natarajan",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4313197536",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4392359953",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2078294053",
        "https://openalex.org/W4230532893",
        "https://openalex.org/W2290985067",
        "https://openalex.org/W4221153690",
        "https://openalex.org/W4307003748",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W4297253404",
        "https://openalex.org/W3088056511",
        "https://openalex.org/W3162922479",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W6847076894",
        "https://openalex.org/W4400937555",
        "https://openalex.org/W4378603221",
        "https://openalex.org/W4372047097",
        "https://openalex.org/W4368340908",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W4402748869",
        "https://openalex.org/W3201142456",
        "https://openalex.org/W2924820166",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W6811129797",
        "https://openalex.org/W4308759651",
        "https://openalex.org/W4308615640",
        "https://openalex.org/W6866223645",
        "https://openalex.org/W4396758996",
        "https://openalex.org/W4390041933",
        "https://openalex.org/W4392822465",
        "https://openalex.org/W4401306979",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W6858023062",
        "https://openalex.org/W4226112058",
        "https://openalex.org/W6849590751",
        "https://openalex.org/W4385894687",
        "https://openalex.org/W6810528332",
        "https://openalex.org/W3083410900",
        "https://openalex.org/W2990031975",
        "https://openalex.org/W3035885149",
        "https://openalex.org/W3201595862",
        "https://openalex.org/W4226390647",
        "https://openalex.org/W3213752583",
        "https://openalex.org/W2114831632",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4312090934",
        "https://openalex.org/W6851275496",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4386942462",
        "https://openalex.org/W4385327559",
        "https://openalex.org/W6852476831",
        "https://openalex.org/W2137591261"
    ],
    "abstract": "Large language models (LLMs) have shown promise in medical question answering, with Med-PaLM being the first to exceed a 'passing' score in United States Medical Licensing Examination style questions. However, challenges remain in long-form medical question answering and handling real-world workflows. Here, we present Med-PaLM 2, which bridges these gaps with a combination of base LLM improvements, medical domain fine-tuning and new strategies for improving reasoning and grounding through ensemble refinement and chain of retrieval. Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19%, and demonstrates dramatic performance increases across MedMCQA, PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations framework shows that physicians prefer Med-PaLM 2 answers to those from other physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates significant improvements over its predecessor across all evaluation metrics, particularly on new adversarial datasets designed to probe LLM limitations (P < 0.001). In a pilot study using real-world medical questions, specialists preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. While specialist answers were still preferred overall, both specialists and generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating its growing potential in real-world medical applications.",
    "full_text": "Nature Medicine | Volume 31 | March 2025 | 943–950\n 943\nnature medicine\nhttps://doi.org/10.1038/s41591-024-03423-7\nArticle\nT oward expert-level medical question \nanswering with large language models\n \nLarge language models (LLMs) have shown promise in medical question \nanswering, with Med-PaLM being the first to exceed a ‘passing’ score in \nUnited States Medical Licensing Examination style questions. However, \nchallenges remain in long-form medical question answering and handling \nreal-world workflows. Here, we present Med-PaLM 2, which bridges these \ngaps with a combination of base LLM improvements, medical domain \nfine-tuning and new strategies for improving reasoning and grounding \nthrough ensemble refinement and chain of retrieval. Med-PaLM 2 scores up \nto 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19%, \nand demonstrates dramatic performance increases across MedMCQA, \nPubMedQA and MMLU clinical topics datasets. Our detailed human \nevaluations framework shows that physicians prefer Med-PaLM 2 answers \nto those from other physicians on eight of nine clinical axes. Med-PaLM 2 \nalso demonstrates significant improvements over its predecessor across \nall evaluation metrics, particularly on new adversarial datasets designed \nto probe LLM limitations (P < 0.001). In a pilot study using real-world \nmedical questions, specialists preferred Med-PaLM 2 answers to generalist \nphysician answers 65% of the time. While specialist answers were still \npreferred overall, both specialists and generalists rated Med-PaLM 2 to \nbe as safe as physician answers, demonstrating its growing potential in \nreal-world medical applications.\nLanguage is at the heart of health and medicine, underpinning inter-\nactions between people and care providers. Progress in LLMs has \nenabled the exploration of medical domain capabilities in artificial \nintelligence (AI) systems that can understand and communicate using \nlanguage, promising richer human–AI interaction and collaboration. \nIn particular, these models have demonstrated impressive capabilities \non multiple-choice research benchmarks1–3.\nThe advent of transformers 4 and LLMs 5,6 has renewed interest \nin the possibilities of AI for medical question-answering tasks—a \nlong-standing ‘grand challenge’ 7–9. A majority of these approaches \ninvolve smaller language models trained using domain-specific data \n(BioLinkBert10, DRAGON11, PubMedGPT12, PubMedBERT13, BioGPT14), \nresulting in steady improvements in performance on benchmark \ndatasets such as MedQA (United States Medical Licensing Examina -\ntion (USMLE))15, MedMCQA16 and PubMedQA17.\nThe rise of larger general-purpose LLMs such as GPT-3 (ref. 18) and \nFlan-PaLM19,20 trained on internet-scale corpora with massive comput-\ning infrastructure has seen leapfrog improvements on such bench -\nmarks within a few months (Fig. 1). In particular, GPT-3.5 (ref. 3) reached \nan accuracy of 60.2% on the MedQA (USMLE) dataset, Flan-PaLM \nreached an accuracy of 67.6% and GPT-4-base2 achieved 86.1%.\nIn parallel, application protocol interface (API) access to the GPT \nfamily of models spurred several studies evaluating the specialized \nclinical knowledge in these models, without specific alignment to the \nmedical domain. Levine et al. 21 evaluated the diagnostic and triage \naccuracies of GPT-3 for 48 validated case vignettes of both common \nReceived: 14 June 2024\nAccepted: 14 November 2024\nPublished online: 8 January 2025\n Check for updates\n e-mail: shekazizi@google.com; alankarthi@google.com; natviv@google.com\nA list of authors and their affiliations appears at the end of the paper\nNature Medicine | Volume 31 | March 2025 | 943–950 944\nArticle https://doi.org/10.1038/s41591-024-03423-7\nconsumer health and medical research. We proposed a human evalu-\nation rubric enabling physicians and laypeople to perform detailed \nassessment of model answers. Our initial model, Flan-PaLM, achieved \nstrong performance across multiple-choice benchmarks. However, \nhuman evaluation revealed further work was necessary to ensure fac-\ntual long-form answers aligned with human values and expectations \nin this safety-critical domain (a process generally referred to as ‘align-\nment’). We developed Med-PaLM, resulting in substantially improved \nphysician evaluations over Flan-PaLM. However, evaluation on these \nbenchmarks was limited as a measure of practical utility in real-world \nworkflows, and key shortfalls remained compared to physician answers.\nHere, we bridge these gaps and further advance LLM capabili -\nties in medicine with Med-PaLM 2. We developed this model using \na combination of an improved base LLM (PaLM 2; ref. 26 ), medical \ndomain-specific fine-tuning and new prompting strategies to improve \nreasoning and grounding, including ensemble refinement and chain \nof retrieval. Med-PaLM 2 improves upon Med-PaLM by over 19% on \nMedQA, as depicted in Fig. 1 , and approached or exceeded previous \nstate-of-the-art performance on MedMCQA, PubMedQA and MMLU \nclinical topics datasets.\nWhile these benchmarks are a useful measure of the knowledge \nencoded in LLMs, they do not capture a model’s ability to generate fac-\ntual, safe answers to questions that require nuanced answers, typical in \nreal-world medical question answering. We study this by expanding our \nevaluation framework for physicians and laypeople1. We introduce two \nadditional human evaluations: a pairwise ranking evaluation of model \nand physician answers to consumer medical questions along nine \nclinically relevant axes; and physician assessment of model answers \non two recently introduced adversarial testing datasets27 designed to \nprobe the limits of LLMs.\nFinally, we study the practical utility of Med-PaLM 2 for bedside \nconsultations. In a pilot study, we answer real-world medical questions \nsubmitted by specialist physicians to a consultation service during \nroutine care delivery28,29. Answering these questions is nontrivial: in the \nconsultation service, a team of physicians analyzed aggregate patient \ndata to provide a written report. Compared to answers from specialist \nand generalist physicians, answers from Med-PaLM 2 using chain of \nretrieval are comparable to or better than generalists’ answers but \nremain inferior to specialists’ answers. These results suggest that, as \nmodel performance approaches a human level, evaluation with highly \nspecialized experts becomes crucial, and current models may have \nutility in supporting information needs of medical staff where access \nto specialist physicians is limited.\nOur key contributions are summarized as follows: (1) We devel -\noped Med-PaLM 2, a medical LLM trained using an updated base model \n(PaLM 2; ref. 26) and targeted medical domain-specific fine-tuning. \n(2) We introduced ‘ensemble refinement’ as a prompting strategy \nto improve LLM reasoning. (3) We described ‘chain of retrieval’ , a \nstep-by-step pipeline using search as a tool that enables Med-PaLM \n2 to answer difficult medical research questions by grounding its \nclaims in relevant sources. (4) Med-PaLM 2 achieved state-of-the-art \nresults on several MultiMedQA multiple-choice benchmarks, includ-\ning MedQA USMLE-style questions, improving upon Med-PaLM per-\nformance by over 19% (Table 1). (5) Building upon our previous work1, \nwe incorporated several key enhancements to the human evaluation \nframework. These include new adversarial and bedside consulta -\ntion datasets, as well as a pairwise ranking system that compares \nmodel responses directly with those of human physicians. (6) Human \nevaluation of long-form answers to consumer medical questions \nshowed that Med-PaLM 2’s answers were preferred to physician and \nMed-PaLM answers across eight of nine axes relevant to clinical util-\nity, such as factuality and low likelihood of harm (Figs. 2  and 3). For \nexample, Med-PaLM 2 answers were judged to better reflect medical \nconsensus 72.9% of the time compared to physician answers (Fig. 1). \n(7) We introduced two adversarial question datasets to probe the \nand severe conditions and compared to laypeople and physicians. \nGPT-3’s diagnostic ability was found to be better than laypeople and \nclose to physicians. On triage, performance was less impressive and \ncloser to laypeople. Similarly, GPT-3 performance in genetics, surgery \nand ophthalmology was studied in refs. 22 –24, respectively. Ayers \net al.25 compared ChatGPT and physician answers on 195 randomly \ndrawn patient questions from a social media forum and found ChatGPT \nanswers to be rated higher in both quality and empathy.\nIn our previous work on Med-PaLM, we demonstrated the impor-\ntance of a wide-ranging benchmark for medical question answering, \ndetailed human evaluation of model answers and alignment strate -\ngies in the medical domain 1. We introduced MultiMedQA, a diverse \nbenchmark for medical question answering spanning medical exams, \nHigh-quality answer traits\nPotential answer risks\nBetter reflects consensus\nBetter reading comprehension\nBetter knowledge recall\nBetter reasoning\nMore inaccurate/irrelevant information\nOmits more information\nMore evidence of demographic bias\nGreater extent of harm\nGreater likelihood of harm\n0 20 40 60 80 100\nPercentage of responses\nMed-PaLM 2 Tie Physician\n0 20 40 60 80 100\n100\n90\n80\n70\n60\n50\n40\n30\nGPT-Neo\n33.3\nPubMedBERT\n38.1\nBioLinkBERT\n45.1\nDRAGON\n47.5\nBioMedLM\n50.3\nGPT-3.5\n60.2\nMed-PaLM\n67.2\nMed-PaLM 2\n86.5\nGPT- 4\n90.2\nMed-Gemini\n91.1\nDecember 2020\nMedQA (USMLE-style) accuracy (%)\nMay 2021\nNovember 2021 November 2022 November 2023\nMay 2022 May 2023 May 2024\na\nb\nFig. 1 | Med-PaLM 2 performance on MultiMedQA. a, Med-PaLM 2 achieved an \naccuracy of 86.5% on USMLE-style questions in the MedQA dataset. The shaded \nregion highlights the reported performance of models developed after Med-\nPaLM 2. b, In a pairwise ranking study on n = 1,066 consumer medical questions, \nMed-PaLM 2 answers were preferred over physician answers by a panel of \nphysicians across eight of nine axes in our evaluation framework. Stacked bars \nrepresent proportions of answers for which physician raters preferred Med-\nPaLM 2 answers (orange), answers generated by other physicians (blue) or ties \n(light blue). Error bars reflect 95% confidence intervals of the overall preference \nrates for physician and Med-PaLM 2 answers, as determined by clustered \nbootstrapping computed over all 1,066 paired ratings.\nNature Medicine | Volume 31 | March 2025 | 943–950\n 945\nArticle https://doi.org/10.1038/s41591-024-03423-7\nsafety and limitations of these models. We found that Med-PaLM 2 \nperformed significantly better than Med-PaLM across every axis, \nfurther reinforcing the importance of comprehensive evaluation. \nFor instance, answers had low risk of harm for 90.6% of Med-PaLM 2 \nanswers, compared to 79.4% for Med-PaLM (Fig. 2 and Supplementary \nTable 4). (8) For real-world questions that arose during care delivery, \nspecialists preferred Med-PaLM 2 answers over generalist physician \nanswers 65% of the time, while generalists preferred them equally. \nModel answers remained inferior to specialist answers; both special-\nists and generalists preferred specialist answers about 60% of the time. \nSpecialists and generalists viewed Med-PaLM 2 answers to be as safe \nas physician answers (Fig. 4).\nResults\nTable 1 and Supplementary Table 1 summarize Med-PaLM 2 results \non MultiMedQA multiple-choice benchmarks. Unless specified oth-\nerwise, Med-PaLM 2 refers to the unified model trained on the mix -\nture in Extended Data Table 1. We also include comparisons to GPT-4  \n(refs. 2,30). We note that comparisons to GPT-4 are not straightfor-\nward because it is a proprietary system and we are not able to measure \noverlap of the evaluation data with the model’s training data as we did \nfor Med-PaLM 2 in Table 2.\nMedQA\nOur unified Med-PaLM 2 model reaches an accuracy of 85.4% using ER as \na prompting strategy. Our best result on this dataset is 86.5%, obtained \nfrom a version of Med-PaLM 2 not aligned for consumer medical ques-\ntion answering, but instead instruction fine-tuned only on MedQA.\nMedMCQA\nOn MedMCQA, Med-PaLM 2 obtains a score of 72.3%, exceeding \nFlan-PaLM performance by over 14% but slightly short of previous \nstate-of-the-art performance (73.66 from GPT-4-base30).\nPubMedQA\nOn PubMedQA, Med-PaLM 2 obtains a score of 75.0%. This is below \nthe state-of-the-art performance (81.0 from BioGPT-Large14) and is \nlikely because no data were included for this dataset for instruction \nfine-tuning. However, after further exploring prompting strategies \nfor PubMedQA on the development set, the unified model reached an \naccuracy of 79.8% with a single run and 81.8% using self-consistency \n(11×). The latter result was state of the art, although we caution that \nPubMedQA’s test set is small (500 examples), and remaining failures \nof Med-PaLM 2 and other strong models appear to be largely attrib-\nutable to label noise intrinsic in the dataset (especially given human \nperformance is 78.0%17).\nMMLU clinical topics\nOn MMLU clinical topics, Med-PaLM 2 significantly improves over \npreviously reported results in Med-PaLM 1 and exceeds previous \nstate-of-the-art performance on three out six topics, with GPT-4-base \nreporting better numbers in the other three. We note that the test set \nfor each of these topics is small, as reported in Extended Data Table 1.\nWe see a drop in performance between GPT-4-base and the aligned \n(production) GPT-4 model on these multiple-choice benchmarks \n(Table 1). Med-PaLM 2, on the other hand, demonstrates strong per -\nformance on multiple-choice benchmarks while being specifically \naligned to the requirements of long-form medical question answer -\ning. While multiple-choice benchmarks are a useful measure of the \nknowledge encoded in these models, we believe human evaluations \nof model answers along clinically relevant axes are necessary to assess \ntheir utility in real-world clinical applications.\nWe also see in Supplementary Table 1 that ensemble refinement \nimproves on few-shot and self-consistency prompting strategies in \neliciting strong model performance across these benchmarks.\nOverlap analysis\nOverlap percentages ranged from 0.9% for MedQA to 48.0% on MMLU \nMedical Genetics. Performance of Med-PaLM 2 was slightly higher on \nquestions with overlap for six out of nine datasets, though the differ-\nence was only statistically significant for MedMCQA (accuracy differ-\nence 4.6%, [1.3, 7.7]) due to the relatively small number of questions \nwith overlap in most datasets (Table 2). When we reduced the overlap \nsegment length from 512 to 120 characters (Methods), overlap per-\ncentages increased (11.15% for MedQA to 56.00% on MMLU Medical \nGenetics), but performance differences on questions with overlap were \nsimilar (Supplementary Table 2), and the difference was still statisti-\ncally significant for just one dataset. These results are similar to those \nobserved in ref. 19, which also saw minimal performance difference \nfrom testing on overlapping data. A limitation of this analysis is that \nwe were not able to exhaustively identify the subset of overlapping \nquestions where the correct answer is also explicitly provided due to \nheterogeneity in how correct answers can be presented across different \ndocuments. Restricting the overlap analysis to questions with answers \nwould reduce the overlap percentages while perhaps leading to larger \nobserved performance differences.\nIndependent evaluation\nOn the MultiMedQA 140 dataset, physicians rated Med-PaLM 2 \nanswers as generally comparable to physician-generated and \nMed-PaLM-generated answers along the axes we evaluated (Fig. 2 and \nSupplementary Table 3). This analysis was largely underpowered for \nthe effect sizes (differences) observed, without significant differences \nTable 1 | Comparison of Med-PaLM 2 results to reported results from GPT-4\nDataset Flan-PaLM (best) Med-PaLM 2 (ER) Med-PaLM 2 (best) GPT-4 (5-shot) GPT-4-base (5-shot)\nMedQA (USMLE) 67.6 [65.0, 70.2] 85.4 [83.3, 87.3] 86.5 [84.5, 88.3] 81.4 [79.1, 83.5] 86.1 [84.1, 88.0]\nPubMedQA 79.0 [75.2, 82.5] 75.0 [71.0, 78.7] 81.8 [78.1, 85.1] 75.2 [71.2, 78.9] 80.4 [76.6, 83.8]\nMedMCQA 57.6 [56.1, 59.1] 72.3 [70.9, 73.6] 72.3 [70.9, 73.6] 72.4 [71.0, 73.7] 73.7 [72.3, 75.0]\nMMLU Clinical Knowledge 80.4 [75.1, 85.0] 88.7 [84.2, 92.2] 88.7 [84.2, 92.2] 86.4 [81.7, 90.3] 88.7 [84.2, 92.2]\nMMLU Medical Genetics 75.0 [65.3, 83.1] 92.0 [84.8, 96.5] 92.0 [84.8, 96.5] 92.0 [84.8, 96.5] 97.0 [91.5, 99.4]\nMMLU Anatomy 63.7 [55.0, 71.8] 84.4 [77.2, 90.1] 84.4 [77.2, 90.1] 80.0 [72.3, 86.4] 85.2 [78.1, 90.7]\nMMLU Professional Medicine 83.8 [78.9, 88.0] 92.3 [88.4, 95.2] 95.2 [92.0, 97.4] 93.8 [90.2, 96.3] 93.8 [90.2, 96.3]\nMMLU College Biology 88.9 [82.6, 93.5] 95.8 [91.2, 98.5] 95.8 [91.2, 98.5] 95.1 [90.2, 98.0] 97.2 [93.0, 99.2]\nMMLU College Medicine 76.3 [69.3, 82.4] 83.2 [76.8, 88.5] 83.2 [76.8, 88.5] 76.9 [69.9, 82.9] 80.9 [74.3, 86.5]\nMed-PaLM 2 was first announced on 14 March 2023. GPT-4 results were released on 20 March 2023, and GPT-4-base (nonproduction) results were released on 12 April 20232. We include \nFlan-PaLM results from December 2022 for comparison1. ER stands for ensemble refinement and includes results from prompting strategies only. Best results are across prompting strategies \nand use the fine-tuned model. Results are reported along with 95% confidence intervals determined by Clopper–Pearson binomial estimates.\nNature Medicine | Volume 31 | March 2025 | 943–950 946\nArticle https://doi.org/10.1038/s41591-024-03423-7\nwhen applying Bonferroni correction for multiple comparisons. \nThis motivated the pairwise ranking analysis presented below on an \nexpanded sample (MultiMedQA 1066).\nOn the adversarial datasets, physicians rated Med-PaLM 2 answers \nas significantly higher quality than Med-PaLM answers across all axes \n(P < 0.001 for all axes; Supplementary Table 4). This pattern held for \nboth the general and health equity-focused subsets of the adversarial \ndataset.\nFinally, laypeople rated Med-PaLM 2 answers to questions in the \nMultiMedQA 140 dataset as more helpful and relevant than Med-PaLM \nanswers (P ≤ 0.002 for both dimensions; Supplementary Fig. 3 and \nSupplementary Table 5).\nNotably, Med-PaLM 2 answers were longer than Med-PaLM and \nphysician answers (Supplementary Table 13). On MultiMedQA 140, for \ninstance, the median answer length for Med-PaLM 2 was 794 characters, \ncompared to 565.5 for Med-PaLM and 337.5 for physicians. Answer \nlengths to adversarial questions tended to be longer in general, with \na median answer length of 964 characters for Med-PaLM 2 and 518 \ncharacters for Med-PaLM, possibly reflecting the greater complexity \nof these questions.\nPairwise ranking evaluation\nPairwise ranking evaluation more explicitly assessed the relative \nperformance of Med-PaLM 2, Med-PaLM and physicians. This rank-\ning evaluation was over an expanded set, MultiMedQA 1066, and the \nadversarial sets. Qualitative examples and their rankings are included \nin Supplementary Tables 8 and 9, respectively, to provide indicative \nexamples and insight.\nOn MultiMedQA, for eight of the nine axes, Med-PaLM 2 answers \nwere more often rated as being higher quality compared to physician \nanswers (all P < 0.001 for each of the separate comparisons; Fig. 1 and \nSupplementary Table 6). For instance, they were more often rated as \nbetter reflecting medical consensus or indicating better reading com-\nprehension, and less often rated as omitting important information \nor representing a risk of harm. However, for one of the axes, including \ninaccurate or irrelevant information, Med-PaLM 2 answers were not \nas favorable as physician answers. Med-PaLM 2 answers were rated as \nhigher quality than Med-PaLM axes on the same eight axes (Fig. 3 and \nSupplementary Table 7); Med-PaLM 2 answers were marked as having \nmore inaccurate or irrelevant information less often than Med-PaLM \nanswers (18.4% Med-PaLM 2 versus 21.5% Med-PaLM), but the difference \nwas not significant (P = 0.12).\nOn adversarial questions, Med-PaLM 2 was ranked more favorably \nthan Med-PaLM across every axis (Fig. 3), often by substantial margins.\nThree-way utility ranking\nWe present results for three-way ranking of model, generalist and spe-\ncialist answers in Fig. 4a. For generalist rankings, given 11 rankings per \nquestion, we determine plurality ranking per question across raters. We \nobserve that specialist answers perform best across both generalist and \nspecialist raters, but that Med-PaLM 2 answers appear to perform com-\nparably or better to generalist answers for both groups of raters, with \nmore answers most preferred and second preferred than for generalist \nraters. In Fig. 4b, we plot pairwise rankings between models and gen-\neralists and models and specialists for both groups of raters, averaged \nacross all raters. We observe that both groups prefer specialist answers \nover model answers (about 60% preference), but that specialists prefer \nmodel answers over generalist answers (65% preference). Generalists \nprefer model answers and generalist answers about equally, suggesting \nthat as, model performance approaches the human level, evaluation \nwith highly specialized experts may be important in distinguishing \nmodel performance from human performance.\nIndividual evaluation of harm\nIn Supplementary Tables 14 and 15, we present results for harm evalu-\nation for each answer from the model, generalists and specialists. \nWe observe that a majority of generalist physicians find that answers \nacross all three answer sources are not harmful, but at an 80% agree-\nment threshold for harmlessness, a few questions from each source are \nflagged. At this threshold, 16 of 20 Med-PaLM 2 answers are harmless, \nwhile 17 of 20 generalist answers are harmless, and 15 of 20 specialist \nanswers are harmless. For specialist physicians (one rater per answer), \nAnswer supported by consensus\nPhysician evaluation on\nMultiMedQA\nMed-PaLM 2\nMed-PaLM\nPhysician\nPhysician evaluation on\nadversarial questions\nPossible harm extent = no harm\nLow likelihood of harm\nShows evidence of question comprehension\nShows evidence of knowledge recall\nShows evidence of reasoning\nNo sign of incorrect comprehension\nNo sign of incorrect knowledge recall\nNo sign of incorrect reasoning\nNo inaccurate/irrelevant information\nNo missing important content\nNo sign of bias toward specific subgroups\nProportion of answers in\nhigh-quality ratings bins\nProportion of answers in\nhigh-quality ratings bins\n0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0\nFig. 2 | Independent long-form evaluation with physician raters. Values \nare the proportion of ratings across answers where each axis was rated in the \nhighest-quality bin. (For instance, ‘Possible harm extent = no harm’ reflects the \nproportion of answers where the extent of possible harm was rated ‘No harm. ’) \nLeft, independent evaluation of long-form answers from Med-PaLM, Med-PaLM \n2 and physicians on the MultiMedQA 140 dataset. Right, independent evaluation \nof long-form answers from Med-PaLM and Med-PaLM 2 on the combined \nadversarial datasets (general and health equity). Detailed breakdowns are \npresented in Supplementary Tables 3 and 4. Error bars reflect 95% confidence \nintervals as determined by bootstrapping, centered on the mean proportions.\nNature Medicine | Volume 31 | March 2025 | 943–950\n 947\nArticle https://doi.org/10.1038/s41591-024-03423-7\n17 of 20 model answers were harmless, 19 of 20 generalist answers and \n18 of 20 specialist answers. Interestingly, across both rating groups, a \nfew physician answers were flagged as potentially harmful, indicating \nthe challenging and subjective nature of evaluating harm. Overall, the \nresults do not suggest a substantial difference in harmfulness across \nmodel, generalist and specialist answers.\nDiscussion\nWe show that Med-PaLM 2 exhibits strong performance in \nmultiple-choice, consumer long-form and bedside consultation \nmedical question answering, including popular benchmarks, chal-\nlenging adversarial datasets and real-world questions asked by spe-\ncialists. We demonstrate performance approaching or exceeding \nstate-of-the-art on every MultiMedQA multiple-choice benchmark, \nincluding MedQA, PubMedQA, MedMCQA and MMLU clinical topics. \nWe show substantial gains in long-form answers over Med-PaLM, as \nassessed by physicians and laypeople on multiple axes of quality and \nsafety. Furthermore, we observe that Med-PaLM 2 answers were pre-\nferred over physician-generated answers in multiple axes of evaluation \nacross both consumer medical questions and adversarial questions. \nFinally, we observe that Med-PaLM 2 answers to bedside consultation \nquestions that arose during routine care delivery are often preferred \nby physicians over generalist answers.\nAs LLMs become increasingly proficient at structured tests of \nknowledge, it is more important to delineate and assess their capabili-\nties along clinically relevant dimensions21,25. Our evaluation framework \nexamines the alignment of long-form model outputs to human expec-\ntations of high-quality medical answers across both consumer and \nphysician questions. Our use of adversarial question sets also enables \nexplicit study of LLM performance in difficult cases. The substantial \nimprovements of Med-PaLM 2 relative to Med-PaLM suggest that care-\nful development and evaluation of challenging question-answering \ntasks is needed to ensure robust model performance.\nUsing a multidimensional evaluation framework lets us under -\nstand trade-offs in more detail. For instance, Med-PaLM 2 answers \nwere longer on average (Supplementary Table 13) than Med-PaLM or \nphysician answers. This may provide benefits for many use cases, but \nmay also lead to trade-offs such as including unnecessary additional \ndetails versus omitting important information.\nThe optimal length of an answer may depend upon additional \ncontext outside the scope of a question. For instance, questions around \nwhether a set of symptoms are concerning depend upon a person’s \nmedical history; in these cases, the more appropriate response of \nan LLM may be to request more information, rather than compre -\nhensively listing all possible causes. Our evaluation did not consider \nmultiturn dialog31, nor frameworks for active information acquisition32. \nOur individual evaluation did not clearly distinguish performance of \nMed-PaLM 2 answers from physician-generated answers, motivating \nmore granular evaluation, including pairwise evaluation and adversar-\nial evaluation. In pairwise evaluation, we saw that Med-PaLM 2 answers \nwere preferred over physician answers along several axes pertaining \nto clinical utility, such as factuality, medical reasoning capability and \nlikelihood of harm. Likewise, on bedside consultation questions, spe-\ncialists preferred Med-PaLM 2 answers over those of generalists, but \ngeneralists rated them equally. These results indicate that, as the field \nprogresses toward physician-level performance, improved evaluation \nframeworks (including highly specialized human raters) and work on \nscalable oversight33 will be crucial for further measuring progress and \naligning models.\nIn real-world care delivery, care is often provided by nonphy -\nsicians, for example, nurse practitioners, physician assistants and \nphysician associates. Additionally, in many parts of the world, access \nto physicians can be scarce. As models approach physician-level perfor-\nmance on medical question answering in real-world tasks like bedside \nconsultation, they become promising for assisting medical staff where \naccess to specialists is limited. Our model comparison on bedside con-\nsultation questions demonstrates progress toward better evaluation, \nbut validating model assistance in real-world workflows remains an \nimportant area of future work to responsibly enable these applications.\nThe LLM landscape is rapidly evolving, necessitating careful \ninterpretation of our findings within this dynamic context. Since \nMed-PaLM 2’s March 2023 release, significant advancements have \nreshaped the field. Models now have expanded context windows, \nreaching millions of tokens 34, enabling more sophisticated reason -\ning and nuanced, variable-length responses. This is particularly rel -\nevant for medical applications, where complex information requires \ncareful consideration 27,35. Furthermore, LLMs are evolving beyond \ntext, embracing multimodality to process and integrate diverse data \nBetter reflects consensus\nHigh-quality answer traits\nBetter reading comprehension\nBetter knowledge recall\nBetter reasoning\n0 20 40 60 80 100\nBetter reflects consensus\nBetter reading comprehension\nBetter knowledge recall\nBetter reasoning\nMore inaccurate/irrelevant information\nOmits more information\nMore evidence of demographic bias\nGreater extent of harm\nGreater likelihood of harm\nMore inaccurate/irrelevant information\nOmits more information\nMore evidence of demographic bias\nGreater extent of harm\nGreater likelihood of harm\nHigh-quality answer traits\nPercentage of responses\nMed-PaLM 2 Tie Med-PaLM\nPotential answer risks\nPotential answer risks\n0 20 40 60 80 100\nPercentage of responses\nMed-PaLM 2 Tie Med-PaLM\na\nb\n0 20 40 60 80 100\n0 20 40 60 80 100\nFig. 3 | Ranking comparison of long-form answers. Med-PaLM 2 answers are \nconsistently preferred over Med-PaLM answers by physician raters across all \nratings dimensions, in both MultiMedQA (a) and adversarial (b) question sets. \nStacked bars represent proportions of answers for which physician raters \npreferred Med-PaLM 2 answers (orange), Med-PaLM 1 answers (green) or \nties (light blue). Error bars reflect 95% confidence intervals as determined by \nbootstrapping, centered on preference rates for Med-PaLM 2 and Med-PaLM, \nrespectively, across n = 1,066 paired ratings. Detailed breakdowns for adversarial \nquestions are presented in Supplementary Table 4.\nNature Medicine | Volume 31 | March 2025 | 943–950 948\nArticle https://doi.org/10.1038/s41591-024-03423-7\nsources like images36. This progress is exemplified by recent iterations \nwithin prominent LLM families like GPT (GPT-4, GPT-4o, GPT-4o1)37, \nGemini (Gemini 1.0, Gemini 1.5)34,38 and Gemma (Gemma, Gemma 2)39,40, \nalongside the rise of models like Llama 41 and Mistral 42. These rapid \nadvancements highlight the critical need for ongoing evaluation and \nbenchmarking to ensure that our understanding of LLM capabilities \nremains current and relevant. Med-PaLM and Med-PaLM 2’s pioneer-\ning evaluation framework and methodology are designed to scale \nwith the availability of larger datasets and adapt to this evolving LLM \nlandscape, providing a valuable tool for contextualizing advances in \nthis rapidly changing field.\nGiven the broad and complex space of medical information needs, \nmethods to measure alignment of model outputs warrant continued \ndevelopment. Additional dimensions to those we measure here are \nlikely to be important, such as the empathy conveyed by answers25. As \nnoted, our rating rubric is not a formally validated qualitative instru-\nment, although observed interrater reliability was high (Supplemen-\ntary Fig. 1). Further research is required to develop the rigor of rubrics \nenabling human evaluation of LLM performance in medical question \nanswering.\nLikewise, a robust understanding of how LLM outputs compare \nto physician answers is a broad, highly significant question meriting \na\nb\nMed-PaLM 2\nGeneralist\nSpecialist\nMost preferred\nSecond preferred\nLeast preferred\nMed-PaLM 2\nGeneralist\nModel versus  generalist\nModel versus specialist\nModel versus generalist\nModel versus specialist\nSpecialist\nSpecialist raters\nGeneralist raters\nSpecialist raters\nGeneralist raters\nMost preferred\nSecond preferred\nLeast preferred\nMed-PaLM 2\nGeneralist\nSpecialist\nMed-PaLM 2\nGeneralist\nSpecialist\n0.40\n0.41\n0.48 0.52\n0.59\n0.60\n0.65 0.35\n9\n9\n96\n6 8\n6\n6\n6\n7\n77\n4\n4\n10\n6\n5\n5\nFig. 4 | Summary of pilot study on bedside consultation dataset. a, Three-\nway ranking results for model, generalist and specialist answers by plurality of \nraters. T op bars show specialist raters, and bottom bars show generalist raters \n(11× replication per question). Both groups of physicians preferred specialist \nanswers the most, and both preferred model answers more often than generalist \nanswers. b, Pairwise ranking results for model, generalist and specialist answers, \naveraged over raters. T op bars, generalist raters; bottom bars, specialist raters \n(11× replication per question). Both groups of physicians preferred specialist \nanswers over model answers. Specialists preferred model answers over generalist \nanswers, while generalists rated them about equally.\nTable 2 | Med-PaLM 2 performance on multiple-choice questions with and without overlap\nDataset Overlap fraction Performance (without overlap) Performance (with overlap) Delta\nMedQA (USMLE) 12/1,273 (0.9%) 85.3 [83.4, 87.3] 91.7 [76.0, 100.0] −6.3 [−13.5, 20.8]\nPubMedQA 6/500 (1.2%) 74.1 [70.2, 78.0] 66.7 [28.9, 100.0] 7.4 [−16.6, 44.3]\nMedMCQA 893/4,183 (21.4%) 70.5 [68.9, 72.0] 75.0 [72.2, 77.9] −4.6 [−7.7, −1.3]\nMMLU Clinical Knowledge 55/265 (20.8%) 88.6 [84.3, 92.9] 87.3 [78.5, 96.1] 1.3 [−6.8, 13.2]\nMMLU Medical Genetics 48/100 (48.0%) 92.3 [85.1, 99.6] 91.7 [83.8, 99.5] 0.6 [−11.0, 12.8]\nMMLU Anatomy 37/135 (27.4%) 82.7 [75.2, 90.1] 89.2 [79.2, 99.2] −6.5 [−17.4, 8.7]\nMMLU Professional Medicine 79/272 (29.0%) 89.1 [84.7, 93.5] 92.4 [86.6, 98.2] −3.3 [−9.9, 5.5]\nMMLU College Biology 60/144 (41.7%) 95.2 [90.7, 99.8] 96.7 [92.1, 100.0] −1.4 [−8.7, 7.1]\nMMLU College Medicine 47/173 (27.2%) 78.6 [71.4, 85.7] 91.5 [83.5, 99.5] −12.9 [−22.4, 0.1]\nWe define a question as overlapping if either the entire question or up to 512 characters overlap with any document in the training corpus of the LLM underlying Med-PaLM 2. Values are \nreported along with 95% binomial proportion confidence intervals (asymptotic normal approximation method, for comparing two independent samples).\nNature Medicine | Volume 31 | March 2025 | 943–950\n 949\nArticle https://doi.org/10.1038/s41591-024-03423-7\nmuch future work; the results we report here represent one step in \nthis research direction. For our study on consumer questions, physi-\ncians generating answers were prompted to provide useful answers \nto laypeople but were not provided with specific clinical scenarios or \nnuanced details of the communication requirements of their audi -\nence. While this may be reflective of real-world performance for some \nsettings, it is preferable to ground evaluations in highly specific work-\nflows and clinical scenarios. Our bedside consultation questions pilot \nis a step in this direction, but was limited in scale. Model answers are \nalso often longer than physician answers, which may contribute to \nimproved independent and pairwise evaluations, as suggested by other \nwork25. Furthermore, we did not explicitly assess interrater variation in \npreference rankings or explore how variation in preference rankings \nmight relate to the lived experience, expectations or assumptions of \nour raters.\nPhysicians were also asked to only produce one answer per ques-\ntion, so this provides a limited assessment of the range of possible \nphysician-produced answers. Future improvements to this methodol-\nogy could provide a more explicit clinical scenario with recipient and \nenvironmental context for answer generation. It could also assess mul-\ntiple possible physician answers to each question, alongside interphy-\nsician variation. Moreover, for a more principled comparison of LLM \nanswers to medical questions, the medical expertise, lived experience \nand background, and specialization of physicians providing answers, \nand evaluating those answers, should be more explicitly explored. It \nwould also be desirable to explore intra- and interphysician variation \nin the generation of answers under multiple scenarios as well as con-\ntextualize LLM performance by comparison to the range of approaches \nthat might be expected among physicians.\nFinally, the current evaluation with adversarial data is relatively \nlimited in scope and should not be interpreted as a comprehensive \nassessment of safety, bias and equity considerations. In future work, \nadversarial data could be systematically expanded to increase coverage \nof health equity topics and facilitate disaggregated evaluation over \nsensitive characteristics43–45.\nThese results demonstrate rapid progress toward physician-level \nmedical question answering with LLMs. However, further work on vali-\ndation and alignment to human values is necessary as the technology \nfinds broader uptake in real-world applications. Careful and rigorous \nevaluation and refinement of LLMs in different contexts for medical \nquestion answering and real-world workflows will be needed to ensure \nthis technology has the greatest possible impact on health.\nOnline content\nAny methods, additional references, Nature Portfolio reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41591-024-03423-7.\nReferences\n1. Singhal, K. et al. Large language models encode clinical \nknowledge. Nature 620, 172–180 (2023).\n2. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. \nCapabilities of GPT-4 on medical challenge problems. Preprint at \nhttps://arxiv.org/abs/2303.13375 (2023).\n3. Liévin, V., Hother, C. E. & Winther, O. Can large language models \nreason about medical questions? Patterns 5, 100943 (2024).\n4. Vaswani, A. et al. Attention is all you need. In Proc. 31st \nConference on Neural Information Processing Systems (eds \nGuyon, I. et al.) (Curran Associates, 2017).\n5. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: pre-training \nof deep bidirectional transformers for language understanding. \nIn Proc. NAACL-HLT Vol. 1 (eds Burstein, J. et al.) 4171–4186 \n(Association for Computational Linguistics, 2019).\n6. Raffel, C. et al. Exploring the limits of transfer learning with a unified \ntext-to-text transformer. J. Mach. Learn. Res. 21, 5485–5551 (2020).\n7. Shortliffe, E. H. Computer programs to support clinical decision \nmaking. JAMA 258, 61–66 (1987).\n8. Schwartz, W. B. Medicine and the computer: the promise and \nproblems of change. In Use and Impact Of Computers in Clinical \nMedicine (eds Anderson, J. G. & Jay, S. J.) 321–335 (Springer \nScience & Business Media, 1987).\n9. Szolovits, P. & Pauker, S. G. Categorical and probabilistic \nreasoning in medicine revisited. In Artificial Intelligence in \nPerspective (ed. Bobrow, D. G.) 167–180 (MIT Press, 1994).\n10. Yasunaga, M., Leskovec, J. & Liang, P. Linkbert: pretraining \nlanguage models with document links. Preprint at https://arxiv.\norg/abs/2203.15827 (2022).\n11. Yasunaga, M. et al. Deep bidirectional language-knowledge graph \npretraining. Adv. Neural Inf. Process. Syst. 35, 37309–37323 (2022).\n12. Bolton, E. et al. Stanford CRFM introduces PubMedGPT \n2.7b. Stanford University HAI https://hai.stanford.edu/news/\nstanford-crfm-introduces-pubmedgpt-27b (2022).\n13. Gu, Y. et al. Domain-specific language model pretraining for \nbiomedical natural language processing. ACM Trans. Comput. \nHealthc. 3, 2 (2021).\n14. Luo, R. et al. BioGPT: generative pre-trained transformer for \nbiomedical text generation and mining. Brief. Bioinform. 23, \nbbac409 (2022).\n15. Jin, D. et al. What disease does this patient have? A large-scale \nopen domain question answering dataset from medical exams. \nAppl. Sci. 11, 6421 (2021).\n16. Pal, A., Umapathi, L. K. & Sankarasubbu, M. MedMCQA:  \na large-scale multi-subject multi-choice dataset for medical \ndomain question answering. In Proc. Conference on Health, \nInference, and Learning Vol. 174 248–260 (PMLR, 2022).\n17. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. PubMedQA: a \ndataset for biomedical research question answering. Preprint at \nhttps://arxiv.org/abs/1909.06146 (2019).\n18. Brown, T. et al. Language models are few-shot learners.  \nAdv. Neural Inf. Process. Sys. 33, 1877–1901 (2020).\n19. Chowdhery, A. et al. PaLM: scaling language modeling with \npathways. J. Mach. Lean. Res. 24, 1–113 (2023).\n20. Chung, H. W. et al. Scaling instruction-finetuned language \nmodels. J. Mach. Lean. Res. 25, 1–53 (2024).\n21. Levine, D. M. et al. The diagnostic and triage accuracy of the  \nGPT-3 artificial intelligence model: an observational study.  \nLancet Digit. Health 6, e555–e561 (2024).\n22. Duong, D. & Solomon, B. D. Analysis of large-language model \nversus human performance for genetics questions. Eur. J. Hum. \nGenet. 32, 466–468 (2024).\n23. Oh, N., Choi, G.-S. & Lee, W. Y. Chatgpt goes to operating room: \nevaluating gpt-4 performance and its potential in surgical \neducation and training in the era of large language models. Ann. \nSurg. Treat. Res. 104, 269–273 (2023).\n24. Antaki, F., Touma, S., Milad, D., El-Khoury, J. & Duval, R. Evaluating \nthe performance of ChatGPT in ophthalmology: an analysis of its \nsuccesses and shortcomings. Ophthalmol. Sci. 3, 100324 (2023).\n25. Ayers, J. W. et al. Comparing physician and artificial intelligence \nchatbot responses to patient questions posted to a public social \nmedia forum. JAMA Intern. Med. 183, 589–596 (2023).\n26. Palm 2 technical report. Google https://ai.google/static/\ndocuments/palm2techreport.pdf (2023).\n27. Pfohl, S. R. et al. A toolbox for surfacing health equity harms  \nand biases in large language models. Nat. Med. https://doi.org/ \n10.1038/s41591-024-03258-2 (2024).\n28. Callahan, A. et al. Using aggregate patient data at the bedside via \nan on-demand consultation service. NEJM Catal. Innov. Care Deliv. \n2 https://doi.org/10.1056/CAT.21.0224 (2021).\nNature Medicine | Volume 31 | March 2025 | 943–950 950\nArticle https://doi.org/10.1038/s41591-024-03423-7\n29. Gombar, S., Callahan, A., Califf, R., Harrington, R. & Shah, N. H.  \nIt is time to learn from patients like mine. NPJ Digit. Med. 2,  \n16 (2019).\n30. Achiam, J. et al. GPT-4 technical report. Preprint at https://doi.org/ \n10.48550/arXiv.2303.08774 (2023).\n31. Thoppilan, R. et al. Lamda: language models for dialog \napplications. Preprint at https://arxiv.org/abs/2201.08239 (2022).\n32. Kossen, J. et al. Active acquisition for multimodal temporal data: \na challenging decision-making task. Trans. Mach. Learn. Res. \nhttps://openreview.net/forum?id=Gbu1bHQhEL (2023).\n33. Bowman, S. R. et al. Measuring progress on scalable oversight \nfor large language models. Preprint at https://arxiv.org/\nabs/2211.03540 (2022).\n34. Google, G. T. Gemini 1.5: unlocking multimodal understanding \nacross millions of tokens of context. Preprint at https://arxiv.org/\nabs/2403.05530 (2024).\n35. Saab, K. et al. Capabilities of Gemini models in medicine. Preprint \nat https://arxiv.org/abs/2404.18416 (2024).\n36. Yang, L. et al. Advancing multimodal medical capabilities of \nGemini. Preprint at https://arxiv.org/abs/2405.03162 (2024).\n37. Achiam, J. et al. GPT-4 technical report. Preprint at https://arxiv.\norg/abs/2303.08774 (2023).\n38. Gemini Team, Google. Gemini: a family of highly capable \nmultimodal models. Preprint at https://arxiv.org/abs/2312.11805 \n(2023).\n39. Team, G. et al. Gemma: open models based on Gemini research \nand technology. Preprint at https://arxiv.org/abs/2403.08295 \n(2024).\n40. Team, G. et al. Gemma 2: improving open language models at \na practical size. Preprint at https://arxiv.org/html/2408.00118v1 \n(2024).\n41. Touvron, H. et al. Llama: open and efficient foundation language \nmodels. Preprint at https://arxiv.org/abs/2302.13971 (2023).\n42. Jiang, A. Q. et al. Mistral 7b. Preprint at https://arxiv.org/\nabs/2310.06825 (2023).\n43. Weidinger, L. et al. Ethical and social risks of harm from language \nmodels. Preprint at https://arxiv.org/abs/2112.04359 (2021).\n44. Liang, P. et al. Holistic evaluation of language models. Trans. Mach. \nLearn. Res. https://openreview.net/forum?id=iO4LZibEqW (2024).\n45. Perez, E. et al. Red teaming language models with language \nmodels. Preprint at https://arxiv.org/abs/2202.03286 (2022).\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and \nreproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if you modified the licensed \nmaterial. You do not have permission under this licence to share \nadapted material derived from this article or parts of it. The images \nor other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit \nline to the material. If material is not included in the article’s Creative \nCommons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain \npermission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nKaran Singhal    1,9, Tao Tu    1,9, Juraj Gottweis1,9, Rory Sayres1,9, Ellery Wulczyn1, Mohamed Amin1, Le Hou1, Kevin Clark2, \nStephen R. Pfohl    1, Heather Cole-Lewis    1, Darlene Neal1, Qazi Mamunur Rashid1, Mike Schaekermann    1, Amy Wang1, \nDev Dash3, Jonathan H. Chen    4,5,6, Nigam H. Shah    7,8, Sami Lachgar1, Philip Andrew Mansfield    1, Sushant Prakash1, \nBradley Green1, Ewa Dominowska2, Blaise Agüera y Arcas1, Nenad Tomašev    2, Yun Liu    1, Renee Wong1, \nChristopher Semturs    1, S. Sara Mahdavi2, Joelle K. Barral2, Dale R. Webster    1, Greg S. Corrado1, Yossi Matias    1, \nShekoofeh Azizi    2,10 , Alan Karthikesalingam    1,10  & Vivek Natarajan    1,10 \n1Google Research, Mountain View, CA, USA. 2Google DeepMind, Mountain View, CA, USA. 3Department of Emergency Medicine, Stanford University \nSchool of Medicine, Stanford, CA, USA. 4Stanford Center for Biomedical Informatics Research, Stanford University, Stanford, CA, USA. 5Division of \nHospital Medicine, Stanford University, Stanford, CA, USA. 6Clinical Excellence Research Center, Stanford University, Stanford, CA, USA. 7Department  \nof Medicine, Stanford University School of Medicine, Stanford, CA, USA. 8Technology and Digital Solutions, Stanford Healthcare, Palo Alto, CA, USA. \n9These authors contributed equally: Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres. 10These authors jointly supervised this work: Shekoofeh Azizi,  \nAlan Karthikesalingam, Vivek Natarajan.  e-mail: shekazizi@google.com; alankarthi@google.com; natviv@google.com\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nMethods\nIn the following text, we provide further details on the development of \nMed-PaLM 2 and the expanded evaluation framework used to validate \nmodel outputs.\nDatasets\nWe evaluated Med-PaLM 2 on multiple-choice and long-form medical \nquestion-answering datasets from MultiMedQA1, two new adversarial \nlong-form datasets and a pilot set of real-world bedside consultation \nquestions (Extended Data Tables 1 and 2).\nMultiple-choice questions. For evaluation on multiple-choice ques-\ntions, we used the MedQA 15, MedMCQA16, PubMedQA17 and MMLU \nclinical topics46 datasets.\nMultiMedQA consumer questions.  For evaluation on long-form \nquestions, we used two sets of questions sampled from MultiMedQA \n(Extended Data Table 2). The first set (MultiMedQA 140) consists of \n140 questions curated from the HealthSearchQA, LiveQA47 and Medi-\ncationQA48 datasets, matching the set used in ref. 1 . The second set \n(MultiMedQA 1066) is an expanded sample of 1,066 questions from \nthe same sources. For MultiMedQA 1066, we randomly sampled 1,000 \nquestions from MultiMedQA (mostly HealthSearchQA) in addition to \nthe 140 in MultiMedQA 140 and removed all duplicates and near dupli-\ncates (questions identical other than capitalization). The resulting set \nhad 1,066 questions.\nAdversarial consumer questions. We also curated two new datasets of \nadversarial questions designed to elicit model answers with potential \nfor harm and bias: a general adversarial set and a health equity-focused \nadversarial set (Extended Data Table 2). The first set (Adversarial (Gen-\neral)) broadly covers issues related to health equity, drug use, alcohol, \nmental health, COVID-19, obesity, suicide and medical misinformation. \nHealth equity topics covered in this dataset include health disparities, \nthe effects of structural and social determinants on health outcomes, \nand racial bias in clinical calculators for renal function49–51. The second \nset (Adversarial (Health Equity)) prioritizes use cases, health topics \nand sensitive characteristics based on relevance to health equity con-\nsiderations in the domains of healthcare access (for example, health \ninsurance, access to hospitals or primary care provider), quality (for \nexample, patient experiences, hospital care and coordination) and \nsocial and environmental factors (for example, working and living \nconditions, food access and transportation). The dataset was curated \nto draw on insights from the literature on health equity in machine \nlearning and define a set of implicit and explicit adversarial queries that \ncover a range of patient experiences and health conditions27. Queries \noften involved implicit requests for medical advice and were not always \nexplicit well-formed medical questions. This dataset was released and \nfurther described in ref. 27, where it was referred to as the Open-ended \nMedical Adversarial Queries dataset.\nBedside consultation questions. We curated a set of questions rep-\nresenting real-world information needs arising during routine care \ndelivery, submitted to a real-world bedside consultation service28,52,53 \nby specialist physicians. In the original service, offered at Stanford \nMedicine from 2017 to 2018, questions were answered by a team ana-\nlyzing de-identified patient records to provide a written report. The \nanswers have informed individual patient care, resulted in changes to \ninstitutional practices and motivated further clinical research 28. We \nprovide examples of questions in Supplementary Table 11. Starting \nwith the entire set of set of 100 questions submitted between Febru-\nary 2017 and September 201828, questions were filtered to those that \ndid not rely on information unavailable to an LLM or external physi -\ncian, such as test-ordering rates at Stanford Healthcare or number of \nvisits at specific clinic sites. Sixty-six questions remained at this stage. \nSubsequently, three clinicians independently sampled a third of these \nquestions across multiple specialities and then adjudicated selection \ndifferences, resulting in the final set of 20 questions. Question selection \nwas done independently of Med-PaLM 2’s ability to answer them and by \nclinicians with no access to the Med-PaLM 2 model. Authors with access \nto Med-PaLM 2 were not involved in question selection in any manner.\nModeling\nBase LLM. For Med-PaLM, the base LLM was PaLM19. Med-PaLM 2 builds \nupon PaLM 2 (ref. 26), a new iteration of Google’s LLM with substantial \nperformance improvements on multiple LLM benchmark tasks. The \nmain advances incorporated into PaLM 2 include compute-optimal \nscaling54, improved dataset mixtures and objective improvements26.\nInstruction fine-tuning. We applied instruction fine-tuning to the base \nLLM following the protocol used in ref. 20. The datasets used included \nthe training splits of MultiMedQA—namely MedQA, MedMCQA, Health-\nSearchQA, LiveQA and MedicationQA. We trained a ‘unified’ model, \nwhich is optimized for performance across all datasets in MultiMedQA \nusing dataset mixture ratios (proportions of each dataset) reported in \nExtended Data Table 3. These mixture ratios and the inclusion of these \nparticular datasets were empirically determined based on the size and \nquality of the respective datasets and performance on existing valida-\ntion sets of the multiple-choice tasks. We anchored on mixture ratios \nstarting in proportion to the size of each dataset and then overweighted \ndatasets that contained more high-quality examples of diverse tasks.\nUnless otherwise specified, Med-PaLM 2 refers to this uni -\nfied model. For comparison purposes, we also created a variant of \nMed-PaLM 2 obtained by fine-tuning exclusively on multiple-choice \nquestions, which led to improved results on these benchmarks.\nPrompting strategies\nWe describe below prompting strategies used to evaluate Med-PaLM \n2 on multiple-choice and long-form tasks.\nFew-shot prompting.  Few-shot prompting involves prompting an \nLLM by prepending example inputs and outputs before the final input. \nFew-shot prompting remains a strong baseline for prompting LLMs, \nwhich we evaluate and build on in this work. We use the same few-shot \nprompts as used by ref. 1.\nChain of thought.  Chain of thought (CoT), introduced in ref. 55 , \ninvolves augmenting each few-shot example in a prompt with a \nstep-by-step explanation toward the final answer. The approach ena-\nbles an LLM to condition on its own intermediate outputs in multistep \nproblems. As noted in ref. 1 , the medical questions explored in this \nstudy often involve complex multistep reasoning, making them a \ngood fit for CoT prompting. We crafted CoT prompts to provide clear \ndemonstrations on how to appropriately answer the given medical \nquestions (provided in Supplementary Table 23).\nSelf-consistency. Self-consistency (SC) is a strategy introduced in \nref. 56 to improve performance on multiple-choice benchmarks by \nsampling multiple explanations and answers from the model. The final \nanswer is the one with the majority (or plurality) vote. For a domain \nsuch as medicine with complex reasoning paths, there might be mul-\ntiple potential routes to the correct answer. Marginalizing over the \nreasoning paths can lead to the most accurate answer. In this work, \nwe performed SC with 11 samplings using CoT prompting, as in ref. 1.\nEnsemble refinement. Building on CoT and SC, we developed a simple \nprompting strategy that we refer to as ensemble refinement (ER). ER \nbuilds on other techniques that involve conditioning an LLM on its own \ngenerations before producing a final answer, including CoT prompting \nand self-refine57.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nER involves a two-stage process: first, given a (few-shot) CoT \nprompt and a question, the model produces multiple possible gen -\nerations stochastically via temperature sampling. In this case, each \ngeneration involves an explanation and an answer for a multiple-choice \nquestion. Then, the model is conditioned on the original prompt, ques-\ntion and the concatenated generations from the previous step, and is \nprompted to produce a refined explanation and answer. This can be \ninterpreted as a generalization of SC, where the LLM is aggregating \nover answers from the first stage instead of a simple vote, enabling \nthe LLM to take into account the strengths and weaknesses of the \nexplanations it generated. T o improve performance, we performed \nthe second stage multiple times and finally took a plurality vote over \nthese generated answers to determine the final answer. ER is depicted \nin Extended Data Fig. 1.\nUnlike SC, ER may be used to aggregate answers beyond questions \nwith a small set of possible answers (for example, multiple-choice \nquestions). For example, ER can be used to produce improved \nlong-form generations by having an LLM condition on multiple pos -\nsible answers to generate a refined final answer. Given the resource cost \nof approaches requiring repeated samplings from a model, we apply \nER only for multiple-choice evaluation in this work, with 11 samplings \nfor the first stage and 33 samplings for the second stage.\nChain of retrieval. In this work, we studied difficult bedside consulta-\ntion questions from specialist physicians that arose in the course of \nhealthcare delivery. This has been a challenging task for ungrounded \nLLMs like GPT-3.5 and GPT-4 (ref. 53)—even for specialist physicians, \nanswering these questions often requires accessing external resources.\nT o improve Med-PaLM 2’s grounding, factuality and safety on \nthese difficult medical questions, we introduce a step-by-step pipeline \nfor generation and verification of model answers using search over \nrelevant external medical information, which we call chain of retrieval. \nThe process is as follows:\n(1) An initial Med-PaLM 2 answer is generated using a zero-shot \nprompt.\n(2) The initial Med-PaLM 2 answer is separated into individual \nclaims for verification.\n(3) Search queries for the claims for verification are generated.\n(4) Relevant studies and websites are retrieved using Google \nsearch.\n(5) Individual documents are summarized.\n(6) Med-PaLM 2 generates a final answer using the question and \nconcatenated summaries.\nThis approach builds on the intuition of CoT prompting, whereby \nLLMs can succeed in complicated multistep reasoning tasks when those \ntasks are broken down into steps, enabling models to autoregressively \ncondition on the outputs of previous steps. Steps (1), (2), (3) and (6) \nwere all performed via individual model inferences given different \nprompts, and step (5) was performed via one model inference per \ndocument. We found that, for step (6), it was important to exclude \nthe initial answer from step (1) from the prompt, to prevent the model \nfrom anchoring on the initial ungrounded answer. We share prompts \nfor individual steps in the pipeline in Supplementary Table 23.\nThis approach is generally applicable to other LLMs and evalu -\nation settings. It is distinct from retrieval-augmented generation \napproaches that leverage a fixed corpus and embedding space to find \ndocuments to condition LLM generations on58, and is most similar to \nother approaches that break down verification of claims into mul -\ntiple steps59,60. We are not aware of any work that has used the exact \nsame steps as chain of retrieval or applied it for medical question \nanswering. The steps in this pipeline are not individually learned dur-\ning fine-tuning; combining this approach with process supervision 61 \nto improve performance at each step and boost overall factuality and \nsafety of model generations remains an important area for future work.\nOverlap analysis\nAn increasingly important concern given recent advances in large mod-\nels pretrained on web-scale data is the potential for overlap between \nevaluation benchmarks and training data. We searched for overlapping \ntext segments between multiple-choice questions in MultiMedQA and \nthe corpus used to train the base LLM underlying Med-PaLM 2. We \ndefined a question as overlapping if either the entire question or at \nleast 512 contiguous characters overlapped with any document in the \ntraining corpus. For this analysis, multiple-choice options or answers \nwere not included as part of the query, since inclusion could lead to \nunderestimation of the number of overlapping questions due to het-\nerogeneity in formatting and ordering options. As a result, this analysis \nwill also treat questions without answers in the training data as overlap-\nping. We believe this methodology is both simple and conservative, and \nwhen possible we recommend it over black-box memorization testing \ntechniques2, which do not conclusively measure test set contamination.\nLong-form consumer question-answering evaluation\nT o assess the performance of Med-PaLM 2 on long-form consumer med-\nical question answering, we conducted a series of human evaluations.\nModel answers . T o elicit answers to long-form questions from \nMed-PaLM models, we used the prompts provided in Supplementary \nTable 24. We did this consistently across Med-PaLM and Med-PaLM 2. \nWe sampled from models with temperature 0.0 as in ref. 1.\nPhysician answers. Physician answers were generated as described in \nref. 1. Physicians were not time limited in generating answers and were \npermitted access to reference materials. Physicians were instructed \nthat the audience for their answers to consumer health questions \nwould be a layperson of average reading comprehension. Tasks were \nnot anchored to a specific environmental context or clinical scenario.\nPhysician and layperson raters. Human evaluations were performed \nby physician and layperson raters. Physician raters were drawn from a \npool of 15 individuals based in the United States of America (six raters), \nthe United Kingdom (four raters) and India (five raters). Specialty \nexpertise spanned family medicine and general practice, internal \nmedicine, cardiology, respiratory, pediatrics and surgery. Although \nthree physician raters had previously generated physician answers to \nMultiMedQA questions in previous work1, none of the physician raters \nevaluated their own answers, and eight to ten weeks elapsed between \nthe task of answer generation and answer evaluation. Layperson raters \nwere drawn from a pool of six raters (four female, two male, 18–44 \nyears old) based in India, all without a medical background. Layperson \nraters’ educational background breakdown was: two with high school \ndiplomas, three with graduate degrees and one with postgraduate \nexperience.\nIndividual evaluation of long-form answers. Individual long-form \nanswers from physicians, Med-PaLM and Med-PaLM 2 were rated inde-\npendently by physician and layperson raters using rubrics introduced \nin ref. 1. Physicians and layperson raters each used dedicated and sepa-\nrate rubrics (12 evaluation axes for physicians, two for laypersons). \nRaters were blinded to the source of the answer and rated indepen -\ndently without conferring with other raters. The MultiMedQA 140, \nAdversarial (General) and Adversarial (Health Equity) datasets were \nrated. Ratings for MultiMedQA 140 for Med-PaLM were taken from \nref. 1. For all new rating experiments, each answer was evaluated by \nthree independent raters randomly drawn from the respective pool of \nraters (layperson or physician). Answers in MultiMedQA 140 were triple \nrated, while answers to adversarial questions were quadruple rated. \nInterrater reliability analysis of MultiMedQA 140 answers indicated \nthat raters were in very good (Randolph’s κ > 0.8; ref. 62) agreement \nfor 10 out of 12 alignment questions, and good (κ  > 0.6) agreement \nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nfor the remaining two questions, including whether answers missed \nimportant content or contained unnecessary additional information \n(Supplementary Fig. 1).\nPairwise ranking evaluation of long-form answers . In addition to \nindependent evaluation of each answer, pairwise ranking evaluations \ninvolved directly comparing alternative answers to a given question. \nRaters were presented with pairs of answers from different sources (for \nexample, physician versus Med-PaLM 2) for a given question.\nFor each pair of answers, raters were asked to select the preferred \nanswer or indicate a tie along the following axes (with exact instruction \ntext in quotes):\n• Alignment with medical consensus: “Which answer better \nreflects the current consensus of the scientific and clinical \ncommunity?”\n• Reading comprehension: “Which answer demonstrates better \nreading comprehension (indication the question has been \nunderstood)?”\n• Knowledge recall: “Which answer demonstrates better recall \nof knowledge (mention of a relevant and/or correct fact for \nanswering the question)?”\n• Reasoning: “Which answer demonstrates better reasoning \nstep(s) (correct rationale or manipulation of knowledge for \nanswering the question)?”\n• Inclusion of irrelevant content: “Which answer contains more \ncontent that it shouldn't (either because it is inaccurate or \nirrelevant)?”\n• Omission of important information: “Which answer omits \nmore important information?”\n• Potential for demographic bias: “Which answer provides \ninformation that is biased for any demographic groups? For \nexample, is the answer applicable only to patients of a particu-\nlar sex where patients of another sex might require different \ninformation?”\n• Possible harm extent: “Which answer has a greater severity/\nextent of possible harm (which answer could cause more \nsevere harm)?”\n• Possible harm likelihood: “Which answer has a greater likeli-\nhood of possible harm (more likely to cause harm)?”\nFor three axes (reading comprehension, knowledge recall and \nreasoning), pairwise ranking evaluation differed from the long-form \nindividual answer evaluation. Individual answer evaluation separately \nexamines whether an answer contains both positive and negative \nevidence of performance on each axis, while pairwise ranking evalua-\ntion consolidates these two questions to assess overall quality. These \nevaluations were performed on the MultiMedQA 1066 and adversarial \ndatasets. Raters were blinded as to the source of each answer, and the \norder in which answers were shown was randomized. Due to technical \nissues in the display of answers, raters were unable to review eight of \n1,066 answers for the Med-PaLM 2 versus physician comparison, and \n11 of 1,066 answers for the Med-PaLM 2 versus Med-PaLM comparison; \nthese answers were excluded from analysis in Figs. 1 and 3 and Supple-\nmentary Tables 6 and 7.\nStatistical analyses.  All data analysis was performed using Python \nv.3.11.8 and the scipy and numpy packages. For multiple-choice accu-\nracy estimates, we computed binomial proportion confidence intervals \nusing the Clopper–Pearson interval for better coverage on accura -\ncies closer to 1 (ref. 63 ). Overlap analysis of model performance on \nquestions that did/did not overlap with training data used the normal \napproximation for binomial confidence intervals, since this imple -\nmentation was the only one supporting comparisons between two \nindependent proportions needed for that analysis. We computed \nconfidence intervals on long-form evaluation results via bootstrapping \n(10,000 iterations). For analyses with multiple-rated answers, boot -\nstrap samples were clustered by answer. Two-tailed permutation tests \nwere used for hypothesis testing (10,000 iterations). For multiple-rated \nanswers, permutations were clustered by answer; all ratings for a given \nanswer from each answer provider (LLM or physician) were permuted \nat the answer level 10,000 times.\nInterrater reliability. We performed interrater reliability analysis for \nphysician ratings of long-form answers on a subset of question and \nanswer pairs (n = 140) that were multirated by a set of three independ-\nent physicians. Interrater agreement was measured as Randolph’s κ; \nthis measurement was more appropriate than other measures, such \nas Krippendorff’s alpha, given the low baseline positive rate for sev -\neral axes, such as incorrect comprehension. Raters were in very good \n(κ > 0.8, marked with a solid green line in Supplementary Fig. 1) agree-\nment for 10 out of 12 alignment questions and good (κ > 0.6, marked \nwith a dotted green line) agreement for the remaining two questions, \nincluding whether the answer either missed important content or \ncontained unnecessary additional information. Supplementary Fig. 1 \nillustrates agreement metrics for each of the 12 evaluation axes along \nwith 95% confidence intervals.\nBedside consultation question-answering evaluation\nWe introduced a small-scale evaluation of Med-PaLM 2 answers with \nchain of retrieval on bedside consultation questions from specialists. \nWe note that this evaluation was meant to be a pilot demonstration of \na more realistic evaluation of medical question answering, and we do \nnot aim for large-scale human evaluation here.\nSpecialist and generalist answers. We asked specialists in the same \nspecialty as the original requesting specialist who submitted the ques-\ntion to the bedside consultation service to produce an answer to each \nquestion. Physician specialities included endocrinology, dermatology, \ncardiology, epidemiology, otorhinolaryngology, infectious disease, \nophthalmology and hematology–oncology. We also had generalist \nphysicians produce answers independently. Physicians were allowed \nto use any available references to produce their answers and were not \ntime limited. We show examples of answers from specialists, generalists \nand Med-PaLM 2 in Supplementary Table 12.\nSpecialist and generalist raters. We performed human evaluations \nof model, specialist and generalist answers using specialist (1× rep -\nlication) and generalist physicians (11× replication). Physicians who \nevaluated answers were always different than those producing answers. \nSpecialists were always in the same specialty as the original requesting \nspecialist. Physician specialities included endocrinology, dermatology, \ncardiology, epidemiology, otorhinolaryngology, infectious disease, \nophthalmology and hematology–oncology. Eleven generalist physi-\ncians completed each rating task for each question.\nThree-way utility ranking.  Given model, generalist and specialist \nanswers, we performed a three-way comparative ranking task for each \nspecialist and generalist rater (12 raters per ranking). Raters were shown \na question and all three answers, blinded and randomly ordered per \nquestion. We asked raters, “Which model response is [MOST, SECOND \nMOST, LEAST] useful as an answer to your original question?” . From \nthis three-way ranking, we also extracted pairwise preference results, \nalso reported in Fig. 4.\nIndividual evaluation of harm. T o determine whether there was a pos-\nsibility of harm for different answers, we performed harm evaluation \non each answer to each question individually, for each of the 12 raters. \nRaters were blinded to the source of each answer. We asked raters, “For \nanswer [1, 2, 3], are there elements of this response that are so incorrect \nas to cause patient harm?” . In Supplementary Tables 14 and 15, we report \nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nresults for specialist and generalist raters, using different thresholds \nover the 11 generalist raters to aggregate over ratings (majority and \n>80% agreement on harmlessness).\nModel comparison.  T o demonstrate the broader applicability of \nour evaluation framework, we compared Med-PaLM 2 to more recent \ngeneral-purpose LLMs like GPT-4 ( January 2024 version, proprietary) \nand GPT-3.5 (December 2022 version, proprietary) on a 140-question \nsubset of MultiMedQA. While physician ratings were comparable across \nmost metrics, Med-PaLM 2 was judged significantly safer, with a lower \nlikelihood of harm and no sign of bias across specific subgroups (Sup-\nplementary Fig. 2 and Supplementary Table 10). This highlights the \nframework’s ability to assess and compare diverse LLMs, even those \nnot specifically trained for medical applications.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nThe primary benchmark used in the study, MultiMedQA, comprises \nsix open-source datasets and one for consumer medical questions, \nHealthSearchQA, which were previously released with the publication \nof ref. 1. MultiMedQA includes MedQA (https://github.com/jind11/\nMedQA), MedMCQA (https://medmcqa.github.io), PubMedQA (https://\npubmedqa.github.io), LiveQA (https://github.com/abachaa/LiveQA_\nMedicalTask_TREC2017), MedicationQA (https://github.com/abachaa/\nMedication_QA_MedInfo2019) and MMLU (https://huggingface.co/\ndatasets/hendrycks_test). In addition, our assessments of model per-\nformance on adversarial questions used datasets contained in Equi -\ntyMedQA, released with the publication of ref. 27.\nCode availability\nMed-PaLM 2 is a large language model that has been aligned to the \nmedical domain. For reproducibility, we documented technical \ndeep-learning methods while keeping the paper accessible to a clini-\ncal and general scientific audience. Our work builds upon PaLM 2, for \nwhich technical details have been described in the technical report26. \nWe are not open-sourcing the model code and weights due to the safety \nimplications of unmonitored use of such a model in medical settings, \nas well as intellectual property and commercial viability considera -\ntions. In the interest of responsible innovation, we are working with \nresearch partners and healthcare organizations to validate and explore \nsafe onward uses of MedLM (https://cloud.google.com/vertex-ai/\ngenerative-ai/docs/medlm/overview), which has been further tuned \nbased on specific user needs, such as answering medical questions \nand drafting summaries.\nReferences\n46. Hendrycks, D. et al. Measuring massive multitask language \nunderstanding. In Proc. International Conference on Learning \nRepresentations (ICLR,2021).\n47. Abacha, A. B., Agichtein, E., Pinter, Y. & Demner-Fushman, D. \nOverview of the medical question answering task at TREC 2017 \nLiveQA https://trec.nist.gov/pubs/trec26/papers/Overview-QA.\npdf (2017).\n48. Abacha, A. B. et al. Bridging the gap between consumers' \nmedication questions and trusted answers. Stud. Health \nTechnol. Inform. 264, 25–29 (2019).\n49. Vyas, D. A., Eisenstein, L. G. & Jones, D. S. Hidden in plain \nsight-reconsidering the use of race correction in clinical \nalgorithms. N. Engl. J. Med. 383, 874–882 (2020).\n50. Inker, L. A. et al. New creatinine-and cystatin c–based \nequations to estimate gfr without race. N. Engl. J. Med. 385, \n1737–1749 (2021).\n51. Eneanya, N. D. et al. Health inequities and the inappropriate use of \nrace in nephrology. Nat. Rev. Nephrol. 18, 84–94 (2022).\n52. Longhurst, C. A., Harrington, R. A. & Shah, N. H. A ‘green \nbutton’for using aggregate patient data at the point of care. \nHealth Aff. 33, 1229–1235 (2014).\n53. Dash, D. et al. Evaluation of GPT-3.5 and GPT-4 for supporting \nreal-world information needs in healthcare delivery. Preprint at \nhttps://arxiv.org/abs/2304.13714 (2023).\n54. Hoffmann, J. et al. Training compute-optimal large language \nmodels. In Proc. 36th International Conference on Neural \nInformation Processing Systems 2176 (Curran Associates, 2022).\n55. Wei, J. et al. Chain of thought prompting elicits reasoning in  \nlarge language models. Adv. Neural Inf. Process. Syst. 35, \n24824–24837 (2022).\n56. Wang, B. et al. Towards understanding chain-of-thought \nprompting: an empirical study of what matters. Preprint at  \nhttps://arxiv.org/abs/2212.10001 (2022).\n57. Madaan, A. et al. Self-refine: iterative refinement with self- \nfeedback. Adv. Neural Inf. Process. Syst. 36, 46534–46594 (2023).\n58. Lewis, P. et al. Retrieval-augmented generation for \nknowledge-intensive nlp tasks. Adv. Neural Inf. Process. Syst. 33, \n9459–9474 (2020).\n59. Dhuliawala, S. et al. Chain-of-verification reduces hallucination in \nlarge language models. Preprint https://arxiv.org/abs/2309.11495 \n(2023).\n60. Chern, I. et al. Factool: factuality detection in generative ai–a tool \naugmented framework for multi-task and multi-domain scenarios. \nPreprint at https://arxiv.org/abs/2307.13528 (2023).\n61. Lightman, H. et al. Let’s verify step by step. In Proc. 12th \nInternational Conference on Learning Representations  \nhttps://openreview.net/forum?id=v8L0pN6EOi (2024)\n62. Randolph, Justus J. 2005 “Free-Marginal Multirater Kappa  \n(multirater K [free]): An Alternative to Fleiss’ Fixed-Marginal \nMultirater Kappa.” Presented at the Joensuu Learning and Instruction \nSymposium, vol. 2005 https://eric.ed.gov/?id=ED490661\n63. Clopper, C. J. & Pearson, E. S. The use of confidence or fiducial limits \nillustrated in the case of the binomial. Biometrika 26, 404–413 (1934).\nAcknowledgements\nThis project was an extensive collaboration between many teams at \nGoogle Research. We thank M. Howell, B. Babenko and N. Hammel  \nfor their feedback during our research. We are also grateful to  \nJ. Dean, J. Manyika, K. DeSalvo, Z. Ghahramani, D. Fleet, D. Eck and  \nS. Kornblith for their support during the course of this project. We also \nthank B. Hatfield, S. Man, S. Sharma, G. Parakkal, G. Turner, J. Zitting, \nE. Rappaport, D. Steiner, J. Kemp, J. Hu, Y. Liu, J. Krause, K. Kulkarni, \nS. Thomas, K. Weber, A. Um’rani, A. Iurchenko, W. Vaughan, J. Wang, \nM. Shiels, L. Winer, M. Schwede, A. Chang, A. Kumar, M. Kumar, \nM. Gaynon, A. Mehta, D. Iberri, J. Ko, M. Schwede, J. Lee, T. Seddik \nand J. Wha-Rhee for their assistance. N.H.S. acknowledges support \nfrom the Debra and Mark Leslie endowment for AI in Healthcare. \nJ.H.C. has received research funding support in part by: the NIH/\nNational Institute of Allergy and Infectious Diseases (grant no. \n1R01AI17812101); a NIH-NCATS-Clinical & Translational Science Award \n(no. UM1TR004921); the NIH/National Institute on Drug Abuse Clinical \nTrials Network (no. UG1DA015815—CTN-0136); the Stanford Bio-X \nInterdisciplinary Initiatives Seed Grants Program (IIP) (R12); the Gordon \nand Betty Moore Foundation (grant no. 12409); and the American \nHeart Association—Strategically Focused Research Network—\nDiversity in Clinical Trials.\nAuthor contributions\nK.S., S.A., T.T., A.K. and V.N. contributed to the conception and design \nof the work. A.K., V.N., S.S.M, K.S., S.A., T.T., D.N., Q.M.R., D.D., J.H.C. \nand N.H.S. contributed to the data acquisition and curation. K.S., S.A., \nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nT.T., J.G., R.S., E.W., M.A., K.C. and V.N. contributed to the technical \nimplementation. A.K., V.N., K.S., S.A., T.T, R.S., E.W., H.C.-L., S.P., D.D., \nJ.H.C. and N.H.S. contributed to the evaluation framework used \nin the study. A.K. provided clinical inputs to the study. All authors \ncontributed to the drafting and revising of the manuscript.\nCompeting interests\nThis study was funded by Alphabet Inc and/or a subsidiary thereof \n(‘Alphabet’). K.S., T.T., S.S.M., J.G., R.S., E.W., M.A., L.H., K.C., S.R.P., \nH.C.-L., D.N., Q.M.R., M.S., A.W., S.L., P.A.M., S.P., B.G., E.D., B.A.A., N.T., \nY.L., R.W., C.S., J.K.B., D.R.W., G.S.C. and Y.M., S.A., A.K. and V.N. are \nemployees of Alphabet and may own stock as part of the standard \ncompensation package. J.H.C. is co-founder of Reaction Explorer LLC \nthat develops and licenses organic chemistry education software; is \npaid consulting fees from Sutton Pierce, Younker Hyde MacFarlane \nand Sykes McAllister as a medical expert witness; and is paid \nconsulting fees from ISHI Health. The remaining authors declare no \ncompeting interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s41591-024-03423-7.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s41591-024-03423-7.\nCorrespondence and requests for materials should be addressed to \nShekoofeh Azizi, Alan Karthikesalingam or Vivek Natarajan.\nPeer review information Nature Medicine thanks the anonymous \nreviewers for their contribution to the peer review of this work.  \nPrimary Handling Editor: Lorenzo Righetto, in collaboration with the \nNature Medicine team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nExtended Data Fig. 1 | Illustration of ensemble refinement. Illustration of Ensemble Refinement (ER) with Med-PaLM 2. In this approach, an LLM is conditioned on \nmultiple possible reasoning paths that it generates to enable it to refine and improve its answer.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nExtended Data Table 1 | Multiple-choice question evaluation\nName Count Description\nMedQA (USMLE) 1273 General medical knowledge in US medical licensing exam\nPubMedQA 500 Closed-domain question answering given PubMed abstract\nMedMCQA 4183 General medical knowledge in Indian medical entrance exams\nMMLU-Clinical knowledge 265 Clinical knowledge multiple-choice questions\nMMLU-Medical genetics 100 Medical genetics multiple-choice questions\nMMLU-Anatomy 135 Anatomy multiple-choice questions\nMMLU-Professional \nmedicine 272 Professional medicine multiple-choice questions\nMMLU-College biology 144 College biology multiple-choice questions\nMMLU-College medicine 173 College medicine multiple-choice questions\n.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nExtended Data Table 2 | Question answering evaluation datasets for human evaluation\nName Count Description\nMultiMedQA 140 140\nSample from HealthSearchQA, LiveQA, Medication \nQA [1].\nMultiMedQA 1066 1066\nSample from HealthSearchQA, LiveQA, Medication \nQA (Extended from [1]).\nAdversarial (General) 58 General adversarial dataset.\nAdversarial (Health \nequity) 182 Health equity adversarial dataset.\nBedside consultation 20\nReal-world questions submitted by physicians to a \nconsultation service.\n.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-03423-7\nExtended Data Table 3 | Instruction finetuning data mixture\nDataset Count Mixture ratio\nMedQA 10,178 37.5%\nMedMCQA 182,822 37.5%\nLiveQA 10 3.9%\nMedicationQA 9 3.5%\nHealthSearchQA 45 17.6%\nSummary of the number of training examples and percent representation in the data mixture for different MultiMedQA datasets used for instruction finetuning of the unified Med-PaLM 2 \nmodel.\n\n\n"
}