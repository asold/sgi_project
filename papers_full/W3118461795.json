{
    "title": "Transformer-based Language Model Fine-tuning Methods for COVID-19 Fake News Detection",
    "url": "https://openalex.org/W3118461795",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2475518380",
            "name": "Chen Ben",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1862245032",
            "name": "Chen Bin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2359375434",
            "name": "Gao De-hong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2388609978",
            "name": "Chen, Qijin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221786831",
            "name": "Huo, Chengfu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2131612684",
            "name": "Meng, Xiaonan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2303387167",
            "name": "Ren, Weijun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2012237231",
            "name": "Zhou Yang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963207607",
        "https://openalex.org/W2556096924",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3006647218",
        "https://openalex.org/W2890619523",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W2990625131",
        "https://openalex.org/W2782896998",
        "https://openalex.org/W2998604177",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3024622987",
        "https://openalex.org/W2783555701",
        "https://openalex.org/W2610995990",
        "https://openalex.org/W3153760598",
        "https://openalex.org/W3157497262",
        "https://openalex.org/W2963857521",
        "https://openalex.org/W3036607812",
        "https://openalex.org/W2963341956"
    ],
    "abstract": "With the pandemic of COVID-19, relevant fake news is spreading all over the sky throughout the social media. Believing in them without discrimination can cause great trouble to people's life. However, universal language models may perform weakly in these fake news detection for lack of large-scale annotated data and sufficient semantic understanding of domain-specific knowledge. While the model trained on corresponding corpora is also mediocre for insufficient learning. In this paper, we propose a novel transformer-based language model fine-tuning approach for these fake news detection. First, the token vocabulary of individual model is expanded for the actual semantics of professional phrases. Second, we adapt the heated-up softmax loss to distinguish the hard-mining samples, which are common for fake news because of the disambiguation of short text. Then, we involve adversarial training to improve the model's robustness. Last, the predicted features extracted by universal language model RoBERTa and domain-specific model CT-BERT are fused by one multiple layer perception to integrate fine-grained and high-level specific representations. Quantitative experimental results evaluated on existing COVID-19 fake news dataset show its superior performances compared to the state-of-the-art methods among various evaluation metrics. Furthermore, the best weighted average F1 score achieves 99.02%.",
    "full_text": "Transformer-based Language Model Fine-tuning\nMethods for COVID-19 Fake News Detection\nBen Chen, Bin Chen, Dehong Gao, Qijin Chen, Chengfu Huo,\nXiaonan Meng, Weijun Ren, and Yang Zhou\nAlibaba Group, Hangzhou, China\n{chenben.cb,cb242829,dehong.gdh,qijin.cqj,\nchengfu.hcf,xiaonan.mengxn, afei, yngzhou}\n@alibaba-inc.com\nAbstract. With the pandemic of COVID-19, relevant fake news is spreading all\nover the sky throughout the social media. Believing in them without discrimina-\ntion can cause great trouble to people’s life. However, universal language models\nmay perform weakly in these fake news detection for lack of large-scale annotated\ndata and sufﬁcient semantic understanding of domain-speciﬁc knowledge. While\nthe model trained on corresponding corpora is also mediocre for insufﬁcient\nlearning. In this paper, we propose a novel transformer-based language model\nﬁne-tuning approach for these fake news detection. First, the token vocabulary\nof individual model is expanded for the actual semantics of professional phrases.\nSecond, we adapt the heated-up softmax loss to distinguish the hard-mining sam-\nples, which are common for fake news because of the disambiguation of short\ntext. Then, we involve adversarial training to improve the model’s robustness.\nLast, the predicted features extracted by universal language model RoBERTa and\ndomain-speciﬁc model CT-BERT are fused by one multiple layer perception to\nintegrate ﬁne-grained and high-level speciﬁc representations. Quantitative exper-\nimental results evaluated on existing COVID-19 fake news dataset show its supe-\nrior performances compared to the state-of-the-art methods among various evalu-\nation metrics. Furthermore, the best weighted average F1 score achieves 99.02%.\nKeywords: COVID-19 · Fake news · Adversarial training · Knowledge Fusion.\n1 Introduction\nThe development of social media, such as Twitter and MicroBlog, has greatly facili-\ntated people’s lives. We can get real-time news from almost anywhere in the world.\nHowever, fabrications, satires, and hoaxes mixed in real reports often mislead people’s\njudgments, especially during the pandemic. For example, ”CDC Recommends Mothers\nStop Breastfeeding To Boost Vaccine Efﬁcacy” 1 and ”Consuming alcohol beverages\nor vodka will reduce risk of COVID-19 infection” 2 are two common rumors during\nthe epidemic and caused panic among the masses. Therefore, fake news detection is\nnecessary and we hope to design an effective detector which could quickly distinguish\n1 https://www.snopes.com/fact-check/breast-practices/\n2 https://www.usatoday.com/story/news/factcheck/2020/03/20/\narXiv:2101.05509v3  [cs.CL]  10 Feb 2023\n2 Ben Chen et al.\nwhether the news is fake or not according to its title or summary. It is usually formulated\nas the sequence classiﬁcation problem in general.\nText classiﬁcation is a fundamental task in natural language processing (NLP), and\ntransformer-based language models have achieved excellent performance in general\ndomains thanks to large corresponding corpora and ﬁned-designed pre-training skills\n(MLM/NSP/SOP) [13, 15, 14]. However, they usually perform weak for speciﬁc do-\nmain. One main reason is the professional phrases are rare in general corpora and ex-\nisting tokenizers (e.g. byte-pair-encoding, wordpiece, and sentencepiece 3) would split\nthem into many sub-tokens, and this operation hampers their actual semantics. Even if\ndata of speciﬁc domain is collected and used for down-stream ﬁne-tuning, the lim-\nited token vocabulary also fails to get the full meaning. Recently, [12, 18, 16] have\ndevoted to collected amounts of COVID-19 data. Especially for [16], it also further\ntrains one transformer-based model named CT-BERT with part of these annotated data,\nmaking a 10-30% marginal improvement compared to its base model on classiﬁcation,\nquestion-answering and chatbots tasks related to COVID-19. But for the speciﬁc fake\nnews detection, its insufﬁcient learning of limited corpus contents and incomplete hard\nsamples mining make it hard to achieves one impressive result. Furthermore, exces-\nsive further-training weakens the model’s understanding of common sense, resulting in\nsome incomprehensible mistakes.\nIn these paper, we try to optimize the performance of transformed-based language\nmodels for COVID-19 fake news detection. For individual model, ﬁrstly the token vo-\ncabulary is expanded with most frequent professional phrases for getting the actual\nsemantics without no split. Second, we adapt the heated-up softmax loss to distinguish\nthe hard-mining samples, which are common for fake news because of the disambigua-\ntion of short text. Then, adversarial training [6] is involved to improve the model’s\ngeneralization and robustness. Last, the predicted features extracted by universal lan-\nguage model RoBERTa [15] and domain-speciﬁc model CT-BERT [16] are fused by\none multiple layer perception to integrate ﬁne-grained universal and high-level speciﬁc\nrepresentations. Quantitative experimental results evaluated on existing COVID-19 fake\nnews dataset [12] show these methods superior performances compared to the state-\nof-the-art methods among various evaluation metrics. Furthermore, the best weighted\naverage F1 score achieves 99.02%.\n2 Related Work\n2.1 Text classiﬁcation task with adversarial training methods\nAdversarial training is ﬁrstly designed to increase the model robustness through adding\nsmall perturbations to training data, but it also increases the generalization ultimately\n[6]. In the ﬁeld of computer vision, mainstream gradient-based attack [1], optimization-\nbased attack [2] and generation-based attack [3] have achieved impressive results. In\nrecent years, more and more adversarial training tips [4–6] have been proposed for nat-\nural language processing tasks. Different from computer vision, where the image pixel\nis continuous in the ﬁxed space, so it is suitable to add noise perturbation based on\n3 https://github.com/huggingface/tokenizers\nLanguage Model Fine-tuning Methods for COVID-19 Fake News Detection 3\ngradient method. However, in natural language processing tasks, the input is usually a\ndiscrete text sequence, so it is impossible to realize the antagonistic training by adding\ndisturbance to the one-hot layer of the input text. From the Goodfellow’s work [4], they\nrealized the adversarial training by adding embedding disturbance into the embedding\nlayer. Usually in the CV task, according to empirical conclusions, the adversarial train-\ning tends to make the performance of the model on the normal samples worse, while\nin the NLP task, the work of [4] shows that the generalization ability of the model is\nstronger. In this paper, we use the method of adversarial training to improve the gener-\nalization ability of the model in the task of Fake New detection.\n2.2 Model fusion approaches for text classiﬁcation\nTraditional machine learning models have proved that ensemble learning play an im-\nportant roles in improving the model effect, such as Bagging and Boosting. The main\nreason lies in the complementary feature among models helps the model to make cor-\nrect judgment on the results. In recent years, a series of model fusion methods have also\nappeared in the ﬁeld of deep learning. The methods for model fusion mainly include\nfeature-based methods[7] and score-based methods[8]. Feature level fusion method is\nsuitable for models of different categories, such as CNN and BiGRU, and can extract\nword-level features and syntax-level features simultaneously[7]. While the fusion meth-\nods based on the score level are more applicable to similar structure for models, which\nobtain the ﬁnal result by voting. This paper combines CT-BERT and RoBERTa by using\nthe fusion method at the score level, while ensuring the universality of the model and\nits professionalism simultaneously in the task of Fake News Detection.\n2.3 Fake news detection\nFake news is intentionally written to mislead readers to believe false information, which\nmakes it difﬁcult and nontrivial to detect based on news content. Most existing meth-\nods identify false news by combining richer data, which provide a data repository that\ninclude not only news contents and social contents, but also dynamic information[18].\nHowever, such methods lack universality and cannot play a role in new ﬁelds. Hence,\nsemi-supervised and unsupervised methods are proposed[9], which try to make a trade-\noff between the amount of training data and the ﬁnal training accuracy. Based on the\nexcellent representational ability of deep pre-trained model (E.g, BERT, ALBERT), our\nmethod tries to get a well result by utilizing a small amount of data from domain special\nﬁelds.\n3 Methodology\n3.1 Problem Deﬁnition\nAs described above, in this paper we convert COVID-19 fake news detection as one\ntypical single sentence classiﬁcation, which means the proposed detector can judge\nwhether one news sentence is true or false according to its semantic meaning. It can\n4 Ben Chen et al.\nbe expressed formally as: giving a news sentence x = t1,t2,t3,t4,t5,...., the detector\nshould predict one correct label y ⊆{0,1}. And so the corresponding optimization\ngoal is to learn the θand maximize the loss function L(y|x,θ).\n3.2 Our proposed network\nAs shown in Fig.1 is the structure of our proposed network. It is derived from the most\nfamous language model −BERT, and we involve some practical methods to enhance\nthe ultimate performance from various perspectives. Below we will introduce each mod-\nule in detail.\nTraining with additional tokens For speciﬁc domain text, there are many professional\nphrases and existing tokenizers will split them into many sub-tokens, resulting in a\nmisunderstanding of their actual meanings. In this paper, we count 6 most frequent\ntokens in train and validation data which will be split with original method and add\nthem in the existing token vocabulary of CT-BERT. There are:\n• covid−19, covid19, coronavirus, pandemic, indiaﬁghtscorona, lockdown.\nSubsequent ablation experiments will prove the effectiveness of this method.\nOptimization with heated-up softmax loss function For binary classiﬁcation task,\nwe utilize the cross entropy loss as our loss function. In order to remind model to pay\nmore attention to the hard mining examples, we introduce the heated-up softmax to\nreplace the origin activation function[10]. Initially, the heated-up softmax is expressed\nas follow:\np(m|x) = exp(zm/T)∑M\nj=1 exp(zj/T)\n= exp(αzm)∑M\nj=1 exp(αzm)\n(1)\nwhere αdenotes the temperature parameter. As can be seen from Eq.1, the larger the\nparameter αis, the greater the gradient value is for the hard mining samples, and the\nmodel pays more attention to the hard mining samples. The smaller the α value, the\nsmaller the attention to hard mining sample, boundary sample and easy sample. There-\nfore, at the beginning of the training, we ﬁrst set a large αand let the model focus on\nlearning the difﬁcult sample, and then reduce the α. The model began to pay attention\nto the boundary sample, and when the hard mining sample and boundary sample were\nalmost completed, then further reduced α to ﬁne tuning. As depicted in Fig.1(b), the\nfactor αaffect the distribution of the last output layer, which is efﬁcient for classify the\nhard example to adjust the αas training going.\nGradient-based adversarial training Adversarial training [11] is a novel regular-\nization method for classiﬁers to improve model robustness for small, approximately\nworst case perturbations. The loss function we used with adversarial training unit can\nbe formulated as:\n−log p(y|x+ radv; θ) where radv = argminr,||r||<=ϵ log p(x|x+ r; ˆθ) (2)\nLanguage Model Fine-tuning Methods for COVID-19 Fake News Detection 5\n(a)\n(b)\nFig. 1: The framework of our proposed method. (a) is the overall framework of the\nfused models and (b) is three improved modules added to each model. The bottom part\nshows the result of adding new tokens to the existing vocabulary. The middle red boxes\nrepresent the gradient perturbations added to the word embedding. And the top blue\nhistograms exhibit how the heated-up softmax affects the distribution.\nwhere radv denotes the perturbations , ˆθdenotes the parameters of current network, and\nθ denotes the parameters after one-step gradient optimization of the network. We can\nﬁnd from the Eq.2 that our core objective is to add a small perturbation rwhich could\ndisable the current classiﬁer, then optimize the classiﬁer, and optimize the network to\nmaximize the model’s ability to correctly classify samples. However, we cannot cal-\nculate this value exactly in general, because exact minimization with respect to r is\nintractable for many interesting models such as neural networks. Goodfellow et al. [2]\nproposed to approximate this value by linearizing log p(y|x; ˆθ) around x. With a linear\napproximation and a L2 norm constraint in Eq.(2), the ﬁnal adversarial perturbation is\nradv = −ϵ g\n||g||2\nwhere g= ∇xlogp(y|x; ˆθ) (3)\nThis perturbation can be easily computed using backpropagation in neural networks.\nIn Fig.1(b), the red box represents the perturbation calculated by Eq.3. perturba-\ntion is added to the embedding layer, and the robustness of the model is enhanced by\nadversarial training.\n6 Ben Chen et al.\n4 Experimental Results\nIn this section, we will evaluate performance of our method in Fake News Detection\ntask. We compared our approach to several pre-trained models that perform well on\nGlue tasks4, including BERT, ALBERT, and RoBERTa, and each model included the\nbasic version (X-Base) and the Large version (X-Large).\nIn order to reﬂect the importance of each module in our model, we also perform the\nablation studies on our experiments. Speciﬁcally, we divide our approach into 3 differ-\nent methods and test the performance of each module: COVID-Twitter-BERT (bench-\nmark model, refer to as CT-BERT model below) [16], CT-BERT-FGM(including ad-\nversarial training module with fast gradient method),CT-BERT-HL (including heated-\nup softmax module),CT-BERT-New-Tokens (including new tokens training ), Ro-CT-\nBERT (ours).\n4.1 Experimental setting\nThe data we used for training and evaluation is the online-collected COVID-19 fake\nnews dataset [12]. The sources of them are various social-media platforms such as\nTwitter, Facebook, Instagram, etc. It contains 6420 / 2140 / 2140 raw news sentence\nfor training / validation / test. The real news sentence is as follow:\nWearing mask canprotectyoufromthevirus.\nWhile the fake one is shown as follow:\nIf youtakeCrocinthriceaday youaresafe.\nAs a side note, for the data preprocessing, we follow the baselines in [12] to remove\nall links, non alphanumeric characters (e.g. unicode emotions) and English stop words,\nwhich all would bring great interference to the effective detection. In order to train the\nmore distinguishable models, After each evaluation we will reserve the mis-classiﬁed\nsamples in training and validation set and replace some (1 ∼2) words with their syn-\nonyms or remove these words directly. The extended data will be added to the training\nset for the next round of training.\nTo evaluate the performance of different methods, three popular metrics are adopted,\nnamely weighted Precision, weighted Recall and weighted F1. The deﬁnitions are as\nfollows:\nPrecisionweighted =\n∑n\ni=1 Precisioni ∗wi\nn (4)\nRecallweighted =\n∑n\ni=1 Recalli ∗wi\nn (5)\nF1weighted = 2 ∗Precisionweighted ∗Recallweighted\nPrecisionweighted + Recallweighted\n(6)\n4 https://gluebenchmark.com/\nLanguage Model Fine-tuning Methods for COVID-19 Fake News Detection 7\nwhere nrepresents the number of classes, wi represents the radio of true instances\nfor each label.\nOur implementation is based on the online available natural language libraryTrans-\nformers5. We adopt the initial learning rate of 2e-5 with warm-up rate of 0.1. The batch\nsize is selected as 64 for training and 128 for validation and test. The temperature pa-\nrameter αis set as 4 in ﬁrst 10 epochs, as 1 in middle 10 epochs and as 0.5 for last 10\nepochs. Each sequence length is limited to 128 tokens. All experiments are performed\nusing PyTorch on a Telsa V100 GPU with the optimizer selected as Adam.\n4.2 Performance in Fake News Detection in English\nIn this paper, we investigate the most cutting-edge models to tackle fake New Detec-\ntion tasks, including the highly versatile BERT [13], ALBERT [14], and RoBERTa [15]\nand their large versions; In addition, we investigate COVID-Twitter-BERT (CT-BERT),\nwhich is trained on a large corpus of Twitter messages on the topic of COVID-19. We\nutilize the pre-trained model ﬁles provided on the ofﬁcial website to initialize corre-\nsponding parameters, and then ﬁne tuning on the COVID-19 dataset [12]. Each model\nused the best result as the ﬁnal experimental result. For the sake of description, we call\nour model asRobust-COVID-Twitter-BERT (Ro-CT-BERT). The experimental results\nare shown in Table 1.\nTable 1: Experimental comparison results on fake news detection task.\nMethod Accuracy Precision Recall F1\nBERT-base 0.978505 0.978574 0.978505 0.978497\nBERT-large 0.980374 0.980407 0.980374 0.980369\nRoBERTa-base 0.983645 0.983755 0.983644 0.983638\nRoBERTa-large 0.985981 0.986081 0.985981 0.985976\nALBERT-base 0.973365 0.973419 0.973365 0.973356\nALBERT-large 0.973832 0.973897 0.973832 0.973823\nALBERT-xlarge 0.974299 0.974665 0.974299 0.974276\nCT-BERT 0.984112 0.984161 0.984112 0.984115\nRo-CT-BERT 0.990187 0.990218 0.990187 0.990185\nFrom the experimental results, We can see that Ro-CT-BERT can get superior per-\nformance than state-of-the-art methods on the metrics weighted average accuracy, pre-\ncision, recall and F1 score. Universal language models BERT, ALBERT get all metric\nvalue lower than 0.981, while RoBERTa is much better than them because of ﬁne-\ndesigned key hyperparameters and larger training data size. Although the CT-BERT\nmodel has been trained in a large number of Messages on the Topic of COVID-19, our\nmodel’s F1 score is still 0.006 points higher than it’s F1 score. This promotion is very\ndifﬁcult because we need to make the correct classiﬁcation for hard mining samples.\nWe attribute the improvement of the model to the hard mining samples learning and\neffective fusion of ﬁne-grained and high-level representations. In the next section, we\nwill demonstrate the effect of each module on the model through ablation experiments.\n5 https://github.com/huggingface/transformers\n8 Ben Chen et al.\n4.3 Ablation studies for Ro-CT-BERT\nIn order to verify the effectiveness of the improved modules involved in Ro-CT-BERT,\nwe also conduct several ablation experiments. We mainly compare the inﬂuence of three\nmodules on the model, which called adversarial training, heated-up softmax loss func-\ntion and addition of new Token. For fairly comparison, we take CT-BERT as the bench-\nmark model and add three modules for subsequent experiments, respectively. These\nthree models are successively referred as attack-training-CT-BERT (CT-BERT-FGM),\nheated-up softmax loss CT-BERT (CT-BERT-HL) and new-taken CT-BERT (CT-BERT-\nNew-Tokens). The model with all three modules is referred as three-modules-CT-BERT\n(CT-BERT-TRM). Results of ablation experiments are shown in Table 2.\nTable 2: Experimental comparison results on Fake News Detection task.\nMethod Accuracy Precision Recall F1\nCT-BERT 0.984112 0.984161 0.984112 0.984115\nCT-BERT-FGM 0.986449 0.986451 0.986449 0.986448\nCT-BERT-HL 0.986916 0.986971 0.986916 0.986912\nCT-BERT-New-Tokens 0.984579 0.984623 0.984579 0.984575\nCT-BERT-TRM 0.987851 0.987888 0.987851 0.987848\nRo-CT-BERT 0.990187 0.990218 0.990187 0.990185\nIt can be seen that compared with the TD-BERT model without any other tricks, the\nclassiﬁcation effect of the other three models is improved to a certain extent, especially\nthe heated-up softmax loss, which increases the generalization ability of the model and\nhas a strong classiﬁcation ability for the hard mining samples. Furthermore, these three\nmodules combined produces one better result. Lastly, Ro-CT-BERT fuse the predicted\nfeatures of CT-BERT-TRM and RoBERTa-TRM (three-module RoBERTa) and get the\nhighest score, indicating that the integration of ﬁne-grained and high-level speciﬁc rep-\nresentations helps to understand the text semantics more comprehensively.\n5 Conclusions\nIn this work, we propose a transformer-based Language Model Fine-tuning approach\nfor COVID-19 Fake News Detection[19]. The length of adopted news sentences for\nthis task is short, and lots of professional phrases are rare in common corpora. These\ntwo distinct features make universal and speciﬁc language models all fail to make a\ncorrect distinction whether news is fake or real. To address these problems, we respec-\ntively introduce new tokens for the speciﬁc model vocabulary for better understand-\ning of professional phrases, model adversarial training to improve the robustness, and\nheated-up softmax loss function to distinguish the hard-mining sample. Lastly, we also\nfuse the predicted features extracted by universal language model and domain-speciﬁc\nmodel to integrate ﬁne-grained and high-level speciﬁc representations. These methods\nare veriﬁed to be useful for improving the transformer-based model’s ability, and ﬁ-\nnally, our approach achieves super performance compare with state-of-the-art methods\non the COVID-19 fake news detection.\nLanguage Model Fine-tuning Methods for COVID-19 Fake News Detection 9\nReferences\n1. Carlini, Nicholas, and David Wagner. ”Towards evaluating the robustness of neural networks.”\n2017 ieee symposium on security and privacy (sp). IEEE, 2017.\n2. Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. ”Explaining and harnessing ad-\nversarial examples.” arXiv preprint arXiv:1412.6572 (2014).\n3. Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song: Generating Ad-\nversarial Examples with Adversarial Networks. CoRR abs/1801.02610 (2018) a service of\nSchloss Dagstuhl - Leibniz Center for Informatics homebrowsesearchabout\n4. Miyato, Takeru, Andrew M. Dai, and Ian Goodfellow. ”Adversarial training methods for semi-\nsupervised text classiﬁcation.” arXiv preprint arXiv:1605.07725 (2016).\n5. Wang, Wenqi, et al. ”Towards a Robust Deep Neural Network in Texts: A Survey.” arXiv\npreprint arXiv:1902.07285 (2019).\n6. Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, Jingjing Liu: FreeLB: Enhanced\nAdversarial Training for Natural Language Understanding. ICLR 2020\n7. Xie, Jinbao, et al. ”Chinese text classiﬁcation based on attention mechanism and feature-\nenhanced fusion neural network.” Computing 102.3 (2020): 683-700.\n8. Bhushan S N B, Danti A. Classiﬁcation of text documents based on score level fusion ap-\nproach[J]. Pattern Recognition Letters, 2017, 94: 118-126.\n9. Bhattacharjee, Sreyasee Das, Ashit Talukder, and Bala Venkatram Balantrapu. ”Active learn-\ning based news veracity detection with feature weighting and deep-shallow fusion.” 2017\nIEEE International Conference on Big Data (Big Data). IEEE, 2017.\n10. Xu Zhang, Felix X. Yu, Svebor Karaman, Wei Zhang, Shih-Fu Chang: Heated-Up Softmax\nEmbedding. CoRR abs/1809.04157 (2018)\n11. Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. ”Explaining and harnessing\nadversarial examples.” arXiv preprint arXiv:1412.6572 (2014).\n12. Patwa, Parth, et al. ”Fighting an Infodemic: COVID-19 Fake News Dataset.” arXiv preprint\narXiv:2011.03327 (2020).\n13. Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of deep bidirectional transformers\nfor language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n14. Lan, Zhenzhong, et al. ”ALBERT: A lite BERT for self-supervised learning of language\nrepresentations.” arXiv preprint arXiv:1909.11942 (2019).\n15. Liu, Yinhan, et al. ”RoBERTa: A robustly optimized BERT pretraining approach.” arXiv\npreprint arXiv:1907.11692 (2019).\n16. M ¨uller, Martin, Marcel Salath ´e, and Per E. Kummervold. ”COVID-Twitter-BERT: A Nat-\nural Language Processing Model to Analyse COVID-19 Content on Twitter.” arXiv preprint\narXiv:2005.07503 (2020).\n17. Sun, Chi , et al. ”How to Fine-Tune BERT for Text Classiﬁcation?.” China National Confer-\nence on Chinese Computational Linguistics Springer, Cham, 2019.\n18. Gautam Kishore Shahi, Durgesh Nandini: FakeCovid-A Multilingual Cross-domain Fact\nCheck News Dataset for COVID-19. CoRR abs/2006.11343 (2020)\n19. Parth Patwa and Mohit Bhardwaj, et al. Overview of CONSTRAINT 2021 Shared Tasks:\nDetecting English COVID-19 Fake News and Hindi Hostile Posts. Proceedings of the First\nWorkshop on Combating Online Hostile Posts in Regional Languages during Emergency Sit-\nuation (CONSTRAINT-2021)"
}