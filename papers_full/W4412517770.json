{
  "title": "Beyond Manual Media Coding: Evaluating Large Language Models and Agents for News Content Analysis",
  "url": "https://openalex.org/W4412517770",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5009296677",
      "name": "Stavros Doropoulos",
      "affiliations": [
        "International Hellenic University"
      ]
    },
    {
      "id": "https://openalex.org/A5092501437",
      "name": "Elisavet Karapalidou",
      "affiliations": [
        "Hellenic Agency for Local Development and Local Government"
      ]
    },
    {
      "id": "https://openalex.org/A5046455102",
      "name": "Polychronis Charitidis",
      "affiliations": [
        "Hellenic Agency for Local Development and Local Government"
      ]
    },
    {
      "id": "https://openalex.org/A5011072799",
      "name": "Sophia Karakeva",
      "affiliations": [
        "Hellenic Agency for Local Development and Local Government"
      ]
    },
    {
      "id": "https://openalex.org/A5039053440",
      "name": "Stavros Vologiannidis",
      "affiliations": [
        "International Hellenic University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4312063257",
    "https://openalex.org/W6869615180",
    "https://openalex.org/W1973704285",
    "https://openalex.org/W2797521656",
    "https://openalex.org/W6634289364",
    "https://openalex.org/W2793897830",
    "https://openalex.org/W4250329919",
    "https://openalex.org/W4411746978",
    "https://openalex.org/W4210764005",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4385569970",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W4360978668",
    "https://openalex.org/W4409166975",
    "https://openalex.org/W4389520295",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4399114781",
    "https://openalex.org/W4363671832",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W4388718403",
    "https://openalex.org/W4401416041",
    "https://openalex.org/W6853118264",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4409657236",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4409796633",
    "https://openalex.org/W4410430985",
    "https://openalex.org/W4411232472",
    "https://openalex.org/W4404782999",
    "https://openalex.org/W4410895429",
    "https://openalex.org/W4407419890",
    "https://openalex.org/W4406266037",
    "https://openalex.org/W4236544382",
    "https://openalex.org/W6851268924",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4409158700",
    "https://openalex.org/W4408413603",
    "https://openalex.org/W4393147158",
    "https://openalex.org/W4399878331",
    "https://openalex.org/W4362508448",
    "https://openalex.org/W1575560391",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "The vast volume of media content, combined with the costs of manual annotation, challenges scalable codebook analysis and risks reducing decision-making accuracy. This study evaluates the effectiveness of large language models (LLMs) and multi-agent teams in structured media content analysis based on codebook-driven annotation. We construct a dataset of 200 news articles on U.S. tariff policies, manually annotated using a 26-question codebook encompassing 122 distinct codes, to establish a rigorous ground truth. Seven state-of-the-art LLMs, spanning low- to high-capacity tiers, are assessed under a unified zero-shot prompting framework incorporating role-based instructions and schema-constrained outputs. Experimental results show weighted global F1-scores between 0.636 and 0.822, with Claude-3-7-Sonnet achieving the highest direct-prompt performance. To examine the potential of agentic orchestration, we propose and develop a multi-agent system using Meta’s Llama 4 Maverick, incorporating expert role profiling, shared memory, and coordinated planning. This architecture improves the overall F1-score over the direct prompting baseline from 0.757 to 0.805 and demonstrates consistent gains across binary, categorical, and multi-label tasks, approaching commercial-level accuracy while maintaining a favorable cost–performance profile. These findings highlight the viability of LLMs, both in direct and agentic configurations, for automating structured content analysis.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6115435361862183
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.5081148147583008
    },
    {
      "name": "Content (measure theory)",
      "score": 0.42217308282852173
    },
    {
      "name": "Natural language processing",
      "score": 0.3339557647705078
    },
    {
      "name": "Information retrieval",
      "score": 0.32878848910331726
    },
    {
      "name": "Mathematics",
      "score": 0.09475290775299072
    },
    {
      "name": "Statistics",
      "score": 0.06681710481643677
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}