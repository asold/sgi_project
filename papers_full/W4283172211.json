{
  "title": "What Does it Mean for a Language Model to Preserve Privacy?",
  "url": "https://openalex.org/W4283172211",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2108336169",
      "name": "Hannah Brown",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2107274534",
      "name": "Katherine Lee",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2904925844",
      "name": "Fatemehsadat Mireshghallah",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2081295151",
      "name": "Reza Shokri",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2913042046",
      "name": "Florian Tram√®r",
      "affiliations": [
        "Google (Switzerland)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2083773633",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4231533450",
    "https://openalex.org/W2806344213",
    "https://openalex.org/W2947160092",
    "https://openalex.org/W2963956191",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W2080957047",
    "https://openalex.org/W2109426455",
    "https://openalex.org/W2049704337",
    "https://openalex.org/W1979839410",
    "https://openalex.org/W3035261884",
    "https://openalex.org/W4237413241",
    "https://openalex.org/W2899519070",
    "https://openalex.org/W2972407156",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W2963919731",
    "https://openalex.org/W3167675683",
    "https://openalex.org/W3172917028",
    "https://openalex.org/W3206084162",
    "https://openalex.org/W1603920809",
    "https://openalex.org/W2996664720",
    "https://openalex.org/W3165327186",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W3175115403",
    "https://openalex.org/W3034975599",
    "https://openalex.org/W3098641803",
    "https://openalex.org/W2111589509",
    "https://openalex.org/W3045882047",
    "https://openalex.org/W3021999948",
    "https://openalex.org/W2135930857",
    "https://openalex.org/W2930926105",
    "https://openalex.org/W3154109599",
    "https://openalex.org/W3003387143",
    "https://openalex.org/W3017330637",
    "https://openalex.org/W2963378725",
    "https://openalex.org/W2037275221",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3123012225",
    "https://openalex.org/W3096738375",
    "https://openalex.org/W3162891415",
    "https://openalex.org/W3170672407",
    "https://openalex.org/W2899206203",
    "https://openalex.org/W2993843842",
    "https://openalex.org/W3027379683",
    "https://openalex.org/W3175987672",
    "https://openalex.org/W4310895557",
    "https://openalex.org/W3021582373",
    "https://openalex.org/W3103245149",
    "https://openalex.org/W4247780151"
  ],
  "abstract": "Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8161829710006714
    },
    {
      "name": "Adversary",
      "score": 0.7415140867233276
    },
    {
      "name": "Exploit",
      "score": 0.6832715272903442
    },
    {
      "name": "Natural language",
      "score": 0.6291001439094543
    },
    {
      "name": "Differential privacy",
      "score": 0.6206492185592651
    },
    {
      "name": "Language model",
      "score": 0.6060864329338074
    },
    {
      "name": "Information privacy",
      "score": 0.5341305732727051
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5149345993995667
    },
    {
      "name": "Internet privacy",
      "score": 0.5119968056678772
    },
    {
      "name": "Language identification",
      "score": 0.4504863917827606
    },
    {
      "name": "Computer security",
      "score": 0.40245404839515686
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3749402165412903
    },
    {
      "name": "Natural language processing",
      "score": 0.3511863350868225
    },
    {
      "name": "Data mining",
      "score": 0.14459332823753357
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}