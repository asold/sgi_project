{
    "title": "LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attention",
    "url": "https://openalex.org/W3014931875",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2385717340",
            "name": "Yin, Junbo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2388015377",
            "name": "Shen, Jianbing",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Guan, Chenye",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2319561891",
            "name": "Zhou, Dingfu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2377678696",
            "name": "Yang, Ruigang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2987391422",
        "https://openalex.org/W2950898568",
        "https://openalex.org/W2937423351",
        "https://openalex.org/W3012125421",
        "https://openalex.org/W2290847742",
        "https://openalex.org/W2964321699",
        "https://openalex.org/W3003618643",
        "https://openalex.org/W2150066425",
        "https://openalex.org/W2897529137",
        "https://openalex.org/W2963020213",
        "https://openalex.org/W2954174912",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2980547801",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2963727135",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2964054038",
        "https://openalex.org/W2925148167",
        "https://openalex.org/W3007182669",
        "https://openalex.org/W2950228652",
        "https://openalex.org/W2979750740",
        "https://openalex.org/W2962991329",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2972211064",
        "https://openalex.org/W2956121407",
        "https://openalex.org/W2968296999",
        "https://openalex.org/W1501856433",
        "https://openalex.org/W2605659599",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2963400571",
        "https://openalex.org/W2963212638",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W2952254971",
        "https://openalex.org/W2939384298",
        "https://openalex.org/W2949708697",
        "https://openalex.org/W2964311892",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2981857055",
        "https://openalex.org/W2116341502",
        "https://openalex.org/W2798965597",
        "https://openalex.org/W2972276718",
        "https://openalex.org/W2555618208",
        "https://openalex.org/W2969741484",
        "https://openalex.org/W2966926453",
        "https://openalex.org/W2964062501",
        "https://openalex.org/W2798930779",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2951517617",
        "https://openalex.org/W2969987486",
        "https://openalex.org/W2798734012",
        "https://openalex.org/W2552391307",
        "https://openalex.org/W2962807143",
        "https://openalex.org/W1485009520",
        "https://openalex.org/W2962159375",
        "https://openalex.org/W637153065",
        "https://openalex.org/W2953941229",
        "https://openalex.org/W3034892601",
        "https://openalex.org/W2982363097",
        "https://openalex.org/W2963149042",
        "https://openalex.org/W3034239557",
        "https://openalex.org/W2924050502",
        "https://openalex.org/W2894705404"
    ],
    "abstract": "Existing LiDAR-based 3D object detectors usually focus on the single-frame detection, while ignoring the spatiotemporal information in consecutive point cloud frames. In this paper, we propose an end-to-end online 3D video object detector that operates on point cloud sequences. The proposed model comprises a spatial feature encoding component and a spatiotemporal feature aggregation component. In the former component, a novel Pillar Message Passing Network (PMPNet) is proposed to encode each discrete point cloud frame. It adaptively collects information for a pillar node from its neighbors by iterative message passing, which effectively enlarges the receptive field of the pillar feature. In the latter component, we propose an Attentive Spatiotemporal Transformer GRU (AST-GRU) to aggregate the spatiotemporal information, which enhances the conventional ConvGRU with an attentive memory gating mechanism. AST-GRU contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module, which can emphasize the foreground objects and align the dynamic objects, respectively. Experimental results demonstrate that the proposed 3D video object detector achieves state-of-the-art performance on the large-scale nuScenes benchmark.",
    "full_text": "LiDAR-based Online 3D Video Object Detection with Graph-based Message\nPassing and Spatiotemporal Transformer Attention\nJunbo Yin1,2, Jianbing Shen1,4âˆ—, Chenye Guan2,3, Dingfu Zhou2,3, Ruigang Yang2,3,5\n1Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China\n2 Baidu Research 3 National Engineering Laboratory of Deep Learning Technology and Application, China\n4 Inception Institute of Artiï¬cial Intelligence, UAE 5 University of Kentucky, Kentucky, USA\n{yinjunbocn, shenjianbingcg}@gmail.com https://github.com/yinjunbo/3DVID\nAbstract\nExisting LiDAR-based 3D object detectors usually focus\non the single-frame detection, while ignoring the spatiotem-\nporal information in consecutive point cloud frames. In this\npaper, we propose an end-to-end online 3D video object de-\ntector that operates on point cloud sequences. The proposed\nmodel comprises a spatial feature encoding component and\na spatiotemporal feature aggregation component. In the\nformer component, a novel Pillar Message Passing Net-\nwork (PMPNet) is proposed to encode each discrete point\ncloud frame. It adaptively collects information for a pil-\nlar node from its neighbors by iterative message passing,\nwhich effectively enlarges the receptive ï¬eld of the pillar\nfeature. In the latter component, we propose an Attentive\nSpatiotemporal Transformer GRU (AST-GRU) to aggregate\nthe spatiotemporal information, which enhances the con-\nventional ConvGRU with an attentive memory gating mech-\nanism. AST-GRU contains a Spatial Transformer Attention\n(STA) module and a Temporal Transformer Attention (TTA)\nmodule, which can emphasize the foreground objects and\nalign the dynamic objects, respectively. Experimental re-\nsults demonstrate that the proposed 3D video object detec-\ntor achieves state-of-the-art performance on the large-scale\nnuScenes benchmark.\n1. Introduction\nLiDAR-based 3D object detection plays a critical role\nin a wide range of applications, such as autonomous driv-\ning, robot navigation and virtual/augmented reality [11, 46].\nThe majority of current 3D object detection approaches [42,\n58, 6, 62, 24] follow the single-frame detection paradigm,\nwhile few of them perform detection in the point cloud\nvideo. A point cloud video is deï¬ned as a temporal se-\nquence of point cloud frames. For instance, in the nuScenes\nâˆ—Corresponding author: Jianbing Shen.\nFigure 1: Occlusion situation in autonomous driving scenar-\nios. Typical single-frame 3D object detector, e.g. [24], often leads\nto false-negative (FN) results (top row). In contrast, our online 3D\nvideo object detector can handle this (bottom row). The grey and\nred boxes denote the predictions and ground-truths, respectively.\ndataset [4], 20 point cloud frames can be captured per sec-\nond with a modern 32-beam LiDAR sensor. Detection in\nsingle frame may suffer from several limitations due to the\nsparse nature of point cloud. In particular, occlusions, long-\ndistance and non-uniform sampling inevitably occur on a\ncertain frame, where a single-frame object detector is in-\ncapable of handling these situations, leading to a deterio-\nrated performance, as shown in Fig 1. However, a point\ncloud video contains rich spatiotemporal information of the\nforeground objects, which can be explored to improve the\ndetection performance. The major concern of construct-\ning a 3D video object detector is how to model the spa-\ntial and temporal feature representation for the consecutive\npoint cloud frames. In this work, we propose to integrate\na graph-based spatial feature encoding component with an\nattention-aware spatiotemporal feature aggregation compo-\nnent, to capture the video coherence in consecutive point\narXiv:2004.01389v1  [cs.CV]  3 Apr 2020\ncloud frames, which yields an end-to-end online solution\nfor the LiDAR-based 3D video object detection.\nPopular single-frame 3D object detectors tend to ï¬rst\ndiscretize the point cloud into voxel or pillar girds [62, 56,\n24], and then extract the point cloud features using stacks\nof convolutional neural networks (CNNs). Such approaches\nincorporate the success of existing 2D or 3D CNNs and usu-\nally gain better computational efï¬ciency compared with the\npoint-based methods [42, 37]. Therefore, in our spatial fea-\nture encoding component, we also follow this paradigm to\nextract features for each input frame. However, a poten-\ntial problem with these approaches lies in that they only fo-\ncus on a locally aggregated feature, i.e., employing a Point-\nNet [39] to extract features for separate voxels or pillars as\nin [62] and [24]. To further enlarge the receptive ï¬elds, they\nhave to apply the stride or pooling operations repeatedly,\nwhich will cause the loss of the spatial information. To al-\nleviate this issue, we propose a novel graph-based network,\nnamed Pillar Message Passing Network (PMPNet), which\ntreats a non-empty pillar as a graph node and adaptively en-\nlarges the receptive ï¬eld for a node by aggregating mes-\nsages from its neighbors. PMPNet can mine the rich geo-\nmetric relations among different pillar grids in a discretized\npoint cloud frame by iteratively reasoning on ak-NN graph.\nThis effectively encourages information exchanges among\ndifferent spatial regions within a frame.\nAfter obtaining the spatial features of each input frame,\nwe assemble these features in our spatiotemporal feature\naggregation component. Since ConvGRU [1] has shown\npromising performance in the 2D video understanding ï¬eld,\nwe suggest an Attentive Spatiotemporal Transformer GRU\n(AST-GRU) to extend ConvGRU to the 3D ï¬eld through\ncapturing dependencies of consecutive point cloud frames\nwith an attentive memory gating mechanism. Speciï¬cally,\nthere exist two potential limitations when considering the\nLiDAR-based 3D video object detection in autonomous\ndriving scenarios. First, in the birdâ€™s eye view, most fore-\nground objects (e.g., cars and pedestrians) occupy small re-\ngions, and the background noise is inevitably accumulated\nas computing the new memory in a recurrent unit. Thus, we\npropose to exploit the Spatial Transformer Attention (STA)\nmodule, an intra-attention derived from [48, 53], to sup-\npress the background noise and emphasize the foreground\nobjects by attending each pixel with the context informa-\ntion. Second, when updating the memory in the recurrent\nunit, the spatial features of the two inputs ( i.e., the old\nmemory and the new input) are not well aligned. In par-\nticular, though we can accurately align the static objects\nacross frames using the ego-pose information, the dynamic\nobjects with large motion are not aligned, which will im-\npair the quality of the new memory. To address this, we\npropose a Temporal Transformer Attention (TTA) module\nthat adaptively captures the object motions in consecutive\nframes with a temporal inter-attention mechanism. This\nwill better utilize the modiï¬ed deformable convolutional\nlayers [65, 64]. Our AST-GRU can better handle the spa-\ntiotemporal features and produce a more reliable new mem-\nory, compared with the vanilla ConvGRU. To summarize,\nwe propose a new LiDAR-based online 3D video object de-\ntector that leverages the previous long-term information to\nimprove the detection performance. In our model, a novel\nPMPNet is introduced to adaptively enlarge the receptive\nï¬eld of the pillar nodes in a discretized point clod frame by\niterative graph-based message passing. The output sequen-\ntial features are then aggregated in the proposed AST-GRU\nto mine the rich coherence in the point cloud video by using\nan attentive memory gating mechanism. Extensive evalua-\ntions demonstrate that our 3Dvideo object detector achieves\nbetter performance against thesingle-frame detectors on the\nlarge-scale nuScenes benchmark.\n2. Related Work\nLiDAR-based 3D Object Detection. Existing works on\n3D object detection can be roughly categorized into three\ngroups, which are LiDAR-based [42, 58, 62, 24, 61, 56],\nimage-based [22, 54, 26, 34, 25] and multi-sensor fusion-\nbased [5, 29, 30, 21, 38] methods. Here, we focus on\nthe LiDAR-based approaches since they are less sensitive\nto different illumination and weather conditions. Among\nthem, one category [62, 57, 24] typically discretizes the\npoint cloud into regular girds ( e.g., voxels or pillars), and\nthen exploits the 2D or 3D CNNs for features extraction.\nAnother category [42, 58, 6] learns 3D representations di-\nrectly from the original point cloud with a point-wise fea-\nture extractor like PointNet++ [39]. It is usually impractical\nto directly apply the point-based detectors in scenes with\nlarge-scale point clouds, for they tend to perform feature ex-\ntraction for every single point. For instance, a keyframe in\nnuScenes dataset [4] contains 300,000 point clouds, which\nare densiï¬ed by 10 non-keyframe LiDAR sweeps within\n0.5 s. Operating on point clouds with such a scale will lead\nto non-trivial computation cost and memory demand. In\ncontrast, the voxel-based methods can tackle this kind of\ndifï¬culty for they are less sensitive to the number of points.\nZhou et al. [62] ï¬rst apply the end-to-end CNNs for voxel-\nbased 3D object detection. They propose to describe each\nvoxel with a V oxel Feature Encoding (VFE) layer, and uti-\nlize cascade 3D and 2D CNNs to extract the deep features.\nThen a Region Proposal Network (RPN) is employed to ob-\ntain the ï¬nal detection results. After that, Lang et al. [24]\nfurther extend [62] by projecting the point clouds to the the\nbirdâ€™s eye view and encoding each discretized gird (named\npillars) with a Pillar Feature Network (PFN).\nBoth the VFE layers and the PFN only take into account\nseparate voxels or pillars when generating the grid-level\nrepresentation, which ignores the information exchange in\nDetection \nHead\nPillar Message \nPassing Network\nAST-GRU\nAST-GRU\nAST-GRU\n2D CNN\nDetection \nHead\nDetection \nHead\nPoint Cloud Video\n Spatial Feature Encoding\n Spatiotemporal Feature Aggregation\n Detection Results\nğ‘°ğ‘»âˆ’ğŸ \nğ‘°ğ‘»âˆ’ğŸ \nğ‘°ğ‘» \nğ’€ğ‘»âˆ’ğŸ \nğ’€ğ‘»âˆ’ğŸ \nğ’€ğ‘» \nğ‘¿ğ‘»âˆ’ğŸ \nğ‘¿ğ‘»âˆ’ğŸ \nğ‘¿ğ‘» \nğ‘¯ğ‘»âˆ’ğŸ \nğ‘¯ğ‘»âˆ’ğŸ \nğ‘¯ğ‘» \nFigure 2: Our online 3D video object detection frameworkincludes a spatial feature encoding component and a spatiotemporal feature\naggregation component. In the former component, a novel PMPNet ( Â§3.1) is proposed to extract the spatial features of each point cloud\nframe. Then, features from consecutive frames are sent to the AST-GRU ( Â§3.2) in the latter component, to aggregate the spatiotemporal\ninformation with an attentive memory gating mechanism.\nlarger spatial regions. In contrast, our PMPNet encodes the\npillar feature from a global perspective by graph-based mes-\nsage passing, and thus promotes the representation with the\nnon-local property. Besides, all these single-frame 3D ob-\nject detectors can only process the point cloud data frame-\nby-frame, lacking the exploration of the temporal informa-\ntion. Though [33] applies temporal 3D ConvNet on point\ncloud sequences, it encounters the feature collapse issue\nwhen downsampling the features in the temporal domain.\nMoreover, it cannot deal with long-term sequences with\nmulti-frame labels. Our AST-GRU instead captures the\nlong-term temporal information with an attentive memory\ngating mechanism, which can fully mine the spatiotempo-\nral coherence in the point cloud video.\nGraph Neural Networks.Graph Neural Networks (GNNs)\nare ï¬rst introduced by Gori et al. [13] to model the intrinsic\nrelationships of the graph-structured data. Then Scarselli et\nal. [41] extend it to different types of graphs. Afterward,\nGNNs are explored in two directions in terms of differ-\nent message propagation strategies. The ï¬rst group [28,\n19, 60, 36, 40] uses the gating mechanism to enable the\ninformation to propagate across the graph. For instance,\nLi et al. [28] leverage the recurrent neural networks to de-\nscribe the state of each graph node. Then, Gilmer et al. [12]\ngeneralizes a framework to formulate the graph reason-\ning as a parameterized message passing network. Another\ngroup [3, 15, 9, 17, 27] integrates convolutional networks\nto the graph domain, named as Graph Convolutional Neural\nNetworks (GCNNs), which update node features via stacks\nof graph convolutional layers. GNNs have achieved promis-\ning results in many areas [9, 10, 51, 2, 52] due to the great\nexpressive power of graphs. Our PMPNet belongs to the\nï¬rst group by capturing the pillar features with a gated mes-\nsage passing strategy, which is used to construct the spatial\nrepresentation for each point cloud frame.\n3. Model Architecture\nIn this section, we elaborate on our online 3D video ob-\nject detection framework. As shown in Fig. 2, it consists of\na spatial feature encoding component and a spatiotemporal\nfeature aggregation component. Given the input sequences\n{It}T\nt=1 with T frames, we ï¬rst convert the point cloud co-\nordinates from the previous frames {It}Tâˆ’1\nt=1 to the current\nframe IT using the GPS data, so as to eliminate the inï¬‚u-\nence of the ego-motion and align the static objects across\nframes. Then, in the spatial feature encoding component,\nwe extract features for each frame with the Pillar Message\nPassing Network (PMPNet) (Â§3.1) and a 2D backbone, pro-\nducing sequential features {Xt}T\nt=1. After that, these fea-\ntures are fed into the Attentive Spatiotemporal Transformer\nGated Recurrent Unit (AST-GRU) (Â§3.2) in the spatiotem-\nporal feature aggregation component, to generate the new\nmemory features {Ht}T\nt=1. Finally, a RPN head is applied\non {Ht}T\nt=1 to give the ï¬nal detection results {Yt}T\nt=1.\nSome network architecture details are provided in Â§3.3.\n3.1. Pillar Message Passing Network\nPrevious point cloud encoding layers (e.g., the VFE lay-\ners in [62] and the PFN in [24]) for voxel-based 3D object\ndetection typically encode each voxel or pillar separately,\nwhich limits the expressive power of the grid-level repre-\nsentation due to the small receptive ï¬eld of each local grid\nregion. Our PMPNet instead seeks to explore the rich spa-\ntial relations among different gird regions by treating the\nnon-empty pillar grids as graph nodes. Such design effec-\ntively reserves the non-Euclidean geometric characteristics\nof the original point clouds and enhance the output pillar\nfeatures with a non-locality property.\nGiven an input point cloud frame It, we ï¬rst uniformly\ndiscretize it into a set of pillars P, with each pillar uniquely\nassociated with a spatial coordinate in the x-y plane as\nğ‘š1,4\nğ‘   \n \nâ„1\nğ‘  \n \nâ„2\nğ‘  \n \nâ„3\nğ‘  \n \nâ„4\nğ‘  \n \nâ„5\nğ‘  \n \nâ„6\nğ‘  \n \nğ‘š3,1\nğ‘   \n \nğ‘š1,3\nğ‘   \n \nğ‘š4,1\nğ‘   \n \nğ‘š2,1\nğ‘   \n \nğ‘š1,2\nğ‘   \n \nğ‘š2,5\nğ‘   \n \nğ‘š5,2\nğ‘   \n \nğ‘š6,2\nğ‘   \n \nğ‘š2,6\nğ‘   \n \nâ„1\nğ‘ +1 \n \nâ„2\nğ‘ +1 \n \nâ„3\nğ‘ +1 \n \nâ„4\nğ‘ +1 \n \nâ„5\nğ‘ +1 \n \nâ„6\nğ‘ +1 \n \nğ‘š3,1\nğ‘ +1 \n \nğ‘š1,3\nğ‘ +1 \n \nğ‘š1,4\nğ‘ +1 \n \nğ‘š4,1\nğ‘ +1 \n \nğ‘š2,1\nğ‘ +1 \n \nğ‘š1,2\nğ‘ +1 \n \nğ‘š2,5\nğ‘ +1 \n \nğ‘š5,2\nğ‘ +1 \n \nğ‘š6,2\nğ‘ +1 \n \nğ‘š2,6\nğ‘ +1 \n \nFigure 3: Illustration of one iteration step for message propa-\ngation, where hi is the state of node vi. In step s, the neighbors\nfor h1 are {h2, h3, h4} (within the gray dash line), presenting the\npillars in the top car. After aggregating messages from the neigh-\nbors, the receptive ï¬eld of h1 is enlarged in step s + 1, indicating\nthe relations with nodes from the bottom car are modeled.\nin [24]. Then, PMPNet maps the resultant pillars to a di-\nrected graph G= (V,E), where node vi âˆˆV represents a\nnon-empty pillar Pi âˆˆP and edge ei,j âˆˆE indicates the\nmessage passed from node vi to vj. For reducing the com-\nputational overhead, we deï¬ne Gas a k-nearest neighbor\n(k-NN) graph, which is built from the geometric space by\ncomparing the centroid distance among different pillars.\nTo explicitly mine the rich relations among different pil-\nlar nodes, PMPNet performs iterative message passing on\nGand updates the nodes state at each iteration step. Con-\ncretely, given a node vi, we ï¬rst utilize a pillar feature net-\nwork (PFN) [24] to describe its initial state h0\ni at iteration\nstep s= 0:\nh0\ni = FPFN(Pi) âˆˆRL, (1)\nwhere h0\ni is a L-dim vector and Pi âˆˆ RNÃ—D presents a\npillar containing N LiDAR points, with each point param-\neterized by Ddimension representation (e.g., the XYZ co-\nordinates and the received reï¬‚ectance). The PFN is real-\nized by applying fully connected layers on each point within\nthe pillar, then summarizing features of all points through a\nchannel-wise maximum operation. The initial node stateh0\ni\nis a locally aggregated feature, only including points infor-\nmation within a certain pillar grid.\nNext, we elaborate on the message passing process. One\niteration step of message propagation is illustrated in Fig. 3.\nAt step s, a node vi aggregates information from all the\nneighbor nodes vj âˆˆâ„¦vi in the k-NN graph. We deï¬ne\nthe incoming edge feature from node vj as es\nj,i, indicating\nthe relation between node vi and vj. Inspired by [55], the\nincoming edge feature es\nj,i is given by:\nes\nj,i = hs\nj âˆ’hs\ni âˆˆRL, (2)\nwhich is an asymmetric function encoding the local neigh-\nbor information. Accordingly, we have the message passed\nfrom vj to vi, which is denoted as:\nms+1\nj,i = Ï†Î¸([hs\ni,es\nj,i]) âˆˆRLâ€²\n, (3)\nwhere Ï†Î¸ is parameterized by a fully connected layer, which\ntakes as input the concatenation of hs\ni and es\nj,i, and yields a\nLâ€²-dim feature.\nAfter computing all the pair-wise relations between vi\nand the neighbors vj âˆˆâ„¦vi of , we summarize the received\nkmessages with a maximum operation:\nms+1\ni = max\njâˆˆâ„¦i\n(ms+1\nj,i ) âˆˆRLâ€²\n, (4)\nThen, we update the node state hs\ni with hs+1\ni for node vi.\nThe update process should consider both the newly col-\nlected message ms+1\ni and the previous state hs\ni. Recurrent\nneural network and its variants [16, 47] can adaptively cap-\nture dependencies in different time steps. Hence, we utilize\nGated Recurrent Unit (GRU) [7] as the update function for\nits better convergence characteristic. The update process is\nthen formulated as follows:\nhs+1\ni = GRU(hs\ni,ms+1\ni ) âˆˆRL, (5)\nIn this way, the new node state hs+1\ni contains the informa-\ntion from all the neighbor nodes of vi. Moreover, a neigh-\nbor node vj also collects information from its own neigh-\nbors â„¦vj . Consequently, after the totally S iteration steps,\nnode viis able to aggregate information from the high-order\nneighbors. This effectively enlarges the perceptual range for\neach pillar grid and enables our model to better recognize\nobjects from a global view.\nNote that each pillar corresponds with a spatial coordi-\nnate in the x-y plane. Therefore, after performing the iter-\native message passing, the encoded pillar features are then\nscattered back as a 3D tensor ËœIt âˆˆRWÃ—HÃ—C, which can be\nfurther exploited by the 2D CNNs. Here, we leverage the\nbackbone network in [62] to further extract features for ËœIt:\nXt = FB(ËœIt) âˆˆRwÃ—hÃ—c, (6)\nwhere FB denotes the backbone network and Xt is the spa-\ntial features of It. Details of the PMPNet and the backbone\nnetwork can be found in Â§3.3.\n3.2. Attentive Spatiotemporal Transformer GRU\nSince the sequential features {Xt}T\nt=1 produced by the\nspatial feature encoding component are regular tensors, we\ncan employ the ConvGRU [1] to fuse these features in our\nspatiotemporal feature aggregation component. However,\nit may suffer from two limitations when directly applying\nthe ConvGRU. On the one hand, the interest objects are rel-\natively small in the birdâ€™s eye view compared with those\nin the 2D images ( e.g., an average of 18 Ã—8 pixels for\ncars with the pillar size of 0.252 m2). This may cause\nthe background noise to dominate the results when com-\nputing the memory. On the other hand, though the static\nobjects can be well aligned across frames using the GPS\ndata, the dynamic objects with large motion still lead to an\ninaccurate new memory. To address the above issues, we\npropose the AST-GRU to equip the vanilla ConvGRU [1]\nwith a spatial transformer attention (STA) module and a\ntemporal transformer attention (TTA) module. As illus-\ntrated in Fig. 4, the STA module stresses the foreground\nobjects in {Xt}T\nt=1 and produces the attentive new input\n{X\nâ€²\nt}T\nt=1, while the TTA module aligns the dynamic ob-\njects in {Htâˆ’1}T\nt=1 and {X\nâ€²\nt}T\nt=1, and outputs the attentive\nold memory {H\nâ€²\ntâˆ’1}T\nt=1. Then, {X\nâ€²\nt}T\nt=1 and {H\nâ€²\ntâˆ’1}T\nt=1\nare used to generate the new memory {Ht}T\nt=1, and fur-\nther produce the ï¬nal detections {Yt}T\nt=1. Before giving\nthe details of the STA and TTA modules, we ï¬rst review the\nvanilla ConvGRU.\nVanilla ConvGRU.GRU model [7] operates on a sequence\nof inputs to adaptively capture the dependencies in differ-\nent time steps with a memory mechanism. ConvGRU is\na variant of the conventional GRU model, which employs\nconvolution operations rather than the fully connected ones,\nto reduce the number of parameters and preserve the spa-\ntial resolution of the input features. ConvGRU has made\npromising results on many tasks [32, 49, 23, 50], and has\nshown better results than the LSTM [43] counterparts in\nterms of the convergence time [8]. More speciï¬cally, Con-\nvGRU contains an update gate zt, a reset gate rt, a can-\ndidate memory ËœHt and a new memory Ht. At each time\nstep, the new memory Ht (also named as the hidden state)\nis computed based on the old memory Htâˆ’1 and the new\ninput Xt, which can be denoted by the following equations:\nzt = Ïƒ(Wz âˆ—Xt + Uz âˆ—Htâˆ’1), (7)\nrt = Ïƒ(Wr âˆ—Xt + Ur âˆ—Htâˆ’1), (8)\nËœHt = tanh(W âˆ—Xt + U âˆ—(rt â—¦Htâˆ’1)), (9)\nHt = (1 âˆ’zt) â—¦Htâˆ’1 + zt â—¦ ËœHt, (10)\nwhere â€˜*â€™ and â€˜ â—¦â€™ denote the convolution operation\nand Hadamard product, and Ïƒ is a sigmoid function.\nW,Wz,Wr and U,Uz,Ur are the 2D convolutional ker-\nnels. When computing the candidate memory ËœHt, the im-\nportance of the old memory Htâˆ’1 and the new input Xt is\ndetermined by the reset gate rt, i.e., the information of ËœHt\nall comes from Xt when rt = 0. Additionally, the update\ngate ztdecides the degree to which the unit accumulates the\nold memory Htâˆ’1, to yield the new memory Ht. In Â§4.2,\nwe show that the vanilla ConvGRU has outperformed the\nsimple point cloud merging [4] and the temporal 3D Con-\nvNet [33]. Next, we present how we promote the vanilla\nConvGRU with the STA and TTA modules.\nSpatial \nTransformer\nC-GRU\n Temporal \nTransformer\nSpatial \nTransformer\nC-GRU\nğ‘¿ğ‘»âˆ’ğŸ \nğ‘¿ğ‘»âˆ’ğŸ \n Temporal \nTransformer\nSpatial \nTransformer\nC-GRU\nğ‘¿ğ‘» \nğ’€ğ‘»âˆ’ğŸ \nğ’€ğ‘»âˆ’ğŸ \nğ’€ğ‘» \nğ‘¿ğ‘»âˆ’ğŸ\nâ€²  \nğ‘¯ğ‘»âˆ’ğŸ \nğ‘¯ğ‘»âˆ’ğŸ \nDetection \nHead\nDetection \nHead\nDetection \nHead\nğ‘¯ğ‘»âˆ’ğŸ \nğ‘¯ğ‘»âˆ’ğŸ \nğ‘¯ğ‘» \nğ‘¿ğ‘»âˆ’ğŸ\nâ€²  \nğ‘¿ğ‘»\nâ€²  \nğ‘¿ğ‘»âˆ’ğŸ\nâ€²  \nğ‘¿ğ‘»\nâ€²  \nğ‘¯ğ‘»âˆ’ğŸ\nâ€²  \nğ‘¯ğ‘»âˆ’ğŸ\nâ€²  \nFigure 4: The detailed architecture of the proposed AST-\nGRU, which consists of a spatial transformer attention (STA)\nmodule and a temporal transformer attention (TTA) module. AST-\nGRU models the dependencies of consecutive frames and pro-\nduces the attentive new memory {Ht}T\nt=1.\nSpatial Transformer Attention.The core idea of the STA\nmodule is to attend each pixel-level feature x âˆˆXt with a\nrich spatial context, to better distinguish a foreground object\nfrom the background noise. Basically, a transformer atten-\ntion receives a query xq âˆˆXt and a set of keys xk âˆˆâ„¦xq\n(e.g., the neighbors of xq), to calculate an attentive output\nyq. The STA is designed as an intra-attention, which means\nboth the query and key are from the same input feature Xt.\nFormally, given a queryxq âˆˆXt at location qâˆˆwÃ—h,\nthe attentive output yq is computed by:\nyq =\nâˆ‘\nkâˆˆâ„¦q\nA(Ï†Q(xq),Ï†K(xk)) â—¦Ï†V(xk), (11)\nwhere A(Â·,Â·) is the attention weight. Ï†K, Ï†Q and Ï†V\nare the linear layers that map the inputs xq,xk âˆˆ Xt\ninto different embedding subspaces. The attention weight\nA(Â·,Â·) is computed from the embedded query-key pair\n(Ï†Q(xq),Ï†K(xk)), and is then applied to the neighbor val-\nues Ï†V(xk).\nSince we need to obtain the attention for all the query-\nkey pairs, the linear layers, Ï†K, Ï†Q and Ï†V, are then\nachieved by the convolutional layers, Î¦K, Î¦Q and Î¦V,\nto facilitate the computation. Speciï¬cally, the input fea-\ntures Xt are ï¬rst embedded as Kt, Qt and Vt âˆˆRwÃ—hÃ—câ€²\nthrough Î¦K, Î¦Qand Î¦V. Then, we adjust the tensor shapes\nof Ktand Qtto lÃ—câ€², where l= wÃ—h, in order to compute\nthe attention weight:\nËœA = softmax(Qt Â·KT\nt ) âˆˆRlÃ—l (12)\nwhere A(Â·,Â·) is realized as a softmax layer to normalize the\nattention weight matrix. After that, ËœA is employed to ag-\ngregate information from the values Vt through a matrix\nmultiplication, generating the attentive output ËœA Â·Vt, with\nthe tensor shape recovered to wÃ—hÃ—câ€². Finally, we ob-\ntain the spatially enhanced features X\nâ€²\nt through a residual\noperation [14], which can be summarized as:\nX\nâ€²\nt = Wout âˆ—( ËœA Â·Vt) + Xt âˆˆRwÃ—hÃ—c (13)\nwhere Wout is the output layer of the attention head that\nmaps the embedding subspace ( câ€²-dim) of ËœA Â·Vt back to\nthe original space (c-dim). In this way, X\nâ€²\nt contains the in-\nformation from its spatial context and thus can better focus\non the meaningful foreground objects.\nTemporal Transformer Attention.To adaptively align the\nfeatures of dynamic objects fromHtâˆ’1 to X\nâ€²\nt, we apply the\nmodiï¬ed deformable convolutional layers [65, 64] as a spe-\ncial instantiation of the transformer attention. The core is\nto attend the queries in Htâˆ’1 with adaptive supporting key\nregions computed by integrating the motion information.\nSpeciï¬cally, given a vanilla deformable convolutional\nlayer with kernel size 3 Ã—3, let wm denotes the learnable\nweights, and pm âˆˆ{(âˆ’1,âˆ’1),(âˆ’1,0),..., (1,1)}indicates\nthe predetermined offset in total M = 9 grids. The out-\nput h\nâ€²\nq for input hq âˆˆHtâˆ’1 at location q âˆˆwÃ—hcan be\nexpressed as:\nh\nâ€²\nq =\nMâˆ‘\nm=1\nwm Â·hq+pm+âˆ†pm, (14)\nwhere âˆ†pm is the deformation offset learnt through a sepa-\nrate regular convolutional layer Î¦R, i.e., âˆ†pm âˆˆâˆ†Ptâˆ’1 =\nÎ¦R(Htâˆ’1) âˆˆRwÃ—hÃ—2r2\n, where the channel number 2r2\ndenotes the offsets in the x-y plane for the r Ã—r convo-\nlutional kernel. We can also reformulate Eq. 14 from the\nperspective of transformer attention as in Eq. 11, such that\nthe attentive output h\nâ€²\nq of query hq is given by:\nh\nâ€²\nq =\nMâˆ‘\nm=1\nwm Â·\nâˆ‘\nkâˆˆâ„¦q\nG(k,q + pm + âˆ†pm) Â·Ï†V(hk),\n(15)\nwhere Ï†V is an identity function, andwmacts as the weights\nin different attention heads [64], with each head correspond-\ning to a sampled key position k âˆˆâ„¦q. G(Â·,Â·) is the at-\ntention weight deï¬ned by a bilinear interpolation function,\nsuch that G(a,b) = max(0,1 âˆ’|aâˆ’b|).\nThe supporting key regions â„¦q play an important role\nin attending hq, which are determined by the deformation\noffset âˆ†pm âˆˆâˆ†Ptâˆ’1. In our TTA module, we compute\nâˆ†Ptâˆ’1 not only through Htâˆ’1, but also through a motion\nmap, which is deï¬ned as the difference of Htâˆ’1 and X\nâ€²\nt:\nâˆ†Ptâˆ’1 = Î¦R([Htâˆ’1,Htâˆ’1 âˆ’X\nâ€²\nt]) âˆˆRwÃ—hÃ—2r2\n, (16)\nwhere Î¦R is a regular convolutional layer with the same\nkernel size as that in the deformable convolutional layer,\nand [Â·,Â·] is the concatenation operation. The intuition is\nthat, in the motion map, the features response of the static\nobjects is very low since they have been spatially aligned in\nHtâˆ’1 and X\nâ€²\nt, while the features response of the dynamic\nobjects remains high. Therefore, we integrate Htâˆ’1 with\nthe motion map, to further capture the motions of dynamic\nobjects. Then, âˆ†Ptâˆ’1 is used to select the supporting key\nregions and further attend Htâˆ’1 for all the query regions\nq âˆˆwÃ—hin terms of Eq. 15, yielding a temporally atten-\ntive memory H\nâ€²\ntâˆ’1. Since the supporting key regions are\ncomputed from both Htâˆ’1 and X\nâ€²\nt, our TTA module can\nbe deemed as an inter-attention.\nAdditionally, we can stack multiple modiï¬ed deformable\nconvolutional layers to get a more accurate H\nâ€²\ntâˆ’1. In our\nimplementation, we adopt two layers. The latter layer takes\nas input [H\nâ€²\ntâˆ’1,H\nâ€²\ntâˆ’1 âˆ’X\nâ€²\nt] to predict the deformation off-\nset according to Eq. 16, and the offset is then used to attend\nH\nâ€²\ntâˆ’1 via Eq. 15. Accordingly, we can now utilize the tem-\nporally attentive memory H\nâ€²\ntâˆ’1 and the spatially attentive\ninput X\nâ€²\nt to compute the new memory Ht in the recurrent\nunit (see Fig. 4). Finally, a RPN detection head is applied\non Ht to produce the ï¬nal detection results Yt.\n3.3. Network Details\nPMPNet. Our PMPNet is an end-to-end differentiable\nmodel achieved by parameterizing all the functions with\nneural networks. Given a discretized point cloud frame\nIt âˆˆRPÃ—NÃ—D with P pillar nodes, FPFN is ï¬rst used to\ngenerates the initial node stateG0 âˆˆRPÃ—Lfor all the nodes\n(Eq. 1), which is realized by a 1 Ã—1 convolutional layer\nfollowed by a max pooling layer that operates on the N\npoints. In each iteration step s, the edge features from the\nK neighbor nodes are ï¬rst collected as ËœGs âˆˆRPÃ—KÃ—2L\n(Eq. 2) with a concatenation operation. Then the message\nfunctions (Eq. 3 and Eq. 4) map the collected features ËœGs\nto Ms âˆˆRPÃ—L\nâ€²\n, through a 1 Ã—1 convolutional layer fol-\nlowed by a max pooling layer performing on the K mes-\nsages. The update function (Eq. 5) then updates the node\nstate using a GRU with fully connected layers, by consider-\ning both the Gs âˆˆRPÃ—L and Ms âˆˆRPÃ—L\nâ€²\n, and outputs\nGs+1 âˆˆRPÃ—L. After S iteration steps, we get the ï¬nal\nnode state GS âˆˆRPÃ—L, and scatter it back to a 3D tensor\nËœItâˆˆRWÃ—HÃ—L (Eq. 6).\nBackbone Module. As in [62], we utilize a 2D backbone\nnetwork to further extract features for ËœItâˆˆRWÃ—HÃ—L, which\nconsists of three blocks of fully convolutional layers. Each\nblock is deï¬ned as a tuple (S,Z,C ). All the blocks have\nZ Ã—Z convolutional kernels with output channel number\nC. The ï¬rst layer of each block operates at stride S, while\nother layers have stride 1. The output features of each block\nare resized to the same resolution via upsampling layers and\nthen concatenated together to merge the semantic informa-\ntion from different feature levels.\nAST-GRU Module.In our STA module, all the linear func-\ntions in Eq. 11 and Wout in Eq. 13 are 1 Ã—1 convolution\nlayers. In our TTA module, the regular convolutional lay-\ners, the deformable convolutional layers and the ConvGRU\nall have learable kernels of size 3 Ã—3.\nMethod Car Pedestrian Bus Barrier T.C. Truck Trailer Moto. Cons. Bicycle Mean\nVIPL ICT [35] 71.9 57.0 34.1 38.0 27.3 20.6 26.9 20.4 3.3 0.0 29.9\nMAIR [44] 47.8 37.0 18.8 51.1 48.7 22.0 17.6 29.0 7.4 24.5 30.4\nPointPillars [24] 68.4 59.7 28.2 38.9 30.8 23.0 23.4 27.4 4.1 1.1 30.5\nSARPNET [59] 59.9 69.4 19.4 38.3 44.6 18.7 18.0 29.8 11.6 14.2 32.4\nWYSIWYG [18] 79.1 65.0 46.6 34.7 28.8 30.4 40.1 18.2 7.1 0.1 35.0\nTolist [35] 79.4 71.2 42.0 51.2 47.8 34.5 34.8 36.8 9.8 12.3 42.0\nOurs 79.7 76.5 47.1 48.8 58.8 33.6 43.0 40.7 18.1 7.9 45.4\nTable 1: Quantitative detection results on the nuScenes 3D object detection benchmark.T.C. presents the trafï¬c cone. Moto. and\nCons. are short for the motorcycle and construction vehicle, respectively. Our 3D video object detector outperforms the single-frame\ndetectors, achieving state-of-the-art performance on the leaderboard.\nDetection Head. The detection head in [62] is applied on\nthe attentive memory features. In particular, the smooth\nL1 loss and the focal loss [31] count for the object bound-\ning box regression and classiï¬cation, respectively. A corss-\nentropy loss is used for the orientation classiï¬cation. For the\nvelocity regression required by the nuScenes benchmark, a\nsimple L1 loss is adopted and shows substantial results.\n4. Experimental Results\n3D Video Object Detection Benchmark.We evaluate our\nalgorithm on the challenging nuScenes 3D object detec-\ntion benchmark [4], since the KITTI benchmark [11] does\nnot provide the point cloud videos. nuScenes is a large-\nscale dataset with a total of 1,000 scenes, where 700 scenes\n(28,130 samples) are for training and 150 scenes (6,008\nsamples) are for testing, resulting 7 Ã—as many annotations\nas the KITTI. The samples (also named as keyframes) in\neach video are annotated every 0.5 s with a full 360-degree\nview, and their point clouds are densiï¬ed by the 10 non-\nkeyframe sweeps frames, yielding around 300,000 point\nclouds with 5-dim representation (x,y,z,r, âˆ†t), where r\nis the reï¬‚ectance and âˆ†t describes the time lag to the\nkeyframe (ranging from 0 s to 0.45 s). Besides, nuScenes\nrequires detecting objects for 10 classes with full 3D boxes,\nattributes and velocities.\nImplementation Details. For each keyframe, we consider\nthe point clouds within range of [âˆ’50,50] Ã—[âˆ’50,50] Ã—\n[âˆ’5,3] meters along the X, Y and Z axes. The pillar reso-\nlution on the X-Y plane is 0.252 m2. The pillar number P\nused in PMPNet is 16,384, sampled from the total 25,000\npillars, with each pillar containing most N = 60 points.\nThe input point cloud is aD= 5 dimensions representation\n(x,y,z,r, âˆ†t), which are then embedded intoL= L\nâ€²\n= 64\ndimensions feature space after the total S = 3 graph it-\neration steps. The convolutional kernels in the 2D back-\nbone are of size Z = 3 and the output channel number C\nin each block is (64,128,256). The upsampling layer has\nkernel size 3 and channel number 128. Thus, the ï¬nal fea-\ntures map produced by the 2D backbone has a resolution\nof 100 Ã—100 Ã—384. We calculate anchors for different\nclasses using the mean sizes and set the matching threshold\naccording to the class instance number. The coefï¬cients of\nthe loss functions for classiï¬cation, localization and veloc-\nity prediction are set to 1, 2 and 0.1, respectively. NMS with\nIOU threshold 0.5 is utilized when generating the ï¬nal de-\ntections. In both training and testing phases, we feed most 3\nconsecutive keyframes to the model due to the memory lim-\nitation. The training procedure has two stages. In the ï¬rst\nstage, we pre-train the spatial features encoding component\nusing the one-cycle policy [45] with a maximum learning\nrate of 0.003. Then, we ï¬x the learning rate to 0.0002 in the\nsecond stage to train the full model. We train 50 epochs for\nboth stages with batch size 3. Adam optimizer [20] is used\nto optimize the loss functions.\n4.1. Quantitative and Qualitative Performance\nWe present the performance comparison of our algo-\nrithm and other state-of-the-art approaches on the nuScenes\nbenchmark in Table 1. PointPillars [24], SARPNET [59],\nWYSIWYG [18] and Tolist [35] are all voxel-based single-\nframe 3D object detectors. In particular, PointPillars is used\nas the baseline of our model. WYSIWYG is a recent al-\ngorithm that extends the PointPillars with a voxelized vis-\nibility map. Tolist uses a multi-head network that con-\ntains multiple prediction heads for different classes. Our\n3D video object detector outperforms these approaches by\na large margin. In particular, we improve the ofï¬cial Point-\nPillars algorithm by 15%. Please note that there is a se-\nvere class imbalance issue in the nuScenes dataset. The ap-\nproach in [63] designs a class data augmentation algorithm.\nFurther integrating with these techniques can promote the\nperformance of our model. But we focus on exploring the\nspatiotemporal coherence in the point cloud video, and han-\ndling the class imbalance issue is not the purpose in this\nwork. In addition, we further show some qualitative re-\nsults in Fig. 5. Besides the occlusion situation in Fig. 1,\nwe present another case of detecting the distant car (the car\non the top right), whose point clouds are especially sparse,\nwhich is very challenging for the single-frame detectors.\nAgain, our 3D video object detector effectively detects the\ndistant car using the attentive temporal information.\n(a) Detection results from the single-frame 3D object detector [24].\n(b) Detection results from our 3D video object detector.\nFigure 5: Detections for the distant cars.The grey and red\nboxes indicate the predictions and ground-truths, respectively.\n4.2. Ablation Study\nIn this section, we investigate the effectiveness of each\nmodule in our algorithm. Since the training samples in the\nnuScenes is 7Ã—as many as those in the KITTI (28,130 vs\n3,712), it is non-trivial to train multiple models on the whole\ndataset. Hence, we use a mini train set for validation pur-\nposes. It contains around 3,500 samples uniformly sampled\nfrom the original train set. Besides, PointPillars [24] is used\nas the baseline detector in our model.\nFirst, we evaluate our PMPNet in the spatial feature en-\ncoding component, by replacing the PFN in PointPillars\nwith PMPNet. As shown in Table 2, it improves the baseline\nby 2.05%. Second, we validate the ability of each module\nin the spatiotemporal feature aggregation component ( i.e.,\nConvGRU, STA-GRU and TTA-GRU) through adding each\nmodule to the PointPillars. We can see that all these mod-\nules achieve better performance than the single-frame de-\ntector. Moreover, we compare AST-GRU with other video\nobject detectors. Since each keyframe in nuScenes contains\npoint clouds merged by previous 10 non-keyframe sweeps,\nthe PointPillars baseline, trained on the merged keyframes,\ncan be deemed as the simplest video object detector. Our\nAST-GRU improves it by 5.98%. Then, we compare AST-\nGRU with the temporal 3D ConvNet-based method by im-\nplementing the late feature fusion module in [33]. Temporal\n3D ConvNet can only access a single keyframe label (0.5 s)\nduring training, and aggregating more labels instead impairs\nthe performance. According to Table 2, the 3D ConvNet-\nbased method surpasses the PointPillars baseline by 1.82%.\nComponents Modules Performance\nmAP âˆ†\nSingle-frame\n3D Object Detector\nPointPillars (PP) 21.30 -\nPP + PMPNet 23.35 +2.05\n3D Video\nObject Detector\nPP + 3D ConvNet 23.12 +1.82\nPP + ConvGRU 23.83 +2.53\nPP + STA-GRU 25.23 +3.93\nPP + TTA-GRU 25.32 +4.02\nPP + AST-GRU 27.28 +5.98\nFull Model 29.35 +8.05\nTable 2: Ablation study for our 3D video object detector.\nPointPillars [24] is the reference baseline for computing the rel-\native improvement (âˆ†).\nAspect Modules Performance\nmAP âˆ†\nInput Lengths\n(Full Model)\nT=1 16.84 -\nT=2 19.34 +2.50\nT=3 20.27 +3.43\nT=4 20.77 +3.93\nT=5 21.52 +4.68\nTable 3: Ablation study for the input lengths.Detection results\nwith one input frame are used as the reference baseline.\nIn contrast, our AST-GRU further outperforms it by 4.16%,\nwhich demonstrates the importance of the long-term tem-\nporal information. Finally, the full model with the PMPNet\nachieves the best performance.\nFinally, we analyze the effect of the input sequence\nlength. Since each keyframe contains quantities of point\nclouds that will increase the memory demand, we con-\nduct this experiment without using the point clouds in non-\nkeyframe sweeps. Experimental results with different input\nlengths are shown in Table. 3, which demonstrates that us-\ning the previous long-term temporal information (2.5 s) can\nconsistently gain better performance in 3D object detection.\n5. Conclusion\nThis paper proposed a new 3D video object detector\nfor exploring the spatiotemporal information in point cloud\nvideo. It has developed two new components: spatial fea-\nture encoding component and spatiotemporal feature aggre-\ngation component. We ï¬rst introduce a novel PMPNet that\nconsiders the spatial features of each point cloud frame.\nPMPNet can effectively enlarge the receptive ï¬eld of each\npillar grid through iteratively aggregating messages on a k-\nNN graph. Then, an AST-GRU module composed of STA\nand TTA is presented to mine the spatiotemporal coherence\nin consecutive frames by using an attentive memory gat-\ning mechanism. The STA focuses on detecting the fore-\nground objects, while the TTA aims to align the dynamic\nobjects. Extensive experiments on the nuScenes benchmark\nhave proved the better performance of our model.\nReferences\n[1] Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville.\nDelving deeper into convolutional networks for learning\nvideo representations. In ICLR, 2016.\n[2] Peter Battaglia, Razvan Pascanu, Matthew Lai,\nDanilo Jimenez Rezende, et al. Interaction networks\nfor learning about objects, relations and physics. In\nNeurIPS, 2016.\n[3] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Le-\nCun. Spectral networks and locally connected networks on\ngraphs. In ICLR, 2014.\n[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-\ntimodal dataset for autonomous driving. arXiv preprint\narXiv:1903.11027, 2019.\n[5] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In CVPR, 2017.\n[6] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast\npoint r-cnn. In ICCV, 2019.\n[7] Kyunghyun Cho, Bart Van Merri Â¨enboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation. Confer-\nence on Empirical Methods in Natural Language Processing,\n2014.\n[8] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and\nYoshua Bengio. Empirical evaluation of gated recurrent\nneural networks on sequence modeling. arXiv preprint\narXiv:1412.3555, 2014.\n[9] Micha Â¨el Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. Convolutional neural networks on graphs with\nfast localized spectral ï¬ltering. In NeurIPS, 2016.\n[10] Lifeng Fan, Wenguan Wang, Siyuan Huang, Xinyu Tang,\nand Song-Chun Zhu. Understanding human gaze communi-\ncation by spatio-temporal graph reasoning. In ICCV, 2019.\n[11] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In CVPR, 2012.\n[12] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol\nVinyals, and George E Dahl. Neural message passing for\nquantum chemistry. In ICML, 2017.\n[13] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A\nnew model for learning in graph domains. In IEEE Interna-\ntional Joint Conference on Neural Networks, 2005.\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016.\n[15] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convo-\nlutional networks on graph-structured data. arXiv preprint\narXiv:1506.05163, 2015.\n[16] Sepp Hochreiter and J Â¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735â€“1780, 1997.\n[17] Jilin Hu, Jianbing Shen, Bin Yang, and Ling Shao.\nInï¬nitely wide graph convolutional networks: Semi-\nsupervised learning via gaussian processes. arXiv preprint\narXiv:2002.12168, 2020.\n[18] Peiyun Hu, Jason Ziglar, David Held, and Deva Ramanan.\nWhat you see is what you get: Exploiting visibility for 3d\nobject detection. In CVPR, 2020.\n[19] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay\nPande, and Patrick Riley. Molecular graph convolutions:\nmoving beyond ï¬ngerprints. Journal of computer-aided\nmolecular design, 30(8):595â€“608, 2016.\n[20] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015.\n[21] Jason Ku, Melissa Moziï¬an, Jungwook Lee, Ali Harakeh,\nand Steven L Waslander. Joint 3d proposal generation and\nobject detection from view aggregation. In IROS, 2018.\n[22] Jason Ku, Alex D Pon, and Steven L Waslander. Monocular\n3d object detection leveraging accurate proposals and shape\nreconstruction. In CVPR, 2019.\n[23] Qiuxia Lai, Wenguan Wang, Hanqiu Sun, and Jianbing Shen.\nVideo saliency prediction using spatiotemporal residual at-\ntentive networks. TIP, 29:1113â€“1126, 2019.\n[24] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. In CVPR, 2019.\n[25] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiao-\ngang Wang. Gs3d: An efï¬cient 3d object detection frame-\nwork for autonomous driving. In CVPR, 2019.\n[26] Peiliang Li, Xiaozhi Chen, and Shaojie Shen. Stereo r-cnn\nbased 3d object detection for autonomous driving. In CVPR,\n2019.\n[27] Tao Li, Zhiyuan Liang, Sanyuan Zhao, Jiahao Gong, and\nJianbing Shen. Self-learning with rectiï¬cation strategy for\nhuman parsing. In CVPR, 2020.\n[28] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard\nZemel. Gated graph sequence neural networks. In ICLR,\n2016.\n[29] Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urta-\nsun. Multi-task multi-sensor fusion for 3d object detection.\nIn CVPR, 2019.\n[30] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun.\nDeep continuous fusion for multi-sensor 3d object detection.\nIn ECCV, 2018.\n[31] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr DollÂ´ar. Focal loss for dense object detection. In ICCV,\n2017.\n[32] Mason Liu and Menglong Zhu. Mobile video object detec-\ntion with temporally-aware feature maps. In CVPR, 2018.\n[33] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-\nous: Real time end-to-end 3d detection, tracking and motion\nforecasting with a single convolutional net. In CVPR, 2018.\n[34] Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. Roi-\n10d: Monocular lifting of 2d detection to 6d pose and metric\nshape. In CVPR, 2019.\n[35] nuTonomy. nuscenes 3d object detection challenge. https:\n//www.nuscenes.org/object-detection?\nexternalData=all&mapData=all&modalities=\nAny/.\n[36] Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina\nToutanova, and Wen-tau Yih. Cross-sentence n-ary relation\nextraction with graph lstms. Transactions of the Association\nfor Computational Linguistics, 5:101â€“115, 2017.\n[37] Charles R Qi, Or Litany, Kaiming He, and Leonidas J\nGuibas. Deep hough voting for 3d object detection in point\nclouds. In ICCV, 2019.\n[38] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J\nGuibas. Frustum pointnets for 3d object detection from rgb-d\ndata. In CVPR, 2018.\n[39] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classiï¬cation\nand segmentation. In CVPR, 2017.\n[40] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen,\nand Song-Chun Zhu. Learning human-object interactions by\ngraph parsing neural networks. In ECCV, 2018.\n[41] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Ha-\ngenbuchner, and Gabriele Monfardini. The graph neural net-\nwork model. IEEE Transactions on Neural Networks, 2008.\n[42] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\ncnn: 3d object proposal generation and detection from point\ncloud. In CVPR, 2019.\n[43] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,\nWai-Kin Wong, and Wang-chun Woo. Convolutional lstm\nnetwork: A machine learning approach for precipitation\nnowcasting. In NeurIPS, 2015.\n[44] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi,\nManuel L Â´opez-Antequera, and Peter Kontschieder. Disen-\ntangling monocular 3d object detection. In ICCV, 2019.\n[45] Leslie N Smith. Cyclical learning rates for training neural\nnetworks. In WACV, 2017.\n[46] Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye\nGuan, Yuchao Dai, Hao Su, Hongdong Li, and Ruigang\nYang. Apollocar3d: A large 3d car instance understanding\nbenchmark for autonomous driving. In CVPR, 2019.\n[47] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to\nsequence learning with neural networks. In NeurIPS, 2014.\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[49] Wenguan Wang, Xiankai Lu, Jianbing Shen, David J Cran-\ndall, and Ling Shao. Zero-shot video object segmentation\nvia attentive graph neural networks. In ICCV, 2019.\n[50] Wenguan Wang, Jianbing Shen, Ming-Ming Cheng, and\nLing Shao. An iterative and cooperative top-down and\nbottom-up inference network for salient object detection. In\nCVPR, 2019.\n[51] Wenguan Wang, Yuanlu Xu, Jianbing Shen, and Song-Chun\nZhu. Attentive fashion grammar network for fashion land-\nmark detection and clothing category classiï¬cation. In\nCVPR, 2018.\n[52] Wenguan Wang, Hailong Zhu, Jifeng Dai, Yanwei Pang,\nJianbing Shen, and Ling Shao. Hierarchical human parsing\nwith typed part-relation reasoning. In CVPR, 2020.\n[53] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018.\n[54] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariha-\nran, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar\nfrom visual depth estimation: Bridging the gap in 3d object\ndetection for autonomous driving. In CVPR, 2019.\n[55] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon. Dynamic\ngraph cnn for learning on point clouds. ACM Transactions\non Graphics, 38(5):146, 2019.\n[56] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-\nded convolutional detection. Sensors, 18(10):3337, 2018.\n[57] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-\ntime 3d object detection from point clouds. In CVPR, 2018.\n[58] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya\nJia. Std: Sparse-to-dense 3d object detector for point cloud.\nIn ICCV, 2019.\n[59] Yangyang Ye, Houjin Chen, Chi Zhang, Xiaoli Hao, and\nZhaoxiang Zhang. Sarpnet: Shape attention regional pro-\nposal network for lidar-based 3d object detection. Neuro-\ncomputing, 379:53â€“63, 2020.\n[60] Victoria Zayats and Mari Ostendorf. Conversation model-\ning on reddit using a graph-structured lstm. Transactions of\nthe Association for Computational Linguistics , 6:121â€“132,\n2018.\n[61] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\nYin, Yuchao Dai, and Ruigang Yang. Iou loss for 2d/3d ob-\nject detection. In International Conference on 3D Vision ,\n2019.\n[62] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning\nfor point cloud based 3d object detection. In CVPR, 2018.\n[63] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and\nGang Yu. Class-balanced grouping and sampling for point\ncloud 3d object detection. arXiv preprint arXiv:1908.09492,\n2019.\n[64] Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, and\nJifeng Dai. An empirical study of spatial attention mecha-\nnisms in deep networks. In ICCV, 2019.\n[65] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De-\nformable convnets v2: More deformable, better results. In\nCVPR, 2019."
}