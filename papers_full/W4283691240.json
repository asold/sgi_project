{
  "title": "Analyzing Encoded Concepts in Transformer Language Models",
  "url": "https://openalex.org/W4283691240",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2256769809",
      "name": "Hassan Sajjad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123720402",
      "name": "Nadir Durrani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2576593349",
      "name": "Fahim Dalvi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228779350",
      "name": "Firoj Alam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103651099",
      "name": "Abdul Khan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097564441",
      "name": "Jia Xu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3004725381",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W3104350794",
    "https://openalex.org/W2773956126",
    "https://openalex.org/W2966280323",
    "https://openalex.org/W2970030610",
    "https://openalex.org/W3164819786",
    "https://openalex.org/W2962935543",
    "https://openalex.org/W4280538731",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W3132711698",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W3034487470",
    "https://openalex.org/W3101717721",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W3174088532",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2020735245",
    "https://openalex.org/W2997244573",
    "https://openalex.org/W3024936740",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2951299559",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2489406233",
    "https://openalex.org/W3034513977",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2963483561",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2594470997",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963503967",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W4246122894",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2145910665",
    "https://openalex.org/W2946817437",
    "https://openalex.org/W3037626499",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2292919134",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2511550932"
  ],
  "abstract": "Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Khan, Jia Xu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3082 - 3101\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nAnalyzing Encoded Concepts in Transformer Language Models\nHassan Sajjad⋄ Nadir Durrani⋄ Fahim Dalvi⋄ Firoj Alam⋄ Abdul Rafae Khan† Jia Xu†\n{hsajjad,ndurrani,faimaduddin,fialam}@hbku.edu.qa\n⋄Qatar Computing Research Institute, HBKU Research Complex, Qatar\n{akhan4,jxu70}@stevens.edu\n†School of Engineering and Science, Steven Institute of Technology, USA\nAbstract\nWe propose a novel framework ConceptX,\nto analyze how latent concepts are encoded in\nrepresentations learned within pre-trained lan-\nguage models. It uses clustering to discover the\nencoded concepts and explains them by align-\ning with a large set of human-defined concepts.\nOur analysis on seven transformer language\nmodels reveal interesting insights: i) the la-\ntent space within the learned representations\noverlap with different linguistic concepts to a\nvarying degree, ii) the lower layers in the model\nare dominated by lexical concepts (e.g., affixa-\ntion), whereas the core-linguistic concepts (e.g.,\nmorphological or syntactic relations) are bet-\nter represented in the middle and higher layers,\niii) some encoded concepts are multi-faceted\nand cannot be adequately explained using the\nexisting human-defined concepts.1\n1 Introduction\nContextualized word representations learned in\ndeep neural network models (DDNs) capture rich\nconcepts making them ubiquitous for transfer learn-\ning towards downstream NLP. Despite their revolu-\ntion, the blackbox nature of the deep NLP models\nis a major bottle-neck for their large scale adapt-\nability. Understanding the inner dynamics of these\nmodels is important to ensure fairness, robustness,\nreliability and control.\nA plethora of research has been carried out to\nprobe DNNs for the linguistic knowledge (e.g. mor-\nphology, syntactic and semantic roles) captured\nwithin the learned representations. A commonly\nused framework to gauge how well linguistic infor-\nmation can be extracted from these models is the\nProbing Framework (Hupkes et al., 2018), where\nthey train an auxiliary classifier using representa-\ntions as features to predict the property of inter-\nest. The performance of the classifier reflects the\n1The code is available at https://github.com/\nhsajjad/ConceptX.\namount of knowledge learned within representa-\ntions. To this end, the researchers have analyzed\nwhat knowledge is learned within the representa-\ntions through relevant extrinsic phenomenon vary-\ning from word morphology (Vylomova et al., 2016;\nBelinkov et al., 2017a) to high level concepts such\nas syntactic structure (Blevins et al., 2018; Marvin\nand Linzen, 2018) and semantics (Qian et al., 2016;\nReif et al., 2019; Belinkov et al., 2017b) or more\ngeneric properties (Adi et al., 2016; Rogers et al.,\n2020).\nIn this work, we approach the representation\nanalysis from a different angle and present a novel\nframework ConceptX. In contrast to relying on\nthe prediction capacity of the representations, we\nanalyze the latent concepts learned within these rep-\nresentations and how knowledge is structured, us-\ning an unsupervised method. More specifically, we\nquestion: i) do the representations encode knowl-\nedge inline with linguistic properties such as word\nmorphology and semantics? ii) which properties\ndominate the overall structure in these representa-\ntions? iii) does the model learn any novel concepts\nbeyond linguistic properties? Answers to these\nquestions reveal how deep neural network models\nstructure language information to learn a task.\nOur inspiration to use the term concept comes\nfrom “concept based explanation” in computer\nvision (Kim et al., 2018; Ghorbani et al., 2019;\nChen et al., 2020). Stock (2010) defined a concept\nas “a class containing certain objects as elements,\nwhere the objects have certain properties”. We\ndefine an encoded concept as a cluster of context-\naware latent representations of words, where the\nrepresentations are encoder layer outputs.\nOur framework clusters contextualized repre-\nsentations using agglomerative hierarchical clus-\ntering (Gowda and Krishna, 1978). The result-\ning clusters represent encoded concepts, captured\nwithin the learned representations (Please see Fig-\nure 1 for illustration). We then use a novel align-\n3082\nFigure 1: ConceptX: i) Extract representations from trained model, ii) Cluster the representations to obtain encoded\nconcepts, iii) Align the concepts to human-defined concepts\nment function that measures the amount of over-\nlap between encoded concepts and a range of pre-\ndefined categories (that we call as human-defined\nconcepts in this paper). We experimented with\naffixes, casing, morphological, syntactic, seman-\ntic, WordNet (Miller, 1995), and psycholinguistic\nconcepts (LIWC Pennebaker et al. (2001)). The\nuse of such a diverse set of human-defined con-\ncepts enables us to cover various abstractions of\nlanguage. In Figure 3 we present a few examples\nof human-defined concepts that were aligned with\nthe encoded concepts.\nWe carry out our study on seven pre-trained\ntransformer models such as BERT (Devlin et al.,\n2019) and XLM-RoBERTa (Conneau et al., 2020),\nwith varying optimization functions, architectural\ndetails and training data. Some notable findings\nemerging from our analysis are as follows:\n• Shallow concepts such as lexical ngrams or\nsuffixes are predominantly captured in the\nlower layers of the network.\n• WordNet and psycholinguistic-based concepts\n(LIWC) are also learned in the lower layers.\n• Middle and higher layers encode concepts that\ncapture core linguistic properties such as mor-\nphology, semantics and syntax.\n• Roughly 50% of the encoded concepts adhere\nto our suite of human-defined linguistic con-\ncepts.\n• The models learn novel concepts that are\nmulti-faceted and cannot be adequately ex-\nplained using the existing human-defined con-\ncepts.\nOur contributions in this paper are as follow: i) We\npresent ConceptX, a framework that interprets\nencoded concepts in the learned representation by\nmeasuring their alignment to the human-defined\nconcepts. ii) We provide a qualitative and quan-\ntitative evidence of how knowledge is structured\nwithin deep NLP models with respect to a large\nsuite of human-defined concepts.\n2 Related Work\nMost of the work done on interpretability in deep\nNLP addresses two questions in particular: (i)\nwhat linguistic (and non-linguistic) knowledge is\nlearned within contextualized representations, Con-\ncept Analysis and (ii) how this information is uti-\nlized in the decision making process, Attribution\nAnalysis (Sajjad et al., 2021). The former thrives on\npost-hoc decomposability, where we analyze repre-\nsentations to uncover linguistic phenomenon that\nare captured as the network is trained towards any\nNLP task (Adi et al., 2016; Conneau et al., 2018;\nLiu et al., 2019a; Tenney et al., 2019; Belinkov\net al., 2020) and the latter characterize the role of\nmodel components and input features towards a\nspecific prediction (Linzen et al., 2016; Gulordava\net al., 2018; Marvin and Linzen, 2018). Our work\nfalls into the former category.\nPrevious studies have explored visualization\nmethods to analyze the learned representations\n(Karpathy et al., 2015; Kádár et al., 2017), atten-\ntion heads (Clark et al., 2019; Vig, 2019), language\ncompositionality (Li et al., 2016) etc. A more com-\nmonly used framework analyzes representations by\ncorrelating parts of the neural network with linguis-\ntic properties, by training a classifier to predict a\n3083\nfeature of interest (Adi et al., 2016; Belinkov et al.,\n2017a; Conneau et al., 2018). Several researchers\nused probing classifiers for investigating the con-\ntextualized representations learned from a variety\nof neural language models on a variety of character-\n(Durrani et al., 2019), word- (Liu et al., 2019a) or\nsub-sentence level (Tenney et al., 2019) linguistic\ntasks. Rather than analyzing the representations as\na whole, several researchers also explored identify-\ning salient neurons within the model that capture\ndifferent properties (Dalvi et al., 2019a; Durrani\net al., 2020; Suau et al., 2020; Mu and Andreas,\n2020) or are salient for the model irrespective of\nthe property (Bau et al., 2019; Wu et al., 2020).\nOur work is inline with (Michael et al., 2020;\nDalvi et al., 2022), who analyzed latent concepts\nlearned in pre-trained models. Michael et al. (2020)\nused a binary classification task to induce latent\nconcepts relevant to a task and showed the presence\nof linguistically motivated and novel concepts in\nthe representation. However, different from them,\nwe analyze representations in an unsupervised fash-\nion. Dalvi et al. (2022) used human-in-the-loop to\nanalyze latent spaces in BERT. Our framework uses\nhuman-defined concepts to automatically generate\nexplanations for the latent concepts. This enabled\nus to scale our study to many transformer models.\nIn a similar work, Mamou et al. (2020) ap-\nplied manifold analysis technique to understand\nthe amount of information stored about object cate-\ngories per unit. Our approach does away from the\nmethodological limitations of probing framework\nsuch as complexity of the probes, effect of random-\nness etc (Belinkov, 2021). However, it is important\nto mention that the two frameworks are orthogonal\nand complement each other.\n3 Methodology\nA vector representation in the neural network\nmodel is composed of feature attributes of the in-\nput words. We group the encoded vector repre-\nsentations using a clustering approach discussed\nbelow. The underlying clusters, that we term as\nthe encoded concepts, are then matched with the\nhuman-defined concepts using an alignment func-\ntion. Formally, consider a Neural Network (NN)\nmodel Mwith Lencoder layers {l1,l2,...ll,...,l L},\nwith H hidden nodes per layer. An input sentence\nconsisting of M words w1,w2,...wi,...,w M is fed\ninto a NN. For each input word i, we compute\nthe node output (after applying the activation func-\ntions) yl\nh(wi) of every hidden node h∈{1,...,H }\nin each layer l, where − →yl(wi) is the vector rep-\nresentation composing the outputs of all hidden\nnodes in layer lfor wi. Our goal is to cluster repre-\nsentations − →yl, from a large training data to obtain\nencoded concepts. We then align these with various\nhuman-defined concepts to obtain an explanation\nof them to build an understanding of how these\nconcepts are represented across the network.\n3.1 Clustering\nWe use agglomerative hierarchical cluster-\ning (Gowda and Krishna, 1978), which we found\nto be effective for this task. It assigns each word\nto a separate cluster and then iteratively combines\nthem based on Ward’s minimum variance criterion\nthat minimizes intra-cluster variance. Distance\nbetween two representations is calculated with\nthe squared Euclidean distance. The algorithm\nterminates when the required K clusters (aka\nencoded concepts) are formed, where K is a\nhyperparameter. Each encoded concept represents\na latent relationship between the words present in\nthe cluster. Appendix C presents the algorithm.\n3.2 Alignment\nNow we define the alignment function between the\nencoded and human-defined concepts. Consider\na human-defined concept as z, where a function\nz(w) = z denotes that z is the human-defined\nconcept of word w. For example, parts-of-speech\nis a human-defined concept and each tag such as\nnoun, verb etc. represents a class/label within the\nconcept, e.g. z(sea) = noun. Similarly, suffix\nis a human-defined concept with various suffixes\nrepresenting a class, e.g. z(bigger) = er. A re-\nverse function of z is a one-to-many function that\noutputs a set of unique words with the given human-\ndefined concept, i.e., z−1(z) = {w1,w2,...,w J},\nlike z−1(noun) = {sea,tree,... }, where J is\nthe total number of words with the human-defined\nconcept of z. Following this notation, an encoded\nconcept is indicated as c, where c(w) = c is a\nfunction of applying encoded concept on w, and\nits reverse function outputs a set of unique words\nwith the encoded concept of c, i.e., c−1(c) =\n{w1,w2,...,w I}, where I is the set size.\nTo align the encoded concepts with the human-\ndefined concepts, we auto-annotate the input data\nthat we used to get the clusters, with the human-\ndefined concepts. We call our encoded concept (c)\n3084\nto be θ-aligned (Λθ) with a human-defined concept\n(z) as follows:\nΛθ(z,c) =\n{\n1, if\n∑\nw′∈z−1\n∑\nw∈c−1 δ(w,w′)\nJ ≥θ\n0, otherwise,\nwhere Kronecker function δ(w,w′) is defined as\nδ(w,w′) =\n{1, if w= w′\n0, otherwise\nWe compute cand Λθ(z,c) for the encoder output\nfrom each layer lof a neural network. To compute\na network-wise alignment, we simply average θ-\nagreement over layers.\n4 Experimental Setup\n4.1 Dataset\nWe used a subset of WMT News 2018 2 (359M\ntokens) dataset. We randomly selected 250k sen-\ntences from the dataset (≈5M tokens) to train our\nclustering model. We discarded words with a fre-\nquency of less than 10 and selected maximum 10\noccurrences of a word type.3 The final dataset con-\nsists of 25k word types with 10 contexts per word.\n4.2 Pre-trained Models\nWe carried out our analysis on various 12-layered\ntransformer models such as BERT-cased (BERT-\nc, Devlin et al., 2019), BERT-uncased (BERT-uc),\nRoBERTa (Liu et al., 2019b), XLNet (Yang et al.,\n2019) and ALBERT (Lan et al., 2019). We also\nanalyzed multilingual models such as multilingual-\nbert-cased (mBERT) and XLM-RoBERTa (XLM-\nR, Conneau et al., 2020) where the embedding\nspace is shared across many languages. This choice\nof models is motivated from interesting differences\nin their architectural designs, training data settings\n(cased vs. un-cased) and multilinguality.\n4.3 Clustering and Alignment\nWe extract contextualized representation of words\nby performing a forward pass over the network us-\ning the NeuroX toolkit (Dalvi et al., 2019b). We\n2http://data.statmt.org/news-crawl/en/\n3Our motivation to select a small subset of data and lim-\niting the number of tokens is as follows: clustering a large\nnumber of high-dimensional vectors is computationally and\nmemory intensive, for example 200k vectors (of size 768 each)\nrequire around 400GB of CPU memory. Applying transforma-\ntions (e.g., PCA) to reduce dimensionality may result in loss\nof information and therefore undesirable. We wanted to stay\ntrue to the original embeddding space.\ncluster representations in every layer intoKgroups.\nTo find an optimum value of K, we experimented\nwith the ELbow (Thorndike, 1953) and Silhou-\nette (Rousseeuw, 1987) methods. However, we\ndid not observe reliable results (see Appendix C).\nTherefore, we empirically selected K = 1000\nbased on finding a decent balance between many\nsmall clusters (over-clustering) and a few large clus-\nters (under-clustering). We found that our results\nare not sensitive to this parameter and generalize\nfor different cluster settings (See Section 5.4). For\nthe alignment between encoded and human-defined\nconcepts, we use θ= 90% i.e., we consider an en-\ncoded concept and a human-defined concept to be\naligned, if they have at least 90% match.\n4.4 Human-defined concepts\nWe experiment with the various Human-defined\nconcepts, which we categorize into four groups:\n• Lexical Concepts: Ngrams, Affixes, Casing,\nFirst and the Last Word (in a sentence)\n• Morphology and Semantics: POS tags (Mar-\ncus et al., 1993) and SEM tags (Abzianidze\net al., 2017)\n• Syntactic: Chunking tags (Tjong Kim Sang\nand Buchholz, 2000) and CCG super-tags\n(Hockenmaier, 2006)\n• Linguistic Ontologies: WordNet (Miller,\n1995) and LIWC (Pennebaker et al., 2001)\nAt various places in this paper, we also refer to\nMorphology, Semantics and Syntactic concepts as\ncore-linguistic concepts. We trained BERT-based\nclassifiers using gold-annotated training data and\nstandard splits for each core-linguistic concepts\nand auto-labelled the selected news dataset using\nthese.4\n5 Analysis\nIn this section, we analyze the encoded concepts\nby aligning them with the human-defined concepts.\n5.1 Overall Alignment\nFirst we present to what extent the encoded con-\ncepts in the entire network align with the human-\ndefined concepts. We compute the overall score\nas the percentage of the aligned encoded concepts\nto the human-defined concepts across layers us-\ning the function described in Section 3.2. We\n4Please see Appendix B for details.\n3085\nBERT-c BERT-uc mBERT XLM-R RoBERTa ALBERT XLNet\nOverall alignment 47.2% 50.4% 66.0% 72.4% 50.1% 51.6% 43.6%\nTable 1: Coverage of human-defined concepts across all clusters of a given model\nFigure 2: Average Alignment (%) between encoded\nconcepts and human-defined concepts\nfound an overall match of at least 43.6% in XL-\nNet and at most 72.4% in XLM-R (See Table 1).\nInterestingly, the multilingual models (mBERT and\nXLM-R) found substantially higher match than the\nmonolingual models. The inclusion of multiple\nlanguages during training causes the model to learn\nmore linguistic properties. Note that the extent of\nalignment with the human-defined concept may not\nnecessarily correlate with its overall performance.\nFor example XLNet performs outperforms BERT\non the GLUE tasks, but aligns less with the human-\ndefined concepts compared to BERT in our results.\nA similar observation was made by Belinkov et al.\n(2020) who also found that the translation quality of\nan NMT model may not correlate with the amount\nof linguistic knowledge learned in the representa-\ntion. Various factors such as: architectural design,\ntraining data, objective function, initialization, etc,\nplay a role in training a pre-trained model. More\ncontrolled experiments are needed to understand\nthe relationship of each factor on the performance\nof the model and on the linguistic learning of the\nmodel.\nWe further investigated per concept5 alignment\n5The first word, last word and prefix concepts showed less\nless than 1% alignment with the encoded concepts. We do not\nto understand which human-defined concepts are\nbetter represented within the encoded concepts.\nFigure 2 presents the results.\nLexical Concepts Pre-trained models encode\nvarying amount of lexical concepts such as casing,\nngrams and suffixes. We found between 7-11% en-\ncoded concepts that align with the casing concept\n(title case or upper case). We observed that most of\nthese encoded concepts consist of named entities,\nwhich were grouped together based on semantics.\nComparing suffixes and ngrams While affixes\noften have linguistic connotation (e.g., the prefix\nanti negates the meaning of the stem and the suffix\nies is used for pluralization), the ngram units that\nbecome part of the vocabulary as an artifact of\nstatistical segmentation (e.g., using BPE (Sennrich\net al., 2016) or Word-piece (Schuster and Nakajima,\n2012)) often lack any linguistic meaning. However,\nmodels learn to encode such information. We found\na match ranging from 1% (BERT-cased) up to 25%\n(XLM-R) when comparing encoded concepts with\nthe suffix concept. A similar pattern is observed\nin the case of the ngram concept (which is a super-\nset of the suffix concept) where a staggering 48%\nmatches were found. Figure 6a shows an ngram\ncluster found in layer 2 of BERT-c.6\nMorphology and Semantics We found that the\nencoded concepts based on word morphology\n(POS) consistently showed a higher match across\nall models in comparison to the other abstract con-\ncepts, aligning a quarter of the encoded concepts\nin the case of mBERT. The alignment with seman-\ntic concepts is relatively lower, with at most 16%\nmatch across models. This reflects that while the\nmodels learn both linguistic properties, morpholog-\nical ontology is relatively preferred compared to\nthe semantic hierarchy.\nSyntactic These concepts capture grammatical\norientation of a word, for example Chunking:B-NP\nis a syntactic concept describing words in the be-\nginning of a noun phrase. CCG:PP/NP is a concept\npresent their results in the interest of space.\n6Appendix A shows more examples of the ngram, suffix,\nLIWC and WordNet clusters.\n3086\n(a) Ngram:ex\n (b) LIWC:religion\n (c) LIWC: Bio\n (d) WordNet:Motion\nFigure 3: Examples of BERT-c encoded concepts aligned with the human-defined concepts\nin CCG super tagging, describing words that takes\na noun phrase on the right and outputs a preposi-\ntion phrase for example “[in[the US]]”. We found\nrelatively fewer matches, a maximum of 7% and\n14% matching encoded concepts for Chunking and\nCCG concepts respectively. The low matches for\nsyntactic concepts suggest that the models do not\nencode the same syntactic hierarchy suggested by\nthese human-defined syntactic tasks.\nLinguistic Ontologies Comparing the encoded\nconcepts with static linguistic ontologies, we found\nWordNet concepts to be the second most aligned\nconcept (11-21%) with the human-defined con-\ncepts. LIWC also shows a relatively higher align-\nment compared to the other human-defined con-\ncepts in a few models (e.g., BERT-c). However,\nthis observation is not consistent across models and\nwe found a range between 5-16% matches. These\nresults present an interesting case where several\nmodels prefer the distinction of lexical ontology\nover abstract linguistic concepts such as morphol-\nogy. Figure 3 shows examples of encoded concepts\naligned with WordNet and LIWC. We see that these\nconcepts are built based on a semantic relationship\ne.g., the clusters in Figure 3b, 3c and 3d group\nwords based on religious, facial anatomy, and spe-\ncific motion-related vocabulary respectively.\nComparing Models The results of multilingual\nmodels (mBERT, XLM-R) are intriguing given that\ntheir encoded concepts are dominated by ngram-\nbased concepts and POS concepts, and their rela-\ntively lesser alignment with the linguistic ontolo-\ngies. On the contrary, several monolingual models\n(BERT-c, ALBERT) showed a better match with\nlinguistic ontologies specially WordNet.\nThe higher number of matches to the ngram (and\nsuffix) concepts in the multilingual models is due to\nthe difference in subword segmentation. The sub-\nword models in XLM-R and mBERT are optimized\nfor multiple languages, resulting in a vocabulary\nconsisting of a large number of small ngram units.\nThis causes the multilingual models to aggressively\nsegment the input sequence, compared to the mono-\nlingual models7 and resulted in highly dominated\nngram-based encoded concepts, especially in the\nlower layers. This may also explain the relatively\nlower match that multilingual models exhibit to the\nlinguistic ontologies. We discuss this further in the\ncontext of layer-wise analysis in Section 5.2.\nComparing BERT cased vs. uncased, interest-\ningly BERT-uc consistently showed higher matches\nfor the core-linguistic concepts (See Figure 2). We\nspeculate that in the absence of casing informa-\ntion, BERT-uc is forced to learn more linguistic\nconcepts, whereas BERT-c leverages the explicit\ncasing information to capture more semantically\nmotivated concepts based on linguistic ontologies.\nThe higher matches in multilingual models in\ncomparison to the monolingual models, and BERT-\nuncased in comparison to BERT-cased suggest that\nthe training complexity is one factor that plays a\nrole in a model’s ability to learn linguistic nuances.\nFor example, multilingual models need to optimize\nmany languages, which is a harder task compared\nto learning one language. Similarly, the absence\nof capitalization in training data makes the learn-\ning task relatively harder for BERT-uc compared to\nBERT-c models, thus resulting in higher matches\nfor BERT-uc. We speculate that the harder the train-\ning task, the more language nuances are learned\nby a model. Belinkov et al. (2020) made a similar\nobservation, where they showed that the linguistic\nknowledge learned within the encoder-decoder rep-\nresentations in NMT models correlates with com-\nplexity of a language-pair involved in the task.\n5.2 Layer-wise Alignment\nNow we study the alignment of human-defined\nconcepts across layers to understand how concepts\n7In our dataset, mBERT has 13% more words after sub-\nword segmentation compared to BERT-c.\n3087\nFigure 4: Layer-wise concept alignment. Y-axis is the normalized number of aligned concepts. The number within\nbrackets of each human-defined concept, e.g. Casing (166), shows the maximum layer-wise match\nevolve in the network. Figure 4 shows results for\nselected models.8 The y-axis is the normalized\nnumber of aligned concepts across layers.\nOverall Trend We observed mostly consistent\npatterns across models except for ALBERT, which\nwe will discuss later in this section. We found that\nthe shallow concepts (such as ngram and suffixes)\nand the linguistic ontologies (LIWC and WORD-\nNET) are better represented in the initial layers and\nexhibit a downward trend in the higher layers of\nthe network. On the contrary the core linguistic\nconcepts (POS, Chunking, etc.) are better repre-\n8See Figure 10 in the Appendix for complete results.\nsented in the higher layers (layer 8-10). The last\nlayers do not show any consistently dominating\nhuman-defined concepts considered in this work.\nWe can generalize on these trends and hypothesize\non how encoded concepts evolve in the network:\nthe initial layers of the pretrained models, group\nwords based on their lexical and semantic similar-\nities where the former is an artifact of subword\nsegmentation. With the inclusion of context and ab-\nstraction in the higher layers, these groups evolve\ninto linguistic manifolds. The encoded concepts\nin the last layers are influenced by the objective\nfunction and learn concepts relevant to the task.\nDurrani et al. (2021) also made similar observation\n3088\nwhen analyzing linguistic concepts in pre-trained\nmodels that are fine-tuned towards different GLUE\ntasks.\nConcept-wise Trend In the following, we dis-\ncuss different concepts in detail. As we mentioned\nearlier, the high presence of ngram and suffix con-\ncepts in the lower layers is due to subword seg-\nmentation. At the higher layers, the models start\nencoding abstract concepts, therefore get better\nalignment with the core linguistic concepts. Cas-\ning shows an exception to other lexical concepts\nand has similar trend to POS and SEM. Upon in-\nvestigating we observed that the words appearing\nin these clusters have a hybrid connotation. For\nexample, more than 98% of the encoded concepts\nthat match with Casing are named entities, which\nexplains the trend. The syntactic concepts observe\npeak in the higher-middle layers and a downward\ntrend towards the end. These findings resonate\nwith the earlier work on interpreting neural net-\nwork representations for BERT. For example Liu\net al. (2019a) also showed that probes trained with\nlayers 7-8 give the highest accuracy when trained\ntowards predicting the tasks of Chunking and CCG\ntagging. Although here, we are targeting a slightly\ndifferent question i.e. how the latent concepts are\nencoded within the representations and how they\nevolve from input to output layers of the network.\nWe observed a downward trend in linguistic on-\ntologies (WordNet, LIWC) as we go from lower\nlayers to higher layers as opposed to the core lin-\nguistic concepts (POS, CCG, etc.). This is because\nof the context independent nature of these concepts\nas opposed to the core-linguistic concepts which\nare annotated based on the context. The embed-\nding layer is non-contextualized, thus shows a high\nmatch with linguistic ontologies. With the availabil-\nity of context in contextualized layers, the encoded\nconcepts evolve into context-aware groups, result-\ning in higher matches with core-linguistic concepts.\nComparing Models While the overall trend is\nconsistent among BERT-uc, mBERT and XLNet\n(and other studied models – Figure 10 in Appendix),\nthe models somewhat differ in the last layers: see\nthe large drop in core-linguistic concepts such as\nPOS and Chunking for XLNet and mBERT in com-\nparison to BERT. This suggests that BERT retains\nmuch of the core-linguistic information at the last\nlayers. Durrani et al. (2020) observed a similar\npattern in their study, where they showed BERT to\nretain linguistic information deeper in the model as\nopposed to XLNet where it was more localized and\npredominantly preserved earlier in the network.\nWhile the overall layer-wise trends of multilin-\ngual models look similar to some monolingual mod-\nels (mBERT vs. XLNet in Fig 4b,c), the former’s\nabsolute layer-wise matches (numbers inside the\nbrackets in Figure 4 e.g. Casing (166)) are gener-\nally substantially higher than the monolingual coun-\nterparts. For example, the POS and SEM matches\nof mBERT are 38.9% and 30% respectively which\nare 18% and 15% higher than BERT-uc. On the\ncontrary, the number of matches with linguistic\nontologies is often lower for multilingual models\n(mBERT LIWC alignment of 65 vs. BERT-uc align-\nment of 186). We hypothesize that the variety of\ntraining languages in terms of their morphological\nand syntactic structure has caused the multilingual\nmodels to learn more core-linguistic concepts in\norder to optimize the training task. Although, the\nknowledge captured within linguistic ontologies is\nessential, it may not be as critical to the training of\nthe model as the linguistic concepts.\nALBERT showed a very different trend from\nthe other models. Note that ALBERT shares param-\neters across layers while the other models have sep-\narate parameters for every layer. This explains the\nALBERT results where we see relatively less varia-\ntion across layers. More interestingly, the encoded\nconcepts in the last layers of ALBERT showed\npresence of all human-defined concepts considered\nhere (see the relatively smaller drop of ALBERT\nalignment curves in Figure 4).\n5.3 Unaligned Concepts\nIn Table 1 we observed that at least 27.6% (in\nXLM-R) and up to 56.4% (in XLNet) encoded\nconcepts did not align with the human-defined con-\ncepts. What concepts do these unaligned clusters\ncontain? In an effort to answer this question, we\nanalyzed these clusters and observed that many of\nthem were compositional concepts that involves\nmore than one fine-grained categories of the human\ndefined concepts. Figure 5a shows an example of\nthe unaligned concept which partly aligns with a\nsemantic category (SEM:geopolitical entity) and a\nmorphological category (POS:adjective). Similarly,\nFigure 5b is a verbs related to cognitive processes\nand Figure 5c shows an unaligned cluster that is\ncomposed of different verb forms (past, present\nand gerunds). The alignment with multiple human-\n3089\n(a) SEM:GPE+POS:JJ\n (b) POS:VB*+LIWC:cogmech\n (c) POS:VB*\nFigure 5: Examples of unaligned encoded concepts: (a) combination of geopolitical entities and their related\nadjectives, (b,c) different forms of verb with specific semantics\ndefined concepts can be used to generate explana-\ntions for these unaligned concepts. For example,\nFigure 5a can be aligned as a mix of geopolitical en-\ntities and adjectives. We also quantitatively verified\nthe number of unaligned encoded concepts that can\nbe explained using composition of different con-\ncepts (See Appendix E: Table 9) and found that a\nmajority of the clusters can be explained using a\ncombination of three pre-defined concepts..\nMoreover, note that encoded concepts are often\nmultifacet i.e., they represent more than one re-\nlationship. For example, the encoded concept in\nFigure 5c consists of different forms of verbs but at\nthe same time, these verbs are semantically similar.\nThe semantic relationship present here is not ade-\nquately captured using the human-defined concepts\nused in this work. These are the novel concepts that\nrequire richer annotations or human-in-the-loop\nsetup to generate adequate explanations.\n5.4 Generalization of Results\nDo the results generalize over different dataset se-\nlection and using different number of clusters? We\nran experiments using different split of the news\ndataset for several models, and also performed\nalignment using different values of K, the num-\nber of clusters. The results are consistent across\nthe board. Please see Appendix F for details.\n6 Conclusion\nWe presented ConceptX, a novel framework for\nanalyzing the encoded concepts within deep NLP\nmodels. Our method uses unsupervised clustering\nto discover latent concepts within the contextual-\nized representations and then aligned these con-\ncepts with a suite of human-defined concepts to\ngenerate explanations for them. Our results illumi-\nnate how DNNs structure language information. A\nfew notable findings are: i) lower layers capture\nshallow linguistic concepts, ii) whereas the abstract\nlinguistic concepts such as morphology and seman-\ntics are preserved higher in the network, iii) the\nextent of alignment varies across different models\nand different human-defined concepts, iv) we found\nthat novel explanations and an improved coverage\nof concepts can be achieved via compositionality.\nReferences\nLasha Abzianidze, Johannes Bjerva, Kilian Evang, Hes-\nsel Haagsma, Rik van Noord, Pierre Ludmann, Duc-\nDuy Nguyen, and Johan Bos. 2017. The parallel\nmeaning bank: Towards a multilingual corpus of\ntranslations annotated with compositional meaning\nrepresentations. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics, EACL ’17, pages 242–\n247, Valencia, Spain.\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi,\nand Yoav Goldberg. 2016. Fine-grained Analysis of\nSentence Embeddings Using Auxiliary Prediction\nTasks. arXiv preprint arXiv:1608.04207.\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2019. Iden-\ntifying and controlling important neurons in neural\nmachine translation. In International Conference on\nLearning Representations.\nYonatan Belinkov. 2021. Probing classifiers:\nPromises, shortcomings, and alternatives. CoRR,\nabs/2102.12452.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017a. What do Neural\nMachine Translation Models Learn about Morphol-\nogy? In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (ACL),\nVancouver. Association for Computational Linguis-\ntics.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2020. On the linguistic\nrepresentational power of neural machine translation\nmodels. Computational Linguistics, 45(1):1–57.\n3090\nYonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2017b. Eval-\nuating Layers of Representation in Neural Machine\nTranslation on Part-of-Speech and Semantic Tagging\nTasks. In Proceedings of the 8th International Joint\nConference on Natural Language Processing (IJC-\nNLP).\nTerra Blevins, Omer Levy, and Luke Zettlemoyer. 2018.\nDeep RNNs encode soft hierarchical syntax. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 14–19, Melbourne, Australia. Associ-\nation for Computational Linguistics.\nZhi Chen, Yijie Bei, and Cynthia Rudin. 2020. Con-\ncept whitening for interpretable image recognition.\nNature Machine Intelligence, 2(12):772–782.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 8440–8451.\nAssociation for Computational Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single vector: Probing sentence\nembeddings for linguistic properties. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (ACL).\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, D. Anthony Bau, and James Glass. 2019a.\nWhat is one grain of sand in the desert? analyzing in-\ndividual neurons in deep nlp models. In Proceedings\nof the Thirty-Third AAAI Conference on Artificial\nIntelligence (AAAI, Oral presentation).\nFahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir Dur-\nrani, Jia Xu, and Hassan Sajjad. 2022. Discovering\nlatent concepts learned in BERT. In International\nConference on Learning Representations.\nFahim Dalvi, Avery Nortonsmith, D. Anthony Bau,\nYonatan Belinkov, Hassan Sajjad, Nadir Durrani, and\nJames Glass. 2019b. Neurox: A toolkit for analyz-\ning individual neurons in neural networks. In AAAI\nConference on Artificial Intelligence (AAAI).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nNadir Durrani, Fahim Dalvi, Hassan Sajjad, Yonatan Be-\nlinkov, and Preslav Nakov. 2019. One size does not\nfit all: Comparing NMT representations of different\ngranularities. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n1504–1516, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNadir Durrani, Hassan Sajjad, and Fahim Dalvi. 2021.\nHow transfer learning impacts linguistic knowledge\nin deep NLP models? In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4947–4957, Online. Association for Computa-\ntional Linguistics.\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4865–4880, Online. Association for Computational\nLinguistics.\nAmirata Ghorbani, James Wexler, James Y Zou, and\nBeen Kim. 2019. Towards automatic concept-based\nexplanations. Advances in Neural Information Pro-\ncessing Systems, 32:9277–9286.\nK Chidananda Gowda and G Krishna. 1978. Agglomer-\native clustering using the concept of mutual nearest\nneighbourhood. Pattern recognition, 10(2):105–112.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless green\nrecurrent networks dream hierarchically. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1195–1205, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nJulia Hockenmaier. 2006. Creating a CCGbank and a\nwide-coverage CCG lexicon for German. In Proceed-\nings of the 21st International Conference on Compu-\ntational Linguistics and 44th Annual Meeting of the\nAssociation for Computational Linguistics, ACL ’06,\npages 505–512, Sydney, Australia.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and ’diagnostic classifiers’ reveal\nhow recurrent and recursive neural networks process\nhierarchical structure.\nAkos Kádár, Grzegorz Chrupała, and Afra Alishahi.\n2017. Representation of linguistic form and func-\ntion in recurrent neural networks. Computational\nLinguistics, 43(4):761–780.\n3091\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078.\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie\nCai, James Wexler, Fernanda Viegas, et al. 2018. In-\nterpretability beyond feature attribution: Quantitative\ntesting with concept activation vectors (tcav). In In-\nternational conference on machine learning, pages\n2668–2677. PMLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. ArXiv:1909.11942.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691, San Diego, California.\nAssociation for Computational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521– 535.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv:1907.11692.\nJonathan Mamou, Hang Le, Miguel Del Rio, Cory\nStephenson, Hanlin Tang, Yoon Kim, and Sueyeon\nChung. 2020. Emergence of separable manifolds\nin deep language representations. In International\nConference on Machine Learning, pages 6713–6723.\nPMLR.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated cor-\npus of English: The Penn Treebank. Computational\nLinguistics, 19(2):313–330.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJulian Michael, Jan A. Botha, and Ian Tenney. 2020.\nAsking without telling: Exploring latent ontologies\nin contextual representations. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6792–6812,\nOnline. Association for Computational Linguistics.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM, 38(11):39–41.\nJesse Mu and Jacob Andreas. 2020. Compositional\nexplanations of neurons. CoRR, abs/2006.14032.\nJames W Pennebaker, Martha E Francis, and Roger J\nBooth. 2001. Linguistic inquiry and word count:\nLiwc 2001. Mahway: Lawrence Erlbaum Associates,\n71(2001):2001.\nPeng Qian, Xipeng Qiu, and Xuanjing Huang. 2016.\nInvestigating Language Universal and Specific Prop-\nerties in Word Embeddings. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1478–1488, Berlin, Germany. Association for Com-\nputational Linguistics.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nPeter Rousseeuw. 1987. Silhouettes: a graphical aid to\nthe interpretation and validation of cluster analysis.\nJ. Comput. Appl. Math., 20(1):53–65.\nHassan Sajjad, Narine Kokhlikyan, Fahim Dalvi, and\nNadir Durrani. 2021. Fine-grained interpretation and\ncausation analysis in deep NLP models. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies: Tutorials,\npages 5–10, Online. Association for Computational\nLinguistics.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In 2012 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 5149–5152. IEEE.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nWolfgang G Stock. 2010. Concepts and semantic rela-\ntions in information science. Journal of the Ameri-\ncan Society for Information Science and Technology,\n61(10):1951–1969.\n3092\nXavier Suau, Luca Zappella, and Nicholas Apostoloff.\n2020. Finding experts in transformer models. CoRR,\nabs/2005.07647.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nRobert L. Thorndike. 1953. Who belongs in the family.\nPsychometrika, pages 267–276.\nErik F. Tjong Kim Sang and Sabine Buchholz. 2000. In-\ntroduction to the CoNLL-2000 shared task chunking.\nIn Fourth Conference on Computational Natural Lan-\nguage Learning and the Second Learning Language\nin Logic Workshop.\nJesse Vig. 2019. A multiscale visualization of attention\nin the transformer model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 37–42,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nEkaterina Vylomova, Trevor Cohn, Xuanli He, and Gho-\nlamreza Haffari. 2016. Word Representation Models\nfor Morphologically Rich Languages in Neural Ma-\nchine Translation. arXiv preprint arXiv:1606.04217.\nJohn Wu, Hassan Belinkov, Yonatan Sajjad, Nadir Dur-\nrani, Fahim Dalvi, and James Glass. 2020. Similarity\nAnalysis of Contextual Word Representation Mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics (ACL),\nSeattle. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\n3093\nAppendix\nA Human-defined concept labels\nA.1 Lexical Concepts:\nNgrams, Affixes, Casing, First and the Last Word.\nA.2 Morphology and Semantics:\nPOS tags: We used the Penn Treebank POS tags\ndiscussed in (Marcus et al., 1993), which consists\nof 36 POS tags and 12 other tags (i.e., punctuation\nand currency symbols). In Table 2, we provide\nPOS tags and their description.\nSEM tags: (Abzianidze et al., 2017) consists of\n73 sem-tags grouped into 13 meta-tags. In Table\n3, we provide a detailed information of the tagset,\nand in Table 5, we provide fine and coarse tags\nmapping.\nA.3 Syntactic:\nChunking tags: For Chunking we used the tagset\ndiscussed in (Tjong Kim Sang and Buchholz,\n2000), which consists of 11 tags as follows: NP\n(Noun phrase), VP (Verb phrase), PP (Prepositional\nphrase), ADVP (Adverb phrase), SBAR (Subor-\ndinate phrase), ADJP (Adjective phrase), PRT\n(Particles), CONJP (Conjunction), INTJ (Interjec-\ntion), LST (List marker), UCP (Unlike coordinate\nphrase). For the annotation, chunks are represented\nusing IOB format, which results in 22 tags in the\ndataset as reported in Table 4.\nCCG super-tags Hockenmaier (2006) devel-\noped, CCGbank, a dataset with Combinatory Cat-\negorial Grammar (CCG) derivations and depen-\ndency structures from the Penn Treebank. CCG is\na lexicalized grammar formalism, which is expres-\nsive and efficiently parseable. It consists of 1272\ntags.\nA.4 Linguistic Ontologies:\nWordNet: (Miller, 1995) consists of 26 lexico-\ngraphic senses for nouns, 2 for adjectives, and 1\nfor adverbs. Each of them represent a supersense\nand a hierarchy can be formed from hypernym to\nhyponym.\nLIWC: Over the past few decades, Pennebaker\net al. (Pennebaker et al., 2001) have designed psy-\ncholinguistic concepts using high frequency words.\nThese word categories are mostly used to study\ngender, age, personality, and health to estimate the\n# Tag Description\n1 CC Coordinating conjunction\n2 CD Cardinal number\n3 DT Determiner\n4 EX Existential there\n5 FW Foreign word\n6 IN Preposition or subordinating conjunction\n7 JJ Adjective\n8 JJR Adjective, comparative\n9 JJS Adjective, superlative\n10 LS List item marker\n11 MD Modal\n12 NN Noun, singular or mass\n13 NNS Noun, plural\n14 NNP Proper noun, singular\n15 NNPS Proper noun, plural\n16 PDT Predeterminer\n17 POS Possessive ending\n18 PRP Personal pronoun\n19 PRP$ Possessive pronoun\n20 RB Adverb\n21 RBR Adverb, comparative\n22 RBS Adverb, superlative\n23 RP Particle\n24 SYM Symbol\n25 TO to\n26 UH Interjection\n27 VB Verb, base form\n28 VBD Verb, past tense\n29 VBG Verb, gerund or present participle\n30 VBN Verb, past participle\n31 VBP Verb, non-3rd person singular present\n32 VBZ Verb, 3rd person singular present\n33 WDT Wh-determiner\n34 WP Wh-pronoun\n35 WP$ Possessive wh-pronoun\n36 WRB Wh-adverb\n37 # Pound sign\n38 $ Dollar sign\n39 . Sentence-final punctuation\n40 , Comma\n41 : Colon, semi-colon\n42 ( Left bracket character\n43 ) Right bracket character\n44 \" Straight double quote\n45 ’ Left open single quote\n46 \" Left open double quote\n47 ’ Right close single quote\n48 \" Right close double quote\nTable 2: Penn Treebank POS tags.\ncorrelation between these attributes and word us-\nage. It is a knowledge-based system where words\nare mapped different high level concepts.\nB BERT-based Sequence Tagger\nWe trained a BERT-based sequence tagger to auto-\nannotate our training data. We used standard splits\nfor training, development and test data for the 4\nlinguistic tasks (POS, SEM, Chunking and CCG\nsuper tagging) that we used to carry out our analy-\nsis on. The splits to preprocess the data are avail-\n3094\nANA (anaphoric) MOD (modality)\nPRO anaphoric & deictic pronouns: he, she, I, him NOT negation: not, no, neither, without\nDEF definite: the, loIT, derDE NEC necessity: must, should, have to\nHAS possessive pronoun: my, her POS possibility: might, could, perhaps, alleged, can\nREF reflexive & reciprocal pron.: herself, each other DSC (discourse)\nEMP emphasizing pronouns: himself SUB subordinate relations: that, while, because\nACT (speech act) COO coordinate relations: so, {,}, {;}, and\nGRE greeting & parting: hi, bye APP appositional relations: {,}, which, {(}, —\nITJ interjections, exclamations: alas, ah BUT contrast: but, yet\nHES hesitation: err NAM (named entity)\nQUE interrogative: who, which, ? PER person: Axl Rose, Sherlock Holmes\nATT (attribute) GPE geo-political entity: Paris, Japan\nQUC concrete quantity: two, six million, twice GPO geo-political origin: Parisian, French\nQUV vague quantity: millions, many, enough GEO geographical location: Alps, Nile\nCOL colour: red, crimson, light blue, chestnut brown ORG organization: IKEA, EU\nIST intersective: open, vegetarian, quickly ART artifact: iOS 7\nSST subsective: skillful surgeon, tall kid HAP happening: Eurovision 2017\nPRI privative: former, fake UOM unit of measurement: meter, $, %, degree Celsius\nDEG degree: 2 meters tall, 20 years old CTC contact information: 112, info@mail.com\nINT intensifier: very, much, too, rather URL URL: http://pmb.let.rug.nl\nREL relation: in, on, ’s, of, after LIT literal use of names: his name is John\nSCO score: 3-0, grade A NTH other names: table 1a, equation (1)\nCOM (comparative) EVE (events)\nEQU equative: as tall as John, whales are mammals EXS untensed simple: to walk, is eaten, destruction\nMOR comparative positive: better, more ENS present simple: we walk, he walks\nLES comparative negative: less, worse EPS past simple: ate, went\nTOP superlative positive: most, mostly EXG untensed progressive: is running\nBOT superlative negative: worst, least EXT untensed perfect: has eaten\nORD ordinal: 1st, 3rd, third TNS (tense & aspect)\nUNE (unnamed entity) NOW present tense: is skiing, do ski, has skied, now\nCON concept: dog, person PST past tense: was baked, had gone, did go\nROL role: student, brother, prof., victim FUT future tense: will, shall\nGRP group: John {,} Mary and Sam gathered, a group of people PRG progressive: has been being treated, aan hetNL\nDXS (deixis) PFT perfect: has been going/done\nDXP place deixis: here, this, above TIM (temporal entity)\nDXT temporal deixis: just, later, tomorrow DAT full date: 27.04.2017, 27/04/17\nDXD discourse deixis: latter, former, above DOM day of month: 27th December\nLOG (logical) YOC year of century: 2017\nALT alternative & repetitions: another, different, again DOW day of week: Thursday\nXCL exclusive: only, just MOY month of year: April\nNIL empty semantics: {.}, to, of DEC decade: 80s, 1990s\nDIS disjunction & exist. quantif.: a, some, any, or CLO clocktime: 8:45 pm, 10 o’clock, noon\nIMP implication: if, when, unless\nAND conjunction & univ. quantif.: every, and, who, any\nTable 3: Semantic tags.\nTask Train Dev Test Tags F1\nPOS 36557 1802 1963 4896.69\nSEM 36928 5301 10600 7396.22\nChunking8881 1843 2011 22 96.91\nCCG 39101 1908 2404 127294.90\nTable 4: Data statistics (number of sentences) on train-\ning, development and test sets using in the experiments\nand the number of tags to be predicted\nable through git repository9 released with Liu et al.\n(2019a). See Table 4 for statistics and classifier\naccuracy.\n9https://github.com/nelson-liu/\ncontextual-repr-analysis\n3095\nC Clustering details\nAlgorithm 1 assigns each word to a separate clus-\nter and then iteratively combines them based on\nWard’s minimum variance criterion that minimizes\nintra-cluster variance. Distance between two vec-\ntor representations is calculated with the squared\nEuclidean distance.\nAlgorithm 1 Clustering Procedure\nInput: − →yl: word representation of words\nParameter: K: the total number of clus-\nters\n1: for each word wi do\n2: assign wi to cluster ci\n3: end for\n4: while number of clusters ̸= Kdo\n5: for each cluster pair ci,ci′ do\n6: di,i′ = inner-cluster difference of com-\nbined cluster ci and ci′\n7: end for\n8: cj,cj′ = cluster pair with minimum value of\nd\n9: merge clusters cj and cj′\n10: end while\nC.1 Selection of the number of Clusters\nThe Elbow curve did not show any optimum clus-\ntering point, with the increase in number of clus-\nters the distortion score kept decreasing, resulting\nin over-clustering (a large number of clusters con-\nsisted of less than 5 words). The over-clustering\nresulted in high but wrong alignment scores e.g.\nconsider a two word cluster having words “good”\nand “great”. The cluster will have a successful\nmatch with “adjective” since more than 90% of the\nwords in the cluster are adjectives. In this way, a lot\nof small clusters will have a successful match with\nmany human-defined concepts and the resulting\nalignment scores will be high. On the other hand,\nSilhouette resulted in under-clustering, giving the\nbest score at number of clusters = 10. We handled\nthis empirically by trying several values for the\nnumber of clusters i.e., 200 to 1600 with step size\n200. We selected 1000 to find a good balance with\nover and under clustering. We understand that this\nmay not be the best optimal point. We presented\nthe results of 600 and 1000 clusters to show that our\nfindings are not sensitive to the number of clusters\nparameter.\nD Coarse vs. Fine-grained Categories\nD.1 Coarse vs. Fine-grained Categories\nOur analysis of compositional concepts showed\nthat several fine-grained concepts could be com-\nbined to explain an unaligned concept. For exam-\nple, by combining verb categories of POS to one\ncoarse verb category, we can align the encoded\nconcept present in Figure 5c. To probe this more\nformally, we collapsed POS and SEM fine-grained\nconcepts into coarser categories (27 POS tags and\n15 SEM tags). We then recomputed the alignment\nwith the encoded concepts. For most of the models,\nthe alignment doubled compared to the fine-grained\ncategorizes with at least 39% and at most 53% per-\ncent match for POS. This reflects that in several\ncases, models learn the coarse language hierarchy.\nWe further questioned how many encoded concepts\ncan be explained using coarse human-defined con-\ncepts. Compared to Table 1, the matches increased\nby at most 17 points in the case of BERT-uc. The\nXLM-R showed the highest matching percentage\nof 81%. The higher alignment suggests that most of\nthe encoded concepts learned by pre-trained mod-\nels can be explained using human-defined concepts.\n(See Appendix D for detailed results).\nD.2 Corase POS and SEM labels\nTables 5 and 6 present results for our mapping\nof fine-grained SEM and POS tags into coarser\ncategories.\nCoarse Fine-grained\nACT QUE\nANA DEF, DST, EMP, HAS, PRO, REF\nATT INT, IST, QUA, REL, SCO\nCOM COM, LES, MOR, TOP\nDSC APP, BUT, COO, SUB\nDXS PRX\nEVE EXG, EXS, EXT, EXV\nLOG ALT, AND, DIS, EXC, EXN, IMP, NIL, RLI\nMOD NEC, NOT, POS\nNAM ART, GPE, HAP, LOC, NAT, ORG, PER, UOM\nTIM DEC, DOM, DOW, MOY , TIM, YOC\nTNS EFS, ENG, ENS, ENT, EPG, EPS,\nEPT, ETG, ETV , FUT, NOW, PST\nUNE CON, ROL\nUNK UNK\nTable 5: SEM: Coarse to Fine-grained mapping\nD.3 Results\nTable 7 presents the alignment results of using\ncoarse POS and SEM concepts. We observed that\n3096\nCoarse Fine-grained\nAdjective JJ, JJR, JJS\nAdverb RB, RBS, WRB, RBR\nConjunctionCC\nDeterminer DT, WDT\nNoun NN, NNS, NNP, NNPS\nNumber CD\nPreposition IN, TO\nPronoun PRP, PRP$, WP, WP$\nVerb VB, VBN, VBZ, VBG, VBP, VBD\nNo Changes$, -LRB-, #, FW, -RRB-, LS, POS, \"\", EX\nSYM, „ :, RP, ., PDT, MD, UH,\nTable 6: POS: Coarse to Fine-grained mapping\nthe alignment doubles in most of the cases which re-\nflects that in several cases, models learn the coarse\nlanguage hierarchy. However, they do not strictly\nadhere to fine-grained categories existed in human-\ndefined concepts. We further extend the alignment\nof coarse POS and SEM categories to the overall\nalignment with the human-defined concepts. Table\n8 presents the results. We see a match of up to\n81% in the case of XLM-R. The high alignment\nsuggests that many of the encoded concepts can be\nexplained using coarse human-defined concepts.\nPOS SEM\nFine Coarse Fine Coarse\nBERT-cased 13% 42% 7% 15%\nBERT-uncased 16% 43% 9% 18%\nmBERT 26% 53% 16% 26%\nXLM-RoBERTa 24% 47% 11% 21%\nRoBERTa 18% 43% 10% 20%\nALBERT 17% 42% 9% 17%\nXLNet 17% 39% 10% 18%\nTable 7: Alignment of fine-grained human defined con-\ncepts compared to coarse categories\nE Compositional Coverage\nTable 9 shows the amount of coverage we obtain\nwhen aligning with the morphological concepts\nwhen allowing 90% of the words in the cluster to\nbe from N concepts.\nBERT-c BERT-uc mBERT XLM-R\nOverall 61.5% 63.6% 77.7% 81.0%\nalignment RoBERTa ALBERT XLNet\n62.9% 64.0% 55.3%\nTable 8: Coverage of human-defined concepts using\ncoarse POS and SEM labels across all clusters from a\ngiven model\nF Robustness of Methodology across\nDatasets and Settings\nFigure 8 shows the layer-wise patterns using 600\nclusters instead of 1000 as used in the main paper.\nWe observe that the overall trends largely remain\nthe same.\nTo further demonstrate the robustness of our\nmethod with respect to dataset, we sub-sampled\nanother dataset from the News corpus with a dif-\nferent vocabulary by selecting words that appear\nbetween 2 to 10 times in the corpus. Note that\nthe selection of vocabulary is due to the memory\nand computation limitations. Figure 9 shows the\nresults using this selection of data. Compared to\nFigure 4, we can see that the overall patterns are\nlargely similar and confirms the robustness of our\nfindings. The slight difference in the patterns of\nWordNet and LIWC are due to the large selection\nof proper nouns in the second set of the data.\nG Layer-wise results\nFigure 10 present layer-wise results for all the un-\nderstudied models.\n3097\n(a)\n (b)\n (c)\nFigure 6: Example clusters: (a) ngram:ace, (b) POS:CD, (c) Chunking:B-VP + Suffix:ed\n(a)\n (b)\n (c)\nFigure 7: Example clusters: (a) LIWC:cause, (b) WORDNET:verb.cognition, (c) WORDNET:noun.artifact\nConcepts BERT-c BERT-uc mBERT XLM-R RoBERTa ALBERT XLNet\n1 13% 16% 26% 24% 18% 17% 17%\n2 11% 12% 20% 23% 13% 13% 12%\n3 14% 13% 14% 18% 11% 15% 9%\n4 6% 6% 4% 4% 5% 5% 3%\n5 2% 1% 1% 1% 2% 1% 1%\n6 1% 0% 0% 0% 1% 1% 0%\nTable 9: Percentage of alignment when an encoded concept is composed of N morphological concepts. As can be\nseen, most concepts are composed of either 1, 2 or 3 morphological concepts, showing that several concepts learned\nby these models are indeed compositional in nature.\n3098\nFigure 8: Layer-wise results using 600 clusters.\n3099\nFigure 9: Layer-wise results on a separately sampled dataset.\n3100\nFigure 10: Layer-wise results.\n3101",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6417326927185059
    },
    {
      "name": "Transformer",
      "score": 0.6180101037025452
    },
    {
      "name": "Computational linguistics",
      "score": 0.43522652983665466
    },
    {
      "name": "Linguistics",
      "score": 0.3957846760749817
    },
    {
      "name": "Natural language processing",
      "score": 0.3929738700389862
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35413265228271484
    },
    {
      "name": "Engineering",
      "score": 0.17995601892471313
    },
    {
      "name": "Electrical engineering",
      "score": 0.12550660967826843
    },
    {
      "name": "Philosophy",
      "score": 0.1080234944820404
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}