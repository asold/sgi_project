{
  "title": "Rethinking Spatial Dimensions of Vision Transformers",
  "url": "https://openalex.org/W3139587317",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4202169409",
      "name": "Heo, Byeongho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3210220256",
      "name": "Yun, Sangdoo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3085681968",
      "name": "Han, Dongyoon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222306723",
      "name": "Chun, Sanghyuk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222804903",
      "name": "Choe, Junsuk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3191246918",
      "name": "Oh, Seong Joon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2970800101",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3132087345",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3166225737",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W2961301154",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W2895976713",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2949603537",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2962716426",
    "https://openalex.org/W2962845550",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3168547821",
    "https://openalex.org/W2963855133",
    "https://openalex.org/W3036438747",
    "https://openalex.org/W2795783309",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3122060391",
    "https://openalex.org/W2962971773",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W2963163009"
  ],
  "abstract": "Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit",
  "full_text": "Rethinking Spatial Dimensions of Vision Transformers\nByeongho Heo1 Sangdoo Yun1 Dongyoon Han1 Sanghyuk Chun1 Junsuk Choe2* Seong Joon Oh1\n1 NA VER AI Lab 2 Department of Computer Science and Engineering, Sogang University\nAbstract\nVision Transformer (ViT) extends the application range\nof transformers from language processing to computer vi-\nsion tasks as being an alternative architecture against\nthe existing convolutional neural networks (CNN). Since\nthe transformer-based architecture has been innovative for\ncomputer vision modeling, the design convention towards\nan effective architecture has been less studied yet. From\nthe successful design principles of CNN, we investigate\nthe role of spatial dimension conversion and its effective-\nness on transformer-based architecture. We particularly at-\ntend to the dimension reduction principle of CNNs; as the\ndepth increases, a conventional CNN increases channel di-\nmension and decreases spatial dimensions. We empirically\nshow that such a spatial dimension reduction is beneÔ¨Åcial\nto a transformer architecture as well, and propose a novel\nPooling-based Vision Transformer (PiT) upon the origi-\nnal ViT model. We show that PiT achieves the improved\nmodel capability and generalization performance against\nViT. Throughout the extensive experiments, we further show\nPiT outperforms the baseline on several tasks such as im-\nage classiÔ¨Åcation, object detection, and robustness evalua-\ntion. Source codes and ImageNet models are available at\nhttps://github.com/naver-ai/pit.\n1. Introduction\nThe architectures based on the self-attention mechanism\nhave achieved great success in the Ô¨Åeld of Natural Lan-\nguage Processing (NLP) [34]. There have been attempts\nto utilize the self-attention mechanism in computer vision.\nNon-local networks [37] and DETR [4] are representative\nworks, showing that the self-attention mechanism is also ef-\nfective in video classiÔ¨Åcation and object detection tasks, re-\nspectively. Recently, Vision Transformer (ViT) [9], a trans-\nformer architecture consisting of self-attention layers, has\nbeen proposed to compete with ResNet [13], and shows that\nit can achieve the best performance without convolution op-\n*Work done as a research scientist at NA VER AI Lab.\neration on ImageNet [8]. As a result, a new direction of net-\nwork architectures based on self-attention mechanism, not\nconvolution operation, has emerged in computer vision.\nViT is quite different from convolutional neural networks\n(CNN). Input images are divided into 16 √ó16 patches and\nfed to the transformer network; except for the Ô¨Årst embed-\nding layer, there is no convolution operation in ViT, and the\nposition interactions occur only through the self-attention\nlayers. While CNNs have restricted spatial interactions, ViT\nallows all the positions in an image to interact through trans-\nformer layers. Although ViT is an innovative architecture\nand has proven its powerful image recognition ability, it fol-\nlows the transformer architecture in NLP [34] without any\nchanges. Some essential design principles of CNNs, which\nhave proved to be effective in the computer vision domain\nover the past decade, are not sufÔ¨Åciently reÔ¨Çected. We thus\nrevisit the design principles of CNN architectures and in-\nvestigate their efÔ¨Åcacy when applied to ViT architectures.\nCNNs start with a feature of large spatial sizes and a\nsmall channel size and gradually increase the channel size\nwhile decreasing the spatial size. This dimension conver-\nsion is indispensable due to the layer called spatial pool-\ning. Modern CNN architectures, including AlexNet [21],\nResNet [13], and EfÔ¨ÅcientNet [32], follow this design prin-\nciple. The pooling layer is deeply related to the receptive\nÔ¨Åeld size of each layer. Some studies [6, 26, 5] show that\nthe pooling layer contributes to the expressiveness and gen-\neralization performance of the network. However, unlike the\nCNNs, ViT does not use a pooling layer and uses the same\nspatial dimension for all layers.\nFirst, we verify the advantages of dimensions conÔ¨Ågu-\nrations on CNNs. Our experiments show that ResNet-style\ndimensions improve the model capability and generaliza-\ntion performance of ResNet. To extend the advantages to\nViT, we propose a Pooling-based Vision Transformer (PiT).\nPiT is a transformer architecture combined with a newly de-\nsigned pooling layer. It enables the spatial size reduction in\nthe ViT structure as in ResNet. We also investigate the ben-\neÔ¨Åts of PiT compared to ViT and conÔ¨Årm that ResNet-style\ndimension setting also improves the performance of ViT.\nFinally, to analyze the effect of PiT compared to ViT, we\narXiv:2103.16302v2  [cs.CV]  18 Aug 2021\n(a) ResNet-50\n56√ó56√ó256\n224√ó224√ó3\n28√ó28√ó51214√ó14√ó10247√ó7√ó2048\n(b) ViT-S/16\n(14√ó14)√ó384Spatial tokens\n1√ó384Class token224√ó224√ó3\n(14√ó14)√ó288Spatial tokens\n1√ó144 Class token\n224√ó224√ó3\n(27√ó27)√ó144 (7√ó7)√ó576\n1√ó288 1√ó576PoolingPooling(c) PiT-S\nFigure 1. Schematic illustration of dimension conÔ¨Ågurations of networks. We visualize ResNet50 [13], Vision Transformer (ViT) [9],\nand our Pooling-based Vision Transformer (PiT); (a) ResNet50 gradually downsamples the features from the input to the output; (b) ViT\ndoes not change the spatial dimensions; (c) PiT involves ResNet style spatial dimension into ViT.\nanalyze the attention matrix of transformer block with en-\ntropy and average distance measure. The analysis shows the\nattention patterns inside layers of ViT and PiT, and helps to\nunderstand the inner mechanism of ViT and PiT.\nWe verify that PiT improves performances over ViT on\nvarious tasks. On ImageNet classiÔ¨Åcation, PiT and outper-\nforms ViT at various scales and training environments. Ad-\nditionally, we have compared the performance of PiT with\nvarious convolutional architectures and have speciÔ¨Åed the\nscale at which the transformer architecture outperforms the\nCNN. We further measure the performance of PiT as a back-\nbone for object detection. ViT- and PiT-based deformable\nDETR [44] are trained on the COCO 2017 dataset [24] and\nthe result shows that PiT is even better than ViT as a back-\nbone architecture for a task other than image classiÔ¨Åcation.\nFinally, we verify the performance of PiT in various envi-\nronments through the robustness benchmark.\n2. Related works\n2.1. Dimension conÔ¨Åguration of CNN\nDimension conversion can be found in AlexNet [21],\nwhich is one of the earliest convolutional networks in com-\nputer vision. AlexNet uses three max-pooling layers. In the\nmax-pooling layer, the spatial size of the feature is reduced\nby half, and the channel size is increased by the convolu-\ntion after the max-pooling. VGGnet [30] uses 5 spatial res-\nolutions using 5 max-pooling. In the pooling layer, the spa-\ntial size is reduced by half and the channel size is doubled.\nGoogLeNet [31] also used the pooling layer. ResNet [13]\nperformed spatial size reduction using the convolution layer\nof stride 2 instead of max pooling. It is an improvement\nin the spatial reduction method. The convolution layer of\nstride 2 is also used as a pooling method in recent archi-\ntectures (EfÔ¨ÅcietNet [32], MobileNet [29, 19]). Pyramid-\nNet [11] pointed out that the channel increase occurs only\nin the pooling layer and proposed a method to gradually\nincrease the channel size in layers other than the pooling\nlayer. ReXNet [12] reported that the channel conÔ¨Åguration\nof the network has a signiÔ¨Åcant inÔ¨Çuence on the network\nperformance. In summary, most convolution networks use a\ndimension conÔ¨Åguration with spatial reduction.\n2.2. Self-attention mechanism\nTransformer architecture [34] signiÔ¨Åcantly increased the\nperformance of the NLP task with the self-attention mecha-\nnism. Funnel Transformer [7] improves the transformer ar-\nchitecture by reducing tokens by a pooling layer and skip-\nconnection. However, because of the basic difference be-\ntween the architecture of NLP and computer vision, the\nmethod of applying to pool is different from our method.\nSome studies are conducted to utilize the transformer archi-\ntecture to the backbone network for computer vision tasks.\nNon-local network [37] adds a few self-attention layers to\nCNN backbone, and it shows that the self-attention mecha-\nnism can be used in CNN. [28] replaced 3 √ó3 convolution\nof ResNet to local self-attention layer. [36] used an atten-\ntion layer for each spatial axis. [2] enables self-attention of\nthe entire spatial map by reducing the computation of the\nattention mechanism. Most of these methods replace 3x3\nconvolution with self-attention or adds a few self-attention\nlayers. Therefore, the basic structure of ResNet is inherited,\nthat is, it has the convolution of stride 2 as ResNet, resulting\nin a network having a dimension conÔ¨Åguration of ResNet.\nOnly the vision transformer uses a structure that uses\nthe same spatial size in all layers. Although ViT did not\nfollow the conventions of ResNet, it contains many valu-\nable new components in the network architecture. In ViT,\nlayer normalization is applied for each spatial token. There-\nfore, layer normalization of ViT is closer to positional nor-\nmalization [22] than a layer norm of convolutional neural\nnetwork [1, 39]. Although it overlaps with the lambda net-\nwork [2], it is not common to use global attention through\nall blocks of the network. The use of class tokens instead\nof global average pooling is also new, and it has been re-\nported that separating tokens increases the efÔ¨Åciency of dis-\ntillation [33]. In addition, the layer conÔ¨Åguration, the skip-\nconnection position, and the normalization position of the\nTransformer are also different from ResNet. Therefore, our\nstudy gives a direction to the new architecture.\n(a) Model capability\n (b) Generalization performance\n (c) Model performance\nFigure 2. Effects of the spatial dimensions in ResNet50 [13]. We verify the effect of the spatial dimension with ResNet50. As shown in\nthe Ô¨Ågures, ResNet-style is better than ViT-style in the model capability, generalization performance, and model performance.\n3. Revisiting spatial dimensions\nIn order to introduce dimension conversion to ViT,\nwe investigate spatial dimensions in network architectures.\nFirst, we verify the beneÔ¨Åts of dimension conÔ¨Åguration in\nResNet architecture. Although dimension conversion has\nbeen widely used for most convolutional architectures, its\neffectiveness is rarely veriÔ¨Åed. Based on the Ô¨Åndings, we\npropose a Pooling-based Vision Transformer (PiT) that ap-\nplies the ResNet-style dimension to ViT. We propose a new\npooling layer for transformer architecture and design ViT\nwith the new pooling layer (PiT). With PiT models, we ver-\nify whether the ResNet-style dimension brings advantages\nto ViT. In addition, we analyze the attention matrix of the\nself-attention block of ViT to investigate the effect of PiT in\nthe transformer mechanism. Finally, we introduce PiT ar-\nchitectures corresponding to various scales of ViT.\n3.1. Dimension setting of CNN\nAs shown in Figure 1 (a), most convolutional architec-\ntures reduce the spatial dimension while increases the chan-\nnel dimension. In ResNet50, a stem layer reduces the spatial\nsize of an image to56√ó56. After several layer blocks, Con-\nvolution layers with stride 2 reduce the spatial dimension by\nhalf and double the channel dimension. The spatial reduc-\ntion using a convolution layer with stride 2 is a frequently\nused method in recent architectures [32, 29, 19, 12]. We\nconduct an experiment to analyze the performance differ-\nence according to the presence or absence of the spatial re-\nduction layer in a convolutional architecture. ResNet50, one\nof the most widely used networks in ImageNet, is used for\narchitecture and is trained over 100 epochs without complex\ntraining techniques. For ResNet with ViT style dimension,\nwe use the stem layer of ViT to reduce the feature to14√ó14\nspatial dimensions while reducing the spatial information\nloss in the stem layer. We also remove the spatial reduction\nlayers of ResNet to maintain the initial feature dimensions\nfor all layers like ViT. We measured the performance for\nseveral sizes by changing the channel size of ResNet.\nFirst, we measured the relation between FLOPs and\ntraining loss of ResNet with ResNet-style or ViT-style di-\nmension conÔ¨Åguration. As shown in Figure 2 (a), ResNet\n(ResNet-style) shows lower training loss over the same\ncomputation costs (FLOPs). It implies that ResNet-style di-\nmensions increase the capability of architecture. Next, we\nanalyzed the relation between training and validation ac-\ncuracy, which represents the generalization performance of\narchitecture. As shown in Figure 2 (b), ResNet (ResNet-\nstyle) achieves higher validation accuracy than ResNet\n(ViT-style). Therefore, ResNet-style dimension conÔ¨Ågura-\ntion is also helpful for generalization performance. In sum-\nmary, ResNet-style dimension improves the model capabil-\nity and generalization performance of the architecture and\nconsequently brings a signiÔ¨Åcant improvement in validation\naccuracy as shown in Figure 2 (c).\n3.2. Pooling-based Vision Transformer (PiT)\nVision Transformer (ViT) performs network operations\nbased on self-attention, not convolution operations. In the\nself-attention mechanism, the similarity between all loca-\ntions is used for spatial interaction. Figure 1 (b) shows the\ndimension structure of this ViT. Similar to the stem layer\nof CNN, ViT divides the image by patch at the Ô¨Årst embed-\nding layer and embedding it to tokens. Basically, the struc-\nture does not include a spatial reduction layer and keeps\nthe same number of spatial tokens overall layer of the net-\nwork. Although the self-attention operation is not limited\nby spatial distance, the size of the spatial area participat-\ning in attention is affected by the spatial size of the feature.\nTherefore, in order to adjust the dimension conÔ¨Åguration\nlike ResNet, a spatial reduction layer is also required in ViT.\nTo utilize the advantages of the dimension conÔ¨Ågura-\ntion to ViT, we propose a new architecture called Pooling-\nbased Vision Transformer (PiT). First, we designed a pool-\ning layer for ViT. Our pooling layer is shown in Figure 4.\nSince ViT handles neuron responses in the form of 2D-\nmatrix rather than 3D-tensor, the pooling layer should sep-\n(a) Model capability\n (b) Generalization performance\n (c) Model performance\nFigure 3. Effects of the spatial dimensions in vision transformer (ViT) [9]. We compare our Pooling-based Vision Transformer (PiT)\nwith original ViT at various aspects. PiT outperforms ViT in capability, generalization performance, and model performance.\nPooling layerSpatial tokens\nClass token\n(ùë§√ó‚Ñé)√óùëë\n1√óùëë\nùë§√ó‚Ñé√óùëëReshape ùë§2√ó‚Ñé2√ó2ùëëDepth-wiseConvolutionReshape\nSpatial tokens(ùë§2√ó‚Ñé2)√ó2ùëë\n1√ó2ùëëFully-connected layer\nClass token\nFigure 4. Pooling layer of PiT architecture.PiT uses the pooling\nlayer based on depth-wise convolution to achieve channel multi-\nplication and spatial reduction with small parameters.\narate spatial tokens and reshape them into 3D-tensor with\nspatial structure. After reshaping, spatial size reduction and\nchannel increase are performed by depth-wise convolution.\nAnd, the responses are reshaped into a 2D matrix for the\ncomputation of transformer blocks. In ViT, there are parts\nthat do not correspond to the spatial structure, such as a\nclass token or distillation token [33]. For these parts, the\npooling layer uses an additional fully-connected layer to ad-\njust the channel size to match the spatial tokens. Our pool-\ning layer enables spatial reduction on ViT and is used for\nour PiT architecture as shown in Figure 1 (c). PiT includes\ntwo pooling layers which make three spatial scales.\nUsing PiT architecture, we performed an experiment to\nverify the effect of PiT compared to ViT. The experiment\nsetting is the same as the ResNet experiment. Figure 3 (a)\nrepresents the model capability of ViT and PiT. At the same\ncomputation cost, PiT has a lower train loss than ViT. Us-\ning the spatial reduction layers in ViT also improves the ca-\npability of architecture. The comparison between training\naccuracy and validation accuracy shows a signiÔ¨Åcant dif-\nference. As shown in Figure 3 (b), ViT does not improve\nvalidation accuracy even if training accuracy increases. On\nthe other hand, in the case of PiT, validation accuracy in-\ncreases as training accuracy increases. The big difference in\ngeneralization performance causes the performance differ-\nence between PiT and ViT as shown in Figure 3 (c). The\nphenomenon that ViT does not increase performance even\nwhen FLOPs increase in ImageNet is reported in ViT pa-\nper [9]. In the training data of ImageNet scale, ViT shows\npoor generalization performance, and PiT alleviates this.\nSo, we believe that the spatial reduction layer is also nec-\nessary for the generalization of ViT. Using the training trick\nis a way to improve the generalization performance of ViT\nin ImageNet. The combination of training tricks and PiT is\ncovered in the experiment section.\n3.3. Attention analysis\nWe analyze the transformer networks with measures on\nattention matrix [35]. We denotes Œ±i,j as (i,j) component\nof attention matrix A ‚ààRM√óN. Note that attention values\nafter soft-max layer is used, i.e. ‚àë\ni Œ±i,j = 1. The attention\nentropy is deÔ¨Åned as\nEntropy = ‚àí1\nN\nN‚àë\nj\n‚àë\ni\nŒ±i,j log Œ±i,j. (1)\nThe entropy shows the spread and concentration degree of\nan attention interaction. A small entropy indicates a con-\ncentrated interaction, and a large entropy indicates a spread\ninteraction. We also measure an attention distance,\nDistance = 1\nN\nN‚àë\nj\n‚àë\ni\nŒ±i,j‚à•pi ‚àípj‚à•1. (2)\npi represents relative spatial location of i-th token\n(xi/W,yi/H) for feature map F ‚ààRH√óW√óC. So, the at-\ntention distance shows a relative ratio compared to the over-\nall feature size, which enables comparison between the dif-\nferent sizes of features. We analyze transformer-based mod-\nels (ViT-S [33] and PiT-S) and values are measured over-\nall validation images and are averaged over all heads of\neach layer. Our analysis is only conducted for the spatial\ntokens rather than the class token following the previous\nstudy [35]. We also skip the attention of the last transformer\nblock since the spatial tokens of the last attention are inde-\npendent of the network outputs.\nThe results are shown in Figure 5. In ViT, the entropy\nand the distance increase as the layer become deeper. It im-\nplies that the interaction of ViT is concentrated to close to-\nkens at the shallow layers and the interaction is spread in\na wide range of tokens at the deep layers. The entropy and\ndistance pattern of ViT is similar to the pattern of trans-\nformer in the language domain [35]. PiT changes the pat-\nterns with the spatial dimension setting. At shallow layers\n(1-2 layers), large spatial size increases the entropy and dis-\ntance. On the other hand, the entropy and distance are de-\ncreased at deep layers (9-11 layers) due to the small spatial\nsize. In short, the pooling layer of PiT spreads the interac-\ntion in the shallow layers and concentrates the interaction\nin the deep layers. In contrast to discrete word inputs of the\nlanguage domain, the vision domain uses image-patch in-\nputs which require pre-processing operations such as Ô¨Ålter-\ning, contrast, and brightness calibration. In shallow layers,\nthe spread interaction of PiT is close to the pre-processing\nthan the concentrated interaction of ViT. Also, compared to\nlanguage models, image recognition has relatively low out-\nput complexity. So, in deep layers, concentrated interaction\nmight be enough. There are signiÔ¨Åcant differences between\nthe vision and the language domain, and we believe that the\nattention of PiT is suitable for image recognition backbone.\n3.4. Architecture design\nThe architectures proposed in ViT paper [9] aimed at\ndatasets larger than ImageNet. These architectures (ViT-\nLarge, ViT-Huge) have an extremely large scale than gen-\neral ImageNet networks, so it is not easy to compare them\nwith other networks. So, following the previous study [33]\nof Vision Transformer on ImageNet, we design the PiT at a\nscale similar to the small-scale ViT architectures (ViT-Base,\nViT-Small, ViT-Tiny). In the DeiT paper [33], ViT-Small\nand ViT-Tiny are named DeiT-S and DeiT-Ti, but to avoid\nconfusion due to the model name change, we use ViT for\nall models. Corresponding to the three scales of ViT (tiny,\nsmall, and base), we design four scales of PiT (tiny, extra\nsmall, small, and base). Detail architectures are described in\nTable 1. For convenience, we abbreviate the model names:\nTiny - Ti, eXtra Small - XS, Small - S, Base - B FLOPs and\nspatial size were measured based on224√ó224 image. Since\nPiT uses a larger spatial size than ViT, we reduce the stride\nsize of the embedding layer to 8, while patch-size is 16 as\nViT. Two pooling layers are used for PiT, and the channel\nincrease is implemented as increasing the number of heads\nof multi-head attention. We design PiT to have a similar\ndepth to ViT, and adjust the channels and the heads to have\n(a) Attention entropy\n(b) Spatial distance of interaction\nFigure 5. Attention analysis. We investigate the attention matrix\nof the self-attention layer. Figure (a) shows the entropy and Ô¨Ågure\n(b) shows the interaction distance. PiT increases the entropy and\nthe distance in shallow layers and decreases in deep layers.\nNetwork Spatial\nsize\n# of\nblocks\n# of\nheads\nChannel\nsize FLOPs\nViT-Ti [33] 14 x 14 12 3 192 1.3B\nPiT-Ti\n27 x 27 2 2 64\n0.7B 14 x 14 6 4 128\n7 x 7 4 8 256\nPiT-XS\n27 x 27 2 2 96\n1.4B 14 x 14 6 4 192\n7 x 7 4 8 384\nViT-S [33] 14 x 14 12 6 384 4.6B\nPiT-S\n27 x 27 2 3 144\n2.9B 14 x 14 6 6 288\n7 x 7 4 12 576\nViT-B [9] 14 x 14 12 12 768 17.6B\nPiT-B\n31 x 31 3 4 256\n12.5B 16 x 16 6 8 512\n8 x 8 4 16 1024\nTable 1. Architecture conÔ¨Åguration. The table shows spatial\nsizes, number of blocks, number of heads, channel size, and\nFLOPs of ViT and PiT. The structure of PiT is designed to be as\nsimilar as possible to ViT and to have less GPU latency.\nsmaller FLOPs, parameter size, and GPU latency than those\nof ViT. We clarify that PiT is not designed with large-scale\nparameter search such as NAS [25, 3], so PiT can be further\nimproved through a network architecture search.\nArchitecture FLOPs # of\nparams\nThroughput\n(imgs/sec) Vanilla +CutMix [41] +DeiT [33] +Distill\n‚öó [33]\nViT-Ti [33] 1.3 B 5.7 M 2564 68.7% 68.5% 72.2% 74.5%\nPiT-Ti 0.7 B 4.9 M 3030 71.3% 72.6% 73.0% 74.6%\nPiT-XS 1.4 B 10.6 M 2128 72.4% 76.8% 78.1% 79.1%\nViT-S [33] 4.6 B 22.1 M 980 68.7% 76.5% 79.8% 81.2%\nPiT-S 2.9 B 23.5 M 1266 73.3% 79.0% 80.9% 81.9%\nViT-B [9] 17.6 B 86.6 M 303 69.3% 75.3% 81.8% 83.4%\nPiT-B 12.5 B 73.8 M 348 76.1% 79.9% 82.0% 84.0%\nTable 2. ImageNet performance comparison with ViT. We compare the performances of ViT and PiT with some training techniques on\nImageNet dataset. PiT shows better performance with low computation compared to ViT.\n4. Experiments\nWe veriÔ¨Åed the performance of PiT through various ex-\nperiments. First, we compared PiT at various scales with\nViT in various training environments of ImageNet training.\nAnd, we extended the ImageNet comparison to architec-\ntures other than Transformer. In particular, we focus on the\ncomparison of the performance of ResNet and PiT, and in-\nvestigate whether PiT can beat ResNet. We also applied PiT\nto an object detector based on deformable DETR [44], and\ncompared the performance as a backbone architecture for\nobject detection. To analyze PiT in various views, we eval-\nuated the performance of PiT on robustness benchmarks.\n4.1. ImageNet classiÔ¨Åcation\nWe compared the performance of PiT models of Table 1\nwith corresponding ViT models. To clarify the computa-\ntion time and size of the network, we measured FLOPs, the\nnumber of parameters, and GPU throughput (images/sec)\nof each network. The GPU throughput was measured on\nNVIDIA V100 single GPU with 128 batch-size. We trained\nthe network using four representative training environ-\nments. The Ô¨Årst is a vanilla setting that trains the network\nwithout complicated training techniques. The vanilla setting\nhas the lowest performance due to the lack of techniques to\nhelp generalization performance and also used for the previ-\nous experiments in Figure 2, 3. The second is training with\nCutMix [41] data augmentation. Although only data aug-\nmentation has changed, it shows signiÔ¨Åcantly better perfor-\nmance than the vanilla setting. The third is the DeiT [33]\nsetting, which is a compilation of training techniques to\ntrain ViT on ImageNet-1k [8]. DeiT setting includes vari-\nous training techniques and parameter tuning, and we used\nthe same training setting through the ofÔ¨Åcial open-source\ncode. However, in the case of Repeated Augment [18], we\nconÔ¨Årmed that it had a negative effect in a small model, and\nit was used only for Base models. The last is a DeiT set-\nting with knowledge distillation. The distillation setting is\nreported as the best performance setting in DeiT [33] pa-\nper. The network uses an additional distillation token and is\ntrained with distillation loss [17] using RegNetY-16GF [27]\nas a teacher network. We used AdamP [16] optimizer for all\nsettings, and the learning rate, weight decay, and warmup\nwere set equal to DeiT [33] paper. We train models over 100\nepochs for Vanilla and CutMix settings, and 300 epochs for\nDeiT and Distill\n‚öó settings.\nThe results are shown in Table 2. Comparing the PiT\nand ViT of the same name, the PiT has fewer FLOPs and\nfaster speed than ViT. Nevertheless, PiT shows higher per-\nformance than ViT. In the case of vanilla and CutMix set-\ntings, where a few training techniques are applied, the per-\nformance of PiT is superior to the performance of ViT. Even\nin the case of a DeiT and distill settings, PiT shows com-\nparable or better performance to ViT. Therefore, PiT can\nbe seen as a better architecture than ViT in terms of per-\nformance and computation. The generalization performance\nissue of ViT in Figure 3 can also be observed in this experi-\nment. Like ViT-S in the Vanilla setting and ViT-B in the Cut-\nMix setting, ViT often shows no increase in performance\neven when the model size increases. On the other hand, the\nperformance of PiT increases according to the model size in\nall training settings. it seems that the generalization perfor-\nmance problem of ViT is alleviated by the pooling layers.\nWe compared the performance of PiT with the convolu-\ntional networks. In the previous experiment, we performed\nthe comparison in the same training setting using the simi-\nlarity of architecture. However, when comparing various ar-\nchitectures, it is infeasible to unify with a setting that works\nwell for all architectures. Therefore, we performed the com-\nparison based on the best performance reported for each\narchitecture. But, it was limited to the model trained us-\ning only ImageNet images. When the paper that proposed\nthe architecture and the paper that reported the best per-\nformance was different, we cite both papers. When the ar-\nchitecture is different, the comparison of FLOPs often fails\nto reÔ¨Çect the actual throughput. Therefore, we re-measured\nthe GPU throughput and number of params on a single\nV100 GPU and compared the top-1 accuracy for the per-\nformance index. Table 3 shows the comparison result. In\nthe case of the PiT-B scale, the transformer-based archi-\nNetwork # of\nparams\nThroughput\n(imgs/sec) Accuracy\nResNet18 [13, 42] 11.7M 4545 72.5%\nMobileNetV2 [29] 3.5M 3846 72.0%\nMobileNetV3 [19] 5.5M 3846 75.2%\nEfÔ¨ÅcientNet-B0 [32] 5.3M 2857 77.1%\nViT-Ti [33] 5.7M 2564 72.2%\nPiT-Ti 4.9M 3030 73.0%\nViT-Ti\n‚öó [33] 5.7M 2564 74.5%\nPiT-Ti\n‚öó 4.9M 3030 74.6%\nResNet34 [13, 38] 21.8M 2631 75.1%\nResNet34D [14, 38] 21.8M 2325 77.1%\nEfÔ¨ÅcientNet-B1 [32] 7.8M 1754 79.1%\nPiT-XS 10.6M 2128 78.1%\nPiT-XS\n‚öó 10.6M 2128 79.1%\nResNet50 [13, 42] 25.6M 1266 80.2%\nResNet101 [13, 42] 44.6M 757 81.6%\nResNet50D [14, 38] 25.6M 1176 80.5%\nEfÔ¨ÅcientNet-B2 [32] 9.2M 1333 80.1%\nEfÔ¨ÅcientNet-B3 [32] 12.2M 806 81.6%\nRegNetY-4GF [27] 20.6M 1136 79.4%\nResNeSt50 [43] 27.5M 877 81.1%\nViT-S [33] 22.1M 980 79.8%\nPiT-S 23.5M 1266 80.9%\nViT-S\n‚öó [33] 22.1M 980 81.2%\nPiT-S\n‚öó 23.5M 1266 81.9%\nResNet152 [13, 42] 60.2M 420 81.9%\nResNet101D [14, 38] 44.6M 354 83.0%\nResNet152D [14, 38] 60.2M 251 83.7%\nEfÔ¨ÅcientNet-B4 [32] 19.3M 368 82.9%\nRegNetY-16GF [27] 83.6M 352 80.4%\nResNeSt101 [43] 48.3M 398 83.0%\nViT-B [9, 33] 86.6M 303 81.8%\nPiT-B 73.8M 348 82.0%\nViT-B\n‚öó [9, 33] 86.6M 303 83.4%\nPiT-B\n‚öó 73.8M 348 84.0%\nTable 3.ImageNet performance. We compare our PiT-(Ti, XS, S,\nand B) models with the counterparts which have a similar number\nof parameters.\n‚öó means a model trained with distillation [33].\ntecture (ViT-B, PiT-B) outperforms the convolutional ar-\nchitecture. Even in the PiT-S scale, PiT-S shows superior\nperformance than convolutional architecture (ResNet50) or\noutperforms in throughput (EfÔ¨ÅcientNet-b3). However, in\nthe case of PiT-Ti, the performance of convolutional ar-\nchitectures such as ResNet34 [13], MobileNetV3 [19], and\nEfÔ¨ÅcientNet-b0 [32] outperforms ViT-Ti and PiT-Ti. Over-\nall, the transformer architecture shows better performance\nthan the convolutional architecture at the scale of ResNet50\nor higher, but it is weak at a small scale. Creating a light-\nweight transformer architecture such as MobileNet is one of\nthe future works of ViT research.\nAdditionally, we conduct experiments on two extended\nSetting Architecture Throughput\n(imgs/sec) Accuracy\nLong\ntraining\n(1000 epochs)\nViT-Ti\n‚öó [33] 2564 76.6%\nPiT-Ti\n‚öó 3030 76.4%\nPiT-XS\n‚öó 2128 80.6%\nViT-S\n‚öó [33] 980 82.6%\nPiT-S\n‚öó 1266 82.7%\nViT-B\n‚öó [33] 303 84.2%\nPiT-B\n‚öó 348 84.5%\nLarge resolution\n(384√ó384)\nViT-B [33] 91 83.1%\nPiT-B 82 83.0%\nViT-B\n‚öó [33] 91 84.5%\nPiT-B\n‚öó 82 84.6%\nTable 4. Extended training settings. We compare the perfor-\nmance of PiT with ViT for long training (1000 epochs) and Ô¨Åne-\ntune on large resolution (384√ó384)\nBackbone Avg. Precision at IOU Params. Latency\nAP AP 50 AP75 (ms / img)\nResNet50 [13] 41.5 60.5 44.3 41.0 M 49.7\nViT-S [33] 36.9 57.0 38.0 34.9 M 55.2\nPiT-S 39.4 58.8 41.5 36.6 M 46.9\nTable 5. COCO detection performance based on Deformable\nDETR [44]. We evaluate the performance of PiT as a pretrained\nbackbone for object detection.\ntraining schemes: long training and Ô¨Åne-tune on large reso-\nlution. Table 4 shows the results. As shown in the previous\nstudy [33], the performance of ViT is signiÔ¨Åcantly improved\non the long training scheme (1000 epochs). So, we vali-\ndate PiT on the long training scheme. As shown in Table 4,\nPiT models show comparable performance with ViT mod-\nels on the long training scheme. Although the performance\nimprovement is reduced than the Distill\n‚öó setting, PiTs still\noutperform ViT counterparts in throughput. Fine-tuning on\nlarge resolution (384 √ó384) is a famous method to train a\nlarge ViT model with small computation. In the large res-\nolution setting, PiT has comparable performance with ViT,\nbut, worse than ViT on throughput. It implies that PiT is\ndesigned for 224 √ó224 and the design is not compatible\nfor the large resolution. However, we believe that PiT can\noutperform ViT with a new layer design for 384 √ó384.\n4.2. Object detection\nWe validate PiT through object detection on COCO\ndataset [24] in Deformable-DETR [44]. We train the detec-\ntors with different backbones including ResNet50, ViT-S,\nand our PiT-S. We follow the training setup of the origi-\nnal paper [44] except for the image resolution. Since the\noriginal image resolution is too large for transformer-based\nbackbones, we halve the image resolution for training and\nStandard Occ IN-A [15] BGC [40] FGSM [10]\nPiT-S 80.8 74.6 21.7 21.0 29.5\nViT-S [33] 79.8 73.0 19.1 17.6 27.2\nResNet50 [13] 76.0 52.2 0.0 22.3 7.1\nResNet50‚Ä† [38] 79.0 67.1 5.4 32.7 24.7\nTable 6. ImageNet robustness benchmarks. We compare three\ncomparable architectures, PiT-B, ViT-S, and ResNet50 on var-\nious ImageNet robustness benchmarks, including center occlu-\nsion (Occ), ImageNet-A (IN-A), background challenge (BGC),\nand fast sign gradient method (FGSM) attack. We evaluate two\nResNet50 models from the ofÔ¨Åcial PyTorch repository, and the\nwell-optimized implementation [38], denoted as ‚Ä†.\ntest of all backbones. We use bounding box reÔ¨Ånement and a\ntwo-stage scheme for the best performance [44]. For multi-\nscale features for ViT-S, we use features at the 2nd, 8th, and\n12th layers following the position of pooling layers on PiT.\nAll detectors are trained for 50 epochs and the learning rate\nis dropped by factor 1/10 at 40 epochs.\nTable 5 shows the measured AP score on val2017. The\ndetector based on PiT-S outperforms the detector with ViT-\nS. It shows that the pooling layer of PiT is effective not only\nfor ImageNet classiÔ¨Åcation but also for pretrained backbone\nfor object detection. We measured single image latency with\na random noise image at resolution600√ó400 PiT based de-\ntector has lower latency than detector based on ResNet50 or\nViT-S. Although PiT detector cannot beat the performance\nof the ResNet50 detector, PiT detector has better latency,\nand improvement over ViT-S is signiÔ¨Åcant. Additional in-\nvestigation on the training settings for PiT based detectors\nwould improve the performance of the PiT detector.\n4.3. Robustness benchmarks\nIn this subsection, we investigate the effectiveness of the\nproposed architecture in terms of robustness against input\nchanges. We presume that the existing ViT design concept,\nwhich keeps the spatial dimension from the input layer to\nthe last layer, has two conceptual limitations: Lack of back-\nground robustnessand sensitivity to the local discriminative\nvisual features. We, therefore, presume that PiT, our new\ndesign choice with the pooling mechanism, performs better\nthan ViT for the background robustness benchmarks and the\nlocal discriminative sensitivity benchmarks.\nWe employ four different robustness benchmarks. Oc-\nclusion benchmark measures the ImageNet validation ac-\ncuracy where the center 112 √ó112 patch of the images\nis zero-ed out. This benchmark measures whether a model\nonly focuses on a small discriminative visual feature or not.\nImageNet-A (IN-A) is a dataset constructed by collecting\nthe failure cases of ResNet50 from the web [15] where the\ncollected images contain unusual backgrounds or objects\nwith very small size [23]. From this benchmark, we can in-\nfer how a model is less sensitive to unusual backgrounds\nor object size changes. However, since IN-A is constructed\nby collecting images (queried by 200 ImageNet subclasses)\nwhere ResNet50 predicts a wrong label, this dataset can\nbe biased towards ResNet50 features. We, therefore, em-\nploy background challenge (BGC) benchmark [40] to ex-\nplore the explicit background robustness. The BGC dataset\nconsists of two parts, foregrounds, and backgrounds. This\nbenchmark measures the model validation accuracy while\nkeeping the foreground but adversarially changing the back-\nground from the other image. Since BGC dataset is built\nupon nine subclasses of ImageNet, the baseline random\nchance is 11.1%. Lastly, we tested adversarial attack robust-\nness using the fast gradient sign method (FGSM) [10].\nTable 6 shows the results. First, we observe that PiT\nshows better performances than ViT in all robustness bench-\nmarks, despite they show comparable performances in the\nstandard ImageNet benchmark (80.8 vs. 79.8). It supports\nthat our dimension design makes the model less sensitive to\nthe backgrounds and the local discriminative features. Also,\nwe found that the performance drops for occluded samples\nby ResNet50 are much dramatic than PiT; 80.8 ‚Üí74.6,\n5% drops for PiT, 79.0 ‚Üí67.1, 15% drops for ResNet50.\nThis implies that ResNet50 focuses more on the local dis-\ncriminative areas, by the nature of convolutional operations.\nInterestingly, in Table 6, ResNet50 outperforms ViT vari-\nants in the background challenge dataset (32.7 vs. 21.0).\nThis implies that the self-attention mechanism unintention-\nally attends more backgrounds comparing to ResNet design\nchoice. Overcoming this potential drawback of vision trans-\nformers will be an interesting research direction.\n5. Conclusion\nIn this paper, we have shown that the design principle\nwidely used in CNNs - the spatial dimensional transfor-\nmation performed by pooling or convolution with strides,\nis not considered in transformer-based architectures such\nas ViT; ultimately affects the model performance. We have\nÔ¨Årst studied with ResNet and found that the transformation\nin respect of the spatial dimension increases the computa-\ntional efÔ¨Åciency and the generalization ability. To leverage\nthe beneÔ¨Åts in ViT, we propose a PiT that incorporates a\npooling layer into Vit, and PiT shows that these advantages\ncan be well harmonized to ViT through extensive experi-\nments. Consequently, while signiÔ¨Åcantly improving the per-\nformance of the ViT architecture, we have shown that the\npooling layer by considering spatial interaction ratio is es-\nsential to a self-attention-based architecture.\nAcknowledgement We thank NA VER AI Lab members for\nvaluable discussion and advice. NSML [20] has been used for\nexperiments. We thank the reviewers for the productive feed-\nback.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 2\n[2] Irwan Bello. Lambdanetworks: Modeling long-range inter-\nactions without attention. In International Conference on\nLearning Representations, 2021. 2\n[3] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct\nneural architecture search on target task and hardware.arXiv\npreprint arXiv:1812.00332, 2018. 5\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision , pages 213‚Äì229. Springer, 2020.\n1\n[5] Nadav Cohen, Or Sharir, and Amnon Shashua. On the ex-\npressive power of deep learning: A tensor analysis. In Con-\nference on learning theory, pages 698‚Äì728. PMLR, 2016. 1\n[6] Nadav Cohen and Amnon Shashua. Inductive bias of deep\nconvolutional networks through pooling geometry. In Inter-\nnational Conference on Learning Representations, 2016. 1\n[7] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V\nLe. Funnel-transformer: Filtering out sequential redun-\ndancy for efÔ¨Åcient language processing. arXiv preprint\narXiv:2006.03236, 2020. 2\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248‚Äì255. Ieee, 2009. 1, 6\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 1, 2, 4, 5, 6, 7\n[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial examples. arXiv\npreprint arXiv:1412.6572, 2014. 8\n[11] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyra-\nmidal residual networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n5927‚Äì5935, 2017. 2\n[12] Dongyoon Han, Sangdoo Yun, Byeongho Heo, and\nYoungJoon Yoo. Rethinking channel dimensions for efÔ¨Å-\ncient model design. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n732‚Äì741, 2021. 2, 3\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 1, 2, 3, 7, 8\n[14] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-\nyuan Xie, and Mu Li. Bag of tricks for image classiÔ¨Åca-\ntion with convolutional neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2019. 7\n[15] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. arXiv\npreprint arXiv:1907.07174, 2019. 8\n[16] Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon\nHan, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, and Jung-\nWoo Ha. Adamp: Slowing down the slowdown for momen-\ntum optimizers on scale-invariant weights. In International\nConference on Learning Representations, 2021. 6\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015. 6\n[18] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoeÔ¨Çer, and Daniel Soudry. Augment your batch: Improving\ngeneralization through instance repetition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8129‚Äì8138, 2020. 6\n[19] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1314‚Äì1324, 2019. 2,\n3, 7\n[20] Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim,\nHeungseok Park, Soeun Park, Hyunwoo Jo, KyungHyun\nKim, Youngil Yang, Youngkwan Kim, Nako Sung, and Jung-\nWoo Ha. NSML: meet the mlaas platform with a real-world\ncase study. CoRR, abs/1810.09957, 2018. 8\n[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiÔ¨Åcation with deep convolutional neural net-\nworks. Advances in neural information processing systems ,\n25:1097‚Äì1105, 2012. 1, 2\n[22] Boyi Li, Felix Wu, Kilian Q Weinberger, and Serge Belongie.\nPositional normalization. In H. Wallach, H. Larochelle, A.\nBeygelzimer, F. d'Alch¬¥e-Buc, E. Fox, and R. Garnett, edi-\ntors, Advances in Neural Information Processing Systems ,\nvolume 32. Curran Associates, Inc., 2019. 2\n[23] Xiao Li, Jianmin Li, Ting Dai, Jie Shi, Jun Zhu, and Xiaolin\nHu. Rethinking natural adversarial examples for classiÔ¨Åca-\ntion models. arXiv preprint arXiv:2102.11731, 2021. 8\n[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740‚Äì755.\nSpringer, 2014. 2, 7\n[25] Hanxiao Liu, Karen Simonyan, and Yiming Yang.\nDarts: Differentiable architecture search. arXiv preprint\narXiv:1806.09055, 2018. 5\n[26] Filip Radenovi ¬¥c, Giorgos Tolias, and Ond Àárej Chum. Fine-\ntuning cnn image retrieval with no human annotation. IEEE\ntransactions on pattern analysis and machine intelligence ,\n41(7):1655‚Äì1668, 2018. 1\n[27] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll ¬¥ar. Designing network design\nspaces. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 10428‚Äì\n10436, 2020. 6, 7\n[28] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch¬¥e-Buc, E. Fox, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems ,\nvolume 32. Curran Associates, Inc., 2019. 2\n[29] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510‚Äì4520, 2018. 2, 3, 7\n[30] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2\n[31] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1‚Äì9, 2015.\n2\n[32] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105‚Äì6114. PMLR,\n2019. 1, 2, 3, 7\n[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training\ndata-efÔ¨Åcient image transformers & distillation through at-\ntention. In International Conference on Machine Learning ,\npages 10347‚Äì10357. PMLR, 2021. 2, 4, 5, 6, 7, 8\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 1, 2\n[35] Jesse Vig and Yonatan Belinkov. Analyzing the structure of\nattention in a transformer language model. arXiv preprint\narXiv:1906.04284, 2019. 4, 5\n[36] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. InEuropean\nConference on Computer Vision , pages 108‚Äì126. Springer,\n2020. 2\n[37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794‚Äì7803, 2018. 1, 2\n[38] Ross Wightman. Pytorch image mod-\nels. https://github.com/rwightman/\npytorch-image-models, 2019. 7, 8\n[39] Yuxin Wu and Kaiming He. Group normalization. In Pro-\nceedings of the European conference on computer vision\n(ECCV), pages 3‚Äì19, 2018. 2\n[40] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander\nMadry. Noise or signal: The role of image backgrounds in\nobject recognition. arXiv preprint arXiv:2006.09994, 2020.\n8\n[41] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classiÔ¨Åers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6023‚Äì6032, 2019. 6\n[42] Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon\nHan, Junsuk Choe, and Sanghyuk Chun. Re-labeling im-\nagenet: from single to multi-labels, from global to local-\nized labels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 2340‚Äì\n2350, 2021. 7\n[43] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R\nManmatha, et al. Resnest: Split-attention networks. arXiv\npreprint arXiv:2004.08955, 2020. 7\n[44] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2, 6, 7, 8",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.690214991569519
    },
    {
      "name": "Architecture",
      "score": 0.671195924282074
    },
    {
      "name": "Transformer",
      "score": 0.6590428948402405
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5874319076538086
    },
    {
      "name": "Pooling",
      "score": 0.5621006488800049
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5513701438903809
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4533587396144867
    },
    {
      "name": "Computer engineering",
      "score": 0.44436508417129517
    },
    {
      "name": "Computer vision",
      "score": 0.37770429253578186
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37646999955177307
    },
    {
      "name": "Engineering",
      "score": 0.16356906294822693
    },
    {
      "name": "Voltage",
      "score": 0.14088502526283264
    },
    {
      "name": "Electrical engineering",
      "score": 0.12536421418190002
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [],
  "cited_by": 67
}