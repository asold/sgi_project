{
    "title": "Joint specular highlight detection and removal in single images via Unet-Transformer",
    "url": "https://openalex.org/W4306734859",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2224932065",
            "name": "Zhongqi Wu",
            "affiliations": [
                "Institute of Automation",
                "Shandong Institute of Automation"
            ]
        },
        {
            "id": "https://openalex.org/A2074636145",
            "name": "Jianwei Guo",
            "affiliations": [
                "Shandong Institute of Automation",
                "Institute of Automation"
            ]
        },
        {
            "id": "https://openalex.org/A3109961818",
            "name": "Chuanqing Zhuang",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2110516816",
            "name": "Jun Xiao",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2629390613",
            "name": "Dong-Ming Yan",
            "affiliations": [
                "Shandong Institute of Automation",
                "Institute of Automation"
            ]
        },
        {
            "id": "https://openalex.org/A2119310824",
            "name": "Xiaopeng Zhang",
            "affiliations": [
                "Shandong Institute of Automation",
                "Institute of Automation"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2110158442",
        "https://openalex.org/W2346725554",
        "https://openalex.org/W3080514906",
        "https://openalex.org/W2963739397",
        "https://openalex.org/W2737550054",
        "https://openalex.org/W3047669229",
        "https://openalex.org/W3141300019",
        "https://openalex.org/W2954876976",
        "https://openalex.org/W2116093366",
        "https://openalex.org/W1991135461",
        "https://openalex.org/W2070080778",
        "https://openalex.org/W2162749542",
        "https://openalex.org/W2147933509",
        "https://openalex.org/W1990309126",
        "https://openalex.org/W2567309586",
        "https://openalex.org/W2964315494",
        "https://openalex.org/W2984647237",
        "https://openalex.org/W3209859545",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2109341431",
        "https://openalex.org/W2623660146",
        "https://openalex.org/W2765946592",
        "https://openalex.org/W3092667514",
        "https://openalex.org/W4229971152",
        "https://openalex.org/W2119737311",
        "https://openalex.org/W2153736712",
        "https://openalex.org/W1969683175",
        "https://openalex.org/W2079686637",
        "https://openalex.org/W2408719804",
        "https://openalex.org/W2896054467",
        "https://openalex.org/W2983829898",
        "https://openalex.org/W37886468",
        "https://openalex.org/W2154847188",
        "https://openalex.org/W2596145394",
        "https://openalex.org/W3136036486",
        "https://openalex.org/W2117940447",
        "https://openalex.org/W201340017",
        "https://openalex.org/W2139698815",
        "https://openalex.org/W1981692923",
        "https://openalex.org/W2068294844",
        "https://openalex.org/W2116330865",
        "https://openalex.org/W3148327892",
        "https://openalex.org/W2793160340",
        "https://openalex.org/W3109498686",
        "https://openalex.org/W3196860301",
        "https://openalex.org/W2996883610",
        "https://openalex.org/W3185905868",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W6608687280",
        "https://openalex.org/W2982389402",
        "https://openalex.org/W2475287302",
        "https://openalex.org/W2982788674",
        "https://openalex.org/W1988700013",
        "https://openalex.org/W2754978346",
        "https://openalex.org/W3210157813",
        "https://openalex.org/W3193639341"
    ],
    "abstract": "Abstract Specular highlight detection and removal is a fundamental problem in computer vision and image processing. In this paper, we present an efficient end-to-end deep learning model for automatically detecting and removing specular highlights in a single image. In particular, an encoder—decoder network is utilized to detect specular highlights, and then a novel Unet-Transformer network performs highlight removal; we append transformer modules instead of feature maps in the Unet architecture. We also introduce a highlight detection module as a mask to guide the removal task. Thus, these two networks can be jointly trained in an effective manner. Thanks to the hierarchical and global properties of the transformer mechanism, our framework is able to establish relationships between continuous self-attention layers, making it possible to directly model the mapping between the diffuse area and the specular highlight area, and reduce indeterminacy within areas containing strong specular highlight reflection. Experiments on public benchmark and real-world images demonstrate that our approach outperforms state-of-the-art methods for both highlight detection and removal tasks.",
    "full_text": "Computational Visual Media\nhttps://doi.org/10.1007/s41095-022-0273-9 Vol. 9, No. 1, March 2023, 141–154\nResearch Article\nJoint specular highlight detection and removal in single images\nvia Unet-Transformer\nZhongqi Wu1,2, Jianwei Guo1,2(\u0000 ), Chuanqing Zhuang2, Jun Xiao2 (\u0000 ), Dong-Ming Yan1,2,\nand Xiaopeng Zhang1,2\nc⃝ The Author(s) 2022.\nAbstract Specular highlight detection and removal is\na fundamental problem in computer vision and image\nprocessing. In this paper, we present an eﬃcient end-\nto-end deep learning model for automatically detecting\nand removing specular highlights in a single image.\nIn particular, an encoder–decoder network is utilized\nto detect specular highlights, and then a novel Unet-\nTransformer network performs highlight removal; we\nappend transformer modules instead of feature maps\nin the Unet architecture. We also introduce a highlight\ndetection module as a mask to guide the removal task.\nThus, these two networks can be jointly trained in\nan eﬀective manner. Thanks to the hierarchical and\nglobal properties of the transformer mechanism, our\nframework is able to establish relationships between\ncontinuous self-attention layers, making it possible\nto directly model the mapping between the diﬀuse\narea and the specular highlight area, and reduce\nindeterminacy within areas containing strong specular\nhighlight reﬂection. Experiments on public benchmark\nand real-world images demonstrate that our approach\noutperforms state-of-the-art methods for both highlight\ndetection and removal tasks.\nKeywords specular highlight detection; specular high-\nlight removal; Unet-Transformer\n1 National Laboratory of Pattern Recognition, Institute of\nAutomation, Chinese Academy of Sciences, Beijing 100190,\nChina. E-mail: Z. Wu, wuzhongqi2019@ia.ac.cn; J. Guo,\njianwei.guo@nlpr.ia.ac.cn (\u0000 ), D.-M. Yan, yandongming@\ngmail.com; X. Zhang, xiaopeng.zhang@ia.ac.cn.\n2 The School of Artiﬁcial Intelligence, University of\nChinese Academy of Sciences, Beijing, China. E-mail:\nC. Zhuang, zhuangchuanqing19@mails.ucas.ac.cn; J. Xiao,\nxiaojun@ucas.ac.cn (\u0000).\nManuscript received: 2021-11-10; accepted: 2022-02-03\n1 Introduction\nSpecular highlights are commonly observed in images.\nHowever, they can interfere with solutions for many\ncomputer vision and image processing tasks, including\nimage segmentation [ 1–3], photometric stereo [ 4],\nbinocular stereo [ 5], and text detection [ 6]. Hence,\neﬀective detection and removal of specular highlights\ncan be beneﬁcial in various real-world applications.\nIn recent decades, many approaches have been\nproposed to address the challenging problems of\nspecular highlight detection and removal. Most\nexisting detection methods are based on various forms\nof thresholding operations [ 7, 8]. These methods are\nbased on the strict premise that the light is white,\nand the brightest pixels form specular highlights. As\nfor specular highlight removal, traditional methods\ncan be roughly classiﬁed into three categories [\n9],\nusing color [ 10, 11], polarization information [ 5],\nor illumination estimation [ 12–14]. Most methods\nmake simple assumptions concerning speciﬁc scenes\nor speciﬁc materials, so are diﬃcult to apply to\nthe complex situations in real-world images. We\nhave observed that scenes with specular highlights\nin the real-world have two common characteristics:\nﬁrstly, specular highlights are usually small and\nsparsely distributed, and secondly, the colors of\nthe highlights are similar to the color of the light\nsource. However, the brightest areas in a real image\nmay not be highlights, but caused by overexposure\nor excessive reﬂections between objects (see the\nbottom row of Fig. 1). As a result, existing\ntraditional methods cannot accurately locate specular\nhighlights, nor can they eliminate the semantic\nambiguity between white (or close to white) materials\nand highlights in complex real scenes, especially\n141\n\n142 Z. Wu, J. Guo, C. Zhuang, et al.\nFig. 1 Specular highlight removal and detection results from our\nmethod. Left: input images with specular highlights. Middle: after\nhighlight removal. Right: detected highlights.\nwhen the image simultaneously contains refraction,\nreﬂection, and transmission, particularly involving\nmetal and glass. Some recent approaches have started\nto utilize deep neural networks [ 15–17] to remove\nspecular highlights from a single image. However,\nmost are trained on synthetic data in a supervised\nmanner, and their detection and removal abilities\nusually deteriorate signiﬁcantly for real-world images.\nFurthermore, existing methods usually regard highlight\ndetection and removal as two separate tasks, and do\nnot use detection results to guide specular highlight\nremoval.\nIn this paper, we propose a new deep neural model\nwhich jointly detects and removes specular highlights\nin a single image. To accomplish this, we utilize the\npopular Unet to detect specular highlight areas, and\nuse this information to guide the specular highlight\nremoval network. Inspired by the great success of\ntransformers in recent computer vision tasks [\n18],\nwhich construct hierarchical feature expressions by\ndividing images or feature maps into smaller windows,\nwe integrate an efficient Swin transformer into\nour highlight removal network. Doing so is also\nsupported by our previous observations about specular\nhighlights, since the Swin transformer [19] works well\nto capture global features and establish relationships\nbetween continuous self-attention layers. This enables\ninteraction and connection between windows of the\nprevious layer, which greatly improves the expressive\nability of the model. As a result of introducing the\ntransformer module, our specular highlight removal\nnetwork is capable of leveraging high-level contextual\nclues to reduce indeterminacy within areas containing\nstrong specular highlight reflections. Furthermore, we\nalso use highlight detection results as a mask to guide\nthe removal task, which can reduce color defects. To\nsum up, the key contributions of this work are:\n•\na joint specular highlight detection and removal\nnetwork that works well for single real-world\nimages, and\n• an eﬃcient Unet-Transformer module for specular\nhighlight removal, where detection results are\nused as a guidance mask to reduce the eﬀects of\nchromatic aberration.\n2 Related work\nIn this section, we brieﬂy review previous specular\nhighlight detection and removal methods.\n2.1 Specular highlight detection\nEarly studies proposed various methods for specular\nhighlight detection task based on a color constancy\nmodel [20–22]. Zhang et al. [ 23] formulated specular\nhighlight detection as non-negative matrix factori-\nzation (NMF) [24] based on the assumption that the\nnumber of specular highlights is small. Li et al. [ 8]\nproposed an adaptive robust principal component\nanalysis (Adaptive-RPCA) method to robustly detect\nspecular highlights in endoscope image sequences.\nRecently, Fu et al. [ 25] presented a large-scale\ndataset for specular highlight detection in real-world\nimages. Based on the dataset, they also proposed a\ndeep neural network leveraging multi-scale context-\ncontrasted features to accurately detect specular\nhighlights. However, this dataset does not contain\ncorresponding diﬀuse images, so it cannot be used for\nlearning specular highlight removal.\n2.2 Specular highlight removal\n2.2.1 Using color\nTan et al. [26] proposed to use the spatial distribution\nof color to overcome specular reﬂection separation\nambiguity in real images. Shen et al. [ 11] separated\nhighlight reﬂections in a color image by selecting\nan appropriate body color for each pixel by error\nanalysis of chromaticity. Shen and Cai [ 27] extended\nthis work to improve the robustness of the algorithm.\nFor real images, specular reﬂections can be eﬀectively\nremoved [ 28–30] using a dichromatic reﬂection\nmodel [10]. Akashi and Okatani [ 31] formulated the\n\nJoint specular highlight detection and removal in single images via Unet-Transformer 143\nseparation of reﬂections as a sparse non-negative\nmatrix factorization problem without spatial priors.\nLikewise, Guo et al. [ 32] imposed the non-negativity\nconstraint on the weighting matrix and enhanced\nrobustness. Fu et al. [ 33] designed an optimization\nframework for simultaneously estimating diﬀuse\nand specular highlight images from a single image.\nThey recovered the diﬀuse components of specular\nhighlight regions by encouraging sparseness of the\nencoding coeﬃcients. However, such methods cannot\nsemantically distinguish whether a bright area in the\nimage is a highlight or a white object: when light-\ncolored areas and highlights coexist, these algorithms\nmay suﬀer from large errors.\n2.2.2 Using polarization\nUnlike color-based methods, to avoid the color\ndistortion caused by illumination, polarization-based\nmethods take advantage of polarimetric information.\nNayar et al. [34] presented a method that separates\nthe diﬀuse and specular components of brightness\nin single images, using color and polarization\ninformation at the same time to obtain constraints on\nthe reﬂection component of each pixel. By using two\nor more images of surface reﬂection, Umeyama and\nGodin [35] proposed a stable separation algorithm for\ndiﬀuse and specular reﬂections based on independent\ncomponent analysis. Further, Wang et al. [ 36]\npresented an eﬃcient specular removal method\nbased on polarization images through global energy\nminimization. Wen et al. [ 37] recently introduced\na new polarization guided model to generate a\npolarization chromaticity image. They conducted\nspecular reﬂection separation by optimizing a global\nenergy function. However, these polarization-based\nmethods rely on strictly controlled light sources and\nare only suitable in certain speciﬁc scenarios. In real\nscenes, some specular highlight components are still\nretained in the diﬀuse reﬂection images.\n2.2.3 Using illumination estimation\nIllumination-estimation-based methods can coarsely\nremove highlights [12, 13, 38]; they either focus on\nestimating the distribution direction of the light\nsource or estimating the illumination color. There are\ntwo methods for estimating illumination color. One is\nto estimate illumination color based on the specular\nreﬂections [14, 39], and the other is to analyze the\nsurface color based on the color constant of the\nprior model [\n40–42]. Lin et al. [ 43] presented an\ninteractive method by introducing specular highlight\nremoval as an inpainting process. Tan et al. [ 44, 45]\nseparated specular illumination using the concept of\ninverse intensity space. However, these methods are\noften susceptible to complex lighting and chromaticity\nissues.\n2.2.4 Using deep-learning\nDeep-learning-based methods have been widely\nused for removing specular highlights in single\nimage; handcrafted priors are replaced by data-\ndriven learning [\n15–17, 46–48]. Shi et al. [ 15] pre-\nsented an encoder–decoder convolutional neural\nnetwork (CNN) to handle the non-Lambertian object\nintrinsic decomposition problem. Funke et al. [ 46]\nproposed a CycleGAN based network for specular\nhighlight removal from a single endoscopic image.\nTo train this network, they constructed a dataset\nby extracting small image patches with specular\nhighlights and patches without highlights from the\nendoscopic video. Lin et al. [ 16] presented a\nfully CNN, trained on a synthetic dataset. The\nnetwork can work out the intricate relationships\nbetween an image and its diﬀuse parts. Muhammad\net al. [17] presented Spec-Net and Spec-CGAN, aimed\nat removing high intensity specularities from low\nchromaticity facial images. Yi et al. [\n49] presented\nan unsupervised method for specular reﬂection layer\nseparation using multi-view images. Wu et al. [47, 48]\nproposed a novel generative adversarial network\n(GAN) for specular highlight removal based on\npolarization theory. Fu et al. [ 50] developed a\nmulti-task network for joint highlight detection and\nremoval, based on a large-scale dataset with manual\nannotation. However, while numerous researchers\nhave considered specularity removal, current methods\nstill leave specular highlight residuals and chromatic\naberrations in real-world scenes.\n3 Methodology\n3.1 Overview\nIn this paper, we propose an end-to-end network\nstructure to jointly detect and remove the specular\nhighlights from real-world images. Given a single\nimage with specular highlights as input, our goal\nis to detect the locations of the highlights and\nrestore diﬀuse reﬂection in such highlight areas. Our\nnetwork architecture comprises two branches and is\n\n144 Z. Wu, J. Guo, C. Zhuang, et al.\nsummarized in Fig. 2. The highlight detection branch\nuses a pure Unet to output a set of masks locating\nthe highlights. Then the second branch uses a novel\nUnet-Transformer-based highlight removal module\nto obtain the corresponding diﬀuse reﬂection image\n(i.e., without specular highlights). We exploit the\nassumption that the locations of highlights provide\na strong prior for highlight removal, so we use the\nhighlight detection result to guide the removal task.\nDetails of our network architecture and loss\nfunctions follow, then implementation details,\nincluding training dataset and settings.\n3.2 Specular highlight detection\nThe specular highlight detection task can be regarded\nas a binary classiﬁcation task, which outputs 0 in\nnon-highlight areas and 1 in highlights. To this end,\nour proposed specular highlight detection network\nis based on an encoder–decoder framework [ 51]. As\nshown in Fig. 2, the network takes the input image\nwith specular highlights Is and outputs a mask M′\nindicating specular highlight regions. We adopt a\nfully convolutional architecture consisting of four\ndownsampling layers (the encoder) and corresponding\nupsampling layers (the decoder). The purpose of\nthe encoder is to extract feature maps from the\nhighlight image. Speciﬁcally, each downsampling\nlayer consists of two 3 ×3 convolutions, each followed\nby a ReLU, and a 2 ×2 max-pooling operation. The\ndecoder is used to output the pixel classiﬁcation result.\nIt consists of three parts: upsampling of a feature\nmap followed by a 2\n×2 convolution in which the\nnumber of feature channels is halved, concatenation\nwith the corresponding cropped feature map from\ndownsampling, and two 3 ×3 convolutions each of\nwhich is followed by a ReLU. Cropping is necessary\ndue to the loss of border pixels in every convolution.\n3.3 Specular highlight removal\nExisting specular highlight removal methods do not\nachieve satisfactory results for transparent objects\nand complex scenes (e.g., in which transmission and\nreﬂection exist at the same time), which usually\nrequire global contextual reasoning. To solve this\nproblem, our key idea is to exploit the self-attention\nmechanism [ 52] of transformers to enhance the\nconnection between a specular highlight area and\nthe surrounding area. Our method appears to be the\nﬁrst application of transformers to specular highlight\nremoval.\nAs Fig. 2 shows, we ﬁrst perform a patch partition\noperation to preprocess the input images Is and M′.\nThis partition changes the image from H ×W ×3\nto a tensor of size H/4 ×W/4 ×48. Then in stage\n1 (see Fig. 2), a linear embedding layer is used to\nconvert an image with dimensions ( H/4, W/4, 48)\ninto a feature vector with dimensions (H/4, W/4, C).\nIn stages 2–4, the patch merging layer divides the\nFig. 2 Proposed specular highlight detection and removal framework. Given an input image Is with specular highlights, an encoder–decoder\ndetection network outputs a mask M′indicating highlight areas. Using the detection results as guidance, a Unet-Transformer provides a\nspecular-free diﬀuse image Id′.\n\nJoint specular highlight detection and removal in single images via Unet-Transformer 145\ninput patches into 4 parts and concatenates them.\nThis processing down-samples the feature resolution\nby a factor of 2. Since the concatenation operation\nincreases the feature dimension by a factor of 4, a\nlinear layer is applied to the concatenated features\nto unify the feature dimension to twice the original\ndimension.\nHere we adopt a powerful transformer architecture,\na Swin transformer [ 19], recently used in image\nclassiﬁcation and segmentation, to generate the\nhierarchical feature maps in linear time. The Swin\ntransformer is constructed using shifted windows, as\nshown in Fig. 3; two consecutive Swin transformer\nblocks are presented. Each Swin transformer block\nis composed of a LayerNorm (LN) layer, a multi-\nhead self attention module, a residual connection,\nand a 2-layer multilayer perceptron (MLP) with a\nnon-linear Gaussian error linear unit (GELU). The\nwindow-based multi-head self attention (W-MSA)\nmodule and the shifted window-based multi-head self\nattention (SW-MSA) module are applied in the two\nsuccessive transformer blocks. The transformer uses\nlinear projections to compute a set of queries Q, keys\nK, and values V, and takes a weighted sum of value\nvectors according to a similarity distribution between\nquery and key vectors:\nQ= BHWC1, K = BHWC2, V = BHWC3\n(1)\nAttention(Q,K,V ) = Softmax(QKT/\n√\nd+ B)V\n(2)\nwhere Q is a matrix of nq query vectors, B is the\nlearnable relative positional encoding, K and V\nboth contain nk keys and values, all with the same\ndimensionality, and d is a scaling factor.\nIn the implementation of the encoder, the C-\ndimensional tokenized inputs with resolution H/4 ×\nW/4 are fed into the Swin transformer block to\nFig. 3 A Swin transformer block [ 19]. LN: LayerNorm layer. W-\nMSA: window-based multi-head self-attention module. SW-MSA:\nshifted window-based multi-head self-attention module.\nperform representation learning. Note that in this\nprocess the feature dimension and resolution remain\nunchanged. Then the patch merging layer reduces\nthe number of tokens (using 2 ×downsampling) and\nincreases the feature dimension by a factor of 2. This\nprocedure is repeated three times in the encoder.\nFinally, the decoder, consisting of four upsampling\nlayers, is used to output the generated diﬀuse\nimage Id′.\nHowever, in some cases, the specular highlight may\nbe located in a large area with strong intensity, and\ndirect removal will cause chromatic aberration in\nnon-highlight areas. In order to reduce chromatic\naberration, we use the specular highlight detection\nmask to guide specular highlight removal.\n3.4 Loss functions\nTo jointly train the network, we integrate the two\nmodules above in a uniﬁed network architecture.\nNetwork training is supervised by an eﬃcient loss\nfunction with three components: specular detection\nloss, context loss, and style loss.\n3.4.1 Specular detection loss\nCross-entropy loss is commonly used for solving\nedge detection and semantic segmentation problems.\nSimilarly, we use this loss for the specular highlight\ndetection task, formulating it as LBCE:\nLBCE = −\n∑\ni\n[Mi log(M′\ni) + (1−Mi) log(1−M′\ni)]\n(3)\nwhere i indexes each pixel, Mi is an element of\nthe ground-truth highlight mask M, and M′\ni is the\npredicted probability of the pixel belonging to a\nspecular highlight area.\n3.4.2 Pixel loss\nFollowing Ref. [53], we use pixel loss to reduce the\nintensity and texture diﬀerence between the generated\ndiﬀuse image Id′and the ground-truth image Id:\nLpixel = α∥Id −Id′∥2\n2 + β(∥∇xId −∇xId′∥1+\n∥∇yId −∇yId′∥1) (4)\nWe set α = 0.2 and β= 0.4 in all of our experiments.\n3.4.3 Style loss\nStyle loss is usually used in image style transfer\ntasks [54]. We use this loss to add constraints on\nthe pixel and feature space:\nLstyle = σ∥ψ(Id′) −ψ(Id)∥1 (5)\nwhere σ = 120, ψ(·) = φ(·)φ(·)T is the Gram\nmatrix [54], where φ are feature maps of pre-trained\n\n146 Z. Wu, J. Guo, C. Zhuang, et al.\nVGG-16 [55]. The selected layer indices in VGG-16\nfor style loss are 0, 5, 10, 19, 28.\n3.4.4 Total loss\nTo summarize, our total loss function is deﬁned as\nLG = ω1LBCE + ω2Lpixel + ω3Lstyle (6)\nwhere we set ω1 = 1.0, ω2 = 1.0, and ω3 = 0.08 in\nour experiments.\n3.5 Implementation details\n3.5.1 Datasets\nWe trained our method on the SHIQ dataset [ 50],\nwhich provides specular highlight images, corre-\nsponding diﬀuse images, and highlight mask images.\nThese specular highlight images were collected from\nthe MIW dataset [ 56], which contains many hard\nshiny materials (e.g., metal, plastics, glass). Note\nthat instead of capturing real diﬀuse images, the\ncorresponding diﬀuse images were generated by the\nRPCA method [57]. However, the results generated\nby RPCA still contain specular residuals, so Fu\net al. [\n50] only cropped high-quality local images\n(with paired specular image and specular-free diﬀuse\nimage) to build the SHIQ dataset. We used 9825\ngroups for training and 999 groups for testing. The\nresolution of each image was 200 ×200.\n3.5.2 Training settings\nOur network was implemented in PyTorch on an\nNVIDIA GeForce GTX1080Ti graphics card. We\ntrained the network on our training set for 60 epochs\nusing the Adam optimizer [ 58]. The initial learning\nrate was set to 10−4 and reduced using an attenuation\ncoeﬃcient of 0.8 every 5 epochs until reaching 10 −5.\nWe also augmented the SHIQ dataset by randomly\nmirror-ﬂipping images and adding noise.\n4 Experimental results\nIn this section, we start with several experiments\nthrough visually inspecting our results on the\ndataset to demonstrate the eﬀectiveness of our\nproposed neural network. Then we compare our\ndetection and removal results with current state-of-\nthe-art approaches with qualitative and quantitative\nevaluations.\n4.1 Detection and removal results\nFigure 4 shows our specular highlight detection\nand removal results on some representative images\nselected from the SHIQ dataset. These examples\ninclude objects with diﬀerent materials such as\ntransparent plastic, glass, and metal. Specular\nhighlight removal is particularly challenging for\ntransparent objects as there are both reﬂection and\ntransmission components, making it diﬃcult restore\nspecular highlight areas. As the ﬁgure illustrates, our\nmethod can accurately locate the specular highlights,\nand can also eﬀectively remove them from such\nobjects. The fourth row shows that our method can\nstill give satisfactory results for large highlight areas,\nand areas including reﬂections.\n4.2 Comparisons\n4.2.1 Highlight detection\nWe ﬁrst compare highlight detection results from our\nmethod with those from previous methods, including\ntwo traditional methods (NMF [ 8] and ATA [ 23]),\nand two state-of-the-art deep-learning-based methods\n(SHDN [ 25] and JSHDR [ 50]). For quantitative\nevaluation, we adopt two commonly used metrics,\nnamely, detection accuracy and balance error rate\n(BER). Their deﬁnitions are\nAcc = TP + TN\nTP + TN + FP + FN (7)\nBER = 1\n2\n( FP\nTN + FP + FN\nTN + TP\n)\n(8)\nwhere TP is the number of true positives, TN true\nnegatives, FP false positives, and FN false negatives.\nA higher value of accuracy and a lower value of\nBER indicate better detection results. Table 1 reports\nquantitative comparison results on the SHIQ testing\ndata. As we can see, JSHDR [ 50] and our method\nachieve the best results, while our method is slightly\nbetter in terms of accuracy.\n4.2.2 Highlight removal\nWe also compare our approach to various highlight\nremoval competitors, including two traditional app-\nroaches (Shen et al. [ 11], Yamamoto et al. [ 59]), and\nTable 1 Quantitative comparison of our method with state-of-the-art\nhighlight detection methods. The best results are given in bold\nMethod Acc↑ BER↓\nNMF 0.70 18.8\nATA 0.71 24.4\nSHDN 0.91 6.18\nJSHDR 0.93 5.92\nOurs 0.97 5.92\n\nJoint specular highlight detection and removal in single images via Unet-Transformer 147\nFig. 4 Specular highlight detection and removal results using our neural network: (a) input images with specular reﬂections, (b) ground-truth\ndiﬀuse images, (c) ground-truth masks of the specular highlights, (d) our removal results, and (e) our detection results.\nthree state-of-the-art deep-learning-based approaches\n(Multi-class GAN [16], Spec-CGAN [46], and JSHDR\n[50]). For a fair comparison, we re-trained Multi-\nclass GAN and Spec-CGAN on the SHIQ dataset.\nFigure 5 shows results using the SHIQ testing data\nfor evaluation. We observe that Shen and Cai [27] and\n\n148 Z. Wu, J. Guo, C. Zhuang, et al.\nFig. 5 Visual comparison of highlight removal methods on the SHIQ dataset: (a) input image, (b) ground-truth specular-free diﬀuse image,\nand results from (c) Shen et al. [ 11], (d) Yamamoto et al. [ 59], (e) Multi-class GAN [ 16], (f) Spec-CGAN [ 46], (g) JSHDR [ 50], and (h) our\nmethod.\nJSHDR [50] have local specular highlight residuals,\nespecially on the Garniture and Wrapper scenes\n(in ﬁrst and fourth rows). Yamamoto et al. [ 59]\ninduced color distortion on the surfaces of light\ncolor objects, resulting in dark areas. Multi-class\nGAN [16] and Spec-CGAN [46] leave obvious specular\nhighlight residuals. In comparison, our network\nremoves most of the highlights and produces no dark\nshadows and chromatic aberrations. For quantitative\ncomparison, we adopt three commonly used metrics:\nmean-squared error (MSE), structural similarity\nindex (SSIM), and peak-signal-to-noise ratio (PSNR).\nTable 2 reports these values for diﬀerent methods\nfor Fig. 5. Our network has better scores than all\ncompared methods.\nFinally, in order to compare the capabilities of\napproaches using natural images, we captured a real-\nworld specular testing dataset using a cellphone. As\nFig. 6 shows, traditional methods like Shen et al. [ 11]\neither fail to eﬀectively remove specular highlights\n(see ﬁrst row), or produce chromatic aberrations\n(see rows 2, 3, 5). Yamamoto et al. [ 59] generated\ndistinct dark areas in light and specular highlight\nregions. Multi-class GAN [\n16] can remove some\nspecular highlights from the images, but chromatic\naberration appears (see rows 2, 4). Spec-CGAN [ 46]\nhas obvious specular highlight residuals (see rows 3,\n4, 5). JSHDR [ 50] achieves good results overall, but\nTable 2 Quantitative comparison of highlight removal methods on\nthe SHIQ dataset\nScene\nMethod MSE/10 −2↓ SSIM ↑ PSNR ↑\nShen\net al. 5.93 0.3707 12.27\nYamamoto et al. 27.12 0.0679 5.67\nGarniture Multi-class GAN 0.45 0.9373 23.51\nSpec-CGAN 0.14 0.9597 28.38\nJSHDR 0.08 0.9738 31.00\nOurs 0.04 0.9812 34.47\nShen\net al. 0.21 0.9046 26.79\nYamamoto et al. 10.87 0.5450 9.63\nMetal Multi-class GAN 0.23 0.9666 26.33\nSpec-CGAN 0.21 0.9712 26.85\nJSHDR 0.07 0.9923 34.30\nOurs 0.05 0.9894 32.43\nShen\net al. 4.87 0.2728 13.13\nYamamoto et al. 6.89 0.1828 11.62\nPlastic Multi-class GAN 0.36 0.8627 24.38\nSpec-CGAN 1.24 0.8869 19.07\nJSHDR 0.51 0.9282 22.93\nOurs 0.20 0.9374 26.99\nShen\net al. 10.75 0.2903 24.61\nYamamoto et al. 6.56 0.3824 9.69\nWrapper Multi-class GAN 0.31 0.8746 23.75\nSpec-CGAN 1.04 0.8439 19.83\nJSHDR 0.35 0.9515 24.54\nOurs 0.28 0.9598 25.51\nit still generates distinct dark patches (see rows 2, 3)\nand has obvious specular highlight residuals (see\n\nJoint specular highlight detection and removal in single images via Unet-Transformer 149\nFig. 6 Visual comparison on natural images in the wild: (a) input, and results from (b) Shen et al. [ 11], (c) Yamamoto et al. [ 59], (d)\nMulti-class GAN [16], (e) Spec-CGAN [46], (f) JSHDR [50], and (g) our method.\nrows 4, 5). In contrast, our network can eﬀectively\nremove specular highlights without generating dark\nshadows or chromatic aberrations. Our neural\nnetwork generalises well.\n4.3 Ablation studies\n4.3.1 Highlight detection\nIt should be noted that, when the highlight detection\nand removal networks are jointly trained, the\ndetection results are slightly inferior to when only\nusing the highlight detection network. This is be-\ncause the highlight detection probability is used\nas a weight when the diﬀuse reﬂection image\nis ﬁnally generated; the two have a connection\nrelationship via the gradient which aﬀects the\ndetection result. To demonstrate this, we conducted\nexperiments to evaluate the inﬂuence of the specular\nhighlight removal network and Swin transformer on\nthe specular detection network. Table 3 reports\nquantitative results of this ablation study. As ob-\nserved, when the removal network or the Swin\nTable 3 Quantitative comparison of detection network settings. The\nbest result in each case is shown in bold\nMethod Accuracy ↑ BER ↓\nOurs without removal 0.92 6.98\nOurs without Swin transformers 0.93 6.31\nOur full method 0.97 5.92\ntransformer is removed from our network, the balance\nerror rate metric is slightly worse than for SHDN [ 25]\nand JSHDR [ 50], while our full method is slightly\nbetter than the other methods.\n4.3.2 Highlight removal\nTo verify the eﬀectiveness of our network architecture\nand loss functions, we compared our network with\nablated versions. Visual examples from the ablation\nstudies are shown in Fig. 7 and corresponding\nquantitative results are given in Table 4. As we can\nsee, our full model achieves the best performance.\nAs shown in Fig. 7(c), Fig. 7(d), and Fig. 7(g),\nthe network without Swin transformers produces\nobvious chromatic aberration. The network without\n\n150 Z. Wu, J. Guo, C. Zhuang, et al.\nFig. 7 Visual examples from ablation studies for the proposed network: (a) input, (b) ground-truth, and results from (c) our method without\nSwin transformer, (d) our method without highlight mask, (e) our method without pixel loss, (f) our method without style loss, and (g) our\noverall method.\nhighlight mask produces artifacts in the highlights,\nindicating the usefulness of the highlight mask as\nguidance in local highlight areas. As shown in\nFigs. 7(e)–7(g), results obtained without pixel loss\nhave highlight remnants (see rows 1, 3). Results\nwithout style loss produce artifacts (see rows 1, 2)\nand highlight remnants (see row 3). Our full loss\n(see Fig. 7(g)) provides cleaner results with fewer\nhighlight remnants and no artifacts. The quantitative\nresults in the Table. 4 further show that our full\nTable 4 Quantitative comparison of ablation study, corresponding\nto Fig. 7. “ −” means without\nScene\nMethod MSE/10−2↓ SSIM ↑ PSNR ↑\n−Swin\n0.08 0.8852 30.58\n−mask 0.04 0.9798 34.19\nFaucet −pixel loss 0.09 0.9781 30.64\n−style loss 0.04 0.9815 33.65\nfull method 0.03 0.9822 34.65\n−Swin\n0.25 0.8801 25.85\n−mask 0.10 0.9859 29.86\nBottle −pixel loss 0.05 0.9874 32.42\n−style loss 0.07 0.9882 31.72\nfull method 0.05 0.9886 33.08\n−Swin\n1.06 0.8248 19.77\n−mask 0.05 0.9972 32.80\nYellow plastic −pixel loss 0.07 0.9951 31.63\n−style loss 0.04 0.9975 33.92\nfull method 0.04 0.9976 33.61\nnetwork achieves the best results. We can also see\nthat Swin transformers play a more important role\nthan highlight masks for highlight removal.\n4.4 Limitations\nWe have successfully applied our method for detecting\nand removing specular highlights to a variety of single\nimages. However, our neural network in common with\nmany state-of-the-art methods may fail to remove\nlarge specular highlight areas, as shown in Fig. 8,\nwhere the large areas lack meaningful and reliable\ncontextual cues to help restore them. Furthermore,\nout network cannot handle images with text due to\na lack of training data richness: when the specular\nhighlight covers part of the text, it is challenging to\nFig. 8 Examples of failures: (a) input image, (b) our highlight\nremoval results, and (c) our detection results.\n\nJoint specular highlight detection and removal in single images via Unet-Transformer 151\nremove the highlights. To handle such cases, a text\ndetection branch or a text-aware loss might help [ 60].\n5 Conclusions and future work\nThis work has solved the challenging problem of\njoint specular highlight detection and removal in\na single image, using an end-to-end deep learning\nframework that consists of two networks: an encoder–\ndecoder network for highlight detection, and a\nUnet-Transformer network for highlight removal.\nWe also use the detection results as guidance to\nensure that the highlight removal network pays\nmore attention to the highlight areas. A variety\nof experiments on public benchmark datasets and\nmany challenging real images have shown the\neﬀectiveness of our neural network. Our source\ncode is publicly available at\nhttps://github.com/\njianweiguo/specularityRemoval.\nIn future, we hope to remove specular highlights\nfrom complex scenes with rich textures. We also\nplan to construct a large dataset and design a\nmore eﬀective text-related loss to promote text-\naware highlight removal. Finally, we will explore the\nrelationship between specular highlights and object\ngeometry, such as ﬂat, spherical, and cylindrical\nhighlights, which may help to accurately locate and\nremove specular highlights.\nAcknowledgements\nThis work was partially funded by the National\nNatural Science Foundation of China (U21A20515,\n62172416, 62172415, U2003109), and Youth\nInnovation Promotion Association of the Chinese\nAcademy of Sciences (2022131).\nDeclaration of competing interest\nThe authors have no competing interests to declare\nthat are relevant to the content of this article.\nReferences\n[1] Arbel´ aez, P.; Maire, M.; Fowlkes, C.; Malik, J.\nContour detection and hierarchical image segmentation.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence Vol. 33, No. 5, 898–916, 2011.\n[2] Tao, M. W.; Su, J. C.; Wang, T. C.; Malik, J.;\nRamamoorthi, R. Depth estimation and specular\nremoval for glossy surfaces using point and line\nconsistency with light-ﬁeld cameras.IEEE Transactions\non Pattern Analysis and Machine Intelligence Vol. 38,\nNo. 6, 1155–1169, 2016.\n[3] Ramadan, H.; Lachqar, C.; Tairi, H. A survey\nof recent interactive image segmentation methods.\nComputational Visual Media Vol. 6, No. 4, 355–384,\n2020.\n[4] Khanian, M.; Boroujerdi, A. S.; Breuß, M. Photometric\nstereo for strong specular highlights. Computational\nVisual Media Vol. 4, No. 1, 83–102, 2018.\n[5] Cui, Z. P.; Gu, J. W.; Shi, B. X.; Tan, P.; Kautz,\nJ. Polarimetric multi-view stereo. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 369–378, 2017.\n[6] Xue, M. L.; Shivakumara, P.; Zhang, C.; Xiao, Y.; Lu,\nT.; Pal, U.; Lopresti, D.; Yang, Z. Arbitrarily-oriented\ntext detection in low light natural scene images. IEEE\nTransactions on Multimedia Vol. 23, 2706–2720, 2021.\n[7] Osadchy, M.; Jacobs, D. W.; Ramamoorthi, R. Using\nspecularities for recognition. In: Proceedings of the 9th\nIEEE International Conference on Computer Vision,\n1512–1519, 2003.\n[8] Li, R. Y.; Pan, J. J.; Si, Y. Q.; Yan, B.; Hu, Y.; Qin,\nH. Specular reﬂections removal for endoscopic image\nsequences with adaptive-RPCA decomposition. IEEE\nTransactions on Medical Imaging Vol. 39, No. 2, 328–\n340, 2020.\n[9] Artusi, A.; Banterle, F.; Chetverikov, D. A survey\nof specularity removal methods. Computer Graphics\nForum Vol. 30, No. 8, 2208–2230, 2011.\n[10] Shafer, S. A. Using color to separate reﬂection\ncomponents. Color Research & Application Vol. 10,\nNo. 4, 210–218, 1985.\n[11] Shen, H. L.; Zhang, H. G.; Shao, S. J.; Xin, J. H.\nChromaticity-based separation of reﬂection components\nin a single image. Pattern Recognition Vol. 41, No. 8,\n2461–2469, 2008.\n[12] Brainard, D. H.; Freeman, W. T. Bayesian color\nconstancy. Journal of the Optical Society of America A\nVol. 14, No. 7, 1393–1411, 1997.\n[13] Finlayson, G. D.; Hordley, S. D.; HubeL, P. M. Color\nby correlation: A simple, unifying framework for color\nconstancy. IEEE Transactions on Pattern Analysis and\nMachine Intelligence Vol. 23, No. 11, 1209–1221, 2001.\n[14] Tan, R. T.; Nishino, K.; Ikeuchi, K. Color constancy\nthrough inverse-intensity chromaticity space. Journal\nof the Optical Society of America A Vol. 21, No. 3,\n321–334, 2004.\n\n152 Z. Wu, J. Guo, C. Zhuang, et al.\n[15] Shi, J.; Dong, Y.; Su, H.; Yu, S. X. Learning\nnon-Lambertian object intrinsics across ShapeNet\ncategories. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 5844–\n5853, 2017.\n[16] Lin, J.; El Amine Seddik, M.; Tamaazousti, M.;\nTamaazousti, Y.; Bartoli, A. Deep multi-class\nadversarial specularity removal. In: Image Analysis.\nLecture Notes in Computer Science, Vol. 11482 .\nFelsberg, M.; Forss´ en, P. E.; Sintorn, I. M.; Unger,\nJ. Eds. Springer Cham, 3–15, 2019.\n[17] Muhammad, S.; Dailey, M. N.; Farooq, M.; Majeed,\nM. F.; Ekpanyapong, M. Spec-Net and Spec-CGAN:\nDeep learning models for specularity removal from faces.\nImage and Vision Computing Vol. 93, 103823, 2020.\n[18] Xu, Y. F.; Wei, H. P.; Lin, M. X.; Deng, Y. Y.; Sheng,\nK. K.; Zhang, M. D.; Tang, F.; Dong, W.; Huang, F.;\nXu, C. Transformers in computational visual media:\nA survey. Computational Visual Media Vol. 8, No. 1,\n33–62, 2022.\n[19] Liu, Z.; Lin, Y. T.; Cao, Y.; Hu, H.; Wei, Y.\nX.; Zhang, Z.; Lin, S.; Guo, B. Swin transformer:\nHierarchical vision transformer using shifted windows.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 9992–10002, 2021.\n[20] Maloney, L. T.; Wandell, B. A. Color constancy:\nA method for recovering surface spectral reﬂectance.\nJournal of the Optical Society of America A Vol. 3,\nNo. 1, 29–33, 1986.\n[21] Park, J. B.; Kak, A. C. A truncated least squares\napproach to the detection of specular highlights in\ncolor images. In: Proceedings of the IEEE International\nConference on Robotics and Automation, 1397–1403,\n2003.\n[22] Meslouhi, O.; Kardouchi, M.; Allali, H.; Gadi, T.;\nBenkaddour, Y. Automatic detection and inpainting\nof specular reﬂections for colposcopic images. Central\nEuropean Journal of Computer Science Vol. 1, No. 3,\n341–354, 2011.\n[23] Zhang, W. M.; Zhao, X.; Morvan, J. M.; Chen, L.\nM. Improving shadow suppression for illumination\nrobust face recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence Vol. 41, No. 3, 611–\n624, 2019.\n[24] Hoyer, P. O. Non-negative matrix factorization with\nsparseness constraints. Journal of Machine Learning\nResearch Vol. 5, 1457–1469, 2004.\n[25] Fu, G.; Zhang, Q.; Lin, Q. F.; Zhu, L.; Xiao, C. X.\nLearning to detect specular highlights from real-world\nimages. In: Proceedings of the 28th ACM International\nConference on Multimedia, 1873–1881, 2020.\n[26] Tan, P.; Quan, L.; Lin, S. Separation of highlight\nreﬂections on textured surfaces. In: Proceedings of\nthe IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, 1855–1860, 2006.\n[27] Shen, H.-L.; Cai, Q.-Y. Simple and eﬃcient method for\nspecularity removal in an image. Applied Optics Vol. 48,\nNo. 14, 2711, 2009.\n[28] Shen, H. L.; Zheng, Z. H. Real-time highlight removal\nusing intensity ratio. Applied Optics Vol. 52, No. 19,\n4483–4493, 2013.\n[29] Yang, J. W.; Liu, L. X.; Li, S. Z. Separating specular\nand diﬀuse reﬂection components in the HSI color space.\nIn: Proceedings of the IEEE International Conference\non Computer Vision Workshops, 891–898, 2013.\n[30] Yang, Q. X.; Tang, J. H.; Ahuja, N. Eﬃcient and\nrobust specular highlight removal. IEEE Transactions\non Pattern Analysis and Machine Intelligence Vol. 37,\nNo. 6, 1304–1311, 2015.\n[31] Akashi, Y.; Okatani, T. Separation of reﬂection\ncomponents by sparse non-negative matrix\nfactorization. In: Computer Vision – ACCV\n2014. Lecture Notes in Computer Science, Vol. 9007 .\nCremers, D.; Reid, I.; Saito, H.; Yang, M. H. Eds.\nSpringer Cham, 611–625, 2015.\n[32] Guo, J.; Zhou, Z. J.; Wang, L. M. Single image highlight\nremoval with a sparse and low-rank reﬂection model.\nIn: Computer Vision – ECCV 2018. Lecture Notes\nin Computer Science, Vol. 11208 . Ferrari, V.; Hebert,\nM.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham,\n282–298, 2018.\n[33] Fu, G.; Zhang, Q.; Song, C. F.; Lin, Q. F.; Xiao, C.\nX. Specular highlight removal for real-world images.\nComputer Graphics Forum Vol. 38, No. 7, 253–263,\n2019.\n[34] Nayar, S. K.; Fang, X. S.; Boult, T. Separation of\nreﬂection components using color and polarization.\nInternational Journal of Computer Vision Vol. 21, No.\n163–186, 1997.\n[35] Umeyama, S.; Godin, G. Separation of diﬀuse and\nspecular components of surface reﬂection by use\nof polarization and statistical analysis of images.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence Vol. 26, No. 5, 639–647, 2004.\n[36] Wang, F.; Ainouz, S.; Petitjean, C.; Bensrhair, A.\nSpecularity removal: A global energy minimization\napproach based on polarization imaging. Computer\nVision and Image Understanding Vol. 158, 31–39, 2017.\n[37] Wen, S.; Zheng, Y.; Lu, F. Polarization guided specular\nreﬂection separation. IEEE Transactions on Image\nProcessing Vol. 30, 7280–7291, 2021.\n[38] Sapiro, G. Color and illuminant voting. IEEE Trans-\n\nJoint specular highlight detection and removal in single images via Unet-Transformer 153\nactions on Pattern Analysis and Machine Intelligence\nVol. 21, No. 11, 1210–1215, 1999.\n[39] Imai, Y.; Kato, Y.; Kadoi, H.; Horiuchi, T.;\nTominaga, S. Estimation of multiple illuminants based\non specular highlight detection. In: Computational\nColor Imaging. Lecture Notes in Computer Science,\nVol. 6626. Schettini, R.; Tominaga, S.; Tr´ emeau, A.\nEds. Springer Berlin Heidelberg, 85–98, 2011.\n[40] Forsyth, D. A. A novel algorithm for color constancy.\nInternational Journal of Computer Vision Vol. 5, No. 1,\n5–35, 1990.\n[41] Hansen, T.; Olkkonen, M.; Walter, S.; Gegenfurtner,\nK. R. Memory modulates color appearance. Nature\nNeuroscience Vol. 9, No. 11, 1367–1368, 2006.\n[42] Joze, H. R. V.; Drew, M. S. Exemplar-based\ncolor constancy and multiple illumination. IEEE\nTransactions on Pattern Analysis and Machine\nIntelligence Vol. 36, No. 5, 860–873, 2014.\n[43] Lin, P.; Quan, L.; Shum, H.-Y. Highlight removal by\nillumination-constrained inpainting. In: Proceedings of\nthe 9th IEEE International Conference on Computer\nVision, 164–169, 2003.\n[44] Tan, R. T.; Ikeuchi, K. Separating reﬂection\ncomponents of textured surfaces using a single image.\nIn: Digitally Archiving Cultural Objects . Springer\nBoston MA, 353–384, 2008.\n[45] Tan, T. T.; Nishino, K.; Ikeuchi, K. Illumination\nchromaticity estimation using inverse-intensity\nchromaticity space. In: Proceedings of the IEEE\nComputer Society Conference on Computer Vision\nand Pattern Recognition, I, 2003.\n[46] Funke, I.; Bodenstedt, S.; Riediger, C.; Weitz,\nJ.; Speidel, S. Generative adversarial networks for\nspecular highlight removal in endoscopic images. In:\nProceedings of the SPIE 10576, Medical Imaging 2018:\nImage-Guided Procedures, Robotic Interventions, and\nModeling, 1057604, 2018.\n[47] Wu, Z. Q.; Zhuang, C. Q.; Shi, J.; Xiao, J.; Guo, J. W.\nDeep specular highlight removal for single real-world\nimage. In: Proceedings of the SIGGRAPH Asia 2020\nPosters, Article No. 34, 2020.\n[48] Wu, Z. Q.; Zhuang, C. Q.; Shi, J.; Guo, J. W.; Xiao,\nJ.; Zhang, X. P.; Yan, D.-M. Single-image specular\nhighlight removal via real-world dataset construction.\nIEEE Transactions on Multimedia Vol. 24, 3782–3793,\n2022.\n[49] Yi, R. J.; Tan, P.; Lin, S. Leveraging multi-view image\nsets for unsupervised intrinsic image decomposition\nand highlight separation. Proceedings of the AAAI\nConference on Artiﬁcial Intelligence Vol. 34, No. 7,\n12685–12692, 2020.\n[50] Fu, G.; Zhang, Q.; Zhu, L.; Li, P.; Xiao, C. X. A multi-\ntask network for joint specular highlight detection and\nremoval. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 7748–\n7757, 2021.\n[51] Ronneberger, O.; Fischer, P.; Brox, T. U-\nNet: Convolutional networks for biomedical image\nsegmentation. In: Medical Image Computing and\nComputer-Assisted Intervention – MICCAI 2015.\nLecture Notes in Computer Science, Vol. 9351. Navab,\nN.; Hornegger, J.; Wells, W.; Frangi, A. Eds. Springer\nCham, 234–241, 2015.\n[52] Cao, H.; Wang, Y.; Chen, J.; Jiang, D.; Zhang,\nX.; Tian, Q.; Wang, M. Swin-Unet: Unet-like pure\ntransformer for medical image segmentation. arXiv\npreprint arXiv:2105.05537, 2021.\n[53] Wei, K. X.; Yang, J. L.; Fu, Y.; Wipf, D.;\nHuang, H. Single image reﬂection removal exploiting\nmisaligned training data and network enhancements.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 8170–8179,\n2019.\n[54] Gatys, L. A.; Ecker, A. S.; Bethge, M. Image\nstyle transfer using convolutional neural networks. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2414–2423, 2016.\n[55] Simonyan, K.; Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[56] Murmann, L.; Gharbi, M.; Aittala, M.; Durand,\nF. A dataset of multi-illumination images in the\nwild. In: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 4079–4088, 2019.\n[57] Guo, X. J.; Cao, X. C.; Ma, Y. Robust separation\nof reﬂection from multiple images. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 2195–2202, 2014.\n[58] Kingma, D. P.; Ba, J. Adam: A method for stochastic\noptimization. In: Proceedings of the 3rd International\nConference for Learning Representations, 2015.\n[59] Yamamoto, T.; Kitajima, T.; Kawauchi, R. Eﬃcient\nimprovement method for separation of reﬂection\ncomponents based on an energy function. In:\nProceedings of the IEEE International Conference on\nImage Processing, 4222–4226, 2017.\n[60] Hou, S.; Wang, C.; Quan, W.; Jiang, J.; Yan, D. M.\nText-aware single image specular highlight removal.\nIn: Pattern Recognition and Computer Vision. Lecture\nNotes in Computer Science, Vol. 13022. Springer Cham,\n115–127, 2021.\n\n154 Z. Wu, J. Guo, C. Zhuang, et al.\nZhongqi Wureceived her master degree\nfrom the School of Artiﬁcial Intelligence\nof the University of the Chinese Academy\nof Sciences in 2019. She is currently\nworking towards her Ph.D degree at\nthe National Laboratory of Pattern\nRecognition, Institute of Automation,\nChinese Academy of Sciences. Her\nresearch interests include image processing and computer\nvision.\nJianwei Guois an associate professor\nin the National Laboratory of Pattern\nRecognition, Institute of Automation,\nChinese Academy of Sciences (CASIA).\nHe received his Ph.D. degree in com-\nputer science from CASIA in 2016,\nand bachelor degree from Shandong\nUniversity in 2011. His research interests\ninclude computer vision, computer graphics, and image\nprocessing.\nChuanqing Zhuang is working\ntoward a master degree in School of\nArtiﬁcial Intelligence, the University\nof the Chinese Academy of Sciences.\nHe received his bachelor degree in\nengineering from Tsinghua University\nin 2019. His re-search interests include\ncomputer vision and image processing.\nJun Xiao is a professor in the\nUniversity of the Chinese Academy of\nSciences. He obtained his Ph.D. degree\nin communication and information\nsystem from the Graduate University\nof the Chinese Academy of Sciences in\n2008. His research interests include\ncomputer graphics, computer vision,\nimage processing, and 3D reconstruction.\nDong-Ming Yan is a professor in\nthe National Laboratory of Pattern\nRecognition, Institute of Automation,\nChinese Academy of Sciences. He\nreceived his Ph.D. degree in computer\nscience from Hong Kong University\nin 2010, and his master and bachelor\ndegrees in computer science and\ntechnology from Tsinghua University in 2005 and 2002,\nrespectively. His research interests include image processing,\ngeometric processing, and visualization.\nXiaopeng Zhang received his Ph.D.\ndegree in computer science from the\nInstitute of Software, Chinese Academic\nof Sciences in 1999, where he is a\nprofessor. He received a National Scien-\ntiﬁc and Technological Progress Prize\n(second class) in 2004 and a Chinese\nAward of Excellent Patents in 2012. His\nmain research interests include image processing, computer\ngraphics, and computer vision.\nOpen Access This article is licensed under a Creative\nCommons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduc-\ntion in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link\nto the Creative Commons licence, and indicate if changes\nwere made.\nThe images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and\nyour intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder.\nTo view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nOther papers from this open access journal are available\nfree of charge from http://www.springer.com/journal/41095.\nTo submit a manuscript, please go to https://www.\neditorialmanager.com/cvmj.\n"
}