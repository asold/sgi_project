{
  "title": "Calibrated Language Models Must Hallucinate",
  "url": "https://openalex.org/W4399521940",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A222220925",
      "name": "Adam Tauman Kalai",
      "affiliations": [
        "OpenAI (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4225510629",
      "name": "Santosh S. Vempala",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367365526",
    "https://openalex.org/W2074154707",
    "https://openalex.org/W4366552817",
    "https://openalex.org/W2023943903",
    "https://openalex.org/W4224903240",
    "https://openalex.org/W4286905034",
    "https://openalex.org/W4404534210",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3199958362",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4377010360",
    "https://openalex.org/W2933254221",
    "https://openalex.org/W3167303745",
    "https://openalex.org/W2043845504",
    "https://openalex.org/W4402684046",
    "https://openalex.org/W4380995299"
  ],
  "abstract": "Recent language models generate false but plausible-sounding text with surprising frequency. Such \"hallucinations\" are an obstacle to the usability of language-based AI systems and can harm people who rely upon their outputs. This work shows that there is an inherent statistical lower-bound on the rate that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer LM architecture or data quality. For \"arbitrary\" facts whose veracity cannot be determined from the training data, we show that hallucinations must occur at a certain rate for language models that satisfy a statistical calibration condition appropriate for generative language models. Specifically, if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a \"Good-Turing\" estimate), even assuming ideal training data without errors. One conclusion is that models pretrained to be sufficiently good predictors (i.e., calibrated) may require post-training to mitigate hallucinations on the type of arbitrary facts that tend to appear once in the training set. However, our analysis also suggests that there is no statistical reason that pretraining will lead to hallucination on facts that tend to appear more than once in the training data (like references to publications such as articles and books, whose hallucinations have been particularly notable and problematic) or on systematic facts (like arithmetic calculations). Therefore, different architectures and learning algorithms may mitigate these latter types of hallucinations.",
  "full_text": null,
  "topic": "Hallucinating",
  "concepts": [
    {
      "name": "Hallucinating",
      "score": 0.82439124584198
    },
    {
      "name": "Computer science",
      "score": 0.7209466695785522
    },
    {
      "name": "Language model",
      "score": 0.6015478372573853
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48197001218795776
    },
    {
      "name": "Machine learning",
      "score": 0.4333963990211487
    },
    {
      "name": "Natural language processing",
      "score": 0.3843016028404236
    }
  ]
}