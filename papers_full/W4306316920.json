{
    "title": "Adaptive Graph Spatial-Temporal Transformer Network for Traffic Forecasting",
    "url": "https://openalex.org/W4306316920",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2896269191",
            "name": "Aosong Feng",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A258781275",
            "name": "Leandros Tassiulas",
            "affiliations": [
                "Yale University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4214612132",
        "https://openalex.org/W2999301586",
        "https://openalex.org/W2903871660",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3094588037",
        "https://openalex.org/W2996847713",
        "https://openalex.org/W1973943669",
        "https://openalex.org/W2069929199",
        "https://openalex.org/W2963076818",
        "https://openalex.org/W2901504064",
        "https://openalex.org/W2997848713"
    ],
    "abstract": "Traffic forecasting can be highly challenging due to complex spatial-temporal correlations and non-linear traffic patterns. Existing works mostly model such spatial-temporal dependencies by considering spatial correlations and temporal correlations separately, or within a sliding temporal window, and fail to model the direct spatial-temporal correlations. Inspired by the recent success of transformers in the graph domain, in this paper, we propose to directly model the cross-spatial-temporal correlations on the adaptive spatial-temporal graph using local multi-head self-attentions. We then propose a novel Adaptive Graph Spatial-Temporal Transformer Network (ASTTN), which stacks multiple spatial-temporal attention layers to apply self-attention on the input graph, followed by linear layers for predictions. Experimental results on public traffic network datasets, METR-LA PEMS-BAY, PeMSD4, and PeMSD7, demonstrate the superior performance of our model.",
    "full_text": null
}