{
    "title": "On Isotropy Calibration of Transformer Models",
    "url": "https://openalex.org/W4285272403",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100688231",
            "name": "Yue Ding",
            "affiliations": [
                "University of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A5058287278",
            "name": "Karolis Martinkus",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A5089481900",
            "name": "Damián Pascual",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A5073027507",
            "name": "Simon Clematide",
            "affiliations": [
                "University of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A5078339613",
            "name": "Roger Wattenhofer",
            "affiliations": [
                "ETH Zurich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3173902720",
        "https://openalex.org/W3125516434",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2963578173",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W3104162037",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2518186251",
        "https://openalex.org/W3107826490",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W3105816068",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2977944219",
        "https://openalex.org/W4289761690",
        "https://openalex.org/W2996657533",
        "https://openalex.org/W3022104542",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3098275893",
        "https://openalex.org/W4288796528",
        "https://openalex.org/W2966610483",
        "https://openalex.org/W3034487470",
        "https://openalex.org/W3175752238",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W4287391163"
    ],
    "abstract": "Different studies of the embedding space of transformer models suggest that the distribution of contextual representations is highly anisotropic - the embeddings are distributed in a narrow cone. Meanwhile, static word representations (e.g., Word2Vec or GloVe) have been shown to benefit from isotropic spaces. Therefore, previous work has developed methods to calibrate the embedding space of transformers in order to ensure isotropy. However, a recent study (Cai et al., 2021) shows that the embedding space of transformers is locally isotropic, which suggests that these models are already capable of exploiting the expressive capacity of their embedding space. In this work, we conduct an empirical evaluation of state-of-the-art methods for isotropy calibration on transformers and find that they do not provide consistent improvements across models and tasks. These results support the thesis that, given the local isotropy, transformers do not benefit from additional isotropy calibration.",
    "full_text": "Proceedings of the Third Workshop on Insights from Negative Results in NLP, pages 1 - 9\nMay 26, 2022 ©2022 Association for Computational Linguistics\nOn Isotropy Calibration of Transformers\nYue Ding∗2, Karolis Martinkus∗1, Damián Pascual∗1,\nSimon Clematide2, Roger Wattenhofer1\n1ETH Zürich 2University of Zürich\nyue.ding@uzh.ch damianp@ethz.ch martinkus@ethz.ch\nsiclemat@cl.uzh.ch wattenhofer@ethz.ch\nAbstract\nDifferent studies of the embedding space of\ntransformer models suggest that the distribu-\ntion of contextual representations is highly\nanisotropic — the embeddings are distributed\nin a narrow cone. Meanwhile, static word rep-\nresentations (e.g., Word2Vec or GloVe) have\nbeen shown to beneﬁt from isotropic spaces.\nTherefore, previous work has developed meth-\nods to calibrate the embedding space of trans-\nformers in order to ensure isotropy. However,\na recent study (Cai et al., 2021) shows that\nthe embedding space of transformers is locally\nisotropic, which suggests that these models\nare already capable of exploiting the expres-\nsive capacity of their embedding space. In\nthis work, we conduct an empirical evaluation\nof state-of-the-art methods for isotropy calibra-\ntion on transformers and ﬁnd that they do not\nprovide consistent improvements across mod-\nels and tasks. These results support the the-\nsis that, given the local isotropy, transformers\ndo not beneﬁt from additional isotropy calibra-\ntion.\n1 Introduction\nThe impressive performance of transformer mod-\nels (Vaswani et al., 2017) across almost all areas of\nNatural Language Processing (NLP) has sparked in-\ndepth investigations of these models. A remarkable\nﬁnding is that the contextual representations com-\nputed by transformers are strongly anistropic (Etha-\nyarajh, 2019), i.e., they are unevenly distributed\nand localized in a narrow cone of the embedding\nspace. This discovery, labeled as the representa-\ntion degeneration problem by Gao et al. (2019) is\nsurprising since it suggests that most of the expres-\nsive capacity of these high-dimensional spaces is\nneglected by transformers.\nFurthermore, previous work on static word repre-\nsentations, e.g., GloVE (Pennington et al., 2014) or\nWord2Vec (Mikolov et al., 2013), established that\n∗First three authors in alphabetic order\nisotropy is a desirable property in non-contextual\nembedding spaces (Mu and Viswanath, 2018). In-\ndeed, Mu and Viswanath (2018) and Liu et al.\n(2019a) showed that post-processing static word\nembeddings in order to increase isotropy improves\ntheir performance in downstream tasks. Based on\nthese results, recent work has developed methods\nto correct the anisotropy of the contextual represen-\ntations generated by transformers (Gao et al., 2019;\nWang et al., 2019b; Li et al., 2020). These isotropy\ncalibration methods have been reported to produce\nsmall gains in performance on some NLP tasks.\nHowever, in a recent study, Cai et al. (2021)\nshow that the space of contextual embeddings of\ntransformers is locally isotropic. By analyzing low\ndimensional sub-spaces the authors identify iso-\nlated clusters and manifolds and argue that isotropy\ndoes exist in these manifolds. In the same line,\nLuo et al. (2021) and Kovaleva et al. (2021) ﬁnd\nthat in BERT (Devlin et al., 2019) almost all of\nthe embeddings present large values in the same\ntwo components of the embedding vector. These\nlarge components distort our understanding of the\nembedding spaces by making all the representa-\ntions have high cosine similarity. In this work,\nwe perform an extensive empirical evaluation of\nisotropy calibration methods across different tasks\nand models to determine if they provide consistent\nimprovements. Our results question the utility of\nisotropy calibration in transformers, implicitly sup-\nporting the argument that transformers do already\nbeneﬁt from local isotropy (Cai et al., 2021).\n2 Related Work\nSince the appearance of the transformer architec-\nture and its multiple variants, of which BERT (De-\nvlin et al., 2019) stands out as the most researched\nmodel, a lot of effort has been devoted to under-\nstanding their inner workings (Rogers et al., 2020).\nUnlike static word embeddings such as GloVE or\nWord2Vec, transformers build contextual embed-\n1\ndings, i.e., dynamic representations that aggregate\ninformation from other context words. These rep-\nresentations have sparked a lot of research interest.\nWu et al. (2020) showed that different transformer\narchitectures produce similar contextual representa-\ntions. Chronis and Erk (2020) studied the similarity\nand relatedness of contextual representations in the\nembedding spaces of BERT, while Brunner et al.\n(2019) studied how identiﬁable the intermediate\nrepresentations of BERT are with respect to the\ninput. Zhao et al. (2020) quantiﬁed the contextual\nknowledge of BERT and Zhao et al. (2021) ana-\nlyzed the embedding spaces of BERT in order to\nquantify the non-linearity of its layers.\nFollowing the discovery of anisotropy in trans-\nformers (Gao et al., 2019; Ethayarajh, 2019), dif-\nferent isotropy calibration methods have been de-\nveloped to correct this phenomenon. Gao et al.\n(2019) and Zhang et al. (2020) introduced reg-\nularization objectives that affect the embedding\ndistances. Zhou et al. (2021) presented a module\ninspired by batch-norm that regularizes the embed-\ndings towards isotropic representations. Wang et al.\n(2019b) proposed to control the singular value de-\ncay of the output layer of transformers and Li et al.\n(2020) used normalizing ﬂows to map transformer\nembeddings to an isotropic space. However, Cai\net al. (2021) show that contextual representations\nare locally isotropic and suggest that this property\nallows transformers to exploit their full expressive\ncapacity, questioning the utility of isotropy calibra-\ntion.\n3 Isotropy Calibration Methods\nThe output distribution of transformers is typically\nparameterized as a softmax function:\nP(Yi = yi|hi) = exp(hT\ni WI(yi))\n∑N\nj=1 exp(hT\ni Wj)\n,\nwhere W ∈RN×d is the output weight matrix,\nd is the embedding dimension, N is the output\nsize, yi is the i-th output, I(yi) is the index of yi\nand h is the contextual embedding produced by\nthe model. Since this constitutes a shared space\nbetween model embeddings h ∈H and output\nembeddings, isotropy at the output distribution can\nbe enforced by calibrating either H or W.\nWe experiment with three prominent methods\nfor isotropy calibration on transformers:\nCosine Regularization. Gao et al. (2019) intro-\nduce a simple regularization term that minimizes\nthe cosine similarity between any two output em-\nbeddings in order to increase the aperture of the\ncone that contains the embeddings. This regular-\nization term is given by:\nRcos = λc\n1\n|V|2\nn∑\ni\nn∑\nj̸=i\nˆwT\ni ˆwj ,\nwhere wi is the embedding of the i-th token in the\nvocabulary V, ˆw = w\n||w||and λc is the regulariza-\ntion constant.\nSpectrum Control. Wang et al. (2019b) increase\nisotropy by mitigating the fast decay of the sin-\ngular value distribution of the output matrix W.\nThey decompose W using Singular Value Decom-\nposition (SVD), such that W = UΣV T, where\nΣ ∈ Rd×d is the diagonal matrix of singular\nvalues. Then, they add a regularization term to\nguide the singular value distribution towards a pre-\nspeciﬁed slow-decaying prior distribution. This\nterm spreads the variance away from the ﬁrst few\ndominating singular values, increasing the isotropy\nof the space. They propose the following two regu-\nlarization terms:\nRpol(Σ) = λp\nd∑\nk=1\n(σk −c1kγ)2 ,\nfor polynomial singular value decay; and\nRexp(Σ) = λe\nd∑\nk=1\n(σk −c1 exp(−c2kγ))2 ,\nfor exponential decay, where λe, λp, c1 and c2\nare regularization constants, σk is the k-th largest\nsingular value and γis a parameter which controls\nthe rate of singular value decay.\nFlow Model. Li et al. (2020) propose a method\nthat leverages normalizing ﬂows to learn an invert-\nible mapping f−1\nφ between the embedding space of\nthe transformer model and an isotropic (Gaussian)\nspace Z. First, an invertible ﬂow model (Kingma\nand Dhariwal, 2018) fφ is trained to generate trans-\nformer embedding vectors h from Gaussian noise\nz:\nz ∼pZ(z), h = fφ(z) .\nThen, the model fφ is inverted to map transformer\nembeddings h to the new (and isotropic) output\nembedding space Z.\n2\nSST-2 MRPC CoLA RTE WNLI STS-B QNLI MNLI QQP\nModel Accuracy F1 Mat. corr. Accuracy Accuracy Pearson corr. Accuracy Match acc. Mismatch acc. Accuracy\nBERT 91.44±0.52 88.80±0.99 53.16±1.82 58.97±1.8253.52±4.88 80.86±2.11 88.78±0.57 81.02±0.17 81.78±0.40 89.31±0.06+Cosreg 90.71±1.00 88.17±0.38 46.94±4.29 56.43±5.16 50.23±4.95 78.23±2.19 89.58±0.19 81.20±0.41 82.04±0.21 89.26±0.10+Spectrum-Pol90.86±1.35 81.22±0 0 49.58 ±3.6256.34±0 NaN 81.24 ±4.45 64.33±27.80 64.76±27.48 87.15±2.23+Spectrum-Exp91.21±0.37 81.22±0 0 50.90 ±3.4556.34±0 NaN 86.42 ±0.42 62.43±24.97 63.12±25.20 89.16±0.45+Flow 91.09±0.54 86.99±0.89 51.19±1.81 54.27±1.46 48.36±5.86 78.88±3.46 86.21±3.38 80.65±0.46 81.15±0.21 89.36±0.10\nRoBERTa 92.97±0.6385.35±8.5253.67±3.3253.19±0.55 54.46±0.81 83.10±2.87 91.00±0.4685.16±0.28 85.19±0.15 89.85±0.13+Cosreg 92.66±0.2389.17±2.2848.99±5.6153.67±1.1653.52±1.41 28.44±44.84 90.89±0.1985.41±0.09 85.64±0.22* 89.87±0.12+Spectrum-Pol88.08±0.99 81.22±0 0 52.71 ±0 57.28±1.62* NaN 83.89 ±2.46 50.63±29.72 51.14±29.29 81.76±12.76+Spectrum-Exp90.71±1.09 81.22±0 0 52.95 ±0.42 56.34±0 NaN 82.25 ±3.14 84.46±0.51 84.77 0.41 80.95±13.89\nDistilBERT 88.23±1.7987.97±1.0244.11±2.09 56.68±0.62 51.17±5.69 23.63±41.08 87.53±0.13 78.84±0.27 79.50±0.32 88.28±0.25+Cosreg 88.53±1.55 87.88±1.3643.13±0.85 58.24±1.7852.11±2.44 -0.50±2.08 87.15±0.84 78.69±0.17 79.42±0.28 88.38±0.05+Spectrum-Pol88.80±0.37 81.22±0 0 54.15 ±2.5055.87±0.81 NaN 85.47 ±0.96 78.39±0.17 79.13±0.05 88.41±0.43+Spectrum-Exp88.92±0.6781.22±0 0 54.27 ±2.7155.87±0.81 NaN 86.25 ±0.80 78.38±1.34 79.03±0.34 88.12±0.58\nTable 1: Performance for different models and calibration methods on GLUE; * denotes signiﬁcantly better perfor-\nmance than the corresponding uncalibrated model (p< 0.05, two-sample t-test). The NaN and 0 scores are caused\nby the model always predicting the same class.\n4 Experiments\nWe evaluate the impact of each of these calibration\nmethods on state-of-the-art transformer models in\nthree prominent areas of Natural Language Pro-\ncessing: language understanding, machine trans-\nlation, and summarization. For all of the models,\nwe use the implementation and ﬁne-tuning param-\neters from HuggingFace (Wolf et al., 2020) (cf.\nAppendix B). We run each experiment three times\nand report the mean and standard deviation. Fine-\ntuning time is reported on a Nvidia Titan RTX\nGPU.\nTo characterize the isotropy of the output embed-\nding space we adopt the I1 and I2 isotropy mea-\nsures from (Wang et al., 2019b), with I1(W) ∈\n[0,1] and I2(W) ≥0. Larger I1(W) and smaller\nI2(W) indicate more isotropic embeddings (cf.\nApp. A for details).\n4.1 Language Understanding\nWe consider three representative transformer mod-\nels with different sizes, BERT-base (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019b), and Distil-\nBERT (Sanh et al., 2020). We evaluate these mod-\nels on the development set of GLUE (Wang et al.,\n2019a), a well-known benchmark for language un-\nderstanding that consists of nine different tasks.\nDue to the high computational cost of ﬂow calibra-\ntion and the large number of tasks, we apply this\nmethod only on BERT to save resources.\nIn Table 1 we report the performance per task\nof the calibrated and uncalibrated models. We ob-\nserve the same pattern for all three models. In\nthe overwhelming majority of cases, the calibrated\nmodels perform comparably to or worse than the\nuncalibrated ones, with calibration improving per-\nformance with statistical signiﬁcance ( p <0.05,\ntwo-sample t-test) only in RoBERTa for WNLI\nwith exponential decay and MNLI mismatched\nwith cosine regularization. More speciﬁcally, co-\nsine regularization and ﬂow calibration (in BERT)\ndo not affect performance much, while spectrum\ncontrol in some cases produces severe performance\ndegradation or even prevents learning, e.g., CoLA\nand STS-B. Furthermore, ﬂow calibration adds a\nlarge training overhead, requiring on average 4.2\ntimes more time per training epoch.\nThese results reveal that no isotropy calibration\nmethod yields consistently better performance than\nthe uncalibrated models in language understanding\ntasks.\n4.2 Machine Translation\nWe test multilingual BART (M-BART) (Liu et al.,\n2020) on English-Romanian and German-English\nWMT16 (Bojar et al., 2016) translation datasets.\nIn Table 2 we report BLUE scores, compute time,\nand the isotropy metrics, for the uncalibrated and\ncalibrated models. To reduce the high compu-\ntational cost of ﬂow calibration, we apply this\nmethod only on a reduced version of 50 000 sam-\nples for both tasks, English-Romanian and German-\nEnglish translation. As a reference, we also provide\nthe scores of the uncalibrated model on the small\ndatasets. We ﬁnd, that while cosine regularization\ndoes not signiﬁcantly affect either BLEU scores or\nisotropy metrics, both variants of spectrum control\nimprove isotropy but produce a performance degra-\ndation of over 3 and 5 BLEU points in the English-\nRomanian and German-English tasks respectively,\nwhile requiring 25% to 50% more computation\n3\nEN-RO DE-EN\nModel BLEU (↑) I1(↑) I2(↓) Time (min) BLEU(↑) I1(↑) I2(↓) Time (min)\nM-BART 26.15±0.08 0.88±0.01 0.60±0 108 ±0 22.81 ±0.35 0.89±0.01 0.60±0 176 ±0\n+Cosreg 26.07±0.10 0.88±0.01 0.60±0 110 ±0 23.03±0.27 0.89±0.01 0.60±0 188 ±1\n+Spectrum-Pol 22.94±0.18 1.00±0 0.02 ±0 176±2 16.27 ±0.06 1.00±0 0.02 ±0 265±0\n+Spectrum-Exp 22.92±0.05 1.00±0 0.02 ±0 170±1 16.24 ±0.12 1.00±0 0.02 ±0 230±18\nM-BART (small dataset)9.09±1.02 0.88±0 0.60±0 9±0 11.61±2.25 0.88±0 0.60 ±0 9±0\n+Flow 8.57±2.52 0.89±0 0.60 ±0 95±0 10.93 ±0.70 0.88±0 0.60 ±0 96±1\nTable 2: Multilingual BART performance, isotropy ( I1 and I2) and ﬁne-tuning time per epoch with different\ncalibration methods for English - Romanian and German - English translation. Due to computational cost, the ﬂow\nmethod was tested only on a smaller version of the EN-RO dataset with 50 000 sentences.\ntime. On the other hand, ﬂow calibration yields\ncomparable BLEU score to the uncalibrated model\nbut requires on average 10.5 times more computa-\ntion per epoch. These results suggest a negative\nand counter-intuitive relation between isotropy and\ndownstream performance: when isotropy increases,\nperformance decreases. We observe a similar trend\nfor language understanding in Appendix C.\nOverall, and in line with the results in the previ-\nous section, isotropy calibration in machine trans-\nlation tends to degrade performance and increase\nthe computational budget.\n4.3 Summarization\nWe evaluate BART (Lewis et al., 2020) on the\nCNN/DM summarization task (Hermann et al.,\n2015); again we use a reduced dataset (20 000 ar-\nticles) for ﬂow calibration. The results in Table 3\nshow that none of the calibrated models performs\nsigniﬁcantly better than their uncalibrated counter-\nparts in terms of ROUGE score (Lin, 2004) (cf.\nAppendix D). Cosine regularization does not af-\nfect performance nor isotropy, while spectrum con-\ntrol improves isotropy (I1 and I2) at the cost of a\nsmall performance drop. The ﬂow model performs\ncomparably to uncalibrated BART but requires 5.5\ntimes more computation. Overall, we ﬁnd no ev-\nidence that isotropy calibration provides gains in\nsummarization.\n5 Discussion\nOur extensive evaluation shows that none of the\nconsidered isotropy calibration methods produce\nconsistent improvements over the uncalibrated\nmodels across tasks, domains and architectures.\nIn fact, we observe a negative relation between\nisotropy calibration and downstream performance.\nThe most aggressive method, i.e., spectrum con-\ntrol, produces the largest improvement in isotropy\nCNN / Daily Mail\nModel R-1 (↑) I1(↑) I2(↓) Time (min)\nBART 38.21±0.05 0.95±0.01 0.25±0 246±8\n+Cosreg 38.21±0.05 0.95±0.01 0.25±0 240±8\n+Spectrum-Pol37.36±0.08 0.99±0 0.04 ±0 245±20\n+Spectrum-Exp37.43±0.08 0.99±0 0.04 ±0 230±18\nBART (small d.)36.56±0.25 0.94±0 0.25 ±0 17±0\n+Flow 36.15±0.30 0.94±0 0.25 ±0 95±2\nTable 3: ROUGE-1 score, isotropy ( I1 and I2), and\nﬁne-tuning time per epoch with different calibration\nmethods on BART for summarization. Due to compu-\ntational cost, the ﬂow calibration method was tested on\na smaller version of the dataset.\nmetrics as well as the most signiﬁcant performance\ndrop. On the other hand, the effect of cosine reg-\nularization and ﬂow calibration is small in both,\nisotropy and performance.\nAccording to Cai et al. (2021), the local isotropy\nof the embedding space of transformers may enable\nthem to exploit their full expressive capacity. Fur-\nthermore, concurrent ﬁndings by Luo et al. (2021)\nand Kovaleva et al. (2021) reveal that certain com-\nponents of the contextual embeddings consistently\npresent very large magnitudes, which distort the\ncosine distances in the embedding space and ques-\ntions their anisotropy. This could explain why ad-\nditional isotropy calibration does not consistently\nimprove the performance of transformers in down-\nstream tasks.\nIn light of our results, we discourage isotropy\ncalibration of transformers as a means of improving\ndownstream performance. However, we believe\nthat further investigation of the embedding space\nof transformers may be beneﬁcial to increase our\nability to interpret these models and improve their\narchitecture.\n4\nReferences\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A latent variable model\napproach to pmi-based word embeddings. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:385–399.\nOndrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck,\nAntonio Jimeno Yepes, Philipp Koehn, Varvara\nLogacheva, Christof Monz, Matteo Negri, Aure-\nlie Neveol, Mariana Neves, Martin Popel, Matt\nPost, Raphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the 2016 conference\non machine translation. In Proceedings of the First\nConference on Machine Translation, pages 131–198,\nBerlin, Germany. Association for Computational\nLinguistics.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Watten-\nhofer. 2019. On identiﬁability in transformers. In\nInternational Conference on Learning Representa-\ntions.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embed-\nding space: Clusters and manifolds. In International\nConference on Learning Representations.\nGabriella Chronis and Katrin Erk. 2020. When is a\nbishop not like a rook? when it’s like a rabbi! multi-\nprototype bert embeddings for estimating semantic\nrelationships. In Proceedings of the 24th Confer-\nence on Computational Natural Language Learning,\npages 227–244.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tieyan Liu. 2019. Representation degenera-\ntion problem in training natural language generation\nmodels. In International Conference on Learning\nRepresentations.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS, pages 1693–1701.\nDurk P Kingma and Prafulla Dhariwal. 2018. Glow:\nGenerative ﬂow with invertible 1x1 convolutions. In\nAdvances in Neural Information Processing Systems,\nvolume 31. Curran Associates, Inc.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,\nand Anna Rumshisky. 2021. BERT busters: Out-\nlier dimensions that disrupt transformers. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021 , pages 3392–3405, Online.\nAssociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nTianlin Liu, Lyle Ungar, and Joao Sedoc. 2019a. Un-\nsupervised post-processing of word vectors via con-\nceptor negation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 33, pages\n6778–6785.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZiyang Luo, Artur Kulmizev, and Xiao-Xi Mao. 2021.\nPositional artefacts propagate through masked lan-\nguage model embeddings. In ACL.\nTomas Mikolov, Kai Chen, Gregory S. Corrado, and\nJeffrey Dean. 2013. Efﬁcient estimation of word rep-\nresentations in vector space. In ICLR.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\ntop: Simple and effective postprocessing for word\nrepresentations. In International Conference on\nLearning Representations.\n5\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, pages 6000–6010.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019a.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019.\nLingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu,\nGuangtao Wang, and Quanquan Gu. 2019b. Improv-\ning neural language generation with spectrum con-\ntrol. In International Conference on Learning Rep-\nresentations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nJohn Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Dur-\nrani, Fahim Dalvi, and James Glass. 2020. Similar-\nity analysis of contextual word representation mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 4638–4655.\nZhong Zhang, Chongming Gao, Cong Xu, Rui Miao,\nQinli Yang, and Junming Shao. 2020. Revisit-\ning representation degeneration problem in language\nmodeling. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning: Findings, pages 518–527.\nMengjie Zhao, Philipp Dufter, Yadollah\nYaghoobzadeh, and Hinrich Schütze. 2020. Quanti-\nfying the contextualization of word representations\nwith semantic class probing. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing: Findings, pages 1219–1234.\nSumu Zhao, Damián Pascual, Gino Brunner, and Roger\nWattenhofer. 2021. Of non-linearity and commuta-\ntivity in bert. In 2021 International Joint Confer-\nence on Neural Networks (IJCNN), pages 1–8.\nWenxuan Zhou, Bill Yuchen Lin, and Xiang Ren. 2021.\nIsobn: Fine-tuning bert with isotropic batch normal-\nization. Proceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence, 35(16):14621–14629.\n6\nA Isotropy Metrics\nTo characterize the isotropy of the output embedding space we adopt the I1 and I2 isotropy measures\nfrom (Wang et al., 2019b).\nI1(W) = minv∈V Z(v)\nmaxv∈V Z(v) ,\nis based on the observation by (Arora et al., 2016), that the partition function Z(v) = ∑n\ni=1 exp(vTwi)\nshould be close to a constant for any unit vector v if the embedding matrix W is isotropic. Here, we\nabuse notation and wi ∈W is the i-th row of the embedding matrix W. Following (Mu and Viswanath,\n2018) we use the set of eigenvectors of WTW as V . The second measure\nI2(W) =\n√∑\nv∈V (Z(v) −¯Z(v))2\n|V|¯Z(v)2 ,\nis the sample standard deviation of the partition function Z(v) normalized by its average ¯Z(v). This way,\nI1(W) ∈[0,1] and I2(W) ≥0. Larger I1(W) and smaller I2(W) indicate more isotropic embeddings.\nB Model Hyperparameter Conﬁguration\nFor all the models used in his work we use the implementation from HuggingFace and follow their\ninstructions for the hyperparameters. In particular, we use the following conﬁgurations:\nBERT and DistilBERT. Learning rate 2e−5 without scheduling, batch size 32, 3 training epochs for all\nGLUE tasks except for MRPC and WNLI, for which we train during 5 epochs.\nRoBERTa. Learning rate of 1e−5 for all GLUE tasks except for SST-2 and STS-B, for which the\nlearning rate is set to 1e−5, same number of epochs as for BERT and DistilBERT, batch size of32.\nM-BART and BART. Learning rate of 3e−5 with polynomial decay, batch size 48, and 5 training\nepochs.\nC Isotropy Scores on GLUE\nHere, in Table 4, we present the isotropy scores obtained in our evaluation of GLUE with BERT, RoBERTa,\nand DistilBERT, which were not included in the main text due to lack of space.\nThe isotropy metrics I1 and I2 show the opposite trend to the performance metrics. An improvement\nin isotropy reﬂects a decrease in downstream performance. This way, we see that across models and\ntasks, cosine regularization and ﬂow calibration (for BERT) have a small impact on isotropy and that the\nperformance of the models calibrated with these techniques is close to the that of the uncalibrated models.\nOn the other hand, spectrum control produces a very signiﬁcant increase in isotropy, with many tasks\nreaching a I1 of 1.00; while in Table 1 we see how it produces strong performance degradation. This,\nfurther suggests a negative relation between isotropy and the downstream performance of transformers.\n7\nSST-2 MRPC CoLA\nModel I1(↑) I2(↓) I1(↑) I2(↓) I1(↑) I2(↓)\nBERT 0.91 ±0.01 0.4 ±0 0.91 ±0.01 0.38 ±0.01 0.91 ±0.01 0.39 ±0.01\n+Cosreg 0.91±0.2 0.39 ±0.02 0.92 ±0.01 0.39 ±0.2 0.91 ±0.01 0.39 ±0.01\n+Spectrum-Pol 1.00±0 0.007 ±0.003 1.00 ±0 7e−4 ±3e−4 1.00±0 6 e−4 ±1e−4\n+Spectrum-Exp 0.99±0.01 0.02 ±0.02 1.00±0 6 e−4 ±2e−4 1.00±0 7e−4 ±3e−4\n+Flow 0.92±0.01 0.40 ±0 0.91 ±0.01 0.40 ±0 0.91 ±0.01 0.39 ±0.01\nRoBERTa 0.91 ±0.01 0.39 ±0.01 0.92 ±0.01 0.39 ±0.01 0.91 ±0.01 0.40 ±0.01\n+Cosreg 0.92±0.01 0.40 ±0.01 0.91 ±0.01 0.39 ±0.01 0.91 ±0.01 0.40 ±0.01\n+Spectrum-Pol 1.00±0 0.008±0.002 1.00±0 5e−4 ±4e−4 1.00±0 5 e−4 ±2e−4\n+Spectrum-Exp 1.00±0 0.005 ±0.004 1.00 ±0 1 e−4 ±2e−4 1.00±0 6e−4 ±4e−4\nDistilBERT 0.91 ±0.01 0.38 ±0.01 0.92 ±0.01 0.39 ±0.01 0.92 ±0.01 0.38 ±0.01\n+Cosreg 0.91±0.01 0.39 ±0.01 0.92 ±0.01 0.38 ±0.01 0.92 ±0.01 0.38 ±0.01\n+Spectrum-Pol 1.00±0.01 0.012±0.016 1.00±0 7 e−4 ±5e−4 1.00±0 11 e−4 ±9e−4\n+Spectrum-Exp 1.00±0.01 0.009 ±0.010 1.00 ±0 7 e−4 ±5e−4 1.00±0 11 e−4 ±9e−4\nRTE WNLI STS-B\nModel I1(↑) I2(↓) I1(↑) I2(↓) I1(↑) I2(↓)\nBERT 0.92 ±0.01 0.39 ±0.02 0.91 ±0.01 0.39 ±0.02 0.95 ±0 0.22 ±0.01\n+Cosreg 0.92±0.01 0.40 ±0.03 0.91 ±0.01 0.40 ±0.01 0.95 ±0.01 0.23 ±0.01\n+Spectrum-Pol 1.00±0 2 e−4 ±1e−4 1.00±0 1 e−4 ±2e−4 1.00±0 0.002±0\n+Spectrum-Exp 1.00±0 3e−4 ±2e−4 1.00±0 2e−4 ±3e−4 1.00±0 13 e−4 ±6e−4\n+Flow 0.92±0.01 0.39 ±0.01 0.92 ±0.01 0.39 ±0.02 0.95 ±0.01 0.23 ±0.01\nRoBERTa 0.91 ±0.01 0.40 ±0.01 0.91 ±0.01 0.39 ±0.01 0.95 ±0.01 0.23 ±0.01\n+Cosreg 0.91±0 0.41 ±0 0.91 ±0.01 0.40 ±0.01 0.95 ±0 0.23 ±0.01\n+Spectrum-Pol 1.00±0 3 e−4 ±2e−4 1.00±0 3 e−4 ±1e−4 1.00±0 7 e−4 ±3e−4\n+Spectrum-Exp 1.00±0 3 e−4 ±2e−4 1.00±0 3 e−4 ±1e−4 1.00±0 15e−4 ±13e−4\nDistilBERT 0.92 ±0.01 0.38 ±0.01 0.92 ±0 0.39 ±0.01 0.95 ±0 0.22 ±0.01\n+Cosreg 0.92±0 0.38 ±0.01 0.92 ±0.01 0.38 ±0.01 0.95 ±0 0.22 ±0.01\n+Spectrum-Pol 1.00±0 2 e−4 ±3e−4 1.00±0 1 e−4 ±2e−4 1.00±0 9 e−4 ±1e−4\n+Spectrum-Exp 1.00±0 2 e−4 ±3e−4 1.00±0 1 e−4 ±2e−4 1.00±0 9 e−4 ±1e−4\nQNLI MNLI QQP\nModel I1(↑) I2(↓) I1(↑) I2(↓) I1(↑) I2(↓)\nBERT 0.92 ±0.01 0.39 ±0.01 0.93 ±0.01 0.32 ±0 0.92 ±0.01 0.39 ±0.01\n+Cosreg 0.92±0.01 0.39 ±0.01 0.93 ±0.01 0.32 ±0.01 0.9 ±0 0.39 ±0.01\n+Spectrum-Pol 0.99±0.01 0.06 ±0.02 0.95 ±0.01 0.21 ±0.04 0.92 ±0.02 0.39 ±0.06\n+Spectrum-Exp 1.00±0 5 e−4 ±1e−4 0.98±0.01 0.08 ±0.03 0.97 ±0.03 0.12 ±0.12\n+Flow 0.92±0.01 0.39 ±0.01 0.93 ±0 0.31 ±0 0.92 ±0.01 0.39 ±0.01\nRoBERTa 0.91 ±0.01 0.40 ±0.01 0.93 ±0.01 0.32 ±0 0.92 ±0.01 0.39 ±0\n+Cosreg 0.92±0.01 0.40 ±0.01 0.93 ±0.01 0.93 ±0.01 0.32 ±0.01 0.39 ±0\n+Spectrum-Pol 1.00±0 0.005 ±0.003 0.96±0.03 0.15 ±0.13 0.99±0.2 0.04 ±0.07\n+Spectrum-Exp 1.0±0.01 0.012±0.015 0.98±0.01 0.10 ±0.04 0.99 ±0.01 0.04 ±0.06\nDistilBERT 0.92 ±0 0.38 ±0.01 0.93 ±0.01 0.32 ±0 0.92 ±0.1 0.38 ±0.01\n+Cosreg 0.92±0.01 0.39 ±0.01 0.93 ±0 0.32 ±0 0.992 ±0.01 0.39 ±0.01\n+Spectrum-Pol 0.99±0.01 0.03 ±0.04 0.93 ±0.01 0.29 ±0.01 0.93 ±0.03 0.36 ±0.17\n+Spectrum-Exp 1.00±0.01 0.02 ±0.03 0.97 ±0.1 0.13 ±0.01 0.95 ±0.01 0.25 ±0.01\nTable 4: Isotropy of the embedding space of the different transformer model and calibration method combinations\non GLUE tasks.\n8\nD Complete Summarization Results\nHere we report the complete summarization results, including the ROUGE-2 and ROUGE-L metrics,\nomitted in the main text.\nCNN / Daily Mail\nModel R-1 (↑) R-2 (↑) R-L (↑) I2(↑) I2(↓) Time (min)\nBART 38.21 ±0.05 17.62 ±0.03 27.06 ±0.08 0.95 ±0.01 0.25 ±0 246 ±8\n+Cosreg 38.21 ±0.05 17.62 ±0.03 27.06 ±0.08 0.95 ±0.01 0.25 ±0 240 ±8\n+Spectrum-Pol 37.36 ±0.08 16.60 ±0.08 25.26 ±0.09 0.99 ±0 0.04 ±0 245 ±20\n+Spectrum-Exp 37.43 ±0.08 16.62 ±0.01 26.30 ±0.05 0.99 ±0 0.04 ±0 230 ±18\nBART (small dataset) 36.56 ±0.25 15.62 ±0.07 25.05 ±0.07 0.94 ±0 0.25 ±0 17 ±0\n+Flow 36.15 ±0.30 15.40 ±0.23 24.79 ±0.19 0.94 ±0 0.25 ±0 95 ±2\nTable 5: Complete BART summariation performance, embedding space isotropy and ﬁne-tuning time per epoch\nusing different calibration methods on the CNN / DailyMail dataset. Due to computational cost, the ﬂow calibration\nmethod was tested on a smaller version of the dataset with 20 000 articles.\nThe performance in terms of ROUGE-2 and ROUGE-L scores follows the same patterns as ROUGE-\n1. Similar to language understanding and machine translation, increasing isotropy does not improve\nperformance.\n9"
}