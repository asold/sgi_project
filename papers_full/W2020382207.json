{
  "title": "A Neural Syntactic Language Model",
  "url": "https://openalex.org/W2020382207",
  "year": 2005,
  "authors": [
    {
      "id": "https://openalex.org/A2138905157",
      "name": "Ahmad Emami",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2019534230",
      "name": "FREDERICK JELINEK",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2138905157",
      "name": "Ahmad Emami",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2019534230",
      "name": "FREDERICK JELINEK",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W180232814",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W183625566",
    "https://openalex.org/W2155693943",
    "https://openalex.org/W6679957877",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W2121651659",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2110882317",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W2087946919",
    "https://openalex.org/W48812538",
    "https://openalex.org/W6685180779",
    "https://openalex.org/W2118373646",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W4246367117",
    "https://openalex.org/W3037515705",
    "https://openalex.org/W2143431851",
    "https://openalex.org/W145476170",
    "https://openalex.org/W2145777524",
    "https://openalex.org/W7071444332",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W6601713930",
    "https://openalex.org/W6640281019",
    "https://openalex.org/W2118170533",
    "https://openalex.org/W1988425770",
    "https://openalex.org/W4238619744",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W2024490156",
    "https://openalex.org/W1530801890",
    "https://openalex.org/W3140710042",
    "https://openalex.org/W1785663456",
    "https://openalex.org/W2167980204",
    "https://openalex.org/W4231741839",
    "https://openalex.org/W2101081776",
    "https://openalex.org/W4233630311",
    "https://openalex.org/W2999905431",
    "https://openalex.org/W2097009961",
    "https://openalex.org/W2056590938",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W1808032177",
    "https://openalex.org/W644826662",
    "https://openalex.org/W2565808444",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W43096115",
    "https://openalex.org/W1508567213",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W1810157568",
    "https://openalex.org/W2135258175",
    "https://openalex.org/W1831314980",
    "https://openalex.org/W2015093644",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W3121926921",
    "https://openalex.org/W3129711340",
    "https://openalex.org/W2171645483"
  ],
  "abstract": null,
  "full_text": "Machine Learning, 60, 195–227, 2005\n2005 Springer Science + Business Media, Inc. Manufactured in The Netherlands.\nA Neural Syntactic Language Model∗\nAHMAD EMAMI emami@jhu.edu\nFREDERICK JELINEK jelinek@jhu.edu\nCenter for Language and Speech Processing, The Johns Hopkins University, Baltimore, MD\nEditors: Dan Roth and Pascale Fung\nAbstract. This paper presents a study of using neural probabilistic models in a syntactic based language model.\nThe neural probabilistic model makes use of a distributed representation of the items in the conditioning history,\nand is powerful in capturing long dependencies. Employing neural network based models in the syntactic based\nlanguage model enables it to use efﬁciently the large amount of information available in a syntactic parse in\nestimating the next word in a string. Several scenarios of integrating neural networks in the syntactic based\nlanguage model are presented, accompanied by the derivation of the training procedures involved. Experiments\non the UPenn Treebankand the Wall Street Journalcorpus show signiﬁcant improvements in perplexity and\nword error rate over the baseline SLM. Furthermore, comparisons with the standard and neural net basedN-gram\nmodels with arbitrarily long contexts show that the syntactic information is in fact very helpful in estimating the\nword string probability. Overall, our neural syntactic based model achieves the best published results in perplexity\nand WER for the given data sets.\nKeywords: statistical language models, neural networks, speech recognition, parsing\n1. Introduction\nStatistical language models are widely used in ﬁelds dealing with speech or natural lan-\nguage, speech recognition, machine translation, and information retrieval to name a few.\nFor example, in the accepted statistical formulation of the speech recognition problem\n(Jelinek, 1998), the recognizer seeks to ﬁnd the word string:\nˆW = arg max\nW\nP(A|W ) P(W )( 1 )\nwhere A denotes the observed speech signal,P(A|W) is the probability of producingA when\nW is spoken, and P(W)i st h eprior probability W was spoken.\nThe role of a statistical language model is to assign a probabilityP(W) to any given word\nstring W = w1w2 ...w n. This is usually done in a left-to-right manner by factoring the\nprobability:\nP(W ) = P(w1w2 ...w n) = P(w1)\nn∏\ni=2\nP\n(\nwi |W i−1\n1\n)\n∗This work was supported by the National Science Foundation under grant No. IIS-0085940.\n196 A. EMAMI AND F. JELINEK\nwhere the sequence of words w1w2 ...w j is denoted by W j\n1 . Ideally the language model\nwould use the entire history W i−1\n1 to make its prediction for word wi. However, data\nsparseness is a crippling problem with language models; hence all practical models employ\nsome sort of equivalence classiﬁcation of histories W i−1\n1 :\nP(W ) ≈ P(w1)\nn∏\ni=2\nP\n(\nwi |/Phi1\n(\nW i−1\n1\n))\n(2)\nwhere /Phi1(W i−1\n1 ) denotes the class of the word string ( w1 ...w i−1). Research in language\nmodeling is concerned with ﬁnding efﬁcient and yet powerful classiﬁcation schemes.\nThe most widely used language models are the so called N-gram models, where a word\nstring W i−1\n1 is classiﬁed into word string W i−1\ni−N+1. N-grams models perform surprisingly\nwell given their simple structure, but lack the ability to use longer histories for word\nprediction (locality problem), and they still suffer from severe data sparseness problems.\nThe Structured Language Model (SLM) aims at overcoming the locality problem by\nconstructing syntactic parses of a word string and using the information from these partial\nparses to predict the next word (Chelba & Jelinek, 2000). In this way the SLM also\naddresses one other shortcoming of the N-gram model (the use of surface lexical words\nonly) by using information from the deeper syntactic structures of the word strings. The\nStructured Language Model has shown improvement over N-gram models in perplexity as\nwell as in reducing speech recognizer’s word error rate (Chelba & Jelinek, 2000).\nAnother approach in tackling data sparseness is the use of a distributed representation for\nwords. It has been shown that this approach, while using a neural network as the probability\nfunction estimator, leads to signiﬁcantly improved results (Bengio et al., 2003). The main\nadvantage of the neural network based model is that unlike the N-gram model, it shows\ngreat capability of using long and enriched probabilistic dependencies.\nIn this paper we investigate neural network models for the Structured Language Model.\nIn a given syntactic parse of a sentence, there is a large amount of information that one\nwould like to use in estimating the probability for the given word string. A neural network\nmodel serves as a good probabilistic model for the SLM because of its capability of using\nlong and enriched dependencies. We will present several scenarios of integrating the neural\nnetwork models in the SLM framework, and derive their corresponding training algorithms.\nExperiments show that our models achieve a signiﬁcant reduction in both perplexity and\nword error rate (WER) over the baseline models.\nThe paper is organized as follows; Section 2 gives an introduction to the neural proba-\nbilistic model. In Section 3 we discuss the Structured Language Model and in Section 4 we\npresent the different scenarios of integrating the neural net model into the SLM. Finally,\nexperimental results are presented in Section 5.\n1.1. Relation to previous work\nMost of the research in language modeling is focused on studying ways of using information\nfrom a longer context span than what is usually captured byN-gram language models. The\nA NEURAL SYNTACTIC LANGUAGE MODEL 197\nidea of using syntactical structure of a sentence in estimating its probability has been\ninvestigated in detail (Chelba & Jelinek, 2000; Charniak, 2001; Roark, 2001; Van Uystel,\nVan Compernolle, & Wambacq, 2001). These models overcome the limitations of the N-\ngram models by using longer contexts as well as the syntactical structure of the word string.\nFurthermore, efforts have been made to use better smoothing techniques as well as better\nprobabilistic dependencies for the Structured Language Model (Kim, Khudanpur, & Wu\n2001; Chelba & Xu, 2001; Xu, Chelba, & Jelinek, 2002).\nThe idea of learning a distributed representation for symbolic data and concepts has been\npresent in the neural network community for a long time (Hinton, 1986). In fact, distributed\nrepresentations and neural networks were used in Elman (1991) for ﬁnding grammatical\nstructure; however, the results were limited to artiﬁcial and over-simpliﬁed problems.\nThe idea of using neural networks for natural language processing has been also studied\nearlier (e.g. Miikkulainen & Dyer, 1991). Some have argued that the neural networks\nare not computationally adequate to learn the complexities of natural language (Fodor &\nPylyshyn, 1988). In Xu and Rudnicky (2000), neural networks were used for the speciﬁc task\nof language modeling, however the networks didn’t contain any hidden units and the input\nwas limited to one word only; therefore the capability of the model in using more complex\nstructures and longer contexts was not examined. Recent work has successfully used the\nneural network models in large-scale language modeling problems (Bengio, Ducharme, &\nVincent, 2001; Bengio et al., 2003).\nNeural networks have also been used in learning the grammatical structure in natural\nlanguage. In Lawrence, Giles, and Fong (1996), neural net models were trained and used\nto determine the grammatical correctness of a given sentence. Similarly, a connectionist\nparser was designed and investigated by Ho and Chan (1999). An improved recurrent\nneural network has been used by Henderson (2000, 2003) to model a left-corner parser and\nhas achieved state-of-the-art results. Similar to our model, the improved recurrent network\nemploys a prediction step and uses an energy function (softmax) for probability estimation.\nMoreover, the recurrent network makes it possible to use an unbounded parse history by\nlearning a ﬁnite representation of the past input.\nIt should be mentioned here that distributed and vector-space representations have also\nbeen used outside the connectionist formalism domain. For example in information retrieval\narea, feature vectors are learned for words and documents and the distance between these\nvectors is used as the basis for query search (Deerwester et al. , 1990). The same idea has\nbeen successfully applied to the statistical language modeling task, showing improvement\nover N-gram models (Bellegarda, 1997).\nThe function of the neural network we use in this paper is analogous in form to a\nMaximum-Entropy model (Berger, Pietra, & Pietra, 1996) (see Eq. (6)). However, in the\nMaximum-Entropy approach the features are chosen by the designer and are kept unchanged\nduring training. A statistical parser based on maximum-entropy models has been developed\nby Ratnaparkhi (1997) and has shown state-of-the-art performance in parsing accuracy.\nThis paper brings together and extends the results from previous work regarding the use of\nneural network models for the Structured Language Model (Emami, Xu, & Jelinek, 2003;\nXu, Emami, & Jelinek, 2003; Emami, 2003; Emami & Jelinek, 2004). The relationship\nbetween different previously used approaches are explained in detail and the advantages\n198 A. EMAMI AND F. JELINEK\nand disadvantages, as well as the relative performance of each training method is discussed\nthoroughly.\n2. Neural probabilistic model\nThe fundamental problem of language modeling, as well as of any other task involving\ndiscrete random variables with large sets of allowed values, is the curse of dimensionality.\nFor example, in the case of a language model with a vocabulary of size|V |, the number of\nparameters of an N-gram model is |V |N−1(|V |− 1); which amounts to an order of 1023 free\nparameters for typical values of |V |= 50,000 and N = 5. Since there is never enough\ndata to train models of this size, it is necessary to use some equivalence classiﬁcation of\nthe word strings preceding the word to be predicted (Eq. (2)).\nIn dealing with the curse of dimensionality for discrete random variables we must\nrecognize that the probability function has to account for every possible combination of\nthe random variables involved. There is no inherent property to help with the probability\nestimation of unseen events based on the occurrence of observed “similar” samples. Hence,\nfor a discrete probability model a large number of free parameters need to be estimated. In\ncontrast, estimation is easier in the case of a continuous probability density function, due\nto the smoothness in the conditioning variables. Because of this, the continuous probability\nmodels have in general much fewer parameters than their discrete counterparts.\nThe main idea behind distributed representationis to map random variables from the\noriginal high-dimensional discrete space into a low-dimensional continuous one. As a\nresult, the estimation problem would not be anymore as severely crippled by the curse of\ndimensionality associated with high-dimensional discrete random variables. Consequently,\nif the assumption can be made that the function (probability) to be estimated has local\nsmoothness properties, any standard learning technique (such as a multi-layered neural\nnetwork) can be used to estimate the function.\nThe model achieves generalization because “similar” word strings are assumed to have\nsimilar (close) representations, and because of the smoothness property, small changes in\nthe representations will induce only a small change in the estimated probability. Thus the\npresence of any particular event in the training data will increase the probability of not only\nthat event, but also of a combinatorial number of “similar” (neighboring) events.\nClearly the choice of the mapping discussed above is very important since the perfor-\nmance of the function estimation is limited by the mapping used at its input. Furthermore the\ndistributed representations should fulﬁll the smoothness assumption which is the underlying\nconcept of the approach.\nThe concept of distributed representation of words, using a neural network as the function\nestimator, has been successfully used to implement a large scale language model (Bengio,\nDucharme, & Vincent, 2001). Their approach usessimultaneous learning of the distributed\nrepresentations (feature vectors) and the neural network parameters. The word represen-\ntations are learned just on the basis of how they can help with the estimation task. In this\npaper we use the approach and the architecture proposed in Bengio, Ducharme, and Vincent\n(2001), with modiﬁcations to suit our particular application.\nA NEURAL SYNTACTIC LANGUAGE MODEL 199\nFigure 1. The neural network architecture.\nIn short, our model can be described as follows: a feature vectoris associated with each\nt o k e ni nt h einput vocabulary—i.e., the set of tokens that can be used for prediction. The\nprobability of the next word is then computed by a neural network that takes as its input the\nconcatenation of all input feature vectors. This probability is produced for every possible\nnext word belonging to the output vocabulary. In general, the two vocabularies can be\nseparate and different from each other. The feature vectors and the parameters of the neural\nnetwork are learned simultaneously during training.\n2.1. Model detail\nThe conditional probability function P(y|x\n1, x2,..., xm ) where xi and y are from the input\nand output vocabularies Vi and Vo respectively, is estimated in two parts:\n1. A mapping that associates with each word in the input vocabularyVi a real valued vector\nof ﬁxed dimension.\n2. A conditional probability function which takes as its input the concatenation of the\nfeature vectors of the input items x1, x2,..., xm . The function estimates a probability\ndistribution (a vector) over Vo,t h eith element being the conditional probability of the\nith member of Vo. This probability function is realized by a standard multi-layer neural\nnetwork.\nThe training data consist of a sequence of events x1, x2,..., xm → y that are presented\nto the network one at a time. In all the experiments in this paper we use a standard fully\nconnected multi-layered neural network with one hidden layer.\n2.1.1. Probability computation: Forward pass.The model architecture is given in Figure\n1. The weights preceding the hidden and output layer are denoted by L and S respectively.\nFollowing the output layer is asoftmax function ensuring that the ﬁnal outputs are properly\nnormalized probabilities.\nPresented with an eventx1, x2,..., xm → y the neural network computes the conditional\nprobability P(y|x1, x2,..., xm ). As the ﬁrst step, each of the input variablesxk is mapped to\nits feature vector ⃗f (xk ) using a simple table lookup. The input to the network then consists\n200 A. EMAMI AND F. JELINEK\nof the ordered concatenation of the feature vectors:\nF : Vi → Rd , xk →F ⃗f (xk ) k = 1,2,..., m (3.1)\n⃗f = ( ⃗f (x1), ⃗f (x2),..., ⃗f (xm )) (3.1)\nwhere d is the dimensionality of the feature vectors and F is the actual mapping.\nThe hidden layer takes this input and after a linear transformation, passes it through a\nstandard non-linear sigmoid (tanh) function:\ngk = tanh\n(m·d∑\nj=1\nf j Lkj + B1\nk\n)\nk = 1,2,..., h (4)\nwhere gk is the kth output of the hidden layer, fj is the jth input to the network, Lkj and B1\nk\nare weights and biases of the hidden layer respectively, andh is the number of hidden units.\nThe output of the hidden layer constitutes the input to the output layer, which transforms\nit linearly before passing it through the softmax layer:\nzk =\n∑\nj\ngj Skj + B2\nk k = 1,2,..., |Vo| (5)\npk = ezk\n∑\nj ez j\nk = 1,2,..., |Vo| (5)\nwhere the weights and biases of the output layer are denoted bySkj and B2\nk respectively. The\nsoftmax function (Eq. (6)) ensures that the outputs are properly normalized; and in general\nis well suited for a network trained to learn a probability distribution (Bridle, 1989). In a\nsense, the softmax is a generalization of the logistic function to multiple outputs. It also\nhas very convenient mathematical properties; its gradient is easy to compute and hence is\nreadily integrated into the back-propagation algorithm.\nThe kth output of the neural network, corresponding to the kth element y\nk of the output\nvocabulary, is the model’s estimate of the sought conditional probability, that is pk =\nP(y = yk | x1,..., xm ).\nThe parameters of the model are the feature vectors (table F), weight matrices L and S,\nand biases B1 and B2.\n2.1.2. Training: Backward pass.Training is achieved by searching for parameters/Theta1,t h e\nweights and biases of the neural network and the values of feature vectors, that maximize\nthe penalized log-likelihood of the training corpus:\nL = 1\nn\n∑\nt\nlog P\n(\nyt |xt\n1, ...,xt\nm ; /Theta1\n)\n− R(/Theta1)( 7 )\nA NEURAL SYNTACTIC LANGUAGE MODEL 201\nwhere P(yt |xt\n1,..., xt\nm ) is the probability of the tth word, n is the training data size, and\nR(/Theta1) is a regularization term, which in our case is the L2 norm squared of hidden and\noutput layer weights (excluding biases) times a factor:\nR(/Theta1) = ω ·\n(∑\ni,j\nL2\nij +\n∑\ni,j\nS2\nij\n)\n(8)\nwith ω being the weight decay factor. Regularization is used to penalize solutions with very\nlarge parameters (weights).\nStochastic gradient descent is used to train the model; the training is carried out sequen-\ntially with the parameters being updated after presentation of each event to the network.\nSequential (as opposed to batch) training is specially helpful in the case of language models\nwhere the data is redundant, i.e. the data set contains multiple copies of the same event. The\nsequential training is better able to take advantage of this redundancy because each of the\nidentical events is presented, and the model’s parameters are updated, one at a time. The\nalgorithm is made stochastic by randomizing (in batches) the order of the events at the start\nof each iteration. This makes the search in weight space stochastic, making the algorithm\nless likely to be trapped in a local minimum.\nFor each event ( x\n1,..., xm → y), each parameter θ is increased by a factor of the\ngradient of the objective function L to that parameter:\nθ ← θ + η∂(log P(y|x1, ...,xm ) − ω · θ2)\n∂θ (9)\nwhere η is the learning rate. The weight decay is used only if θ is a hidden or output layer\nweight.\nUsing standard back-propagation (LeCun, 1985; Rumelhart, Hinton, & Williams, 1986;\nWerbos, 1974) it is straightforward to compute the gradient for every parameter of the\nmodel. Starting at the output of the network ⃗y (where gradient computation is trivial), the\ngradients in each layer can easily be computed using the gradients in the succeeding layer.\nConsequently all the parameter updates in the network are found by recursively computing\nthe gradients backwards, starting from the output and going through all the layers, ﬁnishing\nat the feature vectors. Note that for any given event ( x\n1,..., xm → y), only the feature\nvectors of the involved variables (x1,..., xm ) are updated.\n2.2. Model complexity\nThe parameters of the neural network model consist of the feature vectors plus the hidden\nand output layer weights and biases. During training the time taken by each event is the\ntime spent doing a full forward pass followed by a full backward phase (back-propagation).\nDuring evaluation though, the time needed for each event is the same as a forward phase\npass only.\n2.2.1. Number of parameters.The parameters can be broken into three parts:\n202 A. EMAMI AND F. JELINEK\n• feature vectors|Vi | vectors, each of dimension d, for a total of (d ·| Vi |) parameters.\n• hidden layer (m · d × h) matrix and h biases for a total of (( m · d + 1) · h)\nparameters.\n• output layer (h ×| Vo|) matrix and |Vo| biases for a total number of parameters of\n((h + 1) ·| Vo|).\nSo the total number of parameters of the network is:\n(d ·| Vi |+ (m · d + 1) · h + (h + 1) ·| Vo|) (10)\nWe should note here that in a general setting, it is possible for each input variable to\nhave its own separate vocabulary and moreover, its own separate feature vectors (mapping)\nfor the common items in the vocabularies. However, unless there is a large amount of\ntraining data available, it is better to tie the feature vectors by using a single mapping for\nall the input variables. Not doing so would result in a considerable increase in the number\nof free parameters of the model. Note that even when features are tied, the neural net is\nstill able to distinguish among different input variables by tuning the hidden layer weights\naccordingly.\n2.2.2. Time complexity. Evaluation Presenting the network with an event the forward pass\nis broken into three parts:\n• input layer.A simple table look-up of m features, each a d-dimensional vector; (m · d)\noperations\n• hidden layer.Matrix-vector multiplication plus addition of the biases and passing through\nthe non-linear function (Eq. (4)) for a total of (h · m · d + 2h) operations\n• output layer.Matrix-vector multiplication plus addition of the biases (Eq. (5)) and pass-\ning through the softmax layer – 2 |V\no|ops (Eq. (6))—for a total of ( |Vo|· h + 3|Vo|)\noperations\nSo the total number of operations for evaluation of each event is:\n(\nd · m · (h + 1) + 2h + (h + 3)|Vo|\n)\n(11)\nTraining. During training, both a full forward and a backward pass is needed for each\nevent. The forward phase complexity was derived above. The back-propagation is again\nsegmented into three parts:\n• output layer.Matrix-vector multiplication – (h ·| V\no|) ops, plus weight and bias update –\n(h ·| Vo|+| Vo|) ops\n• hidden layer. Pointwise vector-vector multiplication – (3 h) ops, plus weight and bias\nupdate – (m · d · h) ops\n• feature vectors.Matrix-vector multiplication – (h ·m ·d) ops, plus feature update – (m ·d)\nops\nA NEURAL SYNTACTIC LANGUAGE MODEL 203\nwhich gives us the total number of training operations per event (including the forward\npass):\n(5h + dm (3h + 2) + (3h + 4)|Vo|) (12)\nIt should be noted that the model size increases only linearly with vocabulary size and\ncontext length. Compare that withpolynomial and exponential increase in standard N-gram\nmodel size with vocabulary size and context length respectively, and a great advantage of the\nneural network model becomes clear: it can handle longer contexts and richer vocabularies\nwithout the need to estimate an increasingly large number of extra free parameters.\nIt should be also noted that the input vocabulary has no ‘direct’ effect on the model’s\ntime complexity. So the context (predicting) vocabulary can be freely extended without\naffecting either the training or evaluation time. However, the training data time might\nincrease indirectly because a bigger training data, as well as a larger number of hidden units\nmay be required for the model to learn the enriched dependencies.\nFor typical values of the variables involved, the model size is dominated by the values of\nV\ni and Vo while the time complexity (for both training and evaluation) is dominated by only\nthe terms involving Vo. For this reason the effect of context length in typical applications\nis negligible on both the model size and complexity.\n2.3. Implementation\nThe neural network model is computationally rather expensive, especially compared to\nstandard N-gram models, mainly because of high dimensionality of the output layer which\nin turn is due to the required normalization (partition function). For this reason it was\nnecessary to have the model trained and/or evaluated in parallel on multiple CPUs. There\nare alternate ways to make the model parallel; we chose to do so by splitting the data among\nthe CPUs evenly. All the CPUs have access to and update the same parameters, and this\nrequires us either to have each CPU broadcast its parameter updates to all other CPUs, or\nalternatively, to use a shared memory structure. The former is impractical due to the high\nvolume of inter-CPU communication involved.\nThe algorithm was implemented on an IBM RS/6000 SP system using Message Passing\nInterface (MPI) library for parallel implementation—see (Gropp, Lusk, & Skjellum, 1999)\nfor an introduction to MPI. Each node consisted of 16 CPUs and we restricted each\njob to run only on one node, thus avoiding the slower inter-node communications. Our\nimplementation didn’t employ any synchronization across CPUs (except at the beginning\nof each iteration); each CPU can read and write freely to the parameters independent of\nother CPUs. This entails the risk of accessing a parameter during a forward pass by a CPU\nbefore another CPU is done updating it in its backward phase. Alternatively, a parameter\ncan get over-written before it is ever used. However, we expected these effects to be minimal\nbecause the parameter update for a single event is very small and consequently it should\nnot be a matter of concern as long as not too many parameter updates are overwritten.\nFurthermore, the random updates of this particular parallel implementation add another\n204 A. EMAMI AND F. JELINEK\nlevel of randomness to the stochastic gradient descent algorithm. Even though we never\ntried to analyze or record the risks involved with our implementation, the results showed\nthat there is not much to worry about. The same shared memory implementation was used\nindependently by Bengio et al. (2003). They also used another parallel implementation\nwhich worked by splitting the algorithm across the parameters. This implementation has\nthe advantage of not requiring shared memory and hence can be used on any CPU cluster.\nFinally, most of the required computations involve matrix and vector operation. We took\nadvantage of the IBM’s Engineering and Scientiﬁc Subroutine Library (ESSL), which is\nhighly optimized for the particular machine architecture we used. We used the subroutines\nonly for the matrix-vector multiplications in the backward phase—where a matrix has to\nbe transposed before the operation is carried out—and it gave us a total of 4 fold speedup\nin training time. Using the ESSL subroutines for the other matrix operations didn’t lead to\nany noticeable gain. One could also use the optimized BLAS library (Lawson et al., 1979),\nespecially for machine architectures where proprietary subroutines are not available.\n2.4. Vocabulary limitation\nThe training of the neural network model is a time consuming process. Therefore it would be\nvery useful if the training and evaluation time of the network could be reduced. As pointed\nout in Section 2.2.2, for typical values of vocabulary size and number of features and hidden\nunits, the bottleneck of the algorithm is at the output unit where most of the calculations\nare carried out. So one straightforward solution to make the network work faster is to\nreduce the output vocabulary size. For example in word error rate (WER) experiments the\noutput vocabulary can be limited to a certain number of most frequent words, which would\nbe a fraction of the actual vocabulary (Schwenk & Gauvain, 2002). Both the training and\nevaluation time are reduced proportionally with the reduction in output vocabulary size. For\nthe words outside the output vocabulary the probabilities from a standard N-gram model\ncan be used. Note that in this case the probabilities need not to be properly normalized as\nthat’s not a requirement in WER experiments. We used this approach in most of our WER\nexperiments; the effect on the performance is minimal because the token Out Of V ocabulary\n(OOV) rate with respect to the output vocabulary is small.\n2.5. Preliminary results\nPreliminary experiments were carried out using a neural network as a word-based N-gram\nmodel (i.e. (N-1) previous words used to predict the next word). In this way we could get\nan insight for the neural network model and the soundness of our implementation.\nThe perplexity results where carried out on the UPenn corpus (details in Section 5).\nThe corpus is in fact a treebank, though we use only the words in this experiment. This\nchoice of corpus was made so that we can later compare the word-based results to those\nof the SLM based ones. Our particular partitioning of the corpus contains 929564, 73760,\nand 82430 words in the training, held-out, and test set respectively. Table 1 shows the\nindependent test set perplexities of a word-based neural network language model with\ndifferent context lengths. All the networks have 100 hidden units and use feature vectors\nA NEURAL SYNTACTIC LANGUAGE MODEL 205\nTable 1. UPenn perplexity: Word-based NN.\nModel no-intpl +3g m +5g m\nNN-3 gm 170 132 126\nNN-5 gm 157 125 121\nNN-7 gm 154 123 119\nNN-9 gm 153 122 118\nTable 2. UPenn perplexity: N-gram models.\n3g m 5g m 7g m 9g m\nPPL 148 141 141 141\nof 30 dimensions. The (adaptive) learning rate starts at 0.001 and decreases as the model\napproaches convergence. This same conﬁguration is going to be used all throughout most\nof this paper. For comparison, the perplexity results of standard back-off models are shown\nin Table 2. We used interpolated Kneser-Ney smoothing (Kneser & Ney, 1995; Ney, Essen,\n& Kneser, 1994) which is considered to produce the best results among currently used\nsmoothing techniques; see Chen & Goodman (1999) and Goodman (2001) for a review.\nAs can be observed in Table 2 the test set perplexity for standard word-based models\nsaturates at a context length of 4 (5-gram). This has to do with the fact that the chance of\nencountering the same exact N-gram in both training and test set decreases dramatically as\nN becomes larger; and the standard back-off or interpolated smoothings lack the capability\nto use the statistics of a particular N-gram to estimate the probability of a semantically or\nsyntactically similar word string.\nThe no-intpl column in Table 1 shows the performance of the neural network models by\nthemselves. All the other columns denote linear interpolation of the neural network model\nwith the corresponding standardN-gram models with the single interpolation weight found\non a separate held-out set (Jelinek & Mercer, 1980). The widely used bucketing scheme\n(context based interpolation weights) would probably lead to a slight improvement in the\nresults but it was not used on the assumption that it will not have any substantial effect on\nthe performance of the models relative to each other. Interpolation with N-gram models\nwith N larger than 5 did not make any change in the perplexity, as we might have suspected\nfrom the above observation.\nIt can be seen that the neural network model, when combined with N-gram models,\nimproves the perplexity signiﬁcantly. The best neural net model achieves a 16.3% relative\nimprovement over the best back-off model. Another observation is that the neural net\nperplexity saturates slower than theN-gram model as the context length is increased, which\nindicates that the neural network model can make better use of longer contexts.\nTable 3 shows Word Error Rate (WER) results when the neural net model was used to\nre-score the K-best list output by a speech recognizer. We evaluated our models in the WSJ\nDARPA’93 HUB1 test setup; more details of which are given in Section 5. The original\nK-best list had a WER of 13.7%. The columns+lattice and +l + 5 gm denote interpolation\n206 A. EMAMI AND F. JELINEK\nTable 3. WSJ WER: word-based NN.\nModel no-intp +lattice +5g m + l + 5g m\nNN-5 gm (full) 14.4 13.4 13.3 12.8\nNN-5 gm (4 k) 14.0 13.2 13.3 12.7\nNN-8 gm (4 k) 13.7 13.1 13.1 12.6\nTable 4. WSJ WER: N-gram models.\nModel no-intp + lattice\n3-gram 13.9 13.4\n5-gram 14.0 13.3\nwith the original lattice, and both the lattice and the back-off 5-gram model respectively.\nThe lattice scores are those of a 3-gram model trained on a larger data set (Section 5).\nThe interpolation weights were found by performing a grid search on the test set itself (the\nresults are almost the same if the fair method of ﬁnding the weights on the heldout set\nis used). Two types of output vocabularies were used; full, and limited to the 4,000 most\nfrequent words, denoted in the table by ‘full’ and ‘4 k’, and trained for a maximum of\n20 and 30 iterations respectively. For comparison reasons, the results for standard N-gram\nmodels are given in Table 4. Again, as in the case of perplexity, the neural net model shows\nits capability, reducing the baseline WER signiﬁcantly. In this case the best results are\nobtained when the neural net model is integrated with the N-gram models.\nOne observation is that, the number of iterations aside, the limited output vocabulary did\nnot hurt the performance. Consequently, we will employ the limited vocabulary architecture\nfor all the future WER experiments in this paper.\nIt should be noted that the neural network model does not perform best as a standalone\nmodel; rather the best results are achieved when it is used in conjunction with the standard\nN-gram model. A speculative explanation for this behavior is that what the neural network\nlearns from the text is somewhat different from what a regular N-gram model does. The\nneural net shows considerable capacity in using long contexts, but it might not be able to\ncapture some plain localities. On the other hand, N-gram models are very capable of using\nthese localities by way of simple counting and memorizing. This can explain why the best\nperplexity is attained when the two models are combined.\nIt also seems that the feature vectors obtained after training are only suited for use with the\ncorresponding neural net. We tried clustering the feature vectors using K-means algorithm,\nbut the resulting word classes didn’t have any consistent semantic or syntactic similarity.\nAlternatively (Bengio, Ducharme, & Vincent, 2001) tried initializing the feature vectors\nusing LSA (Deerwester et al., 1990; Bellegarda, 1997) but noticed no improvements in\neither in perplexity or convergence speed.\nA NEURAL SYNTACTIC LANGUAGE MODEL 207\nFigure 2. A word-parse k-preﬁx.\n3. Structured language model\nAn extensive presentation of the SLM can be found in Chelba and Jelinek (2000). Like most\nother language models it predicts the next word in a string of words based on an equivalence\nclassiﬁcation of the word preﬁx (Eq. (2)). In the case of SLM, this classiﬁcation is in fact\na mixture of multiple classiﬁcations /Phi1l (Wk−1),l = 1 ... N weighted by their probabilities\nP(/Phi1l (Wk−1)|Wk−1).\nThe SLM was designed to be used as the language model in the decoder component of a\nspeech recognition system. This constraints the model to proceed from left to right through\nthe word sequence. A two-pass decoding strategy, such as N-best re-scoring would not\nbind the SLM to work in this left to right fashion, enabling it to use the whole sentence for\npredicting its probability. Even though the SLM hasn’t been used in a ﬁrst pass decoding\ncapacity yet—due to the complexity of integrating it in a decoder – the left to right model\nphilosophy of its original design is maintained.\nThe SLM will attempt to build the syntactic structure incrementally while traversing the\nsentence left-to-right. It will assign a probability P(W,T) to every word sequence W and\nparse T, that is every possible POS tag assignment, binary branching parse, non-terminal\nlabel, and headword annotation for every constituent of T.\nLet W be a sentence of n words to which we have prepended the sentence beginning\nmarker ⟨s⟩ and appended the sentence end marker ⟨/s⟩ so that w\n0 =⟨s⟩ and wn+1 =⟨/s⟩.\nLet Wk = w0 ...w k be the word k-preﬁxof the sentence—the words from the beginning\nof the sentence up to the current position k— and let Wk Tk be the word-parse k-preﬁx.\nTo stress this point, a word-parse k-preﬁx contains only those binary subtrees whose span\nis completely included in the word k-preﬁx, excluding w0 =⟨s⟩. Single words along with\ntheir POS tag can be regarded as root-only trees. Figure 2 shows a word-parse k-preﬁx:\nh_0, .., h_{-m} are the exposed heads, each head being a pair (headword, non-terminal\nlabel), or (word, POS tag) in the case of a root-only tree. Determining the exposed heads\nfrom the word-parse k-preﬁx at a given position k in the input sentence is a deterministic\nprocedure.\nA complete parse —Figure 3—is deﬁned as a binary parse of the (⟨s⟩,S B )\n(w1,t1) ... (wn,tn)(⟨/s⟩,SE) – SB/SE is a distinguished POS tag for⟨s⟩⟨/s⟩, respectively—\nwith the restrictions that:\n1. (⟨/s⟩,T O P )is the only allowed head.\n208 A. EMAMI AND F. JELINEK\nFigure 3. Complete parse.\nFigure 4. Finite State Representation of the SLM.\n2. ( w1,t1) ... (wn,tn)(⟨/s⟩,SE) forms a constituent headed by (⟨/s⟩,T O P ’ ); the model\nallows parses where (⟨/s⟩,T O P ’ )is the head of any constituent that dominates ⟨/s⟩ but\nnot ⟨s⟩.\nThe SLM operates by means of three probabilistic components:\n• PREDICTOR predicts the next word wk+1 given the word-parse k-preﬁx Wk Tk and then\npasses control to the TAGGER.\n• TAGGER predicts the POS tag tk+1 of the next word given the word-parse k-preﬁx and\nthe newly predicted word wk+1, and then passes control to the CONSTRUCTOR.\n• CONSTRUCTOR grows the already existing binary branching structure by repeatedly\ngenerating transitions from the following set:(unary, NTlabel), (adjoin-left, NTlabel)\nor (adjoin-right, NTlabel), until it passes control to the PREDICTOR by taking anull\ntransition. NTlabel is the non-terminal label assigned to the newly built constituent and\n{left, right} speciﬁes which child node the new headword is percolated from.\nThe ﬁnite state machine in Figure 4 presents a simpliﬁed operation of the model—it does\nnot illustrate how the model deals with unary transitions.\nIt is easy to see that any given word sequence with a complete parse (see Figure 3)\nand headword annotation is generated by a unique sequence of model actions. This will\nprove very useful in initializing our model parameters from a treebank—see section 3.6.2.\nConversely, a generative model running according to the description above can only generate\na complete parse.\nA NEURAL SYNTACTIC LANGUAGE MODEL 209\n3.1. Probabilistic model\nThe joint probability P(W,T) of a word sequence W and a complete parse T can be broken\ninto:\nP(W, T )\n=\nn+1∏\nk=1\n[P(wk |Wk−1Tk−1) · P(tk |Wk−1Tk−1,wk ) · P(T ′\nk−1|Wk−1Tk−1,wk ,tk )]\nP(T ′\nk−1|Wk−1Tk−1,wk ,tk ) =\nNk∏\ni=1\nP\n(\npk\ni |Wk−1Tk−1,wk ,tk , pk\n1 ... pk\ni−1\n)]\n(13)\nwhere.\n• Wk−1Tk−1 is the word-parse (k − 1)-preﬁx\n• wk is the word predicted by PREDICTOR\n• tk is the tag assigned to wk by the TAGGER\n• T ′\nk−1 is the incremental parse structure attached to Tk−1 in order to generate Tk = Tk−1 ∥\nT ′\nk−1; it is the parse structure built on top ofTk−1 and the newly predicted word wk;t h e∥\nnotation stands for concatenation\n• Nk −1 is the number of operations the CONSTRUCTOR executes at sentence positionk\nbefore passing control to the PREDICTOR (the Nk − th operation at position k is the\nnull transition); Nk is a function of T\n• pk\ni denotes the ith CONSTRUCTOR operation carried out at position k in the word\nstring; the operations performed by the CONSTRUCTOR ensure that all possible binary\nbranching parses, with all possible headword and non-terminal label assignments for the\nw1 ...w k word sequence, can be generated. The pk\n1 ... pk\nNk sequence of CONSTRUC-\nTOR operations at position k grows the word-parse ( k − 1)-preﬁx into a word-parse\nk-preﬁx.\nIn short, the SLM is based on three types of conditional probabilities, P(wk |Wk−1Tk−1),\nP(tk |wk , Wk−1Tk−1) and P(pk\ni |Wk Tk ), each of which needs to be parameterized and esti-\nmated from the training data.\n3.2. Model parameterization\nTo be able to estimate the model components we need to make appropriate equivalence\nclassiﬁcation of the conditioning part for each component. The equivalence classiﬁcation\nshould identify the strong predictors in the context and allow reliable estimation from the\ntreebank. The choice in the SLM relies heavily on exposed heads; the experiments in Chelba\n(1997) show that exposed heads provide good information for the PREDICTOR component\nof the language model; (Collins, 1996) shows that they are useful for high accuracy parsing,\nmaking them also the favorite choice for the CONSTRUCTOR model as well. Experiments\nhave shown that exposed heads are also useful in the TAGGER component model. Since\n210 A. EMAMI AND F. JELINEK\nthe word to be tagged is itself a very strong predictor of the POS tag, the equivalence\nclassiﬁcation of the TAGGER model is limited to include only the non-terminal (NT)\nlabels of the two most recent exposed heads.\nP(wk |Wk−1Tk−1) ≈ P(wk |[Wk−1Tk−1]) = P(wk |h0, h−1) (14)\nP(tk |wk , Wk−1Tk−1) ≈ P(tk |wk ,[Wk−1Tk−1]) = P(tk |wk , h0.tag , h−1.tag ) (15)\nP\n(\npk\ni |Wk Tk\n)\n≈ P\n(\npk\ni |[Wk Tk ]\n)\n= P\n(\npk\ni |h0, h−1\n)\n(16)\nwhere [Wk Tk ] denotes an equivalence classiﬁcation of the word-parse preﬁx Wk Tk suitable\nfor estimating each of the above conditional probabilities.\nIt is worth noting that the standard 3-gram model belongs to the parameter space of\nthe SLM as deﬁned above: if the binary branching structure developed by the parser was\nalways right-branching—the null transition having probability 1 in the CONSTRUCTOR\nmode—and we mapped the POS tag vocabulary to a single type, then the model would\nbecome equivalent to a trigram language model.\n3.3. Pruning strategy\nSince the model uses smoothed models, all possible parse trees, each with all possible\nheadword annotations have non-zero probabilities. Consequently the number of possible\nparses for a given word string of lengthk grows faster than exponentially withk. Therefore\nit is necessary to use some pruning scheme to keep only the most likely parses. The pruning\nstrategy used by the model is a synchronous multi-stack search algorithm.\nEach stack stores partial parses that have been constructed by the same number of\nPREDICTOR and same numberof CONSTRUCTOR operations. The partial parses in the\nstacks are ranked according to their probabilitiesP(Wk , Tk ). The hypotheses (partial parses)\nin each stack are expanded by ﬁrst expanding them with all non- null CONSTRUCTOR\noperations (sending the new hypotheses to their appropriate stacks). Subsequently, each\nhypothesis is extended by a null CONSTRUCTOR operation and sent to the stack with\none morePREDICTOR operation and same numberof CONSTRUCTOR operations. The\npruning is controlled by two parameters:\n• maximum stack depth—the maximum number of hypotheses each stack can contain at\nany given time.\n• log-probability threshold—the difference between the log-probability score of the most\nlikely and least likely hypotheses can not be larger than a speciﬁed threshold.\nFor the experiments in this paper we use a maximum stack depth of 10 and a log-\nprobability threshold of 6.91 (= log(1000)).\n3.4. Language model\nThe left-to-right operation constraint on the SLM requires that the probability of the\nword at position k + 1 be estimated using only the information available from Wk Tk —the\nA NEURAL SYNTACTIC LANGUAGE MODEL 211\npreceding words and the partial parses that span them. This gives us the following word\nlevel probability formulation:\nP(wk+1|Wk ) =\n∑\nTk ∈Sk\nρ(Wk , Tk )P(wk+1|Wk Tk ) (17.1)\nρ(Wk , Tk ) = P(Wk Tk )\n/ ∑\nTk ∈Sk\nP(Wk Tk ), (17.2)\nwhere Sk is the set of partial parses contained in the stacks at stage k (all the stacks with\nexactly k PREDICTOR operations). Note that since partial parse scoresρ(Wk , Tk ) sum to 1,\nthe word level probabilities P(wk+1|Wk ) are properly normalized assuming that the model\nP(wk+1|Wk Tk ) is normalized as well. This makes it possible to report proper word level\nperplexities for the language model.\nOn the other hand, if the left-to-right operation constraint is lifted, the sentence level\nprobability can be computed using:\n˜P(W ) =\nN∑\nk=1\nP\n(\nW, T (k))\n≤ P(W ) (18)\nwhere T (k) is one of the N-best parses for the entire sentence W, found using the pruning\nstrategy described earlier. This probability assignment is clearly deﬁcient (unless no pruning\nis used), but it can be used to re-score the N-best list output of a speech recognizer. Moreover,\nas will be explained later, it is useful to justify the model parameter re-estimation technique\nemployed by the SLM.\n3.5. The SCORER\nAs described above, the SLM is comprised of three components, the PREDICTOR, the\nCONSTRUCTOR, and the TAGGER. However, the probability model P(w\nk |Wk−1Tk−1)\nassociated with the PREDICTOR is used in two capacities in the operation of the SLM.\nOne is in constructing and assigning probabilities to the partial parses (Eq. (13)), and the\nother is in word level probability assignment (Eq. (17.1)). These two models can in general\nbe parameterized and estimated separately of each other; each using a different context\nequivalence classiﬁcation scheme. We shall distinguish between the two, calling the ﬁrst\nmodel the PREDICTOR and the second one the SCORER.\nThis is desirable because many of the partial parses that were initially used to predict\nthe probability of the next word are not going to survive the pruning and therefore won’t\nparticipate in the “ N-best training” stage of the model (described in the next section).\nEstimating a separate SCORER enables us to make up partly for this weakness and train a\nmodel to achieve a higher likelihood on the training data.\nIn many of the experiments in this paper only the SCORER component will be modeled\nby a more complex architecture and the PREDICTOR will remain unchanged from the\n212 A. EMAMI AND F. JELINEK\nbaseline SLM. The main reason is a merely practical one; the SCORER model can be\neasily upgraded, not requiring any changes in the parses used for training.\n3.6. Model estimation\nThe previous sections described the structure of the SLM and the deﬁnition of its com-\nponents. The models need to be estimated from training data which may be in the form\nof a treebank with complete parses provided for each sentence. Each parse is binarized\nand headword percolated; both binarization and headword percolationare rule-based and\ndeterministic procedures (see Chelba & Jelinek, 2000).\nThe SLM model estimation takes place in three stages:\n1. Initialization of the model components’ parameters from the training treebank.\n2. Increasing the training data likelihood as given by the deﬁcient probability estimation in\nEq. (18). This involves the employment of the “N-best training” algorithm (see Section\n3.6.2).\n3. Estimation of the SCORER component so that the likelihood of the training data as given\nby Eq. (17.1) is increased. The SCORER is initialized by copying the PREDICTOR\nestimated in the previous stage.\n3.6.1. First stage: Parameter initialization.In the ﬁrst stage, the complete parses in train-\ning set are directly used, after they have undergone binarization and headword percolation.\nEach parse (W,T) can be identiﬁed by a derivation d(W,T) which is the sequence of steps\nthat the SLM would take to construct that given parse. Each step is either a PREDIC-\nTOR, TAGGER, or CONSTRUCTOR operation and is in general in the form of an event\n(x\n1,..., xm → y) where ( x1,..., xm ) is the context information used in taking the step,\nand y is the actual operation performed by the component. The set of these operations\nobtained from the parsed data makes up our training set from which it is straightforward to\ntrain each of the individual components. The training algorithm would of course depend on\nthe model used for each component. For example, in the case of interpolated or back-off\nmodels, the training would consist of a simple counting of the events. Alternatively, in\nthe case of a neural network, the training would be performed by the back-propagation\nalgorithm, presenting (x\n1,..., xm ) to its input and maximizing the log-probability of y at\nits output.\n3.6.2. Second stage: N-best EM re-estimation.In this stage each of the three component\nmodels are re-estimated with the objective of increasing the likelihood of the training data\ncomputed by the probability given in Eq. (18). The approach is in the form of maximum\nlikelihood estimation from incomplete data, with W being the observed and T—the parse\nstructure along with POS and non-terminal tags and headword annotation for a given W—\nthe hidden data. Therefore this stage of the training procedure makes use of the Expectation\nMaximization (EM) algorithm (Dempster, Laird, & Rubin, 1977).\nThe EM algorithm requires that all the hidden events—parses T in this case—be consid-\nered when computing expected likelihood (EM auxiliary function in the E-step). However,\nA NEURAL SYNTACTIC LANGUAGE MODEL 213\nas mentioned in Section 3.3, this is not feasible due to the large number of parses involved.\nTherefore a variant of the EM algorithm is used which only considers the “N-best” parses\nfound by the search and pruning strategy described in Section 3.3. Intuitively the “N-best”\nEM algorithm tries to maximize anapproximation of the true likelihood and can be thought\nof as a compromise between full forward-backward ( N = all possible parses) and Viterbi\n(N = 1) training of the hidden Markov models.\nEven though it maximizes an approximate objective function, it still can be proved that\nthe “N-best” EM algorithm does converge to a local maximum (see Chelba & Jelinek,\n2000) for discussion and comments). Also, for a presentations of different variants of the\nEM algorithm the reader is referred to Byrne, Gunawardana and Khudanpur (1998).\n3.6.3. Third stage: SCORER estimation. The previous stage of the training tries to\nincrease an approximation of the likelihood of the training data; therefore the trained\nPREDICTOR model is not optimal. We can partly make up for this by estimating a separate\nSCORER maximizing the true word level probabilities as given by Eq. (17.1). In this\nstage of the training, a separate SCORER model is trained using partial parses and their\ncorresponding weights ρ(W\nk , Tk ). The partial parses and their scores are obtained by using\nthe SLM trained in the previous stages. It should be noted that Eq. (17.1) is analogous to\nthe likelihood of a hidden Markov model with ﬁxed transition probabilities (but dependent\non position k) speciﬁed by the values ρ(W\nk , Tk ).\n4. Neural net based SLM\nAs we mentioned in Section 3, all the functionality of the Structured Language Model is\ngoverned by its four components, the PREDICTOR, the TAGGER, the CONSTRUCTOR,\nand the SCORER. By the baseline SLM we denote the model where the components\nare parameterized according to Eqs. (14)–(16), with the SCORER being the same as the\nPREDICTOR, and where each component is modeled by a bucketed linear interpolation\nmodel (Jelinek & Mercer, 1980), characterized by the recursive equations:\nP\nm (y|x1,..., xm ) = λ(x1,..., xm ) ··· Pm−1(u | x1,..., xm−1)\n+(1 − λ(x1,..., xm )) ··· fr (u | x1,..., xm )\nP−1(y) = uniform (Y) (19)\nwhere λ’s are interpolation coefﬁcients and fr and P−1 refer to relative frequency and\nuniform distributions respectively. This is essentially an N-gram type model – with N =\nm + 1 – and thus comes with the shortcomings mentioned in the earlier sections, mainly\nthe inability to use long or rich dependencies. However, there is a considerable amount of\ninformation in a partial parse that one would like to use, and an N-gram type model does\nnot simply prove to be adequate enough for such a purpose.\nPrevious efforts have tried to improve the Structured Language Model in terms of both\nlanguage modeling capability and parsing accuracy by using more conditioning information\nfor the model components. In Chelba and Xu (2001), each headword’s NT tag was\naugmented by one or both of its children NT tags; (Xu, Chelba, & Jelinek, 2002) extended\n214 A. EMAMI AND F. JELINEK\nthis work by adding to the conditioning context of the CONSTRUCTOR model the NT tag\nof the second previous headword ( h−2) as well. These experiments showed that the SLM\nperforms better in all aspects—PPL, WER, and parsing accuracy—when richer probabilistic\ndependencies are used for its components.\nSo it is clearly desirable to use as much conditioning information as possible for the\nSLM component models. However, as mentioned above, the N-gram type models used\nin the baseline model are not well equipped for this task. In fact, in all the enriching\nexperiments mentioned above, severe data sparseness problems were observed. It is then\nnatural to try a more powerful model for the SLM components. The neural network model\nis a perfect candidate because of its excellent capability in handling larger vocabularies\n(enriching) and longer contexts (extending dependencies). As explained in Section 2.2, the\nneural network model complexity increases negligibly with context length and only linearly\nwith vocabulary size in practical settings.\nIdeally, a neural network model would be used for each component of the SLM, using\nas much conditioning information as possible, and trained by going through the stages\noutlined in Section 3.6. However, training and experimenting with the neural network\nmodels (specially when integrated in the SLM) is a very time consuming task, and thus\ninstead of implementing a rigorous integration we started with more limited settings;\nsometimes using the neural network model for only one component, other times skipping\none or two stages of the SLM model estimation procedure (Section 3.6). Our belief was\nthat if the incomplete (and sometimes ad hoc) combination of neural network and the SLM\ncan be shown to be helpful, then the viability of a complete and rigorous integration will\nbe evident.\nOverall, we have used neural networks as SLM component models in three different\nscenarios which we describe below:\n1. Mismatched SCORER training. The main goal in this scenario was to integrate the\nneural net model in the SLM while keeping the task simple. Therefore we decided to\nhave only one of the SLM components modeled by the neural network. The SCORER is\na perfect candidate, largely because “upgrading” it will only affect the language model\nestimation part of the SLM, keeping the parse construction machinery intact. Also, there\nis a lot of potential in “upgrading” the scorer because it has a high perplexity relative to\nother components.\nAgain, for the sake of simplicity, instead of training the model on the partial parses\nas required by Eq. (17.1), we used only the annotated (parsed) data set in training the\nneural net SCORER. In other words, in this scenario only the ﬁrst stage of the SLM\ntraining procedure (Section 3.6) is carried out, training (only) the SCORER as if it\nwere a PREDICTOR. The original PREDICTOR from the baseline model model is kept\nunchanged.\nFigure 5 shows the schematic of mismatched SCORER training. During evaluation\n(solid lines), a neural net SCORER that is trained on PREDICTOR events is used to\nevaluate and combine the test set SCORER events (partial parses) constructed by the\nbaseline SLM. The two streams output by the baseline SLM are the actual partial parses\nand their corresponding weights respectively.\nA NEURAL SYNTACTIC LANGUAGE MODEL 215\nFigure 5. Mismatched Scorer Training.\nClearly, this training procedure is not optimal. The SCORER is trained to maximize\nthe likelihood function involving probabilities as in Eq. (14), but it is used according to\nEq. (17.1). On the other hand, one can see that the two sets are equivalent if there was\nonly one partial parse for every word preﬁx. So the training is sub-optimal in the sense\nthat it is based on the “wrong” (sub-optimal) data set. However, one can assume more\nor less that the two data sets are close enough; that is, the events encountered by the\nPREDICTOR are in general similar to the events that the SCORER uses in estimating\nword level probabilities.\n2. EM training. In contrast to the previous scenario, this setting involves modeling all\nthe SLM components by a neural network. First, the initial stage of the SLM model\nestimation procedure (Section 3.6) is carried out, training the neural network based PRE-\nDICTOR, TAGGER, and CONSTRUCTOR. In the second stage, the newly estimated\nneural network models will be re-trained according to the “ N-best” EM re-estimation\nprocedure. This would require the SLM model estimated in the ﬁrst stage to be used to\nﬁnd the N-best parses for each sentence in the training set. Notice that in this scenario,\nunlike the ﬁrst one, the neural network models are also involved in parse construction\nand pruning.\nIn the E-step of the EM algorithm, the expected likelihood of the training data is calcu-\nlated using the parameters from the previous iteration of the algorithm. This likelihood\nexpectation—EM auxiliary function—is dependent on the current model parameters:\nQ(/Theta1,ˆ/Theta1) =\n∑\nT\nP(T |W ; ˆ/Theta1)l o gP(W, T ; /Theta1) (20)\n216 A. EMAMI AND F. JELINEK\nwhere ˆ/Theta1and /Theta1denote the old and new (to be estimated) parameters. In the M-step of the\nalgorithm, those parameters/Theta1are found that maximize the above expected likelihood:\n/Theta1new = argmax/Theta1Q(/Theta1,ˆ/Theta1) = argmax/Theta1\n∑\nT\nP(T |W ; ˆ/Theta1)l o gP(W, T ; /Theta1) (21)\nwhere P(T |W ; ˆ/Theta1) is the ﬁxed probability (weight) assigned to the complete parse T.\nUsing Eq. (13) we can decompose the joint log-probability log P(W, T ; /Theta1) into parts,\neach involving a single SLM component:\nlogP (W, T ; /Theta1)\n=\nn+1∑\nk=1\nlogP(wk |Wk−1Tk−1) +\nn+1∑\nk=1\nlogP(tk | Wk−1Tk−1,wk )\n+\nn+1∑\nk=1\nNk∑\ni=1\nlog P\n(\npk\ni | Wk−1Tk−1,wk ,tk , pk\n1 ... pk\ni−1\n)\n(22)\nAs can be seen, the joint log-probability is a summation of SLM component log-\nprobabilities. The summations are over the operations (events) carried out to construct\nthe particular complete parse T. Clearly, the gradient of the EM auxiliary function can\nin turn be broken up into component-wise constituents, and hence the contribution of\neach component to the total gradient can be separately computed. This makes the EM\ntraining of the neural networks very straightforward, each component neural network can\nbe trained separate and independently of other components, maximizing its likelihood\nover its own set of operations (events). The N-best EM training for the neural network\nbased SLM is summarized in Algorithm 1. As described in Sections 3.3 and 3.6.2, the\nre-estimation procedure is limited to the N-best parses for each sentence. Considering\nall the possible parses for each sentence is simply intractable (exponential growth with\nsentence length). So the summation in Eqs. (20) and (21) would be over the “ N-best”\nparses rather than all the possible ones. Clearly this is an approximation of the real\nauxiliary function, with the error depending on the value ofN as well as the skewness of\nthe distribution of the partial parses. Note that if N = 1, then this algorithm is basically\nthe same as the ﬁrst stage of the SLM training procedure except that the parses are\nconstructed by the model itself rather than taken from an external source. It should be\nnoted here that the actual weights P(T | W ; ˆ/Theta1) are normalized to ensure they add up to\none.\nFinally, the SCORER is copied from the PREDICTOR and used to calculate the language\nmodel probabilities. So the third stage of the SLM training procedure is skipped in this\nscenario.\nFigure 6 depicts the EM training procedure. The arrows marked byE0 refer to the initial-\nization stage (training neural nets on the gathered counts) whileEn(n > 1) corresponds\nto later stages of the training were the components are re-trained on the N-best parses\nwith fractional counts.\nA NEURAL SYNTACTIC LANGUAGE MODEL 217\nFigure 6. EM Training.\n3 SCORER estimation. This scenario is exactly the same as the ﬁrst except that in this\ncase the SCORER is trained to maximize the correct objective function computed\nusing probabilities in Eq. (17.1). In short, in this scenario the ﬁrst stage of the training is\ncarried out using the baseline model, then the second stage (“N-best”) training is skipped\naltogether, and ﬁnally the training is completed by estimating a neural network based\nSCORER on the partial parses constructed by the baseline model. The gradient descent\nalgorithm is slightly more involved in this case because of the summation over partial\nparses in Eq. (17.1). If we denote the context in the ith event (x\ni\n1,..., xi\nm → yi )b y hi,\n218 A. EMAMI AND F. JELINEK\nand assume that there are k(i) contexts (partial parses) at position i,w eh a v e :\n∂\n∂θ logP(yi ) = ∂\n∂θ\n(\nlog\n(k(i)∑\nk=1\nρk\ni · P\n(\nyi | hk\ni\n)\n))\n= 1\n∑k(i)\nk=1 ρk\ni · P\n(\nyi | hk\ni\n) ∂\n∂θ\n(k(i)∑\nk=1\nρk\ni · P\n(\nyi | hk\ni\n)\n)\n= 1\nP(yi )\nk(i)∑\nk=1\nρk\ni · ∂\n∂θ P\n(\nyi | hk\ni\n)\n(23)\nwhere ρk\ni denotes the score ρ(W k\ni T k\ni )o ft h ekth partial parse at ith position.\nThe gradient descent procedure is accordingly summarized in Algorithm 2. Note that\nthe algorithm is analogous to weighted mini-batch training, with a batch being the set\nof partial parses at a given position.\nThe partial parses are simply all the stack entries at each position (equal number of\nPREDICTOR operations) of the word string. Note that in this scenario, neither of the\nPREDICTOR, TAGGER, CONSTRUCTOR components are involved. Also the same\nset of partial parses (built by the baseline model) are used all throughout training, so\nthere is no need re-run the SLM over the training data at the start of each iteration. All\nsaid, this scenario is the most time consuming one simply because of the sheer large\nnumber of partial parses involved. Even in the “N-best” training case, the number of the\nevents used in training is much smaller because only a few of the partial parses in each\nposition will actually make it to be part of a complete parse.\nThe training is depicted in Figure 7. As can be seen, this training is similar to that of\nFigure 5 with the difference that both training and evaluation go through the same path.\nIn implementing the SCORER training, the inputs to the softmax function ( z\nk’s in\nEq. (6) were normalized by substracting from all of them the maximum input value\nzmax = maxk zk . In this way, precision related problems that might have occurred due to\na very large zk were avoided.\nA NEURAL SYNTACTIC LANGUAGE MODEL 219\nFigure 7. Matched scorer training.\nWe should note here that if the SCORER training was carried out on the partial parses\nconstructed by the model trained in the second scenario (rather than the baseline), then\nwe would have a full integration of the neural network models in the SLM (see next).\n4. Full training.It is possible to model all the components of the SLM by neural nets and\nhave them trained on the appropriate training events. Such a comprehensive training\nwill start with EM training (scenario 2) and will later train a separate SCORER by\ntraining it on the partial parses generated by the just trained SLM (similar to scenario\n3). So basically, a full and comprehensive training of the SLM involving all the four\ncomponents consists of a scenario 2 training followed by scenario 3 training of the\nSCORER. This can be thought of as replacing the ‘baseline SLM’ block in Figure 7 by\nthe fully trained SLM in scenario 2.\n5. Experiments\nThe baseline Structured Language Model uses the model parameterizations formulated in\nEqs. (14)–(16). In this section we experiment with using a neural net model in the different\nscenarios described in the previous section, while extending the conditioning contexts for\nthe neural net component.\nOur experimental setup is as follows: for perplexity results we used the UPenn Treebank\nportion of the WSJ corpus. The UPenn Treebank contains 24 sections of hand-parsed\nsentences. We used sections 00-20 as the training data, and Sections 21 and 22 and 23\nand 24 as the held-out and test sets, respectively. The three sets contained 930 k, 74 k,\nand 82 k words respectively. We used an open vocabulary consisting of 10 k words. Note\nthat no vocabulary limitation (see Section 2.4) were used in perplexity results since the\nprobabilities are required to be normalized. The text was normalized in the following ways:\n220 A. EMAMI AND F. JELINEK\nnumbers in Arabic form are replaced by a single token ‘N’, punctuations are removed, all\nwords are mapped to lowercase, and extra information in the parses (such as traces) are\nignored. There are a total of 40 part-of-speech (POS) and 54 non-terminal (NT) tags.\nThe WER experiments consisted of the re-scoring of the K-best list output by a speech\nrecognizer. We evaluated our models on the WSJ DARPA’93 HUB1 test setup. The same\nsetup was used in Chelba and Jelinek (2000), Roark (2001), Chelba and Xu (2001), and\nEmami, Xu, and Jelinek (2003). The test set is a collection of 213 utterances (10 different\nspeakers) for a total of 3446 words. The 20 k words open vocabulary and baseline 3-gram\nmodel are standard ones provided by NIST and LDC—see Chelba and Jelinek (2000) for\ndetails. The lattice and K-best lists were generated using the standard 3-gram trained on 45\nM words of the WSJ corpus (Paul & Baker, 1992). The baseline SLM was trained on 19\nM words of WSJ text automatically parsed by the parser in Ratnaparkhi (1997). Note that\nthere are memory constraints in using more data for the baseline SLM simply because the\nsize of the N-gram type components grow linearly with the number of training dataN-gram\ntypes. In order to be able to compare the results, the neural net models were also limited to\nthe same 19 M words, even though the neural net model is not constrained by the memory\nlimitations.\nWe used the same neural net conﬁguration for all the experiments, with 100 hidden units\nand 30 dimensional feature vectors. The learning rate and weight decay factor were set to\n10\n−3 and 10−4 respectively, with the learning rate decreasing adaptively as more events are\npresented to the network. Since the inputs to the networks are always a mixture of words\nand NT/POS tags, while the output probabilities are over words in the PREDICTOR, POS\ntags in the TAGGER, and adjoin actions in the CONSTRUCTOR, separate input and output\nvocabularies had to be used. Furthermore, the output vocabulary was limited to the 5 k most\nfrequent words for all the WER experiments (Section 2.4)—the OOV rate with respect to\nthis limited vocabulary was found to be 6.2% on the training data. The parameters of neural\nnets were randomly initialized with a uniform distribution centered at zero. All the networks\nwere trained for a maximum of 30 iterations unless otherwise stated. The held-out set was\nused for early stopping, however we didn’t observe any overﬁtting behavior. We observe\nthat the held-out perplexities do increase a few times during training, but in the end the best\nheld-out perplexity is attained at the last or very close to last iteration.\nIn order to study the behavior of the SLM when longer context is used for conditioning\nthe probabilities, we gradually increased the context of the PREDICTOR/SCORER model.\nFirst, the third exposed previous head was added. Since the syntactic head gets the head\nword from one of the children, either left or right, the child that does not contain the head\nword (hence called opposite child) is never used later on in predicting. This is particularly\nnot appropriate in a prepositional phrase because the preposition is always the head word\nof the phrase in the UPenn Treebank annotation. Therefore, we also added the opposite\nchild of the ﬁrst exposed previous head into the context for predicting.\nTables 5 and 6 show the perplexity and the WER results respectively for the mismatched\nneural net SCORER training (Section 4). For the perplexity results the neural net SCORER\nwas trained on the hand-parsed sentences for 50 iterations. The row SLM denotes the\nbaseline SLM model, while the rows 2 HW, 3 HW, and (3 +1)HW refer to conditioning\ncontexts consisting of 2 previous heads, 3 previous heads, and 3 previous heads plus the ﬁrst\nA NEURAL SYNTACTIC LANGUAGE MODEL 221\nTable 5. Mismatched NN SCORER; UPenn Perplexity.\nModel no-intpl +slm +3g m +5g m\nSLM 161 161 137 132\n2 HW 174 137 127 123\n3 HW 161 132 123 119\n(3 + 1) HW 155 129 121 117\nAll-3 152 128 120 117\n2HW + 2 w 154 129 122 118\nTable 6. Mismatched NN SCORER; WSJ WER.\nModel no-intpl +slm +lattice +5gm +all\nLattice 13.7 12.6 13.7 13.3 12.6\nSLM 12.7 12.7 12.6 12.7 12.6\n2 HW 13.5 12.7 12.7 12.5 12.3\n3 HW 13.7 12.7 12.9 12.7 12.3\n(3 + 1) HW 13.2 12.4 12.8 12.5 12.4\nprevious opposite head respectively. The columns +3 gm and +5 gm denote interpolation\nwith Kneser-Ney smoothed 3-gram and 5-gram models respectively (trained on the same\n19 M words data set as the baseline SLM in WER experiments)—see Tables 2 and 4 for\nperplexity and WER. All the three neural net models are also interpolated with each other\nand the results are given in the row All-3. The interpolation weights were found on the\nheld-out data and in most cases were close to 0.5; ﬁnding the weights on the test set itself\ndoes not improve the performance noticeably. The row 2 HW +2wi nT a b l e5r e f e r st o\nthe case where the input context is increased from the 2 HW case to include the trigram\ninformation as well. In other words, the SCORER uses the 2 previous headwords as well as\nthe 2 immediate previous words in assigning a probability to the next word. In comparing\nthis row to that of 2 HW it can be observed that adding the trigram information to the model\nindeed boosts the performance considerably. Another observation is that interpolating this\nmodel (which already has the 3-gram information) with a regular trigram (+3 gm column)\nstill improves the perplexity. Our explanation for this is that the probability distributions\nlearned by the neural net and the regular trigram are not closely correlated and therefore\nthey have some mutually exclusive information.\nIn WER tables, the columns marked by+slm and+lattice denote linear interpolation with\nthe baseline SLM and the lattice word language model (3-gram trained on the whole WSJ\ncorpus) scores respectively. The +all notation refers to interpolation with baseline SLM,\nlattice, and the 5-gram model, all at the same time. Furthermore, in WER experiments,\nthe interpolation weights are found on the test set itself using grid search. A strictly\nfair experiment should ﬁnd the weights on some independent and unseen set; however\n222 A. EMAMI AND F. JELINEK\nTable 7. N-best EM training; UPenn Perplexity.\nModel, EM itr. no-intpl +slm +3g m +5g m\n2 HW E0 162.5 130.9 123.8 119.3\n2 HW E1 158.2 129.5 123.0 118.6\n(3 + 1) HW E0 151.2 124.4 119.1 115.2\n(3 + 1) HW E1 147.9 123.2 118.5 114.7\n2 HW (full) E0 137.6 121.8 116.9 113.1\n(3 + 1) HW (full) E0 129.0 114.6 111.3 108.4\noptimizing the weights on the test set itself should not be a problem as long as the experiment\nis for comparison purposes and the approach is applied to all the models.\nIt can be seen from the tables that a neural net based SCORER—even though trained as a\nPREDICTOR—leads to signiﬁcant improvement in perplexity and WER over the baseline\nSLM. It should be also noted that extending the conditioning dependencies consistently\nimproves the perplexity.\nIn the second scenario of integrating neural nets into the SLM, all three components—\nPREDICTOR, TAGGER, and CONSTRUCTOR—are modeled by a neural network (Sec-\ntion 4). In the ﬁrst step the components are trained on the treebank parses (iteration zero;\nE0)—similar to the ﬁrst scenario with the difference that the TAGGER and the CON-\nSTRUCTOR are modeled by a neural net as well. Subsequently, the newly trained SLM\nis used to obtain “ N-best” parses for each sentence in the training data, and then the\ncomponents are re-estimated using the “ N-best” EM training algorithm. Because of time\nconstraints we performed the EM re-estimation for only one iteration (E1). We also used\nonly the 10 best parses for each sentence ( N = 10). The results are given in Table 7 (Xu,\nEmami, & Jelinek, 2003) . The different conditioning context correspond to the PREDIC-\nTOR model only. The probabilistic dependencies of the TAGGER and CONSTRUCTOR\nare the same as in the baseline model. Furthermore, the SCORER is copied from the trained\nPREDICTOR. By comparing the E0 (iteration 0) rows to the results in Table 5 one can\nobserve that using a neural net model for the components involved in parse construction\nfurther decreases the perplexity. This can be attributed to the construction of better partial\nparses by the neural net based components. It is also clear from the table that the “N-best”\nEM re-estimation reduces the perplexity consistently for all situations; however, the reduc-\ntion is minimized when the neural net based model is interpolated with baseline orN-gram\nmodel.\nThe last 2 rows of the table show the results for the full training of the model (scenario\n4), where a separate SCORER is trained on the partial parses constructed by the models in\nsecond and fourth rows; 2 HW-E0 and (3+ 1) HW-E0. As can be observed from the table,\nby training a separate SCORER the perplexity of the model is signiﬁcantly reduced.\nIn the third scenario, we trained a neural net SCORER on the partial parses constructed\non the training data by the baseline SLM. In order to reach convergence faster we did not\nrandomly initialize the neural net, instead the parameters were copied from the SCORER\ntrained in the ﬁrst scenario. Subsequently, the network was trained for 30 and 7 iterations\nA NEURAL SYNTACTIC LANGUAGE MODEL 223\nTable 8. Macthed NN SCORER; UPenn perplexity.\nModel no-intpl +slm +3g m +5g m\n2 HW 141 125 119 115\n3 HW 136 121 116 112\n(3 + 1) HW 131 117 113 110\nAll-3 122 114 110 107\nTable 9. NN SCORER; WSJ WER.\nModel no-intpl +slm +lattice +5g m +all\n2 HW 12.8 12.3 12.4 12.3 12.0\n3 HW 12.9 12.7 12.9 12.6 12.4\n(3 + 1) HW 12.5 12.3 12.4 12.1 12.0\nfor perplexity and WER experiments respectively. It is worth mentioning that reducing the\nnumber of iterations is very important in this case because of the large number of partial\nparses involved.\nThe UPenn test set perplexities are given in Table 8 (Emami, 2003). There were a total of\n10 M partial parses — compare to 1 M for the ﬁrst scenario –; an average of 11.12 partial\nparses per word. Similarly the WER results are presented in Table 9 (Emami & Jelinek,\n2004). Here the total number of partial parses was 148 M—compare to 19 M parses of the\nﬁrst scenario.\nThe best overall results are decidedly achieved—in terms of both perplexity and WER—\nwhen we use a neural net SCORER trained to maximize the word level probabilities. With\nthe best models we achieved a perplexity of 107 and a word error rate of 12 .0%; which\nare, as far as we know, better than any published results for the same setup (Emami, 2003;\nEmami & Jelinek, 2004).\nComparing the results in Tables 7 and 8 (last 2 rows), we observe that small reductions in\nperplexity are achieved if we train a matched SCORER for an “N-best” EM trained model\n(i.e. full training). However the improvement is practically small, and we conclude that the\nbest (or near best) results are attainable by training only a matched SCORER (all other\ncomponents kept unchanged), and without employing the “N-best” the EM training.\nIt is worth mentioning that in most of the experiments, the neural network by itself (no\ninterpolation) performed worse than the standard N-gram models. The exception is when\nwe train a matched SCORER (Tables 8 and 9, and last 2 rows of Table 7), where the neural\nnet based SLM outperforms all the other models by itself.\nIt is easy to compare the ﬁrst and third scenarios and explain the signiﬁcant difference\nin their performance. Both procedures estimated a SCORER, however in the ﬁrst scenario\nthe SCORER was trained as a PREDICTOR; therefore there is a mismatch between the\nobjective the model was trained for and its actual use and thus the estimated model is sub-\noptimal. In contrast, in the third scenario the correct log-likelihood function was optimized,\n224 A. EMAMI AND F. JELINEK\nFigure 8. SCORER perplexity with 2 HWs as context.\nat the cost of much longer training time, which was due to the large number of partial parses\ninvolved.\nFigure 8 shows the learning curves of the neural network SCORER when the context used\nis the 2 previous headwords (2 HW). The curves on the left side of each ﬁgurecorrespond\nto the mismatched training of the SCORER (scenario 1), while the curves on the right\nshow the learning process in the third scenario—when NN SCORER is estimated from\nthe appropriate partial parses. All the training perplexities are a slight over-approximation\nof the actual ones because of the sequential behavior of the stochastic gradient descent\nalgorithm in updating the parameters. On the other hand, the held-out perplexities are\ncalculated after the iteration is over, and hence are exact. The curves for the two scenarios\nare placed one after the other because the parameters in the third scenario are initialized\nby copying them from the SCORER trained in the ﬁrst scenario. The discontinuities in the\nﬁgures are due to the fact that different data is used in the two different scenarios (1 and 3).\nIn the end, it is interesting to compare the SLM and word based ( N-gram) models. By\ncomparing the results in Tables 1 and 3 to those in this section it can be seen that the best\nword-based model is far inferior compared to the best SLM model. We might conclude\nfrom here that the syntactical structure of a sentence is in fact helpful in discriminating\namong competing hypotheses.\n6. Conclusions and future work\nBy using neural network models in the SLM, we achieved signiﬁcant improvements in PPL\nand WER over both the baseline N-gram model and SLM. Three scenarios for integrating\nneural networks in SLM where presented. Overall, the best studied model gave a 24.1%\nrelative reduction in PPL over the best N-gram model and a 18.9% relative reduction over\nthe baseline SLM. Our experiments also showed that the neural network models enhance\nthe discriminative characteristic of the SLM by achieving a 4.8% relative reduction in WER\nA NEURAL SYNTACTIC LANGUAGE MODEL 225\nover the baseline SLM. The corresponding reductions is 9.8% over the standard N-gram\nmodel.\nOverall, our experiments show that by using the syntactic information from the partial\nparses generated by the SLM, and by employing a probability estimation function that does\nnot overﬁt as fast as the N-gram models (neural nets in our case), it is possible to improve\nsigniﬁcantly over the regular word N-gram models (either standard or neural net N-gram).\nIn our study the full integration training, where the “N-best” EM re-estimation is followed\nby SCORER training, gave the best perplexity. Therefore in future work the fully integrated\nmodel should be use for the WER experiments.\nOne would also like to experiment with different and extended probabilistic dependencies\nfor the neural net models; given the amount of information available in a syntactic parse\nand the neural net capability in using it, this would most likely lead to improvements in\nperplexity. Alternatively, we intend to train our models to minimize speciﬁcally the WER,\nwhich would entail major modiﬁcations to the model’s architecture.\nWe observed in our experiments that the neural net models give a different “view” of\nthe data set than that of the N-gram models. Thus a natural extension of our work is to\ncombine the neural net and N-gram models at the component level, rather than the current\nword-level interpolation.\nWe believe that the results of the “N-best” EM training can still be improved. Note that\nthe objective function used was an approximation of the real EM auxiliary function, and\nthat it is possible that the neural network was not optimally trained by learning from the\nseverely pruned data set. As future work we intend to develop and use a different EM\ntraining procedure where the training of all model components is carried out on the partial\nparses stored in the stacks, instead of the last N-best parses. Intuitively, the number of\npartial parses used by this left-to-right EM training is larger than the “N-best” one, and the\ncorresponding objective criteria is a better approximation of the true EM auxiliary function.\nWe should note that this EM training would substitute for both the second and third stage\nof the SLM training procedure outlined in Section 3.6; therefore there would be no longer\na need for a separate SCORER component.\nAcknowledgments\nThe authors would like to thank Peng Xu for helpful discussions and for the preparation of\nthe data sets used in this work. We would also like to thank Ciprian Chelba for stimulating\ndiscussions and helpful tips. The authors would also like to thank the Center for Imaging\nScience at the Johns Hopkins University for the use of the RS/6000 SP machine provided\nby IBM corporation.\nReferences\nBellegarda, J. R. (1997). A latent semantic analysis framework for large–span language modeling. InProceedings\nof the 5th European Conference on Speech Communication and Technology(pp. 1451&1454). V ol. 3. Rhodes,\nGreece.\nBengio, Y ., Ducharme, R., & Vincent, P. (2001). A neural probabilistic language model. Advances in Neural\nInformation Processing Systems, 13, 933–938.\n226 A. EMAMI AND F. JELINEK\nBengio, Y ., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neuralprobabilistic language model. Journal of\nMachine Learning Reseach, 3, 1137–1155.\nBerger, A. L., Pietra, S. A. D., & Pietra, V . J. D. (1996). A maximum entropyapproach to natural language\nprocessing. Computational Linguistics, 22:1, 39–72.\nBridle, J. S. (1989). Probabilistic interpretation of feedforward classiﬁcation network outputs, with relationships\nto statistical patternrecognition. In F. Fougelman-Soulie and J. Herault (Eds.), Neuro-computing: Algorithms,\narchitectures and applicatations(pp. 227&236).\nByrne, W., Gunawardana, A., & Khudanpur, S. (1998). Information geometry and EMvariants. Technical Report\nCLSP Research Note (17). Department of Electrical andComputer Engineering, The Johns Hopkins University,\nBaltimore, MD.\nCharniak, E. (2001). Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting\nand 10th Conference of the European Chapter of ACL(pp. 116–123). Toulouse, France.\nChelba, C. (1997). A structured language model. In ACL-EACL, Student Section(pp. 498&500). Madrid, Spain.\nChelba, C., & Jelinek, F. (2000). Structured language modeling.Computer Speech and Language, 14:4, 283–332.\nChelba, C., & Xu, P. (2001). Richer syntactic dependencies for structuredlanguage modeling. In Proceedings of\nthe Automatic Speech Recognition and Understanding Workshop. Madonna di Campiglio, Trento-Italy.\nChen, S. F. & Goodman, J. (1999). An empirical study of smoothing techniquesfor language modeling.Computer\nSpeech and Language, 13, 359–394.\nCollins, M. (1996). A new statistical parser based on bigram lexicaldependencies. In Proceedings of the 34th\nAnnual Meeting of the Association for Computational Linguistics(pp. 184&191). Santa Cruz, CA.\nDeerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A. (1990). Indexing by latent\nsemantic analysis. Journal of the American Society of Information Science, 41:6, 391–407.\nDempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood fromincomplete data via the EM\nalgorithm. Journal of the Royal Statistical Society, 39, 1–38.\nElman, J. L. (1991). Distributed representations, simple recurrent networks,and grammatical structure. Machine\nLearning, 7, 195–225.\nEmami, A. (2003). Improving a connectionist based syntactical language model. In Proceedings of the 8th\nEuropean Conference on Speech Communication and Technology(pp. 413–416), V ol. 1. Geneva, Switzerland.\nEmami, A., & Jelinek, F. (2004). Exact training of a neural syntactic languagemodel. InProceedings of the IEEE\nInternational Conference onAcoustics, Speech and Signal Processing. Montreal,Quebec.\nEmami, A., Xu, P., & Jelinek, F. (2003). Using a connectionist model in asyntactical based language model. In\nProceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing(pp. 372–375).\nV ol. I. Hong Kong.\nFodor, J. A. & Pylyshyn, Z. W. (1988). Connectionism and cognitive structure: A critical analysis.Cognition, 28,\n3–71.\nGoodman, J. (2001). A bit of progress in language modeling. Technical Report MSR-TR-2001-72, Microsoft\nResearch, Redmond, W A.\nGropp, W., Lusk, E., & Skjellum, A. (1999). Using MPI: Portable parallelProgramming with themessage-passing\ninterface. Cambridge: MA: MIT Press.\nHenderson, J. (2000). A neural network parser that handles sparse data. In Proceedings of 6th International\nWorkshop on Parsing Technologies(pp. 123–134). Trento, Italy.\nHenderson, J. (2003). Inducing history representations for broad coveragestatistical parsing. In Proceedings of\nthe North American Chapter of Association Computational Linguistics and Human Language Technology\nConference HLT-NAACL.\nHinton, G. E. (1986). Learning distributed representations of concepts. In R. G. M. Morris (Ed.), Parallel dis-\ntributed processing:Implications for psychology and Neurobiology(pp. 46–61). Oxford, UK: Oxford University\nPress.\nHo, E. & Chan, L. (1999). How to design a connectionist holistic parser. Neural Computation, 11:8, 1995–2016.\nJelinek, F. (1998). Statistical methods for speech recognition. Cambridge, MA and London: MIT Press.\nJelinek, F. and Mercer, R. L. (1980). Interpolated estimation of Markov sourceparameters from sparse data. In\nProceedings of Workshop on Pattern Recognition in Practice(pp. 381–397). Amsterdam, The Netherlands:\nNorth Holland Publishing Co.\nA NEURAL SYNTACTIC LANGUAGE MODEL 227\nKim, W., Khudanpur, S., & Wu, J. (2001). Smoothing issues in the structuredlanguage model. In Proceedings of\nthe 7th European Conference on Speech Communication and Technology(pp. 717–720). Alborg, Denmark.\nKneser, R., & Ney, H. (1995). Improved backing-off for m-gram languagemodeling. In Proceedings of the IEEE\nInternational Conference on Acoustics, Speech and Signal Processing(pp. 181&184), V ol. I.\nLawrence, S., Giles, C. L., & Fong, S. (1996). Can recurrent neural networkslearn natural language grammars?.\nIn Proceedings of the IEEE International Conference on Neural Networks(pp. 1853&1858). Piscataway, NJ:\nIEEE Press.\nLawson, C. L., Hanson, R. J., Kincaid, D. R., & Krogh, F. T. (1979). Basiclinear algebra subprograms for fortran\nusage. ACM Transactions on Mathematical Software, 5:3, 308–323.\nLeCun, Y . (1985). A learning scheme for asymmetric threshold networks. In Proceedings of Cognitiva 85(pp.\n599–604). Paris, France.\nMiikkulainen, R. & Dyer, M. G. (1991). Natural language processing withmodular neural networks and distributed\nlexicon. Cognitive Science, 15, 343–399.\nNey, H., Essen, U., & Kneser, R. (1994). On structuring probabilisticdependencies in stochastic language model-\ning.. Computer Speech and Language, 8, 1–38.\nPaul, D. B., & Baker, J. M. (1992). The design for the wall street journal-based CSR corpus. In Proceedings of\nthe DARPA SLS Workshop.\nRatnaparkhi, A. (1997). A linear observed time statistical parser based onmaximum entropy models. In Second\nConference on Empirical Methods in Natural Language Processing(pp. 1–10). Providence, RI.\nRoark, B. (2001). Robust probabilistic predictive syntactic processing: Motivations, models and applications.\nPh.D. thesis, Brown University, Providence, RI.\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Leaning internalrepresentations by error propagation.\nIn D. E. Rumelhart & J. L. McClelland (Eds.), Paralleldistributed processing, I. Cambridge, MA: MIT Press.\nSchwenk, H., & Gauvain, J.-L. (2002). Connectionist language modeling for largevocabulary continuous speech\nrecognition. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,\n(pp. 765–768). V ol. II. Orlando, FL.\nVan Uystel, D. H., Van Compernolle, D., & Wambacq, P. (2001). Maximum-likelihood training of the PLCG-based\nlanguage model. In Proceedings of the Automatic Speech Recognition andUnderstanding Workshop. Madonna\ndi Campiglio, Trento-Italy.\nWerbos, P. J. (1974). Beyond regression: New tools for prediction and analysisin the behavioral sciences. Ph.D.\nthesis, Harvard University, Cambridge, MA.\nXu, P., Chelba, C., & Jelinek, F. (2002). A study on richer syntacticdependencies for structured language modeling.\nIn Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics. Philadelphia, PA.\nXu, P., Emami, A., & Jelinek, F. (2003). Training connectionist models for thestructured language model. In M.\nCollins, & M. Steedman (Eds.), Proceedings of the 2003conference on empirical methods in natural language\nprocessing. Sapporo, Japan: (pp. 160–167). Association for Computational Linguistics.\nXu, W., & Rudnicky, A. (2000). Can artiﬁcial neural networks learn languagemodels? In Proceedings of 6th\nInternational Conference on Spoken Language Processing. Beijing, China.\nReceived October 15, 2003\nRevised June 26, 2004\nAccepted November 15, 2004",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9644932746887207
    },
    {
      "name": "Treebank",
      "score": 0.8527896404266357
    },
    {
      "name": "Computer science",
      "score": 0.7996057271957397
    },
    {
      "name": "Language model",
      "score": 0.767009973526001
    },
    {
      "name": "Artificial intelligence",
      "score": 0.675133466720581
    },
    {
      "name": "Natural language processing",
      "score": 0.6227439045906067
    },
    {
      "name": "Artificial neural network",
      "score": 0.606602668762207
    },
    {
      "name": "Parsing",
      "score": 0.56315016746521
    },
    {
      "name": "Word (group theory)",
      "score": 0.5123699903488159
    },
    {
      "name": "String (physics)",
      "score": 0.4954031705856323
    },
    {
      "name": "Word error rate",
      "score": 0.4412723183631897
    },
    {
      "name": "Probabilistic logic",
      "score": 0.43414148688316345
    },
    {
      "name": "Linguistics",
      "score": 0.1413552463054657
    },
    {
      "name": "Mathematics",
      "score": 0.08090168237686157
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ]
}