{
    "title": "Sparse Binary Transformers for Multivariate Time Series Modeling",
    "url": "https://openalex.org/W4385565515",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3006722088",
            "name": "Matt Gorbett",
            "affiliations": [
                "Colorado State University"
            ]
        },
        {
            "id": "https://openalex.org/A2132385391",
            "name": "Hossein Shirazi",
            "affiliations": [
                "San Diego State University",
                "Colorado State University"
            ]
        },
        {
            "id": "https://openalex.org/A1966110232",
            "name": "Indrakshi Ray",
            "affiliations": [
                "Colorado State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2555077524",
        "https://openalex.org/W3195023262",
        "https://openalex.org/W6774302960",
        "https://openalex.org/W6959340700",
        "https://openalex.org/W3004207920",
        "https://openalex.org/W2982438846",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3007007518",
        "https://openalex.org/W4285048954",
        "https://openalex.org/W4282581586",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W2786827964",
        "https://openalex.org/W6803845004",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3008515144",
        "https://openalex.org/W3034234149",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W3099971460",
        "https://openalex.org/W2743617586",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W2950361482",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W4283318673",
        "https://openalex.org/W3173539742",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W3188872815",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W3042807565",
        "https://openalex.org/W4287828539",
        "https://openalex.org/W1994530392",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W4241886172",
        "https://openalex.org/W2892181857",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W3212890323",
        "https://openalex.org/W2604272474",
        "https://openalex.org/W4286850694",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W3106543020"
    ],
    "abstract": "Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attention mask to allow computation only at the current time step. Together, each compression technique and attention modification substantially reduces the number of non-zero operations necessary in the Transformer. We measure the computational savings of our approach over a range of metrics including parameter count, bit size, and floating point operation (FLOPs) count, showing up to a 53× reduction in storage size and up to 10.5× reduction in FLOPs.",
    "full_text": null
}