{
    "title": "EHR Foundation Models Improve Robustness in the Presence of Temporal Distribution Shift",
    "url": "https://openalex.org/W4224284596",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2912170880",
            "name": "Lin Lawrence Guo",
            "affiliations": [
                "SickKids Foundation",
                "Hospital for Sick Children",
                "Institute for Clinical Evaluative Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2181729714",
            "name": "Ethan Steinberg",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4224515476",
            "name": "Scott Lanyon Fleming",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2484832969",
            "name": "José Posada",
            "affiliations": [
                "Universidad del Norte"
            ]
        },
        {
            "id": "https://openalex.org/A4224515477",
            "name": "Joshua Lemmon",
            "affiliations": [
                "Hospital for Sick Children",
                "SickKids Foundation",
                "Institute for Clinical Evaluative Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2885265381",
            "name": "Stephen R Pfohl",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2659277696",
            "name": "Nigam Shah",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4222399703",
            "name": "Jason Fries",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2169865483",
            "name": "Lillian Sung",
            "affiliations": [
                "Hospital for Sick Children",
                "SickKids Foundation",
                "Institute for Clinical Evaluative Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2912170880",
            "name": "Lin Lawrence Guo",
            "affiliations": [
                "Stanford University",
                "Hospital for Sick Children",
                "SickKids Foundation",
                "Institute for Clinical Evaluative Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2181729714",
            "name": "Ethan Steinberg",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4224515476",
            "name": "Scott Lanyon Fleming",
            "affiliations": [
                "Universidad del Norte"
            ]
        },
        {
            "id": "https://openalex.org/A2484832969",
            "name": "José Posada",
            "affiliations": [
                "Institute for Clinical Evaluative Sciences",
                "Hospital for Sick Children",
                "SickKids Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A4224515477",
            "name": "Joshua Lemmon",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2885265381",
            "name": "Stephen R Pfohl",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2659277696",
            "name": "Nigam Shah",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4222399703",
            "name": "Jason Fries",
            "affiliations": [
                "Institute for Clinical Evaluative Sciences",
                "Hospital for Sick Children",
                "SickKids Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A2169865483",
            "name": "Lillian Sung",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2784499877",
        "https://openalex.org/W2028138594",
        "https://openalex.org/W4221077687",
        "https://openalex.org/W3174786846",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W3177617320",
        "https://openalex.org/W2604834158",
        "https://openalex.org/W1914974178",
        "https://openalex.org/W2079776874",
        "https://openalex.org/W3196529262",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2954540134",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W3112116031",
        "https://openalex.org/W2964233659",
        "https://openalex.org/W3030156796",
        "https://openalex.org/W3013689372",
        "https://openalex.org/W1126991912",
        "https://openalex.org/W2157365260",
        "https://openalex.org/W3043023066",
        "https://openalex.org/W2799695199",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W2177350256",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W2971616138",
        "https://openalex.org/W2954707123",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3040002795",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3111740750",
        "https://openalex.org/W3163602117",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W3101083045"
    ],
    "abstract": "ABSTRACT Background Temporal distribution shift negatively impacts the performance of clinical prediction models over time. Pretraining foundation models using self-supervised learning on electronic health records (EHR) may be effective in acquiring informative global patterns that can improve the robustness of task-specific models. Objective To evaluate the utility of EHR foundation models in improving the in-distribution (ID) and out-of-distribution (OOD) performance of clinical prediction models. Methods The cohort consisted of adult inpatients admitted between 2009-2021. Gated recurrent unit (GRU)- and transformer (TRANS)-based foundation models were pretrained on EHR of patients admitted between 2009-2012 and were subsequently used to construct patient representations (CLMBR). These representations were used to learn logistic regression models (CLMBR GRU and CLMBR TRANS ) to predict hospital mortality, long length of stay, 30-day readmission, and ICU admission. We compared CLMBR GRU and CLMBR TRANS with baseline logistic regression models learned on count-based representations (count-LR) and end-to-end (ETE) GRU and transformer models in ID (2009-2012) and OOD (2013-2021) year groups. Performance was measured using area-under-the-receiver-operating-characteristic curve, area- under-the-precision-recall curve, and absolute calibration error. Results Models trained on CLMBR generally showed better discrimination relative to count-LR in both ID and OOD year groups. In addition, they often matched or were better than their ETE counterparts. Finally, foundation models’ performance in the self-supervised learning task tracked closely with the ID and OOD performance of the downstream models. Conclusions These results suggest that pretraining foundation models on electronic health records is a useful approach for developing clinical prediction models that perform well in the presence of temporal distribution shift.",
    "full_text": "TITLE: EHR Foundation Models Improve Robustness in the Presence of Temporal Distribution \nShift \nAUTHORS: 1Lin Lawrence Guo PhD, 2Ethan Steinberg, 2Scott Lanyon Fleming, 3Jose Posada, \n1Joshua Lemmon, 2Stephen R Pfohl PhD, 2Nigam Shah MD, PHD, *2Jason Fries PhD, *1,4Lillian \nSung MD, PhD \n*co-senior authors \nAFFILIATIONS:  \n1Program in Child Health Evaluative Sciences, The Hospital for Sick Children, Toronto, ON, \nCanada \n2Stanford Center for Biomedical Informatics Research, Stanford University, Palo Alto, CA \n3Universidad del Norte, Colombia \n5Division of Haematology/Oncology, The Hospital for Sick Children, Toronto, ON \nADDRESS FOR CORRESPONDANCE: \nLillian Sung MD, PhD \nDivision of Haematology/Oncology \nThe Hospital for Sick Children,  \n555 University Avenue, Toronto, Ontario, M5G1X8, Canada \nTelephone: 416-813-5287 \nFax: 416-813-5979 \nEmail: Lillian.sung@sickkids.ca\n \nKEY WORDS: electronic health records, foundation model, model robustness, machine \nlearning, self-supervised learning \nWORD COUNT: Abstract 250; Text 3,702; Tables 0; Figures 6;  \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nABSTRACT \nBackground: Temporal distribution shift negatively impacts the performance of clinical \nprediction models over time. Pretraining foundation models using self-supervised learning on \nelectronic health records (EHR) may be effective in acquiring informative global patterns that \ncan improve the robustness of task-specific models.  \nObjective: To evaluate the utility of EHR foundation models in improving the in-distribution (ID) \nand out-of-distribution (OOD) performance of clinical prediction models.\n  \nMethods: The cohort consisted of adult inpatients admitted between 2009-2021. Gated \nrecurrent unit (GRU)- and transformer (TRANS)-based foundation models were pretrained on \nEHR of patients admitted between 2009-2012 and were subsequently used to construct patient \nrepresentations (CLMBR). These representations were used to learn logistic regression models \n(CLMBR\nGRU and CLMBRTRANS) to predict hospital mortality, long length of stay, 30-day \nreadmission, and ICU admission. We compared CLMBRGRU and CLMBRTRANS with baseline \nlogistic regression models learned on count-based representations (count-LR) and end-to-end \n(ETE) GRU and transformer models in ID (2009-2012) and OOD (2013-2021) year groups. \nPerformance was measured using area-under-the-receiver-operating-characteristic curve, area-\nunder-the-precision-recall curve, and absolute calibration error.  \nResults:  Models trained on CLMBR generally showed better discrimination relative to count-LR \nin both ID and OOD year groups. In addition, they often matched or were better than their ETE \ncounterparts. Finally, foundation models’ performance in the self-supervised learning task \ntracked closely with the ID and OOD performance of the downstream models.  \nConclusions: These results suggest that pretraining foundation models on electronic health \nrecords is a useful approach for developing clinical prediction models that perform well in the \npresence of temporal distribution shift. \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nINTRODUCTION \nThe large increase in the adoption of electronic health records (EHR) has enabled the \nuse of machine learning to develop highly performant clinical prediction models that have the \npotential to improve the care of patients[1]. However, the non-stationary healthcare environment \ncan bring about changes in the data distribution between model development and \ndeployment[2], which can degrade the model’s performance over time[3] and consequently its \nclinical utility[4]. In this study, we explored temporal distribution shift alongside the suitability of \nfoundation models[5] – deep neural networks trained on large-scale unlabeled data using self-\nsupervised learning – and whether they can be adapted via transfer learning to improve the \nrobustness of clinical prediction models in the presence of temporal distribution shift. \nThe cause of temporal distribution shift in clinical medicine is often subtle[6] and the \nextent of its impact on model performance is heterogeneous across tasks[3, 7-9]. Nonetheless, \nthe consequence of the impact on patient care and physician’s trust can be severe. An example \nis the widely implemented Epic sepsis model developed on data collected between 2013-2015 \nthat performed below expectation when evaluated at Michigan Medicine on data collected \nbetween 2018-2019 and resulted in a large number of spurious alerts[4].   \nRecent approaches that mitigate the impact of temporal distribution shift on model \nperformance in clinical medicine largely rely on model monitoring and updating policies that do \nnot leverage the entire patient population available[10]. In addition, proactive approaches using \ndomain generalization and adaptation have shown little to no success[3].  \nTo date, few studies have explored learning contextualized patient representations at \nscale. Findings from domains outside of clinical medicine suggest significant performance[11] \nand robustness[12, 13] benefits to pretraining foundation models. In this study, we adopt \nCLMBR,[14] an EHR foundation model pretrained on patient timelines comprised of sequential \nstructured medical codes using autoregressive sequence modeling as the self-supervised \nlearning task. Transfer of the structure learned by CLMBR from the entire patient population to \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \ndownstream models have demonstrated performance benefits compared to standard baselines \nincluding count-based models, especially when the number of patient records was small. \nIn this study, we evaluated the utility of CLMBR in mitigating the impact of temporal \ndistribution shift on model performance. We hypothesized that the global patterns embedded in \nCLMBR can be adapted into models that perform better than count-based representations in \nout-of-distribution (OOD) years. In addition, we characterized the robustness of popular \narchitectures used in clinical settings, namely gated recurrent unit (GRU)[14] and transformers \n(TRANS)[15, 16]. Further, to understand the contribution of CLMBR, we evaluated both GRU \nand TRANS end-to-end (ETE) models. Therefore, the objectives were to compare the in-\ndistribution (ID) and OOD performance of CLMBRGRU, CLMBRTRANS, ETEGRU and ETETRANS \ncompared to models trained using count-based representations.  \n \nMETHODS \nData Source \nWe used data from the STAnford medicine Research data Repository (STARR)[17]. \nData in STARR are routinely collected in the EHR of Stanford Medical Center, comprised of \nStanford Health Care (primarily adult-directed care) and Lucile Packard Children’s Hospital \n(primarily pediatric-directed care). These data are mapped to the Observational Medical \nOutcomes Partnership Common Data Model (OMOP-CDM), which facilitates multi-center \nobservational research[18, 19]. The resulting dataset was named STARR-OMOP. This study \nused de-identified data in which dates were jittered by up to 30 days but were accurate within a \npatient timeline. Because of de-identification, the requirement for Institutional Review Board \napproval was waived by Stanford Medical Center. \n \nCohort \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nWe included adult patients over the age of 18 with admissions to the inpatient unit \nbetween EHR inception (2009) and August 22, 2021. Admissions to the inpatient unit were \neither direct or transfers from the emergency department. Encounters with clinic visit alone and \nencounters in which patient death or discharges occurred on the same day of admission were \nexcluded. For patients with multiple admissions, one was randomly selected so that each \npatient was only represented once in the dataset.  \n \nOutcomes \n We defined four clinical outcomes. Hospital mortality was defined as a patient death \noccurring during the hospital stay. Long length of stay (LOS) (long LOS) was defined as a \nhospital admission of seven or more days. Readmission in 30 days (30-day readmission) was \ndefined as a readmission to an inpatient unit within 30 days after discharge. Intensive care unit \n(ICU) admission was defined as a patient transfer to the intensive care unit during the hospital \nadmission. Each outcome was considered as a binary classification task where the prediction \ntime (also the index time) was set as 11:59PM on the day of admission for the hospital mortality, \nlong LOS and ICU admission tasks, and 11:59PM on the day of discharge for the 30-day \nreadmission prediction task. For the 30-day readmission task, we removed patients who were \nre-admitted on the day of discharge, and for the ICU admission task, we removed patients \ntransferred on the day of admission since these events would have occurred before prediction \ntime. \n \nPatient Representations \n EHR data corresponding to a particular patient can be treated as a sequence of days \nthat is ordered by time, d\n1 … d N, where each day consists of a set of events represented by \nmedical codes such as diagnoses, lab tests, procedures, and medication administrations or \nprescriptions as examples. In this study, we considered two approaches to construct patient \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nrepresentations over the patient timelines as illustrated in Figure 1: count-based representations \nand CLMBR.   \n \nCount based representations: The count-based representations were constructed using an \nopen-source count-based featurizer[20] which follows standard practices for patient count-\nbased featurization.[1, 21] This approach constructed patient representations as binary features \nbased on counts of both unique OMOP CDM concepts and derived elements recorded prior to \nthe time of prediction. The feature set consisted of demographic and clinical features. \nDemographic features included sex at birth, race, ethnicity, and age at admission discretized \ninto five-year intervals. Clinical features were constructed as the concatenation of the results of \na time-dependent extraction procedure applied independently to data elements recorded in time \nbins defined relative to the time of prediction. The time bins were as follows: 24 hours prior, 1-7 \ndays prior, 8-30 days prior, and 31 days-any time prior. The time-dependent extraction \nprocedure identified all unique OMOP CDM concepts from the following OMOP CDM tables: \ncondition occurrence (diagnosis codes), drug exposure (administration or prescription of \nmedications), procedure occurrence, measurement (includes laboratory tests), device exposure \n(exposure to implantable objects, medical equipment, supplies, and instruments) and \nobservation (non-standardized tests or clinical observations). Continuous measurement results \nwere represented as binary indicators for abnormal results for each measurement on the basis \nof whether the result was above or below the reference range.  \n \nClinical language model-based representations – CLMBR: The core idea behind CLMBR is that \nif a sequence model is able to predict sets of medical codes over a patient timeline, then that \nmodel may have discovered informative global patterns that can be re-used in various other \nclinical prediction tasks. Note that the term “language model” in CLMBR merely reflects the \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nsimilarity in the computations involved between sequence modeling of medical codes and \nlanguage modeling, and therefore does not indicate natural language processing of any kind. \n First, we mapped clinical codes for labs, medications, diagnoses, and procedures to a \nfinite vocabulary of discrete symbols. This vocabulary was then mapped into a clinical ontology \nto reduce sparsity and used to construct patient sequences for the CLMBR encoder. The \nmedical codes were obtained from the same OMOP CDM tables as used for count-based \nrepresentations except for the observation table. The Unified Medical Language System \n(UMLS)[22] was used to extend each medical code to the set of parents in its native ontology \nwhen applicable (ICD10 for diagnoses, CPT or MTHH for procedures, and ATC for \nmedications). For instance, the occurrence of the ICD10 code “H61.23” for the diagnosis of \nimpacted cerumen, bilateral, resulted in two additional parent codes, namely “H61.2” (impacted \ncerumen) and “H61” (other disorders of external ear).  \nWe chose GRU and transformer as the architectures for our sequence models as they \nhave each demonstrated success in the sequence modeling of medical codes in the EHR[14, \n18, 19, 23]. To construct patient representations, sets of codes for each day in the patient \ntimeline were first passed through the embedding bag layer of the networks, which computes \nthe mean code embedding for each day. Next, each mean embedding was concatenated with a \nvector that captured time information including the patient’s age on that day, the time delta from \nthe previous day, whether that day was the first day of the sequence, and the log transform of \nthe age and time delta. Patient representations were then computed by feeding the \nconcatenated vectors into the GRU or transformer (see Supplementary Methods for details on \narchitecture), followed by a linear layer with output size equal to the number of dimensions of \nthe patient representation, which was set to 800 in this study.  \nTo predict the set of codes for a given day, di, the patient representation from the \nprevious day, di-1, was used. We formulated the set prediction problem as a series of \nindependent binary classification problems, where the probability of a given code was computed \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nvia a sigmoid transformation of the dot product between the code embedding and the patient \nrepresentation. To deal with the computational complexity of the matrix product induced by the \nlarge code space, we used a variant of the hierarchical softmax optimization[24] in which we \nreplaced the softmax transformations with sigmoid transformations. The hierarchical structures \nof the code space were the same as the ones used for ontology extension (e.g., the hierarchical \nstructure in the ICD10 vocabulary for ICD10 codes).  We used the binary cross entropy loss as \nthe loss function during training.  \nOnce the sequence models were trained, we used them to construct representations \n(the output of the linear layer) for each patient in the cohort to be used by downstream models \nfor clinical prediction tasks (CLMBRGRU and CLMBRTRANS). For hospital mortality, long LOS, and \nICU admissions, patient representations were obtained up until the day of admission, whereas \nfor 30-day readmission patient representations were obtained up until the day of discharge.   \n  \nExperimental Setup \n First, we established baseline model performance for each of the four clinical prediction \ntasks and investigated whether model performance degraded over time as a result of temporal \ndistribution shift. We trained logistic regression models on count-based representations \nconstructed for patients admitted between 2009-2012 (count-LR) and evaluated the models on \nall years from 2009-2021. The years on which the models were trained (2009-2012) constituted \nthe ID years and the subsequent years (2013-2021) constituted the OOD years for the baseline \nexperiment. We also included oracle models that were trained and evaluated on each of the \nOOD years for comparison. \nNext, we compared ID and OOD performance for four different representation \nconstruction and modeling approaches relative to count-LR, namely CLMBR\nGRU, CLMBRTRANS, \nETEGRU, and ETETRANS. For this experiment, ID years were 2009-2012 and the OOD years were \n2013-2016 and 2017-2021.  For statistical comparisons, we focused on comparing CLMBRGRU \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nand CLMBRTRANS vs. count-LR, CLMBRGRU and CLMBRTRANS vs. their respective ETE \narchitecture, and CLMBRGRU vs. CLMBRTRANS. To gain insight into relative performance, we \nsubtracted the model’s OOD performance in 2017-2021 by its ID performance in 2009-2012 for \nthe same five representation construction and modeling approaches. To limit multiple testing, \ndescribing relative OOD performance and OOD statistical comparisons only focused on 2017-\n2021 and not 2013-2016. \nWe also examined the contribution of CLMBR to downstream model performance by \nexamining the Pearson correlation between each sequence model’s pretraining performance  \nand the downstream logistic regression performance in each clinical prediction task. \nPerformance was measured using binary cross-entropy loss.  \nAs a sensitivity analysis, we trained and compared task-specific models on count-based \nrepresentations and CLMBR using light gradient-boosted machines (LightGBM) instead of \nlogistic regression. In addition, to aid clinical interpretation of the changes in performance \nbetween count-LR and CLMBR-based models, we quantified the numbers of decisions that \nwould have been affected if the CLMBR-based models were used instead of count-LR for tasks \nin which performance degraded over time. Specifically, we selected the better performing \nCLMBR model (CLMBR\nGRU vs CLMBRTRANS) and calculated the proportion of patients that would \nhave been classified correctly and incorrectly with CLMBR-based models instead of count-LR \nacross various risk thresholds.  \n \nModel development: The cohort was divided into training (70%), validation (15%), and test \n(15%) sets with random sampling stratified by the year in which the admission occurred. We \nextracted count-based representations and CLMBR for each patient admission. For count-\nbased representations, we additionally pruned features with less than 25 observations in the \ntraining set. We then pruned the same features from the validation and test sets. To tune GRU \nand transformer for CLMBR, we performed grid search over the hyperparameter settings for \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \neach architecture separately. For GRU, the hyperparameters consisted of learning rate, L2 \nregularization strength and dropout rate. For transformer, we additionally tuned the number of \ntransformer layers and the rate for code dropout. Supplementary Methods detail the \nhyperparameter grid and the selected hyperparameter settings. We trained GRUs and \ntransformers on the timelines of 80% of the patients in the training set of 2009-2012 (29026 \npatients, ~1.1 million patient days, and ~41.5 million medical codes), and selected \nhyperparameter settings based on model performance in the left out 20% of the training set. \nThen, using the selected GRU and transformer, we constructed CLMBRGRU and CLMBRTRANS for \neach patient in the cohort.  \nAfter computing the representations, we trained logistic regression with L2-regularization \nand LightGBM models on count-based representations and CLMBR for each clinical outcome in \nthe training set of 2009-2012. Hyperparameter tuning was done on L2 strength, which ranged \nfrom 10\n-6 to 102 in increments of powers of 10. We selected hyperparameter values based on \nthe model’s binary cross entropy loss in the validation set of 2009-2012. The ETE models were \ntrained for each clinical outcome separately on the training set of 2009-2012, and \nhyperparameter tuning was conducted using the same grid as CLMBR\nGRU and CLMBRTRANS for \nfair comparisons. We selected hyperparameter settings for ETE models based on model \nperformance in the validation set of 2009-2012. Oracle models for each OOD year (2013-2021) \nas comparisons for count-LR were trained on count-based representations in the training set of \nthe OOD year, and hyperparameters were selected based on performance in the validation set \nof the OOD year. Supplementary Methods provide details on the selected hyperparameter \nsetting for logistic regression and ETE models for each clinical prediction task.  \n CLMBR and ETE models were implemented using Pytorch[25] and were trained on two \nNvidia V100 GPUs. We used the Sci-kit Learn’s[26] implementation of logistic regression. \nAnalyses were implemented in Python 3.8[27].  \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nModel evaluation: We evaluated each model’s discrimination performance in the test sets using \nthe area-under-the-receiver-operating-characteristic curve (AUROC) and the calibrated area-\nunder-the-precision-recall curve (AUPRC)[28]. The calibrated AUPRC computes the precision \nusing a reference outcome prevalence, here set as the prevalence in the ID year group 2009-\n2012. Thus, the calibrated AUPRC is invariant to change in outcome prevalence in OOD years \nand allows us to better interpret its variation over time. We used the absolute calibration error \n(ACE)[20] as a measure of calibration. ACE is similar to the integrated calibration index[29] but \napplies a logistic regression estimator to the logit of the predicted probability outputs rather than \nlocally weighted least squares and is thus more computationally efficient. \n \nStatistical Analysis: For each metric, we computed the median and 95% confidence interval (CI) \nof the distribution over performance in the test set obtained from 1000 bootstrap samples. To \ncompare models, we computed the 95% CI of the differences between a pair of models over \n1000 bootstrap samples. Statistical significance was defined as comparisons where the 95% CI \ndid not cross 0.    \n \nRESULTS \n Supplementary Table 1 presents cohort characteristics for each year and outcome \nprevalence. Figure 2 shows the impact of temporal distribution shift on performance (AUROC, \nAUPRC, and ACE) of count-LR trained on admissions from 2009-2012. Model degradation \noccurred in the OOD years (2013-2021) for long LOS and ICU admission prediction tasks, with \nlarger degradations observed in 2017-2021.  \n Figure 3 shows the relative performance of CLMBR-based and ETE models compared \nto count-LR in ID (2009-2012) and OOD (2013-2016 and 2017-2021) year groups (see \nSupplementary Tables 2 and 3 for statistical results of pre-specified comparisons). First, \nCLMBRGRU and CLMBRTRANS outperformed count-LR in discrimination performance in both ID \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nand OOD year groups across all tasks except for 30-day readmission. In terms of calibration, ID \nimprovement was observed for 30-day readmission for CLMBRTRANS vs. count-LR but other \ncomparisons between CLMBRGRU or CLMBRTRANS vs. count-LR did not show significant \ndifferences. In contrast, OOD (2017-2021) calibration results showed deterioration in hospital \nmortality and 30 day readmission tasks. Second, in general, the ID performance of CLMBRGRU \nand CLMBRTRANS were similar to or better than ETEGRU and ETETRANS respectively. However, \ndifferences in OOD 2017-2021 performance varied across tasks: CLMBR models generally \nperformed better in predicting ICU admission, and the ETE models generally performed better in \npredicting long LOS. Third, CLMBRGRU outperformed CLMBRTRANS in discrimination \nperformance in tasks other than 30-day readmission prediction for ID and for OOD hospital \nmortality and long LOS tasks.  \nFigure 4 shows relative OOD performance compared to ID performance across \nrepresentation construction and modeling approaches (see Supplementary Table 4 for statistical \nresults). Comparison of CLMBR\nGRU or CLMBRTRANS vs. count-LR showed heterogeneous \nresults. In terms of relative discrimination, both CLMBRGRU and CLMBRTRANS had significantly \nbetter relative AUROC vs. count-LR for long LOS but were similar for other tasks. In terms of \nrelative calibration, CLMBRGRU and CLMBRTRANS were significantly worse than count-LR for \nhospital mortality and 30-day readmission tasks. Other comparisons of relative performance \nwere heterogeneous. \nFigure 5 plots the average binary cross entropy loss of GRU sequence models (trained \nusing various hyperparameter settings) in the pretraining validation set against the average \nbinary cross entropy loss of their downstream logistic regression models in each task and year \ngroup (see Supplementary Figure 1 for the same analysis conducted on CLMBRTRANS). The \nperformance of GRU sequence models had high correlations (Pearson correlation coefficient) \nwith the ID and OOD performance of their downstream logistic regression models in all tasks \nexcept for 30-day readmission. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nFigure 6 plots the proportion of patients re-classified differently using CLMBRGRU instead \nof count-LR in long LOS and ICU admission. There were generally more correct re-\nclassifications than incorrect re-classifications across risk thresholds and year groups for long \nLOS. For ICU admission, there were more correct re-classifications in lower thresholds.  \nThe sensitivity analysis where we trained and compared task-specific models on count-\nbased representations and CLMBR using LightGBM instead of logistic regression showed \nqualitatively similar findings (Supplementary Experiment).    \n \nDISCUSSION \n We observed count-LR models resulted in large performance degradation over time for \nsome tasks, namely long LOS and ICU admission. Models trained on CLMBR generally \ndisplayed better discrimination relative to count-LR in ID and OOD year groups but could result \nin worse OOD calibration. In addition, models trained on CLMBR often matched and were \nsometimes even better than their ETE counterparts. Finally, in general, CLMBR\nGRU performed \nbetter than CLMBRTRANS, and its performance in the autoregressive sequence modeling task \ntracked closely with the ID and OOD performance of the downstream models for the majority of \ntasks considered.  \nLarge-scale self-supervised pretraining takes place less frequently and enables machine \nlearning practitioners to focus on rapid adaptation of these foundation models to downstream \ntasks. This research paper contributes more evidence that this approach brings not only \nperformance benefits over traditional count-based models, but robustness benefits in the \npresence of temporal distribution shift. These benefits decrease the need for model retraining \nand preserves the clinical utility of models deployed into practice. The strong relationship \nbetween sequence modeling performance and the performance of downstream clinical \nprediction tasks suggests that the observed favorable ID and OOD performance can be directly \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nattributed to the self-supervised pretraining. Whether the potential for worse calibration is \nclinically meaningful will depend on the specific use case. \nThe transformer architecture was more difficult to tune and performed worse than the \nGRU in both CLMBR-based models and ETE models. This may be explained by the smaller \ntraining set size used in this study, which totaled to ~1.1 million patient days and ~41.5 million \nmedical codes across ~29 thousand patients. In comparison, state of the art transformer-based \nnatural language processing models leveraged much larger volumes of data, for example, \n~3,300 times larger (137 billion tokens) for BERT[11], and ~53,000 times larger (2.2 trillion \ntokens) for RoBERTa[30].  \nWe examined two attributes of OOD performance, namely absolute performance and \nrelative (to ID) performance. It is notable that while CLMBR resulted in generally better absolute \ndiscrimination than count-LR, improvement in relative discrimination was more modest, with \nimprovement only being observed for long LOS. It is likely that absolute performance is more \nmeaningful to clinicians since better relative performance does not necessarily indicate better \nabsolute performance[31]. However, decision makers may be more concerned about using a \nmodel that does not perform as well as that originally promised (relative performance). It is for \nthat reason that we choose to report both aspects. \nDespite the reasonable performance of CLMBR-based models in OOD year groups, they \nare not immune to the impact of distribution shift as seen in predicting ICU admissions and long \nLOS. Increasing parameter size and training set size, which have demonstrated benefits in \nother modalities[30, 32], could provide additional improvements to robustness. In addition, \nstructure and certain types of invariances could be incorporated during pretraining or at the \nadaptation stage using metadata[33], regularization[34], or contrastive learning[35].  \nThe strengths of this study include the evaluation of a novel approach to self-supervised \nrepresentation learning on electronic health records, namely CLMBR, in both ID and OOD \nsettings. Another strength is the adoption of interpretable metrics to evaluate the clinical impact \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nof using CLMBR-based models instead of models trained on count-based representations. \nHowever, this study is limited as we only used a single dataset with a limited number of tasks. \nPerformance of CLMBR may differ in other settings and with other asks. Another limitation is \nthat we lack insight into scenarios in which CLMBR may be more or less helpful. \nIn conclusion, models trained on CLMBR generally displayed better discrimination \nrelative to count-LR in both ID and OOD year groups. Models trained on CLMBR often matched \nor were better than their ETE counterparts. Finally, autoregressive sequence modeling \nperformance tracked closely with the ID and OOD performance of the downstream models. \nThese results suggest that pretraining EHR foundation models is a useful approach for \ndeveloping clinical prediction models that perform well ID as well as OOD. \n \nEthics approval and consent to participate \nThis study used de-identified data and so the requirement for Institutional Review Board \napproval and participant informed consent were waived by Stanford Medical Center. \n \nData availability statement \nThe Stanford Medicine Research Data Repository is not made publicly available. The code for \nall analyses is open-source and available at https://github.com/som-\nshahlab/temp_ds_shift_robustness \n \nAcknowledgments \nNot applicable. \n \nCompeting interests \nThe authors declare that they have no competing interests. \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nFunding \nNo external funding was received for the study. \n \nAuthors’ contributions \nLLG, JF, and LS conceptualized and designed the study with input from all authors. L.L.G. \nperformed all experiments. ES and SRP contributed to the codebase. LLG, JF, and LS analyzed \nand interpreted results with input from all authors. LLG wrote the manuscript. All authors revised \nand commented on the manuscript. All authors read and approved the final manuscript.  \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nREFERENCES \n \n1 Rajkomar A, Oren E, Chen K, et al. Scalable and accurate deep learning with electronic health \nrecords. NPJ Digital Medicine 2018, 1(1):1-10. \n2 Moreno-Torres JG, Raeder T, Alaiz-Rodríguez R, et al. A unifying view on dataset shift in \nclassification. Pattern recognition 2012, 45(1):521-30. \n3 Guo LL, Pfohl SR, Fries J, et al. Evaluation of domain generalization and adaptation on \nimproving model robustness to temporal dataset shift in clinical medicine. Scientific Reports \n2022, 12(1):2726. \n4 Wong A, Otles E, Donnelly JP, et al. External validation of a widely implemented proprietary \nsepsis prediction model in hospitalized patients. JAMA Internal Medicine 2021, 181(8):1065-70. \n5 Bommasani R, Hudson DA, Adeli E, et al. On the opportunities and risks of foundation \nmodels. arXiv preprint arXiv:2108.07258 2021. \n6 Finlayson SG, Subbaswamy A, Singh K, et al. The Clinician and Dataset Shift in Artificial \nIntelligence. N Engl J Med 2021, 385(3):283-86. \n7 Davis SE, Lasko TA, Chen G, et al. Calibration drift in regression and machine learning \nmodels for acute kidney injury. J Am Med Inform Assoc 2017, 24(6):1052-61 %@ 67-5027. \n8 Strobl AN, Vickers AJ, Van Calster B, et al. Improving patient prostate cancer risk \nassessment: Moving from static, globally-applied to dynamic, practice-specific risk calculators. J \nBiomed Inform 2015, 56:87-93. \n9 Janssen KJ, Moons KG, Kalkman CJ, et al. Updating methods improved the performance of a \nclinical prediction model in new patients. J Clin Epidemiol 2008, 61(1):76-86. \n10 Guo LL, Pfohl SR, Fries J, et al. Systematic Review of Approaches to Preserve Machine \nLearning Performance in the Presence of Temporal Dataset Shift in Clinical Medicine. Applied \nClinical Informatics 2021, 12(04):808-15. \n11 Devlin J, Chang M-W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for \nlanguage understanding. arXiv preprint arXiv:1810.04805 2018. \n12 Hendrycks D, Mazeika M, Kadavath S, et al. Using self-supervised learning can improve \nmodel robustness and uncertainty. Advances in Neural Information Processing Systems 2019, \n32. \n13 Radford A, Kim JW, Hallacy C, et al. Learning transferable visual models from natural \nlanguage supervision. In: International Conference on Machine Learning. PMLR; 2021: 8748-\n63.14 Steinberg E, Jung K, Fries JA, et al. Language models are an effective representation \nlearning technique for electronic health record data. J Biomed Inform 2021, 113:103637. \n15 Li Y, Rao S, Solares JRA, et al. BEHRT: transformer for electronic health records. Scientific \nreports 2020, 10(1):1-12. \n16 Rasmy L, Xiang Y, Xie Z, et al. Med-BERT: pretrained contextualized embeddings on large-\nscale structured electronic health records for disease prediction. NPJ digital medicine 2021, \n4(1):1-13. \n17 Datta S, Posada J, Olson G, et al. A new paradigm for accelerating clinical data science at \nStanford Medicine. arXiv preprint arXiv:2003.10534 2020. \n18 Hripcsak G, Duke JD, Shah NH, et al. Observational Health Data Sciences and Informatics \n(OHDSI): opportunities for observational researchers. Studies in health technology and \ninformatics 2015, 216:574. \n19 Voss EA, Makadia R, Matcho A, et al. Feasibility and utility of applications of the common \ndata model to multiple, disparate observational health databases. J Am Med Inform Assoc 2015, \n22(3):553-64. \n20 Pfohl SR, Foryciarz A and Shah NH. An empirical characterization of fair machine learning \nfor clinical risk prediction. J Biomed Inform 2021, 113:103621. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \n21 Reps JM, Schuemie MJ, Suchard MA, et al. Design and implementation of a standardized \nframework to generate and evaluate patient-level prediction models using observational \nhealthcare data. J Am Med Inform Assoc 2018, 25(8):969-75. \n22 Bodenreider O. The unified medical language system (UMLS): integrating biomedical \nterminology. Nucleic acids research 2004, 32(suppl_1):D267-D70. \n23 Choi E, Bahadori MT, Schuetz A, et al. Doctor ai: Predicting clinical events via recurrent \nneural networks. In: Machine learning for healthcare conference. PMLR; 2016: 301-18. \n24 Morin F and Bengio Y. Hierarchical probabilistic neural network language model. In: \nInternational workshop on artificial intelligence and statistics. PMLR; 2005: 246-52.25 Paszke A, \nGross S, Massa F, et al. Pytorch: An imperative style, high-performance deep learning library. \nAdvances in neural information processing systems 2019, 32. \n26 Pedregosa F, Varoquaux G, Gramfort A, et al. Scikit-learn: Machine learning in Python. the \nJournal of machine Learning research 2011, 12:2825-30. \n27 Van Rossum G and Drake F. Python language reference, version 3.8. Python Software \nFoundation 2019. \n28 Siblini W, Fréry J, He-Guelton L, et al. Master your metrics with calibration. In: International \nSymposium on Intelligent Data Analysis. Springer; 2020: 457-69. \n29 Austin PC and Steyerberg EW. The Integrated Calibration Index (ICI) and related metrics for \nquantifying the calibration of logistic regression models. Statistics in medicine 2019, \n38(21):4051-65. \n30 Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach. In arXiv \npreprint arXiv:1907.11692 2019. \n31 Taori R, Dave A, Shankar V, et al. Measuring Robustness to Natural Distribution Shifts in \nImage Classification. Advances in Neural Information Processing Systems 33 2020: \narXiv:2007.00644. \n32 Brown T, Mann B, Ryder N, et al. Language models are few-shot learners. Advances in \nneural information processing systems 2020, 33:1877-901. \n33 Xie SM, Kumar A, Jones R, et al. In-N-Out: Pre-Training and Self-Training using Auxiliary \nInformation for Out-of-Distribution Robustness. In arXiv preprint 2020: arXiv:2012.04550. \n34 Bardes A, Ponce J and LeCun Y. VICReg: Variance-Invariance-Covariance Regularization \nfor Self-Supervised Learning. In arXiv preprint 2021: arXiv:2105.04906. \n35 Chen T, Kornblith S, Norouzi M, et al. A Simple Framework for Contrastive Learning of \nVisual Representations. In International conference on machine learning 2020: \narXiv:2002.05709. \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nFigure 1. An overview of the two approaches of constructing patient representations \nused in this study. The purple box in the construction of count-based representations \nrepresents the reference range comparison and binary feature construction procedures \nfor a specific time-bin. The construction of CLMBR illustrates the self-supervised \npretraining stage, hence the inclusion of the self-supervised learning objective. The \nconstruction of CLMBR for the purpose of transfer learning (e.g., for predicting hospital \nmortality) does not include the self-supervised learning objective. \nAbbreviations: CLMBR: clinical language model-based representations. \n  \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nFigure 2. The impact of temporal distribution shift on the performance (AUROC, \nAUPRC, and ACE) of logistic regression models trained on count-based \nrepresentations. Shaded regions indicate time windows in which performance in out-of-\ndistribution years (2013-2021) is worse (red) or better (green) than performance in the \nin-distribution year group (2009-2012). Oracle models were trained and evaluated on \neach of the out-of-distribution years. Error bars indicate 95% confidence interval \nobtained from 1000 bootstrap iterations.  \nAbbreviations: AUROC: area under the receiver operating characteristics curve; \nAUPRC: area under the precision recall curve; ACE: absolute calibration error; LOS: \nlength of stay; ICU: intensive care unit.  \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nFigures 3. Change in performance (AUROC, AUPRC, and ACE) across representation \nconstruction and modeling approaches relative to count-based logistic regression \nmodels (Count-LR) in 2009-2012 (09-12), 2013-2016 (13-16), and 2017-2021 (17-21). \nPerformance of count-LR was subtracted to obtain relative performance. Green regions \nindicate the range of values that is better with respect to count-LR, and red regions \nindicate the range of values that is worse. Error bars indicate 95% confidence interval \nobtained from 1000 bootstrap iterations. Supplementary Tables 2 and 3 detail the \nstatistical results.  \nAbbreviations: AUROC: area under the receiver operating characteristics curve; \nAUPRC: area under the precision recall curve; ACE: absolute calibration error; LOS: \nlength of stay; ICU: intensive care unit; CLMBR: clinical language model-based \nrepresentation; GRU: gated recurrent unit; Trans: transformer; LR: logistic regression; \nETE: end-to-end. \n \n \n  \ns \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nFigure 4. Relative out-of-distribution (OOD) performance (AUROC, AUPRC, and ACE) \nacross representation construction and modeling approaches. Each relative \nperformance was obtained by subtracting the model’s OOD performance in 2017-2021 \n(17-21) by its in-distribution (ID) performance in 2009-2012 (09-12) represented by the \nsolid line. Colored regions indicate the range of values that is worse (red) or better \n(green) with respect to the relative OOD performance of logistic regression models \ntrained on count-based representations. Error bars indicate 95% confidence interval \nobtained from 1000 bootstrap iterations. Supplementary Table 5 details the statistical \nresults.  \nAbbreviations: AUROC: area under the receiver operating characteristics curve; \nAUPRC: area under the precision recall curve; ACE: absolute calibration error; LOS: \nlength of stay; ICU: intensive care unit; CLMBR: clinical language model-based \nrepresentation; GRU: gated recurrent unit; Trans: transformer; LR: logistic regression; \nETE: end-to-end. \n \n \n  \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nFigure 5. Correlation between the GRU sequence model’s validation performance and \nthe performance of the downstream logistic regression models in each clinical prediction\ntasks. Performance for both the sequence model and the logistic regression model were \nmeasured using binary cross entropy loss. Each point in the scatter plot represents the \nGRU’s performance in the validation set and its downstream logistic regression model’s \nperformance in the test set. Each GRU sequence model was pretrained using a \ndifferent hyperparameter setting from the hyperparameter grid. Shaded error envelope \nrepresents the 95% confidence interval around the regression line.  \nAbbreviations:  GRU: gated recurrent unit; LOS: length of stay; ICU: intensive care unit.\n \n \n \n  \non \nre \n’s \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint \nFigure 6. The proportion of patients correctly and incorrectly re-classified by \nCLMBRGRU. Green bars indicate the proportion of patients that were incorrectly \nclassified by count-LR but were correctly re-classified by CLMBRGRU. Orange bars \nindicate the proportion of patients that were correctly classified by count-LR but were \nincorrectly re-classified by CLMBRGRU. Together, these represent the percentage of \ndecisions that would have been affected if CLMBRGRU models were used instead of \ncount-LR.  \nAbbreviations:  count-LR: logistic regression models trained on count-based \nrepresentations; CLMBR\nGRU: logistic regression models trained on gated recurrent unit-\nbased CLMBR. CLMBR: clinical language model-based representations; LOS: length of \nstay; ICU: intensive care unit. \n \n \nof \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2022. ; https://doi.org/10.1101/2022.04.15.22273900doi: medRxiv preprint "
}