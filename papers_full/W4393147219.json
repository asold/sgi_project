{
  "title": "ProAgent: Building Proactive Cooperative Agents with Large Language Models",
  "url": "https://openalex.org/W4393147219",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3094334018",
      "name": "Ceyao Zhang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2100326591",
      "name": "Kaijie Yang",
      "affiliations": [
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2126281837",
      "name": "Siyi Hu",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2127972637",
      "name": "Zihao Wang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Beijing Institute for General Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2108177453",
      "name": "Guanghe Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2158111498",
      "name": "Yihang Sun",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2101963880",
      "name": "Cheng Zhang",
      "affiliations": [
        "Peking University",
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2108076753",
      "name": "Zhaowei Zhang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Beijing Institute for General Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2527181777",
      "name": "Anji Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2655487545",
      "name": "Song Chun Zhu",
      "affiliations": [
        "Beijing Institute for General Artificial Intelligence",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2145609423",
      "name": "Xiaojun Chang",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2112753947",
      "name": "Junge Zhang",
      "affiliations": [
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1975390783",
      "name": "Feng Yin",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2152053742",
      "name": "Yitao Liang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2113349909",
      "name": "Yaodong Yang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A3094334018",
      "name": "Ceyao Zhang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2100326591",
      "name": "Kaijie Yang",
      "affiliations": [
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2126281837",
      "name": "Siyi Hu",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2127972637",
      "name": "Zihao Wang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2108177453",
      "name": "Guanghe Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2158111498",
      "name": "Yihang Sun",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2101963880",
      "name": "Cheng Zhang",
      "affiliations": [
        "Peking University",
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2108076753",
      "name": "Zhaowei Zhang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2527181777",
      "name": "Anji Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2655487545",
      "name": "Song Chun Zhu",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2145609423",
      "name": "Xiaojun Chang",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2112753947",
      "name": "Junge Zhang",
      "affiliations": [
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1975390783",
      "name": "Feng Yin",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2152053742",
      "name": "Yitao Liang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2113349909",
      "name": "Yaodong Yang",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970894611",
    "https://openalex.org/W4307417992",
    "https://openalex.org/W4283218993",
    "https://openalex.org/W3156295478",
    "https://openalex.org/W3122267274",
    "https://openalex.org/W2993490502",
    "https://openalex.org/W6792091866",
    "https://openalex.org/W4366340846",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W4320343013",
    "https://openalex.org/W4296414573",
    "https://openalex.org/W4221140624",
    "https://openalex.org/W6797779983",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2794643322",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4297161808",
    "https://openalex.org/W6802513241",
    "https://openalex.org/W2041367235",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3213030159",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W6846739993",
    "https://openalex.org/W4281622133",
    "https://openalex.org/W3148143011",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4286748781",
    "https://openalex.org/W2991046523",
    "https://openalex.org/W4225536362",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W4382239238",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4383473935",
    "https://openalex.org/W2980061931",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4308014717",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3107615218",
    "https://openalex.org/W3123636359",
    "https://openalex.org/W4295598622",
    "https://openalex.org/W4379540388",
    "https://openalex.org/W4200632533",
    "https://openalex.org/W4377131114",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4321011670",
    "https://openalex.org/W4362656036",
    "https://openalex.org/W3172688350",
    "https://openalex.org/W4286902222",
    "https://openalex.org/W4388650961",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W3040507763",
    "https://openalex.org/W4366731969",
    "https://openalex.org/W4389520747",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4394662461"
  ],
  "abstract": "Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents' capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer the intentions of teammates from observations. It then updates its beliefs in alignment with the teammates' subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding 10% compared to the current state-of-the-art method. For more information about our project, please visit https://pku-proagent.github.io.",
  "full_text": "ProAgent: Building Proactive Cooperative Agents with Large Language Models\nCeyao Zhang1,2*†, Kaijie Yang3*, Siyi Hu4*, Zihao Wang2,5, Guanghe Li2, Yihang Sun2,\nCheng Zhang2, Zhaowei Zhang2,5, Anji Liu2, Song-Chun Zhu5, Xiaojun Chang4, Junge Zhang3,\nFeng Yin1, Yitao Liang2, Yaodong Yang2‡\n1SSE, The Chinese University of Hong Kong, Shenzhen\n2Institute for Artificial Intelligence, Peking University\n3Institute of Automation, Chinese Academy of Sciences\n4ReLER, AAII, University of Technology Sydney\n5National Key Laboratory of General Artificial Intelligence, BIGAI\nceyaozhang2@link.cuhk.edu.cn, yaodong.yang@pku.edu.cn\nAbstract\nBuilding agents with adaptive behavior in cooperative tasks\nstands as a paramount goal in the realm of multi-agent sys-\ntems. Current approaches to developing cooperative agents\nrely primarily on learning-based methods, whose policy gen-\neralization depends heavily on the diversity of teammates\nthey interact with during the training phase. Such reliance,\nhowever, constrains the agents’ capacity for strategic adap-\ntation when cooperating with unfamiliar teammates, which\nbecomes a significant challenge in zero-shot coordination\nscenarios. To address this challenge, we propose ProAgent,\na novel framework that harnesses large language models\n(LLMs) to create proactive agents capable of dynamically\nadapting their behavior to enhance cooperation with team-\nmates. ProAgent can analyze the present state, and infer\nthe intentions of teammates from observations. It then up-\ndates its beliefs in alignment with the teammates’ subse-\nquent actual behaviors. Moreover, ProAgent exhibits a high\ndegree of modularity and interpretability, making it easily in-\ntegrated into various of coordination scenarios. Experimen-\ntal evaluations conducted within the Overcooked-AI envi-\nronment unveil the remarkable performance superiority of\nProAgent, outperforming five methods based on self-play and\npopulation-based training when cooperating with AI agents.\nFurthermore, in partnered with human proxy models, its per-\nformance exhibits an average improvement exceeding 10%\ncompared to the current state-of-the-art method. For more in-\nformation about our project, please visit https://pku-proagent.\ngithub.io.\nIntroduction\nLarge Language Models (LLMs) have rapidly emerged as\npowerful tools, achieving remarkable advancements across\nvarious domains, including long conversations (Ouyang\net al. 2022), reasoning (Bubeck et al. 2023), and text genera-\ntion (Brown et al. 2020). These models, by leveraging a vast\n*These authors contributed equally.\n†Work done when Ceyao Zhang visited Peking University.\n‡Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\namount of training data, can capture and embody a signifi-\ncant amount of common sense knowledge. Notable LLM-\nbased agents like SayCan (Ahn et al. 2022), ReAct (Yao\net al. 2023), DEPS (Wang et al. 2023b), RAP (Hao et al.\n2023), Reflexion (Shinn et al. 2023), and JARVIS-1 (Wang\net al. 2023c) have demonstrated the ability to make deci-\nsions interactively through appropriate prompts or feedback.\nHowever, these works have primarily focused on explor-\ning the potential of LLMs as individual agents, whether in\ngames or robotics. The untapped potential lies in investigat-\ning how LLM-based agents can effectively cooperate with\nother AI agents or humans.\nThis research delves into the capabilities of LLMs in\ntackling the intricate challenges of multi-agent coordina-\ntion (Yang and Wang 2021; Zhang, Yang, and Bas ¸ar 2021;\nGronauer and Diepold 2022), particularly in the realm of\npolicy generalization (Strouse et al. 2021; Zhao et al. 2023;\nLi et al. 2023b, 2024). Current approaches (Carroll et al.\n2019; Jaderberg et al. 2017; Strouse et al. 2021; Zhao et al.\n2023; Li et al. 2023b, 2024) to developing cooperative\nagents rely primarily on learning-based methods, whose pol-\nicy generalization depends heavily on the diversity of team-\nmates they interact with during the training phase. Such re-\nliance, however, constrains the agents’ capacity for strate-\ngic adaptation when cooperating with unfamiliar teammates,\nwhich becomes a significant challenge in zero-shot coordi-\nnation scenarios. We present ProAgent, an innovative and\nadaptable framework specifically designed to excel in coor-\ndination scenarios alongside novel agents. ProAgent com-\nprises four essential modules: Planner, Verificator,\nController and Memory, along with the mechanism of\nBelief Revision. These modules synergistically en-\nable ProAgent to actively predict teammates’ intentions and\nachieve adaptive cooperative reasoning and planning with-\nout the need for prior training or finetuning. To assess\nthe adaptive cooperative capabilities of ProAgent, we con-\nducted performance evaluations using the well-established\nmulti-agent coordination testing suite, Overcooked-AI (Car-\nroll et al. 2019). In this environment, two players must work\ntogether to maximize their score. The empirical findings\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17591\nmemory\nYou (Alice): hand hold nothing. \nBob: hand holds nothing.\nKitchen states: pot empty\nBob will choose to pickup(onion).\nTarget: cook soup and deliver…\nSkill pool: pickup(onion), cook \nsoup…\nReflection Loop\nstate\naction\nenvironment\nlanguage state\nbelief\nplan\nverificator\ncontroller\nstore\nProAgent\nloop\nknowledge\nstate\nbelief\nPlan Validation\nAchieved!\nFail?\nverificator\nProAgent\nPath \nPlannercontroller\nteammate\nknowledge\nfeedback\nfeedback\nIntention of Bob: “\npickup(dish)”\nPlan of you: “\npickup(onion)”\nmemory\n Bob\nBelief RevisionReal behavior of Bob: “\npickup(onion)”\nI succeeded at pickup(onion)\nError message: xxx\nPlease analysis again and \nreplan the skill based on \nthe current scene.\nAnalysis: The pot is empty. Both \nYou and Bob have empty hands. \nPrompt\nFigure 1: Overview of our proposed ProAgent framework including the coordination task workflow (left) and inner details of\nProAgent pipeline (right). The teammate agent’s decision-making loop is formed by the blue solid arrows in the outer circle,\nwhile the decision process of the ProAgent is formed by the middle gray dotted box and the outer gray solid arrows. ProAgent\ncommences its operation by translating the initial state into natural language. Then thePlanner adeptly analyzes the provided\nlanguage state in conjunction with historical information stored in the Memory. This analytical process allows the model to\ndiscern the intentions of the teammate and devise a high-levelskill for the agent accordingly. The belief about predicted intention\nwill be updated through theBelief Revision mechanism, which involves comparing it with the subsequent actual behavior\nof the teammate agent. As to the planned skill, the Verificator validates whether it can be performed under the current\nstate. In case of skill failure, the Verificator will assess the skill’s preconditions and provide a detailed explanation for the\nencountered issue. Should the need arise, ProAgent enters into a re-plan loop, initiating a recalibration process. On the other\nhand, if the skill is deemed viable, the Controller further dissects it into several executive low-levelactions, to be executed\nwithin the environment.\nfrom our evaluations reveal the following key insights: 1)\nProAgent demonstrates remarkable proficiency in coordinat-\ning with various types of AI teammates across diverse sce-\nnarios. 2) ProAgent exhibits a notable preference for collab-\norating with rational teammates, such as the human proxy,\nwhich showcases human-like behavior and suggests its ac-\ntive effort to understand teammates’ intentions to enhance\ncooperation. These results collectively highlight the effec-\ntiveness of ProAgent as a cooperative agent across a wide\nrange of scenarios.\nIn summary, our work makes three key contributions:\nFirstly, we successfully integrate LLMs into the field of\ncooperative multi-agents and propose the ProAgent frame-\nwork, which serves as a comprehensive guideline for lever-\naging the powerful reasoning and planning capabilities of\nLLMs in cooperative settings. Secondly, we demonstrate\nthe remarkable capability of our ProAgent to interpretably\nanalyze the current scene, explicitly infer teammates’ inten-\ntions, and dynamically adapt its behavior accordingly. This\nproactive nature empowers ProAgent to actively collaborate\nwith teammates, enabling more efficient cooperative sce-\nnarios. Thirdly, through a comprehensive series of experi-\nments, we provide compelling evidence of ProAgent’s supe-\nriority over other agents when engaging in cooperation with\ndiverse types of teammates.\nRelated Works\nReasoning and Planning with Large Language Models.\nIn the realm of LLMs (Huang and Chang 2023; Mialon et al.\n2023; Bubeck et al. 2023), reasoning often entails decom-\nposing intricate queries into sequential intermediate steps,\nreferred to as Chain-of-Thought (CoT; Wei et al. 2022; Ko-\njima et al. 2022), to attain a final solution. Some research\nfocuses on minimizing errors as the number of steps in-\ncreases (Wang et al. 2023a), while others explore decom-\nposition techniques that break down complex problems into\nsimpler subproblems (Zhou et al. 2023). Recent endeavors\nhave translated LLMs’ reasoning capability into planning\nby constructing a monologue with feedback (Welleck et al.\n2023; Shinn et al. 2023; Paul et al. 2023) to facilitate the\nreasoning and planning process. Notably, the challenge of\nopen-ended long-term planning in the MineDojo environ-\nment (Fan et al. 2022) has been addressed by utilizing LLMs\nas central planners (Wang et al. 2023b,c), thereby demon-\nstrating the extensive capabilities of LLMs-based agents\nin overcoming complex decision-making tasks. As of the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17592\ncamera-ready version of our paper, the application of LLM-\nbased agents in cooperative games remains little explored.\nLi et al. (2023a) employs a centralized LLM-based planner\nfor both two players. In contrast, our framework adopts a de-\ncentralized planning paradigm and uses one planner for one\nplayer. While Zhang et al. (2023) also decentralized plan-\nning, their method facilitates cooperation through explicit\ncommunication. Our work, on the other hand, fosters coop-\neration by observing and inferring the intentions of team-\nmates.\nMulti-agent Coordination. The goal of multi-agent coor-\ndination is to enable multiple autonomous agents to collabo-\nrate effectively towards a shared goal (Rashid et al. 2018; Hu\nand Foerster 2020; Hu et al. 2021a; Yu et al. 2022; Zhong\net al. 2023). However, traditional approaches have limita-\ntions in fixed task settings and struggle to handle multiple\ntasks or unseen scenarios. One approach to address this chal-\nlenge is to enable an agent to learn multiple tasks concur-\nrently (Hu et al. 2021b; Meng et al. 2022; Wen et al. 2022).\nHowever, these methods may still limit the agent’s coop-\neration ability in familiar tasks and fail to handle unseen\ntasks or new agent interactions. Another line of research fo-\ncuses on zero-shot coordination (ZSC), utilizing Population-\nBased Training (PBT; Strouse et al. 2021; Zhao et al. 2023;\nLupu et al. 2021; Lucas and Allen 2022; Li et al. 2023b,\n2024) and Theory of Mind (ToM; Hu et al. 2021a; Wu et al.\n2021; Wang et al. 2021) to facilitate adaptive policy devel-\nopment for coordinating with various counterparts without\nprior coordination experience. However, these ZSC methods\ndemand significant computational resources for data collec-\ntion and model optimization, and the resulting policies often\nlack interpretability.\nMethod\nThe overview of our ProAgent framework, as is depicted in\nFig. 1, involves constant interaction between agents and the\nenvironment. The inference pipeline of ProAgent is a hier-\narchical process that involves multiple interactions between\nthe LLMs and the task at hand. We break down the pipeline\ninto five key stages:\nKnowledge Library and State Grouding. The pipeline\nstarts with acquiring Knowledge Library specific to the cur-\nrent task and transforming the raw tensor state information\ninto Language-based State description that the LLM can ef-\nfectively comprehend.\nHigh-level Skill Planning. Receiving the aligned\nlanguage-based state, the LLM-based Planner then ana-\nlyzes the current scene, infers the intentino about the team-\nmate agent’s intentions, and plans a skill for the current\nagent.\nBelief Revision. The belief in the teammate agent’s inten-\ntion is further corrected by theBelief Revision mech-\nanism.\nSkill Validation and Action Execution. The selected\nskill will be validated by the Verificator and a replan\nis needed if the current skill fails. If a valid skill is selected,\nand the Controller module decomposes it into low-level\nactions, allowing ProAgent to effectively interact with the\ntask or environment. The controller can be rule-based, or\nRL-based methods.\nMemory Storage. Throughout the pipeline, all relevant\ninformation involved in the prompt, planning process, val-\nidation process, and belief revision process is stored in\nthe Memory module. This accumulated knowledge helps\nin making informed decisions and adjusting behavior over\ntime.\nPrompt Construction\nKnowledge library The planning ability of LLMs is\nclosely related to the prompt at the beginning, which is\nalso the standard practice in automated planning. ProA-\ngent is no exception, and the knowledge library should be\nfed into LLMs at the initial stage before the cooperation\ntask begins. The main difficulty lies in how to build struc-\ntured knowledge. In practice, we find that the best com-\nbination of knowledge library needs to be described from\nthree perspectives, including Instructions, Skills,\nand Examples.\nFigure 2: A template to construct the knowledge library.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17593\nAs shown in Fig 2, Instructions are for LLMs to\nunderstand the objective of the task and information about\nother cooperative agents. Skills is designed to regulate\nthe planning pattern of the LLM, defining which skills are le-\ngal and which are not. We also enforce the format of LLMs’\nresponses to follow the CoT: output analysis and then plan\naccording to the analysis instead of directly outputting a\nplan. Examples is an optional component of the three.\nIts main functionality is to provide real cases for LLMs to\nstrengthen their memories and behave following the regula-\ntions set by Skills. Normally, Examples should contain\na scene description followed by an analysis and the desired\nbehavior, such as the selected skill. With these three parts,\nLLMs can understand the task and what is expected of them\nin the subsequent planning and reasoning stages.\nGrounding tensor state to language-based state To fa-\ncilitate interaction between LLMs and the environment, it is\nessential to establish a bridge between the original symbolic\nstate provided by the environment and the language-based\nstate for LLMs. In most scenarios, the raw state is not di-\nrectly applicable to LLMs’ usage. Hence, finding an effec-\ntive alignment between the original symbolic state and the\nlanguage-based state is crucial to enhancing LLMs’ accu-\nrate understanding of the current situation. To illustrate this,\nwe present a simplified example based on the Overcooked-\nAI environment, demonstrating how the state can be trans-\nformed into language within our ProAgent framework. With\nthe knowledge library and initial state information prepared,\nProAgent is equipped to tackle the cooperative task along-\nside its teammates. This marks the transition to the subse-\nquent stage, where ProAgent engages in reasoning and plan-\nning, progressing step by step to achieve its objectives. An\nillustrative instance can be found in Fig 3.\nFigure 3: Grounding the tensor state to language-based state.\nCooperative Reasoning and Planning\nProAgent is a specialized system tailored for cooperative\ntasks, where information from teammate agents plays a piv-\notal role in the coordination process. Existing works mainly\nutilize information in two ways: firstly, through explicit in-\ncorporation, involving communication and exchange of in-\nformation before decision-making; secondly, through im-\nplicit modeling of teammate agents to facilitate cooperative\nlearning. Each approach comes with its own set of advan-\ntages and disadvantages concerning cooperative reasoning\nand planning: The integration of teammate information can\nbe achieved efficiently by sending teammate agent informa-\ntion to LLMs. However, this approach may jeopardize the\noverall generalization of ProAgent’s reasoning capabilities.\nOn the other hand, modeling the teammate agent offers a\nmore flexible approach, while the modeling process is in-\nherently unstable as the teammate agent’s strategy may con-\ntinuously evolve, demanding additional resources for main-\ntenance.\nIn order to strike a balance between the generalization\nability of built agents and the efficiency of incorporating\nteammate information, particularly for LLMs that possess\nexcellent reasoning capabilities but face challenges in fine-\ntuning or learning extra belief modules, ProAgent introduces\nthree core components along with a cooperative reasoning\nand planning mechanism. The three modules encompass:\n1) The Memory module, which stores information about\ntask trajectory and general knowledge in the task domain.\n2) The Verificator module, consisting of one compo-\nnent for skill failure analysis and another for transforming\nskills into atomic actions. 3) The Controller module,\ndedicated to the transformation of skills into atomic actions.\nTo further align the LLMs’ belief regarding the teammate\nagent’s intentions with actual behavior, and thereby contin-\nually enhance prediction accuracy, ProAgent implements the\nBelief Revision mechanism. This process effectively\nstrengthens the LLMs’ beliefs, leading to improved cooper-\native reasoning and planning.\nMemory Module: Leveraging History for Cooper-\native Behavior In ProAgent, the Memory module\nplays a crucial role in supporting information stor-\nage and retrieval processes. It consists of two compo-\nnents: Knowledge Library and Trajectory. The\nKnowledge Library acts as a persistent repository, re-\ntaining a comprehensive record of the task, including its lay-\nout, rules, and demonstrations throughout game play ses-\nsions. On the other hand, the Trajectory component\nserves as a python list. It stores essential information, such\nas the latest Language-based State, Analysis,\nBelief of teammates’ intentions, and the Skill used.\nWhen needed, only specific parts of the Memory are re-\ntrieved, depending on the chosen strategy, such as the\nrecent-K strategy1 or relevent-K strategy2. Those\nstrategies focus on the immediate context, facilitating effi-\ncient decision-making and planning during ongoing interac-\ntions. Overall, the Memory module significantly enhances\nProAgent’s capacity to access pertinent information and co-\noperate efficiently with teammate agents. By leveraging past\nexperiences and learning from historical data, the Memory\n1only retrieve the K most recent trajectories.\n2retrieve the most releventK trajectories based on their embed-\nding similarity.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17594\nmodule empowers ProAgent to make informed decisions\nduring cooperation tasks.\nPlanner Module: Reasoning with Chain of Thought\nWith the history information and current state descrip-\ntion ready, ProAgent utilizes the strong reasoning ability\nof LLMs to make decisions in the current situation. The\nPlanner module, which follows the Chain of Thought\n(CoT) approach commonly used in LLMs’ reasoning and\nplanning work (Yao et al. 2023; Hao et al. 2023; Shinn et al.\n2023). Instead of directly outputting a plan, the Planner\nmodule makes the final decision step by step. The provided\ninformation is first thoroughly analyzed, and the intention of\nthe teammate agent’s plan for the current step is predicted.\nBased on this Analysis and the Belief about the team-\nmate agent, LLMs formulate a plan that ensures it is the\nmost reasonable and effective strategy for the given situa-\ntion. In the experiment part, we design three level prompts\n(L1: directly planning without analysis and intention; L2:\nwith analysis but no intention; L3: with both analysis and\nintention) and conduct an ablation study to assess how this\ndesign enhances ProAgent’s performance in a cooperative\nscenario.\nVerificator Module: Analyzing Skill Failures With\nMulti-rounds Prompts In the cooperative setting, the\nVerificator module plays a crucial role in scrutinizing\nand identifying any unreasonable or flawed planning gener-\nated by the LLMs. Its primary function involves analyzing\nthe underlying reasons for these inadequacies and providing\nvaluable insights and suggestions for improvement. In the\nProAgent framework, this process entails conducting a thor-\nough investigation through multiple rounds of prompt and\nresponse between the agent and the LLMs.\nTo illustrate this process, we first employ a three-\nround prompt and response approach, including\nPreconditions Check, Double-check and\nError Conclusion. It’s important to note that the\nnumber of rounds or the specific interaction style is not\nrestricted, and we found in experiments that usually one\nround prompt is enough.\nThe Preconditions Check involves signaling the\nLLMs if the current plan is illegal due to internal checks be-\nfore its actual execution. A robust internal checking mecha-\nnism can prevent failures when the LLMs haven’t fully un-\nderstood the consequences of their chosen skill under the\ncurrent state. In the Overcooked-AI example, we design the\ncondition check prompt by leveraging both the current scene\nand the failed skill as inputs. We employ a trigger prompt to\nenable the LLMs to individually verify each precondition of\nthe skill and pinpoint the specific one that led to the failure.\nTo aid in solving multi-step reasoning problems, prompt-\ning techniques like CoT are also adopted. An instance of\nthe trigger prompt in Overcooked-AI could be: ”Analysis of\nwhy I cannot execute this skill in the current scene step by\nstep.” or just ”Why did Player 0 fail ?” The preconditions of\neach skill can be expressed either in natural language or in\npseudo-code form, which can be more effective as proposed\nin previous works (Liang et al. 2023; Singh et al. 2023).\nBelief revision: Rectifying Belief on Teammate Agents\nThe Belief Revision mechanism plays a pivotal role\nin rectifying any incorrect beliefs during cooperation. ProA-\ngent makes predictions about their teammates’ future behav-\nior and stores relevant analyses in their memory. At the be-\nginning, the observed behavior of the teammate agent may\ndeviates from the assumed intentions recorded in Memory.\nIn subsequent steps, ProAgent verifies the accuracy of their\npredictions and corrects any erroneous beliefs. Specifically,\nthe Belief Revision mechanism works to remember\nall past infered intentions and really behaviors. The really\nbehaviors enforces ProAgent to learn from ground truth,\nwhich help it to revise the wrong belief, thereby avoid-\ning similar mistakes in the future. When we use L3 level\nprompts with revision, those information will be added into\nthe prompts for next query. This iterative process allows\nProAgent to refine their beliefs over time and enhance their\nability to make accurate predictions about their teammate’s\nintentions. In summary, the Belief Revision mecha-\nnism ensures that ProAgent maintains accurate and up-to-\ndate information about their teammate agent’s real behavior.\nBy referencing the Belief part of Memory before mak-\ning decisions, ProAgent continually improves the accuracy\nof their beliefs regarding their teammate’s future behavior.\nController Module: Grounding High-Level Skills to\nLow-Level Actions Based on the modules and mecha-\nnisms discussed above, ProAgent effectively engages in co-\noperative reasoning and plans a high-level skill. However, it\nis worth noting that there is a gap between the skill space\nand the environment’s action space. Therefore, we also need\na Controller module which is imperative, aiming to con-\nvert language-based skills into low-level actions that can\nbe executed in the environment. Although this transforma-\ntion process is closely tied to the specific task at hand,\nmaking the Controller module highly flexible, it ne-\ncessitates the establishment of fixed rules capable of de-\ncomposing the skill into multiple steps of low-level actions\nand providing a feedback signal to the reasoning compo-\nnent once the action is fully executed. The controller can\nbe a rule-based path search algorithm or a policy trained by\nlanguage-grounded reinforcement learning (Hanjie, Zhong,\nand Narasimhan 2021; Ding et al. 2023; Hu and Sadigh\n2023; Du et al. 2023) methods. Considering that the con-\ntroller is not our main concern, we choose the built-in con-\ntroller in the Overcooked-AI environment , which is imple-\nmented based on the search strategy. On this basis, we made\nsmall improvements so that when a road blockage is found,\na new path will be searched again. A better controller can\ndefinitely reach better performance.\nExperiments\nExperimental Settings\nFollowing previous works on cooperative AI and human-\nAI cooperation, we choose Overcooked-AI as our test en-\nvironment, in which two agents swiftly prepare and serve\nsoups by placing up to three ingredients in a pot, cook-\ning the soup, filling the soup with the dish, and delivering\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17595\nLayout Baseline\nAI Agents ProAgent (ours)SP PBT\nFCP MEP COLE\nCramped Room 168\n.5 ± 15.2 178. 8 ± 16.5 196. 3 ± 16.8 185 ± 15 163.8 ± 24.1 197.3 ± 6.1\n172.8 ± 16.1 179. 8 ± 26.8 196 ± 11.9 178. 2 ± 15.6 169. 2 ± 16.8 194. 2 ± 10.5\nAsymmetric Adv\nantages 183.3 ± 27.5 182. 2 ± 27.9 185. 7 ± 22.7 155. 7 ± 63.9 201. 3 ± 34.5 228.7 ± 23\n177.8 ± 24.6 152. 3 ± 64.5 167. 8 ± 21.3 184 ± 41.8 165. 5 ± 33.3 229.8 ± 21.9\nCoordination Ring 122 ± 17\n.2 141.3 ± 28 148. 8 ± 19.4 167. 2 ± 22.4 168. 8 ± 26.1 175.3 ± 29\n133.3 ± 23.7 141. 3 ± 27.5 145. 7 ± 17.1 159. 3 ± 25.3 158. 3 ± 27.1 183 ± 31.7\nFor\nced Coordination 6.7 ± 6.7 15. 3 ± 17.1 44. 7 ± 36.4 23. 3 ± 19.8 24 ± 21.8 49.7 ± 33.1\n30.2 ± 21.9 61.7 ± 46 32. 2 ± 30.2 39. 3 ± 16.9 57. 3 ± 36.4 31 ± 33.9\nCounter Cir\ncuit 64.7 ± 45.8 64. 7 ± 45.9 58. 3 ± 37.5 74. 3 ± 39.1 95. 5 ± 25.2 126.3 ± 32.3\n60.7 ± 40.8 54. 3 ± 49.1 60 ± 38.3 81. 5 ± 27.5 100. 8 ± 31.1 128.5 ± 28.1\nTable 1: Performance for all AI agent pairs. Each column represents the average reward and standard error of one algorithm\nplaying with all others. For each layout, the first row represents the scenario where the agent takes the role of Player 0, and\nthe AI partner takes the role of Player 1. The second row depicts the vice-versa scenario. The best results for each layout are\nhighlighted in bold.\nthe soup. Agents must dynamically allocate tasks and coop-\nerate effectively. Five classical layouts are used: Cramped\nRoom, Asymmetric Advantages, Forced Coordination, Co-\nordination Ring, andCounter Circuit. A detailed description\nof each layout can be found in the appendix.\nOur primary concern behind this work is how well the\nagents developed so far based on ZSC methods can cooper-\nate with diverse teammates, ranging from different AI agents\nto humans. In previous works on Overcooked-AI, the coop-\nerative performance of an agent is often evaluated with two\nheld-out populations: self-play (SP) agent and human proxy\nmodel. We conduct a comparative analysis between our pro-\nposed ProAgent and five alternatives prevalent in the field in-\ncluding SP (Tesauro 1994; Carroll et al. 2019), PBT (Jader-\nberg et al. 2017), FCP (Strouse et al. 2021), MEP (Zhao et al.\n2023), and COLE (Li et al. 2023b, 2024). We combined the\nabove six algorithms in pairs to construct 36 pairs. For ex-\nample, we choose the SP algorithm as player 0 and the PBT\nalgorithm as player 1, and these two algorithms can form an\nagent pair (SP, PBT). Since the two players are not all homo-\ngeneous, we will also form a (PBT, SP) algorithm pair. For\neach algorithm pair, we ran five episodes and collected the\nmean and standard variation of the episode returns. Besides,\nwe also select the human proxy model proposed by (Carroll\net al. 2019) to test the agent’s ability to cooperate with hu-\nmans. In the main experiments, we use L2 level prompts and\nrecent-1 strategy.\nCollaborating with AI Agents\nQuantitative Results Table 1 illustrates the average per-\nformance of SP, PBT, FCP, MEP, COLE, and ProAgent\nwhen paired with all the others. For each layout, the first\nrow represents the scenario where the agent takes the role of\nPlayer 0, and the AI partner takes the role of Player 1. The\nsecond row depicts the vice-versa scenario. The results indi-\ncate that ProAgent outperforms the baselines in all layouts\nwhen acting as Playe 0. Taking the role of Player 1, ProA-\ngent only slightly underperforms FCP in cramped room lay-\nout and loses to PBT in forced coordination layout. We will\nexamine this failure further in the appendix. In previous\nstudies, it is rare to compare different AI agent combina-\ntions with each other, and our experimental results also re-\nveal that none of the other ZSC methods is consistently bet-\nter than other methods. Considering that ProAgent requires\nno specific training with distinct teammates and in distinct\nlayouts, it presents a stronger adaptive ability than the other\nAI agents. These results show our LLM-based agent is a bet-\nter cooperator.\nQualitative Results To gain deeper insights into the fun-\ndamental components of effective cooperation, we perform a\nqualitative examination of our ProAgent’s behaviors exhib-\nited during our experiments, leading us to identify several\ncooperative behaviors.\nProAgent excels in making strategic plans. For example,\nwhen pot one is cooking and pot two lacks an onion, we\nobserved that ProAgent would prioritize putting one onion\ninto pot two. After this, the agent will fetch the plate. At\nthe same time, cooking can be completed in the first pot,\nand this agent with a plate can directly fill the plate with\nsoup. This process is very effective. Besides, after making a\nfailure plan, ProAgent can promptly recognize this failure,\nand make a new and often better plan.\nProAgent demonstrates a remarkable capacity to dy-\nnamically adjust low-level actions while executing high-\nlevel plans. For instance, when ProAgent intends to deposit\nan onion into a pot, it’s underlying Controller identi-\nfies a blocked path caused by its teammate. Swiftly, the\nController will identify an alternative interconnected\nroute, skillfully bypassing any potential obstructions. This\nadaptive strategy enables ProAgent to discover unhindered\npathways. Moreover, when Planner has no clear goal, the\nController will move randomly. This dynamic operation\nhelps ProAgent to break the deadlock caused by other AI\nagents due to conventions formed during the training phase.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17596\nFigure 4: Performance with human proxy partners. In each layout, the reward bar represents the average performance of one\nalgorithm collaborating with the unseen human proxy partners over 400 timesteps on five BC models, and the error lines\nrepresent the standard error. The hashed bars indicate the rewards obtained where the starting positions are switched.\nCollaborating with Humans\nApart from cooperation with AI agents, our concern also\ninvolves the generalization to human partners. Due to the\nlimitation of collecting human interaction data, we follow\nthe previous work (Carroll et al. 2019) that uses a behav-\nior cloning (BC) model trained on human data as a proxy\nof humans. Fig. 4 presents the average cumulative rewards\nachieved for 400 timesteps by ProAgent when engaged in\ncollaboration with BC. The reported outcomes encompass\nboth the mean value and standard error across five distinct\nBC models. Analysis of the experimental findings reveals\nthat across the five environments, ProAgent outperforms the\nbaseline in four environments, exhibiting particularly note-\nworthy superiority when functioning as Player 0 in the con-\ntext of Forced Coordination. Notably, the positioning dis-\ncrepancy between the left and right starting positions had a\nnegligible impact on ProAgent’s performance. However, this\ndifference led to substantial performance disparities among\nthe baselines, particularly in asymmetric layouts, where the\ncumulative rewards achieved by all baselines were superior\nin the left position compared to the right position, consistent\nwith the findings in COLE (Li et al. 2023b, 2024).\nDiscussion\nDoes analysis and belief help in better planning? To\ngauge the influence ofanalysis and intention on the accuracy\nand efficiency of decisions made by the Planner Mod-\nule, we conducted an ablation study within the context of\nthe Cramped Room layout. The experiment considered three\ndistinct conditions and their respective scores were: 1) 204\nfor L3 level prompts (with both analysis and intention), 2)\n184 for L2 level prompts (with analysis but no intention),\nand 3) 100 for L1 level prompts (making a skill plan directly,\nneither analysis nor intention). We believe that the signifi-\ncance of analysis in the Planner Module lies in its provi-\nsion of in-context for final planning just as CoT will improve\nthe effect of reasoning. Additionally, inferring teammate in-\ntentions provides further improvements.\nIs Verificator effective in feedback-based reasoning?\nUpon removing the Verificator Module and allowing\nProAgent to engage in planning without feedback, we com-\nputed success rates over 100 steps. Notably, the success rate\ndropped significantly to 20%, underscoring the critical role\nof our Verificator Module in furnishing feedback when\nthe Planner Module generates inaccurate plans.\nConclusion\nIn this work, we propose ProAgent, a proactive LLM-based\nagent framework, with the primary objective of address-\ning the multi-agent coordination predicament. By leverag-\ning the inherent faculties of LLMs encompassing common\nsense comprehension and language-centric task understand-\ning, coupled with explicit mechanisms for reasoning and\nplanning, ProAgent demonstrates remarkable performance\nwithin various coordination scenarios. Experiments on co-\noperating with both AI agents and human proxies in the\nOvercooked-AI demonstrate the effectiveness of ProAgent\nover state-of-the-art methods. Moreover, ProAgent’s reason-\ning and planning are based on natural language, which is\ninterpretable and friendly to humans. These encouraging re-\nsults pave the way for further advancements in both coop-\nerative multi-agent and human-compatible AI systems built\nupon LLMs.\nAcknowledgements\nThis work is sponsored by the National Natural Science\nFoundation of China (62376013), by the Basic Research\nProject No. HZQB-KCZYZ-2021067 of Hetao Shenzhen-\nHK S& T Cooperation Zone, Beijing Municipal Science\n& Technology Commission (Z231100007423015), by the\nShenzhen Outstanding Talents Training Fund 202002, by\nthe Guangdong Research Projects No. 2017ZT07X152\nand No. 2019CX01X104, by the Guangdong Provincial\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17597\nKey Laboratory of Future Networks of Intelligence (Grant\nNo. 2022B1212010001), by the Shenzhen Key Labora-\ntory of Big Data and Artificial Intelligence (Grant No.\nZDSYS201707251409055), by the NSFC under Grant No.\n62271433, and by Shenzhen Science and Technology Pro-\ngram under Grant No. JCYJ20220530143806016 and No.\nRCJC20210609104448114.\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes, O.;\nDavid, B.; Finn, C.; Fu, C.; Gopalakrishnan, K.; Hausman,\nK.; Herzog, A.; Ho, D.; Hsu, J.; Ibarz, J.; Ichter, B.; Irpan,\nA.; Jang, E.; Ruano, R. J.; Jeffrey, K.; Jesmonth, S.; Joshi,\nN. J.; Julian, R.; Kalashnikov, D.; Kuang, Y .; Lee, K.-H.;\nLevine, S.; Lu, Y .; Luu, L.; Parada, C.; Pastor, P.; Quiambao,\nJ.; Rao, K.; Rettinghouse, J.; Reyes, D.; Sermanet, P.; Siev-\ners, N.; Tan, C.; Toshev, A.; Vanhoucke, V .; Xia, F.; Xiao, T.;\nXu, P.; Xu, S.; Yan, M.; and Zeng, A. 2022. Do As I Can,\nNot As I Say: Grounding Language in Robotic Affordances.\narXiv:2204.01691.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language Models are Few-Shot Learners.\nIn Advances in neural information processing systems, vol-\nume 33, 1877–1901.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early Experiments\nwith GPT-4. arXiv:2303.12712.\nCarroll, M.; Shah, R.; Ho, M. K.; Griffiths, T.; Seshia, S.;\nAbbeel, P.; and Dragan, A. 2019. On the Utility of Learning\nabout Humans for Human-AI Coordination. In Advances in\nneural information processing systems, volume 32.\nDing, Z.; Zhang, W.; Yue, J.; Wang, X.; Huang, T.; and Lu,\nZ. 2023. Entity Divider with Language Grounding in Multi-\nAgent Reinforcement Learning. InInternational Conference\non Machine Learning, 8103–8119. PMLR.\nDu, Y .; Watkins, O.; Wang, Z.; Colas, C.; Darrell, T.;\nAbbeel, P.; Gupta, A.; and Andreas, J. 2023. Guiding Pre-\ntraining in Reinforcement Learning with Large Language\nModels. arXiv:2302.06692.\nFan, L.; Wang, G.; Jiang, Y .; Mandlekar, A.; Yang, Y .; Zhu,\nH.; Tang, A.; Huang, D.-A.; Zhu, Y .; and Anandkumar, A.\n2022. MineDojo: Building Open-Ended Embodied Agents\nwith Internet-Scale Knowledge. InNIPS Processing Systems\nDatasets and Benchmarks Track.\nGronauer, S.; and Diepold, K. 2022. Multi-Agent Deep Re-\ninforcement Learning: A survey. Artificial Intelligence Re-\nview, 1–49.\nHanjie, A. W.; Zhong, V . Y .; and Narasimhan, K. 2021.\nGrounding Language to Entities and Dynamics for Gener-\nalization in Reinforcement Learning. In International Con-\nference on Machine Learning, 4051–4062. PMLR.\nHao, S.; Gu, Y .; Ma, H.; Hong, J. J.; Wang, Z.; Wang, D. Z.;\nand Hu, Z. 2023. Reasoning with Language Model is Plan-\nning with World Model. arXiv:2305.14992.\nHu, H.; and Foerster, J. N. 2020. Simplified Action Decoder\nfor Deep Multi-Agent Reinforcement Learning. In Interna-\ntional Conference on Learning Representations.\nHu, H.; Lerer, A.; Cui, B.; Pineda, L.; Brown, N.; and Foer-\nster, J. 2021a. Off-Belief Learning. In International Confer-\nence on Machine Learning, 4369–4379. PMLR.\nHu, H.; and Sadigh, D. 2023. Language Instructed Rein-\nforcement Learning for Human-AI Coordination. In Pro-\nceedings of the 40th International Conference on Machine\nLearning. PMLR.\nHu, S.; Zhu, F.; Chang, X.; and Liang, X. 2021b. UPDeT:\nUniversal Multi-agent Reinforcement Learning via Policy\nDecoupling with Transformers. arXiv:2101.08001.\nHuang, J.; and Chang, K. C.-C. 2023. Towards Reasoning\nin Large Language Models: A Survey. arXiv:2212.10403.\nJaderberg, M.; Dalibard, V .; Osindero, S.; Czarnecki, W. M.;\nDonahue, J.; Razavi, A.; Vinyals, O.; Green, T.; Dun-\nning, I.; Simonyan, K.; Fernando, C.; and Kavukcuoglu,\nK. 2017. Population Based Training of Neural Networks.\narXiv:1711.09846.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large Language Models are Zero-Shot Reasoners.\nIn Advances in neural information processing systems, vol-\nume 35, 22199–22213.\nLi, W.; Qiao, D.; Wang, B.; Wang, X.; Jin, B.; and Zha, H.\n2023a. Semantically Aligned Task Decomposition in Multi-\nAgent Reinforcement Learning. arXiv:2305.10865.\nLi, Y .; Zhang, S.; Sun, J.; Du, Y .; Wen, Y .; Wang, X.; and\nPan, W. 2023b. Cooperative Open-ended Learning Frame-\nwork for Zero-shot Coordination. InProceedings of the 40th\nInternational Conference on Machine Learning. PMLR.\nLi, Y .; Zhang, S.; Sun, J.; Zhang, W.; Du, Y .; Wen, Y .; Wang,\nX.; and Pan, W. 2024. Tackling Cooperative Incompatibility\nfor Zero-Shot Human-AI Coordination. arXiv:2306.03034.\nLiang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,\nB.; Florence, P.; and Zeng, A. 2023. Code as Policies: Lan-\nguage Model Programs for Embodied Control. In 2023\nIEEE International Conference on Robotics and Automation\n(ICRA), 9493–9500. IEEE.\nLucas, K.; and Allen, R. E. 2022. Any-Play: An Intrinsic\nAugmentation for Zero-Shot Coordination. In International\nFoundation for Autonomous Agents and Multiagent Systems,\n853–861.\nLupu, A.; Cui, B.; Hu, H.; and Foerster, J. 2021. Trajec-\ntory Diversity for Zero-Shot Coordination. In International\nconference on machine learning, 7204–7213. PMLR.\nMeng, L.; Wen, M.; Yang, Y .; Le, C.; Li, X.; Zhang, W.;\nWen, Y .; Zhang, H.; Wang, J.; and Xu, B. 2022. Offline\nPre-trained Multi-Agent Decision Transformer: One Big Se-\nquence Model Tackles All SMAC Tasks. arXiv:2112.02845.\nMialon, G.; Dess `ı, R.; Lomeli, M.; Nalmpantis, C.; Pa-\nsunuru, R.; Raileanu, R.; Rozi `ere, B.; Schick, T.; Dwivedi-\nYu, J.; Celikyilmaz, A.; Grave, E.; LeCun, Y .; and Scialom,\nT. 2023. Augmented Language Models: a Survey.\narXiv:2302.07842.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17598\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and\nLowe, R. 2022. Training Language Models to Follow In-\nstructions with Human Feedback. In Advances in Neural\nInformation Processing Systems, volume 35, 27730–27744.\nPaul, D.; Ismayilzada, M.; Peyrard, M.; Borges, B.;\nBosselut, A.; West, R.; and Faltings, B. 2023. RE-\nFINER: Reasoning Feedback on Intermediate Representa-\ntions. arXiv:2304.01904.\nRashid, T.; Samvelyan, M.; Schroeder, C.; Farquhar, G.; Fo-\nerster, J.; and Whiteson, S. 2018. QMIX: Monotonic Value\nFunction Factorisation for Deep Multi-Agent Reinforcement\nLearning. In International Conference on Machine Learn-\ning, 4295–4304. PMLR.\nShinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K. R.;\nand Yao, S. 2023. Reflexion: Language Agents with Verbal\nReinforcement Learning. In Thirty-seventh Conference on\nNeural Information Processing Systems.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2023.\nProgPrompt: Generating Situated Robot Task Plans using\nLarge Language Models. In 2023 IEEE International Con-\nference on Robotics and Automation (ICRA), 11523–11530.\nIEEE.\nStrouse, D.; McKee, K.; Botvinick, M.; Hughes, E.; and Ev-\nerett, R. 2021. Collaborating with Humans without Human\nData. In Advances in Neural Information Processing Sys-\ntems, volume 34, 14502–14515.\nTesauro, G. 1994. TD-Gammon, a self-teaching backgam-\nmon program, achieves master-level play. Neural computa-\ntion, 6(2): 215–219.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q. V .; Chi, E. H.;\nNarang, S.; Chowdhery, A.; and Zhou, D. 2023a. Self-\nConsistency Improves Chain of Thought Reasoning in Lan-\nguage Models. In The Eleventh International Conference on\nLearning Representations.\nWang, Y .; Zhong, F.; Xu, J.; and Wang, Y . 2021. ToM2C:\nTarget-oriented Multi-agent Communication and Coopera-\ntion with Theory of Mind. In International Conference on\nLearning Representations.\nWang, Z.; Cai, S.; Chen, G.; Liu, A.; Ma, X.; and Liang, Y .\n2023b. Describe, Explain, Plan and Select: Interactive Plan-\nning with LLMs Enables Open-World Multi-Task Agents. In\nThirty-seventh Conference on Neural Information Process-\ning Systems.\nWang, Z.; Cai, S.; Liu, A.; Jin, Y .; Hou, J.; Zhang, B.;\nLin, H.; He, Z.; Zheng, Z.; Yang, Y .; Ma, X.; and Liang,\nY . 2023c. JARVIS-1: Open-World Multi-Task Agents\nwith Memory-Augmented Multimodal Language Models.\narXiv:2311.05997.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; brian ichter;\nXia, F.; Chi, E. H.; Le, Q. V .; and Zhou, D. 2022. Chain\nof Thought Prompting Elicits Reasoning in Large Language\nModels. In Advances in Neural Information Processing Sys-\ntems, volume 35, 24824–24837.\nWelleck, S.; Lu, X.; West, P.; Brahman, F.; Shen, T.;\nKhashabi, D.; and Choi, Y . 2023. Generating Sequences\nby Learning to Self-Correct. In The Eleventh International\nConference on Learning Representations.\nWen, M.; Kuba, J.; Lin, R.; Zhang, W.; Wen, Y .; Wang, J.;\nand Yang, Y . 2022. Multi-Agent Reinforcement Learning\nis a Sequence Modeling Problem. In Advances in Neural\nInformation Processing Systems, volume 35, 16509–16521.\nWu, S. A.; Wang, R. E.; Evans, J. A.; Tenenbaum, J. B.;\nParkes, D. C.; and Kleiman-Weiner, M. 2021. Too Many\nCooks: Bayesian Inference for Coordinating Multi-Agent\nCollaboration. Topics in Cognitive Science, 13(2): 414–432.\nYang, Y .; and Wang, J. 2021. An Overview of Multi-Agent\nReinforcement Learning from Game Theoretical Perspec-\ntive. arXiv:2011.00583.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK. R.; and Cao, Y . 2023. ReAct: Synergizing Reasoning and\nActing in Language Models. In The Eleventh International\nConference on Learning Representations.\nYu, C.; Velu, A.; Vinitsky, E.; Gao, J.; Wang, Y .; Bayen, A.;\nand Wu, Y . 2022. The Surprising Effectiveness of PPO in\nCooperative Multi-Agent Games. In Advances in Neural In-\nformation Processing Systems, volume 35, 24611–24624.\nZhang, H.; Du, W.; Shan, J.; Zhou, Q.; Du, Y .; Tenenbaum,\nJ. B.; Shu, T.; and Gan, C. 2023. Building Cooperative\nEmbodied Agents Modularly with Large Language Models.\narXiv:2307.02485.\nZhang, K.; Yang, Z.; and Bas ¸ar, T. 2021. Multi-Agent Rein-\nforcement Learning: A Selective Overview of Theories and\nAlgorithms. Handbook of reinforcement learning and con-\ntrol, 321–384.\nZhao, R.; Song, J.; Yuan, Y .; Hu, H.; Gao, Y .; Wu, Y .;\nSun, Z.; and Yang, W. 2023. Maximum Entropy Popula-\ntion Based Training for Zero-Shot Human-AI Coordination.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, 5, 6145–6153.\nZhong, Y .; Kuba, J. G.; Feng, X.; Hu, S.; Ji, J.; and Yang,\nY . 2023. Heterogeneous-Agent Reinforcement Learning.\narXiv:2304.09870.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V .; and\nChi, E. H. 2023. Least-to-Most Prompting Enables Com-\nplex Reasoning in Large Language Models. In The Eleventh\nInternational Conference on Learning Representations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17599",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.48561254143714905
    }
  ]
}