{
  "title": "SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery",
  "url": "https://openalex.org/W2986232138",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Honda, Shion",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Shi, Shoi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3042902480",
      "name": "Ueda Hiroki R",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2768348081",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2749279690",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2962764460",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W2886791556",
    "https://openalex.org/W2766447205",
    "https://openalex.org/W2901476322",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W2747592475",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2785947426",
    "https://openalex.org/W2565684601",
    "https://openalex.org/W2900281422",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2604296437",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W2963819570",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963676163",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W2964121744"
  ],
  "abstract": "In drug-discovery-related tasks such as virtual screening, machine learning is emerging as a promising way to predict molecular properties. Conventionally, molecular fingerprints (numerical representations of molecules) are calculated through rule-based algorithms that map molecules to a sparse discrete space. However, these algorithms perform poorly for shallow prediction models or small datasets. To address this issue, we present SMILES Transformer. Inspired by Transformer and pre-trained language models from natural language processing, SMILES Transformer learns molecular fingerprints through unsupervised pre-training of the sequence-to-sequence language model using a huge corpus of SMILES, a text representation system for molecules. We performed benchmarks on 10 datasets against existing fingerprints and graph-based methods and demonstrated the superiority of the proposed algorithms in small-data settings where pre-training facilitated good generalization. Moreover, we define a novel metric to concurrently measure model accuracy and data efficiency.",
  "full_text": "SMILES Transformer: Pre-trained Molecular Fingerprint\nfor Low Data Drug Discovery\nShion Honda1,2,3, Shoi Shi1,2,3, Hiroki R. Ueda1,2,3\n1University of Tokyo\n2International Research Center for Neurointelligence\n3RIKEN Center for Biosystems Dynamics Research\nshion honda@ipc.i.u-tokyo.ac.jp, {sshoi0322-tky,uedah-tky}@umin.ac.jp\nAbstract\nIn drug-discovery-related tasks such as virtual screening,\nmachine learning is emerging as a promising way to pre-\ndict molecular properties. Conventionally, molecular ﬁnger-\nprints (numerical representations of molecules) are calculated\nthrough rule-based algorithms that map molecules to a sparse\ndiscrete space. However, these algorithms perform poorly for\nshallow prediction models or small datasets. To address this\nissue, we present SMILES Transformer. Inspired by Trans-\nformer and pre-trained language models from natural lan-\nguage processing, SMILES Transformer learns molecular ﬁn-\ngerprints through unsupervised pre-training of the sequence-\nto-sequence language model using a huge corpus of SMILES,\na text representation system for molecules. We performed\nbenchmarks on 10 datasets against existing ﬁngerprints and\ngraph-based methods and demonstrated the superiority of the\nproposed algorithms in small-data settings where pre-training\nfacilitated good generalization. Moreover, we deﬁne a novel\nmetric to concurrently measure model accuracy and data efﬁ-\nciency.\n1 Introduction\nRecently, deep learning has emerged as a powerful machine\nlearning technology. When applied to big data, deep learn-\ning can show equal or even better performance than humans\nin many domains such as computer vision (He et al. 2016),\nnatural language processing (NLP) (Devlin et al. 2019;\nYang et al. 2019), making decisions (Silver et al. 2017), and\nmedicine (Jin, Barzilay, and Jaakkola 2018). Based on pro-\njected performance benchmarks, deep learning is expected\nto be useful a tool to handle time-consuming tasks.\nDrug discovery is a process to ﬁnd a new drug for a dis-\nease of interest from a chemical library and validate its efﬁ-\ncacy and safety in clinical trials. This process usually takes\nmore than a decade and is costly, and therefore may be im-\nprovable by deep learning methods. Indeed, deep learning\nhas been applied to the process of drug discovery including\nquantitative structure-property relationships (QSPR) predic-\ntion (Duvenaud et al. 2015; Xu et al. 2017), molecule gener-\nation and lead optimization (G´omez-Bombarelli et al. 2018;\nJin, Barzilay, and Jaakkola 2018), retrosynthesis planning\n(Segler, Preuss, and Waller 2018; Schwaller et al. 2018), and\ncompound-protein afﬁnity prediction ( ¨Ozt¨urk, ¨Ozg¨ur, and\nOzkirimli 2018).\nIn order to apply machine learning to drug discovery,\nmolecular data must be transformed into a readable format\nfor machine learning. One major approach is to transform\nmolecular data into a simpliﬁed molecular input line en-\ntry system (SMILES), a text representation of molecules\nthat is commonly used in many databases (Xu et al. 2017;\nG´omez-Bombarelli et al. 2018). Recently, graph-based ap-\nproaches (Duvenaud et al. 2015; Kearnes et al. 2016) have\nbeen proposed, which usually show better performance than\ntext-based approaches, such as SMILES, in QSPR tasks. In\nthese studies, the models are designed for large fully-labeled\ntraining data settings, which requires huge labeled datasets\nand a QSPR model for one-shot learning (Altae-Tran et\nal. 2017). However, in most cases, it is difﬁcult to prepare\nlarge labeled datasets of experimentally validated molecular\nproperties or afﬁnities to proteins, so that graph-based ap-\nproaches might have limited application. Therefore, the de-\nvelopment of a high-performing algorithm for small datasets\nwill be required.\nGiven recent progress in the NLP ﬁeld (Peters et al. 2018;\nDevlin et al. 2019; Yang et al. 2019), a pre-training approach\nmay be a promising way to address this challenge. Lan-\nguage model pre-training can exploit huge unlabeled cor-\npora to learn the representations of words and sentences\nand then the pre-trained model is ﬁne-tuned to downstream\ntasks using a relatively smaller set of labeled data. In-\ndeed, pre-training approaches have been implemented in the\ncheminformatics ﬁeld: a pre-trained sequence-to-sequence\nlearning models (seq2seq) composed of RNNs (Sutskever,\nVinyals, and Le 2014) or variational autoencoders (V AE)\n(Kingma and Welling 2014) by decoding SMILES from\nthe learned representations (G ´omez-Bombarelli et al. 2018;\nXu et al. 2017; Kusner, Paige, and Hern´andez-Lobato 2017;\nGoh et al. 2018; Bjerrum and Sattarov 2018; Winter et al.\n2019). However, these studies did not demonstrate perfor-\nmance in small data settings. In other words, the perfor-\nmance on small data settings of pre-training approaches in\nthe cheminformatics ﬁeld has not been evaluated yet. In this\nstudy, by applying the latest pre-training method in the NLP\nﬁeld to cheminformatics, we propose a new approach called\narXiv:1911.04738v1  [cs.LG]  12 Nov 2019\nFigure 1: The illustration of SMILES Transformer pre-\ntraining and ﬁngerprint extraction.\nSMILES Transformer (ST) that shows higher performance\non small data settings than other approaches. ST is based on\na Transformer (Vaswani et al. 2017) pre-trained in an un-\nsupervised way that produce continuous, data-driven ﬁnger-\nprints of molecules given SMILES. These ﬁngerprints grasp\nthe semantics of molecules and can be fed to arbitrary pre-\ndictive models for many downstream tasks.\nIn order to evaluate the QSPR performance on small data\nsettings, we focused on data efﬁciency. However, because\nthere are few works focusing on data efﬁciency, which met-\nric should be used is elusive. The most related work may\nbe done by (Wu et al. 2018), where model performance is\nevaluated against the size of the training set and data efﬁ-\nciency is emphasized as well as the best score. In this study,\nwe propose a novel scalar metric to evaluate data efﬁciency.\nOur proposed model is described in Figure 1.\nTo sum up, our contributions include the following:\n• We propose a data-driven ﬁngerprinting model, SMILES\nTransformer, which works well with simple predictors\nand enables state-of-the-art data efﬁciency in 5 out of 10\ndatasets in MoleculeNet.\n• We pre-train Transformers with unlabeled SMILES to\nlearn their representations and show the potential of text-\nbased models compared to baseline models including\ngraph convolutions.\n• We propose a scalar metric for data efﬁciency that mea-\nsures model performance under different sizes of training\ndata.\nIn the ﬁrst section, we will explain how ST is trained\nand the ﬁngerprints are extracted. In the second section, we\ndeﬁne the metric for data efﬁciency. In the third section,\nwe will compare the performance of ST ﬁngerprints against\nother methods using 10 different datasets from MoleculeNet\nand more deeply inspect the pre-trained ST including latent\nspace visualization. Finally, we discuss possible future di-\nrections.\n2 Methods\nIn this section, we introduce the SMILES Transformer archi-\ntecture, pre-training settings, and how to design ST ﬁnger-\nprints. We then propose a novel metric for data efﬁciency.\n2.1 SMILES Transformer\nModel Architecture Unlike RNNs, Transformers\n(Vaswani et al. 2017) do not have recurrent connections\nand are therefore more stable and faster to converge.\nMoreover, they empirically show better featurization\nperformance on long sequences and complicated prob-\nlems than RNNs. Hence, they are chosen as the de\nfacto standard models in NLP (Devlin et al. 2019;\nYang et al. 2019).\nWe built an encoder-decoder network with 4 Transformer\nblocks for each with PyTorch (Paszke et al. 2017). Each\nTransformer block has 4-head attentions with 256 embed-\nding dimensions and 2 linear layers.\nPre-training settings We pre-trained ST with 861,000 un-\nlabeled SMILES randomly sampled from ChEMBL24, a\ndataset of bioactive and real molecules (Gaulton et al. 2016).\nThe SMILES was split into symbols (e.g., ’c’, ’Br’, ’=’, ’(’,\n’2’) and then the symbols were one-hot encoded to input to\nthe network. To alleviate bias for the canonical representa-\ntion of SMILES, we randomly transformed them every time\nthey were used by the SMILES enumerator (Bjerrum 2017).\nFollowing the original paper (Vaswani et al. 2017), we used\nthe sum of token encoding and positional encoding to in-\nput to the network. The network was trained for 5 epochs to\nminimize the cross entropy between the input SMILES and\nthe output probability by the Adam optimizer (Kingma and\nBa 2015). After convergence, the network achieved a per-\nplexity of 1.0, meaning perfect decoding from the encoded\nrepresentations.\nFingerprint extraction As the outputs of the Transform-\ners are contextualized word-level representations, ST out-\nputs a sequence of symbol-level (atom-level) represen-\ntations. Therefore, we need to pool them to obtain the\nmolecule-level representations (ﬁngerprints). We concate-\nnated the four vectors to get the ﬁngerprints: mean and max\npooled output of the last layer, the ﬁrst output of the last and\nthe penultimate layer. Now we have a 1024-dimensional ﬁn-\ngerprint for each molecule from ST. This ﬁngerprint is de-\nsigned to have the same dimensionality with the baseline\nwe use for, the extended-connectivity ﬁngerprint (ECFP)\n(Rogers and Hahn 2010).\n2.2 Data Efﬁciency Metric (DEM)\nHere we discuss how to measure the data efﬁciency of a pre-\ndictive model f in terms of the metric m. Intuitively, data\nefﬁciency can be measured by averaging the metricm of the\nmodel f trained with different sizes of the training data.\nMore formally, let (X, Y) denote the whole available\ndataset and (Xi, Yi) denote the test data sampled from\n(X, Y) at the rate of 1 −i. Then, the training data and the\nmodel trained with them can be represented as (X \\Xi, Y\\\nYi) and fi, respectively. The metric m should be chosen to\nTable 1: Summarized information of MoleculeNet (Wu et al. 2018). ”R” and ”C” in the type column indicates regression and\nclassiﬁcation respectively.\nCategory Dataset Tasks Type Mols Metric Description\nPhysical\nchemistry\nESOL 1 R 1128 RMSE Aqueous solubility\nFreeSolv 1 R 643 RMSE Hydration free energy\nLipophilicity 1 R 4200 RMSE Octanol/water distribution coefﬁcient (logD)\nBiophysics\nMUV 17 C 93127 PRC-AUC 17 tasks from PubChem BioAssay\nHIV 1 C 41913 ROC-AUC Ability to inhibit HIV replication\nBACE 1 C 1522 ROC-AUC Binding results for inhibitors of human BACE-1\nPhysiology\nBBBP 1 C 2053 ROC-AUC Blood-brain barrier penetration\nTox21 12 C 8014 ROC-AUC Toxicity measurements\nSIDER 27 C 1427 ROC-AUC Adverse drug reactions on 27 system organs\nClinTox 2 C 1491 ROC-AUC Clinical trial toxicity and FDA approval status\nTable 2: Comparison of data efﬁciency metric (DEM) with the baseline models on 10 datasets from MoleculeNet (Wu et al.\n2018). The up/down arrows show that the higher/lower score is better, respectively.\nDataset ESOL ↓ FrSlv ↓ Lipo ↓ MUV ↑ HIV ↑ BACE ↑ BBBP ↑ Tox21 ↑ Sider ↑ ClinTox ↑\nST+MLP (Ours) 1.144 2.246 1.169 0.009 0.683 0.719 0.900 0.706 0.559 0.963\nECFP+MLP 1.741 3.043 1.090 0.036 0.697 0.769 0.760 0.616 0.588 0.515\nRNNS2S+MLP 1.317 2.987 1.219 0.010 0.682 0.717 0.884 0.702 0.558 0.904\nGraphConv 1.673 3.476 1.062 0.004 0.723 0.744 0.795 0.687 0.557 0.936\nbe suitable for the tasks. That is, in classiﬁcation tasks m\nshould be the area under the receiver operation character-\nistics (ROC-AUC) or the F1 score and in regression tasks\nm should be the R2 score or the root mean squared error\n(RMSE).\nNow the proposed Data Efﬁciency Metric (DEM) is for-\nmulated as:\nMDE(f, m) = 1\n|I|\n∑\ni∈I\nm(fi, Xi, Yi) (1)\nSince we used various datasets with a wide range of sizes\nin the experiment described below, the percentage of the\ntraining data i should be increased exponentially. There-\nfore, i is doubly increased from 1.25% to 80%, i.e., I =\n{0.0125, 0.025, ...,0.4, 0.8}.\n3 Experiments\nWe conducted ﬁve experiments to see how SMILES Trans-\nformer works from different perspectives. First, we evalu-\nated the performance of ST against other baseline models\non 10 chemical datasets. Second, we visualized the latent\nspace to answer the question: why do ST ﬁngerprints work\nwell for certain datasets? Third, we applied linear models to\nST and other ﬁngerprints in order to validate that ST maps\nmolecules to a good latent space by minimizing the contribu-\ntion of the models themselves. Fourth, we evaluated our ST\nand baseline models on a stratiﬁed dataset by the lengths of\nSMILES to see when ST provides an advantage. Finally, we\ncompared the maximum performance of ST against state-of-\nthe-art models under large data settings.\n3.1 Performance on Downstream Tasks\nDatasets We evaluated the performance of our pre-trained\nSMILES Transformer on 10 datasets from MoleculeNet (Wu\net al. 2018), a benchmark for molecular property prediction.\nThese datasets were chosen because they do not use 3D in-\nformation and the sizes are not too large. The datasets are\ndifferent from each other in their domains, task types, and\nsizes.\n• Physical chemistry: ESOL, FreeSolv, and Lipophilicity\n• Biophysics: MUV , HIV , and BACE\n• Physiology: BBBP, Tox21, SIDER, and ClinTox\nThe information about each dataset is summarized in Ta-\nble 1. For the evaluation metrics, we used the root mean\nsquared error (RMSE) for the regression tasks and the\narea under the receiver operating characteristic curve (ROC-\nAUC) or the area under the precision-recall curve (PRC-\nAUC) for the classiﬁcation tasks as suggested in (Wu et al.\n2018).\nBaseline models We compared our pre-trained SMILES\nTransformer to the following three baseline models for\nmolecular property prediction tasks:\n• ECFP4 (Rogers and Hahn 2010) is a hand-crafted ﬁnger-\nprint. It hashes multi-scaled substructures to integers and\nmakes a ﬁxed-length binary vector where 1 indicates the\nexistence of the assigned substructure and 0 for the ab-\nsence. ECFP4 counts substructures with the diameters up\nto 4.\n• RNNS2S (Xu et al. 2017) is another text-based pre-\ntrained ﬁngerprint that adopts RNN Seq2seq for the\nmodel architecture.\n• GraphConv (Duvenaud et al. 2015) learns and predicts\nthe target value directly through graph convolution oper-\nations, rather than extracting ﬁngerprints and building an-\nother model for supervised downstream tasks. Although\nGraphConv is not a task-agnostic ﬁngerprint, we include\nit here as the state-of-the-art model.\nWe used RDKit (Landrum 2016) to compute ECFP4\nand DeepChem (Ramsundar et al. 2019) implementa-\ntion of GraphConv (with the default hyperparmeters). For\nRNNS2S, we implemented it with PyTorch (Paszke et al.\n2017) and pre-trained it with the same dataset as ST. The\nencoder and the decoder are both 3-layer bidirectional gated\nrecurrent units (GRUs) (Cho et al. 2014) with 256 hidden\nvector dimensions. We obtained the same dimension of ﬁn-\ngerprint as ST by concatenating two outputs from the last\nand the penultimate layer.\nExperiment settings In the downstream tasks, we used\nsimple models, such as multilayer perceptron (MLP) clas-\nsiﬁers and regressors with the same default hyperparameters\nin scikit-learn (Pedregosa et al. 2011) in order to evaluate the\nperformance of the three ﬁngerprints, themselves as much\nas possible. All of these ﬁngerprints have 1,024 dimensions.\nThe datasets were randomly split (stratiﬁed for classiﬁca-\ntion) to train sets and test sets by the percentage i. Note that\nwe did not use a scaffold split suggested in (Wu et al. 2018).\nWe ran 20 trials for each split and report the mean score\nand standard deviation in Figure 2 and DEM in Table 2. The\nmetrics were chosen as recommended in MoleculeNet.\nResults Table 2 shows DEM of the 4 models. ST achieves\nthe best score in 5 out of 10 datasets, followed by ECFP and\nGraphConv.\nSee Figure 2 for the performance change against the train\nsize. In ESOL, FreeSolv, BBBP, and ClinTox, ST performs\nthe best at almost all points by a signiﬁcant margin and es-\npecially high scores when the train size is small compared\nto the other models. In Tox21, ST supports good prediction\nalong RNNS2S, but is beaten by GraphConv as the train size\nincrease. In Lipophilicity, MUV , BACE, and SIDER, ECFP\nor GraphConv can predict better than ST.\n3.2 Visualization of the Latent Space\nTo inspect why our ST ﬁngerprints lead to good predic-\ntive performance, we visualized the latent space and decode\nsome samples from it. For each dataset in MoleculeNet, we\nconducted the following procedure:\n1. Calculate the ST ﬁngerprint (1024-dimension) of each\nmolecule.\n2. Reduce their dimensions to 2 with t-SNE (Maaten and\nHinton 2008).\n3. Plot the reduced features into a 2-dimensional space col-\noring by the target value.\n4. Choose a trajectory in the 2-dimensional space and divide\nit into 12 points.\n5. Find the nearest neighbors of the 12 points and draw the\ncorresponding molecules.\nWe show the result of the three datasets where ST ﬁn-\ngerprints work especially well in Figure 3, that is, FreeSolv,\nBBBP, and ClinTox. In FreeSolv, it can be seen that there\nis a clear gradation from upper left to lower right, and the\nmolecule becomes simpler (i.e., less loops and branches)\nalong the trajectory. In BBBP and ClinTox, the categorical\ntarget values are successfully separated, but there is no clear\ntrends in the decoded molecules.\n3.3 Application of Simple Predictive Models\nIn Section 3.1, we used MLP for the predictive model to ST,\nRNNS2S, and ECFP, expecting that combining it with these\nﬁngerprints would work comparably or better than Graph-\nConv. Here we used the simplest models to measure the pure\neffect of the ﬁngerprints. To be speciﬁc, adding L2 regular-\nization to avoid overﬁtting, we used ridge regression for re-\ngression tasks and logistic regression with L2 penalty for\nclassiﬁcation tasks. We excluded MUV and SIDER from\nthis experiment because their highly imbalanced columns\ncaused errors in the solver and ROC-AUC functions imple-\nmented in scikit-learn (Pedregosa et al. 2011). We followed\nthe same procedure as in Section 3.1 except for the model\nselection and datasets and the results are shown in Table 3.\nOur ST ﬁngerprints with linear models achieved the best\nscores in 5 out of 8 datasets, indicating that the ST ﬁnger-\nprint is a strong ﬁngerprint that leads to the best performance\nregardless of model selection.\n3.4 Stratiﬁed Scores by the Size of Molecules\nWe conducted another study to inspect when ST has an\nadvantage against other models. We stratiﬁed the BBBP\ndataset by the lengths of SMILES (similar to the sizes of the\nmolecules) into 5 groups and evaluated within each group.\nThe scores and the distributions of the lengths of SMILES\nare shown in Figure 4.\nFigure 4 indicates that the ROC-AUC score of ST in-\ncreases along the length of SMILES, which is a similar trend\nto the other text-based ﬁngerprint, RNNS2S. On the other\nhand, GraphConv shows more or less the same performance\nregardless of the SMILES lengths. These results suggest that\nlonger SMILES give ST richer information for better dis-\ncrimination.\n3.5 Comparison with Record Scores\nFinally, we compared the maximum performance of ST un-\nder the large data setting with the reported scores in Molecu-\nleNet. Since the ST ﬁngerprint is proven to be better than the\nRNNS2S ﬁngerprint, we omitted it and instead added an-\nother graph-based model named Weave (Kearnes et al. 2016)\nto the baselines. In this experiment, the datasets were split\ninto train, validation, test sets with the proportion of 80%,\n10%, 10%. The validation sets were used for hyperparam-\neter tuning and the test sets were only used for calculating\nthe scores. To fairly compare with the reported scores, the\ndatasets HIV , BACE, BBBP used a scaffold split and the oth-\ners were split randomly. We choose the model and hyperpa-\nrameter set achieving the best validation score with optuna\n(Akiba et al. 2019), from a linear model with L2 penalty,\nFigure 2: Comparison of model performance against different train size on the 10 datasets. The top row indicates the results for\nthe physical chemistry datasets, the second row indicates biophysics, and the two bottom rows indicate physiology, respectively.\nThe scores were averaged over 20 trials and the error bars are the standard deviations\nFigure 3: Visualization of the latent space of SMILES Transformer. For three datasets, FreeSolv, BBBP, and ClinTox, the\ndimensions of ST ﬁngerprints of the molecules are reduced to 2 with t-SNE (Maaten and Hinton 2008). Then, the nearest\nneighbors of the 12 data points on a trajectories are plotted on the latent space (left panel). The 12 points are decoded to\nmolecules and shown in the right panel. The color bar of the top left panel indicates the standardized free energy.\nTable 3: Comparison of data efﬁciency metric (DEM) with the baseline models on the 8 datasets from MoleculeNet (Wu et al.\n2018). The predictive models are ridge regression and logistic regression with L2 penalty. The up/down arrows show that the\nhigher/lower score is better, respectively.\nDataset ESOL ↓ FrSlv ↓ Lipo ↓ HIV ↑ BACE ↑ BBBP ↑ Tox21 ↑ ClinTox ↑\nST (Ours) 1.140 2.452 1.213 0.696 0.720 0.895 0.711 0.958\nECFP 1.678 2.843 1.174 0.727 0.790 0.825 0.710 0.704\nRNNS2S 1.288 2.881 1.194 0.688 0.727 0.884 0.709 0.915\nTable 4: Comparison of the best achieved scores with the record scores on the 8 datasets from MoleculeNet (Wu et al. 2018).\nThe scores of ECFP, GraphConv, and Weave are the reported scores in MoleculeNet. The up/down arrows show that the\nhigher/lower score is the better, respectively.\nDataset ESOL ↓ FrSlv ↓ Lipo ↓ HIV ↑ BACE ↑ BBBP ↑ Tox21 ↑ ClinTox ↑\nSplitting random random random scaffold scaffold scaffold random random\nST (Ours) 0.72 1.65 0.921 0.729 0.701 0.704 0.802 0.954\nECFP 0.99 1.74 0.799 0.792 0.867 0.729 0.822 0.799\nGraphConv 0.97 1.40 0.655 0.763 0.783 0.690 0.829 0.807\nWeave 0.61 1.22 0.715 0.703 0.806 0.671 0.820 0.832\nMLP, and LightGBM (Ke et al. 2017). We conducted three\nindependent runs and reported the average scores in Table 4\nST achieves ﬁrst place only in ClinTox, but performs\ncomparable to ECFP and graph-based models in the other\ndatasets. We can conclude that our ST ﬁngerprints, if care-\nfully tuned, are still useful even when the large number of\nlabels are available.\n4 Conclusions\nIn this paper, we propose SMILES-Transformer, a data-\ndriven molecular ﬁngerprint produced by a Transformer-\nbased seq2seq pre-trained with a huge set of unlabeled\nSMILES. ST ﬁngerprints were shown to work well with any\npredictive model in MoleculeNet downstream tasks and is\neffective especially when there is not enough labeled data.\nWhen large labeled data are available, ST ﬁngerprints work\ncomparable to other state-of-the-art baselines such as Graph-\nConv. We also propose DEM, a novel metric for data efﬁ-\nciency. In terms of DEM, the ST ﬁngerprint is better than\nexisting methods in 5 out of 10 downstream tasks.\nFuture work can continue in three directions. First, re-\nplacing the Transformer in ST with Transformer-XL, an\nextended model that can handle much longer sequences,\nwill alleviate the length limit of ST. Second, ST will be\neven stronger when trained in a multi-task fashion as done\nin ChemNet (Goh et al. 2018): predicting automatically-\ncalculated molecular descriptors (e.g., molecular weight,\nLogP) as well as decoding the input SMILES. This will\nhelp the model to learn more chemistry-relevant representa-\ntions. Finally, making use of the information of enumerated\nSMILES is one of the keys to improving text-based molecu-\nlar representations. As done in (Bjerrum and Sattarov 2018),\na set of different SMILES of the same molecule can be used\nto restrict the latent space.\nOur implementation for SMILES-Transformer is avail-\nable at https://github.com/DSPsleeporg/smiles-transformer\nFigure 4: ROC-AUC scores on each stratiﬁed group by the\nlengths of SMILES (left) and the distributions of the lengths\nof SMILES (right) of BBBP dataset.\nReferences\nAkiba, T.; Sano, S.; Yanase, T.; Ohta, T.; and Koyama, M.\n2019. Optuna: A next-generation hyperparameter optimiza-\ntion framework. In ACM SIGKDD, 2623–2631. ACM.\nAltae-Tran, H.; Ramsundar, B.; Pappu, A. S.; and Pande, V .\n2017. Low data drug discovery with one-shot learning. ACS\nCent. Sci.3(4):283–293.\nBjerrum, E., and Sattarov, B. 2018. Improving chemical\nautoencoder latent space and molecular de novo generation\ndiversity with heteroencoders. Biomolecules 8(4):131.\nBjerrum, E. J. 2017. Smiles enumeration as data augmen-\ntation for neural network modeling of molecules. arXiv\npreprint arXiv:1703.07076.\nCho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning\nphrase representations using rnn encoder–decoder for statis-\ntical machine translation. In EMNLP, 1724–1734.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In NAACL-HLT, 4171–4186.\nDuvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Bombarell,\nR.; Hirzel, T.; Aspuru-Guzik, A.; and Adams, R. P. 2015.\nConvolutional networks on graphs for learning molecular\nﬁngerprints. In NeurIPS, 2224–2232.\nGaulton, A.; Hersey, A.; Nowotka, M.; Bento, A. P.; Cham-\nbers, J.; Mendez, D.; Mutowo, P.; Atkinson, F.; Bellis, L. J.;\nCibri´an-Uhalte, E.; et al. 2016. The chembl database in\n2017. Nucleic acids research45(D1):D945–D954.\nGoh, G. B.; Siegel, C.; Vishnu, A.; and Hodas, N. 2018. Us-\ning rule-based labels for weak supervised learning: a chem-\nnet for transferable chemical property prediction. In ACM\nSIGKDD, 302–310. ACM.\nG´omez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.;\nHern´andez-Lobato, J. M.; S ´anchez-Lengeling, B.; She-\nberla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams,\nR. P.; and Aspuru-Guzik, A. 2018. Automatic chemical\ndesign using a data-driven continuous representation of\nmolecules. ACS Cent. Sci.4(2):268–276.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770–778.\nJin, W.; Barzilay, R.; and Jaakkola, T. 2018. Junction tree\nvariational autoencoder for molecular graph generation. In\nICML, 2328–2337.\nKe, G.; Meng, Q.; Finley, T.; Wang, T.; Chen, W.; Ma, W.;\nYe, Q.; and Liu, T.-Y . 2017. Lightgbm: A highly efﬁcient\ngradient boosting decision tree. In NeurIPS, 3146–3154.\nKearnes, S.; McCloskey, K.; Berndl, M.; Pande, V .; and Ri-\nley, P. 2016. Molecular graph convolutions: moving beyond\nﬁngerprints. J. Comput. Aided Mol. Des.30(8):595–608.\nKingma, D. P., and Ba, J. 2015. Adam: A method for\nstochastic optimization. In ICLR.\nKingma, D. P., and Welling, M. 2014. Auto-encoding vari-\national Bayes. In ICLR.\nKusner, M. J.; Paige, B.; and Hern´andez-Lobato, J. M. 2017.\nGrammar variational autoencoder. In ICML, 1945–1954.\nJMLR. org.\nLandrum, G. 2016. Rdkit: Open-source cheminformatics.\nMaaten, L. v. d., and Hinton, G. 2008. Visualizing data\nusing t-sne. J. Mach. Learn. Res.9(Nov):2579–2605.\n¨Ozt¨urk, H.; ¨Ozg¨ur, A.; and Ozkirimli, E. 2018. Deepdta:\ndeep drug–target binding afﬁnity prediction. Bioinformatics\n34(17):i821–i829.\nPaszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;\nDeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer,\nA. 2017. Automatic differentiation in pytorch. In NIPS.\nPedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V .;\nThirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss,\nR.; Dubourg, V .; et al. 2011. Scikit-learn: Machine learning\nin python. J. Mach. Learn. Res.12(Oct):2825–2830.\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized\nword representations. In NAACL-HLT, 2227–2237.\nRamsundar, B.; Eastman, P.; Walters, P.; Pande, V .; Leswing,\nK.; and Wu, Z. 2019. Deep Learning for the Life Sciences.\nO’Reilly Media. https://www.amazon.com/Deep-Learning-\nLife-Sciences-Microscopy/dp/1492039837.\nRogers, D., and Hahn, M. 2010. Extended-connectivity ﬁn-\ngerprints. J. Chem. Inf. Model.50(5):742–754.\nSchwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Bekas, C.;\nand Lee, A. A. 2018. Molecular transformer for chemical re-\naction prediction and uncertainty estimation. arXiv preprint\narXiv:1811.02633.\nSegler, M. H.; Preuss, M.; and Waller, M. P. 2018. Planning\nchemical syntheses with deep neural networks and symbolic\nai. Nature 555(7698):604.\nSilver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.;\nHuang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton,\nA.; et al. 2017. Mastering the game of go without human\nknowledge. Nature 550(7676):354.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence to\nsequence learning with neural networks. In NeurIPS, 3104–\n3112.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, 5998–6008.\nWinter, R.; Montanari, F.; No´e, F.; and Clevert, D.-A. 2019.\nLearning continuous and data-driven molecular descriptors\nby translating equivalent chemical representations. Chem.\nSci. 10(6):1692–1701.\nWu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Ge-\nniesse, C.; Pappu, A. S.; Leswing, K.; and Pande, V . 2018.\nMoleculenet: a benchmark for molecular machine learning.\nChem. Sci.9(2):513–530.\nXu, Z.; Wang, S.; Zhu, F.; and Huang, J. 2017. Seq2seq\nﬁngerprint: An unsupervised deep molecular embedding for\ndrug discovery. In ACM BCB, 285–294. ACM.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR.; and Le, Q. V . 2019. Xlnet: Generalized autoregres-\nsive pretraining for language understanding. arXiv preprint\narXiv:1906.08237.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7327849268913269
    },
    {
      "name": "Transformer",
      "score": 0.6576659679412842
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5605716705322266
    },
    {
      "name": "Machine learning",
      "score": 0.536519467830658
    },
    {
      "name": "Language model",
      "score": 0.5204351544380188
    },
    {
      "name": "Generalization",
      "score": 0.46828243136405945
    },
    {
      "name": "Virtual screening",
      "score": 0.4640829563140869
    },
    {
      "name": "Data mining",
      "score": 0.4364074170589447
    },
    {
      "name": "Natural language processing",
      "score": 0.4349609315395355
    },
    {
      "name": "Training set",
      "score": 0.4345030188560486
    },
    {
      "name": "Drug discovery",
      "score": 0.36473071575164795
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3598514795303345
    },
    {
      "name": "Bioinformatics",
      "score": 0.10048183798789978
    },
    {
      "name": "Engineering",
      "score": 0.09090065956115723
    },
    {
      "name": "Mathematics",
      "score": 0.08949357271194458
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210147536",
      "name": "University hospital Medical Information Network",
      "country": "JP"
    }
  ],
  "cited_by": 153
}