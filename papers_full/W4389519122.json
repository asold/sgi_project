{
  "title": "Editing Common Sense in Transformers",
  "url": "https://openalex.org/W4389519122",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2306383502",
      "name": "Anshita Gupta",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2968397797",
      "name": "Debanjan Mondal",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": null,
      "name": "Akshay Sheshadri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110443328",
      "name": "Wenlong Zhao",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2068388382",
      "name": "Xiang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2785635075",
      "name": "Sarah Wiegreffe",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1990453627",
      "name": "Niket Tandon",
      "affiliations": [
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205857304",
    "https://openalex.org/W4306313145",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W3106976604",
    "https://openalex.org/W4385569933",
    "https://openalex.org/W3168698433",
    "https://openalex.org/W4385571289",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W3211384591",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W2075210427",
    "https://openalex.org/W4287820586",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W4285265395",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3106185885",
    "https://openalex.org/W4287887895",
    "https://openalex.org/W4296878971",
    "https://openalex.org/W4315881234",
    "https://openalex.org/W3152884768",
    "https://openalex.org/W4394743141",
    "https://openalex.org/W4389519586",
    "https://openalex.org/W4386566752",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2963353834",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W4283815582",
    "https://openalex.org/W4282980384",
    "https://openalex.org/W1501375624"
  ],
  "abstract": "Editing model parameters directly in Transformers makes updating open-source transformer-based models possible without re-training. However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers’ reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., MEMITCSK. GPT-2 Large and XL models edited using MEMITCSK outperform best-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel evaluation dataset, PROBE\\SET, that contains unaffected and affected neighborhoods, affected paraphrases, and affected reasoning challenges. MEMITCSK performs well across the metrics while fine-tuning baselines show significant trade-offs between unaffected and affected metrics. These results suggest a compelling future direction for incorporating feedback about common sense into Transformers through direct model editing.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8214–8232\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEditing Common Sense in Transformers\nAnshita Gupta♠∗ Debanjan Mondal♠∗ Akshay Krishna Sheshadri♠∗\nWenlong Zhao♠ Xiang Lorraine Li♣∗ Sarah Wiegreffe♡∗ Niket Tandon♡∗\n♠University of Massachusetts Amherst, ♣University of Pittsburgh, ♡Allen Institute for AI\n{anshitagupta,debanjanmond,asheshadri,wenlongzhao}@cs.umass.edu\nxianglli@pitt.edu, wiegreffesarah@gmail.com, nikett@allenai.org\nAbstract\nEditing model parameters directly in\nTransformers makes updating open-source\ntransformer-based models possible without\nre-training (Meng et al., 2023). However, these\nediting methods have only been evaluated on\nstatements about encyclopedic knowledge\nwith a single correct answer. Commonsense\nknowledge with multiple correct answers,\ne.g., an apple can be green or red but not\ntransparent, has not been studied but is as\nessential for enhancing transformers’ reliability\nand usefulness. In this paper, we investigate\nwhether commonsense judgments are causally\nassociated with localized, editable parameters\nin Transformers, and we provide an affirmative\nanswer. We find that directly applying the\nMEMIT editing algorithm results in sub-par\nperformance, and propose to improve it for\nthe commonsense domain by varying edit\ntokens and improving the layer selection\nstrategy, i.e., MEMIT CSK . GPT-2 Large and XL\nmodels edited using MEMIT CSK outperform\nbest-fine-tuned baselines by 10.97% and\n10.73% F1 scores on PEP3k and 20Q datasets.\nIn addition, we propose a novel evaluation\ndataset, PROBE SET , that contains unaffected\nand affected neighborhoods, affected para-\nphrases, and affected reasoning challenges.\nMEMIT CSK performs well across the metrics\nwhile fine-tuning baselines show significant\ntrade-offs between unaffected and affected\nmetrics. These results suggest a compelling\nfuture direction for incorporating feedback\nabout common sense into Transformers\nthrough direct model editing.1\n1 Introduction\nTransformer-based language models (LMs) have\nachieved great success in NLP (Brown et al., 2020)\nbut they still exhibit factual mistakes (Lewis et al.,\n2020; Shuster et al., 2021), commonsense mistakes\n∗Co-first and last authors. Lorraine’s work done at AI2.\n1Code and datasets for all experiments are available at\nhttps://github.com/anshitag/memit_csk\nMEMITCSK \nediting for \ns,v,o \ns v o\nEdited GPT \nModel \ny = True\ny = False\nGPT Model \nx = Soil absorbs oil\nSemantic \nGeneralization \nEvaluation\nAffected \nReasoning\nAffected \nNeighborhood\nUnaffected \nNeighborhood\nAffected \nParaphrase\nRocks \nabsorbs oil\nSoil  \nsoaks up oil\nGround  \ntakes in oil\nSoil is porous, so it \ncan absorb oil\nFigure 1: Proposed framework – MEMIT CSK , for editing\nand evaluating plausible commonsense knowledge in\nTransformers. Given a plausible <Subject, Verb, Ob-\nject> commonsense statement, MEMIT CSK edits param-\neters at different token and layer locations (described\nin §3). Edited model is evaluated for semantic gener-\nalization (depicted in dark blue box) and configuration\ngeneralization defined in §3.\n(Bender and Koller, 2020; Marcus, 2021; Talmor\net al., 2019; Bhargava and Ng, 2022), and consis-\ntency errors (Tam et al., 2022; Devaraj et al., 2022;\nWeng et al., 2020). Retraining or finetuning LMs to\novercome these errors is costly and uninterpretable.\nTo address this, prior research (Meng et al., 2022,\n2023) has shown that model predictions often cor-\nrelate strongly with certain neuron activations and\nparameter editing methods can effectively correct\nencyclopedic factual mistakes.\nHowever, it remains unclear whether these edit-\ning methods can scale beyond encyclopedic facts\nto fix commonsense errors in Transformers. Com-\nmonsense knowledge involves more uncertainty\nand variation than encyclopedic knowledge. Con-\nsider a subject-verb-object triple (s,v,o ). In the\nencyclopedic domain, s and v often map to one\n“o”, e.g., the Eiffel Tower is located in the city of\n“Paris”. On the contrary, commonsense knowledge\nis harder to enumerate and sand vcan be mapped\nto many “o”, e.g., an apple has colors that can plau-\nsibly be “green”, “red”, “yellow”, “white”, and\ntheir interpolation. We aim to answer (i) whether\n8214\ncommonsense plausibility information is also lo-\ncalized in specific hidden states of Transformers ,\nand if so, (ii) can model editing on those units ef-\nfectively repair incorrect commonsense plausibility\njudgments?\nTo this end, we focus on the subject-verb-object\nbinary plausibility classification task utilizing two\ncommonsense datasets, 20 Questions (20Q, Po-\nrada et al. (2021)) and Physical Plausibility Com-\nmonsense (PEP3k, Wang et al. (2018)). We per-\nform causal mediation analysis (Pearl, 2001; Vig\net al., 2020; Meng et al., 2022) on GPT-2 Large\nand XL models and their fine-tuned checkpoints\n(Base-finetuned models), at various part-of-speech\nlocations. While the zero-shot models perform\npoorly on the task and exhibit no causal pattern,\nwe find clear causal associations between predic-\ntions and localized parameters at subject, verb, and\nobject locations in the Base-finetuned models. We\nthen investigate if we can edit relevant parameters\nin the Base-finetuned models to correct their mis-\ntakes. While directly applying the MEMIT editing\nalgorithm (Meng et al., 2023) to edit subject tokens\nresults in sub-par performances, we extend MEMIT\nto MEMIT CSK by editing various token locations\nand improving the edit layer selection strategy.\nWe demonstrate the advantage of MEMIT CSK\ncompared to fine-tuning the model (“repair-\nfinetuning”)2 from two angles: semantic gener-\nalization and configuration generalization. Se-\nmantic generalization requires that commonsense\njudgments are repaired while their paraphrases,\nneighbors, and reasoning-based queries are also an-\nswered correctly – some should be affected and oth-\ners unaffected by the editing. We create a PROBE\nSET for 20Q and PEP3k datasets to contain effi-\ncacy, unaffected neighborhood, affected neighbor-\nhood, affected paraphrase, and affected reasoning\nchallenges. We also evaluate configuration gen-\neralization for each method to determine whether\na strategy (hyperparameter combination) picked\non an EDIT VALIDATION SET can achieve good\nperformance on a separate EDIT SET . Our pro-\nposed framework for editing and evaluating com-\nmonsense knowledge in transformers is depicted in\nFig. 1.\nOur contributions are five-fold. (1) We show\nstrong causal associations between commonsense\njudgments and localized parameters in Base-\n2We refer to fine-tuning to repair incorrect predictions as\n“repair-finetuning” to differentiate from the initial fine-tuning\nwe perform to fit GPT2 for the task (“base-finetuning”).\nfinetuned GPT-2 Large and XL models. (2) We\nextend the MEMIT editing algorithm toMEMIT CSK\nby varying edit tokens and improving the edit layer\nselection strategy, resulting in 4.58% and 1.99% F1\nimprovement for GPT-2 XL on EDIT VALIDATION\nSET of PEP3k and 20Q. (3) GPT-2 XL edited by\nMEMIT CSK outperforms repair-finetuned baselines\nby 10.97% and 10.73% F1 on the EDIT SET of\nPEP3k and 20Q, exhibiting favorable configuration\ngeneralization. (4) GPT-2 XL edited byMEMIT CSK\nperforms well across the affected and unaffected\nmetrics in our constructed PROBE SET for seman-\ntic generalization, while fine-tuned baselines ex-\nhibit significant tradeoffs between unaffected and\naffected metrics. (5) We show that edited mod-\nels achieve clearer associations between judgments\nand localized parameters on previously incorrectly\npredicted samples, solidifying the correlation be-\ntween causal analyses and performances. These\nresults suggest a compelling future direction of in-\ncorporating feedback about common sense in trans-\nformers on the fly through direct model editing.\n2 Background\nThe MEMIT (Mass Editing Memory in a Trans-\nformer) method proposed by Meng et al. (2023)\ndemonstrates its effectiveness in editing up to\n10,000 factual associations in transformer models\non zsRE (Levy et al., 2017) and their proposed\nCOUNTERFACT dataset, designed to test factual\nrecall. We describe some background here but oth-\nerwise refer the reader to Appendix A.1 and Meng\net al. (2022, 2023) for a more detailed description.\n2.1 Causal Tracing\nGiven a model, the method takes a concatenation\nof subject sand verb vas input prompt xand pre-\ndicts the corresponding object o as prediction y.\nFor a correctly-predicted (x,y) pair, causal trac-\ning consists of the following three steps: Clean\nrun – The input prompt is provided to the model\nand the predicted probability of the correct object,\nP [y], is calculated; Corrupted run – The subject\ntokens are corrupted with noise and the correspond-\ning probability of the ground truth object, P∗[y],\nis computed; Corrupted-with-restoration run –\nThe same corrupted input is given, but at a certain\ntoken iand layer l, the model is forced to output\nthe clean state activation from the clean run. In\nthis setting, the probability of the correct object,\nP∗, clean h(l)\ni\n[y], is computed.\n8215\nTotal effect (TE) is defined as P [y] −P∗[y],\nwhile the indirect effect (IE) of a specific hidden\nstate hl\ni is defined as P∗, clean h(l)\ni\n[y] −P∗[y]. The\naverage total effect (ATE) and average indirect ef-\nfect (AIE) are computed across multiple examples\nfor each hidden state.\nSevered Causal Tracing: To disentangle the im-\npact of MLP and attention in each layer, MEMIT\nanalyzed the effect on the attention layer by fixing\nthe MLP output at the corrupted run value, so that\nit is unaffected when inserting clean state hl\ni. This\ncan be viewed as severing the MLP effect when\nanalyzing the effect on attention. Similarly, this\ncan be done by severing attention layers.\n2.2 Memory Editing\nMEMIT identified the crucial parameters signifi-\ncantly impacting the model’s prediction through\ncausal tracing. They selected the layer with the\nhighest AIE and its preceding layers as the edit\nlayers R. We extend MEMIT’s editing strategy, de-\nscribed in Meng et al. (2023), to the commonsense\ndomain.\n3 Method\nWe now set out to investigate our main research\nquestion: is commonsense plausibility informa-\ntion also localized in specific MLP hidden states\nof an LM, and, if so, can MEMIT-style editing\neffectively repair incorrect commonsense plausi-\nbility judgments?\nTo investigate this, we conduct experiments that\naddress important sub-questions, focusing specifi-\ncally on the commonsense plausibility task (Porada\net al., 2021). The task is to predict a label y ∈\n{True,False}given an input triple x = (s,v,o ).\nAn example can be seen in Fig. 13.\n3.1 Is high task performance needed to\nachieve a strong causal tracing result?\nBecause model parameter editing relies on select-\ning a token and layer position based on the maxi-\nmum AIE, we hypothesize that model performance\nmay impact the resulting causal tracing graph. In\n3This dataset framing addresses challenges with the direct\nanalog of the factual recall task (predicting y = o given\nx= (s,v)) in the commonsense setting. Since there can be\nmultiple correct object completions for commonsense, e.g.,\nrice, meat, bread are all valid completions for the phrase\nPeople eat _, evaluating only the argmax completion does not\nrigorously assess a model’s understanding of plausibility of\nevents.\nparticular, since a model that performs near-random\non a task will also perform close-to-random during\na corrupted run, overall AIEs may be low as a result.\nThis relationship has not been investigated in prior\nwork — in contrast to the factual encyclopedic\ndatasets used in previous studies, the zero-shot per-\nformance of language models on the commonsense\nplausibility task can be poor. Thus, we perform\ncausal tracing on commonsense datasets in two ex-\nperimental settings: zero-shot (Meng et al., 2022),\nand after fine-tuning models on plausibility tasks;\nwe refer to this fine-tuning as base-finetuning.\n3.2 Does the part of speech and model layer\nlocations affect causal tracing conclusions\nand edit success?\nPrior work on editing encyclopedic knowledge fo-\ncuses on subject corruption and editing since fac-\ntual knowledge is mostly associated with the sub-\nject and the object is directly predicted. In contrast,\ncommon sense and plausibility judgments depend\non each element of the sentence. Therefore, we an-\nalyze three types of corruption and edit locations:\nsubject, verb, and object.\nMEMIT (Meng et al., 2023) edits a five-layer\nwindow whose last layer has the highest AIE in the\nsevered causal graph. This strategy only consid-\nered the last layer effect but ignored all the other\nlayers in the window. To mitigate this, we consider\nedit layers as a hyperparameter and search from\na list of MEMIT’s five-layer window and also the\nwindow having max moving average of AIE4. A\ndetailed explanation of our layer selection strategy\nis presented in Appendix A.7. We denote our mod-\nified editing method with varying edit tokens and a\nmore robust layer selection strategy as MEMIT CSK .\n3.3 Does MEMIT CSK exhibit configuration\ngeneralization?\nPrior work on model editing tunes hyperparame-\nters and reports performances of editing algorithms\non the same data splits. We study configuration\ngeneralization – whether editing hyperparameters\npre-selected on some data can be effectively trans-\nferred to an unseen data split. The motivation is that\nrunning parameter sweeps on new data points for\nediting can be time-consuming and costly. Since\ncommonsense knowledge is innumerable, it is fa-\nvorable if users may provide contextual feedback\n4We also consider neighboring windows shifted by 1 layer,\nexploring windows of size 3 and 5 within this space.\n8216\nto change model behaviors on the fly using pre-\nselected hyperparameters. We thus create an EDIT\nVALIDATION SET and an EDIT SET for each dataset.\nWe select hyperparameters on the EDIT VALIDA -\nTION SET and study the transferability of the best-\nfound setting of MEMIT CSK and repair-finetuning\nbaselines to EDIT SET (§5.3).\n3.4 Does MEMIT CSK exhibit semantic\ngeneralization?\nIt is not enough to report the success of a direct\nediting method on the original dataset since edit\nmethods can (and should) have propagational ef-\nfects on instances beyond the dataset (Meng et al.,\n2022). To compare and assess semantic general-\nization of updates, we augment incorrectly pre-\ndicted samples with neighborhood instances and\nparaphrases that should be affected by an edit, sim-\nilar to the prior fact editing work. We additionally\ninclude neighborhood instances that should not be\naffected. Performance on the unaffected neigh-\nborhood measures the update’s specificity, while\nperformance on the affected neighborhoods and\naffected paraphrases indicates its generalization.\nAdditionally, editing the plausibility of a com-\nmonsense statement should affect reasoning chains\ninvolving that statement. Entities and knowledge\nare interconnected, often requiring updates to one\ncomponent of commonsense knowledge when mod-\nifying another. To this end, we add a fourth cat-\negory of augmentations, affected reasoning, to\ntest whether successful edits correct aspects of a\nmodel’s commonsense reasoning. The augmenta-\ntions, which form the PROBE SET , are excluded\nduring editing and solely used for evaluation pur-\nposes. We provide examples in Fig. 1 and Table 1.5\n3.5 Does MEMIT CSK outperform finetuning\nfor repairing commonsense knowledge?\nTo answer our main research question, we com-\npare MEMIT CSK applied to the MLP hidden states\nmost strongly identified by our causal tracing ex-\nperiments against finetuning baselines, which we\nrefer to as repair-finetuning. We compare both\nmethods’ performance on edit efficacy (how many\nincorrect predictions are fixed), overall F1 score\nand relapse (how much the edit hurts by changing\npreviously correct predictions), and semantic gen-\neralization metrics. Unlike prior work, we also in-\nvestigate whether such improvements exhibit them-\n5More details about dataset construction are in §4.2.1.\nselves in causal patterns by repeating the causal\ntracing experiments on the MEMIT CSK -edited and\nrepair-finetuned models, to solidify the tie between\ndiscovery and correction.\n4 Experimental Setup\n4.1 Models\nWe perform experiments on GPT-2 Large and\nXL (Radford et al., 2019). 6 We finetune check-\npoints from Huggingface Transformers (Wolf\net al., 2020) on Training Sets to obtain Base-\nfinetuned models (Base Model), whose mis-\ntakes are then repaired by MEMIT CSK or repair-\nfinetuning. The base-finetuning hyperparameters\nare in Appendix A.5. All predictions are made by\narg maxy∈{True,False}p(y|x) where xis a common-\nsense subject-verb-object statement.\n4.2 Data and Evaluation\nWe use Porada et al. (2021)’s versions of two com-\nmonsense plausibility datasets, PEP3k and 20Q.\nWe build three splits from each dataset: Training\nSet, EDIT VALIDATION SET , and EDIT SET . Since\nzero-shot GPT-2 Large and XL perform poorly on\nPEP3k and 20Q out-of-the-box, we create Training\nSets for base-finetuning the models on the task.\nThe Training Set and EDIT VALIDATION SET\nare formed by randomly dividing the validation set\nfrom Porada et al. (2021) into an 80%-20% split.\nThe EDIT SET is created using the test set from Po-\nrada et al. (2021). Because both datasets’ instances\nare unnatural (e.g., “man swallow paintball”), we\nuse GPT-3 text-davinci-003 to reformat them\ninto natural language while retaining the (s,v,o )\nformat, e.g., “A man swallows a paintball”. More\ndetails and dataset statistics are in Appendix A.2.\nWe report three metrics on the EDIT VALIDA -\nTION SET and EDIT SET : F1 Score (↑), a measure\nof overall performance; Efficacy (↑), the percent-\nage of previously-incorrect predictions which are\ncorrected by an update method; and Relapse (↓),\nthe percentage of instances which were previously\npredicted correctly but are now predicted incor-\nrectly following an update.\n4.2.1 Constructing the PROBE SET\nFor the subset of EDIT SET that was incorrectly pre-\ndicted by both GPT-2 Large and XL Base Model,\n6We report GPT-2 XL results in the main paper. GPT-2\nLarge results and similar findings are in the Appendix.\n8217\nStatement PlausibilityLabel UnaffectedNeighborhoodAffectedNeighborhood Affected Paraphrase Affected Reasoning\nPEP3k Rocksabsorbs oil Dirtabsorbs oil Ground takes in oil Oil is liquid, so it spreads over surfaceSoil absorbsfire Soilconsumesoil Dirt soaks up oil Soil is porous, so it can absorb oilSoil absorbs oil True Soil absorbsgrease Land absorbs oil\nHousekick ball Plantkick ball Tree was used to propel a ball Tree doesn’t have legsTree kickrock Treestrikeball Tree was used to kick a ball Legs are needed to kick ballTree kick ball False Tree kicksphere Tree was used to hit a ball\n20Q Treesblock sun Shadesblock sun Sunglasses act as a shield from sun Sunglasses have dark lensesSunglasses blockrain Sunglassesobscuresun Sunglasses obstruct the sun’s light Dark lenses reduce light that enters eyesSunglasses block sun True Sunglasses blocklight Sunglasses filter out sun’s brightness\nComputersmake noiseFixturesmake noise Furniture can be noisy Furnishings are inanimate objectsFurnishings makecolorFurnishingsproducenoise Furniture can create sound Inanimate objects cannot make noiseFurnishings make noise False Furnishings makesoundFurniture can be a source of noise\nTable 1: Examples chosen through random sampling from the PEP3k and 20QPROBE SET . Unaffected neighborhood\nsamples are created by individually augmenting the subject and object with different, but relevant instances from\nthe source statement. Likewise, affected neighborhood samples are created by individually augmenting the subject,\nverb, and object with synonymous instances from the source statement. Further details are in §4.2.1.\nwe augment each instance with neighborhood in-\nstances that should or should not be affected by an\nedit that fixes the incorrect prediction on the dataset\ninstance using GPT-3 (details in Appendix A.9).\nWe combine the incorrectly predicted instances\nfrom EDIT SET and the per-instance augmentations\nto form the PROBE SET for evaluating semantic\ngeneralization. Dataset examples are in Table 1\nand statistics in Appendix A.2.\nUnaffected Neighborhood. To evaluate the\nspecificity of the edits, for each {s,v,o }, we gener-\nate a set of relevant but different instances (s′,v,o )\nand (s,v,o ′) that should not change when {s,v,o }\nis edited. The metric measures the percentage\nof post-update predictions arg maxP(s′,v,o ) and\narg maxP(s,v,o ′) that remain equivalent to pre-\nupdate predictions.\nAffected Neighborhood. To assess the im-\npact of changes on similar meaning prompts for\neach (s,v,o ), we generate a set of synonyms\nas (s′,v,o ), (s,v′,o) and (s,v,o ′). The score\nmeasures the percentage of post-update predic-\ntions arg maxP(s′,v,o ), arg maxP(s,v′,o) and\narg maxP(s,v,o ′) which are equal to the ground\ntruth label for (s,v,o ).\nAffected Paraphrase. To evaluate the im-\npact on synonymous prompts, we generate a\nset of paraphrases as (s′,v′,o′). Since para-\nphrases should also be successfully edited, the\nmetric is the percentage of post-update predictions\narg maxP(s′,v′,o′) which are equal to the ground\ntruth label for (s,v,o ).\nAffected Reasoning. To assess the updated\nmodel’s connectivity, we generate a two-step chain\nof valid reasoning prompts {R1,R2}. For instance,\nwith the phrase “Furnishings do not make noise”,\nR1 could be “Furnishings are inanimate objects”,\nand R2 = “Inanimate objects cannot make noise”.\nThe metric is the percentage of post-update predic-\ntions arg maxP(R1) and arg maxP(R2) which\nare equal to the True label.\n4.3 Editing and Finetuning Methods\nWe select hyperparameters to maximize F1 on the\nEDIT VALIDATION SET (§3.3). For editing, we\nsearch for the edit layer range, edit token position\n(last {s,v,o }), and learning rate. For the repair-\nfinetuning baseline, we search for the learning rate,\nbatch size, and the number of epochs.\nFor editing, we perform causal tracing on the\ncorrectly-predicted samples of the EDIT VALIDA -\nTION SET to inform layer selection. We apply\nrepair-finetuning and editing methods to repair\nincorrect predictions on EDIT VALIDATION SET ,\nEDIT SET , and PROBE SET .\nWe explore two variants of repair-finetuning.\nRFTFixed Epoch uses the same exact configuration\nfound on EDIT VALIDATION SET . We hypothe-\nsize that it is prone to overfitting due to the ab-\nsence of early stopping. To maximize the potential\nof repair-finetuning, we analyze another variant\nRFTEarly Stop, which runs for a maximum of 10\nepochs and selects the checkpoint with the highest\nF1 score on the entire EDIT SET . This should miti-\ngate overfitting and reduce relapse. In contrast, the\nediting experiments always use the exact configu-\nration obtained from EDIT VALIDATION SET .\n8218\n5 Results & Discussion\n5.1 High task performance is crucial for\nachieving strong causal tracing results\nZero-shot prompting produced near random accu-\nracies (51.30% and 51.87% on the EDIT VALIDA -\nTION SET split of PEP3k and 20Q respectively for\nGPT-2 XL) and chaotic causal patterns with no lo-\ncalization as shown in Fig. 27. In contrast, the Base\nModel exhibited significantly superior performance\n(77.12% on PEP3k and 73.96% on 20Q) and the\nresulting causal patterns were more distinct with\na substantially higher AIE and strong localization.\nTherefore, we deduce that a significant correlation\nexists between high task performance and strong\ncausal patterns, and use the Base Model for editing\nexperiments.\n(a) Base Model with 77.12% accuracy.\n(b) Zero-shot model with 51.30% accuracy.\nFigure 2: Base-finetuned vs. Zero-shot GPT-2 XL\ncausal tracing on PEP3k EDIT VALIDATION SET . Pat-\nterns are unclear for the Zero-shot model while they are\ndistinct for the Base Model. Consistent observations are\nfound for the 20Q dataset (Fig. 6).\n5.2 Targeted part of speech and layer\nlocations affect causal tracing conclusions\nand edit success\nAs shown in Fig. 3, the last token at the later layers\nhas a high AIE which is trivial since fixing hidden\nstates or MLPs in those layers restores most of the\nrequired information. We also observed strong AIE\n7Normalization with domain conditional Pointwise Mutual\nInformation (PMI; Holtzman et al., 2021) did not result in any\nsignificant improvement with accuracy PMIDC = 52.94% on\nPEP3k.\nat the earlier layers for the corrupted tokens. This\nfinding is non-trivial and emphasizes the impor-\ntance of earlier layers while predicting plausibility.\nAIE is more pronounced at the last corrupted token\ncompared to the first corrupted token consistently\nacross all models and datasets. Therefore, we fo-\ncus on the last (s,v,o ) editing. Additional causal\ntracing results are present in Appendix A.10.\nFig. 4 compares the average AIE at last corrupted\ntoken for unmodified, severed MLP and Attention\ncausal graphs for all edited tokens. We notice a\nclear gap in AIE for MLP graphs at the earlier\nlayers. This observation aligns with previous ob-\nservations in MEMIT for encyclopedic knowledge.\nIn contrast to encyclopedic facts, we observed the\nhighest AIE in earlier MLP layers instead of middle\nlayers. This demonstrates the importance of earlier\nlayers in commonsense predictions. Interestingly,\nin the object corruption plot, we observed a small\npeak at the beginning, before the highest AIE later.\nWe thus expanded the hyperparameter space to in-\nclude the initial layer windows for the object edit\nlayers. Table 2 presents edit layers included in hy-\nperparameter search with the max moving average\nof AIE, comparing windows of size 3 and 5 using\ndifferent editing tokens {s,v,o }. In all cases, the\nmax moving average resulted in a different set of\nlayers selected than MEMIT, where the max AIE\nlayer is used to edit 5 layers- the selected layer and\nthe previous 4 layers.\nModel Edit TokenLayer with\nMax AIE\nLayers with Max Moving\nAverage AIE\nWindow=3 Window=5\nGPT-2 Large Last Subject 8 8,9,10 8,9,10,11,12\nGPT-2 Large Last Verb 4 4,5,6 4,5,6,7,8\nGPT-2 Large Last Object 12 11,12,13 10,11,12,13,14\nGPT-2 XL Last Subject 5 4,5,6 2,3,4,5,6\nGPT-2 XL Last Verb 5 5,6,7 3,4,5,6,7\nGPT-2 XL Last Object 12 10,11,12 9,10,11,12,13\nTable 2: Layer with max AIE and set of layers with max\nmoving average AIE for the PEP3k EDIT VALIDATION\nSET\nThese two changes resolve in MEMIT CSK . Ta-\nble 3 compares original MEMIT 8 (only sub-\nject edit with fixed edit layers) with the best-\nperforming edit of MEMIT CSK on EDIT VALIDA -\nTION SET . MEMIT CSK consistently outperforms\nMEMIT across datasets and models.\n8Detailed results are in Appendix A.4.\n8219\nBest AIE on\nlast subject token = 0.078\nBest AIE on\nlast subject token = 0.085\nBest AIE on\nlast subject token = 0.029\n(a) Subject corruption\nBest AIE on\nlast verb token = 0.146\nBest AIE on\nlast verb token = 0.148\nBest AIE on\nlast verb token = 0.033\n(b) Verb corruption\nBest AIE on\nlast object token = 0.186\nBest AIE on\nlast object token = 0.184\nBest AIE on\nlast object token = 0.056\n(c) Object corruption\nFigure 3: Causal tracing for GPT-2 XL Base Model on PEP3k EDIT VALIDATION SET when different tokens are\ncorrupted, {s,v,o }(in order). See Appendix A.10 for GPT-2 Large and 20Q results.\nFigure 4: Severed causal tracing results for {s,v,o }for\nGPT-2 XL base on PEP3k EDIT VALIDATION SET\nDataset Model MEMIT\nF1 Score %\nMEMITCSK\nF1 Score %\nPEP3K GPT-2 Large 88.53 93.78 (+5.25)\nGPT-2 XL 90.51 95.09 (+4.58)\n20Q GPT-2 Large 85.31 87.09 (+1.78)\nGPT-2 XL 90.32 92.31 (+1.99)\nTable 3: Comparison of MEMIT and best performing\nMEMIT CSK on EDIT VALIDATION SET . MEMIT editing\nis on s, while MEMIT CSK is on best among {s,v,o }.\n5.3 MEMIT CSK exhibits configuration\ngeneralization\nTable 4 reports GPT-2 XL results for EDIT VALI -\nDATION SET and EDIT SET 9. The GPT-2 Large\nresults are in Appendix A.6 Table 12. For the EDIT\nVALIDATION SET performance, the verb edit F1\nscore is higher by +17.97% compared to the Base\n9The best hyperparameters are detailed in Appendix A.5.\nThe KL divergence and cut-off factors can potentially enhance\nthe performance for editing methods, see Appendix A.8.\n8220\nDatasetUpdate\nMethod Edit Token Edit Layers EDIT VALIDATION SET EDIT SET\nF1 Score % Efficacy % Relapse % F1 Score % Efficacy % Relapse %\nPEP3k\nBase Model - - 77.12 0 0 76.47 0 0\nRFTEarly Stop - - 90.16 (+13.05) 97.14 11.87 80.93 (+4.46) 50.83 9.82\nRFTFixed Epoch- - 90.16 (+13.05) 97.14 11.87 56.89 (-19.58) 98.89 55.25\nEdit Last Subject 1,2,3,4,5 90.51 (+13.39) 80 6.36 84.72 (+8.25) 77.22 12.98\nEdit Last Verb 6,7,8 95.09 (+17.97) 92.86 4.24 91.90 (+15.43) 88.33 7.00\nEdit Last Object 3,4,5 94.43 (+17.32) 91.43 4.66 86.69 (+10.22) 72.78 8.97\n20Q\nBase Model - - 74.73 0 0 75.77 0 0\nRFTEarly Stop - - 85.71 (+10.98) 80.46 12.40 77.36 (+1.60) 30.97 7.8\nRFTFixed Epoch- - 85.71 (+10.98) 80.46 12.40 48.02 (-27.74) 88.63 64.96\nEdit Last Subject 2,3,4,5,6 92.31 (+17.58) 79.69 3.43 86.46 (+10.70) 65.73 6.90\nEdit Last Verb 3,4,5,6,7 82.64 (+7.91) 44.53 4.49 79.03 (+3.27) 35.91 7.11\nEdit Last Object 1,2,3 91.12 (+16.39) 89.06 8.18 88.09 (+12.33) 76.60 8.21\nTable 4: Configuration generalization results based on the best hyperparameters identified forEDIT VALIDATION SET\nand applied to EDIT SET for GPT-2 XL. The editing methods display high configuration generalization compared to\nrepair-finetuning. Refer to §5.3 for further discussion. GPT-2 Large results are in Appendix A.6 Table 12.\nModel in PEP3k. The object edit F1 score is higher\nby +17.58% in 20Q. This indicates the importance\nof varying editing tokens. The best editing method\noutperforms repair-finetuning baseline consistently\nfor both datasets with much lower relapsed scores.\nThe editing method continues to perform well\nafter transferring the best hyperparameters to EDIT\nSET ; in comparison, both repair-finetuning base-\nlines performance drops significantly. Noticeably,\nRFTFixed Epoch method has high efficacy but a much\nhigher relapse score, between 38.36-64.96%, caus-\ning a significant decrease in the F1 score due to\noverfitting. The three editing methods on {s,v,o }\noutperform the repair-finetuning methods by 10.54-\n15.43% for the updated F1 score, exhibiting a bet-\nter configuration generalization performance.\n5.4 MEMIT CSK exhibits semantic\ngeneralization\nTable 5 shows GPT-2 XL results on PROBE\nSET 10. Compared to the editing methods, the repair-\nfinetuning baselines struggle to balance the affected\nand unaffected samples. RFTEarly Stop performs\nwell in unaffected neighborhoods but struggles\nwith the affected statements (measured by aver-\nage). RFTFixed Epoch reached higher performance\non affected subsets but suffered with unaffected\nneighborhoods. In comparison, the editing meth-\nods showed balanced improvements across metrics.\nWe also noticed that the affected neighborhood\nscores are generally high except for the specific\nediting token; e.g., while editing the object token,\nthe affected object neighborhood score is low.\n10Base Model has 0% efficacy on PROBE SET by design.\nVerb corruption, best AIE on last verb token = 0.200\n(a) RFTEarly Stop model with 90.16% F1 Score\nVerb corruption, best AIE on last verb token = 0.468\n(b) MEMIT CSK vedited model with 95.09% F1 Score\nFigure 5: Causal tracing for GPT-2 XL models on suc-\ncessfully corrected statements in the PEP3k EDIT VAL -\nIDATION SET . For the RFTEarly Stop model, we observe\nsimilar patterns as Fig. 3 for both token corruptions. For\nthe edited model, an improved pattern is observed at v.\n5.5 MEMIT CSK outperforms fine-tuning for\nrepairing commonsense knowledge\nTo measure improvement, we re-conduct causal\nanalysis via each token {s,v,o }corruption us-\ning successfully edited statements. Fig. 5 dis-\nplays the causal graphs for best-performing edit: v\nedited model and the best repair-finetuned model:\nRFTEarly Stop based on Table 4.\nFor RFTEarly Stop (F1 Score 90.16%), the overall\n8221\nDatasetUpdate\nMethod Edit TokenEfficacy\n%\nUnaffected\nNeighborhood %\nAffected\nNeighborhood % Affected\nParaphrase\n%\nAffected\nReasoning\n%\nAverage\nUnaffected\n%\nAverage\nAffected\n%Subject Object Subject Verb Object\nPEP3k\nBase Model - 0 100 100 21.01 23.45 23.3 33.64 31.13 100 26.51\nRFTEarly Stop - 39.63 77.21 76.98 33.88 37.5 39.16 39.91 55.66 77.09 41.22\nRFTFixed Epoch- 99.62 24 27.78 86.35 84.31 87.15 68.7 62.26 25.89 77.75\nEdit Last Subject 72.08 81.21 64.15 27.7556.83 58.67 47.92 35.85 72.68 45.40\nEdit Last Verb 87.92 59.24 57.06 57.36 34.47 71.05 37.41 31.13 58.15 46.28\nEdit Last Object 75.47 58.11 57.82 57.5254.35 27.86 43.99 34.72 57.96 43.69\n20Q\nBase Model - 0 100 100 33.02 24.78 29.38 33.70 39.38 100 32.05\nRFTEarly Stop - 21.52 87.01 86.89 36.21 34.72 37.01 38.27 35.41 86.95 36.32\nRFTFixed Epoch- 79.27 38.85 37.89 67.40 64.41 64.44 60.20 40.72 38.37 59.43\nEdit Last Subject 61.94 87.22 71.00 34.48 57.47 58.82 51.75 37.93 79.11 48.09\nEdit Last Verb 35.70 90.18 83.21 37.44 29.80 47.25 35.91 37.80 86.69 37.64\nEdit Last Object 72.18 76.24 92.26 61.90 60.37 33.76 47.87 41.90 84.25 49.16\nTable 5: Efficacy and semantic generalization results on PROBE SET for GPT-2 XL. Balanced improvements are\nobserved for editing methods across metrics, with the sand oedits performing the best. Refer to §5.4 for a detailed\ndiscussion. GPT-2 Large results are in Appendix A.6 Table 13.\ncausal pattern and AIE remain similar to the Base\nModel in Fig. 3. In contrast, the vedited model (F1\nScore 95.09%) shows an enhanced AIE for all types\nof corruption. Specifically, a high AIE of 0.468 is\nrecorded at the last verb token for verb corruption.\nThese findings confirm that localization and AIE\nimprove for the edited model at the edit location.\n6 Related Work\nEarly works on model editing focused on updat-\ning individual neurons using constrained finetuning\n(Sinitsin et al., 2020; Zhu et al., 2020) or hypernet-\nworks (De Cao et al., 2021; Mitchell et al., 2022a;\nHase et al., 2023b). A related line of work has\nfocused on storing updates in an external memory\n(Jin et al., 2021; Mitchell et al., 2022b; Tandon\net al., 2022, inter alia). Recent works (Hoelscher-\nObermaier et al., 2023; Zhong et al., 2023; Brown\net al., 2023; Onoe et al., 2023) offer more compre-\nhensive evaluations for fact-editing methods.\nInspired by the linear associative memory prop-\nerty of feedforward layers in Transformers (Ander-\nson, 1972; Geva et al., 2021, 2022) and success\nwith the approach in convolutional models (Bau\net al., 2020), recent works have proposed to edit\nMLP weights directly (Meng et al., 2022; Dai et al.,\n2022; Yao et al., 2022). In the encyclopedic factual\ndomain, Meng et al. (2022) proposed to edit single\nfacts by fitting a Rank One Model Edit (ROME)\nto the parameters of an MLP layer, and showed it\noutperformed prior methods. Our work builds on\nMeng et al. (2023), which extended this approach\nto thousands of edits by altering the weights of a\nrange of MLP layers. Hase et al. (2023a) demon-\nstrate that many early edit layers can work well\nwith MEMIT ; this partially motivates our exten-\nsive layer hyperparameter search. Recent work by\nCohen et al. (2023) proposes a dataset for evalua-\ntion of a variety of ripple effects in editing methods\nwith factual knowledge and concludes that models\nfail to capture these effects. All aforementioned\nworks focus on encyclopedic factual knowledge,\nunlike ours.\n7 Conclusion\nThis paper demonstrates strong causal relations be-\ntween commonsense plausibility judgments and\nearly MLP layers in Transformers. These param-\neters are directly editable for repairing common-\nsense mistakes. We improve the MEMIT parameter\nediting algorithm to MEMIT CSK for commonsense\nplausibility prediction by varying edit tokens and\nby improving the layer selection strategy. GPT-2\nLarge and XL models edited by MEMIT CSK out-\nperform repair-finetuned baselines by more than\n10% F1 score on EDIT SET . Additionally, we con-\nstruct a PROBE SET that contains unaffected and\naffected neighborhoods, affected paraphrases, and\naffected reasoning challenges for comprehensive\nevaluation. MEMIT CSK effectively generalizes on\nrelated and unrelated neighborhoods annotated in\nour PROBE SET , exhibiting semantic generalization\nwhile repair-finetuned baselines demonstrate sig-\nnificant trade-offs between unaffected and affected\nmetrics. These results indicate a compelling di-\nrection of incorporating feedback about common\nsense in transformers on the fly through direct\nmodel editing.\n8222\nLimitations\nIn this work, we experiment with repairing com-\nmonsense mistakes by the GPT-2 Large and XL\nmodels. We are unable to investigate larger open-\nsourced models like GPT-J (Wang and Komat-\nsuzaki, 2021) and GPT-NeoX (Black et al., 2022)\ndue to resource limitations. Investigating the re-\nsearch questions described in §3 on larger models\nis a natural next step. We focus on the binary plau-\nsibility prediction task but envision that parameter\nediting could improve models on various common-\nsense tasks in future work.\nOur experiments show that the optimal edit to-\nken (subject, verb, or object) varies among datasets.\nThe specific location of a single generalized opti-\nmal edit token, if it exists, requires further inves-\ntigation, while different editing methods for com-\nmonsense knowledge can be proposed.\nEthics Statement\nThis study proposes a framework to evaluate and\ncorrect commonsense mistakes in GPT-2 models,\nfocusing on predicting the plausibility of com-\nmonsense statements. Commonsense knowledge\nis highly contextualized and varies significantly\nacross locations and cultures. Biases and stereo-\ntypes present in edit datasets may inadvertently\nlead to erroneous and potentially harmful model\njudgments. Malicious actors may exploit model\nediting to incorporate false information into mod-\nels. It is crucial to employ meticulously curated\ndatasets in future research and during the deploy-\nment of these models in real-world scenarios.\nAcknowledgments\nThis research was conducted at the University of\nMassachusetts Amherst under the Industry Men-\ntorship Program led by Prof. Andrew McCallum.\nWe are grateful for their support and resources pro-\nvided for this research.\nReferences\nJames A. Anderson. 1972. A simple neural network\ngenerating an interactive memory. Mathematical\nBiosciences, 14(3):197–220.\nDavid Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu,\nand Antonio Torralba. 2020. Rewriting a deep gen-\nerative model. In Computer Vision – ECCV 2020 ,\npages 351–369, Cham. Springer International Pub-\nlishing.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nPrajjwal Bhargava and Vincent Ng. 2022. Common-\nsense knowledge reasoning and generation with pre-\ntrained language models: A survey. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 36, pages 12317–12325.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95–136, virtual+Dublin. Association\nfor Computational Linguistics.\nDavis Brown, Charles Godfrey, Cody Nizinski,\nJonathan Tu, and Henry Kvinge. 2023. Robustness\nof edited neural networks. ArXiv preprint.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nRoi Cohen, Eden Biran, Ori Yoran, Amir Globerson,\nand Mor Geva. 2023. Evaluating the ripple effects of\nknowledge editing in language models.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493–\n8502, Dublin, Ireland. Association for Computational\nLinguistics.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6491–\n6506, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nAshwin Devaraj, William Sheffield, Byron Wallace, and\nJunyi Jessy Li. 2022. Evaluating factuality in text\n8223\nsimplification. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7331–7345,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghan-\ndeharioun. 2023a. Does localization inform editing?\nsurprising differences in causality-based localization\nvs. knowledge editing in language models. ArXiv\npreprint.\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-\nnitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2023b. Methods for measuring, up-\ndating, and visualizing factual beliefs in language\nmodels. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 2714–2731, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nJason Hoelscher-Obermaier, Julia Persson, Esben Kran,\nIoannis Konstas, and Fazl Barez. 2023. Detecting\nedit failures in large language models: An improved\nspecificity benchmark. ArXiv preprint.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn’t\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nXisen Jin, Arka Sadhu, Junyi Du, and Xiang Ren. 2021.\nGradient-based editing of memory examples for on-\nline task-free continual learning. In Advances in\nNeural Information Processing Systems.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 333–342, Vancouver,\nCanada. Association for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nGary Marcus. 2021. Experiments testing gpt-3’s ability\nat commonsense reasoning: results. Blogpost.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in gpt. In Advances in Neural Information\nProcessing Systems, volume 35, pages 17359–17372.\nCurran Associates, Inc.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian,\nYonatan Belinkov, and David Bau. 2023. Mass-\nediting memory in a transformer. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022a. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D Manning, and Chelsea Finn. 2022b. Memory-\nbased model editing at scale. In Proceedings of the\n39th International Conference on Machine Learning,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 15817–15831. PMLR.\nYasumasa Onoe, Michael JQ Zhang, Shankar Padman-\nabhan, Greg Durrett, and Eunsol Choi. 2023. Can\nlms learn new entities from descriptions? challenges\nin propagating injected knowledge. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics, Toronto, Canada. Associ-\nation for Computational Linguistics.\nJudea Pearl. 2001. Direct and indirect effects. In\nProceedings of the Seventeenth Conference on Un-\ncertainty in Artificial Intelligence , UAI’01, page\n411–420, San Francisco, CA, USA. Morgan Kauf-\nmann Publishers Inc.\nIan Porada, Kaheer Suleman, Adam Trischler, and\nJackie Chi Kit Cheung. 2021. Modeling event plau-\nsibility with consistent conceptual abstraction. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1732–1743, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblogpost.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\n8224\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In International Conference on\nLearning Representations.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDerek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah\nKwan, Mohit Bansal, and Colin Raffel. 2022. Evalu-\nating the factual consistency of large language mod-\nels through summarization. ArXiv.\nNiket Tandon, Aman Madaan, Peter Clark, and Yiming\nYang. 2022. Learning to repair: Repairing model out-\nput errors after deployment using a dynamic memory\nof feedback. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 339–352,\nSeattle, United States. Association for Computational\nLinguistics.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Investigating gender bias in lan-\nguage models using causal mediation analysis. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 12388–12401. Curran Associates,\nInc.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nSu Wang, Greg Durrett, and Katrin Erk. 2018. Model-\ning semantic plausibility by injecting world knowl-\nedge. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers), pages 303–308, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nRongxiang Weng, Heng Yu, Xiangpeng Wei, and Wei-\nhua Luo. 2020. Towards enhancing faithfulness for\nneural machine translation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2675–2684,\nOnline. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYunzhi Yao, Shaohan Huang, Li Dong, Furu Wei,\nHuajun Chen, and Ningyu Zhang. 2022. Kformer:\nKnowledge injection in transformer feed-forward lay-\ners. In Natural Language Processing and Chinese\nComputing, pages 131–143, Cham. Springer Interna-\ntional Publishing.\nZexuan Zhong, Zhengxuan Wu, Christopher D. Man-\nning, Christopher Potts, and Danqi Chen. 2023.\nMquake: Assessing knowledge editing in language\nmodels via multi-hop questions. ArXiv preprint.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\nArXiv preprint.\nA Appendix\nA.1 Causal Tracing Background\nGiven a model, the method takes a concatenation\nof subject s and verb v as input prompt x, then\npredicts the corresponding object oas prediction y.\nFor example, for the statement “Paris is the capital\nof ”, a model is tasked with predicting “ France”\nas the most-likely next token. Taking a correctly\npredicted x,y pair, Causal tracing consists of the\nfollowing three steps:\nStep 1: clean run. Given the input prompt\nx, they collect all hidden activation values{\nhl\ni |i∈[1,T] ,l ∈[1,L]\n}\nfrom the model, where\nT is number of input tokens in x and L is num-\nber of model layers. Concretely, for each input x,\nhl\ni(x) =hl−1\ni (x) +al\ni(x) +ml\ni(x) where al\ni is the\nattention value and ml\ni is the corresponding MLP\nvalue. The predicted probability of the correct ob-\nject is denoted as P [y].\nStep 2: corrupted run. In this setting, cer-\ntain part of the input prompt x is corrupted\nwith noise. In a clean run, x is embedded\nas\n[\nh(0)\n1 ,h(0)\n2 ...h (0)\nT\n]\n. However, here, they set\nh(0)\ni := h(0)\ni + ϵ, for all tokens i in the subject\ntoken11. The probability of ground truth value y\nproduced in this run is denoted as P∗[y]. Note that\nthe model prediction is likely to be incorrect due to\nthe noisy input.\n11ϵ∼N(0, v) and vis taken as three times the empirical\nstandard deviation of the embeddings corresponding to the\nsubject tokens.\n8225\nDataset NTrain NEV NE NP\nPEP3k 1,225 306 1,531 265\n20Q 2,006 507 2,548 381\nTable 6: Number of samples in the Training Set, EDIT\nVALIDATION SET , EDIT SET , and PROBE SET .\nType NPEP3k N20Q\nOriginal statement 265 381\nUnaffected subject neighborhood 1,325 1,894\nUnaffected object neighborhood 1,325 1,900\nAffected subject neighborhood 1,290 1,856\nAffected verb neighborhood 1,288 1,832\nAffected object neighborhood 1,292 1,848\nAffected paraphrase 1,323 1,905\nAffected reasoning 530 754\nTable 7: Number of samples in the PROBE SET .\nStep 3: corrupted-with-restoration-run. The\nmodel runs inference using the noisy input embed-\nding created in the corrupted run, with the differ-\nence that the model is also forced to output the\nclean state activation hˆl\nˆi at certain token ˆiand layer\nˆl. If the model successfully produces the correct\noutput using a small number of clean states, there\nis likely to be a strong casual relationship between\nthese states and the model output. The probability\nof the correct object is denoted as P∗, clean h(l)\ni\n[y].\nThe three runs produced P [y], P∗[y] and\nP∗, clean hl\ni\n[y]. Two metrics are then defined to mea-\nsure the states effect between these runs. Total\neffect (TE) is calculated as P [y]−P∗[y], while the\nindirect effect (IE) of a specific hidden state hl\ni is\ncalculated as P∗, clean h(l)\ni\n[y] −P∗[y]. The average\ntotal effect, ATE and average indirect effect, i.e.\nAIE, are computed across multiple examples for\neach hidden state.\nA.2 Datasets\nPhysical Event Plausibility (PEP3k; Wang\net al., 2018) consists of 3,062 statements in (subject\ns, verb v, object o) format about semantically plau-\nsible and implausible events. It covers a wide range\nof possible (but not necessarily common) events\nwith high annotator label agreement.\n20 Questions (20Q)12 is a dataset of 5,096 com-\nmonsense statements written by crowd annotators\n12https://github.com/allenai/twentyquestions\nin games of “20 questions” and labeled as plausible\nor implausible. We use the ( s,v,o) format of the\ndataset constructed by Porada et al. (2021), where\nx= (s,v,o ) and y∈{True,False }.\nExamples from each dataset are given in Table 1.\nStatistics of our created data splits are in Tables 6\nand 7\nA.3 Base Model vs. Zero-Shot for 20Q\nDataset\nComparison of Base Model and zero-shot model\nfor the 20Q dataset is in Fig. 6.\nA.4 Original MEMIT Editing Results\nTable 8 shows the detailed metrics and editing pa-\nrameters for MEMIT applied onEDIT VALIDATION\nSET .\nDataset ModelEditTokenEditLayersF1 Updated% Efficacy% Relapse%\nPEP3KGPT-2 Large Subject 4,5,6,7,8 88.53 (+13.36) 76.32 7.39GPT-2 XL Subject 1,2,3,4,5 90.51 (+13.39) 80 6,36\n20Q GPT-2 Large Subject 1,2,3,4,5 85.31 (+12.92) 71.43 9.26GPT-2 XL Subject 1,2,3 90.32 (+15.59) 84.38 7.65\nTable 8: Editing results after applying original MEMIT\non EDIT VALIDATION SET .\nA.5 Hyperparameters\nBase Finetuning The GPT-2 Large and XL mod-\nels are initially finetuned on the training set with the\nnext-token prediction objective. Table 9 presents\nthe optimal hyperparameters identified for the base-\nfinetuning method.\nDataset Model Learning Rate Batch Size Epochs\nGPT-2 Large 0.00009961 64 1020q GPT-2 XL 0.00001432 64 10\nGPT-2 Large 0.00002298 8 10PEP3k GPT-2 XL 0.00001023 32 20\nTable 9: Base Model hyperparameters for Training Set\nRepair Finetuning\nTable 10 shows the best hyperparameters for the\nrepair-finetuning method. The method was very\nsensitive to small changes in learning rate while\nthe other parameters worked well over a long range\nof values. Note that we use early stopping and\nrestore the weights to the best performing model\nbased on the F1 score.\n8226\n(a) Base Model with 73.96% accuracy.\n (b) Zero-shot model with 51.87% accuracy.\nFigure 6: Zero-shot vs. Base Model causal tracing results for GPT-2 XL on 20Q EDIT VALIDATION SET .\nDataset Model Learning Rate Batch Size Epochs\n20q GPT-2 Large 0.000003451 8 7\nGPT-2 XL 0.000001589 32 9\nPEP3k GPT-2 Large 0.00000474 32 7\nGPT-2 XL 0.000001313 8 10\nTable 10: Hyper-parameters for RFTFixed Epoch tuned for\nEDIT VALIDATION SET and applied to EDIT SET and\nPROBE SET\nMEMIT CSK\nThe Table 11 shows the hyper-parameters for the\nediting method. The method was slightly sensitive\nto the learning rate and very sensitive to the edit\ntoken. Note that a KL divergence factor of 0.0625\nwas used as the default value for all editing experi-\nments. Appendix A.8 contains an ablation study of\nthe KL divergence factor.\nDataset Model Edit Token Layers Learning Rate\n20q\nGPT-2 Large\nLast Subject 3,4,5 0.7868\nLast Verb 2,3,4,5,6 0.09393\nLast Object 1,2,3 0.6276\nGPT-2 XL\nLast Subject 2,3,4,5,6 0.04108\nLast Verb 3,4,5,6,7 0.01936\nLast Object 1,2,3 0.02689\nPEP3k\nGPT-2 Large\nLast Subject 4,5,6,7,8 0.32\nLast Verb 4,5,6,7,8 0.682\nLast Object 1,2,3,4,5 0.433\nGPT-2 XL\nLast Subject 1,2,3,4,5 0.1253\nLast Verb 6,7,8 0.08719\nLast Object 3,4,5 0.04107\nTable 11: Hyper-parameters for the editing method\ntuned for EDIT VALIDATION SET and applied to EDIT\nSET and PROBE SET .\nA.6 GPT-2 Large Results for Configuration\nand Semantic Generalization\nThe GPT-2 Large results for configuration gener-\nalization experiments are in Table 12. The GPT-2\nLarge results for semantic generalization experi-\nments are in Table 13.\nA.7 Layer Selection Strategy\nFor demonstration purposes let’s assume our model\nhas only 10 layers. The average indirect effects of\nthese layers at our desired edit token (let’s assume\nlast verb token) are:\n[0.0,0.1,0.2,0.3,0.5,0.4,0.4,0.3,0.2,0,0]\nLet’s also assume that we are considering only\n5 layer windows. The highest average indirect\neffect is observed at the 5th layer with value 0.5.\nAccording to MEMIT, the optimal edit layers will\nbe a 5 layer window ending at the highest AIE\nlayer, in this case it will be the layers 1,2,3,4,5.\nNow let’s calculate the moving average of 5 layer\nwindows. The moving average of layers 1-5 is\n(0.0 + 0.1 + 0.2 + 0.3 + 0.5)/5 = 0.22, similarly\nthe moving average of layers 2-6 will be (0.1 +\n0.2 + 0.3 + 0.5 + 0.4)/5 = 0.3 and so on. The\nmoving averages of all 5 layer windows are:\n[0.22,0.3,0.36,0.38,0.36,0.26]\nThe maximum moving average is observed for\nlayers 4-8 with value 0.38. In our method, we\nwould also consider layers 4-8 as in our hyperpa-\nrameter search space along with layers 1-5.\nA.8 Ablation Study\nKL Divergence Factor\nThe Table 14 shows how the performance of the\nediting method changes when varying the KL Di-\nvergence Factor in terms of Accuracy and F1 score.\nThe ablation study is conducted using the GPT-2\nLarge model on the PEP3k dataset, and the verb\ntoken is used for editing in the EDIT VALIDATION\nSET dataset. The chosen hyperparameters align\nwith those presented in Table 11.\n8227\nDatasetUpdate\nMethod Edit Token Edit Layers EDIT VALIDATION SET EDIT SET\nF1 Score % Efficacy % Relapse % F1 Score % Efficacy % Relapse %\nPEP3k\nBase Model - - 75.16 0 0 76.22 0 0\nRFTEarly Stop - - 95.75 (+20.59) 94.74 3.91 80.92 (+4.70) 40.93 6.60\nRFTFixed Epoch- - 95.75 (+20.59) 94.74 3.91 51.08 (-19.14) 100 55.70\nEdit Last Subject 4,5,6,7,8 88.53 (+13.36) 76.32 7.39 79.36 (+3.14) 54.95 12.77\nEdit Last Verb 4,5,6,7,8 93.78 (+18.62) 96.05 6.96 89.08 (+12.86) 93.68 12.34\nEdit Last Object 1,2,3,4,5 88.41 (+13.25) 86.84 10.87 77.65 (+1.43) 78.57 21.85\n20Q\nBase Model - - 72.39 0 0 74.07 0 0\nRFTEarly Stop - - 91.32 (+18.93) 97.86 11.17 76.45 (+2.37) 48.23 13.69\nRFTFixed Epoch- - 91.32 (+18.93) 97.86 11.17 69.92 (-4.15) 94.61 38.36\nEdit Last Subject 3,4,5 85.33 (+12.94) 75 10.63 81.97 (+7.90) 67.18 12.66\nEdit Last Verb 2,3,4,5,6 77.64 (+5.25) 38.57 7.36 77.33 (+3.26) 33.44 7.22\nEdit Last Object 1,2,3 87.09 (+14.71) 82.14 10.9084.61 (+10.54) 80.43 13.79\nTable 12: Configuration generalization results based on the best hyperparameters identified for theEDIT VALIDATION\nSET and applied to the EDIT SET for GPT-2 Large. The editing method displays high configuration generalization\nwhile both variants of the repair-finetuning method have a lower F1 Score on the EDIT SET . Refer to §5.3 for further\ndiscussion.\nDatasetUpdate\nMethod Edit TokenEfficacy\n%\nUnaffected\nNeighborhood %\nAffected\nNeighborhood % Affected\nParaphrase\n%\nAffected\nReasoning\n%\nAverage\nUnaffected\n%\nAverage\nAffected\n%Subject Object Subject Verb Object\nPEP3k\nBase Model - 0 100 100 19.77 18.94 23.84 34.16 32.08 100 25.76\nRFTEarly Stop - 30.57 83.92 82.64 30.93 30.67 35.53 38.85 33.77 83.28 33.95\nRFTFixed Epoch- 100.00 19.47 26.79 93.02 92.86 92.03 80.73 36.23 23.13 78.97\nEdit Last Subject 58.87 79.01 69.51 27.05 50.70 54.26 45.73 40.57 74.26 43.66\nEdit Last Verb 96.23 44.15 48.06 69.92 38.28 83.82 41.65 33.02 53.34 53.34\nEdit Last Object 82.26 44.15 73.28 71.78 69.25 36.15 53.74 46.04 58.71 55.3\n20Q\nBase Model - 0 100 100 30.23 22.93 27.11 32.76 27.72 100 28.15\nRFTEarly Stop - 29.66 88.07 87.05 39.17 37.34 39.23 42.05 27.59 87.56 37.08\nRFTFixed Epoch- 95.01 55.91 45 71.22 77.07 70.07 66.29 30.10 50.45 62.95\nEdit Last Subject 67.98 79.57 57.79 35.08 63.26 63.91 53.96 31.70 68.68 49.58\nEdit Last Verb 32.55 89.55 84.16 37.93 26.80 46.10 35.17 28.51 86.85 34.90\nEdit Last Object 81.89 66.95 85.32 71.98 71.67 34.79 51.60 35.68 76.13 53.14\nTable 13: Efficacy and semantic generalization results for the PROBE SET for GPT-2 Large. Balanced improvements\nare observed for editing methods across metrics, with the object token editing method performing the best. In\ncomparison, the repair-finetuning models show skewed performance between unaffected and affected metrics. Refer\nto §5.4 for a detailed discussion.\nCut-Off Factor\nThis hyperparameter is introduced to “early stop”\nthe optimization step.13 When the probability of yi\nexceeds this cut-off factor upon adding the residual\nδi to the transformer’s hidden state hL\ni , the opti-\nmization step is stopped.\nThe Table 15 demonstrates how the performance\nof the editing method changes when varying the\n“Cut-Off” Factor in terms of Accuracy and F1 score.\nThe ablation study is conducted using the GPT-2\nLarge model on the PEP3k dataset, with the verb\ntoken used for editing in the EDIT VALIDATION\nSET dataset. The chosen hyperparameters align\nwith those presented in Table 11.\n13Please refer to (Meng et al., 2023) for details of the\noptimization equation.\nA.9 Constructing the PROBE SET\nWe prompt text-davinci-003 zero-shot to con-\nstruct the augmentations for each test instance; the\nprompts are given in:\n• Affected Paraphrase: Fig. 7\n• Affected Reasoning: Fig. 8\n• Affected Neighborhood: Fig. 9\n• Unaffected Neighborhood: Fig. 10\nWe prompt the model for 5 possible instances,\nbut it can sometimes return the same value multiple\ntimes. We filter out poorly-formatted instances and\nmanually clean the filtered data to remove things\nlike empty statements or incorrect parsing from\n8228\nKL Div. Factor Efficacy Accuracy F1 score\nBase M - 75.16 75.16\n0.001 88.16 91.83 91.81\n0.0025 88.16 91.83 91.81\n0.005 89.47 92.16 92.14\n0.0075 89.47 92.16 92.14\n0.01 89.47 92.16 82.06\n0.025 86.84 91.17 91.15\n0.05 92.10 92.16 92.13\n0.0625 90.79 91.83 91.81\n0.075 93.42 92.81 92.80\n0.1 92.11 91.83 91.81\n0.25 93.42 91.18 91.14\n0.5 92.11 90.85 90.83\n0.75 93.42 91.5 91.48\n1 92.11 91.18 91.16\nTable 14: Ablation study of the KL Divergence Factor\non the GPT-2 Large model edited using the verb token\non layers l∈2,3,4,5,6 in the EDIT VALIDATION SET\nsplit of PEP3k. Note that default KL Factor of 0.0625 is\nused to report the performance of all editing methods.\nCut-Off Factor Efficacy Accuracy F1 score\nBase M - 75.16 75.16\n0.7 53.95 83.33 82.97\n0.725 55.26 83.33 82.97\n0.75 55.26 83.00 82.61\n0.775 55.26 83.00 82.61\n0.8 56.58 83.66 83.35\n0.825 63.16 84.97 84.73\n0.85 77.63 89.87 89.80\n0.875 82.90 91.50 91.47\n0.9 90.79 92.81 92.80\n0.925 90.79 93.14 93.13\n0.95 88.16 91.50 91.48\nNo factor 90.79 91.83 91.81\nTable 15: Ablation study of the “Cut-Off” Factor on\nthe GPT-2 Large model edited using the verb token on\nlayers l ∈2,3,4,5,6 for PEP3k EDIT VALIDATION\nSET . Note that the default value of “No Factor” is used\nto report the performance of all editing methods, i.e.,\nthere was no “early stopping” of the optimization step.\nGPT output to expected key-value pairs. We manu-\nally evaluate some examples to ensure quality. In\nsummary, there can be up to 5 augmentations per\naugmentation type for each instance.\nA.10 Causal Analysis Results\nThe Figs. 12 to 14 shows the causal graphs for the\nGPT2-Large Base Model on the 20Q dataset, the\nProvide 5 paraphrases of: Furnishings make noise\n-----------------------------------------------\n1. Furniture can be noisy.\n2. Furniture can create sound.\n3. Furniture can produce noise.\n4. Furniture can be a source of sound.\n5. Furniture can be a source of noise.\nFigure 7: Prompt to generate affected paraphrase for\n“Furnishings make noise (false)”\nFurnishings do not make noise. Explain this\nwith a 2-step reasoning chain of very short,\nsimple, connected sentences:\n-----------------------------------------------\n1. Furnishings are inanimate objects.\n2. Inanimate objects cannot make noise.\nFigure 8: Prompt to generate affected reasoning neigh-\nborhood for “Furnishings make noise (false)”\nGPT2-XL Base Model on the 20Q dataset, and the\nGPT2-Large Base Model on the PEP3k dataset.\nFor each of the editing locations, we see that the\nLast Token has higher AIE towards the later layers\nof the model which is consistent with the results of\nMEMIT on encyclopedic knowledge. Focusing on\nthe subject, verb, and object tokens, we see that all\nof them show high AIE in the early layers of the\ncorrupted tokens and that the effect on the corre-\nsponding last corrupted token is more pronounced\nthan that of the first corrupted token. This shows\nthat selecting the last subject/verb/object token and\nthe early layers of the model should give good re-\nsults for the editing method. These patterns are\nconsistent across all the models and datasets.\n8229\nGiven the text: Furnishings make noise\nsubject token: Furnishings\nobject token: noise\nQ1. In the text, replace just the subject token\nwith a different word. The replaced text should\nbe a valid sentence. The replaced token can be\na hyponym or similar word of the original\nsubject token. Write up to 5 such variants.\nQ2. In the text, replace just the verb token\nwith a different word. The replaced text should\nbe a valid sentence. The replaced token can be\na verb that follows or precedes the original\nverb token. Write up to 5 such variants.\nQ3. In the text, replace just the object token\nwith a different word. The replaced text should\nbe a valid sentence. The replaced token can be\na hyponym or similar word of the original\nobject token. Write up to 5 such variants.\n-----------------------------------------------\nQ1.\n1. Appurtenances make noise\n2. Fixtures make noise\n3. Accoutrements make noise\n4. Decorations make noise\n5. Adornments make noise\nQ2.\n1. Furnishings create noise\n2. Furnishings emit noise\n3. Furnishings generate noise\n4. Furnishings produce noise\n5. Furnishings yield noise\nQ3.\n1. Furnishings make sound\n2. Furnishings make clamor\n3. Furnishings make din\n4. Furnishings make racket\n5. Furnishings make uproar\nFigure 9: Prompt to generate affected neighborhood for\n“Furnishings make noise (false)”\nGiven:\ntext: Furnishings make noise\nsubject token: Furnishings\nobject token: noise\nQ1. Replace the subject token with a completely\nunrelated word and make a new text. Make 5 such\nreplacements.\nQ2. Replace the object token with a completely\nunrelated word and make a new text. Make 5 such\nreplacements.\n-----------------------------------------------\n1. Replacing the subject token:\na. Cars make noise\nb. Animals make noise\nc. People make noise\nd. Plants make noise\ne. Computers make noise\n2. Replacing the object token:\na. Furnishings make music\nb. Furnishings make laughter\nc. Furnishings make light\nd. Furnishings make heat\ne. Furnishings make color\nFigure 10: Prompt to generate unaffected neighborhood\nfor “Furnishings make noise (false)”\nYou are given an input sentence. Fix the\ngrammar and write the grammatical sentence.\ninput: furnishing make noise\n-----------------------------------------------\noutput: furnishings make noise\nFigure 11: Prompt to fix grammar in a triple\n“furnishing make noise”\n8230\n(a) Subject corruption\n(b) Verb corruption\n(c) Object corruption\nFigure 12: Causal tracing results for GPT-2 XL Base Model on 20Q EDIT VALIDATION SET when different parts of\nthe input are corrupted.\n(a) Subject corruption\n(b) Verb corruption\n(c) Object corruption\nFigure 13: Causal tracing results for GPT-2 Large Base Model on 20Q EDIT VALIDATION SET when different parts\nof the input are corrupted.\n8231\n(a) Subject corruption\n(b) Verb corruption\n(c) Object corruption\nFigure 14: Causal tracing results for GPT-2 Large Base Model on PEP3k EDIT VALIDATION SET when different\nparts of the input are corrupted.\n8232",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8493149280548096
    },
    {
      "name": "Computer science",
      "score": 0.7823514938354492
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.5459988117218018
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48889151215553284
    },
    {
      "name": "Natural language processing",
      "score": 0.46373453736305237
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.45256829261779785
    },
    {
      "name": "Language model",
      "score": 0.43039003014564514
    },
    {
      "name": "Machine learning",
      "score": 0.38902825117111206
    },
    {
      "name": "Domain knowledge",
      "score": 0.31736719608306885
    },
    {
      "name": "Electrical engineering",
      "score": 0.06867706775665283
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    }
  ],
  "cited_by": 6
}