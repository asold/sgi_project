{
    "title": "Named entity recognition in aerospace based on multi-feature fusion transformer",
    "url": "https://openalex.org/W4390674929",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2128428783",
            "name": "Jing Chu",
            "affiliations": [
                "Chang'an University",
                "Xi’an University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2109801487",
            "name": "Yumeng Liu",
            "affiliations": [
                "Xi’an University of Posts and Telecommunications",
                "Chang'an University"
            ]
        },
        {
            "id": "https://openalex.org/A2072702379",
            "name": "QI Yue",
            "affiliations": [
                "Xi’an University of Posts and Telecommunications",
                "Chang'an University"
            ]
        },
        {
            "id": "https://openalex.org/A2735846802",
            "name": "Zixuan Zheng",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A4201644406",
            "name": "Xiaokai Han",
            "affiliations": [
                "Xi’an University of Posts and Telecommunications",
                "Chang'an University"
            ]
        },
        {
            "id": "https://openalex.org/A2128428783",
            "name": "Jing Chu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109801487",
            "name": "Yumeng Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2072702379",
            "name": "QI Yue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2735846802",
            "name": "Zixuan Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4201644406",
            "name": "Xiaokai Han",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962904552",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2559281960",
        "https://openalex.org/W2042188227",
        "https://openalex.org/W4252110927",
        "https://openalex.org/W6600654476",
        "https://openalex.org/W6683738474",
        "https://openalex.org/W2598789561",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2915716523",
        "https://openalex.org/W1607093392",
        "https://openalex.org/W2965690110",
        "https://openalex.org/W6766351971",
        "https://openalex.org/W2970323499",
        "https://openalex.org/W6767179725",
        "https://openalex.org/W2912473624",
        "https://openalex.org/W3208408757",
        "https://openalex.org/W4317651364",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W6698701979",
        "https://openalex.org/W2963338481",
        "https://openalex.org/W2567657016",
        "https://openalex.org/W3034379414"
    ],
    "abstract": "Abstract In recent years, along with the rapid development in the domain of artificial intelligence and aerospace, aerospace combined with artificial intelligence is the future trend. As an important basic tool for Natural Language Processing, Named Entity Recognition technology can help obtain key relevant knowledge from a large number of aerospace data. In this paper, we produced an aerospace domain entity recognition dataset containing 30 k sentences in Chinese and developed a named entity recognition model that is Multi-Feature Fusion Transformer (MFT), which combines features such as words and radicals to enhance the semantic information of the sentences. In our model, the double Feed-forward Neural Network is exploited as well to ensure MFT better performance. We use our aerospace dataset to train MFT. The experimental results show that MFT has great entity recognition performance, and the F 1 score on aerospace dataset is 86.10%.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports\nNamed entity recognition \nin aerospace based \non multi‑feature fusion transformer\nJing Chu 1, Yumeng Liu 1, Qi Yue 1, Zixuan Zheng 2 & Xiaokai Han 1*\nIn recent years, along with the rapid development in the domain of artificial intelligence and \naerospace, aerospace combined with artificial intelligence is the future trend. As an important basic \ntool for Natural Language Processing, Named Entity Recognition technology can help obtain key \nrelevant knowledge from a large number of aerospace data. In this paper, we produced an aerospace \ndomain entity recognition dataset containing 30 k sentences in Chinese and developed a named entity \nrecognition model that is Multi‑Feature Fusion Transformer (MFT), which combines features such as \nwords and radicals to enhance the semantic information of the sentences. In our model, the double \nFeed‑forward Neural Network is exploited as well to ensure MFT better performance. We use our \naerospace dataset to train MFT. The experimental results show that MFT has great entity recognition \nperformance, and the  F1 score on aerospace dataset is 86.10%.\nUnprecedented advances have been made in the domain of aerospace, with manned spaceflight technology being \nprogressively commercialized, such as SpaceX’s Dragon spacecraft. In combination with artificial intelligence, \nsome of the complex operational steps in the aerospace domain will become simpler and autonomy can be \nexploited more in accordance with their prescribed tasks. Similar to humans, artificial intelligence first needs to \nlearn prior knowledge. Text is one of the main storage forms of human knowledge. Therefore, it is particularly \nimportant to acquire knowledge accurately and quickly from a large number of aerospace text materials.\nNamed Entity Recognition (NER) is an essential technology to extract knowledge from documents. Its pur-\npose is to extract words with actual meaning from text, including names of people, places, institutions and proper \nnouns. Unlike English, which has spaces as natural separators, Chinese entity recognition first needs to perform \nword segmentation on Chinese sentences, which makes Chinese named entity recognition more challenging. \nFor example, if a sentence containing 12 characters is entity recognized, use C = {c1,c2,…,c12} to express. the result \nmay be w1, w2 or w3 (w3 contains both w1 and w2), but only w3 is the correct result.\nIn order to reduce the impact of word segmentation errors, Zhang et al. proposed a Lattice structure that can \nconsider both characters and words, and this structure was used on the Lattice-LSTM (Lattice Long-Short Term \nMemory)1. As shown in Fig. 1, the structure matches the sentence with the lexicon to obtain all potential words \ncontained in the sentence, and then performs feature extraction for each character and the matched words in \nthe sentence. The Lattice will use the contextual information to determine which of w1, w2 and w3 is the correct \nword segmentation result, avoiding the recognition failure caused by word segmentation errors. Li et al. modified \nthe Lattice structure to be combined with Transformer-XL 2, and proposed FLAT (Flat-Lattice-Transformer) 3. \nFLAT uses a Flat-Lattice, which places the words matched from the lexicon at the end of the input sentence and \ndetermines the position of these words in the sentence through position encoding. However, this method ignores \nthe radical information of Chinese characters.\nAs a character evolved from hieroglyphs, the radicals of Chinese characters usually contain a lot of informa-\ntion. This information could be used to further enhance the semantic information. Dong et al. used Bidirectional \nLong Short-Term Memory (Bi-LSTM) to decompose the Chinese character structure to obtain character-level \nembeddings and demonstrated the effectiveness of the  method4. However, the Long Short Term Memory (LSTM) \nhas insufficient parallel computing ability and its performance is lower than that of the Transformer.\nTo address these issues, we used a NER model based on Multi-Feature Fusion Transformer (MFT). The model \nis based on FLAT and uses a One-Dimensional Convolutional Neural Network (1D-CNN) to integrate informa-\ntion about the corresponding radicals of Chinese characters. The MFT uses features from Chinese characters, \nwords and radicals to make it computationally efficient and reduce errors in word segmentation by introducing \nOPEN\n1School of Automation, Xi’an University of Posts & Telecommunications, 618 West Chang’an Street, Chang’an \nDistrict, Xi’an 710121, China. 2School of Astronautics, Northwestern Polytechnical University, 127 West Youyi \nRoad, Beilin District, Xi’an 710072, China. *email: hxk1256422591@163.com\n2\nVol:.(1234567890)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nradical information to enhance semantic information. The structure of the Transformer has also been adapted \nto make it perform the task of named entity recognition better.\nIn order to recognize entities in aerospace contexts, the MFT model needs to be trained on the aerospace \ndomain corpus. At present, there is no publicly available Chinese named entity recognition dataset in the aero-\nspace domain. Therefore, within the scope permitted by law, we used the web crawler technology to obtain \nrelevant data from Baidu Encyclopedia and other websites. To that end, we have produced an aerospace dataset \nthat contains 30 K Chinese sentences to complete the model training and testing.\nRelated work\nAerospace named entity recognition belongs to the specific field of named entity recognition, but it still belongs to \nthe research in the field of named entity recognition. Deep learning has advanced rapidly in recent years and vari-\nous named entity recognition methods based on deep learning have appeared. As one of the early deep learning \nmodels, LSTM was applied to the named entity recognition task by  Hammerton5. However, LSTM only extracts \nfeatures in a sentence from a single direction. To solve this problem, Huang et al. used BiLSTM that combined \nwith Conditional random fields (CRF) for the entity recognition task and had achieved satisfactory results. \nIn addition to temporal models that can be used for semantic  modeling6, Collobert et al. used Convolutional \nNeural Networks (CNN) as NER model encoders to model local semantic features of sentences and generate \ncorresponding labels with CRF as  decoders7. Dos Santos used an improved CNN model for Natural Language \nProcessing (NLP), 1D-CNN, to recognize entities. Experiments show that this improvement is very  effective8. \nHowever, these methods only perform feature extraction on characters or words one by one. For this reason, \nVaswani et al. proposed a Transformer model based on a self-attentive mechanism, which provides a new idea \nfor named entity recognition. The method not only improves the recognition accuracy of the model, but also \nreduces the training time of the  model9. Dai et al. believe that the modeling ability of long-term dependency is \ncrucial to the language model, which is also the defect of Transformer, so they improved it and proposed Trans-\nformer_XL model, which improves the modeling ability of Long-Term dependency by 80%. However, Guo et al. \nbelieve that named entity recognition is different from other language models and should pay more attention to \nthe modeling of local  semantics10. They propose a lightweight Star Transformer model. Experiments show that \nthis model is more suitable for NER tasks.\nChinese named entity recognition methods are classified into character-based named entity recognition \nmethods and word-based entity recognition methods. Character-based approaches lose word information in \nsentences, and word-based approaches are more influenced by the quality of the segmentation. Liu et al. discuss \ncharacter-based and word-based approaches separately and conclude that character-based approaches are empiri-\ncally better  choices11. However, some researchers have tried to combine the two methods by combining lexicon \ninformation on a character-based approach. Gui et al. proposed Lexicon Rethinking Convolutional Neural \nNetwork (LR-CNN), which uses a lexicon to assist the model in the determination of entity  boundaries12. Zhang \net al. proposed Lattice LSTM, which reinforces semantic and entity boundaries by using a lexicon. Gui et al. \nproposed a Lexicon-Based Graph Neural Network (LGN), where the graph neural network is used to introduce \nthe latent word information matched by the dictionary into the model to complete the entity recognition  task13. \nLi et al. proposed FLAT, which uses relative position encoding to recover lattice structure information. Since \nLattice is compatible with Transformer, the performance of the model is further improved.\nIn terms of structural features of Chinese characters, Dong et al. introduced the structural information of \nChinese characters into the NER model for the first time and used Bi-LSTM for the feature extraction of Chinese \nradicals; this method achieved the best performance on the MSRA dataset. Meng et al. used images of Chinese \ncharacters to assist in completing NER by leveraging the image information of Chinese characters to take advan-\ntage of the strokes and structural features of Chinese  characters14.\nThere are also many named entity recognition works in the aerospace domain. Xu et al. crawled relevant texts \nfrom NASA ’s official website to produce a spacecraft named entity recognition dataset and used CRF to complete \nthe entity recognition  task15. Boan Tong et al. used the book World Spacecraft Encyclopedia as the data source \nfor constructing the spacecraft-related dataset and performed migration learning through the Bert-BiGRU-\nCRF (Bidirection Gated Recurrent Unit, BiGRU) model to fine-tune the model parameters in the spacecraft  \ndomain corpus to accomplish the entity recognition task in the spacecraft  domain16. Tikayat et al. developed an \nEnglish-language aerospace dataset with which they fine-tuned BERT for better recognition performance in the \nC1 C2 C3 C4 C5 C6\nw1\nC7 C8 C9 C10 C11 C12\nw2\nw3\nFigure 1.  Use the lexicon to match the Chinese sentence to the words w1, w2 and w3, these Chinese words make \nit easier for the NER model to determine entity boundaries. Whether the entity in a sentence is w1 or w2 or w3 \ncan be determined by the NER model using contextual semantics.\n3\nVol.:(0123456789)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\naerospace  domain17. In this paper, we will develop a Chinese aerospace dataset and propose a new recognition \nmethod based on the characteristics of Chinese.\nAerospace dataset\nSince there is no publicly available named entity recognition dataset in the aerospace domain, we use the crawler \nsystem to obtain relevant corpus from the data on Internet websites such as Wikipedia to the extent permitted by \nlaws and use Label Studio for manual labelling. A dataset of aerospace domain with 29,953 sentences and 51,482 \nentities is made. The construction process of the aerospace dataset is shown in Fig. 2.\nFirst, we use a crawler based on the Scrapy framework to obtain aerospace data from Wikipedia and China \nAerospace News, then we filtered the corpus to remove contents that are not relevant to the domain. After that, \nwe sliced the corpus in sentences and ensured that each sentence contained at least two aerospace entities. Finally, \nthe corpus was labeled in the BIO format with the help of Label Studio. An example of the BIO Labeling format \nis shown in Fig. 3, where ‘B’ stands for ‘Begin’ and is used to annotate the head of the entity, ‘I’ stands for ‘Inside’ \nand is used to annotate the rest of the entity and ‘O’ is for ‘Outside’ and is used to annotate the non-entity.\nEntities are categorized into aerospace companies and organizations (ACAO), Airports and spacecraft launch \nsites (AASLS), Type of aerospace vehicle (TOAV), Constellations and satellites (CAS), Space missions and projects \n(SMAP), Scientists and astronauts (SAA), aerospace technology and equipment (ATAE). 7 types. In this paper, \n80% of the data in the dataset is used to train the model, 10% is used to validate the model, 10% is used to test \nthe model. The main information of the aerospace dataset is shown in the Table 1.\nWikipedia\nAcquisitiono ft he corpus\nChinaA erospace News\nScreeningo ft he corpus to remove\nirrelevant content\nScreeningo ft he corpus\nEnsure that each sentence contains two\nor more namede ntities\nSegmentationo ft he corpus\nBIOl abelingd ataw ithL abel Studio\nData labeling\nFigure 2.  Extracting relevant corpus from Wikipedia and Chinese space news, screening and segmentation of \nthe corpus and labeling it in BIO format using Label Studio.\nO OOOOOB IIIII\nc1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12\nw3\nFigure 3.  Annotation of sentences containing 12 Chinese characters using the BIO labeling method.\nTable 1.  The main information of aerospace dataset.\nType Train Dev Test\nSentence 24,000 2976 2977\nEntity 41,222 5080 5180\nACAO 2023 254 206\nAASLS 453 59 54\nTOAV 3467 418 441\nCAS 1413 165 153\nSMAP 7380 897 930\nSAA 550 55 84\nATAE 25,922 3232 3312\n4\nVol:.(1234567890)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nMulti‑feature fusion transformer\nSince the word and radical information are very important features for Chinese characters. So in this paper we \nuse MFT that can fuse these information as a named entity recognition model. The network structure of the \nMFT model is shown in Fig.  4. The model first extracts the radical embedding of Chinese characters through \n1D-CNN, then fuses it with the Lattice sequence embedding output by the FLAT-Lattice model and encodes \nit as the input of the Flat-Lattice model, which is encoded as inputs to the Double Feed-forward Multi-head \nSelf-attention (DFMS) encoder module, and finally decodes the corresponding label sequences by CRF . In the \nDFMS encoder module, MFT has exploited the structure of the Transformer by adding a Feed-Forward Neural \nNetwork (FFN) before the multi-headed self-attention module. This sandwich structure of the Transformer \nshows better performance in the NER.\nFlat‑lattice module\nSimilar to the Flat-Lattice module in the FLAT model, the Flat-Lattice module in the MFT uses a lexicon to \nmatch the input sentence to obtain the potential words contained in the sentence and encodes the positions of \ncharacters and these potential words in order to construct the Lattice. The structure of the Flat-Lattice module \nis shown in Fig. 5. For example, if a sentence containing 12 characters is entity recognized. Match the sentence \nwith the lexicon to get the potential words w1, w2 and w3. These matched potential words are placed at the end \nof the sentence as candidates for the entities in the sentence, which together with the sentence form the lattice \nsequence LS = {ls1,…,lsn}. The tokens in the Flat-Lattice are then located using the head position and tail position \nto restore the Lattice structure information.\nNext, for the Flat-Lattice sequence, we need to convert it to Flat-Lattice sequence embedding and encode \nit by positions. The Lattice sequence embedding LE = {le 1,…,len} can be obtained by matching LS in a pre-\ntrained embedding table. Their positional embeddings, on the other hand, are calculated respectively by using \nEqs. (1)–(7).\n஥ ส֥൫ ဒ ྟ ᄛ ದ٦ԯ۽ӱԯӱӱ\n஥ ส֥൫ ဒ ྟ ᄛ ದ٦ԯ۽ӱ\nFlat-LatticeM odule RadicalF eature Module\n࠭ն俹 Ϣഐ ጗ൔ ઔዱ ᙂളۥ\nӚ ದ٦ࠫ\n१۽१\n຦ <pad> <pad> <pad>\nRadical\nEmbedding\nLattice\nEmbedding\nEmbedded Fusion\nDouble Feed ForwardM ulti-headS elf-attentionM odule\nConditionalR andomF ield\nO OOOOO BM MM ME\nFigure 4.  The flat lattice module and the radical feature module represent the embedding of the Chinese \nsentence respectively, and the double feed-forward multi-self-attention module encodes these embeddings, \nwhich are finally decoded by the conditional random fields to obtain the label sequence.\nc1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12 w1 w2 w3\n.BUDIJOH\nCZMFYJDPO\n1 2 3 4 5 6 7 8 9 10 11 12 7 7 9\n1 2 3 4 5 6 7 8 9 10 11 12 10 12 12\nHead\nTail\nc1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12\nFigure 5.  Flatten the lattice by using the head and tail positions of Chinese characters and words to record the \nposition of each token in the lattice structure.\n5\nVol.:(0123456789)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nwhere Rij in Eq. ( 1) represents the relative position encoding between token  i and token j with ⊕ representing \nthe concatenation operator and W P being the learnable parameter. P hh\ndij represents the encoding of the relative \ndistance between the head positions of token i  and token j , P ht\nd ij , P th\nd ij and P tt\nd ij have similar meaning, which is \ncalculated using the same formula as the position code calculation in Transformer, P hh\ndij as shown in Eqs. (2)–(3), \nwhere k represents the index of the position coding dimension, demb represents the position coding dimension, \nd hh\nij  represents the distance between the head positions of token i and token j, d ht\nij  , d th\nij  , and d tt\nij have similar mean-\ning, and they are calculated by Eqs. (4)–(7), where head[i] denotes the head position of token i and tail[j] denotes \nthe tail position of token j.\nRadical feature module\nIn Chinese, some characters such as \"river\", \"lake\" and \"sweat\" are related to water, so they all contain the same \nradical. The radicals in Chinese characters are similar to the root affixes in English. As a kind of characters \nevolved from hieroglyphs, Chinese characters contain a lot of semantic features in their radicals. In order to use \nthese semantic features to enhance the semantic information of sentences, Radical Feature Module splits each \nChinese character into multiple radicals by radical dictionary and encodes these radicals by 1D-CNN to obtain \nthe radical encoding of the corresponding Chinese character.\nTake the radical encoding of a sentence containing 12 characters as an example. Each character in the sen -\ntence is matched in the radical dictionary to obtain the radical group corresponding to each character, where \nthe character with the highest number of radicals contains 3 radicals, then the size of the convolution kernel of \n1D-CNN is set to 3 and the step size is also 3. The remaining words with less than 3 radicals are filled in with \n\"<PAD>\", a symbol used exclusively for filling in deep learning. The convolution process is shown in Fig. 6 . By \nthis method, we can obtain the corresponding radical embedding sequence RE  = {re1,…,ren} for the sentence.\nRadical Feature Module only extracts the radical feature of the characters in the input sentence, and the radi-\ncal feature of the potential words in the sentence is not extracted. This results in that the lengths of LE and RE  \nare different. In order to facilitate the subsequent fusion of them, Radical Feature Module also uses \"< PAD >\" \nto fill in the radical sequence embedding RE, so that the lengths of LE  and RE are consistent with each other.\nFinally, the lattice sequence embedding and the radical feature sequence embedding are concatenated to \nobtain the sequence embedding E = {e1,…,en}, as shown in Eqs. (8) and (9).\nDFMS encoder module\nThere are two kinds of neural networks in DFMS encoder, which are Self-Attention Neural Network and Feed-\nforward Neural Network. The structure is shown in Fig. 7, where the Self-attentive Neural Network is the same \nas the self-attentive network in Transformer_XL, which uses relative position coding, with the aim of improving \nthe model’s ability to model long-term dependencies. DFMS has a Double Feedforward Neural Network, with \nthe self-attentive neural network added between them. This structure has proven to be effective in  Conformer18. \nResidual connections and normalization are also required between each layer of neural networks.\nWe use the sequence embedding E, which fuses LE and RE, as the input to the DFMS encoder. As shown in \nEqs. (10)–(14). Firstly, the sequence embedding E enters into the first layer of FFN calculation, and the output \nresult enters into the Self-attentive Neural Network for self-attention coding after the layer normalization and \nresidual connection. The coded result also needs the residual connection and layer normalization processing. In \nthe end, after the second FFN calculation, the final encoding of the encoder is obtained.\n(1)Rij= ReLU((Phh\ndij ⊕ Pht\ndij ⊕ Pth\ndij ⊕ Ptt\ndij)W p)\n(2)P2k\nd = sin(d/1000 2k/demb )\n(3)P2k+1\nd = cos(d/1000 2k/demb )\n(4)d (hh )\nij = head [i]− head [j]\n(5)d (ht )\nij = head [i]− tail[j]\n(6)d (th)\nij = tail[i]− head [j]\n(7)d (tt)\nij = tail[i]− tail[j]\n(8)E = LE ⊕ RE\n(9)E = LE + RE\n(10)E∗ = E+ ((ReLU(EW 1 + b1))W 2 + b2)\n6\nVol:.(1234567890)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nwhere Wq, Wk, Wv and WR are the query mapping matrix, the key mapping matrix, the value mapping matrix \nand the position mapping matrix respectively, all of which are learnable parameters. With W q, Wk and Wv, the \nsequence embedding E is mapped to the query matrix Q, the key matrix K and the value matrix V respectively. \nIn Eq. (12) u and v are also learnable parameters that are used to ensure that the attentional bias of the query \nvector remains constant for different  tokens2. W1, W2, W3, W4, b1, b2, b3 and b4 are learnable parameters of the \nFeedforward Neural Network.\nCRF decoder module\nConditional random fields are often used in machine learning-based named entity recognition methods. Ben -\nefiting from its excellent performance, CRF is usually used as decoders based on neural network named entity \nrecognition models. CRF is a conditional probability distribution model that can be used to solve prediction \nproblems. Cuong et al. propose that CRF can be used to solve the labeling problem and derive the most sensible \nlabel in conjunction with the  semantic19. In the named entity recognition task, CRF takes the input sequence of \n(11)[Q, K, V ]= E∗[W q, W k, W v]\n(12)Sij= (Q i+ u)TKj+ (Q i+ v)TRijW R\n(13)A = E∗ + Softmax(S)V\n(14)X = A + ((ReLU(AW 3 + b3))W 4 + b4)\n...\nr1 r4 r22 r23 r24r3<pad>r2 <pad>\nre1 re2 re3 re4 re5 re6 re7 re8 re9 re10 re11 re12\nc1 c2 c12\nFigure 6.  Extraction of the radicals in Chinese characters using 1D convolution to obtain the radical \nembedding for each character.\nFeedforwardN euralN etwork\nFeedforwardN euralN etwork\nMuti-head Self Attention\nResidual Connection &L ayernorm\nResidual Connection &L ayernorm\nResidual Connection &L ayernorm\nFigure 7.  Using double feed-forward neural networks to clip multi-head self-attention modules with residual \nconnections and layer normalization between them.\n7\nVol.:(0123456789)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nobservations as the set of random variables X and the output sequence of labels as Y. As shown in Eqs. (15)–(16), \nfor a sequence X = {x1,…,xn}, the corresponding sequence of labels is Y = {y1,…,yn}. The probability of y is P.\nwhere tk is the transfer eigen-function and sl is the state eigenfunction, taking values of 1 or 0. λk and ul are the \ncorresponding weight coefficients, which are learnable parameters.\nExperiment\nIn the experiments of this paper, we compare the performance of the MFT model with some mainstream named \nentity recognition models on our aerospace dataset.\nIn addition, in order to verify whether the MFT model is only effective on our aerospace dataset, we also \nconduct performance comparison experiments on some commonly used and public named entity recognition \ndatasets such as Weibo and Resume datasets. Finally, we conduct an effectiveness study on the MFT model to \nverify the effectiveness of our model structure.\nEvaluation indicator\nCommon evaluation criteria used in Named Entity Recognition tasks are precision (P), recall (R) and  F1 score. \n (F1). They are calculated respectively by using the formulas (17)–(19). Precision is the percentage of labels pre-\ndicted by the model that are correctly predicted. Recall is the number of samples in the sample that are correctly \npredicted. As precision and recall are mutually exclusive metrics, a combined metric  F1 score is also needed to \njudge the recognition performance of the model.\nwhere TP denotes a positive sample with a correct prediction, FP denotes a negative sample with a failed pre-\ndiction, FN denotes a positive sample with a failed prediction and TN denotes a negative sample with a correct \nprediction.\nDataset\nIn this paper, we constructed an Aerospace Named Entity Recognition dataset with data from Wikipedia and the \nChina Aerospace News website. It contains 30 k sentences and 53,788 entities. We predefined seven entity types \nbased on the contents of the data, which were labeled by six annotators dividing the work among themselves, \nand the results were confirmed and validated by a manager. The whole labeling process took about one month. \nWe divide the dataset in the ratio of 8:1:1 to get the training dataset, developing dataset and testing dataset for \ntraining and testing our model. The dataset information is shown in Table 1. We used two mainstream Chinese \nNER datasets, the Weibo  dataset20,21 and the Resume  dataset1. The corpus of the Weibo dataset is mainly drawn \nfrom social media and contains four types of entities: Person, Location, Organization and Geopolitic. The corpus \nof the Resume dataset is mainly from Sina Finance. and was made by manually labeling named entities with \nYEDDA system. Table 2 shows the main information of both datasets.\n(15)P (y|x) =\nexp (∑\ni,k\n/afii9838k tk (yi−1 ,yi,x,i) + ∑\ni,l\nu lsl(yi,x,i))\nZ (x)\n(16)Z (x ) =\n∑\ny\nexp (\n∑\ni,k\n/afii9838k tk (yi−1 ,yi,x ,i) +\n∑\ni,l\nu lsl(yi,x ,i))\n(17)Precision= TP\nTP + FP\n(18)Recall= TP\nTP + FN\n(19)F1 = 2 × Precision× Recall\nPrecision+ Recall\nTable 2.  Main information of weibo and resume datasets.\nDataset Type Train (K) Dev (K) Test (K)\nWeibo\nSentence 1.4 0.27 0.27\nChar 73.8 14.5 14.8\nResume\nSentence 3.8 0.46 0.48\nChar 124.1 13.9 15.1\n8\nVol:.(1234567890)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nExperimental environment and parameters\nIn our experiments, we used the same word lexicon and pre-trained character and word embeddings as in the \nLattice-LSTM, Radical lexicon from https:// github. com/ kfcd/ chaizi. All comparison model codes are provided \nby the original authors. Our model was trained on an Ubuntu system using an RTX 3060.\nThe hyperparameters are set differently for different datasets. The hyperparameter setting for MFT are shown \nin Table 3. The hyperparameters are set differently for different datasets. On the aerospace dataset MFT consists \nof 9,765,018 trainable parameters. On the resume dataset, MFT consists of 9,319,506 trainable parameters.\nExperimental results\nIn this study, we use the  F1 score as a criterion for judging the performance of the models, so the precision and \nrecall of the models are the results achieved by the model with the highest  F1 score on the test set.\nAerospace dataset\nThe experimental results of MFT on the aerospace dataset are shown in Table 4. The experimental results indi-\ncate that MFT performs well, with a significant performance improvement of 0.97% in  F1 score compared to the \nbaseline model FLAT, the recall rate increased by 0.77%, the precision, is 1.16%. LR-CNN and LGN performed \nworse on the aerospace dataset than on the other datasets, while the LSTM combined with Lattice achieved an \n F1 score of 71.33%, which is 9.88% lower than our MFT model.\nThe adoption of the pre-training model BERT by MFT results in a substantial improvement in each perfor-\nmance. Although MFT + BERT does not perform as well as FLAT + BERT in terms of recall, both  F1 and P have \nto perform better.\nFigure 8 shows the  F1 curve of each model during training on the aerospace dataset, and the performance \nimprovement of MFT in terms of  F1 score is obvious compared to LGN, Lattice-LSTM and LR-CNN. Compared \nto FLAT, MFT has a faster improvement in  F1 score in the early stage of training. From the precision curve of \neach model in Fig. 9, it can be seen that MFT performs much better than FLAT in terms of precision during the \ntraining process, and after the 100th Epoch MFT’s precision curve is higher than FLAT’s precision curve almost \neverywhere. However, the recall curves of all models in Fig.  10 show that there is not much difference between \nthe performance of MFT and FLAT with respect to the recall criterion, so the improvement in the overall perfor-\nmance metric  F1 score of MFT mainly comes from the improvement in the recognition precision of the model.\nTable 5 shows the recognition of MFT for different classes of entities on the aerospace dataset. The best rec-\nognized entity type is AEAT with  F1 score of 83.48% followed by TOAV . The worst recognition rate is AASLS \nwith  F1 score of 61.11% and also AASLS has the least number of entities. Thus the recognition effectiveness of \nthe model is directly related to the amount of data.\nWeibo dataset\nTable 6 shows the experimental results of MFT on the Weibo dataset. Compared with other comparison models, \nMFT has a greater performance improvement with  F1 score of 64.38%. LR-CNN has the best performance in \nterms of precision, but the recall rate is 15.03% lower compared to MFT and the  F1 score is 7.84% lower. The com-\nprehensive performance of the model is improved to a higher level when MFT uses BERT to pre-train the model.\nResume dataset\nThe experimental results of MFT on the Resume dataset are shown in Table  7. The experiments demonstrate \nthat the Double Feed-forward Neural Network and the radical information of Chinese characters do bring \nTable 3.  Hyperparameter setting for MFT.\nDataset Nhead Dhead DFFN Epoch Layer Batch\nAerospace 16 8 384 200 2 5\nWeibo 16 8 384 200 1 5\nResume 8 16 384 200 2 10\nTable 4.  Aerospace NER results. Significant values are in bold.\nModels P R F1\nLatticeLSTM1 70.58 72.08 71.33\nLR-CNN12 70.53 64.35 67.35\nLGN13 71.66 72.6 72.13\nFLAT3 80.47 80.01 80.24\nMFT 81.63 80.78 81.21\nFLAT +  BERT3 85.33 85.49 85.41\nMFT + BERT 86.77 85.44 86.10\n9\nVol.:(0123456789)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nperformance improvements to the model with  F1 score of 95.78%, precision of 96.05% and recall rate of 95.52%, \nall of which are better than other models.\nExperiments of feature fusion method\nTo study the effect on the MFT model after using different fusion methods on LE and RE, we conducted experi-\nments on MFT on all three datasets. The experimental results are shown in Table  8. On the Weibo dataset and \nthe Resume dataset, concatenating LE and RE performed better than adding them together. In contrast, For the \naerospace dataset, concatenating LE and RE together still outperforms FLAT despite a decrease in precision, \nwhile the  F1 and recall of MFT are improved, especially the recall by 0.98%.\nExperiments of FFN\nThe Conformer being used to solve the speech recognition problem contains a double half-step FFN, while the \nMFT contains a double full-step FFN. In order to verify whether double full-step FFN can bring more perfor -\nmance improvement than double half-step FFN in the named entity recognition task, we set up experiments on \nthe impact of different FFN weight connection methods on the model performance. The experimental results \nare shown in Table 9. Compared to the double half-step FFN, the double full-step FFN is more suitable for the \nNamed Entity Recognition task.\nFigure 8.  F1 Curves during training of all models on the aerospace dataset. MFT’s  F1 curve is essentially above \nFL AT.\n10\nVol:.(1234567890)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nEffectiveness study\nThere are two main improvements of the MFT model, namely, the radical information of Chinese characters \nwas added to enhance the semantics, and double FFN was used to improve the feature encoding capability of the \nmodel. In order to verify whether all these improvements bring performance benefits to MFT, we disassemble \nthe model structure and conduct experiments on each of the three datasets. As shown in Table 10, we removed \nthe Double FFN of the MFT and the  F1 scores of the MFT dropped by 0.47%, 0.5%, 0.19% on the Aerospace, \nWeibo, and Resume datasets, respectively, after which we proceeded to remove the Radical Feature Module of \nMFT and revert to FLAT, the  F1 scores of MFT dropped by 0.5%, 3.56%, 0.14%, respectively. Results in Table \nshow that both improvements on the MFT are effective.\nThe effect of the radical feature on the attention of the model is intuitive, as can be seen in Fig. 11, where FLAT \nhas a more focused attention score, while MFT adds extra attention to the information of FLAT. In such a way, \nthe attention to key information is ensured not to be distracted. This allows MFT to converge faster than FLAT \nduring the training of the model, and as shown in Fig.  12, where the loss curve of MFT is lower and decreases \nfaster than that of FLAT.\nConclusions\nIn this paper, we propose an Aerospace Named Entity Recognition method based on multi-feature fusion Trans-\nformer. Big data from Wikipedia and China Aerospace News are obtained as corpus by crawlers and the aero -\nspace dataset is produced using a manual labelling method. We train and test the MFT on our dataset and the \nexperimental results demonstrate that our model has excellent performance, due to the fact that the radical \nfeatures of the Chinese characters and the double Feed-forward Neural Network can provide a boost to the \nrecognition rate of the MFT.\nIn future work, a wider range of Chinese features, such as the pronunciation and graphics of Chinese char -\nacters, could also be incorporated for a multimodal approach. However, incorporating more diverse features \nmay introduce invalid elements or noise, which may lead to an increase in model parameters. To mitigate this \nproblem, future work may also require filtering of features to reduce the model size and save computational costs.\nFigure 9.  Precision curve during training of the comparison model on the aerospace dataset. MFT has a \nsignificantly higher precision rate curve than FLAT.\n11\nVol.:(0123456789)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nFigure 10.  Recall curves during training of all models on the aerospace dataset. MFT’s recall is a bit lower than \nFLAT, but at the 160th epoch it’s basically equal to FLAT’s level.\nTable 5.  MFT’s  F1 scores for different entity types.\nType Train Dev Test F1\nACAO 2023 254 206 75\nAASLS 453 59 54 61.11\nTOAV 3467 418 441 80.95\nCAS 1413 165 153 64.05\nSMAP 7380 897 930 78.49\nSAA 550 55 84 61.90\nATAE 25,922 3232 3312 83.48\nTable 6.  Weibo NER results. Significant values are in bold.\nModels P R F1\nLatticeLSTM1 52.71 53.92 53.13\nLR-CNN12 65.06 50.00 56.54\nLGN13 – – 60.21\nFLAT3 – – 60.32\nMFT 63.72 65.03 64.38\nFLAT +  BERT3 – – 68.55\nMFT + BERT 68.33 71.09 69.71\n12\nVol:.(1234567890)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nTable 7.  Main results on resume NER. Significant values are in bold.\nModels P R F1\nLatticeLSTM1 94.81 94.11 94.46\nLR-CNN12 95.37 94.84 95.11\nLGN13 95.28 95.46 95.37\nFLAT3 – – 95.45\nMFT 96.05 95.52 95.78\nFLAT +  BERT3 – – 95.86\nMFT + BERT 96.24 95.77 96.01\nTable 8.  Result of different feature fusion method.\nDataset Method P R F1\nAerospace\nConcatenation 81.63 80.78 81.21\nAddition 82.03 79.80 80.89\nWeibo\nConcatenation 63.72 65.03 64.38\nAddition 66.08 58.09 61.83\nResume\nConcatenation 96.05 95.52 95.78\nAddition 94.03 93.35 94.72\nTable 9.  Result of different FFN.\nDataset Type P R F1\nAerospace\nDouble half-step FFN 81.27 79.59 80.43\nSingle full-step FFN 82.63 78.85 80.74\nDouble full-step FFN 81.63 80.78 81.21\nWeibo\nDouble half-step FFN 66.08 58.09 61.83\nSingle full-step FFN 67.14 60.92 63.88\nDouble full-step FFN 63.72 65.03 64.38\nResume\nDouble half-step FFN 94.97 95.09 95.03\nSingle full-step FFN 95.64 95.42 95.59\nDouble full-step FFN 96.05 95.52 95.78\nTable 10.  Result of ablation study.\nDataset Model architecture P R F1\nAerospace\nMFT 81.63 80.78 81.21\nDouble full-step FFN 82.63 78.85 80.74\nRadical feature module 80.47 80.01 80.24\nWeibo\nMFT 63.72 65.03 64.38\nDouble full-step FFN 67.14 60.92 63.88\nRadical feature module – – 60.32\nResume\nMFT 96.05 95.52 95.78\nDouble full-step FFN 95.64 95.42 95.59\nRadical feature module – – 95.45\n13\nVol.:(0123456789)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\nData availability\nThe datasets generated and analyzed during the current study are available in the GitHub repository, https://  \ngithub. com/ Coder- XIAOK AI/ Aeros pace_ NERda tasets.\nReceived: 25 February 2023; Accepted: 23 December 2023\nReferences\n 1. Zhang, Y . & Y ang, J. Chinese NER using lattice LSTM. In 56th Annual Meeting of the Association-for-Computational-Linguistics \n(ACL), Melbourne, Australia, 1554–1564 (2018).\nFigure 11.  Visualization of attention for MFT and FLAT. MFT has a broader focus and more semantic features \nare extracted by the self-attentive mechanism.\nFigure 12.  Loss curves for MFT and FLAT. MFT converges faster than the FLAT model and has lower losses.\n14\nVol:.(1234567890)Scientific Reports |          (2024) 14:827  | https://doi.org/10.1038/s41598-023-50705-0\nwww.nature.com/scientificreports/\n 2. Dai, Z. et al. Transformer-XL: Attentive language models beyond a fixed-length context. In 57th Annual Meeting of the Association-\nfor-Computational-Linguistics (ACL), Florence, Italy, 2978–2988 (2019).\n 3. Li, X., Y an, H., Qiu, X. & Huang, X. FLAT: Chinese NER Using Flat-Lattice Transformer 6836–6842 (Association for Computational \nLinguistics, 2020).\n 4. Dong, C., Zhang, J., Zong, C., Hattori, M. & Di, H. Character-based LSTM-CRF with radical-level features for Chinese named entity \nrecognition. In 5th International Conference on Natural Language Processing and Chinese Computing (NLPCC). 24th International \nConference on Computer Processing of Oriental Languages (ICCPOL), 239–250. (Kunming Univ Sci & Technol, 2016).\n 5. Hammerton, J. Named entity recognition with long short-term memory. In Proceedings of the Seventh Conference on Natural \nLanguage Learning at HLT-NAACL 2003, 172–175 (2003).\n 6. Huang, Z., Xu, W . & Yu, K. Bidirectional LSTM-CRF models for sequence tagging (2015) arXiv preprint arXiv: 1508. 01991.\n 7. Collobert, R. et al. Natural language processing (almost) from scratch. J. Mach. Learn. Res. 12, 2493–2537 (2011).\n 8. Dos Santos, C. & Gatti, M. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of COLING \n2014, the 25th International Conference on Computational Linguistics: Technical Papers, 69–78 (2014).\n 9. Vaswani, A et al. Attention is all you need. In 31st Annual Conference on Neural Information Processing Systems (NIPS). Long Beach, \nCA (2017).\n 10. Guo, Q., Qiu, X., Liu, P ., Shao, Y ., Xue, X. & Zhang, Z. Star-transformer. In Proceedings of the 2019 Conference of the North (2019).\n 11. Liu, Z., Zhu, C. & Zhao, T. Chinese named entity recognition with a sequence labeling approach: based on characters, or based on \nwords? In 6th International Conference on Intelligent Computing, Changsha, Peoples R China, 634–640. (2010).\n 12. Gui, T. et al. CNN-based Chinese NER with Lexicon rethinking. In 28th International Joint Conference on Artificial Intelligence , \nMacao, Peoples R China, 4982–4988 (2019).\n 13. Gui, T. et al. A lexicon-based graph neural network for Chinese NER. In Proceedings of the 2019 Conference on Empirical Methods \nin Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), \n1040–1050 (2019).\n 14. Meng, Y . et al. Glyce: Glyph-vectors for Chinese character representations. In 33rd Conference on Neural Information Processing \nSystems (NeurIPS), Vancouver, Canada (2019).\n 15. Xu, J., Zhu, J., Zhao, R., Zhang, L. & Li, J. J. Astronatuics named entity recognition based on CRF algorithm. Electron. Des. Eng.  \n25, 42–46 (2017).\n 16. Tong, B., Pan, J., Zheng, L. & Wang, L. Research on named entity recognition based on bert-BiGRU-CRF model in spacecraft field. \nIn 2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology \n(CEI), 747–753 (IEEE, 2021).\n 17. Tikayat Ray, A., Pinon-Fischer, O. J., Mavris, D. N., White, R. T. & Cole, B. F . aeroBERT-NER: Named-entity recognition for \naerospace requirements engineering using BERT. In AIAA SCITECH 2023 Forum, 2583 (2023).\n 18. Gulati, A. et al. Conformer: convolution-augmented transformer for speech recognition. (2020). arXiv preprint arXiv: 2005. 08100.\n 19. Cuong, N. V ., Y e, N., Lee, W . S. & Chieu, H. L. Conditional random field with high-order dependencies for sequence labeling and \nsegmentation. J. Mach. Learn. Res. 15, 981–1009 (2014).\n 20. He, H. & Xu, S. F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media 713–718 (Asso-\nciation for Computational Linguistics, 2017).\n 21. Peng, N. & Dredze, M. Improving named entity recognition for Chinese social media with word segmentation representation \nlearning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Berlin: ACL 2016, 149–155 \n(2016).\nAuthor contributions\nQ.Y . proposed a new named entity recognition method for aerospace and wrote a new code for the recognition \nmethod. Y .L. constructed the new aerospace named entity recognition dataset and validated and compared the \nproposed method. J.C. validated and compared the new proposed method, and completed the manuscript writing \nand calibration. Z.Z. provided the named entity recognition corpus. X.H. wrote part of the code and participated \nin the writing and correction of the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to X.H.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}