{
  "title": "Discourse Representation Structure Parsing with Recurrent Neural Networks and the Transformer Model",
  "url": "https://openalex.org/W2948235742",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5014019150",
      "name": "Jiangming Liu",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5030503109",
      "name": "Shay B. Cohen",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5041024491",
      "name": "Mirella Lapata",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2594470997",
    "https://openalex.org/W3101220575",
    "https://openalex.org/W6739398559",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W2574872930",
    "https://openalex.org/W6750427945",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2788465415",
    "https://openalex.org/W2898774411",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2952435224",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2798546337",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2625800120",
    "https://openalex.org/W3022195534",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "We describe the systems we developed for Discourse Representation Structure (DRS) parsing as part of the IWCS-2019 Shared Task of DRS Parsing.1 Our systems are based on sequence-to-sequence modeling. To implement our model, we use the open-source neural machine translation system implemented in PyTorch, OpenNMT-py. We experimented with a variety of encoder-decoder models based on recurrent neural networks and the Transformer model. We conduct experiments on the standard benchmark of the Parallel Meaning Bank (PMB 2.2). Our best system achieves a score of 84.8% F1 in the DRS parsing shared task.",
  "full_text": "Discourse Representation Structure Parsing with Recurrent\nNeural Networks and the Transformer Model\nJiangming Liu Shay B. Cohen Mirella Lapata\nInstitute for Language, Cognition and Computation\nSchool of Informatics, University of Edinburgh\n10 Crichton Street, Edinburgh EH8 9AB\njiangming.liu@ed.ac.uk, {scohen,mlap}@inf.ed.ac.uk\nAbstract\nWe describe the systems we developed for Discourse Representation Structure (DRS) parsing\nas part of the IWCS-2019 Shared Task of DRS Parsing. 1 Our systems are based on sequence-to-\nsequence modeling. To implement our model, we use the open-source neural machine translation\nsystem implemented in PyTorch, OpenNMT-py. We experimented with a variety of encoder-decoder\nmodels based on recurrent neural networks and the Transformer model. We conduct experiments on\nthe standard benchmark of the Parallel Meaning Bank (PMB 2.2.0). Our best system achieves a score\nof 84.8% F1 in the DRS parsing shared task.\n1 Introduction\nDiscourse Representation Theory is a popular theory of meaning representation designed to account for\na variety of linguistic phenomena, including the interpretation of pronouns and temporal expressions\nwithin and across sentences (Kamp and Reyle, 1993). The Groningen Meaning Bank (GMB; Bos et al.\n2017) provides a large collection of English texts annotated with Discourse Representation Structures\n(DRS), while the Parallel Meaning Bank (PMB; Abzianidze et al. 2017) provides DRSs in English,\nGerman, Italian and Dutch. Furthermore, the PMB introduces clause representation, as shown on the top\nof Figure 1.\nWith the recent introduction of neural network learning to the Natural Language Processing commu-\nnity, several neural DRS parsers have been developed for the problem of DRS parsing, i.e. the problem\nof taking a document or a sentence as input, and outputting their corresponding DRS. Liu et al. (2018)\nconvert box-style DRSs to tree-style DRSs and propose the three-step tree DRS parser on the GMB,\nwhile van Noord et al. (2018) adopt a neural machine translation approach to parse sentences to their\nclause-style DRSs on PMB. Due to the different standard of annotations between GMB and PMB, and\nthat the IWCS-2019 Shared Task of DRS Parsing mainly focuses on averagely short sentences in PMB\nannotations, our systems take sentences as input and output a clause-style DRS of PMB represented as a\nsequence for the IWCS-2018 Shared Task of DRS parsing (Abzianidze et al., 2019).\n2 The Parsing System\nFigure 2 shows the data pipeline in our system for both training and parsing. There are three main\nparts: (a) The component Preprocess, which prepares the input data to make it suitable for training and\nparsing models; (b) The component Neural Modelwhich is based on OpenNMT; (c) The component\nPostprocess which contains some rules to ensure the system output is a well-formed DRSs.\n1https://competitions.codalab.org/competitions/20220\nb1 REF e1 b1 live \"v . 01\"e1\nb1 REF t1 b1 time \"n . 08\"t1\nb1 EQU t1 \"now\" b1 Location e1 x1\nb1 Theme e1 \"speaker\" b2 REF x1\nb1 Time e1 t1 b2 ground floor \"n . 01\"x1\ne1 t1 b1\ntime.n.08(t1)\nt1 = now\nlive.v.01(e1)\nLocation(e1, x1)\nTime(e1, t1)\nTheme(e1, speaker)\nx1 b2\nground floor.n.01(x1)\nFigure 1: The clause representations (top) and box-style representations (bottom) for the sentence I live\non the ground ﬂoor..\nDRS\npreprocess\nTraining\nneural\nmodel\npostprocess\nsentences\nDRSs\nsentence\npreprocess\nParsing\nsentences DRSs\nFigure 2: The framework of our DRS parsing system.\n2.1 Preprocessing\nThe Preprocess step works on the sentences and their DRSs of the training data and on the sentences of\nthe development and the test data. We tried two levels of preprocessing, character-level and word-level.\nCharacter Level We use the scripts of van Noord et al. (2018) to perform character-level preprocessing\nfor sentences and their DRSs. Each sentence is separated into characters where a special symbol “ |||”\nis used to mark a word boundary. 2 The clauses are represented as a character sequence, except for the\nsemantic roles, DRS operators and deictic constants, as shown in Figure 3(a). For example, “b1 REF e1”\nis preprocessed to “$NEW ||| REF”, which means that a new box (b1) is construct and a new referent (e1)\nis introduced by the box; “b2 ground ﬂoor “n.01” x1” is preprocessed to “$0 ||| g r o u n d f l o o r |||\n“ n . 0 1 ” ||| @0”, which means that the sense ground ﬂoor.n.01 is constructed and then assigned to the\nreferent @0, which is latest introduced, where @n (n∈ Z) denotes the referent |n|th latest introduced.3.\nSimilarly, $n (n∈ Z) denotes the box |n|th latest constructed.\nWord Level Each sentence is tokenized using the Moses script4 and then transformed to its lowercase\nform. Clauses are represented as sequences without changing the order, where a special symbol “ |||” is\nused to start a new clause. We rule out quotation marks in clauses (e.g. “tom” is converted to tom) and\n2Here, sentences are not further tokenized than they are in the data, and a token could be I’m or ﬂoor. .\n3if n is positive, @n denotes the referent is nth latest introduced in future.\n4https://github.com/moses-smt/mosesdecoder\nsentence: i ||| l i v e ||| o n ||| t h e ||| g r o u n d ||| f l o o r .\nDRS: $NEW ||| REF *** $0 ||| REF *** $0 ||| EQU ||| @0 ||| “now” ***$0 ||| Theme ||| @-1\n||| “speaker” *** $0 ||| Time ||| @0 ||| @-1 *** $0 ||| l i v e ||| “ v . 0 1 ” ||| @-1 *** $0\n||| t i m e ||| “ n . 0 8 ” ||| @0 *** $0 ||| Location ||| @-1 ||| @1 *** $NEW ||| REF ***\n$0 ||| g r o u n d f l o o r ||| “ n . 0 1 ” ||| @0\n(a) character level\nsentence: i live on the ground ﬂoor .\nDRS: $NEW REF ||| $0 REF ||| $0 EQU @0 now ||| $0 Theme @-1 speaker ||| $0 Time @0\n@-1 ||| $0 live v.01 @-1 ||| $0 time n.08 @0 ||| $0 Location @-1 @1||| $NEW REF ||| $0\nground ﬂoor n.01 @0\n(b) word level\nFigure 3: An example of preprocessing in character level and word level, respectively.\nLSTM\nParameter Value Parameter Value Parameter Value\nlayers 2 batch size 12 global attention general\nrnn size 300 batch type sents copy attention True\nrnn type LSTM optim sgd copy attn type dot\ndropout 0.2 learning rate 0.7 start decay steps 5000\nbridge True learning rate decay 0.7 decay steps 1000\nencoder type brnn max grad norm 5 decoder type rnn\nTransformer\nParameter Value Parameter Value Parameter Value\nlayers 6 batch size 512 encoder type transformer\nrnn size 300 batch type tokens decoder type transformer\ntransformer ff 2048 optim adam position encoding True\nheads 6 learning rate 0.001 copy attn type dot\ndropout 0.2 global attention general max grad norm 5\nbridge True copy attention True\nTable 1: Choice of hyperparameters for our neural network models.\nremain them case-sensitive. Following previous work (van Noord et al., 2018), the indices of variables\nin clauses are relative, as shown in Figure 3(b), which is the same to the character-level preprocessing.\n2.2 Neural Models\nWe adopt Recurrent Neural Networks (RNNs) equipped with Long Shot-Term Memory (LSTM; Hochre-\niter and Schmidhuber 1997) units and the Transformer model (Vaswani et al., 2017) as our neural models.\nFor the model implementation, we use the one provided by the OpenNMT-py toolkit (Klein et al., 2017).\nThe hyperparameters we used are shown in Table 1 which are institutionally set without optimization.\nFine-tuning We propose a ﬁne-tuning approach to enable the system to effectively use more training\ndata in various quality, i.e. bronze and silver data. The ﬁne-tuning approach allows the system train\nto convergence on one dataset (e.g. silver and gold data) and then continues to train to convergence on\nanother dataset (e.g. gold data), where the optimizers are reset.\nLSTM character word\nP R F1 time(h) P R F1 time(h)\nsg-data 73.91 75.00 74.45 13.1 73.81 73.75 73.78 7.8\nsg-data + g-data 86.05 84.78 85.41 +2.0 84.80 82.83 83.80 +0.8\nTransformer character word\nP R F1 time(h) P R F1 time(h)\nsg-data 69.11 69.93 69.52 5.2 75.41 75.36 75.38 5.1\nsg-data + g-data 82.32 81.19 81.75 +0.6 85.76 84.45 85.10 +0.6\nTable 2: Results on test partition of the Parallel Meaning Bank.\nP R F1\nbsg-data 74.27 75.78 75.02\nbsg-data + sg-data 77.74 78.78 78.26\nbsg-data + g-data 86.98 86.55 86.76\nbsg-data + sg-data + g-data 87.04 87.17 87.10\nTable 3: Results on test dataset by word transformer\n2.3 Postprocessing and Evaluation\nWe adopt the postprocessing scripts of van Noord et al. (2018) to transform back the output of our models\nto the clause format, and then use COUNTER (van Noord et al., 2018) as our evaluation metric.\n3 Experiments\nIn this section, we introduce the training data that we used and the results on the PMB benchmarks.\n3.1 Data\nThe training data consists of all of the bronze data (bronze), all of the silver data (silver), and the training\nsection of the gold data (gold). All data is preprocessed. We mix bronze, silver and gold as bsg-data,\nand mix silver and gold as sg-data, and name the training section of gold data as g-data. Meanwhile,\nwe adopt GloVe (Pennington et al., 2014) pre-trained word embeddings 5 to initialize the representation\nof input tokens.\n3.2 Results\nTable 2 shows the results on test data, wheresg-data means that the models are only trained on sg-data,\nand + g-datameans that the models are continually ﬁne-tuned on g-data. With LSTM, the character\nmodel performs marginally better than the word model. However, with Transformer, the word model\nperforms signiﬁcantly better than the character model. With both LSTM and Transformer, ﬁne-tuning\non g-data signiﬁcantly improves the performance. Although the character LSTM is marginally better\nthan the word Transformer, we still prefer the word Transformer as our ﬁnal model, because it could be\ntrained faster.\nTable 3 shows the improved results on test dataset by using word Transformer with bronze data,\nwhere bsg-data means that the model is only trained on bsg-data, + sg-datameans that the model is\ncontinually ﬁne-tuned on sg-data, and + g-datameans that the model is further ﬁne-tuned ong-data. As\nshown in Tables 2 and 3, the improvement gap of ﬁne-tuning on sg-data from bsg-data (3.24% F1) is\nnarrower than that of ﬁne-tuning on g-data from sg-data (8.84% F1). Fine-tuning on g-data may be the\nkey to improve the performance on the test dataset. We believe this is due to the high similarity between\n5https://nlp.stanford.edu/projects/glove/\nchar-LSTM word-LSTM char-transformer word-transformer\nall clauses 85.41 83.80 81.75 85.10\nDRS operators 92.96 93.00 91.67 93.72\nRoles 85.03 82.51 81.22 83.40\nConcepts 83.23 81.99 78.89 83.89\nSynsets-Noun 87.63 87.91 84.34 89.75\nVerbs 73.28 66.38 66.16 68.47\nAdjectives 68.92 71.06 62.45 74.63\nAdverbs 54.55 83.33 50.00 40.00\nTable 4: F1-scores of ﬁne-grained evaluation on test dataset.\ng-data and the test data. Also, we discover that the model trained onbsg-data then ﬁne-tuned on g-data\ncan also have good performance, but slightly worse than the ﬁnal models.\nWe submitted the word Transformer on bsg-data + sg-data + g-dataas our ﬁnal model to the DRS\nparsing shared task. On the test dataset of the shared task, our model achieves 84.80 F1 score.\n3.3 Analysis\nWe further analyze the output of the parsers trained on sg-data + g-datato see what components of\nthe meaning representation are challenging. Table 4 shows the detailed results of Counter, where DRS\nOperators (e.g. negation), Roles (e.g. Agent), Concepts (i.e. predicates), synsets (e.g. “n.01”) are scored\nseparately.\nWe compare four parsing models, LSTM with character-level preprocessing (char-LSTM), LSTM\nwith word-level preprocessing (word-LSTM), Transformer with character-level preprocessing (char-\ntransformer) and Transformer with word-level preprocessing (word-transformer). The char-LSTM and\nword-transformer models both achieve good performance, where word-transformer performs best on\nthe construction of DRS operators, Concepts, Synsets-Noun and Synsets-Adjectives, and char-LSTM\nperforms best on construction of Roles and Synsets-Verbs. The word-LSTM model is mediocre, but\nsigniﬁcantly outperforms the other models on the construction of Synsets-Adverbs with a large gap of\naverage 35.14% F1 score.\n4 Conclusions\nIn this paper, we describe the system for the IWCS-2019 Shared Task of DRS parsing. We found that\nthe character-level LSTM and the word-level transformer are competitive in the task. The training time\nof LSTM models increases as input sequences are longer, while training time are not sensitive to the\nlengths of input sequences in transformer. The output of LSTM models and transformers have different\nerror distributions. There is still a large improvement space for the sequential models.\nAcknowledgments\nWe thank Hessel Haagsma, Lasha Abzianidze, Rik van Noord and Johan Bos for their release of the\nlatest version of PMB (2.2.0). We gratefully acknowledge the support of the European Research Council\n(Lapata, Liu; award number 681760), the EU H2020 project SUMMA (Cohen, Liu; grant agreement\n688139) and Huawei Technologies (Cohen, Liu).\nReferences\nAbzianidze, L., J. Bjerva, K. Evang, H. Haagsma, R. van Noord, P. Ludmann, D.-D. Nguyen, and J. Bos\n(2017). The parallel meaning bank: Towards a multilingual corpus of translations annotated with com-\npositional meaning representations. In Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics, Valencia, Spain, pp. 242–247.\nAbzianidze, L., R. van Noord, H. Haagsma, and J. Bos (2019). The ﬁrst shared task on discourse\nrepresentation structure parsing. In Proceedings of the IWCS 2019 Shared Task on Semantic Parsing.\nBos, J., V . Basile, K. Evang, N. Venhuizen, and J. Bjerva (2017). The groningen meaning bank. In\nHandbook of Linguistic Annotation, pp. 463–496. Springer.\nHochreiter, S. and J. Schmidhuber (1997). Long short-term memory. Neural Computation, 1735–1780.\nKamp, H. and U. Reyle (1993). From discourse to logic: An introduction to modeltheoretic semantics\nof natural language, formal logic and DRT.\nKlein, G., Y . Kim, Y . Deng, J. Senellart, and A. M. Rush (2017). OpenNMT: Open-source toolkit\nfor neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics, Vancouver, Canada, pp. 67–72.\nLiu, J., S. B. Cohen, and M. Lapata (2018). Discourse representation structure parsing. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia,\npp. 429–439.\nPennington, J., R. Socher, and C. D. Manning (2014). Glove: Global vectors for word representation. In\nEmpirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543.\nvan Noord, R., L. Abzianidze, H. Haagsma, and J. Bos (2018). Evaluating scoped meaning repre-\nsentations. In Proceedings of the Eleventh International Conference on Language Resources and\nEvaluation, Miyazaki, Japan, pp. 1685–1693.\nvan Noord, R., L. Abzianidze, A. Toral, and J. Bos (2018). Exploring neural methods for parsing dis-\ncourse representation structures. Transactions of the Association for Computational Linguistics, 619–\n633.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin\n(2017). Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998–\n6008.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8302059769630432
    },
    {
      "name": "Parsing",
      "score": 0.8082354664802551
    },
    {
      "name": "Transformer",
      "score": 0.7563586235046387
    },
    {
      "name": "Natural language processing",
      "score": 0.6773089170455933
    },
    {
      "name": "Machine translation",
      "score": 0.6771021485328674
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6552110314369202
    },
    {
      "name": "Encoder",
      "score": 0.6010361313819885
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5464616417884827
    },
    {
      "name": "Recurrent neural network",
      "score": 0.49445199966430664
    },
    {
      "name": "Task (project management)",
      "score": 0.4667392671108246
    },
    {
      "name": "Artificial neural network",
      "score": 0.44860461354255676
    },
    {
      "name": "Speech recognition",
      "score": 0.37021785974502563
    },
    {
      "name": "Voltage",
      "score": 0.10074150562286377
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}