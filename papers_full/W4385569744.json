{
  "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
  "url": "https://openalex.org/W4385569744",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2305259341",
      "name": "Zhengfu He",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2261684028",
      "name": "Tianxiang Sun",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2098720890",
      "name": "Qiong Tang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A5089564299",
      "name": "Kuanning Wang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2161482855",
      "name": "Xuanjing Huang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4281690218",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W4286910219",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W4287083626",
    "https://openalex.org/W2129069237",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W3168053944",
    "https://openalex.org/W4294329082",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4281661987",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4306802991",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4225598930",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2900260828",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W3128876955"
  ],
  "abstract": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4521–4534\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDiffusionBERT: Improving Generative Masked Language Models with\nDiffusion Models\nZhengfu He∗ Tianxiang Sun∗ Qiong Tang Kuanning Wang\nXuanjing Huang Xipeng Qiu †\nSchool of Computer Science, Fudan University\nShanghai Key Laboratory of Intelligent Information Processing, Fudan University\n{zfhe19,txsun19,wangkn20,xjhuang,xpqiu}@fudan.edu.cn\nqtang22@m.fudan.edu.cn\nAbstract\nWe present DiffusionBERT, a new generative\nmasked language model based on discrete dif-\nfusion models. Diffusion models and many pre-\ntrained language models have a shared training\nobjective, i.e., denoising, making it possible to\ncombine the two powerful models and enjoy\nthe best of both worlds. On the one hand, dif-\nfusion models offer a promising training strat-\negy that helps improve the generation quality.\nOn the other hand, pre-trained denoising lan-\nguage models (e.g., BERT) can be used as a\ngood initialization that accelerates convergence.\nWe explore training BERT to learn the reverse\nprocess of a discrete diffusion process with an\nabsorbing state and elucidate several designs\nto improve it. First, we propose a new noise\nschedule for the forward diffusion process that\ncontrols the degree of noise added at each step\nbased on the information of each token. Sec-\nond, we investigate several designs of incorpo-\nrating the time step into BERT. Experiments on\nunconditional text generation demonstrate that\nDiffusionBERT achieves significant improve-\nment over existing diffusion models for text\n(e.g., D3PM and Diffusion-LM) and previous\ngenerative masked language models in terms\nof perplexity and BLEU score. Promising re-\nsults in conditional generation tasks show that\nDiffusionBERT can generate texts of compa-\nrable quality and more diverse than a series of\nestablished baselines.\n1 Introduction\nDiffusion models (Sohl-Dickstein et al., 2015;\nHo et al., 2020; Song et al., 2021) have recently\nemerged as a new class of state-of-the-art gener-\native models, achieving high-quality synthesis re-\nsults on image data (Ramesh et al., 2022; Rom-\nbach et al., 2022; Saharia et al., 2022). Though\nthese models captured widespread attention from\n∗Equal contribution.\n†Corresponding author.\nxT · · · xt xt−1 · · · x0\n\"[MASK] [MASK] [MASK]\" \"[MASK] world !\" \"Hello world !\"\npθ(xt−1|xt, t)\nq(xt|xt−1)\n(a) Diffusion models for discrete data\nxT · · · xt xt−1 · · · x0\n\"[MASK] [MASK] [MASK]\" \"Hello [MASK] !\" \"Hello world !\"\npθ(xt−1|xt)\nq(xt|xt−1, x0)\n(b) Non-Markovian DiffusionBERT\nFigure 1: In contrast to conventional discrete diffusion\nmodels, DiffusionBERT uses BERT as its backbone to\nperform text generation. The main differences are high-\nlighted in color: (1) DiffusionBERT performs decoding\nwithout knowing the current time step while canonical\ndiffusion models are conditioned on time step. (2) The\ndiffusion process of DiffusionBERT is non-Markovian\nin that it generates noise samples xt conditioning not\nonly on xt−1 but also on x0. Such a non-Markov pro-\ncess is due to our proposed noise schedule.\nnot only the research community but also the pub-\nlic, applying diffusion models to text data is still\nchallenging and under-explored due to the discrete\nnature of the text. A few prior works that explored\nusing diffusion models on text data can be divided\ninto two lines. The first is to extend diffusion mod-\nels to discrete state spaces (Hoogeboom et al., 2021;\nAustin et al., 2021). The second is to perform\nthe diffusion process and its reverse process in the\ncontinuous domain and bridge the continuous and\nthe discrete domain through embedding and round-\ning (Li et al., 2022; Gong et al., 2022). However,\nnone of these works leveraged pre-trained language\nmodels (PLMs, Devlin et al. (2019); Lewis et al.\n(2020); Raffel et al. (2020); Brown et al. (2020);\nQiu et al. (2020)), which are an unmissable treasure\nin the NLP community.\nThis work, to our knowledge, is the first attempt\nto combine diffusion models with PLMs. Such\na combination is built upon a shared training ob-\n4521\njective between diffusion models and PLMs, i.e.,\ndenoising. Diffusion models consist of a forward\nprocess (data to noise) and a reverse process (noise\nto data). In the forward process, a small amount of\nnoise is gradually added to the data. Then, a neural\nnetwork (pθ in Figure 1) is employed to learn the\nreverse process step by step, i.e., learn to denoise.\nSuch a denoising neural network is naturally re-\nlated to a wide class of PLMs that are pre-trained\nwith denoising objectives such as BERT (Devlin\net al., 2019) and BART (Lewis et al., 2020). Hence,\npre-trained denoising language models can serve as\na good starting point to learn the reverse diffusion\nprocess. On the other hand, diffusion models also\noffer a promising training strategy for generative\nPLMs. In contrast to commonly used generative\nPLMs (e.g., GPT (Brown et al., 2020)) that rely\non an autoregressive factorization of the joint prob-\nability, diffusion models provide another way of\nfactorization along the dimension of time and there-\nfore allow the generative model to be not necessar-\nily autoregressive. Thus, diffusion models can be\ncombined with a variety of PLMs that may not be\npre-trained for generation.\nIn the discrete domain, the forward diffusion\nprocess can be implemented by a chain of tran-\nsition matrices that gradually corrupt the clean\ntext. As shown in Figure 1, the clean text \"Hello\nworld !\" is gradually corrupted into \"[MASK]\n[MASK] [MASK]\" during the diffusion process.\nIn this work, we explore using pre-trained denois-\ning language models (e.g., BERT) to learn the re-\nverse diffusion process and demonstrate their ad-\nvantages in accelerating convergence and improv-\ning generation quality. Further, we propose a new\nnoise schedule of the forward process based on the\nprinciple of distributing the corrupted information\nuniformly across the forward process. The noise\nschedule, called spindle schedule, generates noise\nfor xt conditioned not only on xt−1 but also on x0,\nmaking the forward process non-Markovian with-\nout changing the original training objective. Note\nthat the denoising model takes as input xt and time\nstep tto predict xt−1, where tis unseen during the\npre-training of language models so we investigate\nseveral ways of incorporating the time step into\nPLMs. As a result, we find that the best result is\nachieved by throwing away the time information,\nwhich we call time-agnostic decoding (TAD).\nExperimental results on unconditional text gen-\neration demonstrate the benefit of combining dif-\nfusion models with PLMs: the proposed Dif-\nfusionBERT significantly improves the genera-\ntion quality over existing diffusion models for\ntext generation (e.g., D3PM (Austin et al., 2021)\nand Diffusion-LM (Li et al., 2022)) and previ-\nous generative masked language models (e.g.,\nBERT-Mouth (Wang and Cho, 2019)). Diffusion-\nBERT also matches several strong baselines in\nconditional generation tasks and shows superior\ngeneration diversity. The effectiveness of the pro-\nposed spindle schedule and time-agnostic decoding\nis confirmed by ablation studies. In a nutshell, Dif-\nfusionBERT enjoys the best of both worlds.\n2 Background\n2.1 Diffusion Models\nDiffusion models (Sohl-Dickstein et al., 2015; Ho\net al., 2020) are a class of latent variable models\nthat are originally designed for continuous domains.\nA diffusion model is consisting of a forward diffu-\nsion process and a reverse diffusion process. Given\na sample x0 ∼q(x0), a Markov chain of latent\nvariables x1,··· ,xT are produced in the forward\nprocess by progressively adding a small amount of\nGaussian noise to the sample:\nq(xt|xt−1) = N(xt;\n√\n1 −βtxt−1,βtI), (1)\nwhere {βt ∈(0,1)}T\nt=1 is a noise schedule control-\nling the step size of adding noise. Eventually xT\nbecomes an isotropic Gaussian distribution. If βt\nis small enough, the reverse process q(xt−1|xt) is\nalso a Gaussian, which is learned by a parameter-\nized model\npθ(xt−1|xt,t) = N(xt−1; µθ(xt,t),Σθ(xt,t)),\n(2)\nwhere µθ(·) and Σθ(·) can be implemented by a\nU-Net or a Transformer. When conditioning also\non x0, q(xt−1|xt,x0) has a closed form so we can\nmanage to minimize the variational lower bound to\noptimize log pθ(x0):\nLvlb = Eq[DKL(q(xT|x0) ∥pθ(xT))]\n+ Eq[\nT∑\nt=2\nDKL(q(xt−1|xt,x0) ∥pθ(xt−1|xt,t))]\n−log pθ(x0|x1), (3)\nwhere Eq(·) denotes the expectation over the joint\ndistribution q(x0:T).\n4522\n2.2 Diffusion Models in Discrete Domain\nFor discrete domains, each element of xt is a dis-\ncrete random variables with Kcategories. For text\ndata, K = |V|is the size of the vocabulary. De-\nnote xt as a stack of one-hot vectors, the process\nof adding noise can be written as\nq(xt|xt−1) = Cat(xt; p = xt−1Qt), (4)\nwhere Cat(·) is a category distribution and Qt is\na transition matrix that is applied to each token\nin the sequence independently: [Qt]i,j = q(xt =\nj|xt−1 = i). It is easy to obtain that\nq(xt−1|xt,x0) = q(xt|xt−1,x0)q(xt−1|x0)\nq(xt|x0)\n= Cat\n(\nxt−1; p = xtQ⊤\nt ⊙x0Qt−1\nx0Qtx⊤\nt\n)\n, (5)\nwhere Qt = Q1Q2 ···Qt. Note that ⊙is element-\nwise multiplication and the division is row-wise.\nWith q(xt−1|xt,x0) at hand, according to\nEq. (3), we can use a parameterized model\npθ(xt−1|xt,t) to learn the reverse diffusion pro-\ncess.\n3 DiffusionBERT\nIn contrast to recently proposed diffusion models\nfor text, e.g., Diffusion-LM (Li et al., 2022) and\nDiffuSeq (Gong et al., 2022), which are based on\ncontinuous diffusion models, we instead explore\ndiscrete diffusion models to integrate PLMs as the\nbackbone. We first introduce a specific instance\nof discrete diffusion models (Austin et al., 2021),\nwhich considers a transition matrix with an absorb-\ning state for the sake of using PLMs (§ 3.1). Sec-\nondly, we introduce a new noise schedule of the\nforward diffusion process, called spindle schedule,\nwhich is based on the principle of distributing the\ncorrupted information uniformly across the forward\nprocess (§ 3.2). Then, we investigate several alter-\nnatives of incorporating the time step into PLMs\nfor predicting xt−1 given xt and t(§ 3.3). Finally,\nwe explore training DiffusionBERT for conditional\ngeneration with prompts (§ 3.4).\n3.1 Diffusion Models with a Discrete\nAbsorbing State\nTo be combined with pre-trained denoising lan-\nguage models, we incorporate an absorbing state,\ne.g., [MASK] for BERT, in the Markov process. In\nparticular, each token in the sequence either stays\nthe same or transitions to [MASK] with some prob-\nability. Formally, each entry of the transition matrix\nat step tis as follows,\n[Qt]i,j =\n\n\n\n1 if i= j = [M],\nβt if j = [M],i ̸= [M],\n1 −βt if i= j ̸= [M],\n(6)\nwhere [M] is the abbreviation of [MASK]. Such a\nMarkov process converges to a stationary distribu-\ntion q(xT), which places all probability mass on a\nsequence with all [MASK] tokens.\nThe t-step marginal q(xi\nt|xi\n0) can be easily ob-\ntained in a closed form,\nq(xi\nt|xi\n0) =\n{\nαt if xi\nt = xi\n0,\n1 −αt if xi\nt = [M], (7)\nwhere αt = ∏t\ni=1(1 −βi), xi\nt denotes the i-th\ntoken in the sequence at step t. Combining with\nEq. (3) and (5), we can derive a training objective\nto optimize pθ(xt−1|xt,t) and generate a sample\nby performing the reverse diffusion process:\npθ(x0:T) = p(xT)\nT∏\nt=1\npθ(xt−1|xt,t). (8)\n3.2 Spindle Noise Schedule\nThe noise schedule in the continuous domain, such\nas the linear schedule (Ho et al., 2020) and the\ncosine schedule (Nichol and Dhariwal, 2021), has\nshown to be important to the performance of diffu-\nsion models.\nIn contrast to the continuous domain where the\nnoise can be easily controlled by the variance of\nthe Gaussian, (1) it is less obvious how to control\nthe degree of noise added at each step in the dis-\ncrete domain. For the discrete domain, the noise\nschedule βt = (T −t+ 1)−1 has been explored\nfor the case of the uniform transition matrix (Sohl-\nDickstein et al., 2015; Hoogeboom et al., 2021) and\nthe absorbing-state transition matrix (Austin et al.,\n2021). However, (2) such a schedule assumes all\ntokens carry the same amount of information and\ndoes not consider the linguistic difference among\nthe tokens in a sequence . Besides, (3) it violates\nthe easy-first-generation nature of denoising lan-\nguage models. That is, the model tends to generate\ntokens that are most frequently appearing (and is\nleast surprising) in the training corpus to achieve a\n4523\nhigher likelihood. As the context becomes richer,\nmore details come up in the sequence.\nTo address the above issues, we consider a noise\nschedule that (1) measures the added noise at\neach step by the corrupted information and en-\ncourage the corrupted information to be uniformly\ndistributed across the diffusion steps. Since the\ninformation is measured independently for each\ntoken, (2) different tokens in a sequence are as-\nsigned different probabilities of transitioning to the\n[MASK] token. Moreover, inspired by the easy-\nfirst-generation phenomenon, (3) we put the tokens\nin a sequence in descending order of their informa-\ntion and divide them intoT buckets. Each bucket is\nensured to contain the same amount of information.\nThat is, we mask the most informative tokens at the\nstart of the forward process and mask the least in-\nformative tokens at the end of the forward process\nsuch that the learnable reverse process follows an\neasy-first generative behavior.\nIn particular, distributing corrupted information\nuniformly across the forward steps can be formally\ndescribed by\n1 − t\nT =\n∑n\ni=1 H(xi\nt)∑n\ni=1 H(xi\n0) =\n∑n\ni=1 αi\ntH(xi\n0)∑n\ni=1 H(xi\n0) , (9)\nwhere H denotes the entropy, which measures the\namount of information of a random variable, xi de-\nnotes the i-th token in the sequence and ndenotes\nthe length of the sequence. According to Eq. (7),\nαi\nt = ∏t\nj=1(1−βi\nj) denotes the probability that the\ni-th token remains the same at step t, i.e., xi\nt = xi\n0.\nWe expect that αi\nt > αj\nt if H(xi\nt) < H(xj\nt) such\nthat easy (low-information) tokens emerges earlier\nthan hard (high-information) tokens during the re-\nverse process. In practice, the entropy of a given\ntoken H(x) is calculated by the negative logarithm\nof its frequency in the training corpus.\nConsidering these aforementioned properties,\nwe construct αi\nt as follows,\nαi\nt = 1 − t\nT −S(t) ·˜H(xi\n0), (10)\nS(t) = λsin tπ\nT , (11)\n˜H(xi\n0) = 1 −\n∑n\nj=1 H(xj\n0)\nnH(xi\n0) , (12)\nwhere S(t) is introduced to control the effect of\nthe informativeness at time step t. It is designed\nto be sinusoidal to ensure S(0) = S(T) = 0 such\nFigure 2: Each token in a sequence has a specific noise\nschedule depending on how much information is lost\nwhen they are masked. For instance, in the sentence\n\"Bella is sitting over there.\", \"Bella\"\nis the most informative word. Thus it is encouraged to\nbe masked at the early stage so that our model learns to\nrecover it in the last place.\nthat xt can retain all (zero) information whent= 0\n(t= T). The effect of S(t) is controlled by a hy-\nperparameter λ. When λ= 0, the noise schedule is\ndegraded to βt = (T−t+1)−1 as in Sohl-Dickstein\net al. (2015); Hoogeboom et al. (2021); Austin et al.\n(2021). Figure 2 shows how αprogresses during\nthe forward process. The schedule is named as\nspindle due to the shape of the probability curves.\nIn our proposed schedule, the transition proba-\nbility at time step tdepends not only on the current\nstate but also on the original text, making the for-\nward diffusion process non-Markovian. Neverthe-\nless, as revealed by Eq. (5), this does not change\nthe original training objective.\n3.3 The Design Space of Feeding Time Steps\nTypically, a diffusion model takes as input a\nnoised sample and the time step to predict the\ndenoised sample during the reverse process, i.e.,\npθ(xt−1|xt,t). However, t is an additional vari-\nable that is unseen during the pre-training of lan-\nguage models and therefore it is less trivial how to\nfeed the time information into the PLMs. Here we\nexplore three design choices of feeding time steps.\nLayer-wise Time Embedding A straightforward\nchoice is to include the time step as the same way\nas positional encoding, i.e., using the Transformer\nsinusoidal embedding or a learnable MLP in each\nTransformer layer. Note that this way is commonly\nadopted in previous work (Ho et al., 2020; Austin\net al., 2021; Li et al., 2022).\n4524\nPrefix Time Embedding Prompting language\nmodels by prepending trainable soft tokens to the\ninput sequence has shown promising results re-\ncently (Lester et al., 2021; Sun et al., 2022). Hence,\nwe also explore including a time step token embed-\nding v(t) as a prefix of the input token embeddings\n⟨v(x1\nt),v(x2\nt),··· ,v(xn\nt)⟩. In particular, the time\nstep token is inserted in between the [CLS] token\nand the input sequence. These added time step\ntoken embeddings are trained along with the PLM.\nTime-Agnostic Decoding Another alternative is\nnot to explicitly incorporate the time step tbecause\nit can be implied by the noised sample xt. In con-\ntrast to the image data, it is easier to implicitly in-\nfer the diffusion time step by counting the number\nof corrupted tokens (i.e., [MASK]) in the noised\nsequence. In this way, the PLM has to perform iter-\native decoding while being ignorant of the current\ntime step, i.e., pθ(xt−1|xt).\n3.4 Prompting DiffusionBERT for\nConditional Generation\nAn intriguing property of PLMs is understanding\ninstructions provided in the context and performing\ndesired tasks according to the instructions. Inher-\nited from BERT, DiffusionBERT is also able to\nsolve a wide range of conditional generation tasks\nby prompting with task descriptions and task texts\nto be processed (see Appendix.B for examples). To\nexplore the DiffusionBERT for conditional genera-\ntion, we adopt partial denoising (Gong et al., 2022)\nto perform the diffusion process conditioning on\nthe partially corrupted text.\n4 Experiments\n4.1 Tasks and Datasets\nWe train DiffusionBERT on the One Billion Word\ndataset (LM1B) (Chelba et al., 2014) for uncondi-\ntional generation. LM1B is a corpus with about 30\nmillion sentences and a vocabulary of about 793k.\nFor conditional generation, we choose two tasks for\nevaluating DiffusionBERT, namely Question Gen-\neration (QG) and Paraphrasing. Quasar-T (Dhingra\net al., 2017) is a Question Answering dataset con-\ntaining enormous document-question pairs. Gong\net al. (2022) constructed a QG dataset from Quasar-\nT and we follow their data split and task settings.\nFor paraphrasing, we choose Quora Question Pairs\n(QQP)1, a widely used question-pairs dataset with\n1https://www.kaggle.com/c/\nquora-question-pairs\n147K training samples.\n4.2 Baselines\nWe conduct comparison on unconditional text gen-\neration against several non-autoregressive (NAR)\nbaselines: D3PM (Austin et al., 2021), Diffusion-\nLM (Li et al., 2022), and BERT-Mouth (Wang and\nCho, 2019). We consider DiffuSeq (Gong et al.,\n2022) as a baseline for conditional generation. Sev-\neral strong baselines reported in their paper are also\nincluded for further comparison.\nD3PM D3PM is a general framework of discrete\ndiffusion models. We implement an instance of\nD3PM with the absorbing state and a layer-wise\ntime embedding. Both DiffusionBERT and D3PM\nare implemented with a sequence length n = 64\nand diffusion steps T = 2048. During inference,\nwe perform DDIM sampling with time step size of\n16 in each iteration. Hence, the total inference cost\nis 128 iterations.\nDiffusion-LM Diffusion-LM learns an embed-\nding to map discrete text into the continuous space\nwhere it performs Gaussian diffusion process. A\nrounding step is required to map the continuous em-\nbeddings into discrete texts. We re-implemented\nDiffusion-LM with the model architecture of BERT\nand diffusion steps T = 2000. Since the perfor-\nmance drop of Diffusion-LM is bigger than Dif-\nfusionBERT when sampling less steps, we do not\nskip steps during generation.\nBERT-Mouth BERT-Mouth samples text from\nBERT via order-agnostic autoregressive masked\nlanguage modeling. Starting from a sequence of\n[MASK], BERT samples one token at each time\nstep in random order. We continue training BERT\non LM1B for fair comparison.\nDiffuSeq DiffuSeq introduces a conditional text\ngeneration framework for encoder-only diffusion\nmodels. It performs diffusion process only on the\ntarget (i.e. partial denoising). Experimental re-\nsults of DiffuSeq show that generation of diffusion\nmodels is generally more diverse in conditional\ngeneration settings. Such diversity contributes to\nsample quality with the help of Minimum Bayes\nRisk (MBR) decoding (Koehn, 2004) i.e., sampling\nmultiple candidates and re-ranking them by their\nBLEU scores relative to other candidates.\n4525\nMethod Pretrained Schedule Time Step PPL ↓ BLEU↑ Self-BLEU↓\nD3PM (Austin et al., 2021) /enc-37\n(T−t+ 1)−1 LTE 82.34 0.3897 0.2347\n::::TAD 125.15 0.3390 0.2720\n:::::::Spindle LTE 77.50 0.4241 0.2288\nDiffusion-LM (Li et al., 2022) /enc-37Cosine LTE 118.62 0.3553 0.2668\n/enc-33Cosine LTE 132.12 0.3562 0.2798\nBERT-Mouth (Wang and Cho, 2019) /enc-33- - 142.89 0.2867 0.1240\n::::::::::::::DiffusionBERT /enc-33\n(T−t+ 1)−1\nLTE 92.53 0.3995 0.2118\n::::PTE 79.95 0.3886 0.2156\n::::TAD 78.76 0.4213 0.2116\n:::::::Spindle ::::TAD 63.78 0.4358 0.2151\nTable 1: Main results on LM1B. The methods proposed in this work are marked with:::::wavy::::lines. The best results are\nin bold and the second best results are underlined. LTE: layer-wise time embedding. PTE: prefix time embedding.\nTAD: time-agnostic decoding.\n4.3 Experimental Setup\nIn both conditioned and unconditioned settings,\nour DiffusionBERT is based on BERT-BASE -\nUNCASED with about 110M parameters. We\ntrain DiffusionBERT using the AdamW opti-\nmizer (Loshchilov and Hutter, 2019) with learning\nrate of 3e-6, dropout probability of 0.1 and batch\nsize of 256. We use a 10K-step linear warmup\nschedule starting from learning rate of 1e-8. For\ngeneration efficiency and better alignment with\nBERT pre-training objective, we use DDIM sam-\npling (Song et al., 2021) in which BERT generates\nall tokens at first and performs the forward process\nin Eq. 7 to skip time steps. All experiments are\nconducted on NVIDIA A100 Tensor Core GPUs.\nWe use 4 GPUs for training and a single GPU for\nsampling.\nTo sample from DiffusionBERT trained on\nLM1B, we use a top- K filter with K = 30 and\nperform 128 steps of inference to align with the\nsettings in Austin et al. (2021).\nIn the two conditional generation tasks, Diffu-\nsionBERT is trained for 100K steps. Sampling\ninvolves a top-15 filter and 400 inference steps. We\nfollow Gong et al. (2022) in the MBR size|S|= 10\nand sequence length of 128.\n4.4 Unconditional Generation\nOur main results of unconditional sampling are\nincluded in Table 1. We choose BLEU-4 and self-\nBLEU-4 (Zhu et al., 2018) as the metric for gen-\neration quality and diversity, respectively. In par-\nticular, we follow Savinov et al. (2022); Caccia\net al. (2020) to sample 1K sentences to compute\nBLEU score relative to all sentences in the test\nset. Another 1K sentences are sampled for comput-\ning self-BLEU. Overall, DiffusionBERT achieves\nthe best generation quality and diversity trade-off\namong the considered NAR methods. Besides, the\nperplexity of DiffusionBERT with the spindle noise\nschedule is substantially lower. Evidence of lower\nbound is used as a proxy of the perplexity of Diffu-\nsionBERT and D3PM since the exact likelihood of\ndiffusion models is intractable.\nDiffusionBERT vs. Other Generative BERT\nModels We compare DiffusionBERT with\nanother representative generative masked lan-\nguage model, BERT-Mouth (Wang and Cho,\n2019). Experimental results show that Diffusion-\nBERT achieves better performance in terms of the\nperplexity and the BLEU score. We attribute the\nsuperior performance of DiffusionBERT to its one-\ntime sampling of all tokens, which helps Diffusion-\nBERT generate more coherent text, especially in\na long range. Although such decoding may face\nthe problem of multimodality (Gu et al., 2018), in-\nappropriate phrases can be fixed in the upcoming\ndiffusion steps. The probabilistic modeling offers\nmore flexibility in that generated tokens with low\nprobability are more likely to be masked and re-\nsampled. Wang and Cho (2019) also proposed to\ncontinue masking and predicting tokens after the\nwhole sequence is complete. But such randomness\nin the selection and replacement of tokens results\nin low inference speed.\n4526\nFigure 3: BLEU scores on the LM1B test set. Left is\nbetter, lower is better.\nDiscrete vs. Continuous Diffusion Models We\nthen focus on the comparison of discrete and\ncontinuous diffusion models for text generation.\nTo achieve this, we mainly compare Diffusion-\nBERT with Diffusion-LM, which is based on con-\ntinuous diffusion models. As a result, despite of\nits outstanding controlling ability, we show that\nthe texts generated by Diffusion-LM have a lower\nquality than DiffusionBERT. Though both Diffu-\nsionBERT and Diffusion-LM adopt the same con-\nfiguration of Transformer, it is worth noting that\nthe superior performance of DiffusionBERT may\nbe contributed by not only the discrete diffusion\nmodels but also the use of pre-trained models.\nTo disentangle the effect of pre-training and dis-\ncrete/continuous diffusion models, we also explore\ninitializing Diffusion-LM with BERT. As shown\nin Table 1, training Diffusion-LM from BERT ini-\ntialization performs even worse than training from\nscratch. We conjecture that the continuous nature\nof Diffusion-LM is not compatible with the initial-\nization from BERT since the embedding learned\nby BERT may not be suitable for the Gaussian\ndiffusion process. In contrast, the comparison of\nD3PM and DiffusionBERT shows that Diffusion-\nBERT benefits much from the BERT initialization\ndue to its discrete diffusion process.\nEffect of Time Step In terms of both likelihood\nand generation quality, the layer-wise time embed-\nding (LTE) lags far behind the other two time step\ndesigns for DiffusionBERT while time-agnostic\ndecoding (TAD) achieves the best result. By con-\ntrast, D3PM without time step embedding per-\nforms significantly worse. In a nutshell, simpli-\nfying time step design has positive effect on Dif-\nMethod/Metric\nQuality Diversity\nBLEU↑ Rouge-L↑ Self-BLEU↓ Div-4↑\nGRU-attention 0.0651 0.2617 0.9999 0.3178\nTransformer-base 0.0364 0.1994 0.8767 0.4055\nGPT2-base FT 0.0741 0.2714 0.1403 0.9216\nGPT2-large FT 0.1110 0.3215 0.2910 0.8062\nGPV AE-T5 0.1251 0.3390 0.3567 0.7282\nNAR-LeVT 0.093 0.2893 0.983 0.4776\nDiffuSeq 0.1731 0.3665 0.2789 0.8103\nDiffusionBERT 0.0971 0.3420 0.0703 0.9372\n(a) QG\nMethod/Metric\nQuality Diversity\nBLEU↑ Rouge-L↑ Self-BLEU↓ Div-4↑\nGRU-attention 0.1894 0.5129 0.9958 0.3287\nTransformer-base 0.0580 0.2489 0.7717 0.4312\nGPT2-base FT 0.1980 0.5212 0.5480 0.6245\nGPT2-large FT 0.2059 0.5415 0.7325 0.5020\nGPV AE-T5 0.2409 0.5886 0.5604 0.6169\nNAR-LeVT 0.2268 0.5795 0.9995 0.3329\nDiffuSeq 0.2413 0.5880 0.2732 0.8641\nDiffusionBERT0.2420 0.5845 0.1504 0.9770\n(b) Paraphrase\nTable 2: Results on conditional generation tasks. The\nbest and second best results are remarked in bold and\nunderlined, respectively.\nfusionBERT but is quite harmful for D3PM. This\nsuggests that initializing pθ with PLMs enables\nDiffusionBERT to perform generation without ex-\nplicitly providing time information yet achieving\nbetter generation results. The resemblance between\nBERT pre-training objective and absorbing diffu-\nsion models makes it easier for DiffusionBERT to\ngeneralize to noisier scenarios while a Transformer\nencoder trained from scratch needs a specific time-\naware module to model the reverse process.\nEffect of the Spindle Noise Schedule We try our\nproposed spindle noise schedule on both Diffusion-\nBERT and D3PM. The perplexity is improved by\n5.8% and 19% for D3PM and DiffusionBERT, re-\nspectively. Besides, D3PM with the spindle sched-\nule outperforms that with the standard(T−t+1)−1\nschedule in terms of BLEU score. The same trend\nholds for DiffusionBERT but with a smaller mar-\ngin.\n4.5 Quality-Diversity Trade-off\nFigure 3 demonstrates the quality-variation trade-\noff by changing the truncation parameter Kor the\nsampling temperature τ in 10 values2, Diffusion-\nBERT exhibits comparable generation ability with\n2Except for Diffusion-LM and BERT-Mouth since control-\nling temperature has little effect.\n4527\na Transformer decoder trained from scratch and\npushes the Pareto front of NAR generation qual-\nity/diversity trade-off by a large margin. However,\nit still falls behind pre-trained AR models of the\nsame size.\n4.6 Training Efficiency\nOne important feature of DiffusionBERT is that\nwith time-agnostic decoding, all parameters are\ninitialized by pre-trained models. Consequently,\nthe model includes fewer parameters and gets rid\nof adapting new parameters, improving the train-\ning efficiency. We only train DiffusionBERT for\n40% steps of D3PM and 20% steps of DiffuSeq\ntill convergence. Appendix.D provides a detailed\ncomparison of convergence speed between Diffu-\nsionBERT and the diffusion baselines.\n4.7 Conditional Generation\nTo evaluate the generation quality and diversity, we\nchoose two metrics in each aspect. Besides BLEU\nand Self-BLEU scores, we also report Rouge-L\nfor quality and Div-4 score for diversity. Higher\nRouge-L (resp. Div-4) suggests better generation\nquality (resp. diversity).\nAs shown in Table 2, DiffusionBERT achieves\ncompetitive performance on both tasks and sur-\npasses other baselines by a large margin in terms\nof diversity. We attribute such variety to the knowl-\nedge BERT obtained during pre-training and the\ndiffusion generative process (Li et al., 2022). Apart\nfrom lexical or grammatical diversity, Diffusion-\nBERT covers a wider range of semantic meanings.\nMoreover, such diversity contributes to generation\nquality via MBR decoding (Gong et al., 2022).\n5 Related Work\n5.1 BERT for Text Generation\nIt has been shown by Wang and Cho (2019) that\nthe transfer-learning ability of BERT does not only\nhelps to achieve impressive results in natural lan-\nguage understanding but also benefits sequential\nsampling for text generation. However, its bi-\ndirectionality nature holds BERT from matching\nthe decoder-only counterparts (Radford et al., 2018)\nin modeling text from left to right.\n5.2 Diffusion Models for Text\nThis work lies in the line of diffusion models, a\nlatent variable generative framework proposed by\nSohl-Dickstein et al. (2015). It has been archi-\ntecturally improved by Ho et al. (2020) and has\ngained broad attention for its impressive genera-\ntion ability and controllability in image generation\n(Ramesh et al., 2022; Saharia et al., 2022). De-\nspite that, diffusion models for text still struggle\nto match autoregressive models in various gener-\nation tasks. Since the Gaussian noise proposed\nin Sohl-Dickstein et al. (2015) cannot be directly\napplied to discrete data, they also introduced a dis-\ncrete forward process with a Bernoulli transition\nkernel. Hoogeboom et al. (2021) generalized from\nBernoulli to categorical distributions. A more gen-\neral family of discrete diffusion processes was in-\ntroduced in Austin et al. (2021); Hoogeboom et al.\n(2022), including absorbing kernels and combina-\ntions of absorbing and uniform transition kernels.\nLi et al. (2022); Gong et al. (2022) models text in\nthe continuous embedding space, which is closer\nto the settings in earlier works of diffusion models.\n5.3 Non-Autoregressive Text Generation\nAbsorbing discrete diffusion models resembles con-\nditional masked language models (Ghazvininejad\net al., 2019) in that both methods predict the whole\nsequence simultaneously and follows a construct-\ndestruct pattern to iteratively refine the generated\ntext. The difference between those two models has\nbeen discussed in Austin et al. (2021). Savinov\net al. (2022) proposed to approach the problem\nof non-autoregressive text modeling via unrolling\nthe generation path, which resembles the idea of\ndiffusion models for unconditional text generation.\nNon-autoregressive models are also considered in\ntranslation but implemented in various ways, e.g.,\ninsertion/deletion (Gu et al., 2019) and iterative\nsequence alignment (Saharia et al., 2020).\n6 Conclusion\nThis work aims to approach the problem of text\ngeneration with non-autoregressive models. To\nachieve this, we combine pre-trained denoising lan-\nguage models with absorbing-state discrete diffu-\nsion models. The training procedure of our method\nincludes two main deviations from current discrete\ndiffusion models, i.e., a new family of time step de-\nsigns and the spindle noise schedule. The spindle\nnoise assigns a schedule for each token according\nto its frequency in the training corpus. Experimen-\ntal results of unconditional generation demonstrate\nthe success of DiffusionBERT in terms of genera-\n4528\ntion quality and diversity. In constrained settings,\nDiffusionBERT surpasses 7 strong baselines in gen-\neration variety by a large margin and its generation\nquality matches state-of-the-art methods.\nLimitations\nIn this work, we demonstrate the effectiveness of\nthe proposed DiffusionBERT. However, the sam-\npling efficiency in unconditional generation still\nlags behind fine-tuned GPT and we observe a few\nsampled sentences lacking coherence when the pre-\nassigned length is large (e.g., 128). The issue of\ninference efficiency is more severe in constrained\nsettings in that MBR decoding samples multiple\nsentences for one source text. Though it brings\nsignificant improvement in BLEU and Rouge-L\nscores, the sampling time of one batch is several\ntimes that of unconditional generation.\nEthics Statement\nThe proposed DiffusionBERT is a novel approach\nfor text-based diffusion models. In addition,\nwe demonstrate that DiffusionBERT can achieve\nhighly efficient training due to the use of PLMs.\nTherefore, this work helps reduce computation\ncosts and carbon emissions. Though all the datasets\nand PLMs used in our experiments are publicly\navailable and have not been reported to carry social\nbias against any sensitive attributes, more work is\nstill needed to investigate the potential unfairness\nin these datasets and the knowledge BERT carries.\nAcknowledgements\nThis work was supported by the National Natural\nScience Foundation of China (No. 62236004 and\nNo. 62022027).\nReferences\nJacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel\nTarlow, and Rianne van den Berg. 2021. Structured\ndenoising diffusion models in discrete state-spaces.\nIn Advances in Neural Information Processing Sys-\ntems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 17981–17993.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin. 2020.\nLanguage gans falling short. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nCiprian Chelba, Tomás Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robinson.\n2014. One billion word benchmark for measuring\nprogress in statistical language modeling. In INTER-\nSPEECH 2014, 15th Annual Conference of the In-\nternational Speech Communication Association, Sin-\ngapore, September 14-18, 2014, pages 2635–2639.\nISCA.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nBhuwan Dhingra, Kathryn Mazaitis, and William W.\nCohen. 2017. Quasar: Datasets for question answer-\ning by search and reading. CoRR, abs/1707.03904.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel\ndecoding of conditional masked language models.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 6111–6120.\nAssociation for Computational Linguistics.\nShansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,\nand Lingpeng Kong. 2022. Diffuseq: Sequence to se-\nquence text generation with diffusion models. CoRR,\nabs/2210.08933.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O. K.\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In 6th International Con-\nference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. OpenReview.net.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019.\nLevenshtein transformer. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n4529\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 11179–11189.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nEmiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bast-\nings, Ben Poole, Rianne van den Berg, and Tim Sal-\nimans. 2022. Autoregressive diffusion models. In\nThe Tenth International Conference on Learning Rep-\nresentations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net.\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini,\nPatrick Forré, and Max Welling. 2021. Argmax\nflows and multinomial diffusion: Towards\nnon-autoregressive language models. CoRR,\nabs/2102.05379.\nPhilipp Koehn. 2004. Statistical significance tests for\nmachine translation evaluation. In Proceedings of\nthe 2004 Conference on Empirical Methods in Nat-\nural Language Processing , EMNLP 2004, A meet-\ning of SIGDAT, a Special Interest Group of the ACL,\nheld in conjunction with ACL 2004, 25-26 July 2004,\nBarcelona, Spain, pages 388–395. ACL.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 3045–\n3059. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori B. Hashimoto. 2022. Diffusion-\nlm improves controllable text generation. CoRR,\nabs/2205.14217.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. 2022. Dpm-solver: A fast ODE\nsolver for diffusion probabilistic model sampling in\naround 10 steps. CoRR, abs/2206.00927.\nAlexander Quinn Nichol and Prafulla Dhariwal. 2021.\nImproved denoising diffusion probabilistic models.\nIn Proceedings of the 38th International Conference\non Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 ofProceedings of Machine\nLearning Research, pages 8162–8171. PMLR.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nSCIENCE CHINA Technological Sciences.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with CLIP latents.\nCoRR, abs/2204.06125.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022 , pages 10674–10685.\nIEEE.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho,\nDavid J. Fleet, and Mohammad Norouzi. 2022. Pho-\ntorealistic text-to-image diffusion models with deep\nlanguage understanding. CoRR, abs/2205.11487.\nChitwan Saharia, William Chan, Saurabh Saxena, and\nMohammad Norouzi. 2020. Non-autoregressive ma-\nchine translation with latent alignments. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 1098–1108. Associa-\ntion for Computational Linguistics.\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski,\nErich Elsen, and Aäron van den Oord. 2022. Step-\nunrolled denoising autoencoders for text generation.\nIn The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-\n29, 2022. OpenReview.net.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-\neswaranathan, and Surya Ganguli. 2015. Deep un-\nsupervised learning using nonequilibrium thermody-\nnamics. In Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, volume 37 of JMLR Work-\nshop and Conference Proceedings, pages 2256–2265.\nJMLR.org.\n4530\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2021.\nDenoising diffusion implicit models. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022. Black-box tuning for\nlanguage-model-as-a-service. In Proceedings of the\n39th International Conference on Machine Learning,\nICML 2022, Baltimore, Maryland, USA.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a markov random\nfield language model. CoRR, abs/1902.04094.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\nSIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018,\npages 1097–1100. ACM.\nA Generation Process\nDuring sampling phase, all three generative masked\nlanguage models show an easy-first generative be-\nhavior through time. Table 3 demonstrates an ex-\nample in unconditional settings. When the context\nis extremely sparse, the model tends to generate\ntokens that is most frequently appearing (and is\nleast surprising) in the training corpus to achieve a\nhigher likelihood, though the generated sequences\nexhibit no consistency. As the context becomes\nricher, more details come up in the sequence. This\nphenomenon indicates the discrepancy between\ntraining and sampling: absorbing discrete diffu-\nsion models for text generation corrupt all tokens\nindependently with the same noise schedule during\ntraining, while the neural network prefers tokens\nthat are less informative when most of the input is\nmasked.\nB Templates of Conditional Generation\nWe show in Table 4 how the source sentences in\nSeq2seq tasks are transformed into our input tem-\nplate. Instead of simply concatenating source and\ntarget text, such format offers DiffusionBERT with\nmore information to generate the desired outputs.\nC Length of Generated Text\nNAR methods have long been faced with the prob-\nlem of fixed length generation. Unlike their AR\ncounterparts, NAR methods are generally not able\nto dynamically determine the end of the sequence\nFigure 4: Curve of validation ELBO during training.\nduring generation. Especially in constrained set-\ntings, length of the optimal output depends greatly\non the source sentence and cannot be assigned in ad-\nvance. Existing solutions include length prediction\nmodules (Gu et al., 2019) and generating [PAD]\ntokens (Gong et al., 2022). DiffusionBERT adopts\nthe latter method for target length determination. In\nparticular, we set the overall sequence length to 128\nand train DiffusionBERT to predict all [MASK]\ntokens according to the instruction. Predictions\nconsist of the generated target text followed by a\n[SEP] token and a series of [PAD] tokens. As\nshown in Table 4, the target length is dynamic de-\npending on the position of [SEP].\nD Training Speed\nThanks to Time Agnostic Decoding, we introduce\nno additional parameters into our backbone. Thus\ntraining DiffusionBERT equals to finetuning BERT\nto generate text, which is relatively easier than train-\ning from scratch. In unconditional training, Diffu-\nsionBERT converges remarkably faster than D3PM.\nEven if the training budget is cut to 30% that of\nD3PM, DiffusionBERT is still able to match the\nperformance reported in Table 1. Figure 4 demon-\nstrates the curve of validation ELBO in the training\nprocess. Such superiority in convergence speed\nalso holds in constrained settings. Besides, with the\nhelp of PLMs, DiffusionBERT can be well trained\nwith smaller batch size and requires less compu-\ntational resources. Though trained with half the\nnumber of GPU cores, DiffusionBERT achieves\na 80% convergence acceleration compared to Dif-\nfuSeq on the QQP dataset.\n4531\nBERT-Mouth\nt= 0 [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\nt= 8 [MASK]of [MASK]five[MASK]remain[MASK]in [MASK].\nt= 16 two of[MASK]five structures remain[MASK]this location .\nt= 24 five of[MASK]the windows remain at this location .\nt= 32 most of even the windows stand still this day .\nD3PM\nt= 0 [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\nt= 8 [MASK] [MASK] [MASK] [MASK]been[MASK] [MASK] [MASK] [MASK].\nt= 16 [MASK] [MASK] [MASK] [MASK]been[MASK] [MASK]the[MASK].\nt= 24 [MASK] [MASK] [MASK]also been[MASK]by the[MASK].\nt= 32 the man has also been arrested by the police .\nDiffusionBERT\nt= 0 [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\nt= 8 [MASK], [MASK] [MASK] [MASK] [MASK] [MASK]that[MASK] .\nt= 16 today ,[MASK]will be[MASK] [MASK]that[MASK] .\nt= 24 today ,[MASK]will be remembered for that mistake .\nt= 32 today , he will be remembered for that mistake .\nTable 3: Examples generated by three generative masked language models. All three models yield words of higher\nfrequency when tis small and tend to generate more informative tokens as the reverse diffusion process goes on.\nTask Question Generation Paraphrase\nTemplate Answer: <src>. Question: <tgt> The sentence \"<src>\" is equal to \"<tgt>\"\nInput ExampleAnswer: She’s into video games. Question:M M M M M M M MThe sentence \"Ava feels happy.\" is equal to \"M M M M M M M M\nGeneration ExamplesWhat is Ava doing now?[SEP] [PAD] Ava feels positive.[SEP] [PAD] [PAD] [PAD]Is Ava free now?[SEP] [PAD] [PAD] Ava is in a cheerful state.[SEP]\nTable 4: Instruction templates and their corresponding generation examples in conditional generation. <src> and\n<tgt> refers to the source and target text in one data sample, respectively. M is the abbreviation of [MASK] token.\nMethod Steps Inference Time (secs) PPL\nDiffusionBERT\n2 0.66 313.57\n8 1.39 91.01\n16 1.80 75.66\n64 4.25 65.83\n128 7.53 63.78\n512 27.48 54.63\nDiffusion-LM 2000 83.67 112.12\nBERT-Mouth 64 2.18 142.89\n512 14.39 86.78\nGPT 64 1.55 38.7\nTable 5: Comparison of inference time and perplexity\namong baselines and DiffusionBERTin unconditional\ngeneration.\nE Sampling Speed\nWith the x0-parameterization proposed in Song\net al. (2021) and Austin et al. (2021), Diffusion-\nBERT is able to perform inference with any given\nbudget by controlling the step size in the reverse\nprocess. We also control the sampling time of\nBERT-Mouth by adjusting the max iteration count\nof its mask-predict process. We list the decod-\ning speed and the corresponding perplexity on\nthe LM1B test set in Table 5. Overall, Diffu-\nsionBERT exhibits competitive performance even\nwhen it reaches comparable speed to GPT and out-\nperforms BERT-Mouth in efficiency-performance\ntradeoff.\nRecent works have proposed some higher-order\nODE solvers to accelerate diffusion models in con-\ntinuous domain (Lu et al., 2022). By leveraging the\nall-tokens-in-one-forward nature of NAR text gen-\neration, we look forward to discovering the poten-\ntial of DiffusionBERT in sampling speed. Specif-\nically, the generation time of AR models is lim-\nited by the sequence length n. While Diffusion-\nBERT decomposes the sampling process through\ntime t, which can be optimized by advanced diffu-\nsion model sampler so that tis much smaller than\nn. We leave this for future work.\nF Implementation of Evaluation Metrics\nWe evaluate the performance of Diffusion-\nBERT with 3 n-gram-based methods and Rouge-L.\nThe n-gram methods, namely BLEU-4, Self-BLEU-\n4 and Div-4, are implemented based on NLTK.\nWe use the implementation in torchmetrics\nto compute Rouge-L scores.\n4532\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nThe limitations are discussed in the ﬁrst section after the conclusion.\n□\u0013 A2. Did you discuss any potential risks of your work?\nThe potential risks are discussed in the second section after the conclusion.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and 1. Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4.1 Tasks and Datasets, 4.2 Baselines\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4.1 Tasks and Datasets, 4.2 Baselines\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAll the datasets used in the submission (listed in 4.1 Tasks and Datasets) are publicly accessible for\nresearch use\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4.1 Tasks and Datasets\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nIn the Ethics Statement section.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4.1 Tasks and Datasets\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4.1 Tasks and Datasets\nC □\u0013 Did you run computational experiments?\n4 Experiments and Appendix D\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4.3 Experimental Setup and Appendix D\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4533\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4.3 Experimental Setup\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4.4 Unconditional Generation\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix F Implementation of Evaluation Metrics\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4534",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8521440029144287
    },
    {
      "name": "Perplexity",
      "score": 0.8180437088012695
    },
    {
      "name": "Language model",
      "score": 0.6577552556991577
    },
    {
      "name": "Initialization",
      "score": 0.5206048488616943
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5187664031982422
    },
    {
      "name": "Generative grammar",
      "score": 0.508258044719696
    },
    {
      "name": "Security token",
      "score": 0.4731001853942871
    },
    {
      "name": "Noise (video)",
      "score": 0.46178609132766724
    },
    {
      "name": "Process (computing)",
      "score": 0.44788122177124023
    },
    {
      "name": "Generative model",
      "score": 0.44149312376976013
    },
    {
      "name": "Machine learning",
      "score": 0.4010079503059387
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    }
  ]
}