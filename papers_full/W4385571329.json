{
  "title": "Language acquisition: do children and language models follow similar learning stages?",
  "url": "https://openalex.org/W4385571329",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092118905",
      "name": "Linnea Evanson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2404476342",
      "name": "Yair Lakretz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226093966",
      "name": "Jean-Remi King",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3009352632",
    "https://openalex.org/W2990241049",
    "https://openalex.org/W4200390713",
    "https://openalex.org/W2483390977",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2922523190",
    "https://openalex.org/W1990111101",
    "https://openalex.org/W2101509422",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W1972102750",
    "https://openalex.org/W2063525438",
    "https://openalex.org/W3211225612",
    "https://openalex.org/W2020944885",
    "https://openalex.org/W2115099665",
    "https://openalex.org/W4309419356",
    "https://openalex.org/W4212828284",
    "https://openalex.org/W2805003518",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2165056311",
    "https://openalex.org/W4387489715",
    "https://openalex.org/W2907911668",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W1989083219",
    "https://openalex.org/W4322766928",
    "https://openalex.org/W2063303346",
    "https://openalex.org/W2140661818",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W2947012833",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W3213014097",
    "https://openalex.org/W3129394092",
    "https://openalex.org/W2034626235",
    "https://openalex.org/W3159684727",
    "https://openalex.org/W4283643570",
    "https://openalex.org/W2889781226",
    "https://openalex.org/W2986889180",
    "https://openalex.org/W4286903739",
    "https://openalex.org/W1514872638",
    "https://openalex.org/W2165627680",
    "https://openalex.org/W3037191812",
    "https://openalex.org/W4390854704",
    "https://openalex.org/W2121853833",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3154603286",
    "https://openalex.org/W3168194750",
    "https://openalex.org/W2802524196",
    "https://openalex.org/W2169947519",
    "https://openalex.org/W3014415613",
    "https://openalex.org/W2038594453",
    "https://openalex.org/W3038040931"
  ],
  "abstract": "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of human children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some – but not all – learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 12205–12218\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLanguage acquisition: do children and language models follow similar\nlearning stages?\nLinnea Evanson\nMeta AI Paris;\nLaboratoire des systèmes perceptifs\nÉcole normale supérieure\nPSL University\nlinnea.evanson8@gmail.com\nYair Lakretz∗\nCognitive Neuroimaging Unit\nCEA, INSERM\nUniversité Paris-Saclay\nNeuroSpin Center\nyair.lakretz@gmail.com\nJean-Rémi King∗\nMeta AI Paris;\nLaboratoire des systèmes perceptifs\nÉcole normale supérieure\nPSL University\njeanremi@meta.com\nAbstract\nDuring language acquisition, children follow a\ntypical sequence of learning stages, whereby\nthey first learn to categorize phonemes before\nthey develop their lexicon and eventually mas-\nter increasingly complex syntactic structures.\nHowever, the computational principles that lead\nto this learning trajectory remain largely un-\nknown. To investigate this, we here compare\nthe learning trajectories of deep language mod-\nels to those of children. Specifically, we test\nwhether, during its training, GPT-2 exhibits\nstages of language acquisition comparable to\nthose observed in children aged between 18\nmonths and 6 years. For this, we train 48 GPT-\n2 models from scratch and evaluate their syn-\ntactic and semantic abilities at each training\nstep, using 96 probes curated from the BLiMP,\nZorro and BIG-Bench benchmarks. We then\ncompare these evaluations with the behavior of\n54 children during language production. Our\nanalyses reveal three main findings. First, sim-\nilarly to children, the language models tend\nto learn linguistic skills in a systematic order.\nSecond, this learning scheme is parallel: the\nlanguage tasks that are learned last improve\nfrom the very first training steps. Third, some –\nbut not all – learning stages are shared between\nchildren and these language models. Overall,\nthese results shed new light on the principles\nof language acquisition, and highlight impor-\ntant divergences in how humans and modern\nalgorithms learn to process natural language.\n1 Introduction\nLanguage acquisition is marked by a series\nof successive stages (Dupoux, 2018; Kuhl, 2004;\nWerker, 2018). Within their first year of existence,\nhumans infants successively acquire prosody con-\ntours (Mehler et al., 1988), phonetic categories\n(Werker and Tees, 1984; Kuhl et al., 1992; Mazuka\net al., 2011) and frequent words (Tincoff and\nJusczyk, 1999; Bergelson and Swingley, 2012).\n∗Equal Contribution\nThey then learn to produce basic syntactic struc-\ntures (e.g. “The boy sang” or “The boy fell”), ques-\ntions (e.g. “What sound does a cow make?”) and\nnested syntactic structures (e.g. “The boy that I saw\nsang”), at approximately 12, 30, and 42 months,\nrespectively (Friedmann et al., 2021). Even though\nsome children may take slightly longer to learn\nthan others, there is a set order in which children\nacquire various syntactic structures (Friedmann and\nReznick, 2021).\nOur understanding of the entire learning trajec-\ntory of children remains very coarse, however. This\npartly stems from the difficulty of measuring lin-\nguistic skills in young children. In babies, exper-\nimenters typically measure eye gaze and sucking\nrate while children process linguistic stimuli, as\nthese reflexive behaviors are known to increase dur-\ning surprising events. Such “implicit” approaches\nhave successfully been used to assess whether non-\nspeaking infants detect linguistic violations (Za-\nmuner, 2006), distinguish lexical from grammat-\nical words (Shi et al., 1999) or discriminate their\nnative language from a foreign language (Mehler\net al., 1988; Kuhl et al., 2006; Nazzi et al., 2000).\nIn older children, linguistic skills can also be more\nexplicitly measured from spontaneous speech and\nsentence repetition. For example, a recent study by\nFriedmann et al. (2021), to which we compare our\nwork in this paper, quantified the extent to which\n18 month to 6 year-old children produce variably\ncomplex syntactic structures. For both of these ap-\nproaches, however, the measures from children at\nsuch early ages can be noisy and fragmented.\nInterestingly, these issues do not apply to mod-\nern language models. Deep learning architectures\ntrained to predict words from their proximal con-\ntexts have proved immensely effective at learning\nto process natural language (Radford et al., 2019;\nDevlin et al., 2019). Unlike humans, these algo-\nrithms can be easily probed during training, at any\ntime point and rate, and with unlimited number of\n12205\ntest stimuli, without interfering with their language\nacquisition (Jawahar et al., 2019; Manning et al.,\n2020; Bowman and Dahl, 2021). Furthermore,\nhigh-performing deep nets have been shown to im-\nplicitly (Lakretz et al., 2019; Gulordava et al., 2018)\nor explicitly learn to represent and use syntactic\nstructures (Manning et al., 2020), as well as to use\nfeatures such as concreteness and lexical class to\nlearn language (Chang and Bergen, 2022). Finally,\nand importantly, these deep neural networks have\nrecently been shown to represent lexical, syntactic\nand compositional representations similarly to the\nadult brain (Jain and Huth, 2018; Toneva and We-\nhbe, 2019; Caucheteux and King, 2022; Pasquiou\net al., 2022, 2023; Caucheteux et al., 2023). Evi-\ndencing similar learning trajectories in children and\nlanguage models could thus provide an invaluable\nframework to better understand the computational\nprinciples underlying language acquisition.\nHere, we compare the trajectory of language\nacquisition between human children and modern\nlanguage models. We focus on three main ques-\ntions. First, do these models learn linguistic skills\nin a systematic order? Second, is this trajectory se-\nquential or parallel? Third, is this trajectory similar\nto that of children? These hypotheses are illustrated\nin Figure 1.\nSpecifically, we train 48 GPT-2 architectures\n(Radford et al., 2019) from scratch, using a stan-\ndard next-word prediction objective. We then eval-\nuate, at each training step, their linguistic abilities\nwith 96 semantic and syntactic probes curated from\nthe BLiMP, Zorro and BIG-Bench benchmarks\n(Warstadt et al., 2020; Huebner et al., 2021; Sri-\nvastava et al., 2022). Finally, we compare a subset\nof these probes to the behavior of 54 children aged,\nbetween 18 months and 6 years (Friedmann et al.,\n2021).\n2 Approach\n2.1 Language models\nWe consider two main language models. First, we\nuse a pretrained language model – GPT-2 – as pro-\nvided by HuggingFace 1 and pretrained on 40 GB\nof data (Radford et al., 2019). Second, we sepa-\nrately train 48 versions of a 12-layer GPT-2 model\nfrom scratch. We train each model on WikiText103\n(Merity et al., 2016) with a distinct random seed\nto set its initial parameters and data-loader. Each\nmodel is evaluated on all linguistic probes every\n1https://huggingface.co/gpt2\n100 training steps. Further training details are pro-\nvided in Appendix B.\n2.2 Zero-shot linguistic probes\nZero-shot linguistic probes are sentences or phrases\ncrafted to evaluate whether a model has learned a\nparticular linguistic skill, without training or fine-\ntuning the model on that particular skill. In prac-\ntice, a zero-shot probe consists of comparing the\nestimated probability of a grammatical sentence\nwith that of a matched ungrammatical sentence.\nThis two-alternative forced-choice approach can\nbe compared to \"acceptability judgements\", classi-\ncally used in linguistics (Warstadt et al., 2019).\nWe evaluate our models on 96 different linguis-\ntic probes, curated from three open source bench-\nmarks, the details of which are presented in Ap-\npendix C.\nSpecifically, we compare the probability of each\nsentence in a grammatical/ungrammatical pair by\nevaluating the sum of the logarithm of the loss\noutput by the softmax layer:\nng∑\ni=0\nlog(f(Xg)i) <\nnu∑\nj=0\nlog(f(Xu)j) (1)\nwith f the softmax layer of the language model,\nXg and Xu the grammatical and ungrammatical\nsentences, respectively, and ng and nu, the number\nof tokens in the grammatical and ungrammatical\nsentences, respectively.\nThe accuracy of a given probe is the percent-\nage of pairs where the estimated probability of the\ngrammatical sentence is higher than that of the un-\ngrammatical sentence.\n2.3 Assessing learning trajectory\nTo evaluate whether the trajectory of language ac-\nquisition is shared across models, we rank the\nprobes by their \"acquisition time\", i.e. the num-\nber of steps taken by a model to reach 90% of its\nfinal accuracy on a particular probe, for each model\nindependently. We then assess the correlation of\nranks between all pairs of the 48 models and take\nthe average of these correlations. To estimate the\nstatistical significance of this average correlation\nwe redo this calculation for all possible model pairs\nafter shuffling the ranks of one of the models in\neach pair. We repeat this permutation 1,000 times,\ngetting 1,000 values for this shuffled correlation. If\nin all cases this shuffled correlation is lower than\n12206\nthe true average correlation, then the order of ac-\nquisition time is shared across models with p <\n0.001.\n2.4 Parallel versus Sequential learning\nLanguage acquisition may be characterized by a\n\"sequential\" or a \"parallel\" learning scheme (Fig-\nure 1). \"Sequential\" learning designates the case\nwhere a complex skill does not start to be learned\nbefore simpler skills are mastered. By contrast,\n\"Parallel\" learning designates the case where all\nskills are acquired simultaneously, but at differ-\nent speeds. The null hypothesis is that the order\nin which an agent learns linguistic skills varies\nacross agents. To determine the learning scheme of\nlanguage models, we consider whether the probes\nhave a positive derivative in the first three check-\npoints (parallel learning) or not (sequential learn-\ning), and whether they have statistically different\nlearning rates (by performing a one-way ANOV A\ntest) across the three groups.\n2.5 Assessing linguistic skill from children’s\nbehavior\nFriedmann et al. (2021) studied 54 Hebrew-\nspeaking children between the ages of 18 - 71\nmonths and investigated the emergence of 11 lin-\nguistic phenomena, which the authors propose to\norganize into three stages (details in Appendix A).\nFor our analysis we select the following tests, one\nfrom each stage:\n• Stage 1: Simple sentences in subject-verb\n(SV) order\n• Stage 2: Wh Questions\n• Stage 3: Relative Clauses\nData collection consisted of spontaneous speech\nsamples produced by each child at home. Each\nsample was then manually annotated to detect the\npresence of each of the linguistic phenomena. A\nlinguistic phenomenon was considered learned if\nand only if it was present in the speech sample.\nSpeech samples had a mean length of 151 utter-\nances per sample and standard deviation of 37. The\naggregated data was made available directly in the\noriginal paper (under Creative Commons Attribu-\ntion 4.0 International License), and here used for\ncomparison with our language models. In Table 1\nwe show which probes in the models matched with\nthese tests.\n3 Results\nWe aim to compare the learning trajectories of\ndeep language models to those observed in 54 chil-\ndren aged between 18 months and 6 years. For\nthis, we trained variants of GPT-2 models (Radford\net al., 2019) from 48 different random seeds with\nthe WikiText103 dataset (Merity et al., 2016) and\nevaluated each model on 96 linguistic probes every\n100 steps.\nAt the end of this training, 64 probes (66%) were\nachieved above chance level (50% accuracy) by\nall models. In comparison, a pretrained version\nof GPT-2 large (Radford et al., 2019) provided\nby Hugging Face2, and trained on a much larger\ndataset3, achieves above-chance performance on\n93 of the 96 probes.\n3.1 A systematic learning trajectory\nFor clarity, we focus on the learning dynamics of\nthe probes that ultimately achieve above-chance\nperformance in our training. Figure 2 lists all\nprobes learned above chance level, ordered by their\naverage acquisition time. We perform the permu-\ntation analysis outlined in 2.3, to evaluate whether\nthe order of acquisition is shared between models,\nand find that their order of acquisition is correlated\nwith R = 0.743 and p < 0.001. These results sug-\ngest that there is a systematic learning trajectory\namong models.\n3.2 Learning is parallel across linguistic tasks\nAre these linguistic skills learned sequentially or\nin parallel (Figure 1)? To address this question,\nwe evaluate whether each linguistic probe starts to\nimprove from the very first training steps but with\ndifferent rates (i.e. a “parallel” learning scheme)\nor, on the contrary, whether some probes only start\nto improve once others have reached a particular\nperformance (i.e. a “sequential” learning scheme).\nAs the individual learning trajectories of each probe\nwere noisy, we group the 64 linguistic probes into\nthree categories: early, middle and late acquisition\ntime (Figure 3).\nOverall, we observe parallel learning between\nthe three groups: their performances all increase\nfrom the beginning of training: 95% of tests in\nall three groups have a positive derivative within\nthe first three hundred steps. However, they have\ndifferent learning rates, as evaluated with a one-\nway ANOV A test on the learning rate (i.e. change\n2https://huggingface.co/tftransformers/gpt2-large\n340 GB compared to the 181 MB of WikiText103\n12207\n1 2 3\nTraining step\nchance\nlevel\nskill\nacquiredAccuracy\nAgent 1\nAgent 2\nSequential Learning Dynamics\nT est A\nT est B\nT est C\n1 2 3\nTraining step\nchance\nlevel\nskill\nacquired Agent 1\nAgent 2\nParallel Learning Dynamics\nTraining step\nchance\nlevel\nskill\nacquired\nNull Hypothesis\n1 2 3\nAge of Children\nskill\nobserved\n1 2 3\nAge of Children\nskill\nobserved\nAge of Children\nskill\nobserved\nFigure 1: Hypotheses. Skill performance (y-axis) as a function of training (x-axis) illustrated on three tasks (colors)\nand two agents (model or children). Sequential learning implies that the learning of a complex skill (e.g. C, shown\nin red) does not start before simplest skills (e.g. A in green and B in blue) are fully learned. Parallel learning implies\nthat all skills are acquired simultaneously, but at different speeds. Sequential and parallel learning may cross an\narbitrary performance threshold at the same training step. The frequency at which we can probe artificial networks\nduring learning is much greater than what is realistically possible in children, giving us the timescale granularity to\ndistinguish sequential and parallel learning trajectories. We also present a null hypothesis, that artificial networks\nwith different random seeds may learn skills in a different order.\nStage Children Language Model\n1 Simple sentences in Subject-Verb (SV) order SV agreement across simple sentences\n2 Wh-questions SV agreement in questions\n3 Relative Clauses (RCs) SV agreement across object RCs\nTable 1: Linguistic phenomena in children selected for comparison with probes in the language models.\nof accuracy over time) obtained in each group and\nin each model (p <10−23).\n3.3 Comparison with children\nDo these learning trajectories match the behavior\nof human children? For the three probes that cor-\nrespond to the three stages identified in children’s\nlanguage acquisition (Table 1), we observe that the\norder in which these three probes are learned by\nthe language models is the same as those of chil-\ndren (Figure 4). This effect is robust across random\nseeds: 46 of our 48 GPT-2 models follows this\norder, where chance level is (1/3!)46 = 1.60e−36.\nFor this subset of language abilities, models and\nchildren thus seem to acquire syntactic skills in a\nsimilar order.\n3.4 Learning of theoretically-defined stages is\nparallel\nIn part 3.1, we showed that GPT-2 learns its lan-\nguage abilities in parallel. Does this learning\nscheme also characterize the three syntactic skills\ninvestigated in children? To address this question,\nwe now look at the learning curves of the skills\ndefined in Table 1, as well as an additional probe:\nNounpp, as it can be separated into congruent and\nincongruent cases which is important for the anal-\nysis in section 3.5. Overall, we observe that these\nprobes are all learned in parallel in the model (Fig-\nure 5A).\n3.5 Models use both syntax and heuristics\nBoth an understanding of syntactic rules and a su-\nperficial heuristics can lead to above-chance per-\nformances on these probes. Indeed, in many sen-\ntences (e.g. The cat [subject] of the lady [attractor]\nis [verb] hungry.), the number of the verb is con-\ngruent with the number of the adjacent attractor,\neven if the two are not related syntactically. To\nverify that the GPT-2 models effectively learn the\nsyntactic rules, we thus separately examine con-\ngruent and incongruent cases. Incongruent cases\nrequire knowledge of the syntax of the sentence\nas the correct verb number is different from the\nnumber of the attractor. Empirically, we observe\nthat the models do not learn the incongruent case\nin stage three above chance level, and just barely\nreach chance level on the incongruent case in stage\ntwo (Figure 5B), indicating that our models are us-\ning heuristics rather than syntactic rules to achieve\nhigh accuracy on the congruent cases (leading to\nabove chance performance on the probe overall in\nFigure 5A). On the contrary, the pretrained GPT-\n12208\nDrop Argument\nAnimate Subject Trans\nFiller-Gap-Wh Question Subject\nExistential There Quantifiers 1\nPrinciple A Case 1\nCase-Subjective Pronoun\nSentential Negation Npi Licensor Present\nT ough Vs Raising 2\nPassive 2\nWh Questions Subject Gap\nWh Questions Subject Gap Long Distance\nWh Vs That No Gap\nWh Vs That No Gap Long Distance\nExpletive It Object Raising\nArgument Structure-Swapped Arguments\nAnimate Subject Passive\nExistential There Object Raising\nPrinciple A Domain 1\nFiller-Gap-Wh Question Object\nIrregular-Verb\nIrregular Past Participle Verbs\nArgument Structure-Transitive\nQuantifiers-Superlative\nPassive 1\nLeft Branch Island Echo Question\nRegular Plural Subject Verb Agreement 1\nTransitive\nIsland-Effects-Coordinate Structure Constraint\nAgreement Subject Verb-Across Prepositional Phrase\nAnaphor Number Agreement\nAgreement Determiner Noun-Across 1 Adjective\nAgreement Subject Verb-In Simple Question\nArgument Structure-Dropped Argument\nDeterminer Noun Agreement Irregular 2\nDeterminer Noun Agreement 1\nDeterminer Noun Agreement 2\nAnaphor Agreement-Pronoun Gender\nShort Nested Inner\nQuantifiers-Existential There\nBinding-Principle A\nRegular Plural Subject Verb Agreement 2\nDeterminer Noun Agreement Irregular 1\nDeterminer Noun Agreement With Adjective 1\nCoordinate Structure Constraint Object Extraction\nNpi Licensing-Only Npi Licensor\nPrinciple A Domain 2\nLong Nested Inner\nDeterminer Noun Agreement With Adj Irregular 2\nIrregular Plural Subject Verb Agreement 1\nAgreement Subject Verb-Across Relative Clause\nCausative\nIrregular Past Participle Adjectives\nDeterminer Noun Agreement With Adj Irregular 1\nLong Nested Outer\nDeterminer Noun Agreement With Adj 2\nExistential There Subject Raising\nShort Nested Outer\nNounpp\nSimple\nPrinciple A Case 2\nEllipsis N Bar 2\nIrregular Plural Subject Verb Agreement 2\nDistractor Agreement Relational Noun\nEllipsis-N Bar\n[Correct]/[Incorrect] ExampleLinguistic Probe\nThe Lutherans couldn't [skate around]/[disagree with].\n[Danielle]/[The eye] visited Irene.\nChris reached []/[who] the bear [that]/[] is washing trains.\nThere aren't [many][all] lights darkening.\nTara thinks that [she]/[herself] sounded like Wayne.\n[They gave the person the tour]/[The person gave they the tour].\nThose banks had [not][really] ever lied.\nRachel was [apt]/[exciting] to talk to Alicia.\nMost cashiers are [disliked]/[flirted].\nCheryl thought about []/[who] some dog that upset Sandra.\nBruce knows [who]/that person that Dawn likes [that]/[] argued about a lot of guys.\nDanielle finds out [that]/[who] many organizations have alarmed Chad.\nChristina forgot [that]/[who] all plays that win worry Dana.\nRegina [wanted]/[forced] it to be obvious that Maria thought about Anna.\n[They built the mouse that farm]/[The mouse built that farm they].\nAmanda was respected by some [waitresses]/[picture].\nWilliam has [declared]/[obliged] there to be no guests getting fired.\nCarlos said that Lori helped [him]/[himself].\nLaura got [the suit that the bird cut]/[what the suit cut the bird].\nSarah [spoke]/[spoken] without thinking last night.\nEdward [hid]/[hidden] the cats.\nWill robert [eat]/[force]?\nNo bird could catch [more than]/[at least] six plants.\nJeffrey's sons are [insulted]/[smiled] by Tina's supervisor.\n[David would cure what snake]/[What would David cure snake]?\nJeffrey [hasn't]/[haven't] criticized Donald.\nA lot of actresses' nieces have [toured]/[coped] that art gallery.\nWhat did sarah [and the person work for]/[work for and the person]?\nThe [brother][brothers] by the lion is red.\nMany teenagers were helping [themselves]/[herself].\nLook at this happy [piece]/[pieces].\nWhat color was the [piece]/[pieces]?\nMy brother moves [fast]/[to].\nThose ladies walk through [those]/[that] oases.\nCraig explored that grocery [store]/[stores].\nCarl cures [those]/[that] horses.\nShe will give [herself]/[himself] the wire.\nThe actor that the boy [attracts]/[attract].\nThere was [a]/[most] leg that anne made.\nSarah thinks about herself [making]/[makes] a tree.\nThe [dress]/[dresses] crumples.\nPhillip was lifting this [mouse]/[mice].\nTracy praises those lucky [guys]/[guy].\nWho will [Elizabeth and Gregory cure]/[Elizabeth cure and Gregory]?\n[Only]/[Even] his rabbit will ever be in her magic.\nMark imagines Erin might admire [herself]/[himself].\nThe actor that the boy beside the woman [attracts]/[attract].\nThat adult has brought [that]/[those] purple octopus.\nThis goose [isn't]/[weren't] bothering Edward.\nThe [pages]/[page] that i like were dirty.\nAaron [breaks]/[appeared] the glass\nThe [hidden]/[hid] offspring aren't confident.\nThis person shouldn't criticize this upset [child]/[children].\nThe actor that the boy beside the woman attracts [greets]/[greet].\nSome actors buy [these]/[this] gray books.\nThere was [bound]/[unable] to be a fish escaping.\nThe actor that the boy attracts [blocks]/[block].\nThe athlete behind the bike [approves]/[approve].\nThe athlete [admires]/[admire].\nStacy imagines herself [praising]/[praises] this actress.\nCurtis's boss discussed four [sons][happy sons] and Andrew discussed five [sick sons][sick].\nThe [woman]/[women] cleans every public park.\nA sketch of lights [doesn't]/[don't] appear.\nAllen got one [roman]/[] brain and chris got two []/[roman].\n0 2500 5000 7500 10000 12500 15000\nTraining Step\nAquisition Time\nFigure 2: The performance of the models on each linguistic probe over time is smoothed using a moving average\nfilter with window size of 6 checkpoints then the number of steps required to reach 90% of final accuracy (acquisition\ntime) is calculated. Probes are ordered by increasing average acquisition time. Results shown for 48 models. Only\nprobes which have final accuracy greater than chance (50%) are shown. This demonstrates that probes tend to be\nlearned in the same order by all agents with R = 0.743, p < 0.001, disproving the null hypothesis.\n2 large achieves above 75% accuracy also on the\nincongruent cases of these probes. Thus for the\nmodels trained on the WikiText103, syntax is only\nlearned for stages one and two, and heuristics seem\nto explain the above chance accuracy in stage three.\nA larger training dataset is required to learn syn-\ntax and not only heuristics for the most difficult\nexamples.\n3.6 Impact of number biases in congruent and\nincongruent sentences\nIn previous work, it was found that a variety of\nlanguage models have a bias towards plural English\nverbs, and several studies (Jumelet et al., 2019;\n12209\n0 2000 4000 6000 8000 10000 12000 14000 16000\nTraining Step\n40\n50\n60\n70\n80\n90Accuracy (%) Chance Level\nModel Performance on 3 Groups of Linguistic Probes\nGroup 1\nGroup 2\nGroup 3\nProbes not Learned\nFigure 3: Linguistic probes grouped into 3 groups to\navoid plotting one line per probe. The probes in each\ngroup correspond to the colours in Figure 2. Probes\nwhich have final accuracy less than chance (50%) are\nplaced in their own group, and tend to zero likely due to\nbiases towards plural verbs in English (c.f. 3.6). Shading\nis standard error of the mean across probes in the group.\nThis figure demonstrates that linguistic skills are learned\nin parallel not in sequence.\nLakretz et al., 2021a,b) determined that LSTM-\nbased models have a default number and gender\nprediction preference. To examine whether number\nbias has a significant effect on our analysis, we\ncompare congruent sentences with only singular or\nonly plural verbs and incongruent sentences with a\nplural or a singular verb. Accuracy on predicting\nplural verbs increases sharply from the start of the\ntraining and then drops. By contrast, the accuracy\nof singular cases first drops and then rises (Figure\n5C), indicating that the models are biased towards\nplural verbs at the beginning of training. This bias\nis overcome for the stage one probe but for stage\ntwo and three it remains throughout training. This\nexplains the initial downward trend in Group 3\nand why the unlearned probes tend toward zero in\nFigure 3.\n4 Discussion\nThe stages followed by children to acquire\nlanguage has been the topic of intense research\n(Dupoux, 2018; Kuhl, 2004; Werker, 2018). While\nthis learning trajectory is becoming clearer for sub-\nlexical representations (Dupoux, 2018), the acquisi-\ntion of higher-level syntactic and semantic process-\ning remains largely unclear. Here, we approach this\nlong-lasting question through the lens of a deep lan-\nguage architecture, GPT-2 (Radford et al., 2019),\nto test whether this model follows a learning trajec-\ntory similar to children.\n4.1 Language acquisition: similarities and\ndifferences between humans and GPT-2\nFirst, we show that GPT-2 models tend to learn a\nbattery of linguistic phenomena (Warstadt et al.,\n2020; Lakretz et al., 2019; Huebner et al., 2021) in\na consistent order. It is the reliability of the acquisi-\ntion trajectory that allows a direct comparison with\nthe learning trajectory of children (Friedmann et al.,\n2021). However, this consistency in GPT-2 mod-\nels may result from two non-mutually exclusive\nfactors, that remain to be disentangled: either the\nacquisition time of each linguistic phenomenon re-\nlates to its frequency in natural language (e.g. Sim-\nple subject-verb-complement are more frequent in\nnatural language than nested syntactic structures;\nKarlsson 2007), and/or it relates to their intrinsic\ncomplexity (e.g. sentences with nested structure re-\nquire more operations to be composed than simple\nsentences). Future work systematically controlling\nfor these relative frequencies is thus necessary to\ndistinguish these two possibilities, and would build\nupon work by Weber et al. (2021) who found that\nless frequent linguistic phenomena can be learned\nfrom fewer examples, though later in training.\nSecond, we show that the order in which lin-\nguistic skills are acquired is similar between chil-\ndren and GPT-2 – at least on the syntactic phenom-\nena that were evaluated in these two cohorts, and\nwith the limitation of using number agreement as\na proxy to whether the models acquire the corre-\nsponding syntactic structure. Similarly to children,\nGPT-2 models master subject-verb agreement in\nSV sentences before they master it in questions,\nor across nested center-embedded clauses (object-\nrelative clauses). This result thus complements\na series of studies comparing modern language\nmodels and humans. For example, a recent study\nshowed that transformers trained on child-directed\ndata can achieve comparable accuracy on linguis-\ntic probes to large pre-trained models (Huebner\net al., 2021). Similarly, several studies have re-\ncently shown that the representations of GPT-2\nbecome increasingly similar to those of the adult\nhuman brain during its training (Caucheteux and\nKing, 2022). Finally, Lavechin et al. (2022) showed\nthat models trained on audio in a self-supervised\nfashion learn phoneme and lexical abilities in a\nsimilar trajectory to children.\n12210\n18 22 22 23 23 23 24 24 25 26 29 30 32 35 35 36 37 38 39 41 48 53 58 59 62 64 68 70\nAge (months)\nSv Simple\nWh Adjunct Excl Why\nRelative Clause\nLinguistic T est\nLinguistic Stages in Children\nThe boy built a tower\nWhat sound does a cow make?\nI will take another shape that I want\nObserved Examples\n0 600 1200 1800 2400 3000 3600 4200 4800 5400 6000 6600 7200 7800 8400 9000 9600 10200 10800 11400 12000 12600 13200 13800 14400 15000 15600\nTraining Step\nSubject Verb Agreement\nAgreement Subject Verb-In Simple Question\nShort Nested Outer \nLinguistic T est\nLinguistic Stages in Artificial Neural Networks\nThis goose isn′t/weren′t bothering Edward\nWhat color was the piece/pieces?\nThe actor that the boy attracts blocks/block\nCorrect/Incorrect Examples\nT est 1 T est 2 T est 3\nFigure 4: Comparing language model performance on linguistic probes to children’s performance. Example\nsentences observed in children were originally in Hebrew (Friedmann et al., 2021). Non-white indicates the\nphenomena is learned by the agent. The threshold for considering a probe learned by the model is performance\nabove 55%.\n0 2000 4000 6000 8000 10000120001400016000\nTraining Step\n50\n60\n70\n80\n90Accuracy (%)\nChance\nSubject Verb Agreement\nSimple \nNounpp \nAgreement Subject Verb-In \nSimple Question\nShort Nested Outer \nA\n0 2000 4000 6000 8000 10000120001400016000\nTraining Step\n20\n40\n60\n80Accuracy (%)\nChance\nSubject Verb Agreement on Incongruent Cases\nSimple  S\nNounpp  SP\nShort Nested Outer  SP B\n0 2500 5000 7500 10000 12500 15000\nTraining Step\n20\n40\n60\n80Accuracy (%)\nSubject Verb Agreement: Simple\n P  S\n0 2500 5000 7500 10000 12500 15000\nTraining Step\n20\n40\n60\n80\nSubject Verb Agreement: NounPP\n SS\n SP\n PS\n PP\n0 2500 5000 7500 10000 12500 15000\nTraining Step\n20\n40\n60\n80\nSubject Verb Agreement: Short Nested Outer\n SS\n SP\n PS\n PP\nC\nFigure 5: Subject verb agreement. Shading is standard error of the mean across seeds. S: Singular. P: Plural. A.\nParallel learning is observed in the three stages defined by Friedmann et al. (2021), when results are averaged across\ncongruent and incongruent cases. B. Subject verb agreement on incongruent cases which indicate whether the model\nunderstands syntax. The networks do not learn some syntax structures as the incongruent case of Short Nested\nOuter does not reach above chance level. C. Development trajectories of the bias towards plural number in English.\n4.2 A work in progress\nIt is important to stress that significant work re-\nmains to be done before drawing any definitive con-\nclusions about the similarities between language\nacquisition in humans and algorithms.\nFirst, we only consider a single architecture\n(GPT-2, Radford et al. (2019)) with a unique tex-\ntual corpus (WikiText103). Testing whether our\nresults hold true independently of the model and\ntraining corpus remains an important milestone for\nfuture research.\nSecond, linguistic abilities are not tested with the\nsame protocols in children and in the models: the\nmodels are explicitly tested on next word predic-\ntion, with a two-alternative forced-choice metric,\nwhereas children were implicitly evaluated on their\nability to spontaneously use specific syntactic struc-\n12211\ntures during natural speech.\nThird, there were only three linguistic features\nthat were directly comparable between the model\nprobes and the data in children, and all were syntac-\ntic. This leaves a significant margin of progress to\nmodulate our conclusion, and investigate whether\nthe lexicon, narrative structures, pragmatics and\nworld-knowledge are acquired in the same order in\nhumans and algorithms.\nFourth, the order in which some linguistic skills\nwere learned by GPT-2 does not trivially fit with\nlinguistic theory. For example, the probe “Sim-\nple”, which examines subject-verb agreement in a\nsimple sentence, was one of the last probes to be\nlearned by GPT-2 (it is part of group three in Figure\n2). By contrast, \"Wh Questions Subject Gap Long\nDistance\" was among the first probes to be to be\nlearned, even though it would be expected to be\nmuch harder than “Simple”. This unexpected result\nmay be due to the way we approximate \"Acquisi-\ntion Time\", namely, the moment when the probes\nreaches 90% of the final accuracy. Consequently,\nprobes with very low final accuracy could end up\nwith a shorter Acquisition Time, because noise\nmay lead to crossing the 90% threshold relatively\nquickly.\nFinally, we show that our models appear to use\nheuristics rather than a deep understanding of syn-\ntax for the most difficult linguistic probes (incon-\ngruent numbers between verbs and their attrac-\ntors) and were biased towards plural English verbs.\nWhile our models learn only 66% of tasks to above\nchance level, a larger GPT-2 pretrained on consid-\nerably more texts successfully perform on 97% of\nthe tasks, and has an accuracy above 75% on the\nincongruent examples, meaning this bias and re-\nliance on heuristics could potentially be solved by\ntraining on a larger dataset.\nIn sum, additional work remains necessary to\nidentify the exact elements of convergence and di-\nvergence between the acquisition of language in\nmodels and in children.\n4.3 Fueling the debate between nativism\nversus empiricism\nThe present study fuels a long-lasting debate on\nthe acquisition of language. While “empiricists“\nargue that language can be acquired with a statis-\ntical approach (Clark, 2002; Kolodny et al., 2015;\nChater and Christiansen, 2018; McCauley and\nChristiansen, 2019), “nativists“ maintain that this\nability depends on a core and innate operation, spe-\ncific to humans (Chomsky, 1959, 1971).\nThe present study shows how modern language\nmodels may contribute to resolving this debate, by\nsystematically studying which components of a\nmodel (e.g. architecture) or properties of the train-\ning data (e.g., frequency of sentence structures)\ncontribute to shape the trajectory of language ac-\nquisition. Claims about an innate Universal Gram-\nmar could be understood as an inductive bias of a\nlanguage model, implemented in its architecture\nand dynamics, which tightly constrains learning\ntrajectories across models. If this bias is hierar-\nchical (rather than linear) then this could lead to\nlearning trajectories that follow the structure of the\nsyntactic tree, consistently with the hypothesis of\nthree linguistic stages presented by Friedmann et al.\n(2021) in humans and what we find in this study\nin language models. Indeed, neural language mod-\nels have been previously shown to have a weak\ninductive bias towards hierarchical processing (Mc-\nCoy et al., 2020; Kharitonov and Chaabouni, 2020),\nwhich can partially explain our results.\nThis result echos the recent observation that syn-\ntactic trees spontaneously emerge in the middle\nlayers of neural language models (Hewitt and Man-\nning, 2019). Together, these elements thus suggest\nthat modern neural networks provide fruitful mod-\nels of language acquisition and could reconcile or\nsettle the confronting theories of language acquisi-\ntion (Warstadt and Bowman, 2022).\n4.4 Conclusion\nOverall, the similarities identified between children\nand GPT-2 suggest that there may be a small set of\nmeans by which to efficiently acquire language.\nThis result is anything but trivial: humans and\ndeep neural networks have extraordinarily differ-\nent architectures, training, and language exposure.\nIf generalized, this systematic learning trajectory\nwould support the existence of an intrinsic hierar-\nchy of linguistic structures that both machines and\nhumans must climb, be that through inductive bi-\nases or properties of the training data, to master the\nfaculty of language. And while these hypotheses\nremain open, the path to resolve them has never\nbeen clearer.\n12212\nAcknowledgements\nWe would like to thank Dieuwke Hupkes, Naama\nFriedmann, Marco Baroni and the attendees of the\nEviL meetings for their comments and suggestions.\nThis project has received funding from the Eu-\nropean Union’s Horizon 2020 research and innova-\ntion program under the Marie Skłodowska-Curie\ngrant agreement No 945304, for L.E for her work\nat PSL. This work was funded in part by FrontCog\ngrant ANR-17-EURE-0017 for the work of L.E.\nand J.R.K. for their work at PSL.\nReferences\nElika Bergelson and Daniel Swingley. 2012. At\n6–9 months, human infants know the meanings of\nmany common nouns. Proceedings of the National\nAcademy of Sciences, 109:3253–3258.\nSamuel R. Bowman and George Dahl. 2021. What\nwill it take to fix benchmarking in natural language\nunderstanding? Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4843–4855.\nCharlotte Caucheteux, Alexandre Gramfort, and Jean-\nRémi King. 2023. Evidence of a predictive coding\nhierarchy in the human brain listening to speech. Na-\nture Human Behaviour, pages 1–12.\nCharlotte Caucheteux and Jean Rémi King. 2022.\nBrains and algorithms partially converge in natural\nlanguage processing. Communications Biology, 5(1).\nTyler A. Chang and Benjamin K. Bergen. 2022. Word\nacquisition in neural language models. Transactions\nof the Association for Computational Linguistics,\n10:1–16.\nNick Chater and Morten H Christiansen. 2018. Lan-\nguage acquisition as skill learning. Current opinion\nin behavioral sciences, 21:205–208.\nNoam Chomsky. 1959. Review of verbal behavior.\n35(1):26–58. Publisher: Linguistic Society of Amer-\nica.\nNoam Chomsky. 1971. Problems of Knowledge and\nFreedom. New York,: W.W. Norton.\nAlexander Clark. 2002. Unsupervised language ac-\nquisition: Theory and practice. arXiv preprint\ncs/0212024.\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nKristina Toutanova Google, and AI Language. 2019.\nBert: Pre-training of deep bidirectional transformers\nfor language understanding. Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186.\nEmmanuel Dupoux. 2018. Cognitive science in the\nera of artificial intelligence: A roadmap for reverse-\nengineering the infant language-learner. Cognition,\n173:43–59.\nNaama Friedmann, Adriana Belletti, and Luigi Rizzi.\n2021. Growing trees: The acquisition of the left\nperiphery. Glossa: a journal of general linguistics,\n39(1).\nNaama Friedmann and Julia Reznick. 2021. Stages\nrather than ages in the acquisition of movement struc-\ntures: Data from sentence repetition and 27696 spon-\ntaneous clauses. Glossa: a journal of general lin-\nguistics, 39(1).\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless green\nrecurrent networks dream hierarchically. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1195–1205, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nJohn Hewitt and Christopher D Manning. 2019. A struc-\ntural probe for finding syntax in word representations.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4129–4138.\nPhilip Huebner. 2022. Unmasked. https://\ngithub.com/phueb/UnMasked. (Accessed\n2023/05/24).\nPhilip A Huebner, Elior Sulem, Cynthia Fisher, and Dan\nRoth. 2021. BabyBERTa : Learning More Grammar\nWith Small-Scale Child-Directed Language. Pro-\nceedings of the 25th Conference on Computational\nNatural Language Learning, pages 624–646.\nShailee Jain and Alexander Huth. 2018. Incorporating\ncontext into language encoding models for fmri. In\nAdvances in Neural Information Processing Systems,\nvolume 31. Curran Associates, Inc.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does bert learn about the structure of\nlanguage? Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657.\nJaap Jumelet, Willem Zuidema, and Dieuwke Hupkes.\n2019. Analysing neural language models: Contex-\ntual decomposition reveals default reasoning in num-\nber and gender assignment. Proceedings of the 23rd\nConference on Computational Natural Language\nLearning (CoNLL), pages 1–11.\nFred Karlsson. 2007. Constraints on multiple center-\nembedding of clauses. Journal of Linguistics,\n43(2):365–392.\n12213\nEugene Kharitonov and Rahma Chaabouni. 2020. What\nthey do when in doubt: a study of inductive biases in\nseq2seq learners. arXiv:2006.14953.\nOren Kolodny, Arnon Lotem, and Shimon Edelman.\n2015. Learning a generative probabilistic grammar\nof experience: A process-level model of language\nacquisition. Cognitive Science, 39(2):227–267.\nPatricia K. Kuhl. 2004. Early language acquisition:\ncracking the speech code. Nature Reviews Neuro-\nscience, 5:831–843.\nPatricia K Kuhl, Erica Stevens, Akiko Hayashi,\nToshisada Deguchi, Shigeru Kiritani, and Paul Iver-\nson. 2006. Fast-track report infants show a facilita-\ntion effect for native language phonetic perception\nbetween 6 and 12 months. Developmental Science,\n9:13–21.\nPatricia K. Kuhl, Karen A. Williams, Francisco Lac-\nerda, Kenneth N. Stevens, and Bjorn Lindblom. 1992.\nLinguistic experience alters phonetic perception in\ninfants by 6 months of age. Science, 255:606–608.\nYair Lakretz, Théo Desbordes, Dieuwke Hupkes, and\nStanislas Dehaene. 2021a. Causal transformers per-\nform below chance on recursive nested constructions,\nunlike humans. arXiv:2110.07240.\nYair Lakretz, Dieuwke Hupkes, Alessandra Vergallito,\nMarco Marelli, Marco Baroni, and Stanislas Dehaene.\n2021b. Mechanisms for handling nested dependen-\ncies in neural-network language models and humans.\nCognition, 213:104699. Special Issue in Honour of\nJacques Mehler, Cognition’s founding editor.\nYair Lakretz, German Kruszewski, Theo Desbordes,\nDieuwke Hupkes, Stanislas Dehaene, and Marco Ba-\nroni. 2019. The emergence of number and syntax\nunits in LSTM language models. Association for\nComputational Linguistics, pages 11–20.\nMarvin Lavechin, Maureen De Seyssel, Hadrien Ti-\nteux, Hervé Bredin, Guillaume Wisniewski, Alejan-\ndrina Cristia, and Emmanuel Dupoux. 2022. Statis-\ntical learning bootstraps early language acquisition.\nPsyArXiv.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the Ability of LSTMs to Learn\nSyntax-Sensitive Dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nChristopher D. Manning, Kevin Clark, John Hewitt,\nUrvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artificial neural networks\ntrained by self-supervision. Proceedings of the Na-\ntional Academy of Sciences of the United States of\nAmerica, 117:30046–30054.\nReiko Mazuka, Yvonne Cao, Emmanuel Dupoux, and\nAnne Christophe. 2011. The development of a\nphonological illusion: A cross-linguistic study with\njapanese and french infants. Developmental Science,\n14:693–699.\nStewart M McCauley and Morten H Christiansen.\n2019. Language learning as language use: A cross-\nlinguistic model of child language development. Psy-\nchological review, 126(1):1.\nR Thomas McCoy, Robert Frank, and Tal Linzen. 2020.\nDoes syntax need to grow on trees? sources of hier-\narchical inductive bias in sequence-to-sequence net-\nworks. Transactions of the Association for Computa-\ntional Linguistics, 8:125–140.\nJacques Mehler, Peter Jusczyk, Ghislaine Lambertz,\nNilofar Halsted, Josiane Bertoncini, and Claudine\nAmiel-Tison. 1988. A precursor of language acquisi-\ntion in young infants. Cognition, 29:143–178.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv:1609.07843.\nThierry Nazzi, Peter W. Jusczyk, and Elizabeth K. John-\nson. 2000. Language discrimination by english-\nlearning 5-month-olds: Effects of rhythm and famil-\niarity. Journal of Memory and Language, 43:1–19.\nAlexandre Pasquiou, Yair Lakretz, John Hale, Bertrand\nThirion, and Christophe Pallier. 2022. Neural lan-\nguage models are not born equal to fit brain data,\nbut training helps. In ICML 2022-39th International\nConference on Machine Learning, page 18.\nAlexandre Pasquiou, Yair Lakretz, Bertrand Thirion,\nand Christophe Pallier. 2023. Information-restricted\nneural language models reveal different brain re-\ngions’ sensitivity to semantics, syntax and context.\narXiv:2302.14389.\nYada Pruksachatkun, Phil Yeres, Haokun Liu, Jason\nPhang, Phu Mon Htut, Alex Wang, Ian Tenney, and\nSamuel R. Bowman. 2020. jiant: A software toolkit\nfor research on general-purpose text understanding\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 109–117, Online. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Se-\nmantic Scholar. (Accessed 2023-05-04).\nRushen Shi, Janet F Werker, and James L Morgan. 1999.\nNewborn infants’ sensitivity to perceptual cues to\nlexical and grammatical words. Cognition, 72:B11–\nB21.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\nGarriga-Alonso, et al. 2022. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models. arXiv 2206.04615.\nRuth Tincoff and Peter W. Jusczyk. 1999. Some be-\nginnings of word comprehension in 6-month-olds.\nPsychological Science, 10:172–175.\n12214\nMariya Toneva and Leila Wehbe. 2019. Interpreting and\nimproving natural-language processing (in machines)\nwith natural language-processing (in the brain). In\nAdvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nAlex Warstadt and Samuel R Bowman. 2022. What\nartificial neural networks can tell us about human\nlanguage acquisition. arXiv:2208.07998.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng Fu Wang, and Samuel R.\nBowman. 2020. Erratum: “blimp: The benchmark of\nlinguistic minimal pairs for english”. Transactions of\nthe Association for Computational Linguistics, 8:867–\n868.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nLucas Weber, Jaap Jumelet, Elia Bruni, and Dieuwke\nHupkes. 2021. Language modelling as a multi-task\nproblem. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 2049–2060,\nOnline. Association for Computational Linguistics.\nJanet F. Werker. 2018. Perceptual beginnings to\nlanguage acquisition. Applied Psycholinguistics,\n39:703–728.\nJanet F. Werker and Richard C. Tees. 1984. Cross-\nlanguage speech perception: Evidence for perceptual\nreorganization during the first year of life. Infant\nBehavior and Development, 7:49–63.\nTania S. Zamuner. 2006. Sensitivity to word-final\nphonotactics in 9- to 16-month-old infants. Infancy,\n10:77–95.\nAppendix\nA Tests in Children\nDetailed description of tests available in children,\nin the three linguistic stages defined by (Friedmann\net al., 2021):\n• Stage 1: Subject-Verb Simple, Subject-Verb\nUnaccusative, Verb-Subject Unaccusative\n• Stage 2: Root WH-Argument, WH-Adjunct\nExcluding Why, Preposed Adverb, Root y/n\n• Stage 3: Why, Relative Clause, Topicalisation,\nEmbedding\nThe probes chosen for comparison (stated in Ta-\nble 1), were the only probes that matched well with\none of the test available in children. In addition\nNounpp was examined in the models, as it fits into\nthe linguistic stage 2, and, as it is part of the BIG-\nBench probes, could be separated into congruent\nand incongruent sentences.\nB Model Training\nTo evaluate linguistic abilities of a high-\nperformance language model, we first use the\nHuggingFace pretrained GPT-2 large which has\n774M parameters and is trained on 40GB of data.\nThis model has one-shot perplexity of 22 on Wiki-\nText103 (Radford et al., 2019).\nThen, to evaluate how linguistic abilities vary\nwith language acquisition, we separately trained 48\nmodels (each with a distinct random seed which\nset the model’s initial parameters and the seed of\nthe dataloader) using the 12-layer GPT-2 architec-\nture (Radford et al., 2019) provided by Hugging-\nFace4 on WikiText103 (Merity et al., 2016) with\na learning rate of 10−5 and a batch size of 16 dis-\ntributed over 8 GPUs, making a total batch size\nof 64 and a context size of 128. Training was\nstopped when then validation loss plateaued, reach-\ning final perplexity of 28 after 10 epochs. This is\nlower perplexity than the one-shot performance of\nthe HuggingFace pretrained 12-layer GPT-2 which\nwas 37.5, which is logical as our model was trained\nspecifically on this dataset.\nIn all cases we used the pretrained tokenizer\nwhich has vocabulary size of 50,257. All other\nparameters were the default training arguments for\nthe transformer provided by HuggingFace. The\nHuggingFace architectures are publicly available\nunder an MIT license, and WikiText103 is available\nunder Creative Commons Attribution-ShareAlike\nLicense.\nC Linguistic Probe Benchmarks\nWe use three different zero-shot benchmarks.\nThe first benchmark, ‘BLiMP’ (The Benchmark\nof Linguistic Minimal Pairs for English) (Warstadt\net al., 2020) contains 67 different probes, each in\nthe form of 1,000 pairs of grammatical and un-\ngrammatical sentences designed to isolate a spe-\ncific linguistic phenomenon. Adult human perfor-\nmance on BLiMP is 96.4% (Warstadt et al., 2020).\nThe second benchmark, ‘Zorro’ 5, was developed\nwith a vocabulary frequent in child-directed cor-\npora. Zorro contains 13 probes, each consisting of\n2,000 pairs of sentences. Finally, the third bench-\nmark is the Subject-Verb Agreement Task of BIG-\n4https://huggingface.co/gpt2\n5https://github.com/phueb/Zorro\n12215\nBench (Srivastava et al., 2022; Lakretz et al., 2019,\n2021b; Linzen et al., 2016; Gulordava et al., 2018).\nWe focus on the syntactic probes, namely: \"Sim-\nple English\" which contains 600 pairs, \"NounPP\"\nwhich contains 2,400 pairs, and \"Short Nested In-\nner\", \"Short Nested Outer\", \"Long Nested Inner\"\nand \"Long Nested Outer\" which each contain 4,096\npairs of grammatical and ungrammatical sentences.\nAccuracy on a linguistic probe is evaluated with\nthe Jiant (Pruksachatkun et al., 2020) and Un-\nMasked method (Huebner, 2022). In practice, sen-\ntences are input to the model in batches of 300,\nwith padding on the left to make all sentences the\nlength of the longest sentence in the batch. The\nlogit values of punctuation are discarded when esti-\nmating the probability of a sentence.\nZorro, Jiant and UnMasked are publicly avail-\nable under the MIT License, BLiMP under a CC\nBY License, and BIG-Bench under the Apache\nLicense 2.0.\n12216\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n12217\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n12218",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.769534707069397
    },
    {
      "name": "Language acquisition",
      "score": 0.7060408592224121
    },
    {
      "name": "Lexicon",
      "score": 0.6697883009910583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.600568413734436
    },
    {
      "name": "Natural language processing",
      "score": 0.5658957958221436
    },
    {
      "name": "Categorization",
      "score": 0.5485674738883972
    },
    {
      "name": "Natural language",
      "score": 0.5160476565361023
    },
    {
      "name": "Language identification",
      "score": 0.4305616319179535
    },
    {
      "name": "Language model",
      "score": 0.4258174002170563
    },
    {
      "name": "Process (computing)",
      "score": 0.4109494984149933
    },
    {
      "name": "Psychology",
      "score": 0.23395153880119324
    },
    {
      "name": "Mathematics education",
      "score": 0.11980339884757996
    },
    {
      "name": "Programming language",
      "score": 0.1084718406200409
    }
  ]
}