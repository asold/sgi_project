{
    "title": "R-Trans: RNN Transformer Network for Chinese Machine Reading Comprehension",
    "url": "https://openalex.org/W2917001081",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2096897347",
            "name": "Shan-Shan Liu",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2097597093",
            "name": "Sheng Zhang",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2002306827",
            "name": "Xin Zhang",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A63849503",
            "name": "Hui Wang",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2096897347",
            "name": "Shan-Shan Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097597093",
            "name": "Sheng Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2002306827",
            "name": "Xin Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A63849503",
            "name": "Hui Wang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6623987585",
        "https://openalex.org/W2752201871",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W6632455782",
        "https://openalex.org/W2963963993",
        "https://openalex.org/W6754314248",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W6682691769",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2740747242",
        "https://openalex.org/W6678890848",
        "https://openalex.org/W2521709538",
        "https://openalex.org/W2890170470",
        "https://openalex.org/W2552027021",
        "https://openalex.org/W2889048825",
        "https://openalex.org/W2551396370",
        "https://openalex.org/W2557764419",
        "https://openalex.org/W2890820068",
        "https://openalex.org/W2963957489",
        "https://openalex.org/W2566011400",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963769536",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2516930406",
        "https://openalex.org/W4295253143",
        "https://openalex.org/W2794325560",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2605058246",
        "https://openalex.org/W2125436846",
        "https://openalex.org/W2963863909",
        "https://openalex.org/W3103111734",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2126209950",
        "https://openalex.org/W2951534261",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W1660390307",
        "https://openalex.org/W2963625095",
        "https://openalex.org/W2962883855",
        "https://openalex.org/W2888302696",
        "https://openalex.org/W2962808855",
        "https://openalex.org/W2600024417"
    ],
    "abstract": "Machine reading comprehension (MRC) has gained increasingly wide attention over the past few years. A variety of benchmark datasets have been released, which triggers the development of quite a few MRC approaches based on deep learning techniques. However, most existing models are designed to address English MRC. When applying them directly to Chinese documents, the performance often degrades considerably because of some special characteristics of Chinese, the inevitable segmentation errors in particular. In this paper, we present the RNN Transformer network to tackle the Chinese MRC task. To mitigate the influence of incorrect word segmentation and mine sequential information of whole sentences, deep contextualized word representations and bidirectional gated recurrent units networks are adopted in our model. The extensive experiments have been conducted on a very large scale Chinese MRC corpus, viz., the Les MMRC dataset. The results show that the proposed model outperforms the baseline and other prevalent MRC models notably, and established a new state-of-the-art record on the Les MMRC dataset.",
    "full_text": "Received January 24, 2019, accepted February 20, 2019, date of publication February 25, 2019, date of current version March 13, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2901547\nR-Trans: RNN Transformer Network for Chinese\nMachine Reading Comprehension\nSHANSHAN LIU\n , SHENG ZHANG\n , XIN ZHANG, AND HUI WANG\nScience and Technology on Information Systems Engineering Laboratory, College of Systems Engineering, National University of Defense Technology,\nChangsha 410073, China\nCorresponding author: Xin Zhang (ijunzhanggm@gmail.com)\nABSTRACT Machine reading comprehension (MRC) has gained increasingly wide attention over the\npast few years. A variety of benchmark datasets have been released, which triggers the development of\nquite a few MRC approaches based on deep learning techniques. However, most existing models are\ndesigned to address English MRC. When applying them directly to Chinese documents, the performance\noften degrades considerably because of some special characteristics of Chinese, the inevitable segmentation\nerrors in particular. In this paper, we present the RNN Transformer network to tackle the Chinese MRC\ntask. To mitigate the inﬂuence of incorrect word segmentation and mine sequential information of whole\nsentences, deep contextualized word representations and bidirectional gated recurrent units networks are\nadopted in our model. The extensive experiments have been conducted on a very large scale Chinese MRC\ncorpus, viz., the Les MMRC dataset. The results show that the proposed model outperforms the baseline\nand other prevalent MRC models notably, and established a new state-of-the-art record on the Les MMRC\ndataset.\nINDEX TERMS Contextualized word representation, deep learning, machine reading comprehension.\nI. INTRODUCTION\nMachine reading comprehension (MRC), which empowers\nmachine with the ability to answer questions based on the\nprovided passage, has become an active area of research in\nnatural language processing (NLP) in the past few years.\nVarious deep neural models, e.g., Bi-DAF [1], R-Net [2],\nDCN [3] and Mnemonic Reader [4], have been proposed for\nEnglish MRC and achieved impressing results.\nCompared to English, there are few successful stories\nregarding Chinese MRC. We argue that this is mainly due to\nsome special characteristics of Chinese. In particular, since\nthere is no delimiter between Chinese words, word segmen-\ntation is often an indispensable step for further processing in\nChinese NLP, which is itself a very challenging task. Usually,\nsegmentation errors are unavoidable and would propagate\nto downstream tasks. For better understanding of this side-\neffect on Chinese MRC, Table 1 gives two such examples,\nin which the segmentation results are generated using the\nJieba toolkit (https://pypi.org/project/jieba/). In Example 1,\nthe character sequence ‘‘\n’’ is expected to be treated as\na single word referring to Twitter, but Jieba segments it into\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Ashish Mahajan.\n‘‘\n ’’ (push) and ‘‘\n ’’ (special), which have no relation with\nTwitter and would make it much more difﬁcult for accurate\nunderstanding of the input question. In Example 2, the input\nquestion is indeed to ask people’s evaluation of Brooks, and\nhence the character sequence ‘‘\n’’ is supposed to be\ndivided as ‘‘\n (is)\n (what).’’ However, Jieba identiﬁes\nit as a single word meaning why in Chinese, which leads\nit nearly impossible to comprehend the true intent of the\nquestion.\nThe ambiguities caused by incorrect segmentations can\nbe resolved using local and global context. In Example 1,\nthe former and later word of ‘‘\n ’’(Twitter) are\n‘‘\n ’’(Facebook) and ‘‘\n ’’(Wechat), respectively,\nwhich constitute the context and both refer to social media\nplatforms. If machine can mine this contextual information,\nit is easy to infer the meaning of ‘‘\n ’’ even though there\nis an error made by segmentation toolkits. In Example 2,\nthe meaning of whole sentence plays an important role\nto comprehend the true intent of the question. However,\nword2vec [5] or GloVe [6] which is commonly utilized\nin existing English MRC models is incapable of produc-\ning different vectors for one word according to context.\nTherefore, to mitigate the inﬂuence of incorrect segmen-\ntations to downstream tasks especially in Chinese MRC,\n27736\n2169-3536 \n 2019 IEEE. Translations and content mining are permitted for academic research only.\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nVOLUME 7, 2019\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\nmore powerful representation learning approaches are in\ndemand.\nIn addition, existing MRC models, e.g., QANet [7], suffer\nfrom two limitations. Firstly, Convolution Neural Network\n(CNN) is an essential component in QANet, but compared to\nRecurrent Neural Network (RNN) which is good at extracting\nglobal information, CNN can only mine local information.\nSecondly, these models often integrate many layers into their\nnetworks, which leads them to contain a large number of\nparameters, and makes the training and testing procedures\ncost very high computational resources. Thus, it is hard\nto operate them in some resource-constrained settings like\nmobile or embedded devices.\nIn this article, we propose RNN Transformer network\n(R-Trans) for Chinese MRC task which deals with the above\nproblems in the following three aspects. At ﬁrst, as exam-\nples in Table 1 illustrate, taking contextual information into\naccount can relieve the effect of incorrect segmentation, but\nthe distributed representations cannot change according to\ncontext. Hence deep contextualized word embeddings are\nadded to the embedding layer to extract contextual informa-\ntion more effectively and contribute to the answer prediction.\nSecondly, to address the problem that CNN is not sufﬁcient\nto extract global information, inspired by QANet, we retain\nthe Transformer introduced by Vaswani et al. [8] and add\nbidirectional Gated Recurrent Units (GRU) networks to our\nR-Trans model. Thirdly, some network pruning is performed\nto cut down redundant parameters and accelerate training and\ninference.\nTABLE 1. Two examples of incorrect segmentations.\nOverall, the main contributions of this article are as\nfollows:\n• We incorporate deep contextualized word representation\ninto the embedding layer and our case study demon-\nstrates that it can mitigate the inﬂuence of errors made\nby segmentation toolkits.\n• We add a bidirectional GRU network on top of the\nembedding layer to better extract global information.\nFurthermore, we prune the network and reduce the\namount of parameters. Our R-Trans model, compared\nwith QANet, not only improve the accuracy of the\nanswer but also speed up training and inference.\n• We conduct various experiments on a large scale\nChinese MRC dataset, Les MMRC and experimental\nevaluations show that our R-Trans model achieves state-\nof-the-art results.\nII. RELATED WORK\nIn the following, we give a brief introduction to the research\nclosely related to our work, i.e., machine reading comprehen-\nsion and word representation.\nA. MACHINE READING COMPREHENSION\nMachine reading comprehension (MRC), which involves a\nvariety of complex techniques such as semantic understand-\ning and knowledge reasoning, has become an important task\nof natural language processing and is far more challenging.\nBenchmark datasets have stimulated recent progress in\nMRC. They can be roughly divided into four categories: mul-\ntiple choice (McTest [9], ARC [10], RACE [11]), cloze-style\n(CNN/Daily News [12], CBT [13], CLOTH [14]), extractive\n(SQuAD [15], NewsQA [16], QuAC [17]) and abstractive\n(MS MARCO [18], NarrativeQA [19]).\nEnd-to-end Neural Networks have featured predominantly\nin MRC and several deep learning models have been intro-\nduced to address this problem. Wang and Jiang [20] combine\nmatch-LSTM and Pointer Net to predict the boundary of the\nanswer. Wang et al. [21] design a multi-perspective context\nmatching model to identify the answer span by matching the\npassage with the question from multiple perspectives. The\nattention mechanism, which emphasizes important parts of\nthe context related to the question, is widely used in MRC\nmodels. Seo et al. [1] apply the bidirectional attention ﬂow\nmechanism to obtain the interactive information between\nquestions and passages. Xiong et al. [3] propose a dynamic\ncoattention network to iteratively estimate the answer span.\nWang et al. [2] add gated attention-based recurrent networks\nto stress different importance of words in the passage to\nanswer the particular question. To speed up training and infer-\nence, Yu et al. [7] substitute recurrent neural networks with\nconvolution and self-attention. Different from models men-\ntioned above, others are multi-pass reasoners which revisit the\nquestion and the passage more than one time. Shen et al. [22]\nintroduce a termination state in the inference with the use\nof reinforcement learning, while Liu et al. [23] apply the\nstochastic dropout to the answer module which contributes to\nmulti-step reasoning. Moreover, thinking that the extractive\nmodel is not sufﬁcient, Tan et al. [24] develop an answer\ngeneration model to elaborate the ﬁnal answers with the sub-\nspans predicted by the extraction model.\nMethods using deep learning techniques have presented\ntheir advantage when addressing English MRC task. In con-\ntrast, there exists relatively little work on Chinese MRC due\nto the lack of high-quality Chinese dataset. The releasing of\nVOLUME 7, 2019 27737\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\nBaidu company’s large-scale Chinese dataset, DuReader [4],\nsparks research interest into Chinese MRC. But due to fea-\ntures of DuReader, researchers pay much attention to address-\ning the problem of multi-passage MRC [25]–[27], which\nselects the correct answer from a set of candidate passages for\na question. In comparison, we focus on special characteristics\nof Chinese and propose more powerful model to tackle single-\npassage Chinese MRC task.\nB. WORD REPRESENTATION\nHow to represent words to enable machine get their meanings\nis a basic task in natural language processing. Traditional\nmethods use one-hot representation [28], a vector with all\nzeroes except in one position corresponding to the word,\nto encode individual words. However, such representations\nare sparse and high-dimensional when the vocabulary size\nis large. In addition, these methods cannot mine the relation\nbetween words.\nTo address the shortcomings, distributed word rep-\nresentation called word embeddings is proposed by\nRumelhart et al. [29]. Various techniques to generate this\nrepresentation have been introduced, among which the most\npopular ones are word2vec and GloVe. Mikolov et al. [5] pro-\npose two methods to represent words, CBOW and skip-gram,\nwhich are capable to represent words with continuous vectors\nin low dimension. GloVe put forward by Pennington et al. [6]\ncombine co-occurrence matrix and local context window to\nefﬁciently obtain vector representations for words. Vectors\nproduced by these methods have been widely applied in\ndownstream NLP tasks, such as machine translation [30],\nname entity recognition [31], sentiment analysis [32] and\ndialogue systems [33].\nWord representation also plays an important role\nin machine reading comprehension. As research of\nDhingra et al. [34] shows, the minor choices made in word\nrepresentation can lead to substantial differences in the ﬁnal\nperformance of the reader. The most common method to\nrepresent words in MRC models is to utilize both word-\nlevel and character-level embeddings, which maps each\nword to a vector space using pre-trained word embedding\nmodel [1], [2], [21], [35], [36]. However, this method seems\nnot sufﬁcient as it just simply concatenate word-level and\ncharacter-level embeddings and vectors produced for one\nword cannot change according to context. To address these\nproblems, Peters et al. [37] introduce deep contextualized\nword representations called ELMo which are pre-trained by\nlanguage model ﬁrst and ﬁne-tuned according to downstream\ntasks. Seeing the limitation of unidirectional language models\nused in ELMo, Devlin et al. [38] propose BERT, which\nutilize bidirectional Transformer to encode both left and right\ncontext to representations.\nAlthough word representations like ELMo and BERT show\npromising performances in various natural language pro-\ncessing tasks, they are scarcely applied to Chinese domain.\nBecause of their contextualized features, those representa-\ntions are thought to be helpful to mitigate the inﬂuence of\nincorrect word segmentation in Chinese documents. Consid-\nering that Chinese words have more abundant meanings than\ncharacters, we pre-train ELMo by ourselves based on Chinese\nwords instead of using BERT released by Google based on\nChinese characters in this article.\nIII. PROPOSED MODEL\nFig. 1 presents the framework of our R-Trans model.\nTo be speciﬁc, our model has ﬁve layers, which are Input\nEmbedding Layer, Embedding Encoder Layer, Context-\nQuery Attention Layer, Model Encoder Layer and Output\nLayer and we will give an elaborate illustration in the fol-\nlowing part.\nA. INPUT EMBEDDING LAYER\nCompared to common techniques which obtain representa-\ntion of each word by concatenating its word embedding and\ncharacter embedding, we add the results of ELMo encoder in\naddition.\nIn terms of the ELMo embeddings, we will give a more\ndetailed description. We pre-train a bidirectional Language\nModel (biLM) with corpus of news collected from a variety\nof Internet pages for easy transfer learning. Given a sentence\nof N words, (w1,w2,..., wN ), we ﬁrst compute a word repre-\nsentation xLM\nk independent of the context, which is the input\nof a multiple-layer bidirectional Long Short Term Memory\n(biLSTM) with residual connections. At each position k,\na forward LSTM layer generates a context-dependent repre-\nsentation − →h LM\nk,i where i =1,..., L, while a backward one\noutputs ← −h LM\nk,i in a similar way. Thus, each word wk has 2L +1\nrepresentations through a L-layer biLM.\nRk ={x LM\nk ,− →h LM\nk,i ,← −h LM\nk,i |i =1,..., L}\n={h LM\nk,i |i =1,..., L} (1)\nELMo projects outputs of all layers in the biLM into a\nsingle vector by a linear combination\ns =softmax(w)\nELMok =γ\nL∑\ni=0\nsihLM\nk,i (2)\nwhere γand w are parameters which are supposed to be tuned\naccording to the speciﬁc task during training. Thus, we get the\nELMo embedding of given word denoted as xe.\nThen, we pre-train the word into a 300-dimension vector\nwith GloVe [6] and then store the results in the embedding.\nNamely, we look up this table in Input Embedding Layer\nto get the word embedding for each word. As for the char-\nacter embeddings, Convolutional Neural Networks (CNN)\nare applied. Each character in the word is embedded into a\n64-dimension vector, which is fed to the CNN as 1D inputs.\nAfter max-pooling the entire width, the outputs are the results\nof character embeddings, a ﬁxed-size vector for the word.\nAt last, for the given word x, its word embedding xw,\ncharacter embedding xc and ELMo embedding xe are\n27738 VOLUME 7, 2019\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\nFIGURE 1. The framework of R-Trans model.\nconcatenated together and through a two-layer highway\nnetwork, the representation of word can be denoted as\n[xw;xc;xe].\nB. EMBEDDING ENCODER LAYER\nQANet removes all RNN in previous models with convolu-\ntion and self-attention for sake of speed-up, but convolution\noperations pay much attention to local information and can-\nnot deal with sequential text as well as RNN. Hence, we add\na bidirectional Gated Recurrent Units network (GRU) [30],\na variant of RNN, on top of the embeddings to encode tem-\nporal interactions between words and concatenate the outputs\nof two GRUs at each time step. The outputs of bidirectional\nGRUs can be denoted as follows:\n− →h t =GRU[xw;xc;xe]\n← −h t =GRU[xw;xc;xe]\nHt =[− →h t ;← −h t ] (3)\nwhere − →h t is the output of forward GRU and ← −h t is the output\nof backward one.\nThen the outputs of bidirectional GRUs are fed to the\nresidual block, a stack of the Encoder Block, to alleviate the\nimpact of gradient explosion and vanishing. Based on the\narchitecture of Transformer introduced by Vaswani et al. [8],\nthe Encoder Block adds convolution layer before self-\nattention layer and feed-forward layer. Rather than traditional\nconvolutions, the depthwise separable ones are utilized for\ntheir efﬁcient memory and better generalization. For self-\nattention layer, we change the number of heads to 1, which\nsimplify the multi-head attention mechanism to a scaled dot-\nproduct attention. The number of the Encoder Block in resid-\nual block is 1.\nC. CONTEXT-QUERY ATTENTION LAYER\nThis layer, which obtains interactive information between\ncontext and question by computing both context-to-query\nattention and query-to-context attention, is common in most\nexisting machine reading comprehension models.\nThe encoded context and question are denoted as C and Q\nrespectively. We apply the trilinear function to compute\nthe similarity between context and question. The similarity\nmatrix S ∈RI×J is computed by\nSij =α(Ci,Qj) (4)\nwhere αis the trilinear function, Ci is i-th context word and\nQj is j-th question word.\nAfter normalizing each row of S with softmax function,\nthe attended question vectors for the whole context can be\ncalculated as\nA =softmax(S)QT (5)\nTo signify which context words are critical to answer the\nquestion because of high similarity with question words,\nthe query-to-context attention is calculated as:\nB =softmax(ST )CT (6)\nVOLUME 7, 2019 27739\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\nhere we use softmax function to normalize each column of S.\nD. MODEL ENCODER LAYER\nCompared to original QANet model, we reduce the number\nof model encoder block to 2. Besides, for each model encoder\nblock, the number of the Encoder Block introduced above\ndecreases from 7 to 3. The input of this layer is denoted as\n[c,a,c⊙a,c⊙b], where ⊙is the element-wise multiplication,\na and b are a row of matrix A and B respectively. Different\nfrom the Encoder Block applied in the Embedding Encoder\nLayer, the number of convolution layer is 2 and the kernel\nsize is 5.\nE. OUTPUT LAYER\nThe goal of this layer is to predict the probability of each posi-\ntion in the given passage to be the begin or end of the answer.\nTo achieve this goal, we combine the outputs of two model\nencoder blocks in previous layer, denoted as M0 and M1\nrespectively, and the probabilities are computed as:\npb =softmax(W1[M0;M1])\npe =softmax(W2[M0;M1]) (7)\nwhere W1 and W2 are trainable parameters. Like the previous\nreading comprehension models, we minimize the sum of the\nnegative log probabilities of the true begin and end position\nby the predicted distributions:\nL(θ) =−1\nN\nN∑\ni\n[log(pb\ny1\ni\n) +log(pe\ny2\ni\n)] (8)\nwhere y1\ni and y2\ni are the ground-truth begin and end position\nof example i.\nIV. EXPERIMENTS\nIn this section, we introduce our experiments in detail.\nTo begin with, we give a description of the dataset and\nmetrics. Data pre-processing and experimental settings are\nillustrated next. In the end, we talk about the results.\nA. DATASET\nThe dataset used in this article is Les MMRC, a large scale\nChinese machine reading comprehension dataset, which is\nreleased by the 28th Research Institute of China Electronics\nTechnology Group Corporation. Les MMRC consists of more\nthan 50,000 news articles collected from a variety of Internet\npages. For each passage, about ﬁve questions are asked thus\nthe dataset contains 250,000 questions or so in total, including\n200,000 for the training set and 50,000 for the test set.\nDifferent from the test set, questions in the training set have\nground-truth answers responded by human. In this article,\nto evaluate different training strategies, we select 10% of data\nfrom the training set randomly as the development set.\nTypes of questions in Les MMRC are various, asking about\nopinion, fact, deﬁnition, number and so on. Most of that\ncan be answered by a span in original passage, just like\nSQuAD. However, it is more challenging that some answers\nto the questions need inference and conclusion, which cannot\nbe found in the passage directly. As the example presented\nin Fig. 2, to answer the question how many people died,\nthe machine needs to inference from the original passage\nsegmentation that ﬁve people on board and two people on\nthe ground died to give the right answer seven.\nFIGURE 2. An example of the question which cannot be answer by a span\nin the original passage.\nB. METRICS\nIn this article, two metrics, Rouge-L and BLEU-4, are used\nas performance evaluation indicators for the model.\nRouge (Recall-Oriented Understudy for Gisting\nEvaluation) is commonly used in various natural language\nprocessing tasks like machine translation and summarization.\nL in Rouge-L is the initial of LCS, short for the longest com-\nmon subsequence, which means Rouge-L applies the longest\ncommon subsequence to measure the similarity between text.\nRouge-L can be calculated as follows:\nRlcs =LCS(X,Y )\nm (9)\nPlcs =LCS(X,Y )\nn (10)\nFlcs =(1 +β)2RlcsPlcs\nRlcs +β2Plcs\n(11)\nwhere LCS(X ,Y ) is the length of the longest common subse-\nquence of X and Y , m and n are the length of reference text\nand candidate text respectively, Rlcs, Plcs represent recall and\nprecision while Flcs is the score of Rouge-L.\nBLEU (Bilingual Evaluation Understudy) is usually used\nto evaluate the quality of machine translation by measuring\nthe similarity between the sentences translated by machine\nand human. The more similar, the higher BLEU score. The\ncalculations are as follows:\nAt ﬁrst, we calculate the probability of the N-gram in\ncandidate sentence appears in reference sentence:\nPn =\n∑\nC∈{Candidate}\n∑\nN-gram∈C Countclip(N-gram)\n∑\nC′∈{Candidate}\n∑\nN-gram′∈C′Count(N-gram) (12)\nIf the length of candidate sentence is too short, the accuracy\nof BLEU score will decrease. To alleviate that, the penalty\n27740 VOLUME 7, 2019\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\nfactor BP is introduced, which can be calculated as below:\nBP =\n{\n1, c >r\ne1−r\nc , c ≤r (13)\nFinally, the BLEU is computed as:\nBLEU =BP ·exp(\nN∑\nn=1\nwn log pn) (14)\nwhen calculating BLEU-4, N equals 4 and wn is 1\n4 .\nC. BASELINES\nTo prove the superiority of our model, we compare it with\nother baselines, which are listed as below:\nQANet: QANet proposed by Yu et al. [7], replaces\nrecurrent neural networks, which are primarily used in\ncurrent machine reading comprehension models because\nof their sequential features, with convolution in the ﬁrst\nplace. Compared to prevalent recurrent models, it not\nonly achieves equivalent accuracy on SQuAD, but also\nis faster in training and inference. With data augmenta-\ntion technique, QANet reaches a competitive performance\non SQuAD.\nR-Net: This model [2] proposed by the Microsoft Research\nAsia, pays more attention to the interaction between questions\nand the passage by adding a gated attention-based recurrent\nnetworks. To aggregate evidence from the whole passage to\ninfer the answer, a self-matching mechanism is also intro-\nduced. R-Net achieves better results on the SQuAD and MS\nMARCO than former models.\nD. DATA PRE-PROCESSING\nData pre-processing plays a signiﬁcant role in the machine\nreading comprehension task. As the data in original dataset\nmay be noisy, invalid or duplicate, and the structure of that\nis deﬁned by the database initially, it is difﬁcult to train\nthe models directly using the data without pre-processing.\nAs a result, we have to do some data pre-processing before\ntraining.\n1) PASSAGE PRUNING\nFirstly, we do some common pre-processing, including\nreplacing Chinese punctuation and number with English\nones, deleting special characters which appear at the begin-\nning and end of the answers and removing invalid data whose\npassage, question or answer is vacant and reduplicative one\nwhose questions and answers for same passage are repeating.\nAfter doing that, there is still an apparent problem in\noriginal dataset that some passages are too long. According\nto the data analysis of Les MMRC, about half of passages\nhave more than 500 words, which will lead to the increase\nof parameters and in turn make it hard to train the model.\nTherefore, how to prune long passages to the limited length\non the condition that information loss is as small as possible\nis an important task in MRC pre-processing.\nIn this article, the strategies applied to prune long passages\nare as follows:\nAt ﬁrst, we deﬁne the upper limited length of passage as\nL taking features of dataset into consideration. If the length of\npassage is shorter than L, there is no need to prune, otherwise\ntruncation is divided into two cases according to whether\npassages are segmented or not:\n• If the passage is segmented, we apply the longest com-\nmon substring to measure the similarity between ques-\ntions and paragraphs. The more similar the question and\nparagraph are, the higher score the paragraph gets. The\nﬁrst k paragraphs with the highest score are selected in\ndescending order to be concatenated as the new passage\nif the length of that does not exceed the limit. As is\nknown that the ﬁrst sentence usually contains more\ninformation than others in the paragraph, we add the\nﬁrst sentence of each paragraph to the processed passage\nprovided that the length is within the upper limit.\n• If the passage is not segmented, like the strategy intro-\nduced above, we also use the longest common substring\nto calculate the similarity between the question and each\nsentence at ﬁrst, and then concatenate the most similar t\nsentences as the new passage without changing their\norder.\n2) ANSWER LABELLING\nThe model in this article is extractive, which assumes that the\nanswer corresponds to a span in the passage and the task is to\npredict this span. Although there are passages, questions and\ntheir corresponding answers in the training set, the locations\nof answers in the passage are not labeled. As the task of MRC\nis supervised, we need to label the answer in pre-processing.\nWe match the answer with the original passage, and there\nare three cases according to the number of matches:\n• If the number of matches is one, we directly label that\nlocation.\n• When the number of matches is more than one, it is\nsupposed that the answer is inclined to appear in the part\nof passage which is more similar to the question. Hence,\nwe label the location which is near to the begin or end of\nthe longest common substring of passage and question.\n• Irregularly labeled answers which have redundant\nspaces or incorrect punctuation result in missing\nmatches with the passage. To label those answers,\nwe calculate the longest common subsequence of the\nquestion and passage, whose length is more than 70 per-\ncent of the answer’s length will be labeled as the location\nof that answer.\nAccording to answer labeling, we know that the part of the\npassage, which is not labeled, has nothing to do with the pre-\ndiction generally, so the passage can be further pruned. If the\nend label of the answer is within the upper limit L, the part\nafter the end label can be pruned. Otherwise, we truncate\nthe text from title to the begin label of the answer randomly\nand then remove the text after the end label which is out of\nrange.\nVOLUME 7, 2019 27741\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\nE. EXPERIMENTAL SETTINGS\nWhen training the models, the parameters are set as presented\nin Table 2. We save both the best model with the highest\nRouge-L score and the last one before terminating the training\nprocess to make it easy for training continuously.\nTABLE 2. Parameters settings utilized in R-Trans model.\nF. RESULTS\nTable 3 illustrates the evaluation results of several models on\nLes MMRC. The ﬁrst column is the index and the second\none is the models in this experiment. The third and fourth\ncolumns present Rouge-L score and BLEU-4 score on the test\nset separately.\nTABLE 3. The evaluation results of diffierent models on Les MMRC.\nRow 1 to 3 show the results of models whose word rep-\nresentation just use the concatenation of character and word\nembeddings in the traditional way. Although the original\nQANet model outperforms the R-Net in both Rouge-L and\nBLEU-4 score, our R-Trans model achieves the best perfor-\nmance among these three models, improving Rouge-L and\nBLEU-4 score by (1.28%, 0.94%) compared to the original\nQANet.\nModels in row 4 to 7 apply other word representation\nstrategies like ELMo and part-of-speech tags (POS tags). The\ninclusion of POS tags is to show that richer word represen-\ntations contribute to higher accuracy of answer prediction.\nBoth POS tags and ELMo can improve the performance of\nmodels while ELMo is more powerful as Rouge-L score\nand BLEU-4 score of QANet utilized ELMo are higher than\nQANet with POS tags. Compared to the results in row 1 to 3,\nrow 5 to 7 illustrate that ELMo indeed makes a contribution to\ndifferent downstream models, however, its effort differs from\nmodels. As can be seen in the last row, our R-Trans model\nwith ELMo reaches the highest Rouge-L and BLEU-4 score,\nwhich indicates that more sematic information can be mined\nowing to the contextualized word representation and in turn\nenhance the accuracy of answer prediction.\nG. DISCUSSION\n1) COMPARATIVE ANALYSIS WITH QANet\nTo illustrate the superiority of our model to its baseline\nQANet, we conduct comparative analysis in this part. We use\nthe same experimental environment and parameters settings\nto compare the amounts of parameters, train time and infer-\nence time of our model and QANet. The result is shown\nin Table 4 and presents that our model is 1.84 and 2.50 faster\nthan QANet in training and inference speed with cutting down\nnearly 400,000 parameters.\nTABLE 4. Speed and parameters comparison results.\nWe plot the Rouge-L/BLEU-4 score on Les MMRC test\nset in Fig. 3 for R-Trans and QANet. The horizontal axis is\nnumber of iteration during training while the vertical axis is\nRouge-L/BLEU-4 score. We can see that both Rouge-L and\nBLEU-4 curves of our model not only converge faster but\nalso reach higher values than that of QANet, showing better\nperformance of our R-Trans in predicting answers.\nFIGURE 3. The Rouge-L and BLEU-4 curves of QANet and R-Trans while\ntraining.\n2) ABLATION ANALYSIS\nWe conduct ablation studies mainly on components of Input\nEmbedding Layer of our proposed model in order to evaluate\nthe performance of different representation methods.\n27742 VOLUME 7, 2019\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\nTABLE 5. Ablation analysis results.\nAs shown in Table 5, the ﬁrst row presents the result of our\nmodel, and its ablations are listed below. We can see from\nRow 2 to 4 that word-level, character-level embeddings and\nELMo all contribute to the model’s performance while ELMo\nis more crucial. Ablating ELMo results a performance drop\nof 1.49% and 3.63% on Rouge-L and BLEU-4, respectively,\nwhich is far greater than the degradation caused by removing\nword level or character level embeddings.\nHowever, word level and character level embeddings are\nindeed indispensable. From Row 5 to Row 7, we remove\ntwo components each time to investigate the effort of sin-\ngle representation. To be concrete, removing both ELMo\nand word level embeddings means we only use character\nlevel embeddings in Input Embedding Layer. The ablation\nof word level and character level embeddings accounts for\n2.57% and 4.18% on Rouge-L and BLEU-4, which indi-\ncates that just applying ELMo to represent words is not\nsufﬁcient.\nWe interpret these phenomena as follows: the word\nlevel embeddings can represent the semantics of each\nword while the character level embeddings are good\nat handling out-of-vocabulary (OOV) words. Moreover,\nELMo, which is deep contextualized, can encode contex-\ntual information effectively and help to relieve the ambi-\nguity caused by incorrect word segmentation in Chinese\ntext. As they cannot replace each other, combining three of\nthem together makes the most contribution to upgrading the\nperformance.\nTABLE 6. Case study.\nVOLUME 7, 2019 27743\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\n3) CASE STUDY\nTo demonstrate how contextualized word representation\ninﬂuences the prediction of ﬁnal answers, we conduct a case\nstudy in Table 6 with three examples selected from the outputs\nof our R-Trans model.\nIn Example 1, Jieba treats ‘‘\n’’(quota) as a single word,\nleading to a wrong answer Hundreds of places given by model\nwithout ELMo. However, model with ELMo is able to infer\nfrom surrounding context that the question asks about How\nmany additional people and gives the right answer Hundreds\nof people in spite of segmentation errors. In Example 2,\nwe can see that model with ELMo gets the intent of ques-\ntion and answers correctly while model without ELMo is\nmisled by erroneous segmentation and answers with a rea-\nson. In Example 3, contextualized word representations show\ntheir advantage in controlling the boundary of the answer.\nAlthough ‘‘\n’’(kilobit) is segmented as ‘‘\n (thousand)|\n(bite),’’ model with ELMo adds the missing part\n‘‘\n ’’ compared to the answer given by model without\nELMo, which makes the syntax of sentence complete.\nTo summarize, effectively encoding contextual informa-\ntion indeed contributes to mitigating the inﬂuence of incorrect\nword segmentations in Chinese documents and upgrading the\ndownstream answer prediction.\nV. CONCLUSION\nIn this article, we introduce R-Trans model with deep con-\ntextualized word representation to address the problems in\nChinese MRC task. Our model not only spends less time in\ntraining and inference but also mitigates the inﬂuence made\nby erroneous segmentation to downstream answer prediction.\nAs experimental results demonstrate, our R-Trans model sur-\npasses baseline models with state-of-the-art performance on\nLes MMRC.\nAfter analyzing the answer responded by our model,\nwe ﬁnd that questions which need inference and conclusion\ncannot be correctly answered in general. That may be the\nweakness of extractive MRC models. Our future work would\nlike to introduce external knowledge to our model by con-\nstructing knowledge graph to achieve better results.\nREFERENCES\n[1] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi. (2016). ‘‘Bidirec-\ntional attention ﬂow for machine comprehension.’’ [Online]. Available:\nhttps://arxiv.org/abs/1611.01603\n[2] W. Wang, N. Yang, F. Wei, B. Chang, and M. Zhou, ‘‘Gated self-matching\nnetworks for reading comprehension and question answering,’’ in Proc.\n55th Annu. Meeting Assoc. Comput. Linguistics, vol. 1, 2017, pp. 189–198.\n[3] C. Xiong, V . Zhong, and R. Socher. (2014). ‘‘Dynamic coattention\nnetworks for question answering.’’ [Online]. Available: https://arxiv.\norg/abs/1611.01604\n[4] W. He et al. (2017). ‘‘DuReader: A chinese machine reading com-\nprehension dataset from real-world applications.’’ [Online]. Available:\nhttps://arxiv.org/abs/1711.05073\n[5] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Distributed\nrepresentations of words and phrases and their compositionality,’’ in Proc.\nAdv. Neural Inf. Process. Syst., 2013, pp. 3111–3119.\n[6] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[7] A. W. Yu et al. (2018). ‘‘QANet: Combining local convolution with\nglobal self-attention for reading comprehension.’’ [Online]. Available:\nhttps://arxiv.org/abs/1804.09541\n[8] A. Vaswani et al., ‘‘Attention is all you need,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 5998–6008.\n[9] M. Richardson, C. J. Burges, and E. Renshaw, ‘‘Mctest: A challenge dataset\nfor the open-domain machine comprehension of text,’’ in Proc. Conf.\nEmpirical Methods Natural Lang. Process., 2013, pp. 193–203.\n[10] P. Clark et al. (2018). ‘‘Think you have solved question answer-\ning? try arc, the ai2 reasoning challenge.’’ [Online]. Available: https://\narxiv.org/abs/1803.05457\n[11] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy. (2017). ‘‘RACE: Large-scale\nreading comprehension dataset from examinations.’’ [Online]. Available:\nhttps://arxiv.org/abs/1704.04683\n[12] K. M. Hermann et al., ‘‘Teaching machines to read and comprehend,’’ in\nProc. Adv. Neural Inf. Process. Syst., 2015, pp. 1693–1701.\n[13] F. Hill, A. Bordes, S. Chopra, and J. Weston. (2015). ‘‘The goldilocks prin-\nciple: Reading children’s books with explicit memory representations.’’\n[Online]. Available: https://arxiv.org/abs/1511.02301\n[14] Q. Xie, G. Lai, Z. Dai, and E. Hovy. (2017). ‘‘Large-scale cloze\ntest dataset created by teachers.’’ [Online]. Available: https://arxiv.\norg/abs/1711.03225\n[15] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. (2016). ‘‘Squad:\n100 000+questions for machine comprehension of text.’’ [Online]. Avail-\nable: https://arxiv.org/abs/1606.05250\n[16] A. Trischler et al. (2016). ‘‘Newsqa: A machine comprehension dataset.’’\n[Online]. Available: https://arxiv.org/abs/1611.09830\n[17] E. Choi et al. (2018). ‘‘QuAC: Question answering in context.’’ [Online].\nAvailable: https://arxiv.org/abs/1808.07036\n[18] T. Nguyen, M. Rosenberg, S. Xia, J. Gao, and D. Li. (2016). ‘‘Ms marco:\nA human generated machine reading comprehension dataset.’’ [Online].\nAvailable: https://arxiv.org/abs/1611.09268\n[19] T. Kočiský et al., ‘‘The narrativeqa reading comprehension challenge,’’\nTrans. Assoc. Comput. Linguistics, vol. 6, pp. 317–328, Jul. 2018.\n[20] S. Wang and J. Jiang. (2016). ‘‘Machine comprehension using\nmatch-LSTM and answer pointer.’’ [Online]. Available: https://arxiv.\norg/abs/1608.07905\n[21] Z. Wang, H. Mi, W. Hamza, and R. Florian. (2016). ‘‘Multi-perspective\ncontext matching for machine comprehension.’’ [Online]. Available:\nhttps://arxiv.org/abs/1612.04211\n[22] Y . Shen, P.-S. Huang, J. Gao, and W. Chen, ‘‘ReasoNet: Learning to stop\nreading in machine comprehension,’’ in Proc. 23rd ACM SIGKDD Int.\nConf. Knowl. Discovery Data Mining, 2017, pp. 1047–1055.\n[23] X. Liu, Y . Shen, K. Duh, and J. Gao. (2017). ‘‘Stochastic answer\nnetworks for machine reading comprehension.’’ [Online]. Available:\nhttps://arxiv.org/abs/1712.03556\n[24] C. Tan, F. Wei, N. Yang, B. Du, W. Lv, and M. Zhou. (2017).\n‘‘S-NET: From answer extraction to answer generation for machine\nreading comprehension.’’ [Online]. Available: https://arxiv.org/abs/\n1706.04815\n[25] Z. Li, J. Xu, Y . Lan, J. Guo, Y . Feng, and X. Cheng, ‘‘Hierarchical answer\nselection framework for multi-passage machine reading comprehension,’’\nin Proc. China Conf. Inf. Retr., 2018, pp. 93–104.\n[26] M. Yan et al. (2018). ‘‘A deep cascade model for multi-document reading\ncomprehension.’’ [Online]. Available: https://arxiv.org/abs/1811.11374\n[27] J. Liu, W. Wei, M. Sun, H. Chen, Y . Du, and D. Lin, ‘‘A multi-answer multi-\ntask framework for real-world machine reading comprehension,’’ in Proc.\nConf. Empirical Methods Natural Lang. Process., 2018, pp. 2109–2118.\n[28] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval. Read-\ning, MA, USA: Addison-Wesley, 2011.\n[29] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‘‘Learning represen-\ntations by back-propagating errors,’’ Nature, vol. 323, no. 6088, p. 533,\n1986.\n[30] K. Cho et al. (2014). ‘‘Learning phrase representations using RNN\nencoder-decoder for statistical machine translation.’’ [Online]. Available:\nhttps://arxiv.org/abs/1406.1078\n[31] J. P. Chiu and E. Nichols. (2015). ‘‘Named entity recognition\nwith bidirectional LSTM-CNNs.’’ [Online]. Available: https://arxiv.\norg/abs/1511.08308\n[32] P. Nakov, A. Ritter, S. Rosenthal, F. Sebastiani, and V . Stoyanov,\n‘‘Semeval-2016 task 4: Sentiment analysis in Twitter,’’ in Proc. 10th Int.\nWorkshop Semantic Eval. (SEMEVAL), 2016, pp. 1–18.\n27744 VOLUME 7, 2019\nS. Liuet al.: R-Trans: RNN Transformer Network for Chinese MRC\n[33] I. V . Serban, A. Sordoni, Y . Bengio, A. C. Courville, and J. Pineau,\n‘‘Building end-to-end dialogue systems using generative hierarchical neu-\nral network models,’’ in Proc. AAAI, 2016, pp. 3776–3784.\n[34] B. Dhingra, H. Liu, R. Salakhutdinov, and W. W. Cohen. (2017).\n‘‘A comparative study of word embeddings for reading comprehension.’’\n[Online]. Available: https://arxiv.org/abs/1703.00993\n[35] D. Weissenborn, G. Wiese, and L. Seiffe. (2017). ‘‘Fastqa: A simple and\nefﬁcient neural architecture for question answering.’’ [Online]. Available:\nhttps://arxiv.org/abs/1703.04816\n[36] M. Hu, Y . Peng, Z. Huang, X. Qiu, F. Wei, and M. Zhou. (2017). ‘‘Rein-\nforced mnemonic reader for machine reading comprehension.’’ [Online].\nAvailable: https://arxiv.org/abs/1705.02798\n[37] M. E. Peters et al. (2018). ‘‘Deep contextualized word representations.’’\n[Online]. Available: https://arxiv.org/abs/1802.05365\n[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. (2018). ‘‘Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.’’\n[Online]. Available: https://arxiv.org/abs/1810.04805\nSHANSHAN LIU was born in Gansu, China, in\n1994. She received the bachelor’s degree in admin-\nistration from the School of Information Manage-\nment, Nanjing University, in 2017. She is currently\npursuing the master’s degree in management sci-\nence and engineering and the Ph.D. degree with\nthe National University of Defense Technology.\nHer research interests include natural language\nprocessing, data mining, and social computing.\nSHENG ZHANGreceived the B.S. degree in sys-\ntems engineering and the M.S. degree in manage-\nment science and engineering from the National\nUniversity of Defense Technology, Changsha,\nChina, in 2015 and 2017, respectively, where he is\ncurrently pursuing the Ph.D. degree with the Col-\nlege of Systems Engineering. His research inter-\nests include natural language processing, deep\nlearning, and data mining.\nXIN ZHANGreceived the B.S. and Ph.D. degrees\nin system engineering from the National Uni-\nversity of Defense Technology, China, in 2000\nand 2006, respectively, where he is currently\na Professor with the Science and Technology\non Information Systems Engineering Laboratory,\nCollege of Systems Engineering. His research\ninterests include crossmodal data mining, informa-\ntion extraction, and event analysis.\nHUI WANG received the B.S., M.S., and Ph.D.\ndegrees in system engineering from the National\nUniversity of Defense Technology, China, in 1990,\n1998, and 2005, respectively, where he is currently\na Professor with the Science and Technology\non Information Systems Engineering Laboratory,\nCollege of Systems Engineering. His research\ninterests include natural language processing, deep\nlearning, data mining, and social analysis.\nVOLUME 7, 2019 27745"
}