{
  "title": "A comparative study of pre-trained language models for named entity recognition in clinical trial eligibility criteria from multiple corpora",
  "url": "https://openalex.org/W4294763143",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2103418678",
      "name": "Jianfu Li",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A1976683342",
      "name": "Qiang Wei",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2251577483",
      "name": "Omid Ghiasvand",
      "affiliations": [
        "ZBW – Leibniz-Informationszentrum Wirtschaft"
      ]
    },
    {
      "id": "https://openalex.org/A2099349455",
      "name": "Miao Chen",
      "affiliations": [
        "Covance (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2751360390",
      "name": "Victor Lobanov",
      "affiliations": [
        "Covance (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2113027333",
      "name": "Chunhua Weng",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2103418678",
      "name": "Jianfu Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1976683342",
      "name": "Qiang Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2251577483",
      "name": "Omid Ghiasvand",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099349455",
      "name": "Miao Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751360390",
      "name": "Victor Lobanov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113027333",
      "name": "Chunhua Weng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2066578320",
    "https://openalex.org/W2410097662",
    "https://openalex.org/W2460367506",
    "https://openalex.org/W2604410201",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3081304278",
    "https://openalex.org/W2949445525",
    "https://openalex.org/W2768488789",
    "https://openalex.org/W2947903144",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2979250794",
    "https://openalex.org/W2114039834",
    "https://openalex.org/W2107435951",
    "https://openalex.org/W2027233318",
    "https://openalex.org/W2949176808",
    "https://openalex.org/W1774566842",
    "https://openalex.org/W3095092693",
    "https://openalex.org/W3004227146",
    "https://openalex.org/W6637919703",
    "https://openalex.org/W2590861467",
    "https://openalex.org/W2911661483",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W6601211009",
    "https://openalex.org/W2769851464",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W1034374084",
    "https://openalex.org/W6679820424",
    "https://openalex.org/W2167490942",
    "https://openalex.org/W2963917673",
    "https://openalex.org/W2890494294",
    "https://openalex.org/W4302075920",
    "https://openalex.org/W3106231035",
    "https://openalex.org/W1168171428"
  ],
  "abstract": "Abstract Background Clinical trial protocols are the foundation for advancing medical sciences, however, the extraction of accurate and meaningful information from the original clinical trials is very challenging due to the complex and unstructured texts of such documents. Named entity recognition (NER) is a fundamental and necessary step to process and standardize the unstructured text in clinical trials using Natural Language Processing (NLP) techniques. Methods In this study we fine-tuned pre-trained language models to support the NER task on clinical trial eligibility criteria. We systematically investigated four pre-trained contextual embedding models for the biomedical domain (i.e., BioBERT, BlueBERT, PubMedBERT, and SciBERT) and two models for the open domains (BERT and SpanBERT), for NER tasks using three existing clinical trial eligibility criteria corpora. In addition, we also investigated the feasibility of data augmentation approaches and evaluated their performance. Results Our evaluation results using tenfold cross-validation show that domain-specific transformer models achieved better performance than the general transformer models, with the best performance obtained by the PubMedBERT model (F1-scores of 0.715, 0.836, and 0.622 for the three corpora respectively). The data augmentation results show that it is feasible to leverage additional corpora to improve NER performance. Conclusions Findings from this study not only demonstrate the importance of contextual embeddings trained from domain-specific corpora, but also shed lights on the benefits of leveraging multiple data sources for the challenging NER task in clinical trial eligibility criteria text.",
  "full_text": "Li et al. \nBMC Medical Informatics and Decision Making          (2022) 22:235  \nhttps://doi.org/10.1186/s12911-022-01967-7\nRESEARCH\nA comparative study of pre-trained language \nmodels for named entity recognition in clinical \ntrial eligibility criteria from multiple corpora\nJianfu Li1*  , Qiang Wei1, Omid Ghiasvand2, Miao Chen3, Victor Lobanov3, Chunhua Weng4 and Hua Xu1 \nFrom The Fourth International Workshop on Health Nature Language Processing (HealthNLP 2021) \nVirtual. 9 August 2021\nAbstract \nBackground: Clinical trial protocols are the foundation for advancing medical sciences, however, the extraction \nof accurate and meaningful information from the original clinical trials is very challenging due to the complex and \nunstructured texts of such documents. Named entity recognition (NER) is a fundamental and necessary step to pro-\ncess and standardize the unstructured text in clinical trials using Natural Language Processing (NLP) techniques.\nMethods: In this study we fine-tuned pre-trained language models to support the NER task on clinical trial eligibility \ncriteria. We systematically investigated four pre-trained contextual embedding models for the biomedical domain (i.e., \nBioBERT, BlueBERT, PubMedBERT, and SciBERT) and two models for the open domains (BERT and SpanBERT), for NER \ntasks using three existing clinical trial eligibility criteria corpora. In addition, we also investigated the feasibility of data \naugmentation approaches and evaluated their performance.\nResults: Our evaluation results using tenfold cross-validation show that domain-specific transformer models \nachieved better performance than the general transformer models, with the best performance obtained by the \nPubMedBERT model (F1-scores of 0.715, 0.836, and 0.622 for the three corpora respectively). The data augmentation \nresults show that it is feasible to leverage additional corpora to improve NER performance.\nConclusions: Findings from this study not only demonstrate the importance of contextual embeddings trained from \ndomain-specific corpora, but also shed lights on the benefits of leveraging multiple data sources for the challenging \nNER task in clinical trial eligibility criteria text.\nKeywords: Clinical trial, Eligibility criteria, Named entity recognition, Pre-trained language model\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nBackground\nClinical trial protocols define important details about \ndesign and execution of clinical trials, which are the \nfoundation for advancing medical sciences. An impor -\ntant section of clinical trials is the eligibility criteria \n(EC), which is often described in free text and not read -\nily amenable for computer processing [1]. Formal repre -\nsentations developed in the past years have been used to \noptimize patient recruitment; but often require labori -\nous manual effort to convert free text EC to structured \nrepresentations [2, 3]. To address this challenge, natural \nlanguage processing (NLP) techniques have also been \nOpen Access\n*Correspondence:  Jianfu.Li@uth.tmc.edu\n1 School of Biomedical Informatics, The University of Texas Health Science \nCenter at Houston, Houston, TX, USA\nFull list of author information is available at the end of the article\nPage 2 of 9Li et al. BMC Medical Informatics and Decision Making          (2022) 22:235 \ninvestigated to process the EC text in clinical trials and \nconvert them into standard representations in an efficient \nand effective manner [4, 5]. Named entity recognition \n(NER) is a fundamental and necessary step for extracting \nand standardizing EC using NLP . Recent deep learning \napproaches based on pre-trained language models such \nas Bidirectional Encoder Representations from Trans -\nformers (BERT) [6] have shown promising results in \nmany NLP tasks including NER. Many transformer-based \nmodels using BERT and its variants have been studied for \nbiomedical NER tasks, mainly for clinical notes in elec -\ntronic health records (EHR) or articles in biomedical bib -\nliographic databases. Few studies have applied BERT and \nits variants to NER tasks for clinical trial documents [5]. \nMore specifically, there is no study that has systematically \nexplored and compared performance of different BERT \nmodels on NER of EC in clinical trial documents.\nIn this study, we proposed to investigate different pre-\ntrained language models (including both those trained \nfrom the general English domain and those specifically \ntrained for the biomedical domain) for the NER tasks \non EC of clinical trial documents. We systematically \ncompared four biomedical domain-specific pre-trained \ncontextual embedding models (named BioBERT [7], \nBlueBERT [8], PubMedBERT [9], and SciBERT [10]) \nand two general-domain models (named BERT and \nSpanBERT [11]), for extracting diverse types of clini -\ncally relevant entities from three annotated clinical tri -\nals corpora: (1) 470 in-house drug development study \nprotocols annotated by Covance [5], (2) 230 Alzheimer’s \ndisease (AD) clinical trial documents from ClinicalTrials.\ngov (named EliIE) [4], and (3) 1000 interventional, Phase \nIV clinical trials selected from ClinicalTrials.gov (named \nChia) [12]. In addition, we investigated the feasibility of \ndata augmentation approaches to leveraging different \ndatasets to improve NER performance in EC.\nRelated work\nNER has been extensively studied and has shown its great \nuse of supporting downstream applications in the medi -\ncal domain, such as drug repurposing and clinical deci -\nsion support [13, 14]. A lot of work has been focused on \nNER tasks for clinical reports, e.g., clinical concepts rec -\nognition, including rule-based, machine learning-based, \nand deep learning-based methods [15–21]. Many shared \ntasks have been organized and several annotated cor -\npora of clinical notes have been created and made pub -\nlicly available. For example, the well-known 2010 i2b2/\nVA Workshop on NLP Challenges for Clinical Records \ncontained a task for concept extraction from clinical dis -\ncharge summaries, the objective of which was to extract \nmedical problems, treatments, and lab tests from patient \nreports [16]. Another example is the 2018 National NLP \nClinical Challenges, which hosted shared tasks such as \nextraction of adverse drug events (ADEs) from narra -\ntive discharge summaries [17]. Recently, as the newly \ndeveloped pre-trained language models including BERT \nand its variants achieved the state-of-the-art perfor -\nmance in a number of NLP tasks including NER, more \nand more studies have examined those pre-trained trans-\nformer-based models on NER tasks for clinical notes and \nreported superior performance [22, 23].\nClinical trial protocols, which provide detailed infor -\nmation about trial design and execution, are another \ntype of important textual data in healthcare. In the past \ndecade, researchers have worked on extracting and \nstandardizing content of clinical trial documents (e.g., \nEC sections), with the goal to promote computerized \napplications during trial execution (e.g., automated cri -\nteria matching for trial recruitment). Different methods \nand tools have been developed for NER tasks that aim to \nextract key clinical concepts from EC and other sections \nof clinical trial protocols, including rule-based, machine \nlearning-based, and hybrid methods [4, 24, 25]. In [4], an \nopen-source information extraction tool called EliIE was \ndeveloped, and it consists of four components: (1) entity \nand attribute recognition, (2) negation detection, (3) rela-\ntion extraction, and (4) concept normalization and out -\nput structuring. EliIE used the conditional random field \n(CRF) algorithm for its NER task and achieved an overall \nF1 score of 0.786 on 7 types of entities. Zhang and Fush -\nman [26] proposed rule-based strategies that extracted \nnamed entities using MetaMap and used them for clas -\nsifying criteria. Yuan et al. [27] further developed a new \nnatural language interface named Criteria2Query, which \nautomatically transformed eligibility criteria to SQL \nqueries for searching patients from clinical databases in \nOMOP Common Data Model. Like the EliIE tool, Cri -\nteria2Query also applied machine learning methods for \nNER and relation extraction tasks. More recently, Chen \net al. [5] investigated deep learning models on NER from \nEC of clinical trials. In their study, BERT and BioBERT \nhave been examined to extract entities from clinical trial \nprotocols and they show improved performance, com -\npared with traditional machine learning algorithms. \nNevertheless, there is no comprehensive study that sys -\ntematically investigates different contextual embeddings \nfor NER in EC section of clinical trial documents. Recent \nstate-of-the-art pre-trained language models that are \ndeveloped for the biomedical domain (e.g., BlueBERT [8] \nand PubMedBERT [9]) have not been applied to clinical \ntrial documents yet.\nIn addition, annotated corpora for NER in the EC sec -\ntion of clinical trial protocols have been developed in \nmultiple studies, including (1) EliIE [4], which contains \n230 annotated protocols of Alzheimer’s Disease (AD) \nPage 3 of 9\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:235 \n \nclinical trial from ClinicalTrials.gov; (2) Covance [5], \nwhich contains 470 annotated drug development study \nprotocols collected from in-house studies by Covance; \nand (3) Chia [12], which contains 1000 annotated pro -\ntocols randomly selected from interventional Phase IV \nclinical trials registered in ClinicalTrials.gov. In addition \nto entities, both Covance and Chia also annotated modi -\nfiers to main clinical entities. Table  1 and 2 show some \nstatistics of entities in the three corpora. Although such \nexisting corpora provide great opportunities for method \ndevelopment and evaluation for NER in EC text, to the \nbest of our knowledge, currently there is no study that \nhas investigated NER approaches and systems across \nmultiple clinical trial corpora.\nThe purpose of this study is twofold: (1) we want to sys-\ntematically examine the performance of different state-\nof-the-art pre-trained language models (from both open \ndomains and the biomedical domain) on NER for EC in \nclinical trial protocols; and (2) we plan to compare NER \nperformance across multiple EC corpora and explore the \nfeasibility of leveraging multiple data sources to improve \nNER performance in EC.\nMaterials and methods\nDataset\nIn this study, we included all three corpora listed in \nTable 1: EliIE, Covance, and Chia. Among them, EliIE \nand Covance share similar annotation guidelines, \nalthough Covance contains more entity types than that in \nEliIE. The Chia corpus contains more fine-grained anno -\ntations of entity types and relations, e.g., including dis -\njoint, nested, and overlapping entities. As such non-flat \nannotations require specific NER methods, we converted \nChia annotations to continuous, non-overlapping entities \nonly, to make them similar to EliIE and Covance anno -\ntations to ease the comparison. We applied two rules in \nthis conversion: (1) for nested entities, we kept the out -\nside entity only and removed the annotation of the nested \none (Fig.  1-Left); and (2) we merged the disjoint entities \nto form a longer, continuous entity (Fig. 1-Right).\nFor Chia, there are two distinct datasets titled With \nScopes and Without Scopes describing the inclusion or \nexclusion of Scope entities. The two datasets differs only \nin their utilization of Scope entity within the annota -\ntion model. We chose the Without Scopes dataset and \nTable 1 Basic information and statistics of entities in the three EC corpora for NER\nCorpus EliIE Covance Chia\nNumber of documents 230 470 1000\nSource Clinicaltrials.org In-house by Covance clinicaltrials.org\nDisease Areas Alzheimer’s disease only All diseases All diseases\nTable 2 Main entities (entity types)—Count (number of occurrence) in the three EC corpora; numbers in the parentheses are nested \noccurrence for Chia corpus\nEliIE Covance Chia\nMain entities Count Main entities Count Main entities Count\nCondition 4138 Condition 21,022 Condition 12,039 (127)\nDrug 1465 Drug 13,671 Drug 3801 (24)\nQualifier 1715 Qualifier_Modifier 12,953 Qualifier 4157 (127)\nMeasurement 1029 Measurement 7732 Measurement 3305 (9)\nProcedure_Device 652 Procedure 5635 Procedure 3595 (54)\nObservation 1765 Observation 12,391 Observation 1216 (19)\nTemporal_measurement 812 Temporal_constraint 11,326 Temporal 3580 (1066)\nAnatomic_location 83 Anatomic_location 648 Negation 843 (0)\nNegation_Cue 1551 Device 386 (2)\nEvent 4053 Multiplier 671 (8)\nPermission_Cue 2108 Person 1666 (2)\nDemographics 869 Value 4002 (60)\nDevice 360 Visit 165 (1)\nRefractory_condition 662 Mood 616 (13)\nInvestigational_product 559 Reference_point 934 (116)\nPage 4 of 9Li et al. BMC Medical Informatics and Decision Making          (2022) 22:235 \ncombined the inclusion and exclusion together for each \nannotated EC file for our evaluation.\nPre‑trained language models\nThis study systematically investigated six state-of-the-\nart transformer-based language models: two from open \ndomains: BERT and SpanBERT; and four for the biomed-\nical domain: BioBERT, BlueBERT, PubMedBERT, and \nSciBERT.\nBERT: A bidirectional deep transformer encoder model \npre-trained on general domain corpora using masked \nlanguage modeling (MLM) and next sentence prediction \n(NSP). The large model architecture has 24 transformer \nblocks with a hidden size of 1024 and 16 attention heads. \nThe total number of parameters is 340 million. The model \nwas trained on general English corpus from Wikipedia \nand BooksCorpus [28].\nSpanBERT: A pre-trained transformer model extended \nBERT by: (1) masking contiguous random spans instead \nof random tokens, and (2) training the span boundary \nrepresentations without relying on the individual token \nrepresentations within it.\nBioBERT: The first domain-specific BERT based model \npre-trained on biomedical corpora. BioBERT was initial -\nized with weights from BERT at first, then pretrained \nwith additional corpus from large biomedical domain \n(PubMed abstracts and PMC full-text articles). BioBERT \nutilized WordPiece tokenization [29] to address the out-\nof-vocabulary issue so that any new words would be \nrepresented with subsequent subwords. It was shown \nto achieve better performance than the original BERT \nmodel on several biomedical NLP tasks like NER, rela -\ntion extraction, and question answering.\nBlueBERT: A pre-trained domain-specific transformer \nmodel by continual pretraining of BERT on biomedical \nand clinical corpora. Similar to BioBERT, BlueBERT was \ninitialized with BERT firstly and then continue to pre -\ntrain the model using the large biomedical and clinical \ndomain (PubMed abstracts and clinical notes MIMIC-\nIII). The Biomedical Language Understanding Evaluation \n(BLUE) benchmark evaluated on five tasks with ten cor -\npora shows that the BERT model pre-trained on PubMed \nabstracts and MIMIC-III clinical notes achieved better \nperformance than most state-of-the-art models.\nPubMedBERT: A pre-trained domain-specific trans -\nformer model by pretraining from scratch on a large \nbiomedical domain. It generated the vocabulary and pre-\ntrained from scratch to extend the uncased BERT Base \nmodel over a collection of PubMed abstracts and full \nPubMed Central articles.\nSciBERT: A pre-trained domain-specific transformer \nmodel by pre-training from scratch on biomedicine and \ncomputer science domain. It generated the vocabulary \nand pre-trained from scratch to extend the cased BERT \nBase model over a random sample of 1.14 M papers from \nSemantic Scholar (18% papers from the computer science \ndomain and 82% from PMC).\nNER using transformer models\nFigure  2 shows the architecture of the NER task using \npre-trained transformer models. The NER task is formu -\nlated as a sequence labeling task, to assign a predefined \nB/I/O tag to each token of the sequence, where “B” rep -\nresents the beginning of an entity, “I” represents tokens \ninside an entity, and “O” represents all other nonentity \nwords. At first, the annotated sentences in each corpus \nwere preprocessed and transformed into the “BIO” for -\nmat (e.g., sentence boundary detection and initial tokeni -\nzation) by CLAMP (Clinical Language Annotation, \nModeling, and Processing toolkit) [30], then the input \ninstances were processed by appending with a special \ntoken [CLS] at the beginning of the text. The processed \ninputs were tokenized based on the pre-trained language \nmodel’s vocabulary and then fed into the language model. \nThen the contextual representations of the tokenized \nprocessed input were generated. Finally, the NER task \nis done by using an additional linear classification layer \non the contextual representations to predict token tags. \nTo address the out-of-vocabulary (OOV) problem, the \ntransformer models usually split original words into mul-\ntiple pieces of sub-tokens, using a special tag “##” to be \ninserted in the front of the following sub-tokens.\nAll transformer models were downloaded from the \nHuggingFace website (https:// huggi ngface. co/ models). \nAll NER models were trained using an NER package \nFig. 1 Examples of conversions of non-flat entities in the Chia corpus. Left: Nested entities; Right: Disjoint entities\nPage 5 of 9\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:235 \n \ndeveloped on the Transformers library implemented by \nthe HuggingFace team [31] using PyTorch.\nExperiments and evaluation\nFor each corpus, a tenfold cross-validation (train/dev/\ntest subsets with a ratio of 80%:10%:10%) was used to \ntrain and evaluate the performance of the NER models. \nBased on the state-of-the-art research in [35] and our \nprevious experience, the following hyperparameters \nwere used for all the models (Table 3 ).\nWe evaluated the performance of all the transformer-\nbased NER models using both the strict and relaxed \nmicro precision, recall, and F1-score [32], where strict \nmeans that an entity is correctly identified if both \nthe boundary and entity type is same as those in gold \nstandard, the relaxed means that an entity is correctly \nidentified if its entity type is correct and its boundary \noverlaps with that in the gold annotations.\nFor the data augmentation experiment, we trained \nNER models by directly combining additional corpora \n(EliIE, Chia, and EliIE + Chia) with the training set of \nthe Covance corpus and then evaluated their perfor -\nmance on the test set of the Covance corpus.\nResults\nTable 4 shows the strict and relaxed micro P/R/F1 scores \nof six transformer-based models for NER in EC of trials \non three corpora from Covance, EliIE, and Chia. Among \nall models, the PubMedBERT achieved the best per -\nformance on all three datasets, with strict and relaxed \nF1-scores of 0.715 (0.835), 0.832 (0.900), and 0.622 \n(0.744), respectively. To report the statistical signifi -\ncance of the differences among the results of the various \nexperiments, the Wilcoxon rank sum tests [33], were also \napplied to compare the strict F1 metric of PubMedBERT \nwith the other pre-trained models across the three cor -\npora. Compared with the general domain pre-trained \nBERT model, the PubMedBERT improved the F1-scores \nby 1%, 2.9%, and 2.4% on Covance, EliIE and Chia cor -\npora respectively. Different transformer models also \nshowed consistent patterns for performance on the three \ncorpora—all models achieved highest performance on \nthe EliIE corpus and the lowest performance on the Chia \ncorpus, with Covance in the middle. Moreover, the vari -\nations of the same model on different corpora were large \n(e.g., more than 20% in F1 score between EliIE and Chia), \nindicating the intrinsic differences between those anno -\ntated corpora in EC of trials.\nTable 5 shows the detailed results of the PubMedBERT \nmodel for each entity in the three corpora. Our results \nshowed large differences in performance for different \ntypes of entities: F1-measures ranged from 0.429 to 0.830 \nfor the Covance corpus, 0.507 to 0.881 for the EliIE cor -\npus, and 0.015 to 0.808 for the Chia corpus.\nTable  6 shows the results of the data augmentation \nexperiments on common entities. When the EliIE cor -\npus was added to the training set of the Covance cor -\npus, it slightly improved the overall performance on \nthe test set of Covance—F1 score was improved from \nFig. 2 Architecture of the NER task using pre-trained transformer models\nTable 3 Hyperparameters used for all the transformer models\nHyperparameters Value\ntraining epochs 10\nLearning rate 5.00E−05\nAdam epsilon 1.00E−08\nTraining batch size 8\nMaximum sequence length 256\nPage 6 of 9Li et al. BMC Medical Informatics and Decision Making          (2022) 22:235 \n0.715 to 0.721. However, when Chia or Chia + EliIE was \nadded to the training set of Covance, it dropped the \noverall F1 score on the test set of Covance.\nTable 7 shows the computational time per epoch for \nall the models that trained on the three corpora using \na single NVIDIA A100 GPU. Different models also \nshowed consistent patterns for time complexity on the \nthree corpora—all models spent longest time on the \nCovance corpus (with training data size 7.1  MB) and \nthe shortest time on the EliIE corpus (with training \ndata size 1.0 MB), with Chia in the middle (with train -\ning data size 4.0 MB).\nDiscussion\nIn this study, we systematically investigated general and \ndomain-specific pre-trained language models for NER \nin EC text using three clinical trials corpora. Experi -\nmental evaluation shows that the PubMedBERT model \nachieved the best overall performance in all three cor -\npora among six models. It achieved strict F1-scores \nTable 4 The strict and relaxed overall performance on the test sets of COVANCE, ELIIE, and CHIA corpora\nBold values were calculated using the Wilcoxon rank sum test. The Wilcoxon rank sum test is a non-parametric test method that determines whether the means of \nstrict F1 scores (Bold values) from the 10-fold experiments of the PubMedBERT model and each other model (BERT, SpanBERT, BioBERT, SciBERT) are statistically \ndifferent from each other based on ranks rather than the original F1 scores of the experiments. The detailed definition of the Wilcoxon rank sum test can be found in \nthe reference [33] as shown in the manuscript\nNumbers in the parentheses are results based on relaxed criteria\n*Indicates p < 0.05 when comparing to other pre-trained models\nModels Covance EliIE Chia\nP R F1 P R F1 P R F1\nBERT 0.691\n(0.810)\n0.719\n(0.849)\n0.705\n(0.829)\n0.810\n(0.877)\n0.842\n(0.917)\n0.826\n(0.896)\n0.577\n(0.701)\n0.620\n(0.761)\n0.598\n(0.730)\nSpanBERT 0.692\n(0.810)\n0.718\n(0.847)\n0.705\n(0.828)\n0.813\n(0.879)\n0.843\n(0.917)\n0.828\n(0.897)\n0.593\n(0.711)\n0.628\n(0.758)\n0.610\n(0.734)\nBioBERT 0.694\n(0.812)\n0.722\n(0.851)\n0.708\n(0.831)\n0.810\n(0.879)\n0.837\n(0.915)\n0.823\n(0.896)\n0.589\n(0.707)\n0.632\n(0.765)\n0.609\n(0.735)\nBlueBERT 0.689\n(0.807)\n0.718\n(0.848)\n0.703\n(0.827)\n0.811\n(0.880)\n0.838\n(0.917)\n0.824\n(0.898)\n0.590\n(0.702)\n0.616\n(0.737)\n0.603\n(0.719)\nPubMedBERT 0.704\n(0.820)\n0.727\n(0.851)\n0.715*\n(0.835)\n0.817\n(0.881)\n0.847\n(0.920)\n0.832*\n(0.900)\n0.606\n(0.724)\n0.639\n(0.765)\n0.622*\n(0.744)\nSciBERT 0.696\n(0.813)\n0.723\n(0.850)\n0.709\n(0.831)\n0.813\n(0.883)\n0.839\n(0.915)\n0.825\n(0.899)\n0.589\n(0.709)\n0.634\n(0.768)\n0.611\n(0.737)\nTable 5 The strict performance of the PubMedBERT model for each main entity across the three corpora\nMain entities Covance Main entities EliIE Main entities Chia\nP R F1 P R F1 P R F1\nCondition 0.783 0.806 0.795 Condition 0.871 0.892 0.881 Condition 0.742 0.773 0.757\nDrug 0.734 0.762 0.748 Drug 0.850 0.881 0.865 Drug 0.747 0.798 0.771\nQualifier_Modifier 0.597 0.599 0.598 Qualifier 0.780 0.814 0.796 Qualifier 0.444 0.486 0.462\nMeasurement 0.786 0.818 0.801 Measurement 0.863 0.871 0.866 Measurement 0.669 0.689 0.678\nProcedure 0.651 0.674 0.662 Procedure_device 0.725 0.765 0.742 Procedure 0.574 0.630 0.600\nObservation 0.651 0.679 0.664 Observation 0.754 0.792 0.771 Observation 0.278 0.260 0.267\nTemporal_constraint 0.717 0.751 0.733 Temporal_measurement 0.807 0.829 0.815 Temporal 0.552 0.638 0.592\nAnatomic_location 0.458 0.407 0.429 Anatomic_location 0.519 0.499 0.507 Negation 0.569 0.626 0.595\nNegation_Cue 0.500 0.502 0.501 Device 0.528 0.515 0.520\nEvent 0.814 0.848 0.830 Multiplier 0.374 0.406 0.388\nPermission_Cue 0.578 0.635 0.604 Person 0.795 0.824 0.808\nDemographics 0.714 0.743 0.727 Value 0.727 0.745 0.735\nDevice 0.565 0.567 0.559 Visit 0.504 0.579 0.530\nRefractory_condition 0.519 0.586 0.547 Mood 0.302 0.360 0.325\nInvestigational_product 0.657 0.630 0.641 Reference_point 0.398 0.524 0.453\nPage 7 of 9\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:235 \n \nof 0.715 and 0.832 on the Covance and EliIE corpora \nrespectively, which were better than previously pub -\nlished results on these corpora (e.g., F1 of 0.708 for \nCovance in [5 ] and F1 of 0.786 on EliIE in [4 ]). These \nfindings indicate that domain-specific language mod -\nels are valuable for NER in EC and it worth further \ninvestigation.\nBERT and SpanBERT were pre-trained using gen -\neral corpora from English Wikipedia and BooksCorpus. \nDomain-specific models were built by either continu -\nously fine-tuning on the top of BERT using biomedical \ncorpora (e.g., BioBERT and BlueBERT) or training lan -\nguage models from scratch using biomedical corpora \n(e.g., PubMedBERT and SciBERT), thus providing more \nmeaningful and representative word embeddings for \ndownstream domain-specific tasks. As shown in Table  4, \nPubMedBERT and SciBERT also show slightly better \nperformance than BioBERT and BlueBERT. One of the \nreasons could be that they have better vocabulary cover -\nage on clinical trial documents, as they are trained from \nscratch using biomedical vocabularies. Table  8 shows the \npercentages of vocabulary coverage of BERT, PubMed -\nBERT, and SciBERT on words from the three corpora \nof clinical trial protocols, which obviously indicates a \nsmaller OOV problem for PubMedBERT. The reason that \nPubMedBERT outperformed SciBERT could be related \nto the training corpora—the SciBERT model was pre-\ntrained from scratch using mixed domain corpora from \nboth computer science and biomedicine. Nevertheless, \nthe differences of performance between any domain-spe -\ncific models are small.\nA large performance variation was observed among \nthree corpora (e.g., F1 scores of 0.715, 0.832, and 0.622 \non Covance, EliIE, and Chia respectively, for the same \nPubMedBERT model), and patterns were consistent for \nall models (e.g., EliIE > Covance > Chia), which indicates \nthe intrinsic differences among three annotated corpora, \nincluding (1) information models (e.g., types of enti -\nties and relations included); (2) annotation schemes and \nguidelines (e.g., whether to allow nested or disjoint enti -\nties); (3) sub-domains of samples (e.g., EliIE is from AD \ntrials only); and (4) sample sizes. All models have better \nperformance on the EliIE corpus probably due to that it \ncontains trials from AD only and the types of entities are \nrelatively simple. The low performance of Chia is prob -\nably mainly related to its complex and notable non-flat \nTable 6 The strict performance for the common main entities of COVANCE with augment corpora using the PubMedBERT model\nBold values were calculated using the Wilcoxon rank sum test. The Wilcoxon rank sum test is a non-parametric test method that determines whether the means of \nstrict F1 scores (Bold values) from the 10-fold experiments of the PubMedBERT model and each other model (BERT, SpanBERT, BioBERT, SciBERT) are statistically \ndifferent from each other based on ranks rather than the original F1 scores of the experiments. The detailed definition of the Wilcoxon rank sum test can be found in \nthe reference [33] as shown in the manuscript\n*Indicates p < 0.05 when comparing to the original Covance corpus\nMain entities Covance Covance + EliIE Covance + Chia Covance + EliIE + Chia\nP R F1 P R F1 P R F1 P R F1\nCondition 0.783 0.806 0.795 0.784 0.808 0.796 0.765 0.801 0.783 0.767 0.799 0.782\nDrug 0.734 0.762 0.748 0.734 0.761 0.747 0.731 0.754 0.742 0.727 0.756 0.741\nMeasurement 0.786 0.818 0.801 0.783 0.814 0.798 0.751 0.790 0.770 0.748 0.786 0.766\nObservation 0.651 0.679 0.664 0.651 0.678 0.664 0.643 0.657 0.650 0.650 0.661 0.655\nProcedure 0.651 0.674 0.662 0.652 0.660 0.656 0.636 0.665 0.650 0.632 0.663 0.647\nQualifier_Modifier 0.597 0.599 0.598 0.602 0.595 0.598 0.580 0.572 0.576 0.584 0.579 0.581\nTemporal_constraint 0.717 0.751 0.733 0.720 0.751 0.735 0.707 0.750 0.728 0.707 0.748 0.727\nOverall 0.704 0.727 0.715 0.712 0.731 0.721* 0.697 0.720 0.708 0.697 0.721 0.709\nTable 7 Computational time for training all the models on three \ncorpora\nModels Training time (seconds per epoch)\nCovance EliIE Chia\nBERT 518.4 69.9 212.3\nSpanBERT 520.3 70.5 212.3\nBioBERT 343.4 30.9 92.6\nBlueBERT 529.8 69.6 212.6\nPubMedBERT 395.7 30.7 92.5\nSciBERT 341.7 30.5 92.3\nTable 8 Percentages of vocabulary coverage of BERT, \nPubMedBERT, and SCIBERT in ELIIE, COVANCE, and CHIA\nEliIE (%) Covance (%) Chia (%)\nBERT 47.5 28.1 34.3\nPubMedBERT 63.2 44.4 53.4\nSciBERT 54.8 34.1 41.9\nPage 8 of 9Li et al. BMC Medical Informatics and Decision Making          (2022) 22:235 \nannotation schemes, as it stated that Chia was the first \nclinical trial corpus with considerable size annotated in \na non-flat mode which supported annotations of nesting \nand disjoint entities [12]. When we applied rules to con -\nvert disjoint, nested or overlapping entities to continuous \nand non-overlapping entities in the preprocessing mod -\nule, it may cause other issues such as reducing some types \nof entities while removing the inner nested entities or \nbringing certain noise while merging the disjointed enti -\nties, which would inevitably lower their performance. As \nthe performance on Chia is not optimal, more advanced \nmethods should be investigated to further improve NER \nsystems to handle nested, disjoint, or overlapping entities \nin EC [34].\nOur experiment that directly combined different cor -\npora shows slight improvement when adding EliIE to \nthe training set of Covance, the Wilcoxon rank sum tests \nshow that the improvement is statistically significant with \np < 0.05, therefore indicating it is worth investigating such \ndata augmentation approaches for NER tasks in clinical \ntrial documents. The reason that adding Chia to Covance \ndid not improve the model performance is probably due \nto the differences of annotation schemes and guidelines \nbetween Covance and Chia. As stated in [5], the Covance \ncorpus was constructed following a similar guideline as \nthat of EliIE. Our next step is to investigate more sophis -\nticated data augmentation algorithms, e.g.,, different \ndomain adaptation methods [35–37].\nThere are limitations in this study. We mainly explored \npre-trained language models on the NER tasks only. \nHowever, to support downstream applications, modifiers \nof clinical entities and standard codes of those entities \nshould be identified as well. Therefore, our next step is to \nexplore pre-trained language models on relation extrac -\ntion tasks [15] in EC text. Furthermore, it is interesting \nto develop a robust mechanism to process the complex, \nnon-flat annotations in Chia.\nConclusion\nIn this study, we systematically compared BERT and \nits variants for NER in clinical trial eligibility criteria \ntext and our results show that the PubMedBERT, which \ntrained domain-specific language models from scratch \nusing PubMed abstracts and full-text articles, achieved \nthe best performance across multiple corpora, although \nvariation among different models is small. However, large \nperformance gaps were observed among different clinical \ntrial corpora, calling for in-depth analysis of variations \namong different types of clinical trials, so that more gen -\neralizable approaches can be developed for all types of \ntrial documents.\nAbout this supplement\nThis article has been published as part of BMC Medical Informatics and Deci-\nsion Making Volume 22 Supplement 3 2023: Selected articles from the Fourth \nInternational Workshop on Health Nature Language Processing (HealthNLP \n2021). The full contents of the supplement are available at https:// bmcme \ndinfo rmdec ismak. biome dcent ral. com/ artic les/ suppl ements/ volume- 22- suppl \nement-3.\nAuthor contributions\nHX, CW, MC, VL, and JL conceived and designed the research; MC, VL, and \nCW participated in dataset construction and annotation; JL, QW, and OG \nperformed the experiments and analyzed the data; JL and HX drafted the \nmanuscript; HX, CW, MV, VL, QW, and OG offered insights and guidance for \nmanuscript revision; all authors checked and approved the final manuscript.\nFunding\nThis research was partially supported by the Covance contract to the \nUTHealth. CW was supported by NLM Grant R01LM009886.\nAvailability of data and materials\nThe Chia dataset is publicly available on figshare at https:// doi. org/ 10. 6084/ \nm9. figsh are. 11855 817. To access the EliIE and Covance datasets, please con-\ntact Dr. Chunhua Weng and Dr. Miao Chen for further details and permission, \nrespectively.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nDr. Xu and The University of Texas Health Science Center at Houston have \nresearch-related financial interests in Melax Technologies, Inc.\nAuthor details\n1 School of Biomedical Informatics, The University of Texas Health Science \nCenter at Houston, Houston, TX, USA. 2 German National Library of Econom-\nics, Hamburg, Germany. 3 Covance by Labcorp, Princeton, USA. 4 Department \nof Biomedical Informatics, Columbia University, New York, USA. \nReceived: 21 June 2022   Accepted: 10 August 2022\nReferences\n 1. Weng C, Tu SW, Sim I, Richesson R. Formal representations of eligibility \ncriteria: a literature review. J Biomed Inform. 2011;43(3):451–67.\n 2. Hripcsak G, Ryan PB, Duke JD, Shah NH, Park RW, Huser V, et al. Char -\nacterizing treatment pathways at scale using the OHDSI network. Proc \nNatl Acad Sci. 2016;113(27):7329–36.\n 3. He Z, Wang S, Borhanian E, Weng C. Assessing the collective popula-\ntion representativeness of related type 2 diabetes trials by combining \npublic data from Clinical Trials.gov and NHANES. Stud Health Technol \nInform. 2015;216:569.\n 4. Kang T, Zhang S, Tang Y, Hruby GW, Rusanov A, Weng C. EliIE: an open-\nsource information extraction system for clinical trial eligibility criteria. \nJ Am Med Inf Assoc. 2017;24(April):1062–71.\n 5. Chen M, Du F, Lan G, Lobanov V. Using pre-trained transformer deep \nlearning models to identify named entities and syntactic relations \nfor clinical protocol analysis. In: AAAI spring symposium: combining \nmachine learning with knowledge engineering. 2020.\n 6. Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep \nbidirectional transformers for language understanding. In: Proceedings \nof the conference on NAACL HLT 2019. Association for computational \nlinguistics (ACL); 2019; p. 4171–86.\nPage 9 of 9\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:235 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 7. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained \nbiomedical language representation model for biomedical text min-\ning. Bioinformatics. 2020;36(4):1234–40.\n 8. Peng Y, Yan S, Lu Z. Transfer learning in biomedical natural language \nprocessing: An evaluation of BERT and ELMo on ten benchmarking \ndatasets. In: Proceedings of the BioNLP 2019 workshop. 2019; p. 58–65.\n 9. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, Naumann T, Gao J, \nPoon H. Domain-specific language model pretraining for biomedical \nnatural language processing. ACM Trans Comput Healthc (HEALTH). \n2021;3(1):1–23.\n 10. Beltagy I, Lo K, Cohan A. SciBERT: a pretrained language model for sci-\nentific text. In: Proceedings of the 2019 conference on EMNLP-IJCNLP . \n2019; p. 3615–20.\n 11. Joshi M, Chen D, Liu Y, Weld DS, Zettlemoyer L, Levy O. SpanBERT: \nImproving pre-training by representing and predicting spans. Trans Assoc \nComput Linguist. 2019;8:64–77.\n 12. Kury F, Butler A, Yuan C, Fu L heng, Sun Y, Liu H, et al. Chia, a large anno-\ntated corpus of clinical trial eligibility criteria. Sci Data. 2020;7(1).\n 13. Armengol-Estapé J, Soares F, Marimon M, Krallinger M. PharmacoNER tag-\nger: a deep learning-based tool for automatically finding chemicals and \ndrugs in spanish medical texts. Genom Inform. 2019;17(2).\n 14. Wang Y, Wang L, Rastegar-Mojarad M, Moon S, Shen F, Afzal N, et al. \nClinical information extraction applications: a literature review. J Biomed \nInform. 2017;2018(77):34–49.\n 15. Wei Q, Ji Z, Li Z, Du J, Wang J, Xu J, Xiang Y, Tiryaki F, Wu S, Zhang Y, Tao \nC. A study of deep learning approaches for medication and adverse \ndrug event extraction from clinical text. J Am Med Inform Assoc. \n2020;27(1):13–21.\n 16. Uzuner Ö, South BR, Shen S, DuVall SL. 2010 i2b2/VA challenge on \nconcepts, assertions, and relations in clinical text. J Am Med Inform Assoc. \n2011;18(5):552–6.\n 17. Henry S, Buchan K, Filannino M, Stubbs A, Uzuner O. 2018 n2c2 shared \ntask on adverse drug events and medication extraction in electronic \nhealth records. J Am Med Inform Assoc. 2020;27(1):3–12.\n 18. Uzuner Ö, Solti I, Cadag E. Extracting medication information from clinical \ntext. J Am Med Inform Assoc. 2010;17(5):514–8.\n 19. Jiang M, Chen Y, Liu M, Rosenbloom ST, Mani S, Denny JC, et al. A study \nof machine-learning-based approaches to extract clinical entities and \ntheir assertions from discharge summaries. J Am Med Inform Assoc. \n2011;18(5):601–6.\n 20. Tang B, Cao H, Wu Y, Jiang M, Xu H. Clinical entity recognition using struc-\ntural support vector machines with rich features. In: Proceedings of the \nACM sixth international workshop on data and text mining in biomedical \ninformatics 2012; p. 13–20.\n 21. Giorgi JM, Bader GD. Transfer learning for biomedical named entity recog-\nnition with neural networks. Bioinformatics. 2018;34(23):4087–94.\n 22. Yang X, Bian J, Hogan WR, Wu Y. Clinical concept extraction using trans-\nformers. J Am Med Inform Assoc. 2020;27(12):1935–42.\n 23. Kim Y, Lee JH, Choi S, Lee JM, Kim JH, Seok J, et al. Validation of deep \nlearning natural language processing algorithm for keyword extrac-\ntion from pathology reports in electronic health records. Sci Rep. \n2020;10(1):1–9.\n 24. Tu SW, Peleg M, Carini S, Rubin D, Sim I. ERGO: a template-based expres-\nsion language for encoding eligibility criteria. Technical report, 2009. \n(Accessed 03/20/2022 from https:// stora ge. googl eapis. com/ google- \ncode- archi ve- downl oads/ v2/ code. google. com/ ontol ogy- of- clini cal- resea \nrch/ ERGO_ Techn ical_ Docum entat ion. pdf)\n 25. Tu SW, Musen MA. The EON model of intervention protocols and \nguidelines. In: Proceedings of the AMIA annual fall symposium. american \nmedical informatics association; 1996; p. 587.\n 26. Zhang K, Demner-Fushman D. Automated classification of eligibility cri-\nteria in clinical trials to facilitate patient-trial matching for specific patient \npopulations. J Am Med Inform Assoc. 2017;24(4):781–7.\n 27. Yuan C, Ryan PB, Ta C, Guo Y, Li Z, Hardin J, et al. Criteria2Query: a natural \nlanguage interface to clinical databases for cohort definition. J Am Med \nInform Assoc. 2019;26(4):294–305.\n 28. Zhu Y, Kiros R, Zemel R, Salakhutdinov R, Urtasun R, Torralba A, et al. Align-\ning books and movies: towards story-like visual explanations by watching \nmovies and reading books. In: Proceedings of the IEEE international \nconference on computer vision. 2015; p. 19–27.\n 29. Wu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W, Krikun M, Cao \nY, Gao Q, Macherey K, Klingner J. Google’s neural machine translation \nsystem: bridging the gap between human and machine translation. arXiv \npreprint arXiv: 1609. 08144. 2016.\n 30. Soysal E, Wang J, Jiang M, Wu Y, Pakhomov S, Liu H, et al. CLAMP–a toolkit \nfor efficiently building customized clinical natural language processing \npipelines. J Am Med Inform Assoc. 2018;25(3):331–6.\n 31. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. Hug-\ngingFace’s transformers: state-of-the-art natural language processing. \nIn: Proceedings of the 2020 EMNLP (Systems Demonstrations), 2020; p. \n38–45.\n 32. Stubbs A, Kotfila C, Uzuner Ö. Automated systems for the de-identifica-\ntion of longitudinal clinical narratives: overview of 2014 i2b2/UTHealth \nshared task Track 1. J Biomed Inform. 2015;58:S11–9.\n 33. Hogg RV, Tanis EA, Zimmerman DL. Probability and statistical inference. \nUpper Saddle River: Pearson/Prentice Hall; 2010.\n 34. Tang B, Chen Q, Wang X, Wu Y, Zhang Y, Jiang M, et al. Recognizing \ndisjoint clinical concepts in clinical text using machine learning-based \nmethods. In: AMIA annual symposium proceedings. American Medical \nInformatics Association; 2015; p. 1184.\n 35. Dahlmeier D, Ng HT. Domain adaptation for semantic role labeling in the \nbiomedical domain. Bioinformatics. 2010;26(8):1098–104.\n 36. Peng N, Dredze M. Multi-task domain adaptation for sequence tagging. \nIn: Proceedings of the 2nd workshop on representation learning for NLP , \n2016.\n 37. Lin BY, Lu W. Neural adaptation layers for cross-domain named entity rec-\nognition. In: Proceedings of the 2018 conference on empirical methods \nin natural language processing. 2018; p. 2012–22.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Named-entity recognition",
  "concepts": [
    {
      "name": "Named-entity recognition",
      "score": 0.7981308698654175
    },
    {
      "name": "Computer science",
      "score": 0.7943385243415833
    },
    {
      "name": "Natural language processing",
      "score": 0.7506084442138672
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6285363435745239
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5901955366134644
    },
    {
      "name": "Transformer",
      "score": 0.5261608958244324
    },
    {
      "name": "Language model",
      "score": 0.47667741775512695
    },
    {
      "name": "Clinical trial",
      "score": 0.452796071767807
    },
    {
      "name": "Machine learning",
      "score": 0.4466792643070221
    },
    {
      "name": "Information extraction",
      "score": 0.41826361417770386
    },
    {
      "name": "Task (project management)",
      "score": 0.3744334578514099
    },
    {
      "name": "Information retrieval",
      "score": 0.36164146661758423
    },
    {
      "name": "Medicine",
      "score": 0.1523716151714325
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}