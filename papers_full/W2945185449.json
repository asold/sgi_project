{
  "title": "What Syntactic Structures block Dependencies in RNN Language Models?",
  "url": "https://openalex.org/W2945185449",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288398382",
      "name": "Wilcox, Ethan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2511155128",
      "name": "Levy, Roger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3191941820",
      "name": "Futrell Richard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2952817251",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W2788924045",
    "https://openalex.org/W2087946919",
    "https://openalex.org/W2141845152",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2890027146",
    "https://openalex.org/W2125001590",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W2864832950",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2022741721",
    "https://openalex.org/W1595256356",
    "https://openalex.org/W2089034860"
  ],
  "abstract": "Recurrent Neural Networks (RNNs) trained on a language modeling task have been shown to acquire a number of non-local grammatical dependencies with some success. Here, we provide new evidence that RNN language models are sensitive to hierarchical syntactic structure by investigating the filler--gap dependency and constraints on it, known as syntactic islands. Previous work is inconclusive about whether RNNs learn to attenuate their expectations for gaps in island constructions in particular or in any sufficiently complex syntactic environment. This paper gives new evidence for the former by providing control studies that have been lacking so far. We demonstrate that two state-of-the-art RNN models are are able to maintain the filler--gap dependency through unbounded sentential embeddings and are also sensitive to the hierarchical relationship between the filler and the gap. Next, we demonstrate that the models are able to maintain possessive pronoun gender expectations through island constructions---this control case rules out the possibility that island constructions block all information flow in these networks. We also evaluate three untested islands constraints: coordination islands, left branch islands, and sentential subject islands. Models are able to learn left branch islands and learn coordination islands gradiently, but fail to learn sentential subject islands. Through these controls and new tests, we provide evidence that model behavior is due to finer-grained expectations than gross syntactic complexity, but also that the models are conspicuously un-humanlike in some of their performance characteristics.",
  "full_text": "What Syntactic Structures block Dependencies in RNN Language Models?\nEthan Wilcox1, Roger Levy2, and Richard Futrell3\n1Department of Linguistics, Harvard University,wilcoxeg@g.harvard.edu\n2Department of Brain and Cognitive Sciences, MIT, rplevy@mit.edu\n3Department of Language Science, UC Irvine, rfutrell@uci.edu\nAbstract\nRecurrent Neural Networks (RNNs) trained on a language\nmodeling task have been shown to acquire a number of non-\nlocal grammatical dependencies with some success (Linzen,\nDupoux, & Goldberg, 2016). Here, we provide new evidence\nthat RNN language models are sensitive to hierarchical syntac-\ntic structure by investigating the ﬁller–gap dependency and\nconstraints on it, known as syntactic islands. Previous work\nis inconclusive about whether RNNs learn to attenuate their\nexpectations for gaps in island constructions in particular or\nin any sufﬁciently complex syntactic environment. This paper\ngives new evidence for the former by providing control studies\nthat have been lacking so far. We demonstrate that two state-\nof-the-art RNN models are are able to maintain the ﬁller–gap\ndependency through unbounded sentential embeddings and are\nalso sensitive to the hierarchical relationship between the ﬁller\nand the gap. Next, we demonstrate that the models are able\nto maintain possessive pronoun gender expectationsthrough\nisland constructions—this control case rules out the possibil-\nity that island constructions block all information ﬂow in these\nnetworks. We also evaluate three untested islands constraints:\ncoordination islands, left branch islands, and sentential subject\nislands. Models are able to learn left branch islands and learn\ncoordination islands gradiently, but fail to learn sentential sub-\nject islands. Through these controls and new tests, we provide\nevidence that model behavior is due to ﬁner-grained expecta-\ntions than gross syntactic complexity, but also that the models\nare conspicuously un-humanlike in some of their performance\ncharacteristics.\nKeywords: Syntactic Islands, Recurrent Neural Networks,\nBlocking Effects, Acquisition of Syntax\nIntroduction\nRecurrent Neural Networks (RNNs) with Long Short-Term\nMemory architecture (LSTMs) have achieved state-of-the-\nart scores at a number of natural language processing tasks,\nincluding language modeling and parsing (Hochreiter &\nSchmidhuber, 1997; Jozefowicz, Vinyals, Schuster, Shazeer,\n& Wu, 2016). In addition, they have begun to be used\nas a plausible sub-symbolic model for a variety of cogni-\ntive functions, including visual perception and language pro-\ncessing and comprehension (J. Elman, 1990). However, the\ndistributed representations learned by RNNs and neural net-\nworks in general are notoriously opaque, posing a challenge\nfor their interpretability as models of human sentence pro-\ncessing and for their controllability as NLP systems.\nOne recent line of work aims to uncover what these ‘black\nboxes’ learn about language by treating them like human psy-\ncholinguistic subjects. In this psycholinguistic paradigm\nRNNs trained on the language modeling task are fed hand-\ncrafted sentences, designed to expose their underlying syntac-\ntic knowledge (Linzen et al., 2016; McCoy, Frank, & Linzen,\n2018). Much of this work has investigated what RNNs trained\non a language modeling objective are capable of learning\nabout natural syntactic dependencies. For the purposes of\nthis investigation, we deﬁne dependency as any systematic\nco-variation between two words. For example, in one experi-\nment networks were tested as to whether they had learned the\nnumber agreement dependency between a subject and a verb.\nThey were fed with the preﬁx The key to the cabinet... and\ncorrectly gave a higher probability to the grammatical is over\nthe ungrammatical are. Networks were shown to successfully\ncomplete this task for a number of languages, as well as for\nsentences whose content words were replaced with random\nalternatives of the same syntactic category rendering them\nsyntactically licit but semantically implausible (Gulordava,\nBojanowski, Grave, Linzen, & Baroni, 2018).\nBut learning that covariance exists between certain words\nor word forms, without reference to their relative positions,\nis not enough to say that the RNN models have fully learned\na dependency. Natural language dependencies consist of co-\nvariation between two elements incertain syntactic positions.\nAgents must both attend to the structural relationship between\nthe two elements bound by the dependency and ﬁlter out in-\ntervening material in syntactically irrelevant positions. The\nsubject–verb number agreement task above provides com-\npelling evidence that RNNs are capable of the latter: they\nwere able to maintain correct predictions despite a number\nof distractors that mismatched the subject in number, such as\ncabinet in the example provided (Marvin & Linzen, 2018).\nEvidence suggesting that RNN language models are also\nsensitive to the structural relationship between the two bound\nelements has emerged from the study of ﬁller–gap depen-\ndencies (Wilcox, Levy, Morita, & Futrell, 2018; Chowdhury\n& Zamparelli, 2018). The ﬁller–gap dependency is the depen-\ndency between a ﬁller—such as who or what—and and a gap,\nwhich is an empty syntactic position. Crucially, ﬁller–gap de-\npendencies are subject to a number of constraints, known as\nisland constraints, which are a set of structural positions that\nprevent the ﬁller and the gap from entering into a dependency\nwith each other (Ross, 1967). (1-b) gives one example island,\nin which the dependency is blocked by a wh-complementizer.\n(1) a. I know what the guide said that the lion devoured\nyesterday. NO VIOLATION\nb.*I know what the guide said whether the lion devoured\nyesterday. WH-ISLAND ISLAND VIOLATION\nWhile it has been shown that both simple Elman RNNs and\nmore contemporary LSTMs are able to represent the basic\ncovariance between ﬁllers and gaps, as well as other non-\nstructural aspects of dependency, it is still uncertain whether\narXiv:1905.10431v1  [cs.CL]  24 May 2019\nthe models are sensitive to island constraints (J. L. Elman,\n1991). Previous work has demonstrated that two state-of-the-\nart models are sensitive to three of the most-studied island\nconstraints (wh-islands, complex NP islands and adjunct is-\nlands) but insensitive to a fourth (subject islands) (Wilcox et\nal., 2018). Others have concluded that the models are merely\nsensitive to syntactic complexity plus order. Chowdhury and\nZamparelli (2018) compared sentence-level perplexity scores\nobtained by RNN LMs for wh-questions that violate island\nconstraints, and yes-no questions and statements that violate\nno grammatical rules but contain the same syntactic struc-\ntures. While the models obtained better perplexity scores\non the statements compared to the island-violation questions,\nthey performed similarly on the island-violations and non-\nviolating yes/no questions. These results may indicate that\nRNNs are not learning to attenuate their expectations for gaps\nin island constructions in particular, but in any sufﬁciently\ncomplex syntactic environment.\nThis paper adjudicates between these two accounts of\nmodel behavior by providing control studies that have been\nlacking so far. In the ﬁrst section, we demonstrate that two\nstate-of-the-art LSTM models are sensitive to some forms\nof syntactic complexity, but not to others. Models are able\nto maintain the ﬁller–gap dependency through unbounded\nsentential embeddings and yet are sensitive to the hierar-\nchical relationship between the ﬁller and the gap, suggest-\ning that only speciﬁc types of syntactic complexity block\ngap expectations. In the second section, we turn to posses-\nsive pronoun gender dependencies , demonstrating that the\nmodels are able to maintain general expectations through is-\nland constructions—it is not the case that island constructions\nblock all information ﬂow in these networks. In this section\nwe also evaluate three untested islands constraints: coordi-\nnation islands, left branch islands , and sentential subject\nislands. Models are able to learn left branch islands and coor-\ndination islands gradiently, but fail to learn sentential subject\nislands. Through these controls and new tests, we provide\nevidence that model behavior is due to ﬁner-grained expecta-\ntions than gross syntactic complexity, but also that the mod-\nels are conspicuously un-humanlike in some of their perfor-\nmance characteristics.\nMethods\nLanguage Models\nWe assess two state-of-the-art pre-existing LSTM models\ntrained on English text for a language modeling objective.\nThe ﬁrst model, which we refer to as the Google Model ,\nwas trained on the One Billion Word Benchmark and has\ntwo hidden layers with 8196 units each. It uses the output\nof a character-level convolutional neural network (CNN) as\ninput to the LSTM (and was originally presented as the BIG\nLSTM+CNN Inputs) (Jozefowicz et al., 2016). The second\nmodel, which we refer to as the Gulordava Model was se-\nlected for its previous success at learning the subject-verb\nnumber agreement task. It was trained on 90 Million tokens\nof English Wikipedia, and has two hidden layers of 650 units\neach (Gulordava et al., 2018).\nDependent Measure: Surprisal\nIn this work we take a grammatical dependency to be the co-\nvariance between an upstream licensor and a downstream li-\ncensee. We assess the model’s knowledge of the dependency\nby measuring the effect that the licensor has on the surprisal\nof the licensee, or on material immediately following the li-\ncensee when it is a gap. Surprisal, or negative log-conditional\nprobability , S(xi) of a sentence’s ith word xi, tells us how\nstrongly xi is expected under the language model’s probabil-\nity distribution. For sentences out of context, the surprisal is:\nS(xi) =−log p(xi|x1 ... xi−1). Surprisal is known to correlate\ndirectly with processing difﬁculty in humans (Smith & Levy,\n2013; Hale, 2001; Levy, 2008). In this work, we expect that\ngrammatical licensors set up expectations for licensee, reduc-\ning its surprisal compared to minimal pairs in which the licen-\nsor is absent. We derive the word surprisal from the LSTM\nlangauge model by directly computing the negative log of\nthe predicted conditional probability p(xi|x1 ... xi−1) from the\nsoftmax layer.\nExperimental Design: Wh-Licensing Interaction\nThe ﬁller–gap dependency is biconditional: Fillers set up ex-\npectations for gaps and gaps require ﬁllers to be licensed. To\nmeasure this bi-directionality we employ the 2x2 interaction\ndesign proposed in Wilcox et al.. There, the authors mea-\nsure the wh-licensing interaction, which they compute from\nfour sentence variants, given in (2), that contain the four pos-\nsible combinations of ﬁllers and gaps for a speciﬁc syntac-\ntic position. Note that the underscores are for presentational\npurposes only, and were not included in test items. Subse-\nquent examples will be given via the (2-d) example, but all\nfour variants were created in order to compute the licensing\ninteraction.\n(2) a. I know that you insulted your aunt yesterday. [-F ILLER -\nGAP]\nb. *I know who you insulted your aunt yesterday. [+F ILLER\n-GAP]\nc. *I know that you insulted yesterday. [-F ILLER +GAP]\nd. I know who you insulted yesterday. [+F ILLER +GAP]\nIf the ﬁller sets up an expectation for a gap, then the ﬁlled\nsyntactic position where a gap would typically occur should\nbe more surprising in contexts that contain an upstream ﬁller.\nThat is S(b) −S(a) should be a large positive number. If\nthe gap requires a ﬁller to be licensed, then the transition\nfrom the embedded verb to the S-modifying PP ‘yesterday’\nthat skips over the otherwise-required grammatical object\nshould be more surprising in contexts without an upstream\nﬁller. That is, S(d) −S(c) should also be a large negative\nnumber. We can assess how well the model has learned\nboth expectations by measuring the difference of differences:\n[S(b)−S(a)]−[S(d)−S(c)]. This is the wh-licensing interac-\ntion. If the models are learning the ﬁller–gap dependency, we\nexpect this to be a large positive number, with typical models\nshowing about 4 bits of licensing interaction in simple object\nextracted clauses such as (2). Although we might expect the\nα\nηβ\nδ\nζε\nγ\nFigure 1: C-Command in a binary-branching tree structure. γ\nc-commands all the nodes in blue, but does not c-command\nthe black nodes.\nstrongest difference in surprisal between (2-a) and (2-b) to be\non the ﬁlled-gap position, your aunt, this material is elided in\ntwo of the conditions. Therefore, in order to keep the mea-\nsurement site the same across all four conditions, we measure\nwh-licensing interaction in the post-gap prepositional phrase\n(‘yesterday’ in (2)).\nIn previous work using this methodology, RNN knowledge\nof island constraints was assessed by comparing the licensing\ninteraction in island conﬁgurations to that in non-island min-\nimal pairs. Strong evidence for an island constraint would be\nif the wh-licensing interaction dips to zero for a gap in island\nposition, indicating that the model has decoupled expecta-\ntions for ﬁllers from gaps in this position. In practice we look\nfor a signiﬁcant decrease in wh-licensing interaction as indi-\ncation that the models have learned to attenuate their expecta-\ntions for gaps within islands. We derive the statistical signiﬁ-\ncance of the interaction from a mixed-effects linear regression\nmodel, using some-coded conditions (Baayen, Davidson, &\nBates, 2008). We include random intercepts by item but omit\nrandom slopes as we do not have repeated observations within\nitems and conditions (Barr, Levy, Scheepers, & Tily, 2013).\nIn our ﬁgures, error bars represent 95% conﬁdence intervals\nof the contrasts between conditions, computed by subtract-\ning out the by-item means before calculating the intervals as\nadvocated in (Masson & Loftus, 2003). 1\nSyntactic Complexity\nUnboundedness\nThe ﬁller–gap dependency can span through a potentially un-\nbounded number of sentential embeddings. To test whether\nmodels’ expectations were attenuated with greater embed-\nding depth, we created 23 items in ﬁve experimental condi-\ntions with between 0 and 4 layers of embedding and gaps in\neither object or indirect object (goal) position, following the\nexamples in (3), and measured the licensing interaction in the\npost-gap material. (In this and subsequent examples, the ma-\nterial in which the interaction is measured will be highlighted\nin bold.)\n(3) a. I know who you insulted at the party. [OBJECT GAP ,\n0 LAYERS ]\n1Our studies were preregistered on aspredicted.org: To\nsee the preregistrations go to aspredicted.org/blind.php?=X\nwhere X ∈{sz8f5d,2r2eu7,zt73qt,es8rx7,f9pk9f,se6i2e}.\nb. I know who the gardener reported the butler said the\nhostess believed her aunt suspected you insulted at\nthe party. [OBJECT GAP , 4 LAYERS ]\nc. I know who you delivered a challenge to at the\nparty. [GOAL GAP , 0 LAYERS ]\nd. I know who the gardener reported the butler said the\nhostess believed her aunt suspected you delivered a\nchallenge to at the party. [GOAL GAP , 4 LAYERS ]\nThe results for this experiment can be seen in ﬁgure 2, with\nthe object gap results on the top and goal gap results on\nthe bottom. First, we ﬁnd a signiﬁcant interaction between\nﬁllers and gaps resulting in supperaditive reduction of sur-\nprisal (p < 0.001 for all conditions) indicating that both mod-\nels have learned the ﬁller–gap dependency. Starting with the\nobject gap conditions: For the google model, we ﬁnd no\neffect of embedding depth on the wh-licensing interaction\n(p > 0.85 in all cases); for the gulordava model, we ﬁnd a\nsigniﬁcant decrease in wh-licensing interaction only between\nthe no embedding conditions and conditions with 3 or 4 ad-\nditional layers of embedding ( p < 0.001 in both). When the\ngap occurs in the goal position, for the google model, we ﬁnd\nno signiﬁcant effect of embedding depth of the wh-licensing\ninteraction. For the gulordava model, we ﬁnd a generally\nsmaller wh-licensing interaciton, as well as a signiﬁcant ef-\nfect of embedding between the no embedding condition and\nconditions with two or more additional embedding layers\n(p < 0.05, p < 0.05, p < 0.01 for 2 ,3 and 4 layers). We take\nthese results to indicate that the google model has learned the\nunboundedness of the ﬁller–gap dependency whereas the gu-\nlordava model has learned only relative unboundedness and\nshows behavior that reﬂects human performance more than\nhuman competence. However, these results indicate that both\nmodels can, in principle, thread their expectations for gaps\nthrough complex syntactic structures, if we take the number\nof syntactic nodes as a proxy measure for syntactic complex-\nity.\nSyntactic Hierarchy\nAlthough the ﬁller–gap dependency is unbounded, it is sub-\nject to a number of hierarchical constraints, the most basic\nof which is that the ﬁller must be “above” the gap, struc-\nturally. Here, we take this to mean that the ﬁller must c-\ncommand the gap, although the precise relationship is more\ncomplex (Pollard & Sag, 1994). Structurally-speaking node γ\nc-commands node δ if neither node directly dominates the\nother and every node X that dominates γ also dominates\nδ. Figure 1 demonstrates this relationship, with the noes c-\ncommanded by γ highlighted in blue.\nTo assess whether the models had learned this constraint\non the structural relationship we created 24 variants following\nthe examples in (4) and measured the wh-licensing interaction\nin the post-gap PP. If the model has learned the structural con-\nstraints on the ﬁller–gap dependency, an undischarged ﬁller in\nthe matrix clause should not make a gap in subsequent parts\nof the sentence more or less likely, leading to near-zero li-\ncensing interaction in the Matrix Clause condition.\ngoogle gulordava\n0 1 2 3 4 0 1 2 3 4\n0\n1\n2\n3\nNumber of Embeddings\nWh−Licensing Interaction\nUnboundedness: Object Gap\ngoogle gulordava\n0 1 2 3 4 0 1 2 3 4\n0\n2\n4\n6\nNumber of Embeddings\nWh−Licensing Interaction\nUnboundedness: Goal Gap\ngoogle gulordava\nSubject \n Clause\nMatrix \n Clause\nSubject \n Clause\nMatrix \n Clause\n0\n2\n4\nSyntactic Hierarchy\nFigure 2: Effect of sentential embedding and syntactic hierarchy on wh-licensing interaction.\n(4) a. The fact that the mayor knows who the criminal shot\nshocked the jury during the trial. [SUBJECT ]\nb.*The fact that the mayor knows who the criminal shot\nthe teller shocked during the trial. [MATRIX ]\nThe results from this experiment can be seen in Figure 2, on\nthe far right panel. We ﬁnd strong licensing interaction for the\ngrammatical Subject Clause conditions (in red), but a strik-\ning reduction in licensing interaction for the Matrix Clause\nconditions (in blue), which is signiﬁcant for both models\n(p < 0.001). As the results in (2) and Wilcox et al. have\nshown that RNN models are insensitive to linear distance be-\ntween the ﬁller and the gap, we take these results suggest that\nit is the relevant structural properties which block the models’\nexpectations for gaps inside the matrix clause.\nIsland Effects: Gender Expectation vs.\nFiller–Gap Dependency\nIsland constraints are speciﬁc syntactic conﬁgurations that\nblock the ﬁller–gap dependency. One way to show that the\nRNN models are learning island conditions as constraints on\nthe ﬁller–gap dependency is to demonstrate that they are ca-\npable of threading other expectations into island conﬁgura-\ntions. To do this, we used pronoun gender expectation be-\ntween a gendered noun, such as ‘actress’ or ‘husband’, and a\npossessive pronoun such as ‘his’ or ‘her.’. Nouns that carry\novert gender marking or culturally-imbued gender bias set\nup expectations that subsequent pronominals match them in\ngender. Previous work has shown that humans thread ex-\npectations set up by cataphoric pronouns into syntactic is-\nlands (Yoshida, Kazanina, Pablos, & Sturt, 2014). Cataphoric\npronouns are pronouns that precede the nominal element to\nwhich they refer, as in (5).\n(5) Her manager revealed that the studio notiﬁed Judy\nDench about the new ﬁlm.\nBecause cataphoric pronouns are relatively less frequent than\nanaphoric pronouns, which follow the nominal to which they\nrefer, we use sentences such as those in (6) to assess whether\nRNN LMs can thread expectations into island environments.\nWe measure the strength of the gender expectation by calcu-\nlating the difference in surprisal between the matching con-\ndition and the mismatching condition, or S((6-b))-S((6-a)). If\nthe models attenuate their expectation for gender agreement\nin island positions, then we expect an interaction between\nMISMATCH and ISLAND resulting in supperaditivally lower\nsurprisal.\n(6) a. The actress said that they insulted her friends.\n[MATCH , CONTROL ]\nb.#The actress said that they insulted his friends. [MIS -\nMATCH , CONTROL ]\nc. The actress said whether they insulted her friends.\n[MATCH , ISLAND ]\nd.#The actress said whether they insulted his friends.\n[MISMATCH , ISLAND ]\nIn order to test whether the models maintained their gen-\nder expectations through island constructions, we created six\nsuites of experiments following the pattern of (6) for six of\nthe most frequently studied islands constructions. For each of\nthe gender expectation experiments, we created 30 variants,\n15 with masculine subjects and 15 with feminine subjects and\nmeasured the surprisal at the possessive pronoun. The results\nare presented on the bottom row in Figure 3 alongside model\nperformance on the ﬁller–gap dependency for the same syn-\ntactic constructions (top row). For the ﬁller–gap dependency,\nresults for four islands had already been tested in Wilcox et al.\n(2018), which we present alongside novel results for Coordi-\nnation Islands, Sentential Subject Islands and Left-Branch Is-\nlands, the latter separately without a gender expectation con-\ntrol. For these experiments, we created between 20-24 ex-\nperimental items and measured the wh-licensing interaction\nin the post-gap material. We take a reduction in wh-licensing\ninteraction in island constructions and no such reduction in\nthe gender expectation as evidence that the model has both\nlearned the island constraint, and has applied that constraint\nuniquely to the ﬁller–gap dependency.\nWh-Islands The wh-constraint states that the ﬁller–gap\ndependency is blocked by S-nodes introduced by a wh-\ncomplimentizer, as demonstrated in the unacceptability of\n(7-b) compared to (7-a). We created experimental items fol-\nlowing the examples in (7) and measured their gender expec-\ntation and ﬁller–gap dependency (ﬁller–gap dependency ma-\nterials were taken from Wilcox et al.).\n(7) a. I know who Alex said your friend insulted yester-\nday. [CONTROL , FILLER –GAP ]\nb.*I know who Alex said whether your friend insulted\nyesterday. [ISLAND , FILLER –GAP ]\nc. The actress said they insulted {his/her} friends.\n[CONTROL , GENDER EXP .]\nd. The actress said whether they insulted {his/her}\ngoogle gulordava\n0\n1\n2\n3\n4Wh−Licensing Interaction\nWh Islands\ngoogle gulordava\n0\n2\n4\nAdjunct Islands\ngoogle gulordava\n0\n1\n2\n3\n4\nComplex NP Islands\ngoogle gulordava\n0\n2\n4\n6\nCoordination Isl.\ngoogle gulordava\n0.0\n2.5\n5.0\n7.5\nSubject Islands\ngoogle gulordava\n0\n1\n2\n3\n4\n5\nSentential Subj.\ngoogle gulordava\n0\n1\n2\n3\n4\n5Gendered Expectation Effect\ngoogle gulordava\n0\n2\n4\ngoogle gulordava\n0\n1\n2\n3\n4\n5\ngoogle gulordava\n0\n1\n2\n3\n4\n5\ngoogle gulordava\n0\n2\n4\ngoogle gulordava\n0\n1\n2\n3\n4\n5\nFigure 3: Effect of island construction on gender dependency.\nfriends. [ISLAND , GENDER EXP .]\nThe results for this experiment can be seen in the far left panel\nof Figure 3, with island structures graphed in blue and non-\nisland controls in red. We ﬁnd a signiﬁcant difference in li-\ncensing interaction between the island and non-island condi-\ntions for both the google and gulordava models (p < 0.001 for\nboth models), but no such difference in gender expectation.\nAdjunct Islands Gaps cannot be licensed inside an ad-\njunct clause, as demonstrated by the relative unacceptability\nof (8-a) over (8-b).\n(8) a. I know what the librarian placed on the wrong\nshelf. [CONTROL , FILLER –GAP ]\nb.*what the patrong got mad after the librarian placed\non the wrong shelf. [ISLAND , FILLER –GAP ]\nc. The actress thinks they insulted {his/her} perfor-\nmance [CONTROL , GENDER EXP .]\nd. The actress got mad after they insulted {his/her}per-\nformance. [ISLAND , GENDER EXP .]\nThe results for this experiment can be seen in Figure 3, sec-\nond panel from the left. We ﬁnd a signiﬁcant reduction of\nwh-licensing interaction between the control and island con-\nditions in the case of the ﬁller–gap dependency for both mod-\nels ( p < 0.001 google; p < 0.01 gulordava; materials taken\nfrom ]Wilcox et al.). However, we ﬁnd no effect of syntactic\nstructure on the gender effect.\nComplex NP Islands Gaps are not licensed inside S-nodes\nthat are dominated by a lexical head noun, as demonstrated by\nthe relative badness of (9-b) compareid to (9-a).\n(9) a. I know what the actress bought yesterday. [CON -\nTROL , FILLER –GAP ]\nb.*I know what the actress bought the painting that de-\npicted yesterday. [ISLAND , FILLER –GAP ]\nc. The actress said they saw her {his/her}performance.\n[CONTROL , GENDER EXP .]\nd. The actress said they saw the exhibit that featured\n{his/her}performance. [ISLAND , GENDER EXP .]\nWe created items follwing the examples in (9), with ﬁller–\ngap items adopted from (Wilcox et al., 2018). The results\nfrom this experiment can be found in the middle-left panel\nof Figure 3. We found an effect of syntactic location on\nwh-licensing interaction for both models ( p < 0.001 google;\np < 0.01 gulordava) but no such interaction for gender expec-\ntations.\nCoordination Islands The coordination constraint states\nthat a gap cannot occur in one half of a coordinate structure\nas demonstrated by the difference between (10-b) and (10-a),\nin which a whole conjunct has been gapped.\n(10)a. I know what the man bought at the antique shop .\n[CONTROL , FILLER –GAP ]\nb.*I know what the man bought the painting and at the\nantique shop. [ISLAND , FILLER –GAP ]\nc. The ﬁreman knows they talked about {his/her}per-\nformance. [CONTROL , GENDER EXP .]\nd. The ﬁreman knows they talked about the football\ngame and {his/her}performance. [ISLAND , GENDER\nEXP.]\nWe created experimental items following the examples in\n(10). Results can be seen in 3 center-right panel. For the\nﬁller–gap dependency, in both models there is a signiﬁcant\ndifference between the control condition and island condi-\ntions (p < 0.05 for both models). These results indicate that\nthe models have somewhat attenuated expectations for gaps\nwhen they occur in the second half of a coordinate struc-\nture. However, note that, at least for the google model, the\nwh-licensing interaction is signiﬁcantly greater than zero, in-\ndicating that this model still maintains some expectation for\ngaps in this syntactic location. For both models there is no\ndifference in gender expectation between the control and is-\nland conditions).\nSubject Islands Gaps are generally licensed in preposi-\ntional phrases, except when they occur attached to sentential\nsubjects. We created experimental items following the exam-\nples in (11), with ﬁller–gap materials adapted from Wilcox et\nal..\n(11)a. I know what fetched a high price. [CONTROL ,\nFILLER -GAP ]\nb.*I know who the painting that depicted fetched a\nhigh price. [ISLAND , FILLER –GAP ]\nc. The actress said they sold the painting by {his/her}\nfriend. [CONTROL , GENDER EXP .]\nd. The actress said the painting by {his/her}friend sold\nfor a lot of money. [ISLAND , GENDER EXP .]\nThe results from this experiment can be seen in Figure 3, sec-\nond panel from the right. For the ﬁller–gap dependency, we\nfound a signiﬁcant difference between the control and island\ncondition in the case of the gulordava model ( p < 0.01), but\nno such reduction in the case of the google model. For gen-\nder expectation, we found no signiﬁcant difference between\nthe two conditions.\nSentential Subject Islands The sentential subject con-\nstraint states that gaps are not licensed within an S-node that\nplays the role of a sentential subject. To assess whether the\nRNN models had learned this constraint we created items fol-\nlowing the variants in (12).\n(12)a. I know who the seniors defeated last week. [CON -\nTROL , FILLER –GAP ]\nb. I know who for the seniors to defeat will be trivial.\n[ISLAND , FILLER –GAP ]\nc. The ﬁreman knows they will save {his/her}friend.\n[CONTROL , GENDER EXP .]\nd. The ﬁreman knows for them to save {his/her}friend\nwill be difﬁcult. [ISLAND , GENDER EXP .]\nThe results for this experiment can be seen in Figure 3, in the\nfar right panel. We found no decrease in gender expectation\nbetween the control and island conditions for either model.\nLikewise, for the ﬁller–gap dependency we found no signiﬁ-\ncant decrease in wh-licesning interaction between the island\nand non island conditions in either model. These results in-\ndicate that neither model suspends its expectations for gaps\nwithin sentential subjects.\nLeft Branch Islands The left-branch constraint states that\nmodiﬁers which appear on the left branch under an NP cannot\nbe gapped, which accounts for the relative ungrammaticality\nof (13-b) compared to (13-a). Because possessive pronouns\ncannot grammatically occur in left-branches under an NP, this\nexperiment examines only the ﬁller–gap dependency. We cre-\nated 20 items following the examples in (13) and measured\nthe wh-licensing interaction in the post-gap material.\ngoogle gulordava\nWhole \n Object\nLeft \n Branch\nWhole \n Object\nLeft \n Branch\n0\n1\n2\n3\n4Licensing Interaction\nLeft Branch Islands\nFigure 4: Left Branch Islands.\n(13)a. I know what color car you bought last week .\n[WHOLE OBJECT ]\nb. I know what color you bought car last week. [LEFT\nBRANCH ]\nThe results from this experiment can be seen in Figure 4 with\nexperimental conditions on the x-axis and wh-licensing inter-\naction on the y-axis. We see strong wh-licensing interaction\nin the two whole object conditions, but a signiﬁcant reduction\nin licensing interaction when the gap consists of the Adjective\nPhrase modiﬁer ( p < 0.001 for the google model; p < 0.05\nfor the gulordava model). This results indicate that the mod-\nels have learned the left branch islands, insofar as they do not\nexpect left-branching modiﬁers to be extracted without the\nNP to which they are attached.\nFor every condition tested we found that the expectation\nset up by gendered subjects for possessive pronouns is not\naffected by the pronoun’s location inside island constructions.\nFor the three novel structures, we found that the two models\ntested are sensitive to left branch islands and gradiently to\ncoordination islands, but not to sentential subject islands.\nDiscussion\nThe ﬁller–gap dependency has been the focus of intense re-\nsearch for over ﬁfty years because it is both far reaching and\ntightly constrained. It can be threaded through a potentially\nunbounded number of sentential embeddings; yet the ﬁller\nmust syntactically dominate the gap and the dependency is\nsubject to a number of highly-speciﬁc blocking ‘island’ con-\nditions. In this work we have shown that RNNs trained on\na language modeling objective have learned both the power\nand the constraints imposed on this dependency. First, we\nprovided evidence that they are able to thread the dependency\nthrough an unbounded number of sentential embeddings, and\nhave also learned the constraints that govern the syntactic hi-\nerarchy of the ﬁller relative to the gap.\nSecond, using gender expectation effects, we have demon-\nstrated that the models are able to thread some contextually-\ndependent expectations into island constructions, providing\nevidence that previously-observed island effects have been\nlearned for the ﬁller–gap dependency in particular, and are\nnot due to the model’s inability to thread any information\ninto syntactic islands. In addition, we have increased the ex-\nperimental coverage of island effects, demonstrating that the\nmodels were able to learn left-branch islands and gradiently\nlearn coordination islands, but failed to learn sentential sub-\nject islands. This brings the total number of islands learned\nto 5/7 for the google model and 6/7 for the gulordava model.\nAlthough some of the model behavior remains strikingly un-\nlike human acceptability judgements (in e.g. coordination is-\nlands), these experiments demonstrate that sequence models\ntrained on a language modeling objective are able to sepa-\nrate natural language dependencies from each other and learn\ndifferent ﬁne-grained syntactic rules for each.\nReferences\nBaayen, R. H., Davidson, D., & Bates, D. M. (2008). Mixed-\neffects modeling with crossed random effects for sub-\njects and items. Journal of Memory and Language ,\n59(4), 390–412.\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013).\nRandom effects structure for conﬁrmatory hypothesis\ntesting: Keep it maximal. Journal of Memory and Lan-\nguage, 68(3), 255–278.\nChowdhury, S. A., & Zamparelli, R. (2018). Rnn simulations\nof grammaticality judgments on long-distance depen-\ndencies. In Proceedings of the 27th international con-\nference on computational linguistics (pp. 133–144).\nElman, J. (1990). Finding structure in time. Cognitive Sci-\nence, 14(2), 179–211.\nElman, J. L. (1991). Distributed representations, simple re-\ncurrent networks, and grammatical structure. Machine\nlearning, 7(2-3).\nGulordava, K., Bojanowski, P., Grave, E., Linzen, T., & Ba-\nroni, M. (2018). Colorless green recurrent networks\ndream hierarchically. In Proceedings of naacl.\nHale, J. T. (2001). A probabilistic Earley parser as a psy-\ncholinguistic model. In Proceedings of the second\nmeeting of the north american chapter of the associ-\nation for computational linguistics and language tech-\nnologies (pp. 1–8).\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term\nmemory. Neural Computation, 9(8), 1735–1780.\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., & Wu,\nY . (2016). Exploring the limits of language modeling.\narXiv, 1602.02410.\nLevy, R. (2008). Expectation-based syntactic comprehen-\nsion. Cognition, 106(3), 1126–1177.\nLinzen, T., Dupoux, E., & Goldberg, Y . (2016). Assessing\nthe ability of LSTMs to learn syntax-sensitive depen-\ndencies. Transactions of the Association for Computa-\ntional Linguistics, 4, 521–535.\nMarvin, R., & Linzen, T. (2018). Targeted syntac-\ntic evaluation of language models. arXiv preprint\narXiv:1808.09031.\nMasson, M. E., & Loftus, G. R. (2003). Using conﬁdence in-\ntervals for graphically based data interpretation. Cana-\ndian Journal of Experimental Psychology/Revue cana-\ndienne de psychologie exp´erimentale, 57(3), 203.\nMcCoy, R. T., Frank, R., & Linzen, T. (2018). Revisit-\ning the poverty of the stimulus: hierarchical general-\nization without a hierarchical bias in recurrent neural\nnetworks. arXiv preprint arXiv:1802.09091.\nPollard, C., & Sag, I. A. (1994). Head-driven phrase struc-\nture grammar. Stanford, CA: Center for the Study of\nLanguage and Information.\nRoss, J. R. (1967). Constraints on variables in syntax.\nSmith, N. J., & Levy, R. (2013). The effect of word pre-\ndictability on reading time is logarithmic. Cognition,\n128(3), 302–319.\nWilcox, E., Levy, R., Morita, T., & Futrell, R. (2018). What\ndo rnn language models learn about ﬁller-gap depen-\ndencies? arXiv preprint arXiv:1809.00042.\nYoshida, M., Kazanina, N., Pablos, L., & Sturt, P. (2014). On\nthe origin of islands. Language, Cognition and Neuro-\nscience, 29(7), 761–770.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6850476861000061
    },
    {
      "name": "Dependency (UML)",
      "score": 0.6257738471031189
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6089746952056885
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5566617846488953
    },
    {
      "name": "Syntax",
      "score": 0.5215027928352356
    },
    {
      "name": "Natural language processing",
      "score": 0.5209916234016418
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.5085829496383667
    },
    {
      "name": "Subject (documents)",
      "score": 0.4252329468727112
    },
    {
      "name": "Linguistics",
      "score": 0.37940332293510437
    },
    {
      "name": "Artificial neural network",
      "score": 0.1914355754852295
    },
    {
      "name": "Mathematics",
      "score": 0.12597283720970154
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Library science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204250578",
      "name": "University of California, Irvine",
      "country": "US"
    }
  ]
}