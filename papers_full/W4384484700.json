{
  "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
  "url": "https://openalex.org/W4384484700",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5074702297",
      "name": "Jason Holmes",
      "affiliations": [
        "Mayo Clinic in Florida",
        "WinnMed"
      ]
    },
    {
      "id": "https://openalex.org/A5101505879",
      "name": "Zhengliang Liu",
      "affiliations": [
        "University of Georgia"
      ]
    },
    {
      "id": "https://openalex.org/A5103500594",
      "name": "Lian Zhang",
      "affiliations": [
        "Mayo Clinic in Florida",
        "WinnMed"
      ]
    },
    {
      "id": "https://openalex.org/A5102709182",
      "name": "Yuzhen Ding",
      "affiliations": [
        "Mayo Clinic in Florida",
        "WinnMed"
      ]
    },
    {
      "id": "https://openalex.org/A5032470889",
      "name": "Terence T. Sio",
      "affiliations": [
        "Mayo Clinic in Florida",
        "WinnMed"
      ]
    },
    {
      "id": "https://openalex.org/A5113880026",
      "name": "L.A. McGee",
      "affiliations": [
        "Mayo Clinic in Florida",
        "WinnMed"
      ]
    },
    {
      "id": "https://openalex.org/A5089819634",
      "name": "Jonathan B. Ashman",
      "affiliations": [
        "Mayo Clinic in Florida",
        "WinnMed"
      ]
    },
    {
      "id": "https://openalex.org/A5100331094",
      "name": "Xiang Li",
      "affiliations": [
        "Massachusetts General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5100647156",
      "name": "Tianming Liu",
      "affiliations": [
        "University of Georgia"
      ]
    },
    {
      "id": "https://openalex.org/A5032704742",
      "name": "Jiajian Shen",
      "affiliations": [
        "Mayo Clinic in Florida",
        "WinnMed"
      ]
    },
    {
      "id": "https://openalex.org/A5100758871",
      "name": "Wei Liu",
      "affiliations": [
        "Mayo Clinic in Florida",
        "WinnMed"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6851646166",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6781031682",
    "https://openalex.org/W6843243532",
    "https://openalex.org/W4285601618",
    "https://openalex.org/W6848909144",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4312318847",
    "https://openalex.org/W6851077998",
    "https://openalex.org/W6849497746",
    "https://openalex.org/W6850668563",
    "https://openalex.org/W6810817042",
    "https://openalex.org/W4366808937",
    "https://openalex.org/W4220967417",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6811340617",
    "https://openalex.org/W6767858076",
    "https://openalex.org/W6843975101",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W6809583738",
    "https://openalex.org/W6850202480",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W6838431322",
    "https://openalex.org/W2064712772",
    "https://openalex.org/W3011832213",
    "https://openalex.org/W2770163048",
    "https://openalex.org/W2043570794",
    "https://openalex.org/W6850246880",
    "https://openalex.org/W6846981568",
    "https://openalex.org/W6849817994",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4205403018",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4402512745",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4402582321",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4321018175",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4390971130",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4385574286",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385757404",
    "https://openalex.org/W4353007316",
    "https://openalex.org/W4295492613",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4362693613",
    "https://openalex.org/W4298181573",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4383374753"
  ],
  "abstract": "Purpose We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. Methods We developed an exam consisting of 100 radiation oncology physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. The performance of ChatGPT (GPT-4) was further explored by being asked to explain first, then answer. The deductive reasoning capability of ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct answer with “None of the above choices is the correct answer.”). A majority vote analysis was used to approximate how well each group could score when working together. Results ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on average, with improved accuracy when prompted to explain before answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups or Bard (LaMDA). In evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. Conclusion This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.",
  "full_text": "Evaluating large language\nmodels on a highly-specialized\ntopic, radiation oncology physics\nJason Holmes1, Zhengliang Liu2, Lian Zhang1, Yuzhen Ding1,\nTerence T. Sio1, Lisa A. McGee1, Jonathan B. Ashman1,\nXiang Li3, Tianming Liu2, Jiajian Shen1* and Wei Liu1*\n1Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States,2School of Computing,\nThe University of Georgia, Athens, GA, United States,3Department of Radiology, Massachusetts\nGeneral Hospital, Boston, MA, United States\nPurpose: We present theﬁrst study to investigate Large Language Models (LLMs) in\nanswering radiation oncology physics questions. Because popular exams like AP\nPhysics, LSAT, and GRE have large test-taker populations and ample test preparation\nresources in circulation, they may not allow for accurately assessing the true\npotential of LLMs. This paper proposes evaluating LLMs on a highly-specialized\ntopic, radiation oncology physics, which may be more pertinent to scientiﬁca n d\nmedical communities in addition tobeing a valuable benchmark of LLMs.\nMethods: We developed an exam consisting of 100 radiation oncology physics\nquestions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT\n(GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical\nphysicists and non-experts. The performance of ChatGPT (GPT-4) was further\nexplored by being asked to explainﬁrst, then answer. The deductive reasoning\ncapability of ChatGPT (GPT-4) was evaluated using a novel approach\n(substituting the correct answer with “None of the above choices is the\ncorrect answer.”). A majority vote analysis was used to approximate how well\neach group could score when working together.\nResults: ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on\naverage, with improved accuracy when prompted to explain before answering.\nChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer\nchoices across a number of trials, whether correct or incorrect, a characteristic\nthat was not observed in the human test groups or Bard (LaMDA). In evaluating\ndeductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy,\nsuggesting the potential presence of an emergent ability. Finally, although\nChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow\nfor further improvement when scoring based on a majority vote across trials. In\ncontrast, a team of medical physicists were able to greatly outperform ChatGPT\n(GPT-4) using a majority vote.\nConclusion: This study suggests a great potential for LLMs to work alongside\nradiation oncology experts as highly knowledgeable assistants.\nKEYWORDS\nlarge language model, natural langua ge processing, medical physics, arti ﬁcial\nintelligence, ChatGPT\nFrontiers inOncology frontiersin.org01\nOPEN ACCESS\nEDITED BY\nKe Nie,\nRutgers, The State University of New\nJersey, United States\nREVIEWED BY\nJaya Lakshmi Thangaraj,\nUniversity of California, San Diego,\nUnited States\nYang Zhang,\nUniversity of California, Irvine, United States\n*CORRESPONDENCE\nJiajian Shen\nshen.jiajian@mayo.edu\nWei Liu\nliu.wei@mayo.edu\nRECEIVED 08 May 2023\nACCEPTED 12 June 2023\nPUBLISHED 17 July 2023\nCITATION\nHolmes J,Liu Z,Zhang L,Ding Y,\nSio TT,McGee LA,Ashman JB,Li X,Liu T,\nShen J andLiu W (2023) Evaluating large\nlanguage models on a highly-specialized\ntopic, radiation oncology physics.\nFront. Oncol. 13:1219326.\ndoi: 10.3389/fonc.2023.1219326\nCOPYRIGHT\n© 2023 Holmes, Liu, Zhang, Ding, Sio,\nMcGee, Ashman, Li, Liu, Shen and Liu. This is\nan open-access article distributed under the\nterms of theCreative Commons Attribution\nLicense (CC BY).The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that\nthe original publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution or\nreproduction is permitted which does not\ncomply with these terms.\nTYPE Original Research\nPUBLISHED 17 July 2023\nDOI 10.3389/fonc.2023.1219326\n1 Introduction\nThe advent of large language models (LLM) has completely\ntransformed natural language processing (NLP) (1). The traditional\nparadigm of NLP follows the typical pipeline of creating customized\nsolutions for downstream applications through supervised training.\nFor example, a pre-trained BERT (2) model must be modiﬁed with\nadditional network layers and thenﬁne-tuned on labeled training\ndata to perform tasks such as sequence classiﬁcation or question\nanswering. In some situations, it might also be bene ﬁcial or\nnecessary to further pre-train such models on domain speciﬁc\ndata to attain acceptable performance ( 3, 4). For example,\nAgriBERT (5) was pre-trained on agriculture-related text data, to\nproperly address NLP tasks in the food and agriculture domain.\nHowever, the expansive size and exceptional few-shot learning\ncapabilities enable LLMs to solve NLP problems through in-\ncontext learning, which reduces or even eliminates the need for\nannotated training samples ( 6, 7). During in-context learning,\nLLMs generalize from a few examples (or no examples at all)\nbased on prompts, which typically are descriptive user inputs that\ncharacterize desired responses from LLMs (6, 8). For example,\n“summarize the following text” is a straightforward prompt that\nasks the language model to produce a summary for the input text. In\ngeneral, LLMs provides a novel and simpliﬁed workﬂow for NLP\nthat could potentially do away with supervisedﬁne-tuning and its\nassociated intricacies such as hyper-parameter tuning and model\narchitecture modi ﬁcation. Furthermore, in-context learning\nsigniﬁcantly reduces the need for expensive and time-consuming\nhuman annotation (6, 9). It is especially desirable in medicine and\nscience due to the limited data available in these domains (4,\n10– 12).\nIn recent months, the world has witnessed the rise of ChatGPT\n1, which has enjoyed signi ﬁcant global popularity given its\nunprecedented language capabi lities and accessibility to the\ngeneral public through a chatbot interface. ChatGPT is based on\nthe powerful GPT-3 model (6), one of the ﬁrst large language\nmodels in history. The 175-billion-parameters GPT-3 was trained\non a large data collection that encapsulated diverse Internet data\n(including the Common Crawl2 and Wikipedia3). It demonstrates\nexceptional performance in a variety of NLP tasks spanning from\ntext summarization to named entity recognition (NER) through its\ntext generation objective (indeed, many NLP tasks can be translated\nto some forms of text generation). ChatGPT inherits these\ncapabilities from GPT-3, along with the massive knowledge on\ndiverse topics stored in the parameter space. More importantly,\nChatGPT was trained through Reinforcement Learning from\nHuman Feedback (RLHF), a reinforcement learning process that\nincorporates human preferences and human ranked values through\nuser feedback. This process tunes the model to generate outputs that\nare most appealing and relevant to human users. The capabilities of\nChatGPT empowers diverse practical applications ranging from\nessay writing to code generation (13).\nOne of the most powerful LLM to date is GPT-4\n4, a successor to\nGPT-3. While OpenAI has not revealed much about its technical\ndetails yet, GPT-4 has demonstrated superior performance over the\nGPT-3.5-based ChatGPT in various scenarios (9, 12, 14, 15). In fact,\nas of March 2023, GPT-4 is powering Microsoft’s search engine,\nBing (16), which demonstrates the potential of LLM-based search.\nIn addition, unlike its predecessors, GPT-4 is a multi-modal model\nthat accepts image inputs, whic h undoubtedly leads to more\ninteresting applications.\nGPT-4 has been shown to perform exceptionally well on various\nacademic and professional benchmarks (14). For example, GPT-4\npasses the USMLE exam with a >20% margin (17). In fact, GPT-4\nscores at over the 90th percentile on the SAT, the Uniform Bar\nExam and the verbal section of the GRE (see Figure 4 in the“GPT-4\nTechnical Report” (14)), where almost all of them included a\nmultiple-choice component. Indeed, multiple-choice examinations\nare common for evaluating LLMs (14, 18, 19). Most multiple-choice\nexams that have been used to evaluate LLMs are based on topics\nthat are among the most well represented in academics. For\nexample, in 2022, the AP physics exam had 144,526 test-takers\n(20), the LSAT had 128,893 test-takers ( 21), the GRE had\napproximately 342,000 test-takers (22). As a result of the large\nnumbers of test-takers taking these exams as well as the importance\nplaced on scores in determining university admittance, there exists\nan exceeding amount of resources (including text data accessible on\nthe internet). Regardless of the particular LLM under evaluation,\nthe ease of access and overall ubiquity of these tests and relevant test\npreparation materials effectively preclude a high performance when\nevaluating LLMs on these tests. It is therefore important to also\nstudy LLMs on more obscure and specialized topics where the size\nof the training data is likely much smaller. In 2022, there were only\n162 medical school graduates, who applied for radiation oncology\nresidency programs (23). Radiation oncology physics therefore\nrepresents a topic that is relatively unknown to the general\npopulation and may therefore be a more fair test in evaluating\nLLMs as compared to highly represented knowledge-bases. Obscure\ntopics may represent the greatest educational opportunity and also\nthe greatest risk for the general population in the context of LLMs,\nas the responses may be more relied upon while being less accurate\nand with mistakes being less likely to be noticed.\nAn important factor in evaluating the accuracy of LLMs is to\nensure that the test questions are left out of the training data (24),\ni.e. not contaminated. The best way to ensure this is to create new\nquestions for testing. In this study, a multiple-choice examination\nhas been created for this purpose. Four transformer-based LLMs\nhave been chosen for evaluation: ChatGPT (GPT-3.5) (6), ChatGPT\n(GPT-4) (14), Bard (LaMDA) (25), and BLOOMZ (26). These\nresults are compared to radiation oncology experts as well as\nnon-experts. Additionally, ChatGPT (GPT-4) is further explored\non how to improve its answers and on its deductive reasoning\ncapabilities. Experimental results indicate that GPT-4 attains the\n1 https://openai.com/blog/chatgpt\n2 http://commoncrawl.org/\n3 https://www.wikipedia.org/ 4 https://openai.com/research/gpt-4\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org02\nbest performance among LLMs an d outperforms professional\nmedical physicists on average, especially when prompted to explain\nits reasoning before answering the question. We also conduct\nextensive ablation studies and analyses to comprehensively\nmeasure and explain the results.\n2 Related work\n2.1 Large language models\nTransformer-based pre-trained language models (PLMs), such\nas BERT (2) and GPT (27), have revolutionized natural language\nprocessing. Surpassing previous methods (e.g., RNN-based models)\nin numerous tasks, they have promoted interest in and accessibility\nof language models (28). Generally, PLMs can be categorized into\nthree types: autoregressive models (like GPT), masked language\nmodels (such as BERT), and encoder-decoder models (e.g., BART\n(29) and T5 (30)). More recently, there is a rise of very large\nlanguage models, including GPT-3 (6), Bloom (31), PaLM (32), and\nOPT (33). Rooted in the transformer architecture, these models\ndraw inspiration from the likes of BERT and GPT but are developed\nat much larger scales.\nThe objective of large language models is to accurately learn\ncontextual and domain-speciﬁc latent feature representations from\ni n p u tt e x t(28). For example, the vector representation of\n“discharge” might vary considerably between medical and general\ndomains. Smaller language programs often require continual pre-\ntraining and supervisedﬁne-tuning on downstream tasks to achieve\nacceptable performance (3, 4). However, very large language models\ncould potentially eliminate the need for ﬁne-tuning while\nmaintaining competitive results (6).\nBesides the progress in model architecture, scale and training\nstrategies, large language models can be further aligned with human\npreferences through reinforcement learning from human feedback\n(RLHF) (34). This approach has been implemented in various\nLLMs, such as Sparrow (35), InstructGPT (36), and ChatGPT.\nInstructGPT was based on GPT-3 and was trained through a\nprocess during which user preferences were prioritized through\nhuman-generated ranking feedback. As a successor to InstructGPT,\nChatGPT also employs RLHF, focusing on adhering to prompts and\ngenerating comprehensive responses. OpenAI also implemented\nguardrails to prevent the generation of biased and undesirable\noutputs ( 31). ChatGPT has become a highly successful AI\nchatbot, capitalizing on GPT-3.5’s capabilities to facilitate human-\nlike interactions.\nRLHF incorporates human feedback into the generation\nand selection of optimal results by training a reward model\nbased on human annotators ’ rankings of generated outcomes\n(37). This reward model then rewards outputs that best\ncorrespond to human preferences and values. We believe these\ngroundbreaking innovations make ChatGPT the perfect candidate\nfor this study.\nThe recent development of GPT-4 has signiﬁcantly advanced\nthe state-of-the-art of languag em o d e l s .G P T - 4d e m o n s t r a t e s\nenhanced reasoning abilities, cre ativity, image comprehension,\ncontext understanding, and multi-modal abilities, leading to more\nsophisticated and diverse responses. The success of large GPT\nmodels spurs exploration into specialized variants for speci ﬁ\nc\nﬁelds, such as dedicated large language models for medical and\nhealthcare applications, which could potentially revolutionize\nthese domains.\n2.2 Language models and examination\nLarge language models have exceptional natural language\ncomprehension abilities. In addition, they are trained on massive\ndata that supplies substantial knowledge. These characteristics\nmake large language models ideal candidates for academic and\nprofessional benchmarks.\nOpenAI recently released theﬁrst study in the literature that\nevaluates large language models on academic and professional\nexams designed for educated humans (14). The results indicate\nthat GPT-4 performs extremely well on a wide variety of subjects\nranging from the Uniform Bar Exam to GRE. In addition, a study\nfrom Microsoft indicates that GPT-4 can pass USMLE, the\nprofessional exam for medical residents, by a large margin (17).\nThis study is theﬁrst evaluation of large language models in the\nrealms of radiation oncology and medical physics, and we believe it\ncan inspire future research in evaluating LLMs on highly-\nspecialized branches of medicine.\n2.3 Prompt engineering\nCollecting and labeling data for training orﬁne-tuning NLP\nmodels can be resource-intensive and costly, especially in the\nmedical domain ( 4, 9, 12). Recent studies suggest that by\nemploying prompts, large-scale pre-trained language models\n(PLMs) can be adapted to downstream tasks without the need for\nﬁne-tuning (6, 8).\nA prompt consists of a set of instructions that customizes or\nreﬁnes the LLM ’s response. Prompts extend beyond merely\ndescribing the task or specifying output formats. Indeed, they can\nbe engineered to create novel interactions. For example, it is\npossible to prompt ChatGPT to emulate a cybersecurity breach\nwith simulated terminal commands (38). In addition, prompts can\nalso be used to generate additional prompts through a self-\nadaptation process (38).\nThe emergence of prompt engineering signiﬁes the start of a new\nera in natural language processing (8). There is no doubt that carefully\ncrafted prompts have much potential for diverse and sophisticated\napplications. However, determining the ideal prompt poses a unique\nchallenge in the age of large language models. Currently, prompts can\nbe designed manually or generated automatically (8, 39). Although\nautomatically produced prompts may outperform manual prompts in\ncertain tasks (8), they often suffer from poor human-readability and\nexplainability (8, 40). Consequently, manual prompt generation may\nbe favored in domains where interpretability is crucial, such as clinical\npractices and research. In this study, we design a suite of prompts and\nchain-of-thought prompts based on our experience in radiation\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org03\noncology and medical physics and evaluate their impact on large\nlanguage models.\n3 Methods\nA 100-question multiple-choice examination on radiation\noncology physics was created for this study by an experienced\nmedical physicist. This exam includes questions on the following\ntopics: basic physics (12 questions), radiation measurements (10\nquestions), treatment planning (20 questions), imaging modalities\nand applications in radiotherapy (17 questions), brachytherapy (13\nquestions), advanced treatment planning and special procedures\n(16 questions), and safety, quality assurance (QA), and radiation\nprotection (12 questions). The seven exam categories and the\nassociated number of questions for each category follows the\nofﬁcial study guide of American Board of Radiology (41). Of the\n100 questions, 17 require numeric calculation (math-based). The\nexam questions are listed in theAppendix, Section A.\n3.1 Comparison between LLM scores and\nhuman scores\nThe 100-question multiple-choice test on radiation oncology\nphysics was inputted to each LLM in 5 separate trials (Trial 1 - Trial\n5), except BLOOMZ, which was only tested in one trial. Each trial,\nbeginning on a new thread or after reset, began with an initialization\nprompt notifying the LLM that it was about to be tested. Next, the\nLLM was prompted with instructions and 20 questions in batches\nuntil the exam was complete. In each trial, the instructions indicated\nto the LLM that it should only return the correct answer with no\njustiﬁcation. The instructions were included in each batch since it\nwas observed that the LLMs were less likely to follow the\ninstructions otherwise. In cases where the LLM could not accept\n20 questions at a time, batches of 10 questions were used instead\n(Bard). In cases where not all the answers were returned by the\nLLM, the next batch would include the non-answered question(s) as\nwell as the entire next batch. These occurrences were rare. In each\ntest trial, the global prompt and instructions prompt were phrased\ndifferently in order to account for response-noise due to prompt-\nnoise. The initialization prompts and instructions prompts are\ngiven inTable 1.\nLLM test scores and their distributions were compared between\neach other as well as with scores from two human groups, medical\nphysicists and non-experts. The medical physicists group included\nfour experienced board-certiﬁed medical physicists, three medical\nphysics residents, and two medical physics research fellows. The\nnon-expert group included six individuals with advanced degrees in\neither electrical engineering, computer engineering, or computer\nscience, but with no known prior experience or education on\nradiation oncology physics. Each human test-taker was allowed 3\nhours to take the exam, closed book, also permitting the use of a\nbasic calculator. In comparing the LLM scores and human scores,\nthe mean scores, consistency in scores, and conﬁdence in answers\nwere evaluated.\nTo quantify accuracy, the average score was calculated for each\nLLM by averaging the scores from each trial. For the human test\ngroups, individual scores were averaged over the whole group.\nTo quantify the overall consistency of scoring success, the\nstandard deviation and average correlation between trials, deﬁned\nas the average of the upper values of the Pearson correlation matrix\nbetween trials, were calculated. The average correlation indicates\nhow consistent the correct scores were between trials where 1 is\ninterpreted as the distribution being identical, 0 is equivalent to the\ndistribution being purely random, and -1 is interpreted as the\ndistribution being perfectly anti-correlated.\nIn order to quantify the degree of conﬁdence in the answers\ngiven by the LLMs and human groups, the number of correct\nanswers for each question were counted across all trials. For\nexample, if each LLM answered the same question correctly 5\ntimes, then the percentage of questions where all 5 answers were\ncorrect was incremented by 1% (since there are 100 questions).\nAdditionally, the test results were compared to the expected\ndistribution that would occur if the test-taker were guessing at\nrandom. The expected number of correct answers in 5 trials, when\nrandomly guessing, is approximately 0.25×5 = 1.25 on average (98/\nTABLE 1 The LLM prompts used in each trial.\nTrial Initialization prompt Instructions prompt\nTrial 1 I am a radiation therapy researcher. My research group would like to study the answers given by\nChatGPT on the topic of radiation oncology physics. I will now proceed to ask questions about\nradiation oncology physics.\nInstructions: For each multiple choice question,\nprovide the correct answer without\nany justiﬁcation.\nTrial 2 I want to evaluate your knowledge on radiation oncology physics by asking some multiple choice questions. Please give only the question label and the letter\nfor the correct answer.\nTrial 3 Please answer the following practice questions as if you were a resident preparing for\nboard certiﬁcation exams.\nOnly give the correct answer in your response.\nDo not explain your answers.\nTrial 4 We want to test your understanding of radiation oncology physics. For this reason, we have created some\nquestions to ask you.\nIn your response, only report the question label\nand the corresponding answer.\nTrial 5 I will ask you some multiple-choice questions. Instructions: Only respond with the correct\nletter choice.\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org04\n100 questions have 4 choices, 2/100 have 5 choices). Using this\nvalue, the number of correct answer occurrences for each question\ncan be estimated following the resultant Poisson distribution.\nFinally, ChatGPT (GPT-3.5 and GPT-4) and Bard scores were\ncompared to human scores where the scores were calculated based\non majority vote.\n3.2 Improving ChatGPT (GPT-4) accuracy -\nexplain ﬁrst, then answer\nDue to the nature of transformer-based LLMs predicting the\nnext word based on the prior context, it has been shown that the\naccuracy of responses can be improved if a sufﬁciently large LLM is\nprompted to develop the answer in a step-wise manner (24, 42, 43).\nChatGPT (GPT-4) was evaluated using this strategy to see if its\nscore could be improved by prompting it to explain ﬁrst, then\nanswer. The initialization prompt was the same as in Trial 1,\nhowever the instructions prompt for Trial 1 was changed to the\nfollowing: “Instructions: For each multiple choice question, ﬁrst\ngive an explanation for the answer followed by the correct answer\n(letter choice).” These test results were then compared with the\noriginal non-justiﬁed test results.\n3.3 Testing ChatGPT (GPT-4) on its\ndeductive reasoning ability\nIn a multiple-choice question, an LLM will be most successful\nwhen the question and answer are often used in the same context.\nHowever, what happens if the correct answer has no shared context\nwith the question, such as when the answer is“None of the above”?I n\nthis case, the LLM must deduce the correct answer by rejecting all the\nother answers, all of which likely share context with the question.\nThis scenario would seem to be especially challenging for an LLM. To\nstudy the deductive reasoning ability of ChatGPT (GPT-4), each\nquestion of the 100-question multiple-choice exam was modiﬁed.\nEach correct answer was removed and replaced with“None of the\nabove choices is the correct answer.” Such a context-reduction\ntransformation cannot be used on a human since a human would\nnotice the pattern. Because of this, there are likely to be no examples\nof this sort of transformation to be found for tests that were designed\nfor humans and were subsequently used in the training data for\nLLMs. It is assumed, then, that an LLM would not notice this pattern.\nThe modiﬁed exam was given to ChatGPT (GPT-4) using the Trial 1\nprompts and was subsequently tested for improving accuracy by\nexplaining ﬁrst, then answering as described in Section 3.2.\n4 Results\n4.1 Comparison between LLM scores and\nhuman scores\nThe raw marks and mean test scores are shown inFigures 1and\n2A respectively, where the LLM mean test scores represent the\nmean of 5 trials (except for BLOOMZ - 1 trial) and the mean test\nscores for humans represent the mean of their respective groups\n(see Section 3.1). Each LLM was able to outperform the non-expert\nhuman group overall while only ChatGPT (GPT-4) outperformed\nthe medical physicist group. For math-based questions, the medical\nphysicists outperformed ChatGPT (GPT-4).\nA sc a nb eo b s e r v e di nt h er a wm a r k ss h o w ni nFigure 1,e a c hL L M\nand human group showed variability between trials, not only in terms\nof uncertainty in the overall score, but also in terms of the number of\ntimes each question was answered correctly. The standard deviation\nand average correlation between trials are reported inFigures 2B, C.\nThe LLMs were much more consistent in their scores and answers as\ncompared to the human groups, showing both a low standard\ndeviation in scoring and a high average correlation between trials.\nFrom the results shown in 3, Bard slightly outperformed the\nnon-expert group, however both groups performed similarly to a\nrandom guesser. ChatGPT (GPT-3.5 and GPT-4) and the medical\nphysicists showed no similarity to random guessing. ChatGPT\n(GPT-3.5) was either conﬁdent, getting 35% of answers correct in\neach trial, or confused, getting 28% of answers incorrect. ChatGPT\n(GPT-4) was even more conﬁdent, getting 67% of questions correct\nin each trial, however it also showed a propensity for confusion,\ngetting 14% of questions incorrect in each trial. As a group,\nthe medical physicists were neither extremely con ﬁdent, nor\nconfused, however tending towards agreement in selecting the\ncorrect answers.\nFIGURE 1\nRaw marks for each test where the rows are separate tests and the columns are the test questions. Dark shaded squares represent correct answers.\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org05\nAlthough ChatGPT (GPT-3.5 and GPT-4) scored well overall,\ntheir scoring distributions, shown inFigure 3, suggested that if the\nLLMs could work together, there would be very little improvement\nin scoring, since they tended to be either conﬁdent or confused with\nlow variability. Bard (LaMDA) and the non-expert groups would\nalso likely show little improvement in working together as their\nanswers tended towards random success. However, because medical\nphysicists tended towards agreement on correct answers, it would\nbe expected that their score would improve considerably when\nworking together. To test for this, the answers for each group were\ncombined using a “majority vote”. For each question, the most\ncommon answer choice was chosen as the group answer. In the case\nof a tie, one answer among the most common answer choices was\nchosen randomly.Figure 4shows the scoring results when utilizing\na majority vote. As shown, ChatGPT (GPT-3.5 and GPT-4)\nimproved very slightly, 1%. Bard (LaMDA) and the non-expert\ngroup improved by 4% and 3% respectively. However, the medical\nphysicist group improved greatly, by 23%.\n4.2 Improving ChatGPT (GPT-4) accuracy -\nexplain ﬁrst, then answer\nFigure 5 shows the results for having prompted ChatGPT\n(GPT-4) to explain ﬁrst, then answer, therefore allowing the\nanswer to develop. ChatGPT’s (GPT-4) overall score improved by\n5%, exceeding each prior trial. The greatest improvement was in the\nbrachytherapy and math-based questions categories. These results\nare in agreement with prior studies that found this capability to be\nan emergent characteristic for sufﬁciently large LLMs (43). Sample\nA\nB C\nFIGURE 2\nOverall performance and uncertainty in test results.(A) Mean test scores for each LLM by category.(B) Standard deviation in total scores.(C) Average\ncorrelation between trials.\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org06\nresponses from ChatGPT (GPT-4) are given in the Appendix,\nSection A.\n4.3 Testing ChatGPT (GPT-4) on its\ndeductive reasoning ability\nFigure 6shows the results for the deductive reasoning test where\nthe correct answer was replaced by“None of the above choices is the\ncorrect answer” in all 100 questions. Overall, ChatGPT (GPT-4)\nperformed much more poorly as compared to the original test.\nAlthough the performance was generally worse, the explainﬁrst,\nthen answer method was especially important in improving its ability\nto deductively reason through the questions. Without explainingﬁrst,\nChatGPT (GPT-4) got 0% of math-based questions correct, which\nimproved to 65% after incorporating the explainﬁrst, then answer\nmethod, only one question less accurate than the original trial also\nusing the explainﬁrst, then answer method.\nFIGURE 3\nConﬁdence in answers. The number of correct answer occurrences per-question for each LLM and human group. The dashed red curve indicates\nthe expected distribution if the answers were randomly selected based on the Poisson distribution.\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org07\n5 Discussion\nMore than 1 million new cancer cases are diagnosed and more\nthan 600,000 people die from cancer in the US every year.\nRadiotherapy (RT) is a standard treatment option used for nearly\n50% of cancer patients (44– 47). Physics plays an important role in\nradiation oncology due to the complexity and sophistication of\nphysics and engineering adopted in modern radiation therapy.\nTherefore, it is essential for the radiation oncology professionals\nto understand radiation oncology physics well to ensure the safety\nand accuracy of the radiation treatment of cancer patients. The aim\nof this study was to evaluate LLMs on a highly-specialized topic,\nradiation oncology physics, based on a 100-question multiple\nchoice exam that was speciﬁcally designed for this study. The\nexam can be found in the Appendix, Section A. The scoring\nresults from the non-expert group suggest that the general\npopulation knows very little about radiation oncology physics as\ntheir scores were similar to random guessing. Bard (LaMDA)\nslightly outperformed the n on-experts while BLOOMZ and\nChatGPT (GPT-3.5 and GPT-4) greatly outperformed the non-\nexperts. Amazingly, GPT-4 was able to outperform the average\nmedical physicist in nearly all subcategories and improved its\nanswer accuracy when prompted to explain its reasoning before\nanswering (Figures 2A, 5). As a general principle for improving\naccuracy, users should consider prompting ChatGPT to explain\nﬁrst, then answer. ChatGPT (GPT-4) showed a surprising ability to\ndeductively reason in answering all 100 questions where each\ncorrect answer was modiﬁed to be“None of the above choices is\nthe correct answer.”, particularly when it was prompted to explain\nﬁrst, then answer, scoring 55% overall. This result is somewhat\nperplexing and could potentially be an emergent property.\nEmergent properties are known to occur as the number of\nparameters is increased in LLMs (43). This novel method may be\na useful method in determining whether deductive reasoning\nimproves with the number of parameters going forward.\nWhile ChatGPT (GPT-4) outpe rformed medical physicists\noverall, this study has also provided evidence that individual LLMs\ncannot compete with a small number of medical physicists working\ntogether (Figure 4). The likely reason is that humans vary signiﬁcantly\nin capabilities and knowledge from individual to individual, even\nwhen their professional backgrounds are similar. Additionally, while\nan answer in a multiple-choice question will either be correct or\nincorrect, the scoring count distributions shown inFigure 3indicated\nthat the medical physicists were far less likely to be confused, which,\nwhen aggregated over the whole group of medical physicists, allowed\nthem to select the correct answerat a much higher rate in a majority\nvote. When ChatGPT (GPT-3.5 and GPT-4) was wrong, it was\nconﬁdently wrong (confused). Similarly, when it was correct, it was\nconﬁdently correct. Our results indicated that humans with expertise\non a highly-specialized topic knew when to guess, how to guess\nintelligently, and were less likely to be wrong in their reasoning, even\nwhen the correct answer was not chosen. This comparison may not be\ncompletely fair as it is possible that if the exact same human could be\ntested repeatedly in the same manner as ChatGPT (GPT-3.5 and\nGPT-4), they might also repeat answers and show a degree of\nconfusion individually. That point is arguably irrelevant, however, as\nthere are many experienced medical physicists and only few LLMs as\ncapable as GPT-4. The high degree of consistency in correct and\nincorrect answers for ChatGPT (GPT-3.5 and GPT-4) may be a sign\nof over-ﬁtting (or memorization) in regards to radiation oncology\nphysics knowledge. Regardless, being that radiation oncology physics\nis a highly-specialized topic, the performance of ChatGPT (GPT-4)\nwas extraordinary and will likely continue to improve in the near-\nfuture. Practically speaking, this study suggests a great potential for\nradiation oncology experts to work alongside ChatGPT (GPT-4),\nusing it as a highly knowledgeable assistant.\nA weakness in evaluating LLMs using exams such as the one\npresented in this study is that this exam is not representative of the\ndetailed and nuanced daily clinical work being performed by\nmedical physicists and radiation oncology specialists. The relative\nFIGURE 4\nScores by category, tabulated by majority vote among trials for LLMs and within the group for humans.\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org08\nperformance between LLMs and medical physicists on radiation\noncology physics exams reported in this study may therefore\nmisrepresent the degree of equivalency between LLMs and\nindividual medical physic ists. Furthermore, GPT-4 ’sh i g h\nperformance on this certi ﬁcation-like exam, covering a highly\nspecialized topic, suggests a degree of super ﬁciality in the\nknowledge being assessed. Otherwise, we would have to entertain\nthe possibility of GPT-4 being competent enough to fulﬁll the role\nof a medical physicist, which seems highly improbable. The radiation\noncology community, and possibly the wider medical community,\nmay therefore need to reevaluate certiﬁcation procedures, as the\nnecessity for humans to invest signiﬁcant effort in acquiring such\nsuperﬁcial knowledge will diminish as LLMs continue to advance.\nWith this in mind, LLMs could potentially be used as a test for\nsuperﬁciality. Perhaps a greater focus on knowledge not known by\nthe LLM should be more greatly emphasized.\nFIGURE 6\nThe scores for Trial 1 after replacing the correct answer with“None of the above choices is the correct answer.”, a method for testing for deductive\nreasoning, and subsequent improvement as due to using the explainﬁrst, then answer method.\nFIGURE 5\nThe improvement for Trial 1 as due to using the explainﬁrst, then answer method.\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org09\n5.1 Applying large language models in\nradiation oncology\nThis study is a continuation of a line of research that applies state-\nof-the-art NLP methods to radiation oncology. For example, Rezayi\net al. (11) trained BioBERT on a large corpus of radiation oncology\nliterature and a curated and anonymized text dataset from a hospital\nto build ClinicalRadioBERT, a specialized language model for\nradiation oncology. Liao et al. ( 48) proposed a framework of\ndirecting the attention of transformer-based language models to\nmore important input tokens that signiﬁcantly affect classiﬁcation\ndecisions. This method is particularly important for few-shot learning\nwith few annotated samples, which is a common challenge in\nradiation oncology where it is difﬁcult to collect and curate large\namounts of multi-institution patient data that match certain\nrequirements due to the concern of patient privacy. On a related\nnote, ChatGPT has demonstrated superior performance as an\neffective text data augmentation approach over state-of-the-art text\ndata augmentation methods in terms of testing accuracy and\ndistribution of the augmented samples (9), which can also be used\nto address the few-shot learning challenge.\nIn addition, LLMs can be employed for innovative applications\nsuch as data de-identiﬁcation. For example, GPT-4 outperforms\nChatGPT and other language model competitors in de-identifying\nclinical notes with a 99% accuracy ( 12). This is of extreme\nimportance to radiation oncology and all medicine specialities in\ngeneral, since it is often cumbersome to anonymize data for cross-\ninstitution clinical collaboration and academic research. Some other\napplications of language models include building domain-speciﬁc\nknowledge graphs for oncology (49) without manual annotation\nfrom clinicians or other domain experts.\n5.2 Multi-modal models in\nradiation oncology\nMulti-modal models are the future of language model (1) and\nare important in medical diagnosis (50). Some early LLM studies\nwith multi-modal data include ChatCAD (51), a framework to\nintegrate images and texts for computer-aided diagnosis. It supports\nvarious diagnosis networks such as those for lesion segmentation\nand report generation. In this framework, ChatGPT can be used to\nenhance the outputs of these networks.\nGPT-4 supports multi-modal inputs such as images, which\nfurther unlocks the potential of large language models in\nradiation oncology. It is necessary to investigate future models\nand applications that integrate text, images, dosimetric data, and\nother modalities into the diagnosis and treatment pipelines. We\nbelieve such multi-modal models display inherent afﬁnity to the\nhuman brain (1) and future LLM models for medicine can receive\ninspirations from advances in both neuroscience and NLP.\nData availability statement\nThe original contributions presented in the study are included\nin the article/Supplementary Material. Further inquiries can be\ndirected to the corresponding authors.\nAuthor contributions\nJH, JS, and WL contributed to conception and design of the study\nand edited the manuscript. JS designed the 100-question exam,\nsubsequently edited by JH and WL. ZL, XL, and TL advised on\nLLM concepts and contributed to the manuscript. LZ and YD advised\non the concept and experimental design. JH guided the writing\nprocess, wrote initial draft, performed all data analysis. TS, LM, and\nJA advised on clinical concerns and approved the manuscript. All\nauthors contributed to the article and approved the submitted version.\nFunding\nThis research was supported by the National Cancer Institute\n(NCI) Career Developmental Award K25CA168984, President’s\nDiscovery Translational Program of Mayo Clinic, the Fred C. and\nKatherine B. Andersen Foundation Translational Cancer Research\nAward, Arizona Biomedical Research Commission Investigator\nAward, the Lawrence W. and Marilyn W. Matteson Fund for\nCancer Research, and the Kemper Marley Foundation.\nAcknowledgments\nWe would like to thank the many individuals (Yuzhen Ding,\nYiran Cui, Yonghui Fan, Yikang Li, Riti Paul, Nupur Thakur,\nSachin Chhabra, Chenbin Liu, Yang Li, Man Zhao, Yuchao Hu,\nShumei Jia, Lian Zhang, Yao Xu, Hongying Feng, and Yunze Yang)\nthat volunteered to take the radiation oncology physics exam.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations,\nor those of the publisher, the editors and the reviewers. Any product\nthat may be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found online\nat: https://www.frontiersin.org/articles/10.3389/fonc.2023.1219326/\nfull#supplementary-material\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org10\nReferences\n1. Zhao L, Zhang L, Wu Z, Chen Y, Dai H, Yu X, et al. When brain-inspired ai meets\nagi. arXiv preprint(2023), 2303.15935.\n2. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint (2018),\narXiv:1810.04805.\n3. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain-speciﬁc\nlanguage model pretraining for biomedical natural language processing.ACM Trans\nComput Healthc (HEALTH)(2021) 3(1):1– 23.\n4. Liu Z, He M, Jiang Z, Wu Z, Dai H , Zhang L, et al. Survey on natural language\nprocessing in medical image analysis.Zhong nan da xue xue bao. Yi xue ban= J Cent\nSouth University Med Sci(2022) 47(8):981– 93.\n5. Rezayi S, Liu Z, Wu Z, Dhakal C, Ge B, Zhen C, et al. Agribert: knowledge-infused\nagricultural language models for matching food and nutrition.IJCAI (2022), 5150– 6.\ndoi: 10.24963/ijcai.2022/715\n6. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al.Language\nmodels are few-shot learners. (2020), arxiv id: 2005.14165.\n7. Dong Q, Li L, Dai D, Zheng C, Wu Z, Chang B, et al. A survey for in-context\nlearning. (2022), arXiv:2106.07404v2.\n8. Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre-train, prompt, and\npredict: a systematic survey of prompting methods in natural language processing.\nACM Comput Surveys(2023) 55(9):1– 35. doi:10.1145/3560815\n9. Dai H, Liu Z, Liao W, Huang X, Wu Z, Zhao L, et al. Chataug: leveraging chatgpt\nfor text data augmentation.arXiv preprint(2023), 2302.13007.\n10. Kagawa R, Shirasuna M, Ikeda A, Sanuki M, Honda H, Nosato H.One-second\nboosting: a simple and cost-effective intervention for data annotation in machine\nlearning. (2022), arxiv id: 2112.07475.\n11. Rezayi S, Dai H, Liu Z, Wu Z, Hebbar A, Burns AH, et al. Clinicalradiobert:\nknowledge-infused few shot learning for clinical notes named entity recognition. In:\nMachine learning in medical imaging: 13th international workshop, MLMI 2022, held in\nconjunction with MICCAI 2022, Singap ore, September 18, 2022, proceedings .\n(Singapore: Springer (2022). p. 269– 78.\n12. Liu Z, Yu X, Zhang L, Wu Z, Cao C, Dai H, et al. Deid-gpt: zero-shot medical\ntext de-identiﬁcation by gpt-4.arXiv preprint(2023), arXiv:2306.11289.\n13. Qin C, Zhang A, Zhang Z, Chen J, Yasunaga M, Yang D. Is chatgpt a general-\npurpose natural language processing task solver? arXiv preprint (2023),\narXiv:2306.11289.\n14. OpenAI. Gpt-4 technical report. (2023), arxiv id: 2303.08774.\n15. Koubaa A. Gpt-4 vs. gpt-3.5: a concise showdown . (2023). doi: 10.36227/\ntechrxiv.22312330.v2\n16. The Verge.The bing ai bot has been secretly running gpt-4. (2023). Available at:\nhttps://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm\n17. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of gpt-4 on\nmedical challenge problems.arXiv preprint(2023), 2303.13375.\n18. Lampinen AK, Dasgupta I, Chan SCY, Matthewson K, Tessler MH, Creswell A,\net al. Can language models learn from explanations in context?arXiv preprint(2022),\narXiv:2109.08668.\n19. Savelka J, Agarwal A, Bogart C, Sakr M. Large Language models (gpt) struggle to\nanswer multiple-choice questions about code. arXiv preprint (2023),\narXiv:2206.05715v1. doi:10.5220/0011996900003470\n20. College Board.Student score distributions\n(2022) (Accessed 3/31/2023).\n21. Law School Admission Council.Test registrants and test takers(2023) (Accessed\n3/31/2023).\n22. Achievable. Key gre statistics from the 2022 ets gre snapshot report (2023)\n(Accessed 3/31/2023).\n23. National Resident Matching Program.Charting outcomes in the match: senior\nstudents of u.s. md medical schools(2023) (Accessed 3/31/2023).\n24. Bubeck S, Chandrasekaran V, Eldan R, Gehrke J, Horvitz E, Kamar E, et al.\nSparks of artiﬁcial general intelligence: early experiments with gpt-4. (2023), Arxiv ID:\n2303.12712.\n25. Thoppilan R, De Freitas D, Hall J, Shazeer N, Kulshreshtha A, Cheng H-T, et al.\nLamda: language models for dialog applications. (2022), arXiv:2102.08602.\n26. Muennighoff N, Wang T, Sutawika L, Roberts A, Biderman S, Le Scao T, et al.\nCrosslingual generalization through multitaskﬁnetuning. (2022), arXiv:2106.11539.\n27. Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language\nunderstanding by generative pre-training. (2018), 1801.06146.\n28. Kalyan KS, Rajasekharan A, Sangeetha S. Ammus: a survey of transformer-based\npretrained models in natural language processing. (2021), 2003.08271. doi:10.1016/\nj.jbi.2021.103982\n29. Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, et al. Bart:\nDenoising sequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension. arXiv preprint (2019), arXiv:2106.12424v1. doi:\n10.18653/v1/2020.acl-main.703\n30. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text transformer.J Mach Learn Res\n(2020) 21(1):5485– 551.\n31. Le Scao T, Fan A, Akiki C, Pavlick E, Ilić S, Hesslow D, et al. Bloom: a 176b-\nparameter open-access multilingual language model. (2022), 2211.05100.\n32. Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, et al. Palm:\nscaling language modeling with pathways.arXiv preprint(2022), arXiv:2205.03578.\n33. Zhang S, Roller S, Goyal N, Artetxe M, Chen M, Chen S, et al. Opt: open pre-\ntrained transformer language models.arXiv preprint(2022), arXiv ID: arX\n34. Ziegler DM, Stiennon N, Wu J, Brown TB, Radford A, Amodei D, et al. Fine-\ntuning language models from human preferences. arXiv preprint (2019),\narXiv:1909.08593.\n35. Glaese A, McAleese N, Trębacz M, Aslanides J, Firoiu V, Ewalds T, et al.\nImproving alignment of dialogue agents via targeted human judgements.arXiv preprint\n(2022), 2209.14375.\n36. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, et al. Training\nlanguage models to follow instructions with human feedback.Adv Neural Inf Process\nSyst (2022) 35:27730– 44.\n37. Bai Y, Jones A, Ndousse K, Askell A, Chen A, DasSarma N, et al. Training a\nhelpful and harmless assistant with reinforcement learning from human feedback.\narXiv preprint(2022), arXiv:2204.05862.\n38. White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, et al. A prompt pattern\ncatalog to enhance prompt engineering with chatgpt. (2023), arXiv:2306.02245v1.\n39. Gao T, Fisch A, Chen D. Making pre-trained language models better few-shot\nlearners. arXiv preprint(2020), 2012.15723. doi:10.18653/v1/2021.acl-long.295\n40. Taylor N, Zhang Y, Joyce D, Nevado-Holgado A, Kormilitzin A. Clinical prompt\nlearning with frozen language models.arXiv preprint(2022), arXiv:2206.09308.\n41. The American Board of Radiology.Medical physics radiation oncology(2023)\n(Accessed 2023-04-18).\n42. Shinn N, Labash B, Gopinath A.Reﬂexion: an autonomous agent with dynamic\nmemory and self-re\nﬂection. (2023), arXiv:2303.11366.\n43. Wei J, Tay Y, Bommasani R, Raffel C, Zoph B, Borgeaud S, et al.emergent\nabilities of large language models. (2022), arXiv:2201.04451.\n44. Liu W, Frank SJ, Li X, Li Y, Zhu RX, Mohan R. Ptv-based impt optimization\nincorporating planning risk volumes vs robust optimization. Med Phys (2013) 40\n(2):021709. doi:10.1118/1.4774363\n45. Deng W, Younkin JE, Souris K, Huang S, Augustine K, Fatyga M, et al.\nIntegrating an open source monte carlo code “mcsquare ” for clinical use in\nintensity-modulated proton therapy. Med Phys (2020) 47(6):2558– 74. doi: 10.1002/\nmp.14125\n46. Shan J, An Y, Bues M, Schild SE, Liu W. Robust optimization in impt using\nquadratic objective functions to account for the minimum mu constraint.Med Phys\n(2018) 45(1):460– 9. doi:10.1002/mp.12677\n47. Schild SE, Rule WG, Ashman JB, Vora SA, Keole S, Anand A, et al. Proton beam\ntherapy for locally advanced lung cancer: a review.World J Clin Oncol(2014) 5(4):568.\ndoi: 10.5306/wjco.v5.i4.568\n48. Liao W, Liu Z, Dai H, Wu Z, Zhang Y, Huang X, et al. Mask-guided bert for few\nshot text classiﬁcation. arXiv preprint(2023), arXiv:2106.01204.\n49. Cai H, Liao W, Liu Z, Huang X, Zhang Y, Ding S, et al. Coarse-to-ﬁne knowledge\ngraph domain adaptation based on distantly-supervised iterative training. (2022),\narXiv:2206.04206v1.\n50. Cai X, Liu S, Han J, Yang L, Liu Z, Liu T.Chestxraybert: a pretrained language\nmodel for chest radiology report summarization. IEEE Transactions on Multimedia\n(2023) 25:845– 55. doi:10.1109/TMM.2021.3132724\n51. Wang S, Zhao Z, Ouyang X, Wang Q, Shen D. Chatcad: interactive computer-\naided diagnosis on medical image us ing large language models. (2023),\narXiv:2205.13452v1.\n52. OpenAI. Introducing chatgpt(2023) (Accessed 3/31/2023).\nHolmes et al. 10.3389/fonc.2023.1219326\nFrontiers inOncology frontiersin.org11",
  "topic": "Consistency (knowledge bases)",
  "concepts": [
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.646149754524231
    },
    {
      "name": "Test (biology)",
      "score": 0.5870292782783508
    },
    {
      "name": "Radiation oncology",
      "score": 0.577104389667511
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5419074892997742
    },
    {
      "name": "Medical education",
      "score": 0.3549327254295349
    },
    {
      "name": "Psychology",
      "score": 0.33137011528015137
    },
    {
      "name": "Medical physics",
      "score": 0.3266180753707886
    },
    {
      "name": "Oncology",
      "score": 0.3257097005844116
    },
    {
      "name": "Medicine",
      "score": 0.3118000030517578
    },
    {
      "name": "Internal medicine",
      "score": 0.2900754511356354
    },
    {
      "name": "Computer science",
      "score": 0.2795691192150116
    },
    {
      "name": "Biology",
      "score": 0.17592650651931763
    },
    {
      "name": "Radiation therapy",
      "score": 0.17527198791503906
    },
    {
      "name": "Artificial intelligence",
      "score": 0.17011988162994385
    },
    {
      "name": "Ecology",
      "score": 0.11424502730369568
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210146710",
      "name": "Mayo Clinic in Florida",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2802423016",
      "name": "WinnMed",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I165733156",
      "name": "University of Georgia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210087915",
      "name": "Massachusetts General Hospital",
      "country": "US"
    }
  ],
  "cited_by": 120
}