{
  "title": "How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations",
  "url": "https://openalex.org/W2972312591",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286866379",
      "name": "van Aken, Betty",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288377795",
      "name": "Winter, Benjamin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286866381",
      "name": "Löser, Alexander",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287538991",
      "name": "Gers, Felix A.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "L\\\"oser, Alexander",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2962772482",
    "https://openalex.org/W2116516955",
    "https://openalex.org/W2963374347",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2915600176",
    "https://openalex.org/W2938224028",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2962790223",
    "https://openalex.org/W2153676086",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2150593711",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W2770650464",
    "https://openalex.org/W2294798173",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2809925683",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2295676751",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2964165804",
    "https://openalex.org/W2962790689",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2099741732",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2809324505"
  ],
  "abstract": "Bidirectional Encoder Representations from Transformers (BERT) reach state-of-the-art results in a variety of Natural Language Processing tasks. However, understanding of their internal functioning is still insufficient and unsatisfactory. In order to better understand BERT and other Transformer-based models, we present a layer-wise analysis of BERT's hidden states. Unlike previous research, which mainly focuses on explaining Transformer models by their attention weights, we argue that hidden states contain equally valuable information. Specifically, our analysis focuses on models fine-tuned on the task of Question Answering (QA) as an example of a complex downstream task. We inspect how QA models transform token vectors in order to find the correct answer. To this end, we apply a set of general and QA-specific probing tasks that reveal the information stored in each representation layer. Our qualitative analysis of hidden state visualizations provides additional insights into BERT's reasoning process. Our results show that the transformations within BERT go through phases that are related to traditional pipeline tasks. The system can therefore implicitly incorporate task-specific information into its token representations. Furthermore, our analysis reveals that fine-tuning has little impact on the models' semantic abilities and that prediction errors can be recognized in the vector representations of even early layers.",
  "full_text": "How Does BERT Answer Questions?\nA Layer-Wise Analysis of Transformer Representations\nBetty van Aken∗\nbvanaken@beuth-hochschule.de\nBeuth University of Applied Sciences Berlin\nBenjamin Winter∗\nBenjamin.Winter@beuth-hochschule.de\nBeuth University of Applied Sciences Berlin\nAlexander Löser\naloeser@beuth-hochschule.de\nBeuth University of Applied Sciences Berlin\nFelix A. Gers\ngers@beuth-hochschule.de\nBeuth University of Applied Sciences Berlin\nABSTRACT\nBidirectional Encoder Representations from Transformers (BERT)\nreach state-of-the-art results in a variety of Natural Language Pro-\ncessing tasks. However, understanding of their internal functioning\nis still insufficient and unsatisfactory. In order to better under-\nstand BERT and other Transformer-based models, we present a\nlayer-wise analysis of BERT’s hidden states. Unlike previous re-\nsearch, which mainly focuses on explaining Transformer models\nby their attention weights, we argue that hidden states contain\nequally valuable information. Specifically, our analysis focuses on\nmodels fine-tuned on the task of Question Answering (QA) as an\nexample of a complex downstream task. We inspect how QA models\ntransform token vectors in order to find the correct answer. To this\nend, we apply a set of general and QA-specific probing tasks that\nreveal the information stored in each representation layer. Our qual-\nitative analysis of hidden state visualizations provides additional\ninsights into BERT’s reasoning process. Our results show that the\ntransformations within BERT go through phases that are related\nto traditional pipeline tasks. The system can therefore implicitly\nincorporate task-specific information into its token representations.\nFurthermore, our analysis reveals that fine-tuning has little impact\non the models’ semantic abilities and that prediction errors can be\nrecognized in the vector representations of even early layers.\nKEYWORDS\nneural networks, transformers, explainability, word representation,\nnatural language processing, question answering\nACM Reference Format:\nBetty van Aken, Benjamin Winter, Alexander Löser, and Felix A. Gers. 2019.\nHow Does BERT Answer Questions? A Layer-Wise Analysis of Transformer\nRepresentations. In The 28th ACM International Conference on Information\nand Knowledge Management (CIKM ’19), November 3–7, 2019, Beijing, China.\nACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3357384.3358028\n∗Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nCIKM ’19, November 3–7, 2019, Beijing, China\n© 2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-6976-3/19/11. . . $15.00\nhttps://doi.org/10.1145/3357384.3358028\n1 INTRODUCTION\nIn recent months, Transformer models have become more and more\nprevalent in the field of Natural Language Processing. Originally\nthey became popular for their improvements over RNNs in Machine\nTranslation [36]. Now however, with the advent of large models\nand an equally large amount of pre-training being done, they have\nproven adept at solving many of the standard Natural Language\nProcessing tasks. Main subject of this paper is BERT [8], arguably\nthe most popular of the recent Transformer models and the first\nto display significant improvements over previous state-of-the-art\nmodels in a number of different benchmarks and tasks.\nProblem of black box models . Deep Learning models achieve in-\ncreasingly impressive results across a number of different domains,\nwhereas their application to real-world tasks has been moving\nsomewhat more slowly. One major impediment lies in the lack of\ntransparency, reliability and prediction guarantees in these largely\nblack box models.\nWhile Transformers are commonly believed to be moderately\ninterpretable through the inspection of their attention values, cur-\nrent research suggests that this may not always be the case [ 16].\nThis paper takes a different approach to the interpretation of said\nTransformer Networks. Instead of evaluating attention values, our\napproach examines the hidden states between encoder layers di-\nrectly. There are multiple questions this paper will address:\n(1) Do Transformers answer questions decompositionally, in a\nsimilar manner to humans?\n(2) Do specific layers in a multi-layer Transformer network\nsolve different tasks?\n(3) How does fine-tuning influence the network’s inner state?\n(4) Can an evaluation of network layers help determine why\nand how a network failed to predict a correct answer?\nWe discuss these questions on the basis of fine-tuned models on\nstandard QA datasets. We choose the task of Question Answering\nas an example of a complex downstream task that, as this paper\nwill show, requires solving a multitude of other Natural Language\nProcessing tasks. Additionally, it has been shown that other NLP\ntasks can be successfully framed as QA tasks [ 23], therefore our\nanalysis should translate to these tasks as well. While this work\nfocuses on the BERT architecture, we perform preliminary tests on\nthe small GPT-2 model [29] as well, which yield similar results.\narXiv:1909.04925v1  [cs.CL]  11 Sep 2019\nContributions. With the goal to improve understanding ofinternal\nworkings of Transformers we present the following contributions:\nFirst, we propose a layer-wise visualisation of token represen-\ntations that reveals information about the internal state of Trans-\nformer networks. This visualisation can be used to expose wrong\npredictions even in earlier layers or to show which parts of the\ncontext the model considered as Supporting Facts.\nSecond, we apply a set of general NLP Probing Tasks and extend\nthem by the QA-specific tasks of Question Type Classification and\nSupporting Fact Extraction. This way we can analyse the abilities\nwithin BERT’s layers and how they are impacted by fine-tuning.\nThird, we show that BERT’s transformations go through similar\nphases, even if fine-tuned on different tasks. Information about gen-\neral language properties is encoded in earlier layers and implicitly\nused to solve the downstream task at hand in later layers.\n2 RELATED WORK\nTransformer Models. Our analyses focus on BERT, which belongs\nto the group of Transformer networks, named after how represen-\ntations are transformed throughout the network layers. We also\npartly include the more recent Transformer model GPT-2 [29]. This\nmodel represents OpenAI’s improved version of GPT [28] and while\nGPT-2 has not yet climbed leaderboards like BERT has, its larger\nversions have proven adept enough at the language modeling task,\nthat Open-AI has decided not to release their pre-trained models.\nThere are also other Transformer models of note, where a similar\nanalysis might prove interesting in future work. Chief among them\nare the Universal Transformer [ 7] and TransformerXL [ 6], both\nof which aim to improve some of the flaws of the Transformer\narchitecture by adding a recurrent inductive bias.\nInterpretability and Probing . Explainability and Interpretabil-\nity of neural models have become an increasingly large field of\nresearch. While there are a multitude of ways to approach these\ntopics [9, 12, 20], we especially highlight relevant work in the area\nof research that builds and applies probing tasks and methodologies,\npost-hoc, to trained models. There have been a number of recent\nadvances on this topic. While the majority of the current works aim\nto create or apply more general purpose probing tasks [ 2, 4, 33],\nBERT specifically has also been probed in previous papers. Tenney\net al. [34] proposes a novel \"edge-probing\" framework consisting\nof nine different probing tasks and applies it to the contextualized\nword embeddings of ELMo, BERT and GPT-1. Both semantic and\nsyntactic information is probed, but only pre-trained models are\nstudied, and not specifically fine-tuned ones. A similar analysis [11]\nadds more probing tasks and addresses only the BERT architecture.\nQiao et al. [27] focus specifically on analysing BERT as a Ranking\nmodel. The authors probe attention values in different layers and\nmeasure performance for representations build from different BERT\nlayers. Like [34], they only discuss pre-trained models.\nThere has also been work which studies models not through\nprobing tasks but through qualitative visual analysis. Zhang and\nZhu [41] offer a survey of different approaches, though limited\nto CNNs. Nagamine et al . [25] explore phoneme recognition in\nDNNs by studying single node activations in the task of speech\nrecognition. Hupkes et al . [15] go one step further, by not only\ndoing a qualitative analysis, but also training diagnostic classifiers\nto support their hypotheses. Finally, Li et al. [18] take a look at word\nvectors and the importance of some of their specific dimensions on\nboth sequence tagging and classification tasks.\nThe most closely related previous work is proposed by Liu et al.\n[21]. Here, the authors also perform a layer-wise analysis of BERT’s\ntoken representations. However, their work solely focuses on prob-\ning pre-trained models and disregards models fine-tuned on down-\nstream tasks. Furthermore, it limits the analysis to the general\ntransferability of the network and does not analyze the specific\nphases that BERT goes through.\nAdditionally, our work is motivated by Jain and Wallace [16]. In\ntheir paper, the authors argue that attention, at least in some cases,\nis not well suited to solve the issues of explainability and inter-\npretability. They do so both by constructing adversarial examples\nand by a comparison with more traditional explainability methods.\nIn supporting this claim, we propose revisiting evaluating hidden\nstates and token representations instead.\n3 BERT UNDER THE MICROSCOPE\nWe focus our analysis on fine-tuned BERT models. In order to\nunderstand which transformations the models apply to input tokens,\nwe take two approaches: First, we analyse the transforming token\nvectors qualitatively by examining their positions in vector space.\nSecond, we probe their language abilities on QA-related tasks to\nexamine our results quantitatively.\n3.1 Analysis of Transformed Tokens\nThe architecture of BERT and Transformer networks in general al-\nlows us to follow the transformations of each token throughout the\nnetwork. We use this characteristic for an analysis of the changes\nthat are being made to the tokens’ representations in every layer.\nWe use the following approach for a qualitative analysis of these\ntransformations: We randomly select both correctly and falsely\npredicted samples from the test set of the respective dataset. For\nthese samples we collect the hidden states from each layer while\nremoving any padding. This results in the representation of each\ntoken throughout the model’s layers.\nThe model can transform the vector space freely throughout its\nlayers and we do not have references for semantic meanings of po-\nsitions within these vector spaces. Therefore we consider distances\nbetween token vectors as indication for semantic relations.\nDimensionality Reduction. BERT’s pre-trained models use vec-\ntor dimensions of 1024 (large model) and 512 (base model). In order\nto visualize relations between tokens, we apply dimensionality re-\nduction and fit the vectors into two-dimensional space. To that\nend we apply T-distributed Stochastic Neighbor Embedding (t-SNE)\n[35], Principal Component Analysis (PCA) [10] and Independent\nComponent Analysis (ICA) [3] to vectors in each layer. As the re-\nsults of PCA reveal the most distinct clusters for our data, we use\nit to present our findings.\nK-means Clustering. In order to verify that clusters in 2D space\nrepresent the actual distribution in high-dimensional vector space,\nwe additionally apply a k-means clustering [ 22]. We choose the\nFigure 1: Schematic overview of the BERT architecture and\nour probing setup. Question and context tokens are pro-\ncessed by N encoder blocks with a Positional Embedding\nadded beforehand. The output of the last layer is fed into\na span prediction head consisting of a Linear Layer and a\nSoftmax. We use the hidden states of each layer as input to\na set of probing tasks to examine the encoded information.\nnumber of clusters k in regard to the number of observed clusters\nin PCA, which vary over layers. The resulting clusters correspond\nwith our observations in 2D space.\n3.2 Probing BERT’s Layers\nOur goal is to further understand the abilities of the model after\neach transformation. We therefore apply a set of semantic probing\ntasks to analyze which information is stored within the transformed\ntokens after each layer. We want to know whether specific layers\nare reserved for specific tasks and how language information is\nmaintained or forgotten by the model.\nWe use the principle of Edge Probing introduced by Tenney et al.\n[34]. Edge Probing translates core NLP tasks into classification tasks\nby focusing solely on their labeling part. This enables a standard-\nized probing mechanism over a wide range of tasks. We adopt the\ntasks Named Entity Labeling, Coreference Resolution and Relation\nClassification from the original paper as they are prerequisites for\nlanguage understanding and reasoning [39]. We add tasks of Ques-\ntion Type Classification and Supporting Fact Identification due to\ntheir importance for Question Answering in particular.1\nNamed Entity Labeling . Given a span of tokens the model has to\npredict the correct entity category. This is based on Named Entity\nRecognition but formulated as a Classification problem. The task\nwas modeled by [34], annotations are based on the OntoNotes 5.0\ncorpus [38] and contain 18 entity categories.\nCoreference Resolution. The Coreference task requires the model\nto predict whether two mentions within a text refer to the same\nentity. The task was built from the OntoNotes corpus and enhanced\nwith negative samples by [34].\n1The source code is available at: https://github.com/bvanaken/explain-BERT-QA\nRelation Classification. In Relation Classification the model has\nto predict which relation type connects two known entities. The\ntask was constructed by [34] with samples taken from the SemEval\n2010 Task 8 dataset consisting of English web text and nine direc-\ntional relation types.\nQuestion Type Classification . A fundamental part of answering\na question is to correctly identify its question type. For this Edge\nProbing task we use the Question Classification dataset constructed\nby Li and Roth [19] based on the TREC-10 QA dataset [37]. It in-\ncludes 500 fine-grained types of questions within the larger groups\nof abbreviation, entity, description, human, location and numeric\nvalue. We use the whole question as input to the model with its\nquestion type as label.\nSupporting Facts. The extraction of Supporting Facts is essential\nfor Question Answering tasks, especially in the multi-hop case. We\nexamine what BERT’s token transformations can tell us about the\nmechanism behind identifying important context parts.\nTo understand at which stage this distinction is done, we con-\nstruct a probing task for identifying Supporting Facts. The model\nhas to predict whether a sentence contains supporting facts re-\ngarding a specific question or whether it is irrelevant. Through\nthis task we test the hypothesis that token representations contain\ninformation about their significance to the question.\nBoth HotpotQA and bAbI contain information about sentence-\nwise Supporting Facts for each question. SQuAD does not require\nmulti-hop reasoning, we therefore consider the sentence containing\nthe answer phrase the Supporting Fact. We also exclude all QA-pairs\nthat only contain one context sentence. We construct a different\nprobing task for each dataset in order to check their task-specific\nability to recognize relevant parts. All samples are labeled sentence-\nwise with true if they are a supporting fact or false otherwise.\nProbing Setup. Analogue to the authors of [34], we embed input\ntokens for each probing task sample with our fine-tuned BERT\nmodel. Contrary to previous work, we do this for all layers (N = 12\nfor BERT-base and N = 24 for BERT-large), using only the output\nembedding from n-th layer at step n. The concept of Edge Probing\ndefines that only tokens of \"labeled edges\" (e.g. tokens of two related\nentities for Relation Classification) within a sample are considered\nfor classification. These tokens are first pooled for a fixed-length\nrepresentation and afterwards fed into a two-layer Multi-layer\nPerceptron (MLP) classifier, that predicts label-wise probability\nscores (e.g. for each type of relation). A schematic overview of this\nsetting is shown in Figure 1. We perform the same steps on pre-\ntrained BERT-base and BERT-large models without any fine-tuning.\nThis enables us to identify which abilities the model learns during\npre-training or fine-tuning.\n4 DATASETS AND MODELS\n4.1 Datasets\nOur aim is to understand how BERT works on complex downstream\ntasks. Question Answering (QA) is one of such tasks that require a\ncombination of multiple simpler tasks such as Coreference Resolu-\ntion and Relation Modeling to arrive at the correct answer. We take\nSQuAD bAbI\nQuestion What is a common punishment in the UK and Ireland? What is Emily afraid of?\nAnswer detention cats\nContext\nCurrently detention is one of the most common pun-\nishments in schools in the United States, the UK, Ire-\nland, Singapore and other countries. It requires the\npupil to remain in school at a given time in the school\nday (such as lunch, recess or after school); or even to attend\nschool on a non-school day, e.g. \"Saturday detention\" held at\nsome schools. During detention, students normally have to\nsit in a classroom and do work, write lines or a punishment\nessay, or sit quietly.\nWolves are afraid of cats.\nSheep are afraid of wolves.\nMice are afraid of sheep.\nGertrude is a mouse.\nJessica is a mouse.\nEmily is a wolf.\nCats are afraid of sheep.\nWinona is a wolf.\nTable 1: Samples from SQuAD dataset (left) and from Basic Deduction task (#15) of the bAbI dataset (right). Supporting Facts\nare printed in bold. The SQuAD sample can be solved by word matching and entity resolution, while the bAbI sample requires\na logical reasoning step and cannot be solved by simple word matching. Figures in the further analysis will use these examples\nwhere applicable.\nthree current Question Answering datasets into account, namely\nSQUAD [31], bAbI [39] and HotpotQA [40]. We intentionally choose\nthree very different datasets to diversify the results of our analysis.\nSQuAD. As one of the most popular QA tasks the SQuAD dataset\ncontains 100,000 natural question-answer pairs on 500 Wikipedia\narticles. A new version of the dataset called SQuAD 2.0 [ 30] ad-\nditionally includes unanswerable questions. We use the previous\nversion SQuAD 1.1 for our experiments to concentrate on the base\ntask of span prediction. In 2018 an ensemble of fine-tuned BERT\nmodels has outperformed the Human Baseline on this task. The\ndataset is characterised by questions that mainly require to resolve\nlexical and syntactic variations.\nHotpotQA. This Multihop QA task contains 112,000 natural question-\nanswer pairs. The questions are especially designed to combine\ninformation from multiple parts of a context. We focus on the dis-\ntractor-task of HotpotQA, in which the context is composed of both\nsupporting and distracting facts with an average size of 900 words.\nAs the pre-trained BERT model is restricted to an input size of 512\ntokens, we reduce the amount of distracting facts by a factor of\n2.7. We also leave out yes/no-questions (7% of questions) as they\nrequire additional specific architecture, diluting our analysis.\nbAbI. The QA bAbI tasks are a set of artificial toy tasks developed\nto further understand the abilities of neural models. The 20 tasks\nrequire reasoning over multiple sentences (Multihop QA) and are\nmodeled to include Positional Reasoning, Argument Relation Ex-\ntraction and Coreference Resolution. The tasks strongly differ from\nthe other QA tasks in their simplicity (e.g. vocabulary size of 230\nand short contexts) and the artificial nature of sentences.\n4.2 BERT and GPT-2\nIn this section we briefly discuss the models our analysis is based\non, BERT [ 8] and GPT-2 [ 29]. Both of these models are Trans-\nformers that extend and improve on a number of different recent\nideas. These include previous Transformer models [36][28], Semi-\nSupervised Sequence Learning [ 5], ELMo [ 26] and ULMFit [ 13].\nSQuAD HotpotQA Distr. HotpotQA SP bAbI\nBaseline 77.2 66.0 66.0 42.0\nBERT 87.9 56.8 80.4 93.4\nGPT-2 74.9 54.0 64.6 99.9\nTable 2: Results from fine-tuning BERT on QA tasks. Base-\nlines are: BIDAF [32] for SQuAD, the LSTM Baseline for bAbI\nfrom [39] and the HotpotQA baseline from [40] for the two\nHotpot tasks.\nBoth have a similar architecture, and they each represent one half\nof the original Encoder-Decoder Transformer [36]. While GPT-2,\nlike its predecessor, consists of only the decoder half, BERT uses a\nbidirectional variant of the original encoder. Each consists of a large\nnumber of Transformer blocks (12 for small GPT-2 and bert-base,\n24 for bert-large), that in turn consist of a Self-Attention module,\nFeed Forward network, Layer Normalization and Dropout. On top\nof these encoder stacks we add a Sequence Classification head for\nthe bAbI dataset and a Span Prediction head for the other datasets.\nFigure 1 depicts how these models integrate into our probing setup.\n4.3 Applying BERT to Question Answering\nWe base our training code on the Pytorch implementation of BERT\navailable at [14]. We use the publicly available pre-trained BERT\nmodels for our experiments. In particular, we study the monolin-\ngual models bert-base-uncased and bert-large. For GPT-2 the small\nmodel (117M Parameters) is used, as a larger model has not yet\nbeen released. However, we do not apply these models directly, and\ninstead fine-tune them on each of our datasets.\nTraining Modalities . Regarding hyperparameters, we tune the\nlearning rate, batch size and learning rate scheduling according to\na grid search and train each model for 5 epochs with evaluations on\nthe development set every 1000 iterations. We then select the model\nof the best evaluation for further analysis. The input length chosen\nis 384 tokens for the bAbI and SQuAD tasks and the maximum of\nFigure 2: Probing Task results of BERT-base models in\nmacro averaged F1 (Y-axis) over all layers (X-axis). Fine-\ntuning barely affects accuracy on NEL, COREF and REL in-\ndicating that those tasks are already sufficiently covered\nby pre-training. Performances on the Question Type task\nshows its relevancy for solving SQuAD, whereas it is not re-\nquired for the bAbI tasks and the information is lost.\nFigure 3: Probing Task results of BERT-large models in\nmacro averaged F1 (Y-axis) over all layers (X-axis). Perfor-\nmance of HotpotQA model is mostly equal to the model\nwithout fine-tuning, but information is dropped in last lay-\ners in order to fit the Answer Selection task.\n512 tokens permitted by the pre-trained models’ positional embed-\nding for the HotpotQA tasks. For bAbI we evaluate both models\nthat are trained on a single bAbI task and also a multitask model,\nthat was trained on the data of all 20 tasks. We further distinguish\nbetween two settings: Span prediction, which we include for better\ncomparison with the other datasets, and Sequence Classification,\nwhich is the more common approach to bAbI. In order to make\nspan prediction work, we append all possible answers to the end of\nthe base context, since not all answers can be found in the context\nby default. For HotpotQA, we also distinguish between two tasks.\nIn the HotpotQA Support Only (SP) task, we use only the sentences\nlabeled as Supporting Facts as the question context. This simpli-\nfies the task, but more importantly it reduces context length and\nincreases our ability to distinguish token vectors. Our HotpotQA\nDistractor task is closer to the original HotpotQA task. It includes\ndistracting sentences in the context, but only enough to not exceed\nthe 512 token limit.\n5 RESULTS AND DISCUSSION\nTraining Results. Table 2 shows the evaluation results of our best\nmodels. Accuracy on the SQuAD task is close to human perfor-\nmance, indicating that the model can fulfill all sub-tasks required\nto answer SQuAD’s questions. As expected the tasks derived from\nHotpotQA prove much more challenging, with the distractor setting\nbeing the most difficult to solve. Unsurprisingly too, bAbI was easily\nsolved by both BERT and GPT-2. While GPT-2 performs signifi-\ncantly worse in the more difficult tasks of SQuAD and HotpotQA,\nit does considerably better on bAbi reducing the validation error\nto nearly 0. Most of BERT’s error in the bAbI multi-task setting\ncomes from tasks 17 and 19. Both of these tasks require positional\nor geometric reasoning, thus it is reasonable to assume that this is\na skill where GPT-2 improves on BERT’s reasoning capabilities.\nPresentation of Analysis Results . The qualitative analysis of\nvector transformations reveals a range of recurring patterns. In\nthe following, we present these patterns by two representative\nsamples from the SQuAD and bAbI task dataset described in Table\n1. Examples from HotpotQA can be found in the supplementary\nmaterial as they require more space due to the larger context.\nResults from probing tasks are displayed in Figures 2 and 3.\nWe compare results in macro-averaged F1 over all network layers.\nFigure 2 shows results from three models of BERT-base with twelve\nlayers: Fine-tuned on SQuAD,on bAbI tasks and without fine-tuning.\nFigure 3 reports results of two models based on BERT-large with\n24 layers: Fine-tuned on HotpotQA and without fine-tuning.\n5.1 Phases of BERT’s Transformations\nThe PCA representations of tokens in different layers suggest that\nthe model is going through multiple phases while answering a\nquestion. We observe these phases in all three selected QA tasks de-\nspite their diversity. These findings are supported by results of the\napplied probing tasks. We present the four phases in the following\nparagraphs and describe how our experimental results are linked.\n(1) Semantic Clustering. Early layers within the BERT-based mod-\nels group tokens into topical clusters. Figures 4a and 5a reveal this\nbehaviour and show the second layer of each model. Resulting\nvector spaces are similar in nature to embedding spaces from e.g.\nWord2Vec [24] and hold little task-specific information. Therefore,\nthese initial layers reach low accuracy on semantic probing tasks,\nas shown in Figures 2 and 3. BERT’s early layers can be seen as\nan implicit replacement of embedding layers common in neural\nnetwork architectures.\n(2) Connecting Entities with Mentions and Attributes . In the\nmiddle layers of the observed networks we see clusters of entities\nthat are less connected by their topical similarity. Rather, they\nare connected by their relation within a certain input context.\nThese task-specific clusters appear to already include a filtering of\nquestion-relevant entities. Figure 4b shows a cluster with words\nlike countries, schools, detention and country names, in which ’de-\ntention’ is a common practice in schools. This cluster helps to solve\nthe question \"What is a common punishment in the UK and Ireland?\" .\nAnother question-related cluster is shown in Figure 5b. The main\n(a) SQuAD Phase 1: Semantic Clustering. We observe a topical cluster with\n’school’-related and another with ’country’-related tokens.\n(b) SQuAD Phase 2: Entity Matching. The marked cluster contains matched to-\nkens ’detention’, ’schools’ and the countries that are applying this practice.\n(c) SQuAD Phase 3: Question-Fact Matching. The question tokens form a\ncluster with the Supporting Fact tokens.\n(d) SQuAD Phase 4: Answer Extraction. The answer token ’detention’ is separated\nfrom other tokens.\nFigure 4: BERT’s Transformation Phases for the SQuAD example from Table 1. Answer token: Red diamond-shaped. Question\nTokens: Orange star-shaped. Supporting Fact tokens: Dark Cyan. Prominent clusters are circled. The model passes through\ndifferent phases in order to find the answer token, which is extracted in the last layer (#11).\nchallenge within this sample is to identify the two facts thatEmily is\na wolf and Wolves are afraid of cats . The highlighted cluster implies\nthat Emily has been recognized as a relevant entity that holds a\nrelation to the entity Wolf. The cluster also contains similar entity\nmentions e.g. the plural formWolves. We observe analogous clusters\nin the HotpotQA model, which includes more cases of coreferences.\nThe probing results support these observations. The model’s abil-\nity to recognize entities (Named Entity Labeling), to identify their\nmentions (Coreference Resolution) and to find relations (Relation\nRecognition) improves until higher network layers. Figure 6 visu-\nalizes these abilities. Information about Named Entities is learned\nfirst, whereas recognizing coreferences or relations are more diffi-\ncult tasks and require input from additional layers until the model’s\nperformance peaks. These patterns are equally observed in the re-\nsults from BERT-base models and BERT-large models.\n(3) Matching Questions with Supporting Facts . Identifying rel-\nevant parts of the context is crucial for QA and Information Re-\ntrieval in general. In traditional pipeline models this step is often\nachieved by filtering context parts based on their similarity to the\nquestion [17]. We observe that BERT models perform a compara-\nble step by transforming the tokens so that question tokens are\nmatched onto relevant context tokens. Figures 4c and 5c show two\nexamples in which the model transforms the token representation\nof question and Supporting Facts into the same area of the vector\n(a) bAbI Phase 1: Semantic Clustering. Names and animals are clustered.\n (b) bAbI Phase 2: Entity Matching. The determining relation between the entities\n’Emily’ and ’Wolf’ is resolved in a cluster.\n(c) bAbI Phase 3: Question-Fact Matching. In this case the question tokens\nmatch with a subset of Supporting Facts (’Wolves are afraid of cats’). The\nsubset is decisive of the answer.\n(d) bAbI Phase 4: Answer Extraction. The answer token ’cats’ is separated from\nother tokens.\nFigure 5: BERT’s Transformation Phases for the bAbI example from Table 1. The phases are equal to what we observe in\nSQuAD and HotpotQA samples: The formed clusters in the first layers show general language abilities, while the last layers\nare more task-specific.\nspace. Some samples show this behaviour in lower layers. How-\never, results from our probing tasks show that the models hold the\nstrongest ability to distinguish relevant from irrelevant information\nwrt. the question in their higher layers. Figure 2 demonstrates how\nthe performance for this task increases over successive layers for\nSQuAD and bAbI. Performance of the fine-tuned HotpotQA model\nin Figure 3 is less distinct from the model without fine-tuning and\ndoes not reach high accuracy. 2 This inability indicates why the\nBERT model does not perform well on this dataset as it is not able\nto identify the correct Supporting Facts.\n2Note that the model only predicts the majority class in the first five layers and thereby\nreaches a decent accuracy without really solving the task.\nThe vector representations enable us to tell which facts a model\nconsidered important (and therefore matched with the question).\nThis helps retracing decisions and makes the model moretransparent.\n(4) Answer Extraction . In the last network layers we see that\nthe model dissolves most of the previous clusters. Here, the model\nseparates the correct answer tokens, and sometimes other possible\ncandidates, from the rest of the tokens. The remaining tokens form\none or multiple homogeneous clusters. The vector representation\nat this point is largely task-specific and learned during fine-tuning.\nThis becomes visible through the performance drop in general NLP\nprobing tasks, visualized in Figure 6. We especially observe this\nFigure 6: Phases of BERT’s language abilities. Higher saturation denotes higher accuracy on probing tasks. Values are normal-\nized over tasks on the Y-axis. X-axis depicts layers of BERT. NEL: Named Entity Labeling, COREF: Coreference Resolution, REL:\nRelation Classification, QUES: Question Type Classification, SUP: Supporting Fact Extraction. All three tasks exhibit similar\npatterns, except from QUES, which is solved earlier by the HotpotQA model based on BERT-large. NEL is solved first, while\nperformance on COREF and REL peaks in later layers. Distinction of important facts (SUP) happens within the last layers.\nloss of information in last-layer representations in the large BERT-\nmodel fine-tuned on HotpotQA, as shown in Figure 3. While the\nmodel without fine-tuning still performs well on tasks like NEL or\nCOREF, the fine-tuned model loses this ability.\nAnalogies to Human Reasoning . The phases of answering ques-\ntions can be compared to the human reasoning process, including\ndecomposition of input into parts [1]. The first phase of semantic\nclustering represents our basic knowledge of language and the sec-\nond phase how a human reader builds relations between parts of the\ncontext to connect information needed for answering a question.\nSeparation of important from irrelevant information (phase 3) and\ngrouping of potential answer candidates (phase 4) are also known\nfrom human reasoning. However, the order of these steps might\ndiffer from the human abstraction. One major difference is that\nwhile humans read sequentially, BERT can see all parts of the input\nat once. Thereby it is able to run multiple processes and phases\nconcurrently depending on the task at hand. Figure 6 shows how\nthe tasks overlap during the answering process.\n5.2 Comparison to GPT-2\nIn this section we compare our insights from the BERT models\nto the GPT-2 model. We focus on the qualitative analysis of to-\nken representations and leave the application of probing tasks for\nfuture work. One major difference between GPT-2’s and BERT’s\nhidden states is that GPT-2 seems to give particular attention to the\nfirst token of a sequence. While in our QA setup this is often the\nquestion word, this also happens in cases where it is not. During\ndimensionality reduction this results in a separation of two clusters,\nnamely the first token and all the rest. This problem holds true\nfor all layers of GPT-2 except for the Embedding Layer, the first\nTransformer block and the last one. For this reason we mask the\nfirst token during dimensionality reduction in further analysis.\nFigure 7 shows an example of the last layer’s hidden state for\nour bAbI example. Like BERT, GPT-2 also separates the relevant\nSupporting Facts and the question in the vector space. Additionally,\nFigure 7: bAbI Example of the Answer Extraction phase in\nGPT-2. Both the question and Supporting Fact are extracted,\nbut the correct answer is not fully separated as in BERT’s\nlast layers. Also a potential candidate Supporting Fact in\n\"Sheep are afraid of Wolves\" is separated as well.\nGPT-2 extracts another sentence, which is not a Supporting Fact, but\nis similar in meaning and semantics. In contrast to BERT, the correct\nanswer \"cats\" is not particularly separated and instead simply left\nas part of its sentence. These findings in GPT-2 suggest that our\nanalysis extends beyond the BERT architecture and hold true for\nother Transformer networks as well. Our future work will include\nmore probing tasks to confirm this initial observation.\n5.3 Additional Findings\nObservation of Failure States . One important aspect of explain-\nable Neural Networks is to answer the questions of when, why, and\nhow the network fails. Our visualizations are not only able to show\nFigure 8: BERT SQuAD example of a falsely selected answer\nbased on the matching of the wrong Supporting Fact. The\npredicted answer ’lectures’ is matched onto the question as\na part of this incorrect fact (magenta), while the actual Sup-\nporting Fact (cyan) is not particularly separated.\nsuch failure states, but even the rough difficulty of a specific task\ncan be discerned by a glance at the hidden state representations.\nWhile for correct predictions the transformations run through the\nphases discussed in previous sections, for wrong predictions there\nexist two possibilities: If a candidate answer was found that the\nnetwork has a reasonable amount of confidence in, the phases will\nlook very similar to a correct prediction, but now centering on\nthe wrong answer. Inspecting early layers in this case can give\ninsights towards the reason why the wrong candidate was chosen,\ne.g. wrong Supporting Fact selected, misresolution of coreferences\netc. An example of this is shown in Figure 8, where a wrong answer\nis based on the fact that the wrong Supporting Fact was matched\nwith the question in early layers.\nIf network confidence is low however, which is often the case\nwhen the predicted answer is far from the actual answer, the trans-\nformations do not go through the phases discussed earlier. The\nvector space is still transformed in each layer, but tokens are mostly\nkept in a single homogeneous cluster. In some cases, especially\nwhen the confidence of the network is low, the network maintains\nPhase (1), ’Semantic Clustering’ analogue to Word2Vec, even in\nlater layers. An example is depicted in the supplementary material.\nImpact of Fine-tuning . Figures 2 and 3 show how little impact\nfine-tuning has on the core NLP abilities of the model. The pre-\ntrained model already holds sufficient information about words and\ntheir relations, which is the reason it works well in multiple down-\nstream tasks. Fine-tuning only applies small weight changes and\nforces the model to forget some information in order to fit specific\ntasks. However, the model does not forget much of the previously\nlearned encoding when fitting the QA task, which indicates why\nthe Transfer Learning approach proves successful.\nFigure 9: BERT SQuAD example Layer 7. Tokens are color-\ncoded by sentence. This visualization shows that tokens are\nclustered by their original sentence membership suggesting\nfar reaching importance of the positional embedding.\nMaintained Positional Embedding . It is well known that the\npositional embedding is a very important factor in the performance\nof Transformer networks. It solves one major problem that Trans-\nformers have in comparison with RNNs, that they lack sequential\ninformation [36]. Our visualizations support this importance and\nshow that even though the positional embedding is only added once\nbefore the first layer, its effects are maintained even into very late\nlayers depending on the task. Figure 9 demonstrates this behavior\non the SQuAD dataset.\nAbilities to resolve Question Type . The performance curves re-\ngarding the Question Type probing task illustrate another inter-\nesting result. Figure 2 demonstrates that the model fine-tuned on\nSQuAD outperforms the base model from layer 5 onwards. This in-\ndicates the relevancy of resolving the question type for the SQuAD\ntask, which leads to an improved ability after fine-tuning. The\nopposite is the case for the model fine-tuned on the bAbI tasks,\nwhich loses part of its ability to distinguish question types during\nfine-tuning. This is likely caused by the static structure of bAbI\nsamples, in which the answer candidates can be recognized by sen-\ntence structure and occurring word patterns rather than by the\nquestion type. Surprisingly, we see that the model fine-tuned on\nHotpotQA does not outperform the model without fine-tuning in\nFigure 3. Both models can solve the task in earlier layers, which\nsuggests that the ability to recognize question types is pre-trained\nin BERT-large.\n6 CONCLUSION AND FUTURE WORK\nOur work reveals important findings about the inner functioning\nof Transformer networks. The impact of these findings and how\nfuture work can build upon them is described in the following:\nInterpretability. The qualitative analysis of token vectors reveals\nthat there is indeed interpretable information stored within the\nhidden states of Transformer models. This information can be used\nto identify misclassified examples and model weaknesses. It also\nprovides clues about which parts of the context the model consid-\nered important for answering a question - a crucial part of decision\nlegitimisation. We leave the development of methods to further\nprocess this information for future work.\nTransferability. We further show that lower layers might be more\napplicable to certain problems than later ones. For a Transfer Learn-\ning task, this means layer depth should be chosen individually\ndepending on the task at hand. We also suggest further work re-\ngarding skip connections in Transformer layers to examine whether\ndirect information transfer between non-adjacent layers (that solve\ndifferent tasks) can be of advantage.\nModularity. Our findings support the hypothesis that not only do\ndifferent phases exist in Transformer networks, but that specific\nlayers seem to solve different problems. This hints at a modularity\nthat can potentially be exploited in the training process. For exam-\nple, it could be beneficial to fit parts of the network to specific tasks\nin pre-training, instead of using an end-to-end language model task.\nOur work aims towards revealing some of the internal processes\nwithin Transformer-based models. We suggest to direct further\nresearch at thoroughly understanding state-of-the-art models and\nthe way they solve downstream tasks, in order to improve on them.\nACKNOWLEDGMENTS\nOur work is funded by the European Unions Horizon 2020 research\nand innovation programme under grant agreement No. 732328\n(FashionBrain) and by the German Federal Ministry of Education\nand Research (BMBF) under grant agreement No. 01UG1735BX\n(NOHATE) and No. 01MD19003B (PLASS).\nREFERENCES\n[1] Lotfi A Zadeh. 1997. Zadeh, L.A.: Toward a Theory of Fuzzy Information Granu-\nlation and Its Centrality in Human Reasoning and Fuzzy Logic. Fuzzy Sets and\nSystems. ELSEVIER Fuzzy Sets and Systems 90 (1997).\n[2] Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James R. Glass.\n2017. What do Neural Machine Translation Models Learn about Morphology?.\nIn Proceedings of ACL 2017 .\n[3] Pierre Comon. 1994. Independent component analysis, A new concept? Signal\nProcessing 36 (1994).\n[4] Alexis Conneau and Douwe Kiela. 2018. SentEval: An Evaluation Toolkit for\nUniversal Sentence Representations. In Proceedings of LREC 2018 .\n[5] Andrew M. Dai and Quoc V. Le. 2015. Semi-supervised Sequence Learning. In\nProceedings of NIPS 2015 .\n[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Attentive Language Models Beyond a\nFixed-Length Context. CoRR (2019).\n[7] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz\nKaiser. 2018. Universal Transformers. In Proceedings of SMACD 2018 .\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nCoRR (2018).\n[9] F. K. Došilović, M. Brčić, and N. Hlupić. 2018. Explainable artificial intelligence:\nA survey. In MIPRO 2018 .\n[10] Karl Pearson F.R.S. 1901. LIII. On lines and planes of closest fit to systems of\npoints in space. The London, Edinburgh, and Dublin Philosophical Magazine and\nJournal of Science 2 (1901).\n[11] Yoav Goldberg. 2019. Assessing BERT’s Syntactic Abilities. CoRR (2019).\n[12] Riccardo Guidotti, Anna Monreale, Franco Turini, Dino Pedreschi, and Fosca\nGiannotti. 2018. A Survey Of Methods For Explaining Black Box Models. ACM\nComput. Surv. (2018).\n[13] Jeremy Howard and Sebastian Ruder. 2018. Fine-tuned Language Models for Text\nClassification. CoRR (2018).\n[14] Huggingface. 2018. pytorch-pretrained-BERT. (2018). https://github.com/\nhuggingface/pytorch-pretrained-BERT\n[15] Dieuwke Hupkes, Sara Veldhoen, and Willem H. Zuidema. 2017. Visualisation\nand ’diagnostic classifiers’ reveal how recurrent and recursive neural networks\nprocess hierarchical structure. In Proceedings of IJCAI 2018 .\n[16] Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Pro-\nceedings of NAACL 2019 .\n[17] Dan Jurafsky and James H. Martin. 2009. Speech and Language Processing: An In-\ntroduction to Natural Language Processing, Computational Linguistics, and Speech\nRecognition, Chapter 23 . Prentice Hall series in artificial intelligence, Vol. 2. Pren-\ntice Hall, Pearson Education International.\n[18] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding Neural Networks\nthrough Representation Erasure. CoRR (2016).\n[19] Xin Li and Dan Roth. 2002. Learning Question Classifiers. In Proceedings of\nCOLING 2002 .\n[20] Zachary Chase Lipton. 2016. The Mythos of Model Interpretability. ACM Queue\n(2016).\n[21] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, and Noah A.\nSmith. 2019. Linguistic Knowledge and Transferability of Contextual Represen-\ntations. In Proceedings of NAACL 2019 .\n[22] Stuart P. Lloyd. 1982. Least squares quantization in PCM. IEEE Trans. Information\nTheory (1982).\n[23] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018.\nThe Natural Language Decathlon: Multitask Learning as Question Answering.\nCoRR (2018).\n[24] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Esti-\nmation of Word Representations in Vector Space. InWorkshop Track Proceedings\nof ICLR 2013 .\n[25] Tasha Nagamine, Michael Seltzer, and Nima Mesgarani. 2015. Exploring How\nDeep Neural Networks Form Phonemic Categories. In Proceedings of INTER-\nSPEECH 2015 .\n[26] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. In Proceedings of NAACL-HLT 2018 .\n[27] Yifan Qiao, Chenyan Xiong, Zheng-Hao Liu, and Zhiyuan Liu. 2019. Understand-\ning the Behaviors of BERT in Ranking. CoRR (2019).\n[28] Alec Radford. 2018. Improving Language Understanding by Generative Pre-\nTraining. OpenAI Blog (2018).\n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners. OpenAI\nBlog (2019).\n[30] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know:\nUnanswerable Questions for SQuAD. In Proceedings of ACL 2018 .\n[31] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100, 000+ Questions for Machine Comprehension of Text. InProceedings\nof EMNLP 2016 .\n[32] Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. [n.\nd.]. Bidirectional Attention Flow for Machine Comprehension. In Proceedings of\nICLR 2017 .\n[33] Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does String-Based Neural MT\nLearn Source Syntax?. In Proceedings of EMNLP 2016 .\n[34] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy,\nNajoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie\nPavlick. 2019. What do you learn from context? Probing for sentence structure\nin contextualized word representations. In Proceedings of ICLR 2019 .\n[35] Laurens van der Maaten. 2009. Learning a Parametric Embedding by Preserving\nLocal Structure. In Proceedings of AISTATS 2009 .\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Proceedings of NIPS 2017 .\n[37] Ellen Voorhees. 2001. Overview of TREC 2001. In Proceedings of TREC 2001 .\n[38] Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin,\nSameer Pradhan, Lance Ramshaw, and Nianwen Xue. 2011. OntoNotes: A Large\nTraining Corpus for Enhanced Processing . Springer, Heidelberg.\n[39] Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2016. Towards\nAI-Complete Question Answering: A Set of Prerequisite Toy Tasks. InProceedings\nof ICLR 2016 .\n[40] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan\nSalakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for\nDiverse, Explainable Multi-hop Question Answering. In Proceedings of EMNLP\n2018.\n[41] Quan-shi Zhang and Song-chun Zhu. 2018. Visual interpretability for deep\nlearning: a survey. Frontiers of IT & EE (2018).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7764142751693726
    },
    {
      "name": "Transformer",
      "score": 0.741447389125824
    },
    {
      "name": "Security token",
      "score": 0.7357600927352905
    },
    {
      "name": "Encoder",
      "score": 0.7036244869232178
    },
    {
      "name": "Question answering",
      "score": 0.6134443879127502
    },
    {
      "name": "Artificial intelligence",
      "score": 0.537952721118927
    },
    {
      "name": "Natural language processing",
      "score": 0.4725715219974518
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}