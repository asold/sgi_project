{
  "title": "Improving Document Relevancy Using Integrated Language Modeling Techniques",
  "url": "https://openalex.org/W2294504381",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2215533735",
      "name": "Vimala Balakrishnan",
      "affiliations": [
        "University of Malaya",
        "Information Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2172164943",
      "name": "Norshima Humaidi",
      "affiliations": [
        "Universiti Teknologi MARA"
      ]
    },
    {
      "id": "https://openalex.org/A5021037627",
      "name": "Ethel Lloyd-Yemoh",
      "affiliations": [
        "University of Malaya",
        "Information Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2215533735",
      "name": "Vimala Balakrishnan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2172164943",
      "name": "Norshima Humaidi",
      "affiliations": [
        "Universiti Teknologi MARA"
      ]
    },
    {
      "id": "https://openalex.org/A5021037627",
      "name": "Ethel Lloyd-Yemoh",
      "affiliations": [
        null,
        "University of Malaya"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2093390569",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W6637101025",
    "https://openalex.org/W2047959359",
    "https://openalex.org/W6646596273",
    "https://openalex.org/W2250311666",
    "https://openalex.org/W2069917620",
    "https://openalex.org/W6816155459",
    "https://openalex.org/W6647138040",
    "https://openalex.org/W2064262426",
    "https://openalex.org/W2091944322",
    "https://openalex.org/W1969158886",
    "https://openalex.org/W6674809819",
    "https://openalex.org/W2074357889",
    "https://openalex.org/W6686852135",
    "https://openalex.org/W6674830054",
    "https://openalex.org/W2159125322",
    "https://openalex.org/W6684932815",
    "https://openalex.org/W2403513632",
    "https://openalex.org/W1946297417",
    "https://openalex.org/W2006477783",
    "https://openalex.org/W1569899956",
    "https://openalex.org/W2101415761",
    "https://openalex.org/W1978394996",
    "https://openalex.org/W4234752074",
    "https://openalex.org/W173956503",
    "https://openalex.org/W4241787188",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W2186322444",
    "https://openalex.org/W1987680958",
    "https://openalex.org/W2169104544",
    "https://openalex.org/W2098162425",
    "https://openalex.org/W1660390307",
    "https://openalex.org/W2097744935",
    "https://openalex.org/W4233401620",
    "https://openalex.org/W4246960682"
  ],
  "abstract": "This paper presents an integrated language model to improve document relevancy for text-queries. To be precise, an integrated stemming-lemmatization (S-L) model was developed and its retrieval performance was compared at three document levels, that is, at top 5, 10 and 15. A prototype search engine was developed and fifteen queries were executed. The mean average precisions revealed the S-L model to outperform the baseline (i.e. no language processing), stemming and also the lemmatization models at all three levels of the documents. These results were also supported by the histogram precisions which illustrated the integrated model to improve the document relevancy. However, it is to note that the precision differences between the various models were insignificant. Overall the study found that when language processing techniques, that is, stemming and lemmatization are combined, more relevant documents are retrieved.",
  "full_text": "Improving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n45 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \nIMPROVING DOCUMENT RELEVANCY USING INTEGRATED LANGUAGE MODELING \nTECHNIQUES \n  \nVimala Balakrishnan1, Norshima Humaidi2 and Ethel Lloyd-Yemoh3 \n \n1, 3 Department of Information Systems, \nFaculty of Computer Science and Information Technology, \nUniversity of Malaya, 50603 Kuala Lumpur, Malaysia. \n \n2 Faculty of Business and Management, Universiti Teknologi MARA, Puncak Alam Campus. \n \n \nE-mail: vimala.balakrishnan@um.edu.my1, norshima958@puncakalam.uitm.edu.my2, \nethel_lloyd@siswa.um.edu.my3 \n \n \n \nABSTRACT \nThis paper presents an integrated language model to improve document relevancy for text-queries. To be \nprecise, an integrated stemming-lemmatization (S-L) model was developed and its retrieval performance was \ncompared at three document levels, that is, at top 5, 10 and 15. A prototype search engine was developed and \nfifteen queries were executed. The mean average precisions revealed   the S-L model to outperform the baseline \n(i.e. no language processing), stemming and also the lemmatization models at all three levels of the documents. \nThese results were also supported by the histogram precisions which illustrated the integrated model to improve \nthe document relevancy. However, it is to note that the precision differences between the various models were \ninsignificant. Overall the study found that when language processing techniques, that is, stemming and \nlemmatization are combined, more relevant documents are retrieved. \n \nKeywords: Information retrieval, document relevancy, language modeling, stemming, lemmatization, mean \naverage precision \n \n1.0 INTRODUCTION \nThe use of internet all over the world has caused information size to increase, hence making it possible for large \nvolumes of information to be retrieved by the users. However, this phenomenon also makes it difficult for users \nto find relevant information, therefore proper information retrieval techniques are needed. Information retrieval \ncan be defined as “a problem-oriented discipline concerned with the problem of the effective and efficient \ntransfer of desired information between human generator and human user” [1]. In short, information \nretrieval aims to provide users with those documents that will satisfy their information need. \nMany information retrieval algorithms were proposed, and some of the popular ones include the traditional \nBoolean model (i.e. based on binary decisions), vector space model (i.e. compares user queries with documents \nfound in collections and computes their similarities), and probabilistic model (i.e. based on the probability \ntheory to model uncertainties involved in retrieving data), among others. Over the years, information retrieval \nhas evolved to include text retrieval in different languages, and thus giving birth to language models. The \nlanguage model is particularly concerned with identifying how likely it is for a particular string in a specific \nlanguage to be repeated [2]. A popular technique used in the language model is the N-gram model which \npredicts a preceding word based on previous N-1 words [3]. Other popular techniques include stemming and \nlemmatization. \nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n46 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \n \nStemming relates to deciding which documents in a collection should be retrieved to satisfy a user's need for \ninformation. Stemming is used to remove derivational suffixes as well as inflections (i.e. adding inflectional \nmorphemes (i.e. smallest unit of meaning) to a word which indicates grammatical information, so that word \nvariants can be conflated into the same stems). Stop-words are used in eliminating high frequency terms when \ndocuments are being searched, and then the stemming procedure can be used to conflate all word variants back \nto their basic rules using an algorithm [4]. Words found in documents and user queries usually have different \npossible meanings, and therefore care must be taken when retrieving information. Different variants of a word \nmight appear in a document and it is up to the retrieval system to use natural language processing methods in \norder to retrieve the required texts in document collections. The lemmatization, in contrary, uses vocabulary and \nmorphological analysis of words to process a query. It removes inflectional endings and analyzes if query words \nare used as verbs or nouns. Lemmatization also helps to match synonyms using thesaurus, so that when one \nsearches for “hot” the word “warm” is matched as well.  \nBoth stemming and lemmatization play very important roles in increasing relevance and recall capabilities of a \nretrieval system. When these techniques are used, the number of indexes used is reduced as the system will be \nusing one index to present a number of similar words having the same root. Furthermore, the dictionary size is \nalso reduced as all the distinct terms representing a set of documents will be replaced with a single term. Most of \nthe studies in this field focused on stemming and lemmatization separately, and very few studies have looked \ninto integrating them. The current study aims to propose an integrated model, namely stemming-lemmatization \n(S-L) model to improve document retrievals. The performance of the S-L model was compared with a baseline \nalgorithm (i.e. with no language processing), and also with stemming and lemmatization models.   \nThe remainder of the paper is structured as follows: the following section explains stemming, followed by \nlemmatization and the integrated S-L model. The evaluation setup is presented next, followed by results and \ndiscussion. The conclusion and future works conclude the paper. \n2.0 STEMMING \nAs stated previously, stemming has a significant effect on both the efficiency and the effectiveness of \ninformation retrieval. Studies that have used stemming mechanisms reported better retrieval results, not only for \nEnglish based texts, but also those in other languages. For instance, an experiment was conducted on \nmultilingual information retrieval using the thesaurus–based query expansion technique, with results indicating \nthe proposed model to be able to retrieve Italian documents when queries are written in German [5]. Similarly, a \ncombination of translation probabilities from different sources was used to improve search results across \ndifferent languages using a stemming algorithm on the TREC collection [6]. \nIn another study, Ref. [7] developed a stemmer for Tigrinya (i.e. language spoken in the east African countries \nof Eritrea and Ethiopia) words, by combining rule-based and dictionary based stemming techniques. The study \nfound the hybrid approach to have an average accuracy of approximately 89.3%. A rule-based stemmer for \nBengali language which uses stem dictionary for further validation was developed in [8], whereas an enhanced \nstemmer for Arabic was built by integrating light and dictionary based stemming [9]. The latter study found \ntheir stemmer to produce an average accuracy of approximately 96%. A study investigating the effect of \nstemming on Swedish texts found stemming improved retrieval precisions by at least 15% and relative recall by \n18%, depending on the set of rules and the document collection [10].  \nFinally, researchers in [11] compared the effects of four different stemming scenarios on Turkish texts, that is, \nwithout stemming, simple word truncation, the successor variety method and a lemmatizer-based stemmer. The \nstudy found all the three stemming techniques produced similar retrieval performances for the Turkish texts. \nMany other studies have also focused in developing stemmers for languages other than English, such as the \nMalay language [12], Indonesian language [13], Slovene [14], Turkish [10], German [15] and Dutch [16] etc., \nwith results showing improved retrieval performance when stemmers are used. \nThere are various stemming algorithms that have been developed to ensure that words are reduced to their root \nforms, thereby reducing the size of document dictionary, such as the Paice/Husk, Porter, Lovins, Dawson and \nKrovetz stemming algorithms. The Porter’s stemming algorithm is widely used [17], hence it is selected for the \ncurrent study. \nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n47 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \n \n \nFig. 1  Porter’s stemmer [17] \nPorter’s stemming uses a linear step stemmer which seeks to remove suffixes of words (see Fig.1). It basically \ncontains six steps, and at each step there are rules that need to be followed. Whenever conditions in the rules are \nmet, the program executes and the new stem goes through the process again. This loop continues until a suitable \nstem is found [17]. The stemmer deals with vowels (i.e. A, E, I, O, U and Y, only if Y is preceded by a \nconsonant), and consonants. An example of ‘Y’ being considered as a vowel is in the word  “fry”, where the \nconsonants will be  ‘F’ and ‘R’ as compared to the word “buy” where ‘B’ and ‘Y’ can be said to be consonants. \nConsonants are represented by ‘C’ and vowels by ‘V’; therefore words are represented by Formula (1) below: \n \n\nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n48 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \n [C] (VC) m [V]                                                                                                                                       (1) \nWhere  \n[ ]  optional presence of V and C \n(VC) m  the number of times VC repeats \nIn Fig. 1, the stemmer begins at step 1 where past participles and plurals are dealt with. Step 1.1 will work on \nplurals (i.e. words ending with 'es' or 's'), step 1.2 will be responsible for checking 'ed' and 'ing' and if these \nexist, it will then check  if the stem obtained after this removal is enough to form a word. This leads to the final \npart where the new stem is transferred to step 2 where ‘y’ is changed to ‘i’. Step 3 transforms double suffixes \ninto single ones, followed by step 4 which removes other endings. Step 5 will check if conditions for the stem \nare met, and finally the output stem is attained in step 6 [17]. \n \n3.0 LEMMATIZATION \nLemmatization basically removes inflectional endings and return the base or dictionary form of a word. It has \nalso been used in several languages for information retrieval. For instance, three lemmatizers were compared \nusing Turkish language [18]. The authors used (i) Oflazer’s morphological analyzer (OMA) which uses a two \nlevel morphology to build lexicon for a language, (ii) a fixed length truncation in which the first five and seven \ncharacters of a word are used as lemma whereas the rest are truncated, (iii) a dictionary-based Turkish \nlemmatizer and (iv) Zemberek which is a parser that loads binary roots and builds special direct acyclic word \ngraphs with it. Their findings showed that lemmatization improved information retrieval when a minimum \nnumber of terms and maximum lemma lengths are used. They also reported their model to have a better \nperformance than other approaches (i.e. no-lemmatization approach, OMA, Zemberek and fixed length \ntruncation).  \nIn a recent medical sector study, lemmatization along with text processing, information extraction and query \nexpansion were used, with results indicating improved precisions of the search results [19]. In another study, a \nself-learning lemmatizer that could process German documents using a full-form lexicon was designed [20]. \nTheir results showed that the self-learning context-aware lemmatizer for German was a great time saver \ncompared to Wikitionary (i.e. a multilingual web-based project to create a free content dictionary of all words in \nall languages).  \nA non-statistical Arabic lemmatizer using different knowledge resources to generate lemma and its relevant \nfeatures was investigated in [21]. The authors’ experimental results revealed their algorithm to achieve an \naccuracy of 94.8%. Other similar studies on Arabic texts were conducted using light lemmatizers [22, 23]. For \nexample, Ref. [23] presented an approach in which the algorithm searches text words, and finds one of the \npredetermined stop words to differentiate between verbs and nouns. \nIn a recent study, a dictionary- and corpus-independent statistical lemmatizer was developed to deal with out-of-\nvocabulary problem. The proposed algorithm was evaluated on four different datasets in Finnish, Swedish, \nGerman and English, and found it to reach 88 – 108% of the gold standard performance of a commercial \nlemmatizer [24]. Another study on Hungarian language was carried out using NLP-based lemmatization, and \ncompared with simple stemmers [25], with results showing improved mean average precisions for the former \ntechnique. \nOne of the problems of lemmatization is the compound word (i.e. words made of two or more words). This is \nbecause compound words will be split into their components during lemmatization, and each component treated \nas a single word. This may not be accurate in retrievals, therefore attempts have been made to combine both \nlemmatization and stemming. For instance, some studies found stemming used with clustering algorithms to be \nbeneficial in English texts [26], and also other languages [27, 28]. Gupta et al. [29] combined stemming with \npartial lemmatization for Hindi language with results indicating significant improvements than other traditional \napproaches. Another study compared stemming and lemmatization in clustering Finnish text documents, with \nresults indicating the use of lemmatization to be better than stemming [30]. \nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n49 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \nStemming and lemmatization play important roles in increasing relevance capability of a retrieval system. When \nthese techniques are used, it reduces the number of indexes used as it uses one index to present a number of \nsimilar words, which have the same root or stem. A lot of studies have looked into either using stemming or \nlemmatization and practical combinations, but fully integrating both of these algorithms is lacking. This is what \ninspired this study on integrating the stemming and lemmatization techniques. \n \n4.0 STEMMING-LEMMATIZATION MODEL \n \nFig. 2 shows the overall flow for the S-L model. \n \nFig. 2. The Stemming-Lemmatization flow \nIn the proposed S-L model, the Porter’s stemmer was used together with LemmaGen, which is a prebuilt \nlemmatizer. Fig. 2 shows that when users enter a search query, the system will first look for the whole query \ntext, and the retrieved documents will be kept in a buffer. If the query contains more than one word, the system \nwill then split the query into individual words. All suffixes are removed from each word and the root-word that \nis returned is passed to the dictionary to ensure that it is an actual root word, and not a stem. Once this is \ncompleted, the words will then be combined and their context searched to ensure that the context in which the \nquery was used has not been changed. This is handled by the Lzma.dll file that is included in the LemmaGen. \nOnce this check is completed, the derived query is returned to the search engine which will in turn pass it to the \ndatabase for query-document matching. All documents that match the words in the derived query are then \nretrieved and displayed to the user. \n\nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n50 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \n5.0 EVALUATION SETUP  \nThe models in the current study were compared as follows: \n- S-L versus tf-idf (i.e. the term frequency-inverse document frequency (tf-idf) ranking algorithm was used as the \nbaseline in this study. This traditional ranking algorithm checks the retrieved document to see how frequent the \nwords in the search query appears in the document. The document is considered to be more relevant if it \ncontains more occurrences of the query word. Further details of tf-idf can be found in [31]. \n- S-L versus stemming (i.e. the integrated model is compared against stemming) \n- S-L versus lemmatization (i.e. the integrated model is compared against lemmatization) \nAll the models above were compared at top 5, 10 and 15 document levels. The queries were selected manually \nas not all of them required language processing to take place, thus resulting in final 15 queries. \nA prototype search engine was developed in order to evaluate the proposed model. The test collection used was \nthe Communications of the ACM (CACM), which contains 3204 documents, 64 queries, a list of common \nwords that can be used as stop-words and the relevant judgements. Fig. 3 shows a sample screen display for \n“integral transformations”. \n \nFig. 3 Screen display for a search query \n6.0 EVALUATION METRICS \n \nThe effectiveness of the proposed model was evaluated using precision metric and precision histograms. The \naverage precision (AP) is basically the mean of the precision scores after each relevant document is retrieved, as \ndepicted in Formula (2) below: \nAP =    ∑ (୔@୩∗୰ୣ୪(୩))ొ౟సభ\n୲୭୲ୟ୪ ୰ୣ୪ୣ୴ୟ୬୲ ୢ୭ୡ୳୫ୣ୬୲ୱ ୤୭୰ ୟ ୯୳ୣ୰୷                                                                                                   (2)                                                                             \n\nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n51 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \nwhere N is the number of retrieved documents, and rel(k) indicates whether the kth document is relevant or not. \nThe AP is calculated for each query, and then they are averaged over all the queries to produce the mean \naverage precision (MAP). \nThe precision histograms were created to compare how well a model performs compared to another for each \nquery, based on Formula (3) below: [4]  \nܖܗܑܛܑ܋܍ܚ۾܀(ۯ/۰)  = ۾܀ۯ(࢏– )۾܀۰(  ࢏)                                                                           3) \n  \nwhere  \nRPrecision(୅/୆)  is R-precision of algorithm A over algorithm B. \nRP୅(i) refers to the R-Precision of algorithm A when the ith query is run. \nRP୆(i)  refers to the R-Precision of algorithm B when the ith query is run. \n \n7.0 RESULTS AND DISCUSSION \n \nAs stated in the preceding section, the models were compared at top 5, 10 and 15 levels for MAP. Table 1 \ndepicts the MAP values for all the models in the study. \nTable 1. MAP values for all the models \nAlgorithms MAP 5 MAP 10 MAP 15 \ntf-idf 0.693 0.609 0.522 \nStemming 0.708 0.613 0.568 \nLemmatization 0.715 0.623 0.579 \nS-L 0.723 0.628 0.610 \n \nFrom Table 1, it can be noted that all the language processing models outperformed the baseline algorithm. This \nis expected as the baseline algorithm returns results based on the search query only, without taking any further \nprocessing into consideration such as stemming or lemmatization [20, 29].  \nComparisons across the language models show that the integrated S-L model has the highest precisions at all the \nthree document levels as opposed to stemming and lemmatization alone. It can also be noted that the precision \nvalues decrease as the recall (i.e. the number of documents retrieved) increase. This is a common phenomenon \nin information retrieval systems. Further statistical analysis, particularly pair-wise comparisons were made \nbetween these techniques and findings revealed the differences to be insignificant at all three levels (i.e. p > \n0.05). \nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n52 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \n \nFig.  4. S-L Model-Baseline (tf-idf) \nThe precision histograms are only shown between the proposed S-L model against the baseline, stemming and \nlemmatization at top 10 level. Fig. 4 illustrates the histogram for the S-L model against tf-idf, with the S-L \nmodel performing 60% better than the baseline. In other words, nine out of fifteen queries were retrieved with \nbetter precisions compared to the baseline algorithm. The rest of the queries were retrieved at the same \nprecisions. This finding tallies with the MAP values depicted in Table 1, in which the S-L model outperformed \nthe baseline algorithm. \nFig. 5 shows the histogram for S-L model against stemming, with the former performing better than stemming \nfor four queries (i.e. T2, T3, T4 and T5). Stemming performed better for T11 (negative bar) whilst the remaining \nqueries were retrieved at the same precision levels.  \n \nFig. 5. S-L Model-Stemming \n \n\nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n53 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \n \nFig. 6. S-L Model-Lemmatization \n \nFinally, Fig. 6 shows the histogram for S-L model against lemmatization. The S-L model again performed better \nthan lemmatization for approximately 34% of the queries. \nOverall the evaluations show that the integrated model work better than stemming and lemmatization at all three \ndocument levels, that is, at top 5, 10 and 15. Although the precision differences were insignificant, it is believed \nthat the precisions can be further improved when more queries were used in testing. This is the major drawback \nof the current study as most of the queries in the CACM collection were not suitable for language models. \n \n8.0 CONCLUSION AND FUTURE WORK \nThis study looked into the stemming and lemmatization techniques with a further step of combining these two \ntechniques to improve precision in information retrieval. A Stemming-Lemmatization model (S-L model) was \ncreated to improve the document relevancy, and assessment of the efficiency of the model in improving \ndocument relevancy was made as well. Results generally indicated that when language processing is done, the \ndocument relevancy improves. Additionally, it was also found that when both stemming and lemmatization are \ncombined, more relevant documents are retrieved. The main drawback of the study is the lack of appropriate \nqueries provided in the CACM collection, hence future studies should look into the possibility of evaluating the \nintegrated model with more extensive test collections. \n \n9.0 ACKNOWLEDGMENT \n \nThe authors extend their heartfelt gratitude to University of Malaya for supporting this study (UMRG – \nRP028A-14AET) \n \nREFERENCES \n[1] N. J. Belkin, “Anomalous states of knowledge as a basis for information retrieval”, Canadian Journal of \nInformation Science, Vol. 5: pp. 133-143., 1980 \n \n[2] J. M. Ponte, and W. B. Croft, “A language modeling approach to information retrieval”, Proceedings of \nthe 21st Annual International ACM SIGIR Conference on Research and Development in Information \nRetrieval, 1998. \n\nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n54 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \n \n[3] D. Jurafsky, J. H. Martin, A. Kehler, K. Vander Linden, and N. Ward, “Speech and language processing: \nAn introduction to natural language processing, computational linguistics, and speech recognition”, \nUpper Saddle River: Prentice Hall, 2000 \n \n[4] R. Baeza-Yates, and B. Ribeiro-Neto, B, “ Modern information retrieval,” New York, USA: ACM Press, \n1999 \n \n[5] P. Sheridan, and J. P. Ballerini, “Experiments in multilingual information retrieval using the SPIDER \nsystem”, Proceedings of the 19th Annual International ACM SIGIR Conference on Research and \nDevelopment in Information Retrieval, ACM, 1996 \n \n[6] J. Xu, A. Fraser, and R. Weischedel, “Empirical studies in strategies for Arabic retrieval”, Proceedings of \nthe 25th Annual International ACM SIGIR Conference on Research and Development in Information \nRetrieval, ACM, 2002 \n \n[7] O. O. Ibrahim and Y. Mikami, “Stemming Tigrinya Words for Information Retrieval,” Proceedings of \nCOLING 2012: Demonstration Papers, pp. 345–352, 2012 \n \n[8] S. Sarkar and S. Bandyopadhyay, “Study on Rule-Based Stemming Patterns and Issues in a Bengali Short \nStory-Based Corpus”, In 7th International Conference on Natural Language Processing (Poster), 2009  \n \n[9] Y. Alhanini, and M. J. A. Aziz, “The Enhancement of Arabic Stemming by Using Light Stemming and \nDictionary-Based Stemming,” Journal of Software Engineering and Applications, Vol. 4: 522-526, 2011 \n \n[10] J. Carlberger, H. Dalianis, M. Hassel, and O. Knutsson , Improving Precision in Information Retrieval for \nSwedish using Stemming, 2001, Available at : \nhttp://www.nada.kth.se/~xmartin/papers/Stemming_NODALIDA01.pdf \n \n[11] F. Can, S. Kocberber, E. Balcik, C. Kaynak, H. C. Ocalan, and O. M. Vursavas,  “Information retrieval \non Turkish texts”, Journal of the American Society for Information Science and Technology, Vol. 59: \n407-421, 2008  \n \n[12] F. Ahmad, M. Yusoff, and T. M. T. Sembok, “Experiments with a Stemming Algorithm for Malay \nWords,” Journal of The American Society for Information Science, Vol. 47:909–918, 1996 \n \n[13] B. Nazief and M. Adriani, “Confix Stripping: Approach to Stemming Algorithm for Bahasa Indonesia,” \nTechnical report, Faculty of Computer Science, University of Indonesia, Depok, 1996 \n \n[14] M. Popovic and P. Willett “The Effectiveness of Stemming for Natural-Language Access to Slovene \nTextual Data,” Journal of the American Society for Information Science, Vol. 43(5): 384–390, 1992  \n \n[15] M. Braschler, and B. Ripplinger, “How effective is stemming and decompounding for German text \nretrieval?”, Information Retrieval, Vol.7:291-316, 2004 \n \n[16] W. Kraaij and R. Pohlman, “Viewing Stemming as Recall Enhancement,” In Proceedings of the 19th \nAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval, \npp. 40–48, 1996 \n \n[17] M. F. Porter, “An algorithm for suffix stripping”, Electronic Library and Information Systems, Vol. 14: \npp. 130-137, 1980 \n \n[18] O. Ozturkmenoglu, and A. Alpkocak, “Comparison of different lemmatization approaches for \ninformation retrieval on Turkish text collection”, Innovations in Intelligent Systems and Applications \n(INISTA) 2012 International Symposium,  IEEE, 2012 \n \nImproving Document Relevancy Using Integrated Language Modeling Techniques.  pp 45-55 \n55 \nMalaysian Journal of Computer Science.  Vol. 29(1), 2016 \n[19] B. King, L. Wang, I. Provalov, and J. Zhou, “Cengage Learning at TREC 2011 Medical Track”, 2011, \nProceedings of TREC, Available at : http://www-\npersonal.umich.edu/~benking/resources/papers/cengage_medical.pdf \n \n[20] P. Perera, and R. Witte, “A Self-Learning Context-Aware Lemmatizer for German”, Information Systems \nFrontiers, Vol. 8: pp. 47-57, 2006 \n \n[21] T. El-Shishtawy and F. El-Ghannam, “Comparison of Different Lemmatization Approaches for \nInformation Retrieval on Turkish Text Collection”, 2012, Available at \nhttp://arxiv.org/ftp/arxiv/papers/1203/1203.3584.pdf \n \n[22] F. Hammouda and A. Almarimi, \"Heuristic Lemmatization for Arabic Texts Indexation and \nClassification”, Journal of Computer Science, Vol. 6 (6): 660-665, 2010.  \n \n[23] E. Al-Shammari E., and J. Lin, “A Novel Arabic Lemmatization Algorithm,” in Proceedings of the \nsecond workshop on Analytics for noisy unstructured text data, ACM, 2008 \n \n[24] A. Loponen, and K. Järvelin, “A Dictionary- and Corpus-Independent Statistical Lemmatiser for \nInformation Retrieval in Low-Resource Languages”, In: Agosti, M. & al. (Eds.), Multilingual and \nMultimodal Information Access Evaluation, Proceedings of the International Conference in the Cross-\nLanguage Evaluation Forum, Heidelberg: Springer, pp. 3-14, 2010 \n \n[25] P. Halacsy, “Benefits of deep NLP-based lemmatization for information retrieval” 2006, Available at \nhttp://ceur-ws.org/Vol-1172/CLEF2006wn-adhoc-Halacsy2006.pdf \n \n[26] Moohebat, M., Raj, R.G. , Kareem, S.B.A., Thorleuchter, D., “Identifying ISI-indexed articles by their \nlexical usage: A text analysis approach”, Journal of the Association for Information Science and \nTechnology, Vol. 66, No. 3, pp. 501–511. doi: 10.1002/asi.23194 \n \n[27] H. Abu-Salem, M. Al-Omari, and M. W. Evens, “Stemming methodologies over individual query words \nfor an Arabic information retrieval system,” Journal of the American Society for Information Science, \nVol. 50: 524-529, 1999  \n \n[28] M. Rosell, “Improving clustering of Swedish newspaper articles using stemming and compound \nsplitting,” In 14th Nordic Conference on Computational Linguistics 2003  \n \n[29] D. Gupta, Y. R. Kumar and N. Sajan, “Improving Unsupervised Stemming by using Partial \nLemmatization Coupled with Data-based Heuristics for Hindi”, International Journal of Computer \nApplications, Vol. 38: pp. 1- 8, 2012  \n \n[30] T. Korenius, J. Laurikkala, K. Järvelin, M. Juhola, “Stemming and Lemmatization in the Clustering of \nFinnish Text Documents, CIKM’04, 2004 \n \n[31] G. Salton, and S. Buckley, “Term Weighting Approaches in Automatic Text Retrieval,” Information \nProcessing and Management, Vol. 24: pp. 513-523 , 1988 \n \n \n \n \n \n \n ",
  "topic": "Lemmatisation",
  "concepts": [
    {
      "name": "Lemmatisation",
      "score": 0.9489232301712036
    },
    {
      "name": "Computer science",
      "score": 0.8810974359512329
    },
    {
      "name": "Language model",
      "score": 0.6193649172782898
    },
    {
      "name": "Natural language processing",
      "score": 0.6118941307067871
    },
    {
      "name": "Information retrieval",
      "score": 0.5309139490127563
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5067928433418274
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5055129528045654
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I33849332",
      "name": "University of Malaya",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I1323252656",
      "name": "Information Technology University",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I82724352",
      "name": "Universiti Teknologi MARA",
      "country": "MY"
    }
  ],
  "cited_by": 14
}