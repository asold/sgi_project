{
    "title": "Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking",
    "url": "https://openalex.org/W4385570330",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2104494771",
            "name": "Brendan King",
            "affiliations": [
                "University of California, Santa Cruz"
            ]
        },
        {
            "id": "https://openalex.org/A2141750911",
            "name": "Jeffrey Flanigan",
            "affiliations": [
                "University of California, Santa Cruz"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4288288848",
        "https://openalex.org/W4287795696",
        "https://openalex.org/W4223510772",
        "https://openalex.org/W4225553189",
        "https://openalex.org/W4226226396",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W3206345746",
        "https://openalex.org/W2061886377",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4287888319",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W4385573341",
        "https://openalex.org/W4300513200",
        "https://openalex.org/W3168491067",
        "https://openalex.org/W4206496297",
        "https://openalex.org/W3016473712",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3154346839",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W3208705495",
        "https://openalex.org/W2138621090",
        "https://openalex.org/W4226053975",
        "https://openalex.org/W3105480731",
        "https://openalex.org/W4389524333",
        "https://openalex.org/W4226485558",
        "https://openalex.org/W3189817881",
        "https://openalex.org/W4310282763",
        "https://openalex.org/W4385572953",
        "https://openalex.org/W2945475330",
        "https://openalex.org/W4389009440",
        "https://openalex.org/W3206547074",
        "https://openalex.org/W4205857304",
        "https://openalex.org/W2964006684",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4226451629",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W4385571445",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W1548461347",
        "https://openalex.org/W3172943453"
    ],
    "abstract": "There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting. We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST.First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction.We evaluate our approach using MultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zero and few-shot settings.",
    "full_text": "Diverse Retrieval-Augmented In-Context Learning for Dialogue State\nTracking\nBrendan King and Jeffrey Flanigan\nUniversity of California, Santa Cruz\n{bking2,jmflanig}@ucsc.edu\nAbstract\nThere has been significant interest in zero and\nfew-shot learning for dialogue state tracking\n(DST) due to the high cost of collecting and an-\nnotating task-oriented dialogues. Recent work\nhas demonstrated that in-context learning re-\nquires very little data and zero parameter up-\ndates, and even outperforms trained methods\nin the few-shot setting (Hu et al., 2022). We\npropose RefPyDST, which advances the state\nof the art with three advancements to in-context\nlearning for DST. First, we formulate DST as a\nPython programming task, explicitly modeling\nlanguage coreference as variable reference in\nPython. Second, since in-context learning de-\npends highly on the context examples, we pro-\npose a method to retrieve a diverse set of rele-\nvant examples to improve performance. Finally,\nwe introduce a novel re-weighting method dur-\ning decoding that takes into account probabili-\nties of competing surface forms, and produces\na more accurate dialogue state prediction. We\nevaluate our approach using MultiWOZ and\nachieve state-of-the-art multi-domain joint-goal\naccuracy in zero and few-shot settings.1\n1 Introduction\nDialogue state tracking (DST) is an important lan-\nguage understanding task required for supporting\ntask-oriented conversational agents. For each turn\nin a dialogue, the goal of DST is to extract the in-\ntentions and arguments a user communicates into a\nmeaning representation aligned with the capabili-\nties of the system. Often, this can be represented\nas a set of slot-value pairs, using slots defined in a\nsystem schema. For example, if a user asks a hotel\nbooking agent for \"a four-star hotel with some-\nwhere to park\", the agent could extract the state\n{(hotel-stars, 4), (hotel-parking, yes)}.\nAnnotating these turn-level dialogue states is\nchallenging and time-intensive (Budzianowski\net al., 2018). Further, as system capabilities evolve\n1Our code: https://github.com/jlab-nlp/RefPyDST\nExample t-1 state\nprint('agent: do you need further assistance ?')\nprint('user: i am looking for a nearby restaurant')\nstate.restaurant = find_restaurant(area=state.hotel.area)\nstate = State({'hotel': {'area': 'centre', 'people': 4}})\nTurn t-1 predicted stateTurn t-1 predicted state\nTurn t predicted state\nstate.restaurant = find_restaurant(\n    area=state.hotel.area)\nTurn t\nprint('agent: your hotel is booked. anything else?')\nprint('user: can you find us dinner in the same area ?')\nstate = State({'hotel': {'name': state.hotel.name, \n                         'area': 'west'}})\nDST Task & System Deﬁnition\nExample t\nclass Hotel:\n    name: str\n    area: Literal['north', 'west', 'east', 'centre', ...]\n...\nclass State:\n    hotel: Hotel\n...\nInference LM\nRetrieved representative examples\nFigure 1: Our retrieval-augmented in-context learn-\ning approach to DST. We construct a prompt which\nre-frames DST as a Python programming task condi-\ntioned on a system definition and set of retrieved ex-\namples Ek (green). For each dialogue turn t, the goal\nis to take the current state ( state) and turn utter-\nances (print(...)) as ‘input’ and produce a pro-\ngram which updates the state with missing values, i.e.\n(restaurant-area, west). We represent linguistic corefer-\nence explicitly as variable reference (pink)\nover time, the schema and DST requirements\nchange. As such, flexible and data-efficient DST\nmethods are highly valuable.\nFor these reasons, recent work has explored zero\nand few-shot methods for DST. Few-shot methods\noften fine-tune a pre-trained language model (LM)\non DST or a re-framing of the task (e.g. Su et al.,\n2021; Shin et al., 2022; Lin et al., 2021a). While\nthese systems are often data efficient, they are in-\nflexible to changing system definitions, requiring\nre-training as new services are added. To address\nthis, zero-shot methods for domain transfer have\nbeen proposed (e.g. Wu et al., 2019; Hosseini-Asl\net al., 2020; Gupta et al., 2022), but their perfor-\nmance in new domains can significantly depend\non conceptual overlap with training domains (Wu\net al., 2019).\nThe in-context learning framework (ICL)\n(Brown et al., 2020) is particularly appealing in this\nsetting given that it is highly data-efficient and flex-\nible: instead of fine-tuning, ICL methods prompt\na fixed LM with templated examples for a task.\nThis approach requires no re-training when adapt-\ning to schema changes. In recent work, Hu et al.\n(2022) find that prompting a language model with\nexamples for DST in a text-to-SQL format can out-\nperform fine-tuned zero and few-shot methods.\nIn this work, we propose RefPyDST, a retrieval-\naugmented in-context learning approach to DST for\nuse with language models pre-trained on code, such\nas OpenAI Codex (Chen et al., 2021), by building\non recent ICL methods for DST (Hu et al., 2022).\nOur approach advances the state of the art with\nthree key contributions.\nFirst, we develop a novel in-context prompt that\nre-frames DST as text-to-python, explicitly mod-\neling slot value coreferents using variables. We\nprovide an overview of this prompt and example\nof such coreference in Figure 1. We demonstrate\nthat this approach significantly improves system\nperformance in the zero and few-shot settings, and\nparticularly improves accuracy on predictions re-\nquiring coreference resolution.\nSecond, we introduce a novel method for diverse\nsupervised example retrieval, which yields a set\nof in-context examples Ek that are both individu-\nally relevant and collectively representative of the\noutput space, inspired by maximum marginal rel-\nevance (MMR) (Goldstein and Carbonell, 1998).\nOur approach significantly improves performance\nin few-shot settings, overcoming a failure mode\nin supervised example retrieval in which examples\nare each similar to an input x but redundant in the\noutputs they demonstrate.\nThird, we propose a novel scoring method\nP MIβ which compensates for surface-form com-\npetition among sampled LM completions in con-\nstrained generation settings. Inspired by Holtzman\net al. (2021), we re-weigh each completion y by an\nestimate of its a priori likelihood in the task context.\nWe find this improves system performance in both\nthe zero and few-shot settings.\nTogether, our contributions address key chal-\nlenges in DST and in retrieval-augmented ICL gen-\nerally. Our method produces state-of-the-art results\non MultiWOZ 2.1 and 2.4 DST benchmarks across\na variety of few-shot settings. Similarly, we obtain\na new zero-shot state-of-the-art in the multi-domain\nsetting.\n2 Task Definition\nA task-oriented dialogue consists of turns or paired\nutterances between a user and an agent which in-\nterfaces the user with a programmable system. At\neach turn t, the purpose of a DST module is to\nuse the dialogue history up to that turn to predict\na dialogue state yt, which represents the user’s\ngoal and progress in using the system. Let Ai be\nan agent utterance, Ui be a user utterance, and\nCt = [( A1, U1), (A2, U2), ...(At, Ut)]2 be the di-\nalogue history up to turn t. The task is to map\nthe history Ct to a state representation yt. In this\nwork, we predict dialogue states yt which can be\nrepresented as slot-value pairs:\nyt = {(s1, v1), (s2, v2)...(sn, vn)}\nwhere each slot si and the types of values it permits\nare defined in a system schema. For example, an\nagent supporting hotel reservations might have a\nslot ‘hotel-parking’ taking boolean values for con-\nstraining search to hotels that include parking.\nWe can equivalently define this task as predict-\ning state changes, as proposed in Hu et al. (2022).\nLet xt = [ yt−1, (At, Ut)] be a dialogue context\nconsisting of the previous dialogue state prediction\nand utterances for the current turn. Using this turn\ncontext xt, we predict a state change:\n∆yt = {+(si, vi)... − (sj, vj)...}\nwhere yt is computed by applying the difference\n∆yt to yt−1. This approach has two advantages\nfor few-shot in-context learning. First, the turn\ncontext xt requires fewer tokens to represent than\nthe complete historyCt, permitting more in-context\nexamples. Second, the number of distinct state\nchanges ∆yt observed in practice is much smaller\nthan the number of distinct states yt, simplifying\nthe search for relevant examples and the generation\nproblem.\nFor these reasons, we formulate our DST prob-\nlem as mapping from the turn context xt to a state\nchange ∆yt. For readability, we often use ‘turn’ to\nrefer to this turn context xt, distinguishing it from\nthe history Ct or turn number t using notation.\n2For user-initiated dialogues, A1 may be omitted\n3 Methods\nGiven a dialogue turn t, our method produces a\nstate change ∆yt by (1) retrieving a set of in-\ncontext examples Ek, (2) formatting these into a\nprompt fprompt(xt, Ek), (3) generating and scor-\ning possible program solutions (LM completions)\nwith OpenAI Codex (Chen et al., 2021), (4) ex-\necuting the program to compute a state change\n∆yt. Given the state change, we compute the\ncomplete dialogue state yt by applying the differ-\nence to yt−1. We describe our prompting function\nfprompt(xt, Ek), in § 3.1. In § 3.2, we describe\nour method for retrieving a diverse and representa-\ntive set of examples Ek. Finally, we describe our\nmethod for scoring LM completions with a point-\nwise mutual information estimate in § 3.3.\n3.1 Prompting with Text-to-Python\nWe design a novel prompt that re-frames DST\nas a text-to-Python task, allowing us to explic-\nitly represent coreference phenomena and lever-\nage the unique capabilities of language models\npre-trained with code. Figure 1 provides an\noverview. Formally, we define a prompting func-\ntion fprompt(xt, Ek), which takes a test dialogue\nturn xt and a set of k in-context examples Ek =\n{(x1, ∆y1), ...(xk, ∆yk)} and produces a string\nrepresenting the program synthesis task.\nOur prompt (Figure 1) starts with a task defini-\ntion represented as a set of Python classes corre-\nsponding to each DST domain. Each informable\nslot is an attribute in the appropriate class. Type\nhints are used to label categorical slots with their\nvalues and non-categorical slots with the most ap-\npropriate type. The dialogue state is also repre-\nsented as an object which can be manipulated, hav-\ning an attribute per-domain.\nWe represent instances of our programming\nsynthesis task with in-context python examples.\nEach in-context example ([yt−1, At, Ut], ∆yt) is\nrepresented as follows: the previous dialogue state\nyt−1 is represented as a dictionary, mapping slot\nnames to values. Non-categorical values such as\nnames are de-lexicalized by replacing their string\nvalue with a variable referencing their existing\nvalue in the state. Solutions to the programming\ntask are represented as function calls that manip-\nulate the dialogue state. One of the key benefits\nof our formulation of the DST task as python is\nexplicit representation of coreference phenomena.\nFor example, the solution corresponding to a user\ninput “find me a restaurant in the same area as\nmy hotel\" would be state.restaurant\n= find_restaurant(area =\nstate.hotel.area), explicitly modeling the\nresolution of the linguistic coreference.\n3.2 Retrieving Diverse Relevant Examples\nWe propose a method for in-context example selec-\ntion that produces an example set Ek that is both\nrelevant to a test turn xt and diverse, representing\nthe relevant portions of the output space. We first\nlearn an embedding space in which similar state\nchanges have high cosine similarity with one an-\nother (§3.2.1), following (Hu et al., 2022). Using\nthis, we propose a novel method for decoding Ek\nsuch that examples are similar to xt but dissimilar\nto each other (§3.2.2).\n3.2.1 Retriever Training\nWe fine-tune an embedding model to approximate\nthe true similarity between two turn contexts xi, xj\nwith the cosine similaritybetween their encoded\nrepresentations, following prior works (Hu et al.,\n2022; Rubin et al., 2021). Let Dtrain be a set of\ndialogue turns serving as training data for an exam-\nple retriever and selection pool at inference time.\nAs described in §2, each example ei ∈ Dtrain is a\ncontext state-change pair ei = (xi, ∆yi). A single\nexample ei is shown in the green box in Figure 1.\nWe encode an example or query turn contextx =\n[yt−1, (At, Ut)] by concatenating each element of\nthe turn context and passing the result through an\nembedding model3 emb. For two example turn\ncontexts xi, xj, the cosine similarity between their\nembeddings cos(emb(xi), emb(xj)) approximates\ntheir relevance to each other. At inference time, we\ncan embed a test turn xt and retrieve highly similar\nexamples with nearest neighbors search.\nWe fine-tune our embedding model with a super-\nvised contrastive loss, such that high cosine simi-\nlarity of representations correlates with high sim-\nilarity between dialogue state changes, following\nthe procedure in Hu et al. (2022). For our learning\nobjective, we assume a metric that gives the true\nsimilarity between two dialogue state changes for\na pair of turns simF1 , which we define below. For\neach dialogue turn in the training set, we usesimF1\nto define positive and (hard) negative examples as\nthe top and bottom 5% of the current nearest 200\nexamples, respectively. We train each retriever for\n3We use all-mpnet-base-v2 (Song et al., 2020), available\nin sentence-transformers (Reimers and Gurevych, 2019)\n15 epochs using the hyperparameters detailed in\nAppendix C.\nWe define the ground-truth similarity simF1\nbetween two dialogue state changes as follows.\nLet ∆ya = {(sa\n1, va\n1 )...(sa\nm, va\nm)} and ∆yb =\n{(sb\n1, vb\n1)...(sb\nn, vb\nn)} be two dialogue state changes.\nFor any slot value vi exhibiting coreference to\nanother slot sj, we replace vi with sj. For\nexample, the state change corresponding to a\nturn \"I need a taxi to my hotel\" would become\n{(taxi-destination, hotel-name)}, regardless of the\nparticular hotel name value. We then compute true\nstate similarity using the average between the F1\nscore comparing updated slots and the F1 score\ncomparing updated slot-value pairs, as proposed in\nHu et al. (2022):\nsimF1 (∆ya, ∆yb) = 1\n2F1({sa\n1, ...}, {sb\n1, ...})+\n1\n2F1({(sa\n1, va\n1 ), ...}, {(sb\n1, vb\n1), ...})\n3.2.2 Decoding Diverse Examples\nWe propose an adaptation of maximum marginal\nrelevance (MMR) (Goldstein and Carbonell, 1998)\nwhich uses our learned embedding model emb to\nproduce a diverse set of examples Ek that max-\nimizes similarity to xt and minimizes similarity\nbetween examples in Ek. Particularly for encoders\nthat are fine-tuned to approximate output similarity,\nthis yields a set of examples that is more repre-\nsentative of the output space than simply selecting\nthe nearest k, which may all have the same label.\nFormally, we define the ideal set of in-context ex-\namples E∗\nk for an input xt to be the k examples\nsatisfying:\nE∗\nk = argmax\nEk⊂Dtrain\nX\nxi∈Ek\ncos(emb(xt), emb(xi))\n−α\nX\nxi,xj∈Ek\ncos(emb(xi), emb(xj))\nwhere the hyperparameter α is a dissimilarity fac-\ntor and α = 0 corresponds to typical nearest- k\nexample selection. We greedily approximate E∗\nk\nby iteratively selecting the example which max-\nimizes the equation at each step. For more effi-\ncient decoding of Ek with large selection pools,\nwe limit the considered examples to the nearest\nN such that |Dtrain| >> N >> k. For example\nin one run in the 5% MultiWOZ few-shot setting,\n|Dtrain| = 2754, N = 100, and k = 10.\n3.3 Decoding with Point-wise Mutual\nInformation\nWe introduce a new rescoring function, P MIβ, to\nmitigate surface form competition when generating\nfrom language models, that we use for making\npredictions in our setting. P MIβ is an extension\nof P MIDC , which was proposed in Holtzman et al.\n(2021) for mitigating surface form competition in\nthe classification setting. We first describe surface\nform competition and P MIDC (§3.3.1), and then\ndescribe P MIβ, an adaptation of this method to\nthe constrained generative setting with in-context\nexamples (§3.3.2).\n3.3.1 Surface-form Competition\nConditioned on a prompt, a language model assigns\na likelihood to all completing strings, from which\nwe can sample. While string likelihoods can be\nused as a proxy for output class or structure likeli-\nhoods, these are not the same. For example, in our\nDST formulation, many strings can correspond to\nthe same state change ∆yt, or may not correspond\nto a valid state change at all. As such, Holtzman\net al. (2021) argue string likelihoods can be un-\nreliable for scoring the best among a fixed set of\nchoices which may each contain numerous surface\nforms in V ∗. To compensate for this, they propose\nscoring with Domain Conditional Point-wise Mu-\ntual Information (P MIDC = P(y|x,domain)\nP(y|domain) ). This\nre-weighs choices by a priori likelihood of their\nstring form in the task context P(y|domain).\n3.3.2 Scoring with P MIβ\nTo mitigate surface-form competition, we propose\nP MIβ: a prompt conditional pointwise mutual in-\nformation scoring method that adapts P MIDC to\nour constrained generative setting with in-context\nexamples. Doing so requires overcoming two key\nchallenges. First, our choices to score amongst\nare not practically enumerable. Second, the task\ncontext we condition on is partly defined by our\nchoice of in-context examples Ek. We overcome\nthese by first generating a small set of plausible\ncompletions C and their likelihoods according to\na language model. Then, we re-weigh these like-\nlihoods according to an estimate of their a priori\nlikelihood conditioned on only the task context and\nselected examples Ek:\nP MIβ(x; y|Ek) = P(y|fprompt(xt, Ek)))\nP(y|f′\nprompt(Ek))β (1)\nprint('agent: do you need further assistance ?')\nprint('user: i need a restaurant in the west')\nstate = State({'hotel': {'area': 'centre'}})\nstate.restaurant = find_restaurant(area=\"west\")\nprint('agent: anything else ?')\nprint('user: after dinner i need a ride to my\n      hotel in the west')\nstate = State({'hotel': {'name':\n# DST Task and System Definition\nclass Hotel:\n    ...\nInference LM\nInference LM\nstate.hotel.name}})\nprint('agent: do you need further assistance ?')\nprint('user: i need a restaurant in the west')\nstate = State({'hotel': {'area': 'centre'}})\nstate.restaurant = find_restaurant(area=\"west\")\n# DST Task and System Definition\nclass Hotel:\n    ...\nstate.restaurant = find_restaurant(\n    area=\"west\"),\nTask Prompt Inverted Prompt\nFigure 2: An overview of our method (§3.3) for scoring completions y from Codex with P MIβ, which re-weighs\nusing an estimate of the a priori likelihood of y in the context of the task. On the left, is our primary text-to-Python\nprompt fprompt(xt, Ek) (§3.1). We use nucleus sampling to generate a set of reasonable candidates Ctop-p and their\nprobabilities. On the right is an inverted prompt with state changes preceding their inputs, allowing us to produce an\nin-context estimate of the probability of y not conditioned on x\nwhere f′\nprompt is a prompt designed for estimating\nP(y|Ek) without conditioning on xt, described be-\nlow, and β is a hyperparameter for adjusting the\nimpact of re-weighing by a priori likelihood.4\nTo generate the candidate completions C, we\nsample a set of plausible candidates using nucleus\nsampling (Holtzman et al., 2020).\nWhile one could simply use the language model\nto compute P(y) directly, such unconditional es-\ntimates tend to vary wildly. Following Holtzman\net al. (2021), we instead estimate the probability\nof the completion in context, but further account\nfor the use of in-context examples. To do this, we\nconstruct an additional prompt which contains the\nsame problem definition, but reverses the order out-\nputs and inputs. Using this, we can estimate the\nprobability of a completion y in the context of our\ntask and examples without xt, illustrated in Fig-\nure 2. Finally, we select the completion ˆy which\nmaximizes Eq. 1, and parse it to a dialogue state\nchange ∆yt:\nˆy = argmax\ny∈C\nP MIβ(x; y|Ek)\nWe choose a minimum a priori likelihood\nof between 10−7 and 10−5, as estimates for\nP(y|f′\nprompt(Ek)) can be very low, particularly\nwhen rare slot values implied by xt are not present\nin any example. When constructing our candidate\nset C, we choose the five most likely sampled com-\n4While only β = 1 corresponds neatly to a point-wise\nmutual information estimate pmi(xt; y), we find 0 < β <1\nto be more effective in practice. Prior work in terminology\nextraction has also proposed scaling PMI estimates, though in\na different context (Daille, 1994)\npletions under the original prompt. Finally, we\ncanonicalize each completion y when computing\nP(y|f′\nprompt(Ek)) by first parsing it to a dialogue\nstate change, and then re-writing it as a string in\nthe form as if it were an example in Ek. In effect,\nthis normalizes mis-spellings and enforces the ex-\npected order of keyword arguments in the update\nstring, further controlling for high variance in our\nestimates.\n4 Experiments\nWe describe our zero and few-shot experimental\nsetups, evaluation, and baselines. Hyperparameter\nand implementation details can be found in Ap-\npendix C.\n4.1 Experimental Settings\nWe conduct zero and few-shot DST experiments on\nthe MultiWOZ dataset (Budzianowski et al., 2018),\ncontaining over ten thousand multi-domain task-\noriented dialogues crowd-sourced in a wizard-of-oz\nsetup. There are five domains in the validation/test\nsets and a total of thirty informable slots. We evalu-\nate on the newest MultiWOZ 2.4 (Ye et al., 2022a).\nFor comparison with prior work, we also report on\nMultiWOZ 2.1 (Eric et al., 2020).\nWe evaluate performance with standard joint-\ngoal accuracy (JGA) for all of our experiments.\nFor a turn xt, a dialogue state prediction ˆyt is con-\nsidered correct only if all slot names and values\nexactly match the ground-truth state yt.\nFor the few-shot setting, following (Wu et al.,\n2020), we sample 1%, 5%, or 10% of the dia-\nlogues from the training set to serve as a training\nMultiWOZ 2.1 MultiWOZ 2.4\nModel 1% 5% 10% 100% 1% 5% 10% 100%\nTRADE (Wu et al., 2019) 12.6 31.2 36.2 46.0 - - - 55.1\nDiSTRICT (Venkateswaran et al., 2022) 13.4 41.3 49.7 56.1 - - - -\nDS2 (Shin et al., 2022) 33.8 44.2 45.4 52.3 36.8 49.9 51.1 57.9\nIC-DST Codex (Hu et al., 2022) 43.1 47.1 48.7 50.7 48.4 55.4 56.9 62.4\nRefPyDST (ours) 47.3 49.6 50.8 52.0 55.2 62.3 62.5 65.2\nTable 1: Multi-domain JGA evaluated on MultiWOZ 2.1 & 2.4 using samples from 1%, 5%, 10%, and 100% of\nthe training set. Average of three runs is reported. Our method achieves state-of-the-art (bolded) for both dataset\nversions in the 1%, 5%, and 10% few-shot settings. Our method also out-performs all few-shot baselines which\nreport results in the 100% setting on MultiWOZ 2.4. Line distinguishes fine-tuned from in-context learning methods.\nset Dtrain for each experiment. We fine-tune our\nretriever using Dtrain and select in-context exam-\nples from it. We conduct three independent runs\nfor each sample size and report the average JGA\nacross runs. We also perform a single run in the\nfull setting, using 100% of the training data.\nFor the zero-shot setting, there are no labeled\nexamples to select from, but a single formatting\nexample is used for all inference turns, as in (Wang\net al., 2022; Hu et al., 2022). We consider two\nevaluation settings. The first is the typical assess-\nment on all test set dialogues, as in few-shot and\ncomplete training regimes, which we will refer to\nas the standard MultiWOZ benchmark. These re-\nsults allow comparison to few-shot and full-data\nresults, as well as other methods which use zero\nsupervised dialogues in training. We also report re-\nsults on the MultiWOZ ‘leave-one-out’ benchmark\nfor zero-shot transfer methods (Wu et al., 2019),\nreporting JGA considering only slots in each indi-\nvidual domain, as well as the average of these five\nsingle-domain results.\nWe compare to a number of prior state-of-the-art\nzero-shot and few-shot DST methods as baselines.\nThese include DST specific architectures (Wu et al.,\n2019), various fine-tuning methods (Gupta et al.,\n2022; Shin and Van Durme, 2022; Venkateswaran\net al., 2022), and a strong ICL baseline (Hu et al.,\n2022).\n5 Results\nFew-shot DST on MultiWOZ We present few-\nshot and full-shot dialogue state tracking results\non MultiWOZ 2.1 & 2.4 in Table 1. We find that\nour method achieves state-of-the-art in the 1%, 5%,\nand 10% few-shot settings for both MultiWOZ 2.1\n& 2.4, outperforming all fine-tuned methods as\nwell as other in-context learning methods. While\nall methods considered improve with additional\ndata, our method is remarkably data efficient: Ref-\nPyDST achieves 95% of its full-shot performance\nusing only 5% of the training data, on average. In\ncomparison, using 5% of the training data with\nIC-DST Codex only achieves 89% of its full-shot\nperformance.\nZero-shot DST on MultiWOZ We present zero-\nshot multi-domain results on MultiWOZ 2.4 in Ta-\nble 3. We find our method outperforms all zero-\nshot methods, achieving a 12.4% increase in multi-\ndomain JGA over IC-DST Codex, our strongest\nperforming baseline. Comparisons are limited to\nmethods that use zero training data, as opposed\nto transfer methods that train on some MultiWOZ\ndomains and evaluate on others.\nFor comparison with domain transfer methods,\nwe present zero-shot results on the leave-one-out\nbenchmark for MultiWOZ 2.1 & 2.4 in Table 2.\nFollowing prior work, we evaluate only dialogues\nand slots in the held-out domain.5 Evaluating aver-\nage performance in this setting, we find our method\noutperforms all methods except for the current\nstate-of-the-art transfer method, SDT-seq. Their\nmethod outperforms ours by 1.5% on each held-\nout domain on average. However, transfer methods\nsuch as SDT-seq require significant out-of-domain\nDST training data, while ours requires none. De-\nspite this training data disadvantage, our approach\noutperforms all other zero-shot transfer methods.\n6 Analysis & Ablations\nIn this section, we further analyze the performance\ncharacteristics of our method.\n5Prior work on the leave-one-out setting evaluates using\nthe following method: (1) filter to dialogues which contain\nthe held out domain (this can include dialogues in multiple\ndomains) and (2) only check slots in that domain when com-\nputing JGA. (Wu et al., 2019)\nattraction hotel restaurant taxi train Avg.\nMultiWOZ 2.1\nTRADE (Wu et al., 2019) † 20.1 14.2 12.6 59.2 22.4 25.7\nTransferQA (Lin et al., 2021a) † 31.3 22.7 26.3 61.9 36.7 35.8\nDiSTRICT (Venkateswaran et al., 2022) † 33.4 22.4 24.0 66.6 47.7 38.8\nD3ST (Zhao et al., 2022) † 56.4 21.8 38.2 78.4 38.7 46.7\nSDT-seq (Gupta et al., 2022) † 74.4 33.9 72.0 86.4 62.9 65.9\nIC-DST (Hu et al., 2022) 60.0 46.7 57.3 71.4 49.4 57.0\nRefPyDST (ours) 70.9 51.2 65.6 67.1 69.2 64.7\nMultiWOZ 2.4\nIC-DST Codex (Hu et al., 2022) 62.1 53.2 54.9 71.9 51.4 58.7\nRefPyDST (ours) 74.5 56.6 68.2 68.5 76.1 68.8\nTable 2: Zero-shot joint-goal accuracy (JGA) for each domain in MultiWOZ 2.1 & 2.4 in the leave-one-out set up.\nWe report results on each held-out domain and the average held-out domain performance (Avg.) Domain transfer\nmethods (marked with †) learn from dialogues in the other four domains and are tested on the held-out domain.\nUnlike domain transfer methods, IC-DST and our method do not use any DST data. Following prior work, we\nevaluate only dialogues and slots in the held-out domain. For full evaluation of all dialogues in the zero-shot setup,\nsee Table 3.\nMultiWOZ 2.4\nIC-DST Codex (Hu et al., 2022) 35.3\nRefPyDST (ours) 47.9\nTable 3: Zero-shot (zero DST training data) multi-\ndomain JGA evaluated on MultiWOZ 2.4. Our method\nachieves state-of-the-art for this setting. Comparisons\nwith zero-shot transfer methods, which train on subsets\nof the MultiWOZ dataset, can be found in Table 2.\nAblations In order to assess how each part of our\nmethod contributes to performance, we conduct\na leave-one-out ablation, as well as reporting the\nperformance of using only our prompting method.\nEach ablation is conducted using a 20% sample of\nthe development data in the MultiWOZ 2.4 dataset\n(200 dialogues), sampled independently of the set\nused to tune hyperparameters. We present results in\nTable 4 for the zero and 5% few-shot setting. In the\nfew-shot setting, we find leaving out our diverse\nretrieval to be most impactful.\nDoes using Python improve coreference resolu-\ntion? Since our Python prompting method explic-\nitly models coreference through variable reference,\nwe analyzed how our system performed on state\npredictions requiring coreference resolution. Using\ncoreference annotations released with the 2.3 ver-\nsion of the MultiWOZ dataset (Han et al., 2021),\nwe evaluate accuracy on slot values which require\ncoreference to resolve. Our results are presented in\nTable 5. Overall, our full model improves upon the\nbaseline for coreference. Removing Python greatly\nFew-Shot (5%)\nIC-DST (baseline) 52.4\nRefPyDST – Python 54.8\nRefPyDST – diverse 54.6\nRefPyDST – P MIβ 56.1\nRefPyDST (full) 57.9\nZero-Shot\nIC-DST (baseline) 43.0\nRefPyDST – Python 40.7\nRefPyDST – P MIβ 46.0\nRefPyDST (full) 46.7\nTable 4: MultiWOZ joint-goal accuracy in the few-shot\n(5%) and zero-shot settings, leaving out individual com-\nponents of our method. We evaluate on a 20% sample\nof the development set (200 dialogues). For few-shot,\nwe average over three runs, each with independently\nsampled Dtrain. For ablating the removal of our Python\nprompt, we use the Text-to-SQL format from (Hu et al.,\n2022) as a baseline. The alternatives to our diverse re-\ntrieval approach and P MIβ scoring are top-k retrieval\nand greedy decoding, respectively\nreduces our model’s performance, demonstrating\nthe benefit of modeling coreference as Python vari-\nable reference.\nDoes our retrieval method improve demon-\nstrated label diversity? We investigate to what\ndegree our diverse decoding procedure increases\ndiversity in the distribution of demonstrated labels\nfor a given input. To approximate a label, we\nModel 0% 5%\nIC-DST (baseline) 67.7 78.9∗\nRefPyDST (prompt only) 77.1∗ 77.9∗\nRefPyDST – Python 62.9 73.0\nRefPyDST (full) 76.8∗ 81.8\nTable 5: Accuracy on slot value predictions which re-\nquire coreference resolution for the zero-shot (0%) and\nfew-shot (5%). For a given setting (column), ∗ indicates\nthe difference is not statistically significant. All other\ndifferences in a column are significant to p <0.02\ndefine S(ei) as the distinct combination of slot\nnames in the output for an in-context example\nei = (xi, ∆yi), ignoring assigned values.\nFirst, we simply count the average number of\ndistinct combinations of slot names in Ek, shown in\nupper half of Table 6. For each xt, we retrieve a set\nof in-context examples Ek. We count the number\nof distinct slot combinations across each ei ∈ Ek,\nand report the development set average. A value\nof 1 indicates the retriever is fully redundant: all\nk examples demonstrate the same combination of\nslots, while a value of k indicates every example in\nEk is unique.\nSecond, we consider the entropy of slot combi-\nnations present in Ek, shown in the lower half of\nTable 6. For each xt, we again compute S(ei) for\neach retrieved example in Ek. We then compute\nthe specific conditional entropy H(S|X = xt),\nestimating the probability of each slot combina-\ntion p(S|xt) using its frequency in Ek. We report\nthe development set average or conditional entropy\nH(S|X). H(S|X = xt) = 0 indicates a fully re-\ndundant retriever that retrieves the same set of slots\nfor all examples, and a uniform distribution of slot\ncombinations yields H(S|X = xt) = log2(k).6\nWe find our retrieval methods increase the di-\nversity of in-context examples across all settings.\nFor a given training set size, we see that diverse\ndecoding increases the number of distinct ‘labels’,\nmeasured by S(ei), as well as the entropyH(S|X).\nStill, selected examples are not random, as we\ncan see when comparing H(S|X) to a random\nretriever which uniformly samples from Dtrain.7\nFinally, we see that as the size of the training set\nincreases, the diversity in exemplified labels for a\n6While this is true of a uniform distribution over demon-\nstrated slot combinations, we find uniformly sampling from\nDtrain yields an entropy of∼ 2.6, as the distribution of labels\nin the training data is not uniform.\n7In Appendix D, we also compare few-shot task perfor-\nmance for our retrieval method against random retrieval\nNumber of Distinct S in Ek\n1% 5% 10% 100%\nrandom 7.1 7.2 7.2 7.3\ntop-k 3.4 2.2 1.8 1.5\ndiverse (α = .2) 5.3 4.1 3.3 2.2\ndiverse (α = .3) 5.7 4.5 3.5 2.3\ndiverse (α = .5) 7.5 5.7 4.8 2.8\nEntropy H(S|X)\n1% 5% 10% 100%\nrandom 2.6 2.6 2.6 2.6\ntop-k 1.2 0.63 0.47 0.30\ndiverse (α = .2) 1.8 1.5 1.1 0.64\ndiverse (α = .3) 1.9 1.6 1.2 0.68\ndiverse (α = .5) 2.7 2.0 1.7 0.93\nTable 6: We analyze the outputs demonstrated in Ek for\ndifferent in-context example retrieval methods. Above,\nwe show the average number of distinct slot combina-\ntions demonstrated in Ek. Below, we show the condi-\ntional entropy H(S|X) of the distribution of slot com-\nbinations in Ek. We underline the values corresponding\nto methods used in our final models\ngiven choice of α decreases. Increasing training\ndata leads to a higher density of each slot combi-\nnation, requiring more aggressive discounting to\nachieve the same diversity in Ek. As such, we in-\ncrease α with training set size, using α = 0.2 for\n1% and 5% settings and α = 0.3 & α = 0.5 for\n10% and 100% settings, respectively.\n7 Related Work\nDialogue State Tracking There has been a re-\ncent increase in work on the zero and few-shot\nDST systems. Many approaches fine-tune a pre-\ntrained language model by re-framing DST as some\nform of text-to-text or auto-regressive language\nmodeling task (Wu et al., 2020; Peng et al., 2021;\nHosseini-Asl et al., 2020; Su et al., 2021; Shin\net al., 2022; Lin et al., 2021b; Gupta et al., 2022;\nLi et al., 2021; Xie et al., 2022). Many of these\nmethods often exhibit zero-shot transfer capabili-\nties (Wu et al., 2019; Gupta et al., 2022; Li et al.,\n2021; Hosseini-Asl et al., 2020). However, these\napproaches still require re-training when a domain\nis added or changed, and zero-shot transfer perfor-\nmance is dependent on the relatedness of the new\ndomain to existing ones.\nSome recent works instead model DST as an in-\ncontext learning problem (Hu et al., 2022; Xie et al.,\n2022; Madotto et al., 2021), bypassing the need for\nre-training when system definitions change. In par-\nticular, we build on the work of Hu et al. (2022),\nwhich models DST by predicting dialogue state\nchanges at each turn, relying on only a state sum-\nmary and agent/user turn utterances for inference.\nTheir work models DST as a text-to-SQL prob-\nlem, whereas we model it as a Python program-\nming problem with novel methods for selecting\nin-context examples and scoring language model\ncompletions.\nIn-Context Learning Some recent works ex-\nplore the properties of effective in-context exam-\nples. In classification settings, Gao et al. (2021)\nfind random examples can significantly limit perfor-\nmance, and propose using a pre-trained embedding\nmodel to find examples semantically close to x, re-\ntrieving one per class. Other works investigate the\nrole of examples in ICL performance in detail, find-\ning that ICL methods perform best when example\ninputs and test inputs are as close in distribution as\npossible, and when the distribution of exemplified\nlabels closely matches the target distribution (Min\net al., 2022; Liu et al., 2022).\nParalleling this, a number of works across NLP\ntasks propose methods for retrieving relevant in-\ncontext examples. Pasupat et al. (2021) use an\nunsupervised embedding model to embed a test\ninput x and all available examples, retrieving the\nk with highest embedding cosine similarity. Other\nworks use a similar dense retriever but in an em-\nbedding space learned with supervision. Rubin\net al. (2021) fine-tune an example retriever with\ncontrastive learning in which positive examples\nmaximize pLM (y|x, ei). Hu et al. (2022) propose a\ncontrastive learning objective specific to DST, fine-\ntuning an embedding model to embed turns with\nsimilar state changes in proximity to each other.\nRather than use a separate retrieval module, Shin\nand Van Durme (2022) use the LM itself to select\nexamples which are most likely when conditioned\non x. Given a test input x, each of these works\nscores the relevance of an individual example ei to\na test input x and then selects the k most relevant\nones to include in a prompt. In most cases, this\nyields a set of examples Ek which are meaningfully\nsimilar to x. However, considering examples in-\ndividually does not necessarily lead to adequate\nexemplification of the output space. In supervised\nsettings that learn a relevance metric which approx-\nimates output similarity, this can lead to degenerate\nexamples sets Ek which all exemplify the same out-\nput. In contrast to this, we propose a novel method\nfor using this score to construct Ek with examples\nthat are relevant tox while being distinct from each\nother.\nIn concurrent work to our own, Ye et al. (2022b)\npropose a method for decoding diverse examples\nof explanations from a retriever for use in reason-\ning problems, also based on maximum-marginal-\nrelevance (MMR) (Goldstein and Carbonell, 1998).\nTheir work uses unsupervised measures of sim-\nilarity between explanations, where ours uses a\nsupervised retriever which approximates similar-\nity of outputs. Thus, diversity in our example sets\ncorrelates to diversity in exemplified outputs. In\nanother concurrent work to our own (Levy et al.,\n2022) propose a method for diverse example selec-\ntion in a semantic parsing task, using the outputs\nof selected examples to incrementally cover more\nstructures in Ek.\nFor tasks which can be re-framed as program\nsynthesis, a number of works have also developed\nICL methods for use with LMs pre-trained on code\nsuch as Codex and Codegen (Chen et al., 2021;\nNijkamp et al., 2022). Shin and Van Durme (2022)\nuse ICL with Codex to generate Lisp-like programs\nin a dialogue semantic parsing task. Rajkumar et al.\n(2022) evaluate such models capabilities in Text-to-\nSQL problems, and Hu et al. (2022) use a Text-to-\nSQL framing to use Codex for DST. Instead of SQL\nqueries, we generate Python programs, allowing for\nintuitive modeling of phenomena like coreference.\nFinally, recent works have considered adjust-\ning how completion strings are scored with an\nLM. Brown et al. (2020) normalize log-likelihoods\nby length before scoring completions. Zhao et al.\n(2021) re-weigh LM probabilities by learning an\naffine transformation that yields uniform scores\ngiven ‘content-free inputs’. Holtzman et al. (2021)\npropose P MIDC , a method for re-scoring com-\npletions using pointwise mutual information (pmi),\nwhich we adapt to our constrained generative set-\nting.\n8 Conclusion\nWe propose RefPyDST, an in-context learning\nmethod for DST. Our contributions address key\nchallenges in DST and in retrieval-augmented ICL,\nproducing state-of-the-art results on MultiWOZ\nDST benchmarks for few-shot and zero-shot se-\ntups. Future work could apply methods developed\nhere to other in-context learning problems.\n9 Limitations\nWhile in-context learning methods for DST are\npromising in their data efficiency and flexibility to\nnew domains, they typically require very large mod-\nels to perform effectively. At 175 billion param-\neters, OpenAI Codex (Chen et al., 2021) is much\nlarger than some of the fine-tuned approaches to\nDST, though with better performance and ability\nto adapt to new domains without re-training. De-\nspite our advances, there are still significant errors\nwhen applying ICL for DST. As such, ICL may not\nnecessarily be relied on in safety-critical settings.\nAcknowledgements\nWe thank Geetanjali Rakshit, Nilay Patel, Chang-\nmao Li, Chris Toukmaji, Rongwen Zhao, and other\nJLab members for insightful feedback on prelimi-\nnary drafts of this work, and thank the anonymous\nreviewers and area chairs for their detailed and help-\nful feedback. The authors were supported in part\nby the NSF National AI Institute for Student-AI\nTeaming (iSAT) under grant DRL 2019805. The\nopinions expressed are those of the authors and do\nnot represent views of the NSF. We are thankful for\nthe computing resources provided by the Pacific Re-\nsearch Platform’s Nautilus cluster, supported by the\nNational Science Foundation under Award Num-\nbers CNS-1730158, ACI-1540112, ACI1541349,\nOAC-1826967, the University of California Office\nof the President, and the University of California\nSan Diego’s California Institute for Telecommu-\nnications and Information Technology/Qualcomm\nInstitute.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners.\narXiv:2005.14165 [cs]. ArXiv: 2005.14165.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Ultes Stefan, Ramadan Os-\nman, and Milica Gaši ´c. 2018. Multiwoz - a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Eval-\nuating Large Language Models Trained on Code.\nArXiv:2107.03374 [cs].\nBéatrice Daille. 1994. Approche mixte pour l’extraction\nde terminologie : statistique lexicale et filtres linguis-\ntiques.\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\nSanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj\nGoyal, Peter Ku, and Dilek Hakkani-Tur. 2020. Mul-\ntiWOZ 2.1: A Consolidated Multi-Domain Dia-\nlogue Dataset with State Corrections and State Track-\ning Baselines. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n422–428, Marseille, France. European Language Re-\nsources Association.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking Pre-trained Language Models Better Few-\nshot Learners. arXiv:2012.15723 [cs]. ArXiv:\n2012.15723.\nJade Goldstein and Jaime Carbonell. 1998. Summariza-\ntion: (1) using MMR for diversity- based reranking\nand (2) evaluating summaries. In TIPSTER TEXT\nPROGRAM PHASE III: Proceedings of a Workshop\nheld at Baltimore, Maryland, October 13-15, 1998,\npages 181–195, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nRaghav Gupta, Harrison Lee, Jeffrey Zhao, Abhinav\nRastogi, Yuan Cao, and Yonghui Wu. 2022. Show,\nDon’t Tell: Demonstrations Outperform Descrip-\ntions for Schema-Guided Task-Oriented Dialogue.\narXiv:2204.04327 [cs]. ArXiv: 2204.04327.\nR. Hadsell, S. Chopra, and Y . LeCun. 2006. Dimension-\nality reduction by learning an invariant mapping. In\n2006 IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR’06),\nvolume 2, pages 1735–1742.\nTing Han, Ximing Liu, Ryuichi Takanobu, Yixin Lian,\nChongxuan Huang, Dazhen Wan, Wei Peng, and\nMinlie Huang. 2021. MultiWOZ 2.3: A multi-\ndomain task-oriented dialogue dataset enhanced with\nannotation corrections and co-reference annotation.\narXiv:2010.05594 [cs]. ArXiv: 2010.05594 version:\n3.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The Curious Case of Neural Text\nDegeneration. ArXiv:1904.09751 [cs].\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn’t\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020. A Simple\nLanguage Model for Task-Oriented Dialogue. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 20179–20191. Curran Associates,\nInc.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022. In-\nContext Learning for Few-Shot Dialogue State Track-\ning. Number: arXiv:2203.08568 arXiv:2203.08568\n[cs].\nItay Levy, Ben Bogin, and Jonathan Berant. 2022. Di-\nverse Demonstrations Improve In-context Composi-\ntional Generalization. ArXiv:2212.06800 [cs].\nShuyang Li, Jin Cao, Mukund Sridhar, Henghui Zhu,\nShang-Wen Li, Wael Hamza, and Julian McAuley.\n2021. Zero-shot generalization in dialog state track-\ning through generative question answering. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1063–1074, Online.\nAssociation for Computational Linguistics.\nZhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan\nMoon, Zhenpeng Zhou, Paul Crook, Zhiguang Wang,\nZhou Yu, Eunjoon Cho, Rajen Subba, and Pascale\nFung. 2021a. Zero-shot dialogue state tracking via\ncross-task transfer. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 7890–7900, Online and Punta\nCana, Dominican Republic. Association for Com-\nputational Linguistics.\nZhaojiang Lin, Bing Liu, Seungwhan Moon, Paul\nCrook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu,\nAndrea Madotto, Eunjoon Cho, and Rajen Subba.\n2021b. Leveraging Slot Descriptions for Zero-Shot\nCross-Domain Dialogue StateTracking. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5640–5648, Online. Association for Computational\nLinguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nAndrea Madotto, Zhaojiang Lin, Genta Indra\nWinata, and Pascale Fung. 2021. Few-Shot\nBot: Prompt-Based Learning for Dialogue Systems.\narXiv:2110.08118 [cs]. ArXiv: 2110.08118.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the Role of Demonstra-\ntions: What Makes In-Context Learning Work?\narXiv:2202.12837 [cs]. ArXiv: 2202.12837 version:\n1.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. CodeGen: An Open Large Language\nModel for Code with Multi-Turn Program Synthesis.\nArXiv:2203.13474 [cs].\nPanupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.\nControllable semantic parsing via retrieval augmen-\ntation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7683–7698, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2021. SOLOIST:\nBuilding Task Bots at Scale with Transfer Learn-\ning and Machine Teaching. arXiv:2005.05298 [cs].\nArXiv: 2005.05298.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-sql capabilities\nof large language models. ArXiv, abs/2204.00498.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Be-\nrant. 2021. Learning To Retrieve Prompts for In-\nContext Learning. arXiv:2112.08633 [cs]. ArXiv:\n2112.08633.\nJamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea\nMadotto, and Juneyoung Park. 2022. Dialogue sum-\nmaries as dialogue states (DS2), template-guided\nsummarization for few-shot dialogue state tracking.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 3824–3846, Dublin,\nIreland. Association for Computational Linguistics.\nRichard Shin and Benjamin Van Durme. 2022. Few-\nShot Semantic Parsing with Language Models\nTrained on Code. In Proceedings of the 2022 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, pages 5417–5425, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu. 2020. MPNet: Masked and Per-\nmuted Pre-training for Language Understanding.\nArXiv:2004.09297 [cs].\nYixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta,\nDeng Cai, Yi-An Lai, and Yi Zhang. 2021. Multi-\nTask Pre-Training for Plug-and-Play Task-Oriented\nDialogue System. arXiv:2109.14739 [cs]. ArXiv:\n2109.14739.\nPraveen Venkateswaran, Evelyn Duesterwald, and\nVatche Isahagian. 2022. DiSTRICT: Dialogue State\nTracking with Retriever Driven In-Context Tuning.\nArXiv:2212.02851 [cs].\nGengyu Wang, Cheng Qian, Lin Pan, Haode Qi,\nLadislav Kunc, and Saloni Potdar. 2022. Benchmark-\ning language-agnostic intent classification for virtual\nassistant platforms. In Proceedings of the Workshop\non Multilingual Information Access (MIA), pages\n69–76, Seattle, USA. Association for Computational\nLinguistics.\nChien-Sheng Wu, Steven C.H. Hoi, and Caiming Xiong.\n2020. Improving Limited Labeled Dialogue State\nTracking with Self-Supervision. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 4462–4472, Online. Association for\nComputational Linguistics.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-\nAsl, Caiming Xiong, Richard Socher, and Pas-\ncale Fung. 2019. Transferable Multi-Domain State\nGenerator for Task-Oriented Dialogue Systems.\nArXiv:1905.08743 [cs].\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang,\nVictor Zhong, Bailin Wang, Chengzu Li, Connor\nBoyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caim-\ning Xiong, Lingpeng Kong, Rui Zhang, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2022. Unified-\nSKG: Unifying and Multi-Tasking Structured Knowl-\nedge Grounding with Text-to-Text Language Models.\nNumber: arXiv:2201.05966 arXiv:2201.05966 [cs].\nFanghua Ye, Jarana Manotumruksa, and Emine Yil-\nmaz. 2022a. MultiWOZ 2.4: A Multi-Domain Task-\nOriented Dialogue Dataset with Essential Annotation\nCorrections to Improve State Tracking Evaluation. In\nProceedings of the 23rd Annual Meeting of the Spe-\ncial Interest Group on Discourse and Dialogue, pages\n351–360, Edinburgh, UK. Association for Computa-\ntional Linguistics.\nXi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoy-\nanov, Greg Durrett, and Ramakanth Pasunuru.\n2022b. Complementary Explanations for Effective\nIn-Context Learning. ArXiv:2211.13892 [cs].\nJeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu,\nMingqiu Wang, Harrison Lee, Abhinav Rastogi,\nIzhak Shafran, and Yonghui Wu. 2022. Description-\nDriven Task-Oriented Dialog Modeling. Number:\narXiv:2201.08904 arXiv:2201.08904 [cs].\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate Before Use: Im-\nproving Few-Shot Performance of Language Models.\narXiv:2102.09690 [cs]. ArXiv: 2102.09690.\nA Dialogue State Normalization\nReal world task oriented dialogue systems can in-\nterface users with thousands or more entities, such\nas restaurants or hotels in MultiWOZ. Since rea-\nsoning directly over all such entities is intractable,\ndialogue understanding modules often first predict\na surface form (e.g. a restaurant name mentioned\nby a user) which another module links to a canoni-\ncal form (e.g. that restaurants name in a database).\nWhile dialogue state trackers evaluated on Mul-\ntiWOZ do not need to interact with a database,\nhandling of typos and unexpected surface forms\nis important for a realistic assessment of system\nperformance, since predictions for a slot are evalu-\nated on exact string match. As such, most research\nsystems including the baselines in this paper use\nrule-based functions to fix typos and unexpected\nsurface forms. We propose a robust rule-based\nmethod for effective linking of surface forms to\ncanonical forms described below.\nMapping to canonical forms We begin by first\nreading in canonical forms for every informable\nslot in the MultiWOZ system. For categorical\nslots, these are defined in a schema file, as re-\nleased with MultiWOZ 2.1 (Eric et al., 2020). For\nnon-categorical slots, we read in values from the\ndatabase defined with the original MultiWOZ data\ncollection (Budzianowski et al., 2018). Neither\nsource of information contains dialogue data, only\ninformation defining the task. The taxi and train\nservice have informable slots for departure and des-\ntination locations. In addition to the locations listed\nfor these slots in a database (i.e. scheduled train\njourneys), we accept the name of any entity which\nhas an address as a canonical form for these slots.\nFor time slots we consider any time represented in\n\"hh:mm\" form as canonical. Overall, this gives us\na mapping from a slot name si to a set of canonical\nforms C⟩ for all slot names.\nGiven a slot name si and a slot value surface\nform vj, we select the correct canonical form cj as\nfollows: (1) we first generate a set of aliases for vj.\nThese are acceptable re-phrasings of vj, such as\nadding the leading article \"the\", a domain specify-\ning suffix such as \"hotel\" or \"museum\", or switch-\ning numbers to/from digit form (e.g. \"one\" ↔ \"1\").\nWe then consider a surface form vj as mapped to\na canonical form cj if any of the aliases aj ∈ Aj\nis a fuzzy match for the canonical form cj, using\nthe fuzz.ratio scorer in the fuzzywuzzy 8\npackage. We require a score of 90 or higher, and\nverify in the development data that no surface form\nmaps to more than one canonical form.\nChoosing the most likely surface form While in\na real world dialogue system we would only need\nto link to canonical forms, gold dialogue state\nstates in MultiWOZ are themselves annotated\nwith surface forms, not always matching the name\nof the entity in the database and occasionally dis-\nagreeing on an entity name. So as to not alter the\nevaluation process and make sure we can fairly\ncompare to prior work, we use the training data\navailable in each experimental setting to choose the\nmost likely surface form for a given canonical form\ncj. To do this, we simply count the occurrences of\neach surface form in the gold labels of the train-\ning set for that experiment, and select the most\nfrequently occurring one for cj. However for low\ndata regimes, we often do not observe all canonical\nforms. Following numerous prior works, we make\nuse of the ontology file released with the dataset\n(Eric et al., 2020; Ye et al., 2022a), which lists all\nobserved surface forms for a slot name, and treat\neach of these as if we had seen them 10 times. This\nserves as a smoothing factor for selecting the most\nlikely surface form. For the zero-shot experiments,\nwe use only the counts derived from the ontology\nfile, as we have no training data to observe.\nOverall, we find this approach to normalization\nto be robust when compared to other works, which\nrely on hard-coded fixes for commonly observed\ntypos. Further, our normalization can be initialized\nwith any similarly formatted system definition and\ndata set, allowing for use in other domains.\nTo verify that our approach to normalization is\nnot the key factor distinguishing our performance\nfrom previous methods, we apply it to a faithful\n8https://pypi.org/project/fuzzywuzzy/\nre-implementation of our IC-DST Codex baseline\n(Hu et al., 2022) in our ablation in Table 4.\nB Prompt Examples\nPlease see our GitHub repository for prompt exam-\nples: https://github.com/jlab-nlp/RefPyDST.\nC Implementation Details\nC.1 Hyperparameters\nAll hyperparameter tuning is performed using a\n10% split of the development set (100 dialogues)\nand manual tuning. We find that a smaller choice\nfor p (0.7) in nucleus sampling helps performance\nin the zero-shot setting. Similarly, we find that in\norder to select a diverse set of examples, we need\nto scale α. We use α = 0.2 for the 1% & 5% set-\ntings, α = 0.3 for 10%, and α = 0.5 for the full\nsetting. For the full setting, we also increase the the\nnumber of considered examples from the nearest\n100 to nearest 200. Across all settings, we compute\nP MIβ with β = 0.4. We use a robust approach\nto normalizing predicted values (i.e. to resolve\nmis-spellings, etc.) described in Appendix A. We\napply this normalization to our strongest baseline\n(IC-DST Codex) in our ablations (§6). When com-\nputing P(y|f′\nprompt(Ek)), we clip low token log\nprobabilities at 5e-7 in the few-shot setting and 5e-\n4 in the zero-shot setting, as the lack of examples\nleads to poorer calibration in the zero-shot setting.\nWe also clip full-sequence log probabilities at 1e-7\nin the few-shot setting and 1e-5 in the zero-shot\nsetting.\nC.2 Retriever fine-tuning details\nFor both our methods and the re-implementation\nof IC-DST Codex (Hu et al., 2022) used in\nour ablations (§ 6), we fine-tune the retriever\nusing the sentence-transformers pack-\nage (Reimers and Gurevych, 2019), following\nthe procedure of (Hu et al., 2022). We be-\ngin with pre-trained all-mpnet-base-v2 em-\nbedding model, which we use as a retriever\nwith nearest neighbors search 9. Each of our\nretrievers is trained for 15 epochs using the\nOnlineContrastiveLoss, which computes\nthe contrastive loss proposed by Hadsell et al.\n(2006) using only hard positives and hard nega-\ntives. For each dialogue turn in the training set, we\n9We use the scipy implementation: https:\n//docs.scipy.org/doc/scipy/reference/\ngenerated/scipy.spatial.KDTree.html\nFew-Shot (5%)\nRefPyDST (random-k) 43.5\nRefPyDST (top-k) 54.6\nRefPyDST (full) 57.9\nTable 7: MultiWOZ joint-goal accuracy in the 5% few-\nshot setting, ablating different retrieval methods. The\nfull model includes both our trained retriever and di-\nverse example decoding methods (§3.2). Top- k uses\nthe trained retriever but decodes the top- k nearest ex-\namples instead of using our diverse decoding procedure.\nRandom retrieval samples k examples from Dtrain uni-\nformly at random\nuse simF1 to define positive and (hard) negative\nexamples as the top and bottom 5% of the nearest\n200 examples, respectively.\nC.3 Arguments to Codex\nFor all methods, we make requests to Ope-\nnAI Codex with arguments engine =\n’code-davinci-002’, max_tokens =\n120, and stop sequences of either [’-’, ’\\n’,\n’;’, ’#’] (IC-DST Codex baseline replication)\nor [\"\\n\\n\", \"#\", \"print(\"] (ours). For\nmethods which utilize nucleus sampling (Holtzman\net al., 2020) with the top_p parameter. In the\nfew-shot setting, we sample with best_of=10,\nkeeping only n=5 most likely results. In the\nzero-shot setting, we increase best_of to 32.\nD Random Retrieval Ablation\nIn Table 7, we compare our retrieval methods to\nrandom retrieval, on the 20% split of the develop-\nment set used in our previous ablations. For ran-\ndom retrieval, we sample k examples from Dtrain\nuniformly at random to construct Ek. We find this\nsignificantly under-performs our learned retrieval\nmethods, whether selecting the top-k examples or\nusing our diverse decoding approach."
}