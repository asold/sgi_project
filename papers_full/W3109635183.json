{
  "title": "ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis",
  "url": "https://openalex.org/W3109635183",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Liu, Zhouyong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353669994",
      "name": "Luo Shun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2559624230",
      "name": "Li WuBin",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lu, Jingben",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748530759",
      "name": "Wu Yufan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966949309",
      "name": "Sun Shilei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2663581589",
      "name": "Li Chunguo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1264528650",
      "name": "Yang Luxi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2797683898",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2064076387",
    "https://openalex.org/W2963093735",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2897177665",
    "https://openalex.org/W2952809312",
    "https://openalex.org/W1485009520",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2604329646",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2904829482",
    "https://openalex.org/W3035540643",
    "https://openalex.org/W2950533208",
    "https://openalex.org/W1998897200",
    "https://openalex.org/W2586480386",
    "https://openalex.org/W2964151830",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2953823547",
    "https://openalex.org/W2737529999",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W1905052409",
    "https://openalex.org/W79127031",
    "https://openalex.org/W2607738331",
    "https://openalex.org/W2949258649",
    "https://openalex.org/W2738579427",
    "https://openalex.org/W2559767995",
    "https://openalex.org/W2305401973",
    "https://openalex.org/W2103336008",
    "https://openalex.org/W2963760790",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2989705138",
    "https://openalex.org/W3019527251",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2964270168",
    "https://openalex.org/W2964251418",
    "https://openalex.org/W2126579184",
    "https://openalex.org/W1776042733",
    "https://openalex.org/W2973673960",
    "https://openalex.org/W2119781527",
    "https://openalex.org/W3043904900",
    "https://openalex.org/W2520707650"
  ],
  "abstract": "Deep Convolutional Neural Networks (CNNs) are powerful models that have achieved excellent performance on difficult computer vision tasks. Although CNNs perform well whenever large labeled training samples are available, they work badly on video frame synthesis due to objects deforming and moving, scene lighting changes, and cameras moving in video sequence. In this paper, we present a novel and general end-to-end architecture, called convolutional Transformer or ConvTransformer, for video frame sequence learning and video frame synthesis. The core ingredient of ConvTransformer is the proposed attention layer, i.e., multi-head convolutional self-attention layer, that learns the sequential dependence of video sequence. ConvTransformer uses an encoder, built upon multi-head convolutional self-attention layer, to encode the sequential dependence between the input frames, and then a decoder decodes the long-term dependence between the target synthesized frames and the input frames. Experiments on video future frame extrapolation task show ConvTransformer to be superior in quality while being more parallelizable to recent approaches built upon convolutional LSTM (ConvLSTM). To the best of our knowledge, this is the first time that ConvTransformer architecture is proposed and applied to video frame synthesis.",
  "full_text": "ConvTransformer: A Convolutional Transformer Network for Video Frame\nSynthesis\nZhouyong Liu Shun Luo Wubin Li Jingben Lu Yufan Wu Shilei Sun\nChunguo Li Luxi Yang *\nSchool of Information Science and Engineering, Southeast University\n/uni00000027/uni00000039/uni00000029\n/uni00000015/uni00000016/uni00000011/uni00000015/uni00000013/uni00000012/uni00000013/uni00000011/uni0000001b/uni00000013\n/uni00000030/uni00000026/uni00000031/uni00000048/uni00000057\n/uni00000015/uni0000001a/uni00000011/uni0000001a/uni00000017/uni00000012/uni00000013/uni00000011/uni0000001c/uni00000016\n/uni00000016/uni00000016/uni00000011/uni00000013/uni0000001b/uni00000012/uni00000013/uni00000011/uni0000001c/uni0000001b\n/uni00000032/uni00000058/uni00000055/uni00000056\n/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000047/uni00000003/uni00000057/uni00000055/uni00000058/uni00000057/uni0000004b\n/uni00000033/uni00000036/uni00000031/uni00000035/uni00000012/uni00000036/uni00000036/uni0000002c/uni00000030\n/uni00000013\n/uni00000015/uni00000018\n/uni00000018/uni00000013\n/uni0000001a/uni00000018\n/uni00000014/uni00000013/uni00000013\n/uni00000014/uni00000015/uni00000018\n/uni00000014/uni00000018/uni00000013\n/uni00000014/uni0000001a/uni00000018\n/uni00000015/uni00000013/uni00000013\nFigure 1. Example of video frame extrapolation.Top is the extrapolated result, middle is the zoomed local details and bottom is the\nocclusion map that depicts the residuals between extrapolated image and ground truth. ColorBar with gradient from blue to red indicates\nthe residual intensity in the range 0 to 255. The zoomed-in details indicate that ConvTransformer performs better in generating local\nhigh-frequency details, while the occlusion maps demonstrate that ConvTransformer has superiority on accurate pixel intensity prediction.\nAbstract\nDeep Convolutional Neural Networks (CNNs) are pow-\nerful models that have achieved excellent performance on\ndifﬁcult computer vision tasks. Although CNNs perform\nwell whenever large labeled training samples are available,\nthey work badly on video frame synthesis due to objects\ndeforming and moving, scene lighting changes, and cam-\neras moving in video sequence. In this paper, we present a\nnovel and general end-to-end architecture, called convolu-\ntional Transformeror ConvTransformer, for video frame\nsequence learning and video frame synthesis. The core\ningredient of ConvTransformer is the proposed attention\nlayer, i.e., multi-head convolutional self-attention layer, that\nlearns the sequential dependence of video sequence. Con-\nvTransformer uses an encoder, built upon multi-head con-\nvolutional self-attention layer, to encode the sequential de-\npendence between the input frames, and then a decoder de-\ncodes the long-term dependence between the target synthe-\nsized frames and the input frames. Experiments on video\n*Corresponding author\nfuture frame extrapolation task show ConvTransformer to\nbe superior in quality while being more parallelizable to\nrecent approaches built upon convolutional LSTM (ConvL-\nSTM). To the best of our knowledge, this is the ﬁrst time\nthat ConvTransformer architecture is proposed and applied\nto video frame synthesis.\n1. Introduction\nVideo frame synthesis, comprising interpolation and ex-\ntrapolation two subtasks, is one of the classical and funda-\nmental problems in video processing and computer vision\ncommunity. The abrupt motion artifacts and temporal alias-\ning in video sequence can be suppressed with the help of\nvideo frame synthesis, and hence it can be applied to numer-\nous applications ranging from motion deblurring [4], video\nframe rate up-sampling [6, 3], video editing [32, 51], novel\nview synthesis [10] to autonomous driving [48].\nNumerous solutions, over the past decades, have been\nproposed for this problem, and achieved substantial im-\nprovement. Traditional video frame synthesis pipeline usu-\n1\narXiv:2011.10185v2  [cs.CV]  1 Jun 2021\nally involves two consecutive steps, i.e., optical ﬂow estima-\ntion and optical based frame warping [45, 50]. The quality\nof synthesized frames, however, will be easily affected by\nthe estimated optical ﬂow. Recent advancements in deep\nneural networks have successfully improved a number of\ntasks, such as classiﬁcation [16], segmentation [33] and vi-\nsual tracking [18], and also promoted the development of\nvideo frame interpolation and extrapolation.\nLong et al. treated the video frame interpolation as an\nimage generating task, and then trained a generic convo-\nlutional neural network (CNN) to directly interpolate the\nin-between frame [22]. However, due to the difﬁculty for\ngeneric CNN to capture the multi-modal distribution of\nvideo frames, severe blurriness exists in their interpolated\nresults. Lately, Niklaus et al. considered frame interpo-\nlation as a local convolution problem, and proposed adap-\ntive convolutional operation [27] and separable convolu-\ntional process [28] for each pixel in the target interpolated\nframe. However, these kernel-based algorithms typically\nsuffer from heavy computation. Instead of only relying\non CNN, the optical ﬂow embedded neural netoworks are\nalso investigated and applied to interpolated middle frames.\nThe deep voxel ﬂow (DVF) method, for instance, implic-\nitly incorporate 3D optical ﬂow across the time and space\nto synthesize middle frames. Besides, Bao et al. proposed\nan explicitly optical ﬂow information embedded algorithm,\nnamely DAIN [1], in which the depth map [7, 19] and opti-\ncal ﬂow [19] are explicitly generated by the pretrained sub-\nnetworks [19, 7, 19], and is used to guide the contextual\nfeatures capturing pipeline. Although these methods could\ninterpolate perceptually well frames with the accurately es-\ntimated ﬂow generated by the pretrained optical ﬂow esti-\nmation sub-networks [19, 7, 19], the optical ﬂow estimation\nnetworks are sensitive to the artiﬁcial marked dataset. Be-\nsides, these interpolation methods are mainly developed on\ntwo consecutive frames, while the high-order motion infor-\nmation of video frame sequence is ignored, and not well\nexploited. Furthermore, these specially designed interpola-\ntion methods could not be applied to another video frame\nsynthesis task, i.e. video frame extrapolation, because the\nlatter frame used to estimate ﬂow can not be provided in\nextrapolation task.\nUnlike the video frame interpolation task, video frame\nextrapolation can be treated as a conditional prediction\nproblem, that is, the future frames are predicted by the pre-\nvious video frame sequence in a recurrent model. As a pi-\noneer of recurrent-based extrapolation method, Shi et al.\nproposed a convolutional LSTM (ConvLSTM) [34] to re-\ncurrently generate future frames. However, due to the ca-\npability limitation of this simple and generic recurrent ar-\nchitecture, the predicted frames usually suffer from blurri-\nness. With the advances in research, several ConvLSTM\nvariations [40, 24, 12] were proposed to improve the per-\nformance. Speciﬁcally, Villegas et al. investigated a motion\nand content decomposition LSTM model, i.e. MCNet [40],\nto predict future frames. PredNet [24] uses the top-down\ncontext to guide the training process. Besides, the inception\noperation is introduced in LSTM to obtain a broadly view\nfor synthesizing [12]. While the extrapolated frames look\nsomewhat better, the accurate pixel estimation is still chal-\nlenging for multi-modal nature scenes. Additionally, the\nrecurrent model of ConvLSTM and its variations are hard\nto train, and meanwhile suffer from heavy computational\nburden when the length of sequence is large.\nAside from these recurrent models, the plain CNN archi-\ntecture based methods are also investigated to generate fu-\nture frames. Liu et al. developed a deep voxel ﬂow (DVF)\nincorporated neural network to generate the future frames\n[21], in which the 3D optical ﬂow across time and space\nare implicitly estimated. However, due to the lack of multi-\nframes joint training, DVF [21] has a limitation in long-\nterm multiple frames prediction. Different from these im-\nplicitly motion estimation methods, Wu et al. proposed an\nexplicitly motion detection and semantic estimation method\nto extrapolate future frames [46]. Although it performs well\nin situations where there are explicitly foreground motions,\nit suffers from a restriction in nature video scenes, where\nthe distinction between foreground motion target and back-\nground is not clear.\nAlthough these unidirectional prediction methods, i.e.,\nalong the frame sequence, can be applied to video frame in-\nterpolation task, these methods will suffer from heavy per-\nformance degradation, as compared with state-of-the-art in-\nterpolation algorithms. It is because the bi-directional in-\nformation provided by latter frames can not be exploited to\nguide the middle frames generating pipeline in these recur-\nrent and forward CNN based extrapolation models.\nIn order to bridge this gap, we propose, in this work, a\ngeneral video frame synthesis architecture, named convo-\nlutional Transformer (ConvTransformer), which uniﬁes and\nsimpliﬁes the video frame extrapolation and video frame\ninterpolation pipeline as an end-to-end encoder and de-\ncoder problem. A multi-head convolutional self-attention\nis proposed to model the long-range dependence between\nthe frames in video sequence. As compared with pre-\nviously elaborately designed interpolation methods, Con-\nvTransformer is a simple, but efﬁcient architecture in ex-\ntracting the high-order motion information existing in video\nframe sequence. Besides, ConvTransformer, in compar-\nison with previous ConvLSTM based recurrent models\n[34, 40, 24, 12], ConvTransformer can be implemented in\nparallel both in training and testing stage.\nWe evaluate ConvTransformer on several benchmarks,\ni.e. UCF101 [35], Vimeo90K [49], Sintel [13], REDS\n[26], HMDB [17] and Adobe240fps [37]. Experimental re-\nsults demonstrate that ConvTransformer performs better in\n2\nextrapolating future frames when compared with previous\nConvLSTM based recurrent models, and achives competi-\ntive results against state-of-the-art elaborately designed in-\nterpolation algorithms.\nThe main contributions of this paper are therefore as fol-\nlows.\n• A novel architecture named ConvTransformer is pro-\nposed for video frame synthesis.\n• A novel attention assigned multi-head convolutional\nself-attention is proposed for modeling the long-range\nspatial and temporal dependence on video frame se-\nquence.\n• The effectiveness and superiority of the proposed Con-\nvTransformer have been comprehensively analyzed in\nthis paper.\n2. Related Work\n2.1. Video Frame Synthesis\nVideo frame synthesis, including interpolation and ex-\ntrapolation two subtasks, is a hot research topic and has\nbeen extensively investigated in the literature [23, 21, 42,\n27, 28, 39, 40, 41]. In the following, we give a detailed\ndiscussion on video frame interpolation and extrapolation,\nrespectively.\nVideo frame interpolation. To interpolate the middle\nframes between two adjacent frames, traditional approaches\nmainly consist of two steps: motion estimation, and pixel\nsynthesis. With the accurately estimated motion vectors or\noptical ﬂow, the target frames can be interpolated via the\nconventional warping method. However, accurate motion\nestimation still remains a challenging problem because the\nmotion blur, occlusion and brightness change frequently ap-\npears in nature images.\nThe recent years have witnessed signiﬁcant advances in\ndeep neural networks which have shown great power in\nsolving a variety of learning problems [16, 33, 18, 32, 51],\nand have attracted much attention on applying it to video\nframe interpolation task. Long et al. developed a general\nCNN to directly synthesize the middle frames [23]. How-\never, a generic CNN, only stacking several convolutional\nlayers, is limited in capturing the dynamic motion of na-\nture scenes, and thereby their method usually yields se-\nvere visual artifact, e.g. motion blur and ringing artifact.\nLately, Niklaus et al. treated the frame interpolation prob-\nlem as a local convolutional kernel prediction problem, and\nproposed adaptive convolution operation [27] and separa-\nble convolution process [28] for each pixel in frames. To\nsome extent, these methods perform better than previous\ngeneral CNN based method. But even so, high memory\nfootprint is the typical characteristic of these kernel-based\nalgorithms, and thereby restricts its application. Unlike\nthese CNN and kernel-based algorithms, in literature [25],\na pixel-phase information learning network PhaseNet [25]\nwas proposed for frame interpolation. To a certain extent,\nit suppresses the artifacts as compared with previous meth-\nods. The performance, however, will be easily affected by\ncomplicated factors, e.g. large motion and disparity. With\nthe rapid development of exploiting deep neural networks\nfor optical ﬂow estimation [19] and depth map generating\n[7, 19], several implicitly or explicitly ﬂow and depth map\nguided neural networks [14, 2, 29, 1] have been investigated\nfor interpolating middle frames. The typically pipeline of\nthese algorithms lies in optical ﬂow or depth information\nestimating realised by pre-trained sub-networks [19, 7, 19]\nﬁrstly, and then a warping layer is introduced to adaptively\nwarping the input frames subsequently, and ﬁnally a infor-\nmation fusing layer is built to generate the target middle\nframes. These methods work well on anticipating occlusion\nand artifacts, and thus interpolate sharp images. However,\nthese methods are restricted by the pre-trained optical ﬂow\nsub-network PWC-Net [19] and depth map estimation sub-\nnetwork [7, 19] which can be easily affected by the training\nset. Last but not least, the architectures of these methods are\nall specially and elaborately designed, and hence the gener-\nalization to another video frame synthesis task, i.e. video\nframe extrapolation, is limited.\nVideo frame extrapolation.Extrapolating future video\nframes in video content still remains a challenging task be-\ncause of the multi-model of nature videos and the unex-\npected incident in the future. Traditional solutions take this\nproblem as a recurrent prediction problem [34, 40, 24]. Shi\net al. proposed a convolutional LSTM (ConvLSTM) archi-\ntecture [34], a pioneer recurrent model, to generate future\nframes. Given to the ﬂexibility of nature scenes and limited\nrepresentation ability of this simple architecture, the pre-\ndicted frames naturally suffer from blurriness. Lately, sev-\neral improved algorithms based on ConvLSTM architecture\nwere proposed, and, to some extent, achieved better per-\nformance. Speciﬁcally, Villegas et al. proposed a decom-\npostion LSTM model, namely MCNet [40], in which the\nmotion and content are decomposed respectively. In order\nto utilize the contextual information, Lotter et al. proposed\na top-down context guided LSTM model PredNet [24]. Be-\nsides, the inception mechanism is introduced in LSTM to\nobtain a broadly view for synthesizing [12]. Aside from\nthese LSTM based recurrent models, the plain CNN archi-\ntectures based models are also investigated to generate fu-\nture frames. Liu et al. proposed a 3D optical ﬂow, across the\ntime and space, guided neural networks, dubbed DVF [21],\nto extrapolated frames. However, due to the limitation in\nmulti-frames joint training, DVF [21] cannot work well on\nlong-term multiple frames prediction. Apart from these im-\nplicitly motion estimation and utilising methods, Wu et al.\n3\nMulti-Head Convolutional  \nAttention \nFeedForward \nAdd & Norm \nAdd & Norm \nPositional \nEncoding \nInput Sequential \nFrames \nFeature Embedding \nShared Weights Multi-Head Convolutional  \nSelf-Attention \nFeedForward \nAdd & Norm \nAdd & Norm \nN´\u0003\nEncoding Layer \nEncoder \nPositioned 2D Feature Maps \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nConv \nLReLU \nN´\u0003\nDecoder \nGenerated \nMiddle Frames \nPrediction \nSFFN \nSFFN \nSFFN \nPositional \nEncoding \nMiddle Frame Queries \nAdd & Norm \nQuery Self-Attention \nDecoding Layer \n3t-\n3t- 2t-\n2t- 1t-\n1t- 1t+\n1t+ 2t+\n2t+ 3t+\n3t+\n3t-\n3t- 2t-\n2t- 1t-\n1t- 1t+\n1t+ 2t+\n2t+ 3t+\n3t+\n-3 t\n-3 t -2 t\n-2 t -1 t\n-1 t 1t+\n1t+ 2t+\n2t+ 3t+\n3t+\n-3 \nˆ\nt\n-3 \nˆ\nt -2 \nˆ\nt\n-2 \nˆ\nt -1 \nˆ\nt\n-1 \nˆ\nt 1\nˆ\nt+\n1\nˆ\nt+ 2\nˆ\nt+\n2\nˆ\nt+ 3\nˆ\nt+\n3\nˆ\nt+\n-0.5 t\n-0.5 tt t\nt 0.5 t+\n0.5 tt+\n-0.5 \nˆ\nt\n-0.5 \nˆ\nttt\nˆ\nt\nDecode \nˆ\ntt 0.5 \nˆ\nt+\n0.5 \nˆ\nttt+\n-0.5 \nˆ\nt\n-0.5 \nˆ\nt\nˆ\nt\nˆ\nt\n0.5 \nˆ\nt+\n0.5 \nˆ\nt+\nFigure 2. An overview of ConvTransformer GθG. ConvTransformer GθG consists of four parts: feature embedding, encoder, decoder and\nprediction. In feature embedding part, a shared convolutional neural network transforms the input video frame image in RGB format\nto a compact feature maps space. In the following encoder step, several stacked encoding layers incorporating multi-head convolutional\nself-attention module are used to extract the long-term dependence among the input video sequence. Thirdly, the encoded video feature\nmaps sequence and target query frames are passed to the decoder layers. The sequential dependence between target frames and input video\nsequence is decoded in this step. Finally, the target frame sequence in RGB format is generated in the prediction step with the shared\nnetwork SFFN.\nproposed an explicitly motion detection and semantic esti-\nmation method to extrapolate future frames [46]. Although\nit works well when there are explicitly foreground motions\nin frame sequence, e.g. a car and a runner, it suffers from\na restriction in nature video scenes, where the distinction\nbetween the forground and background is not clear.\nAlthough recent years have witness a great progress in\nvideo interpolation and extrapolation two sub-tasks, there is\nstill a lack of a general and uniﬁed video frame synthesis\narchitecture that can perform well on both of these two sub-\ntasks. In order to overcome this issue, we propose an uni-\nﬁed architecture, named convolutional Transformer (Con-\nvTransformer), for video frame synthesis. A simple but efﬁ-\ncient multi-head convolutional self-attention architecture is\nproposed to model the long-range dependence between the\nvideo frame sequence. The experiment results conducted on\nseveral benchmarks typically demonstrate that the proposed\nConvTransformer works well both in video frame interpo-\nlation and extrapolation. To the best of our knowledge, it\nis the ﬁrst time that ConvTransformer architecture is pro-\nposed, and has been successfully applied to video frame\nsynthesis.\n2.2. Transformer Network\nTransformer [38] is a novel architecture for learning\nlong-range sequential dependence, which abandons the tra-\nditional building style of directly using RNN or LSTM ar-\nchitecture. It has been successfully applied to numerous\nnatural language processing (NLP) tasks, such as machine\ntranslation [15] and speech processing [8, 43]. Recently,\nthe basic Transformer architecture has been successfully ap-\nplied to the ﬁeld of image generation [30], image recogni-\ntion [9], and object detection [5]. Speciﬁcally, Nicolas et\nal. proposed the DETR [5] object detection method, and it\nachieves competitive result on COCO dataset as compared\nwith Faster R-CNN [31]. Through collapsing the spatial\ndimensions (two dimensions) into one dimension, DETR\nreasons about the relations of pixels and the global image\ncontext. Although DETR has successfully applied Trans-\nformer for computer vision task object detection, it is hard\nto use the basic Transformer to model the long term depen-\ndence among the two dimensional video frames, which are\nnot only temporally related, but also spatially related. In\norder to overcome this issue, a convolutional Transformer\n(ConvTransformer) is proposed in this work, and has been\nsuccessfully applied to video frame synthesis including in-\nterpolation and extrapolation two subtasks.\n3. Convolutional Transformer Architecture\nThe overall architecture of ConvTransformer GθG, as\nshown in Figure 2, consists of ﬁve main components, that is,\nfeature embedding module FθF, positional encoding mod-\nule PθP, encoder module EθE, decoder module DθD, and\nsynthesis feed-forward networkSθS. In this section, we ﬁrst\nprovide an overview of video frame synthesis pipeline re-\n4\nalised by ConvTransformer architecture, and then make an\nillustrated introduction of the proposed ConvTransformer.\nFinally, the implementation details and training loss are in-\ntroduced.\n3.1. Algorithm Review\nGiven a video frame sequence ˜X= {˜X0,˜X1,··· ,˜Xn}∈\nRH×W×C, where n is the length of sequence and H, W\nand C denote height, width, and the number of channels,\nrespectively, our goal is to synthesize intermediate frames\nˆX = {ˆXi+t0 , ˆXi+t1 ,··· , ˆXi+tk}at time tm ∈ [0,1], or\nextrapolate future frames ˆX= {ˆXn+1, ˆXn+2,···, ˆXn+mk}\nat order mk ∈N. Speciﬁcally, the future frames forecasting\nproblem can be viewed as:\nˆXn+1,···, ˆXn+mk = argmax\nXn+1,···,Xn+mk\nP(Xn+1,···,Xn+mk|˜X)\n(1)\nHere, P(·|·) stands for conditional probability operation.\nFirstly, the Feature Embeddingmodule embeds the in-\nput video frames, and then generates representative fea-\nture maps. Subsequently, the extracted feature maps of\neach frame are added with the positional maps, which are\nused for positional identity. Next, the positioned frame fea-\nture maps are passed as inputs to the Encoder to exploit\nthe long-range sequential dependence among each frame in\nvideo sequence. After getting the encoded high-level fea-\nture maps, the high-level feature maps and positioned frame\nqueries are simultaneously passed into the Decoder, and\nthen the sequential dependence between the query frames\nand input sequential video frames is decoded. Finally, the\ndecoded feature maps are fed into the Synthesis Feed-\nForward Networks (SFFN)to generate the ﬁnal middle\ninterpolated frames or extrapolated frames.\n3.2. Feature EmbeddingFθF\nIn order to extract a compact feature representation for\nsubsequent effective learning, a representative feature is\ncomputed by a 4-layer convolution with Leaky ReLu ac-\ntivation function and hidden dimension dmodel. Given a\nvideo frame Xi ∈RH×W×3, the embedded feature maps\nHi ∈RH×W×dmodel can be presented with the following\nequation:\nJi = FθF(Xi), i ∈[1,n] (2)\nIt is worth mentioning that all input video frames share\nnot only the same embedding net architecture FθF, but also\nthe parameters θF.\n3.3. Positional EncodingPθP\nSince our model contains no recurrence across the video\nframe sequence, some information about relative or abso-\nlute position of the video frames must be injected in the\nframes’ feature maps, so that the order information of the\nvideo frame sequence can be utilized. To this end, “posi-\ntional encodings” are added at each layer in encoder and de-\ncoder. It is noted that the positional encoding in ConvTrans-\nformer is a 3D tensor which is different from that in origi-\nnal Transformer architecture built for vector sequence. The\npositional encoding has the same dimension as the frame\nfeature maps, so that they can be summed directly. In this\nwork, we use sine and cosine functions of different frequen-\ncies to encode the position of each frame in video sequence:\nPos Map(p,(i,j,2k)) = sin(p/100002k/dmodel) (3)\nPos Map(p,(i,j,2k+1)) = cos(p/100002k/dmodel) (4)\nwhere p ∈[1,n] is the positional token, (i,j) represents\nthe spatial location of features and the channel dimension is\nnoted as 2k. That is, each dimension of the positional en-\ncoding corresponds to a sinusoid. The wavelengths form a\ngeometric progression from 2π to 10000 ∗2π. We choose\nthis function because it would allow the model to easily\nlearn to attend relative positions for any ﬁxed offset m,\nPos Map(p+m) can be represented as a linear function of\nPos Map(p).\nGiven an embedded feature maps Ji, the positioned em-\nbedding process can be viewed as the following equation:\nZi = Ji ⊕Pos Map(i), i ∈[1,n] (5)\nwhere the ⊕operation represents element-wise tensor addi-\ntion.\n3.4. EncoderEθE and DecoderDθD\nEncoder: As shown in Figure 2, the encoder is modeled\nas a stack ofNidentical layers consisting of two sub-layers,\ni.e., multi-head convolutional self-attention layer and a sim-\nple 2D convolutional feed-forward network. The residual\nconnection is adopted around each of the two sub-layers,\nfollowed by group normalization [47]. To facilitate these\nresidual connections, all sub-layers in the model, as well\nas the embedding layers, produce outputs of the same di-\nmensional dmodel. Given a positioned feature sequence\nZ= {Z0,Z1,··· ,Zi,··· ,Zn−1,Zn} ∈RH×W×dmodel,\nthe identiﬁed feature sequence ˆZ= {ˆZ0,ˆZ1, ··· ,Zn} ∈\nRH×W×dmodel can be learned, and the encoding operation\ncan be represented as:\nˆZ= EθE(Z) (6)\nDecoder: The decoder is also composed of a stack of N\nidentical layers, which consists of three sub-layers. In ad-\ndition to the two sub-layers as implemented in Encoder, an\nadditional layer called query self-attention is inserted to per-\nform the convolutional self-attention over the output frame\n5\nConvolutional Self-Attention \nConcat \nMulti-Head Attention \nLinear \n0\n0 1\n1\nCNN CNN CNN CNN CNN \nCNN CNN CNN CNN CNN \nEE EE EE EE EE\nAttention Maps \nShared \nShared \nSum \nElement-Wise Production \nValue Maps \nKey Maps \nConcat \nQuery \nMaps \nFeature Maps \nQ K V ǃ ǃ Q K V ǃ ǃ Q K V ǃ ǃ Q K V ǃ ǃ Q K V ǃ ǃ\ni\ni 1n-\n1n- n\nn\n0\n0 1\n1 i\ni 1n-\n1n- n\nn\n0\n00\ni\ni\nn\nn\n0\n0 1\n1 i\ni 1n-\n1n- n\nn\n( ,0) i\n( ,0) ( ,0) ( ,0) ( ,1) i\n( ,1) ( ,1) ( ,1) ( , ) i i \n( , ) ( , ) ( , ) ( , 1) i n -\n( , 1) \n( , 1) \n( , 1) ( , 1) ( , 1) ( , ) i n \n( , ) \n( , ) \n( , ) \nˆ\ni\nˆ\ni\nConvolutional Self-Attention \nC C C C C\nFigure 3. (left) Convolutional Self-Attention. (right) Multi-Head Attention in parallel.\nqueries. Given a query sequence Q= {Q0,Q1,··· ,Qn}∈\nRH×W×d, the decoding process can be conducted as:\nˆQ= DθD( ˆZ,Q) (7)\nIt should be emphasized that the encoding and decoding\nprocess are all conducted in parallel.\n3.5. Multi-Head Convolutional Self-Attention\nWe call our particular attention “Convolutional Self-\nAttention” ( as shown in Figure 3), which is computed upon\nfeature maps. The convolutional self-attention operation\ncan be described as mapping a query map and a set of key-\nvalue map pairs to an output, where the query map, key\nmaps, value maps, and output are all 3D tensors. Given an\ninput comprised of sequential feature maps U= {U0,U1,·\n·· ,Un} ∈RH×W×dmodel, we apply convolutional sub-\nnetwork to generate the query map and paired key-value\nmap of each frame, i.e.,U′= {{Q0,K0,V0},{Q1,K1,V1},·\n··,{Qn,Kn,Vn}}∈ RH×W×dmodel.\nGiven a set of {Qi,Ki,Vi}of frame Ui, the attention\nmap H(i,j) ∈RH×W×1 of frame Ui and Uj can be gen-\nerated by applying a compatible sub-network MθM to the\nquery map Qi with the corresponding key map Kj, which\ncan be represented as following equation:\nH(i,j) = MθM(Qi,Kj) (8)\nAfter getting all the corresponding attention mapH(i) =\n{H(i,1),H(i,2),··· ,H(i,n)}∈ RH×W×1, we make a con-\ncatenation operation of H(i) in the third dimension, and\nthen a SoftMax operation is applied to H(i) ∈RH×W×n\nalong the dimension dim= 3.\nH(i) = SoftMax(H(i))dim, dim = 3 (9)\nFinally, the output ˆVi can be obtained with summation\nof the element wise production with attention map H(i,j)\nand the corresponding value map Vj. This operation can be\nrepresented as:\nV(i) =\nn∑\nj=1\nH(i,j)Vj (10)\nIn order to jointly attend to information from different\nrepresentation subspaces at different feature spaces, a multi-\nhead pipeline is adopted. The process can be viewed as:\nMultiHead(ˆVi) = Concat(ˆVi1 ,···,ˆVih) (11)\n3.6. Synthesis Feed-Forward NetworkSθS\nIn order to synthesize the ﬁnal photo realistic video\nframes, we construct a frame synthesis feed-forward net-\nwork, which consists of 2 cascaded sub-networks built upon\na U-Net-like structure. The frames states ˆQdecoded from\nprevious decoder are fed into SFFN in parallel. This process\ncan be represented as:\nˆXi = SθS( ˆQi), i ∈[1,N′] (12)\n3.7. Initialization of Query SetQ\nAs an indispensable part of Decoder, query set Qis crit-\nical for accurate extrapolation and interpolation. Speciﬁ-\ncally, given 4 input frames X = {X1,X2,X3,X4}, Con-\nvTransformer extrapolates 3 frames ˆX = {ˆX1, ˆX2, ˆX3}.\nThe query Qi equals to the embedded feature maps of the\nlast input frame, i.e.,J4. On the other hand, given a 6-frame\nsequence X = {Xt−3,Xt−2,Xt−1,Xt+1,Xt+2,Xt+3},\n6\nTable 1. Details about trainset, validationset and testset\nTraining Validation Test\nVimeo90K\n( 64612 sequences)\nAdobe240fps\n(2120 sequences)\nVimeo90K\n( 20 sequences)\nAdobe240fps\n(120 sequences)\nViemo90K (935 sequences)\nUCF101 (2533 sequences)\nAdobe240fps (2660 sequences)\nSintel (1581 sequences)\nHMDB (2684 sequences)\nConvTransormer interpolates 3 frames between frameXt−1\nand Xt+1, i.e., ˆX= {ˆXt−0.5, ˆXt, ˆXt+0.5}. The query Qi is\ncalculated by the element-wise average calculation of two\nadjacent frames’ feature maps, i.e.,Jt−1 and Jt+1. Speciﬁ-\ncally, Qi = Jt−1∆Jt+1, where the operation∆ is element-\nwise average calculation.\n3.8. Training LossLGθG\nIn this work, we choose the most widely used content\nloss, i.e., pixel-wise MSE loss, to guide the optimizing pro-\ncess of ConvTransformer. The MSE lossis calculated as:\nLGθG\n= 1\nN′\nN′\n∑\ni=1\nˆXi −Yi\n\n2\n(13)\nHere, N′stands for the number of synthesized results, ˆXi\nrepresents the synthesized target frame, and the Yi is the\ncorresponding groundtruth.\n4. Experiments and Analysis\nIn this section, we provide the details for experiments,\nand results that demonstrate the performance and efﬁciency\nof ConvTransformer, and compare it with previous pro-\nposed specialized video frame extrapolation methods, elab-\norately designed video frame interpolation algorithms and\ngeneral video frame synthesis solutions on several bench-\nmarks. Besides, to further validate the proposed Con-\nvTransformer, we conducted several ablation studies.\n4.1. Datasets\nTo create the trainset of video frame sequence, we\nleverage the frame sequence from the Vimeo90K [49] and\nAdobe240fps [37] dataset. On the other hand, we also\nexploit several other widely used benchmarks, including\nUCF101 [36], Sintel [13], REDS [26] and HMDB [17], for\ntesting. Table 1 represents the details about training, valida-\ntion and testing sets.\n4.2. Training Details and Parameters Setting\nPytorch platform is used in our experiment for train-\ning. The experimental environment is the Linux opera-\ntion system Ubuntu 16.04 LTS running on a sever with an\nAMD Ryzen 7 3800X CPU at 3.9GHz and a NVIDIA RTX\n3090 GPU. In order to guarantee the convergence of Con-\nvTransformer, the Adam optimizer is adopted for training,\nin which, the initial learning rate is set to 10−4 and is re-\nduced with exponential decay, where the decay rate is 0.95\nwhile the decaying quantity is 20000. The whole training\nproceeds for 6 ×105 iterations. Besides, the length of input\nsequence is 4 for video frame prediction, while for the video\ninterpolation task, the length of the input sequence is 6. The\nmore setting details about ConvTransformer are represented\nin appendix ﬁle.\n4.3. Comparisons with state-of-the-arts\nIn order to evaluate the performance of the proposed\nConvTransformer, we compare our ﬁnally trained Con-\nvTransformer on several public benchmarks with state-of-\nthe-art video frame synthesis method DVF [21], representa-\ntive ConvLSTM based video frame extrapolation algorithm\nMCNet [40], and specially designed video frame interpo-\nlation solutions, i.e., SepConv [28], CyclicGen [20], DAIN\n[1] and BMBC [29]. For a fair comparison, we reimple-\nmented and retrained these methods with the same trainset\nfor training our ConvTransformer. Two widely used image\nquality metrics, i.e., peak signal to noise ratio (PSNR) [11]\nand structural similarity (SSIM) [44], are adopted as the ob-\njective evaluation criteria. The quantitative results of video\nframe extrapolation are tabulated in Table 2, while Table 3\nrepresents the quantitative comparison of video frame in-\nterpolation. Besides, the visual comparisons of synthesized\nimages with zoomed details and residual occlusion maps are\nillustrated in Figure 1 and Figure 4, respectively.\nAs observed in Table 2, the proposed ConvTransformer\nhas given rise to better performance than DVF [21] and\nMCNet [40]. More concretely, taking the next frame ex-\ntrapolation as an example, the relative performance gains of\nConvTransformer over the DVF and MCNet [40] models,\nin terms of index PSNR, are 2.7140dB and 1.8983dB on\nVimeo 90k [49], 1.6819dB and 2.2137dB on Adeobe240fps\n[37], as well as 0.1321dB and 1.6734dB on UCF101 [36].\nBesides, ConvTransformer, in terms of average comparison,\nachieves 1.5094dB and 1.9285dB advantage on DVF [21]\nand MCNet [40] respectively. Additionally, the superior-\nity of ConvTransformer becomes larger in multiple future\nframes extrapolation. Speciﬁcally, ConvTransformer gains\nan advantage of 2.22dB in PSNR criterion over the method\nDVF [21], while it is 1.68dB in previous next frame extrap-\nolation on the same benchmark Adobe240fps [37].\nFigure 1 visualizes the qualitative comparisons. As\nobserved from the zoomed-in regions in Figure 1, the\nproposed ConvTransformer could extrapolate more photo-\nrealistic frames, while the predicted frames generated from\nprevious methods DVF [21] and MCNet [40] suffer from\nimage degradation, such as image distortion and local\nsmother. Besides, the residual occlusion maps suggest that\nConvTransformer has superiority on accurate pixel value es-\ntimation, as compared with DVF [21] and MCnet [40].\n7\n/uni00000016/uni00000018/uni00000011/uni00000013/uni0000001b/uni00000012/uni00000013/uni00000011/uni0000001c/uni0000001a\n/uni00000036/uni00000048/uni00000053/uni00000026/uni00000052/uni00000051/uni00000059\n/uni00000015/uni0000001b/uni00000011/uni0000001a/uni0000001b/uni00000012/uni00000013/uni00000011/uni0000001c/uni0000001a\n/uni00000027/uni00000024/uni0000002c/uni00000031\n/uni00000015/uni00000018/uni00000011/uni00000016/uni00000017/uni00000012/uni00000013/uni00000011/uni0000001c/uni00000016\n/uni00000026/uni0000005c/uni00000046/uni0000004f/uni0000004c/uni00000046/uni0000002a/uni00000048/uni00000051\n/uni00000015/uni00000016/uni00000011/uni00000015/uni00000018/uni00000012/uni00000013/uni00000011/uni0000001c/uni00000014\n/uni00000026/uni0000005c/uni00000046/uni0000004f/uni0000004c/uni00000046/uni0000002a/uni00000048/uni00000051/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048\n/uni00000016/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000012/uni00000013/uni00000011/uni0000001c/uni0000001a\n/uni00000025/uni00000030/uni00000025/uni00000026\n/uni00000015/uni00000019/uni00000011/uni00000015/uni00000016/uni00000012/uni00000013/uni00000011/uni0000001c/uni00000016\n/uni00000027/uni00000039/uni00000029\n/uni00000016/uni00000019/uni00000011/uni00000017/uni00000014/uni00000012/uni00000013/uni00000011/uni0000001c/uni0000001b\n/uni00000032/uni00000058/uni00000055/uni00000056\n/uni00000033/uni00000036/uni00000031/uni00000035/uni00000012/uni00000036/uni00000036/uni0000002c/uni00000030\n/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000047/uni00000003/uni00000057/uni00000055/uni00000058/uni00000057/uni0000004b\n/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000015/uni00000013/uni00000013\nFigure 4. Visual Comparisons of ConvTransformer with other state-of-the-art video frame interpolation methods: SepConv [28], DAIN\n[1], CyclicGen [20], CyclicGen-large [20], BMBC [29] and DVF [21].\nTable 2. Video frame extrapolation: Quantitative evaluation of ConvTransformer with state-of-the-art methods.\nModel Next frame Next 3 framesUCF101 [36] Adobe240fps [37] Vimeo90K [49] Average UCF101 [36] Adobe240fps [37] Vimeo90K [49] AveragePSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nDVF [21]29.1493 0.9181 28.7414 0.9254 27.8021 0.9073 28.5642 0.9169 26.1174 0.8779 25.8625 0.8598 24.5277 0.8432 25.5025 0.8603MCNet [40]27.6080 0.8504 28.2096 0.8796 28.6178 0.8726 28.1451 0.8675 25.0179 0.7766 24.9485 0.7721 25.4455 0.7671 25.1373 0.7719Ours 29.2814 0.9205 30.4233 0.9457 30.5161 0.9406 30.0736 0.9356 26.7584 0.8874 28.0835 0.9045 27.1441 0.8926 27.3286 0.8948\nIn a nutshell, the quantitative and qualitative results\nprove that the proposed ConvTransformer, incorporating\nmulti-head convolutional self-attention mechanism, can ef-\nﬁciently model the long-range sequential dependence in\nvideo frames, and then extrapolate high quality future\nframes.\nExcept for the video frame extrapolation comparison,\nwe also make a video frame interpolation comparison with\nthe popular and state-of-the-art methods, including the gen-\neral video frame synthesis method DVF [21] and ﬁve spe-\ncialized interpolation solutions, namely namely SepConv\n[28], CyclicGen [20], CyclicGen-large [20], DAIN [1] and\nBMBC [29]. The performance indices of these methods\nare illustrated in Table 3. We can easily ﬁnd that the Con-\nvTransformer has attained better performance over the pre-\nvious synthesis method DVF [21]. In view of the PSNR\nand SSIM index, the proposed ConvTransformer has intro-\nduced a relative performance gain of 1.36dB and 0.0205 on\naverage. On the other hand, as compared with specially\ndesigned interpolation methods, ConvTransformer outper-\nforms most solutions, and achieves competitive results as\ncompared with best algorithm DAIN [1]. Concretely, con-\nsidering the PSNR and SSIM criteria, ConvTransformer\nachieves 31.57dB and 0.9151 on average, which is better\nthan popular methods SepConv-Lf [28], CyclicGen [20],\nCyclicGen-large [20] and BMBC [29]. Although Con-\nvTransformer does not outperform state-of-the-art interpo-\nlation method DAIN, the performance gap between them\nis not so large. Furthermore, as compared with elaborately\ndesigned algorithm DAIN [1], ConvTransformer is a more\ngeneral model, which not only works well on video frame\ninterpolation, but also performs well on video frame extrap-\nolation. On the contrary, the solution DAIN [1] could not\nbe used for the video frame extrapolation task, because the\nlatter frame , used for estimating and depth map and optical\nﬂow map, cannot be provided in video frame extrapolation\ntask.\nAccording to the visual and quantitative comparisons\nabove, two dominant conclusions can be drawn. First, Con-\nvTransformer is an efﬁcient model for synthesizing photo-\nrealistic extrapolation and interpolation frames. Second, in\ncontrast to the specially designed extrapolation and inter-\npolation methods, i.e., MCnet [40], SepConv [28], Cyclic-\nGen [20], CyclicGen-large [20], DAIN [1] and BMBC [29],\nConvTransformer is a unify and general architecture, that is,\nit performs well in both two subtasks.\n4.4. Ablation Study\nIn order to evaluate and justify the efﬁciency and supe-\nriority of each part in the proposed ConvTransformer archi-\ntecture, several ablation experiments have been conducted\nin this work. Speciﬁcally, we gradually modify the baseline\nConvTransformer model and compare their differences.\n4.4.1 Investigation of Positional Encoding and Resid-\nual Connection\nWe separately eliminate the positional encoding module,\nresidual connection operation and both of them, and dub\nthese three degradation networks as ConvTransformer-wo-\npos, ConvTransformer-wo-res and ConvTransformer-wo-\nres-wo-pos, in which, the abbreviation wo represents with-\nout, pos indicates positional encoding and res is an abbrevi-\nation of residual connection. These three degradation algor-\ntihms are trained with the same trainset and implementation\napplied to ConvTransformer. We tabulate their performance\non extrapolation task in terms of objective quantitative in-\ndex PSNR and SSIM in Table 4, and visual comparisons in\nFigure 5.\nAs summarized in Table 4, ConvTransformer has at-\ntained the best performance and achieves a relative perfor-\nmance gain of 0.1971dB, 1.4001dB and 1.4813dB in com-\nparison with three degradation models ConvTransformer-\n8\nTable 3. Video frame interpolation: Quantitative evaluation of ConvTransformer with state-of-the-art methods.\nModel Sintel [13] UCF101 [36] Adobe240fps [37] HMDB [17] Vimeo [49] REDS [26] Average\nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nSpecialized\nSepConv-Lf [28]31.68 0.9470 32.51 0.9473 36.36 0.9844 33.90 0.9483 33.49 0.9663 21.32 0.6965 31.54 0.9149\nDAIN [1] 31.37 0.9452 32.720.950636.15 0.9837 33.89 0.9487 33.95 0.9701 21.85 0.7203 32.65 0.9197\nCyclicGen [20]31.54 0.9104 33.03 0.9303 35.60 0.9691 34.16 0.9242 33.04 0.9319 21.29 0.5686 31.44 0.8724\nCyclicGen-large [20]31.19 0.9004 32.43 0.9239 34.89 0.9626 33.75 0.9208 32.04 0.9163 20.92 0.5465 30.87 0.8617\nBMBC [29] 27.01 0.9223 27.92 0.9412 28.58 0.9569 28.42 0.9384 30.61 0.9629 21.21 0.7056 27.29 0.9045\nGeneral DVF [21] 30.71 0.9303 32.31 0.9454 33.24 0.9613 33.59 0.9469 30.99 0.9379 20.44 0.6460 30.21 0.8946\nOurs 31.44 0.9469 32.48 0.950436.42 0.9844 33.37 0.9492 34.030.9637 21.68 0.695931.57 0.9151\n24.83/0.71\n-wo-pos-wo-res\n24.70/0.71\n-wo-pos-w-res\n26.04/0.76\n-w-pos-wo-res\n26.11/0.76\nConvTransformer\nPSNR/SSIM\nGroud truth\n0\n50\n100\n150\n200\n250\nFigure 5. Effect of positional encoding and residual connection. The ﬁrst three columns represent the visual results of ablation experiments.\nThe second row represents the zoomed-in local details, while the third row shows the occlusion map with color bar displaying the pixel\nresidual in the range of 0 to 255. The visual comparison in zoomed-in local details indicate that positional encoding module and reisudal\nconnection is helpful for synthesizing photo-realistic images.\nTable 4. Comparative results achieved with the ablation of residaul\nconnection and positional encoding.\nModel Residual Connection Positional Encoding PSNR SSIM\nConvTransformer ! ! 29.8883 0.9383\nConvTransformer-wo-res# ! 29.6912 0.9367\nConvTransformer-wo-pos! # 28.4882 0.9173\nConvTransformer-wo-res-wo-pos# # 28.4070 0.9104\nwo-pos, ConvTransformer-wo-res and ConvTransformer-\nwo-res-wo-pos, respectively. Besides, visual comparisons\nin Figure 5 efﬁciently conﬁrm these quantitative analysis.\nAs shown in Figure 5, the stairs represented in zoomed-in\narea typically demonstrate that ConvTransformer with po-\nsitional encoding and residual connection has an advance\nin suppressing artifacts and preserving local high-frequency\ndetails.\nWe also visualize the convergence process of these three\ndegradation networks in Figure 7. The convergence curves\nare consistent with the quantitative and qualitative compar-\nisons above. As observed in Figure 7, we can ﬁnd that the\nperformance of ConvTransformer can easily be affected by\nthe module positional encoding, and the residual connection\ncan stabilize the training process.\nTo sum up everything that has been stated so far, the posi-\ntional encoding and residual connection architecture is ben-\neﬁcial for our proposed ConvTransformer to perform well.\nTable 5. Comparative results achieved by ConvTransformer with\ndifferent head numbers\nModel ConvTransoformer-H-1 ConvTransoformer-H-2 ConvTransoformer-H-3 ConvTransoformer-H-4\nPSNR 33.0054 33.7861 34.0274 34.2438\nSSIM 0.9641 0.9695 0.9714 0.9731\n4.4.2 Investigation of Multi-Head Numbers Setting\nThe head numbers H is a hyperparameter that allows Con-\nvTransformer to jointly attend to information from differ-\nent representation subspaces at different positions. In order\nto justify the efﬁciency of multi-head architecture, several\nmulti-head variation experiments have been implemented in\nthis work, and the quantitative index in terms of PSNR and\nSSIM, and visual examples are illustrated in Table 5 and\nFigure 6, respectively.\nAs listed in Table 5, ConvTransformer-H-4, in the\nlight of index PSNR, gains a relative performance gain of\n1.2438dB in comparison with ConvTransformer-H-1. Be-\nsides, the visual comparisons, in Figure 6, indicate that\nConvTransformer-H-4 generates more photo-realistic im-\nages. From these quantitative and visual comparisons, we\ncan ascertain that more heads are helpful for ConTrans-\nformer architecture to incorporate information from differ-\nent representation subspaces at different frame states.\n9\n/uni00000016/uni00000017/uni00000011/uni00000015/uni00000018/uni00000012/uni00000013/uni00000011/uni0000001c/uni00000019\n/uni00000014/uni00000003/uni0000004b/uni00000048/uni00000044/uni00000047\n/uni00000016/uni00000019/uni00000011/uni00000013/uni00000016/uni00000012/uni00000013/uni00000011/uni0000001c/uni0000001a\n/uni00000015/uni00000003/uni0000004b/uni00000048/uni00000044/uni00000047/uni00000056\n/uni00000016/uni0000001a/uni00000011/uni00000013/uni00000018/uni00000012/uni00000013/uni00000011/uni0000001c/uni0000001b\n/uni00000016/uni00000003/uni0000004b/uni00000048/uni00000044/uni00000047/uni00000056\n/uni00000016/uni0000001a/uni00000011/uni0000001a/uni00000017/uni00000012/uni00000013/uni00000011/uni0000001c/uni0000001b\n/uni00000017/uni00000003/uni0000004b/uni00000048/uni00000044/uni00000047/uni00000056\n/uni00000033/uni00000036/uni00000031/uni00000035/uni00000012/uni00000036/uni00000036/uni0000002c/uni00000030\n/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000047/uni00000003/uni00000057/uni00000055/uni00000058/uni00000057/uni0000004b\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000014/uni00000018/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000015/uni00000018/uni00000013\nFigure 6. Visual comparisons of ConvTransformer variants with different head numbers. The zoomed-in details are shown in the second\nrow, and the third row illustrates the occlusion maps. The zoomed-in details demonstrate that ConvTransformer with more heads has an\nadvantage in synthesizing details.\n0 100000 200000 300000 400000\nIteration times\n14\n16\n18\n20\n22\n24\n26Peak Signal to Noise Ratio(dB)\nw_res/w_pos\nw_res/wo_pos\nwo_res/w_pos\nwo_res/wo_pos\nFigure 7. Plots of PSNR convergence curves of ConvTransformer\nwith the ablation of residual connection and positional encoding.\nw res/w pos represents ConvTransformer with residual connec-\ntion and positional encoding, w res/wo pos represents ConvTran-\nformer with the ablation of positional encoding, and wo res/w pos\nindicates that the residual connection is ablated.\nTable 6. Quantitative evaluation of ConvTransformer with varia-\ntion number of layers in Encoder and Decoder\nModel ConvTransoformer-L-1 ConvTransoformer-L-2 ConvTransoformer-L-3 ConvTransoformer-L-5\nPSNR 34.2438 34.4623 34.4989 34.6031\nSSIM 0.9731 0.9741 0.9744 0.9754\n4.4.3 Investigation of Layer Numbers Setting\nThe layer numbers N is a hyperparameter which allows us\nto vary the capacity and computational cost of the encoder\nmodule and decoder module in ConvTransformer. To inves-\ntigate the trade-off between performance and computational\ncost mediated by this hyperparameter, we conduct experi-\nments with ConvTransformer for a range of differentNval-\nues. Such modiﬁed networks are named ConvTransformer-\nL-1, ConvTransformer-L-2, and so on. These layer varia-\ntion networks were fully trained, and the performance in-\nFigure 8. Plots of PSNR convergence curves of ConvTransformer\nunder different head numbers, i.e., 1 head, 2 heads, 3 heads and 4\nheads.\ndices of these networks are illustrated in Table 6.\nAs tabulated in Table 6, we can easily ﬁnd that the\nConvTransformer-L-5 has acquired the optimal perfor-\nmance. In view of the PSNR index, ConvTransformer-L-\n5 has introduced a relative performance gain of 0.3593dB\nin comparison with ConvTransformer-L-1. Consequently,\nto achieve an excellent performance, it is desirable that the\nencoder and decoder contain more layers. However, more\nlayers will take much more time for the networks to conver-\ngence, and consume more memory in training and testing.\nIn summary, although more layers will improve the repre-\nsentative ability of ConvTransformer, we should set appro-\npriate value N, in practice, and take the training time and\nmemory burden into consideration.\n4.5. Long-Term Frame Sequence Dependence Anal-\nysis\nIn order to verify whether the proposed multi-head con-\nvolutional self-attention could efﬁciently capture the long-\n10\nX0 X1 X2 X3 X4 X5\nHead 0\nHead 1\nHead 2\nHead 3\nFigure 9. The attention maps Hi for decoding query Qi on input sequence {X0, X1, ...,X5}under different heads. ColorBar with gradient\nfrom yellow to blue represents the attention value in the range 0 to 1. The comparisons along each vertical column demonstrate that\neach head responds for exploiting different dependence. For example, the attention map Hi−(1,3), looking like a yellow map, indicates\nthat frame X3 contributes local high-frequency information for target synthesize frame in Head 1. Besides, the attention map Hi−(2,2),\npredominated by blue, represents that X2 supplies low-pass information for synthesizing target frame in Head 2.\nrange sequential dependence within a video frame se-\nquence, we visualize the attention maps of decoder query\nQi on input sequence Xon decoder layer 1. As shown in\nFigure 9, the corresponding attention mapHi−(k,j) normal-\nized in range [0, 1] represents the exploiting of input frame\nXj for synthesizing the target frame ˆQi in head k. It should\nbe emphasized that the attention value close to 1 is drawn\nwith color blue, while the color yellow represents the atten-\ntion value close to 0.\nWith the vertical comparison in different positions, such\nas X0, X1, X2, X3, X4 and X5, we can ﬁnd that differ-\nent heads respond for incorporating different information\nfor synthesizing target frames. For instance, attention map\nHi−(1,0) predominant by blue indicates that attention layer\nin head 1 with position 0 mainly response for capturing\nthe low-pass background dependence between frame X0\nand target frame. Besides, along the lines of Hi−(1,0), the\nattention map Hi−(1,2) looking like a yellow map, revels\nthat frame X2 supplies local high-frequency information for\nsynthesizing target frame in head 1.\nIn summary, through the analysis above and attention\nmaps shown in Figure 9, a conclusion can be drawn that\nthe proposed multi-head convolutional self-attention can ef-\nﬁciently model different long-term information dependen-\ncies, i.e., foreground information, background information\nand local high-frequency information.\n5. Conclusion\nIn this work, we propose a novel video frame synthe-\nsis architecture ConvTransformer, in which the multi-head\nconvolutional self-attention is proposed to model the long-\nrange spatially and temporally relation of frames in video\nsequence. Extensive quantitative and qualitative evaluations\ndemonstrate that ConvTransfomer is a concise, compact and\nefﬁcient model. The successful implementation of Con-\nvTransformer sheds light on applying it to other video tasks\nthat need to exploit the long-term sequential dependence in\nvideo frames.\nReferences\n[1] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang,\nZhiyong Gao, and Ming-Hsuan Yang. Depth-aware video\nframe interpolation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 3703–\n3712, 2019. 2, 3, 7, 8, 9\n[2] Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao,\nand Ming-Hsuan Yang. Memc-net: Motion estimation and\nmotion compensation driven neural network for video inter-\npolation and enhancement. IEEE transactions on pattern\nanalysis and machine intelligence, 2019. 3\n[3] W. Bao, X. Zhang, L. Chen, L. Ding, and Z. Gao. High-order\nmodel and dynamic ﬁltering for frame rate up-conversion.\nIEEE Transactions on Image Processing, 27(8):3813–3826,\n2018. 1\n[4] T. Brooks and J. T. Barron. Learning to synthesize motion\nblur. In 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 6833–6841, 2019. 1\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020. 4\n11\n[6] R. Castagno, P. Haavisto, and G. Ramponi. A method for\nmotion adaptive frame rate up-conversion. IEEE Trans-\nactions on Circuits and Systems for Video Technology ,\n6(5):436–446, 1996. 1\n[7] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-\nimage depth perception in the wild. In D. Lee, M. Sugiyama,\nU. Luxburg, I. Guyon, and R. Garnett, editors, Advances in\nNeural Information Processing Systems, volume 29, 2016. 2,\n3\n[8] Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer:\na no-recurrence sequence-to-sequence model for speech\nrecognition. In 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pages\n5884–5888. IEEE, 2018. 4\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 4\n[10] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep\nstereo: Learning to predict new views from the world’s im-\nagery. In 2016 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 5515–5524, 2016. 1\n[11] Alain Hor ´e and Djemel Ziou. Image quality metrics: Psnr vs.\nssim. In International Conference on Pattern Recognition,\nICPR 2010, Istanbul, Turkey, 23-26 August 2010, 2010. 7\n[12] M. Hosseini, A. S. Maida, M. Hosseini, and G. Raju.\nInception-inspired lstm for next-frame video prediction. 2,\n3\n[13] Joel Janai, Fatma Guney, Jonas Wulff, Michael J Black, and\nAndreas Geiger. Slow ﬂow: Exploiting high-speed cameras\nfor accurate and diverse optical ﬂow reference data. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3597–3607, 2017. 2, 7, 9\n[14] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan\nYang, Erik Learned-Miller, and Jan Kautz. Super slomo:\nHigh quality estimation of multiple intermediate frames for\nvideo interpolation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 9000–\n9008, 2018. 3\n[15] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of\nNAACL-HLT, pages 4171–4186, 2019. 4\n[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.\nWeinberger, editors, Advances in Neural Information Pro-\ncessing Systems, volume 25, pages 1097–1105. Curran As-\nsociates, Inc., 2012. 2, 3\n[17] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.\nHMDB: a large video database for human motion recog-\nnition. In Proceedings of the International Conference on\nComputer Vision (ICCV), 2011. 2, 7, 9\n[18] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu. High performance\nvisual tracking with siamese region proposal network. In\n2018 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 8971–8980, 2018. 2, 3\n[19] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. In 2018 IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2041–2050. IEEE Computer Society, 2018. 2, 3\n[20] Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu\nChuang. Deep video frame interpolation using cyclic frame\ngeneration. In Proceedings of the 33rd Conference on Artiﬁ-\ncial Intelligence (AAAI), 2019. 7, 8, 9\n[21] Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and\nAseem Agarwala. Video frame synthesis using deep voxel\nﬂow. In Proceedings of the IEEE International Conference\non Computer Vision, pages 4463–4471, 2017. 2, 3, 7, 8, 9\n[22] G. Long, L. Kneip, J. M. Alvarez, and H. Li. Learning image\nmatching by simply watching video. European Conference\non Computer Vision, 2016. 2\n[23] Gucan Long, Laurent Kneip, Jose M Alvarez, Hongdong Li,\nXiaohu Zhang, and Qifeng Yu. Learning image matching by\nsimply watching video. In European Conference on Com-\nputer Vision, pages 434–450. Springer, 2016. 3\n[24] William Lotter, Gabriel Kreiman, and David Cox. Deep pre-\ndictive coding networks for video prediction and unsuper-\nvised learning. In 5th International Conference on Learning\nRepresentations, ICLR 2017, 2017. 2, 3\n[25] Simone Meyer, Oliver Wang, Henning Zimmer, Max Grosse,\nand Alexander Sorkine-Hornung. Phase-based frame inter-\npolation for video. In 2015 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). 3\n[26] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik\nMoon, Sanghyun Son, Radu Timofte, and Kyoung Mu\nLee. Ntire 2019 challenge on video deblurring and super-\nresolution: Dataset and study. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) Work-\nshops, June 2019. 2, 7, 9\n[27] S. Niklaus, L. Mai, and F. Liu. Video frame interpolation via\nadaptive convolution. In 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 2270–\n2279, 2017. 2, 3\n[28] S. Niklaus, L. Mai, and F. Liu. Video frame interpolation\nvia adaptive separable convolution. In 2017 IEEE Interna-\ntional Conference on Computer Vision (ICCV) , pages 261–\n270, 2017. 2, 3, 7, 8, 9\n[29] Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su\nKim. Bmbc: Bilateral motion estimation with bilat-\neral cost volume for video interpolation. arXiv preprint\narXiv:2007.12622, 2020. 3, 7, 8, 9\n[30] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In International Conference on Machine\nLearning, pages 4055–4064. PMLR, 2018. 4\n[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. In Advances in Neural Information\nProcessing Systems (NIPS), 2015. 4\n[32] W. Ren, J. Zhang, X. Xu, L. Ma, X. Cao, G. Meng, and W.\nLiu. Deep video dehazing with semantic segmentation.IEEE\n12\nTransactions on Image Processing, 28(4):1895–1908, 2019.\n1, 3\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn Nassir Navab, Joachim Hornegger, William M. Wells, and\nAlejandro F. Frangi, editors, Medical Image Computing and\nComputer-Assisted Intervention – MICCAI 2015, pages 234–\n241, Cham, 2015. Springer International Publishing. 2, 3\n[34] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,\nWai-Kin Wong, and Wang-chun Woo. Convolutional lstm\nnetwork: A machine learning approach for precipitation\nnowcasting. Advances in neural information processing sys-\ntems, 28:802–810, 2015. 2, 3\n[35] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 2\n[36] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. Computer ence, 2012. 7, 8, 9\n[37] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo\nSapiro, Wolfgang Heidrich, and Oliver Wang. Deep video\ndeblurring for hand-held cameras. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 1279–1288, 2017. 2, 7, 8, 9\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 4\n[39] Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru\nErhan, Quoc V Le, and Honglak Lee. High ﬁdelity video\nprediction with large stochastic recurrent neural networks.\nIn NeurIPS, 2019. 3\n[40] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin,\nand Honglak Lee. Decomposing motion and content for nat-\nural video sequence prediction. In Proceedings of the In-\nternational Conference on Learning Representations, ICLR,\n2017. 2, 3, 7, 8\n[41] Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn,\nXunyu Lin, and Honglak Lee. Learning to generate long-\nterm future via hierarchical prediction. InInternational Con-\nference on Machine Learning, ICML, 2017. 3\n[42] Carl V ondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. In Proceedings\nof the 30th International Conference on Neural Information\nProcessing Systems, pages 613–621, 2016. 3\n[43] Yongqiang Wang, Abdelrahman Mohamed, Due Le, Chunxi\nLiu, Alex Xiao, Jay Mahadeokar, Hongzhao Huang, Andros\nTjandra, Xiaohui Zhang, Frank Zhang, et al. Transformer-\nbased acoustic modeling for hybrid speech recognition.\nIn ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pages\n6874–6878. IEEE, 2020. 4\n[44] Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and\nEero P. Simoncelli. Image quality assessment: From error\nvisibility to structural similarity. IEEE Transactions on Im-\nage Processing, 13(4), 2004. 7\n[45] Manuel Werlberger, Thomas Pock, Markus Unger, and Horst\nBischof. Optical ﬂow guided tv-l 1 video interpolation and\nrestoration. In International Workshop on Energy Minimiza-\ntion Methods in Computer Vision and Pattern Recognition ,\npages 273–286. Springer, 2011. 2\n[46] Yue Wu, Rongrong Gao, Jaesik Park, and Qifeng Chen.\nFuture video synthesis with object motion prediction. In\n2020 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 5538–5547, 2020. 2, 4\n[47] Yuxin Wu and Kaiming He. Group normalization. In Pro-\nceedings of the European conference on computer vision\n(ECCV), pages 3–19, 2018. 5\n[48] H. Xu, Y . Gao, F. Yu, and T. Darrell. End-to-end learning\nof driving models from large-scale video datasets. In 2017\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 3530–3538, 2017. 1\n[49] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and\nWilliam T Freeman. Video enhancement with task-\noriented ﬂow. International Journal of Computer Vision ,\n127(8):1106–1125, 2019. 2, 7, 8, 9\n[50] Zhefei Yu, Houqiang Li, Zhangyang Wang, Zeng Hu, and\nChang Wen Chen. Multi-level video frame interpolation:\nExploiting the interaction among different levels. IEEE\nTransactions on Circuits and Systems for Video Technology,\n23(7):1235–1248, 2013. 2\n[51] C. Lawrence Zitnick, Sing Bing Kang, Matthew Uytten-\ndaele, Simon Winder, and Richard Szeliski. High-quality\nvideo view interpolation using a layered representation.Acm\nTransactions on Graphics, 23(3):600–608, 2004. 1, 3\n13\nAppendix\nA. Appendix-1: The details about module SFFN\nConvBlock \nConvBlock \nDeconvBlock \nConv \nDeconvBlock \nConv \nResidual Connection \nDecoded High Level \nFeature Maps \nSynthesised Target \nFrames \n3 C H W ´ ´ ´ \n3 3 H W ´ ´ ´ \nConvBlock \nConv \nLRelu \nConv \nLRelu \nConv \nLRelu \nConvBlock \nConv \nLRelu \nConv \nLRelu \nDeconvBlock \nDeConv \nLRelu \nConv \nLRelu \nConv \nLRelu \nDeconvBlock \nDeConv \nLRelu \nConv \nLRelu \nAdd Operation \n3 3 H W ´ ´ ´ \nFigure 10. The design of module SFFN. SFFN consists of two U-Net like sub networks, i.e., SFFN1 and SFFN2. The pooling operation,\nfor simplicity, and skip connection in U architecture are omitted. SFFN1 contains 5 layers, while SFFN2 is deeper, and includes 10 layers.\nThe basic unit ConvBlock contains two convolutional layers, and DeconvBlock contains one deconvolutional layer and one convolutional\nlayer. Given decoded feature maps sequence 3 ∗C ∗W ∗H, the target synthesized frame sequence 3 ∗3 ∗H ∗W in RGB format is\ngenerated with the use of SFFN1 and SFFN2. A residual connection, in order to accelerate convergence, is built between the SFFN1 and\nthe ﬁnal output.\nB. Appendix-2: The prove of the effectiveness of Positional Map proposed in ConvTransformer\nAs shown in Figure 11, the position of each frame is ﬁrst encoded with positional vector (PosVector) along the channel\ndimension, and then these PosVectors are expanded to the target positonal map (PosMap) through the repeating operation\nalong the vertical (height) and horizontal (width) orientation. According to equations (3) and (4), for arbitrary pixel coordinate\n(i,j) in frame p+ m, the PosMap(p+m,(i,j)) can be represented as follows.\nHorizontal \nVertical \nPosMap 6 \nĂĂ \nHorizontal \nVertical \nPosMap 2 \nHorizontal \nVertical \nPosMap 1 \nPosVector 1 PosVector 2 PosVector 3 PosVector 4 PosVector 5 PosVector 6 \nPosVector to PosMap \nFigure 11. An illustration for positional map generating.\nPos Map (p+ m,(i,j,2 k)) = sin( wk(p + m)) = sin( wkp) cos( wkm) + cos( wkp) sin( wkm) (14)\n14\nPos Map (p+ m,(i,j,2k+1)) = cos( wk(p + m)) = cos( wkp) cos( wkm) −sin( wkp) sin( wkm) (15)\n[\nPosMap p+ m, ( i,j, 2 k)\nPosMap p+ m, ( i,j, 2 k+1)\n]\n=\n[\ncos( wk m ) sin( wk m )\n− sin( wk m ) cos( wk m )\n][\nsin( wk p)\ncos( wk p)\n]\n(16)\n[\nPosMap p + m, ( i,j, 2 k )\nPosMap p + m, ( i,j, 2 k +1)\n]\n= M\n[\nPosMap p, ( i,j, 2 k )\nPosMap p, ( i,j, 2 k +1)\n]\n(17)\nwk represents 100002k/dmodel. As illustrated in equations (14), (15), (16) and (17), the PosMapp+m can be represented as a\nlinear function of PosMapp, which proves that the proposed PosMap could efﬁciently model relative position relationship\nbetween different frames.\nC. Appendix-3:The parameters setting about ConvTransformer\nTable 7 tabulates the parameter setting about one ConvTransformer. There are 4 heads, 7 encoder layers and 7 decoder\nlayers.\nD. Appendix-4: Attention Calculation Process\nThe calculation process of multi-head convolutional self-attention inEncoder, query self-attention in Decoder and multi-\nhead convolutional attention in Decoder are represented in Figure 12.\n15\nTable 7. The parameter setting of ConvTransformer for synthesizing 3 target frames. It includes 7 encoder layers and 7 decoder layers,\nwhile each attention layer of encoder and decoder consists of 4 heads. Thedmodel is 128, and hence, the depth of each head is 32. In each\nhead, the Q Net generates the query feature maps, while the K V Net generates the key feature maps and value feature maps.\nModule Settings Output SizeInput Sequence - - - s×3×h×wFeature Embedding [Conv2D 3×3s= 1n= 128]×4 s×128×h×w\nEncoder\n\n\nMulti Head Convolutional Self Attention(Conducted on input frame sequence)\n\n\n\nHead0\n\nQNetEnc0 [Conv2D 3×3s= 1n= 32]KVNetEnc0 [Conv2D 3×3s= 1n= 32]AttNetEnc0 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead1\n\nQNetEnc1 [Conv2D 3×3s= 1n= 32]KVNetEnc1 [Conv2D 3×3s= 1n= 32]AttNetEnc1 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead2\n\nQNetEnc2 [Conv2D 3×3s= 1n= 32]KVNetEnc2 [Conv2D 3×3s= 1n= 32]AttNetEnc2 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead3\n\nQNetEnc3 [Conv2D 3×3s= 1n= 32]KVNetEnc3 [Conv2D 3×3s= 1n= 32]AttNetEnc3 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\n\nFeed Forward [Conv2D 3×3s= 1n= 128]\n\n\n×7 (numberofencodinglayers) s×128×h×w\nQuery Sequence - - - 3×128×h×w\nDecoder\n\n\nQuery Self Attention(Conducted on query frame sequence)\n\n\n\nHead0\n\nQNetQue0 [Conv2D 3×3s= 1n= 32]KVNetQue0 [Conv2D 3×3s= 1n= 32]AttNetQue0 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead1\n\nQNetQue1 [Conv2D 3×3s= 1n= 32]KVNetQue1 [Conv2D 3×3s= 1n= 32]AttNetQue1 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead2\n\nQNetQue2 [Conv2D 3×3s= 1n= 32]KVNetQue2 [Conv2D 3×3s= 1n= 32]AttNetQue2 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead3\n\nQNetQue3 [Conv2D 3×3s= 1n= 32]KVNetQue3 [Conv2D 3×3s= 1n= 32]AttNetQue3 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\n\nMulti Head Convolutional Self Attention(Conducted on query frame sequenceand encodedinput frame sequence)\n\n\n\nHead0\n\nQNetDec0 [Conv2D 3×3s= 1n= 32]KVNetDec0 [Conv2D 3×3s= 1n= 32]AttNetDec0 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead1\n\nQNetDec1 [Conv2D 3×3s= 1n= 32]KVNetDec1 [Conv2D 3×3s= 1n= 32]AttNetDec1 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead2\n\nQNetDec2 [Conv2D 3×3s= 1n= 32]KVNetDec2 [Conv2D 3×3s= 1n= 32]AttNetDec2 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\nHead3\n\nQNetDec3 [Conv2D 3×3s= 1n= 32]KVNetDec3 [Conv2D 3×3s= 1n= 32]AttNetDec3 [Conv2D 3×3s= 1n= 1]\n\n\n\n\n\n\nFeed Forward [Conv2D 3×3s= 1n= 128]\n\n\n×7 (numberofdecodinglayers) s×128×h×w\nPrediction\n\n\nSFFN\n\n\nSFFN0\n\n\nConvBlock0[Conv2D 3×3s= 1n= 256Conv2D 3×3s= 1n= 256\n]\nConvBlock1[Conv2D 3×3s= 1n= 512Conv2D 3×3s= 1n= 512\n]\nDeconvBlock0[Conv2D 3×3s= 1n= 256Conv2D 3×3s= 1n= 256\n]\nDeconvBlock1[Conv2D 3×3s= 1n= 128Conv2D 3×3s= 1n= 128\n]\nConv\n\nConv2D 3×3s= 1n= 64Conv2D 3×3s= 1n= 32Conv2D 1×1s= 1n= 3\n\n\n\n\nSFFN1\n\n\nConvBlock0[Conv2D 3×3s= 1n= 32Conv2D 3×3s= 1n= 32\n]\nConvBlock1[Conv2D 3×3s= 1n= 64Conv2D 3×3s= 1n= 64\n]\nConvBlock2[Conv2D 3×3s= 1n= 128Conv2D 3×3s= 1n= 128\n]\nConvBlock3[Conv2D 3×3s= 1n= 256Conv2D 3×3s= 1n= 256\n]\nConvBlock4[Conv2D 3×3s= 1n= 512Conv2D 3×3s= 1n= 512\n]\nDeconvBlock0[Conv2D 3×3s= 1n= 256Conv2D 3×3s= 1n= 256\n]\nDeconvBlock1[Conv2D 3×3s= 1n= 128Conv2D 3×3s= 1n= 128\n]\nDeconvBlock2[Conv2D 3×3s= 1n= 64Conv2D 3×3s= 1n= 64\n]\nDeconvBlock3[Conv2D 3×3s= 1n= 32Conv2D 3×3s= 1n= 32\n]\nConv [Conv2D 1×1s= 1n= 3]\n\n\n\n\n\n\ns×3×h×w\nOutput Sequence - - - s×3×h×w\n16\nK_V_Net \nConv \n ×  × ℎ × \" Input \nFrame Sequence \nQ_Net \nConv \n ×  × ℎ × \" \nExpand \ndim=0 \nExpand \ndim=0 \nExpand \ndim=1 \n ×  × ℎ × \" \nRepeat \ndim=0 \nRepeat \ndim=0 \nRepeat \ndim=0 \n1 × ×  × ℎ × \" 1 × ×  × ℎ × \" \nReshape Reshape Reshape \n × 1 × × ℎ × \" \n ×  ×  × ℎ × \"  ×  ×  × ℎ × \"  ×  ×  × ℎ × \" \nConcate \ndim=1 \n( ∗ ) × × ℎ × \" ( ∗ ) × × ℎ × \" ( ∗ ) × × ℎ × \" \n( ∗ ) × (2 ) ×ℎ × \" \nAtt_Net \n( ∗ ) × 1 ×ℎ× ! \nAttention Maps \n( ∗ ) × × ℎ × \" \nWeighted Value Maps \n( ∗ ) × × ℎ × \" \nWeighted Value Maps \nSum \ndim=1 \nReshape \n ×  ×  × ℎ × \" \n × 1 × × ℎ × \" \nSequeeze \ndim=1 \n ×  × ℎ × \" \nK_V_Net \nConv \n ×  × ℎ × \" Input \nFrame Sequence \nQ_Net \nConv \n ×  × ℎ × \" \nExpand \ndim=0 \nExpand \ndim=0 \nExpand \ndim=1 \n ×  × ℎ × \" \nRepeat \ndim=0 \nRepeat \ndim=0 \nRepeat \ndim=0 \n1 × ×  × ℎ × \" 1 × ×  × ℎ × \" \nReshape Reshape Reshape \n × 1 × × ℎ × \" \n ×  ×  × ℎ × \"  ×  ×  × ℎ × \"  ×  ×  × ℎ × \" \nConcate \ndim=1 \n( ∗ ) × × ℎ × \" ( ∗ ) × × ℎ × \" ( ∗ ) × × ℎ × \" \n( ∗ ) × (2 ) ×ℎ × \" \nAtt_Net \n( ∗ ) × 1 ×ℎ× ! \nAttention Maps \n( ∗ ) × × ℎ × \" \nWeighted Value Maps \nSum \ndim=1 \nReshape \n ×  ×  × ℎ × \" \n × 1 × × ℎ × \" \nSequeeze \ndim=1 \n ×  × ℎ × \" \nEncoder \nConvolutional Self-Attention on \nInput Frame Sequeence \nK_V_Net \nConv \n×  × ℎ × \" Query \nFrame Sequence \nQ_Net \nConv \n×  × ℎ × \" \nExpand \ndim=0 \nExpand \ndim=0 \nExpand \ndim=1 \n×  × ℎ × \" \nRepeat \ndim=0 \nRepeat \ndim=0 \nRepeat \ndim=0 \n1 ××  × ℎ × \" 1 ××  × ℎ × \" \nReshape Reshape Reshape \n× 1 × × ℎ × \" \n× ×  × ℎ × \" × ×  × ℎ × \" × ×  × ℎ × \" \nConcate \ndim=1 \n(∗ ) × × ℎ × \" (∗ ) × × ℎ × \" (∗ ) × × ℎ × \" \n(∗ ) × (2 ) ×ℎ × \" \nAtt_Net \n(∗ ) × 1 ×ℎ× ! \nAttention Maps \n(∗ ) × × ℎ × \" \nWeighted Value Maps \nSum \ndim=1 \nReshape \n× ×  × ℎ × \" \n× 1 × × ℎ × \" \nSequeeze \ndim=1 \n×  × ℎ × \" \nK_V_Net \nConv \n ×  × ℎ × \" Encoded \nInput FrameSequence \nQ_Net \nConv \n ×  × ℎ × \" \nExpand \ndim=0 \nExpand \ndim=0 \nExpand \ndim=1 \nRepeat \ndim=0 \nRepeat \ndim=0 \nRepeat \ndim=0 \n1 × ×  × ℎ × \" 1 × ×  × ℎ × \" \nReshape Reshape Reshape \n× 1 × × ℎ × \" \n×  × ! × ℎ × # ×  × ! × ℎ × # ×  × ! × ℎ × # \nConcate \ndim=1 \n(∗ ) × × ℎ × \" (∗ ) × × ℎ × \" (∗ ) × × ℎ × \" \n(∗ ) × (2 ) ×ℎ × \" \nAtt_Net \n(∗ ) × 1 ×ℎ× ! \nAttention Maps \n(∗ ) × × ℎ × \" \nWeighted Value Maps \nSum \ndim=1 \nReshape \n×  × ! × ℎ × # \n× 1 × × ℎ × \" \nSequeeze \ndim=1 \n×  × ℎ × \" \n×  × ℎ × \" \nQuery \nFrame Sequence \nDecoder \nQuery Self-Attention \n(Conducted on Query Frame Sequence) \nConvolutional -Attention \n(Conducted on Query Frame Sequence \nand Encoded Input Frame Sequence) \n×  × ℎ × \" \nFigure 12. The calculation process of multi-head convolutional self-attention in Encoder, query self-attention in Decoder and multi-head\nconvolutional attention in Decoder.\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8340650796890259
    },
    {
      "name": "Artificial intelligence",
      "score": 0.67473965883255
    },
    {
      "name": "Convolutional neural network",
      "score": 0.649628758430481
    },
    {
      "name": "Computer vision",
      "score": 0.5274030566215515
    },
    {
      "name": "Encoder",
      "score": 0.5066619515419006
    },
    {
      "name": "Frame (networking)",
      "score": 0.5052211880683899
    },
    {
      "name": "Transformer",
      "score": 0.4947945475578308
    },
    {
      "name": "Residual frame",
      "score": 0.42920950055122375
    },
    {
      "name": "Reference frame",
      "score": 0.3734533190727234
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3601503372192383
    },
    {
      "name": "Computer network",
      "score": 0.07216969132423401
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 59
}