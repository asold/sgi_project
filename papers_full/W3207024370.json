{
  "title": "GNN-LM: Language Modeling based on Global Contexts via GNN",
  "url": "https://openalex.org/W3207024370",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2660934061",
      "name": "Meng, Yuxian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352057708",
      "name": "Zong Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2214920734",
      "name": "Li, Xiaoya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117238540",
      "name": "Sun Xiao-fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222792781",
      "name": "Zhang, Tianwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1841886282",
      "name": "Wu, Fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225948551",
      "name": "Li Jiwei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2159782014",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W3090145300",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W3035691519",
    "https://openalex.org/W2041404167",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3038572442",
    "https://openalex.org/W2077815765",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2988841832",
    "https://openalex.org/W2962767366",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W3118184560",
    "https://openalex.org/W3177401493",
    "https://openalex.org/W2795933031",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2594978815",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W3160195908",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W3101707147",
    "https://openalex.org/W3118895645",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3037854022",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3020775333",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W2124509324",
    "https://openalex.org/W3118018449",
    "https://openalex.org/W3174977793",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W3170427498",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2889283903",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2963430354",
    "https://openalex.org/W2963653811",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3012625278",
    "https://openalex.org/W2889646190",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962946486",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3035531963",
    "https://openalex.org/W3126880001",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2951714314"
  ],
  "abstract": "Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. \\footnote{The code can be found at https://github.com/ShannonAI/GNN-LM",
  "full_text": "Published as a conference paper at ICLR 2022\nGNN-LM: L ANGUAGE MODELING BASED ON\nGLOBAL CONTEXTS VIA GNN\nYuxian Meng1, Shi Zong2, Xiaoya Li1, Xiaofei Sun1, Tianwei Zhang3, Fei Wu4, Jiwei Li1,4\n1Shannon.AI, 2Nanjing University,3Nanyang Technological University,4Zhejiang University\n{yuxian_meng, xiaoya_li, xiaofei_sun, jiwei_li}@shannonai.com, szong@nju.edu.cn\ntianwei.zhang@ntu.edu.sg, wufei@zju.edu.cn\nABSTRACT\nInspired by the notion that ‚Äú to copy is easier than to memorize ‚Äù, in this work,\nwe introduce GNN-LM, which extends vanilla neural language model (LM) by\nallowing to reference similar contexts in the entire training corpus. We build a\ndirected heterogeneous graph between an input context and its semantically related\nneighbors selected from the training corpus, where nodes are tokens in the input\ncontext and retrieved neighbor contexts, and edges represent connections between\nnodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate\ninformation from similar contexts to decode the token. This learning paradigm\nprovides direct access to the reference contexts and helps improve a model‚Äôs\ngeneralization ability. We conduct comprehensive experiments to validate the\neffectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity\nof 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the\nvanilla LM model), and shows substantial improvement on One Billion Word and\nEnwiki8 datasets against strong baselines. In-depth ablation studies are performed\nto understand the mechanics of GNN-LM. 1\n1 I NTRODUCTION\nLanguage modeling (LM) is a basic and long-standing task in natural language processing (Shannon,\n2001; Bahl et al., 1983; Chen & Goodman, 1999; Mikolov et al., 2012; Xie et al., 2017). It aims\nat predicting the upcoming token given the sequence of previous context consisting of a sequence\nof tokens. A common practice to train a language model is to enforce the model to maximize the\nprobability of the upcoming ground-truth token at training time. At test time, the next token to predict\ncould be the one with the highest probability (via greedy search) or the one that maximizes a window\nof tokens through the beam search strategy. This form of training-test procedure can be viewed as a\nprocess of memorization, or doing a close-book examination, if we compare the training data to a\nbook and inference to doing an examination: The process of iterating N epochs over the training\ndata is comparable to reviewing the book N times and the model needs to memorize what is the most\nlikely to appear given speciÔ¨Åc context based on the training data. At test time, the book needs to be\nclosed, i.e., the model does not have means to refer to the training data at test time, and the model has\nto invoke related memory to predict the next token during inference.\nThere are two limitations to this close-book examination strategy: (1) the memorization-based\nlanguage models are usually hard to memorize the knowledge of hard examples (e.g., long-tail cases\nin the training set); (2) memory required to memorize the whole training data is usually intensive. The\ndifÔ¨Åculty of resolving these two problems can be substantially alleviated if the model can be provided\nwith related contexts from the training set so that the model can reference them for decisions. This\nprocess can be viewed as a strategy different from memorization or close-book examination ‚Äì copy,\nor in other words, open-book examination. For example, given a preÔ¨Åx ‚ÄúJ. K. Rowling is best known\nfor writing‚Äù and we want to predict the upcoming token, a language model will more easily generate\ntoken ‚ÄúHarry‚Äù if it can refer to the context ‚ÄúJ. K. Rowling wrote the Harry Potterfantasy series‚Äù.\nMotivated by the observation that ‚Äúto copy is easier than to memorize‚Äù, or ‚Äúan open-book exam is\neasier than to a close-book exam ‚Äù, in this work, we introduce a new language modeling scheme\n1The code can be found at https://github.com/ShannonAI/GNN-LM\n1\narXiv:2110.08743v5  [cs.CL]  4 May 2022\nPublished as a conference paper at ICLR 2022\nInput \nContext \nùíÑùíï\nThe movie is\nBase LM\nùíâùë°\nùëê(1): This movie is great .\nùëê(2):Those movies are bad .\nùëê(3):This movie is what I like.\nNeighbor \nContexts \nThe movie is\nmovies is what\nmovie is great\nmovies are bad\nGraph Construction (ùíç = ùíì = ùüè)\nTraining\nDatastore\nInter-context edge\nIntra-context edge\nInput \nquery\nùíÑùíï ùëéùëú ùëéùëú ùëéùëú\nùëéùëõ ùëéùëõ ùëéùëõ ùëéùëõ ùëéùëõ ùëéùëõ\nùëéùëõ ùëéùëõ ùëéùëõ\nùëéùëú\nùëéùëõ\nNode from the \noriginal context\nNode from the \nneighbor context\nFigure 1: An overview of the proposed GNN-LM model pipeline. Left: Given an input context\nct = (w1,¬∑¬∑¬∑ ,wt‚àí1) (here the context is ‚Äú The movie is ‚Äù), a base LM model encodes it into a\nhigh-dimensional representation ht, which is then used to query the training datastore to retrieve\nthe nearest contexts along with the visited tokens (marked in red). Right: The tokens in the input\ncontext and the retrieved tokens comprise a graph and are viewed as two types of nodes: nodes from\nthe original text and nodes from the neighbor text. Intra-context edges link tokens within the same\ninput, and inter-context edges link tokens from the retrieved contexts to the original context. After\nmodeling the graph as a whole with GNNs, we use the updated representation of wt‚àí1 (token ‚Äúis‚Äù in\nthis example) to compute the likelihood of the next token.\n‚Äì GNN-LM, which provides an LM model with the ability to reference similar contexts from the\nentire training corpus as cues for prediction. The similar contexts, deÔ¨Åned as the kneighbors of the\ninput in the training corpus, are served as additional references for the model to predict the next\ntoken. To integrate retrieved neighbors with the input, we build a directed heterogeneous graph\non top of the input and the extracted contexts, where nodes are the tokens and edges represent the\nconnections between them. We deÔ¨Åne two types of nodes ‚Äì the original node from the input context\nand the neighbor node from the extracted contexts, and two types of edges ‚Äì the inter-context edge\nand the intra-context edge that respectively associate inter (i.e., between retrieved contexts and input\ncontexti.e., context within the input) and intra (i.e., context within the inputi.e., context in the retrieved\nsentences) contexts. A graph neural network (GNN) is employed to aggregate information from\nboth inter-context and intra-context, which is used to generate the target token. We observe that the\nproposed scheme retrieves the related contexts as references, making it signiÔ¨Åcantly easier for the\nmodel to predict upcoming words in the LM task.\nWe further combine GNN-LM with kNN-LM (Khandelwal et al., 2019), an orthogonal technique\nenhancing language models, to improve the overall performance of our model. We carry out\nexperiments on three widely used language modeling benchmarks: WikiText-103, One Billion\nWord and Enwik8. Experimental results show that our proposed framework outperforms the strong\nbaseline on all three benchmarks. SpeciÔ¨Åcally, applying the GNN-LM framework to a strong base\nLM leads to substantial performance boost (-1.9 perplexity) on WikiText-103, and combining with\nkNN-LM achieves a new state-of-the-art perplexity of 14.8 ‚Äì a 3.9 point improvement over the base\nLM. We perform comprehensive analyses including complexity analysis and the effects of different\ncomponents to better understand the mechanics of GNN-LM.\n2 GNN-LM\n2.1 O VERALL PIPELINE\nWe present the overall pipeline of our model in Figure 1. At each time stept, a neural language model\n(LM) f(¬∑) Ô¨Årst encodes a sequence of context tokens ct = (w1,w2,...,w t‚àí1) to a high-dimensional\nrepresentation ht = f(ct) ‚ààRd, where dis the dimension of hidden states. Then a transformation\nmatrix W ‚ààRV√ód is used to estimate the probability of the t-th token p(wt|ct) =softmax(Wht),\nwhere V is the size of the vocabulary. We augment the vanilla neural language model by allowing it\nto reference samples in the training set that are similar to the current decoded sequence. Concretely,\nwe leverage a novel self-attention augmented Graph Neural Network (GNN) on top of the vanilla LM\nto enable message passing between the context cand retrieved reference tokens from the training\n2\nPublished as a conference paper at ICLR 2022\nset, updating the representation ht generated by the vanilla LM. The updated representation, which\naggregates additional information from reference tokens, is then used to estimate pLM(wt|ct).\n2.2 G RAPH CONSTRUCTION\nThe Ô¨Årst step of our proposed framework is to build a graph capturing the connections between the\ncontext tokens ct = (w1,w2,...,w t‚àí1) and those similar to ct in the training set. To this end, we\nconstruct a directed heterogeneous graph, where the nodes are tokens from ct or the tokens from\nthe neighbor contexts retrieved from the training set, and the edges represent different relationships\nbetween the nodes to be discussed below.\nFormally, we deÔ¨Åne a graph as G= (V,E,A,R,œÑ,œÜ ), where Vis a collection of nodes v and E\nis a collection of edges e. We deÔ¨Åne two types of nodes A= {ao,an}, where ao means that the\nnode is within the input ct. an means the node is in N(ct), the set of extracted contexts within\nthe neighborhood of ct. We also deÔ¨Åne two types of edges R= {rinter,rintra}, where rinter means\ninter-context connection (from an nodes to ao nodes) and rintra means intra-context connection\n(between two nodes of same type). Each token within the input is a node of type ao, and edges of\ntype rintra are constructed from node wi to wj (i‚â§j), which can be viewed as a graph interpretation\nof the transformer structure. Both nodes and edges are associated with their respective type mapping\nfunctions œÑ(v) :V‚ÜíA and œÜ(e) :E‚ÜíR .\nFor an input context ct, we retrieve k nearest neighbors N(ct) = {c(1)\nt1 ,..., c(k)\ntk }of ct from the\ntraining set as follows: we Ô¨Årst use ht to query the cached representations of all tokens for training\nsamples, where the cached representations are obtained by a pretrained LM. The distance is measured\nby the cosine similarity2, and we retrieve the top K tokens denoted by {w(i)\nj }. The superscript (i)\ndenotes the i-th training sample and the subscript j denotes the j-th time step. w(i)\nj thus means that\nthe j-th time step of the i-th training sample is retrieved as one of the nearest neighbors to ht. w(i)\nj is\nexpanded to c(i)\nj by adding both left and right contexts, where c(i)\nj = {w(i)\nj+p}r\np=‚àíl, where land r\nrespectively denote the left and right window size. The corresponding representations {h(i)\nj+p}r\np=‚àíl\nare used as the initialized node embeddings\nDifferent from kNN-LM (Khandelwal et al., 2019) that uses w(i)\nj+1, which is the token right after\nthe retrieved token w(i)\nj , to directly augment the output probability, we explicitly take advantage of\nall contextual tokens near w(i)\nti as additional information in the form of graph nodes. In this way,\nthe model is able to reference similar contexts in the training set and leverage the corresponding\nground-truth target tokens via the heterogeneous graph built on both the original input tokens and the\ncontext reference tokens.\nFor the neighbor context window size l and r, we set l = r = 1 in all experiments. During\nexperiments, we Ô¨Ånd that using shallow (i.e. 3) GNN layers and adding rintra edges between adjacent\ntokens can alleviate overÔ¨Åtting. Since a 3-layer GNN only aggregates information from 3-hop nodes\nin the graph, using larger land rhave no inÔ¨Çuence on GNN representations.\n2.3 GNN ON THE CONSTRUCTED GRAPH\nWe now use graph neural networks (GNNs) to aggregate and percolate the token information based\non the graph constructed in Section 2.2. In this work, to accommodate the modeling of rintra from\nnode wi to wj (i‚â§j) within ct, where Transformer with self-attention is usually adopted, we extend\nthe self-attention mechanism to rinter, and construct a self-attention augmented GNN.\nSpeciÔ¨Åcally, the l-th layer representation of node nis computed by (here we use the superscript [l] to\nrepresent the l-th layer):\nh[l]\nn = Aggregate\n‚àÄs‚ààN(n)\n(Attention(s,e,n ) ¬∑Feature(s,e,n )) +h[l‚àí1]\nn . (1)\nAttention(s,e,n ) estimates the importance of the source node son target node nwith relationship e,\nFeature(s,e,n ) is the information feature that sshould pass to n, and Aggregate(¬∑) aggregates the\n2In practice, we use FAISS (Johnson et al., 2019) for fast approximate kNN search.\n3\nPublished as a conference paper at ICLR 2022\nneighborhood message with the attention weights. To draw on the information in the heterogeneous\ngraph, we use different sets of parameters for different node types œÑ(¬∑) and different edge types œÜ(¬∑)\nakin to Hu et al. (2020).\nAttention Similar to the multi-head attention mechanism of Transformer (Vaswani et al., 2017),\nthe Attention(¬∑,¬∑,¬∑) operator in our model consists of hheads, which compute attention weights\nindependently, followed by concatenation to get the Ô¨Ånal output. For simplicity, we only describe the\nsingle-head situation below. For each edge (s,e,n ), the representation of target node nis mapped to\na query vector Q(n), and the representation of source node sis mapped to a key vector K(s). The\nscaled inner-production is then used to compute the attention weight between Q(n) and K(s), which\nis further normalized over all edges that have the same edge type:\nK(s) =Wk\nœÑ(s)h[l‚àí1]\ns , Q(n) =Wq\nœÑ(n)h[l‚àí1]\nn ,\nAttention(s,e,n ) = 1\nZexp\n(\nK(s)WATT\nœÜ(e)Q(n)‚ä§¬∑¬µ‚ü®œÑ(s),œÜ(e),œÑ(n)‚ü©‚àö\nd\n)\n,\nZ =\n‚àë\ns‚Ä≤‚ààN(n),e‚Ä≤‚ààœÜ(e)\nAttention(s‚Ä≤,e‚Ä≤,n),\n(2)\nwhere d is the hidden dimensionality, and Wq\nœÑ(s) ‚àà Rd√ód, Wk\nœÑ(n) ‚àà Rd√ód, WATT\nœÜ(e) ‚àà Rd√ód ,\n¬µ‚ààR|A|√ó|R|√ó|A|are learnable model parameters.\nFeature Parallel to the calculation of attention weights, we propagate information from source\nnode sto target node n. The single-head feature is deÔ¨Åned by:\nFeature(s,e,n ) =Wv\nœÑ(s)h[l‚àí1]\ns WFEA\nœÜ(e), (3)\nwhere Wv\nœÑ(s) ‚ààRd√ód and WFEA\nœÜ(e) ‚ààRd√ód are learnable model parameters.\nAggregate Aggregate(¬∑) weight-sums the feature Message(s,e,n ) within the vicinity using\nAttention(s,e,n ), and the result is then linearly projected into a d-dimensional representation:\nAggregate(¬∑) =Wo\nœÑ(n)\n(\n‚äï\n‚àÄs‚ààN(n)\n(Attention(s,e,n ) ¬∑Feature(s,e,n ))\n)\n(4)\nwhere ‚äïis element-wise addition and Wo\nœÑ(n) ‚ààRd√ód is model parameter. The representation of\ntoken wt‚àí1 from the last layer is used to compute the language model probability pLM(wt|ct).\n2.4 kNN BASED PROBABILITY FOR NEXT TOKEN\nWe further incorporate the proposed model with kNN (Khandelwal et al., 2019; 2020; Meng et al.,\n2021), a related but orthogonal technique, to improve the performance of our model. It extends a\nvanilla LM by linearly interpolating it with a k-nearest neighbors (kNN) model. Concretely, for each\ninput context ct = (w1,w2,...,w t‚àí1), we retrieve the knearest neighbors N(ct) ={c(1)\nt1 ,..., c(k)\ntk },\nand compute the kNN based probability for the next token by:\np(wt|ct) =ŒªpkNN(wt|ct) + (1‚àíŒª)pLM(wt|ct),\npkNN(wt|ct) = 1\nZ\nk‚àë\ni=1\n1wt=w(i)\nti\nexp\n(\ncos(f(ct),f(c(i)\nti ))/T\n)\n, (5)\nwith Zbeing the normalization factor, f(¬∑) is the neural language model encoding contexts to high\ndimensional representations, cos(¬∑,¬∑) is cosine similarity, and Œªand T are hyperparameters.3\n3The original version of kNN-LM (Khandelwal et al., 2019) uses negative L2 distance as vector similarity,\nand does not have hyperparameter T. We followed Khandelwal et al. (2020) to add hyperparameter T and\nfollowed Meng et al. (2021) to use cosine similarity.\n4\nPublished as a conference paper at ICLR 2022\n3 E XPERIMENTS\nWe conduct experiments on three widely-used language modeling datasets: WikiText-103 (Merity\net al., 2016), One Billion Word (Chelba et al., 2013) and Enwik8 (Mahoney, 2011). For all experi-\nments, we add a 3-layer self-attention augmented GNN on top of the pretrained base LM, and use\nthe same hidden dimension and number of heads as our base LM. We retrieve k = 1,024 nearest\nneighbors for each source token, among them the top 128 neighbors are used in graph, and all of them\nare used in computing the kNN-based probability pkNN(wt|ct). For the neighbor context window\nsize land rin Section 2.2, we set l= 1and r= 1.\n3.1 T RAINING DETAILS\nKNN Retrieval In order to reduce memory usage and time complexity, in practice we use FAISS\n(Johnson et al., 2019) for fast approximate kNN search. Concretely, we quantized each dense vector\nto qbytes, followed with a clustering of all vectors to Cclusters. During retrieval, we only search\nin 32 clusters whose centroids are nearest to query vector. For WikiText-103 and Enwik8 datasets,\nwhich contain approximately 100M tokens, we set q= 128and c= 4,096. For One Billion Word\ndataset, we set q= 64and c= 1,048,576 (220) for faster search.\nData Leakage Prevention When searching for the knearest neighbors of ct = (w1,w2,...,w t‚àí1),\nwe need to make sure each reference neighbor token does not leak information for wt. SpeciÔ¨Åcally,\nwe should not retrieve ct+1 = (w1,w2,...,w t) as reference, otherwise the model prediction is trivial\nto optimize since the information of target token is already included in the graph. Let T be the\nmaximum sequence length andLbe the number of layers. Practically, the representation of each token\nis dependent on previous T and T √óLtokens for Transformer and Transformer-XL, respectively.\nTherefore we ignore all the neighboring nodes within this interval in graph construction during\ntraining. During inference, we do not impose this constraint.\nFeature Quantization The input node representations of the graph neural network H[0] are gener-\nated by a pretrained neural language model. To accelerate training and inference, we wish to cache\nall token representations of the entire training set. However, frequently accessing Terabytes of data is\nprohibitively slow. To address this issue, we followed Meng et al. (2021) to use product quantization\n(PQ) (Jegou et al., 2010; Ge et al., 2013) to compress the high-dimensional representation of each\ntoken. In our experiments, quantizing representations from 1,024-dimension Ô¨Çoating-point dense\nvectors to 128 bytes reduces the memory consumption from 2.3TB to 96GB for the One Billion Word\ndataset, thus making the end-to-end model training feasible.\n3.2 M AIN RESULTS\nWikiText-103 WikiText-103 is the largest available word-level language modeling benchmark with\nlong-term dependency. It contains 103M training tokens from 28K articles, and has a vocabulary of\naround 260K. We use the base version of deep Transformer language model with adaptive embeddings\n(Baevski & Auli, 2018) as our base LM. This model has 16 decoder layers. The dimensionality of\nword representations is 1,024, the number of multi-attention heads is 16, and the inner dimensionality\nof feedforward layers is 4,096. During training, data is partitioned into blocks of 3,072 contiguous\ntokens. During evaluation, blocks are complete sentences totaling up to 3,072 tokens of which the Ô¨Årst\n2,560 tokens serve as context to predict the last 512 tokens. As shown in Table 1, GNN-LM reduces\nthe base LM perplexity from 18.7 to 16.8, which demonstrates the effectiveness of the GNN-LM\narchitecture. The combination of GNN and kNN further boosts the performance to 14.8, a new\nstate-of-the-art result on WikiText-103.\nOne Billion Word One Billion Word is a large-scale word-level language modeling dataset of\nshort-term dependency. It does not preserve the order of sentences, contains around 768M training\ntokens and has a vocabulary of around 800k. We adopt the very large version of Transformer model\nin Baevski & Auli (2018) as our base LM. Results in Table 2 show that GNN-kNN-LM helps base\nLM reduce 0.5 perplexity with only 27M additional parameters. For comparison, Baevski & Auli\n(2018) use 560M additional parameters to reduce perplexity from 23.9 to 23.0.\n5\nPublished as a conference paper at ICLR 2022\nModel # Param Test ppl ( ‚Üì)\nHebbian + Cache (Rae et al., 2018) 151M 29.9\nTransformer-XL (Dai et al., 2019) 257M 18.3\nTransformer-XL + Dynamic Eval (Krause et al., 2019) 257M 16.4\nCompressive Transformer (Rae et al., 2019) - 17.1\nKNN-LM + Cache (Khandelwal et al., 2019) 257M 15.8\nSandwich Transformer (Press et al., 2020a) 247M 18.0\nShortformer (Press et al., 2020b) 247M 18.2\nSegaTransformer-XL (Bai et al., 2021) 257M 17.1\nRouting Transformer (Roy et al., 2021) - 15.8\nbase LM (Baevski & Auli, 2018) 247M 18.7\n+GNN 274M 16.8\n+GNN+kNN 274M 14.8\nTable 1: Test perplexity on WikiText-103 dataset.\nModel # Param Test ppl ( ‚Üì)\nLSTM+CNN (Jozefowicz et al., 2016) 1.04B 30.0\nHigh-Budget MoE (Shazeer et al., 2016) 5B 28.0\nDynamicConv (Wu et al., 2018) 0.34B 26.7\nMesh-TensorÔ¨Çow (Shazeer et al., 2018) 4.9B 24.0\nEvolved Transformer (Shazeer et al., 2018) - 28.6\nTransformer-XL (Dai et al., 2019) 0.8B 21.8\nAdaptive inputs (base) (Baevski & Auli, 2018) 0.36B 25.2\nAdaptive inputs (large) (Baevski & Auli, 2018) 0.46B 23.9\nbase LM (Baevski & Auli, 2018) 1.03B 23.0\n+kNN 1.02B 22.8\n+GNN 1.05B 22.7\n+GNN+kNN 1.05B 22.5\nTable 2: Test perplexity on One Billion Word dataset.\nEnwik8 Enwik8 is a character-level language modeling benchmark that consists of 100M characters\nfrom English Wikipedia articles, and has a vocabulary of 208. For base LM, we use Transformer-\nXL (Dai et al., 2019) with 12 layers, 8 heads, 512 dimensional embedding and 2,048 dimensional\ninner feed forward layer. Table 3 shows that GNN-kNN-LM outperforms base LM by 0.03 Bit per\nCharacter (BPC), achieving 1.03 BPC with only 48M parameters, comparable to 18L Transformer-XL\nwith 88M parameters.\nModel # Param BPC ( ‚Üì)\n64L Transformer (Al-Rfou et al., 2019) 235M 1.06\n18L Transformer-XL (Dai et al., 2019) 88M 1.03\n24L Transformer-XL (Dai et al., 2019) 277M 0.99\n24L Transformer-XL + Dynamic Eval (Krause et al., 2019) 277M 0.94\nLongformer (Beltagy et al., 2020) 102M 0.99\nAdaptive Transformer (Sukhbaatar et al., 2019) 209M 0.98\nCompressive Transformer (Rae et al., 2019) 277M 0.97\nSandwich Transformer (Press et al., 2020a) 209M 0.97\n12L Transformer-XL (Dai et al., 2019) 41M 1.06\n+kNN 41M 1.04\n+GNN 48M 1.04\n+GNN+kNN 48M 1.03\nTable 3: Bit per Character on the Enwik8 dataset.\n6\nPublished as a conference paper at ICLR 2022\n4 A NALYSIS\n4.1 C OMPLEXITY ANALYSIS\nSpace Complexity In our model, we consider knearest neighbors for each token ci in context;\nthe number of nodes in the graph is ktimes larger than vanilla LM during training. Accordingly,\ntraining GNN requires approximately k times larger memory than vanilla LM, since we have to\nmaintain hidden representations of each node for backward propagation. We propose two strategies\nto alleviate the space issue: (1) For all datasets, we Ô¨Årst train with a smaller k = 32, then further\nÔ¨Ånetune the model with a larger k = 128; and (2) For datasets with extremely long dependency\n(e.g., WikiText-103), we truncate the context to a smaller length (e.g., 128) instead of the original\nlonger context (e.g., 3,072) used by vanilla Transformer (Baevski & Auli, 2018). Note that we build\nGNN model on top of the vanilla Transformer, and the parameters of Transformer are Ô¨Åxed when\nGNN parameters are being trained. Hence, the GNN could exploit long dependency information\nlearned by Transformer without having to build a large graph with long context. Figure 2(a) shows\nthe comparison of base LM and GNN-LM on GPU memory usage with variant kin WikiText-103.4\n8 16 32 64 128\nk\n100\n200\n300\n400GPU memory (GB)\n(a)\nGNN-LM\nBase LM\n8 16 32 64 128\nk\n2500\n5000\n7500\n10000\n12500\n15000\n17500word per second\n(b)\nGNN-LM\nBase LM\n8 16 32 64 128\nk\n15\n16\n17\n18perplexity\n(c)\nGNN-LM\nBase-LM\nFigure 2: Comparisons between base LM and GNN-LM on WikiText-103 with respect to different k.\n(a) GPU memory usage. (b) Speed (word per second). (c) Test perplexity.\nTime Complexity Both GNN and Transformer consist of two basic modules: the feed forward\nlayer and the attention layer. Let |V|be the number of nodes and |E|be the number of edges in the\ngraph. Then the time complexity of the feed forward layer is O(|V|) and the time complexity of the\nattention layer is O(|E|). The GNN model increases |V|by (l+ r+ 1)ktimes in the graph, and adds\n(l+ r+ 1)k|V|edges to the graph. Note that |E|= |V|2 in Transformer, and thus the increased time\ncomplexity is acceptable if k‚â™|V|holds. Figure 2(b) shows the comparison between base LM and\nGNN-LM in speed in WikiText-103. We observe that the speed of GNN-LM is approximately 8 to\n20 times slower than the base LM (Baevski & Auli, 2018) with respect to different k.\nIt is worth noting that the overhead of the proposed model comes from kNN retrieval, which can be\ndone in advance and thus does not result in time overhead when running the model. SpeciÔ¨Åcally,\nthe time overhead for retrieval comes from two processes: 1) building data indexes with token\nrepresentations in the train set; 2) collecting nearest neighbors by querying the data indexes. For\nWikiText-103, building data indexes takes approximately 24 hours on a CPU machine with 64 cores.\nAnd querying data indexes for all tokens in train set takes approximately 30 hours.\n4.2 A BLATION STUDY\nNumber of Neighbors per TokenThe number of neighbors per source token (i.e., k) signiÔ¨Åcantly\ninÔ¨Çuences how much information could be retrieved from the training set. Figure 2(c) shows that test\nperplexity monotonically decreases when kincreases from 8 to 128. This trend implies that even\nlarger improvements can be achieved with a larger value ofk.\nNeighbor Quality We evaluate the quality of kNN retrieval by examining whether the target token\nto predict (i.e., wt) is the same as the token that comes right after the retrieved nearest sequence using\n4We note base LM uses a context length of 3,072, while the context length of GNN-LM is 128. We scale up\nthe value of GNN-LM 24 times for fair comparison.\n7\nPublished as a conference paper at ICLR 2022\nkNN recall range [0, 4) [4, 27) [27, 137) [137, 463) [463, 1024]\nbase LM -7.14 -3.84 -2.21 -1.19 -0.30\n+GNN+kNN -7.15 -3.46 -1.71 -0.80 -0.21\nabsolute improvement -0.01 0.38 0.50 0.39 0.09\nrelative improvement -0.0% 10% 23% 33% 32%\nTable 4: Comparison of base LM and GNN-LM in different kNN recall buckets. We report average\nlog probabilities within each bucket, and compute the absolute and relative improvement.\nthe recall metric. Given a sample ct = (w1,w2,...,w t‚àí1) and its kNN N(ct) ={c(1)\nt1 ,..., c(k)\ntk }, the\nquality of kNN retrieval is deÔ¨Åned by\nR(ct) =\nk‚àë\ni=1\n1\n[\nwt = w(i)\nti\n]\n, (6)\nwhere wt is the target token to predict at time step t, and w(i)\nti is the token that comes right after the\ni-th retrieved neighbor. We calculate and then divide all samples in the WikiText-103 test set by the\nrecall value into 5 buckets, with each bucket containing around 50k tokens. Results are reported in\nTable 4. We observe that GNN-kNN-LM gains more relative improvements to base LM when the\nquality of kNN retrieval reaches a relatively high level.\nRepresentation inkNN We Ô¨Ånally study the effect of using different representations in the kNN\nscoring function in Section 2.4. We experiment with two types of representations: (1) from the last\nlayer of Transformer, which is the default setting, and (2) from the last layer of GNN. The model\nperformances with different choices for query and key are reported in Table 5. Results show that\nusing GNN representations for both query and key leads to the best performance. It suggests that\nGNN learns better representations for context similarity. We also observe that the performance is\nmarginally worse when both query and key are using Transformer representations. Considering that\nbuilding an additional datastore for GNN representations is computationally intensive, in practice we\ncan directly use Transformer representations (the default setting).\nQuery Repres. Key Repres. Test ppl ( ‚Üì)\nTransformer Transformer 14.82\nTransformer GNN 15.16\nGNN Transformer 14.97\nGNN GNN 14.76\nTable 5: Test perplexity on WikiText-103 with different representations as query and key.\n4.3 E XAMPLES\nTable 6 presents two examples showing the input and the corresponding extracted three neighbor\ncontexts. The two examples demonstrate that the extracted contexts have a strong connection in\nsemantics to the input, and thus leveraging the neighboring information will beneÔ¨Åt model predictions.\n5 R ELATED WORK\nLanguage Modeling Traditional methods for language modeling use n-gram statistics to compute\nthe probability of the next token given the(n‚àí1)-gram context (Bahl et al., 1983; Nadas, 1984; Chen\n& Goodman, 1999). With the development of neural language models (NLMs) (Mikolov et al., 2012),\ndeep learning based methods begin to dominate the learning paradigm of language modeling. For\nexample, Jozefowicz et al. (2016) built a strong language model by combining the LSTM (Schuster\n& Paliwal, 1997) model and the CNN structure; Melis et al. (2017); Merity et al. (2017) applied a\nvariety of regularizations to LSTMs and achieved state-of-the-art results; Baevski & Auli (2018)\nproposed adaptive input embeddings, which can improve performance while drastically reducing the\n8\nPublished as a conference paper at ICLR 2022\nInput: In 2000 Boulter had a guest @-@ starring\nExtracted 1: In 2009 , Beghe had a guest @-@ starring role on the television show Californication .\nExtracted 2: had previously worked on Hack , for a guest @-@ starring episode arc on the show .\nExtracted 3: and because of Patrick Stewart ‚Äôs hilarious guest @-@ starringrole as \" Number One . \"\nInput: Tourism is a vital industry in Manila , and\nExtracted 1: a large audience in Mogadishu , and was widely sold prior to the civil war .\nExtracted 2: industry is well established , with Mumbai Port being one of the oldest and most\nExtracted 3: transportation has become a large business in Newark , accounting for more than 17\nTable 6: Two examples showing the input context and the corresponding extracted three neighbors.\nThe bold token is the gold token to predict, and the underlined are the extracted context tokens.\nnumber of model parameters. On top of Transformer (Vaswani et al., 2017), considerable efforts have\nbeen devoted to building stronger and more efÔ¨Åcient language models (Shazeer et al., 2018; Dai et al.,\n2019; Beltagy et al., 2020; Press et al., 2020b;a). BERT (Devlin et al., 2018) proposed the Masked\nLanguage Modeling (MLM) pretraining paradigm to train a deep bidirectional Transformer model;\nRoBERTa (Liu et al., 2019) removed the Next Sentence Prediction (NSP) task in BERT; XLNet\n(Yang et al., 2019) generalized BERT pretraining to the autoregressive manner; Span-level BERTs\n(Lewis et al., 2019; Song et al., 2019; Joshi et al., 2020) introduced span-level masks rather than just\nrelying on token-level masks. ELECTRA (Clark et al., 2020) proposed to detect token replacement\nas opposed to token generation, improving both the efÔ¨Åciency and effectiveness of pretraining. Sun\net al. (2021) extends BERT to accommodate glyph information.\nGraph Neural Networks Graph neural networks (GNNs) capture the dependencies and relations\nbetween nodes connected with edges, which propagate features across nodes layer by layer (Scarselli\net al., 2008; Kipf & Welling, 2016; Hamilton et al., 2017). GNNs have demonstrated effectiveness in\na wide variety of tasks in natural language processing such as text classiÔ¨Åcation (Yao et al., 2019;\nLin et al., 2021), machine translation (Bastings et al., 2017), question answering (Song et al., 2018;\nDe Cao et al., 2018), recommendation (Wu et al., 2019) and information extraction (Li et al., 2020a).\nFor example, Guo et al. (2019) proposed Star Transformer, a Transformer backbone but replaces\nthe fully-connected structure in self-attention with a star-like topology. Ye et al. (2019) adopted a\nÔ¨Åne-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP). Li et al. (2020b)\nproposed to learn word connections speciÔ¨Åc to the input via reinforcement learning.\nRetrieval-augmented Models Retrieving contexts from another corpus as additional information\nimproves the model‚Äôs robustness towards infrequent data points. A typical application of retrieval-\naugmented models is open-domain question answering, which solicits related passages from a\nlarge open-domain database to answer a given question. The dominant approach is to cache dense\nrepresentations of the passages and retrieve the closest ones to the input during inference (Lewis\net al., 2020b; Karpukhin et al., 2020; Xiong et al., 2020; Lee et al., 2020; Li et al., 2020b). Lewis\net al. (2020a) proposed to Ô¨Årst extract a set of related texts and condition on them to generate the\ntarget text. allowing for strong zero-shot performance. Besides open-domain QA, other tasks such as\nlanguage modeling (Khandelwal et al., 2019; Guu et al., 2020), machine translation (Zhang et al.,\n2018; Tu et al., 2018; Jitao et al., 2020), text classiÔ¨Åcation (Lin et al., 2021), and task-oriented\ndialog generation (Fan et al., 2020; Thulke et al., 2021) also beneÔ¨Åt from the additionally retrieved\ninformation. For example, Khandelwal et al. (2019) retrieved knearest neighbors from a large-scale\nunannotated corpus and interpolates with the decoded sentence for language modeling. Khandelwal\net al. (2020); Meng et al. (2021) retrieved kNNs from the parallel translation corpus to augment the\nmachine translation outputs. However, these methods retrieve related texts independently.\n6 C ONCLUSION AND FUTURE WORK\nIn this work, we propose GNN-LM, a new paradigm for language modeling that extends vanilla\nneural language model by allowing to reference similar contexts in the entire training corpus. High\ndimensional token representations are used to retrieve knearest neighbors of the input context as\nreference. We build a directed heterogeneous graph for each input context, where nodes are tokens\nfrom either the input context or the retrieved neighbor contexts, and edges represent connections\n9\nPublished as a conference paper at ICLR 2022\nbetween tokens. Graph neural networks are then leveraged to aggregate information from the retrieved\ncontexts to decode the next token. Experimental results show that our proposed method outperforms\nstrong baselines in standard benchmark datasets, and by combining with kNN LM, we are able\nto achieve state-of-the-art results on WikiText-103. In future work, we will consider improving\nefÔ¨Åciency for building the graph and retrieving nearest neighbors.\nREFERENCES\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, volume 33, pp. 3159‚Äì3166, 2019.\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nInternational Conference on Learning Representations, 2018.\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous\nspeech recognition. IEEE transactions on pattern analysis and machine intelligence, (2):179‚Äì190,\n1983.\nHe Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, and Ming Li. Segatron:\nSegment-aware transformer for language modeling and understanding. 2021.\nJasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima‚Äôan. Graph con-\nvolutional encoders for syntax-aware neural machine translation. In Proceedings of the 2017\nConference on Empirical Methods in Natural Language Processing, pp. 1957‚Äì1967, Copenhagen,\nDenmark, September 2017. Association for Computational Linguistics.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005, 2013.\nStanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language\nmodeling. Computer Speech & Language, 13(4):359‚Äì394, 1999.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. Electra: Pre-training\ntext encoders as discriminators rather than generators. In International Conference on Learning\nRepresentations, 2020.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a Ô¨Åxed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978‚Äì2988, 2019.\nNicola De Cao, Wilker Aziz, and Ivan Titov. Question answering by reasoning across documents\nwith graph convolutional networks. arXiv preprint arXiv:1808.09920, 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with\nknn-based composite memory for dialogue. arXiv preprint arXiv:2004.12744, 2020.\nTiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization.IEEE transactions\non pattern analysis and machine intelligence, 36(4):744‚Äì755, 2013.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. Star-\ntransformer. arXiv preprint arXiv:1902.09113, 2019.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.\n10\nPublished as a conference paper at ICLR 2022\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\nAdvances in neural information processing systems, pp. 1024‚Äì1034, 2017.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In\nProceedings of The Web Conference 2020, pp. 2704‚Äì2710, 2020.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search.\nIEEE transactions on pattern analysis and machine intelligence, 33(1):117‚Äì128, 2010.\nXU Jitao, Josep M Crego, and Jean Senellart. Boosting neural machine translation with similar\ntranslations. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 1580‚Äì1590, 2020.\nJeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 2019.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Spanbert:\nImproving pre-training by representing and predicting spans. Transactions of the Association for\nComputational Linguistics, 8:64‚Äì77, 2020.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\nVladimir Karpukhin, Barlas OÀòguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv\npreprint arXiv:2004.04906, 2020.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172,\n2019.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest neighbor\nmachine translation. arXiv preprint arXiv:2010.00710, 2020.\nThomas N Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional networks.\narXiv preprint arXiv:1609.02907, 2016.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of trans-\nformer language models. arXiv preprint arXiv:1904.08378, 2019.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of\nphrases at scale. arXiv preprint arXiv:2012.12624, 2020.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461,\n2019.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke\nZettlemoyer. Pre-training via paraphrasing. arXiv preprint arXiv:2006.15020, 2020a.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented genera-\ntion for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020b.\nBo Li, Wei Ye, Zhonghao Sheng, Rui Xie, Xiangyu Xi, and Shikun Zhang. Graph enhanced dual\nattention network for document-level relation extraction. In Proceedings of the 28th International\nConference on Computational Linguistics, pp. 1551‚Äì1560, 2020a.\nXiaoya Li, Yuxian Meng, Mingxin Zhou, Qinghong Han, Fei Wu, and Jiwei Li. Sac: Accelerating\nand structuring self-attention via sparse adaptive connection. arXiv preprint arXiv:2003.09833,\n2020b.\n11\nPublished as a conference paper at ICLR 2022\nYuxiao Lin, Yuxian Meng, Xiaofei Sun, Qinghong Han, Kun Kuang, Jiwei Li, and Fei Wu. Bertgcn:\nTransductive text classiÔ¨Åcation by combining gcn and bert. arXiv preprint arXiv:2105.05727,\n2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nMatt Mahoney. Large text compression benchmark, 2011.\nG√°bor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589, 2017.\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei Zhang, and Jiwei Li. Fast\nnearest neighbor machine translation. arXiv preprint arXiv:2105.14528, 2021.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm\nlanguage models. arXiv preprint arXiv:1708.02182, 2017.\nTom√°≈° Mikolov et al. Statistical language models based on neural networks. Presentation at Google,\nMountain View, 2nd April, 80:26, 2012.\nArthur Nadas. Estimation of probabilities in the language model of the ibm speech recognition\nsystem. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(4):859‚Äì861, 1984.\nOÔ¨År Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their\nsublayers. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 2996‚Äì3005, 2020a.\nOÔ¨År Press, Noah A Smith, and Mike Lewis. Shortformer: Better language modeling using shorter\ninputs. arXiv preprint arXiv:2012.15832, 2020b.\nJack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap. Fast parametric learning with activation\nmemorization. In International Conference on Machine Learning, pp. 4228‚Äì4237. PMLR, 2018.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.\nCompressive transformers for long-range sequence modelling. In International Conference on\nLearning Representations, 2019.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. EfÔ¨Åcient content-based sparse\nattention with routing transformers. Transactions of the Association for Computational Linguistics,\n9:53‚Äì68, 2021.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The\ngraph neural network model. IEEE Transactions on Neural Networks, 20(1):61‚Äì80, 2008.\nMike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE transactions on\nSignal Processing, 45(11):2673‚Äì2681, 1997.\nClaude Elwood Shannon. A mathematical theory of communication. ACM SIGMOBILE mobile\ncomputing and communications review, 5(1):3‚Äì55, 2001.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and\nJeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 2016.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorÔ¨Çow: Deep\nlearning for supercomputers. In NeurIPS, 2018.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence\npre-training for language generation. In International Conference on Machine Learning , pp.\n5926‚Äì5936, 2019.\n12\nPublished as a conference paper at ICLR 2022\nLinfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, and Daniel Gildea. Exploring\ngraph-structured passage representation for multi-hop reading comprehension with graph neural\nnetworks. arXiv preprint arXiv:1809.02040, 2018.\nSainbayar Sukhbaatar, √âdouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span\nin transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pp. 331‚Äì335, 2019.\nZijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xiang Ao, Qing He, Fei Wu, and Jiwei Li.\nChinesebert: Chinese pretraining enhanced by glyph and pinyin information. arXiv preprint\narXiv:2106.16038, 2021.\nDavid Thulke, Nico Daheim, Christian Dugast, and Hermann Ney. EfÔ¨Åcient retrieval augmented\ngeneration from unstructured knowledge for task-oriented dialog.arXiv preprint arXiv:2102.04643,\n2021.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang. Learning to remember translation history\nwith a continuous cache. Transactions of the Association for Computational Linguistics, 6:407‚Äì420,\n2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998‚Äì6008, 2017.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. In International Conference on Learning Representations,\n2018.\nShu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based\nrecommendation with graph neural networks. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, volume 33, pp. 346‚Äì353, 2019.\nZiang Xie, Sida I Wang, Jiwei Li, Daniel L√©vy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. Data\nnoising as smoothing in neural network language models. arXiv preprint arXiv:1703.02573, 2017.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and\nArnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text\nretrieval. arXiv preprint arXiv:2007.00808, 2020.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\nXlnet: Generalized autoregressive pretraining for language understanding. Advances in neural\ninformation processing systems, 32, 2019.\nLiang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classiÔ¨Åcation. In\nProceedings of the AAAI conference on artiÔ¨Åcial intelligence, volume 33, pp. 7370‚Äì7377, 2019.\nZihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-transformer: Modelling\nlong-range context via binary partitioning. arXiv preprint arXiv:1911.04070, 2019.\nJingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig, and Satoshi Nakamura. Guiding\nneural machine translation with retrieved translation pieces. arXiv preprint arXiv:1804.02559,\n2018.\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7933341860771179
    },
    {
      "name": "Security token",
      "score": 0.7056588530540466
    },
    {
      "name": "Perplexity",
      "score": 0.6915795207023621
    },
    {
      "name": "Graph",
      "score": 0.5348739624023438
    },
    {
      "name": "Language model",
      "score": 0.5105530023574829
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5046368837356567
    },
    {
      "name": "Context (archaeology)",
      "score": 0.45349133014678955
    },
    {
      "name": "Natural language processing",
      "score": 0.4496394693851471
    },
    {
      "name": "Word (group theory)",
      "score": 0.4400857090950012
    },
    {
      "name": "Generalization",
      "score": 0.438178688287735
    },
    {
      "name": "Point (geometry)",
      "score": 0.4327540993690491
    },
    {
      "name": "Theoretical computer science",
      "score": 0.22749477624893188
    },
    {
      "name": "Mathematics",
      "score": 0.08116123080253601
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 18
}