{
  "title": "RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause Extraction",
  "url": "https://openalex.org/W2966024403",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2098083858",
      "name": "Rui Xia",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095770634",
      "name": "Mengran Zhang",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2786535501",
      "name": "Zixiang Ding",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2892381115",
    "https://openalex.org/W2538796508",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W25301398",
    "https://openalex.org/W101809282",
    "https://openalex.org/W1992605069",
    "https://openalex.org/W2950868217",
    "https://openalex.org/W2161624371",
    "https://openalex.org/W2020111801",
    "https://openalex.org/W4231458041",
    "https://openalex.org/W2182096631",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2771971003",
    "https://openalex.org/W2963711448",
    "https://openalex.org/W2251120499",
    "https://openalex.org/W1564070220",
    "https://openalex.org/W799573885",
    "https://openalex.org/W2907213813",
    "https://openalex.org/W2773167282",
    "https://openalex.org/W2766095568"
  ],
  "abstract": "The emotion cause extraction (ECE) task aims at discovering the potential causes behind a certain emotion expression in a document. Techniques including rule-based methods, traditional machine learning methods and deep neural networks have been proposed to solve this task. However, most of the previous work considered ECE as a set of independent clause classification problems and ignored the relations between multiple clauses in a document. In this work, we propose a joint emotion cause extraction framework, named RNN-Transformer Hierarchical Network (RTHN), to encode and classify multiple clauses synchronously. RTHN is composed of a lower word-level encoder based on RNNs to encode multiple words in each clause, and an upper clause-level encoder based on Transformer to learn the correlation between multiple clauses in a document. We furthermore propose ways to encode the relative position and global predication information into Transformer that can capture the causality between clauses and make RTHN more efficient. We finally achieve the best performance among 12 compared systems and improve the F1 score of the state-of-the-art from 72.69% to 76.77%.",
  "full_text": "RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause Extraction\nRui Xia , Mengran Zhang and Zixiang Ding\nSchool of Computer Science and Engineering, Nanjing University of Science and Technology, China\nfrxia, zhangmengran, dingzixiangg@njust.edu.cn\nAbstract\nThe emotion cause extraction (ECE) task aims at\ndiscovering the potential causes behind a certain\nemotion expression in a document. Techniques\nincluding rule-based methods, traditional machine\nlearning methods and deep neural networks have\nbeen proposed to solve this task. However, most\nof the previous work considered ECE as a set of\nindependent clause classiﬁcation problems and ig-\nnored the relations between multiple clauses in\na document. In this work, we propose a joint\nemotion cause extraction framework, named RNN-\nTransformer Hierarchical Network (RTHN), to en-\ncode and classify multiple clauses synchronously.\nRTHN is composed of a lower word-level encoder\nbased on RNNs to encode multiple words in each\nclause, and an upper clause-level encoder based\non Transformer to learn the correlation between\nmultiple clauses in a document. We furthermore\npropose ways to encode the relative position and\nglobal predication information into Transformer\nthat can capture the causality between clauses and\nmake RTHN more efﬁcient. We ﬁnally achieve the\nbest performance among 12 compared systems and\nimprove the F1 score of the state-of-the-art from\n72.69% to 76.77%.\n1 Introduction\nEmotion cause extraction (ECE) is a ﬁne-grained task of emo-\ntion analysis, which aims at discovering the potential causes\nbehind a certain emotion expression in the text. The ECE\ntask was ﬁrst proposed and deﬁned as a word-level sequence\nlabeling problem in [Lee et al., 2010]. To solve the shortcom-\nings of describing emotion cause at word/phrase level,[Gui et\nal., 2016a] released a new corpus and re-formalized the ECE\ntask as a clause-level classiﬁcation problem. This corpus has\nreceived much attention in the following study and has be-\ncome a benchmark dataset for ECE research. Figure 1 gives\nan example to the annotation of their corpus. In this example,\na document is composed of ﬁve clauses. The emotion expres-\nsion “happy” is contained in Clausec4 and the corresponding\ncause “the thief was caught” is contained in Clause c3 (We\ncall Clause c4 and Clause c3 emotion expression clause and\nFigure 1: An example of the emotion cause extraction task.\nemotion cause clause, respectively). The goal of ECE is to\npredict for each clause in a document, whether this clause\ncontains an emotion cause, given the annotation of the emo-\ntion expression.\nRule-based methods and traditional machine learning\nmethods have been proposed to address this problem. In re-\ncent years, deep neural networks have also been applied to\nthis task and achieved state-of-the-art performance [Cheng et\nal., 2017; Gui et al., 2017; Li et al., 2018; Ding et al., 2019;\nYu et al., 2019].\nHowever, most of the previous work considered ECE as\na set of independent clause classiﬁcation problems. For ex-\nample, under this framework, Example 1 will be formal-\nized as ﬁve independent classiﬁcation tasks (Clause c1 to\nClause c5). Although this framework was straight-forward,\nit ignores the relations between multiple clauses for emo-\ntion cause inference. There are two types of relationships\nbetween clauses: 1) Correlation: two clauses with similar se-\nmantics are supposed to have similar probabilities being the\nemotion cause. 2) Causality: incorporating the information\nof the other clauses in the document can help infer the cur-\nrent clause in a global view. It was observed from the corpus\n[Gui et al., 2016a] that more than 99% of the documents have\nonly one or two causes. If one clause has a high probability\nbeing an emotion cause, the probabilities that other clauses\nbeing emotion causes should be reduced; conversely, if no\nhigh-conﬁdence emotion cause clauses have been observed,\nthe probability of predicting the current clause being an emo-\ntion cause should be increased.\nIn this work, we propose a hierarchical network ar-\nchitecture based on RNN and Transformer, named RNN-\nTransformer Hierarchical Network (RTHN), to model the re-\nlations between multiple clauses in a document and classify\nthem synchronously in a joint framework. RTHN is com-\nposed of two layers: 1) The lower layer is a word-level en-\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5285\ncoder consisting of multiple RNNs, each of which corre-\nsponds to one clause, in turn encodes the words in the clause\nand combines them to obtain the clause representation; 2)\nThe upper layer is a clause-level encoder based on a stacked\nTransformer, where the clause representations are repeatedly\nlearned and updated by incorporating the relations between\nmultiple clauses, and ﬁnally feed to a softmax layer for syn-\nchronous classiﬁcation.\nWe further propose ways to encode the relative position\nand global prediction information which have been proven to\nbe important features in ECE, and gain further improvements.\nOn one hand, the attention mechanism in Transformer learns\nthe correlation between clauses. On the other hand, the en-\ncoding of global prediction the causality between clauses.\nThe main contributions of this work can be summarized as\nfollows:\n1. We propose a new hierarchical network architecture\nbased on RNNs and Transformer for the ECE task. To\nthe best of our knowledge, it is the ﬁrst time that Trans-\nformer has been used to solve ECE problems. It demon-\nstrates excellent performance in learning the correlation\nbetween multiple clauses in ECE.\n2. We further encode the relative position and global pred-\nication information into the Transformer framework. It\ncan capture the causality between clauses and achieve\nextra improvements.\n3. The effectiveness of our model is demonstrated on the\nbenchmark ECE corpus. We ﬁnally achieve the best per-\nformance among 12 compared systems and improve the\nF1 score of the state-of-the-art from 72.69% to 76.77%1.\n2 Related Work\n[Lee et al., 2010 ] manually constructed a small-scale emo-\ntion cause corpus based on the Academia Sinica Balanced\nChinese Corpus and ﬁrst proposed the emotion cause extrac-\ntion (ECE) task. In this corpus, the spans of both emotion\nexpression and emotion cause were annotated and the ECE\ntask was deﬁned as a word-level sequence labeling prob-\nlem. Subsequent research proposed either rule-based meth-\nods or machine learning methods to solve this problem based\non manually designed rules or features [Chen et al., 2010;\nLee et al., 2013].\n[Li and Xu, 2014 ] constructed an emotion cause corpus\nbased on Chinese microblog posts, and proposed a rule-based\nmethod to infer and extract the emotion cause by importing\nknowledge and theories from other ﬁelds such as sociology.\n[Gao et al., 2015a] and [Gao et al., 2015b] further designed a\nset of complex rules considering a cognitive emotion model\nand emotions categories to extract emotion cause on this cor-\npus. [Gui et al., 2014 ] also constructed a microblog emo-\ntion cause corpus based on NLPCC 2013 emotion analysis\nshare task, and proposed a machine learning method based on\nSVMs and conditional random ﬁelds (CRFs) to extract emo-\ntion causes. Similar as [Lee et al., 2010 ], the ECE task in\n1The source code can be obtained at\nhttps://github.com/NUSTM/RTHN\nthese corpora was deﬁned and evaluated as a sequence label-\ning problem.\nThere were also some individual studies that conducted\nECE research on their own corpus [Russo et al., 2011;\nNeviarouskaya and Aono, 2013; Ghaziet al., 2015; Song and\nMeng, 2015; Yadaet al., 2017]. They normally regarded ECE\nas a sequence labeling problem and employed rule-based or\ntraditional machine learning algorithms to solve it.\n[Gui et al., 2016a] and [Gui et al., 2016b] released a Chi-\nnese emotion cause corpus from a public SINA city news and\nproposed a multi-kernel based method for emotion cause ex-\ntraction. Different from previous corpora, the ECE task in this\ncorpus was deﬁned as a clause classiﬁcation problem, where\nthe goal is to predict for each clause in a document, whether\nthis clause is an emotion cause, given the annotation of emo-\ntion expression. It was also evaluated by the clause-level Pre-\ncision, Recall and F1 score metrics. This corpus has received\nmuch attention in the following study and has become a\nbenchmark dataset for ECE research. Several traditional ma-\nchine learning methods including structure representation and\nmulti-kernel learning has been proposed in[Gui et al., 2016b;\nXu et al., 2017]. [Gui et al., 2016a] proposed a tree structure-\nbased representation method to describe the events in emo-\ntion cause extraction on this corpus. In recent two years,\ndeep learning techniques have also been applied to emotion\ncause extraction. For example, [Cheng et al., 2017] used long\nshort-term memory (LSTM), [Gui et al., 2017 ] proposed a\ndeep memory network, and [Li et al., 2018 ] proposed a co-\nattention neural network, for emotion cause prediction.\nMost of the above work considered the ECE task on this\ncorpus as a set of independent clause classiﬁcation tasks and\nignored the relations between multiple clauses in a document.\nTo address this, [Ding et al., 2019] converted the task to a re-\nordered clause classiﬁcation problem. The predictions of pre-\nvious clauses were used as features for predicting subsequent\nclauses. However, their approach depends on the clause or-\nder and can only use the predictions of the previous clauses\nbut not the subsequent clauses. In contrast, RTHN proposed\nin this work is a joint emotion cause extraction framework\nthat models and classiﬁes multiple clauses in a document syn-\nchronously.\n[Yu et al., 2019 ] proposed a three-level (word-phrase-\nclause) hierarchical network based on CNNs and LSTMs.\nThe multiple clauses are modeled with LSTMs in their ap-\nproach. By contrast, in this work Transformer is used as the\nclause-level encoder to model the relations between multi-\nple clauses. We will empirically prove that Transformer has\nshown signiﬁcantly better performance as a clause-level en-\ncoder, in comparison with RNNs.\n3 Approach\n3.1 Overall Architecture\nIn this paper, we represent a document containing multi-\nple clauses as d = fc1;:::;c i;:::;c jdjg, where ci is the i-\nth clause in d. Each clause ci consists of multiple words\nci = fwi;1;:::;w i;t;:::;w i;jcijg.\nIn this work, we propose a RNN-Transformer Hierarchical\nNetwork (RTHN) to model such a “word-clause-document”\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5286\nFigure 2: The framework of our RTHN model.\nhierarchical structure2. The overall architecture of RTHN is\nshown in Figure 2. It contains two layers: the lower layer is\na word-level encoder consisting of multiple Bi-LSTM mod-\nules, each of which corresponds to a clause (Section 3.2); the\nupper layer is a clause-level encoder consisting of a stacked\nTransformer module, where the clause representations ob-\ntained at lower layer are repeatedly updated by incorporating\nthe relations between clauses (Section 3.3), relative position\nand global prediction (Section 3.4), and ﬁnally feed to a soft-\nmax layer for synchronous classiﬁcation.\n3.2 Word Level Encoder Based on RNNs\nThe lower layer is a word-level encoder consisting of multiple\nBi-LSTMs. Each clause corresponds to a Bi-LSTM module,\nwhich accumulate the context information for each word of\nthe clause. The hidden state of the t-th word in the i-th clause\nhi;t is obtained based on a bi-directional LSTM. A word-level\nattention mechanism is then adopted to get the clause repre-\nsentation ri by a weighted sum of hidden states of all the\nwords in the clause. Here we omit the details of Bi-LSTM\nand attention for limited space.\nAs has mentioned in the Introduction, most of the previous\nstudies considered ECE as a set of independent clause classi-\nﬁcation problem. This framework normally has only one en-\ncoding layer (word-level encoder). In this work, the proposed\nRTHN model is a 2-layer hierarchical network containing not\nonly word-level encoders at the lower layer but also a clause-\nlevel encoder at the upper layer.\n2The document is usually very short in the benchmark corpus\n[Gui et al., 2016a]. Most of them contain less than 20 clauses. We\ntherefore ignore the sentence level between clause and document.\n3.3 Clause Level Encoder Based on Transformer\nIn this work, Transformer[Vaswaniet al., 2017] is used as the\nclause-level encoder at the upper layer to encode the relations\nbetween multiple clauses in a document.\nThe standard Transformer consists of a stack of N layers.\nEach layer has two sub-layers: the ﬁrst is a multi-head self-\nattention mechanism; the second is a fully connected feed-\nforward network.\nMulti-head Self-attention\nThe unit of multi-head self-attention mechanism is the major\ncomponent in the Transformer. For each clause ci, the repre-\nsentation ri obtained at the word-level encoder is used as the\ninput xi by adding a positional embedding pi:\nxi = ri + pi: (1)\nIn our setting, the Transformer contains a query vector qi,\na key vector ki and a value vector vi for each clause ci in the\ndocument:\nqi = ReLU(xiWQ); (2)\nki = ReLU(xiWK); (3)\nvi = ReLU(riWV ); (4)\nwhere WQ, WK and Wv are learnable weight matrix of query,\nkeys and values, respectively.\nFor each clause ci, self attention learns a set of weights\n\fi = f\fi;1;\fi;2;:::;\f i;jdjg, which measures the extent of all\nthe input clauses [c1;::;c k;:::;c jdj] answer the query qi:\n\fi;j = exp(qi \u0001kj)\nP\nj0exp(qi \u0001kj0): (5)\nThe output is a weighted sum of the values of all clauses:\nzi =\nX\nj\n\fi;jvj: (6)\nThis allows that the representation of each clause can encode\na global level information on all the clauses in the document,\nrather than rely solely on the hidden state of one clause.\nMoreover, the multi-head attention is employed with the\nnumber of heads as 5.\nFeed-Forward Network\nThe attention sublayer is then followed by a fully connected\nFeed-Forward Network (FFN) sublayer:\nei = ReLU(ziW1 + b1)W2 + b2: (7)\nNote that both of the above two sublayers use the residual\nconnection followed by normalization layer at its output:\noi = Normalize(ei + xi): (8)\nAs has mentioned, Transformer is a stack ofN layers each\nof which includes attention and FFN sublayers. Let ldenote\nthe index of Transformer layers. The output of the previous\nlayer will be used as the input of the next layer:\nx(l+1)\ni = o(l)\ni : (9)\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5287\n3.4 Encoding Relative Position and Global\nPrediction\nIn this section, we propose to further encode the relative posi-\ntion and global prediction which have been proven to be two\nexplicit clues in ECE.\nRelative Position Embedding\nIn standard Transformer, a positional encoding was already\napplied to represent the of position information. But what it\nreﬂects is the absolute positional information of the word in\nthe sentence.\nIn our task, relative position (RP) is deﬁned as the relative\ndistance between the current clause and the emotion expres-\nsion clause. For example, -1 is the RP of the clause left to\nthe emotion expression clause, +2 the relative position of the\nsecond clause right to the emotion expression clause, and so\non.\nRelative position is more important than absolute posi-\ntion in ECE, because people are more inclined to explain the\ncauses near the emotion expression. Therefore, the clauses\nwith smaller relative position (rather than absolute position)\nare more likely to be an emotion cause of the given emotion\nexpression.\nIn this work, we use relative position embedding\n(RPE) to encode such relative position information. The\nRPEs for all the clauses in a document are denoted by\nfrpe1;rpe2;:::;rpe jdjgwhere rpei denotes the RPE of clause\nci. Instead of Equation (1), rpei is concatenated to the clause\nrepresentation ri, as the input of the Transformer:\nxi = ri \brpei: (10)\nGlobal Prediction Embedding\nAs we have mentioned, there are two types of relationships\nbetween clauses: correlation and causality. In addition to us-\ning the attention mechanism in Transformer to capture the\ncorrelation between clauses, we furthermore propose to ap-\npend a new global prediction sublayer to the end of each\nTransformer layer to introduce more causality.\nGlobal Prediction (GP) denotes the prediction labels of all\nthe clauses in a document. As can be observed in the ECE\ncorpus [Gui et al., 2016a], more than 99% of the documents\nhave only one or two causes. If one clause in the document is\npredicted as an emotion cause with high conﬁdence, the prob-\nability that other clauses are predicted as emotional causes\nshould be reduced; conversely, if there are no other emotional\ncause clauses with high conﬁdence in the document, the prob-\nability that the current clause is predicted to be an emotion\ncause should be increased. Therefore, GP is an important\nclue for emotion cause extraction.\nFirstly, we get the prediction label of each clause li 2\nf+1;\u00001gbased on the output representations oi:\nli  softmax(Woi + b): (11)\nSecondly, we sort the predicted labels of different clauses\naccording to their relative positions [. . . , -2, -1, 0, +1, +2, . . . ]\nand build the following global prediction vector\nGP = [:::;li\u00002 ;li\u00001 ;li0 ;li+1 ;li+2 ;:::]: (12)\nwhere lirp denotes the prediction label of the clause at rela-\ntive position rp and the current position (i.e). Note that we\nmask the prediction at the current position to avoid potential\ninterference. For example, if the rp of current clause is +1,\nwe let li+1 = 0. GP represents different combinations of\nall clause predictions and is then encoded by an embedding\ncalled Global Prediction Embedding GPE:\nGPE = Tanh(WgpeGP + bgpe); (13)\nwhere Wgpe and bgpe are learnable matrix and bias.\nIn stacking, the average GPE of previous layers is con-\ncatenated to the output representation o(l)\ni and used as the in-\nput of the next layer’s input:\nx(l+1)\ni = o(l)\ni \bAve\nGPE(l); (14)\nwhere Ave GPE(l) = 1\nl\nP\nl GPE(l).\n3.5 Multiple Clause Classiﬁcation\nAfter a stack of N layers, we obtain the ﬁnal clause repre-\nsentation o(N)\ni for each clause, and employ an extra softmax\nfunction to yield the ﬁnal prediction distribution\n^yi = softmax(W(N)\nc o(N)\ni + b(N)\nc ): (15)\nThe training objective is to minimize the cross-entropy loss\nacross all the clauses:\nLoss= \u0000\nX\nd2Corpus\njdjX\ni=1\nyi \u0001log(^yi) +\u0015jj\u0012jj2; (16)\nwhere yi is the ground-truth distribution of clause ci. A L2-\nnorm regulation is also adopted with \u0015denoting the tradeoff\nweight.\n4 Experiments\n4.1 Dataset and Experimental Settings\nWe evaluate our RTHN model on the benchmark ECE cor-\npus [Gui et al., 2016a ], which was the mostly used corpus\nfor emotion cause extraction. The same as [Gui et al., 2017],\nwe randomly divide the data with the proportion of 9:1, with\n9 folds as training data and remaining 1 fold as testing data.\nThe following results are reported in terms of an average of\n10-fold cross-validation. The performance measures are Pre-\ncision (P), Recall (R), and F1 all deﬁned at clause level.\nWe use the word embedding provided by NLPCC. It was\npre-trained on a 1.1 million Chinese Weibo corpora with the\nword2vec toolkit [Mikolov et al., 2013]. Similar performance\ncan be obtained by using the embedding in [Gui et al., 2017].\nThe dimension of word embedding, RP embedding and GP\nembedding is set to be 200, 50 and 50, respectively. The hid-\nden units of LSTM in word-level encoder is set to be 100.\nThe dimension of the hidden states in Tranformer is 200, and\nthe dimensions of query, key and value are 250, 250, and 200\nrepectively.\nThe maximum numbers of words in each clause and\nclauses in each document are set to be 75 and 45, respec-\ntively. The network is trained based on the Adam optimizer\nwith a mini-batch size 32 and a learning rate 0.005.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5288\nP R F1\nRB [Lee et al., 2010] 0.6747 0.4287 0.5243\nCB [Russo et al., 2011] 0.2672 0.7130 0.3887\nRB+CB 0.5435 0.5307 0.5370\nRB+CB+SVM 0.5921 0.5307 0.5597\nNgrams+SVM 0.4200 0.4375 0.4285\nWord2vec+SVM 0.4301 0.4233 0.4136\nMulti-Kernel [Gui et al., 2016a] 0.6588 0.6927 0.6752\nCNN [Kim, 2014] 0.6215 0.5944 0.6076\nMemnet [Gui et al., 2017] 0.7076 0.6838 0.6955\nCANN [Li et al., 2018] 0.7721 0.6891 0.7266\nPAE-DGL [Ding et al., 2019] 0.7619 0.6908 0.7242\nHCS [Yu et al., 2019] 0.7388 0.7154 0.7269\nRTHN (layer 1) 0.7696 0.7333 0.7501\nRTHN (layer 2) 0.7644 0.7566 0.7601\nRTHN (layer 3) 0.7697 0.7662 0.7677\nRTHN (layer 4) 0.7604 0.7699 0.7646\nRTHN (layer 5) 0.7592 0.7684 0.7634\nTable 1: Performance of our RTHN model and the other baseline\nsystems on the emotion cause corpus [Gui et al., 2016a].\n4.2 Compared Systems\nWe compare our model with the following 12 baseline sys-\ntems:\n1) RB is a rule based method [Lee et al., 2010];\n2) CB is common-sense based method [Russo et al., 2011];\n3) RB+CB is a combination of RB and CB;\n4) RB+CB+SVM is a SVM classiﬁer trained on features in-\ncluding rules [Lee et al., 2010] and Chinese Emotion Cog-\nnition Lexicon [Xu et al., 2017];\n5) Ngrams+SVM denotes a SVM classiﬁer that uses the un-\nigram, bigram and trigram features. It was a baseline sys-\ntem in [Gui et al., 2017];\n6) Multi-kernel is a multi-kernel based method proposed in\n[Gui et al., 2016a];\n7) Word2vec+SVM denotes a SVM classiﬁer using word\nembeddings learned by Word2vec as features;\n8) CNN is the basic convolutional neural network proposed\nby [Kim, 2014];\n9) Memnet is convolutional multiple-slot deep memory net-\nwork proposed by [Gui et al., 2017];\n10) CANN is a co-attention neural network model with emo-\ntional context awareness [Li et al., 2018];\n11) PAE-DGL is a reordered prediction model that incorprates\nrelative position information and dynamic global label\n[Ding et al., 2019];\n12) HCS is a CNN-RNN based three-level hierarchical net-\nwork based clause selection [Yu et al., 2019].\n4.3 Main Results\nThe past clause-level approaches regarded the ECE task as\na set of independent clause classiﬁcation problems. By ob-\nserving the corpus, we found that the proportions of emotion\ncause clauses and non-emotion-cause clauses were 18.36%\nand 81.64%, respectively. It is a serious class-imbalance clas-\nsiﬁcation problem and the model tends to predict the clause\nas non-emotion-cause more often. This is also the reason why\ntheir Recall scores were quite low (the highest was 0.6908).\nBy contrast, it can found in Table 1 that the Recall scores\nof the hierarchical models (HCS and RTHN) are signiﬁcantly\nhigher than pervious methods. This is because they can cap-\nture the relations of multiple clauses which help inferring the\ncurrent clause. For example, if no other clauses in a doc-\nument have been detected as an emotion cause, the model\nwill increase the probability of the current clause being pre-\ndicted as an emotion cause. This ﬁnally increases the Recall\nscore. In particular, RTHN achieves a much higher Recall\nscore (0.7699) than other methods (the improvement is more\nthan 7% over the clause-level methods including CANN and\nPAE-DGL, and more than 5% over HCS), but without reduc-\ning the Precision score (only slightly lower than CANN but\nstill higher than the other baselines).\nWe also plot the Precision-Recall (PR) Curves of three\nmethods (CANN, PAE-DGL and RTHN) in Figure 3. It can\nbe seen that the PR curve of RTHN is basically at the top-\nright of the other curves for most cases, and the area under\nRTHN PR curve is also signiﬁcantly larger than the others.\nIt further proves the superiority of the overall performance of\nour RTHN model.\nIn Figure 4, We display the attention weights learned by\nTransformer, by using the example in Figure 1 as a test doc-\nument. The height of the j-th column for the i-th clause de-\nnotes the weight of clause cj in representing the clause ci:\n\fi;j (see Equation 5). Figure 4 can be observed from two\nangles.\nFirstly, for each clause, the weight at the emotion expres-\nsion clause is the largest and gradually becomes smaller to-\nwards both sides. This shows that Transformer can automati-\ncally capture the relative distance information: the smaller the\nrelative distance, the larger the weight assigned. The distribu-\ntion of weights looks similar to a normal distribution centered\non the emotion expression clause.\nSecondly, by observing different clauses, we can ﬁnd that\nthe clause with higher probability being an emotion cause\ntends to have more concentrated distribution; On the con-\ntrary, clauses with smaller probabilities being an emotion\ncause tend to have more uniform distribution. In Figure 4\nthe weight distribution of clause c3 is the most concentrated,\nwhere clause c4 is exactly the ground-truth emotion cause.\nThe largest weight, \f3;4 (0.66) also happens to be the weight\nof the emotion cause clause and emotion expression clause.\nThis phenomenon is very common across different docu-\nments in our experiments. It further conﬁrms our model’s\neffectiveness in capturing the relationships between multiple\nclauses in emotion cause inference.\n4.4 The Effectiveness of Encoding Relative\nPosition and Global Prediction\nIn order to further examine the effects of encoding relative\nposition and global prediction in RTHN, we carry out an ab-\nlation study by designing the following RTHN variants:\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5289\nFigure 3: The Precision-Recall (PR) Curves of three methods\n(CANN, PAE-DGL and RTHN).\nFigure 4: The distribution of attention weights learned in Trans-\nformer for each clause of the example in Figure 1.\n\u000fRTHN-No-GPE (RTHN after removing global predic-\ntion encoding);\n\u000fRTHN-No-RPE (RTHN after removing relative position\nembedding);\n\u000fRTHN-APE (RTHN using absolute position embedding\ninstead of relative position embedding).\nThe results are reported in Table 2. We can observe that\nafter reducing global prediction encoding, the F1 score of\nRTHN-No-GLE decreases more than 3% (0.7314). The re-\nmoval of relative position encoding results in a greater perfor-\nmance degradation (RTHN-No-RPE: 0.4145). By applying\nthe absolute position embedding, RTHN-APE still perform\npoorly (0.5694). All these results demonstrate that RPE and\nGPE are important two factors in RTHN.\n4.5 Why Using RNN-Transformer Combination\nIn RTHN, RNNs and Transformer are used as the word-level\nand clause-level encoders respectively. To investigate the ef-\nfectiveness of the RNN-Transformer combination, we further\ndesign two other combinations for hierarchical modeling:\n\u000fRRHN (RNN-RNN hierarchical network). Multiple Bi-\nLSTMs with attention are used as word-level encoders\nP R F1\nRTHN-No-GPE 0.7369 0.7276 0.7314\nRTHN-No-RPE 0.4588 0.3804 0.4145\nRTHN-APE 0.5800 0.5618 0.5694\nRTHN 0.7697 0.7662 0.7677\nTable 2: The effect of global prediction and different ways of using\nposition information.\nP R F1 Training Time (s)\nRRHN 0.7831 0.7273 0.7534 732\nTTHN 0.7123 0.6798 0.6952 281\nRTHN 0.7697 0.7662 0.7677 360\nTable 3: Performance of different combinations of RNN and Trans-\nformer.\nand a 3-layer stacked Bi-LSTMs is used as the clause-\nlevel encoder.\n\u000fTTHN (Transformer-Transformer hierarchical network).\nTransformer is used as both word-level and clause-level\nencoders.\nNote that relative position and global prediction are en-\ncoded in RRHN and TTHN the same way as that in RTHN.\nIn Table 3, we report their performance as well as the training\ntime on a GTX-1080Ti GPU server. It can be observed that\nboth RRHN and TTHN perform less effectively than RTHN.\nBut what surprised us a bit is that TTHN’s performance is\nsigniﬁcantly behind RTHN and RRHN. One possible reason\nis that we only use a layer of Transformer in the word-level\nencoder. Moreover, due to the advantages of parallel comput-\ning, Transformer’s training time is shorter than RNN that can\nonly perform serial operations.\n5 Conclusions\nThe emotion cause extraction task was normally regarded as\na set of independent clause classiﬁcation problems where the\nrelations between multiple clauses in a document were ig-\nnored. In this work, we propose a joint emotion cause extrac-\ntion framework, called RNN-Transformer Hierarchical Net-\nwork (RTHN), that can model and classify multiple clauses\nin a document synchronously. Transformer has demonstrated\nsuperior performance in capturing the correlations between\nmultiple clauses. Moreover, we proposed ways to encode two\nexplicit factors in ECE (i.e., relative position and global pre-\ndiction) that can capture the causality between clauses and\nmake RTHN more efﬁcient for emotion cause extraction. The\nexperimental results on a benchmark ECE corpus veriﬁed the\neffectiveness and superiority of our approach, in comparison\nwith state-of-the-art techniques in ECE.\nAcknowledgements\nThe work was supported by the Natural Science Foundation\nof China (No. 61672288), and the Natural Science Founda-\ntion of Jiangsu Province for Excellent Young Scholars (No.\nBK20160085).\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5290\nReferences\n[Chen et al., 2010] Ying Chen, Sophia Yat Mei Lee,\nShoushan Li, and Chu-Ren Huang. Emotion cause detec-\ntion with linguistic constructions. In Proceedings of the\n23rd International Conference on Computational Linguis-\ntics (COLING), pages 179–187, 2010.\n[Cheng et al., 2017] Xiyao Cheng, Ying Chen, Bixiao\nCheng, Shoushan Li, and Guodong Zhou. An emotion\ncause corpus for chinese microblogs with multiple-user\nstructures. ACM Transactions on Asian and Low-Resource\nLanguage Information Processing, 17(1):6, 2017.\n[Ding et al., 2019] Zixiang Ding, Huihui He, Mengran\nZhang, and Rui Xia. From independent prediction to re-\nordered prediction: Integrating relative position and global\nlabel information to emotion cause identiﬁcation. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence\n(AAAI), 2019.\n[Gao et al., 2015a] Kai Gao, Hua Xu, and Jiushuo Wang.\nEmotion cause detection for chinese micro-blogs based on\necocc model. In Paciﬁc-Asia Conference on Knowledge\nDiscovery and Data Mining (PAKDD), pages 3–14, 2015.\n[Gao et al., 2015b] Kai Gao, Hua Xu, and Jiushuo Wang.\nA rule-based approach to emotion cause detection for\nchinese micro-blogs. Expert Systems with Applications,\n42(9):4517–4528, 2015.\n[Ghazi et al., 2015] Diman Ghazi, Diana Inkpen, and Stan\nSzpakowicz. Detecting emotion stimuli in emotion-\nbearing sentences. In International Conference on Intel-\nligent Text Processing and Computational Linguistics (CI-\nCLing), pages 152–165, 2015.\n[Gui et al., 2014] Lin Gui, Li Yuan, Ruifeng Xu, Bin Liu,\nQin Lu, and Yu Zhou. Emotion cause detection with\nlinguistic construction in chinese weibo text. In Natural\nLanguage Processing and Chinese Computing (NLPCC),\npages 457–464. 2014.\n[Gui et al., 2016a] Lin Gui, Dongyin Wu, Ruifeng Xu, Qin\nLu, and Yu Zhou. Event-driven emotion cause extraction\nwith corpus construction. InEmpirical Methods in Natural\nLanguage Processing (EMNLP), pages 1639–1649, 2016.\n[Gui et al., 2016b] Lin Gui, Ruifeng Xu, Qin Lu, Dongyin\nWu, and Yu Zhou. Emotion cause extraction, a challeng-\ning task with corpus construction. In Chinese National\nConference on Social Media Processing, pages 98–109,\n2016.\n[Gui et al., 2017] Lin Gui, Jiannan Hu, Yulan He, Ruifeng\nXu, Qin Lu, and Jiachen Du. A question answering ap-\nproach to emotion cause extraction. In Empirical Methods\nin Natural Language Processing (EMNLP), pages 1593–\n1602, 2017.\n[Kim, 2014] Yoon Kim. Convolutional neural networks for\nsentence classiﬁcation. In Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751, 2014.\n[Lee et al., 2010] Sophia Yat Mei Lee, Ying Chen, and Chu-\nRen Huang. A text-driven rule-based system for emotion\ncause detection. In Proceedings of the NAACL HLT 2010\nWorkshop on Computational Approaches to Analysis and\nGeneration of Emotion in Text, pages 45–53, 2010.\n[Lee et al., 2013] Sophia Yat Mei Lee, Ying Chen, Chu-Ren\nHuang, and Shoushan Li. Detecting emotion causes with\na linguistic rule-based approach 1. Computational Intelli-\ngence, 29(3):390–416, 2013.\n[Li and Xu, 2014] Weiyuan Li and Hua Xu. Text-based emo-\ntion classiﬁcation using emotion cause extraction. Expert\nSystems with Applications, 41(4):1742–1749, 2014.\n[Li et al., 2018] Xiangju Li, Kaisong Song, Shi Feng, Dal-\ning Wang, and Yifei Zhang. A co-attention neural net-\nwork model for emotion cause analysis with emotional\ncontext awareness. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4752–4757, 2018.\n[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai\nChen, Greg S Corrado, and Jeff Dean. Distributed repre-\nsentations of words and phrases and their compositional-\nity. In Advances in Neural Information Processing Systems\n(NIPS), pages 3111–3119, 2013.\n[Neviarouskaya and Aono, 2013] Alena Neviarouskaya and\nMasaki Aono. Extracting causes of emotions from text. In\nProceedings of the Sixth International Joint Conference on\nNatural Language Processing (IJCNLP), pages 932–936,\n2013.\n[Russo et al., 2011] Irene Russo, Tommaso Caselli,\nFrancesco Rubino, Ester Boldrini, and Patricio Mart ´ınez-\nBarco. Emocause: an easy-adaptable approach to emotion\ncause contexts. In Proceedings of the 2nd Workshop on\nComputational Approaches to Subjectivity and Sentiment\nAnalysis (WASSA), pages 153–160, 2011.\n[Song and Meng, 2015] Shuangyong Song and Yao Meng.\nDetecting concept-level emotion cause in microblogging.\nIn Proceedings of the 24th International Conference on\nWorld Wide Web (WWW), pages 119–120, 2015.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems (NIPS), pages 5998–6008, 2017.\n[Xu et al., 2017] Ruifeng Xu, Jiannan Hu, Qin Lu, Dongyin\nWu, and Lin Gui. An ensemble approach for emotion\ncause detection with event extraction and multi-kernel\nsvms. Tsinghua Science and Technology, 22(6):646–659,\n2017.\n[Yada et al., 2017] Shuntaro Yada, Kazushi Ikeda, Keiichiro\nHoashi, and Kyo Kageura. A bootstrap method for au-\ntomatic rule acquisition on emotion cause extraction. In\nIEEE International Conference on Data Mining Work-\nshops, pages 414–421, 2017.\n[Yu et al., 2019] Xinyi Yu, Wenge Rong, Zhuo Zhang,\nYuanxin Ouyang, and Zhang Xiong. Multiple level hier-\narchical network-based clause selection for emotion cause\nextraction. IEEE Access, 7(1):9071–9079, 2019.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5291",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.802268385887146
    },
    {
      "name": "ENCODE",
      "score": 0.7445797920227051
    },
    {
      "name": "Transformer",
      "score": 0.7392639517784119
    },
    {
      "name": "Encoder",
      "score": 0.7305437326431274
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7062790393829346
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5900822877883911
    },
    {
      "name": "Natural language processing",
      "score": 0.4371635317802429
    },
    {
      "name": "Artificial neural network",
      "score": 0.3713458776473999
    },
    {
      "name": "Machine learning",
      "score": 0.3430958092212677
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33779144287109375
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 110
}