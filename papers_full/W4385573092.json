{
    "title": "SLING: Sino Linguistic Evaluation of Large Language Models",
    "url": "https://openalex.org/W4385573092",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2443900910",
            "name": "Yixiao Song",
            "affiliations": [
                "Amherst College"
            ]
        },
        {
            "id": "https://openalex.org/A2948613238",
            "name": "Kalpesh Krishna",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2104177732",
            "name": "Rajesh Bhatt",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2068391019",
            "name": "Mohit Iyyer",
            "affiliations": [
                "Amherst College"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W3158631574",
        "https://openalex.org/W4226258784",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2391263222",
        "https://openalex.org/W4230707461",
        "https://openalex.org/W3197876970",
        "https://openalex.org/W3187018546",
        "https://openalex.org/W2005755780",
        "https://openalex.org/W4300466035",
        "https://openalex.org/W4242266491",
        "https://openalex.org/W566301461",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2918996109",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W2528524026",
        "https://openalex.org/W2006210554",
        "https://openalex.org/W4205857304",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W648860100",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3193521099",
        "https://openalex.org/W2895201073",
        "https://openalex.org/W2963612262",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W3155192455",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2970390493",
        "https://openalex.org/W2187736019",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2293774272",
        "https://openalex.org/W3186655327",
        "https://openalex.org/W2894847200",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W4245748374",
        "https://openalex.org/W4289552613",
        "https://openalex.org/W3205301716",
        "https://openalex.org/W1498441903",
        "https://openalex.org/W1544883939",
        "https://openalex.org/W2346613914",
        "https://openalex.org/W2164371876",
        "https://openalex.org/W1973208101",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2212398078",
        "https://openalex.org/W2963751529"
    ],
    "abstract": "To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates the acceptability contrast of a specific syntactic or semantic phenomenon (e.g., The keys are lost vs. The keys is lost), and an LM should assign lower perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang et al., 2021), which also contains Chinese minimal pairs and was created by translating the vocabulary of the English BLiMP dataset, the minimal pairs in SLING are derived primarily by applying syntactic and lexical transformations to naturally-occurring, linguist-annotated sentences from the Chinese Treebank 9.0, thus addressing severe issues in CLiMP’s data generation process. We test 18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show that the average accuracy for LMs is far below human performance (69.7% vs. 97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested LMs, even much larger ones. Additionally, we find that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4606–4634\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nSLING : Sino L ING uistic Evaluation of Large Language Models\nYixiao Song♢ Kalpesh Krishna♠ Rajesh Bhatt♢ Mohit Iyyer♠\n♢Department of Linguistics, UMass Amherst\n♠Manning College of Information and Computer Sciences, UMass Amherst\n{yixiaosong,bhatt}@umass.edu\n{kalpesh,miyyer}@cs.umass.edu\nAbstract\nTo understand what kinds of linguistic knowl-\nedge are encoded by pretrained Chinese lan-\nguage models (LMs), we introduce the bench-\nmark of Sino LING uistics (SLING ), which con-\nsists of 38K minimal sentence pairs in Man-\ndarin Chinese grouped into 9 high-level lin-\nguistic phenomena. Each pair demonstrates\nthe acceptability contrast of a specific syntactic\nor semantic phenomenon (e.g., The keys are\nlost vs. The keys is lost), and an LM should\nassign lower perplexity to the acceptable sen-\ntence. In contrast to the CLiMP dataset (Xiang\net al., 2021), which also contains Chinese min-\nimal pairs and was created by translating the\nvocabulary of the English BLiMP dataset, the\nminimal pairs in SLING are derived primarily\nby applying syntactic and lexical transforma-\ntions to naturally-occurring, linguist-annotated\nsentences from the Chinese Treebank 9.0, thus\naddressing severe issues in CLiMP’s data gener-\nation process. We test 18 publicly available pre-\ntrained monolingual (e.g., BERT-base-zh,\nCPM) and multi-lingual (e.g., mT5, XLM) lan-\nguage models on SLING . Our experiments\nshow that the average accuracy for LMs is far\nbelow human performance (69.7% vs. 97.1%),\nwhile BERT-base-zh achieves the highest\naccuracy (84.8%) of all tested LMs, even much\nlarger ones. Additionally, we find that most\nLMs have a strong gender and number (sin-\ngular/plural) bias, and they perform better on\nlocal phenomena than hierarchical ones.1\n1 Introduction\nWhile large-scale pretrained language models\n(LMs) have achieved considerable downstream suc-\ncess (Devlin et al., 2019; Xue et al., 2021; Brown\net al., 2020, a.o.), it remains challenging to evalu-\nate how much linguistic knowledge they have ac-\nquired. One approach is to design minimal pairs\nconsisting of two sentences that differ only in a\n1The SLING data and code can be found https://\ngithub.com/Yixiao-Song/SLING_Data_Code.\nA: 他们在吃饭了。 \n(They are already in the process \nof having a meal.)\nB: 他们在吃了饭。 \n(They are already in the process \nof having ﬁnished a meal.)\nChinese Speakers (97.1% acc.)\nPretrained LMs (69.7% acc.)\nB \n A \n Contribution #1:\n38K minimal edit pairs in \nChinese across nine \nsyntactic and semantic \nlinguistic phenomena\nContribution #2:\n18 large Chinese LMs \nevaluated (monolingual as \nwell as multilingual like \nmT5, XLM). LMs \nunderperform native \nspeakers (97% vs 70%)\nFigure 1: An illustration of the SLING dataset. The A\nsentence is acceptable but B, a minimal edit counterpart\nof A, is not. LMs see one sentence at a time and are\nexpected to assign a lower (pseudo-)perplexity to the ac-\nceptable sentence. Overall, LMs underperform Chinese\nnative speakers on SLING (97% vs 70%), making it an\nexciting benchmark for future Chinese LM research.\ncritical word or phrase, which renders only one of\nthe sentences acceptable (e.g., The keys are lost\nvs. The keys is lost ). If an LM is sensitive to\nthe phenomenon exemplified by the minimal pair\n(in this case, plurality), it should assign a lower\nperplexity to the acceptable sentence. This method-\nology can be used to test an LM’s understanding\nof a wide range of linguistic phenomena; for ex-\nample, the BLiMP dataset (Warstadt et al., 2020)\ncontains 67K minimal pairs automatically gener-\nated via manually-constructed grammars that span\n12 high-level English phenomena.\nCan we create similar datasets to study linguistic\nphenomena in a different language, such as Chi-\nnese? As a first step in this direction, Xiang et al.\n(2021) introduce CLiMP, a Chinese dataset of min-\nimal pairs. However, we identify two major issues\nwith CLiMP’s construction process: (1) its vocabu-\nlary is translated from BLiMP’s vocabulary, which\ndue to morphological differences between English\nand Chinese (e.g., the latter lacks numeral or ver-\nbal inflections) results in a large number of unin-\n4606\ntelligible sentences; and (2) the grammatical tem-\nplates for several phenomena (anaphor agreement,\nclassifier-noun agreement, and filler-gap dependen-\ncies) are inadequately designed, which along with\nthe vocabulary issue results in minimal pairs that\ndo not have any clear contrast.2\nTo address these issues, we introduce\nSLING (Sino LING uistics benchmark), a dataset\nof 38K minimal pairs to study nine Chinese\nlinguistic phenomena, many of which are unique\nto the Chinese language. Instead of translating\nBLiMP, we construct SLING primarily using the\nChinese Treebank 9.0 (Xue et al., 2016), which\nwas annotated by trained linguists (see Table 1 for\na comparison). We extract subtrees from human-\nvalidated constituency parses in this treebank and\nthen carefully edit them using manually-designed\nlinguistic templates to create minimal pairs. SLING\ndoes not suffer from the issues we found in CLiMP,\nand it additionally includes semantic as well as\nsyntactic phenomena, seven of which are not found\nin CLiMP. A human validation ofSLING with 16\nnative speakers confirms that its minimal pairs\nunambiguously show the acceptability contrast\nacross all phenomena, yielding an almost perfect\ninter-annotator agreement (Fleiss’ κ= 0.88).\nWe evaluate a total of 18 publicly-available pre-\ntrained LMs on SLING , including monolingual Chi-\nnese (e.g., bert-base-chinese, PanGu-α)\nand multilingual models (e.g., mT5, XLM-R). Our\nresults reveal that: (1) no LM consistently out-\nperforms others on SLING ; (2) larger LMs do not\nnecessarily outperform smaller ones; (3) monolin-\ngual Chinese LMs generally perform better than\nmultilingual ones; and (4) humans significantly out-\nperform all LMs (97.1% vs 69.7% average across\nLMs). We observe that the ranking of models on\nCLiMP differs from that on SLING : for example,\nbert-chinese-base is the best-performing\nmodel on SLING (average accuracy 84.8%), while\nchinese-pert-base performs best on CLiMP\n(81.2%). This result is due in part to the issues in\nCLiMP’s construction process, as well as the differ-\nent phenomena that we test in SLING . Additionally,\nSLING is more discriminative than CLiMP (i.e.,\nLMs vary more across the phenomena in terms of\naccuracy), which makes it more useful as a diag-\nnostic benchmark especially given the large gap\n2Note that although Xiang et al. (2021) report a high hu-\nman accuracy of 97.1% on CLiMP, this number is calculated\nusing majority vote of 16 annotators, and the inter-annotator\nagreement is not reported.\nCLiMP S LING\nvocab. source BLiMP’s vocab. translated Chinese Treebank 9.0\nvocab. size\nactual 1272 types\n(w/ 230 proper names) 11988 types\n(claimed 3456)\ngrammar 9 syntax phenomena 3 semantics + 6 syntax\n(16 paradigms) (5 syntax differ from CLiMP)\n(38 paradigms)\nevaluated LMs\nmonolingual only 10 mono- & 8 multilingual\n1 bert-base-chinese 1 LSTM\n3 LSTM 3 Causal LMs\n2 5-gram 14 Masked LMs\nTable 1: An comparison between CLiMP (Xiang et al.,\n2021) and SLING . SLING is created with a natural and\ndiverse vocabulary, covers new semantic and syntac-\ntic Chinese linguistic phenomena, and is evaluated on\nlarge pretrained LMs, including multilingual models\nlike mT5.3\nbetween human and model performance.\n2 Evaluating Chinese LMs with Minimal\nPairs: CLiMP and Its Shortcomings\nUsing minimal pairs to detect a function of a single\nelement (e.g., phoneme, affix, or word) is a com-\nmon practice in linguistics. In Figure 1, by chang-\ning the position of 了, sentence A is transformed\ninto the ungrammatical sentence B, and we know\nhow the two aspect markers 在 and 了 interacts. In\nthis paper, following BLiMP and CLiMP, we call\neach major grammatical category a phenomenon,\nand minimal pair types within each phenomenon\nparadigms. The A and B sentences in Figure 1\nform a minimal pair of a paradigm in the aspect\nphenomenon of SLING .4\nXiang et al. (2021) created CLiMP to evaluate 9\nChinese syntactic phenomena with 16 paradigms.\nHowever, the dataset suffers from two major issues:\n(1) faulty minimal pair generation templates and (2)\nits translated vocabulary. In this section, we discuss\nthe issues in detail and show why they hamper\nCLiMP’s utility as a diagnostic dataset for LMs.\nCLiMP’s minimal pairs often do not show the\ndesired acceptability contrast. This problem is\nespecially prominent in the ba construction, bind-\ning/anaphor, and filler-gap dependency phenomena,\non which Xiang et al. (2021) conclude that LMs\nperform poorly. The templates used to generate\ndata for these phenomena are the primary cause of\nthese errors, as we show below.\n4More examples of minimal pairs can be found in Ap-\npendix D.\n4607\nba construction: Many minimal pairs associated\nwith this construction do not exhibit the acceptabil-\nity contrast.5 We examine the first 50 minimal pairs\nof this phenomenon in CLiMP and discover that 6\npairs actually have the wrong acceptability label:\nSentences CLiMP Actual\n报告把大学转移了。The report relocated the university.✓ ✗\n报告被大学转移了。The report was relocated by the university.✗ ✓\nat least 9 minimal pairs contain two acceptable\nsentences:\nSentences CLiMP Actual\n吴宇涛把图书馆调查了。Wu investigated the library.✓ ✓\n吴宇涛被图书馆调查了。Wu was investigated by the library.✗ ✓\nand 4 pairs are unintelligible or nonsensical:\nSentences CLiMP Actual\n王萍把嘴举了Wang lifted a mouth. ✓ ✗\n王萍被嘴举了Wang was lifted by a mouth.✗ ✗\nThe primary reason for the low quality of these\npairs is that CLiMP does not carefully control the\nsource of unacceptability (Abrusán, 2019), which\nwe discuss further in the Limitations section. Spe-\ncific to the ba construction, CLiMP does not in-\nclude essential information about thematic rela-\ntions6 in the vocabulary. Another contributing\nfactor is the small size of the CLiMP vocabulary,\nwhich is translated from that of BLiMP despite\nmany annotated features of BLiMP not applying\nto Chinese (e.g., number features, verb forms, or\ncases). For example, the English verb buy has\nsix forms in BLiMP, listed in Table 2, which dif-\nfer from each other in seven verb-related features.\nThese inflections are useful in English for distin-\nguishing sentence acceptability in several BLiMP\nphenomena (e.g., Passive, Irregular Forms, and\nSubject-Verb Agreement); however, they do not\napply to Chinese because the language lacks in-\nflection, and thus they cannot help construct Chi-\nnese paradigms. In Chinese, the same forms can\nbe represented and built based on the three words\nshown in bold: mai (buy), (zheng) zai (progres-\nsive marker), and le (perfective marker). They do\n5The ba construction is a way to move the object from its\nbase position (after a verb) to the position before the verb. The\nconstruction expresses the meaning of settlement and focuses\non what is happening to the object.\n6A thematic relation represents the semantic relation that a\nnoun phrase bears with respect to an event denoted by a verb.\nFor example, the thematic relation that John holds to the verb\neat in John eats an apple.is that of agent, which means John\nis the agent of an apple eating event.\nnot need to be redundantly listed in the vocabu-\nlary. After removing the redundant word types,\nCLiMP’s vocabulary size is 1,272 (including 230\nproper names), not 3,456 as Xiang et al. (2021)\nreport. This lack of diversity in the vocabulary con-\ntributes to the generation of nonsensical sentences\nusing their minimal pair templates.\nChinese English Features\nmai buy bare\nzheng zaimai buying ing\nmaile bought finite, past\nmai le bought en\nmai buy finite, pres\nmai buys finite, pres, 3sg\nTable 2: An example of the repetitive word types in\nCLiMP’s vocabulary (mai here). ing = progressive, en\n= participle, pres = present, 3sg = third person singular.\nBinding and anaphor paradigms: These two\nparadigms test whether the gender feature of the\nobject anaphor agrees with that of the subject. Is-\nsues in the binding and anaphor paradigms stem\nfrom the fact that CLiMP uses proper names, which\nwere added to CLiMP’s vocabulary in addition to\nthe one translated from BLiMP. However, Chinese\nproper names do not always unambiguously show\ngender. If the gender of the subject is ambiguous\nas in (1) where Ye Zi can be either gender (simi-\nlarily for Alex in English), the performance of the\nLMs is not representative of whether they know the\nfunction of the reflexive anaphor, which is exactly\nwhat the binding and anaphor paradigms want to\ntest.\n(1) 叶梓逃离了他/ 她自己。\nYe Zi escaped from him- / herself.\nOther issues with these two paradigms are dis-\ncussed in detail in Appendix D.2.\nFiller-gap paradigm: To create minimal pairs\nfor the filler-gap paradigm in CLiMP, Xiang et al.\n(2021) use what they call the topicalization con-\nstruction. However, (2a), taken from CLiMP, does\nnot contain a filler-gap topicalization dependency.\nA real topicalization filler-gap structure should be\nthe one in (2b), in which the direct object of the\nverb buy is topicalized and moved to the begin-\nning of the sentence, leaving a (gap) at its base\ngenerated position (Huang et al., 2009, Section\n6.1). Unfortunately, the minimal pairs associated\n4608\n这本小说 (this novel)\n(NP-OBJ (DP (DT 这) (CLP (M 本))) (NP (NN 小说)))\n两套小说 (two sets of novels)\n(NP (QP (CD 两) (CLP (M 套))) (NP (NN 小说)))\n一户家庭 (a house of family)\n(NP-PRD (QP (CD 一) (CLP (M 户) ) ) (NP (NN 家庭) ) )\n………….\nStep 1: Search the Chinese Treebank sentence \nparse trees for certain linguistic structures, like \nclassiﬁer-noun pairs in this example. Also \nsearch for compound nouns and verb-object \nphrases (used in other phenomena).\nChinese Treebank 9.0 (3.25M sentence + parses)\nStep 2: Pass extracted structures (M-NN pairs here) through Chinese grammar \ntemplates to construct minimal pairs via edit operations. 38 templates used, \n1000 pairs constructed per template (38K total minimal pairs)\n三套小说\n三户小说\n三本家庭小说\n三户家庭小说\n三户小说家庭\n三本小说家庭\nDT M NN\nM DT NN\nClauses should be in \ncorrect order\n这本小说\n本这小说\n这户家庭\n户这家庭\nCD M1 NN1\nCD M2 NN1\nClassiﬁer should be \ncompatible with noun\nCD M1 NN2 NN1\nCD M2 NN2 NN1\nClassiﬁer should \nmatch farther noun\n……….\n三户家庭\n三套家庭\n……….\n……….\nNN    M\n小说  本, 套\n家庭  户\n    ……….\n1.2K NN-M pairs found \nin TreeBank, stored as \none to many map\nFigure 2: An illustration of the minimal pair generation process used to construct S LING .\nwith this paradigm are generated based on an erro-\nneous template, which means no conclusions can\nbe drawn from model performance on it.\n(2) a. 门，我买了这东西。\nDoor, I bought this thing.\nb. 门，我买了(gap)。\nDoor, I bought (gap).\n3 Creating the S LING Benchmark\nThis section describes our process of generating\nminimal pairs for SLING . We make use of the\nChinese Treebank 9.0 (Xue et al., 2016), a Chi-\nnese corpus with linguist-annotated constituency\nparses that contains 2,084,387 words. This tree-\nbank allows us to use naturally-occurring sentences\nto construct our minimal pairs, unlike the synthetic\nand sometimes nonsensical sentences of CLiMP.\nAlso, unlike CLiMP, whose linguistic templates\nrely solely on one grammar book (Po-Ching and\nRimmington, 2015), our linguistic templates are\nconstructed by a native Chinese linguist (the first\nauthor of this paper) based on multiple works in\nlinguistics. Details of the construction of each phe-\nnomenon and the cited works can be found in Ap-\npendix D. The general minimal pair generation\nprocess is to identify a linguistic pattern, search\nfor relevant linguistic structures in the Treebank,\nand form minimal pairs by applying hand-crafted\ntransformation rules on the extracted structures.\nFigure 2 provides an overview of this process, with\nthe same running example as this section.\n3.1 Corpus: Chinese Treebank 9.0\nChinese Treebank 9.0 is a corpus of parsed text\n(3,247,331 Chinese and foreign characters) from\nvarious resources, both formal and colloquial. The\nTreebank contains 132,080 sentences; we extract\na subset of these sentences that contains linguistic\nstructures of interest and then manipulate those\nsentences to create minimal pairs for SLING .\n3.2 Pattern Search\nThe most important patterns and corresponding\nstrings extracted from the Treebank are classifier-\nnoun phrases, compound noun phrases, and verb-\nobject phrases. To demonstrate the extraction pro-\ncess, we will use classifier-noun phrases as an\nexample. We extract classifier-noun phrases by\nsearching for subtrees that have NP as their root\nnode and contain a classifier M, for example, (3).\n(3) (NP-OBJ (DP (CD 两)\n(CLP (M 套)))\n(NP (NN 小说)))\nFor each sub-tree, a classifier-noun pair is extracted\nas shown in Figure 2. Because each noun may\nhave multiple compatible classifiers, a dictionary is\ncreated with the nouns as keys and the compatible\nclassifiers as the values. Compound noun phrases\nand verb-object phrases are extracted in a similar\nway but stored as sub-trees only.\n3.3 Sentence Generation\nMinimal pairs are generated based on linguistic\ntemplates and the extracted strings. Using the\nclassifier-noun agreement phenomenon as an exam-\nple, the template is CD M Noun. For the accept-\nable phrases, the M is taken from the classifiers that\nare compatible with the noun in the dictionary. For\nthe unacceptable phrases, M is randomly chosen\nfrom a classifier list (after making sure it is not in\nthe list of compatible classifiers).\n4609\nPhenomenonAcceptable Example Unacceptable Example Syn Sem Distractor Distance Hierarchy\nAlternative\nQuestion\ntamen shi laoshi haishi mujiang?\nthey are teacher or carpenter\n“Are they teachers or carpenters?”\ntamen shi laoshi haishi mujiangma?\nthey are teacher or carpenterSP\n✓\nAnaphor\n(Gender)\nnan dianyuan kanjianleta(他)-ziji.\nmale shop assistant saw himself\n“The male shop assistant saw himself.\"\nnan dianyuan kanjianleta(她)-ziji.\nmale shop assistant saw herself\n✓ ✓ ✓\nAnaphor\n(Number)\nnan dianyuan menkanjianletamen-ziji.\nmale shop assistantPL saw themselves\n“The male shop assistants saw themselves.\"\nnan dianyuan men kanjianleta-ziji.\nmale shop assistantPL saw himself\n✓ ✓ ✓\nAspect ta qunian zhiding zhengce le.\nhe last year establish policyAS\n“He established policies last year.\"\nta mingnian zhiding zhengce le.\nhe next year establish policyAS\n✓ ✓\nClassifier-\nNoun\nyi mingtielu jingcha\noneM railway policeman\n“a railway policeman\"\nyi tiaotielu jingcha\noneM railway policeman\n(tiaois a wrong classifier forpoliceman)\n✓ ✓ ✓ ✓\nDefiniteness\nEffect\nzheli/nali youyi jia yingyuan.\nhere/there exist oneM cinema\n“Here/there exists a cinema.\"\nzheli/nali youzhe/na/meijia yingyuan.\nhere/there existDT/DT/everyM cinema\n✓\nPolarity Item tabufazhan renhe youhao guanxi.\nshe not develop any friendly relations\n“She does not develop any friendly relations.\"\nta fazhan renhe youhao guanxi.\nshe develop any friendly relations\n✓\nRelative\nClause\nta jianle na ge zhizhile baoli de nü jingcha.\nshe sawDT Mstoped crimeDECfemale police\n“She saw the female police officer who stopped the crime.\"\nta jianle na ge ta zhizhile baoli de nü jingcha.\nshe sawDT Mshe stoped crimeDECfemale police\n✓ ✓\nWh-fronting tamen shang ge yue daodi goujie leshenme?\nthey last M month on earth collude withASwhat\n“What on earth did they collude with last month?\"\nshenmetamen shang ge yue daodi goujie le?\nwhat they last M month on earth collude withAS\n✓\nTable 3: An overview of the phenomena present in SLING along with their properties. The table indicates whether\nthe paradigms within each phenomena represent syntactic (syn) or semantic (sem) knowledge, whether they involve\na distractor (e.g., the roses in the vase are/*is . . . ), whether there are long distance dependencies (e.g., these\nbeautiful red blooming roses), and whether the LMs need hierarchical knowledge of the language (e.g., Figure 3) to\ndistinguish acceptable sentences from unacceptable ones. Details of each phenomenon are given in Appendix D.\nIn addition to phrases extracted from the Tree-\nbank, we also extract the transitive verbs7 used in\nCLiMP’s anaphor and binding phenomena, 8 and\nfor certain phenomena we also utilize word lists\n(e.g., locations, pronouns, and occupations) to build\nthe minimal pairs. Finally, for each paradigm in\nSLING , we generate one thousand minimal pairs.\n3.4 Phenomena\nAs summarized in Table 3,SLING includes 9 major\nChinese linguistic phenomena in syntax and seman-\ntics. Several minimal pair paradigms are designed\nto test an LM’s robustness to distance and distrac-\ntors in a dependency relation as well as whether\nthey have the essential linguistic knowledge of hi-\nerarchy in Chinese; more details are provided in\nAppendix D. Here we describe the gist of each phe-\nnomenon. The alternative question phenomenon\ntests the knowledge that the disjunctor haishi and\nthe polar question marker ma may not co-occur.\nIn the anaphor agreement phenomenon, we first\nuse baselines to test the LMs’ gender and number\n7The transitive verbs from CLiMP are used in a small\nportion of the minimal pairs in SLING ’s Anaphora dataset,\nwhich requires transitive verbs that take animate subjects and\nobjects. The acceptability contrast of sentences does not rely\non those verbs. Extracting such verbs from the Treebank was\nimpossible because animacy of nouns is not encoded in the\nparse.\n8The vocabulary and data generation code of CLiMP can\nbe found here https://github.com/beileixiang/\nCLiMP.\nbias (see Appendix D.2). Then, the morpheme\nziji (self) is added to test if the LMs knows the\nfunction of ziji and agree the gender/number of the\nreflexive with the sentence subject. To avoid the\nissue caused by Chinese proper names in CLiMP,\nwe use gender + occupation as the subject of sen-\ntences to clearly indicate the gender. The aspect\nphenomenon tests the knowledge of the perfective\naspect markers le and guo in the sense of their in-\nteraction with tense and the progressive marker zai.\nThe classifier-noun agreement is observed when\na noun is modified by a numeral or demonstrative.\nOne noun can be compatible with more than one\nclassifier and the matching can be idiosyncratic.\nThe definiteness effect phenomenon is established\non the observation that demonstrative zhe (this)/na\n(that) and the quantifier mei (every) may not oc-\ncur in the post-verbal position of an existential you\n(there is) sentence. Polarity items (PI) are words\nor phrases whose occurrence is restricted to certain\ncontexts (e.g., negative or affirmative). We test two\nnegative PIs, renhe (any) and shenme (what), as\nwell as one positive PI huoduo huoshao (more or\nless). Chinese relative clauses exhibit a filler-gap\ndependency relationship. If the gap is a simple sub-\nject or direct object position, no resumptive noun\nor pronoun is allowed. Lastly, the wh-fronting\nphenomenon shows that in absence of a specific\ncontext (e.g., an echo question), a wh phrase must\nstay in situ.\n4610\n3.5 Human Validation\nTwo rounds of human validation were conducted\non PCIbex (Zehr and Schwarz, 2018) to verify the\nquality of the generated minimal pairs. 9 Eleven\nstudents from the University of Massachusetts\nAmherst were recruited as annotators for the first\nround, and five for the second round. Each student\nhas finished at least senior high school in China,\nand they all use Chinese on a daily basis. For\nthe first round evaluation, every annotator rated 20\npairs from each of the 30 paradigms (not the base-\nlines).10 The annotators were shown one minimal\npair at a time and asked to choose the more accept-\nable sentence. In total, the annotation task took 1.5\nto 2 hours on average, and the annotators were paid\n$40 each. Details on the second annotation round\ncan be found in Appendix E. The final raw human\naccuracy mean over all paradigms is 97.12% (me-\ndian = 97.27%, SD = 2.29%). The inter-annotator\nagreement as measured by Fleiss’ κis 0.8823, indi-\ncating almost perfect agreement (Landis and Koch,\n1977).\n4 Experimental Setup\nEvaluated Models: There are many pub-\nlicly available pretrained monolingual Chinese\nLMs and multilingual LMs. While Xiang et al.\n(2021) only test bert-base-chinese, three\nLSTM LMs, and two 5-gram LMs in their work\non CLiMP, we experiment with the 18 LMs\nlisted in Table 4. 11 There are 6 pairs of LMs\n(color coded in Table 4) in which one model\nis either trained with more parameters than the\nother in the pair or with larger training data. 12\nAlthough lstm-zh-cluecorpussmall and\ngpt2-zh-cluecorpussmall also differ in\ntheir model structure, we pair them to see whether\na Transformer-based architecture leads to better\nmodel performance. We run the same suite of LMs\non CLiMP, show the results in Table 7, and discuss\n9After the first round, the human accuracy on the two com-\npound noun paradigms were61.36% and 77.27%. To improve\nthe quality of SLING , we revised the generation process of the\ntwo paradigms and re-evaluated their quality.\n10Ten practice and 24 filler item pairs were created to test\nwhether the annotators understood and paid attention to the\ntask. Those pairs are irrelevant to the paradigms of interest.\nAll annotators did these tests with 100% accuracy.\n11Most LMs tokenize an input sentence into characters but\nCPM-Generate and PanGu-αoccasionally cuts an input\ninto words, and the ByT5 models use bytes.\n12The mengzi-bert-base-fin model is\nmengzi-base further trained with 20G extra finan-\ncial news and research reports.\nLM Param Tr. Size Source\n(monolingual models)lstm-zh-cluecorpussmall 25.8M 14G (Zhao et al., 2019)gpt2-zh-cluecorpussmall 102M 14G (same as above)CPM-Generate 2.6B 100GB (Zhang et al., 2021a)PanGu-α 2.6B 1.1TB (Zeng et al., 2021)bert-base-zh 110M 25M sent. (Devlin et al., 2019)zh-pert-base 110M 5.4B (Cui et al., 2022)zh-pert-large 330M 5.4B (same as above)mengzi-bert-base 103M 300G (Zhang et al., 2021b)mengzi-bert-base-fin 103M 320G (same as above)ernie-1.0 110M 173M sent. (Sun et al., 2019)\n(multilingual models)GPT-3-Davinci 175B (Brown et al., 2020)XLM-R-base 270M 2.5TB (Conneau et al., 2020)XLM-R-large 550M 2.5TB (same as above)BERT-base-multiling-cased 110M (Devlin et al., 2019)MT5-small 300M 26.76TB (Xue et al., 2021)MT5-large 1.23B 26.76TB (same as above)Byt5-small 300M 26.76TB (Xue et al., 2022)Byt5-large 1.23B 26.76TB (same as above)\nTable 4: The set of Chinese language models evaluated\nin this work. We consider both large monolingual mod-\nels and multilingual models (separated by double line).\nTr. size = training data size; zh = Chinese; sent. = sen-\ntences. Color coded LM pairs were released in the same\npaper, and differ in size or training data.\nthem in Section 5.6.\nEvaluation: To evaluate the performance of an\nLM on SLING , we use perplexity for the causal\nLMs and pseudo-perplexity (Salazar et al., 2020)\nfor the masked LMs (see Appendix B for details).\nGiven a minimal pair, the LMs should assign a\nlower (pseudo-)perplexity to the acceptable sen-\ntence. The accuracy of each LM on a paradigm\nis the proportion of the minimal pairs in which\nthe model assigns the acceptable sentence a lower\n(pseudo-)perplexity.\nWhy perplexity? We choose to use perplexity\ninstead of other metrics (e.g., raw probability) be-\ncause some phenomena in SLING have systematic\ndifference in sentence length within minimal pairs\n(e.g., Polarity Item, Relative Clause). Thus, we\nrequire a length-normalized metric like perplex-\nity, since metrics such as probability can prefer\nshorter sentences by nature (Wu et al., 2016; Koehn\nand Knowles, 2017; Brown et al., 2020; Holtz-\nman et al., 2021). Additionally, perplexity (or\npseudo-perplexity) is applicable to all phenomena\nand all LMs that are tested in SLING (details in\nAppendix B). We considered other evaluation met-\nrics such as prefix methods (Linzen et al., 2016;\nGulordava et al., 2019; Wilcox et al., 2019), by-\nword surprisal (Futrell et al., 2018), and training\nan acceptability classifier (Warstadt et al., 2019)\nbut eventually decided not to use them for reasons\n4611\ndetailed in Appendix C.\n5 Results & Analysis\nTable 5 reports the human performance and the\nresults of the LMs on each phenomenon.13 Over-\nall, LM performance (bert-base-zh 84.8% be-\ning the best) lags far behind human performance\n(97.1%). Looking into each phenomenon, although\nsome LMs occasionally perform better than hu-\nmans (e.g., in the definiteness effect), no single LM\nperforms consistently well. Comparing the mono-\nlingual LMs to the multilingual ones, the former\nperforms in general better than the latter.14 In the\nfollowing subsections, we provide analyses of the\nmodel performance from the aspects of model size,\ndistance, and hierarchy. By-phenomenon results\nand analyses are in Appendix F.\n5.1 Model Size\nTo investigate whether a larger model performs bet-\nter on SLING , two-tailed pairwise Wilcoxon signed\nrank tests were conducted on each LM pair in Ta-\nble 4. The tests indicated that the performance of\nthe LMs in the pert and mengzi LM pairs statis-\ntically significantly differed from each other while\nthere is no statistical difference in other LM pairs.\nFurther one-tailed pairwise Wilcoxon signed rank\ntests on these two pairs revealed (unintuitively) that\nthe smaller LMs (pert-base, mengzi-base)\nperform better than the larger ones (pert-large,\nmengzi-fin). The test results can be found in\nTable 9 in Appendix G.3. The finding here coin-\ncides with the conclusion drawn in BLiMP and\nCLiMP that increasing model size does not neces-\nsarily improve the model performance.\n5.2 LMs are Affected by Distance\nThe classifier-noun phenomenon was designed to\ntest if the LMs are affected by distance in a depen-\ndency. For example, in (4), the classifier is sepa-\nrated from the noun by a long adjective,15 making\nthe local dependency distant. The noun phrase can\nalso be a compound noun (5), in which case the\nclassifier should agree with the second noun.\n13The accuracy of each paradigm in all phenomena can be\nfound in Appendix G.2, along with a visualization in Figure 7.\n14The poor performance of PanGu-αis partially due to its\nstrong bias toward singular number in the anaphor (number)\nphenomenon.\n15In SLING , the long adjective is chosen to be eight charac-\nters of two conjoined adjectives modified by an adverb very\nas in (4-5).\n(4) 三户非常优秀且高效的家庭\n3 households of very excellent and efficient families\n(5) 三本非常优秀且高效的家庭小说\n3 copies of very excellent and efficient family fiction\nTwo two-tailed paired Wilcoxon signed rank\ntests were conducted to compare the simple noun\nparadigm with and without a long adjective as well\nas the ones with compound nouns. The results\nindicated that there was a statistically significant\ndifference between the model performance when\nthe long adjective was present and absent in the\nsimple noun paradigms. There was no such dif-\nference in the compound noun paradigm. Further\none-tailed Wilcoxon signed rank tests showed that,\nwith a long adjective, the LM performance of the\nsimple noun paradigms decreased. The p values\nare reported in Table 10.\n5.3 LMs struggle with Hierarchy\nAll LMs struggle with hierarchical phenomena and\nare vulnerable to linear closeness. This is shown in\nthe results for the anaphor and classifier-noun phe-\nnomena. The anaphor phenomenon was designed\nto test whether the LMs prefer linear or hierarchi-\ncal closeness. For the LMs to correctly choose the\nacceptable sentences, they should prefer hierarchi-\ncal closeness. In the example in Figure 3, DP5 can\nonly agree in its gender feature with DP1, which\nis hierarchically closer. If the LMs are distracted\nby the linearly closer DP3, they would pick the\nunacceptable sentence in which the DP5 is herself.\nS\nVP\nV’\nDP4\nD’\nNPtax returnD’s\nDP5\nhimself1\nVapplied-for\nPP\nDP2\nD’\nNPkioskD’s\nDP3\nfemale director\nPat\nDP1\nmale scholar\nFigure 3: The syntax structure of the sentence 男学\n者在女导演的店里申请了他自己的退税。 (The male\nscholar applied for his own tax return at the female\nfilm director’s shop.) The reflexive anaphor himself\nmust be bound by DP1, which is hierarchically closer,\nrather than DP3, which is linearly closer. Details of the\ntree can be found in Appendix D.\nTwo two-tailed paired Wilcoxon signed rank\ntests were conducted on the male and female\nanaphor paradigms with and without a PP respec-\ntively. The results show that there is a statistically\nsignificant decrease in the performance when the\n4612\nPhenomenon humanlstmgpt2-zhCPMPanGubert-base-zhpert-basepert-largemengzi-basemengzi-base-finerniexlm-R-basexlm-R-largebert-base-multimt5-smallmt5-largebyt5-smallbyt5-largegpt3\nAlternative question97.313.5 47.4 85.8 10.0 93.1 89.8 79.2 75.6 73.0 94.353.1 56.9 6.5 45.3 10.3 25.9 55.1 14.9Anaphor (gender)98.574.967.571.199.0 88.360.8 50.3 92.2 89.381.6 59.5 61.0 82.550.6 37.7 53.9 37.763.2Anaphor (number)96.599.6 100 92.3 0.0 99.9 99.8 98.8 80.3 75.5 99.595.2 85.2 94.7 27.3 7.3 93.6 73.0 99.9Classifier-noun 96.479.985.752.774.8 95.394.9 82.2 93.9 93.594.4 87.1 90.2 87.568.0 84.3 52.7 53.089.1Aspect 97.652.4 71.9 61.2 55.8 84.1 81.6 68.4 76.3 78.3 74.354.1 68.9 45.0 49.8 65.1 55.3 50.9 71.5Definiteness effect96.897.099.470.468.5 96.495.4 73.9 96.6 96.188.7 63.5 72.8 94.172.2 49.0 14.2 9.081.5Polarity item 92.090.3 86.0 78.9 79.6 72.0 90.4 94.7 97.9 98.2 81.396.5 96.5 44.2 78.2 81.6 59.5 62.9 85.9Relative clause 99.172.144.950.414.3 34.238.0 89.3 18.9 13.133.1 43.7 48.7 13.242.2 50.2 2.8 18.365.2whfronting 100 100 99.7 93.7 94.3 99.8 99.8 99.6 99.8 99.4 99.897.4 99.4 67.8 81.1 98.6 13.1 44.7 100\nAverage over phenomena97.175.578.072.955.1 84.883.4 81.8 81.3 79.683.0 72.2 75.4 59.557.2 53.8 41.2 45.074.6\nTable 5: The average percentage accuracy of the LMs and human performance on each phenomenon (random\nguessing is 50%). Overall, humans significantly outperform all LMs. No LM performs well on all phenomena, but\nmonolingual LMs perform better than multilingual ones. A larger model size does not imply better performance. The\nvertical line separates the mono/multilingual models. The anaphor phenomenon accuracies include the baselines.\ndistractor is present. 16 The descriptive and test\nstatistics can be found in Table 11.\nThe classifier-noun phenomenon is designed to\ntest whether the LMs are aware of the right head-\nedness of Chinese compound noun and match the\nclassifier with the second noun in a compound noun\nrather than the first one (cf. (4) and (5)). If the LMs\ndo not have this knowledge but prefer linear close-\nness, they would choose the wrong sentence in a\nminimal pair. The statistics and the results of two\ntwo-tailed Wilcoxon signed rank tests in Table 12\nshow that the LMs performed worse when the dis-\ntractor was present.\n5.4 Strong Gender and Number Bias\nBecause the LMs can have gender and number bias,\nin the anaphor phenomena, we use baselines (e.g.,\nThe male baker likes him / her.) to test the bias.17\nThe higher the accuracy number is, the more biased\na LM is towards him. Figure 9 in Appendix G.3\nshows that, with a male subject, only four mono-\nlingual LMs (gpt2-zh, CPM, pert-base, and\nernie) are gender neutral. When the subject is\nfemale, all LMs are biased towards a female object\n(see Figure 12).\nOne reviewer raised concern that the anaphora\nresolution in those baselines can only be reliably\nsolved in context of the preceding text, which is\ntrue in real life situations. However, in our test\nsetting, since there is no context, the models should\n16This is even the case in the female paradigms where the\nLMs are strongly biased. The female baseline row in Table 8\nshows that when the sentence subject is female, and there is\nno need for the object to agree with the subject of the sentence,\nthe LMs strongly biased towards a female object. Detailed\nexplanation of the baselines can be found in Appendix D.2.\n17The Chinese baseline has the same structure as this En-\nglish translation.\nideally be gender neutral on average (Bordia and\nBowman, 2019).\nThe LMs also have number bias. A baseline\nexample is The three male bakers like them / him.\nThe higher the accuracy number is, the more bi-\nased a LM is towards them. As seen in the results\nin Table 8 (Appendix G.3), while most LMs are\nbiased to a plural object when the subject is plural,\nPanGu-αis strongly biased to a singular object.\nThe purpose of the baselines is to reliably test\nwhether the LMs know that the gender/number\nof ziji (self) should agree with the subject’s gen-\nder/number in the paradigms. As it turn out, the\nfemale and number features are not useful for our\npurpose because the LMs already achieve a ‘high’\naccuracy in the baselines, making it ambiguous\nwhether the high accuracy in non-baselines is be-\ncause they know the function of ziji (self) or they\nare just biased. The male self paradigm, on the\nother hand, shows that most monolingual LMs\nwere able to use ziji as a hint to agree the gender\nof the subject and object. Among the multilingual\nLMs, only gpt3-davinci achieved a meaning-\nful accuracy increase.\n5.5 Vulnerable to Uncertainty\nIn the current study, haishi, le, and wh phrases can\nhave more than one usage depending on contexts.\nThe observation is that the LMs performed worse\non the paradigms with those phrases. This is most\nobvious in the aspect and polarity item phenomena.\nIn the aspect phenomenon, the possible position\nof guo is relatively fixed compared to le, and there\nis no interaction between guo and the progressive\nmarker. The LMs performed better on the guo\nparadigms than on le.\nIn the polarity item phenomenon, the contexts\n4613\nwhere the positive polarity item more or less can\noccur is more restricted than any, which is more\nrestricted to wh phrases. And we see that the LM\nperformance is the best on more or less, followed\nby any, and the worst on wh phrases.\n5.6 Evaluating Our Set of 18 LMs on CLiMP\nWe ran the 18 LMs on CLiMP and compare\nmodel rankings and performance on CLiMP and\nSLING . We observe major differences: the best LM\non SLING is bert-base-chinese (84.8%),\nand on CLiMP it is chinese-pert-base\n(81.22%). That said, monolingual LMs perform\nbetter than multilingual LMs on both datasets. 18\nWhile the average performance of the LMs on both\ndatasets is similar (SLING 69.7%, CLiMP 70.1%),\non average LMs have significantly larger variation\nacross phenomena on SLING (SD = 24.1%) than\non CLiMP (SD = 13.2%). Thus, SLING is more\ndiscriminative of the strengths and weaknesses of\nLMs, as LMs tend to be more polarized to one\ndirection across phenomena in SLING compared\nto those in CLiMP. Finally, because CLiMP does\nnot test the LMs’ bias in the gender and number\nfeatures for their binding and anaphor paradigms,\nthe LM performance on these two paradigms is\nuninformative since we do not know what role the\nbias plays in the tests. SLING corrects this issue by\nincluding 8 baseline paradigms and shows that the\nLMs can be strongly biased (see Section 5.4).\n6 Conclusion\nWe present SLING , a new benchmark for evaluat-\ning Chinese linguistic knowledge in large scale pre-\ntrained LMs. Unlike the existing CLiMP dataset,\nin which we identify several critical issues, we con-\nstruct SLING from naturally-occurring sentences\nin the Chinese Treebank. Our results show that\nmonolingual Chinese LMs achieve better perfor-\nmance on SLING than multilingual LMs. We find\nthat LMs are better at handling local dependencies\nthan long-range dependencies or with distractors,\nand that they are better at syntactic rather than se-\nmantic phenomena. Overall, there remains a large\ngap between LM and human performance.\nLimitations\nAs a benchmark of evaluating LMs’ Chinese lin-\nguistic knowledge, SLING covers 9 major Chi-\n18Kendall Tau correlation of the two rankings for monolin-\ngual LMs is 0.42 and for multilingual LMs is 0.79.\nnese grammatical phenomena with 38k minimal\npairs. However, there are still phenomena that are\nimportant but not included in the current work:\nfor example, the ba and bei constructions. For\nthose structures, unacceptability can have differ-\nent sources (e.g., syntax or pragmatics). 19 Sim-\nple syntactic structure restrictions are not enough.\nWhen deciding which phenomena to include in\nSLING , we deliberately avoid such cases because\nthe (un)acceptability of these phenomena can be\nmitigated by contextual or world knowledge. As a\nresult, human judgement can vary significantly. As\nan example, take the bei construction (Passive): the\nsentence 王萍被嘴举了(Wang was lifted by a mouth)\nis wildly bizarre to some people, while for others,\nit is acceptable because it is possible to imagine a\nworld in which each body part is a mighty character\nthat can lift things. Such “unacceptable” sentences\nare different from The roses is red., which cannot\nbe resolved by any context.\nAnother limitation is that even though Chinese\nTreebank 9.0 contains a rich and diverse vocabu-\nlary, it can still be inadequate at times. For example,\nfor the classifier-noun agreement phenomenon in\nSLING , we were not able to extract enough high-\nquality compound nouns and thus had to manu-\nally create 196 minimal pairs, as described in Ap-\npendix E. One possible way to get around this\nlimitation is to train a parser on the Treebank and\nuse it to automatically parse even more raw Chi-\nnese data. We leave this for future work.\nEthical Considerations\nFollowing best practices (McMillan-Major et al.,\n2021), we plan to open source our dataset along\nwith a data card. We will follow the templates used\nin the GEM benchmark (Gehrmann et al., 2021)20\nand HuggingFace Datasets repository (Lhoest et al.,\n2021).21 Overall, our project had a small computa-\ntional cost since we did not need to do any model\ntraining. We performed inference on all 18 LMs on\na single RTX8000 GPU with 48GB memory. All in-\nference experiments in this paper can be completed\nwithin a day on the single GPU.\n19For possible sources of unacceptability of a sentence,\nplease see (Abrusán, 2019).\n20https://gem-benchmark.com/data_cards\n21https://huggingface.co/docs/datasets/\nv1.12.0/dataset_card.html\n4614\nAcknowledgements\nFirst and foremost, we would like to thank all the\nanonymous reviewers for their valuable comments.\nWe also thank the native Chinese speakers who\nhelped us obtain human performance numbers on\nSLING . We are very grateful to Brian Dillon and\nSimeng Sun for helping formulate the project idea\nin the early stages of the project. We are also thank-\nful to Yutao Zhou and all the participants in the\nSemantics Workshop at UMass Linguistics and the\nUMass NLP group for comments and suggestions\nduring the project. Kalpesh Krishna was supported\nby the Google PhD Fellowship awarded in 2021.\nReferences\nBarbara Abbott. 1993. A pragmatic account of the\ndefiniteness effect in existencial sentences. Journal\nof Pragmatics, 19(1):39–55.\nMárta Abrusán. 2019. Semantic anomaly, pragmatic\ninfelicity, and ungrammaticality. Annual Review of\nLinguistics, 5:329–351.\nSigrid Beck. 2006. Intervention effects follow from\nfocus interpretation. Natural Language Semantics,\n14(1):1–56.\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Student Research Workshop,\npages 7–15, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901.\nLi Chen. 2012. Chinese polarity items. Ph.D. thesis,\nCity University of Hong Kong.\nLisa Lai-Shen Cheng. 1994. Wh-words as polarity\nitems. Chinese Languages and Linguistics, 2:615–\n640.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nYiming Cui, Ziqing Yang, and Ting Liu. 2022. Pert: Pre-\ntraining bert with permuted language model. arXiv\npreprint arXiv:2203.06906.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT, pages 4171–\n4186.\nRichard Futrell, Ethan Wilcox, Takashi Morita, and\nRoger Levy. 2018. RNNs as psycholinguistic sub-\njects: Syntactic state and grammatical dependency.\narXiv preprint arXiv:1809.01329.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khy-\nathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Dur-\nmus, Ondˇrej Dušek, Chris Chinenye Emezue, Varun\nGangal, Cristina Garbacea, Tatsunori Hashimoto, Yu-\nfang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng\nJi, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal\nLadhak, Aman Madaan, Mounica Maddela, Khyati\nMahajan, Saad Mahamood, Bodhisattwa Prasad\nMajumder, Pedro Henrique Martins, et al. 2021.\nThe GEM benchmark: Natural language generation,\nits evaluation and metrics. In Proceedings of the\n1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021), pages 96–120,\nOnline. Association for Computational Linguistics.\nAnastasia Giannakidou, Claudia Maienborn, Klaus von\nHeusinger, and Paul Portner. 2019. Negative and\npositive polarity items. Semantics—Sentence and\ninformation structure, pages 69–134.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2019. Colorless green\nrecurrent networks dream hierarchically. Proceed-\nings of the Society for Computation in Linguistics ,\n2(1):363–364.\nJohn Hale. 2001. A probabilistic earley parser as a\npsycholinguistic model. In Second meeting of the\nnorth american chapter of the association for com-\nputational linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\n4615\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2733–2743, Hong Kong,\nChina. Association for Computational Linguistics.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn’t\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJianhua Hu and Haihua Pan. 2008. Focus and the basic\nfunction of Chinese existential you-sentences. In\nExistence: Semantics and syntax , pages 133–145.\nSpringer.\nCheng-Teh James Huang, Yen-hui Audrey Li, and Yafei\nLi. 2009. The syntax of Chinese, volume 10. Cam-\nbridge University Press Cambridge.\nEdward L Keenan. 1987. A semantic definition of\n“indefinite NP\". In Eric J. Reuland and Alice\nG. B. Ter Meulen, editors, The Representation of\n(In)Definiteness, pages 286–317. Mit Press.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. InProceedings\nof the First Workshop on Neural Machine Translation,\npages 28–39, Vancouver. Association for Computa-\ntional Linguistics.\nRajesh Kumar. 2013. The syntax of negation and the\nlicensing of negative polarity items in Hindi. Rout-\nledge.\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nBiometrics, pages 159–174.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nJing Lin and Anastasia Giannakidou. 2015. No exhaus-\ntivity for the mandarin NPI shenme. Unpublished\nManuscript.\nJo-Wang Lin. 1998. On existential polarity-wh-\n0phrases in Chinese. Journal of East Asian Linguis-\ntics, 7(3):219–255.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nAngelina McMillan-Major, Salomey Osei, Juan Diego\nRodriguez, Pawan Sasanka Ammanamanchi, Sebas-\ntian Gehrmann, and Yacine Jernite. 2021. Reusable\ntemplates and guides for documenting datasets and\nmodels for natural language processing and gener-\nation: A case study of the HuggingFace and GEM\ndata and model cards. In Proceedings of the 1st Work-\nshop on Natural Language Generation, Evaluation,\nand Metrics (GEM 2021) , pages 121–135, Online.\nAssociation for Computational Linguistics.\nHaihua Pan and Peppina Lee. 2004. The role of pragmat-\nics in interpreting the Chinese perfective markers-guo\nand-le. Journal of Pragmatics, 36(3):441–466.\nYip Po-Ching and Don Rimmington. 2015. Chinese: A\ncomprehensive grammar. Routledge.\nJulian Salazar, Davis Liang, Toan Q Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al.\n2019b. What do you learn from context? Probing for\nsentence structure in contextualized word representa-\ntions. In 7th International Conference on Learning\nRepresentations, ICLR 2019.\nIldikó Tóth. 1999. Negative polarity item licensing in\nHungarian. Acta Linguistica Hungarica, 46(1):119–\n142.\nElena V oita and Ivan Titov. 2020. Information-theoretic\nprobing with minimum description length. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nLeslie Fu-mei Wang. 2002. From a motion verb to an\naspect marker: A study of guo in Mandarin Chinese.\nConcentric: Studies in Linguistics, 28(2):57–84.\n4616\nLianqing Wang. 1994. Origin and development of clas-\nsifiers in Chinese. Ph.D. thesis, The Ohio State Uni-\nversity.\nYu-Fang Flora Wang and Miao-Ling Hsieh. 1996. A\nsyntactic study of the Chinese negative polarity\nitem renhe. Cahiers de linguistique-Asie orientale,\n25(1):35–62.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020. Blimp: The benchmark of linguistic\nminimal pairs for English. Transactions of the Asso-\nciation for Computational Linguistics, 8:377–392.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nHuiying Wen. 2020. Relative clauses in Mandarin Chi-\nnese. Queen Mary’s Occasional Papers Advancing\nLinguistics (OPAL, no. 46).\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019. Structural super-\nvision improves learning of non-local grammatical\ndependencies. In Proceedings of NAACL-HLT, pages\n3302–3312.\nYing Wu. 2010. “haishi” de duoyixing yu xide nandu\n[the polysemy and the acquisition difficulty ofhaishi].\nTCSOL Studies, pages 41–48.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nFei Xia. 2000. The segmentation guidelines for the\npenn chinese treebank 3.0. IRCS Technical Reports\nSeries. 37.\nBeilei Xiang, Changbing Yang, Yu Li, Alex Warstadt,\nand Katharina Kann. 2021. CLiMP: A benchmark for\nChinese language model evaluation. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 2784–2790, Online. Association for\nComputational Linguistics.\nLiejiong Xu. 1995. Definiteness effects on Chinese\nword order. Cahiers de linguistique-Asie orientale,\n24(1):29–48.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291–306.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In NAACL-HLT.\nNianwen Xue, Xiuhong Zhang, Zixin Jiang, Martha\nPalmer, Fei Xia, Fu-Dong Chiou, and Meiyu Chang.\n2016. Chinese Treebank 9.0 LDC2016T13.\nKeiko Yoshimura. 2007. Focus and polarity: even and\nonly in Japanese. The University of Chicago.\nJeremy Zehr and Florian Schwarz. 2018. Penncontroller\nfor internet based experiments.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,\nZhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\nWang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yi-\nfan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu,\nQi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao\nTao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing\nJiang, Han Zhang, Lingfeng Deng, Yehong Zhang,\nZhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo,\nShanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng\nJin, Qun Liu, and Yonghong Tian. 2021. Pangu- α:\nLarge-scale autoregressive pretrained Chinese lan-\nguage models with auto-parallel computation. arXiv\npreprint arXiv:2104.12369.\nZhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian\nGu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji,\nJian Guan, et al. 2021a. CPM: A large-scale genera-\ntive Chinese pre-trained language model. AI Open,\n2:93–99.\nZhuosheng Zhang, Hanqing Zhang, Keming Chen,\nYuhang Guo, Jingyun Hua, Yulong Wang, and Ming\nZhou. 2021b. Mengzi: Towards lightweight yet inge-\nnious pre-trained models for chinese. arXiv preprint\narXiv:2110.06696.\nZhe Zhao, Hui Chen, Jinbin Zhang, Xin Zhao, Tao\nLiu, Wei Lu, Xi Chen, Haotang Deng, Qi Ju, and\nXiaoyong Du. 2019. UER: An open-source toolkit\nfor pre-training models. EMNLP-IJCNLP.\nMin Zhou and Jingquan Han. 2012. A phase-based\napproach to the derivation of relative constructions\nin Mandarin Chinese. Journal of Foreign Languages,\n3(002).\nAlessandro Zucchi. 1995. The ingredients of definite-\nness and the definiteness effect. Natural Language\nSemantics, 3(1):33–78.\n4617\nA Ngram Count of CLiMP and S LING\nCLiMP contains 16K minimal pairs (32K sen-\ntences) and SLING 38K (76K sentences). The av-\nerage sentence length in CLiMP is 11.8 (median =\n11) and in SLING is 12.5 (median = 12). Because\nof the difficulty of defining what counts as a word\nin Chinese, we report one to four ngram counts of\ntypes in Table 6, together with the word type counts\nreturned by Jieba.22 Because SLING has more sen-\ntences which can lead to larger type counts, we\nrandomly shuffled the sentences and took 32K sen-\ntences to calcuate the ngram and Jieba counts of\nword types.\nCLiMP S LING -32K S LING -76K\n1gram 1033 2756 2886\n2gram 22289 33031 43122\n3gram 62353 64257 92972\n4gram 102772 87532 133900\nJieba 2335 9872 11987\nTable 6: Counts of one to four ngram types in CLiMP\nand SLING and word type counts by Jieba.\nOne reason for having 1K sentence pairs in each\nparadigm is to cancel out the potential influence\nof word frequency on the perplexity of sentences.\nHaving a diverse vocabulary surely helps in this\nsense.\nB Metrics\nCausal LMs Perplexity (PPL) is used for causal\nLMs to decide the preferred sentences. Each token\nwis assigned a probability pgiven the prefix being\nseen. The perplexity is calculated based on the\nlog likelihood (L). For a sentence of length m, its\nperplexity is calculated as below:\nL= 1\nM\nm∑\ni=1\nlog p(wi|w1...wi−1)\nPPL = exp(−L)\nEach sentence in a minimal pair is assigned a per-\nplexity value. The one with the lower perplexity is\ntaken as the good sentence that the models choose.\nMasked LMs Pseudo-perplexity values (pseudo-\nPPL) are used to evaluate masked LMs (Salazar\net al., 2020). Concretely, tokens in a sentence is\nmasked one after another ( wj). The masked lan-\nguage models return a probability distribution over\n22https://github.com/fxsjy/jieba\nthe vocabulary in the masked position given the\ncontext surrounding it. For a sentence of length m,\nits pseudo-perplexity is calculated as follows:\nw\\i = w1...wi−1,wi+1...wm\npseudo-L= 1\nM\nm∑\ni=1\nlog p(wi|w\\i)\npseudo-PPL = exp(−L)\nC Related Work: Methods of Evaluating\nLinguistic Knowledge and Their\nLimitations in SLING\nTo investigate what kind of and how much lin-\nguistic knowledge large-scale pretrained LMs have\ncompared to human, previous works have focused\non limited LMs and probed into the internal en-\ncoding of the linguistic knowledge (Tenney et al.,\n2019a,b; Clark et al., 2019). Other works inves-\ntigate the LMs’ linguistic knowledge of a small\nsubset of English syntactic grammar by using\nprefix methods (Linzen et al., 2016; Gulordava\net al., 2019; Wilcox et al., 2019), by-word surprisal\n(Futrell et al., 2018), or trained an acceptability\nclassifier (Warstadt et al., 2019).\nPrefix method Linzen et al. (2016) focus on En-\nglish subject-verb dependencies and use a prefix\nmethod for evaluation, which requires LMs to as-\nsign probabilities to the next word given a prefix.\nThe grammatical next word is expected to have a\nhigher probability (e.g., The keys are vs. *The keys\nis). The task includes local subject-verb dependen-\ncies (e.g., The keys are vs. *The keys is) as well as\ndependencies in distance with distractors (e.g., The\nroses in the vase by the door are vs. * The roses\nin the vase by the door is). The prefix method is\nadopted in later works, for example, Gulordava\net al. (2019) and Wilcox et al. (2019).\nThe limitation of the prefix methods is that it\nmostly applies to inflectional grammatical phenom-\nena in a dependency relationship. For Chinese, a\nlanguage that largely lacks inflection, the usage\nof the methods is very limited. Taking SLING as\nan example, the prefix methods are not applicable\nto all nine phenomena because the minimal pairs’\nacceptability depends on:\n• the presence/absence of a crucial word (Alter-\nnative Question, Anaphor (number), Aspect,\nPolarity Item, Relative Clause);\n• the word order (Aspect, wh fronting);\n4618\n• the choice of a crucial word in the middle\nof a sentence whose acceptability depends\non the part of sentence that is after the word\n(Anaphor (gender), Classifier-Noun, Definite-\nness Effect, Polarity Item, Relative Clause).\nBy-word surprisal Another evaluation method,\ninspired by the controlled psycholinguistic exper-\nimentation, is the by-word surprisal 23 and sen-\ntence completion methods proposed by Futrell et al.\n(2018) to explore LMs’ knowledge of syntax. The\nsurprisal reflects whether LMs are affected by the\npresence/absence of critical words in grammatical\nconfigurations. In the sentence completion task,\nLMs completes a sentence given a prefix. Human\nannotators then judge the grammaticality of the\ncompleted sentences.\nThe by-word surprisal method solves one limi-\ntation of the prefix methods (i.e., the acceptability\ndepends on the presence/absence of a crucial word)\nbut still does not account for the other two listed\nabove. The sentence completion method faces sim-\nilar restrictions and cannot be applied in a large\nscale because it requires human judgement of the\ncompleted sentences.\nAcceptability classifier Warstadt et al. (2019)\ntrained an acceptability classifier to perform a\ngrammaticality judgement task, which consists of\nsentences collected from the linguistics literature\nmarked for their acceptability.\nThere are several limitations of training a classi-\nfier. First, it involves many debatable design deci-\nsions (e.g., hyper-parameters). Second, LMs may\nlearn the task from the training data (Hewitt and\nLiang, 2019; V oita and Titov, 2020). Our goal is to\nmeasure the linguistic capability ofpretrained LMs\nwithout additional help from a training dataset that\nhas the same distribution as the test set.\nOverall, the previous methods are either only ap-\nplicable to a subset of linguistic grammar or depend\non the performance of a classifier. The minimal\npair method used in BLiMP breaks through these\nlimitations.\nMinimal pair method To cover a wide range\nof linguistic phenomenon, Warstadt et al. (2020)\nintroduced minimal pair evaluation for LMs and\ncreated the Benchmark of Linguistic Minimal Pairs\nfor English (BLiMP). It evaluates the linguistic\n23Surprisal is the log inverse probability of a word given its\nprefix (Hale, 2001).\nknowledge of twelve English grammatical phenom-\nena including syntax and semantics. Each of them\nconsists of minimal pair paradigms representing\ndifferent aspects of the phenomena. All minimal\npairs are code-generated using templates created by\nlinguists and an annotated vocabulary that contains\n3000 words. The dataset is human validated.\nThe results on BLiMP show that the LMs tested\nin BLiMP are good at local dependency relations\n(e.g., morphology agreement) but bad at phenom-\nena involving hierarchy and semantic knowledge.\nConcerning the training size and model size, while\nincreasing training size can improve model perfor-\nmance, increasing model size does less so.\nOther possible metrics and their limitations\nOther possible metrics are probability and a\nmasked-token method. However, probability is not\na suitable metric to use in SLING for at least two\nreasons. First, probability is only useful for min-\nimal pairs whose sentences have the same length.\nOtherwise, probability by nature prefers shorter\nsentences. Second, the sentences in a minimal pair\nneed to have similar word orders. This is because\ntokenizers might tokenize a sentence in different\nways depending on the word order, causing the\nsentence length of the sentences in a minimal pair\nto be different. In the masked-token method, we\ncan mask out the crucial word in each sentence\nin a minimal pair and ask a LM to give probabil-\nity of the two masked words. This method is not\napplicable to causal LMs. For masked LMs, it is\nonly applicable to Anaphor (gender), Classifier-\nNoun, and Definiteness Effect in SLING where the\nword order does not change. In those cases, since\nSLING uses minimal pairs, the masked token in\nthose phenomena will be exactly the part in which\nthe sentences in a minimal pair differ. Hence, the\nmasked-token method will return the same results\nas the pseudo-perplexity.\nD Linguistic Phenomena\nThe current work focuses on six syntax and three\nsemantics phenomena in Chinese. Table 3 offers\nan overview. There are 30 test paradigms. The\nanaphor phenomenon has 8 baseline paradigms\nto detect LMs’ gender (male/female) and number\n(singular/plural) biases.\nAll phenomena have at least one paradigm that\ncan be solved by checking the linear order of tokens.\nSome phenomena require a negative co-occurrence\nof words. For example, in the alternative question\n4619\nphenomenon, the disjuntor haishi and the polar\nquestion particle ma may not co-occur. Other phe-\nnomena require a positive co-occurrence. For ex-\nample, in the polarity item phenomenon, the gram-\nmaticality of renhe (any) dependes on the occur-\nrence of negation.\nThree phenomena contain paradigms that require\nthe LMs to use the knowledge of hierarchy. If LMs\nuse linear closeness rather than hierarchical close-\nness, they will wrongly assign a lower perplexity to\nthe unacceptable sentence in a minimal pair. The\nanaphor phenomenon, for example, contains such\nparadigms.\nThe anaphor, classifier-noun agreement, and rel-\native clause phenomena have paradigms that test\nLMs’ robustness to distractors and long distance\ndependencies. A distractor is an element that in-\ntervenes between the head and its dependent in a\ndependency/agreement relation. For example, in\nThe roses in the vase are . . . ,roses and are are in a\ndependency relation, and vase is the distractor. By\ndistance, it is meant to be the case that the head\nand its dependent is separated from each other (e.g.,\nthese beautiful red blooming roses).\nThis section introduces phenomena in turn. If a\nphenomenon is in CLiMP, a comparison between\nCLiMP and the current work will be provided.\nD.1 Alternative Questions with haishi\nChinese alternative questions (AltQ) are most reli-\nably marked by the disjunctor haishi (Huang et al.,\n2009). Although haishi has different usages (Wu,\n2010), when it is used as the disjunctor, the po-\nlar question particle ma (SP) cannot occur. Min-\nimal pairs like (6) test whether LMs are aware\nof this. The paradigm concerns only linear co-\noccurrence.24\n(6) tamen\nthey\nshi\nare\nlaoshi\nteacher\nhaishi\nor\nmujiang\ncarpenter\n(*ma)?\n(*SP)\n“Are they teachers or carpenters?\"\nD.2 Anaphor\nMandarin Chinese has two reflexive pronouns: ziji\nand ta(men)-ziji. The former is morphologically\nsimple with no person, number, or gender features.\nThe latter, ta(men)-ziji, has the pronoun ta which\nencodes gender features in writing: 她 for singular\nfemale third person, 他 for singular male third per-\nson, and 它 for singular non-human third person.\n24The notation (*ma) in (6) means that the sentence is good\nwithout ma but bad with it.\nThe character men indicates plurality. Because of\nthis morphological richness, ta(men)-ziji is used\nto form minimal pairs. Since CLiMP contains the\nbinding phenomenon, their implementation will\nbe first introduced, followed by the binding phe-\nnomenon in the current work.\nBinding Phenomenon in CLiMP Xiang et al.\n(2021) use singular female and male third person\nreflexives ta-ziji to test the LMs’ knowledge of\nbinding. There are two paradigms. The first one\nhas a simple SVO structure in which the object is\nan anaphor and needs to match the gender feature\nof the subject. The second paradigm involves a\ndistractor between the antecedent and the reflexive\n(e.g., DP2 in Figure 4). The distractor is different\nfrom the true antecedent in its gender feature. The\ndistractor is linearly closer to the reflexive but hier-\narchically farther. It turns out that the LMs struggle\nwith this paradigm. The results show that the LMs\ndid no better than chance. One of the acceptable\nbinding sentences in Xiang et al. (2021) is cited\nbelow. We provide its syntax in Figure 4. The cor-\nresponding unacceptable sentence changes herself\nto himself.\n(7) Huang Xiuying\nfemale.name\ndanxin\nworry-about\nWang Hao\nmale.name\nzhihou\nafter\nguanchaguo\nobserve\nta-ziji.\nherself\n“After Huang Xiuying worried about Wang\nHao, she observed herself.\"\nS1\nS3\nVP\nDP4\nherselfVobserved\nDP3\npro1\nAdvP\nAdvafterS2\nVP\nDP2\nmale nameverbworried about\nDP1\nfemale name\nFigure 4: The syntax structure of (7).\nAlthough, by comparing the two paradigms, Xi-\nang et al. (2021) find the models are bad at dealing\nwith hierarchy and distractors, there are four short-\ncomings in the minimal pair design that weaken\nthe strength of the observation. First, it was not\ntested whether the LMs knew the gender of the\nproper names. Because Chinese names do not al-\nways clearly indicate the gender, this can cause\nthe LMs guessing randomly. Second, the syntax\nof the second paradigm is complex because it in-\nvolves ellipsis.25 With the presence of ellipsis, it is\n25The ellipsis is presented as pro1 in DP3 in Figure 4. The\n4620\nnot for sure that the models did bad because they\npreferred a linearly closer agreement or because\nthey couldn’t recover the omitted subject correctly.\nThird, CLiMP does not have a baseline for the gen-\nder biases of the LMs. Hence, we cannot know if\nthe models know the function of ziji or they simply\nprefer one gender. Fourth, CLiMP does not have\nseparate corpora for the two genders. Thus, we do\nnot know if the LMs are bad in both female and\nmale reflexive agreements or only in one of them.\nParadigms in Current Work To amend the four\nshortcomings, the current work includes baseline\nparadigms to test LMs’ gender bias. Sentences\nhave a simple SVO structure. Instead of using\nproper names as the subject, the paradigms use\ngender plus occupations to indicate the gender of a\nnoun. The female and male reflexive agreements\nare tested separately.\nTo form the baseline minimal pairs for the male\nreflexive agreement, an occupation and a transitive\nverb were chosen randomly. Following the verb is\neither a male or female pronoun. Example (8) is\none resulting minimal pair.\n(8) nan\nmale\ndianyuan\nshop assistant\nbaituole\ngot rid of\nta\nhim\n/\n/\nta.\nher\n“The male shop assistant got rid of him.\"\nBoth sentences are acceptable. The purpose is to\nsee whether the models are gender biased when\nthere is no clue for any gender agreement. Other\nbaselines are formed in the same way.\nWith the baseline being established, the mini-\nmal pairs for the reflexive agreement are created\nby adding ziji to the end of the sentences in the\nbaselines. This turns (8) into (9). Because the pres-\nence of ziji, the gender of ta should agree with the\ngender of the male shop assistant. Hence, himself\nis acceptable but herself is not. Such agreement\ncan be solved by linear closeness.\n(9) nan\nmale\ndianyuan\nshop assistant\nbaituole\ngot rid of\nta-\nhim-\n/\n/\n*ta-ziji.\n*herself\n“The male shop assistant got rid of himself.\"\nThe next paradigm tests whether LMs prefer a\nlinearly closer or a hierarchically closer noun as the\nantecedent of an anaphor. An example is (10). The\nsyntax of the grammatical sentence is in Figure 5.\n(10) nan\nmale\nxuezhe\nscholar\nzai\nat\nnü\nfemale\ndaoyan\ndirector\nde\nDEG\ndian\nshop\nshenqingle\napplied-for\nta-\nhim-\n/\n/\n*ta-ziji\n*herself\nde\nDEG\ntuishui.\ntax return\nindex 1 indicates its antecedent is DP1.\n“The male scholar applied for his own tax\nreturn at the female film director’s shop.\"\nS\nVP\nV’\nDP4\nD’\nNPtax returnDpossessive\nDP5himself1\nVapplied-for\nPP\nDP2\nD’\nNPkioskDpossessive\nDP3female director\nPat\nDP1male scholar\nFigure 5: The syntax structure of the sentence in (10)\nwith himself being bound by DP1.\nLike Figure 4, Figure 5 involves a distractorDP3\nbut has no ellipsis. It is a SVO sentence with a\npreposition phrase (PP) modifying the verb phrase.\nThe antecedent of DP5 can only be DP1 which c-\ncommands himself while DP3 is embedded deeply\nin PP. DP1 is hierarchically closer to himself while\nDP3 is linearly closer. The LMs will fail if they\nhave no knowledge of hierarchical structure.\nThe current work also uses the number feature\nto test LMs. Baselines are used to see if the tested\nLMs are biased to singularity or plurality. The\ngender feature is kept constant so that any distinct\nbehaviour is only caused by the number feature.\nD.3 Aspect Marker le and guo\nThe morphemes le and guo often function as per-\nfective aspect markers.26 Although they can occur\nin sentences of various tenses, without the help of\na future oriented adverb together with morphemes\nas cai or jiu, they only occur in sentences of past\ntenses. A paradigm is built on this observation. An\nexample is in (11).\n(11) ta\nhe\nqu\nlast\n/\n/\n*ming\n*next\nnian\nyear\nzhiding\nestablish\nzhengce\npolicy\nle.\nAS\n“He established policies last year.\"\nThe next paradigm is based on a restriction on\nguo that it cannot co-occur with the progressive\nmarker zai, as in (12).\n(12) tamen\nthey\nzai\nAD\nshi\ntry\n(*guo)\n(*AS)\nna\nDT\nge\nM\nfuwu.\nservice\n“They are trying out that service.\"\nThe above paradigms can be solved linearly but\nthe interaction between le and zai requires the\nknowledge of hierarchy. The morpheme le can\nco-occur with zai if le takes scope over zai but not\n26For the other usages ofle and guo, see Huang et al. (2009),\nWang (2002), and Pan and Lee (2004), among others.\n4621\nthe other way. Based on this, two paradigms are\nformed. The first one (13) tests the knowledge that\nle cannot scope under zai. The other paradigm (14)\nshows that le can scope over zai.\n(13) tamen\nthey\nzai\nAD\nguancha\nobserve\n(*le)\n(*AS)\nxuanju.\nelection\n“They are observing the election.\"\n(14) a. tamen\nthey\nzai\nAD\njiao\npay\nfakuan\nfine\nle.\nAS\n“They are (already in the process of)\npaying the fine.\"\nb. * tamen\nthey\nzai\nAD\njiao\npay\nle\nAS\nfakuan.\nfine\nD.4 Classifier-Noun Agreement\nClassifiers are pervasive in Mandarin Chinese. 27\nThey match with nouns and indicate in what unit\na noun is quantified (Huang et al., 2009). The\ndifficulty in classifier-noun agreement is that the\nmatching can be idiosyncratic, and one noun can\nbe compatible with multiple classifiers.\nCLiMP includes the classifier-noun agreement\nphenomenon which consists of three paradigms.\nHowever, because the variables in their minimal\npairs are not well controlled, the experiment results\nare not conclusive.\nClassifier-Noun Agreement in CLiMP Their\nfirst paradigm is the local classifier-noun matching.\nThe second paradigm inserts an adjective with two\nto four characters between the classifier and the\nnoun to increase the distance of the two. There is\nno distractor in the adjective. The third paradigm\nfurther increase the distance by having a relative\nclause instead of an adjective. Without showing\nthe results of each paradigm, Xiang et al. (2021)\nreport that the mean of the model performance is\n71.66% (median 70.1%). Chinese BERT performs\nthe best (92.9%). The overall human accuracy of\nthe paradigms is 99.7%.\nThere are two issues with the paradigms. First,\nsome minimal pairs do not show a clear contrast.\nExample (15) is taken from CLiMP, in which the\nclassifier jia is intended to be unacceptable. How-\never, both liang and jia are compatible with the\nnoun bike.\n27In the current paper, the word ‘classifier’ is used as a\ncover term for both classifiers and measure words. For the\ndifferences between classifiers and measure words, interested\nreaders can refer to Wang (1994).\n(15) Sun Yingying\nfemale name\nzhengzai\nPROG\nreng\nthrow\nyi\none\nliang\nM\n/\n/\n*jia\n*M\nzixingche.\nbike\n“Sun Yingying is throwing a bike.\"\nThe reason for the issue is that each noun in the\nCLiMP vocabulary is associated with only one clas-\nsifier. However, as mentioned before, the classifier-\nnoun matching can be a many to many relation.\nThe second issue is the relative clauses in the third\nparadigm. Some relative clauses contain a distrac-\ntor. In certain cases, the distractor even matches\nthe classifier.\nParadigms in Current Work The current work\nhas five paradigms for the classifier-noun agree-\nment. To avoid the issues in CLiMP, we built a\nclassifier-noun dictionary. Each noun is associ-\nated with a group of classifiers. When creating the\nminimal pairs, it is ensured that the classifier in the\nunacceptable sentences is not listed as a compatible\nclassifier of the noun.\nIn the five paradigms, one paradigm tests models’\nknowledge of the linear order of demonstratives\n(DT) or numerals (CD) and classifiers (M) before a\nnoun. The other four paradigms test LMs’ knowl-\nedge of classifier-noun agreement.\nThe first of the four paradigms involves local\nclassifier-noun agreement. The second paradigm\ninserts a long adjective between the classifier and\nthe noun but, still, no knowledge of hierarchy is\nneeded. The third paradigm is based on compound\nnouns. An example is given in (16).\n(16) yi\none\nming\nM\n/\n/\n*tiao\n*M\ntielu\nrailway\njingcha\npoliceman\n“a railway policeman\"\nA Chinese compound noun can be formed by two\nnouns, noun1 (railway) and noun2 (policeman),\nwith noun1 modifying noun2. The classifier\nagrees with noun2 (Huang et al., 2009). Hence,\nnoun1 functions as a distractor. In (16), ming is\nthe classifier for policeman while tiao is for rail-\nway. The last paradigm adds a long adjective after\nthe classifier in the third paradigm. For the com-\npound noun paradigms, the knowledge of hierarchy\nis needed. That is, the LMs should know the right-\nheadedness of Chinese compound nouns.\nD.5 Definiteness Effect\nIt has long been noticed that certain strong deter-\nminers cannot be in the postverbal position in an\n4622\nEnglish existential there-sentence (Keenan, 1987;\nAbbott, 1993; Zucchi, 1995). Similar effects have\nbeen observed in Chinese (Xu, 1995; Hu and Pan,\n2008). The phenomenon to be tested here involves\nChinese you (have), a close counterpart to thethere-\nconstruction. The demonstratives zhe (this) and na\n(that) as well as the quantifier mei (every) are used\nas an equivalence to the strong determiners in En-\nglish. The phrase yi (one) + M is used as a counter-\npart of English weak determiners. This paradigm\ncan be solved by checking the linear co-occurrence\nof two elements, here/there and the strong deter-\nminers. An example is in (17).\n(17) a. zheli/nali\nhere/there\nyou\nexist\nyi\none\njia\nM\nyingyuan.\ncinema\n“Here/there exists a cinema.\"\nb. * zheli/nali\nhere/there\nyou\nexist\nzhe/na/mei\nDT/DT/every\njia\nM\nyingyuan.\ncinema\nD.6 Polarity Items\nPolarity items (PI) are common in natural lan-\nguages (Tóth, 1999; Yoshimura, 2007; Kumar,\n2013; Giannakidou et al., 2019, a.o.). English, for\nexample, has any, ever, and yet, etc. In Chinese,\nrenhe (any) and shenme (what) are two actively\ninvestigated negative PIs. They occur in negation,\npolar questions, and conditionals (Cheng, 1994;\nWang and Hsieh, 1996; Lin, 1998; Chen, 2012;\nLin and Giannakidou, 2015). The phenomenon\ncontains three paradigms. There is no complex hi-\nerarchical structure involved. All paradigms can be\nsolved by just checking the linear co-occurrence\nor absence of certain tokens. The first one con-\ncerns renhe (any). The acceptability contrast is\nestablished by the presence of negation.28\n(18) ta\nshe\n*(bu)\nnot\nfazhan\ndevelop\nrenhe\nany\nyouhao\nfriendly\nguanxi.\nrelations\n“She does not develop any friendly rela-\ntions.\"\nThe second paradigm involves shenme, a multi-\nfunctional phrase. It is often seen in wh-questions\n(e.g., niyou chieat shenmewhat “what do you eat?\").\nHowever, shenme also occurs in the contexts where\ntypical negative PIs occur. The acceptability con-\ntrast is manipulated by the presence of negation.\nYet, to avoid a wh-question reading, the adverb\nshenzhi (even) is used, which can occur in affirma-\ntive or negative contexts but not in wh-questions as\nit can be a focus intervener (Beck, 2006).\n28The notation *(bu) means that the sentence is unaccept-\nable without bu.\n(19) tamen\nthey\nshenzhi\neven\n*(mei)\nnot\nsheji\ninvolve\nshenme\nwhat\nliyi.\ninterests\n“They weren’t even involved in any interests.\"\nThe last paradigm in the current phenomenon\nfocuses on the adverb huoduo huoshao (more or\nless). It is less studied than renhe (any) or shenme\n(what). Nonetheless, by searching in the corpus\nCCL29, it is confirmed that there is no sentence in\nwhich bu or mei (not) negates the verb within 10\ncharacters before or after huoduo huoshao. Hence,\nthe acceptability of the minimal pairs is built on\nthe absence of negation.30\n(20) tamen\nthey\nhuoduohuoshao\nmore-or-less\n(*mei)\n(*not)\nfadong\nstart\nle\nAS\njingong.\nattack\n“They more or less started the attack.\"\nD.7 Relative Clauses\nRelative clauses in Mandarin Chinese are head-\nfinal, meaning a modifying clause occurs before\na modified noun. This characteristic is tested in\nCLiMP. Another characteristic of Chinese relative\nclauses is that it is a filler-gap construction and,\nin the gap position, a resumptive noun is out of\nthe question, and a resumptive pronoun cannot oc-\ncur freely. As cited in Wen (2020), Zhou and Han\n(2012) point out that resumptive pronouns may not\noccur in simple subject or direct object positions.\nThe current study uses this property and constructs\nminimal pairs as in (21). If the LMs are not aware\nof the relative clause structure in those sentences,\nthey can perform poorly because of the local coher-\nence created by the filled-in gaps.\n(21) ta\nshe\njiandao\nsee\nle\nAS\nna\nDT\nge\nM\n(*nü\n(*female\njingcha\npolice\n/\n/\nta)\nshe)\nzhizhi\nstop\nle\nAS\nbaoli\nviolence\nde\nDEC\nnü\nfemale\njingcha.\npolice\n“She saw the female police officer who stopped the\nviolence.\"\n29CCL is a Chinese corpus curated by Center for\nChinese Linguistics at Peking University. It contains\n581,794,456 characters in its Contemporary Chinese cor-\npus. Text sources include transcribed spoken language,\nnewspaper, practical writing, literature, etc. Details can\nbe found at http://ccl.pku.edu.cn:8080/ccl_\ncorpus/corpus_statistics.html.\n30The minimal pairs of this paradigm differ in two aspects.\nFirst, the acceptable sentences contain le but the unacceptable\nones do not. Second, the acceptable sentences do not contain\nmei but the unacceptable ones do. This seems render the pairs\nnot minimally distinct. However, the morpheme mei is a nega-\ntion that encodes the perfective aspect. This is what le does\nin the acceptable sentences. Keeping le in the unacceptable\nsentences will make them unacceptable for a reason that is\nnot at issue here. Hence, even though on the surface the two\nsentences are not minimally distinct, they semantically are.\n4623\nD.8 Wh-fronting\nAs mentioned in Section D.6, shenme is frequently\nused to form wh-questions. In canonical wh-\nquestions, the wh-phrases stay in situ (Huang et al.,\n2009). Without a very specific appropriate con-\ntext, wh-fronting is unacceptable. Hence, no matter\nwhether shenme alone functions as an object or\nmodifies a noun as in (22), the noun phrase con-\ntaining it cannot be fronted. To force a question\nreading of shenme, the phrase jiujing or daodi (on\nearth) are added. There is no complex hierarchy in\nthe sentences and the wh phrases are all objects.\n(22) a. tamen\nthey\nshang\nlast\nge\nM\nyue\nmonth\ndaodi\non earth\ngoujie\ncollude with\nle\nAS\nshenme\nwhat\n(heidao)?\nmobster\n“What (mobster) on earth did they collude with\nlast month?\"\nb. * shenme\nwhat\nheidao\nmobster\ntamen\nthey\nshang\nlast\nge\nM\nyue\nmonth\ndaodi\non earth\ngoujie\ncollude with\nle?\nAS\nE Second Round of Human Validation\nThe minimal pairs of the two compound noun\nparadigms were refined. Among the 2000 new\nminimal pairs, 1804 were code generated and 196\nwere manually created. To verify the minimal pair\nquality, a second round of human validation was\nconducted. Five annotators (3 female, 2 male) with\nan average age of22.2 were recruited the same way\nas described in Section 3.5.\nTwenty pairs of sentences were randomly sam-\npled from both the code generated and manually\ncreated minimal pairs from each paradigm. The\npractice and filler items were used. Each anno-\ntator rated 114 pairs. They did the practice and\nfiller items with 100% accuracy. The task took\nless than 10 minutes. The annotators were paid\n$5. The raw accuracy on the new validated pairs\nwas 95.25% (κ= 0.8823). The manually created\nminimal pairs had a higher accuracy than the code\ngenerated ones ( 97.5% vs. 93%). After the sec-\nond round, the raw human accuracy mean over all\nparadigms is 97.12%.\nF By-phenomenon Results and Analyses\nAltQ The multi-lingual LMs either prefer the\nsentences with ma or perform near chance. Al-\nthough the mono-lingual LMs perform better, only\nbert-base-zh and ernie have an accuracy\nhigher than 90%. There can be multiple reasons\nfor the unsatisfactory performance. First, haishi is\nmulti-functional, which might cause the LMs be-\ning unsure of its disjunctor usage. Second, ma only\noccurs in interrogative contexts, which can make\nthe LMs prefer having it. Third, the LMs do not\nhave a global view of the sentences but only attend\nto parts of them, which can be the reason of their\nrandom guessing.31\nAnaphor (Gender) The LMs are gender bi-\nased. Figure 9 shows that, with a male subject,\nonly four mono-lingual LMs ( gpt2-zh, CPM,\npert-base, and ernie) are gender neutral.\nWhen the subject is female, all LMs are biased\n(see Figure 12). The mono-lingual LMs strongly\nprefer a female object.\nOn one hand because the LMs are strongly bi-\nased, using the female gender to test the anaphor\nphenomenon is inconclusive. Compare Figure 13\nto Figure 12, it is unclear whether the LMs achieved\na high accuracy because they knew ziji or just be-\ncause they liked the female feature. The male\nself paradigm, on the other hand, shows that most\nmono-lingual LMs were able to use ziji as a hint to\nagree the gender of the subject and object. Among\nthe multi-lingual LMs, only gpt3-davinci\nachieved a meaningful accuracy increase.\nTurning to the female self with PP paradigm in\nFigure 14, even thought the mono-lingual LMs pre-\nfer the female feature in the baseline, when there\nis a male distractor in the PP which is linearly\ncloser to the reflexive, the LMs are affected, re-\nflected as a decrease in the accuracy. Fewer multi-\nlingual LMs are affected by the distractor. As a\nmatter of fact, XLM-large and ByT5-small\neven have an increase in accuracy. On the male self\nwith PP paradigm, only the mengzi models and\ngpt3-davinci are relatively unaffected by the\ndistractor.\nAnaphor (Number) The plural number feature\nis used to elicit the anaphor agreement. The fea-\nture is imposed on the subject by using numeral +\nclassifier or the plural marker men, or both. The\nplural feature on the object reflexive is reflected\nby adding men to it. As it turns out, the number\nfeature is not a good choice because most LMs are\nstrongly biased (see Table 8).\nAspect Compared to le, guo has a fixed position\nin a VP and cannot take a wide scope over the\nprogressive marker zai. The results show that the\n31The A haishi B disjunction and ma being at the end of a\nquestion are both locally grammatical.\n4624\nLMs performed better on the guo paradigms than\non le. There is no obvious reason why CPM in\nFigure 18 performs extremely bad.\nClassifier-noun agreement The first paradigm\ntested the LMs’ knowledge of the relative order of a\ndemonstrative and classifier. Figure 20 shows that,\nexcept for the CPM, PanGu-α, mt5, and ByT5\nmodels, all LMs’ accuracy are comparable to the\nhuman annotators.\nComparing the paradigms with simple nouns\n(Figure 21 and 22) to the ones with compound\nnouns (Figure 23 and 24), the multi-lingual models\nare more severely affected by the existence of a\ndistractor (i.e., noun1 in a compound noun) than\nthe mono-lingual ones. The LMs are less affected\nby the distance created by the long adjective (Fig-\nure 21 vs. Figure 22, and Figure 23 vs. Figure 24).\nDefiniteness Effect Except for CPM, PanGu-α\nand pert-large, all mono-lingual models have\na decent accuracy. On the multi-lingual side, the\nByT5 models are especially bad.\nPolarity item Among the three PIs, huoduo hu-\noshao (more or less) reliably occurs only in affir-\nmative contexts. The negative PIs, renhe (any) and\nshenme (what), can occur in negative, interrogative,\nand affirmative contexts. Fifteen out of eighteen\nLMs reached an accuracy on huoduo huoshao com-\nparable or even better than human. On the other\ntwo PIs, although there are quite a few LMs per-\nform even better than human, overall, the accuracy\nvalues are worse and uneven.\nRelative clause In the resumptive noun paradigm,\nonly CPM and pert-large have a satisfying per-\nformance. The other models are either near chance\n(lstm and mt5-small) or strongly deviated by\nthe repeated filler in the gap position. The reason\ncould be that the LMs are vulnerable to repetition,\nor to local grammaticality. When the gap in the\nrelative clause is filled by a pronoun that matches\nthe gender of the head noun, fewer than half of the\nLMs are able to notice the minimal pair contrast.\nWh-fronting All mono-lingual models performed\nwell. Probably because wh in situ is a prominent\nfeature of Mandarin Chinese. Except for the mt5\nand ByT5 models, most multi-lingual models did\nwell. The gpt3-davinci model even reaches a\n100% accuracy.\nG Results\nG.1 CLiMP\nThe results are reported in Table 7 and Figure 6.\nG.2 SLING\nThe results are reported in Table 8 and Figure 7 to\nFigure 33.\nG.3 Statistic Tests\nThe results are reported in Table 9 to Table 12.\n4625\nlstmgpt2-zhCPMPanGubert-base-zhpert-basepert-largemengzi-basemengzi-base-finerniexlm-R-basexlm-R-largebert-base-multimt5-smallmt5-largebyt5-smallbyt5-largegpt3\nanaphor_agreement_gender_1000 82.6 79.5 79.9 92.6 86.2 90.5 71.1 96.1 96.2 93.7 82.1 78.0 73.0 46.2 69.3 55.4 49.4 83.3binding_gender_1000.csv 49.1 45.1 51.3 61.2 50.8 51.5 39.6 64.8 64.0 54.7 48.4 50.6 44.4 51.7 44.7 51.7 51.6 47.1\nba_construction_1000 51.2 72.0 59.3 19.2 69.0 69.1 73.3 59.0 68.0 70.4 73.3 71.1 55.4 34.6 49.3 80.0 64.5 70.9\nclassifier_1000.csv 90.8 95.1 57.1 76.0 95.6 95.4 78.8 89.3 90.2 96.5 85.6 90.8 87.8 58.6 77.4 49.9 51.7 93.1classifier_adj_1000.csv 80.3 91.9 55.5 69.1 93.2 94.3 76.9 90.4 90.7 95.8 81.1 88.0 84.7 58.4 74.1 50.6 50.7 88.3classifier_clause_1000.csv 71.9 84.6 52.2 66.5 90.0 93.2 77.4 86.3 85.4 92.6 77.7 83.2 81.7 61.4 70.9 49.9 51.2 97.6\ncoverb_instrument_1000.csv 62.7 82.7 36.0 54.1 91.1 97.3 63.9 92.6 93.8 96.3 89.3 90.4 60.0 52.0 80.7 54.9 55.7 87.6coverb_with_1000.csv 78.0 78.3 61.7 73.5 84.7 88.6 73.3 88.6 86.0 88.5 85.0 88.3 76.7 81.8 82.8 56.7 48.3 84.7\nfiller_gap_dependency_1000.csv 79.1 86.7 62.3 91.9 62.4 80.2 90.9 86.3 82.7 70.1 67.9 60.3 78.2 80.3 46.0 62.3 63.3 68.2\nhead_final_clause_1000.csv 68.3 77.0 86.5 65.6 53.1 83.9 73.3 82.5 78.9 78.0 76.2 87.1 72.0 85.2 85.8 43.6 60.6 73.0\npassive_formal_1000.csv 69.2 61.6 47.0 61.6 67.7 67.3 44.0 46.4 47.1 68.7 55.0 48.1 73.2 57.3 51.4 54.2 52.4 54.5\nverb_complement_direction_1000.csv 67.0 75.2 81.4 80.1 93.0 91.4 85.9 83.3 89.2 71.6 90.5 88.4 38.5 50.7 55.2 42.7 56.1 73.2verb_complement_duration_1000.csv 96.1 99.1 83.6 82.6 90.2 96.4 89.1 98.4 96.8 94.1 86.4 90.4 76.3 64.6 51.0 12.7 18.9 55.4verb_complement_frequency_1000.csv 98.5 99.2 48.8 75.6 97.8 91.5 78.7 75.9 75.0 87.5 23.6 21.5 90.9 69.8 71.4 44.2 32.5 96.0verb_complement_res_adj_1000.csv 82.9 87.5 25.9 59.3 87.6 87.0 49.3 85.5 84.2 92.5 90.2 91.6 64.4 71.9 88.0 74.9 74.2 79.3verb_complement_res_verb_1000.csv 99.4 98.5 96.7 90.1 96.2 88.8 68.9 85.9 87.2 92.3 53.6 66.1 92.4 65.0 78.6 27.5 33.2 97.0\nAverage over 8 phenomena 71.7 77.8 61.5 65.9 74.3 81.2 69.7 77.5 77.7 79.6 71.9 72.4 70.4 62.1 64.3 55.0 54.7 73.9Std-dev over 8 phenomena 11.4 11.9 12.5 21.2 15.0 11.1 14.3 16.0 14.2 10.6 10.0 14.8 9.6 16.3 15.4 12.2 7.4 12.2\nTable 7: Eighteen LMs’ performance on CLiMP.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n20\n40\n60\n80\n100\nAccuracy\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\nMulti-lingual\nFigure 6: The box represents the inter-quartile range of the human and LM accuracy, with an orange line at the\nmedian accuracy and a green triangle at the mean. The whiskers extend from the box by 1.5 times. Dots are the\naccuracy values that past the end of the whiskers.\nhuman\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\nMulti-lingual\nFigure 7: The box represents the inter-quartile range of the human and LM accuracy, with an orange line at the\nmedian accuracy and a green triangle at the mean. The whiskers extend from the box by 1.5 times. Dots are the\naccuracy values that past the end of the whiskers.\n4626\nhumanlstmgpt2-zhCPMPanGubert-base-zhpert-basepert-largemengzi-basemengzi-base-finerniexlm-R-basexlm-R-largebert-base-multimt5-smallmt5-largebyt5-smallbyt5-largegpt3Alternative question haishi 97.313.5 47.4 85.8 10.0 93.1 89.8 79.2 75.6 73.0 94.3 53.1 56.9 6.5 45.3 10.3 25.9 55.1 14.9\nAnaphor (gender)\nmale_baseline 98.750.054.110042.750.42.6 74.5 87.651.216.326.7 93.738.328.981.774.767.5male_self 98.299.6 87.6 57.7 100 92.6 80.9 7.6 99.6 98.9 88.9 28.3 61.9 99.6 10.5 46.9 48.6 59.2 91.6pp_male 99.664.9 37.2 41.3 99.9 75.7 39.9 16.2 91.4 91.6 71.8 13.9 14.7 71.5 37.4 25.3 34.3 28.7 89.6female_baseline 92.880.291.295.985.685.394.989.3 91.296.490.562.8 82.864.039.240.648.040.8female_self 97.793.4 80.8 89.9 97.3 98.2 95.5 93.9 99.7 98.1 99.7 98.2 71.0 82.7 84.6 47.4 61.0 44.0 48.5pp_female 98.641.8 64.5 95.4 98.6 86.7 26.9 83.5 78.1 68.6 65.8 97.5 96.2 76.1 70.0 31.1 71.6 18.9 23.2\nAnaphor (number)\ncl_male_baseline10010076.40.0 99.710097.782.3 67.810094.381.7 99.780.67.0 99.999.599.9cl_self_male 95.599.5 100 98.7 0.1 99.7 100 99.3 78.5 69.4 99.8 98.5 88.1 97.3 68.1 5.0 99.4 61.7 99.8cl_female_baseline99.510080.60.0 99.699.991.280.0 65.899.267.542.3 98.519.65.9 91.865.299.0cl_self_female97.398.7 100 98.6 0.0 100 99.6 98.9 72.6 83.4 99.6 94.3 69.6 89.6 2.1 8.1 63.1 42.9 99.1men_male_baseline10010048.40.1 99.810098.366.3 43.599.684.477.5 10045.15.8 98.710099.8menself_male97.3100 100 79.9 0.0 99.9 100 100 94.1 85.8 99.7 98.4 85.2 99.5 52.3 5.2 100 77.4 100men_female_baseline10010050.50.0 98.099.996.665.4 48.197.654.463.6 99.67.110.199.499.996.3menself_female95.9100 100 80.9 0.0 99.8 100 98.9 95.0 93.0 98.6 90.5 85.8 89.4 2.2 14.2 99.9 99.4 99.9cl_men_male_baseline10010086.00.0 99.910099.173.1 49.510088.166.7 10043.31.3 99.999.998.8cl_menself_male95.999.7 100 99.6 0.0 100 100 99.6 85.4 74.2 99.9 98.4 92.8 100 38.2 2.5 99.8 64.2 100cl_men_female_baseline10010088.80.0 99.499.596.359.7 42.299.950.839.5 100 4.5 2.9 98.799.294.3cl_menself_female97.399.8 100 96.2 0.0 100 99.4 95.9 56.3 47.0 98.1 91.3 89.6 92.4 0.7 8.5 99.2 92.5 99.9\nClassifier-noun agreement\ndem_cl_swap99.699.6 99.8 52.5 85.7 99.8 99.5 92.3 94.2 92.2 99.0 92.8 94.1 98.9 78.5 81.4 63.0 57.5 98.3cl_simple_noun98.695.6 96.7 61.2 85.0 98.5 98.4 88 96.4 96.6 95.9 94.5 95.9 92.4 77.9 90.4 50.1 53.1 96.3cl_adj_simple_noun93.292.1 95.5 58.9 77.1 96.5 96.5 83.8 95.8 96.1 95.2 93.6 95.2 91.0 58.4 85.5 51.6 52.6 94.5cl_comp_noun94.056.2 70.7 45.6 66.3 91.3 89.9 74.2 90.6 90.3 91.1 71.2 78.5 74.7 63.3 83.6 51.8 48.6 78.3cl_adj_comp_noun96.556.2 65.6 45.1 59.9 90.3 90.1 72.9 92.7 92.1 90.9 83.3 87.4 80.4 62.1 80.8 46.8 53.3 77.9\nAspect\npast_tense_guo99.183.9 85.6 79.7 72.4 95.5 91.4 60.5 88.7 87.6 92.7 79.6 81.7 54.5 48.6 71.6 58.8 45.3 85.8zai_guo 98.255.6 88.2 78.6 65.4 97.9 98.2 90.3 87.3 88.9 97.7 61.3 84.5 65.7 67.3 89.5 49.6 54.2 91.0past_tense_le 99.176.2 70.7 78.8 73.9 65.2 61.4 39.5 81.0 77.3 51.6 49.5 64.6 37.4 31.2 37.9 53.9 43.0 57.7zai_no_le 95.017.4 50.8 0.8 16.1 85.2 86.7 72.8 55.2 62.5 75.6 31.9 51.4 44.6 56.2 81.4 53.9 43.0 63.1zai_le_scope 96.428.8 64.1 68.0 51.4 76.9 70.1 79.1 69.5 75.4 53.7 48.2 62.1 22.8 45.6 45.0 60.3 68.8 60.0\nDefiniteness effectdemonstrative96.894.1 99.3 48.3 49.3 98.2 98.2 82.4 97.4 96.5 94.9 55.1 65.4 92.5 59.8 25.5 27.8 16.1 70.4every 96.899.8 99.5 92.5 87.7 94.6 92.6 65.3 95.8 95.6 82.4 71.9 80.1 95.7 84.5 72.5 0.6 1.9 92.6\nPolarity item any 90.587.6 89.9 95.9 93.6 65.8 86.3 94.9 97.4 97.6 75.6 93.2 95.0 33.8 61.8 83.7 60.0 45.2 63.8even_wh 91.485.3 70.3 42.3 47.7 52.4 87.4 99.1 99.5 99.5 70.8 99.1 99.6 7.1 77.6 97.4 33.0 66.6 96.2more_or_less 94.198.0 97.7 98.6 97.6 97.9 97.5 90.0 96.9 97.6 97.6 97.3 94.9 91.8 95.1 63.8 85.6 77.0 97.7\nRelative clauseresumptive noun10050.9 4.1 82.1 16.7 25.6 15.6 98.5 5.4 4.6 12.1 7.0 3.6 0.2 56.1 26.1 0.0 0.1 39.4resumptive pronoun98.293.2 85.7 18.6 11.8 42.7 60.4 80.0 32.4 21.6 54.1 80.3 93.7 26.2 28.3 74.2 5.5 36.5 90.9\nwhfronting bare_wh 100100 99.9 96.6 99.7 100 100 99.7 99.7 98.9 99.6 99.6 99.7 75.6 86.1 98.4 7.0 36.6 100wh_as_modifier100100 99.4 90.7 88.8 99.5 99.5 99.4 99.8 99.8 99.9 95.2 99.0 60.0 76.1 98.8 19.2 52.8 100\nAverage over 9 phenomena97.175.5 78.0 72.9 55.1 84.8 83.4 81.8 81.3 79.6 83.0 72.2 75.5 59.5 57.2 53.8 41.2 45.0 75.0\nTable 8: Eighteen LMs’ performance on SLING. The blue marked lines are baselines. The baselines are supposed\nto have an accuracy of 50%, meaning the LMs are gender/number neutral.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\nAccuracy\n13.5\n47.4\n85.8\n10\n93.1 89.8\n79.2 75.6 73\n94.3Mono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n97.27\n53.1 56.9\n6.5\n45.3\n10.3\n25.9\n55.1\n14.9\nMulti-lingual\nrandom\nhuman\nalternative question\nFigure 8: The LM accuracy on the alternative question phenomenon.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n98.7\n50 54.1\n100\n42.7\n50.4\n2.6\n74.5\n87.6\n51.2\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n16.3\n26.7\n93.7\n38.3\n28.9\n81.7\n74.7\n67.5\nMulti-lingual\nrandom\nanaphor male baseline\nFigure 9: The LM bias towards a male object when the subject is male.\n4627\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n99.6\n87.6\n57.7\n100\n92.6\n80.9\n7.6\n99.6 98.9\n88.9\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n98.18\n28.3\n61.9\n99.6\n10.5\n46.9 48.6\n59.2\n91.6\nMulti-lingual\nrandom\nhuman\nanaphor male self\nFigure 10: The LM accuracy on the anaphor male self paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n64.9\n37.2 41.3\n99.9\n75.7\n39.9\n16.2\n91.4 91.6\n71.8\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n99.55\n13.9 14.7\n71.5\n37.4\n25.3\n34.3 28.7\n89.6\nMulti-lingual\nrandom\nhuman\nanaphor male self with PP\nFigure 11: The LM accuracy on the anaphor male self with PP paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n92.8\n80.2\n91.2 95.9\n85.6 85.3\n94.9 89.3 91.2 96.4Mono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n90.5\n62.8\n82.8\n64\n39.2 40.6\n48\n40.8\nMulti-lingual\nrandom\nanaphor female baseline\nFigure 12: The LM bias towards a female object when the subject is female.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n93.4\n80.8\n89.9\n97.3 98.2 95.5 93.9 99.7 98.1 99.7Mono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n97.7398.2\n71\n82.7 84.6\n47.4\n61\n44 48.5\nMulti-lingual\nrandom\nhuman\nanaphor female self\nFigure 13: The LM accuracy on the anaphor female self paradigm.\n4628\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n41.8\n64.5\n95.4 98.6\n86.7\n26.9\n83.5 78.1\n68.6 65.8\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n98.6497.5 96.2\n76.1 70\n31.1\n71.6\n18.9 23.2\nMulti-lingual\nrandom\nhuman\nanaphor female self with PP\nFigure 14: The LM accuracy on the anaphor female self with PP paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n83.9 85.6 79.7\n72.4\n95.5 91.4\n60.5\n88.7 87.6 92.7\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n99.09\n79.6 81.7\n54.5 48.6\n71.6\n58.8\n45.3\n85.8\nMulti-lingual\nrandom\nhuman\nguo in past tense\nFigure 15: The LM accuracy on the guo in past tense paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n55.6\n88.2\n78.6\n65.4\n97.9 98.2\n90.3 87.3 88.9\n97.7\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n98.18\n61.3\n84.5\n65.7 67.3\n89.5\n49.6 54.2\n91\nMulti-lingual\nrandom\nhuman\nguo & zai\nFigure 16: The LM accuracy on the guo & zai paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n76.2 70.7\n78.8 73.9\n65.2 61.4\n39.5\n81 77.3\n51.6\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n99.09\n49.5\n64.6\n37.4 31.2 37.9\n53.9\n43\n57.7\nMulti-lingual\nrandom\nhuman\nle in past tense\nFigure 17: The LM accuracy on the le in past tense paradigm.\n4629\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\nAccuracy\n17.4\n50.8\n0.8\n16.1\n85.2 86.7\n72.8\n55.2\n62.5\n75.6\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n95.0\n31.9\n51.4\n44.6\n56.2\n81.4\n53.9\n43\n63.1\nMulti-lingual\nrandom\nhuman\nzai & V le Obj\nFigure 18: The LM accuracy on the zai & V le Obj paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy28.8\n64.1 68\n51.4\n76.9\n70.1\n79.1\n69.5 75.4\n53.7\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n96.36\n48.2\n62.1\n22.8\n45.6 45\n60.3\n68.8\n60\nMulti-lingual\nrandom\nhuman\nzai & le: scope\nFigure 19: The LM accuracy on the zai & le scope paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n99.6 99.8\n52.5\n85.7\n99.8 99.5\n92.3 94.2 92.2 99\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n99.5592.8 94.1 98.9\n78.5 81.4\n63 57.5\n98.3\nMulti-lingual\nrandom\nhuman\ndemonstrative & classifer\nFigure 20: The LM accuracy on the demonstrative & classifier paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n95.6 96.7\n61.2\n85.0\n98.5 98.4\n88\n96.4 96.6 95.9\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n98.6494.5 95.9 92.4\n77.9\n90.4\n50.1 53.1\n96.3\nMulti-lingual\nrandom\nhuman\nclassifier & simple noun\nFigure 21: The LM accuracy on the classifier & simple noun paradigm.\n4630\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n92.1 95.5\n58.9\n77.1\n96.5 96.5\n83.8\n95.8 96.1 95.2\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n93.1893.6 95.2 91\n58.4\n85.5\n51.6 52.6\n94.5\nMulti-lingual\nrandom\nhuman\nclassifer & adj simple noun\nFigure 22: The LM accuracy on the classifier & adj. simple noun paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\nAccuracy\n56.2\n70.72\n45.58\n66.29\n91.26 89.89\n74.22\n90.62 90.3 91.14Mono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n94.0\n71.2\n78.5 74.68\n63.3\n83.57\n51.82 48.56\n78.27\nMulti-lingual\nrandom\nhuman\nclassifer & compound noun\nFigure 23: The LM accuracy on the classifier & compound noun paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\nAccuracy\n56.2\n65.59\n45.11\n59.94\n90.34 90.13\n72.87\n92.65 92.13 90.93\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n96.5\n83.29 87.41\n80.36\n62.09\n80.79\n46.78\n53.33\n77.9\nMulti-lingual\nrandom\nhuman\nclassifer & adj compound noun\nFigure 24: The LM accuracy on the classifier & adj compound noun paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n94.1 99.3\n48.3 49.3\n98.2 98.2\n82.4\n97.4 96.5 94.9\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n96.82\n55.1\n65.4\n92.5\n59.8\n25.5 27.8\n16.1\n70.4\nMulti-lingual\nrandom\nhuman\ndef. effect: demonstrative\nFigure 25: The LM accuracy on the definiteness effect with demonstrative paradigm.\n4631\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n99.8 99.5\n92.5 87.7\n94.6 92.6\n65.3\n95.8 95.6\n82.4\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n96.82\n71.9\n80.1\n95.7\n84.5\n72.5\n0.6 1.9\n92.6\nMulti-lingual\nrandom\nhuman\ndef. effect: every\nFigure 26: The LM accuracy on the definiteness effect with every paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n87.6 89.9 95.9 93.6\n65.8\n86.3\n94.9 97.4 97.6\n75.6\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n90.4593.2 95\n33.8\n61.8\n83.7\n60\n45.2\n63.8\nMulti-lingual\nrandom\nhuman\nPI: any\nFigure 27: The LM accuracy on the polarity item any paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n85.3\n70.3\n42.3 47.7 52.4\n87.4\n99.1 99.5 99.5\n70.8\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n91.36\n99.1 99.6\n7.1\n77.6\n97.4\n33\n66.6\n96.2\nMulti-lingual\nrandom\nhuman\nPI: wh\nFigure 28: The LM accuracy on the polarity item wh paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n98 97.7 98.6 97.6 97.9 97.5\n90\n96.9 97.6 97.6\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n94.0997.3 94.9 91.8 95.1\n63.8\n85.6\n77\n97.7\nMulti-lingual\nrandom\nhuman\nPI: more or less\nFigure 29: The LM accuracy on the polarity item more or less paradigm.\n4632\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n50.9\n4.1\n82.1\n16.7\n25.6\n15.6\n98.5\n5.4 4.6\n12.1\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n100.0\n7 3.6 0.2\n56.1\n26.1\n0 0.1\n39.4\nMulti-lingual\nrandom\nhuman\nRC: resumptive noun\nFigure 30: The LM accuracy on the relative clause with resumptive noun paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\nAccuracy\n93.2\n85.7\n18.6 16.7\n42.7\n60.4\n80\n32.4\n21.6\n54.1\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n98.18\n80.3\n93.7\n26.2 28.3\n74.2\n5.5\n36.5\n90.9\nMulti-lingual\nrandom\nhuman\nRC: resumptive pronoun\nFigure 31: The LM accuracy on the relative clause with resumptive pronoun paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n100 99.9 96.6 99.7 100 100 99.7 99.7 98.9 99.6\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n100.099.6 99.7\n75.6\n86.1\n98.4\n7\n36.6\n100Multi-lingual\nrandom\nhuman\nbare wh fronting\nFigure 32: The LM accuracy on the bare wh fronting paradigm.\nlstm\ngpt2-zh\nCPM\nPanGu\nbert-base-zh\npert-basepert-large\nmengzi-base\nmengzi-fin\nernie\n0\n20\n40\n60\n80\n100\nAccuracy\n100 99.4\n90.7 88.8\n99.5 99.5 99.4 99.8 99.8 99.9\nMono-lingual\nxlm-R-basexlm-R-large\nbert-base-multi\nmt5-smallmt5-largebyt5-smallbyt5-large\ngpt3-davinci\n50\n100.095.2 99\n60\n76.1\n98.8\n19.2\n52.8\n100Multi-lingual\nrandom\nhuman\nwh in DP fronting\nFigure 33: The LM accuracy on the wh in DP fronting paradigm.\n4633\nLM1 LM2 two-tailed greater lesser\nlstm gpt2-zh 0.617 ——– ——–\npert-base pert-large 0.009** 0.005** 0.996\nmengzi-base mengzi-fin 0.004** 0.002** 0.998\nxlm-R-base xlm-R-large 0.913 ——– ——–\nmt5-small mt5-large 0.293 ——– ——–\nbyt5-small byt5-large 0.277 ——– ——–\nTable 9: The p values of the Wilcoxon signed rank tests\nof LM pairs.\ndata two-tailed greater lesser\nsimple & simple w/ adj. 0.000*** 0.000*** 1.000\ncompound & comp. w/ adj. 1 ——– ——–\nTable 10: The results of the Wilcoxon signed rank tests\nof the simple noun with/withouth a long adjective and\nthe ones with compound nouns.\ndata min. median mean max. SD pvalue\nmale self 7.6 84 .25 70 100 31 .27 0.002**male pp 13.9 40.6 52 .52 99.9 29.42\nfemale self44 91 .65 82 .44 99.7 19.54 0.008**female pp 18.9 70.8 66 .36 98.6 26.85\nTable 11: Descriptive statistics of the anaphor (fe)male\nself and (fe)male self with PP paradigms. The p values\nare from the Wilcoxon signed rank tests.\ndata min. median mean max. SD pvalue\nsimple 50.1 95 .05 86.83 98.5 15 .75 0.000***compound 45.58 74.45 73.12 91.26 15.26\nsimple w/ adj.51.6 92 .85 83.88 96.5 16 .57 0.000***comp. w/ adj.45.11 79.13 73.77 92.65 16.43\nTable 12: Descriptive statistics of the classifier & (adj.)\nsimple noun and compound noun paradigms. The p\nvalues are from the Wilcoxon signed rank tests.\n4634"
}