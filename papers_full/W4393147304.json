{
  "title": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting",
  "url": "https://openalex.org/W4393147304",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1978909455",
      "name": "Lei Shu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2890762804",
      "name": "Liangchen Luo",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2952769878",
      "name": "Jayakumar Hoskere",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2106569361",
      "name": "Yun Zhu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2168580450",
      "name": "Yinxiao Liu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2068608856",
      "name": "Simon Tong",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2152170083",
      "name": "Jindong Chen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2083084074",
      "name": "Lei Meng",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3022915354",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2798240283",
    "https://openalex.org/W2948120406",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W6775150526",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W4229019162",
    "https://openalex.org/W3143226010",
    "https://openalex.org/W4287855128",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W6891932496",
    "https://openalex.org/W2150824314",
    "https://openalex.org/W6676984168",
    "https://openalex.org/W2589277916",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2793585215",
    "https://openalex.org/W6630268475",
    "https://openalex.org/W2948138438",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W3099309639",
    "https://openalex.org/W2970553031",
    "https://openalex.org/W2985963903",
    "https://openalex.org/W2534253848",
    "https://openalex.org/W6691077208",
    "https://openalex.org/W2948259865",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4385573924",
    "https://openalex.org/W4389523706",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W3197754201",
    "https://openalex.org/W4297947984",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3034639488",
    "https://openalex.org/W4293138840",
    "https://openalex.org/W3033129824",
    "https://openalex.org/W2953280096",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2102443632",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4386290290",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W4224275713",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3101600240",
    "https://openalex.org/W3175315904",
    "https://openalex.org/W2976223659",
    "https://openalex.org/W3012990076",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2953209111",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4285240908",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3035125262",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4298101430"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in creative tasks such as storytelling and E-mail generation. However, as LLMs are primarily trained on final text results rather than intermediate revisions, it might be challenging for them to perform text rewriting tasks. Most studies in the rewriting tasks focus on a particular transformation type within the boundaries of single sentences. In this work, we develop new strategies for instruction tuning and reinforcement learning to better align LLMs for cross-sentence rewriting tasks using diverse wording and structures expressed through natural languages including 1) generating rewriting instruction data from Wiki edits and public corpus through instruction generation and chain-of-thought prompting; 2) collecting comparison data for reward model training through a new ranking function. To facilitate this research, we introduce OpenRewriteEval, a novel benchmark covers a wide variety of rewriting types expressed through natural language instructions. Our results show significant improvements over a variety of baselines.",
  "full_text": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting\nLei Shu\u0003, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Yinxiao Liu,\nSimon Tong, Jindong Chen, Lei Meng\u0003\nGoogle Research\nfleishu, luolc, jayakumar, yunzhu, canoee, simon, jdchen, leimengg@google.com\nAbstract\nLarge Language Models (LLMs) have demonstrated impres-\nsive capabilities in creative tasks such as storytelling and E-\nmail generation. However, as LLMs are primarily trained on\nﬁnal text results rather than intermediate revisions, it might\nbe challenging for them to perform text rewriting tasks. Most\nstudies in the rewriting tasks focus on a particular transfor-\nmation type within the boundaries of single sentences. In this\nwork, we develop new strategies for instruction tuning and re-\ninforcement learning to better align LLMs for cross-sentence\nrewriting tasks using diverse wording and structures expressed\nthrough natural languages including 1) generating rewriting\ninstruction data from Wiki edits and public corpus through\ninstruction generation and chain-of-thought prompting; 2) col-\nlecting comparison data for reward model training through a\nnew ranking function. To facilitate this research, we introduce\nOPEN REWRITE EVAL, a novel benchmark covers a wide va-\nriety of rewriting types expressed through natural language\ninstructions. Our results show signiﬁcant improvements over\na variety of baselines.\nIntroduction\nText rewriting plays an essential role in a wide range of\nprofessional and personal written communications. It can\nbe conceptualized as a form of controllable text generation\n(Zhang et al. 2022a) , where a speciﬁed textual input is mod-\niﬁed based on the user’s requirement. Several categories of\ntext rewriting have been extensively researched, such as para-\nphrasing (Siddique, Oymak, and Hristidis 2020; Xu et al.\n2012), style transfer (Riley et al. 2020; Zhang, Ge, and Sun\n2020; Reif et al. 2021), and sentence fusion (Mallinson et al.\n2022).\nRecent advances in Large Language Models (LLMs) have\nshown impressive zero-shot capabilities in a wide range of\ntext generation tasks expressed through natural language\ninstructions (Chung et al. 2022). However, user expectation\nfor text rewriting is high and any unintended edits by the\nmodel negatively impact the user’s satisfaction. Given that\nthe LLMs can be hard to control (Qin et al. 2023) and prone to\ngenerating “hallucinated” content (Ji et al. 2023), we propose\nnew methods to ensure that the model is properly trained and\nevaluated.\n\u0003These authors contributed equally.\nCopyright c\r2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nSource Text:The way in which we feed our children determines\nhow healthy they are. It also determines how nicely they develop\nand develop. Children need to be fed a selection of foods each\nday in order to make sure their physical, emotional, and mental\nhealth. All foods are from 1 of the basic food groups. Milk\nand cheese come from the dairy group, for instance, and green\nbeans come from the vegetable group. Bread comes from the\ngrain group and beef comes from the meat group. Chocolate\ncomes from the fat and sugars group. Our children need so many\nservings per day from each of these food groups to maintain\ngood health.\nInstruction: Rewrite the text so that it is easy to understand.\nTarget Text:What we give our kids to eat affects how healthy\nthey are and how they grow. Kids need to eat different types\nof food each day to stay physically, emotionally, and mentally\nhealthy. All foods belong to one of the ﬁve food groups: dairy,\nvegetables, grains, meat and beans, and fruits. Milk and cheese\nare dairy foods, green beans are vegetables, bread is a grain, beef\nis a meat, and chocolate is a fat and sugar food. To stay healthy,\nkids need to eat a certain number of servings from each food\ngroup every day.\nInstruction: Summarize the text.\nTarget Text:Feeding children a variety of foods from the ﬁve\nfundamental food groups (dairy, vegetables, grains, meat, and\nfats/sweets) is essential for their overall health and development,\nincluding their physical, emotional, and mental well-being.\nTable 1: Rewrite examples. The input for RewriteLM con-\ntains an instruction and a source text. The target is a rewrite.\nWe present one source text and two instructions here for\nillustration purpose.\nWe present a strong model — RewriteLM 1, an instruction-\ntuned large language model for cross-sentence text rewriting.\nSimilar to InstructGPT (Ouyang et al. 2022), we perform\nsupervised ﬁne-tuning and reinforcement learning (RL) steps.\nHowever, InstructGPT heavily relies on human labelers to\ncollect instruction data (for supervised ﬁne-tuning) and com-\nparison data (for reward training). We instead develop the fol-\nlowing data and training strategies with minimal human inter-\nvention: 1) For instruction tuning, we extract cross-sentence\n1Github: https://github.com/google-research/google-research/\ntree/master/rewritelm\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18970\nedits from Wiki edits and improves its instruction. To im-\nprove the diversity of the dataset, we also generate synthetic\ninstruction and rewrite output from public corpus through\nchain-of-thought prompting and post-processing 2) For rein-\nforcement learning, we collect comparison data for reward\nmodel training through a new ranking function, which assess\nthe quality of rewrite along several dimensions including\ncontent preservation, hallucination, linguistic variability, and\nlength text change.\nTo properly test the capability of RewriteLM, we introduce\na new benchmark OPEN REWRITE EVAL by collecting human-\ngenerated text rewrites with natural language instructions.\nUnlike the previous benchmarks for text rewriting, which\nmostly had restricted types (Reif et al. 2021; Mallinson et al.\n2022) and performed within the boundaries of single sen-\ntences (Riley et al. 2020; Siddique, Oymak, and Hristidis\n2020; Mallinson et al. 2022), our benchmark is designed for\nresearch on cross-sentence text rewrite and covers a wide\nvariety of rewriting types expressed through natural language\ninstructions.\nWe conduct empirical studies to evaluate the model perfor-\nmance on the OPEN REWRITE EVAL benchmark. The results\nshow that even current state-of-the-art pretrained LLMs have\npoor performance on open-ended rewriting tasks. LLMs ﬁne-\ntuned on general-purpose instruction datasets like Flan-PaLM\n(Chung et al. 2022) and Alpaca (Taori et al. 2023) have bet-\nter performance compared with the pretrained foundation\nmodels, but still have room for improvement. The proposed\nRewriteLMs, including Rewrite-PaLM and Rewrite-PaLM 2,\nboth outperform their corresponding foundation models by a\nsigniﬁcant margin. They also outperform other instruction-\ntuned LLMs, showcasing the effectiveness of the generated\ntraining data. Applying reinforcement learning on top of the\nsupervised tuned Rewrite-PaLM 2 further improves its per-\nformance, resulting in a new state-of-the-art model Rewrite-\nRLr/w-PaLM 2 for text rewriting.\nOur main contributions can be summarized as follows:\n\u000fA new benchmark, OPEN REWRITE EVAL, designed for re-\nsearch on cross-sentence rewrite and covering a wide vari-\nety of rewriting types expressed through natural language\ninstructions, such as formality, expansion, conciseness,\nparaphrasing, tone and style transfer. Unlike previous\nbenchmarks, which were primarily focused on speciﬁc\nrewrite types within the boundaries of single sentences,\nour benchmark is speciﬁcally designed to facilitate cross-\nsentence rewrites with open-ended natural language in-\nstructions. To the best of our knowledge, no such dataset\nhas existed previously.\n\u000f\nNew strategies for instruction tuning and reinforcement\nlearning to better align LLMs for cross-sentence rewrit-\ning tasks using diverse wording and structures expressed\nthrough natural languages including 1) generating rewrit-\ning instruction data from Wiki edits and public cor-\npus through instruction generation and chain-of-thought\nprompting 2) collecting comparison data for reward model\ntraining through a new ranking function. We demon-\nstrate that RewriteLM model achieved the state-of-the-art\nperformance in cross-sentence rewriting tasks on Open-\nRewriteEval.\nRelated Work\nText Editing. The majority of the research on rewriting\ncurrently focuses on a particular set of editing tasks at the\nsentence level, such as paraphrase (May 2021), style trans-\nfer (Tikhonov et al. 2019), spelling correction (Napoles, Sak-\naguchi, and Tetreault 2017), formalization (Rao and Tetreault\n2018), simpliﬁcation (Xu et al. 2016) and elaboration (Iv et al.\n2022). (Faltings et al. 2020) trained an editing model to fol-\nlow instructions using Wikipedia data. However, their focus\nwas solely on edits limited to a single sentence. PEER (Schick\net al. 2022) can follow human-written instructions for updat-\ning text in any domain, but is still limited by the edit types\navailable on Wikipedia. Moreover, it was only evaluated on\na small set of edit types from a human-deﬁned instruction\nevaluation benchmark (Dwivedi-Yu et al. 2022).\nInstruction Tuning. Instruction tuning has shown to im-\nprove model performance and generalization to unseen tasks\n(Chung et al. 2022; Sanh et al. 2022). InstructGPT (Ouyang\net al. 2022) extends instruction tuning further with reinforce-\nment learning with human feedback (RLHF), which heavily\nrelies on human labelers to collect instruction data and model\noutput rankings for training. The focus of these works was\nprimarily on extensively researched tasks and benchmarks,\nwhich do not include open-ended text rewriting.\nData Augmentation via LLM. A common data augmen-\ntation approach involves utilizing trained LLMs to generate\nmore data, which is subsequently incorporated as training\ndata to enhance the model’s performance (He et al. 2019; Xie\net al. 2020; Huang et al. 2022). PEER (Schick et al. 2022)\nleverage LLMs to inﬁll missing data and then use this syn-\nthetic data to train other models. Self-Instruct (Wang et al.\n2022a; Taori et al. 2023) improves its ability to accurately\nfollow instructions by bootstrapping off its own generated\noutputs. Our work builds upon similar ideas and leverages\nthe power of LLMs to enhance existing datasets and generate\nadditional synthetic datasets.\nMethods\nIn this section, we discuss the training data (Section Training\nDataset) and the training procedure (Section Modeling) for\nthe proposed RewriteLM models. Table 2 provides a compre-\nhensive overview of the training data’s statistics.\nTraining Dataset\nWiki Instruction Dataset We examine Wiki revisions and\nextract long-form, high quality edits that contain substantial\nchanges. We also use the associated edit summary of the\nrevision as a proxy for the instructions. We describe edit ex-\ntraction, edit ﬁltering, and instruction improvement in details:\n\u000fEdit Extraction:We initiate the instruction tuning data\ncollection process by gathering Wikipedia revision his-\ntory, where each revision record includes the original text,\nrevision differences, and an edit summary written by the\nrevision author. We extract text block differences between\neach consecutive snapshots of a Wikipedia article and\nthe associated edit summary, following the approach in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18971\nQ1: What kind of text is the following {Nothing hurts more than the distance that we have created between us. I \nsometimes wish you were by my side and make my life complete. It’s becoming hard for me to live without you.}?\nA1: a romantic longing note\nQ2: What is a relevant writing prompt or edit instruction for text {Nothing hurts more than the distance that we \nhave created between us. I sometimes wish you were by my side and make my life complete. It’s becoming hard for me \nto live without you.}?\nA2: make this more poetic\n...\nQ1: What kind of text is the following {Providing tools that deliver a consistent employee experience anywhere and \nat any time can bridge the gap between teams, no matter where they're working from. Employees also need tools that \nsimply help them get their work done. Being able to easily move between applications and quickly access \ninformation from any device is key. Creating a dedicated Space for shared projects is one way to provide \ndistributed teams with a central hub to search and share files across applications, assign tasks, brainstorm, and \ngive feedback at any time, with a persistent record of all interactions. Similarly, the ability to quickly pivot \nto a Meet call from Gmail, Chat, or Docs ensures they don’t lose momentum when collaborating from anywhere.}\nModel Input\nA1: A boring sales pitch\nQ2: What is a relevant writing prompt or edit instruction?\nA2: make it more engaging\nModel Output\nFigure 1: Chain-of-thought (CoT) approach to generating rewrite instructions. The answer to the second question in the output is\nthe generated instruction.\nSchick et al. (2022). In the rest of the section, we may use\nthe terms source text, target text and comment to denote\nthe text before revision, the text after revision and the edit\nsummary of a revision record, respectively.\n\u000fEdit Filtering:In order to create long-form, high-quality\nedits with substantial changes, we remove revision records\nthat meet any of the following criteria: (i) the edit sum-\nmary indicates low-quality content of a snapshot, such\nas containing “revert” or “vandalism” keywords; (ii) the\nedit summary contains keywords indicating a format-only\nchange (e.g.bold-facing or hyperlinks), which is not a fo-\ncus of this work; (iii) the source text contains two or fewer\nsentences.\n\u000fInstruction Improvement:The raw comment may not\ndirectly meet our data requirements, which can be empty,\ncontain irrelevant descriptions to the revision, or not de-\nscribe the editing behavior (e.g.only describes the deﬁ-\nciencies of source text). We take the following steps to\nenhance the quality of the instructions: (i) Extract revision\nrecords where the edit summary starts with a verb de-\nscribing an edit intent (e.g.“make the text easier to read”);\n(ii) Fine-tune PaLM2-XXS to generate comments from\n<source>-<target> text pairs as well as learn to\ncontrol the length and speciﬁcity of the instructions. We\nuse the heuristic that if a comment mentions a word from\nthe edit then it is a detailed instruction. (iii) Generate\ndetailed comments for all <source>-<target> pairs\nusing the model trained in the previous steps.\nSynthetic Instruction Dataset The Wiki instruction\ndataset is limited by the available edit types found on\nWikipedia. To collect a more diverse and representative in-\nstruction dataset, we ﬁrst use chain-of-thoughts prompting\nand few-shot prompting to generate instructions, and then\ngenerate the target text from a general purpose LLM model:\n\u000fInstruction generation: By applying a 3-shot chain-of-\nthought (CoT) prompting method to text inputs from any\ndomain (see Figure 1), we can leverage the knowledge ac-\nquired by the PaLM2-L during pre-training. This enables\nthe LLM to produce more diverse instructions beyond\nWiki edit types. CoT contains two QA stages: Text de-\nscription (answering “What kind of text is the following”)\nand Instruction generation(answering “What is a rel-\nevant writing prompt or edit instruction for text”). The\nanswer to the second question is the generated instruction.\n\u000fTarget generation:Given the source text and the gener-\nated instructions, we generate the model outputs with a\ngeneral purpose instruction tuned LLM (text-bison-0012)\nand ﬁlter them in a post-processing step (see Section\nHeuristic Post-Processing).\nHeuristic Post-Processing In order to improve the qual-\nity of the instruction datasets, we do the following post-\nprocessing: (1) In general, rewriting should preserve the\noverall meaning of the text, and thus, we employ Natural\nLanguage Inference (NLI; See Section Baselines) to detect\n“hallucinations” from the source to the target text and vice\nversa. If the “hallucination” is in the target text and ﬁxable\nusing simple heuristic rules, we remove the “hallucination”\n2https://cloud.google.com/vertex-ai/docs/generative-ai/model-\nreference/text\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18972\nSize Inst Len Src Len Tar Len Len Ratio Edit Dist Edit Ratio Rouge1\nAll 24384 6 :85 118 :86 141:09 1 :20 115 :44 0 :97 60 :95\nWiki 18196 7 :38 112 :17 98 :39 0 :90 77 :69 0 :70 64 :77\nSynthetic 6188 5 :30 138 :54 266:63 2 :10 226 :43 1 :78 49 :72\nTable 2: RewriteLM Training Data Statistics: This table includes statistics for the entire training set (“All”), data derived from\nWikipedia (“Wiki”, Section Wiki Instruction Dataset), and synthetic data generated from large language models (“Synthetic”,\nSection Synthetic Instruction Dataset). Metrics are the number of examples (Size); the average number of words in instructions\n(Inst Len), source texts (Src Len), and target texts (Tar Len); the average length fraction (Len Ratio) between the target and\nsource texts; the average edit distance (Edit Dist) between source and target; the ratio of edit distance to source text length (Edit\nRatio); and the Rouge1 score comparing source and target texts. All measurements are conducted at the word-level.\nfrom the target text and keep the instance. (2) For any other\ndetected “hallucination”, we ﬁlter the instance. (3) If the dif-\nference between the source and target texts is unexpectedly\nsmall, we also ﬁlter the instance.\nModeling\nSupervised Fine-Tuning (SFT). Given a pretrained lan-\nguage model Mbase, we ﬁne-tune it using the instruction tun-\ning dataset discussed in Section Training Dataset, producing\na model MSFT. We employ the decoder-only Transformer ar-\nchitecture for our experiments, details of which are explained\nin Section Experiments and Results. For both models, the\ninput is formed by concatenating <instruction> and\n<source> with a newline, while the output is <target>.\nReward Modeling (RM)Firstly, we sample prompt data\n(instruction and source) from our training dataset, and sam-\nple outputs from the pretrained language model Mbase and\nﬁnetuned model MSFT.\nSecondly, in constrast to InstructGPT, where human label-\ners are used to rank the outputs, we develop a new approach\nto rank model outputs without any human effort for collecting\npreference data for reward model training. We deﬁne a new\nscoring function to measure the quality of the rewrite trans-\nformation through several heuristics (see Section Heuristic\nPost-Processing). For an input output pair px; tq, the quality\nscore is deﬁned as follows:\nQpx;tq\u0010\n$\n''\n'\n'\n'''\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'\n&\n''\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'\n'%\n0;\nif EditRatiopx;tq  aor\nNLIpx;tq  bor\nNLIpt;xq  cor\npIshorten& LenRatiopx;tq¡ d1q\n0;\nif EditRatiopx;tq  aor\nNLIpx;tq  bor\nNLIpt;xq  cor\npIelaborate& LenRatiopx;tq  d2q\n1; otherwise\n(1)\n, where a \u00101:2, b \u00100:7, c \u00100:7, d1 \u00100:6, and d2 \u00102.\nIshorten means a shorten task, and Ielaborate means a ex-\npanding or elaboration task. These are decided simply based\non keyword matches. If a px; tqpair fails to meet any of the\nheuristic rules, it is assigned a quality score of 0; otherwise,\na score of 1 is given. If the model outputs from the same\nprompt are all good or are all bad, we will discard the exam-\nple. If some outputs are good and some are bad, we will select\nthe top-ranked ones (based on probability in top-p or top-k\nsampling) from good outputs and bad outputs respectively.\nFinally, we ﬁnetune a pre-trained reward modelRbase using\nthe comparison data collected above. This is different from\nInstructGPT (Ouyang et al. 2022), which trains the reward\nmodel from scratch after obtaining a supervised tuned model.\nSince Rbase is pretrained on general-purpose preference data\nand not specialized for open-ended rewriting, additional ﬁne-\ntuning is crucial.\nThe reward model, denoted as r\u0012, employs a transformer-\nbased architecture with a linear regression output layer. It\nis trained with tgood and tbad which represent the good and\nbad targets respectively. The training loss function for the re-\nward model is the entropy of the normalized score difference\nbetween the good and bad targets.\nlossp\u0012q\u0010\u0001 E\npx;tgood;tbadq\u0012D\nlog\n\u0001\n\u001b\n\u0000\nr\u0012px;tgoodq\u0001r\u0012px;tbadq\n\b\t\n(2)\nReinforcement Learning. Finally, we further optimize\nthe supervised ﬁne-tuned model MSFT by employing rein-\nforcement learning (Ouyang et al. 2022), guided by the scores\nprovided by the ﬁne-tuned reward model RSFT. This process\nresults in the ﬁnal model, Mrewrite.\nEvaluation Framework\nOpenRewriteEval — A New Benchmark for Text\nRewriting\nTo facilitate the evaluation of open-ended rewriting, we have\ncurated a new dataset called O PEN REWRITE EVAL, which\nfocuses on open instructions, long-form text, and large ed-\nits. Each example in the dataset consists of a three-tuple\np<instruction>; <source>; <target>q.\nOPEN REWRITE EVAL consists of six datasets DFormality,\nDParaphrase, DShorten, DElaborate, DMixedWiki and DMixedOthers.\nFor DFormality, DParaphrase, and DShorten, we use a ﬁxed set\nof instruction. For the rest of the datasets, we asked human\nannotators to attach appropriate instructions to each source\ntext and then rewrite them accordingly. Table 3 provides in-\nformation on the size of each task and the average word-level\nlengths of instructions, source text, and target text. O PEN -\nREWRITE EVAL captures how people naturally rewrite, which\nusually include changes across multiple sentences. This sets\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18973\nNLI\nSize Inst Len Src Len Tar Len Len Ratio Edit Dist Edit Ratio Rouge1 src-tar tar-src\nAll 1629 6.40 132.71 143.53 1.12 90.79 0.71 67.19 0.94 0.95\nDFormality 200 5.10 114.73 119.23 1.12 62.51 0.56 68.93 0.87 0.98\nDParaphrase 102 3 211.02 195.97 1 121.2 0.54 68.57 1 1\nDShorten 102 4.49 211.02 165.68 0.8 72.2 0.37 79.26 1 1\nDElaborate 102 8.64 211.02 378.47 2.07 234.33 1.34 56.52 0.92 1\nDMixedWiki 606 7.54 103.3 97.57 0.98 65.36 0.64 71.86 0.94 0.92\nDMixedOthers 517 6.17 127.8 145.74 1.18 100.89 0.82 60.51 0.95 0.95\nTable 3: Statistics of OPEN REWRITE EVAL the number of examples (Size); the average number of words in instructions (Inst\nLen), source texts (Src Len), and target texts (Tar Len); the average length fraction (Len Ratio) between the target and source\ntexts; the average edit distance (Edit Dist) between source and target; the ratio of edit distance to source text length (Edit Ratio);\nand the Rouge1 score comparing source and target texts for the full set and the subtasks. All are measured at the word-level. NLI\n(src-tar, tar-src) are the NLI scores between the source text and the gold reference.\nus apart from existing benchmarks such as EditEval (Dwivedi-\nYu et al. 2022), which are limited to rewrites within single\nsentences. See Edit ratio (dividing the edit distance by the\nlength of the source text): OPEN REWRITE EVAL (0.37-1.34;\nsee Table 3) vs EditEval (0.17-0.59 ). Appendix B Human\nRewrite Guideline provides detailed guidelines for the rewrite\nannotations.\nAutomatic Evaluation Metrics\nWe employ various metrics to evaluate the model’s perfor-\nmance including\n\u000fNLI (Bowman et al. 2015) andReversed NLI(i.e.reverse\nthe premise and the hypotheses) score over the source-\nprediction pair. NLI and Reversed NLI scores illustrate\nthe model prediction’s content presentation and factuality\nquality. We use the off-the-shelf NLI predictor introduced\nby (Honovich et al. 2022).\n\u000fEdit Distance Raito (Edit Ratio). Edit distance (Ristad\nand Yianilos 1998) measures the word-level textural dif-\nference between two pieces of text. We report the rela-\ntive edit distance between the prediction and source text,\ni.e.dividing the edit distance by the length of the source\ntext. The edit ratio represents the proportion of the source\ntext that has been modiﬁed. It is undesirable if the edit\ndistance is small because this indicates the prediction is\nprimarily identical to the source text. Ideally, we expect\nto see this value to be neither excessively high (indicating\nthe entire content has been changed) nor excessively low\n(indicating that only minor rewriting occurred thereby\ndiminishing the perceived effectiveness of the system).\n\u000fSARI (Xu et al. 2016) is an n-gram based metric measures\nhow a close a prediction is relative to the source text and\nthe reference text by rewarding words added, kept, or\ndeleted. SARI computes the arithmetic mean of n-gram\nF1-scores for each of the three operations.\n\u000fGLEU (Napoles et al. 2015) measures the precision of the\nn-grams in the model’s prediction that match the reference.\nIt is a variant of BLEU (Papineni et al. 2002). GLEU is\ncustomized to penalize only the changed n-grams in the\ntargets, as unmodiﬁed words do not necessarily need to\nbe penalized in the rewriting task.\n\u000fUpdate-ROUGE (Updated-R)(Iv et al. 2022) measures\nthe recall of n-grams between the model’s prediction\nand the references. It is a modiﬁed version of ROUGE\n(Lin and Hovy 2003). Updated-R speciﬁcally computes\nROUGE-L on the updated sentences rather than the full\ntext.\nWhen evaluating quality, it is desirable to have a higher\nvalue of NLI. Additionally, a higher Edit Ratio within a\nreasonable range is preferred. However, it’s important to note\nthat considering these metrics independently is insufﬁcient.\nIn some cases, predictions with a low edit ratio may still have\nhigh NLI scores. Conversely, a large edit ratio can contain\nhallucinations if the NLI scores are low. Additionally, higher\nvalues of SARI, GLEU, and Update-ROUGE indicate that\nthe predictions are more similar to the gold reference text.\nHuman Evaluation\nWe conduct human evaluation on randomly selected 80 ex-\namples from the OPEN REWRITE EVAL dataset with ﬁve lan-\nguage experts. The rating use a 3-point Likert scale (0-Bad,\n1-Medium, or 2-Good) for the following features: 1) Instruc-\ntion Success:whether the rewrite accurately follows the in-\nstruction provided. 2) Content Preservation:whether the\nrewritten text preserves the essential content and meaning\nof the source text, regardless of its writing style or quality.\n3) Factuality: Checks the accuracy and truthfulness of the\nanswer’s content. 4)Coherency: whether the rewritten text\nis easy to understand, non-ambiguous, and logically coher-\nent when read by itself (without checking against the source\ntext). 5) Fluency: Examines the clarity, grammar, and style\nof the written answer. The detailed rating guideline is in\nAppendix C Human Rating Guideline.\nExperiments and Results\nThis section provides an overview of our experimental set-\ntings, baselines, and result analysis. Detailed information\nabout the hyperparameters can be found in Appendix D\nHyper-parameter Setting.\nBaselines\nWe use the following baseline models for quality comparison\nin the later sections:\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18974\nEdit Ratio NLI (s-p) NLI (p-s) SARI GLEU Update-R\nPretrained LLMs\nPaLM (Chowdhery et al. 2022) 62B 0:31 0 :25 0 :11 28 :24 0 :74 11 :99\nPaLM 2 (Passos et al. 2023) S 1.22 0:63 0 :37 28 :62 0 :48 8 :14\nLLaMA (Touvron et al. 2023) 65B 0:71 0 :83 0 :83 27 :98 2 :10 21 :35\nInstruction-Tuned LLMs\nAlpaca (Taori et al. 2023) 13B 0:11 0 :90 0 :85 36 :12 6 :81 34 :88\nAlpaca-PaLM 2 S 0:12 0 :9 0 :84 38 :51 8 :31 36 :56\nVicuna (Chiang et al. 2023) 13B 0:23 0 :89 0 :77 39 :05 6 :84 33 :31\nFlan-PaLM (Chung et al. 2022) 62B 0:12 0 :58 0 :42 24 :52 1 :87 6 :23\nInsGPT (text davinci 001) - 0:09 0 :66 0 :61 27 :17 3 :72 18 :69\nChatGPT (GPT 3.5 Turbo) - 0:13 0 :95 0 :87 40 :04 8 :47 37 :78\nRewriteLMs\nRewrite-PaLM 62B 0:14 0 :88 0 :76 37 :02 7 :40 36 :68\nRewrite-PaLM 2 S 0:25 0 :93 0 :79 40 :92 9.64 39:36\nRewrite-RL-PaLM 2 S 0:27 0 :94 0 :81 40.97 9:43 39 :36\nRewrite-RLr/w-PaLM 2 S 0:29 0.96 0.87 40:66 9.64 40.10\nTable 4: Model Performance on OPEN REWRITE EVAL. Edit distance ratio (Edit Ratio) between the model prediction and the\nsource text; NLI score with source as premise and model prediction as hypothesis (NLI s-p) and vice versa (NLI p-s); SARI,\nGLEU and Updated-ROUGE (Updated-R) between the gold reference and the model prediction are reported here.\nJFL TRK AST WNC FRU WFI\nSARI GLEU SARI SARI SARI SARI Update-R SARI Updated-R\nCopy - 26.7 40.5 26.3 20.7 31.9 29.8 0 33.6 -\nTk (Wang et al. 2022b) 3B 31.8 39 32.8 29.9 31.3 12.6 3.6 1.3 4.5\nT0 (Sanh et al. 2022) 3B 42 38.8 34.4 32.3 22.3 14.2 9.6 5.1 16.3\nT0++ (Sanh et al. 2022) 11B 34.7 43.2 32.9 28.2 29.3 12.6 3.7 4.4 8.1\nPEER-3 (Schick et al. 2022) 3B 55.5 54.3 32.5 30.5 53.3 39.1 30.9 34.4 18.7\nPEER-11 (Schick et al. 2022) 11B 55.8 54.3 32.1 29.5 54.5 39.6 31.4 34.9 20.4\nOPT (Zhang et al. 2022b) 175B 47.3 47.5 32.6 31.8 31.2 35.9 27.3 26.7 11.2\nGPT-3 (Brown et al. 2020) 175B 50.3 51.8 33 30.5 31.7 36 21.5 27.2 10.6\nInsGPT (Ouyang et al. 2022) 175B 61.8 59.3 38.8 38 35.4 36.3 24.7 23.6 16.1\nPaLM 2 (Passos et al. 2023) S 36.07 2.18 34.32 35.92 25.2 24.28 26.39 11.41 20.42\nRewrite-PaLM 2 (Ours) S 56.95 40.38 40.81 42.11 37.11 37.51 53.54 26.55 47.06\nRewrite-RLr/w-PaLM 2 (Ours) S 55 22.89 40.87 41.71 37.81 38.56 53.93 29.25 49.53\nTable 5: Model Performance on EditEval (Dwivedi-Yu et al. 2022).\n\u000fPaLM (Chowdhery et al. 2022) is a large, densely acti-\nvated transformer-based language model that can generate\ntext in an open-ended fashion.\n\u000fPaLM 2(Passos et al. 2023), is an advanced language\nmodel which surpasses its predecessor PaLM in terms\nof multilingual and reasoning abilities while being more\ncomputationally efﬁcient. It is a Transformer-based model\nthat underwent training using a blend of objectives. In this\npaper, we employ PaLM 2-S. This “S” size is comparable\nto LLaMA/Alpaca/Vicuna-13B, which is why we opted to\ntrain using it rather than the largest PaLM 2. Note that the\nspeciﬁc number of parameters for the PaLM 2 series has\nnot been made public. Instead, the PaLM 2 Tech Report\nuses T-shirt sizes to represent model sizes, ranging from\nXXS to L. We follow its notations.\n\u000fLLaMA (Touvron et al. 2023) is an efﬁcient, open-source\nfoundation language model.\n\u000fFlan-PaLM (Chung et al. 2022) is ﬁne-tuned on a large\nvariety of tasks and chain-of-thought data using PaLM as\nthe base model.\n\u000fAlpaca (Taori et al. 2023) is a language model that is ﬁne-\ntuned from LLaMA using 52,000 instruction-following\ndemonstrations.\n\u000fAlpaca-PaLM: We ﬁne-tune the PaLM model on Alpaca\ninstruction-following datasets.\n\u000fVicuna (Chiang et al. 2023) is an open-source chatbot\ntrained by ﬁne-tuning LLaMA on user-shared conversa-\ntions collected from ShareGPT3.\n\u000fChatGPT (gpt-3.5-turbo)4 and InsGPT (text-davinci-\n001) (Ouyang et al. 2022) are members of the GPT family,\ndeveloped by OpenAI.\nWe follow the same zero-shot prompt setting for all the base-\nline models as Schick et al. (2022). The pre-trained models\nwithout any instruction tuning generally exhibit slightly lower\nperformance in following instructions compared to the in-\nstruction tuned models under zero-shot scenario.\n3https://sharegpt.com/\n4https://openai.com/chatgpt\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18975\nInstruction\nSuccess\nContent\nPreservation\nFactuality Coherence Fluency A VG\nAgreement 0.784 0.781 0.769 0.933 0.804 0.814\nHuman Expert 1.833 1.949 1.985 1.99 1.99 1.949\nAlpaca 13B 1.441 1.754 1.934 1.962 1.977 1.814\nAlpaca-PaLM 2 1.489 1.719 1.99 2 2 1.839\nChatGPT 1.478 1.775 1.959 1.962 1.975 1.83\nRewrite-PaLM 2 1.641 1.777 1.927 2 2 1.869\nRewrite-RLr/w-PaLM 2 1.648 1.835 1.959 1.985 2 1.886\nTable 6: Human Evaluation Results.\nResults on OPEN REWRITE EVAL Benchmark\nThe automatic evaluation results for the OPEN REWRITE E-\nVAL dataset are presented in Table 4. Rewrite-PaLM and\nRewrite-PaLM 2 are supervised ﬁne-tuned versions (as dis-\ncussed in Section Modeling) based on PaLM, and PaLM 2, re-\nspectively. Rewrite-RL-PaLM 2 and Rewrite-RLr/w-PaLM 2\nare reinforcement learning models tuned over Rewrite-PaLM\n2. The reward model from the former does not use our syn-\nthetic preference dataset (as discussed in Section Modeling),\nwhereas the reward model from the latter incorporates it.\nAs shown in Table 4, our RL tuned model Rewrite-RLr/w-\nPaLM 2 has the highest scores in almost all the metrics (i.e.,\nNLI scores, SARI, GLEU, and Update-R). This indicates\nthat our model is good at generating outputs faithful to the\noriginal input, while other models might generate more “hal-\nlucinations”. For edit ratio, Rewrite-RL r/w-PaLM 2 has a\nbetter score than all the models except PaLM 2. Pre-trained\nmodels such as PaLM 2 without any instruction tuning are\nprone to generating “hallucinations”, resulting in a signiﬁ-\ncantly high edit ratio score (i.e.1:22). Therefore, our model\nis good at keeping all the essential content and meaning of\nthe source text, while also being able to rewrite with varied\nlanguage and structures. Given that Rewrite-RLr/w-PaLM 2\nconsistently outperforms Rewrite-RL-PaLM 2 across nearly\nall metrics, this strongly suggests the effectiveness and value\nof employing synthetic preference data.\nResults on EditEval\nWe also evaluated the performance of our models using the\npublicly available sentence-level rewrite benchmark EditE-\nval5 (Dwivedi-Yu et al. 2022). This benchmark comprises\nvarious datasets that cover different language tasks. Speciﬁ-\ncally, JFL (Napoles, Sakaguchi, and Tetreault 2017) focuses\non language ﬂuency; TRK (Xu et al. 2016) and AST (Alva-\nManchego et al. 2020) target at sentence simpliﬁcation; WNC\n(Pryzant et al. 2020) addresses text neutralization; FRU (Iv\net al. 2022)) and WFI (Petroni et al. 2022) involve updating\ninformation that requires external references.\nWe only report the results on EditEval datasets that con-\ntaining more than 100 test examples (see Table 5). The re-\nsults of LLM baselines and the Copy baseline (which treats\nthe source text as the prediction) are taken directly from the\nEditEval paper (Dwivedi-Yu et al. 2022). We can observe that\n5https://github.com/facebookresearch/EditEval\nthe zero-shot performance of Rewrite-PaLM 2 and Rewrite-\nRLr/w-PaLM 2 is mostly on par with or better than the best\nbaselines (i.e.PEER-11 and InsGPT). While our model is\nspeciﬁcally designed for long-form text rewriting, it does\nnot sacriﬁce its capability to handle sentence-level rewriting\ntasks.\nResults on Human Evaluation\nThe human evaluation results, detailed in Table 6, reveal no-\ntable insights. The inter-annotator agreements, quantiﬁed us-\ning the Fleiss kappa coefﬁcient (Fleiss 1971), underscore the\nreliability of the evaluations. Notably, Rewrite-PaLM 2 and\nRewrite-RWr/w-PaLM 2 demonstrate superior performance\nover Alpaca, Alpaca-PaLM 2, and ChatGPT in instruction\nsuccess and content preservation. This alignment with the au-\ntomatic evaluation metrics underscores the efﬁcacy of these\nmodels in adhering to given instructions while maintaining\nthe integrity of the original content. In terms of coherence\nand ﬂuency, all models, including the human rewrites, scored\nabove 1.96, indicative of their ability to generate clear, un-\nambiguous, and logically coherent outputs. Such high scores\nsuggest that these models’ outputs are not only understand-\nable but also align closely with human-level language proﬁ-\nciency. Human expertise still prevails in aspects of instruction\nsuccess and content preservation, suggesting room for further\nimprovement in model performance to reach human-level\nproﬁciency in rewriting tasks.\nConclusion\nWe introduce a novel benchmark for text rewriting with a\nfocus on cross-sentence rewrites, covering a wide variety of\nrewriting types expressed through natural language instruc-\ntions. We present new data generation and training strategies\nto better teach LLMs to perform rewriting tasks. Our model,\nRewriteLM, achieves the state-of-the-art results on O PEN -\nREWRITE EVAL benchmark.\nA OPEN REWRITE EVAL Data\nData Sources. The source texts for the DFormality, DParaphrase,\nDShorten, and DElaborate categories are from various datasets,\nincluding Multi-News (Fabbri et al. 2019), Wikipedia (Guo\net al. 2020), PG-19 book (Rae et al. 2019), BIG-\nPATENT (Sharma, Li, and Wang 2019), BillSum (Kornilova\nand Eidelman 2019), government reports (Huang et al. 2021),\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18976\nscientiﬁc papers (Cohan et al. 2018), Enron email (Zhang\nand Tetreault 2019), Reddit (Hamilton, Ying, and Leskovec\n2017), IMDB, and Yelp reviews (Maas et al. 2011; Zhang,\nZhao, and LeCun 2015). The DMixedWiki have the source texts\nfrom Wikipedia (Guo et al. 2020) and DMixedOthers contains\nC4 (Raffel et al. 2020) and human written ones.\nB Human Rewrite Guideline\n\u000fRaters align source text to the instruction, and then rewrite.\nIf the source text is already met the instruction, for exam-\nple, “make it more formal”, then treat the source text as\ntarget text and rewrite less formal (put at source side).\n\u000fEnsure (1) the content preservation between source and\nrewrite; (2) maximum word change; and (3) source and\ntarget texts are well aligned with instruction. For example,\nif the instruction is to “make it more polite”, then ensure\nthat the target text is much more polite than the source\ntext.\n\u000fElaborate: the rewrite matches source text’s tone and for-\nmat. Add more relevant information and ideas, but do not\nmake up facts.\n\u000fRephrase: the rewrite matches source text’s tone, ver-\nbosity, format and max changes to existing words.\n\u000fShorten: the rewrite matches source text’s tone and for-\nmat, trims unnecessary words, simpliﬁes sentences, makes\nthem more concise.\n\u000fInformal-to-Formal: Rewrite the given paragraph so that\nit is more formal in style. To make the text more formal,\ntry to: (1) Replace informal words associated with chatty\nspoken styles (such as slang and contractions) with more\nformal vocabulary. (2) Make the text impersonal: avoid re-\nferring directly to the author(s) or reader(s), or expressing\nsubjective opinions. (3) Use strictly standard grammatical\nforms.\n\u000fFormal-to-Informal: Rewrite the given paragraph so that\nit is less formal in style. To make your writing less formal,\ntry to: (1) Replace long or uncommon words with relaxed,\neveryday terms. You may include contractions (such as\nchanging “cannot” to “can’t” if it helps the text ﬂow bet-\nter. (2) Where appropriate, identify the author and the\nreader to make the text more relatable. (For example, you\nmight be able to change “It is believed that...” to “I think\ntha...”) (3) If a sentence is very long or stifﬂy phrased, try\nbreaking it up or rearranging it, even if this doesn’t ﬁt the\nstrictest rules of standard grammar.\nC Human Rating Guideline\nInstruction Success: The ability of the model to adhere to\nthe given instruction is evaluated in this criterion. It is:\n\u000fScore 2 (Fully/Mostly Followed): if the model output en-\ntirely adheres to the provided instructions, demonstrating\na clear understanding and implementation of the given\ntask. Or the output mostly adheres to the instructions, with\nminor deviations or errors.\n\u000fScore 1 (Partially Followed): if the model output shows\nsome adherence to the instructions but deviates signiﬁ-\ncantly in certain aspects or fails to completely implement\nthem, leading to partial fulﬁllment of the task.\n\u000fScore 0 (Not Followed/Mostly Ignored): if the model out-\nput largely ignores the provided instructions, making it\nevident that the task has not been understood or imple-\nmented properly. Or despite some slight adherence, the\noutput largely deviates from the intended task as per the\ninstructions.\nContent Preservation: The essential content and meaning\nof the reference is preserved in the rewrite, independent of\nits style or the quality of the writing. It is:\n\u000fScore 2 (Fully/Mostly Preserved): if the rewrite is an\nexcellent representation of the content in the reference,\nwith no omissions. Or the rewrite mostly matches the\ncontent of the reference, but one or two elements of the\nmeaning have been lost.\n\u000fScore 1 (Half Preserved): if some of the content is present\nin the rewrite but approximately the same amount is miss-\ning.\n\u000fScore 0 (Not Preserved/Mostly Lost): if the rewrite is\nentirely unrelated to the reference. Or despite some slight\nsimilarities, the rewrite is hard to recognize as being based\non the reference.\nFactuality: The rewrite only provides as much information\nas is present in the reference, without adding anything. It\nis not misleading and does not make any false statements\n(unless these were also present in the reference).\n\u000fScore 2 (Fully/Mostly faithful): Everything in the rewrite\nis grounded in the reference. Or the rewrite says some-\nthing that is not mentioned in the reference or contradicts\nthe reference, but it is not an important addition or it is\nhard to say whether the statement is true or false.\n\u000fScore 1 (Partly faithful): The rewrite adds signiﬁcant fac-\ntual statements to the reference. These may be inaccurate\nor otherwise not based on the reference, but do not entirely\nundermine the faithfulness of the rewrite as a whole.\n\u000fScore 0 (Not/Slightly faithful): The rewrite is mostly\nwrong, made up, or contradicts what is in the reference\ntext.\nCoherence: The rewrite is coherent if, when read by itself\n(without checking against the reference), it’s easy to under-\nstand, non-ambiguous, and logically coherent. On the other\nhand, the rewrite is not coherent if it’s difﬁcult to understand\nwhat it is trying to say.\n\u000fScore 2 (Good): The whole of the rewrite is mostly ﬂuent\nand easy to read, independent of any reference content.\nSome speciﬁc parts of the rewrite could be more natu-\nrally phrased, but overall it is fairly clear and easy to\nunderstand.\n\u000fScore 1 (Neutral): The rewrite is comprehensible, though\nnot on the ﬁrst read or only with some effort.\n\u000fScore 0 (Bad): The rewrite is very hard to understand,\nexcept by checking against the reference.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18977\nFluency: The rewrite is considered ﬂuent if it follows all\nthe rules of its language, including spelling, grammar and\npunctuation. It reads as though it was written by someone\nwho speaks English as their ﬁrst language.\n\u000fScore 2 (Flawless/Good): The rewrite is grammatically\ncorrect, contains no spelling errors, and follows all other\nlinguistic rules. An average English speaker would not\nsee anything that looks “wrong”. Or there are just one\nor two linguistic errors or non-standard formulations, but\nnothing serious.\n\u000fScore 1 (Flawed): The rewrite contains a number of er-\nrors of different types, but these errors, even when taken\ntogether, do not make the text signiﬁcantly harder to un-\nderstand.\n\u000fScore 0 (Poor): The rewrite contains a large number of\nerrors, so that some sections of the text are hard to under-\nstand, but other parts are more manageable.\nD Hyper-parameter Setting\nWe use 64 Tensor Processing Units (TPU) V3 chips for ﬁne-\ntuning. The batch size is 32, and the maximum training step\nis 5000. We use the Adafactor optimizer (Shazeer and Stern\n2018) with a learning rate of0:003. Both the input and output\nsequence lengths are set to 1024 tokens. The training dropout\nrate is 0:1. During inference, the temperature is set to 0:5,\nand the top-K value is 40.\nAcknowledgments\nThe authors would like to thank Tony Mak, Chang Li, Abhan-\nshu Sharma, Matt Shariﬁ, Hassan Mansoor, Daniel Kim, Reut\nAharony and Nevan Wichers for their insightful discussions\nand support.\nReferences\nAlva-Manchego, F.; Martin, L.; Bordes, A.; Scarton, C.;\nSagot, B.; and Specia, L. 2020. ASSET: A Dataset for Tuning\nand Evaluation of Sentence Simpliﬁcation Models with Mul-\ntiple Rewriting Transformations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 4668–4679. Online: Association for Computational\nLinguistics.\nBowman, S.; Angeli, G.; Potts, C.; and Manning, C. D. 2015.\nA large annotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, 632–642.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33: 1877–\n1901.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language mod-\neling with pathways. arXiv preprint arXiv:2204.02311.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-ﬁnetuned language models. arXiv\npreprint arXiv:2210.11416.\nCohan, A.; Dernoncourt, F.; Kim, D. S.; Bui, T.; Kim, S.;\nChang, W.; and Goharian, N. 2018. A Discourse-Aware\nAttention Model for Abstractive Summarization of Long\nDocuments. In Proceedings of NAACL-HLT, 615–621.\nDwivedi-Yu, J.; Schick, T.; Jiang, Z.; Lomeli, M.; Lewis, P.;\nIzacard, G.; Grave, E.; Riedel, S.; and Petroni, F. 2022. EditE-\nval: An Instruction-Based Benchmark for Text Improvements.\narXiv.\nFabbri, A. R.; Li, I.; She, T.; Li, S.; and Radev, D. 2019.\nMulti-News: A Large-Scale Multi-Document Summarization\nDataset and Abstractive Hierarchical Model. In Proceedings\nof the 57th Annual Meeting of the Association for Computa-\ntional Linguistics, 1074–1084.\nFaltings, F.; Galley, M.; Hintz, G.; Brockett, C.; Quirk, C.;\nGao, J.; and Dolan, B. 2020. Text editing by command.arXiv\npreprint arXiv:2010.12826.\nFleiss, J. L. 1971. Measuring nominal scale agreement among\nmany raters. Psychological bulletin, 76(5): 378.\nGuo, M.; Dai, Z.; Vrandeˇci´c, D.; and Al-Rfou, R. 2020. Wiki-\n40b: Multilingual language model dataset. In Proceedings\nof the 12th Language Resources and Evaluation Conference,\n2440–2452.\nHamilton, W.; Ying, Z.; and Leskovec, J. 2017. Inductive\nrepresentation learning on large graphs. Advances in neural\ninformation processing systems, 30.\nHe, J.; Gu, J.; Shen, J.; and Ranzato, M. 2019. Revisiting\nself-training for neural sequence generation. arXiv preprint\narXiv:1909.13788.\nHonovich, O.; Aharoni, R.; Herzig, J.; Taitelbaum, H.; Kuk-\nliansy, D.; Cohen, V .; Scialom, T.; Szpektor, I.; Hassidim,\nA.; and Matias, Y . 2022. TRUE: Re-evaluating Factual Con-\nsistency Evaluation. In Proceedings of the Second DialDoc\nWorkshop on Document-grounded Dialogue and Conversa-\ntional Question Answering, 161–175.\nHuang, J.; Gu, S. S.; Hou, L.; Wu, Y .; Wang, X.; Yu, H.; and\nHan, J. 2022. Large language models can self-improve.arXiv\npreprint arXiv:2210.11610.\nHuang, L.; Cao, S.; Parulian, N.; Ji, H.; and Wang, L. 2021.\nEfﬁcient Attentions for Long Document Summarization. In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 1419–1436.\nIv, R.; Passos, A.; Singh, S.; and Chang, M.-W. 2022. FRUIT:\nFaithfully reﬂecting updated information in text. In Proceed-\nings of the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, 3670–3686.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\nBang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of hal-\nlucination in natural language generation. ACM Computing\nSurveys, 55(12): 1–38.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18978\nKornilova, A.; and Eidelman, V . 2019. BillSum: A Corpus\nfor Automatic Summarization of US Legislation. EMNLP-\nIJCNLP 2019, 48.\nLin, C.-Y .; and Hovy, E. 2003. Automatic evaluation of sum-\nmaries using n-gram co-occurrence statistics. In Proceedings\nof the 2003 human language technology conference of the\nNorth American chapter of the association for computational\nlinguistics, 150–157.\nMaas, A.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y .; and\nPotts, C. 2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th annual meeting of the association\nfor computational linguistics: Human language technologies,\n142–150.\nMallinson, J.; Adamek, J.; Malmi, E.; and Severyn, A. 2022.\nEdit5: Semi-autoregressive text-editing with t5 warm-start.\narXiv preprint arXiv:2205.12209.\nMay, P. 2021. Machine translated multilingual STS bench-\nmark dataset.\nNapoles, C.; Sakaguchi, K.; Post, M.; and Tetreault, J. 2015.\nGround truth for grammatical error correction metrics. In\nProceedings of the 53rd Annual Meeting of the Associa-\ntion for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume\n2: Short Papers), 588–593.\nNapoles, C.; Sakaguchi, K.; and Tetreault, J. 2017. JFLEG:\nA Fluency Corpus and Benchmark for Grammatical Error\nCorrection. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computational Lin-\nguistics: Volume 2, Short Papers, 229–234. Valencia, Spain:\nAssociation for Computational Linguistics.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al.\n2022. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing\nSystems, 35: 27730–27744.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nPassos, A.; Dai, A.; Richter, B.; Choquette, C.; Sohn, D.;\nSo, D.; Lepikhin, D. D.; Taropa, E.; Ni, E.; Moreira, E.;\nMishra, G.; Yu, J.; Clark, J.; Meier-Hellstern, K.; Robinson,\nK.; V odrahalli, K.; Omernick, M.; Krikun, M.; Moussalem,\nM.; Johnson, M.; Du, N.; Firat, O.; Bailey, P.; Anil, R.; Ruder,\nS.; Shakeri, S.; Qiao, S.; Petrov, S.; Garcia, X.; Huang, Y .;\nTay, Y .; Cheng, Y .; Wu, Y .; Xu, Y .; Zhang, Y .; and Nado, Z.\n2023. PaLM 2 Technical Report. Technical report, Google\nResearch.\nPetroni, F.; Broscheit, S.; Piktus, A.; Lewis, P.; Izacard, G.;\nHosseini, L.; Dwivedi-Yu, J.; Lomeli, M.; Schick, T.; Mazar´e,\nP.; et al. 2022. Improving Wikipedia Veriﬁability with AI.\nPryzant, R.; Martinez, R. D.; Dass, N.; Kurohashi, S.; Ju-\nrafsky, D.; and Yang, D. 2020. Automatically neutralizing\nsubjective bias in text. In Proceedings of the aaai conference\non artiﬁcial intelligence, volume 34, 480–489.\nQin, C.; Zhang, A.; Zhang, Z.; Chen, J.; Yasunaga, M.;\nand Yang, D. 2023. Is ChatGPT a general-purpose nat-\nural language processing task solver? arXiv preprint\narXiv:2302.06476.\nRae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap,\nT. P. 2019. Compressive Transformers for Long-Range Se-\nquence Modelling. arXiv:1911.05507.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text trans-\nformer. The Journal of Machine Learning Research, 21(1):\n5485–5551.\nRao, S.; and Tetreault, J. 2018. Dear Sir or Madam, May\nI Introduce the GYAFC Dataset: Corpus, Benchmarks and\nMetrics for Formality Style Transfer. In Proceedings of\nthe 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), 129–140.\nReif, E.; Ippolito, D.; Yuan, A.; Coenen, A.; Callison-\nBurch, C.; and Wei, J. 2021. A recipe for arbitrary text\nstyle transfer with large language models. arXiv preprint\narXiv:2109.03910.\nRiley, P.; Constant, N.; Guo, M.; Kumar, G.; Uthus, D.;\nand Parekh, Z. 2020. TextSETTR: Few-shot text style\nextraction and tunable targeted restyling. arXiv preprint\narXiv:2010.03802.\nRistad, E. S.; and Yianilos, P. N. 1998. Learning String-\nEdit Distance. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 20(5): 522–532.\nSanh, V .; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika,\nL.; Alyafeai, Z.; Chafﬁn, A.; Stiegler, A.; Le Scao, T.; Raja,\nA.; et al. 2022. Multitask Prompted Training Enables Zero-\nShot Task Generalization. In ICLR 2022-Tenth International\nConference on Learning Representations.\nSchick, T.; Dwivedi-Yu, J.; Jiang, Z.; Petroni, F.; Lewis, P.;\nIzacard, G.; You, Q.; Nalmpantis, C.; Grave, E.; and Riedel,\nS. 2022. PEER: A Collaborative Language Model. arXiv\npreprint arXiv:2208.11663.\nSharma, E.; Li, C.; and Wang, L. 2019. BIGPATENT: A\nLarge-Scale Dataset for Abstractive and Coherent Summa-\nrization. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2204–2213.\nShazeer, N.; and Stern, M. 2018. Adafactor: Adaptive learn-\ning rates with sublinear memory cost. In International Con-\nference on Machine Learning, 4596–4604. PMLR.\nSiddique, A.; Oymak, S.; and Hristidis, V . 2020. Unsuper-\nvised paraphrasing via deep reinforcement learning. In Pro-\nceedings of the 26th ACM SIGKDD international conference\non knowledge discovery & data mining, 1800–1809.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stan-\nford Alpaca: An Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford alpaca.\nTikhonov, A.; Shibaev, V .; Nagaev, A.; Nugmanova, A.; and\nYamshchikov, I. P. 2019. Style Transfer for Texts: Retrain,\nReport Errors, Compare with Rewrites. In Proceedings of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18979\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), 3936–\n3945.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efﬁcient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2022a. Self-Instruct: Align-\ning Language Model with Self Generated Instructions. arXiv\npreprint arXiv:2212.10560.\nWang, Y .; Mishra, S.; Alipoormolabashi, P.; Kordi, Y .;\nMirzaei, A.; Arunkumar, A.; Ashok, A.; Dhanasekaran, A. S.;\nNaik, A.; Stap, D.; et al. 2022b. Benchmarking generalization\nvia in-context instructions on 1,600+ language tasks. arXiv\npreprint arXiv:2204.07705.\nXie, Q.; Luong, M.-T.; Hovy, E.; and Le, Q. V . 2020. Self-\ntraining with noisy student improves imagenet classiﬁcation.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 10687–10698.\nXu, W.; Napoles, C.; Pavlick, E.; Chen, Q.; and Callison-\nBurch, C. 2016. Optimizing Statistical Machine Translation\nfor Text Simpliﬁcation. Transactions of the Association for\nComputational Linguistics, 4: 401–415.\nXu, W.; Ritter, A.; Dolan, W. B.; Grishman, R.; and Cherry,\nC. 2012. Paraphrasing for style. In Proceedings of COLING\n2012, 2899–2914.\nZhang, H.; Song, H.; Li, S.; Zhou, M.; and Song, D.\n2022a. A survey of controllable text generation using\ntransformer-based pre-trained language models. arXiv\npreprint arXiv:2201.05337.\nZhang, R.; and Tetreault, J. 2019. This Email Could Save\nYour Life: Introducing the Task of Email Subject Line Gen-\neration. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 446–456.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen,\nS.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022b.\nOpt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068.\nZhang, X.; Zhao, J.; and LeCun, Y . 2015. Character-level\nconvolutional networks for text classiﬁcation. Advances in\nneural information processing systems, 28.\nZhang, Y .; Ge, T.; and Sun, X. 2020. Parallel data aug-\nmentation for formality style transfer. arXiv preprint\narXiv:2005.07522.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18980",
  "topic": "Rewriting",
  "concepts": [
    {
      "name": "Rewriting",
      "score": 0.8682241439819336
    },
    {
      "name": "Computer science",
      "score": 0.6819096803665161
    },
    {
      "name": "Language model",
      "score": 0.47663983702659607
    },
    {
      "name": "Programming language",
      "score": 0.45747044682502747
    },
    {
      "name": "Natural language processing",
      "score": 0.4108181595802307
    },
    {
      "name": "Linguistics",
      "score": 0.3657187819480896
    },
    {
      "name": "Philosophy",
      "score": 0.08306962251663208
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 16
}