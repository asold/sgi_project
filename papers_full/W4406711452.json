{
  "title": "Empowering PET imaging reporting with retrieval-augmented large language models and reading reports database: a pilot single center study",
  "url": "https://openalex.org/W4406711452",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4286980352",
      "name": "Choi, Hongyoon",
      "affiliations": [
        "Seoul National University Hospital",
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A1482880182",
      "name": "Lee Dong-joo",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Kang, Yeon-koo",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": null,
      "name": "Suh, Minseok",
      "affiliations": [
        "Seoul National University Hospital",
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4362608470",
    "https://openalex.org/W4379378720",
    "https://openalex.org/W4323652488",
    "https://openalex.org/W3024545783",
    "https://openalex.org/W4391487347",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4385287322",
    "https://openalex.org/W2946833416",
    "https://openalex.org/W4323066307",
    "https://openalex.org/W4283155959",
    "https://openalex.org/W4380355878",
    "https://openalex.org/W4391221150",
    "https://openalex.org/W4383302171",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4383896353",
    "https://openalex.org/W3090301213",
    "https://openalex.org/W3207171866"
  ],
  "abstract": null,
  "full_text": "ORIGINAL ARTICLE\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nhttps://doi.org/10.1007/s00259-025-07101-9\nIntroduction\nThe integration of Large Language Models (LLMs) into the \nclinical domain has heralded a new era in healthcare innova-\ntion, particularly in the realm of medical imaging reports [1, \n2]. LLMs, with their sophisticated zero-shot learning capa -\nbilities, have shown promise in parsing, summarizing, and \ngenerating complex medical texts, thereby enhancing the \nefficiency and accuracy of clinical documentation and deci-\nsion-making processes [3]. Their application extends across \nvarious specialties, aiming to revolutionize how healthcare \n \r Hongyoon Choi\nchy1000@snu.ac.kr\n1 Department of Nuclear Medicine, Seoul National University \nHospital, 101 Daehak-ro, Jongno-gu, Seoul  \n03080, Republic of Korea\n2 Department of Nuclear Medicine, Seoul National University \nCollege of Medicine, Seoul, Republic of Korea\n3 Portrai, Inc., Seoul, Republic of Korea\nAbstract\nPurpose The potential of Large Language Models (LLMs) in enhancing a variety of natural language tasks in clinical fields \nincludes medical imaging reporting. This pilot study examines the efficacy of a retrieval-augmented generation (RAG) LLM \nsystem considering zero-shot learning capability of LLMs, integrated with a comprehensive database of PET reading reports, \nin improving reference to prior reports and decision making.\nMethods We developed a custom LLM framework with retrieval capabilities, leveraging a database of over 10 years of PET \nimaging reports from a single center. The system uses vector space embedding to facilitate similarity-based retrieval. Queries \nprompt the system to generate context-based answers and identify similar cases or differential diagnoses. From routine clini-\ncal PET readings, experienced nuclear medicine physicians evaluated the performance of system in terms of the relevance \nof queried similar cases and the appropriateness score of suggested potential diagnoses.\nResults The system efficiently organized embedded vectors from PET reports, showing that imaging reports were accurately \nclustered within the embedded vector space according to the diagnosis or PET study type. Based on this system, a proof-of-\nconcept chatbot was developed and showed the framework’s potential in referencing reports of previous similar cases and \nidentifying exemplary cases for various purposes. From routine clinical PET readings, 84.1% of the cases retrieved relevant \nsimilar cases, as agreed upon by all three readers. Using the RAG system, the appropriateness score of the suggested poten-\ntial diagnoses was significantly better than that of the LLM without RAG. Additionally, it demonstrated the capability to \noffer differential diagnoses, leveraging the vast database to enhance the completeness and precision of generated reports.\nConclusion The integration of RAG LLM with a large database of PET imaging reports suggests the potential to support \nclinical practice of nuclear medicine imaging reading by various tasks of AI including finding similar cases and deriving \npotential diagnoses from them. This study underscores the potential of advanced AI tools in transforming medical imaging \nreporting practices.\nKeywords PET reports · Large language model · Retrieval-augmented generation · Artificial intelligence\nReceived: 14 October 2024 / Accepted: 17 January 2025 / Published online: 23 January 2025\n© The Author(s) 2025\nEmpowering PET imaging reporting with retrieval-augmented large \nlanguage models and reading reports database: a pilot single center \nstudy\nHongyoon Choi1,2,3  · Dongjoo Lee3 · Yeon-koo Kang1 · Minseok Suh1,2\n1 3\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nprofessionals interact with and leverage vast amounts of \nmedical data for patient care.\nDespite the growing interest and proven benefits of LLMs \nin many areas of medicine, their potential has not been \nfully explored in the realm of nuclear medicine imaging, \nparticularly PET imaging reporting. Despite the potential \nof ChatGPT to revolutionize content creation by generat -\ning human-like text [ 4], specific applications leveraging \nLLMs in the nuclear medicine field, particularly for imaging \nreports, has not been explored. PET imaging, which is per -\nformed for a variety of purposes and conditions, produces \ncomplex data requiring thorough analysis and interpreta -\ntion, playing a critical role in clinical decision-making [ 5, \n6]. There is a need for advanced tools to aid in referencing \npast reports, sourcing cases for educational purposes, and \nconducting differential diagnoses, especially as the use of \nPET, which encompasses various radiotracers and diseases, \nbecomes more widespread. This unmet need presents a sig-\nnificant opportunity for LLMs to improve the specificity \nand relevance of PET report generation. By leveraging prior \nreports and analogous case studies, LLMs can provide clini-\ncians with valuable insight, aiding them in making informed \ndecisions.\nIn this study, we introduce a pioneering approach to PET \nimaging reporting by developing and accessing a custom-\nbuilt, retrieval-augmented generation (RAG) LLM frame -\nwork [ 7]. This system leverages a comprehensive large \ndatabase of PET reading reports. By embedding these \nreports into a vector space for efficient retrieval based on \nsimilarity metrics, our framework aims to enhance PET \nimaging reporting in three key ways: (1) Assisting PET \nreading experts by referencing past reports, enabling them \nto review similar cases and outcomes during the diagnostic \nprocess. (2) Supporting educational purposes by identify -\ning appropriate cases for case-centered study. (3) Facilitat -\ning interactive queries related to PET reading for clinicians, \nbased on a database of past reports. This proof-of-concept \nstudy seeks to demonstrate the feasibility and benefits of \nintegrating advanced LLM capabilities with a vast reposi -\ntory of PET imaging data, aiming to set enhanced medical \nimaging reporting practices.\nMaterials and methods\nDataset\nThis study was conducted at a single center, utilizing reading \nreports of PET imaging data sourced from the clinical data \nwarehouse (CDW) of the SUPREME Platform. We extracted \ndata spanning from 2010 to 2023, comprising reports from \n118,107 patients across 211,813 cases. Institutional Review \nBoard (IRB) approval was secured from our hospital (IRB \nNo. 2401-090-1501), with the requirement for written \ninformed consent waived due to the retrospective nature of \nthe study and the use of deidentified information. The data-\nset encompassed reading reports for all cases, along with \nthe exam date, exam name, a deidentified research identifier \n(ID), sex, and date of birth (year-month format).\nModel architecture\nIn this study, we designed a proof-of-concept chatbot sys -\ntem for efficiently querying reading reports from a substan-\ntial dataset. It was based on ‘RAG’ [ 7]. The adaptability of \nthis system allows for the utilization of various database \nformats, including but not limited to ‘csv’ files, to accom -\nmodate different sources of reading reports. This system \namalgamates state-of-the-art language model technologies \nwith sophisticated natural language processing and infor -\nmation retrieval techniques, aiming to deliver precise, con -\ntextually relevant responses to inquiries concerning PET \nimaging reading reports. The overall workflow of this sys -\ntem is illustrated in Fig. 1.\nThe architecture of our system is underpinned by a \nseries of modular components, each crucial for interpret -\ning and responding to user queries. At the forefront is a \nsentence embedding layer, crafted to process intricate \ntexts and queries by transforming sentences into vectors. \nThis transformation facilitates subsequent processing by \nvarious mathematical models. We employed the Sentence \nTransformer model, specifically the “paraphrase-multilin -\ngual-MiniLM-L12-v2”  (   h t  t p s  : / / h  u g  g i n  g f a c  e . c  o / s  e n t e n c e \n- t r a n s f o r m e r s / p a r a p h r a s e - m u l t i l i n g u a l - M i n i L M - L 1 2 - v 2     ) , \nrenowned for its ability to comprehend and paraphrase texts \nacross multiple languages—a necessary feature consider -\ning the bilingual nature (English and Korean) of the reading \nreports in our dataset. To manage and retrieve PET reading \nreports effectively, our system incorporates a vector storage \nmechanism, Chroma (Chroma,  h t t p s : / / w w w . t r y c h r o m a . c o \nm /     ) . Chroma organizes textual data into a searchable vec -\ntor space by converting text into numerical vectors derived \nfrom the sentence embeddings. This conversion enables the \nsystem to execute advanced retrieval operations, identify -\ning responses that are semantically relevant to the queries \nposed. The retrieval after embedding to Chroma was per -\nformed using the cosine similarity of the query text vectors, \nretrieving the top-k texts from the database as context for \ngenerating prompts for the LLM. We set this top-k value \nto k = 5.\nAfter retrieving the related context, specifically previ -\nous PET reports, a question-answering (QA) component \nwas integrated. This QA mechanism excels at comprehend-\ning user queries, sourcing the most pertinent documents \n1 3\n2453\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nfrom the dataset, and formulating informative responses \nthat precisely address the queries. To generate prompts, the \nsystem integrates retrieved texts as contexts along with the \nreader’s question to create a full prompt. For example, the \nprompt includes the text: “Give an answer by only referring \nto the context, include the address within the context in the \nanswer, and clearly number the answer ,” along with ( con-\ntext), which contains the retrieved reports, and ( question), \nrepresenting the reader’s query. For the generation of these \nresponses, we incorporated the Llama-3 (7-billion param -\neter model) language model [8] and the system architecture \nwas based on Langchain [9].\nVisualization of vector embedding\nFollowing the process of sentence embedding, the result -\ning vectors were stored in a vector database. These vectors \nplayed a crucial role in identifying similarities between var-\nious texts, including the queries submitted to the system. To \nfacilitate a deeper understanding of how PET reading reports \nare represented within this vector space, we employed \nt-distributed Stochastic Neighbor Embedding (t-SNE) for \nvisualization purposes [ 10]. Specific keywords associated \nwith imaging reports, such as “lung cancer,” “breast can -\ncer,” “lymphoma,” “methionine PET,” and “PSMA PET,” \nwere chosen for this analysis. The objective was to ascer -\ntain whether reports containing these selected terms would \nnaturally form distinct clusters within the vector space. This \napproach aimed to visually demonstrate the effectiveness of \nour vector embedding process in grouping similar reports, \nthereby providing insights into the semantic relationships \nand similarities between different PET reports in the dataset.\nTest examples\nIn the evaluation of prototype chatbots designed for navi -\ngating an extensive database of PET reading reports, we \nfocused on testing their ability to accurately retrieve reports \nsimilar to those specified in user queries and to assist in dif-\nferential diagnosis by referencing previous reports. This \ninvolved assessing the proficiency in identifying cases with \nspecific diagnoses or imaging findings and their capability \nto extract relevant information to support nuclear medicine \nexperts in diagnosing complex cases. The testing protocol \nsimulated real-world scenarios, presenting the chatbots with \ndiverse clinical questions to comprehensively evaluate their \nutility in clinical decision-making and their effectiveness in \nleveraging the vast database to enhance the accuracy and \nrelevance of their responses.\nFig. 1  Workflow of the Chatbot System for Querying PET Imag -\ning Reading Reports. The overall workflow of the proof-of-concept \nsystem designed for efficient querying of reading reports from a \nsubstantial dataset is illustrated. The system integrates the Retrieval-\nAugmented Generation (RAG) model with advanced language model \ntechnologies, natural language processing, and information retrieval \ntechniques. The workflow demonstrates the process from user query \ninput through to the delivery of the relevant reading report, showcas -\ning the operational framework and interaction with different sources \nof reading reports\n \n1 3\n2454\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nResults\nClustered unstructured PET reports by sentence \nembedding\nWe analyzed PET imaging reports from 118,107 patients, \ntotaling 211,813 cases, by converting them into vector \nembeddings. These embeddings were then visualized on a \nt-SNE plot to demonstrate dimensionality reduction and the \nclustering of reports with similar characteristics (Fig. 2A). \nEach point on this plot represents a unique PET imaging \nreport, with a specific case highlighted in red for illustrative \npurposes, including its original report. By examining the \ndistribution of these clusters, we observed distinct group -\nings based on diagnostic terms and exam types, indicating \nthat reports with similar clinical contexts naturally grouped \ntogether in the embedding space. For instance, to evaluate \nthe representational efficacy of the embeddings, reports con-\ntaining key diagnostic terms such as ‘lung cancer’, ‘breast \ncancer’, and ‘lymphoma’, as well as those pertaining to spe-\ncific types of exams like ‘C-11 methionine PET’ and ‘Ga-68 \nPSMA-11 PET’, were marked on the plot. The clusters con-\ntaining ‘lung cancer’ exhibited substantial cohesion, poten-\ntially reflecting the higher prevalence of lung cancer cases in \nour dataset, while distinct clusters also emerged for ‘breast \ncancer,’ ‘lymphoma,’ and specific PET modalities such as \nC-11 methionine PET and Ga-68 PSMA-11 PET (Fig. 2B). \nThese cohesive clusters highlight clinically meaningful pat-\nterns, suggesting that sentence embeddings from unstruc -\ntured reports could be leveraged to make a context for using \nLLM for question and answering. The formation of these \ndistinct clusters underscores the text embedding ability in \nPET reports to reflect the semantic similarity among cases, \noffering potential clinical utility in identifying disease-spe -\ncific patterns and retrieving relevant texts.\nLLM with RAG chatbot-assisted querying and \nsuggested diagnosis\nUsing the prototype chatbot, we tested its efficacy in iden -\ntifying cases pertinent to specific user queries. A notable \ninstance involved the chatbot’s response to the query, \n“Identify cases of breast cancer with metastasis to internal \nmammary lymph nodes ,” where it proficiently located and \npresented relevant cases from the database of prior reading \nreports (Fig. 3A) (More examples are presented in Supple -\nmentary Video 1). This example demonstrates how clini -\ncians or trainees could rapidly find comparable cases for \nreference, potentially aiding diagnostic reasoning or edu -\ncational purposes. The retrieved cases included key details \nfrom prior reports, allowing users to cross-reference imaging \nfindings, disease progression, and final outcomes in patients \nEvaluation of queried similar cases and potential \ndiagnoses\nFrom daily routine PET exams, we simulated prompts to \nevaluate relevance and appropriateness by three indepen -\ndent nuclear medicine physicians. We extracted 19 cases \nfrom routine PET exams and their reports to evaluate two \ntasks: query performance for similar cases and potential \ndiagnoses from findings. To evaluate query performance \nfor similar cases, we used the text from the conclusions of \nthe PET reports to generate prompts such as “find similar \ncases and summarize the reports.” For evaluating potential \ndiagnoses, specific texts from the findings sections of the \nreports were used to generate prompts to “suggest poten -\ntial diagnoses for this finding.” Examples of conclusions \nand findings used for these prompts are summarized in \nSupplementary Table 1. Three nuclear medicine physicians \nindependently scored the system’s answers for medical rele-\nvance on a scale of 1 (poor), 2 (fair), and 3 (good). The gold \nstandard for these evaluations was the consensus judgment \nof these experienced physicians, who assessed the medical \nrelevance and accuracy of the system’s responses based on \ntheir expert knowledge and clinical experience. To assess \nthe effect of the RAG on the performance of the LLM, we \ncompared the appropriateness scores of the LLM with and \nwithout RAG using the Wilcoxon rank-sum test. This com-\nparative analysis helped determine the added value of the \nRAG framework in enhancing the relevance and accuracy \nof the generated responses.\nIn addition to performance evaluations based on physi -\ncian scoring, a quantitative assessment was conducted to \nevaluate the accuracy of conclusions generated from find -\nings. By inputting text from the findings section, the LLM \nwith and without RAG was tested for its ability to generate \nconclusion texts for reading reports, simulating diagnostic \nreasoning. (prompt: “ Write a concise conclusion , includ -\ning a potential diagnosis, in one or two sentences ”). These \ngenerated conclusions were compared to the actual con -\nclusion reports  described by nuclear medicine physicians. \nThe comparisons were quantified using the ROUGE-L met-\nric (Recall-Oriented Understudy for Gisting Evaluation), \nwhich measures the alignment between generated and \nreference texts by focusing on the longest common subse -\nquences (LCS) while accounting for word order [11, 12]. To \nassess the overall quantitative performance, the ROUGE-L \nF-score—representing the harmonic mean of precision and \nrecall—was calculated for both the LLM with RAG and \nwithout RAG. This evaluation highlights the impact of the \nRAG framework on improving the alignment and relevance \nof the generated conclusions.\n1 3\n2455\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nFig. 2  Visualization of PET Imaging Report Embeddings Using \nt-SNE. ( A) t-SNE plot illustrates PET imaging report embeddings \nfrom 118,107 patients, totaling 211,813 cases. Each point on the plot \nrepresents a unique report, with a selected case highlighted in red to \nshow an example of an original report. (B) t-SNE plots showcases the \nclustering efficacy of the embeddings, highlighting how reports con -\ntaining key diagnostic terms like ‘lung cancer’, ‘breast cancer’, ‘lym -\nphoma’, and specific types of exams such as ‘C-11 methionine PET’ \nand ‘Ga-68 PSMA-11 PET’ form distinct clusters. These clusters indi-\ncate the embeddings’ capability to reflect the similarity among cases, \ndemonstrating the potential of this method in facilitating the identifica-\ntion and visualization of related PET imaging reports\n \n1 3\n2456\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nFig. 3 Examples of Chatbot Responses to Queries. ( A) An example \ncase displays an instance of the chatbot’s capability to accurately iden-\ntify and present relevant cases in response to a user query about breast \ncancer with metastasis to internal mammary lymph nodes. It highlights \nthe capacity to navigate a vast database of previous reading reports \nto identify relevant cases. ( B) An example of the utility of system in \ngenerating differential diagnoses is displayed. This is demonstrated \nthrough the chatbot’s response to a query, where it offers a detailed list \nof potential diagnoses along with reference identifiers. As an example, \nby employing identifiers within the PACS system (in this example, \nwe used deidentified information), prior imaging cases could be refer-\nenced for understanding cases and supporting decision making\n \n1 3\n2457\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nDiscussion\nIn this study, we have explored the integration of LLMs into \nthe PET imaging reporting process, presenting a novel pro-\ntotype chatbot based on RAG capable of retrieving relevant \ncases and offering differential diagnoses based on specific \nuser queries. This LLM with RAG represents a feasibility \nfor medical purposes in nuclear medicine imaging field, \nparticularly by incorporating contextual understanding \nfrom previous PET imaging reports to respond to queries \nfrom nuclear medicine physicians. This approach marks a \ndeparture from simple chatbot functionalities, introducing \na system that integrates with the clinical workflow to pro -\nvide contextually relevant information and insights. This \nproof-of-concept not only validates the utility of LLMs in \nenhancing the PET reporting process but also underscores \nthe potential of AI-assisted tools to augment diagnostic \naccuracy and clinical decision-making in nuclear medicine.\nThe RAG model combines the strengths of information \nretrieval and generative AI to offer precise and informa -\ntive answers to complex medical queries. It works by first \nretrieving relevant documents or data points from a vast \ndatabase—in this case, a collection of PET imaging reports. \nFollowing this, the model uses the retrieved information as \na context to generate responses that are not only relevant \nbut also enriched with the specificity and detail required for \ndecision-making. This method allows the system to pro -\nvide answers that are deeply informed by historical cases \nand existing medical knowledge, thereby supporting physi-\ncians in diagnosing and managing patient care with a higher \ndegree of accuracy and confidence. The introduction of RAG \nnot only reduces the risk of hallucinations but also enhances \nthe accuracy of responses by grounding them in specialized, \ndomain-specific data. This is particularly important in PET \nreporting, where the complexity and specificity of the infor-\nmation require expertise-driven answers. RAG provides \na viable solution for effectively applying LLMs to such \nspecialized areas, ensuring more reliable and contextually \nappropriate outputs. In contrast to earlier language models \nthat concentrated on singular tasks [ 13–15], models based \non the RAG framework with LLMs can handle diverse que-\nries and produce varied outputs. The RAG model, distinct \nfrom LLMs that rely solely on their pre-trained datasets, \nactively incorporates pertinent historical information during \nits response generation. Primarily, employing LLMs like \nChatGPT or Gemini directly is constrained by their inabil -\nity to access individual center databases, which restricts \ntheir reference to prior cases and clinical outcomes. In this \nregard, a previous study demonstrated that RAG applica -\ntions can enhance domain-specific decision-making when \nusing LLMs in medical fields, whereas querying and retriev-\ning specific cases to reference previous outcomes in nuclear \nwith similar clinical scenarios. Additionally, we evaluated \nthe chatbot’s functionality in offering differential diagnoses \nby leveraging its integration with LLM. This was exempli -\nfied in a scenario where the chatbot was tasked to provide \ndifferential diagnoses for the condition described as “ Mul-\ntiple mediastinal lymph nodes with increased FDG uptake \nwithout an identified primary site .” The chatbot responded \nwith a detailed list of differential diagnoses, accompanied \nby reference identifiers, thus enabling medical professionals \nto quickly locate and compare relevant case histories, imag-\ning findings, and clinical outcomes (Fig. 3B).\nTaken together, these examples underscore the poten -\ntial for integrating real-world historical data into the deci -\nsion-making process. By referencing prior PET reports \nthrough the RAG framework, clinicians receive contextu -\nally enriched insights, which can be especially valuable for \nless common clinical presentations. This improved retrieval \nand diagnosis suggestion process highlights a practical way \nto apply generative AI tools in nuclear medicine practice, \nwhere rapid access to similar cases and differential diagno -\nses can benefit patient care.\nEvaluating appropriateness for case querying and \ndiagnosis suggestion using LLM with RAG\nIn addition, the appropriateness scores evaluated by nuclear \nmedicine physicians were assessed for two different simu -\nlated tasks: querying similar cases and suggesting potential \ndiagnoses from specific findings. Firstly, for the similar \ncases queried by specific reports, 16 out of 19 (84.2%) were \nappropriately identified, with all three readers rating these \nas better than ‘Fair’ in relevance (Fig. 4A). Furthermore, the \nappropriateness of potential diagnoses for specific findings \nwas evaluated, with 15 out of 19 (78.9%) cases receiving \na better than ‘Fair (2)’ grade from all readers for the sug -\ngested potential diagnoses. To compare the performance of \nthe LLM with and without RAG, the Wilcoxon rank sum \ntest was conducted. The LLM with RAG showed signifi -\ncantly better appropriateness scores compared to the LLM \nwithout RAG (W  = 226; p < 0.05) (Fig. 4B). In addition to \nthe appropriateness assessed by physicians’ scores, the con-\nclusions generated using findings with and without the RAG \nframework were quantitatively evaluated. The ROUGE-L \nF-score, which measures how well the generated conclu -\nsion from findings captures the reference conclusion text, \nwas significantly higher for the RAG framework compared \nto the LLM without RAG (0.16 ± 0.08 vs 0.07 ± 0.03, \np < 0.001; Fig. 4C).\n1 3\n2458\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\ninstitutions rather than serving as a universal model for all \nhospitals. In other words, implementing LLM with RAG \nframeworks to retrieve data specific to each hospital could \nimprove responses to questions directly related to that data. \nIn addition, this feature is especially beneficial in specialty \nfields like nuclear medicine, where insights drawn from \nprevious cases are helpful for informed decision-making in \ncurrent clinical scenarios.\nIn this study, we evaluated the performance of LLM-\nbased answers for two tasks: querying similar cases and sug-\ngesting potential diagnoses based on previous reports. The \nmedicine imaging are specialized tasks addressed by our \nwork [16]. Moreover, due to stringent regulations concern -\ning clinical data and privacy, the transfer of clinical records \nto external AI servers is considered highly sensitive and is \ninherently prohibited in numerous healthcare institutions \n[17, 18]. In this context, implementing a LLM with RAG \nframework that utilizes PET reading reports could address \nthese challenges by facilitating the application of real-\nworld data in each hospital, while also avoiding the vari -\nous data-related regulatory constraints. Although tested in \na single-center study, this approach is tailored to individual \nFig. 4  Evaluation of Appropriateness Scores by Nuclear Medicine \nPhysicians. ( A) The appropriateness of querying similar cases was \nassessed. Using a conclusion text to generate the prompt “find simi -\nlar reports and summarize it,” the system retrieved results. For spe -\ncific reports, 16 out of 19 (84.2%) were appropriately identified, with \nall three readers rating these as better than ‘Fair’ in relevance. ( B) \nThe appropriateness of potential diagnoses for specific findings was \nevaluated. Using specific finding texts to generate prompts for sug -\ngesting potential diagnoses, the responses of system were assessed. \nMedical relevance and appropriateness of the suggested potential \ndiagnoses were evaluated by readers. The system without RAG was \nalso assessed, and the performance of the LLM with and without RAG \nwas represented as a heatmap. The results indicated that the LLM with \nRAG showed significantly better appropriateness scores (p < 0.05). (C) \nThe ROUGE-L F-score was used to quantitatively evaluate the align -\nment between generated conclusions and reference conclusion texts \nfrom finding descriptions. The RAG framework demonstrated signifi -\ncantly higher scores compared to the LLM without RAG (0.16 ± 0.08 \nvs. 0.07 ± 0.03, p < 0.001)\n \n1 3\n2459\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nthereby enhancing their diagnostic skills and understanding \nof nuclear medicine [4, 22]. Furthermore, the ability of this \nsystem to reference previous cases when providing differ -\nential diagnoses enriches the educational content with prac-\ntical, real-world examples, fostering critical thinking and \ndecision-making skills among trainees.\nOne potential application of this system is its ability to \ncorrelate imaging findings with follow-up clinical results, \nincluding final diagnoses and clinical outcomes because the \nRAG LLM can reference previous reports. These previous \nreferences allow readers to find similar cases and trace their \nfuture clinical outcomes or final diagnoses. By integrating \nhistorical data-driven context into the imaging interpreta -\ntion process, the system offers an opportunity to provide a \nholistic view of the clinical journey of similar cases, from \nimaging to final outcome [ 23, 24]. This comprehensive \napproach facilitates a more nuanced understanding of the \npotential implications of specific imaging findings, guiding \nphysicians in crafting PET imaging interpretation that are \ninformed by both the current condition and comparable past \ncases. The insights derived from this analysis are invaluable \nfor informing differential diagnosis, predicting patient out -\ncomes, and even anticipating potential complications. Such \ninsights are crucial for bridging the gap between imaging \nfindings and patient management strategies, ultimately con-\ntributing to improved patient care.\nHowever, the study also acknowledges certain limita -\ntions, including the inherent risk of generating inaccurate \ninformation (hallucinations) and the current model’s reli -\nance on textual data [ 20, 21]. Additionally, due to limita -\ntions in retrieval performance, the system showed poor \nappropriateness score in retrieving rare cases and their \nrelated potential diagnosis. This affects the overall per -\nformance and quality, as experienced physicians would \nfind the system most useful for rare or atypical cases. To \naddress this, using better LLM models that allow for a larger \nnumber of tokens and can reference more previous reports \nsimultaneously could mitigate these issues. However, this \nrequires further study and development. Additionally, while \nour RAG approach avoids the pitfalls of overfitting through \nthe use of pre-trained language models without additional \ntraining, it is important to recognize the limitations inher -\nent in the database composition. The variability in disease \nprevalence across different hospitals may impact the perfor-\nmance of similar case retrieval, potentially limiting the gen-\neralizability of our findings. Further studies involving more \ndiverse and representative datasets are necessary to validate \nand enhance the robustness of our tool. A larger, multicenter \nstudy is required to validate the approach across different \nclinical settings, given the variations in PET indications and \ndisease prevalence across centers. These differences could \nimpact the model’s performance. However, this approach \nsimilar case retrieval demonstrated good performance, cor -\nrectly identifying similar cases in nearly 90% of instances. \nThe use of the sentence transformer in our retrieval method \nof RAG provides an advantage in handling PET reports as \nunstructured text data, which is common in large-scale hos-\npital settings. Unlike traditional query systems that require \nstructured tagging, our approach allows for effective data \nretrieval without the need for extensive pre-processing, \nmaking it more adaptable and practical for real-world \nclinical applications. However, the system showed limita -\ntions with rare cases; for example, it failed to appropriately \nretrieve a case of scalp angiosarcoma due to its rarity. In this \ncase, we could consider incorporating a database specifi -\ncally labeled with rare cases and implementing a weighting \nsystem to prioritize their retrieval during queries. Address -\ning the retrieval of rare case-related data is a crucial aspect \nof applying LLMs in medical fields. Managing a database \nenriched with rarity information could significantly enhance \nthe performance of LLMs with RAG, particularly in PET \nreporting, by improving their ability to handle uncommon \nand complex cases effectively [ 19]. We also assessed the \nuse of RAG for generating answers. By leveraging con -\ntexts from previous PET reports, RAG provided reliable \nand medically relevant responses. In particular, during the \ngeneration of potential diagnoses, RAG could reference pre-\nvious cases, which helped readers perceive the answers as \nreliable and relevant, mitigating the hallucination effect—a \ncommon issue with LLMs in medical applications [20, 21]. \nDespite the positive results, the system with RAG has limi-\ntations, especially with rare cases, and the potential diag -\nnoses could be influenced by the contexts of queried cases, \nreducing the number of suggested diagnoses. Nonetheless, \nthe ability of RAG to reference relevant cases that clinicians \nand readers can review adds a crucial layer of validation, \nreducing the potential risks associated with noise and com -\nplex multi-disease scenarios. This approach distinguishes \nit from the direct application of LLMs for PET reading-\nrelated questions. Additional optimized methods for using \nRAG to identify rare cases and incorporate more context \nwill enhance the system’s performance. In addition, while \nour evaluation relied on expert judgment as the gold stan -\ndard, we acknowledge the inherent subjectivity in human \nassessments, which may impact reproducibility. To address \nthis, we have provided the prompts used in Supplementary \nTable 1, allowing for testing across various LLM systems. \nFuture studies should incorporate objective metrics and \nmore diverse, representative datasets to further enhance the \ngeneralizability and robustness of our approach.\nThe application of our system extends beyond diagnos -\ntic support, serving as a valuable educational resource. By \nfacilitating access to similar cases, it enables medical prac -\ntitioners and trainees to explore diverse clinical scenarios, \n1 3\n2460\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\nNumber: 1711137868, RS-2020-KD000006) and the NA VER Digital \nBio Innovation Research Fund, funded by NA VER Corporation (Grant \nNo. 3720230020).\nData availability Due to personal information protection policies, the \ncomplete datasets of reading reports are not available outside the hos -\npital server. Sample data are included in the supplementary materials \nand their related contents can be provided by the corresponding author \nupon reasonable request.\nDeclarations\nCompeting interests H.C. is a co-founder of Portrai.\nEthics approval  The retrospective analysis of human data and the \nwaiver of informed consent were approved by the Institutional Review \nBoard of the Seoul National University Hospital (No. 2401-090-1501).\nConsent to participate  Written informed consent was acquired from \nall patients.\nConsent to publish Not applicable.\nOpen Access   This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit  h t    t p : / / c r e  a  t  i v e  c  o  m m o n s . o \nr g / l i c e n s e s / b y / 4 . 0 /     .  \nReferences\n1. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan \nTF, Ting DSW. Large language models in medicine. Nat Med. \n2023;29:1930–40.\n2. Elkassem AA, Smith AD. Potential use cases for ChatGPT in \nradiology reporting. Am J Roentgenol. 2023.\n3. Doshi R, Amin K, Khosla P, Bajaj S, Chheang S, Forman HP. Uti-\nlizing Large Language Models to Simplify Radiology Reports: \na comparative analysis of ChatGPT3. 5, ChatGPT4. 0, Google \nBard, and Microsoft Bing. medRxiv. 2023:2023.06. 04.23290786.\n4. Alberts IL, Mercolli L, Pyka T, Prenosil G, Shi K, Rominger A, \net al. Large language models (LLM) and ChatGPT: what will the \nimpact on nuclear medicine be? Eur J Nucl Med Mol Imaging. \n2023;50:1549–52.\n5. Monshi MMA, Poon J, Chung V . Deep learning in generating \nradiology reports: a survey. Artif Intell Med. 2020;106:101878.\n6. Tie X, Shin M, Pirasteh A, Ibrahim N, Huemann Z, Castellino SM \net al. Personalized impression generation for PET reports using \nlarge Language models. J Imaging Inf Med. 2024:1–18.\n7. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V , Goyal N, et \nal. Retrieval-augmented generation for knowledge-intensive nlp \ntasks. Adv Neural Inf Process Syst. 2020;33:9459–74.\nleverages LLMs tailored to individual hospital settings \nthrough RAG without requiring complex LLM training, \ndemonstrating its potential utility in this report. In addition \nto the challenges and future perspectives, one of the future \nchallenges, exploring the integration of multimodal data, \nsuch as combining visual and textual analysis, are identified \nas essential steps forward. This future direction promises \nnot only to mitigate the limitations but also to further enrich \nthe system’s utility by providing a more holistic approach to \nmedical query answering and decision support.\nConclusion\nIn conclusion, our suggested AI framework affirm the trans-\nformative potential of AI-assisted tools in nuclear medicine, \nparticularly in the context of PET imaging report analysis. \nThe integration of an RAG LLM with a comprehensive PET \nimaging report database demonstrated feasibility for use in \nreal-world clinical routines in nuclear medicine, particularly \nfor imaging interpretation and reporting. This approach \nenhances the workflow of nuclear medicine physician and \nrelevance of PET report generation, possibly supporting \ndecision-making and providing educational benefits. It \nunderscores the potential role of AI in improving the qual -\nity and efficacy of medical care within nuclear medicine. \nFurthermore, as we look to the future, the development of \nbetter LLM and multimodal models stands as a pivotal next \nstep in overcoming current limitations and fully realizing \nthe benefits of AI in medical imaging. This proof-of-concept \nstudy and proposed framework demonstrated the feasibil -\nity of using LLMs in the clinical routine of nuclear medi -\ncine, particularly by leveraging large report databases and \nshowed promise for improving diagnostics, education, and \npatient management.\nSupplementary Information  The online version contains \nsupplementary material available at  h t t  p s : /  / d o  i . o  r g / 1 0 . 1 0 0 7 / s 0 0 2 5 9 - 0 \n2 5 - 0 7 1 0 1 - 9     .  \nAcknowledgements We employed ChatGPT, developed by OpenAI, \nexclusively for grammatical corrections and enhancements in clarity \nand it did not generate any new content.\nAuthor contributions Conceptualization and design: H.C.; data acqui-\nsition: H.C., Y .K., and M.S.; data analysis: H.C. and D.J.L.; original \ndraft preparation: H.C. and Y .K.; review and editing: H.C., Y .K., and \nM.S.; supervision: H.C.; funding acquisition: H.C., and Y .K. All au-\nthors have read and agreed to the submission of the manuscript.\nFunding Open Access funding enabled and organized by Seoul Na -\ntional University Hospital.\nThis research was supported by Korea Medical Device Development \nFund grant funded by the Korea government (the Ministry of Science \nand ICT, the Ministry of Trade, Industry and Energy, the Ministry of \nHealth & Welfare, the Ministry of Food and Drug Safety) (Project \n1 3\n2461\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2025) 52:2452–2462\n17. Minssen T, Vayena E, Cohen IG. The challenges for regulat -\ning medical use of ChatGPT and other large language models. \nJAMA. 2023.\n18. Meskó B, Topol EJ. The imperative for regulatory oversight of \nlarge language models (or generative AI) in healthcare. NPJ Digit \nMed. 2023;6:120.\n19. Wang G, Ran J, Tang R, Chang C-Y , Chuang Y-N, Liu Z et al. \nAssessing and enhancing large language models in rare disease \nquestion-answering. arXiv Preprint arXiv:240808422. 2024.\n20. Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y , et al. Survey of hallucina-\ntion in natural language generation. ACM-CSUR. 2023;55:1–38.\n21. Alkaissi H, McFarlane SI. Artificial hallucinations in ChatGPT: \nimplications in scientific writing. Cureus. 2023;15.\n22. Currie G, Barry K. ChatGPT in nuclear medicine education. J \nNucl Med Technol. 2023;51:247–54.\n23. Silva W, Poellinger A, Cardoso JS, Reyes M. Interpretability-\nguided content-based medical image retrieval. Medical Image \nComputing and Computer Assisted Intervention–MICCAI 2020: \n23rd International Conference, Lima, Peru, October 4–8, 2020, \nProceedings, Part I 23: Springer; 2020. pp. 305 – 14.\n24. Choe J, Hwang HJ, Seo JB, Lee SM, Yun J, Kim M-J, et al. \nContent-based image retrieval by using deep learning for \ninterstitial lung disease diagnosis with chest CT. Radiology. \n2022;302:187–97.\nPublisher’s note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional affiliations.\n8. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y \net al. Llama 2: open foundation and fine-tuned chat models. arXiv \nPreprint arXiv:230709288. 2023.\n9. Topsakal O, Akinci TC. Creating large language model applica -\ntions utilizing langchain: A primer on developing llm apps fast. \nInternational Conference on Applied Engineering and Natural \nSciences; 2023. pp. 1050-6.\n10. Van der Maaten L, Hinton G. Visualizing data using t-SNE. J \nMach Learn Res. 2008;9.\n11. Lin C-Y , Rouge. A package for automatic evaluation of summa-\nries. Text summarization branches out; 2004. pp. 74–81.\n12. Gulden C, Kirchner M, Schüttler C, Hinderer M, Kampf M, \nProkosch H-U, et al. Extractive summarization of clinical trial \ndescriptions. Int J Med Informatics. 2019;129:114–21.\n13. Huemann Z, Lee C, Hu J, Cho SY , Bradshaw TJ. Domain-adapted \nlarge language models for classifying nuclear medicine reports. \nRadiology: Artif Intell. 2023;5:e220281.\n14. Garcia EV . Integrating artificial intelligence and natural language \nprocessing for computer-assisted reporting and report understand-\ning in nuclear cardiology. J Nuclear Cardiol. 2023;30:1180–90.\n15. Mithun S, Jha AK, Sherkhane UB, Jaiswar V , Purandare NC, \nRangarajan V et al. Development and validation of deep learn -\ning and BERT models for classification of lung cancer radiology \nreports. Inf Med Unlocked. 2023:101294.\n16. Zakka C, Shad R, Chaurasia A, Dalal AR, Kim JL, Moor M, et \nal. Almanac—retrieval-augmented language models for clinical \nmedicine. NEJM AI. 2024;1:AIoa2300068.\n1 3\n2462",
  "topic": "Medical diagnosis",
  "concepts": [
    {
      "name": "Medical diagnosis",
      "score": 0.7403258085250854
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6312549114227295
    },
    {
      "name": "Computer science",
      "score": 0.588685929775238
    },
    {
      "name": "Relevance (law)",
      "score": 0.4896726906299591
    },
    {
      "name": "Information retrieval",
      "score": 0.46931955218315125
    },
    {
      "name": "Medical physics",
      "score": 0.4108467698097229
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3926549553871155
    },
    {
      "name": "Medicine",
      "score": 0.3654387593269348
    },
    {
      "name": "Natural language processing",
      "score": 0.35816431045532227
    },
    {
      "name": "Radiology",
      "score": 0.15788453817367554
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}