{
  "title": "Enabling Energy-Efficient Deployment of Large Language Models on Memristor Crossbar: A Synergy of Large and Small",
  "url": "https://openalex.org/W4403534421",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2059211646",
      "name": "Wang, Zhehui",
      "affiliations": [
        "Agency for Science, Technology and Research",
        "Institute of High Performance Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2126751436",
      "name": "Luo Tao",
      "affiliations": [
        "Agency for Science, Technology and Research",
        "Institute of High Performance Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2108992027",
      "name": "Liu Cheng",
      "affiliations": [
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2374118336",
      "name": "Liu Weichen",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2750510906",
      "name": "Goh, Rick Siow Mong",
      "affiliations": [
        "Institute of High Performance Computing",
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A4221416426",
      "name": "Wong, Weng-Fai",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4396653761",
    "https://openalex.org/W3013080934",
    "https://openalex.org/W2805362231",
    "https://openalex.org/W3126444928",
    "https://openalex.org/W4229035884",
    "https://openalex.org/W3205015346",
    "https://openalex.org/W4200364535",
    "https://openalex.org/W2518281301",
    "https://openalex.org/W2293423139",
    "https://openalex.org/W4237346011",
    "https://openalex.org/W2487018225",
    "https://openalex.org/W3018945530",
    "https://openalex.org/W2085424870",
    "https://openalex.org/W3005875844",
    "https://openalex.org/W2399958287",
    "https://openalex.org/W2605663629",
    "https://openalex.org/W3033826949",
    "https://openalex.org/W6763189659",
    "https://openalex.org/W3134195144",
    "https://openalex.org/W2508602506",
    "https://openalex.org/W2613989746",
    "https://openalex.org/W2981124343",
    "https://openalex.org/W2164819109",
    "https://openalex.org/W2155589054",
    "https://openalex.org/W6638915636",
    "https://openalex.org/W2047478832",
    "https://openalex.org/W3045949243",
    "https://openalex.org/W2162279286",
    "https://openalex.org/W4231109836",
    "https://openalex.org/W4239972437",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2909633668",
    "https://openalex.org/W2906472504",
    "https://openalex.org/W600310561",
    "https://openalex.org/W6856529530",
    "https://openalex.org/W2008311222",
    "https://openalex.org/W2048266589",
    "https://openalex.org/W2105102111",
    "https://openalex.org/W2795127895",
    "https://openalex.org/W4378800995",
    "https://openalex.org/W3091835145",
    "https://openalex.org/W48535165",
    "https://openalex.org/W2013028205",
    "https://openalex.org/W3113256013",
    "https://openalex.org/W4382203227",
    "https://openalex.org/W2167313300",
    "https://openalex.org/W2170987901",
    "https://openalex.org/W1878252571",
    "https://openalex.org/W3159868971",
    "https://openalex.org/W2892120106",
    "https://openalex.org/W2167221365",
    "https://openalex.org/W2333849942",
    "https://openalex.org/W2761149222",
    "https://openalex.org/W3034855459",
    "https://openalex.org/W4380874786",
    "https://openalex.org/W3006600764"
  ],
  "abstract": "Large language models (LLMs) have garnered substantial attention due to their promising applications in diverse domains. Nevertheless, the increasing size of LLMs comes with a significant surge in the computational requirements for training and deployment. Memristor crossbars have emerged as a promising solution, which demonstrated a small footprint and remarkably high energy efficiency in computer vision (CV) models. Memristors possess higher density compared to conventional memory technologies, making them highly suitable for effectively managing the extreme model size associated with LLMs. However, deploying LLMs on memristor crossbars faces three major challenges. First, the size of LLMs increases rapidly, already surpassing the capabilities of state-of-the-art memristor chips. Second, LLMs often incorporate multi-head attention blocks, which involve non-weight stationary multiplications that traditional memristor crossbars cannot support. Third, while memristor crossbars excel at performing linear operations, they are not capable of executing complex nonlinear operations in LLM such as softmax and layer normalization. To address these challenges, we present a novel architecture for the memristor crossbar that enables the deployment of state-of-the-art LLM on a single chip or package, eliminating the energy and time inefficiencies associated with off-chip communication. Our testing on BERT showed negligible accuracy loss. Compared to traditional memristor crossbars, our architecture achieves enhancements of up to in area overhead and in energy consumption. Compared to modern TPU/GPU systems, our architecture demonstrates at least a reduction in the area-delay product and a significant 69% energy consumption reduction.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1\nEnabling Energy-Efficient Deployment of\nLarge Language Models on Memristor Crossbar:\nA Synergy of Large and Small\nZhehui Wang, Tao Luo*, Cheng Liu, Weichen Liu, Rick Siow Mong Goh, Weng-Fai Wong\nAbstract—Large language models (LLMs) have garnered substantial attention due to their promising applications in diverse domains.\nNevertheless, the increasing size of LLMs comes with a significant surge in the computational requirements for training and\ndeployment. Memristor crossbars have emerged as a promising solution, which demonstrated a small footprint and remarkably high\nenergy efficiency in computer vision (CV) models. Memristors possess higher density compared to conventional memory technologies,\nmaking them highly suitable for effectively managing the extreme model size associated with LLMs. However, deploying LLMs on\nmemristor crossbars faces three major challenges. Firstly, the size of LLMs increases rapidly, already surpassing the capabilities of\nstate-of-the-art memristor chips. Secondly, LLMs often incorporate multi-head attention blocks, which involve non-weight stationary\nmultiplications that traditional memristor crossbars cannot support. Third, while memristor crossbars excel at performing linear\noperations, they are not capable of executing complex nonlinear operations in LLM such as softmax and layer normalization. To\naddress these challenges, we present a novel architecture for the memristor crossbar that enables the deployment of state-of-the-art\nLLM on a single chip or package, eliminating the energy and time inefficiencies associated with off-chip communication. Our testing on\nBERTLarge showed negligible accuracy loss. Compared to traditional memristor crossbars, our architecture achieves enhancements of\nup to 39× in area overhead and 18× in energy consumption. Compared to modern TPU/GPU systems, our architecture demonstrates\nat least a 68× reduction in the area-delay product and a significant 69% energy consumption reduction.\nIndex Terms—Large Language Model, Natural Language Processing, Model Deployment, Memristor Crossbar, Non-Volatile Memory\n✦\n1 I NTRODUCTION\nL\nARGE language models (LLMs), such as ChatGPT, LLaMA\nand PaLM, have become increasingly popular in recent years\ndue to their ability to leverage vast amounts of professional\nknowledge by fine-tuning the model. This potential has been\ndemonstrated in various domains, including medical and finance\ntechnologies. However, the size of LLMs has been growing rapidly\nwith their increasing accuracy, resulting in huge computational\ncomplexity. Even inferring LLMs require powerful computing\nsystems that consume a significant amount of energy. For example,\nthe inference of ChatGPT requires approximately eight A100\nGPU cards [1], which poses challenges for local deployment,\nparticularly in mobile environments. The OpenAI data center\nfor ChatGPT consumes 23 million kWh based on monthly re-\nquests [2]. This high energy consumption and cost for model\ninference could constrain its further development, particularly as\nmodels become even larger. Thus, there is a significant demand\nfor small-size LLM accelerators that can perform model inference\nmore efficiently and cost-effectively.\nMemristor crossbars are widely considered strong competitors\nfor traditional machine learning accelerators [3]. Numerous mem-\nristor crossbar systems have been proposed [4–8], showcasing\nlow power consumption and low latency compared to classical\nPublished in IEEE TPAMI\n*Corresponding author: Tao Luo, E-mail: tluo001@e.ntu.edu.sg\nZhehui Wang (E-mail: zhehui@connect.ust.hk), Tao Luo and Rick Siow Mong\nGoh are with the Institute of High Performance Computing (IHPC), Agency for\nScience, Technology and Research (A*STAR), #16-16 Connexis, 1 Fusionopolis\nWay, Singapore 138632, Republic of Singapore\nCheng Liu is with the Chinese Academy of Sciences.\nWeichen Liu is with the Nanyang Technological University, Singapore.\nWeng-Fai Wong is with the National University of Singapore.\nDOI Bookmark: 10.1109/TPAMI.2024.3483654\naccelerators. By taking advantage of the physical characteristics\nof memristive storage technology, the analog computation can\nbe performed using memristors [9–11], which greatly boosts the\naccelerator’s performance. Different types of memristors using\nvarious Non-volatile memory (NVM) technologies [12] exist, in-\ncluding resistive random access memory (RRAM), phase-change\nmemory (PCM), spin-transfer torque magnetic random access\nmemory (STT-RAM), and Flash memory. All of them are promis-\ning candidates for building high-efficiency accelerators. For the\npurposes of this discussion, we will use RRAM memristors as a\nrepresentative of these NVM technologies.\nCompared with the current leading memory technologies, such\nas DRAM ( dynamic random-access memory ) and SRAM ( static\nrandom-access memory ), memristors have higher density. For\ninstance, each RRAM memristor occupies only 4 f2 [13] area,\nwhere f refers to the feature size of the chip. Considering the use\nof 4-bit memristors, a footprint of 274 mm2 memory cells area is\nsufficient to store all 175 billion parameters of GPT-3, assuming\nthe 14 nm technology. In contrast, the DRAM and SRAM require\n6f2 [13] and 100 f2 [13] for each bit of information. As a\ncomparison, they demand significantly larger areas of 1646 mm2\nand 27440 mm2 as memory cell area for GPT-3, assuming the\nsame 14 nm technology node. By achieving a substantial decrease\nin physical size, there is a high chance to store the whole neural\nnetwork models within a single chip or package, thus effectively\neliminating the inefficiencies of off-chip communication in terms\nof both time and energy [14].\nDespite the benefits of memristor-based machine learning ac-\ncelerators and their wide applications in neural networks, it is still\ndifficult to directly deploy LLMs on memristor-based accelerators\ndue to three major challenges that constrain their usage.\narXiv:2410.15977v1  [cs.AI]  21 Oct 2024\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2\n• Challenge 1: Extremely large model size. Despite the high\ndensity of memristors, the capacity of the crossbar is still\nlimited by peripheral circuits such as DAC ( digital-analog\nconverter) and ADC ( analog-digital converter ), which oc-\ncupy a significant portion of the chip. Using traditional\narchitectures, it is impossible to deploy the entire Large\nLanguage Model (LLM) on a single chip. For instance, the\nGPT-3 model has over 175 billion parameters, and it would\nrequire 2777 ISAAC chips [9] to store all of its parameters.\nThis significantly weakens the advantages of using memris-\ntor crossbars over TPU and GPU accelerators. Today, with\nthe continuous growth of LLMs, the capacity of traditional\nmemristor crossbars has become a major bottleneck.\n• Challenge 2: Non-weight stationary computations. Tradi-\ntional memristor crossbars are designed for weight-stationary\nmatrix multiplication, where one of the operands is weights\nthat can be pre-stored into the memristors. This is because\ndynamically programming the memristors and changing their\nvalues is both time and energy-consuming for model infer-\nence. However, for most language models that contain multi-\nhead attention blocks, non-weight multiplication is inevitable.\nFor example, we need to compute the matrix multiplication\namong the query, key, and value matrix. In these cases, both\nof the operands are intermediate results from the upstream\noperations. These non-weight stationary multiplications make\nit difficult to deploy LLMs directly on the memristor crossbar.\n• Challenge 3: Complex non-linear operations. The memristor\ncrossbars excel primarily in performing regular linear multi-\nplications, which are relatively straightforward computations.\nHowever, LLM architectures usually incorporate numerous\nnonlinear operations such as Softmax, LayerNorm, and oth-\ners. These non-linear operations often require several steps\nto compute. For instance, we use softmax to normalize an\narray of elements. To achieve this, the exponential value of\neach element is computed, and these values are then summed\nbefore the normalization step takes place. The existence\nof these complex non-linear operations in LLMs makes it\nchallenging to deploy them on memristor crossbars.\nWe propose a new architecture that enables energy-efficient\nmodel inference of LLM on memristor-based machine learning\naccelerators. This new architecture is capable of producing com-\nputation results that are highly comparable to those of traditional\naccelerators, with negligible accuracy loss when compared to\nstate-of-the-art devices such as TPUs and GPUs. In summary, the\nproposed memristor architecture is capable of the following:\n• Fit an entire LLM on a single chip, eliminating the extra time\nand energy caused by off-chip communications.\n• Compatible with non-weight stationary multiplication in\nmulti-head attention blocks of LLM.\n• Able to execute all the operations in LLM by decomposing\nthem into standardized sub-operations.\n• Has lower energy consumption, area, and is more robust over\na wide range of applications.\nThe paper is structured as follows: Section 2 presents a\nliterature review of related works. Section 3 provides background\nknowledge on the LLM and memristor crossbar. Section 4 in-\ntroduces our method that can decompose all the operations in\nLLM into standardized sub-operations. Section 5 introduces our\nproposed memristor crossbar architecture that can support the\nexecution of sub-operations. In Section 6, we quantitatively an-\nMulti-Head Attention\nXZ\nU\nAdd & Normalization\n(a) Component A\nX\nFeed-Forward\nX Z\nU\nAdd & Normalization\n(b) Component B\nX\nFig. 1. Two basic components that compose layers in LLM. For example,\na layer in BERT consists of one component A and one component B.\nalyze our design and compare it with state-of-the-art approaches.\nFinally, we draw conclusions in Section 7.\n2 R ELATED WORK\nBefore the emergence of non-volatile memory, two major cat-\negories of memory devices were prevalent [13]: SRAM ( static\nrandom-access memory ) and DRAM ( dynamic random-access\nmemory). An SRAM cell consists of six transistors, occupying\na relatively larger area [15]. On the other hand, a DRAM cell\ncomprises a transistor and a capacitor, storing one bit of informa-\ntion [16]. With the development of emerging memory systems like\nRRAM (resistive random access memory), even higher density\nhas been achieved. Each RRAM cell consists of a memristor (3D-\nstacked) and a transistor [17]. Unlike DRAM and SRAM, the\nRRAM is capable of storing multiple bits in each cell, rather than\njust one bit [18]. Utilizing RRAM increases the chance to deploy\nlarge-scale neural network models within a single chip or package,\nthus effectively bypassing inefficient off-chip communication.\nMemristor crossbars have shown great potential in computing\ndeep neural networks (DNNs) with higher energy efficiency than\ntraditional neural network accelerators such as TPU and GPU, in\ntraditional computer vision applications [17]. There are two basic\narchitectures using two different formats of data storage: multi-\nbit memristor and single-bit memristor. In the multi-bit design,\neach memristor stores multiple bits of information. Examples\ninclude [18, 19]. This architecture has a very high density of data\nstorage as each weight only occupies one or a few memristors.\nAnother design is the single-bit memristor, where we need mul-\ntiple memristors to store a single weight. Examples are [20, 21].\nSince the data is computed in a digital way with binarized data,\nThe design excels in handling unstable environments like noise,\nbut it sacrifices density for robustness.\nIn the literature, several improvements have been made to\nthe basic architectures of memristor crossbars to increase their\ncomputing efficiency on DNNs. Among them, PRIME [22],\nISAAC [9], and PipeLayers [23] are the most popular and typical\ndesigns. These architectures combine features from both multi-\nbit and single-bit designs and demonstrate higher energy effi-\nciency, throughput, and computation density compared to tradi-\ntional accelerators. Another type of device that people commonly\nused to compute neural networks analogously today is SRAM\ncrossbar. For example, Vesti is an SRAM-based neural network\naccelerator [24]. Compared to NVM, SRAM crossbar is easier\nto reconfigure the memory content. However, its density is only\n4% of traditional NVM devices such as RRAM [13], making it\nunsuitable for storing extremely large neural network models.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3\nTABLE 1\nOperations in LLM. The abbreviations WS and NW stand for\nweight-stationary and non-weight-stationary, respectively.\n(a) Multi-Head Attention\nNo. Formulation Type No. Formulation Type\n1 X · Wq = Q WS 4 Q · KT /√dk = S NW\n2 X · Wk = K WS 5 Softmax(S) · V = Y NW\n3 X · Wv = V WS 6 Y · Wo = Z WS\n(b) Feed-Forward\nNo. Formulation Type No. Formulation Type\n1 X · Wa = Y WS 2 ReLU(Y) · Wb = Z WS\n(c) Add & Normalization\nNo. Formulation Type\n1 ¯U = LayerNorm(Z + X) = LayerNorm(U) NW\n3 P RELIMINARIES\nIn this section, we will first introduce the background knowledge\nof language models and their internal operations. Next, we will\npresent the typical architecture of the memristor crossbar and\ndiscuss its characteristics.\n3.1 Operations in Large Language Models\nTake the language model BERT as an example. BERT utilizes a\nseries of encoder layers to process input data. Each encoder layer\nin BERT consists of two fundamental components, represented as\ncomponent A and component B in Figure 1. In other language\nmodels, the layers are also structured using the same two com-\nponents (Component A is masked in some models). Component\nA comprises a multi-head attention block followed by an add\nand normalization block. Within this component, the multi-head\nattention block takes the input denoted as X and produces an\noutput denoted as Z. The add and normalization block then takes\nthe sum of X and Z as input and generates an output denoted as ¯U.\nOn the other hand, Component B consists of a feed-forward block\nfollowed by an add and normalization block. The notation for the\ninput X, output Z, and final output ¯U in Component B follow a\nsimilar convention as used in Component A.\nThe details of the multi-head attention block are shown in\nTable 1(a). Assuming the input sequence matrix X, it is first\nmultiplied with three weight matrices individually: Wq, Wk, and\nWv. This generates matrices Q, K, and V, respectively. The next\nstep is to multiply the matrix Q with KT /√dk, which represents\nthe transposed K scaled by a factor /√dk. Here dk is a fixed value\nindicating the width of the matrix K. This multiplication operation\nresults in a matrix S, representing the attention scores between\nthe query and the key. Next, we apply the softmax function to\nnormalize S, and the result is denoted as Softmax(S). This softmax\nfunction ensures that the values in each row of Softmax( S) range\nfrom 0 to 1 and sum up to 1, representing the importance of\nrelative elements. It is defined in Equation (1).\ns′\nij = esij /(esi1 + esi2 + ··· + esin ) (1)\nHere, sij represents the element in matrix S, while s′\nij represents\nthe corresponding element in matrix Softmax( S). Once we ob-\ntained Softmax(S), we multiply it with matrix V, yielding a matrix\n78w1 - 23w2\n78\n23\n+\n0\n-\nw1\nw20\nDAC ADC\n(a)\nDAC ADC Perip. (2%)\nRegister (1%) Memristor (1%)\n51 %45 % (b)\nFig. 2. (a) A multi-bit memristor crossbar with 2 × 2 cells ; (b) The area\nbreakdown of a 128x128 memristor crossbar [9], using a shared ADC.\nY. Subsequently, we perform a matrix multiplication between the\nmatrix Y and another weight matrix Wo, producing the output\nmatrix Z.\nThe details of the feed-forward block are illustrated in Ta-\nble 1(b). The process begins with the input matrix X, which\nundergoes multiplication by the weight matrix Wa, resulting in an\nintermediate matrix Y. Subsequently, Y is multiplied by another\nweight matrix Wb, producing the final output matrix Z.\nThe details of the add and normalization block can be found\nin Table 1(c). There is one operation called LayerNorm, which\ntransforms the matrix U (the sum of X and Z) into ¯U. This\ntransformation is defined by Equation (2).\n¯uij = uij − E(ui∗)p\nVar(ui∗) + ϵ · γ + β (2)\nIn this equation, uij and ¯uij represent the corresponding\nelements in matrix U and ¯U, respectively. The parameters ϵ, γ,\nand β are trainable parameters and remain fixed during inference.\nThe terms E(ui∗) and Var(ui∗) denote the mean and variance,\nrespectively, of the elements in the i-th row of the matrix U.\nAs indicated in Table 1, operations 4 and 5 in (a), as well as\noperation 1 in (c), are classified as non-weight stationary (NW)\ncomputations. On the other hand, the remaining operations are all\ncategorized as weight-stationary (WS) computations.\n3.2 Traditional Memristor Crossbar\nTraditional memristor-based machine learning accelerators are op-\ntimized for weight-stationary matrix multiplications, making them\nwell-suited for deploying most computer vision (CV) models.\nFigure 2(a) provides an example of a memristor crossbar with2×2\ncells. This is a multi-bit design. Each cell stores a single parameter\nof the neural network, with the conductance of the cell represent-\ning the weight stored. A digital-to-analog converter (DAC) is used\nto transform activation values into voltages applied to the memory\ncell. By Ohm’s Law, the current flowing through the cell equals the\napplied voltage multiplied by the conductance. Finally, an analog-\nto-digital converter (ADC) converts the combined current from\nvarious memory cells back into digital data. Kirchhoff’s Current\nLaw ensures that the current sum is equivalent to the summation\nof the products.\nDespite the higher density of memristors compared to DRAM\nand SRAM, the effective density of the entire system remains\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4\nTABLE 2\nWe decompose the operations within the multi-head attention block into standardized sub-operations, illustrated in Equation (3) and (4). The third\ncolumn uses the abbreviations WS and NW to represent weight-stationary and non-weight stationary, respectively.\nNo. F(X · Y) = F(Z) Type X · colt(Y) colt(Z) F F(colt(Z)) t ∈\n1 XWq = Q WS\n\n\nx11 ··· x1m\n...\n...\n...\nxn1 ··· xnm\n\n\n\n\nwq\n1t\n...\nwq\nmt\n\n\n\n\nq1t = x11wq\n1t + ··· x1mwq\nmt\n...\nqnt = xn1wq\n1t + ··· xnmwq\nmt\n\n N.A.\n\n\nq1t\n...\nqnt\n\n [1, m]\n2 XWk√dk\n= K√dk\nWS\n\n\nx11 ··· x1m\n...\n...\n...\nxn1 ··· xnm\n\n\n\n\nwk\n1t\n...\nwk\nmt\n\n\n\n\nk1t = x11wk\n1t + ··· x1mwk\nmt\n...\nknt = xn1wk\n1t + ··· xnmwk\nmt\n\n /√dk\n\n\nk1t√dk\n...\nknt√dk\n\n\n[1, m]\n3 XWv = V WS\n\n\nx11 ··· x1m\n...s\n...\n...\nxn1 ··· xnm\n\n\n\n\nwv\n1t\n...\nwv\nmt\n\n\n\n\nv1t = x11wv\n1t + ··· x1mwv\nmt\n...\nvnt = xn1wv\n1t + ··· xnmwv\nmt\n\n N.A.\n\n\nv1t\n...\nvnt\n\n [1, m]\n4 EXP{QKT\n√dk\n} = EXP{S} NW\n\n\nq11 ··· q1m\n...\n...\n...\nqn1 ··· qnm\n\n\n\n\nkt1√dk\n...\nktm√dk\n\n\n\n\ns1t = q11kt1√dk\n+ ··· q1mktm√dk\n...\nsnt = qn1kt1√dk\n+ ··· qnmktm√dk\n\n\nEXP\n\n\nes1t\n...\nesnt\n\n [1, n]\n5 EXP{S} ·1 = a NW\n\n\nes11 ··· es1n\n...\n...\n...\nesn1 ··· esnn\n\n\n\n\n1\n...\n1\n\n\n\n\na1 = es11 + es12 + ··· es1n\n...\nan = esn1 + esn2 + ··· esnn\n\n N.A.\n\n\na1\n...\nan\n\n {1}\n6 EXP{S}V ⊘ (a · 1T )\n= R ⊘ (a · 1T ) NW\n\n\nes11 ··· es1n\n...\n...\n...\nesn1 ··· esnn\n\n\n\n\nv1t\n...\nvnt\n\n\n\n\nr1t = es11 v1t + ··· es1n vnt\n...\nrnt = esn1 v1t + ··· esnn vnt\n\n /ai\n\n\nr1t\na1\n...\nrnt\nan\n\n\n[1, m]\n7 R ⊘ (a · 1T ) · Wo + X\n= Z + X WS\n\n\nr11\na1\n··· r1m\na1\n...\n...\n...\nrn1\nan\n··· rnm\nan\n\n\n\n\nwo\n1t\n...\nwo\nmt\n\n\n\n\nz1t = r11\na1\nwo\n1t + ··· r1m\na1\nwo\nmt\n...\nznt = rn1\nan\nwo\n1t + ··· rnm\nan\nwo\nmt\n\n\n+xit\n\n\nz1t + x1t\n...\nznt + xnt\n\n [1, m]\nlow due to the necessity of implementing high-bit DACs/ADCs\nin the input and output circuits of the crossbar, which occupy a\nsignificant area on the chip. The area breakdown of a classical\n128 × 128 memristor crossbar in Figure 2 (b) illustrates this.\nEven with the sharing of the ADC among the 128 columns [9],\nthe DACs and ADCs still demand approximately 51% and 45%\nof the total area, respectively. The ISAAC [9] crossbar archi-\ntecture reduces the area overhead of the DAC at the expense of\nlonger computation time. Despite this, only around 2% of the\narea being allocated to memristors. While the PRIME [22] and\nPipeLayer [23] approaches eliminate the need for ADCs, their\ninput or output circuits still contain numerous capacitors, which\nconsume a significant amount of chip area.\nThe substantial area overhead caused by implementing input\nand output circuits (e.g., DACs and ADCs) within the memristor\ncrossbar, as noted in previous studies [25, 26], diminishes the\nadvantage of the high density offered by RRAM devices in\ncomparison to alternative techniques. Due to the considerable\ntime and energy consumption associated with memristor program-\nming, dynamic modification of stored values during inference\nis infrequent. It becomes necessary to pre-store the entire set\nof parameters within the memristors. However, because of the\nX\nY\nZPeripheralMemristor CrossBar Z `\nf2:  Z= F(Z)f1:Z = XɯY `\nFig. 3. The sub-operation has two phases. The first phase involves the\nlinear multiplication between X and Y, while the second phase involves\nthe addition function F applied to the multiplication result Z.\nsubstantial area overhead of peripheral circuits, the total area\noverhead for the large neural network model is huge. Therefore,\nwith classical architecture design for memristor crossbar, we have\nto deploy the large model on multiple chips with inefficient off-\nchip communication in terms of time and energy. This significantly\nlimits the application of the memristor crossbar in LLM.\n4 S TANDARDIZED OPERATION DECOMPOSITION\nThe LLM consists of a wide range of operations, including both\nlinear operations such as weight stationery and non-weight station-\nary multiplication, as well as non-linear operations like softmax\nand layer normalization. The diversity of operations within the\nLLM presents challenges for a single hardware module to effi-\nciently perform all the required functions. Without optimization,\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5\nTABLE 3\nWe decompose the operations within the feed-forward block into standardized sub-operations, illustrated in Equation (3) and (4). The third column\nuses the abbreviations WS and NW to represent weight-stationary and non-weight stationary, respectively.\nNo. F(X · Y) = F(Z) Type X · colt(Y) colt(Z) F F(colt(Z)) t ∈\n1 ReLU(XWa + ba)\n= ReLU(Y) WS\n\n\nx11 ··· x1m 1\n...\n...\n...\n...\nxn1 ··· xnm 1\n\n\n\n\nwa\n1t\n...\nwa\nmt\nba\nt\n\n\n\n\ny1t = x11wa\n1t + ··· x1mwa\nmt + ba\nt\n...\nynt = xn1wa\n1t + ··· xnmwa\nmt + ba\nt\n\n ReLU\n\n\nReLU(y1t)\n...\nReLU(ynt)\n\n [1, h]\n2 ReLU(Y) · Wb + bb\n+ X = Z + X WS\n\n\ny11 ··· y1h 1\n...\n...\n...\n...\nyn1 ··· ynh 1\n\n\n\n\nwb\n1t\n...\nwb\nht\nbb\nt\n\n\n\n\nz1t = y11wb\n1t + ··· y1hwb\nht + bb\nt\n...\nznt = yn1wb\n1t + ··· ynhwb\nht + bb\nt\n\n +xit\n\n\nz1t + x1t\n...\nznt + xnt\n\n [1, m]\nTABLE 4\nWe decompose the operations within the layer normalization function into standardized sub-operations, illustrated in Equation (3) and (4). The\nthird column uses the abbreviations WS and NW to represent weight-stationary and non-weight stationary, respectively.\nNo. F(X · Y) = F(Z) Type X · colt(Y) colt(Z) F F(colt(Z)) t ∈\n1 E(ut∗) = rowt(U) · 1 NW\nh\nut1 ut2 ··· utm\ni\n\n\n1\n...\n1\n\n\nmX\ni=1\nuti = ut1 + ut2 + ··· + utm /m\nmX\ni=1\nuti/m [1, n]\n2 E(u2\nt∗) = rowt(U)·rowt(U)T NW\nh\nut1 ut2 ··· utm\ni\n\n\nut1\n...\nutm\n\n\nmX\ni=1\nu2\nti = u2\nt1 + u2\nt2 + ··· + u2\ntm /m\nmX\ni=1\nu2\nti/m [1, n]\nit would be necessary to employ separate hardware modules to\nhandle the diverse computational requirements. To enable the\nseamless implementation of LLM on a unified hardware module,\nwe decompose all the operations of LLM into standardized sub-\noperations, as illustrated in Equation (3).\nF(X · Y) = F(Z) (3)\nAs depicted in Figure 3, each standardized sub-operation within\nthe LLM consists of a fundamental linear operation executed\nby memristor-based crossbars and an additional F executed by\nperipheral module. The linear operation involves multiplying a\nmatrix X with a matrix Y, resulting in a matrix Z. Depending on\nthe specific context, this linear operation can be either weight sta-\ntionary or non-weight stationary. In the case of weight-stationary\nmultiplication, the matrix Y can be replaced by a weight matrix\ndenoted as W. The additional operator F can be either linear,\nsuch as multiplication, addition, or non-linear, incorporating func-\ntions like the exponential function (EXP), Rectified Linear Unit\n(ReLU), division, and various others.\nFor easier hardware implementation, we can further decom-\npose each sub-operation into multiple sessions, as shown in\nEquation (4). We denote col t(Y) and colt(Z) as the t-th column\nof matrices Y and Z, respectively. In each session, we only\ncompute the multiplication between X and the vector col t(Y).\nThis operation can be performed by any hardware that supports\nmatrix-vector multiplications.\nF(X · colt(Y)) = F(colt(Z)), t = 1, 2, 3 ··· (4)\nThe standardized operation decomposition for LLM is listed\nin Table 2 for the multi-head attention block, Table 3 for the feed-\nforward block, and Table 4 for the layer normalization function.\nIn the third column of each table, we label the type of each\nsub-operation. The abbreviation WS represents weight-stationary\nmultiplication, and NW represents non-weight stationary multipli-\ncation. In the fourth and fifth columns, we provide the details of\nmatrix Z, vector colt(Y), and vector colt(Z). We assume the input\nsequence consists of n tokens and the model has a hidden size of\nm. The sixth and seventh columns list the additional operation F,\nand the corresponding result under this function, i.e., F(colt(Z)).\nThe abbreviation N.A. indicates that no additional function is\nrequired for this sub-operation. In the eighth column, we specify\nthe range of session index t. For either the multi-head attention\nblock (Table 2) and the feed-forward block (Table 3), the final\noutput is obtained as the sum of the input X and the intermediate\noutput Z. This sum, represented as Z+X = U, serves as the input\nto the subsequent add and normalization block.\n4.1 Softmax Operation\nThe softmax operation is a crucial component in multi-head\nattention blocks. After multiplying the matrix Q with KT /√dk,\nthe resulting matrix S undergoes a softmax operation along each\nrow. Subsequently, the softmax result is multiplied by the matrix\nV. The aforementioned computation process can be divided into\nthree standardized sub-operations, represented as sub-operations\n4, 5, and 6 in Table 2. All of these sub-operations are non-\nweight stationary. The details of the decomposition process can\nbe summarized in the following steps.\n1) In sub-operation 4, we multiply matrix Q with KT /√dk to\nobtain matrix S. This operation is performed in n sessions.\nDuring session t, matrix Q is multiplied with the t-th column\nof matrix KT /√dk. We incorporate the additional function\nF as EXP, which transforms each element in matrix S from\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6\nMatrix Mul\nSoftmax\nMatrix Mul\nSub-OP #1\nTime Flow\na\n(a) Before Transformation\nb c\na\nb\nSub-OP #2\nc\nSub-OP #3\n(b) After Transformation\nFig. 4. We decompose the softmax into three parts, and two of these\nparts can be integrated with the preceding and subsequent matrix\nmultiplication operations, resulting in three standardized sub-operations.\n(b) Traditional Architecture(a) Our Architecture\nXbar 2\nW2\nXbar 3\nW3\nXbar 1\nW1\nCompute\nXbar\nW1\nW2\nW3\nDense Xbar\nFig. 5. Overview of our proposed architecture and the traditional ar-\nchitecture. We use unique colors to indicate the locations of weight\nmatrices, denoted as W1, W2, and W3. The dark arrows among the\ncrossbars indicate the data flow direction.\nsij to esij . As a result, we can obtain the matrix EXP (S)\nfrom this sub-operation.\n2) In sub-operation 5, we multiply the matrix EXP (S) with\nvector 1, where vector 1 is defined as a vector with all\nelements being 1. This sub-operation is performed in a single\nsession (t ∈ {1}), with no additional operation F involved.\nEach element in the final output vector, denoted by at, is\nthe summation of all the elements in the i-th row of matrix\nEXP(S), as illustrated in Equation (5).\nai = esi1 + esi2 + ··· + esin =\nX\nj\nesij (5)\n3) In sub-operation 6, we multiply matrix EXP (S) with the\nmatrix V, resulting in matrix R. This operation is also\nperformed in m sessions. During session t, matrix EXP (S)\nis multiplied with the t-th column of matrix V. In the output\nmatrix, element rik in matrix R is calculated as Σj(esij ·vjk).\n4) Finally, we perform scaling on the matrix R. This is done\nby applying division as the additional operation F in sub-\noperation 6. Specifically, the i-th column of matrix R is\nscaled by ai. Mathematically, matrix R is element-wise\ndivided (symbol ⊘) by (a ·1T ). This results in matrix Y (Ta-\nble 1(a)), whose element yik becomes Σj(esij ·vjk)/Σjesij ,\ncorresponding to the softmax results.\nFigure 4 outlines the transformation process for the softmax\noperation, enabling its execution on memristor-based crossbars.\nWe begin by decomposing the softmax into three components.\nAfterwards, two of these components can be integrated with the\nmatrix multiplication operations that precede and follow them,\nyielding three standardized sub-operations.\n4.2 Layer Normalization\nLayer normalization is essential for both multi-head attention\nblocks and feed-forward blocks. Let’s assume the input to the\nnormalization block is U = Z + X. The normalization process\nis applied to each token individually, where the t-th token is\n(b) Traditional Architecture(a) Our Architecture\nTask τa\nStage 1\nTask τa\nTask τa\nTask τa\nTask τa\nStage 2Stage 3\nTask τbTask τc\nTask τb Task τa\nFig. 6. A breakdown of the computation process for both architectures\nacross multiple steps. The gray arrows indicate the time flow.\nrepresented by row t(U) (i.e., the t-th row of the matrix U). We\nuse ut∗ to denote elements in rowt(U). Prior to normalization, it is\nnecessary to determine the means and variances of these elements,\ndenoted as E(ut∗) and E(u2\nt∗), respectively. These values enable\nus to compute the variance using Equation (6).\nVar(ut∗) = E(u2\nt∗) − (E(ut∗))2 (6)\nThe calculation of the means of ut∗ and u2\nt∗ can be decomposed\ninto two standardized sub-operations, as shown in Table 4. Both of\nthem are non-weight stationary. To calculate the sum of elements\nut∗ in rowt(U), we can multiply this row vector with a vector 1\nthat contains all elements as 1. Similarly, by multiplying rowt(U)\nwith itself, we can obtain the sum of squared elementsu2\nt∗. In both\nsub-operations, we incorporate additional operations F multiplied\nby 1/m, where m represents the number of elements in row t(U).\nConsequently, we can obtain E(ut∗) and E(u2\nt∗) from the sub-\noperations. Since there are n tokens in the sequence, the above\ncomputations are executed in n sessions, where each session\ncorresponds to processing one token. Specifically, the t-th token\nin the input sequence is processed during session t.\nAfter obtaining E(ut∗) and E(u2\nt∗), we can utilize equation\n(6) to calculate Var (ut∗). To normalize the vector, we begin by\nsubtracting each element in the vector by its mean E(ut∗) and\nthen multiply it by a scaling factor, denoted as a in Equation (7).\nα = γ/\nq\nVar(ut∗) + ϵ (7)\nHere, γ and ϵ are known parameters from the model. A dedicated\nmodule can be utilized to perform the square root and division\noperations. Due to the need for computation of the aforementioned\nprocess only n times within each LLM layer, the requirement for\nsuch modules is minimal, resulting in a negligible area overhead.\nOnce we obtain α, we have the option of directly multiplying it\nwith ut∗. Alternatively, we can incorporate this multiplication as\nan operation F within the subsequent computation blocks.\n5 M EMRISTOR CROSSBAR ARCHITECTURE FOR\nSTANDARDIZED SUB-OPERATIONS IN LLM\nWe have developed an advanced architecture for memristor cross-\nbars that enables efficient computation of the standardized sub-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7\noperations (Equation (4)) in LLM. Our proposed system utilizes\ntwo types of crossbars: the computation crossbar, which is opti-\nmized for low-energy computing, and the dense crossbar, which\nis designed specifically for deploying large-scale neural networks.\nBoth crossbar types are integrated onto the same chip to eliminate\nthe need for inefficient off-chip communication.\n5.1 Architecture Overview\nFigure 5(a) illustrates the overview of our proposed architecture,\nwhile Figure 5(b) shows an example of a traditional architecture\nwith the same weight capacity. To simplify the analysis, we\nassume that the model contains only three weight matrices. In both\nfigures, we use unique colors to indicate the locations of weight\nmatrices, denoted as W1, W2, and W3. In traditional architecture\n(Figure 5(b)), the weights stored within the crossbars are fixed.\nWeight matrices W1 to W3 are stored in Crossbar- 1 to Crossbar-\n3, respectively. The dark arrows among the crossbars indicate\nthe data flow direction. In contrast, our architecture (Figure 5(a))\nconsists of computation crossbars and dense crossbars. The com-\nputation crossbar executes both weight-stationary and non-weight-\nstationary multiplications, where weight matrices W1 to W3 are\nstored in the dense crossbar. This results in a much smaller area\noverhead than the traditional architecture, due to the high area\nefficiency of the dense crossbars.\nIn Figure 6, we present a breakdown of the computation\nprocess for both architectures across multiple stages. At the initial\nstate, the computation crossbar is empty and contains no data\ninformation. To address this, we have developed a mechanism\nto instantly reconfigure the memory storage in the computation\ncrossbar. For weight-stationary computation, the weight matrix\nWi is transferred into the computation crossbar at Stage i. In total,\nthree stages are required to execute all the computations for task\nτa. For non-weight stationary multiplication, another operand Y\nneeds to be transferred into the compute crossbar.\n5.2 Efficient Encoding for the Sub-Operation\nWe have developed a mechanism that achieves instantaneous\nreconfigurability in the memristor crossbar. It is illustrated in\nFigure 7. In this approach, each memristor can exist in either an\n“on” or an “off” state. When in the ”on” state, we can read a\nfixed data value, denoted as χ, from the memristor. It is important\nto note that the value of χ remains unchanged throughout the\nentire computation process. On the other hand, when in the “off”\nstate, the memristor can only be read as 0. To perform any\nmultiply-accumulate (MAC) operation, we employ an encoding\ntechnique where the weights serve as input, while the activations\ncontrol the state of the memristors. The activation data needs to be\nencoded into multiple digits to enable digit-by-digit computation.\nIn our example, we utilize the balanced septenary (base-7) numeral\nsystem for this encoding.\nIn the balanced septenary numeral system, each digit can take\none of seven possible values: -3, -2, -1, 0, 1, 2, or 3. For instance,\na given data value, let’s say 78, can be encoded into three digits:\n2, -3, 1. The expansion of 78 in the balanced septenary numeral\nsystem can be expressed as 2 × 72 − 3 × 71 + 1 × 70 = 78 .\nBy employing this type of encoding scheme, we can perform\nMAC operations efficiently, optimizing area overhead and energy\nconsumption within the memristor crossbars. To cover all possible\nvalues of a single digit, we implement four memristors for each\n2w1 - 0 w2\nw2\nw2\n+\n1/0\n-\n1/0\n2/02/0\nTime Step 0\n×7DAC\nADC\nw1\nw1\n1/01/0\n2/02/0\n+ -\nTime Step 2\nw2\nw2\nw1\nw1\n+ -\nTime Step 1\nw2\nw2\nw1\nw1\n14w1 - 0 w2×7 77w1 - 21w2\n1/01/0\n2/02/0\n1/01/0\n2/02/0\n1/01/0\n2/02/0\n1/01/0\n2/02/0\nTime Step 0.5 Time Step 1.5\nTime Flow\n78w1 - 23w211w1 - 3w2\nFig. 7. Each memristor can be turned on to output fixed data value or\nturned off to output 0. By controlling the on and off state of the mem-\nristor, we can perform either weight stationary or non-weight stationary\nmultiplication in the crossbar.\nactivation. These memristors are arranged as follows: two mem-\nristors in the positive (left) column, storing 1 and 2, and two in\nthe negative (right) column, also storing −1 and −2.\nTo illustrate the multiplication of a positive weight w1 with\nthe activation 78, we first obtain the three digits representing the\nvalue 78: 2, -3, 1. Each digit corresponds to a specific computing\ntime step. In Figure 7, the top four memristors (colored purple)\nrepresent the activation 78. During step-0 of computation, only\nthe memristors storing 2 are turned on, representing the first digit\n2. The remaining memristors are turned off and indicating value\n0. During step-1, the memristors storing −1 and −2 are activated,\nrepresenting the second digit -3. The other memristors are turned\noff. Step-2 follows a similar rule, with the corresponding mem-\nristors being activated based on the third digit 1. Between every\ntwo steps, we multiply the accumulated result by the base value\n7 since the previously processed digit holds higher significance\nin the overall value. If w1 is negative, we exchange the states of\nthe memristors in the two columns, reflecting the sign change. By\nfollowing this process, we can effectively perform multiplication\noperations between the weight and the activation values, utilizing\nthe four memristors per activation to cover all possible digit values.\n5.3 Robust Computation Crossbar\nWe introduce a computation crossbar that is compatible with the\nabove encoding scheme. In this design, the memristors within\nthe computation crossbar always store the same data value, re-\ngardless of the values of the activations. Once we establish the\nencoding format, there is no need to update the data stored in the\nmemristors. This holds true even when we change neural network\nmodels. Consequently, there is no requirement to implement actual\nmemristors and program them prior to usage. Instead, we employ\nregular resistors with fixed resistance to function as memristors,\neffectively storing the specified data. This approach enhances the\nresilience of the computation crossbar against random telegraph\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n8b\n-\nX1a\nX1b\nX2a\nX2b\n2b\n2b\n1b 1b 1b 1b 1b 1b\nColumn Address Decoder\nScheduler\n-\n(a)\nComputation\nCrossbar\n(b)\nDense\nCrossbar\n2b\n2b\nRegister\n×7\n2\nX\nnb\nnb\nn-bit DAC\nn-bit ADC\n8b\n8b\n8b\n8b\nX1a\nX1b\nX2a\nX2b\n´ W1\n´\n´\n´\n3b2b …\n…\n…\n…\n……\n……\n……\n……\n……\nIdx Act\nEncoder\n+\n8b\n--\n×7\n+\nW2\nBuffer MUX MUX\n…\nControl\n…… …\n2b\n…\n…\nY1\nY2\n´\n1\n…\n…\n…\n…\n…\n+- +-\nF(Z) F(Z)Z Z\n8b 8b\na\nb\na\nb\nFig. 8. (a) In the computation crossbar, we fix the data stored in each resistor and use the attached switch to control the stored data to be either χ\nor 0; We have two types of rows (“a”, “b”) and two types of columns (“+”,“-”); (b) The dense crossbar stores all the weights of the models.\nnoise (RTN) [27]. Unlike memristor, the likelihood of defects\noccurring in the resistors is relatively low after undergoing post-\nfabrication examination [28]. Consequently, the resistors do not\nneed to operate in the low-resistance mode [29] to counteract RTN,\nresulting in energy savings [30].\nWe have devised the computation crossbar that builds upon\nthe classic 1T1R (one-transistor-one-resistor) design [31]. The\nstructure of our computation crossbar can be seen in Figure 8(a).\nIn addition to substituting memristors with conventional resistors,\nwe have made four significant modifications to our design. These\nmodifications are as follows:\n1) In the classical 1T1R structure [31], each memristor is con-\nnected in series with a transistor, serving as a control switch\nto regulate the current flow through the memristor. Typically,\ndevelopers use this switch to enable or disable the program-\nming functionality of the memristor during the computation\nphase [31]. In our new architecture, we retain the transistor-\nswitch design, which is attached to the memristor/resistor,\nand utilize the transistor to control the on-and-off state of the\nmemristor/resistor.\n2) In the balanced septenary (base-7) system, we utilize four\nresistors to handle each input data. These four resistors are\nplaced into two rows (“a”, “b”) and two columns (“+”,“-”). To\noptimize the switch control, we introduce dedicated registers\nthat are directly connected to the memristor switches, storing\nthe control information. The register responsible for storing\nthe encoded data only requires three binary bits. One bit\nis used to select the column, while the remaining two bits\ncontrol the resistors within the selected column. The resistors\nin the unselected column are effectively cut off or deactivated.\n3) In our computation crossbar design, we decompose the\ncomputation into multiple time steps. At each time step,\nthe output needs to be multiplied by the base value of the\nencoding scheme before proceeding to the next time step.\nFor instance, in the balanced septenary encoding system,\nthe base value is 7. To optimize the processing time, we\ncan perform the multiplication by 7 in two steps. First,\nwe utilize shifting operations to the original value by three\nbits, which effectively multiplies the output by 8. Then, we\nsubtract the original value from this result to obtain the\nfinal multiplication by 7. Other base values can use similar\nrules to optimize because all of them can be expressed as\n2i − 1. These operations are executed within shift-and-add\n(S+A) units, whose energy consumption is relatively small\ncompared to other components in the system [9]\n4) An module for processing additional function F is imple-\nmented at the end of the linear computation. This function\ncan be implemented using either a digital circuit or an analog\ncircuit. The analog circuit also offers well-established solu-\ntions for basic operations such as exponential, multiplication,\nsummation, and more [32][33][34]. Similar to the ADC,\nthe peripheral module for function F is shared among the\ncolumns in the crossbar in an interleaved manner [9]. Hence,\nits overall impact on the area cost is minimal.\nAn encoder is employed to convert the activation from the original\nbinary system into the new encoding system. Our design supports\nany type of balanced numeral system for encoding. In general,\neach activation can be encoded using 2S resistors within a\nbalanced base 2S+1 − 1 system. The scaling factor, represented\nby S, is a crucial parameter that influences the characteristics of\nthe encoding. We should choose the right value of S based on\nthe required precision of the activation. In this example, S = 2\nis utilized, representing the balanced septenary (base-7) encoding\nsystem. In Section 6 of our study, we will conduct a comparative\nanalysis of different encoding schemes, ranging from S = 1 to 7,\nin order to identify the ideal encoding base value while considering\na specific precision requirement for activation data.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9\n5.4 Dense Crossbar with High Capacity\nOur system is specifically designed to support large-scale neural\nnetwork models by utilizing an additional crossbar with a substan-\ntial storage capacity. This new hardware is referred to as the “dense\ncrossbar” due to its high density of memristors. The capacity of\nthe dense crossbar is easy to accommodate the size requirements\nof various neural network models, due to the high density of the\nmemristors. The structure of the dense crossbar closely resembles\na traditional memory design, as depicted in Figure 8(b). There are\ntwo significant features:\n1) Low-resolution DAC and ADC: Both the Digital-to-Analog\nConverter (DAC) and Analog-to-Digital Converter (ADC)\nemployed in our system have low resolutions of 1 bit and\n2 bits, respectively. The DAC functions by enabling or\ndisabling the entire column of memristors, while the ADC\nincorporates a sense amplifier to recover the signal. Utilizing\nlow-bit DAC and ADC resolutions significantly enhances\nenergy efficiency and reduces the required area compared to\nhigher-resolution alternatives [25].\n2) Individual column activation: At any given time, only one\ncolumn of memristors is activated within the dense crossbar.\nThis means that the current flowing through one memristor\ndoes not interfere with the current from other memristors in\nadjacent columns. This unique characteristic enables accu-\nrate data retrieval from large-scale crossbars without signal\ninterference or degradation.\nBy incorporating the dense crossbar and computation crossbar\nwithin a single chip or package, we eliminate the inefficiency\nof off-chip communication. This design enables energy-efficient\ndeployment of large-scale neural network models, particularly\nextremely large language models. Furthermore, our dense cross-\nbar provides comprehensive support for various configurations\nof memristors in any number of bits. In the specific example\nillustrated in Figure 8(b), we assume that each memristor can store\ntwo bits of information. This particular configuration represents a\nbalance between the complexity of ADC and the additional area\nneeded for implementing memristors.\nOur architecture is compatible with a wide variety of memris-\ntors types. Memristors are implemented within the dense crossbar\nand arranged as a traditional memory bank. Considering the\nrobustness of the dense crossbar in this particular organizational\nstructure, the resolution and accuracy requirements for the mem-\nristors are relatively flexible. Therefore, our architecture can also\nsupport future advanced memristors with greater performance.\n5.5 Computation Process of Sub-Operation\nWe illustrate the computation process of the sub-operation in Fig-\nure 9, depicting the sequential steps involved, assuming the linear\npart of the sub-operation is a weight-stationary multiplication. We\nuse register W i to store the weight, register X i (including Xia at\nrow a and Xib at row b) to store the activation and register Z to\nstore the computation result. The locations of these registers in the\ncomputation crossbar are shown in Figure 8(a). In this example,\nwe adopt the balanced septenary (base-7) encoding system. Each\nactivation xi,j (highlighted in red) is encoded into three digits.\nFor each digit, we utilize four memristors. Two memristors in the\npositive column (storing 1 in row a and 2 in row b) and two in\nthe negative column (storing −1 in row a and −2 in row b). The\nstates of the four memristors are denoted as a+\ni , a−\ni , b+\ni , and b−\ni ,\nwhere i represents the index of the digital, and a/b along with\nZ\nW1\n…\n…\n…\n…\n…\na2\n+\na2\n-\na1\n+\na1\n-\na0\n+\na0\n-\nb2\n+\nb2\n-\nb1\n+\nb1\n-\nb0\n+\nb0\n-\n…\n…\n…\n…\nX1a a2\n+\na0\n-\nb2\n+\nb0\n-\n…\n…\na2\n+\na0\n-\nb2\n+\nb0\n-\n…\n…\nw1,t w1,t+1\nx1,1\n…\nx2,1\n…\nx1,1\nX1b\nDense XbarComputation Xbar\nIdx … …t t+1\nAct … …\nSession t+1Session t\nz1,t+1z2,tz1,t\nF(Z) … …F(z1,t) F(z2,t) F(z1,t+1)\nFig. 9. The computation process of the sub-operation. Assuming W i\nstores the weight, Xi stores the activation, and Z stores the computation\nresults. The states of the four memristors are denoted as a+\ni , a−\ni , b+\ni ,\nand b−\ni , where i represents the index of the digital, and a/b along with\n+/− denote the location of the memristors.\n+/− denote the location of the memristors. As three digits are\nrequired to encode each activation xi,j, a total of 4 × 3 = 12\nmemristor states are required. During computation, we load the\n12 memristor states of activation xi,j into the respective registers\nXia and Xib digit by digit.\nAs depicted by Equation (4), our proposed approach partitions\nthe computation of sub-operations into multiple sessions. In ses-\nsion t, we load multiple weights from the dense crossbar to the\ncomputation crossbar. For instance, weight w1,t is loaded into\nWi and sequentially multiplied with activations x1,1, x2,1, and so\non. As an example, the system performs the multiply–accumulate\n(MAC) operation by adding the product of w1,t and x1,1 to other\nmultiplication products. The column generates outputs z1,t, z2,t,\nand so forth. They are passed to the additional function block,\ndenoted as F, and we obtain the final result F(z1,t), F(z2,t), and\nso on. Once all possible activations have been traversed in session\nt, session t + 1 begins. New weights are loaded from the dense\ncrossbar to the computation crossbar and the same sequence of\nactivations from session t is repeated. This process continues for\nsubsequent sessions.\nTo expedite the computation time, we have employed a du-\nplication technique for the computation columns associated with\nthe same set of weights. This duplication approach significantly\nenhances the level of parallelism within each session. For instance,\nas illustrated in Figure 8(a), concurrently, we can calculate the\nmultiplication between weights from W i and activations from\nanother activation register X′\ni in the second column of the compu-\ntation crossbar. We load activations with even indexx2i,t into X1a\nand X1b, while activations with odd indicesx2i+1,t are loaded into\nX′\n1a and X′\n1b. This approach can reduce the processing time for\neach session by half. If the crossbar allows the implementation of\ndc columns, the processing time of each session can be further\nreduced to 1/dc of the original value.\nOur architecture is also capable of performing non-weight-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10\nCache T1 Cache T2 Cache S\nPhase 1\nCache D1\nPhase 2\n(a) Cache Management for Sub-Operation #1~#4\nQX S\nQX EXP{S}\nCache T2Cache T1Cache S\nPhase 3\nCache D2\nPhase 4\n(b) Cache Management for Sub-Operation #5~#7\nV ZEXP{S}\nV Z+XEXP{S}\nK/d k\nK/d k\nRӾ(a·1T)\nRӾ(a·1T)\nFig. 10. The cache management of the multi-head attention layer. Five\ncaches are implemented, named Cache D1, D2, T1, T2, and S. Data\nstored within the caches are highlighted with colored blocks, while\nunstored data is represented by dashed blocks.\nstationary multiplication, which is essential for the multi-head\nattention block present in most language models. As depicted in\nFigure 8, the multiplexers connecting the computation crossbar\nand dense crossbar play a crucial role in selecting the appropriate\nsource for computation. They can choose between the weight\nvalues from the dense crossbar or another activation register Y i.\nThe computation process of non-weight stationary multiplication\nis similar to that of weight stationary multiplication.\n5.6 Cache Management System\nTo temporarily store the intermediate results of the model infer-\nence, we need to implement five caches named Cache D1, D2,\nT1, T2, and S. When executing the multi-head attention (MHA)\nlayers, T1 and D2 are used to store the input and output data,\nwhile caches T1 and T2 are employed to store temporary results.\nCache S, on the other hand, is utilized to store the softmax result.\nWhen executing the feed-forward (FF) layers, only Cache D1 and\nCache D2 are utilized to store the input and output data.\nAn example of the cache management flow for the MHA\nblock is depicted in Figure 10. Similar approaches would be taken\nfor cache management in the FF layers. Assuming each input\nsequence consists of two tokens, and each token has a dimension\nof 6, a sequence can be represented as a 2 × 6 matrix. In this\nfigure, data stored within the caches are highlighted with colored\nblocks, while unstored data is represented by dashed blocks. We\ndivide the entire computation process into two parts. The first part\nin Figure 10(a) involves sub-operations 1 ∼4, and the second part\nin Figure 10(b) involves sub-operations 5∼7.\nCache D1 should have the capacity to store the entire matrix\nof the input sequence X. With the input data X, we can compute\nQ, K, and V column by column using our computing block. Cache\nT1 and T2 do not need to store the entire sequence, as subsequent\noperations can still be performed using partial columns from Q\nand K. In this example, Cache T1 and T2 are configured to store\nTABLE 5\nThe cache size of our architectures. If the duplication factor exceeds\nck, then a cache capacity larger than the typical value is required.\nCache Size $ D1 $ D2 $ T1 $ T2 $ S\nTypical ls · dk ls · dk ls · ck ls · ck ls · ls\nMaximum ls · dk ls · dk ls · dk ls · dk ls · ls\nhalf of the columns in the sequence, which is why we need to\nundergo two phases to complete the computation of all columns.\nDuring each phase, the partially computed results of the softmax\nmatrix S are accumulated in cache S. Until the final phase where\nthe result is completed, we perform the additional exponential\nfunction on matrix S to obtain EXP(S).\nOnce the EXP(S) matrix is stored in cache S, we can clear the\ndata stored in caches T1 and T2 and repurpose these two caches.\nSpecifically, the computed matrix V is stored in cache T1, and the\nsubsequent result R is stored in cache T2. Similar to the previous\npart, we divide the computation process into two phases, and in\neach phase, only half of the columns in matrices V and R are\nstored in caches T1 and T2, respectively. During each phase, the\npartially computed results of the output matrix Z are accumulated\nin cache D2. Until the final phase where the result is completed,\nwe perform the additional summation function on matrices Z and\nX to obtain matrix Z+X.\nIf we duplicate more computational crossbar units to achieve\na speedup in computation, it is possible that cache T1 and T2\nmay not be sufficiently large to accommodate the intermediate\ncomputation results. In the extreme scenario, they should have\nthe capacity to store the entire sequence of data, similar to how\ncache D1 and D2 do. To summarize, we have listed the typical\nand maximum sizes of each individual cache in Table 5. Here,\nthe parameters ls and dk represent the number of tokens in the\nsequence and the dimension of each token, respectively. The\nparameter ck represents the number of columns that can be stored\nin cache T1 and T2. If the duplication factor exceeds ck, then a\ncache capacity larger than the typical value is required.\n6 E XPERIMENTS\nOur assessment of LLM accuracy is based on PyTorch implemen-\ntation using Hugging Face’s package. RTN Fluctuation functions\nare applied to activations and weights during computation to\nsimulate device noises. Quantization functions are applied to\nsimulate real device conditions. To ensure fair comparisons, we\nadjust the resistance range of memristors for each crossbar archi-\ntecture, guaranteeing equal accuracy levels across architectures.\nAccuracy evaluation is conducted on language tasks from the\nGLUE dataset [35]. We fine-tuned the pre-trained models for 5\nepochs on tasks SST-2, QQP, MNLI, and QNLI. For the remaining\ntasks, which are relatively small, we fine-tuned them for 10\nepochs. The batch sizes for BERT Base and BERTLarge are 32 and\n16. The experiments are performed on 2080TI GPU cards, with\neach experiment completed within one day.\nWe use simulation tools [36, 37] to evaluate area overhead,\nenergy consumption, and latency. The simulator includes noise\nmodels and non-ideality models for memristors. The noise param-\neters utilized in the simulation are derived from measured data\nobtained from real fabricated memristor devices [17]. These tools\nare built with synthetic data from EDA tools [38] and calibrated\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11\nTABLE 6\nHardware parameters used in the simulations. The cache parameters\napply across the entire architecture level.\nPer Computation Crossbar Per Dense Crossbar\nType Qty. Type Qty.\nDAC 8-bit ×128 1-bit ×64k\nADC 8-bit ×1 2-bit ×1k\nMemristor Resistor 128 ×128 Regular 1k×64k\nRegister 6-bit ×128×64 8-bit ×256\nCache1 D1:256kB D2:256kB T1:16kB T2:16kB S:64kB\n1 Assuming sequence length of 256 tokens with 1024 dimensions\nTABLE 7\nComparison among base values, assuming the required data precision\nfor activations is INT8 (i.e., 8-bit integer)\nBase Value 3 7 15 31 63 127 255\nScale Factor S 1 2 3 4 5 6 7\nDigits/Cycles 6 3 3 2 2 2 1\nScale-Cycle Product 6 6 9 8 10 12 7\nusing experimental data from real chips called Novena [39].\nTable 6 provides a list of the hardware parameters employed in the\nsimulation. This includes our settings for DAC, ADC, memristor,\nand register on a per-crossbar basis, as well as the configurations\nof caches at the architectural level.\nWe conducted tests on various Language Models (LLMs)\nincluding BERT, Phi-1.5 [40], GPT-2, T5, LLaMa, and GPT-3 to\nevaluate the performance of the crossbars. In addition to LLMs, we\nalso evaluated the CV models (ResNet). We assume these models\noperate with 8-bit activations and 8-bit weights. The sizes of the\nmulti-bit, single-bit, and computation crossbars are assumed to be\n128 × 128. The size of the dense crossbar is 1k × 64k, matching\nthat of a DRAM bank [16]. Following the setting in ISAAC, each\nADC is shared among 128 columns as its switching speed is 128\n× faster than the memristors [9].\nWe follow the evaluation methodology employed in three\nhighly cited works: PRIME [22], ISAAC [9], and PipeLayer [23].\nTo make a fair comparison of area overhead and energy consump-\ntion, we utilize the circuit components from ISAAC [9] as a basis\nfor all the architectures. In our experiments, we override the device\nparameter file in the simulator to update the respective parameters.\nThe crossbars consist of DACs, ADCs, registers, memristors,\nand peripheral circuits such as sample-and-hold, shift-and-add,\nencoder, and the operation unit F.\nFor the DACs, we rely on the analysis presented in [25] to\ndetermine their area and energy consumption. The ADC model is\nbased on [26]. Following the approach used in ISAAC, we scale\nthe area and energy of single-bit DACs and 2-bit ADCs using the\nanalytical models from [25]. The sample-and-hold circuit data is\nsourced from [41]. The area and energy consumption of the shift-\nand-add circuit is determined based on the analysis conducted in\nDaDianNao [42]. In the experiment, we choose to utilize a digital\ncircuit to implement function F. Metrics on the unit F and the\nencoder are retrieved from the synthesis report of EDA tools [38].\nWe employ the CACTI 6.5 [43] tool to model the energy and area\nof registers. The energy and area model for memristors is derived\n0\n0.5\n1.0\n1.5\n2.0\n1 3 4 5 6 72\nNormalized Energy\nFactor S (Base 2S+1-1)\n(a) Energy Breakdown\n0\n1.5\n3.0\n4.5\n6.0\n1 3 4 5 6 72\nNormalized Area\nFactor S (Base 2S+1-1)\n(b) Area Breakdown\n0\n2.5\n5.0\n7.5\n10\n1 3 4 5 6 7\n2\n2\nArea-Delay Product\nFactor S (Base 2S+1-1)\n(d) Area-Delay Product\n0\n2\n4\n6\n8\n0 2 4 6 8\nDAC ADC Peripheral Register Memristor\n Factor S (Base 2S+1-1) Minimum Area-Delay Product\nNormalized Latency\nNormalized Area\n(c) Area-Delay Relationship\n1\n2 3\n4 5 6\n7\nFig. 11. Comparison among various scaling factors S (base value\n= 2 S+1 − 1), assuming the required data precision for activations\nis INT8 (i.e., 8-bit integer): (a) Energy breakdown of the computation\ncrossbar; (b) Area breakdown of the computation crossbar; (c) Area-\nlatency distribution; (d) Area-delay product.\nfrom [17]. All these components are assumed to be fabricated\nunder the 32 nm node, the same as ISAAC.\nThe memristors we employed in our experiments is a TaOx-\nbased device, demonstrating approximately 1% error rate when\nconfigured with 32 (5-bit) conductance levels [17]. This is equiv-\nalent to nearly 100% accuracy when programmed with just 4\nconductance levels (2-bit) within our dense crossbar. We chose\nthis device based on the specifications outlined in ISACC [9], in\norder to establish a fair comparison between our architecture and\ntheirs. It offers a precision level sufficient to accommodate our\narchitectures and most other architectures, such as PRIME [22]\nand PipeLayer [23], ensuring reasonable model accuracy.\nIn our evaluation, we compare our work with these three\nstate-of-the-art RRAM solutions: PRIME, ISAAC, PipeLayer, as\nwell as an area-efficient SRAM-based crossbar called Vesti [24].\nIt is important to note that these architectures do not support\nnon-weight stationary multiplications. Therefore, our comparison\nfocuses solely on the weight stationary computation aspect. The\ncalculated metrics for PRIME, ISAAC, and PipeLayer only in-\ncludes modules for WS operations. Our architecture encompasses\nboth the computation and dense crossbars so that we can execute\nthe same set of operations, .\nAdditionally, we compare our work with Google’s TPUv4\naccelerator and Nvidia’s A100 GPU, which represent state-of-\nthe-art high-performance solutions for machine learning applica-\ntions. During the accuracy evaluation of LLM on language tasks,\nTPU/GPU operates in full precision mode, while our architecture\nutilizes quantized weights and activations.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12\nTABLE 8\nThe comparison of accuracy between our architecture and the baseline on the GLUE tasks. Simulation results show that the environmental noise\ncontributes less than 5% to the signal level, which is typical for real devices [44]. Weights and activations are quantized into 8 bits.\nCoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE WNLI\nM.C. Acc. F1 Acc. Pea. S.C. Acc. F1 Acc. MM. Acc. Acc, Acc.\nBERTBase - Baseline 58.03 93.00 88.39 83.58 88.96 88.48 91.00 87.80 83.68 83.54 90.66 65.34 30.99\nBERTBase - Ours 59.07 93.12 89.20 85.05 88.92 88.45 90.91 87.80 83.77 83.53 90.57 62.45 30.99\nBERTLarge - Baseline 62.63 92.43 91.54 87.99 89.72 89.53 91.22 88.17 86.01 86.11 92.37 68.23 57.75\nBERTLarge - Ours 62.43 92.78 91.77 88.48 89.68 89.47 91.13 88.16 86.23 86.15 92.22 68.23 57.75\nTABLE 9\nThe running accuracy of Phi-1.5 [40] and GPT -2 models on our\narchitecture. The label (3-c) refers to 3-cycle computation for each\ncolumn (same as other experiments), and (4-c) refers to 4-cycle.\nPhi-1.5 GPT-2\nBase- Ours Ours Base- Ours Ours\nTask list line (4-c) (3-c) line (4-c) (3-c)\nWinoGrande 0.729 0.729 0.717 0.516 0.519 0.51\nARC Easy 0.762 0.758 0.756 0.438 0.439 0.406\nARC Challenge 0.445 0.45 0.451 0.19 0.183 0.195\nPIQA 0.766 0.762 0.755 0.629 0.637 0.599\nHellaswag 0.48 0.473 0.473 0.289 0.29 0.292\nMMLU 0.418 0.398 0.394 0.229 0.229 0.229\nOpenbookQA 0.386 0.39 0.392 0.164 0.152 0.15\n6.1 Searching for the Optimal Encoding Base\nOur system offers compatibility with various encoding bases, and\nwe can utilize grid search to determine the optimal encoding\nbase that can achieve the highest efficiency under the required\ndata precision for activations. In Table 7, we present a com-\nparison of different encoding bases, assuming the required data\nprecision for activations is INT8 (i.e., 8-bit integer). The table\nclearly demonstrates that utilizing a larger base value enables the\nencoding of the same data into fewer digits, thereby reducing the\nnumber of cycles required for computation. However, the number\nof resistors required to encode the activation (equalling 2S) is\nincreased with a larger base. To ensure a fair comparison across\ndifferent base values, we introduce a metric called the scale-cycle\nproduct, which indicates the minimum latency achieved with one\nunit of memristors. For instance, in the base-7 encoding system\nwith a scale value S = 2, we can use 2× resistors to achieve a\n3-unit latency. Therefore, the scale-cycle product is6×, indicating\nthat the latency would be 6 units if we only had 1× resistor. As\nwe aim for a lower scale-cycle product, the base-3 (S = 1) and\nbase-7 (S = 2) emerge as the optimal choices.\nIn Figure 11(a) and Figure 11(b), we present the energy\nand area breakdown of the computation crossbar. The area of\nthe dense crossbar remains the same across different encoding\nschemes as we store the entire network parameters. Therefore, our\nmain focus is comparing the encoding schemes on the compu-\ntation crossbar. The analysis reveals that a significant portion of\nthe energy consumption and area overhead on the computation\ncrossbar is attributed to the DACs and the ADCs. Among all\nthe base values, factors S = 1 (base-3) and S = 2 (base-\n7) exhibit the lowest energy consumption primarily due to their\nlow scale-cycle product. On the other hand, the area overhead\nexperiences an almost linear increase with the scaling factor S.\nThis is because the number of memristors is proportional to\nthe factor S. Furthermore, Figure 11(c) presents the area-latency\ndistributions, while Figure 11(d) showcases the area-delay product\n(ADP). These figures demonstrate that a larger scaling factor S\nresults in lower latency; however, it does not correspond to a lower\nADP. Notably, when comparing factor S = 2 with factor S = 1,\nabout 9% reduction in ADP is observed. The variations primarily\nstem from the differences in the registers employed.\n6.2 Accuracy/Scores on Language Tasks\nCompared to digital circuits, analog circuits are more vulnerable\nto environmental effects such as noise [27]. Additionally, when\nusing quantized weights and activations, memristors lose precision\ncompared to the full precision version, potentially affecting the\naccuracy of models. To assess the robustness of our architecture,\nwe simulated the environment [36, 37] and tested our architecture\non the GLUE language tasks [35] using BERT models. Simulation\nresults show that the environmental noise contributes less than\n5% to the signal level, which is typical for real devices [44].\nThe results are summarized in Table 8, demonstrating that our\narchitecture performs almost as well as baseline from GPU on both\nBERTBase and BERTLarge applications across all tasks in the GLUE\nbenchmark. In some cases, we even observed a slight increase\nin accuracy. This can be attributed to the errors in the results.\nWe also testing real world datasets on Phi-1.5 [40] and GPT-2.\nThe experiments results in Table 9 demonstrate that our system\nachieved similar accuracy and scores to the baseline.\nBased on the two tables, it can be inferred that the amount\nof noise has a negligible impact on the model’s accuracy. The\nrobustness of our dense crossbar plays a role in stabilizing\nmodel accuracy. To assess the robustness of our architecture, we\nconducted tests in even more challenging noise environments.\nExperiments show that with a noise amplitude 5X stronger, the\noutput from the dense crossbar and the accuracy of the model\nremain unaffected. Moreover, if we choose to program each\nmemristor with 1-bit information using two conductance levels,\nour system can withstand even stronger noise environments, up to\n18X the noise amplitude. Additionally, we believe that the softmax\nfunction within LLMs also plays a crucial role in enhancing noise\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13\n0\n0.8\n1.6\n2.4\n3.2\nMB\nSB\n \n18\nOurs\nResNet-34\n0\n2.5\n5.0\n7.5\n10.0\nMB\nSB\n \n55\nOurs\nResNet-152\n0\n5\n10\n15\n20\nMB\nSB\n \n106\nOurs\nEnergy Consumption (mJ)\nBERTBase\n0\n20\n40\n60\n80\nMB\nSB\n \n378\nOurs\nBERTLarge\n0\n2\n4\n6\n8\nMB\nSB\n \n50\nOurs\nResNet-34\n0\n5\n10\n15\n20\nMB\nSB\n \n129\nOurs\nResNet-152\n0\n8\n16\n24\n32\nMB\nSB\n \n197\nOurs\nArea (mm2)\nBERTBase\n0\n30\n60\n90\n120\nMB\nSB\n \nDAC (Digital-Analog Conv.) ADC (Analog-Digital Conv.) Peripheral (Inc. Shift-and-Add) Register Memristor Cache\n(a) Area Breakdown (b) Energy Breakdown\n701\nOurs\nBERTLarge\nFig. 12. The comparison among the multi-bit (MB) crossbar, the single-bit (SB) crossbar, and our work in (a) area overhead, and (b) energy\nconsumption. Some results exceed the upper bound of the y-scale. In that case, we mark the exact value of the metric on top of the bar.\ntolerance as it stops the accumulation of calculation errors. This\nadvantage sets LLMs apart from applications that demand high\nprecision, making them less susceptible to the negative effects of\nenvironmental noises.\n6.3 Reduction of Area Overhead\nTo reduce the number of ADCs/DACs and consequently decrease\nthe crossbar area, an effective approach is to increase the number\nof rows and columns in a computation crossbar, allowing a single\nADC or DAC to be shared by more memristors. However, this is\nchallenging due to the accumulation of noise from the non-ideal\nbehavior of memristors [45][46][47]. On the other hand, memris-\ntors assembled like a traditional memory bank can have a much\nlarger size as they are more resilient to noise. As each memristor\nin the crossbar works independently, errors do not accumulate\nover the columns [48][49]. With ADCs and DACs shared by more\nmemory cells, this type of architecture has higher area efficiency\nthan traditional computation crossbars. Our architecture improves\narea efficiency by combining these two types of crossbars. As\nillustrated in Figure 5, the computation crossbar continues to use\nthe classical design for analog computing with a small crossbar\ndimension (128 × 128). Model parameters are stored in the dense\ncrossbar, which employs the traditional memory bank design with\na significantly larger crossbar dimension ( 1k × 64k). We enable\nthe reconfiguration of the computation crossbar and transfer the\nweights from the dense crossbar.\nAs illustrated in Figure 12(a), our architecture significantly re-\nduces the area requirement, On average, our architecture achieves\napproximately 6× and 39× savings in area overhead compared to\nthe multi-bit architecture and single-bit architecture, respectively.\nWe tune the configuration (duplicated columns in the computation\ncrossbar) of our architecture to guarantee that the end-to-end delay\nof our architecture is equivalent to these two architectures. In\neither the multi-bit or the single-bit version, all network param-\neters are stored within conventional crossbars. As shown in the\nfigure, the effective density of conventional memristor crossbars\nis relatively low, with approximately 95% of the area occupied\nby DACs and ADCs. In contrast, our architecture stores all the\nparameters in the dense crossbar. Due to the independent nature\nof data read by each memristor, which does not impact other\nmemristors, the DACs and ADCs in our dense crossbar can be\nshared among a larger number of memristors compared to the\nconventional crossbar [16]. Furthermore, the 1-bit DACs and 2-bit\nADCs occupy a considerably smaller portion of the overall area.\nThese features allow us to deploy even larger LLM on a single\nchip with a lower area overhead, thus eliminating the time and\nenergy inefficiencies associated with off-chip communication.\n6.4 Reduction of Energy Consumption\nIn traditional memristor-based crossbar designs, a significant\namount of energy is consumed by memristors [50][51]. One\nmajor drawback of memristors is their inherent issues with non-\nideality and noise, particularly the random telegraph noise (RTN)\n[52][53], which arises as unresolved defects during programming.\nThe amplitude of the fluctuation is roughly proportional to the\nresistance levels [29][54]. To alleviate the negative impact of\nthe fluctuations on the model, developers need to decrease the\nresistance to reduce the fluctuations [55][56]. However, smaller\nresistance values result in larger currents, leading to higher energy\nconsumption [51][57]. We re-architected the crossbars to enhance\ntheir robustness against fluctuations in memristors. First, as our\nalgorithms only require them to store the same data value during\ncomputation, the memristors in the computation crossbar are\nreplaced with regular resistors. Without the programming process,\nthe resistor exhibits significantly smaller fluctuations [28][29].\nSecondly, the memristors in the dense crossbar function like\ntraditional memory bank. Therefore, memristor errors do not accu-\nmulate over the column and can be recovered by the output circuit\nas long as the fluctuations do not exceed the threshold [58][59].\nAs demonstrated in Figure 12(b), our architecture achieves\nsignificant energy savings. On average, our architecture achieves\napproximately 18× and 3× reductions in energy consumption\ncompared to the multi-bit architecture and single-bit architecture,\nrespectively. The figure reveals that the multi-bit architecture\nconsumes more energy due to the need for memristors to operate\nin the low-resistance mode to counter noise, particularly the RTN\n(random telegraph noise) effects [27]. In contrast, the single-bit\narchitecture enhances memristor robustness by storing only one-\nbit information, allowing for higher resistance levels and lower\nenergy consumption [20]. In our computation crossbar, we utilize\nregular resistors as replacements for memristors, leveraging their\ngreater physical robustness against RTN noise [29]. Additionally,\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14\n0\n4\n8\n12\n16\n102         103         104         105 \n(c)\nOurs\nArea (mm2)\nLatency (μS)\nResNet-34\nISAAC \nPRIME\nPipeLayer\nVesti\n(1.8)\n(4.9)\n(2.6)\n(9.8)\n(1.2)\n0\n10\n20\n30\n40\n103         104         105         106 \nOurs\n(d)\nArea (mm2)\nLatency (μS)\nResNet-152\n(3.5)\nISAAC \nPRIME\nPipeLayer\nVesti (5.6)\n(15.0)\n(7.9)\n(26.9)\n0\n16\n32\n48\n64\n102         103         104         105 \n(a)\nOurs\nArea (mm2)\nLatency (μS)\nBERTBase\nISAAC \nPRIME\nPipeLayer\nVesti\n(10.6)\n(29.0)\n(15.1)\n(44.9)\n(5.8)\n0\n60\n120\n180\n240\n102         103         104         105 \nOurs\nRRAM desinty ρr = 6.6×108/mm2 (SOTA Case) RRAM desinty ρr = ∞ (Ideal Case) SRAM desinty ρs = 1.6×1010/mm2  \nArea (mm2)\nLatency (μS)\nBERTBase\nISAAC \nPRIME\nPipeLayer\nVesti (38.0)\n(103.4)\n(53.8)\n(159.7)\n(20.7)\nFig. 13. (a-d) Comparison with the state-of-the-art RRAM architectures: ( P) PRIME [22], ( I) ISAAC [9], ( L) PipeLayer [23], and an area-efficient\nSRAM architecture (V) Vesti [24]. We label the energy consumption (mJ) in the brackets. (e.g., I(10.6) indicates an ISAAC crossbar consuming 10.6\nmJ energy.) All the architectures are adapted to 8-bit activations/weights under the 32 nm technology node, the same as ISAAC [9].\nTABLE 10\nComparison between TPUv4, A100, and our architecture on BERTLarge,\nadapting to INT8 operations under 32 nm technology node.\nADP (mm2· s) Energy (mJ)\nTPU GPU Ours TPU GPU Ours\n2.24 2.04 0.03 71.71 65.84 20.44\nour dense crossbar incorporates two key mechanisms to enhance\nrobustness. First, the data read by each memristor of every column\nis independent and is does not affected by other memristors in the\ncolumns [16]. Second, we employ 1-bit DACs and 2-bit ADCs\nin the input/output circuits, which are more robust than multi-\nbit DACs and ADCs [16]. The incorporation of these robustness\nmechanisms allows the resistors and memristors in our crossbars\nto operate in the low-resistance mode, leading to reduced energy\nconsumption.\n6.5 Comparison with the State-of-the-Art\nIn Figure 13 (a-d), we present a comparison of our architecture\nwith three popular architectures for RRAM memristor crossbars:\nPRIME [22], ISAAC [9], and PipeLayer [23]. These architectures\nrepresent three distinct strategies aimed at reducing the area of\nthe memristor crossbar. PRIME utilizes a sense amplifier and\na dynamic reference voltage source to convert analog data into\ndigital values. ISAAC employs 1-bit DACs as inputs and accu-\nmulates results over multiple time steps. PipeLayer eliminates\nthe need for ADCs by transforming analog signals into spikes\nusing capacitors. The comparison is based on factors such as area,\nlatency, and energy consumption (values provided in brackets).\nFor the purpose of comparison, we adapted each architecture’s\ndesign to accommodate 8-bit weights/activations. In contrast, our\narchitecture is represented by a curve in the graph since we offer\ndifferent solutions by adjusting the number of duplicated columns\nin the computation crossbars. This flexibility allows us to tailor\nthe system to specific requirements and achieve optimal results.\nOur system showcases substantial advancements in various as-\npects, including reductions in area overhead, energy consumption,\nand latency. The enhanced computation parallelism achieved by\nduplicating more columns in the computation crossbar for the\nTABLE 11\nLatency lower bounds of our architecture, utilizing the same\nexperimental setup as shown in Figure 13.\nBound (µs) BERTBase BERTLarge ResNet-34 ResNet-152\nTa\nLB 16.1 43.0 2.7 19.4\nTw\nLB 17.3 61.5 4.3 11.2\nsame set of weights contributes to its low latency. To clearly\ndemonstrate the effectiveness of our architecture and eliminate the\nimpact of memristor improvements on area efficiency, we analyze\ntwo cases for the memristor-based crossbars in Figure 13. In the\nfirst case, all the architectures are compared based on memristors\nwith state-of-the-art density, shown as a purple line. In the second\ncase, we assume the density of the memristors is infinitely high,\nshown as a yellow line. As we can see, even when we eliminate\nthe impact of memristors, our architecture still exhibits better\nperformance than the traditional one in terms of area.\nIn Figure 13 (a-d), we also compare our work with Vesti [24],\nan SRAM-based system that improves area efficiency by reusing\nits SRAM crossbar. Our system exhibits substantially lower energy\nconsumption compared to Vesti, primarily due to reduced off-\nchip communication requirements. Furthermore, in Table 10, we\ncompare our work with state-of-the-art TPU accelerators and\nGPUs that utilize HBM (High Bandwidth Memory) to address\nthe memory wall problem [60]. Leveraging the analog computing\nfeatures, our system outperforms these alternatives, achieving a\nminimum of 68× improvement in ADP (Area-Delay Product) and\n69% energy savings on BERTLarge.\nEven without the impact of NW and non-linear operations,\nour architecture still exhibits advantages in terms of area and\nenergy. To enable these operations, one possible solution for\ntraditional architectures is to incorporate specialized calculation\nunits alongside the memristor crossbar. This additional module\nfurther widens the area gap between our architecture and the\ntraditional ones. For NW operations, our architecture continues\nto exhibit lower energy consumption per operation compared to\ntraditional digital circuits. For example, approximately, the energy\nefficiency of TPUv4 [61] at 7nm technology is 170 watts per 275\ntrillion INT8 operations, which equals approximately 0.62 pJ/OP.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 15\nIn contrast, our architectural experiments reveal an average energy\nconsumption of 20.7 mJ for 77.3 operations, indicating an energy\nefficiency of 0.27 pJ/OP at the 32 nm technology node.\n6.6 Inference Latency Lower Bound\nThe inference latency is constrained either by the minimum\ntransmission time of activations from the cache, denoted as Ta\nLB,\nor by the minimum transmission time of weights from the dense\ncrossbar, denoted as Tw\nLB. With these two parameters, we can esti-\nmate the lower bound of the inference latency using Equation (8).\nTLB = max{Ta\nLB, Tw\nLB} = max{αa · Sa · ba\nBa\n, Nw · bw\nBw\n} (8)\nTo calculate Ta\nLB, we need to estimate the total amount of\ntransmitted data from the cache. This can be achieved by multi-\nplying three variables: αa, representing the activation read times,\nSa, denoting the number of elements in the activation data, and\nba, which indicates the bitwidth of each activation element. Next,\nwe divide the data size by the on-chip communication bandwidth\nfrom the cache, denoted as Ba. In this experiment, we assume\nBa to be 1000 GBps [62], which is a typical bandwidth of L3\ncache in modern CPUs, whose capacity is large enough to hold\nour intermediate results.\nWe can employ similar approaches to calculate Tw\nLB. The total\nsize for weight transmission is represented as Nw · bw, where\nNw denotes the number of parameters in the model, and bw\nstands for the bitwidth of the parameter. This total size is sub-\nsequently divided by Bw, which denotes the bandwidth between\nthe computation crossbar and the dense crossbar. We refer to the\nbandwidth value Bw from the HBM3 standard, assuming it to\nbe 819 Gbps per stack [63]. In Table 11, we provide the lower\nbounds of latency associated with the experiment in Figure 13. As\nthese lower bounds extend beyond the X-axis range, they are not\nexplicitly represented in the figure.\n6.7 Scalability Analysis on Larger Models\nIn recent times, there has been a rapid surge in the size of large\nlanguage models (LLMs), leading to a notable escalation in the\narea overhead of traditional memristor crossbars. This is due to\nthe need for a larger number of memristors to store all the model\nparameters. The physical limitations make it challenging to deploy\nthe model on a single-chip system thus avoiding the inefficiency\ncaused by off-chip communications. Although the utilization of\n3D stacking techniques [64] aids in alleviating this concern, the\noverall area overhead within one chip or package is still limited.\nHence, the growing number of parameters in state-of-the-art LLMs\npresents a challenge for adopting LLMs in memristor crossbars.\nThe objective of this experiment is to thoroughly analyze and\nevaluate the scalability of our memristor crossbar architecture on\nboth current and upcoming LLMs, and to determine whether we\ncan successfully deploy them on a single chip or package.\nIn Figure 14, we compare the area overhead of our architecture\nwith state-of-the-art architectures under various LLMs, including\nGPT-2, T5, LLaMa, and GPT-3. They are compared on the same\n32 nm technology node with ISAAC [9]. The figure clearly illus-\ntrates that as the model size expands, our architecture showcases\na significantly lower increase rate in area overhead compared\nto previous architectures, which experience rapid growth in area\noverhead. As an example, when considering the deployment of\nGPT-3, our architecture demonstrates an area overhead that is\n0\n25\n50\n75\n100\n125\n1b 10b 100b\nArea Overhead (× 1000 mm2)\nModel Size / Number of Parameters (Log Scale)\nPRIME\nPipeLayer\nISAAC\nOurs\nGPT2-XL\nT5-3b\nLLaMa-7b\nT5-11b\nLLaMa-13b\nLLaMa-33b\nLLaMa-65b\nGPT3\nUpper limit of\na single chip\nFig. 14. The area overhead of state-of-the-art architectures and our\narchitecture under large-scale LLMs. They are compared on the same\n32 nm technology node with ISAAC [9]. The dashed line represents the\nupper limit of a single chip (assuming 100mm2 × 128 layers)\nmerely 1/51th of the area occupied by the previous architecture.\nBased on the observed trend, it is anticipated that future LLMs\nwith even more parameters can be deployed in our architecture\nwithin one chip or package under a reasonable area overhead.\n7 C ONCLUSION\nThis paper introduces a novel architecture of memristor crossbar\nthat enables the deployment of state-of-the-art LLM on a single\nchip or package, effectively bypassing the energy and time in-\nefficiencies associated with off-chip communication. It addresses\nthree significant challenges encountered when depolying LLMs on\nmemristor crossbars, namely the large model size, the non-weight\nstationary multiplication, and the complex non-linear operations,\nwhich have traditionally posed significant obstacles for memristor\ncrossbars. The introduced architecture demonstrates substantial\nimprovements in both area and energy efficiency. After testing\nBERTLarge, we found that our architecture incurred negligible ac-\ncuracy loss. In comparison to traditional memristor crossbars, our\ndesign offers remarkable improvements, with up to 39× reduction\nin area overhead and 18× reduction in energy consumption. When\ncompared to modern TPU/GPU systems, our architecture achieves\na minimum of 68× reduction in the area-delay product and\nsignificantly lowers energy consumption by 69%. Furthermore,\nwe observe a 51× improvement in area overhead for GPT-3.\nREFERENCES\n[1] NVIDIA, “NVIDIA A100 Tensor Core GPU,” 2023. [Online]. Available:\nhttps://www.nvidia.com/en-sg/data-center/a100/\n[2] S. A. Khowaja, P. Khuwaja, and K. Dev, “ChatGPT Needs SPADE (Sus-\ntainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review,”\narXiv preprint arXiv:2305.03123, 2023.\n[3] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, and E. Eleftheriou,\n“Memory Devices and Applications for In-memory Computing,” Nature\nnanotechnology, vol. 15, no. 7, pp. 529–544, 2020.\n[4] D. Ielmini and H.-S. P. Wong, “In-memory Computing with Resistive\nSwitching Devices,” Nature electronics, vol. 1, no. 6, pp. 333–343, 2018.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 16\n[5] V . P. Nambiar, J. Pu, Y . K. Lee, A. Mani, T. Luo, L. Yang, E.-K. Koh,\nM. M. Wong, F. Li, W. L. Goh et al. , “0.5 V 4.8 pJ/SOP 0.93uW\nLeakage/core Neuromorphic Processor with Asynchronous NoC and\nReconfigurable LIF Neuron,” in 2020 IEEE Asian Solid-State Circuits\nConference (A-SSCC). IEEE, 2020, pp. 1–4.\n[6] B. C. M. Choong, T. Luo, C. Liu, B. He, W. Zhang, and J. T. Zhou,\n“Hardware-Software Co-Exploration with racetrack Memory based In-\nmemory Computing for CNN Inference in Embedded Systems,” Journal\nof Systems Architecture, vol. 128, p. 102507, 2022.\n[7] T. Luo, L. Yang, H. Zhang, C. Qu, X. Wang, Y . Cui, W.-F. Wong,\nand R. S. M. Goh, “NC-Net: Efficient Neuromorphic Computing Using\nAggregated Subnets on a Crossbar-Based Architecture With Nonvolatile\nMemory,” IEEE Transactions on Computer-Aided Design of Integrated\nCircuits and Systems, vol. 41, no. 9, pp. 2957–2969, 2021.\n[8] L. Yang, H. Zhang, T. Luo, C. Qu, M. T. L. Aung, Y . Cui, J. Zhou,\nM. M. Wong, J. Pu, A. T. Doet al., “Coreset: Hierarchical Neuromorphic\nComputing Supporting Large-scale Neural Networks with Improved\nResource Efficiency,” Neurocomputing, vol. 474, pp. 128–140, 2022.\n[9] A. Shafiee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P.\nStrachan, M. Hu, R. S. Williams, and V . Srikumar, “ISAAC: A Con-\nvolutional Neural Network Accelerator with in-situ Analog Arithmetic\nin Crossbars,” ACM SIGARCH Computer Architecture News , vol. 44,\nno. 3, pp. 14–26, 2016.\n[10] T. Luo, W. Zhang, B. He, and D. Maskell, “A Racetrack Memory based\nIn-memory Booth Multiplier for Cryptography Application,” in 2016\n21st Asia and South Pacific Design Automation Conference (ASP-DAC) .\nIEEE, 2016, pp. 286–291.\n[11] T. Luo, B. He, W. Zhang, and D. L. Maskell, “A Novel Two-stage\nModular Multiplier based on Racetrack Memory for Asymmetric Cryp-\ntography,” in 2017 IEEE/ACM International Conference on Computer-\nAided Design (ICCAD). IEEE, 2017, pp. 276–282.\n[12] A. Chen, “A Review of Emerging Non-V olatile Memory (NVM) Tech-\nnologies and Applications,” SSE, vol. 125, pp. 25–38, 2016.\n[13] F. Zahoor, T. Z. Azni Zulkifli, and F. A. Khanday, “Resistive Ran-\ndom Access Memory (RRAM): an Overview of Materials, Switching\nMechanism, Performance, Multilevel cell (MLC) Storage, Modeling, and\nApplications,” Nanoscale research letters, vol. 15, no. 1, pp. 1–26, 2020.\n[14] K.-I. Oh, L.-S. Kim, K.-I. Park, Y .-H. Jun, J. S. Choi, and K. Kim,\n“A 5-Gb/s/Pin Transceiver for DDR Memory Interface with a Crosstalk\nSuppression Scheme,” IEEE journal of solid-state circuits, vol. 44, no. 8,\npp. 2222–2232, 2009.\n[15] G. Yeap, S. Lin, Y . Chen, H. Shang, P. Wang, H. Lin, Y . Peng, J. Sheu,\nM. Wang, X. Chen et al., “5nm CMOS Production Technology Platform\nFeaturing Full-fledged EUV , and Hgh Mobility Channel FinFets with\nDensest 0.021 µm 2 SRAM Cells for Mobile SOC and High Performance\nComputing Applications,” in 2019 IEEE International Electron Devices\nMeeting (IEDM). IEEE, 2019, pp. 36–7.\n[16] Micron, “8Gb: x4, x8, x16 DDR4 SDRAM Features,” Micron, Tech.\nRep., 2015.\n[17] M. Hu, J. P. Strachan, Z. Li, E. M. Grafals, N. Davila, C. Graves,\nS. Lam, N. Ge, J. J. Yang, and R. S. Williams, “Dot-Product Engine for\nNeuromorphic Computing: Programming 1T1M Crossbar to Accelerate\nMatrix-Vector Multiplication,” in Proceedings of the 53rd annual design\nautomation conference, 2016, pp. 1–6.\n[18] S. Stathopoulos, A. Khiat, M. Trapatseli, S. Cortese, A. Serb, I. Valov,\nand T. Prodromakis, “Multibit Memory Operation of Metal-oxide Bi-\nlayer Memristors,” Scientific reports, vol. 7, no. 1, p. 17532, 2017.\n[19] V . Agrawal, V . Prabhakar, K. Ramkumar, L. Hinh, S. Saha, S. Samanta,\nand R. Kapre, “In-memory Computing Array Using 40nm Multibit\nSONOS Achieving 100 TOPS/W Energy Efficiency for Deep Neural\nNetwork Edge Inference Accelerators,” in 2020 IEEE International\nMemory Workshop (IMW). IEEE, 2020, pp. 1–4.\n[20] Z. Zhu, H. Sun, Y . Lin, G. Dai, L. Xia, S. Han, Y . Wang, and H. Yang,\n“A Configurable Multi-precision CNN Computing Framework Based\non Single Bit RRAM,” in Proceedings of the 56th Annual Design\nAutomation Conference 2019, 2019, pp. 1–6.\n[21] C.-X. Xue, J.-M. Hung, H.-Y . Kao, Y .-H. Huang, S.-P. Huang, F.-C.\nChang, P. Chen, T.-W. Liu, C.-J. Jhang, C.-I. Su et al. , “16.1 A 22nm\n4Mb 8b-precision ReRAM Computing-in-memory Macro with 11.91 to\n195.7 TOPS/W for Tiny AI Edge Devices,” in 2021 IEEE International\nSolid-State Circuits Conference (ISSCC), vol. 64. IEEE, 2021, pp. 245–\n247.\n[22] P. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y . Liu, Y . Wang, and Y . Xie,\n“PRIME: A Novel Processing-in-Memory Architecture for Neural Net-\nwork Computation in ReRAN-based Main Memory,” ACM SIGARCH\nComputer Architecture News, vol. 44, no. 3, pp. 27–39, 2016.\n[23] L. Song, X. Qian, H. Li, and Y . Chen, “PipeLayer: A Pipelined ReRAM-\nbased Accelerator for Deep Learning,” in 2017 IEEE international\nsymposium on high performance computer architecture (HPCA). IEEE,\n2017, pp. 541–552.\n[24] S. Yin, Z. Jiang, M. Kim, T. Gupta, M. Seok, and J.-S. Seo, “Vesti:\nEnergy-Efficient In-Memory Computing Accelerator for Deep Neural\nNetworks,” IEEE Transactions on Very Large Scale Integration (VLSI)\nSystems, vol. 28, no. 1, pp. 48–61, 2019.\n[25] M. Saberi, R. Lotfi, K. Mafinezhad, and W. A. Serdijn, “Analysis of\nPower Consumption and Linearity in Capacitive Digital-to-Analog Con-\nverters Used in Successive Approximation ADCs,”IEEE Transactions on\nCircuits and Systems I: Regular Papers , vol. 58, no. 8, pp. 1736–1748,\n2011.\n[26] L. Kull, T. Toifl, M. Schmatz, P. A. Francese, C. Menolfi, M. Braendli,\nM. Kossel, T. Morf, T. M. Andersen, and Y . Leblebici, “A 3.1 mW\n8b 1.2 GS/s Single-Channel Asynchronous SAR ADC with Alternate\nComparators for Enhanced Speed in 32 nm Digital SOI CMOS,” IEEE\nJournal of Solid-State Circuits, vol. 48, no. 12, pp. 3049–3058, 2013.\n[27] N. Raghavan, R. Degraeve, L. Goux, A. Fantini, D. Wouters, G. Groe-\nseneken, and M. Jurczak, “RTN insight to filamentary instability and\ndisturb immunity in ultra-low power switching HfOx and AlOx RRAM,”\nin 2013 Symposium on VLSI Technology. IEEE, 2013, pp. T164–T165.\n[28] A. Kay, “Analysis and Measurement of Intrinsic Noise in Op Amp\nCircuits Part VIII: Popcorn Noise,” Texas Instruments, 2008.\n[29] D. Ielmini, F. Nardi, and C. Cagli, “Resistance-Dependent Amplitude\nof Random Telegraph-Signal Noise in Resistive Switching Memories,”\nApplied Physics Letters, vol. 96, no. 5, p. 053503, 2010.\n[30] Z. Wang, H. Zhang, T. Luo, W.-F. Wong, A. T. Do, P. Vishnu, W. Zhang,\nand R. S. M. Goh, “NCPower: Power Modelling for NVM-based Neu-\nromorphic Chip,” in International Conference on Neuromorphic Systems\n2020, 2020, pp. 1–7.\n[31] P.-Y . Chen and S. Yu, “Compact Modeling of RRAM Devices and Its\nApplications in 1T1R and 1S1R Array Design,” IEEE Transactions on\nElectron Devices, vol. 62, no. 12, pp. 4022–4028, 2015.\n[32] B. J. MacLennan, Analog Computation. New York, NY: Springer New\nYork, 2009, pp. 271–294. [Online]. Available: https://doi.org/10.1007/\n978-0-387-30440-3 19\n[33] S. Yawale and S. Yawale, Operational Amplifier: Theory and\nExperiments. Springer Nature Singapore, 2021. [Online]. Available:\nhttps://books.google.com.sg/books?id=lZJCEAAAQBAJ\n[34] B. Ulmann, Analog computing. Oldenbourg Wissenschaftsverlag Verlag,\n2013.\n[35] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“GLUE: A Multi-task Benchmark and Analysis Platform for Natural\nLanguage Understanding,” arXiv preprint arXiv:1804.07461, 2018.\n[36] M. K. F. Lee, Y . Cui, T. Somu, T. Luo, J. Zhou, W. T. Tang, W.-F.\nWong, and R. S. M. Goh, “A System-level Simulator for RRAM-based\nNeuromorphic Computing Chips,” ACM Transactions on Architecture\nand Code Optimization (TACO), vol. 15, no. 4, pp. 1–24, 2019.\n[37] T. Luo, X. Wang, C. Qu, M. K. F. Lee, W. T. Tang, W.-F. Wong, and\nR. S. M. Goh, “An FPGA-based hardware emulator for neuromorphic\nchip with RRAM,” IEEE Transactions on Computer-Aided Design of\nIntegrated Circuits and Systems, vol. 39, no. 2, pp. 438–450, 2018.\n[38] L. Lavagno, L. Scheffer, and G. Martin, EDA for IC Implementation,\nCircuit Design, and Process Technology. CRC press, 2018.\n[39] V . P. Nambiar, J. Pu, Y . K. Lee, A. Mani, T. Luo, L. Yang, E. K. Koh,\nM. M. Wong, F. Li, W. L. Goh, and A. T. Do, “0.5V 4.8 pJ/SOP 0.93µW\nLeakage/core Neuromorphic Processor with Asynchronous NoC and\nReconfigurable LIF Neuron,” in 2020 IEEE Asian Solid-State Circuits\nConference (A-SSCC), 2020, pp. 1–4.\n[40] Y . Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y . T. Lee,\n“Textbooks are all you need ii: phi-1.5 technical report,” arXiv preprint\narXiv:2309.05463, 2023.\n[41] M. O’Halloran and R. Sarpeshkar, “A 10-nW 12-bit Accurate Analog\nStorage Cell with 10-aA Leakage,” IEEE journal of solid-state circuits ,\nvol. 39, no. 11, pp. 1985–1996, 2004.\n[42] Y . Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, Z. Xu,\nN. Sun et al. , “DaDianNao: A Machine-Learning Supercomputer,” in\n2014 47th Annual IEEE/ACM International Symposium on Microarchi-\ntecture. IEEE, 2014, pp. 609–622.\n[43] N. Muralimanohar, R. Balasubramonian, and N. Jouppi, “Optimizing\nNUCA Organizations and Wiring Alternatives for Large Caches with\nCACTI 6.0,” in 40th Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO 2007). IEEE, 2007, pp. 3–14.\n[44] B. Feinberg, S. Wang, and E. Ipek, “Making Memristive Neural Network\nAccelerators Reliable,” in 2018 IEEE International Symposium on High\nPerformance Computer Architecture (HPCA). IEEE, 2018, pp. 52–65.\n[45] A. Bhattacharjee, A. Moitra, Y . Kim, Y . Venkatesha, and P. Panda,\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 17\n“Examining the role and limits of batchnorm optimization to miti-\ngate diverse hardware-noise in in-memory computing,” arXiv preprint\narXiv:2305.18416, 2023.\n[46] I. Chakraborty, M. Fayez Ali, D. Eun Kim, A. Ankit, and K. Roy,\n“Geniex: A generalized approach to emulating non-ideality in memristive\nxbars using neural networks,” in 2020 57th ACM/IEEE Design Automa-\ntion Conference (DAC), 2020, pp. 1–6.\n[47] B. Feinberg, S. Wang, and E. Ipek, “Making memristive neural network\naccelerators reliable,” in 2018 IEEE International Symposium on High\nPerformance Computer Architecture (HPCA), 2018, pp. 52–65.\n[48] K. Itoh and K. Itoh, “High signal-to-noise ratio dram design and technol-\nogy,” VLSI Memory Chip Design, pp. 195–248, 2001.\n[49] C. Xu, D. Niu, N. Muralimanohar, R. Balasubramonian, T. Zhang, S. Yu,\nand Y . Xie, “Overcoming the challenges of crossbar resistive memory\narchitectures,” in 2015 IEEE 21st international symposium on high\nperformance computer architecture (HPCA). IEEE, 2015, pp. 476–488.\n[50] G. Murali, X. Sun, S. Yu, and S. K. Lim, “Heterogeneous mixed-\nsignal monolithic 3-d in-memory computing using resistive ram,” IEEE\nTransactions on Very Large Scale Integration (VLSI) Systems , vol. 29,\nno. 2, pp. 386–396, 2020.\n[51] Z. Wang, T. Luo, R. S. M. Goh, W. Zhang, and W.-F. Wong, “Optimizing\nfor in-memory deep learning with emerging memory technology,” IEEE\nTransactions on Neural Networks and Learning Systems, 2023.\n[52] Y . H. Tseng, W. C. Shen, C.-E. Huang, C. J. Lin, and Y .-C. King,\n“Electron trapping effect on the switching behavior of contact rram\ndevices through random telegraph noise analysis,” in 2010 International\nElectron Devices Meeting. IEEE, 2010, pp. 28–5.\n[53] M. Terai, Y . Sakotsubo, Y . Saito, S. Kotsuji, and H. Hada, “Effect of\nbottom electrode of reram with ta 2 o 5/tio 2 stack on rtn and retention,”\nin 2009 IEEE International Electron Devices Meeting (IEDM) . IEEE,\n2009, pp. 1–4.\n[54] D. Lee, J. Leze, M. Jo, J. Park, M. Siddik, and H. Hwang, “Noise-\nanalysis-based model of filamentary switching reram with zrox/hfox\nstacks,” IEEE Electron Device Letters, vol. 32, no. 7, pp. 964–966, 2011.\n[55] T. Zanotti, F. M. Puglisi, and P. Pavan, “Low-bit precision neural network\narchitecture with high immunity to variability and random telegraph noise\nbased on resistive memories,” in 2021 IEEE International Reliability\nPhysics Symposium (IRPS). IEEE, 2021, pp. 1–6.\n[56] Z. Chai, P. Freitas, W. Zhang, F. Hatem, J. F. Zhang, J. Marsland,\nB. Govoreanu, L. Goux, and G. S. Kar, “Impact of rtn on pattern\nrecognition accuracy of rram-based synaptic neural network,” IEEE\nElectron Device Letters, vol. 39, no. 11, pp. 1652–1655, 2018.\n[57] L. Gao, F. Merrikh-Bayat, F. Alibart, X. Guo, B. D. Hoskins, K.-\nT. Cheng, and D. B. Strukov, “Digital-to-analog and analog-to-digital\nconversion with metal oxide memristors for ultra-low power computing,”\nin 2013 IEEE/ACM International Symposium on Nanoscale Architectures\n(NANOARCH). IEEE, 2013, pp. 19–22.\n[58] I. Nam, J. Lim, H. Hwang, K. Cho, and J. Choi, “Quantitative analysis for\nnoise generated from share circuitries within ddr3 dram,” in Proceedings\nof the 21th International Symposium on the Physical and Failure Analysis\nof Integrated Circuits (IPFA). IEEE, 2014, pp. 83–86.\n[59] S. M. Seyedzadeh, D. Kline Jr, A. K. Jones, and R. Melhem, “Miti-\ngating bitline crosstalk noise in dram memories,” in Proceedings of the\nInternational Symposium on Memory Systems, 2017, pp. 205–216.\n[60] Z. Wang, H. Huang, J. Zhang, and G. Alonso, “Shuhai: Benchmarking\nHigh Bandwidth Memory on FPGAs,” in 2020 IEEE 28th Annual\nInternational Symposium on Field-Programmable Custom Computing\nMachines (FCCM). IEEE, 2020, pp. 111–119.\n[61] N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai, N. Patil,\nS. Subramanian, A. Swing, B. Towles et al. , “Tpu v4: An optically\nreconfigurable supercomputer for machine learning with hardware sup-\nport for embeddings,” in Proceedings of the 50th Annual International\nSymposium on Computer Architecture, 2023, pp. 1–14.\n[62] Intel, “Memory Performance in a Nutshell,” 2023. [Online].\nAvailable: https://www.intel.com/content/www/us/en/developer/articles/\ntechnical/memory-performance-in-a-nutshell.html\n[63] S. Hynix, “SK hynix Develops Industry’s First 12-Layer HBM3,\nProvides Samples to Customers,” 2023. [Online]. Available: https:\n//news.skhynix.com/sk-hynix-develops-industrys-first-12-layer-hbm3/\n[64] M. Oota, Y . Ando, K. Tsuda, T. Koshida, S. Oshita, A. Suzuki,\nK. Fukushima, S. Nagatsuka, T. Onuki, R. Hodo et al. , “3D-stacked\nCAAC-In-Ga-Zn oxide FETs with Gate Length of 72nm,” in 2019 IEEE\nInternational Electron Devices Meeting (IEDM). IEEE, 2019, pp. 3–2.\nZhehui Wang received B.S. degree in Electri-\ncal Engineering from Fudan University, China, in\n2010, and Ph.D. degree in Electronic and Com-\nputer Engineering from Hong Kong University of\nScience and Technology, Hong Kong, in 2016.\nHe is currently a Research Scientist with the In-\nstitute of High Performance Computing, Agency\nfor Science, Technology and Research, Singa-\npore. He authored and co-authored more than\n60 research papers in peer-reviewed journals,\nconferences, and books. His research interests\ninclude efficient AI deployment, AI on emerging technologies, hardware-\nsoftware co-design, and high-performance computing.\nTao Luoreceived his bachelor’s degree from the\nHarbin Institute of Technology, Harbin, China, in\n2010, his master’s degree from the University\nof Electronic Science and Technology of China,\nChengdu, China, in 2013, and his Ph.D. de-\ngree from the School of Computer Science and\nEngineering, Nanyang Technological University,\nSingapore, in 2018. He is currently a senior re-\nsearch scientist with the Institute of High Perfor-\nmance Computing (IHPC), Agency for Science,\nTechnology and Research, Singapore (A*STAR),\nSingapore. He has authored over 50 scientific publications in premier\npeer-reviewed international conferences and journals. His current re-\nsearch interests include high-performance computing, machine learn-\ning, hardware–software co-exploration, quantum computing, efficient AI\nand its application.\nCheng Liu is as an Associate Professor at the\nState Key Lab of Processors (SKLP), Institute of\nComputing Technology (ICT), Chinese Academy\nof Sciences (CAS). He received BEng and MEng\ndegrees from Harbin Institute of Technology,\nHarbin, China, in 2007 and 2009 respectively,\nand the PhD degree from the University of Hong\nKong, in 2016. His research interests include\nfault-tolerant computing, domain specific archi-\ntectures, computing in memory, and AI4EDA.\nHe has authored over 70 scientific publications\nin premier international conferences and journals. He won the Best\nPaper Award at the Great Lakes Symposium on VLSI in 2021 and\nIEEE Transactions on Computers in 2021. He is a recipient of Huawei\nOlympusMons Awards in 2024.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 18\nWeichen Liu received his BEng and MEng de-\ngrees from Harbin Institute of Technology, China,\nand PhD degree from the Hong Kong Univer-\nsity of Science and Technology, Hong Kong\nSAR. He is currently an Associate Professor at\nthe College of Computing and Data Science,\nNanyang Technological University, Singapore.\nHe authored and co-authored more than 180 re-\nsearch papers in peer-reviewed journals, confer-\nences, and books. His research interests include\nembedded and real-time systems, multiproces-\nsor systems, network-on-chip, and machine learning acceleration.\nRick Siow Mong Gohreceived the Ph.D. degree\nin Electrical and Computer Engineering from\nthe National University of Singapore, Singapore.\nAssociate Professor (Adj.) Rick Goh is Director\nof Computing & Intelligence at A*STAR’s Insti-\ntute of High Performance Computing (IHPC),\nAssociate Professor (Adj.) at Duke-NUS Medi-\ncal School, Co-Director of A*STAR-EVYD Joint\nLab, Senior Principal Investigator (Adj.) at Sin-\ngapore Eye Research Institute (SERI), and Co-\nDirector of SERI-IHPC Joint Lab. He repre-\nsented A*STAR to co-organise the inaugural AI Health Summit in 2022\nwith SingHealth and Ministry of Health. Rick has co-authored 150+ peer-\nreviewed papers in renowned clinical journals such as Nature Aging,\nNature Genetics, The Lancet Digital Health, and top-tier AI and com-\nputing journals and conferences such as Nature Machine Intelligence,\nNature Communications, IEEE TPAMI, TNNLS, TPDS, Computers, Cy-\nbernetics, Transactions on Medical Imaging, Medical Image Analysis,\nCVPR, CACM, AAAI, IJCAI, MICCAI, and Supercomputing Conference\n(SC). He has recently won multiple highly-competitive AI Singapore Tech\nChallenge and Grand Challenge grants, best paper awards, Healthcare\nAI project awards, and has been recognised with a National Award\n(COVID-19) Commendation Medal.\nWeng-Fai Wong received the BSc degree from\nthe National University of Singapore, in 1988,\nand the DrEngSc degree from the University\nof Tsukuba, Japan, in 1993. He is currently\nan associate professor with the Department of\nComputer Science at the National University of\nSingapore. His research interests include com-\nputer architecture, compilers, and systems for\nmachine learning. He is a senior member of the\nIEEE.",
  "topic": "Memristor",
  "concepts": [
    {
      "name": "Memristor",
      "score": 0.9001013040542603
    },
    {
      "name": "Crossbar switch",
      "score": 0.6743611097335815
    },
    {
      "name": "Computer science",
      "score": 0.6511878967285156
    },
    {
      "name": "Software deployment",
      "score": 0.650336742401123
    },
    {
      "name": "Memistor",
      "score": 0.5399231314659119
    },
    {
      "name": "Microarchitecture",
      "score": 0.4919526278972626
    },
    {
      "name": "Computer architecture",
      "score": 0.36275801062583923
    },
    {
      "name": "Resistive random-access memory",
      "score": 0.3384914994239807
    },
    {
      "name": "Embedded system",
      "score": 0.27722641825675964
    },
    {
      "name": "Telecommunications",
      "score": 0.23520734906196594
    },
    {
      "name": "Electrical engineering",
      "score": 0.22485214471817017
    },
    {
      "name": "Engineering",
      "score": 0.21387061476707458
    },
    {
      "name": "Voltage",
      "score": 0.15677177906036377
    },
    {
      "name": "Software engineering",
      "score": 0.12137305736541748
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I115228651",
      "name": "Agency for Science, Technology and Research",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I3004594783",
      "name": "Institute of High Performance Computing",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ],
  "cited_by": 5
}