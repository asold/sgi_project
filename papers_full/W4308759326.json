{
  "title": "Pure Transformer with Integrated Experts for Scene Text Recognition",
  "url": "https://openalex.org/W4308759326",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5044116374",
      "name": "Yew Lee Tan",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5047970310",
      "name": "Adams Wai‐Kin Kong",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5050571650",
      "name": "Jung‐Jae Kim",
      "affiliations": [
        "Agency for Science, Technology and Research",
        "Institute for Infocomm Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3204479434",
    "https://openalex.org/W3177684257",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W3134064484",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2103804906",
    "https://openalex.org/W3181186176",
    "https://openalex.org/W3198690111",
    "https://openalex.org/W2786962101",
    "https://openalex.org/W2343052201",
    "https://openalex.org/W2963622213",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2997864923",
    "https://openalex.org/W3092937119",
    "https://openalex.org/W2144554289",
    "https://openalex.org/W2008806374",
    "https://openalex.org/W3034414401",
    "https://openalex.org/W2965066169",
    "https://openalex.org/W3082397598",
    "https://openalex.org/W2978036638",
    "https://openalex.org/W2963526661",
    "https://openalex.org/W3119350346",
    "https://openalex.org/W1981283549",
    "https://openalex.org/W3003868038",
    "https://openalex.org/W2146835493",
    "https://openalex.org/W3206651063",
    "https://openalex.org/W3034447740",
    "https://openalex.org/W3178780810",
    "https://openalex.org/W1971822075",
    "https://openalex.org/W2150427047",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W2810983211",
    "https://openalex.org/W2044883027",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2997749585",
    "https://openalex.org/W2998382406",
    "https://openalex.org/W3202415716",
    "https://openalex.org/W3198037736",
    "https://openalex.org/W3175618949",
    "https://openalex.org/W3005436539",
    "https://openalex.org/W3035449864",
    "https://openalex.org/W3110267192",
    "https://openalex.org/W2963712589",
    "https://openalex.org/W3173990630",
    "https://openalex.org/W3042760913",
    "https://openalex.org/W3198359920"
  ],
  "abstract": null,
  "full_text": "Pure Transformer with Integrated Experts for\nScene Text Recognition\nYew Lee Tan1, Adams Wai-Kin Kong1, and Jung-Jae Kim 2\n1 Nanyang Technological University, Singapore\n2 Institute for Infocomm Research, A*STAR, Singapore\nAbstract. Scene text recognition (STR) involves the task of reading\ntext in cropped images of natural scenes. Conventional models in STR\nemploy convolutional neural network (CNN) followed by recurrent neu-\nral network in an encoder-decoder framework. In recent times, the trans-\nformer architecture is being widely adopted in STR as it shows strong\ncapability in capturing long-term dependency which appears to be promi-\nnent in scene text images. Many researchers utilized transformer as part\nof a hybrid CNN-transformer encoder, often followed by a transformer\ndecoder. However, such methods only make use of the long-term de-\npendency mid-way through the encoding process. Although the vision\ntransformer (ViT) is able to capture such dependency at an early stage,\nits utilization remains largely unexploited in STR. This work proposes\nthe use of a transformer-only model as a simple baseline which outper-\nforms hybrid CNN-transformer models. Furthermore, two key areas for\nimprovement were identified. Firstly, the first decoded character has the\nlowest prediction accuracy. Secondly, images of different original aspect\nratios react differently to the patch resolutions while ViT only employ\none fixed patch resolution. To explore these areas, Pure Transformer with\nIntegrated Experts (PTIE) is proposed. PTIE is a transformer model\nthat can process multiple patch resolutions and decode in both the orig-\ninal and reverse character orders. It is examined on 7 commonly used\nbenchmarks and compared with over 20 state-of-the-art methods. The\nexperimental results show that the proposed method outperforms them\nand obtains state-of-the-art results in most benchmarks.\nKeywords: transformer, scene text recognition, integrated experts\n1 Introduction\nScene text recognition (STR) is useful in a wide array of applications such as\ndocument retrieval [36], robot navigation [33], and product recognition [22]. Fur-\nthermore, STR is able to improve the lives of visually impaired by providing them\naccess to visual information through texts encountered in natural scenes [7,12].\nTraditionally, convolutional neural network (CNN) was used as a backbone in\nthe encoder-decoder framework of STR to extract and encode features from the\nimages [5]. Recurrent neural network (RNN) was then used to capture sequence\ndependency and decode the features into a sequence of characters. In recent\narXiv:2211.04963v1  [cs.CV]  9 Nov 2022\n2 Y.L. Tan et al.\ntimes, transformer [37] has been employed in STR models because of its strong\ncapability in capturing long-term dependency. Some researchers have designed\ntransformer-inspired modules [41, 50], while others have utilized it as a hybrid\nCNN-transformer encoder [8] and/or a transformer decoder in STR [20,23].\nScene text usually has the same font, color, and style, thus exhibiting a\ncoherent pattern. These properties suggest that STR has strong long-term de-\npendency. Henceforth, recent works based on hybrid CNN-transformer [8] out-\nperform models with traditional architectures like CNN and RNN. A natural\nfollowing question to ask is — will STR performance be improved by exploit-\ning this dependency earlier, that is, by replacing the hybrid CNN-transformer\nencoder with a transformer-only encoder? The vision transformer (ViT) [6], is\ncompetitive against the most performant CNNs in various computer vision tasks.\nHowever, it remains largely unexploited in STR [1].\nWe discovered that employing ViT as an encoder followed by a transformer\ndecoder gives competitive result in STR. However, there are two areas to improve\non. First, ViT uses a linear layer to project image patches into encodings. The\nanalysis in Section 3 shows that different patch resolutions can have detrimental\nimpact on scene text images of certain word lengths and resizing scales. This\nfinding may apply to other architectures that utilize patches.\nSecond, transformer decoder employs an autoregressive decoding process and\ntherefore, lesser information is available to leading decoded characters as com-\npared with trailing ones. Our analysis indicates that the first character, which is\ndecoded without any information from previous character, has the highest error\nrate. This may also be prevalent in other autoregressive methods.\nTo address the aforementioned areas, we propose a transformer-only model\nthat can process different patch resolutions and decode in both the original and\nreverse character orders (e.g ‘boy’ and ‘yob’). Inspired by the mixture of experts,\nwe call this technique integrated experts. The model can effectively represent\nscene text images of multiple resizing scales. It also complements autoregressive\ndecoding with minimal additional latency as opposed to ensemble.\nIn summary, the contribution of this work is as follows: (1) a strong transformer-\nonly baseline model, (2) identification of areas for improvement in transformer\nfor STR, (3) the integrated experts method which serves to address the areas\nfor improvement, and (4) state-of-the-art results for 6 out of the 7 benchmarks.\nThe rest of the paper is organized as follows. Firstly, Section 2 explores related\nworks. Secondly, Section 3 analyses the areas for improvement in using trans-\nformer in STR. Thirdly, Section 4 discusses the proposed methodology. Following\nwhich, Section 5 reports the experimental results on 7 scene text benchmarks.\nLastly, Section 6 concludes this study.\n2 Related Work\nThe encoder-decoder framework is a popular approach in the field of STR [30].\nTraditionally, CNN was used to encode scene text images and RNN was used to\nmodel sequence dependency and translate the encoded features into a sequence of\nPTIE for STR 3\ncharacters. Shi et al. [34] proposed a CNN encoder followed by deep bi-directional\nlong-short term memory [13] for decoding. In a similar work [35], a rectification\nnetwork was introduced into the encoder in order to rectify the image before\nfeatures are extracted by a CNN.\nAs transformer became a de facto standard for sequence modeling tasks,\nworks that incorporate transformer as the decoder are becoming more common\nin STR. Lu et al. [23] proposed a multi-aspect global context attention module, a\nvariant of global context block [4], as part of the encoder network. A transformer\ndecoder is then used to decode the image features into sequences of characters.\nA similar model was also proposed by Wu et al. [42], utilizing a transformer de-\ncoder which is preceded by a global context ResNet (GCNet). Zhang et al. [50]\nemployed a combination of CNN and RNN as the encoder and a transformer in-\nspired cross-network attention as a part of the decoder in their cascade attention\nnetwork. Similarly, Yu et al. [45] introduced a global semantic reasoning module\nmade up of transformer units, as a module in the decoder.\nApart from being used as/in the decoder, transformer has also been employed\nin the encoder in the form of a hybrid CNN-transformer [3]. Fu et al. [9] proposed\nthe use of hybrid CNN-transformer to extract visual features from scene text\nimages. It is then followed by a contextual attention module, which is made up\nof a variant of transformer, as part of the decoding process. Lee et al. [20] likewise\nutilized a hybrid CNN-transformer encoder and a transformer decoder as their\nrecognition model. In addition, the authors proposed an adaptive 2D positional\nencoding as well as a locality-aware feed-forward module in the transformer\nencoder. With a focus on the positional encoding of transformer, Raisi et al. [31]\napplied a 2D learnable sinusoidal positional encoding which enables the CNN-\ntransformer encoder to focus more on spatial dependencies.\nNon-autoregressive forms of transformer decoder were also proposed in vari-\nous works, coupled with an iterative decoding. Qiao et al. [29] proposed a parallel\nand iterative decoding strategy on a transformer-based decoder preceded by a\nfeature pyramid network as an encoder. In a similar fashion, Fang et al. [8] uti-\nlized a hybrid CNN-transformer based vision model followed by a transformer\ndecoder with iterative correction.\nAs ViT is becoming a more common approach at vision tasks, Ateinza [1]\nproposed ViT as both the encoder and non-autoregressive decoder to streamline\nthe encoder-decoder framework of STR. The ViT is made up of the transformer\nencoder, where the word embedding layer is replaced with a linear layer. By\nutilizing this one stage process, the author is able to achieve a balance on the\naccuracy, speed, and efficiency for STR. However, its recognition accuracy does\nnot achieve state-of-the-art performance.\n3 Areas for Improvement in Transformer\n3.1 Encoder: Impact of patch resolution\nSTR takes cropped images of text from natural scenes as inputs. Therefore, they\ncome in different sizes and aspect ratios. As the images are needed to be of a\n4 Y.L. Tan et al.\nfixed height and width before being passed as inputs into an STR model, one\ncommon approach is to ignore the original aspect ratios and resize them with\nvarying scales. Preserving the original resolutions with padding results in worse\nperformance in the work by Shi et al. [35] which is in line with our experimen-\ntal result (in supplementary material). For ViT, resized images are split into\npatches, which will be flatten and passed through a linear layer followed by the\ntransformer encoder.\nUsing a baseline architecture of ViT encoder with transformer decoder as\ndescribed in Section 4, several models were trained with different patch resolu-\ntions. The distributions of correct predictions were analysed using the relative\nfrequency distribution change [15] as defined in Eq. (1):\nFl,s =\nF2\nl,s−F1\nl,s\nF1\nl,s\nP\nl,s(F2\nl,s−F1\nl,s)P\nl,s F1\nl,s\n(1)\nwhere the subscript l and s represent the word length and scaling factor. The\nscaling factor defined as final width\nfinal height\ninitial height\ninital width, is the scaling of the initial aspect\nratio to the final resized aspect ratio. F1\nl,s and F2\nl,s represent the frequency of\nthe correct predictions at word length l with scale factor s of two models. The\ntraining dataset specified in Section 5.1 is used to compute Fl,s because a large\ndataset is needed to reliably estimate Fl,s at each l and s; the number of samples\nin the benchmark datasets is insufficient.\n(a)\n (b)\nFig. 1.Relative frequency distribution change in correct predictions (a) from a model\ntrained using patch resolution 4 × 8 to a model trained using 8 × 4. (b) two models\ntrained using input patch resolution of 8×4. All models were separately initialized and\ntrained using the same hyperparameters\nFig. 1 visualizes the relative frequency distribution change, where the word\nlength is ranged from 2 to 20 with scaling factor ranging from 0 to 4. Bins with\nfrequency count lesser than 100 are removed. Noting that the remaining count\naccount for 95% of total count, these arrangements will reduce the noise caused\nby bins with low frequency and provide better visuals. In Fig. 1a, F1\nl,s and F2\nl,s\nare calculated from the models trained with patch resolution of 4 × 8 and 8 × 4\nrespectively. In Fig. 1b,F1\nl,s and F2\nl,s are computed with two randomly initialized\nPTIE for STR 5\nmodels trained with the same patch resolution of 8 × 4. As the denominator in\nEq. (1) for Fig. 1a and Fig. 1b is positive, Fl,s > 0 signifies that F2\nl,s produces\nmore correct predictions at l and s than F1\nl,s and vice-versa.\nAs plotted in Fig. 1a, the two models show clear contrast in terms of perfor-\nmance with respect to word length and scaling factor. In specifics, images with\nword length 3-5 and scaling factor of 1.2-2.4 are least affected by the patch res-\nolution used (white region in Fig. 1. Images with (1) word length of 2-3, scaling\nfactor < 1; and (2) word length 2-11, scaling factor > 2.6, favours patch resolu-\ntion of 4×8 (blue regions). The red region represents images that performs better\nwith 8 ×4. These findings suggest that models trained with different resolutions\nare experts for certain word lengths and scales. Furthermore, Fig. 1b shows no\ndistinct contrast in the frequency between the two separately initialized mod-\nels (trained with same patch resolution) as opposed to Fig. 1a. This provides a\nstronger evidence for the impact of different patch resolutions in STR.\n3.2 Decoder: Errors in first character prediction\nTwo baseline models as described in Section 4.1 were randomly initialized and\ntrained separately where one of them uses the original ground-truth texts while\nthe other uses reversed ground-truths. Our experimental results for wrong pre-\ndictions on train dataset are plotted in Fig. 2. It is to be noted that the incorrect\npredictions used to plot Fig. 2 are words with length 5 where there is only one\nincorrectly predicted character for Fig. 2a and Fig. 2b, and two incorrectly pre-\ndicted characters for Fig. 2c and Fig. 2d.\n(a)\n (b)\n (c)\n (d)\nFig. 2.Normalized frequency distributions of wrong predictions for word length 5 at\nthe character indices, conditioned on ground truth characters. (a) Predictions with\none wrong character. (b) Predictions with one wrong character trained on reversed\nground-truths. (c) Predictions with two wrong characters. (d) Predictions with two\nwrong characters trained on reversed ground-truths\nIn Fig. 2a and Fig. 2c, the first decoded character is at index 0. Whereas\nin Fig. 2b and Fig. 2d, the order of character indices was flipped to reflect\nthe reversed ground-truth texts. In the latter case, index 4 would be the first\ndecoded character. As the transformer decoder is autoregressive, the predictions\n6 Y.L. Tan et al.\nare conditioned on ground truth characters in order to evaluate the accuracy on\nindividual character given the correct prior character(s).\nThe experimental results show that both models have the highest error rate\nwhen decoding the first character, and such observations can be seen in other\nword lengths as well as other numbers of incorrect characters. Also, characters\nthat are decoded subsequently tend to have lower error rates, given the correct\nprevious characters inputs. More analysis is in the supplementary material.\npatch resolution \n r 0 (h 0 x w 0)\nLinear Layer \nL0 \nLinear Layer \nL1 \nPositional \nEncoding \nP0enc \nPositional \nEncoding \nP1enc \nVision Transformer \nEncoder\nFlatten Patches Flatten Patches \n(1 x H  x W  x C )\n(2 x len img  x dim )\nEncodingimg\nrepeat\n(4 x len img  x dim )\nconcat\nEmbedding \nE0 \nEmbedding \nE1 \n(1 x len img  x size ·C )\nPositional \nEncoding \nP0dec \nPositional \nEncoding \nP1dec \nrepeat\nEncodingtext Encodingtext\nEncodingtext\nconcat\nTransformer \nDecoder\nProbabilities\nPrediction\n(1 x len text  x dim )\n(2 x len text  x dim )\n(4 x len text  x dim )\n(4 x len text  x cls )\nacademy ymedaca\nprediction may need\nto be reversed\nEncodingimg\npatch resolution  \nr 1 (h 1 x w 1)\nrepeat\nBaseline\nModel\nEncodingtext Encodingtext\nEncodingimg Encodingimg(1 x len img  x dim )\nEncodingimg(2 x len img  x dim )\nFig. 3.Architecture of PTIE. It is to be noted that there is no attention between the\nconcatenated Encoding img. This is also the case for concatenated Encoding text. The\nattention is utilized as per vanilla transformer\n4 Methodology\n4.1 Model Architecture and Approach\nArchitecture of the proposed baseline model is illustrated in Fig. 3. It consists\nof a ViT encoder and a transformer decoder. Inspired by the mixture of experts,\nwe present a transformer with integrated experts, named PTIE, to improve on\nthe areas discussed in Section 3. Each expert, denoted as Expi,j, requires image\npatches of resolution ri and ground-truth texts of type j to be trained where\ni, j∈ {0, 1}. For this work, patch resolution r0 has the dimension of h0 × w0 =\nPTIE for STR 7\n4 × 8, and r1 has that of h1 × w1 = 8 × 4. Both patch resolutions have the same\npatch size size = h0w0 = h1w1. j = 0 represents the use of original ground-truth\ntexts (e.g. ‘academy’), and j = 1 for reversed ground-truths (e.g. ‘ymedaca’).\nFor expert Expi,j, the resized image of dimension H × W × C (height ×\nwidth × channels) will first be split up into patches of resolution ri and then\nflatten. The sequence of flatten patches with length of lenimg is passed through\nlinear layer Li. The output, Encoding img, will then be summed with positional\nencoding Penc\ni before going through the encoder with encoding dimension of\ndim. Similarly, the ground-truths of type j will go through an embedding layer\nEj and outputs Encoding text with sequence length, lentext. It is then summed\nwith Pdec\nj before being passed into the decoder which produces the probabilities\nover the total number of classes, cls. Cross-entropy loss will then be applied to\nthe probabilities with their respective type j ground-truths.\nIn our design, all experts are integrated into 1 model. The parameters in\nthe encoder and decoder are shared. The differentiating factors among them\nare the initial linear/embedding layers as well as the positional encodings. More\nprecisely, each expert shares about 96% of the parameters with the others and\neach sample from the dataset will have 4 sets of input (1 for each expert), namely:\n(1) image split into patches of 4×8 with the original ground-truth text, (2) 4×8\npatches with reversed ground-truth text, (3) 8 ×4 patches with original ground-\ntruth, and (4) 8 × 4 patches with reversed ground-truth. It is to be noted that\nour baseline model mentioned in this work employs only 1 set of input (e.g.\ni = 0, j= 0: 4 × 8 patches with original ground-truth).\nThe manipulation of the dimensions with repeat and concatenation depicted\nin Fig. 3 ensures that PTIE decodes each sample image only once despite having\n4 sets of initial input. This will allow the inference latency to be close to that\nof the baseline model. As an ensemble-inspired method, the model will generate\n4 predictions for a given sample. The output with the highest word probability\n(calculated by the multiplications of characters probability) will then be selected\nas the final prediction. However, different from a standard ensemble, our pro-\nposed model requires only a quarter of the parameters and inference time while\nremaining competitive against an ensemble of models in terms of accuracy.\n4.2 Positional Encoding\nAccording to the study by Ke et al [19], the positional encoding used in the\nvanilla transformer [37] causes noisy correlation with the embeddings of input\ntokens (e.g. characters) and may be detrimental to the model. Therefore, on top\nof the aforementioned proposed model, their strategy of untying the positional\nencoding from the input token embedding was also adopted.\nInstead of summing the positional encoding, a positional attention is instead\ncalculated and then added during the multi-head attention process. The posi-\ntional attention for the encoder, αenc\ni , is calculated as in Eq. (2):\nαenc\ni = 1√\n2d\n(Penc\ni WQ)(Penc\ni WK)T (2)\n8 Y.L. Tan et al.\nwhere Penc\ni is the positional encoding of the patches with resolution ri; d is the\ndimension of positional encodings; WQ and WK are linear layers with the same\nnumber of input and output dimensions. All layers in the encoder share theαenc\ni .\nThe decoder has masked self-attention and cross-attention layers. Their po-\nsitional attentions, αdec\nj and αdec c\ni,j , are calculated as in Eq. (3) and Eq. (4)\nrespectively:\nαdec\nj = 1√\n2d\n(Pdec\nj UQ)(Pdec\nj UK)T (3)\nαdec c\ni,j = 1√\n2d\n(Pdec\nj VQ)(Penc\ni VK)T (4)\nwhere i and j denote the types of patch resolution and ground-truth and UQ,\nUK, VQ, VK are linear layers like WQ and WK. Similarly, all the layers in the\ndecoder share the same positional attentions.\n(a)\n (b)\n (c)\n (d)\nFig. 4.Learned unnormalized positional attention maps in the encoder of PTIE for (a)\nhead 1, resolution=4 × 8; (b) head 1, resolution=8 × 4; (c) head 2, resolution=4 × 8;\n(d) head 2, resolution=8 × 4. The axes represent the indices of the flatten patches\nThe image patches of both resolutions were flatten in row-major order. With\na large amount of parameters sharing, the spatial layouts of flatten patches with\ndifferent patch resolutions for PTIE are handled by the positional encodings as\nshown in Fig. 4. Thus, the unnormalized positional attention maps for patch\nresolution 4 × 8 and 8 × 4 are different.\n4.3 Implementation Details\nThe network was implemented using PyTorch and trained with ADAM optimizer\nwith a base learning rate of 0.02, betas of (0.9, 0.98), and eps of 1e−9, warmup du-\nration of 6000 steps with a decaying rate of min(steps−0.5, steps×warmup−1.5).\nThe models were trained on 5 NVIDIA RTX3090, with a batch size of 640. All\nexperiments were trained for 10 epochs. Images are grayscaled and resized to\na height and width of 32 by 128 without retaining the original aspect ratios.\nStandard augmentation techniques following Fang et al.’s work [8] were applied.\nThe models in all experiments contain 6 encoder layers and 6 decoder layers with\nPTIE for STR 9\ndropout of 0.1. The encoding dimension is 512 with 16 heads for the multi-head\nattention. The feed forward layer has an intermediate dimension of 2048. The\nmodel recognizes 100 classes for training, including 10 digits, 52 case sensitive\nalphabets, 35 punctuation characters, a start token, an end token, and a pad\ntoken. For testing, only 36 case-insensitive alphanumeric characters were taken\ninto consideration as per related works [8,35,43]. Greedy decoding was used with\na maximum sequence length of 30. No rotation strategy [21] was used.\n5 Experimental Results and Analysis\n5.1 Datasets\nSynthetic datasets. Two synthetic datasets were used: MJSynth (MJ) [16],\nwith 9 million samples, and SynthText (ST) [11], containing 8 million images.\nSome works utilized SynthAdd (SA) [21] due to the lack of punctuation in\nMJSynth and SynthText. SA was not used in our training.\nReal datasets. For evaluation, 6 datasets of real scene text images which con-\ntain 7 benchmarks were used. IIIT 5K-Words (IIIT5K) [26] contains 3000 test\nimages. ICDAR 2013 (IC13) [18] contains 1015 testing images as per related\nwork [39]. Two verions of ICDAR 2015 (IC15) [17] , containing 2077 test images\nand 1811 images, were used for evaluation. Street View Text(SVT) [39] consists\nof 647 testing images. Street View Text-Perspective(SVT-P) [28] contains 645\ntesting images. CUTE80 (CT) [32] contains 288 test images. COCO-Text [10]\nwhich contains 42, 618 training images were used for fine-tuning so as to compare\nwith works which uses real datasets in training or fine-tuning.\n5.2 Comparison with State-of-the-Art Methods\nThe results of PTIE are compared with recent works from top conferences and\njournals as shown in Table 1. PTIE achieves state-of-the-art results for most of\nthe benchmarks, even though it has a simple architecture. In particular, PTIE–\nUntied attained the best results in 6 out of 7 benchmarks, outperforming the\nnext best method by 0.9% for SVT, 2.9% for IC15 (2077), 1.8% for IC15 (1811),\nand 0.8% for SVTP. The model loses out to the best accuracy [8] on IC13 by\n0.2% and achieved the third highest accuracy. Similarly, PTIE–Vanilla attained\nthe highest accuracy in 5 benchmarks as compared with recent works. Fig. 5\nshows examples of success and failure cases.\nComparing with works that utilize real datasets, we fine-tune our PTIE mod-\nels with real dataset (COCO-Text [10]) with results shown in Table 2. Through\nfine-tuning, our proposed model attained some improvement in performance.\nThe model is able to outperform the state-of-the-art methods for 4 of the bench-\nmarks and achieved the second highest for 2 bechmarks. Between the PTIE\nmodels trained with ST+MJ, PTIE–Untied has a weighted average (over the\nbenchmarks) of 0.1% higher than PTIE–Vanilla. For ST+MJ+R, PTIE–Untied\nhas a weighted average of 0 .4% higher than PTIE–Vanilla.\n10 Y.L. Tan et al.\nTable 1.Comparison of accuracies on benchmark datasets with works trained using\nsynthetic datasets. PTIE–Untied uses the learnable positional encoding discussed in\nSection 4.2 while PTIE–Vanilla uses it as per vanilla transformer method. The best and\nsecond best results as compared with PTIE–Untied are in bold and underline respec-\ntively. Values in the parenthesis are the difference in accuracy between the proposed\nmodel with the best or next best result. Note that the comparison of results are only\nbetween a PTIE-based model and other related works\nMethod Year Train Regular Text Irregular Text\nDatasets IIIT IC13 SVT IC15 SVT-P CT\n3000 1015 647 2077 1811 645 288\nLuo et al. [24] PR ‘19 ST+MJ 91.2 92.4 88.3 68.8 - 76.1 77.4\nYang et al. [44] ICCV ‘19 ST+MJ 94.4 93.9 88.9 78.7 - 80.8 87.5\nZhan and Lu [47] CVPR ‘19 ST+MJ 93.3 91.3 90.2 76.9 - 79.6 83.3\nWang et al. [40] AAAI ‘20 ST+MJ 94.3 93.9 89.2 74.5 - 80.0 84.4\nWan et al. [38] AAAI ‘20 ST+MJ 93.9 92.9 90.1 - 79.4 84.3 83.3\nZhang et al. [49] ECCV ‘20 ST+MJ 94.7 94.2 90.9 - 81.8 81.7 -\nYue et al. [46] ECCV ‘20 ST+MJ 95.3 94.8 88.1 77.1 - 79.5 90.3\nLee et al. [20] CVPRW ‘20 ST+MJ 92.8 94.1 91.3 79.0 - 86.5 87.8\nYu et al. [45] CVPR ‘20 ST+MJ 94.8 - 91.5 - 82.7 85.1 87.8\nQiao et al. [30] CVPR ‘20 ST+MJ 93.8 92.8 89.6 80.0 - 81.4 83.6\nLu et al. [23] PR ‘21 ST+MJ+SA 95.0 95.3 90.6 79.4 - 84.5 87.5\nRaisi et al. [31] CRV ‘21 ST+MJ 94.8 94.1 90.4 80.5 - 86.8 88.2\nQiao et al. [29] ACMMM ‘21 ST+MJ 95.2 93.4 91.2 81.0 83.5 84.3 90.9\nAtienza [1] ICDAR ‘21 ST+MJ 88.4 92.4 87.7 72.6 78.5 81.8 81.3\nZhang et al. [48] AAAI ‘21 ST+MJ 95.2 94.8 90.9 79.5 82.8 83.2 87.5\nWang et al. [41] ICCV ‘21 ST+MJ 95.8 95.7 91.7 - 83.7 86.0 88.5\nWu et al. [42] ICMR ‘21 ST+MJ 95.1 94.4 90.7 - 84.0 85.0 86.1\nFu et al. [9] ICMR ‘21 ST+MJ 96.2 97.3 93.5 - 84.9 88.2 91.2\nZhang et al. [50] ICMR ‘21 ST+MJ 90.3 96.8 89.5 76.0 - 78.5 78.9\nLuo et al. [25] IJCV ‘21 ST+MJ 95.6 96.0 92.9 81.4 83.9 85.1 91.3\nYan et al. [43] CVPR ‘21 ST+MJ 95.6 - 94.0 - 83.0 87.6 91.7\nBaek et al. [2] CVPR ‘21 ST+MJ 92.1 93.1 88.9 74.7 - 79.5 78.2\nFang et al. [8] CVPR ‘21 ST+MJ 96.2 97.4 93.5 - 86.0 89.3 89.2\nPTIE–Vanilla ST+MJ 96.7 97.1 95.5 83.4 87.4 89.8 91.3\n(+0.5) (-0.3) (+1.5) (+2.0) (+1.4) (+0.5) (-0.4)\nPTIE–Untied ST+MJ 96.3 97.2 94.9 84.3 87.8 90.1 91.7\n(+0.1) (-0.2) (+0.9) (+2.9) (+1.8) (+0.8) (0.0)\nGround truth 4x8\nPrediction\n4x8 Inverted\nPrediction\n8x4\nPrediction\n8x4 Inverted\nPrediction\nsale sale all date all\nscottish scottish scottism referencesuniversity\ngrandstanddehumidifiedgrandstandconcestuousrussian\nFig. 5.Sample images of success and failure cases. The boxed text represents final\noutput from PTIE. More examples are in the supplementary material\n5.3 Ablation Studies\nTransformer-only Encoder. In order to demonstrate the effectiveness of uti-\nlizing transformer-only encoder, 2 models were trained. We used a ViT encoder\nwith transformer decoder as the baseline model and added a 45-layer ResNet [35]\nPTIE for STR 11\nTable 2.Comparison of accuracies on the benchmark datasets. The letter ‘R’ denotes\nthe use of real dataset either in training or fine-tuning. The best and second best results\nin comparison with PTIE–Untied are in bold and underline respectively. Values in the\nparenthesis are the difference in accuracy between the proposed model with the best or\nnext best result. Note that the comparison of results are only between a PTIE-based\nmodel and other related works\nMethod Year Train Regular Text Irregular Text\nDatasets IIIT IC13 SVT IC15 SVT-P CT\n3000 1015 647 2077 1811 645 288\nLi et al. [21] AAAI ‘19 ST+MJ+SA+R 95.0 94.0 91.2 78.8 - 86.4 89.6\nYue et al. [46] ECCV ‘20 ST+MJ+R 95.4 94.1 89.3 79.2 - 82.9 92.4\nWan et al. [38] AAAI ‘20 ST+MJ+R 95.7 94.9 92.7 - 83.5 84.8 91.6\nHu et al. [14] AAAI ‘20 ST+MJ+SA+R 95.8 94.4 92.9 79.5 - 85.7 92.2\nQiao et al. [29]ACMMM ‘21 ST+MJ+R 96.7 95.4 94.7 85.9 88.7 88.2 92.7\nBaek et al. [2] CVPR ‘21 R 93.5 92.6 87.5 76.0 - 82.7 88.1\nLuo et al. [25] IJCV ‘21 ST+MJ+R 96.5 95.6 94.4 84.7 87.2 86.2 92.4\nPTIE–Vanilla ST+MJ+R 96.5 96.1 96.3 84.5 89.0 91.3 88.5\n(-0.2) (+0.5) (+1.6) (-1.4) (+0.3) (+3.1) (-4.2)\nPTIE–Untied ST+MJ+R 96.6 96.6 95.8 85.1 89.2 92.1 91.0\n(-0.1) (+1.0) (+1.1) (-0.8) (+0.5) (+3.9) (-1.7)\non top for the second model. Both models have the same hyperparameters. Com-\nparison with works of similar architecture and method are given in Table 3.\nTable 3.Comparison of accuracies with related works that are heavily based on trans-\nformer. The related works contain slight variations in the transformer architecture as\ndiscussed in Section 2. The reported accuracy is the weighted average over the 6 bench-\nmarks. The total count of 7672 includes IC15 (2077) while 7406 uses IC15 (1811). Note\nthat Lee et al. [20] uses two convolutional layers\nMethod Encoder Decoder Parameters Accuracy\n7672 7406\nRaisi et al. [31]ResNet based + Trans. Trans. - 89.5 -\nLu et al. [23] GCNet based Trans. - 89.3 -\nWu et al. [42]GCNet based + Trans. Trans. - - 90.7\nLee et al. [20] CNN based + Trans. Trans. 55.0M 88.4 -\nResNet based + Trans. Trans. 67.8M 85.7 87.1\nVision Trans. Trans. 45.8M 90.9 92.8\nThe transformer-only model outperforms the other works that employ a\nhybrid CNN-transformer encoder. This shows that competitive results can be\nachieved with just a pure transformer model. Furthermore, our experimental re-\nsults show that adding a ResNet on top of the transformer encoder has a lower\nperformance as compared with just using a vision transformer. Overall, the re-\nsults suggest that exploiting the long-term dependency at an earlier stage in an\nencoder-decoder framework appears to be beneficial for STR.\nComparison with Standard Ensemble. To evaluate the effectiveness of in-\ntegrated experts, 4 separate models were each trained with one of the following\n12 Y.L. Tan et al.\ninputs: (1) 8 × 4 patches with original ground-truth, (2) 8 × 4 patches with re-\nversed ground-truth, (3) 4 × 8 patches with original ground-truth, and (4) 4 × 8\npatches with reversed ground-truth. The ensemble of these 4 models is named\nEnsemble–Diverse and the PTIE trained with the 4 inputs is named PTIE–\nDiverse. The weighted average accuracies of the models over 6 benchmarks are\ntabulated in Table 4. It is to be noted that untied positional encoding was used\nin all experiments of this section.\nTable 4.Weighted average accuracies of mutilple methods on 6 benchmark datasets\n(with 2077 samples from IC15). The naming convention for the methods starts with\nthe patch resolution (e.g. 8 × 4) followed by the type of ground-truth used. “orig.\nGT” stands for the original ground-truth text, and “rev. GT” stands for the reversed\nground-truth\nMethod Parameters Acc\n7672\n8 × 4, orig. GT 45.8M 90.9\n8 × 4, invt. GT 45.8M 90.0\n4 × 8, orig. GT 45.8M 90.5\n4 × 8, invt. GT 45.8M 90.1\nEnsemble–Diverse 183.2M 92.4\n8 × 4, orig. GT (1) 45.8M 90.9\n8 × 4, orig. GT (2) 45.8M 90.7\n8 × 4, orig. GT (3) 45.8M 90.5\n8 × 4, orig. GT (4) 45.8M 90.7\nEnsemble–Identical 183.2M 92.1\nPTIE–Diverse 45.9M 92.4\nPTIE–Identical 45.9M 91.0\nUndoubtedly, the ensemble of the models brought about a significant perfor-\nmance boost. However, the improvement in accuracy comes at the price of requir-\ning a greater amount of model parameters. Ensemble–Diverse needing 183 .2M\nparameters, achieved an accuracy of 92 .4%. In contrast, PTIE–Diverse is able\nto achieve the same result of 92 .4% with only a quarter of the parameters.\nThe effectiveness of different patch resolutions and ground-truth types are\nalso analyzed with 4 randomly initialized models trained with patch resolution of\n8 × 4 and original ground-truth. Their accuracies are shown in Table 4 together\nwith their ensemble (Ensemble–Identical) and PTIE–Identical. Although there\nis only one type of ground-truth and patch resolution, PTIE–Identical is still\ntrained with separate positional encoding, linear layers, and embedding layers as\nper Section 4.1. From the experimental results, accuracy of Ensemble–Identical is\nlower than that of Ensemble–Diverse by 0.3% which highlights the effectiveness of\nusing different resolutions and ground-truth types. Furthermore, PTIE–Identical\nsuffers a 1.4% drop in accuracy indicating that different resolutions and ground-\ntruth types are crucial for PTIE on leveraging the experts through different\npositional encoding, linear layers, and embedding layers.\nPTIE for STR 13\nComparison of latency. Table 5 shows a comparison of latency with other\nrecent works that are open source. To tabulate the latency, inference on the test\nbenchmarks was done with an RTX3090 and batch size of 1. Using 4 sets of well-\ndesigned inputs mentioned in Section 4.1, both PTIE–Diverse and Ensemble–\nDiverse achieved the highest average accuracy. Furthermore, the latency of 52ms\nby PTIE–Diverse is comparable to the baseline (8 × 4, orig. GT) and is a quar-\nter of Ensemble–Diverse. This is because PTIE–Diverse decodes only once per\nsample despite having 4 sets of input, while Ensemble–Diverse needs to decode\n4 times. MLT-19 [27] containing 10,000 real images for end-to-end scene recog-\nnition averages 11.2 texts instances per image. Using a batch size of 11, the\nlatency of PTIE is about 11ms per cropped scene text image (averaging to 0.12s\nper full image). Therefore, it may not be a problem for real-time applications.\nFurthermore, in situations such as applications in forensic science (e.g. parsing\nimages from suspect’s hard disk) or assistance to visually impaired, accuracy\nwould be valued over latency.\nTable 5. Inference time and weighted average accuracy of recent works. The total\ncount of 7672 uses IC15 (2077) on top of the 5 other datasets mentioned in Section 5.1.\n7406 uses IC15 (1811) and 7248 uses IC15 (1811) and a filtered version of IC13. The\nvariation in total count is due to other works using varied set of benchmarks\nMethod Year Avg. accuracyParameters Time\n7672 7406 7248 (mil.) (ms)\nWang et al. [40] AAAI ‘20 86.9 - - 18.4 22\nLu et al. [23] PR ‘21 89.3 - - 54.6 53\nFang et al. [8] CVPR ‘21 - 92.8 - 36.7 27\nYan et al. [43] CVPR ‘21 - - 91.5 29.1 29\n8 × 4, orig. GT 90.9 92.8 92.2 45.8 50\nEnsemble–Diverse 92.4 93.7 93.8 183.2 202\nPTIE–Diverse 92.4 94.1 93.5 45.9 52\n(a)\n (b)\nFig. 6.Relative frequency distribution change in correct predictions of (a) PTIE from\nmodel trained with resolution 4 × 8 and original ground-truth. (b) PTIE from model\ntrained with resolution 8 × 4 and original ground-truth\n14 Y.L. Tan et al.\n5.4 Addressing Areas for Improvement\nAs per Section 3.1, the relative frequency distribution changes of PTIE–Diverse\nfrom the models trained with (1) 4 ×8 patches and (2) 8×4 patches, are plotted\nin Fig. 6a and Fig. 6b respectively. Relative improvement in the predictions is\nseen in most of the lengths and scales for both patch resolutions. This shows\nthat PTIE is effective in utilizing the advantages of both resolutions.\nFurthermore, the frequency distributions in Fig. 7 demonstrate that PTIE–\nDiverse, trained with original and reversed ground-truth, is able to lower the\nprediction error of first character as discussed in Section 3.2. Overall, PTIE is\nable to improve the accuracy in STR by mitigating the problem of the weak first\ncharacter prediction. Non-autoregressive decoding is explored in the supplemen-\ntary material.\n(a)\n (b)\nFig. 7.Normalized frequency distributions of wrong predictions by PTIE for word\nlength 5 conditioned on ground truth characters. (a) Predictions with one wrong char-\nacter. (b) Predictions with two wrong characters\n6 Conclusion\nIn this work, a simple and strong transformer-only baseline was introduced. By\nexploiting the long-term dependency of STR at an earlier stage in the model,\nthe baseline is able to outperform related works which uses hybrid transformer.\nWe then analyzed and discussed two areas for improvement for transformer in\nSTR. The integrated experts method was proposed to address them and state-\nof-the-art results were attained for most benchmarks. As the final predictions\nof PTIE were selected based on word probability, we will explore more selection\nmethods and streamline the processes in PTIE for future work.\nAcknowledgments:This work is partially supported by NTU Internal Funding\n- Accelerating Creativity and Excellence (NTU–ACE2020-03).\nPTIE for STR 15\nReferences\n1. Atienza, R.: Vision transformer for fast and efficient scene text recognition. ICDAR\n(2021)\n2. Baek, J., Matsui, Y., Aizawa, K.: What if we only use real datasets for scene\ntext recognition? toward scene text recognition with fewer labels. In: CVPR. pp.\n3113–3122 (2021)\n3. Bartz, C., Bethge, J., Yang, H., Meinel, C.: Kiss: Keeping it simple for scene text\nrecognition. arXiv preprint arXiv:1911.08400 (2019)\n4. Cao, Y., Xu, J., Lin, S., Wei, F., Hu, H.: Gcnet: Non-local networks meet squeeze-\nexcitation networks and beyond. In: ICCVW. pp. 0–0 (2019)\n5. Chen, X., Jin, L., Zhu, Y., Luo, C., Wang, T.: Text recognition in the wild: A\nsurvey. ACM Comput. Surv. 54(2), 1–35 (2021)\n6. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Mostafa, T.,\nDehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An\nimage is worth 16x16 words: Transformers for image recognition at scale. In: ICLR\n(2021)\n7. Ezaki, N., Kiyota, K., Minh, B.T., Bulacu, M., Schomaker, L.: Improved text-\ndetection methods for a camera-based text reading system for blind persons. In:\nICDAR. pp. 257–261 (2005)\n8. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: Autonomous,\nbidirectional and iterative language modeling for scene text recognition. In: CVPR.\npp. 7098–7107 (2021)\n9. Fu, Z., Xie, H., Jin, G., Guo, J.: Look back again: Dual parallel attention network\nfor accurate and robust scene text recognition. In: ICMR. pp. 638–644 (2021)\n10. Gomez, R., Shi, B., Gomez, L., Numann, L., Veit, A., Matas, J., Belongie, S.,\nKaratzas, D.: Icdar 2017 robust reading challenge on coco-text. In: ICDAR. vol. 1,\npp. 1435–1443 (2017)\n11. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text lo-\ncalisation in natural images. In: CVPR. pp. 2315–2324 (2016).\nhttps://doi.org/10.1109/CVPR.2016.254\n12. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham,\nJ.P.: Vizwiz grand challenge: Answering visual questions from blind people. In:\nCVPR. pp. 3608–3617 (2018)\n13. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n9(8), 1735–1780 (1997)\n14. Hu, W., Cai, X., Hou, J., Yi, S., Lin, Z.: Gtc: Guided training of ctc towards\nefficient and accurate scene text recognition. In: AAAI. vol. 34, pp. 11005–11012\n(2020)\n15. Huang, D., Lang, Y., Liu, T.: Evolving population distribution in china’s border\nregions: Spatial differences, driving forces and policy implications. Plos one15(10),\ne0240592 (2020)\n16. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and\nartificial neural networks for natural scene text recognition. arXiv preprint\narXiv:1406.2227 (2014)\n17. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwa-\nmura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., et al.: Icdar 2015\ncompetition on robust reading. In: ICDAR. pp. 1156–1160 (2015)\n18. Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R.,\nMas, J., Mota, D.F., Almazan, J.A., De Las Heras, L.P.: Icdar 2013 robust reading\ncompetition. In: ICDAR. pp. 1484–1493 (2013)\n16 Y.L. Tan et al.\n19. Ke, G., He, D., Liu, T.Y.: Rethinking positional encoding in language pre-training.\nIn: ICLR (2020)\n20. Lee, J., Park, S., Baek, J., Oh, S.J., Kim, S., Lee, H.: On recognizing texts of\narbitrary shapes with 2d self-attention. In: CVPRW. pp. 546–547 (2020)\n21. Li, H., Wang, P., Shen, C., Zhang, G.: Show, attend and read: A simple and strong\nbaseline for irregular text recognition. In: AAAI. vol. 33, pp. 8610–8617 (July 2019)\n22. Long, S., He, X., Yao, C.: Scene text detection and recognition: The deep learning\nera. IJCV 129(1), 161–184 (2021)\n23. Lu, N., Yu, W., Qi, X., Chen, Y., Gong, P., Xiao, R., Bai, X.: Master: Multi-aspect\nnon-local network for scene text recognition. PR 117, 107980 (2021)\n24. Luo, C., Jin, L., Sun, Z.: Moran: A multi-object rectified attention network for\nscene text recognition. PR 90, 109–118 (2019)\n25. Luo, C., Lin, Q., Liu, Y., Jin, L., Shen, C.: Separating content from style using\nadversarial learning for recognizing text in the wild. IJCV 129(4), 960–976 (2021)\n26. Mishra, A., Alahari, K., Jawahar, C.: Scene text recognition using higher order\nlanguage priors. In: BMVC (2012)\n27. Nayef, N., Patel, Y., Busta, M., Chowdhury, P.N., Karatzas, D., Khlif, W., Matas,\nJ., Pal, U., Burie, J.C., Liu, C.l., et al.: Icdar2019 robust reading challenge on\nmulti-lingual scene text detection and recognition—rrc-mlt-2019. In: ICDAR. pp.\n1582–1587. IEEE (2019)\n28. Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with\nperspective distortion in natural scenes. In: ICCV. pp. 569–576 (2013).\nhttps://doi.org/10.1109/ICCV.2013.76\n29. Qiao, Z., Zhou, Y., Wei, J., Wang, W., Zhang, Y., Jiang, N., Wang, H., Wang, W.:\nPimnet: A parallel, iterative and mimicking network for scene text recognition. In:\nACMMM. pp. 2046–2055 (2021)\n30. Qiao, Z., Zhou, Y., Yang, D., Zhou, Y., Wang, W.: Seed: Semantics enhanced\nencoder-decoder framework for scene text recognition. In: CVPR (June 2020)\n31. Raisi, Z., Naiel, M.A., Younes, G., Wardell, S., Zelek, J.: 2lspe: 2d learnable sinu-\nsoidal positional encoding using transformer for scene text recognition. In: CRV.\npp. 119–126 (2021)\n32. Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A ro-\nbust arbitrary text detection system for natural scene images.\nExpert Systems with Applications 41(18), 8027–8048 (2014).\nhttps://doi.org/https://doi.org/10.1016/j.eswa.2014.07.008, https://www.\nsciencedirect.com/science/article/pii/S0957417414004060\n33. Schulz, R., Talbot, B., Lam, O., Dayoub, F., Corke, P., Upcroft, B., Wyeth, G.:\nRobot navigation using human cues: A robot navigation system for symbolic goal-\ndirected exploration. In: ICRA. pp. 1100–1105 (2015)\n34. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based\nsequence recognition and its application to scene text recognition. PAMI 39(11),\n2298–2304 (2016)\n35. Shi, B., Yang, M., Wang, X., Lyu, P., Yao, C., Bai, X.: Aster: An attentional scene\ntext recognizer with flexible rectification. PAMI 41(9), 2035–2048 (2018)\n36. Tsai, S.S., Chen, H., Chen, D., Schroth, G., Grzeszczuk, R., Girod, B.: Mobile\nvisual search on printed documents using text and low bit-rate features. In: ICIP.\npp. 2601–2604 (2011)\n37. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: NIPS. vol. 30 (2017)\nPTIE for STR 17\n38. Wan, Z., He, M., Chen, H., Bai, X., Yao, C.: Textscanner: Reading characters in\norder for robust scene text recognition. In: AAAI. vol. 34, pp. 12120–12127 (April\n2020)\n39. Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition. In: ICCV.\npp. 1457–1464 (2011)\n40. Wang, T., Zhu, Y., Jin, L., Luo, C., Chen, X., Wu, Y., Wang, Q., Cai, M.: Decou-\npled attention network for text recognition. In: AAAI. vol. 34, pp. 12216–12224\n(April 2020). https://doi.org/10.1609/aaai.v34i07.6903\n41. Wang, Y., Xie, H., Fang, S., Wang, J., Zhu, S., Zhang, Y.: From two to one: A\nnew scene text recognizer with visual language modeling network. In: ICCV. pp.\n14194–14203 (2021)\n42. Wu, L., Liu, X., Hao, Y., Ma, Y., Hong, R.: Naster: Non-local attentional scene\ntext recognizer. In: ICMR. pp. 331–338 (2021)\n43. Yan, R., Peng, L., Xiao, S., Yao, G.: Primitive representation learning for scene\ntext recognition. In: CVPR. pp. 284–293 (2021)\n44. Yang, M., Guan, Y., Liao, M., He, X., Bian, K., Bai, S., Yao, C., Bai, X.: Symmetry-\nconstrained rectification network for scene text recognition. In: ICCV (October\n2019)\n45. Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards accurate\nscene text recognition with semantic reasoning networks. In: CVPR. pp. 12113–\n12122 (2020)\n46. Yue, X., Kuang, Z., Lin, C., Sun, H., Zhang, W.: Robustscanner: Dynamically\nenhancing positional clues for robust text recognition. In: ECCV. pp. 135–151.\nCham (2020)\n47. Zhan, F., Lu, S.: Esir: End-to-end scene text recognition via iterative image recti-\nfication. In: CVPR (June 2019)\n48. Zhang, C., Xu, Y., Cheng, Z., Pu, S., Niu, Y., Wu, F., Zou, F.: Spin: Structure-\npreserving inner offset network for scene text recognition. In: AAAI. vol. 35, pp.\n3305–3314 (2021)\n49. Zhang, H., Yao, Q., Yang, M., Xu, Y., Bai, X.: Autostr: Efficient backbone search\nfor scene text recognition. In: ECCV. pp. 751–767 (2020)\n50. Zhang, M., Ma, M., Wang, P.: Scene text recognition with cascade attention net-\nwork. In: ICMR. pp. 385–393 (2021)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8059242963790894
    },
    {
      "name": "Transformer",
      "score": 0.7702839374542236
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6102611422538757
    },
    {
      "name": "Encoder",
      "score": 0.6099589467048645
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5209813714027405
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38704487681388855
    },
    {
      "name": "Speech recognition",
      "score": 0.34100276231765747
    },
    {
      "name": "Voltage",
      "score": 0.08992457389831543
    },
    {
      "name": "Engineering",
      "score": 0.07835379242897034
    },
    {
      "name": "Electrical engineering",
      "score": 0.07141777873039246
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I115228651",
      "name": "Agency for Science, Technology and Research",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I3005327000",
      "name": "Institute for Infocomm Research",
      "country": "SG"
    }
  ]
}