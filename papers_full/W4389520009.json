{
  "title": "CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models",
  "url": "https://openalex.org/W4389520009",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1973592075",
      "name": "Qian Cheng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2289036883",
      "name": "Han, Chi",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A4298099827",
      "name": "Fung, Yi Ren",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2361432882",
      "name": "Qin, Yujia",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1860873322",
      "name": "Liu Zhiyuan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2604355407",
      "name": "Ji, Heng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W4389524456",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W4387294587",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4388626886",
    "https://openalex.org/W4298184221",
    "https://openalex.org/W4378718568",
    "https://openalex.org/W4389518724",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4385571271",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4390872747",
    "https://openalex.org/W4290771878",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4378942569",
    "https://openalex.org/W4386065691"
  ],
  "abstract": "Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLM's problem-solving paradigm, driving us closer to the next frontier of artificial intelligence. All the codes and data are released.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6922–6939\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCREATOR : Tool Creation for Disentangling Abstract and Concrete\nReasoning of Large Language Models\nCheng Qian1, Chi Han2, Yi R. Fung2, Yujia Qin1, Zhiyuan Liu1∗, Heng Ji1∗\n1Tsinghua University, 2University of Illinois at Urbana-Champaign\nqianc20@mails.tsinghua.edu.cn\nAbstract\nLarge Language Models (LLMs) have made\nsignificant progress in utilizing tools, but their\nability is limited by API availability and the\ninstability of implicit reasoning, particularly\nwhen both planning and execution are involved.\nTo overcome these limitations, we propose\nCREATOR , a novel framework that enables\nLLMs to create their own tools using documen-\ntation and code realization. CREATOR disen-\ntangles abstract tool creation and concrete de-\ncision execution, resulting in improved perfor-\nmance. We evaluate CREATOR on MATH and\nTabMWP benchmarks, respectively consisting\nof challenging math competition problems and\ndiverse tabular contents. Remarkably, CRE-\nATOR outperforms existing chain-of-thought,\nprogram-of-thought, and tool-using baselines.\nAdditionally, we introduce the Creation Chal-\nlenge dataset, featuring 2K diverse questions, to\nemphasize the necessity and benefits of LLMs’\ntool creation ability. Further research demon-\nstrates that leveraging LLMs as tool creators\nfacilitates knowledge transfer, and LLMs ex-\nhibit varying levels of tool creation abilities, en-\nabling them to adapt to diverse situations. The\ntool creation ability revolutionizes the LLM’s\nproblem-solving paradigm, driving us closer to\nthe next frontier of artificial intelligence. All\nthe codes and data are released1.\n1 Introduction\nIn recent years, notable progress has been made in\nlarge language models (LLMs) like GPT-3 (Brown\net al., 2020), Codex (Chen et al., 2021), PaLM\n(Chowdhery et al., 2022), LLaMA (Touvron et al.,\n2023), ChatGPT (OpenAI, 2022), and the latest\nGPT-4 (OpenAI, 2023). These models exhibit im-\npressive capabilities in in-context learning, code\ngeneration, and various Natural Language Process-\ning (NLP) tasks (Feng et al., 2020; Dong et al.,\n∗Corresponding author.\n1https://github.com/qiancheng0/CREATOR\n2022). However, there are still limitations to ad-\ndress, such as the inability to handle up-to-date\ninformation (Yu and Ji, 2023), provide accurate\nmathematical results, or reason over long chains\nof logic (Trivedi et al., 2022; Komeili et al., 2022;\nPatel et al., 2021; Hendrycks et al., 2021; Lu et al.,\n2022b).\nTo overcome these concerns, researchers have\nexplored equipping LLMs with external tools to\nalleviate their memory burden and enhance their\nexpertise (Qin et al., 2023). For instance, integrat-\ning tools such as question-answering systems or\nweb search engines enables LLMs to learn how\nand when to access external resources for problem-\nsolving (Nakano et al., 2021; Schick et al., 2023).\nRecent studies have also incorporated additional\ntools for LLMs, such as GitHub resources, neural\nnetwork models (e.g., Huggingface library), and\ncode interpreters (e.g., Python interpreter), aiming\nto enhance their capabilities (Gupta and Kembhavi,\n2022; Surís et al., 2023; Shen et al., 2023; Liang\net al., 2023; Lu et al., 2023). These tools require\nLLMs to provide detailed plans before utilizing\nthem to solve complex problems.\nHowever, tool-augmented LLMs still encounter\nchallenges (Chen et al., 2022; Gupta and Kemb-\nhavi, 2022; Schick et al., 2023; Surís et al., 2023),\nparticularly in the following aspects. (1) Limi-\ntation in scope : Current approaches focus on a\nlimited number of tools, making it difficult to find\nan appropriate existing tool for new problem types.\n(2) Fragility in reasoning: Given that tasks are\noften complex, reasoning on the fly case-by-case\ncan be fragile to random errors, while humans can\nbenefit from finding robust commonalities among\nmultiple similar questions. (3) Insufficiency in\nerror-handling: Current tool utilization pipelines\nlack automatic and specific error handling, necessi-\ntating improvements in accuracy and robustness to\nensure reliable execution results.\nIn this paper, we propose a novel approach to ad-\n6922\nAbstraction: Quadratic Equation Solving\nCreation External API Platform\nSearch \nEngine\nMath\nSolver\n…\nDecision\nConcretization: Tool Calling 1. Use Math Solver to do query.\n2. Apply Info Extractor to get the answer.\nSelection:\nWolframAlpha, Info Extractor\nExecution\nWolframAlpha Query:\nSolve Eq {-5t^2 + 20t + 40 = 0}\nExecute the program after \nconcatenating tools and their calls\nRectification\nCREATOR (Ours) Tool-Use\nAnswer\n--------------------\nWolframAlpha Returns: -1.46, 5.46Output from Exec: The time is 5.46\nError     → Rectify Creation / Decision\nSuccess → Render Answer\ndef solve_quadratic(a, b, c):\n...\nans = solve_quadratic(-5,20,40)\nt = [x for x in ans if x > 0][0]\nprint(\"The time is\", t)\nThrowing a ball upward from a 40m tall building, its height relative to time is represented \nby h(t) = -5t^2 + 20t + 40. When will it reach the ground?\nQuery\nFigure 1: The difference between CREATOR and a gen-\neral tool-using framework.\ndress these challenges. Rather than treating LLMs\nas users of tools, we empower them to be cre-\nators of tools, enabling them to solve problems\nwith higher accuracy and flexibility. We introduce\nour tool creation framework, CREATOR , which\nleverages LLMs’ ability to create and modify tools\nbased on the problem at hand. Figure 1 illustrates\nthe differences between CREATOR and a general\ntool-using framework. While the tool-using frame-\nwork focuses on reasoning to select and plan API\nusage, our framework emphasizes diversifying tool\nchoices, disentangling abstract and concrete rea-\nsoning, and improving robustness and accuracy.\nSpecifically, CREATOR consists of four stages:\n• Creation: Create generally applicable tools with\ndocumentation and realization through abstract\nreasoning based on the problem.\n• Decision: With available tools, decide when and\nhow to use them to solve the problem.\n• Execution: Execute the program, applying the\nchosen tools to solve the problem.\n• Rectification: Make modifications to tools and\ndecisions based on the execution result.\nBy introducing these four stages, we aim to bet-\nter inspire the LLM’s creativity and enhance the\nparadigm’s robustness. This design sets CREATOR\napart from traditional tool-using and addresses the\nthree challenges we discussed respectively by (1)\nleveraging LLMs to create tools with higher gen-\nerality, reusability, and variety, rather than relying\non a limited number of given APIs; (2) offload-\ning the cognitive burden of LLMs and disentan-\ngling their ability to perform abstract reasoning\n(creation of generalizable tools) and concrete rea-\nsoning (decision-making with details); (3) utilizing\ncode as the medium for tool creation, which is\nmore sensitive to errors, and enabling automatic\nrectification of tools and decisions based on error\ntracebacks.\nTo evaluate our design’s effectiveness, we test\nCREATOR on two existing benchmarks: MATH\n(Hendrycks et al.) and TabMWP (Lu et al., 2022a),\nas well as the Creation Challenge dataset we create.\nThe MATH dataset contains diverse and challeng-\ning math competition problems, while TabMWP in-\ncludes a wide range of tabular contexts for problem-\nsolving. Notably, ChatGPT built on CREATOR\nachieves remarkable average accuracies of 59.7%\nand 94.7% on MATH and TabMWP respectively,\nsurpassing the standard chain-of-thought (CoT)\n(Wei et al., 2022), program-of-thought (PoT) (Chen\net al., 2022), and tool-using baselines by significant\nmargins.\nAs existing benchmarks do not specifically evalu-\nate tool creation, we further introduce the Creation\nChallenge dataset, which consists of novel and chal-\nlenging problems that are inadequately solved us-\ning existing tools or code packages. This dataset\nhighlights the necessity and advantages of LLMs’\ntool creation ability. In addition, we show exper-\nimental results that provide evidence of how tool\ncreation plays a crucial role in promoting knowl-\nedge transfer across similar queries that possess\ncommon core knowledge but differ in specific sce-\nnarios. We also present case studies highlighting\nthe varying levels of tool creation ability observed\nin LLMs, allowing them to better adapt to diverse\nproblem settings.\n2 Related Work\nLarge Language Models. Large Language Mod-\nels (LLMs) have gained attention for their im-\npressive performance in handling various NLP\ntasks, following demonstrations and generating\nhigh-quality texts and codes (Brown et al., 2020;\nChen et al., 2021; Chowdhery et al., 2022; Touvron\net al., 2023). Prompting methods such as chain-\nof-thought (Wei et al., 2022), instruction-following\n(Wang et al., 2022b; Longpre et al., 2023; Chung\net al., 2022; Touvron et al., 2023; Liu et al., 2023),\nand verification mechanisms (Fung et al., 2023)\nhave been developed to guide LLMs in problem-\n6923\nsolving and align their behavior with human expec-\ntations. Our work builds upon these areas, incorpo-\nrating them into our framework and using them as\nbaselines for complex problem-solving.\nTool Use and Tool Creation. As an emerging\nfield within NLP, the active interaction of LLMs\nwith environments is facilitated through tools that\nserve as the medium (Li et al., 2023). Recent stud-\nies address constraints of LLMs, such as the lim-\nited real-time responsiveness and inaccurate cal-\nculations, by incorporating external tools (Trivedi\net al., 2022; Komeili et al., 2022; Patel et al., 2021;\nLu et al., 2022b). These studies augment LLMs\nwith tools like scratch pads, search engines, QA\nsystems, and calculators (Nye et al., 2021; Shuster\net al., 2022; Schick et al., 2023) to improve task\nperformance. More recent efforts integrate LLMs’\ntool-using abilities into a pipeline for task planning,\ntool calling, and result synthesis (Wu et al., 2023;\nShen et al., 2023; Liang et al., 2023). In contrast,\nour work goes further by enabling LLMs to create\ntools instead of relying solely on existing tools. As\nour concurrent works, tool creation ability is also\ninvestigated under LATM framework (Cai et al.,\n2023) and LLM customization (Yuan et al., 2023).\nReasoning and Execution with Program. Rea-\nsoning with programs is an emerging field in NLP,\nwhose goal is to leverage codes to do complicated\ncomputational reasoning instead of using natural\nlanguage thoughts. Chen et al. (2022) show that\ncode generation improves performance on math\ndatasets, while Gao et al. (2022); Wang et al.\n(2022a) further demonstrate the potential of pro-\ngram reasoning on symbolic and algorithmic bench-\nmarks. These efforts present a code-based chain-of-\nthought with linear logic but produce no enhanced\ntools capable of being reused or tested. As the con-\ncept of tool-using emerges, recent studies begin to\nincorporate code interpreters as external tools (Lu\net al., 2023; Mialon et al., 2023; Wang et al., 2023).\nHowever, inCREATOR , we use code as the medium\nfor tool creation rather than an external tool. Our\nframework also excels over PoT as we devise the\ntool creation stage, code rectification stage, and\ndisentangle the logic in complex reasonings.\n3 Design of C REATOR\nDistinct from previous frameworks for tool-using,\nCREATOR leverages the tool creation ability of\nLLMs by incorporating four special stages: cre-\nMethod Create\nTools\nUtilize\nTools\nApply\nCodes\nEmphasize\nReusability\nReasoning\nPattern\nCoT - - - - Linear\nPoT - - ✓ - Linear\nTool Use - ✓ ✓ - Linear\nCREATOR ✓ ✓ ✓ ✓ Non-Linear\nTable 1: A comprehensive comparison of CREATOR\nwith other methods.\nation, decision, execution, and rectification, as illus-\ntrated in Figure 2. The utilization of tool creation\nfor problem-solving is inherently straightforward\nand aligns with LLMs’ innate ability, as illustrated\nlater in Section 5.2. In CREATOR , the main objec-\ntive of design is to instinctively better inspire their\ncreativity, and facilitate more effective use of it.\nPrevious CoT and PoT methods mainly apply\nlinear reasoning to solve target problems, and their\ntask-solving process lacks reusability. However,\nthe tools created in CREATOR can be transferred to\nsolve other queries, and the rectification stage in-\ncorporated makes the reasoning process non-linear.\nWe present a comprehensive comparison between\nCREATOR and other methods in Table 1.\n3.1 Creation Stage\nImplementation Details. In the creation stage\nof CREATOR , we explicitly instruct LLMs with\ndemonstrative examples to create tools and doc-\numentation to solve the problem. The gen-\neral prompt text form is “ ###Instruction\n[INSTRUCTION]\\n [EXAMPLE 1]\\n [EXAMPLE 2]\n...”. Here the instruction text “ [INSTRUCTION]”\ndescribes the goal and format of the output. Each\ndemonstration “ [EXAMPLE x] ” follows format\n“### Question [QST]\\n ### Tool [TOOL]”. Each\n[TOOL] contains documentation text as code com-\nments. A detailed example of prompt text is shown\nin Figure 14.\nTo get these demonstrations, we make a fixed\nset of demonstrations in advance and use them sub-\nsequently for each task. In specific, we randomly\nselect a subset from the training set and prompt the\nLLM with the text instruction for tool creation for\neach query. We then correct the errors in these gen-\nerations (if any) and remove verbose explanations\nbefore using them. Although the demonstrations\nare from the same task as the test queries, they\nare not required to be semantically similar to test\nqueries, as the main purpose is only to inspire the\n6924\nRectification\nIf Error Occurs:\nExecution\nTool: Code Format\nGiven the equations 3x+y=17, 5y+z=14 and \n3x+5z=41, what is the value of the sum x+y+z ?\nQuery\nCreation\nCreated Tool\nsolve_equations(equations) \nDocumentation\nSolves a system of linear equations using numpy.linalg.solve;\nTakes in a list of equations in the form of 2D arrays;\nReturns a list of the solutions to the system.\nDecision\nMake Decisions about How to Apply the Tool \n1. Use the tool to get the solution for equation system\n2. Take the sum over the solutions to find the answer\nDecision\nsolution = solve_equations(equations)\ntarget = sum(solution)\nEXEC\nTraceback:\nAt line ….\n(Error Reasons)\nIf Success:\nPrinted:\nThe result of\nx + y + z  is 12\n--------\nAnswer\nThe Answer for this Query is 12.\ncoef = []\nconst = []\nfor eq in equations:\ncoef.append(eq[:-1])\nconst.append(eq[-1])\nsol = np.linalg.solve(coef, const)\ndef solve_equations(equations):\n...\nequations = [[3,1,0,17], [0,5,1,14], \n[3,0,5,41]]\nsolution = solve_equations(equations)\ntarget = np.sum(solution)\nprint(\"The sum of x + y + z is\", target)\nRealization\nDecision: Code Format\nFigure 2: Overview of our CREATOR framework with four stages: Creation, Decision, Execution, and Rectification.\nWith an LLM like ChatGPT, we successfully leverage its tool creation ability with code as the medium. In each\nstage we apply instructions and demonstrations in prompts, shown in Figures 14 to 16 in Appendices.\nLLM’s creativity and regulate its output format.\nAbility of Abstract Reasoning. The core impor-\ntance of the tool creation stage is to trigger LLM’s\nability to employ abstract thinking to alleviate the\nburden of reasoning during later stages. When\nLLMs create tools, they effectively use abstraction\nto address a particular problem type, necessitating\na focus on the inherent characteristics of the prob-\nlem rather than the specific numerical details. For\nexample, in Figure 2, the LLM concentrates solely\non recognizing the intrinsic nature of the problem\nand creates a tool for solving a three-variable equa-\ntion system, disregarding all the numerical details\nand the specific expression being queried.\n3.2 Decision Stage\nImplementation Details. Similar to the creation\nstage, we instruct LLMs with demonstrations to\ndecide how to use tools with the same prompt text\nform. Each demonstration “ [EXAMPLE x] ” fol-\nlows “### Question [QST]\\n ### Tool [TOOL]\\n\n### Solution [SOL]”, where [SOL] represents\nthe LLM’s decision tool calls in code format. We\nalso derive a fixed demonstration set the same way\nas in the creation stage, only that the LLM is now\nprompted to call the given tools instead of creat-\ning them, and to print out the final answer with\nany important information through ”print(...)” in\ncodes. This [INSTRUCTION] applies both to get\ndemonstrations and to conduct test-time inference,\nwhich ensures that the LLM’s answer can be eas-\nily extracted from printed outputs in subsequent\nstages. A detailed prompt text example is shown in\nFigure 15.\nAbility of Concrete Reasoning. The decision\nstage necessitates the LLM’s meticulous attention\nto rules and details for problem-solving, which we\nrefer to as concrete reasoning. In Figure 2, the so-\nlution obtained from the tool needs to be summed\nfor the final answer. This requires the LLM to un-\nderstand the tool’s outputs and relate them to the\nspecific query to make an informed decision and\nderive the correct answer finally. By separating\ncreation from the decision, CREATOR disentangles\ntwo phases of the LLM’s abilities, which facili-\ntates a smoother elicitation of different aspects of\nknowledge and improves task performance.\n3.3 Execution Stage\nThe execution stage takes the information from pre-\nvious stages to execute the tool leveraging the code\ninterpreter. We do not apply the LLM in this stage,\nand the created tools and the LLM’s decision are\nconcatenated into a cohesive code block for exe-\ncution. The tool is encapsulated within a function\nin the code block, and the LLM’s decision calls it\n6925\nfor problem-solving. During execution, we capture\nany outputs printed (as we have instructed the LLM\nin the decision stage) or errors encountered (by in-\ntercepting error messages in a sub-process). These\ninformation serve as inputs for subsequent stages\nto determine whether an answer can be obtained or\nrectifications are needed.\n3.4 Rectification Stage\nImplementation Details. During the rectifica-\ntion stage, CREATOR has two different options\nbased on the information passed into it. If an\nerror occurs, then the LLM is prompted with\ndemonstrations to rectify the error. Applying a\nsimilar prompt format as before, the format of\ndemonstrations “[EXAMPLE x] ” now changes to\n“### Question [QST]\\n ### Original [ORI]\\n\n### Error [ERR]\\n ### Rectification [REC]”,\nwhere we provide the original tool implementation\nand calling decision in [ORI], offer the error trace-\nbacks [ERR], and concatenate natural language rea-\nsoning on the error with the rectified code in[REC].\nA detailed illustration of the prompt text is shown\nin Figure 16.\nIf the execution is successful, then the answer\nwill be extracted from the captured model’s output\nand compared to the standard answer to measure\naccuracy.\nSignificance. During the rectification process,\nwe provide the LLM with error tracebacks, which\noffer crucial information for it to identify the er-\nror’s location and causes. Armed with this guid-\nance, the LLM can recover from previous mistakes,\nadjust its reasoning process, and attempt to solve\nthe problem once again. Subsequent experiments\nwill demonstrate how the inclusion of rectification\nsignificantly improves the performance of CRE-\nATOR. The success of the rectification stage also\nshowcases the LLM’s ability to recognize miscon-\nceptions and self-correct.\n4 Experiments\nTo evaluate the effectiveness ofCREATOR , we con-\nduct experiments on two established benchmarks:\nMATH (Hendrycks et al.) and TabMWP (Lu et al.,\n2022a). Additionally, we perform experiments on a\nnewly introduced dataset, Creation Challenge, com-\nprising 2K diverse questions that are inadequate to\nsolve using existing tools or code packages. This\nenables us to further demonstrate the necessity and\nadvantages of the LLM’s tool creation ability.\n4.1 Experimental Setup\nSettings. We select ChatGPT as the base model\nfor all methods due to its exceptional capabilities\nin code generation, decision-making, and logical\nreasoning. Refer to Appendices A.1 for more\ndetails. We evaluate CREATOR on two existing\ndatasets: TabMWP, which includes diverse table-\nrelated problems, and MATH, consisting of chal-\nlenging math competition problems. We apply\nthem as they are representative in terms of diversity\nin data format and difficulty. We also assess the per-\nformance of our framework on Creation Challenge,\ncomprising 2K data points, to explore the impact\nof tool creation hints on the LLM’s performance.\nRefer to Appendices A.2 for more details.\nBaselines. We compare CREATOR against four\ntypes of baselines to demonstrate its effectiveness:\n• Vanilla LLM w/ and w/o CoT : The Vanilla\nLLM with CoT employs linear reasoning to\nsolve problems, while Vanilla LLM without CoT\ndirectly generates the answer.\n• PoT: The LLM utilizes a program to reason\nthrough the problem step by step. Besides,\nwe also incorporate rectification into PoT as a\nstronger baseline for a fair comparison.\n• Tool Use: The LLM utilizes the WolframAlpha\nAPI as a general-purpose tool specialized in cal-\nculations. It’s a fair external tool as all queries\nrequire numerical reasoning to some extent.\n• CREATOR -Entangled: The LLM combines the\ncreation and the decision stage in CREATOR in-\nstead of disentangling them, which serves as a\nspecial baseline for ablation study.\n4.2 Creation Challenge\nExisting benchmarks are not originally designed to\nevaluate tool creation, thus unable to fully show-\ncase the necessity and advantages brought by the\nLLM’s tool creation ability. Therefore, we intro-\nduce Creation Challenge to test the LLM’s problem-\nsolving skills under new scenarios, without existing\ntools or code packages that can be directly applied.\nRefer to Appendices B.1 for details about the data\nformat and construction process.\nEvaluation The components of the standard cre-\nated tool in each data point of Creation Challenge\ncan serve as valuable hints for the LLM’s tool\ncreation. Therefore, we extend our experiments\non Creation Challenge to assess the LLM’s tool\n6926\nMethod Setting Algebra Counting &\nProbability Geometry Itmd.\nAlgebra\nNumber\nTheory\nPre-\nAlgebra\nPre-\nCalculus\nAverage\n(weighted)\nVanilla w/o CoT 25.7 25.8 22.4 13.9 18.5 40.9 21.8 25.3\nw/ CoT 50.9 36.1 24.5 17.5 23.2 58.6 16.7 37.9\nPoT (w/o Rec.) w/o CoT 58.2 48.5 35.4 25.8 53.1 66.8 25.0 49.8\nw/ CoT 54.0 47.8 32.5 22.3 48.9 64.5 19.9 46.5\nPoT (w/ Rec.) w/o CoT 63.8 51.9 35.9 28.6 59.2 70.0 28.2 53.9\nw/ CoT 61.4 48.8 34.6 23.7 54.5 67.6 34.6 51.2\nTool Use w/o CoT 47.3 35.1 27.0 20.5 30.8 56.8 31.4 39.0\nw/ CoT 55.3 37.8 28.7 20.5 34.8 61.8 26.9 43.0\nCREATOR\n-Entangled\nw/o Demo. 58.0 53.3 34.2 21.8 55.7 63.4 33.3 49.6\nw/o CoT 64.1 55.7 35.9 42.7 61.6 69.0 37.2 57.2\nw/ CoT 62.7 50.9 33.8 31.4 61.4 68.7 31.4 54.0\nCREATOR\n(ours)\nw/o Demo. 66.6 53.6 33.8 29.4 59.8 68.7 34.6 54.9\nw/o CoT 71.5 55.3 41.4 41.9 60.4 71.7 35.3 59.7\nw/ CoT 63.1 58.1 34.6 35.0 61.8 69.7 32.1 55.7\nTable 2: The accuracy (%) on the test set of MATH dataset leveraging ChatGPT.Rec. represents Rectification.\ncreation ability with varying levels of hint utiliza-\ntion. We encourage future research to explore the\ndataset’s potential through more flexible usage.\n4.3 Experimental Results\nWe present the results on MATH, TabMWP, and\nCreation Challenge respectively in Tables 2 to 4.\nCREATOR achieves an accuracy of 59.7%, 94.7%,\nand 75.5% respectively on three tasks, surpass-\ning all the best performance in baselines by large\nmargins. To illustrate CREATOR ’s advantage, we\npresent a case study showing how it’s better than\nTool Use in Figure 3A. For all tasks, disentan-\ngling the creation and decision stages generally\nresults in better performance, compared to CRE-\nATOR-Entangled. For Creation Challenge, we also\nobserve that hints of tool creation can raise the per-\nformance up to 18.7%. We will further analyze the\nreasons for this improvement in Section 4.4.\n4.4 Results Analysis\nCoT Incompatible with Codes. Table 2 shows\nthe LLM’s performance on MATH problems de-\ncreases consistently when applying CoT under PoT\nmethod and CREATOR , and the opposite trend is\nobserved for TabMWP. We attribute this difference\nto the inherent incompatibility between natural lan-\nguage reasoning and program-based reasoning on\nchallenging problems. MATH problems involve\nintricate calculations and diverse reasoning paths,\nleading to conflicts between natural language and\nprogramming approaches. When CoT is used, the\nLLM tends to generate programs following natural\nlanguage reasoning, which compromises the coher-\nence and unique advantages of programming. In\nMethod Setting Accuracy Successful\nExecution\nStandard w/o CoT 68.2 99.1\nw/ CoT 75.2 99.3\nPoT (w/o Rec.) w/o CoT 80.6 98.5\nw/ CoT 80.0 91.2\nPoT (w/ Rec.) w/o CoT 81.2 99.7\nw/ CoT 87.3 100\nTool Use w/o CoT 77.6 100\nw/ CoT 79.6 100\nCREATOR -Entangled w/o CoT 91.6 100\nw/ CoT 93.5 99.9\nCREATOR (ours) w/o CoT 90.5 99.7\nw/ CoT 94.7 100\nTable 3: The accuracy (%) on the test set of TabMWP\ndataset leveraging ChatGPT. Successful Executionin-\ndicates whether the LLM provides a valid final answer\nthrough words or codes within the rectification limit.\nFigure 3B. we show the adoption of brute-force\nalgorithms and straightforward calculations when\nCoT is not applied yields higher accuracy.\nIn contrast, TabMWP involves simpler calcula-\ntions and more straightforward reasoning paths,\npromoting consistency between natural language\nand programming reasoning. Therefore, the ap-\nplication of CoT enhances performance in these\ncases. We present more case studies to illustrate it\nin Appendices C.1.\nCREATOR is Robust to Challenges. Figure 4 il-\nlustrates the performance of the LLM in relation to\ndifficulty. CREATOR outperforms all the baselines\nfor both tasks and achieves higher accuracy, partic-\nularly for difficult problems. This provides com-\npelling evidence that CREATOR exhibits greater\nresilience to challenges.\n6927\nProblem: In the nth row of Pascal's Triangle where the first row is n=0, the arithmetic mean of the elements is 51.2. \nWhat is the value of n?\nA\ndef pascals_mean(n):\n# Initialization\nrow_sum = 0\nnum_el = n + 1\n# Take loop and add sum\nfor k in range(num_el):\nel = math.comb(n, k)\nrow_sum += el\n# Compute the mean\nmean = row_sum / num_el\nreturn mean\n# Initialization\ntarget = 51.2\nn = 0\n# Take loop until match\nwhile True:\nmean = pascals_mean(n)\nif mean == target:\nbreak\nn += 1\n# Print the answer\nprint(\"The value is\", n)\nCreation: Decision\nAnswer: The value is 9 \nIn Pascal‘s Triangle, the nth row has n+1 elements, and their sum is \n2^n. The arithmetic mean is then $\\frac{2^n}{n+1}$, which is 51.2\nMultiplying both sides by $n+1$ and taking the logarithm:\n$n\\ln(2)+\\ln(n+1)=\\ln(51.2)+\\ln(10)$\nWOLFRAM CALL: solve n*log(2)+log(n+1)=log(512)\nAPI Return: [{'@title': 'Result', 'subpod’: {'plaintext': 'n = (W(1024 \nlog(2)) - log(2))/log(2)≈6.16003'}]\nFinal Answer: the value is approximately 6.16003, but since n \nrepresents an integer, we round up to get n=7.\nProblem: How many x between 0 and 100 exclusive satisfy both conditions? 1. x is a multiple of 4.  2. x is a multiple of 6.\nw/o CoT: w CoT:\nI can use the concept of LCM to solve \nthis problem. In the tool, I can use the \nLCM of 4 and 6 to calculate the \nnumber of integers between 0 and 100 \nthat are multiples of both 4 and 6.\nAnswer: 8 Answer: 9\ndef count_multiples(a, b):\ncount = 0\nfor i in range(1, 101):\nif i % a == 0 and i % b == 0:\ncount += 1\nreturn count\ndef count_multiples(start, end, num):\nreturn (end - 1) // num - \\\n(start - 1) // num\ndef count_x_values():\nlcm = 12 # LCM of 4 and 6\nreturn count_multiples(0, 100, lcm)\nB\nFigure 3: In subfigure A, we show an example in which Tool Use reasoning (left) fails, while CREATOR (right)\nsolves successfully as it derives a new tool for the novel question. In subfigure B, we present a case comparing\nthe answer given by CREATOR with and without CoT. Challenging problems in MATH cause conflicts between\nlanguage and program reasoning.\nMethod Setting Accuracy Successful\nExecution\nStandard w/o CoT 27.9 94.9\nw/ CoT 32.7 99.1\nPoT (w/o Rec.) w/o CoT 59.2 93.5\nw/ CoT 60.7 95.7\nPoT (w/ Rec.) w/o CoT 61.1 98.3\nw/ CoT 62.0 98.9\nCREATOR -Entangled\n(w/o CoT)\nno hint 64.5 99.2\nutility hint 65.8 99.3\nall hint 75.3 99.5\nCREATOR (ours)\n(w/o CoT)\nno hint 63.8 98.7\nutility hint 67.2 99.1\nall hint 75.7 99.5\nTable 4: The accuracy (%) on the Creation Challenge\ntest set leveraging ChatGPT. No hintrepresents normal\nCREATOR framework. Utility hintprovides hints about\nthe utility of the tool, while all hint offers additional\nhints about the possible inputs and outputs of the tool.\nRectification Raises Performance. Figure 5\ndemonstrates the improvement in the LLM’s per-\nformance achieved through the application of the\nrectification stage. Results show rectification can\nincrease the accuracy by approximately 10% of\nthe original value, which proves the necessity and\nrationality of establishing this stage.\nInfluential Factors of Tool Creation. Tables 2\nto 4 highlight two crucial factors affecting the\nLLM’s performance. (1) Separation of Creation\nand Decision: The separation of these two stages\ninherently represents the disentanglement of the\nLLM’s abstract and concrete reasoning, which\nMATH\nTabMWP\nCREATOR (best)\nFigure 4: Comparison of the accuracy of baselines and\nCREATOR w.r.t.problem difficulty.\nleads to improved performance. (2) Availability\nof Hints: In practical scenarios, guidance is often\nnecessary to harness the LLM’s behavior when cre-\nating tools. We demonstrate that providing more\ndetailed hints can significantly improve the LLM’s\nperformance, as they enable easier implementation\nof desired tools and eliminate uncertainty and mis-\ndirections in CoT or tool documentation.\n5 Further Analysis\nIn this section, we further show the advantages\nbrought by the LLM’s tool creation ability and use\ncase studies to demonstrate different aspects of this\nability, which enables them to tackle challenges\n6928\n49.8 53.2 53.9\n80.0\n86.7 87.3\n60.7 61.7 62.0\n57.4 58.7 59.7\n83.2\n91.1 94.7\n74.5 75.2 75.7\nIncrease\n8.2% \nIncrease\n2.1% \nIncrease\n9.1% \nIncrease\n4.0% \nIncrease\n1.6% \nIncrease\n13.8% \nCREATOR Improvement\nPoT Improvement\nFigure 5: The improvement brought by rectification on\nboth PoT and CREATOR . Rectify-N denotes enabling N\nrounds of rectifications.\nSet of Queries, Count\nData Pieces, Count\n100\n300\nTool Create Normal, Acc.\nTool Create with Transfer, Acc.\nIncrease of Acc.\n63.0%\n78.3%\n15.3%\nSets Worse with Transfer\nSets Better with Transfer\n2 / 100\n39 / 100\nTable 5: Results of tool transfer experiment. Tool trans-\nfer improves accuracy by up to 15.3%.\nwith more flexibility and less reasoning burden.\n5.1 Facilitation of Knowledge Transfer\nOne of the main purposes of tool creation lies in its\nreusability. The content of tools represents the ab-\nstraction of knowledge concepts, so the creation of\none tool may help solve problems of various scenar-\nios that share the same core concept. For instance,\na keyword-extraction tool created for sentiment\nanalysis can be applied to other tasks like docu-\nment categorization and topic modeling, as they\nall require the identification and extraction of rel-\nevant keywords for problem-solving. By utilizing\nthe knowledge and logic embedded in the tool, the\nLLM can transfer its understanding to solve similar\nproblems efficiently with higher performance.\nSettings. To validate our hypothesis, we con-\nstruct a small set of questions with 300 data\npoints, detailed in Appendices B.2. We divide data\npoints into 100 sets, where all three queries in one\nset share the same core knowledge concept (key\nmethodology that is universally applicable) but dif-\nfer in scenario (problem background and specific\ndetails inquired).\nSimilar to previous experiments, we use Chat-\nGPT as the base LLM with unchanged detailed\nsettings. We first test all the problems under the\nnormal CREATOR framework respectively. Then,\nwe test if the correct tool created under one sce-\nnario could be applied to the other two, and again\ntest the LLM’s performance.\nResults Analysis. The statistics are presented in\nTable 5. Through the application of transferred\ntools, the LLM’s accuracy can be raised by 15.3%.\nFurther analysis shows that 39 sets of queries are\npositively influenced by this transfer, which high-\nlights the tool creation ability of the LLM can\nfacilitate knowledge transfer, leading to better\nperformance on clusters of problems that share\nsimilar core concepts.\n5.2 Different Levels of LLM’s Tool Creation\nWe discover in experiments that LLM can create\ntools in different levels without special guidance,\nwhich affirms creativity is LLM’s intrinsic emerg-\ning ability. By inspecting the created tools, we\nfind that they can be categorized into three levels,\nwhich provides guidance and reference for future\ndevelopment.\n1. Enhancement of Existing Tool. First, LLMs\ndemonstrate the capability to enhance existing tools\nby encapsulating an existing tool or API and re-\npurposing it to serve different needs. The first case\nof Figure 9 shows how LLM wraps an existing\nweather query API into a new tool that calculates\nthe average temperature.\n2. Concatenation of Multiple Tools. Second,\nthe LLM can create new tools by organizing multi-\nple APIs into a pipeline, enabling it to fulfill spe-\ncific purposes. The second case in Figure 9 shows\nhow the LLM calls two existing APIs three times\nin the new tool for problem-solving.\n3. Hierarchical Tool. Third, the LLM can create\ntools with a clear hierarchy, which establishes clear\ncaller-callee relationships among tools and reduces\nthe burden of repetitive reasoning. The third case\nin Figure 9 illustrates a hierarchical structure where\nthe first tool serves as the callee, while the second\ntool primarily solves the problem.\n6 Conclusion\nWe propose the concept of automatic tool creation\nthrough LLMs and empirically devise CREATOR\n6929\nthat harnesses the capabilities of LLMs as tool cre-\nators. By disentangling LLM’s abstract and con-\ncrete reasoning, CREATOR enables clearer logic\nand enhances overall performance. Through com-\nprehensive evaluations on established benchmarks\nand Creation Challenge, we demonstrate the supe-\nriority and indispensability of CREATOR compared\nto existing CoT, PoT, and tool-using approaches.\nWe anticipate our study will inspire the develop-\nment of more sophisticated AI systems leveraging\nLLM’s tool creation potential.\nLimitations\nOur experiment is limited to two established bench-\nmarks, MATH and TabMWP, along with our newly\nintroduced dataset, Creation Challenge. However,\nit is crucial for future research to expand the appli-\ncation of our framework to encompass a broader\narray of tasks. This will enable a comprehensive\nassessment of the generalizability of our results,\ngoing beyond the scope of our current investiga-\ntion.\nFurthermore, our demonstration of the LLM’s\npotential in tool creation is limited in scope. For\ninstance, the current LLM is also capable of creat-\ning tools even to build a full project pipeline, but\nthe execution ability and correctness of its creation\nstill lack proper evaluations and remain question-\nable. It is incumbent upon future research to delve\ndeeper into the boundaries of LLM’s capabilities\nand establish clear limits regarding its tool creation\npotential.\nEthics Statement\nWe consider the following research issues in this\npaper:\n• Privacy involves safeguarding sensitive informa-\ntion and preventing its unauthorized disclosure.\nWith respect to our framework, privacy becomes\na concern when certain stages require demon-\nstration examples and clear instructions, which\nmay inadvertently contain sensitive information,\nor intentionally designed to prompt the LLM to\nleak privacy. Thus, it is crucial to ensure that per-\nsonal or sensitive information is not disclosed\nto the closed-source LLM, and the private infor-\nmation or knowledge about tool creation in the\nclosed-source LLM should be well-protected.\n• Fairness in AI aims to ensure the outputs and\ndecisions made by AI systems do not perpet-\nuate existing biases or discriminations. When\ncreating tools, care must be taken to mitigate\nbiases in the demonstrations and instructions,\nmonitor the tool’s performance across stages,\nand address any disparities that may arise in the\nwhole generation or rectification process.\n• Transparency involves making AI systems and\ntheir processes understandable and interpretable.\nWhen the language model creates tools un-\nder our framework, it’s essential to have trans-\nparency regarding how those tools are devel-\noped. Developers should document any biases\nor limitations associated with the tools created,\nunderstand the strengths and weaknesses of the\ntools and how the decision is reached, and make\ninformed decisions about their application.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,\nand Denny Zhou. 2023. Large language models as\ntool makers.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\n6930\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. arXiv preprint arXiv:2002.08155.\nYi R. Fung, Tuhin Chakraborty, Hao Guo, Owen Ram-\nbow, Smaranda Muresan, and Heng Ji. 2023. Norm-\nsage: Multi-lingual multi-cultural norm discovery\nfrom conversations on-the-fly. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\nTanmay Gupta and Aniruddha Kembhavi. 2022. Vi-\nsual programming: Compositional visual reasoning\nwithout training. arXiv preprint arXiv:2211.11559.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem\nsolving with the math dataset. Sort, 2(4):0–6.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring mathematical prob-\nlem solving with the math dataset. arXiv preprint\narXiv:2103.03874.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8460–8478.\nSha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li,\nXingyao Wang, Yi Fung, Charles Yu, Joel Tetreault,\nEduard Hovy, and Heng Ji. 2023. Defining a new\nnlp playground. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing Findings.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, et al. 2023. Taskmatrix. ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis. arXiv preprint arXiv:2303.16434.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\nand Ashwin Kalyan. 2022a. Dynamic prompt learn-\ning via policy gradient for semi-structured mathemat-\nical reasoning. arXiv preprint arXiv:2209.14610.\nPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and\nKai-Wei Chang. 2022b. A survey of deep learn-\ning for mathematical reasoning. arXiv preprint\narXiv:2212.10535.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, Edouard Grave, Yann LeCun, and\nThomas Scialom. 2023. Augmented language mod-\nels: a survey.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nOpenAI. 2022. Chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023. Tool learning with foundation\nmodels.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\n6931\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, et al. 2022.\nBlenderbot 3: a deployed conversational agent that\ncontinually learns to responsibly engage. arXiv\npreprint arXiv:2208.03188.\nDídac Surís, Sachit Menon, and Carl V ondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022. Interleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. arXiv\npreprint arXiv:2212.10509.\nXingyao Wang, Sha Li, and Heng Ji. 2022a.\nCode4structure: Code generation for few-shot struc-\nture prediction from natural language. In arxiv.\nXingyao Wang, Hao Peng, Reyhaneh Jabbarvand, and\nHeng Ji. 2023. Learning to generate from textual\ninteractions. In arxiv.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, et al. 2022b. Super-\nnaturalinstructions: Generalization via declarative\ninstructions on 1600+ nlp tasks. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 5085–5109.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. In Advances in\nNeural Information Processing Systems.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xi-\naodong Wang, Zecheng Tang, and Nan Duan.\n2023. Visual chatgpt: Talking, drawing and edit-\ning with visual foundation models. arXiv preprint\narXiv:2303.04671.\nPengfei Yu and Heng Ji. 2023. Self information up-\ndate for large language models through mitigating\nexposure bias. In arxiv.\nLifan Yuan, Yangyi Chen, Xingyao Wang, Yi R. Fung,\nHao Peng, and Heng Ji. 2023. Craft: Customiz-\ning llms by creating and retrieving from specialized\ntoolsets.\n6932\nAppendices\nA Details about Experiment\nA.1 Model Details.\nWe employ GPT-turbo-3.5 as the base model for all\nour experiments. The maximum generation length\nfor all experiments is set to 512, and a tempera-\nture of 0.3 is chosen to encourage deterministic\ngenerations while maintaining a certain degree of\ndiversity, particularly during the creation of tools.\nA.2 Dataset Details.\nFor both the MATH and TabMWP datasets, we\nevaluate questions that have numerical value an-\nswers (e.g. integers or decimals). This is due to\nthe complicated format matching problems (e.g.\nmatching of matrices as answers) that may cause\nbias. The tested questions cover approximately\n80% of all and maintain high diversity, making\nour results still representative. We are planning\nto update our results on all MATH questions ap-\nplying post-processing soon 2, but some matching\nproblems are still hard to solve.\nThe MATH dataset consists of seven math com-\npetition problem domains, namely algebra, count-\ning and probability, geometry, intermediate algebra,\nnumber theory, pre-algebra, and pre-calculus. Each\ndomain is evaluated separately, and the final metric\nis computed as the weighted average score. The\nTabMWP dataset includes a wide range of table\ninformation and problems of different difficulty\nlevels, spanning from grade one to grade eight.\nB Details about New Datasets\nB.1 Creation Challenge Details\nWe begin by constructing a seed dataset that in-\nvolves novel settings and unconventional reason-\ning processes. Subsequently, we utilize the Text-\nDavinci-003 model to expand the dataset in an iter-\native manner. By random sampling from the seed\ndata, we encourage the variety and novelty in the\nproblems and their reasonings.\nFigure 6 illustrates a sample query and its corre-\nsponding solution. Each data entry comprises the\nproblem statement, a standard created tool that can\nbe utilized (including utility, input, output, and real-\nization), a tool-calling decision, and a final answer.\n2https://github.com/openai/prm800k/blob/main/\nprm800k/grading/grader.py\n1 2 3 4 5 6 7 8 9 10\n5 12 23 42 75 122 187 272 379 510\ndef polynomial5(x, y, predict_day):\ncoefficients = np.polyfit(x, y, 5)\nterms = [coefficients[i] * predict_day ** (5 - i) \nfor i in range(6)]\npredict = sum(terms)\nreturn predict\nx = np.array([1,2,3,4,5,6,7,8,9,10])\ny = np.array([5,12,23,42,75,122,187,272,379,510])\n# Fit the data and do prediction\npredict = polynomial5(x, y, 11)\nProblem \nSuppose you are a scientist studying the spread of a virus. You have \ncollected data on the number of new cases reported each day over a 10-day \nperiod. You suspect that the number of cases might be modeled by a \npolynomial function of degree 5. You want to fit a polynomial function to \nthe data using the method of least squares and predict the number of new \ncases on the 11th day.\nSolution –Sample Tool\nSolution –Sample Decision\n----------------------------\nFigure 6: An example query and its solution provided\nin the Creation Challenge dataset.\nB.2 Tool Transfer Dataset Details\nWe create 300 data points in total and divide them\ninto sets of three. Each set of three queries contains\nthree corresponding answers, one standard tool that\ncould be applied in all three scenarios to solve the\nproblem, and three decisions about how to use the\ntool respectively. Similar to the construction of\nCreation Challenge, we manually write the seed\ndata, which includes five sets of queries used as\nexamples to show the format of each data point,\nsample demonstration examples from these seeds,\nand leverage the Text-Davinci-003 to create more\ndata iteratively.\nWe present a sample set from the tool transfer\ndataset we curate in Figure 7. In the set, three\ndifferent scenarios are provided, with each one con-\nsisting of a query, a sample decision, and an answer\n(not listed). Though the scenarios seem unrelated,\nthey share the same core knowledge which can be\ntransferred. In this case, the core knowledge is the\ncalculation of profit. We also provide an example\ntool that can be applied to all these three scenarios\nwith a corresponding introduction. Note that each\nset we define actually represents three data points.\nC More about Experimental Findings\nC.1 CoT Incompatible with Code\nIn this section, we will provide more cases to\nfurther illustrate our arguments made in Sec-\ntion 4.4 about the conflicts between natural lan-\nguage thoughts and program thoughts. We con-\n6933\n----------------------------\nScenario 1: Pricing Strategy\n- Query\nA company produces a product that has a fixed cost of $20,000, a variable \ncost of $10 per unit, and a demand of 10,000 units. The company wants to \nmaximize profit and is considering two pricing strategies. The first strategy \nis to sell the product at $30 per unit, and the second strategy is to sell the \nproduct at $35 per unit. What is the optimal pricing strategy for the \ncompany?\n- Sample Decision\ndef calculate_optimal_units(selling_price, variable_cost, \nfixed_cost, demand):\n# Calculate the profit for each quantity\nprofits = []\nfor quantity in range(1, demand+1):\nprofit = (selling_price * quantity) – (variable_cost * \nquantity) - fixed_cost\nprofits.append(profit)\n# Find the quantity that maximizes profit\noptimal_quantity = profits.index(max(profits)) + 1\n# Return the optimal quantity\nreturn optimal_quantity\nfixed_cost = 20000\nvariable_cost = 10\ndemand = 10000\n# Strategy 1: Selling the product at $30 per unit\nselling_price_1 = 30\noptimal_quantity_1 = calculate_optimal_units(selling_price_1, \nvariable_cost, fixed_cost, demand)\nprofit_1 = (selling_price_1 * optimal_quantity_1) -\n(variable_cost * optimal_quantity_1) - fixed_cost\n# Strategy 2: Selling the product at $35 per unit\nselling_price_2 = 35\n...\n# Determine the optimal pricing strategy\nif profit_1 > profit_2:\n...\nSample Tool (Common For 3 Scenarios)\nThe tool is used to calculate the optimal number of units to produce to \nmaximize profit for a manufacturing company. It takes into account the \nfixed costs, variable costs, selling price, and demand for the product. The \nfunction uses the formula Profit = (Selling Price * Quantity) - (Variable \nCost * Quantity) - Fixed Cost to calculate the profit and returns the optimal \nquantity to produce.\n----------------------------\nScenario 2: Production Planning\n- Query\nA manufacturing company produces a product that has a fixed cost of \n$10,000, a variable cost of $5 per unit, and a selling price of $20 per unit. \nThe company can sell up to 5,000 units of the product at this price. What is \nthe optimal number of units to produce to maximize profit?\n- Sample Decision\n…\n----------------------------\nScenario 3: Capacity Planning\n- Query\nA company produces a product that has a fixed cost of $50,000, a variable \ncost of $15 per unit, and a selling price of $25 per unit. The company has a \nproduction capacity of 10,000 units. What is the optimal number of units to \nproduce to maximize profit?\n- Sample Decision\n…\nFigure 7: An example data point in tool transfer dataset.\nWe provide three scenarios sharing the core knowledge\nand a sample tool that all three scenarios can utilize.\ntrast two additional cases sourced respectively from\nMATH and TabMWP in Figure 8.\nIn the case of MATH, the ambiguity of \"string\nmanipulation\" mentioned in natural language\nthoughts leads the model to create the tool that\nfinds the hundredth digit in a hard-coding manner,\nwhile pure code generation in creating tools can\navoid this problem.\nConversely, for TabMWP, CoT helps tool cre-\nation by avoiding unnecessary complexities in sim-\nple problem-solving. In the second case, the natural\nlanguage thoughts indicate clearly that only simple\nmultiplication should be done, while pure code gen-\neration is trapped in a complex and chaotic logic\nthat is prone to error.\nThese two cases further validate the conflicts\nbetween natural language thoughts and program\nthoughts, especially for challenging problems\nwhich may possess multiple reasoning paths that\ndiffer in suitability for code and natural language.\nC.2 Different Levels of Tool Creation\nWe present in this section more details about the\ndifferent levels of tool creation mentioned in Sec-\ntion 5.2. We present three cases in Figure 9.\nThe enhancement of existing tools in tool cre-\nation is presented in the first case. After the query,\nthe LLM is given an existing API that could be\ncalled for a fixed purpose. This mimics the scenario\nin the real world where an API document is given\nto let one fulfill a particular purpose. At this level,\nthe LLM learns how to create tools first by com-\nprehensively understanding the existing tool and\nthen transferring this knowledge to a new problem\nscenario. In this case, the LLM learns how temper-\nature should be averaged across several days, and\nsubsequently creates a tool that solves the problem.\nThe concatenation of multiple tools in tool cre-\nation is presented in the second case. In this case,\nthe LLM is given several tools to solve the problem,\nbut the usage of each tool is rather simple to fol-\nlow. This level of tool creation requires the LLM to\nplan the use of tools in a logical way and organize\nthem with clear logic. Instead of how to call tools\nto serve a different purpose, this level also illus-\ntrates the LLM’s excellent ability in implementing\na pipeline to solve specific queries through tool\ncreation.\nThe hierarchy of tool creation is presented in\nthe third case. This not only is the most common\nphenomenon that we observe in the experiment\nbut also represents the most advanced aspect of\nthe LLM’s reasoning potential. By creating tools\nwith a clear hierarchy, the LLM is successful in\noffloading more reasoning burdens and thus solving\nthe problem with higher accuracy. In this case,\nis_prime represents only a \"sub-tool\", while the\nmain tool solves the problem with more ease by\ncalling it to help count the valid numbers.\nOverall, the presented case studies provide valu-\nable insights into the tool creation abilities of\n6934\nProblem: What is the digit in the hundredths place of the decimal equivalent of 9/160? \nw/o CoT: w CoT:\nI can use division to solve this \nproblem. I should create a tool that \ncould calculate the decimal \nequivalent of a fraction and use \nstring manipulation to extract the \ndigit in the hundredths place.Answer: 5 Answer: 6\nProblem: Each drawer in the dresser has 10 blouses. How many blouses are in 4 drawers?\nTable: Number of drawers | Number of blouses: 1 | 10; 2 | 20;  3 | 30;  4 | ? \nw/o CoT: w CoT:\nTo solve the problem of finding \nout how many blouses are in 4 \ndrawers, we need to first \ncalculate the number of blouses \nin one drawer, which is 10. In the \ntool, we should multiply that by \nthe target number of drawers.\nAnswer: 160\nAnswer: 40\ndef decimal_digit(n, d, place):\ndecimal = n / d\ndigit = int(decimal*10**place) \\\n% 10\nreturn digit\ndef hundredths_place(n, d):\n# calculate the real value\ndecimal = n / d\nhundredths = str(decimal)[4]\nreturn int(hundredths)\ndef count_blouses(df, num_drawer):\n# get the target row\ndrawer_row = df[df['Number \\\nof drawers'] == num_drawer]\n# get the blouses for that row\nnum_blouse = drawer_row['Number \\\nof blouses'].values[0]\n# calculate the total number\ntotal = num_blouse * num_drawer\nreturn total\ndef calculate_blouses(num_drawer):\n# given the blouses in one drawer\nblouses_per_drawer = 10\n# multiply with number of drawers\ntotal_blouses = blouses_per_drawer \\\n* num_drawer\nreturn total_blouses\nMATH\nTabMWP\nFigure 8: We present two more cases to illustrate the conflicts between program thoughts and natural language\nthoughts.\nTool Creation:\nEnhancement of Existing Tool\ndef average_temperature(city, N):\n# calculate average temperature in next N days\ntotal_temperature = 0\nfor i in range(N):\n# call the tool for N times\nweather_data = ForecastWeather(city, i)\ntemperature = weather_data['temperature']\ntotal_temperature += temperature\nreturn total_temperature / N\nProblem: What’s the average temperature of ShangHai in \nthe next seven days?\nInput: You have access to the following tool\nForecastWeather(city:str, N:int) → dict\nThe tool will return the “UV index”(float) and “temperature” \n(float) in the given city after N days.\nTool Creation:\nConcatenation of Multiple Tools\ndef annulus_with_same_area(R, r):\n# calculate the area of two circles\narea1 = CalculateArea(R)\narea2 = CalculateArea(r)\n# calculate the area of annulus\nannulus = abs(area1 - area2)\nreturn CalculateR(annulus)\nProblem: What is the radii of a circle with same area as the \nannulus formed by two concentric circles with radii R and r?\nInput: R = 10 and r = 5. You are given two APIs:\nCalculateArea(r: float) → float\nCalculateR(area: float) → float\nTwo APIs calculate the area and radii of a circle respectively.\nTool Creation:\nTool Creation:\nHierarchy of Tool Creation\n if n % i == 0:\nreturn False\nreturn True\ndef count_targets(N):\ncount = 0\nfor i in range(1, N+1):\nfor j in range(5, i+1):\nif i % j == 0 and is_prime(j):\ncount += 1\nbreak\nreturn count\nProblem: How many positive integers less than or \nequal to N have a prime factor that is greater than 4?\nInput: N = 100.\nTool Creation:\ndef is_prime(n):\n# determines if a number is prime.\nif n <= 1:\nreturn False\nfor i in range(2, int(n**0.5)+1):\nFigure 9: We present three cases to illustrate the idea of the LLM’s tool creation from different levels.\nLLMs. However, it is important to acknowledge\nthat these studies offer only a glimpse into the vast\npotential of LLMs in this domain. We encourage\nfuture research to explore and harness the full ex-\ntent of LLMs’ tool creation capabilities, further\npushing the boundaries of what can be achieved.\n6935\nD Prompting Details\nAll the methods we present in our main experi-\nments need prompting to formalize the LLM’s re-\nsponse and better inspire its ability.\nPrompting of CREATOR . We present in Fig-\nures 14 to 16 the general prompting format and the\nformats of demonstrative examples, as detailed in\ndetails in 3. For the creation stage, decision stage,\nand rectification stage, we apply demonstration ex-\namples to enhance the LLM’s abstract and concrete\nreasoning ability, while the execution stage intrin-\nsically is unrelated to the LLM. We present one\ndemonstrative example about a query in MATH,\nbut other tasks including TabMWP and Creation\nChallenge also follow this prompting format.\nPrompting of Baselines. Besides CREATOR , we\nalso apply demonstrative examples in prompting\nthe ChatGPT’s CoT, PoT, Tool Use abilities respec-\ntively, presented in Figures 10 to 13. Similar to\nthe prompting of CREATOR , these prompt formats\napply to all tasks in the main experiments, includ-\ning evaluation on MATH, TabMWP, and Creation\nChallenge.\nSpecifically, We separate Tool Use into two parts,\nthe first one aiming to inspire the LLM’s ability to\ncall WolframAlpha properly, and the second one\naiming to prompt the LLM to retrieve the final\nanswer. For CREATOR setting, the prompts are\nseparated according to different stages. Note that\nthe execution stage does not need prompting.\n### Instruction\nYou are given a math question.\nYou should think step by step to solve the problem.\nPlease put your final numerical answer after “Final Answer:”\n### Question\nPoint $P$ lies on the line $x= -3$ and is 10 units from the point $(5,2)$. \nFind the product of all possible $y$-coordinates that satisfy the given \nconditions.\n### Thoughts and Answer\nLet the y-coordinate of point $P$ be denoted by $y$. Since $P$ lies on the \nline $x=-3$, we know that the coordinates of $P$ are $(-3,y)$.\nSince $P$ is 10 units from the point $(5,2)$, we can use the distance \nformula to write an equation:\n$\\sqrt{(-3-5)^2+(y-2)^2}=10$\nSimplifying the equation gives us:\n$(y-2)^2+64=100$\n$(y-2)^2=36$\n$y-2=\\pm6$\n$y=8$ or $y=-4$\nTherefore, the possible $y$-coordinates that satisfy the given conditions are \n$8$ and $-4$.\nThe product of these values is $8 \\times (-4) = -32$.\nFinal Answer: -32\n### Question\n{Demonstration Question}\n### Thoughts and Answer\n{Demonstration Thoughts and Answer}\n…\n### Question\n{Target Question}\n### Thoughts and Answer\n…\n(Expecting “Final Answer” in the last line)\nFigure 10: The instruction and one of the demonstration\nexamples we use when prompting ChatGPT in the CoT\nsetting.\n### Instruction\nYou are given a math question.\nYou should generate a piece of python code to solve the problem.\nPlease show your thoughts in python codes.\n### Question\nPoint $P$ lies on the line $x= -3$ and is 10 units from the point $(5,2)$. \nFind the product of all possible $y$-coordinates that satisfy the given \nconditions.\n### Solution Code\n### Question\n{Demonstration Question}\n### Solution Code\n{Demonstration Code}\n…\n### Question\n{Target Question}\n### Thoughts and Answer\n…\nimport math\n# initilize the coordinates\nx1 = -3\nx2 = 5\ny = 2\n# initialize the distance\nd = 10\n# calculate 2 y-coordinates using the Pythagorean Theorem\ny_possible1 = y + math.sqrt(d**2 - (x2 - x1)**2)\ny_possible2 = y - math.sqrt(d**2 - (x2 - x1)**2)\n# calculate the product\nproduct = y_possible1 * y_possible2\nprint(\"The product of all possible y-coordinates is\", product)\nFigure 11: The instruction and one of the demonstration\nexamples we use when prompting ChatGPT in the PoT\nsetting.\n6936\n### Instruction\nYou are given a math question.\nYou could call WolframAlpha API to help you solve the question.\nAfter seeing a question, you should first generate thoughts and think about \nhow to call the API.\nGenerate \"WOLFRAM:\" in the last line of your response with appropriate \ninputs you'd like to inquiry.\n### Question\nPoint $P$ lies on the line $x= -3$ and is 10 units from the point $(5,2)$. \nFind the product of all possible $y$-coordinates that satisfy the given \nconditions.\n### Response\nLet the y-coordinate of point $P$ be denoted by $y$.\nSince $P$ lies on the line $x=-3$, we know that the coordinates of $P$ are \n$(-3,y)$.\nSince $P$ is 10 units from the point $(5,2)$, we can use the distance \nformula to write an equation:\n$\\sqrt{(-3-5)^2+(y-2)^2}=10$\nWe can call wolframalpha to solve this equation\nWOLFRAM: solve sqrt((-3-5)^2+(y-2)^2)=10\n### Question\n{Demonstration Question}\n### Response\n{Demonstration Response}\n…\n### Question\n{Target Question}\n### Response\n…\n(Expecting “WOLFRAM” in the last line)\nFigure 12: The instruction and one of the demonstration\nexamples we use when prompting ChatGPT in the Tool\nUse setting. This figure shows the first part about Wol-\nframAlpha inquiry.\n### Instruction\nYou are given a math question.\nYou have just called WalframAlpha API to help you solve the question.\nPlease continue to generate your final numerical answer with the return \nfrom WalframAlpha API as reference.\nIf There is an error return from the API, you could continue you thought \nstep by step and give your final answer.\nGenerate \"Final Answer:\" in the last line of with your final numerical \nanswer.\n### Question\nPoint $P$ lies on the line $x= -3$ and is 10 units from the point $(5,2)$. \nFind the product of all possible $y$-coordinates that satisfy the given \nconditions.\n### Response\nLet the y-coordinate of point $P$ be denoted by $y$.\nSince $P$ lies on the line $x=-3$, we know that the coordinates of $P$ are \n$(-3,y)$.\nSince $P$ is 10 units from the point $(5,2)$, we can use the distance \nformula to write an equation:\n$\\sqrt{(-3-5)^2+(y-2)^2}=10$\nWe can call wolframalpha to solve this equation\nWOLFRAM: solve sqrt((-3-5)^2+(y-2)^2)=10\n### WolframAlpha Return\n[{'@title': 'Input interpretation', 'subpod': {'@title': '', 'plaintext': 'solve \nsqrt((-3 - 5)^2 + (y - 2)^2) = 10'}}, {'@title': 'Results', '@primary': 'true', \n'subpod': [{'@title': '', 'plaintext': 'y = -4'}, {'@title': '', 'plaintext': 'y = 8'}], ...]\n### Response Continue\nFrom the WolframAlpha return, we see that there are two possible values for \n$y$, namely $y=-4$ and $y=8$. Therefore, the product of all possible $y$-\ncoordinates is $-4 \\cdot 8 = -32$.\nFinal Answer: -32\n### Question\n{Demonstration Question}\n### Response\n{Demonstration Response Same with the First Part}\n### WolframAlpha Return\n{Demonstration Return Dictionary from WolframAlphaAPI}\n### Response Continue\n{Demonstration of Final Answer Extraction}\n…\n### Question\n{Target Question}\n### Response\n{LLM Response exactly in the First Part}\n### WolframAlpha Return\n{WolframAlpha Return from the First Part}\n### Response Continue\n…\n(Expecting “Final Answer” in the last line)\nFigure 13: The instruction and one of the demonstration\nexamples we use when prompting ChatGPT in the Tool\nUse setting. This figure shows the second part about\nanswer retrieving.\n6937\n### Instruction\nYou are asked to design python tools to help solve a question.\nYou can use math, scipy, numpy, sympy,... or other packages if necessary.\nYou should specify the parameters and returns of your tool and your tool’s \nutility in the documentation.\nYou could create more than one tool if you think they may all help solve the \nproblem.\n### Question\nPoint $P$ lies on the line $x= -3$ and is 10 units from the point $(5,2)$. \nFind the product of all possible $y$-coordinates that satisfy the given \nconditions.\n### Tools\n### Question\n{Demonstration Question}\n### Tools\n{Demonstration Tools Created}\n…\n### Question\n{Target Question}\n### Tools\n…\n(Expecting Python Code Block)\ndef distance_formula(x1, y1, x2, y2):\n\"\"\"\nCalculates the distance between (x1, y1) and (x2, y2).\n\"\"\"\nreturn ((x2-x1)**2 + (y2-y1)**2)**0.5\ndef possible_ys(x, dist, center):\n\"\"\"\nReturns a list of all possible y-coordinates of a point that \nis a distance of `dist` away from `center` and lies on the \nline x = `x`.\n\"\"\"\ny_diff = dist**2 - (center[0]-x)**2\nif y_diff < 0:\nreturn []\nelif y_diff == 0:\nreturn [center[1]]\nelse:\nreturn [center[1] + y_diff**0.5, center[1] - y_diff**0.5]\nFigure 14: The instruction and one of the demonstration\nexamples we use when prompting ChatGPT in the CRE-\nATOR setting. This figure shows the prompts applied in\nthe Creation stage.\n### Instruction\nYou are given a math question, and the related tools that may help you solve \nthe question.\nNow you need to write python code to call the tool to solve the problem.\nRead carefully about the tool's parameters, returns and how to use the tool.\nPlease print out your answer and any information you think it’s important.\n### Question\nPoint $P$ lies on the line $x= -3$ and is 10 units from the point $(5,2)$. \nFind the product of all possible $y$-coordinates that satisfy the given \nconditions.\n### Tool\n### Solution\n### Question\n{Demonstration Question}\n### Tool\n{Demonstration of Tools Created in Creation Stage}\n### Solution\n{Demonstration Solution Code to Print the Answer}\n…\n### Question\n{Target Question}\n### Tool\n{The Tools Generated in Creation Stage}\n### Solution\n…\n(Expecting Python Code Block)\ndef distance_formula(x1, y1, x2, y2):\n\"\"\"\nCalculates the distance between (x1, y1) and (x2, y2).\n\"\"\"\nreturn ((x2-x1)**2 + (y2-y1)**2)**0.5\ndef possible_ys(x, dist, center):\n\"\"\"\nReturns a list of all possible y-coordinates of a point that \nis a distance of `dist` away from `center` and lies on the \nline x = `x`.\n\"\"\"\ny_diff = dist**2 - (center[0]-x)**2\nif y_diff < 0:\nreturn []\nelif y_diff == 0:\nreturn [center[1]]\nelse:\nreturn [center[1] + y_diff**0.5, center[1] - y_diff**0.5]\n# Given point, distance and center\nx = -3\ndistance = 10\ncenter = (5,2)\n# Call the function which will return a list of y coordinates\nys = possible_ys(x=x, dist=distance, center=center)\n# Calculate product of all possible y-coordinates\nproduct = 1\n# Check for the other possible y-coordinate\nfor y in ys:\nproduct *= y\n# Print the answer\nprint(\"The product of all possible y-coordinates is\", product)\nFigure 15: The instruction and one of the demonstration\nexamples we use when prompting ChatGPT in the CRE-\nATOR setting. This figure shows the prompts applied in\nthe Decision stage.\n6938\n### Instruction\nYour original codes have met some errors when executing, please rectify \nyour codes.\nYou should generate the whole new piece of code (including the tools and \nthe decision call), instead of generating a snippet.\nWrap your code in ```python\\n ... \\n``` to make it a one whole code block.\nPay attention to the conditions in question before modifying.\nYour goal is answer the question correctly instead of just execute the code \nsuccessfully.\nIf necessary, you can also generate a whole new tool, changing your way of \nthinking to solve the problem.\n### Question\nPlease calculate the square root of a and b to get c, where a = 4 and b = 9.\n### Original\n### Error\nTraceback (most recent call last):\nFile \"test_code.py\", line 9, in <module>\nc = calculate_c(a, b)\nFile \"test_code0.py\", line 5, in calculate_c\nc = math.sqrt(a * b)\nNameError: name 'math' is not defined\n### Rectification\nThe math is not defined, so I should add the math module before the tool.\nIn detail, I should add `import math` at the very beginning.\n### Question\n{Demonstration Question}\n### Original\n{Demonstration of Tools and Decisions in Previous Stages}\n### Error \n{Demonstration Error Traceback from Execution Stage}\n### Rectification\n{Demonstration Rectified Tools / Decision with Thoughts}\n…\n### Question\n{Target Question}\n### Original\n{Tools and Decisions Made by LLM in Previous Stages}\n### Error\n{Traceback from Execution Stage}\n### Rectification\n…\ndef calculate_c(a, b):\n\"\"\"\nThe function calculate c, which is the square root of a \nmultiply b\n\"\"\"\nc = math.sqrt(a * b)\nreturn c\na = 4\nb = 9\nc = calculate_c(a, b)\nprint(\"The value of c is\", c)\n# Modify here to import at the beginning\nimport math\ndef calculate_c(a, b):\n\"\"\"\nThe function calculate c, which is the aquare root of a \nmultiply b\n\"\"\"\nc = math.sqrt(a * b)\na = 4\nb = 9\nc = calculate_c(a, b)\nprint(\"The value of c is\", c)\nFigure 16: The instruction and one of the demonstration\nexamples we use when prompting ChatGPT in the CRE-\nATOR setting. This figure shows the prompts applied in\nthe Rectification stage.\n6939",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6140762567520142
    },
    {
      "name": "Realization (probability)",
      "score": 0.6120350360870361
    },
    {
      "name": "Frontier",
      "score": 0.5224398970603943
    },
    {
      "name": "Documentation",
      "score": 0.4909811317920685
    },
    {
      "name": "Competition (biology)",
      "score": 0.4567509889602661
    },
    {
      "name": "Code (set theory)",
      "score": 0.4246934652328491
    },
    {
      "name": "Software engineering",
      "score": 0.3789157271385193
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3488416075706482
    },
    {
      "name": "Data science",
      "score": 0.33382993936538696
    },
    {
      "name": "Programming language",
      "score": 0.30090105533599854
    },
    {
      "name": "Political science",
      "score": 0.086428701877594
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ]
}