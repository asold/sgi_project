{
  "title": "Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media",
  "url": "https://openalex.org/W4393146386",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4202153622",
      "name": "Liam Hebert",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2185362157",
      "name": "Gaurav Sahu",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2518401548",
      "name": "Yuxuan Guo",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2921688831",
      "name": "Nanda Kishore Sreenivas",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2618045780",
      "name": "Lukasz Golab",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2000926835",
      "name": "Robin Cohen",
      "affiliations": [
        "University of Waterloo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6849577006",
    "https://openalex.org/W3093699284",
    "https://openalex.org/W4304087063",
    "https://openalex.org/W6785797479",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2785615365",
    "https://openalex.org/W3164419101",
    "https://openalex.org/W4318347863",
    "https://openalex.org/W4315881238",
    "https://openalex.org/W2886509668",
    "https://openalex.org/W3023989664",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3099161326",
    "https://openalex.org/W4285304773",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4285410679",
    "https://openalex.org/W3115453555",
    "https://openalex.org/W4306815344",
    "https://openalex.org/W6797613833",
    "https://openalex.org/W3096959275",
    "https://openalex.org/W2971050273",
    "https://openalex.org/W3159049084",
    "https://openalex.org/W4283799299",
    "https://openalex.org/W4304014444",
    "https://openalex.org/W2923321784",
    "https://openalex.org/W6796410304",
    "https://openalex.org/W3128611684",
    "https://openalex.org/W4281681212",
    "https://openalex.org/W3186178939",
    "https://openalex.org/W2916719435",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W3107176549",
    "https://openalex.org/W3216042640",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W4297816851",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4287186221",
    "https://openalex.org/W3174164406",
    "https://openalex.org/W4304080844",
    "https://openalex.org/W3176580738",
    "https://openalex.org/W3168261013",
    "https://openalex.org/W4385573042",
    "https://openalex.org/W4366966833",
    "https://openalex.org/W3173380736",
    "https://openalex.org/W3174906557"
  ],
  "abstract": "We present the Multi-Modal Discussion Transformer (mDT), a novel method for detecting hate speech on online social networks such as Reddit discussions. In contrast to traditional comment-only methods, our approach to labelling a comment as hate speech involves a holistic analysis of text and images grounded in the discussion context. This is done by leveraging graph transformers to capture the contextual relationships in the discussion surrounding a comment and grounding the interwoven fusion layers that combine text and image embeddings instead of processing modalities separately. To evaluate our work, we present a new dataset, HatefulDiscussions, comprising complete multi-modal discussions from multiple online communities on Reddit. We compare the performance of our model to baselines that only process individual comments and conduct extensive ablation studies.",
  "full_text": "Multi-Modal Discussion Transformer: Integrating Text, Images and Graph\nTransformers to Detect Hate Speech on Social Media\nLiam Hebert, Gaurav Sahu, Yuxuan Guo, Nanda Kishore Sreenivas,\nLukasz Golab, Robin Cohen\nUniversity of Waterloo\nliam.hebert@uwaterloo.ca\nAbstract\nWe present the Multi-Modal Discussion Transformer (mDT),\na novel method for detecting hate speech on online social net-\nworks such as Reddit discussions. In contrast to traditional\ncomment-only methods, our approach to labelling a comment\nas hate speech involves a holistic analysis of text and images\ngrounded in the discussion context. This is done by leverag-\ning graph transformers to capture the contextual relationships\nin the discussion surrounding a comment and grounding the\ninterwoven fusion layers that combine text and image embed-\ndings instead of processing modalities separately. To evalu-\nate our work, we present a new dataset, HatefulDiscussions,\ncomprising complete multi-modal discussions from multiple\nonline communities on Reddit. We compare the performance\nof our model to baselines that only process individual com-\nments and conduct extensive ablation studies.\nIntroduction\nSocial media have democratized public discourse, en-\nabling users worldwide to freely express their opinions and\nthoughts. As of 2023, the social media giant Meta has\nreached 3 billion daily active users across its platforms\n(Meta 2023). While this level of connectivity and access to\ninformation is undeniably beneficial, it has also resulted in\nthe alarming rise of hate speech (Das et al. 2020). This per-\nvasive spread of hateful rhetoric has caused significant emo-\ntional harm to its targets (Vedeler, Olsen, and Eriksen 2019),\ntriggered social divisions and polarization (Waller and An-\nderson 2021), and has caused substantial harm to the mental\nhealth of users (Wachs, G ´amez-Guadix, and Wright 2022).\nThere is an urgent need for a comprehensive solution to au-\ntomate identifying hate speech as a critical first step toward\ncombatting this alarming practice.\nInitially, automated hate speech detection models were\nlimited to text-only approaches such as HateXplain (Mathew\net al. 2021), which classify the text of individual comments.\nSuch methods have two significant weaknesses. First, social\nmedia comments have evolved to include images, which can\ninfluence the context of the accompanying text. For instance,\na comment may be innocuous, but including an image may\ntransform it into a hateful remark. Second, hate speech is\ncontextual. Social media comments are often conversational\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nand are influenced by other comments within the discussion\nthread. For example, a seemingly innocuous comment such\nas “That’s gross!” can become hateful in a discussion about\nimmigration or minority issues.\nOngoing research to address these weaknesses includes\nmulti-modal transformers such as VilT (Kim, Son, and Kim\n2021) that combine images and text for a richer representa-\ntion of comments. Still, they do not account for the contex-\ntual nature of hate speech. Hebert, Golab, and Cohen (2022)\nmodel discussion context with graph neural networks, but\nthey do not discuss how to integrate the interpretation of im-\nages within hateful social media discussions. Furthermore,\nthe sequential nature of the proposed architecture prevents\ntext embeddings from being grounded to other comments in\na graph. The initial semantic content encoded by a comment\nembedding may differ when considered with different sets\nof comments versus in isolation.\nTo overcome the limitations of existing methods, we pro-\npose the Multi-Modal Discussion Transformer (mDT) to\nholistically encode comments with multi-model discussion\ncontext for hate speech detection. To evaluate our work, we\nalso present a novel dataset, HatefulDiscussions, containing\ncomplete multi-modal discussion graphs from various Red-\ndit communities and a diverse range of hateful behaviour.\nWe compare mDT against comment-only and graph meth-\nods and conduct an ablation study on the various compo-\nnents of our architecture. We then conclude by discussing\nthe potential for our model to deliver social value in online\ncontexts by effectively identifying and combating anti-social\nbehaviour in online communities. We also propose future\nwork towards more advanced multi-modal solutions that can\nbetter capture the nuanced nature of online behaviour.\nTo summarize our contributions: 1) We propose a novel\nfusion mechanism as the core of mDT that interweaves\nmulti-modal fusion layers with graph transformer layers, al-\nlowing for multi-modal comment representations actively\ngrounded in the discussion context. 2) We propose a novel\ngraph structure encoding specific to the conversational struc-\nture of social media discussions.3) We introduce a dataset of\n8266 annotated discussions, totalling 18359 labelled com-\nments, with complete discussion trees and images to eval-\nuate the effectiveness of mDT. Our work focuses on Red-\ndit, which consists of branching tree discussions. Our code-\nbase, datasets and further supplemental can be found at\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22096\nFrozen\nBert\nLayers\n[CLS]\nFound\nYour\nFriends\nFrozen\nViT\nLayers\nt0\nt1\nt2\nt3\nb0\nb1\ni0\ni1\ni2\ni3\nBert\nLayers\nViT\nLayers\nb1\nb0 Graph\nTransformer\nt0\nt1\nt2\nt3\nb0\nb1\ni0\ni1\ni2\ni3\nMLP\nMLP\nHate/Not\nHate\nInitial Layers Graph Multi-Modal Fusion Layers\nx K x (N - K) / Z\nx Z\nN = Total Modality Layers (16)\nK = Frozen Pre-Fusion Layers\nZ = Modality Fusion Layer Size\nViT\nLayers\nBert\nLayers\nFigure 1: Multi-Modal Discussion Transformer\ngithub.com/liamhebert/MultiModalDiscussionTransformer.\nRelated Work\nTransformer-based encoding models such as BERT have\nsignificantly improved natural language processing due to\ntheir ability to capture textual semantics (Devlin et al. 2019).\nInspired by these developments, methods such as HateX-\nplain (Mathew et al. 2021) and HateBERT (Caselli et al.\n2021) have been introduced to discern hateful comments on\nsocial platforms, focusing on text alone. The effectiveness of\nthese efforts is intrinsically tied to the diversity of datasets\nthey are trained on. For instance, HateXplain utilized a spe-\ncialized dataset from diverse social platforms like Twitter\nand Gab, emphasizing interpretable hate speech detection.\nOther noteworthy datasets include Gong et al. (2021), study-\ning heterogeneous hate speech (comments containing mixed\nabusive and non-abusive language), and Founta et al. (2018),\nwhich crowdsourced annotation of Twitter abusive content.\nFinally, Zampieri et al. (2019) collected hateful Twitter posts\ncollated through a collaborative semi-supervised approach.\nWhile text is essential, images also contribute to the se-\nmantic context. CLIP introduced an approach to align text\nand image representations via contrastive pre-training (Rad-\nford et al. 2021). ViLBERT (Lu et al. 2019) conceptualized\ndistinct transformers for each modality—images and text,\nwhich are then amalgamated through co-attentional trans-\nformer layers. Subsequent works such as VilT (Kim, Son,\nand Kim 2021) and Nagrani et al. (2021) have devised novel\ninter-modality fusion mechanisms, unifying both modality\ntransformers into one. This integration of multi-modal lan-\nguage grounding has also enriched hate speech detection,\nas evidenced by the HatefulMemes challenge (Kiela et al.\n2020). Additional works such as Liang et al. (2022) em-\nploy graph convolutional networks to merge text and images,\nprimarily for sarcasm detection. Meanwhile, Sahu, Cohen,\nand Vechtomova (2021) leverage generative adversarial net-\nworks to encode these modalities, facilitating congruent rep-\nresentations of comments. Cao et al. (2022) pursue a unique\nstrategy by mapping the paired image to text descriptors, ap-\npending the comment text, and then predicting with a gen-\nerative language model. Finally, (Singh et al. 2022) incor-\nporate image and text representations of product reviews to\naccurately disambiguate complaints.\nDespite the progress, many of these techniques overlook\na vital modality: the context of discussions. The prevailing\nemphasis remains on datasets and techniques that analyze\nsingular comments, bypassing the contextual significance of\nthe prior discussion. By extending Graphormer (Ying et al.\n2022)—a graph transformer network tailored for molecular\nmodelling—Hebert, Golab, and Cohen (2022) consolidate\nlearned comment representations to predict the trajectory of\nhateful discussions. However, this work has limitations; it\nneglects the influence of images and, owing to the absence\nof complete discussion-focused hate speech datasets, resorts\nto approximating ground truth labels using a ready-made ex-\nternal classifier. Our work addresses both limitations, includ-\ning interleaving comment and discussion layers and human\nground truth data.\nMethodology\nMulti-Modal Discussion Transformer (mDT)\nThe mDT architecture consists of Initial Pre-Fusion, Modal-\nity Fusion, and Graph Transformer (Figure 1). The descrip-\ntion below outlines the holistic nature of our solution.\nInitial Pre-Fusion Given a discussion D with comments\nc ∈ D, each represented with text tc and optional image ic,\nwe start with pre-trained BERT and ViT models to encode\ntext and images, respectively. Both models containN layers\nwith the same hidden dimension of d. In our experiments,\nwe utilized BERT-base and ViT-base, which have N = 16\nlayers and d = 768hidden dimensions. Given these models,\nthe Initial Pre-Fusion step consists of the first K layers of\nboth models with gradients disabled (frozen), denoted as\ntk\nc = BERTinit(tc), ik\nc = V iTinit(ic) (1)\nwhere K < N. This step encodes a foundational under-\nstanding of the images and text that make up each comment.\nModality Fusion After creating initial text and image em-\nbeddings tc, ic for all comments c ∈ D in the discussion,\nwe move to the Modality Fusion step. We adopt the bottle-\nneck mechanism proposed by Nagrani et al. (2021) to en-\ncode inter-modality information. We concatenate b shared\nmodality bottleneck tokens B ∈ Rb×d to tc and ic, trans-\nforming the input sequence to [tk\nc || B], [ik\nc || B]. We then\ndefine a modality fusion layer l as\n[tl+1\nc ||Bl+1\nt,c ] =BERTl([tl\nc||Bl\nc]) (2)\n[il+1\nc ||Bl+1\ni,c ] =V iTl([il\nc||Bl\nc]) (3)\nBl+1\nc = Avg(Bl+1\nt,c , Bl+1\ni,c ) (4)\nwhere both modalities only share information through the\nB bottleneck tokens. This design forces both modalities to\ncompress information to a limited set of tokens, improving\nperformance and efficiency. If no images are attached to a\ncomment, then Bl+1\nc = Bl+1\nt .\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22097\nGraph Transformer Then, after Z (< (N−K)) modality\nfusion layers, we deploy Graph Transformer layers to aggre-\ngate contextual information from the other comments in the\ndiscussion1. Given that the tokens in Bc encode rich inter-\nmodality information, we innovate by leveraging these rep-\nresentations to represent the nodes in our discussion graph.\nUsing b0\nc ∈ Bc to represent each comment c ∈ D, we ag-\ngregate each embedding using a transformer model to incor-\nporate discussion context from other comments. Our novel\nutilization of bottleneck tokens to represent graph nodes al-\nlows modality models to maintain a modality-specific pooler\ntoken ([CLS]) as well as a graph context representation (b0).\nSince transformer layers are position-independent, we in-\nclude two learned structure encodings. The first is Centrality\nEncoding, denoted z, which encodes the degree of nodes in\nthe graph (Ying et al. 2022). Since social media discussion\ngraphs are directed, the degree of comments is equivalent to\nthe number of replies a comment receives plus one for the\nparent node. We implement this mechanism as\nh(0)\nc = b0\nc + zdeg(c) (5)\nwhere h(0)\nc is the initial embedding of b0\nc in the graph and\nzdeg(c) is a learned embedding corresponding to the degree\ndeg(c) of the comment.\nThe second structure encoding is Spatial Encoding, de-\nnoted s(c,v), which encodes the graph’s structural relation-\nship between two nodes, c and v. This encoding is added as\nan attention bias term during the self-attention mechanism.\nThat is, we compute the self attention A(c,v) between nodes\nc, vas\nA(c,v) = (hc × WQ)(hv × WK)\n√\nd\n+ s(c,v) (6)\nwhere WQ and WK are learned weight matrices and d is the\nhidden dimension of h.\nIn previous graph transformer networks, s(c,v) is encoded\nas a learned embedding representing the shortest distance\nbetween nodes c and v in the graph (Ying et al. 2022; Hebert,\nGolab, and Cohen 2022). However, this metric does not\nlend itself well to the hierarchical structure of discussions,\nwhere equivalent distances can represent different interac-\ntions. This is best seen in the example discussion illustrated\nin Figure 2. When utilizing the shortest distance to encode\nstructure, the distance between nodes a and c is the same\nas the distance between nodes b and d in this graph. How-\never, b and d represent direct replies to the same parent post,\nwhereas a is two comments underneath c.\nWe propose a novel hierarchical spatial encoding based on\nCantor’s pairing function to account for this. Cantor’s pair-\ning function uniquely maps sets of two numbers into a single\nnumber N × N → N. Given comments a and b, we first cal-\nculate the number of hops upwardu(a,b) and hops downward\nd(a,b) to reach b from a. In the example above, the distance\n1Our implementation can handle discussions up to 516 com-\nments, as we only require a single graph transformer pass to eval-\nuate all comments. The above limit can be exceeded via efficient\nattention mechanisms such as sparse or flash attention.\nDist: 2b d\ncDist: 2\na\nFigure 2: Example Discussion Structure. Each node in the\ndiscussion tree represents a comment. The shortest distance\nbetween (a, c) and (b, d) is equivalent, demonstrating a lack\nof expressiveness towards hierarchy.\nbetween a and d is u(a,b) = 2, d(a,b) = 1. We then com-\npress both numbers into a single index using the proposed\nposition-independent variant of Cantor’s pairing:\ns(c,v) = s(v,c) (7)\n= Cantors(u, d) (8)\n= (u + d)(u + d + 1)\n2 + min(u, d) (9)\nThis uniquely maps N × N → N such that sc,v = sv,c. We\nutilize this function to index learned spatial embeddings in\nthe self-attention mechanism.\nAfter G graph transformer layers, the final representation\nof hG\nc replaces b0\nc for the next set of Z modality fusion lay-\ners. We denote the combination of Z Modality Fusion and\nG Graph Transformer layers as a Graph Multi-Modal Fu-\nsion module. Finally, after (N − K)/Z Graph Multi-Modal\nFusion modules, we predict logits using the final embedding\nof b0\nc and the [CLS] embedding of tc. This novel interweav-\ning of graph transformer and fusion layers through modality\nbottleneck tokens ensures that fusion models create repre-\nsentations grounded in the discussion context. Notably, this\ndiffers from previous approaches that utilize graph neural\nnetworks, which sequentially process individual comments\nbefore applying a set of graph layers.\nHatefulDiscussions Dataset\nTo train our model, we require a complete multi-modal\ndiscussion graph dataset. However, datasets used by other\nworks (Mathew et al. 2021; Zampieri et al. 2019; Kiela\net al. 2020) consist of individual labelled comments and are\npredominately text-only. To address this issue, we curated\na novel benchmark comprising multiple datasets that used\nhuman annotators, which we augmented to include com-\nplete multi-modal discussion graphs. Our final dataset com-\nprises 8266 Reddit discussions with 18359 labelled com-\nments from 850 communities. Note that our architecture can\nextend to other platforms, such as Facebook, Twitter and\nTikTok, as they also discuss in tree structures. Since discus-\nsions on these platforms are typically smaller with less com-\nplexity, using Reddit allows us to best stress-test our model.\nThe first type of hate speech included in our benchmark\nis Identity-Directed and Affiliation-Directed Abuse. We re-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22098\nLabel Count\nDerogatory Slur 4297\nNot Derogatory Slur (NDG) 2401\nHomonym (HOM) 364\nLTI Person Directed Neutral 4116\nLTI Person Directed Hate 1313\nCAD Affiliation Neutral 4892\nCAD Identity Directed Hate 701\nCAD Affiliation Directed Hate 275\nNeutral 11773\nHateful 6586\nTable 1: Label Distribution of Hateful Discussions\ntrieved labelled examples of this from the Contextual Abuse\nDataset (CAD) developed by Vidgen et al. (2021a). Accord-\ning to the authors, Identity-Directed abuse refers to content\ncontaining negative statements against a social category, en-\ncompassing fundamental aspects of individuals’ community\nand socio-demographics, such as religion, race, ethnicity,\nand sexuality, among others. On the other hand, Affiliation-\nDirected abuse is defined as content expressing negativity\ntoward an affiliation, which is described as a voluntary as-\nsociation with a collective, such as political affiliation and\noccupations (Vidgen et al. 2021a). We selected both of these\nforms of abuse from CAD due to the similarity in their defi-\nnitions—abuse that is directed at aspects of a person’s iden-\ntity rather than a specific individual directly.\nNext, slurs form the second type of hateful content within\nour dataset, sampled from the Slurs corpus (Kurrek, Saleem,\nand Ruths 2020). Notably, historically derogatory slurs can\nundergo re-appropriation by specific communities, such as\nthe n-slur in African American Vernacular, transforming\nthem into non-derogatory terms. Therefore, we hypothesize\nthat understanding the contextual nuances surrounding the\nuse of slurs becomes essential in distinguishing between\nnon-derogatory and derogatory instances.\nThe last type of hateful content we include is person-\ndirected abuse, hate speech or offensive content that specif-\nically targets and attacks an individual or a group of indi-\nviduals. We source labelled examples from the Learning to\nIntervene (LTI) dataset by Qian et al. (2019) to include ex-\namples of this abuse requiring context.\nFor each labelled comment, we retrieved the correspond-\ning complete discussion tree using the Pushshift Reddit\nAPI and downloaded all associated images 2. To refine our\ndataset, we filtered out conversations without images and\nconstrained comments to have a maximum degree of three\nand conversations to have a maximum depth of five. By trim-\nming the size of the discussion tree, we reduce computa-\ntional complexity and focus on the most relevant parts of\nthe conversation (Parmentier et al. 2021). We map each re-\ntrieved label to Hateful or Normal and treat the problem as\n2At the time of writing, Reddit has suspended access to the\nPushshift API; however, our dataset remains complete.\nHyperparameter Value\nPre-Fusion Layers (K) 4 - [0, 2, 4, 6]\nModality Fusion Layer Stack (Z) 2 (8 total) - [1, 2, 4]\nGraph Layer Stack (G) 2 (8 total) - [1, 2, 4]\nBottleneck Size (B) 4 - [1, 4, 8, 16, 32]\nMax Spatial Attention 5 - [2, 5, 4096 (inf)]\nAttention Dropout 0.3 - [0, 0.1, 0.3, 0.5]\nActivation Dropout 0.3 - [0, 0.1, 0.3, 0.5]\nGraph Dropout 0.4 - [0, 0.2, 0.4, 0.6]\nHidden Dimension (d) 768\nGraph Attention Heads 12 - [4, 6, 12, 24]\nModality Attention Heads 12\nOptimizer Adam\nBatch Size 48\nEpochs 10 w/ Early Stopping\nLearning Rate 3e−5 → 3e−7\nLearning Rate Scheduler Polynomial Decay\nWarm up Updates 500\nTotal Updates 3350\nPositive Class Weighting 1.5 - [1, 1.5]\nNegative Class Weight 1\nFreeze Initial Encoders Yes - [Yes, No]\nTable 2: mDT Model Hyperparameters. The search space for\neach parameter is denoted by [...]\na binary classification. The distribution of each label can be\nseen in Table 1.\nMost images in HatefulDiscussions, such as the root post,\nappear in the discussion context rather than directly attached\nto labelled comments. In our case, only 424 labelled in-\nstances have an image attached. Still, all 8000 discussions\nhave an image in the prior context. Therefore, the challenge\nbecomes how to interpret and incorporate multi-modal dis-\ncussion context to disambiguate the meaning of comments\nthat may not contain those modalities.\nResults\nExperimental Setup\nWe conduct a 7-fold stratified cross-validation with a fixed\nseed (1) and report the average performance for each model.\nWe report overall accuracy (Acc.) and class-weighted Preci-\nsion (Pre.), Recall (Rec.) and F1 to account for label imbal-\nance. Underlined values denote statistical significance using\nStudent’s t-test with p-value < 0.05 in all results. Our hy-\nperparameter search space can be seen in Table 2. All exper-\niments were run using 2xA100-40GB GPUs, a 12-core Intel\nCPU, 80GB of RAM, and Linux.\nText-only Methods vs. Discussion Transformers\nTo assess the performance of mDT, we compared it against\nseveral state-of-the-art hate speech detection methods. For\ncomment-only approaches, we evaluated BERT-HateXplain\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22099\nMethod Acc. Pre. Rec. F1\nBERT-HateXplain 0.742 0.763 0.742 0.747\nDetoxify 0.687 0.679 0.696 0.677\nRoBertA Dynabench 0.811 0.822 0.811 0.814\nBERT-HatefulDiscuss 0.858 0.858 0.858 0.858\nGraphormer 0.735 0.594 0.759 0.667\nmDT (ours) 0.880\n0.880 0.880 0.877\nTable 3: Performance of mDT against Text-Only Methods\nBottleneck Size Acc. Pre. Rec. F1\n4 0.880 0.880 0.880 0.877\n8 0.863 0.864 0.863 0.863\n16 0.864 0.850 0.853 0.852\n32 0.874 0.872 0.874 0.872\nTable 4: Effect of Bottleneck Size on mDT Performance\n(Mathew et al. 2021), Detoxify (Hanu and Unitary team\n2020), and RoBertA Dynabench (Vidgen et al. 2021b). We\nalso compared mDT against a BERT model trained on the\ntraining set of HatefulDiscussions, referred to as BERT-\nHatefulDiscuss. To compare against previous graph-based\napproaches, we evaluated the text-only Graphormer model\nproposed by (Hebert, Golab, and Cohen 2022).\nOur results (Table 3) show that mDT outperforms all\nevaluated methods across all metrics. Specifically, mDT\nachieves 14.5% higher accuracy and 21% higher F1 score\nthan Graphormer. This indicates that our approach to includ-\ning graph context significantly improved over the previous\napproach incorporating this modality. Although the perfor-\nmance gap between BERT-HatefulDiscussions and mDT is\nnarrower, we still perform better against all text-only meth-\nods. We observed F1 score improvements of 20%, 13%, and\n6.3% over Detoxify, BERT-HateXplain, and RoBertA Dyn-\nabench, respectively.\nEffect of Bottleneck Size\nNext, we investigated the impact of increasing the number\nof bottleneck interaction tokens (B ) in mDT, added during\nthe modality fusion step. Adding more bottleneck tokens re-\nduces the amount of compression required by the BERT and\nViT models to exchange information. Table 4 presents the\nresults, where we find that using four bottleneck tokens leads\nto the best performance. We also observe a slight drop in\nperformance when we increase the number of bottleneck to-\nkens beyond four, indicating the importance of compression\nwhen exchanging modality encodings between models.\nEffect of Constrained Graph Attention\nA recent study by Hebert et al. explored the limitations of\ngraph transformers for hate speech prediction, finding that\ndiscussion context can sometimes mislead graph models into\nmaking incorrect predictions (Hebert et al. 2023). In light\nof this, we explore the impact of constraining the attention\nmechanism of our graph transformer network to only attend\nAttention Window Acc. Pre. Rec. F1\n2 0.866 0.866 0.866 0.866\n5 0.880\n0.880 0.880 0.877\n∞ 0.870 0.861 0.850 0.855\nTable 5: Effect of Constraining Graph Attention\nFusion Layers Acc. Pre. Rec. F1\n6 0.868 0.856 0.854 0.855\n8 0.872 0.871 0.844 0.855\n10 0.866 0.867 0.866 0.862\n12 0.880\n0.880 0.880 0.877\nTable 6: Effect of Fusion Layers\nto nodes within a maximum number of hops away from a\nsource node. We report the results in Table 5 and find that\nconstraining the attention window to 5 hops achieves better\nperformance. However, we also observed that performance\ngains from the 5-hop constraint were lost when we further\nconstrained the attention to only two hops. Our findings sug-\ngest a balance is required when constraining graph attention\nfor optimal performance.\nEffect of Fusion Layers\nNext, we investigate the effect of increasing the number of\nMulti-Modal Fusion Layers (Z ) in our mDT model. To en-\nsure full utilization of the 16 available layers, any unused\nlayers were allocated to the Initial Pre-Fusion step (K). Our\nresults in Table 6 indicate that utilizing 12 fusion layers leads\nto the best performance. Interestingly, the performance gains\ndid not follow a linear trend with the number of fusion lay-\ners. Specifically, we observed that eight fusion layers out-\nperformed ten but were still inferior to 12. Further research\nshould explore the potential benefits of scaling beyond 12\nfusion layers using larger modality models.\nEffect of Images\nWe also investigated the impact of removing images in mDT.\nOur findings (Table 7) support the hypothesis that images\nprovide crucial contextual information for detecting hateful\ncontent: excluding images led to a 4.8% decrease in accu-\nracy and a 4.9% decrease in the F1 score. It is worth noting\nthat even without images, mDT outperformed Graphormer\n(Table 3), indicating that our approach provides substantial\ngains over previous graph-based methods for hate speech de-\ntection beyond just including images. The results of this ex-\nperiment underscore the importance of considering multiple\nUsage of Images Acc. Pre. Rec. F1\nWith Images 0.880 0.880 0.880 0.877\nWithout Images 0.832 0.835 0.822 0.828\nTable 7: Effect of Excluding Images\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22100\nFigure 3: Fine-grained distribution of BERT and mDT mis-\nclassification. (Acronyms above as in Table 1)\nFigure 4: An image present in the discussion context of ex-\nample 3 (Table 8), seen only by mDT, contextualizing com-\nments as potentially hateful\nmodalities for hate speech detection and suggest that future\nresearch should explore further improvements by leveraging\nadditional types of contextual information.\nQualitative Analysis: BERT vs. mDT\nWe next perform a qualitative comparison of the text-only\nBERT model and the proposed mDT architecture. We find\nthat the text-only BERT model misclassifies 385/2717 test\ninstances. Upon passing those test instances through mDT,\nwe found that it corrected BERT’s labels in 161/385 cases.\nWe further note that BERT and mDT predictions disagree\non 264 test instances, which mDT is correct on 161 (61%).\nFigure 3 shows a fine-grained distribution of misclassified\ntest examples by class. Using mDT results in an overall de-\ncrease in misclassifications (385 → 327), with a significant\nreduction in false positives (fewer misclassifications for the\n‘Not Hateful’ class). However, we notice that BERT and\nmDT both struggle to detect the presence of hate speech\nin derogatory slur (DEG) and identity-directed (IdentityDi-\nrectedAbuse) comments.\nTable 8 shows some hateful test instances misclassified by\nthe two models. We note that the main text under consider-\nation (an individual comment) may not exhibit hate speech\non its own; however, considering it with the context (rest of\nthe discussion thread+image) helps mDT correctly classify\nthe test instances as hate speech. Consider the second exam-\nple in Table 8. The word “tranny” is a common acronym for\n“transmission” on social media, but considering the context,\nit is an abusive discussion directed toward the transgender\ncommunity. This is further contextualized by the accompa-\nnying image in the discussion, leading to evidence of hateful\ninterpretations (Figure 4). Images within the discussions for\neach example provide similar contextual evidence, such as\nthe third example and Figure 5.\nWe also found some intriguing test examples where\nadding context proved misleading for the model, while\nBERT confidently classified the main text as hateful. For\ninstance, in the last example in Table 8, comments in the\ncontext are largely non-abusive, misinterpreting the primary\ntext as non-abusive. This suggests that while adding context\nresults in a net decrease in misclassifications, majorly neu-\ntral context might also fool the model. This is likely since\nwe emphasize the discussion context when we obtain the fi-\nnal classification by averaging the text embedding logit with\nthe discussion node embedding (b0\nc).\nFuture Work\nWhile we find mDT to be an effective method for analyz-\ning discussions on social media, we have pointed out how\nit is challenged when the discussion context contains pre-\ndominately neutral comments. To address this, we propose\nusing a first-stage text ranker to compute semantic relevance\nbetween comments to filter unrelated messages.\nWe also note that many contextual signals in social me-\ndia discussions remain untapped beyond text, images, and\ndiscussion structure. Incorporating named entity recognition\ntechniques to integrate deeper analysis of real-world knowl-\nedge would be well-supported by the contextual nature of\nmDT(Mishra et al. 2022).\nPerhaps the most exciting step forward would be to ex-\npand our analysis of individual communities toward learn-\ning indicators of their propensity for hateful conduct. We are\ninterested in capturing the culture of specific platforms con-\ntaining diverse communities, including marginalized com-\nmunities, which exchange unique reclaimed vernacular that\nshould not be misinterpreted as hate. In addition to the exam-\nple given earlier of the African American community, there\nare particular usages as well that arise among platforms sup-\nporting LGBTQ users. The contextual nature of mDT cap-\ntured by graph transformers provides much promise for ad-\nvancing these extensions.\nFinally, the versatility of mDT’s core mechanisms makes\nit a promising tool for many applications beyond hate speech\ndetection. This approach could be applied to other do-\nmains such as online product reviews (Jagdale, Shirsat, and\nDeshmukh 2019), political discourse analysis (Lyu and Luo\n2022), and popularity analysis (Chen et al. 2022; Tan et al.\n2022), where understanding the discussion context is critical\nfor accurate interpretation.\nConclusion\nThis paper presented mDT: a holistic approach to detecting\nhate speech in social media. Our model leverages graphs,\ntext and images to reason about entire discussion threads.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22101\nPrimary T\next Context (only\nseen by mDT) BERT\npred. mDT pred.\nNow\nimagine if virtuous keyboard sjws\nhad their way? Their mascot should be\nRalph Wiggum.\n[. .\n. ] Preferred pronouns: go/f-\nslur/yourself [. . . ]If the Chinese\nin my corner of NZ only sold to\nChinese they’d starve by Thursday.\n[. . . ]They just wanna b-slur about\nsomething because their own life sucks.\nNot Hateful Hateful\nwhoa brah..\nleave my tranny out of this [image] (Figure\n4) [. . . ]that’s f-slur re-\ntarded [. . . ]Just spit my drink [. . . ]\nNot Hateful Hateful\n”That n-slur was\non PCP Johnson”\nLmao\n[image] (Figure\n5) [. . . ]Its’ a common\npattern when dealing with these shoot-\nings. * Kill black dude [. . . ]* Wingnut\nwelfare kicks in as racist f-slur create\ngofundme of over half a million f-slur\ndollars for cops family [. . . ]\nNot Hateful Hateful\nuwu o\nwo uwu [. .\n. ]That is not even close to what\nfeminism is. What you are talking\nabout is radical Feminism [. . . ]Got\nbanned from my sexual minority sub-\nreddit (r/bisexual) for not believing that\nall bisexuals should actually be pansex-\nuals [. . . ]\nHateful Not Hateful\nTable 8: Text instances misclassified by BERT and mDT. Note: The ground truth for all the examples shown here is “Hateful”.\nWe have also redacted chunks of text from the context in the interest of space. The redacted content is shown by [. . . ].\nCore to our approach is the introduction of hierarchical spa-\ntial encodings and coupling of text, image, and graph trans-\nformers through a novel bottleneck mechanism. We also pre-\nsented a new dataset of complete multi-modal discussions\ncontaining a wide spectrum of hateful content, enabling fu-\nture work into robust graph-based solutions for hate speech\ndetection3.\nOne significant contribution is demonstrating how\ndiscussion-oriented multi-modal analysis can improve the\ndetection of anti-social behaviour online. Compared with\nseveral key competitors, our experimental results demon-\nstrate the quantitative improvements stemming from our\nmethod. Notably, we see a 21% improvement in F1 over\nprevious methods to include discussion context, such as\n(Hebert, Golab, and Cohen 2022). Furthermore, our initial\nqualitative analysis demonstrates the valuable impact of our\nholistic multi-modal approach.\nBeyond enhanced holistic discussion analysis, our work\nenables a rich understanding of conversational dynamics,\nenabling community-centric prediction. This is primarily\npowered by our novel improvements to graph transformers,\na method gaining momentum in AI molecular modelling, re-\nvealing their potential to capture the relationships in com-\nplex multi-modal discussions. We hypothesize that this ex-\npressiveness in capturing context can aid in disambiguating\nfalse positives, preventing further marginalization of com-\nmunities, and proactively mitigating hateful behaviours.\nOverall, our approach presents a path forward for address-\n3We leave additional comparisons with image-text multi-modal\nmethods to future work to resolve fairness considerations; these\ncompetitors require comments to be multi-modal, whereas we can\nnotice relevant discussion responses with images.\nFigure 5: An image present in discussion context of example\n2 (Table 8), seen only by mDT, contextualizing comments as\npotentially hateful\ning the issue of hate speech on social media and encourages\nthe exploration of holistic graph-based multi-modal models\nto interpret online discussions. We believe our research can\nhelp foster healthier and more inclusive environments, im-\nproving mental health for individuals online.\nEthics Statement\nOur data collection efforts to create HatefulDiscussions are\nconsistent with Reddit’s Terms of Service and with approval.\nWe removed all user-identifying features and metadata from\nall posts and images to ensure privacy and refrained from\ncontacting users. To best comply with their terms of service,\nwe will be open-sourcing our dataset and pre-trained models\nunder the CC-BY-NC 4.0 license, with the model code under\nthe permissive MIT license.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22102\nAcknowledgements\nThe authors thank the Natural Sciences and Engineering Re-\nsearch Council of Canada, the Canada Research Chairs Pro-\ngram and the University of Waterloo Cheriton Scholarship\nfor financial support. We are also grateful to the reviewers\nfor their valued feedback on the paper.\nReferences\nCao, R.; Lee, R. K.-W.; Chong, W.-H.; and Jiang, J. 2022.\nPrompting for Multimodal Hateful Meme Classification. In\nProceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, 321–332. Abu Dhabi,\nUnited Arab Emirates: Association for Computational Lin-\nguistics.\nCaselli, T.; Basile, V .; Mitrovi´c, J.; and Granitzer, M. 2021.\nHateBERT: Retraining BERT for Abusive Language Detec-\ntion in English. In Proceedings of the 5th Workshop on On-\nline Abuse and Harms (WOAH 2021), 17–25. Online: Asso-\nciation for Computational Linguistics.\nChen, W.; Huang, C.; Yuan, W.; Chen, X.; Hu, W.; Zhang,\nX.; and Zhang, Y . 2022. Title-and-Tag Contrastive Vision-\nand-Language Transformer for Social Media Popularity Pre-\ndiction. In Proceedings of the 30th ACM International Con-\nference on Multimedia, MM ’22, 7008–7012. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450392037.\nDas, M.; Mathew, B.; Saha, P.; Goyal, P.; and Mukherjee,\nA. 2020. Hate Speech in Online Social Media. SIGWEB\nNewsl., (Autumn).\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding.\nFounta, A.; Djouvas, C.; Chatzakou, D.; Leontiadis, I.;\nBlackburn, J.; Stringhini, G.; Vakali, A.; Sirivianos, M.; and\nKourtellis, N. 2018. Large Scale Crowdsourcing and Char-\nacterization of Twitter Abusive Behavior.Proceedings of the\nInternational AAAI Conference on Web and Social Media ,\n12(1).\nGong, H.; Valido, A.; Ingram, K. M.; Fanti, G.; Bhat, S.;\nand Espelage, D. L. 2021. Abusive language detection in\nheterogeneous contexts: Dataset collection and the role of\nsupervised attention. InProceedings of the AAAI Conference\non Artificial Intelligence (AISI Track), volume 35, 14804–\n14812.\nHanu, L.; and Unitary team. 2020. Detoxify. Github.\nhttps://github.com/unitaryai/detoxify.\nHebert, L.; Chen, H. Y .; Cohen, R.; and Golab, L. 2023.\nQualitative Analysis of a Graph Transformer Approach to\nAddressing Hate Speech: Adapting to Dynamically Chang-\ning Content. Workshop in Artificial Intelligence for Social\nGood (AISG) at AAAI.\nHebert, L.; Golab, L.; and Cohen, R. 2022. Predicting Hate-\nful Discussions on Reddit using Graph Transformer Net-\nworks and Communal Context. In 2022 IEEE/WIC/ACM\nInternational Joint Conference on Web Intelligence and In-\ntelligent Agent Technology (WI-IAT), 9–17.\nJagdale, R. S.; Shirsat, V . S.; and Deshmukh, S. N. 2019.\nSentiment analysis on product reviews using machine learn-\ning techniques. In Cognitive Informatics and Soft Comput-\ning: Proceeding of CISC 2017, 639–647. Springer.\nKiela, D.; Firooz, H.; Mohan, A.; Goswami, V .; Singh, A.;\nRingshia, P.; and Testuggine, D. 2020. The hateful memes\nchallenge: Detecting hate speech in multimodal memes.\nKim, W.; Son, B.; and Kim, I. 2021. ViLT: Vision-and-\nLanguage Transformer Without Convolution or Region Su-\npervision. In Proceedings of the 38th International Con-\nference on Machine Learning, 5583–5594. PMLR. ISSN:\n2640-3498.\nKurrek, J.; Saleem, H. M.; and Ruths, D. 2020. Towards a\nComprehensive Taxonomy and Large-Scale Annotated Cor-\npus for Online Slur Usage. In Proceedings of the Fourth\nWorkshop on Online Abuse and Harms, 138–149. Online:\nAssociation for Computational Linguistics.\nLiang, B.; Lou, C.; Li, X.; Yang, M.; Gui, L.; He, Y .; Pei, W.;\nand Xu, R. 2022. Multi-Modal Sarcasm Detection via Cross-\nModal Graph Convolutional Network. InProceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 1767–1777. Dublin,\nIreland: Association for Computational Linguistics.\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. ViLBERT:\npretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. In Proceedings of the 33rd In-\nternational Conference on Neural Information Processing\nSystems, 2, 13–23. Red Hook, NY , USA: Curran Associates\nInc.\nLyu, H.; and Luo, J. 2022. Understanding Political Polar-\nization via Jointly Modeling Users, Connections and Multi-\nmodal Contents on Heterogeneous Graphs. In Proceedings\nof the 30th ACM International Conference on Multimedia ,\nMM ’22, 4072–4082. New York, NY , USA: Association for\nComputing Machinery. ISBN 9781450392037.\nMathew, B.; Saha, P.; Yimam, S. M.; Biemann, C.; Goyal, P.;\nand Mukherjee, A. 2021. Hatexplain: A benchmark dataset\nfor explainable hate speech detection. In Proceedings of the\nAAAI Conference on Artificial Intelligence (AISI Track), vol-\nume 35, 14867–14875.\nMeta. 2023. Meta Reports First Quarter 2023 Results.\nMeta Investor Relations. https://investor.fb.com/investor-\nnews/press-release-details/2023/Meta-Reports-First-\nQuarter-2023-Results/default.aspx.\nMishra, S.; Saini, A.; Makki, R.; Mehta, S.; Haghighi, A.;\nand Mollahosseini, A. 2022. Tweetnerd-end to end entity\nlinking benchmark for tweets. Advances in Neural Informa-\ntion Processing Systems, 35: 1419–1433.\nNagrani, A.; Yang, S.; Arnab, A.; Jansen, A.; Schmid, C.;\nand Sun, C. 2021. Attention Bottlenecks for Multimodal Fu-\nsion. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y .; Liang,\nP.; and Vaughan, J. W., eds., Advances in Neural Informa-\ntion Processing Systems, volume 34, 14200–14213. Curran\nAssociates, Inc.\nParmentier, A.; P’ng, J.; Tan, X.; and Cohen, R. 2021.\nLearning Reddit User Reputation Using Graphical Attention\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22103\nNetworks. In Future Technologies Conference (FTC) 2020,\nVolume 1, 777–789. ISBN 978-3-030-63128-4.\nQian, J.; Bethke, A.; Liu, Y .; Belding, E.; and Wang, W. Y .\n2019. A Benchmark Dataset for Learning to Intervene in\nOnline Hate Speech. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 4755–4764. Hong\nKong, China: Association for Computational Linguistics.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nSahu, G.; Cohen, R.; and Vechtomova, O. 2021. Towards a\nmulti-agent system for online hate speech detection. Second\nWorkshop on Autonomous Agents for Social Good (AASG),\nAAMAS, 2021.\nSingh, A.; Dey, S.; Singha, A.; and Saha, S. 2022. Senti-\nment and Emotion-Aware Multi-Modal Complaint Identifi-\ncation. Proceedings of the AAAI Conference on Artificial\nIntelligence (AISI Track), 36(11): 12163–12171.\nTan, Y .; Liu, F.; Li, B.; Zhang, Z.; and Zhang, B. 2022. An\nEfficient Multi-View Multimodal Data Processing Frame-\nwork for Social Media Popularity Prediction. In Proceed-\nings of the 30th ACM International Conference on Multime-\ndia, MM ’22, 7200–7204. New York, NY , USA: Association\nfor Computing Machinery. ISBN 9781450392037.\nVedeler, J. S.; Olsen, T.; and Eriksen, J. 2019. Hate speech\nharms: a social justice discussion of disabled Norwegians’\nexperiences. Disability & Society, 34(3): 368–383.\nVidgen, B.; Nguyen, D.; Margetts, H.; Rossini, P.; and\nTromble, R. 2021a. Introducing CAD: the Contextual Abuse\nDataset. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 2289–\n2303. Online: Association for Computational Linguistics.\nVidgen, B.; Thrush, T.; Waseem, Z.; and Kiela, D. 2021b.\nLearning from the Worst: Dynamically Generated Datasets\nto Improve Online Hate Detection. In Proceedings of the\n59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers) ,\n1667–1682. Online: Association for Computational Linguis-\ntics.\nWachs, S.; G ´amez-Guadix, M.; and Wright, M. F. 2022.\nOnline hate speech victimization and depressive symptoms\namong adolescents: The protective role of resilience. Cy-\nberpsychology, Behavior, and Social Networking.\nWaller, I.; and Anderson, A. 2021. Quantifying social orga-\nnization and political polarization in online platforms. Na-\nture, 600(7888): 264–268.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.; Shen,\nY .; and Liu, T.-Y . 2022. Do transformers really perform\nbadly for graph representation? Advances in Neural Infor-\nmation Processing Systems, 34: 28877–28888.\nZampieri, M.; Malmasi, S.; Nakov, P.; Rosenthal, S.; Farra,\nN.; and Kumar, R. 2019. Predicting the Type and Target\nof Offensive Posts in Social Media. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 1415–\n1420. Minneapolis, Minnesota: Association for Computa-\ntional Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22104",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7273419499397278
    },
    {
      "name": "Modal",
      "score": 0.5608507394790649
    },
    {
      "name": "Computer science",
      "score": 0.5286296606063843
    },
    {
      "name": "Natural language processing",
      "score": 0.34534305334091187
    },
    {
      "name": "Speech recognition",
      "score": 0.32721781730651855
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3202494978904724
    },
    {
      "name": "Engineering",
      "score": 0.2287026047706604
    },
    {
      "name": "Electrical engineering",
      "score": 0.18853268027305603
    },
    {
      "name": "Voltage",
      "score": 0.1455736756324768
    },
    {
      "name": "Materials science",
      "score": 0.0984356701374054
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    }
  ],
  "cited_by": 12
}