{
  "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
  "url": "https://openalex.org/W4389520362",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1976851905",
      "name": "Fajri Koto",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2105385159",
      "name": "Nurul Aisyah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118600003",
      "name": "Haonan Li",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2096893938",
      "name": "Timothy Baldwin",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "University of Melbourne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998617917",
    "https://openalex.org/W103165144",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4380994432",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W2914507741",
    "https://openalex.org/W4287664280",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2792017515",
    "https://openalex.org/W3086966320",
    "https://openalex.org/W3116295307",
    "https://openalex.org/W4401043917",
    "https://openalex.org/W4385574272",
    "https://openalex.org/W1975764439",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W4327522884",
    "https://openalex.org/W2970780738",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3214173179",
    "https://openalex.org/W4386566878",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W3153266325",
    "https://openalex.org/W4285107336",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4385571031",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W4386365131",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4285221836",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W3110604908",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W4285283606",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4378465252",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4205137784",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4377161536",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4287888537",
    "https://openalex.org/W3199243620",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3101601200",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12359–12374\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models Only Pass Primary School Exams in Indonesia:\nA Comprehensive Test on IndoMMLU\nFajri Koto1 Nurul Aisyah2 Haonan Li1 Timothy Baldwin1,3\n1Department Natural Language Processing, MBZUAI\n2Quantic School of Business and Technology\n3The University of Melbourne\nfajri.koto@mbzuai.ac.ae, nurulaisyah.inc@gmail.com, {haonan.li,timothy.baldwin}@mbzuai.ac.ae\nAbstract\nAlthough large language models (LLMs) are of-\nten pre-trained on large-scale multilingual texts,\ntheir reasoning abilities and real-world knowl-\nedge are mainly evaluated based on English\ndatasets. Assessing LLM capabilities beyond\nEnglish is increasingly vital but hindered due to\nthe lack of suitable datasets. In this work, we in-\ntroduce IndoMMLU, the first multi-task language\nunderstanding benchmark for Indonesian cul-\nture and languages, which consists of questions\nfrom primary school to university entrance ex-\nams in Indonesia. By employing professional\nteachers, we obtain 14,981 questions across 64\ntasks and education levels, with 46% of the\nquestions focusing on assessing proficiency in\nthe Indonesian language and knowledge of nine\nlocal languages and cultures in Indonesia. Our\nempirical evaluations show that GPT-3.5 only\nmanages to pass the Indonesian primary school\nlevel, with limited knowledge of local Indone-\nsian languages and culture. Other smaller mod-\nels such as BLOOMZ and Falcon perform at\neven lower levels.1\n1 Introduction\nThe evaluation of large language models (LLMs)\nhas predominantly relied on English datasets to\nassess language proficiency (Wang et al., 2018;\nBaradaran et al., 2022), reasoning abilities (Zellers\net al., 2019; Huang et al., 2019; Bisk et al., 2020;\nTalmor et al., 2019), and real-world knowledge\n(Hendrycks et al., 2021). LLMs such as GPT-3.5\n(Ouyang et al., 2022), Falcon (Penedo et al., 2023),\nand BLOOMZ (Muennighoff et al., 2022), however,\nare pre-trained on large-scale multilingual data, and\nthus it is critical to evaluate what knowledge they\ncapture and their reasoning abilities in languages\nbeyond English.\nSchool exams serve as a powerful means to as-\nsess the reasoning abilities and real-world knowl-\n1Code and dataset can be found at https://github.com/\nfajri91/IndoMMLU\nFigure 1: Distribution of subject areas and education\nlevels in IndoMMLU. “Hum”, “Social”, “Indo”, and “Lo-\ncal” refer to Humanities, Social Science, Indonesian\nLanguage, and Local Languages and Cultures, respec-\ntively.\nedge of LLMs, given that these tests are meticu-\nlously designed by expert educators, drawing upon\nthe principles of learning science. At various educa-\ntional levels, school exams function as assessment\ntools, evaluating not only language proficiency but\nalso higher-order cognitive skills such as compre-\nhension, analytic abilities, and the application of\nreal-world knowledge across diverse scenarios (No-\nvak, 1988).\nHendrycks et al. (2021) proposed MMLU, a mas-\nsive multitask language understanding benchmark\nin English that is compiled from different exams,\ncovering topics including US history, computer sci-\nence, and high school subjects. Recent progresses\non LLMs such as LLaMA (Touvron et al., 2023)\nand GPT–4 (OpenAI, 2023) use MMLU as one of the\nevaluation datasets. In the GPT-4 technical report,\nautomatic evaluation is further extended to encom-\npass various standardized exams, including SAT,\nGRE, and bar exams.\nWhile there has been a plethora of work on LLM\nevaluation for English (OpenAI, 2023; Katz et al.,\n12359\n2023; Choi et al., 2023; Ryznar, 2023; Chalkidis,\n2023), there has been comparatively little work in\nother languages (Li et al., 2023b; Sengupta et al.,\n2023). Recent work by OpenAI (2023) evaluated\nGPT-4 using a translated version of MMLU, and re-\nported strong performance. While encouraging, us-\ning translations of English evaluation datasets has\nserious shortcomings, including translation noise,\na complete lack of content that is sensitized to the\nlocal language/culture (esp. as most English evalua-\ntion datasets are highly US centric), and conversely,\nthe existence of content that is irrelevant to the lo-\ncal language/culture (e.g. questions relating to US\nlaw or customs) and incongruent with the language-\nspecific evaluation (Liu et al., 2023a).\nIn this paper, we ask professional teachers\n(of Indonesian nationality) to collect exam ques-\ntions from various educational levels in Indonesian\nschools (i.e. primary school, junior high school,\nsenior high school, and university). We categorize\nthe collected questions into different subject areas,\nincluding: (1) STEM (Science, Technology, Engi-\nneering, and Mathematics); (2) Social Science; (3)\nHumanities; (4) Indonesian Language; and (5) Lo-\ncal Languages and Cultures. Figure 1 presents an\noverview of the distribution of the resulting dataset,\nIndoMMLU, across different subject areas and edu-\ncation levels. It is worth mentioning that 21% of\nthe questions specifically focus on the Indonesian\nlanguage, and 25% encompass nine distinct local\nlanguages and cultures that are specific to Indone-\nsia.\nOur contributions can be summarized as follows:\n• We introduce the first IndonesianMMLU dataset,\nnamely IndoMMLU, which comprises 64 tasks\nacross different subject areas and education\nlevels in Indonesia.\n• Our dataset includes exam questions from\nschool grades 1 to 12, as well as university\nentrance exams. This comprehensive cover-\nage allows us to perform fine-grained assess-\nment of the Indonesian language proficiency\nof existing LLMs.\n• Approximately 25% of our data encompasses\nnine distinct local languages and cultures\nin Indonesia, namely Lampungic ( ljp), Ba-\nlinese (ban), Makassarese ( mak), Banjarese\n(bjn), Madurese (mad), Sundanese (sun), Ja-\nvanese (jav), Dayak Ngaju ( nij), and Mi-\nnangkabau.2 These questions are not only in\n2For Minangkabau culture, the Indonesian language is\nunder-represented languages but also incor-\nporate specific cultural content, such as art,\npoetry, and daily life. For Lampungic ( ljp)\nand Makassarese ( mak) in particular, this is\nthe very first NLP resource to be released.\n• We evaluate various multilingual LLMs,\nincluding GPT-3.5 (Ouyang et al., 2022),\nXGLM (Lin et al., 2021), Falcon (Penedo\net al., 2023), BLOOMZ (Muennighoff et al.,\n2022), mT0 (Muennighoff et al., 2022),\nLLaMA (Touvron et al., 2023), and Bactrian-\nX (Li et al., 2023a), across different model\nsizes. We find that only GPT-3.5 passes the\nhighest primary school level exam, and no\nmodels demonstrate familiarity with local In-\ndonesian languages and culture.\n2 Related Work\nEvaluating Large Language Models Various\nbenchmarks have been released to evaluate En-\nglish pre-trained LMs (Devlin et al., 2019; Con-\nneau et al., 2020). Early benchmarks such as GLUE\n(Wang et al., 2018) and SuperGLUE (Wang et al.,\n2019) consist of various natural language under-\nstanding (NLU) tasks of different types with vary-\ning training data sizes. XGLUE (Liang et al., 2020),\nXTREME (Hu et al., 2020), and XTREME-R (Ruder\net al., 2021) serve as multilingual benchmarks of\nmore than 20 languages. For natural language\ngeneration (NLG), the GEM benchmark (Gehrmann\net al., 2021) is a collection of machine translation,\nsummarization, and generated descriptions in many\nlanguages.\nAs LLMs have become larger in size and im-\nproved over the standard benchmarks, there has\nbeen a shift in evaluation practice to focus on\nreasoning abilities (Zellers et al., 2019; Huang\net al., 2019; Bisk et al., 2020; Talmor et al.,\n2019; Koto et al., 2022a), and real-world knowl-\nedge (Hendrycks et al., 2021). In GPT-4 (Ope-\nnAI, 2023), for instance, commonsense reason-\ning is evaluated using HellaSwag (Zellers et al.,\n2019) and WinoGrande (Sakaguchi et al., 2021),\nwhile real-world knowledge is evaluated based on\nschool exams including MMLU (Hendrycks et al.,\n2021), ARC (Clark et al., 2018), andGSM-8K (Cobbe\net al., 2021). Similarly, LLaMA (Touvron et al.,\n2023) was evaluated using school exam problems,\nin addition to closed-book question answering\n(Kwiatkowski et al., 2019; Joshi et al., 2017) and\nused in teaching and exams.\n12360\nGroup Question Answer\nPrimary school 99.6 65.5\nJunior high school 188.3 105.9\nSenior high school 167.7 130.3\nUniversity Entrance Test 204.9 186.2\nSTEM 133.7 102.4\nSocial science 136.2 131.9\nHumanities 113.2 104.4\nLocal languages and cultures 88.4 68.0\nIndonesian language 307.4 161.8\nTable 1: Average question and answer length (in charac-\nters) for each education group and subject area.\nthe RACE reading comprehension benchmark (Lai\net al., 2017).\nIndonesian Pre-trained Language Models and\nBenchmarks Several monolingual pre-trained\nlanguage models have been released for Indone-\nsian, including IndoBERT (Koto et al., 2020b;\nWilie et al., 2020), IndoBERTweet (Koto et al.,\n2021), and IndoBART (Cahyawijaya et al., 2021).\nThese models have been evaluated on NLU (e.g. In-\ndoLEM and IndoNLU) and NLG (e.g. IndoNLG)\nbenchmarks. Component tasks include sentiment\nanalysis (Koto and Rahmaningtyas, 2017; Purwari-\nanti and Crisdayanti, 2019), emotion classification\n(Saputri et al., 2018), hate speech detection, sum-\nmarization (Koto et al., 2020a, 2022b), and trans-\nlation (Cahyawijaya et al., 2021; Koto and Koto,\n2020).\nIn contemporaneous work, Cahyawijaya et al.\n(2023) evaluated several LLMs using existing In-\ndonesian datasets. This collection includes the re-\ncent NusaX dataset (Winata et al., 2023), which\nis a parallel sentiment analysis dataset in 10 In-\ndonesian local languages, created through human\ntranslation. The collection also includes several\nquestion-answering datasets, such as FactQA (Pur-\nwarianti et al., 2007), IDK-MRC (Putri and Oh,\n2022), and TyDiQA (Clark et al., 2020), over news\nand Wikipedia documents. IndoMMLU is different\nin that it explicitly evaluates reasoning, language,\nand cultural abilities in a fine-grained manner from\nthe perspective of education science.\n3 IndoMMLU\nIndoMMLU is a multiple-choice problem set in 64\nsubjects from different education levels, follow-\ning the format of English MMLU (see Figure 2 and\nFigure 3). IndoMMLU, however, is based on the\nTerjadinya hubungan induak\nbako dengan anak pisang,\nkarena adanya ....\nA. Perkawinan        \nB. Satu suku         \nC. Satu nagari \nD. Kaum\nInduak bako and anak pisang is\na relationship in Minangkabau\nfamily because of ....\nA. Marriage\nB. One tribe relationship\nC. One village relationship\nD. One sub-tribe relationship\nMinangkabau culture subject, Mid-term exam, class 7 (SMP)\nMinangkabau culture subject, Mid-term exam, class 6 (SD)\nTari rantak kudo berasal dari ...\nA. Tanah datar\nB. Pesisir Selatan \nC. Pasaman barat\nD. Solok\nRantak kudo dance originally\ncomes from ...\nA. Tanah datar\nB. Pesisir Selatan \nC. Pasaman barat\nD. Solok\nSundanese subject, Mid-term exam, class 4 (SD)\nHal - hal anu ditepikeun dina\nkawih, disebutna .....\nA. rasa\nB. amanat\nC. nada\nD. tema\nMessage that we want to\nexpress in Kawih is ...\nA. feeling\nB. advice\nC. tone\nD. theme\nFigure 2: The first question focuses on the family re-\nlationship between anak pisang “children” and induak\nbako “aunt on the father’s side”. Both terms are com-\nmonly used in Minangkabau but not in the Indonesian\nlanguage. The second and third questions pertain to\ntraditional art. Kawih in the third question means a song\nset to a distinctive beat in Sundanese culture. Left is\nthe original text and right is the English translation for\nillustrative purposes. The bold options are the correct\nanswer keys.\nIndonesian education curriculum, and has more\nfine-grained education levels than MMLU.\nIn Indonesia’s curriculum, schools are catego-\nrized into three levels: (1) six years of primary\nschool (Sekolah Dasar = “SD”), (2) three years of\njunior high school (Sekolah Menengah Pertama =\n“SMP”), and (3) three years of senior high school\n(Sekolah Menengah Atas = “SMA”). At primary\nschool, pupils in all grades are taught the Indone-\nsian language, civics, mathematics, art, sports, and\nreligion. From grade 4 to 6 and in junior high\nschool, pupils additionally learn a foreign language,\na local language/culture, science, and social sci-\nence.3 In senior high school, pupils study more spe-\ncialized natural science and social science subjects,\nincluding physics, chemistry, biology, geography,\nsociology, economy, and history. InIndoMMLU, we\nexplicitly exclude mathematics because the ques-\ntions typically consist primarily of symbols with lit-\n3In a recent curriculum change, science and social science\nhave been added from grade 3.\n12361\nKetentuan mengenai menteri\ndiatur dalam UUD NRI Tahun\n1945, yakni dalam Bab 5\ntentang kementerian negara\ntepatnya pada pasal...\nA. 17       \nB. 17 A     \nC. 17 B\nD. 18\nE. 19\nCivics subject, Final-term exam, class 10 (SMA)\nChemistry subject, University entrance test exam\nAsam oksalat adalah asam\nberbasa dua. Sebanyak 10 mL\nlarutan asam oksalat diencerkan\ndengan air sampai volumenya\n100 mL. Larutan ini digunakan\nuntuk menitrasi 20 mL larutan\nNaOH 0,2 M dengan indikator\nbromtimol biru. Bila titik akhir\ntitrasi diperoleh saat volume\nasam oksalat mencapai 25 mL,\nmaka konsentrasi larutan asam\noksalat awal adalah...\nA. 0,08 M\nB. 0,40 M\nC. 0,80 M\nD. 1,60 M\nE. 3,20 M\nProvisions regarding ministers\nare regulated in the 1945\nConstitution of the Republic of\nIndonesia, namely in Chapter 5\nconcerning state ministries to\nbe precise in the article...\nA. 17\nB. 17 A     \nC. 17 B\nD. 18\nE. 19\nOxalic acid is a two basic acid.\nA total of 10 mL of oxalic acid\nsolution was diluted with water\nto a volume of 100 mL. This\nsolution was used to titrate 20\nmL of 0.2 M NaOH solution with\nbromthymol blue indicator. If\nthe end point of the titration is\nobtained when the volume of\noxalic acid reaches 25 mL, then\nthe concentration of the initial\noxalic acid solution is...\nA. 0,08 M\nB. 0,40 M\nC. 0,80 M\nD. 1,60 M\nE. 3,20 M\nFigure 3: Examples of civics and chemistry exam ques-\ntions. Left is the original text and right is the English\ntranslation for illustrative purposes. The bolded options\nare the answer keys.\ntle language content, and there are existing datasets\nfor mathematical reasoning such asGSM-8K (Cobbe\net al., 2021) and NumGLUE (Mishra et al., 2022).\nThe local language/culture subjects vary across\nprovinces in Indonesia and depend on the local\ngovernment policy. For example, in West Sumatra,\nMinangkabau culture is taught using the Indonesian\nlanguage, while in West Java, pupils are exposed\nto the Sundanese language and culture. Figure 2\nillustrates two exam questions for Minangkabau\nculture, and one exam question for Sundanese.\n3.1 Data Construction\nWe asked seven professional teachers with at least\na bachelor’s degree in education to gather publicly-\navailable school exam questions in Indonesia from\nweb sources.4 They were tasked with gathering\nproblems for specific subject areas and educational\nlevels, as well as metadata such as the source (i.e.\nURL of the source document), school level, class\nlevel, question, multiple-choice options, and the\ncorrect answer key. We instructed the teachers to\n4The seven teachers were selected from 70 applicants.\nGroup Subjects\nSTEM Chemistry (SMA, UE), Biology (SMA,\nUE), Physics (SMA, UE), Science (SD,\nSMP)\nSocial science Geography (SMA, UE), Sociology\n(SMA, UE), Economy (SMA, UE),\nCivics education (SD, SMP, SMA),\nSocial science (SD, SMP)\nHumanities History (SMA, UE), Art (SD, SMP,\nSMA), Sports (SD, SMP, SMA), Islam\nreligion (SD, SMP, SMA), Christian\nreligion (SD, SMP, SMA), Hindu\nreligion (SD, SMP, SMA)\nLocal languages and\ncultures\nLampungic (SD, SMP, SMA), Balinese\n(SD, SMP, SMA), Makassarese (SD,\nSMP, SMA), Banjarese (SD, SMP,\nSMA), Madurese (SD, SMP, SMA),\nMinangkabau culture (SD, SMP),\nDayak Ngaju (SD), Sundanese (SD,\nSMP, SMA), Javanese (SD, SMP,\nSMA)\nIndonesian language Indonesian language (SD, SMP, SMA,\nUE)\nTable 2: Subject areas in IndoMMLU. “SD”, “SMP”,\n“SMA”, “UE” indicate that questions in the subject are\nare available in primary school, junior high school, se-\nnior high school, and university entrance exams, respec-\ntively.\nonly include exams that had accompanying answer\nkeys, and to exclude problems that contained im-\nages. Additionally, we organized an 1-hour work-\nshop to discuss the data collection procedure with\nall the teachers, addressing any questions or con-\ncerns they had. All teachers are paid competitively,\nhigher than the Indonesian average monthly wage.5\n3.2 Quality Control\nTo ensure the accuracy of the data entry pro-\ncess, we randomly checked questions collected by\neach teacher. We manually verified the questions,\nmultiple-choice options, and the corresponding an-\nswer keys based on the given URL, and found that\neach teacher conducted the work accurately. We\nalso additionally performed automatic filtering to\nremove repetitive questions, and remove questions\nthat have no answer key.\n3.3 Data Statistics\nAfter data cleansing, we obtained a total of 14,981\nquestions, distributed over school levels and sub-\njects as detailed in Figure 1; the details of each\nsubject area are in Table 2 and the Appendix.\n5The work for a single teacher was equivalent to a 5-day\nfull-time job.\n12362\nXGLM Falcon BLOOMZ mT0 LLaMA Bactrian-X\n10\n15\n20\n25\n30\n35\n40Accuracy\nFull answer probability\nFirst token probability\nFigure 4: LLM performance (% accuracy) based on: (1)\nthe probability of the full generated answer; and (2) the\nprobability of the first token in the generated answer.\nIndoMMLU consists of 30% primary school, 24%\njunior high school, 32% senior high school, and\n14% university entrance exam questions. Table 1\nshows the average question length for each educa-\ntion level and subject area. We can observe that\nprimary school questions tend to be shorter and uni-\nversity entrance exam questions are longer. Indone-\nsian language questions have the highest average\nlength, while local languages and culture questions\nare around 88 characters on average.\n4 Experiments\n4.1 Set-Up\nWe evaluate 24 multilingual LLMs of different\nsizes in zero-shot and few-shot settings. This\nincludes GPT-3.5 (Ouyang et al., 2022), XGLM\n(Lin et al., 2021), Falcon (Penedo et al., 2023),\nBLOOMZ (Muennighoff et al., 2022), mT0 (Muen-\nnighoff et al., 2022), LLaMA (Touvron et al., 2023)\nand Bactrian-X (Li et al., 2023a).6 We add a sim-\nple prompt in the Indonesian language Ini adalah\nsoal [subject] untuk [level]. Pilihlah salah satu\njawaban yang dianggap benar! “This is a [subject]\nquestion for [level]. Please choose the correct an-\nswer!” prior to the question and multiple-choice\noptions.\nFor closed-source models, we evaluate questions\nby comparing the first generated tokens (e.g., A,\nB, C) and the answer key using a regular expres-\nsion.7 For open-sourced models, we benchmark\ntwo strategies. Given a question and the corre-\nsponding multiple-choice options, we calculate: (1)\n6At the time this research was carried out, we did not have\naccess to the GPT-4 API, and thus leave it to future work.\n7In cases where there is no match, we assign a random\nanswer.\nthe probability of the full generated answer; and (2)\nthe probability of the first token in the generated\nanswer. For the first, we select the answer with the\nhighest normalized log likelihood, and for the sec-\nond, we simply select the key token (e.g., C) with\nthe highest probability among all possible keys.\n4.2 Results\nFigure 4 presents the zero-shot accuracy when us-\ning: (1) the full answer probability; and (2) the\nprobability of the first token in the generated an-\nswer. Among the open-sourced language models\n(LLMs) including XGLM (7.5B), Falcon (40B),\nBLOOMZ (7.1B), mT0xxl (13B), LLaMA (65B),\nand Bactrian-X (13B), we find that estimating the\nanswer based on the probability of the first token in\nthe generated answer generally performs best, with\nthe notable exception of XGLM. Thus, we report\nresults under this configuration in the remaining\nsections; the full results for both settings can be\nfound in the Appendix.\nResults across all models Table 3 shows the av-\nerage accuracy for each subject area across the 24\nmodels. To compute the scores, we disregard the\neducation level of the questions, and average scores\nbased on the subject (e.g. Biology), and finally com-\nbine the scores across all subject areas (e.g. STEM).\nThe random performance varies between 20% to\n27% due to the differing number of multiple-choice\noptions (i.e. three to five).\nOverall, we found that GPT-3.5 attains the high-\nest overall accuracy, albeit low at 53.2%. GPT-\n3.5 is also notably the highest in each subject\narea, except in local languages and culture sub-\njects. Among the open-source models, we observe\nthat mT0xxl (13B) achieves an average accuracy of\n42.5%. Falcon (40B) performs worse than mT0xxl\n(13B) and BLOOMZ (7B).\nPerformance based on model size varies, with\nsmaller models such as BLOOMZ (7B) and mT0xxl\nbeing better than Falcon (40B) and LLaMA (65B).\nWe suspect that this is due to the absence of the\nIndonesian language in Falcon and LLaMA’s pre-\ntraining data. The poor performance of the 13B and\n30B LLaMA models might imply that any “emer-\ngent abilities” of LLMs generally appear in the\nsame or closely-related languages. This is fur-\nther supported by Bactrian-X-LLaMA (13B), a\nLLaMA model fine-tuned on instruction datasets in\n52 languages (including Indonesian), which obtain\na +5% average increment, compared to LLaMA\n12363\nModel (#parameters) STEM Social Humanities Indonesian Local languages AverageScience Language and Cultures\nRandom 21.9 23.4 23.5 24.4 26.6 24.4\nGPT-3.5 (175B) 54.3 62.5 64.0 62.2 39.3 53.2\nXGLM (564M) 22.1 23.0 25.6 25.6 27.5 25.2\nXGLM (1.7B) 20.9 23.0 24.6 24.8 26.6 24.4\nXGLM (2.9B) 22.9 23.2 25.4 26.3 27.2 25.2\nXGLM (4.5B) 21.8 23.1 25.6 25.8 27.1 25.0\nXGLM (7.5B) 22.7 21.7 23.6 24.5 27.5 24.5\nFalcon (7B) 22.1 22.9 25.5 25.7 27.5 25.1\nFalcon (40B) 30.2 34.8 34.8 34.9 29.2 32.1\nBLOOMZ (560M) 22.9 23.6 23.2 24.2 25.1 24.0\nBLOOMZ (1.1B) 20.4 21.4 21.1 23.5 24.7 22.4\nBLOOMZ (1.7B) 31.5 39.3 38.3 42.8 29.4 34.4\nBLOOMZ (3B) 33.5 44.5 39.7 46.7 29.8 36.4\nBLOOMZ (7.1B) 37.1 46.7 44.0 49.1 28.2 38.0\nmT0small(300M) 21.8 21.4 25.7 25.1 27.6 24.9\nmT0base(580M) 22.6 22.6 25.7 25.6 26.9 25.0\nmT0large(1.2B) 22.0 23.4 25.1 27.3 27.6 25.2\nmT0xl (3.7B) 31.4 42.9 41.0 47.8 35.7 38.2\nmT0xxl(13B) 33.5 46.2 47.9 52.6 39.6 42.5\nLLaMA (7B) 22.8 23.1 25.1 26.7 27.6 25.3\nLLaMA (13B) 24.1 23.0 24.4 29.5 26.7 25.3\nLLaMA (30B) 25.4 23.5 25.9 28.4 28.7 26.5\nLLaMA (65B) 33.0 37.7 40.8 41.4 32.1 35.8\nBactrian-X-LLaMA (7B) 23.3 24.0 26.0 26.1 27.5 25.7\nBactrian-X-LLaMA (13B) 28.3 29.9 32.8 35.2 29.2 30.3\nTable 3: Zero-shot performance (% accuracy) of LLMs, combined across education levels. “Average” means the\naverage across all subject areas in IndoMMLU.\n(13B).\nResults across education levels As illustrated\nin Figure 1, IndoMMLU includes detailed education\nlevel metadata, which enables us to gain a deeper\nunderstanding of the capabilities of LLMs in terms\nof human education levels. In the Indonesian con-\ntext, the minimum passing score for exams varies\nacross subjects and typically ranges between 65\nand 70.8 By setting the passing score at 65, we\nassess GPT-3.5 over real-world knowledge capabil-\nities, as shown in Table 4. Green indicates that the\nmodel has successfully passed the subject, while\nred indicates it has failed. This reveals that GPT-3.5\ngenerally performs well on primary school exams\nfor general subjects, but exhibits a lack of under-\nstanding of local languages and culture. In sub-\njects that require less analytical thinking, such as\ncivics and religion, GPT-3.5 tends to achieve higher\nscores in high school exams.\nIndonesian language proficiency of LLMs As\ndiscussed in Section 3, IndoMMLU specifically in-\ncludes Indonesian language exams for all grades\n8This refers to Curriculum 2013 in Indonesia.\n1 2 3 4 5 6 7 8 9 10 11 12 13\nEducation level\n30\n40\n50\n60\n70\n80\n90Accuracy\nGPT-3.5 (175B)\nmT0xxl (13B)\nBLOOMZ (7B)\nFigure 5: Fine-grained accuracy (%) of GPT-3.5,\nmT0xxl, and BLOOMZ in the Indonesian language sub-\nject area. The horizontal line depicts the passing score\nof 65, and the education level of 13 refers to the univer-\nsity entrance exam.\nand education levels, allowing us to assess the In-\ndonesian language proficiency of LLMs. Figure 5\nillustrates that GPT-3.5 achieves its highest accu-\nracy in grade 1, approaching 90%. However, as the\neducation level increases, the model’s performance\ngradually declines. For grades 3 and above, the\n12364\nSubject SD SMP SMA UE\nScience 76.3 67.8 52.8 43.7\nSocial science 84.6 73.1 63.5 48.2\nIndonesian language 74.7 61.8 55.1 42.3\nCivics 64.6 65.2 65.4 –\nSports 66.7 44.7 62.0 –\nArt 73.9 71.2 58.7 –\nIslam religion 78.6 59.9 67.7 –\nChristian religion 83.7 77.6 62.0 –\nHindu religion 66.7 62.0 55.1 –\nSundanese 50.0 45.1 37.9 –\nJavenese 46.1 36.1 36.1 –\nBalinese 32.2 38.5 36.1 –\nMakassarese 33.7 48.8 38.3 –\nBanjarese 50.0 44.4 28.6 –\nLampungic 40.0 30.0 33.3 –\nMadurese 41.0 28.3 35.0 –\nMinangkabau culture38.0 52.2 – –\nDayak Ngaju 31.1 – – –\nTable 4: GPT-3.5 performance (% accuracy) across dif-\nferent education levels. “SD”, “SMP”, “SMA”, “UE”\nindicate primary school, junior high school, senior high\nschool, and university entrance tests, respectively. Red\nindicates that the score is below the minimum passing\nthreshold of 65, while green signifies a score at or above\nthis minimum.\nscores fall below 75, and for classes 7 and above,\nGPT-3.5 fails to pass the exams. We observe that\nthis trend is similar for mT0 xxl and BLOOMZ,\nwhich only pass grades 1, 2, and 3. This fine-\ngrained evaluation provides a valuable benchmark\nfor LLM proficiency in Indonesian.\nLLM performance on local languages and cul-\ntures It is interesting to observe in Table 3 that de-\nspite having only 13B parameters, mT0xxl achieves\nthe highest accuracy on local languages and cul-\ntures. On the other hand, GPT-3.5 with 175B pa-\nrameters achieves competitive accuracy, just 0.3\nabsolute points lower than mT0xxl. To further in-\nvestigate this, Figure 6 displays the accuracy scores\nof each local language and culture subject, reveal-\ning that both mT0xxl and GPT-3.5 excel in different\nsubject areas. mT0 xxl shows greater familiarity\nwith Javanese and Sundanese, with a disparity of\n+10 for both subjects compared to GPT-3.5. GPT-\n3.5 performs better in Dayak Ngaju, Banjarese, and\nMinangkabau culture.\n4.3 Analysis\nFew shot performance Providing several ques-\ntions and the answer key in prompts has been re-\n20 25 30 35 40 45 50 55\nSundanese\nJavanese\nBalinese\nMakassarese\nBanjarese\nLampungic\nMadurese\nMinangkabau\nDayak Ngaju\nmT0xxl\nGPT-3.5\nFigure 6: Zero-shot performance of mT0 xxl and GPT-\n3.5 in local languages and cultures subjects.\n0-shot 1-shot 2-shot 3-shot\n30\n35\n40\n45\n50Accuracy\nFalcon (40B)\nmT0xxl (13B)\nBLOOMZ (7B)\nLLamA (65B)\nFigure 7: Few-shot performance (% accuracy) of\nmT0xxl, BLOOMZ, Falcon, and LLaMA, averaged\nacross all subject areas.\nported to improve model performance (Karimi Ma-\nhabadi et al., 2022; Hendrycks et al., 2021). We\nrun similar experiments with our top-4 best open-\nsource models and observe mixed outcomes in Fig-\nure 7.9 Few-shot inference does not yield improve-\nments in instruction-tuned models like mT0 and\nBLOOMZ, as evidenced by a decrease in accuracy.\nIn contrast, the pure LLMs Falcon and LLaMA\nshow better performance with few-shot inference\ncompared to zero-shot. These findings align with\nthose of Liu et al. (2023b); Li et al. (2023b), where\nfew-shot prompts may lead to unnatural inferences\nfor instruction-tuned models.\nModel confidence Given the top three models in\nTable 3, we assess whether their confidence predic-\ntions (i.e. the predicted likelihood of the predicted\nanswer being correct) corresponds to the actual\naccuracy across 64 tasks. This uncertainty cali-\nbration gives us hints about the model’s reliability\n9Refer to the Appendix for details of the prompts.\n12365\n20 40 60 80\nConfidence (%)\n20\n40\n60\n80Accuracy\nmT0xxl(13B)\nBLOOMZ (7B)\nGPT-3.5 (175B)\nFigure 8: Zero-shot calibration of mT0 xxl, BLOOMZ,\nand GPT-3.5 across 64 tasks. The average standard\ndeviations of the confidence scores across all data points\nare 36.5, 26.4, and 43.9, respectively\nand how to use them appropriately in real-world\nsettings. For mT0 and BLOOMZ, the confidence\nscore is determined through softmax normalization\nover probabilities of the multiple-choice options.\nFor GPT-3.5, we adopt the approach described by\nSi et al. (2022); Wang et al. (2022), using a high-\ntemperature value (0.7) during decoding. For each\nquestion, we generate n different outputs and mea-\nsure self-consistency. The probability of a multiple-\nchoice option is calculated based on the output fre-\nquency. In this experiment, we use n = 7, and\nchoose the most frequently-occurring answer as\nthe final prediction.\nWe average the confidence scores across the\n64 tasks, and display the calibration of mT0,\nBLOOMZ, and GPT-3.5 in Figure 8. We observe\nthat all three models are well-calibrated, with cor-\nrelation scores of r >0.85.\nAdditionally, we examine the relationship be-\ntween confidence scores and question length, as\ndepicted in Figure 9. We found a very weak cor-\nrelation for both mT0 and BLOOMZ. It is worth\nnoting that the confidence score can also be inter-\npreted as a measure of question difficulty, based on\nwhich question length appears to have no bearing\non difficulty.\nImpact of negation In Indonesian school exam\nquestions, the use of negation is common to en-\nhance question difficulty and assess students’ rea-\nsoning abilities. Similarly, in the field of NLP, nega-\ntion is known to increase the difficulty of NLP tasks\n(Truong et al., 2022). To investigate the impact\nof negation, we employ a simple string-matching\nstrategy to identify questions that contain negations\nFigure 9: Correlation between question difficulty and\nquestion length.\nModel W/ negation W/o negation\nIndonesian language\nGPT-3.5 (175B) 58.0 62.7\nmT0xxl (13B) 47.9 53.1\nBLOOMZ (7B) 39.3 50.1\nSocial science\nGPT-3.5 (175B) 66.2 63.0\nmT0xxl (13B) 48.2 47.1\nBLOOMZ (7B) 43.3 48.2\nTable 5: Accuracy (%) for questions with and without\nnegation in the Indonesian language and social science\nsubject areas.\nwithin each subject area.10 We then break down the\naccuracy for the top three models (GPT-3.5, mT0,\nand BLOOMZ) based on the presence or absence of\nnegation. Among the subject areas, Indonesian lan-\nguage and social science are the most prevalent in\nemploying negation, accounting for approximately\n10% in each group. Through manual observation\nof 100 random samples, we verified that 85% of\nthese questions indeed contained negation.\nTable 5 shows the effects of negation on\nIndoMMLU accuracy. For the Indonesian language\nsubject area, negated questions prove to be more\nchallenging, with a decrease in accuracy ranging\nfrom −4 to −10. In social science, mT0 and\nBLOOMZ are similarly more accurate over ques-\ntions without negation. Compared to mT0, how-\never, BLOOMZ is less robust to negation, as indi-\ncated by the −5 accuracy drop.\n10To identify negation, we use strings kecuali “except”,\nyang bukan “which is not”, and yang tidak “which is not”.\n12366\n5 Discussion\nIf LLMs are to be deployed in diverse contexts,\nit is critical to have more work on evaluation for\ndifferent languages and cultures. In Table 1 we ob-\nserved that the models struggle to answer questions\nthat pertain to local languages and cultures across\nall levels of education in Indonesia. Minangkabau\nculture in particular is taught and assessed in the In-\ndonesian language, and yet the limited performance\nin answering questions relating to it underscores\na lack of cultural knowledge, despite reasonable\nresults for the Indonesian language.\nWe also argue that education science should\nplay a more central role in the future evaluation\nof LLMs. Current NLP work has mostly focused\non developing larger models with different tech-\nniques and architectures, and evaluation has pri-\nmarily been in terms of specific NLP tasks. Educa-\ntion science has decades of experience in designing\nassessments to evaluate student progress through\npainstakingly-designed comprehensive tests, which\nthe NLP community should better engage with.\nWith IndoMMLU, we have shown that exam ques-\ntions across fine-grained educational levels offer\na more profound comprehension of model profi-\nciency in the Indonesian language, while also re-\nvealing potential areas for improvement.\n6 Conclusion\nIn this paper, we presented IndoMMLU, a multi-task\nlanguage understanding benchmark for real-world\nevaluation of knowledge in the Indonesian con-\ntext. By leveraging education level metadata, we\nfound that current LLMs like GPT-3.5 are only\nable to pass primary school exams in Indonesia,\nwhile smaller models struggle across nearly in all\neducation levels. Notably, none of the 24 evalu-\nated models perform well in the domain of local\nlanguages and cultures, highlighting the need for\nfurther research in this direction.\nLimitations\nDespite being the largest question-answering\ndataset in the Indonesian context, IndoMMLU still\nhas some limitations, in that it lacks: (1) multi-\nmodal questions; (2) arithmetic reasoning tasks;\nand (3) essay-style questions. First, IndoMMLU is\ncomprised solely of text-based questions, and ques-\ntions with tables and figures are discarded to sim-\nplify data collection. We specifically exclude math\nquestions as they are already well covered by ex-\nisting English math reasoning benchmarks. We\nsuggest that essay questions enable a deeper assess-\nment of comprehension and critical thinking, but\nthat methods for evaluating essay quality across\neducation levels in languages other than English\nare severely lacking.\nEthical Considerations\nThe IndoMMLU dataset used in our study is collected\nfrom publicly-available web resources. In compli-\nance with the Indonesian Copyright Law number\n28 year 2014, specifically article 44, the use, re-\ntrieval, reproduction, and/or modification of works\nand/or related rights products, in whole or in sub-\nstantial part, is not considered a copyright infringe-\nment if the source is fully cited or mentioned for\neducational and research purposes.11\nRegarding our experimental results, it is impor-\ntant to note that they do not provide a definitive\nanswer as to the relative abilities of LLMs, and we\ncaution readers against overinterpreting the find-\nings. While we conclude that GPT-3.5 demon-\nstrates proficiency in passing primary school exams\nin Indonesia based on IndoMMLU, it is essential to\nconsider potential contamination in GPT-3.5’s pre-\ntraining data, which could impact the results. Fur-\nthermore, it is worth noting that real-world student\nassessments encompass not only multiple-choice\nquestions but also practical exams, laboratory work,\nand essay writing.\nReferences\nRazieh Baradaran, Razieh Ghiasi, and Hossein\nAmirkhani. 2022. A survey on machine reading com-\nprehension systems. Natural Language Engineering,\n28(6):683–732.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. PIQA: Reasoning about physical com-\nmonsense in natural language. In Proceedings of\nthe AAAI conference on artificial intelligence, pages\n7432–7439.\nSamuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji,\nGenta Indra Winata, Bryan Wilie, Fajri Koto, Rah-\nmad Mahendra, Christian Wibisono, Ade Romad-\nhony, Karissa Vincentio, Jennifer Santoso, David\nMoeljadi, Cahya Wirawan, Frederikus Hudi, Muham-\nmad Satrio Wicaksono, Ivan Halim Parmonangan,\nIka Alfina, Ilham Firdausi Putra, Samsul Rahmadani,\nYulianti Oenang, Ali Akbar Septiandri, James Jaya,\n11https://wipolex-res.wipo.int/edocs/lexdocs/\nlaws/en/id/id064en.pdf\n12367\nKaustubh Dhole, Arie Suryani, Rifki Afina Putri,\nDan Su, Keith David Stevens, Made Nindyatama\nNityasya, Muhammad Farid Adilazuarda, Ryan Ig-\nnatius Hadiwijaya, Ryandito Diandaru, Tiezheng Yu,\nVito Ghifari, Wenliang Dai, Yan Xu, Dyah Inastra\nDamapuspita, Haryo Akbarianto Wibowo, Cuk Tho,\nIchwanul Muslim Karo Karo, Tirana Noor Fatyanosa,\nZiwei Ji, Graham Neubig, Timothy Baldwin, Sebas-\ntian Ruder, Pascale Fung, Herry Sujaini, Sakriani\nSakti, , and Ayu Purwarianti. 2023. NusaCrowd:\nOpen source initiative for Indonesian NLP resources.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023.\nSamuel Cahyawijaya, Genta Indra Winata, Bryan Wilie,\nKarissa Vincentio, Xiaohong Li, Adhiguna Kun-\ncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Ba-\nhar, Masayu Khodra, Ayu Purwarianti, and Pascale\nFung. 2021. IndoNLG: Benchmark and resources for\nevaluating Indonesian natural language generation.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n8875–8898, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nIlias Chalkidis. 2023. ChatGPT may pass the bar exam\nsoon, but has a long way to go for the LexGLUE\nbenchmark. arXiv preprint arXiv:2304.12202.\nJonathan H Choi, Kristin E Hickman, Amy Monahan,\nand Daniel Schwarcz. 2023. ChatGPT goes to law\nschool. Available at SSRN.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? Try ARC, the AI2 reasoning challenge.\narXiv preprint arXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khy-\nathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin\nDurmus, Ond ˇrej Dušek, Chris Chinenye Emezue,\nVarun Gangal, Cristina Garbacea, Tatsunori\nHashimoto, Yufang Hou, Yacine Jernite, Harsh Jham-\ntani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv\nKumar, Faisal Ladhak, Aman Madaan, Mounica\nMaddela, Khyati Mahajan, Saad Mahamood, Bod-\nhisattwa Prasad Majumder, Pedro Henrique Martins,\nAngelina McMillan-Major, Simon Mille, Emiel van\nMiltenburg, Moin Nadeem, Shashi Narayan, Vitaly\nNikolaev, Andre Niyongabo Rubungo, Salomey\nOsei, Ankur Parikh, Laura Perez-Beltrachini,\nNiranjan Ramesh Rao, Vikas Raunak, Juan Diego\nRodriguez, Sashank Santhanam, João Sedoc,\nThibault Sellam, Samira Shaikh, Anastasia Shimo-\nrina, Marco Antonio Sobrevilla Cabezudo, Hendrik\nStrobelt, Nishant Subramani, Wei Xu, Diyi Yang,\nAkhila Yerukola, and Jiawei Zhou. 2021. The\nGEM benchmark: Natural language generation,\nits evaluation and metrics. In Proceedings of the\n1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021), pages 96–120,\nOnline. Association for Computational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. In Proceedings of ICML 2020.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos QA: Machine reading\ncomprehension with contextual commonsense rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n2391–2401, Hong Kong, China. Association for Com-\nputational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\n12368\nRabeeh Karimi Mahabadi, Luke Zettlemoyer, James\nHenderson, Lambert Mathias, Marzieh Saeidi,\nVeselin Stoyanov, and Majid Yazdani. 2022. Prompt-\nfree and efficient few-shot learning with language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3638–3652, Dublin,\nIreland. Association for Computational Linguistics.\nDaniel Martin Katz, Michael James Bommarito, Shang\nGao, and Pablo Arredondo. 2023. GPT-4 passes the\nbar exam. Available at SSRN 4389233.\nFajri Koto, Timothy Baldwin, and Jey Han Lau. 2022a.\nCloze evaluation for deeper understanding of com-\nmonsense stories in Indonesian. In Proceedings of\nthe First Workshop on Commonsense Representation\nand Reasoning (CSRR 2022) , pages 8–16, Dublin,\nIreland. Association for Computational Linguistics.\nFajri Koto, Timothy Baldwin, and Jey Han Lau. 2022b.\nLipKey: A large-scale news dataset for absent\nkeyphrases generation and abstractive summariza-\ntion. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 3427–\n3437, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nFajri Koto and Ikhwan Koto. 2020. Towards computa-\ntional linguistics in Minangkabau language: Studies\non sentiment analysis and machine translation. In\nProceedings of the 34th Pacific Asia Conference on\nLanguage, Information and Computation, pages 138–\n148, Hanoi, Vietnam. Association for Computational\nLinguistics.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2020a.\nLiputan6: A large-scale Indonesian dataset for text\nsummarization. In Proceedings of the 1st Confer-\nence of the Asia-Pacific Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 598–608, Suzhou, China. Association\nfor Computational Linguistics.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2021.\nIndoBERTweet: A pretrained language model for\nIndonesian Twitter with effective domain-specific vo-\ncabulary initialization. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10660–10668, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nFajri Koto, Afshin Rahimi, Jey Han Lau, and Timo-\nthy Baldwin. 2020b. IndoLEM and IndoBERT: A\nbenchmark dataset and pre-trained language model\nfor Indonesian NLP. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 757–770, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics.\nFajri Koto and Gemala Y Rahmaningtyas. 2017. Inset\nlexicon: Evaluation of a word list for Indonesian sen-\ntiment analysis in microblogs. In 2017 International\nConference on Asian Language Processing (IALP),\npages 391–394. IEEE.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 785–\n794, Copenhagen, Denmark. Association for Compu-\ntational Linguistics.\nHaonan Li, Fajri Koto, Minghao Wu, Alham Fikri\nAji, and Timothy Baldwin. 2023a. Bactrian-\nX: A multilingual replicable instruction-following\nmodel with low-rank adaptation. arXiv preprint\narXiv:2305.15011.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai\nZhao, Yeyun Gong, Nan Duan, and Timothy Bald-\nwin. 2023b. CMMLU: Measuring massive multitask\nlanguage understanding in chinese. arXiv preprint\narXiv:2306.09212.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. XGLUE: A new\nbenchmark dataset for cross-lingual pre-training, un-\nderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nChen Cecilia Liu, Fajri Koto, Timothy Baldwin, and\nIryna Gurevych. 2023a. Are multilingual llms\nculturally-diverse reasoners? an investigation into\nmulticultural proverbs and sayings. arXiv preprint\narXiv:2309.08591.\nChuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu\nDong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng,\nPeiyi Zhang, Qingqing Lyu, et al. 2023b. M3KE: A\nmassive multi-level multi-subject knowledge evalu-\nation benchmark for chinese large language models.\narXiv preprint arXiv:2305.10263.\n12369\nSwaroop Mishra, Arindam Mitra, Neeraj Varshney,\nBhavdeep Sachdeva, Peter Clark, Chitta Baral, and\nAshwin Kalyan. 2022. NumGLUE: A suite of funda-\nmental yet challenging mathematical reasoning tasks.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3505–3523, Dublin, Ireland.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nJoseph Novak. 1988. Learning science and the science\nof learning. Studies in Science Education, 15(1):77–\n101.\nOpenAI. 2023. GPT-4 technical report. ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: Outperforming curated corpora\nwith web data, and web data only. arXiv preprint\narXiv:2306.01116.\nAyu Purwarianti and Ida Ayu Putu Ari Crisdayanti. 2019.\nImproving bi-LSTM performance for Indonesian sen-\ntiment analysis using paragraph vector. In 2019 Inter-\nnational Conference of Advanced Informatics: Con-\ncepts, Theory and Applications (ICAICTA) , pages\n1–5. IEEE.\nAyu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nak-\nagawa. 2007. A machine learning approach for In-\ndonesian question answering system. In Artificial\nIntelligence and Applications, pages 573–578.\nRifki Afina Putri and Alice Oh. 2022. IDK-MRC: Unan-\nswerable questions for Indonesian machine reading\ncomprehension. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6918–6933, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-\ndhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie\nHu, Dan Garrette, Graham Neubig, and Melvin John-\nson. 2021. XTREME-R: Towards more challenging\nand nuanced multilingual evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 10215–10245,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nMargaret Ryznar. 2023. Exams in the time of ChatGPT.\nWashington and Lee Law Review Online, 80(5):305.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial Winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99–106.\nMei Silviana Saputri, Rahmad Mahendra, and Mirna\nAdriani. 2018. Emotion classification on Indonesian\nTwitter dataset. In 2018 International Conference\non Asian Language Processing (IALP), pages 90–95.\nIEEE.\nNeha Sengupta, Sunil Kumar Sahu, Bokang Jia,\nSatheesh Katipomu, Haonan Li, Fajri Koto,\nOsama Mohammed Afzal, Samta Kamboj, Onkar\nPandit, Rahul Pal, et al. 2023. Jais and jais-chat:\nArabic-centric foundation and instruction-tuned open\ngenerative large language models. arXiv preprint\narXiv:2308.16149.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Boyd-Graber, and Li-\njuan Wang. 2022. Prompting GPT-3 to be reliable.\narXiv preprint arXiv:2210.09150.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. LLaMA: Open and ef-\nficient foundation language models. arXiv preprint\narXiv:2302.13971.\nThinh Truong, Timothy Baldwin, Trevor Cohn, and\nKarin Verspoor. 2022. Improving negation detection\nwith negation-focused pre-training. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4188–4193,\nSeattle, United States. Association for Computational\nLinguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. SuperGLUE: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\n12370\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv\npreprint arXiv:2203.11171.\nBryan Wilie, Karissa Vincentio, Genta Indra Winata,\nSamuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,\nSidik Soleman, Rahmad Mahendra, Pascale Fung,\nSyafri Bahar, and Ayu Purwarianti. 2020. IndoNLU:\nBenchmark and resources for evaluating Indonesian\nnatural language understanding. In Proceedings of\nthe 1st Conference of the Asia-Pacific Chapter of the\nAssociation for Computational Linguistics and the\n10th International Joint Conference on Natural Lan-\nguage Processing, pages 843–857, Suzhou, China.\nAssociation for Computational Linguistics.\nGenta Indra Winata, Alham Fikri Aji, Samuel Cahyawi-\njaya, Rahmad Mahendra, Fajri Koto, Ade Romad-\nhony, Kemal Kurniawan, David Moeljadi, Radi-\ntyo Eko Prasojo, Pascale Fung, Timothy Baldwin,\nJey Han Lau, Rico Sennrich, and Sebastian Ruder.\n2023. NusaX: Multilingual parallel sentiment dataset\nfor 10 Indonesian local languages. In Proceedings\nof the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, pages\n815–834, Dubrovnik, Croatia. Association for Com-\nputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791–4800, Florence,\nItaly. Association for Computational Linguistics.\n12371\nA Data Statistics\nTable 6, Table 7, and Table 8 provide detailed statis-\ntics of the question distribution in IndoMMLU.\nSubjects SD SMP SMA UE Total\nScience 488 680 – – 1168\nPhysics – – 297 – 297\nChemistry – – 287 398 685\nBiology – – 457 388 845\nSocial science 300 299 – – 599\nGeography – – 196 294 490\nSociology – – 295 201 496\nEconomics – – 296 192 488\nHistory – – 300 198 498\nCivics 99 300 300 – 699\nIndonesian language 1125 850 857 381 3213\nBalinese 200 123 148 – 471\nMakassarese 98 41 47 – 186\nBanjarese 120 10 14 – 144\nLampungic 93 30 24 – 147\nMadurese 100 93 102 – 295\nSundanese 718 294 145 – 1157\nJavanese 396 298 298 – 992\nDayak Ngaju 109 – – – 109\nMinangkabau culture 153 46 – – 199\nArt 200 200 201 – 601\nSports 49 49 50 – 148\nIslam religion 201 202 300 – 703\nChristian religion 50 49 102 – 201\nHindu religion 49 52 49 – 150\nTotal 4548 3616 4765 2052 14981\nTable 6: Total number of questions for each subject\narea and education level. “SD”, “SMP”, “SMA”, “UE”\nindicate primary school, junior high school, senior high\nschool, and university entrance tests, respectively.\nClass #questions\n1 200\n2 150\n3 195\n4 187\n5 196\n6 197\n7 282\n8 291\n9 277\n10 295\n11 288\n12 274\n12+ 381\nTotal 3213\nTable 7: Total number of questions in the Indonesian\nlanguage subject, including those designated for univer-\nsity entrance tests (12+).\nCategory #question\nSTEM 2995\nSocial science 2772\nHumanities 2301\nIndonesian language 3213\nLocal languages and cultures 3700\nTotal 14981\nTable 8: Total number of questions based on subject\nareas.\nB Few-shot Prompt\nIni adalah beberapa contoh soal\n[SUBJECT].\n[Example-1]\nJawaban: [Answer-1]\n[Example-2]\nJawaban: [Answer-2]\n[Example-3]\nJawaban: [Answer-3]\n[QUESTION]\nJawaban: \nThese are several examples of\n[SUBJECT] question.\n[Example-1]\nAnswer: [Answer-1]\n[Example-2]\nAnswer: [Answer-2]\n[Example-3]\nAnswer: [Answer-3]\n[QUESTION]\nAnswer: \nFigure 10: Illustration of our few-shot prompt template.\nThe English translation on the right is solely for illus-\ntrative purposes. In our experiments, we used up to\nthree examples within the prompt. The placeholders\n[SUBJECT], Example-i, Answer-i, and QUESTION cor-\nrespond to the subject, the i-th question example, the\nanswer key for the i-th question example, and the main\nquestion, respectively.\n12372\nC Zero-shot Performance Based on the Probability of the Full Generated Answer\nModel (#parameters) STEMSocial HumanitiesIndonesian Local languagesAverageScience Language and Cultures\nRandom 21.9 23.4 23.5 24.4 26.6 24.4\nXGLM (564M) 24.2 25.9 27.2 29.0 27.8 26.8\nXGLM (1.7B) 23.7 25.4 27.1 28.4 28.9 26.9\nXGLM (2.9B) 23.6 25.4 28.3 28.8 28.8 26.9\nXGLM (4.5B) 23.9 25.5 29.4 27.9 28.1 27.2\nXGLM (7.5B) 23.5 26.0 29.4 28.6 28.9 27.6\nFalcon (7B) 22.2 25.8 28.4 30.1 27.9 26.8\nFalcon (40B) 25.8 28.4 29.5 32.9 27.7 28.2\nBLOOMZ (560M) 23.0 24.4 23.7 27.2 26.4 24.9\nBLOOMZ (1.1B) 22.9 25.8 26.6 28.3 27.4 26.2\nBLOOMZ (1.7B) 23.7 29.8 29.7 32.8 28.1 28.3\nBLOOMZ (3B) 27.6 32.5 32.6 35.0 27.4 30.0\nBLOOMZ (7.1B) 26.8 32.9 33.5 36.5 28.1 30.5\nmT0small(300M) 24.0 26.1 27.0 29.8 30.8 27.8\nmT0base(580M) 23.9 25.5 27.6 30.1 30.5 27.7\nmT0large(1.2B) 25.1 27.5 27.9 33.6 29.6 28.2\nmT0xl(3.7B) 28.5 36.1 35.3 40.7 34.3 34.2\nmT0xxl(13B) 30.1 38.1 40.9 43.2 34.5 36.4\nLLamA (7B) 23.7 25.6 28.0 29.0 28.3 27.0\nLLamA (13B) 24.0 25.4 27.7 29.4 29.6 27.4\nLLamA (30B) 24.3 26.4 29.5 29.8 28.5 27.7\nLLamA (65B) 26.7 29.3 32.4 32.9 29.0 29.7\nBactrian-X-LLamA (7B) 23.8 25.4 28.7 29.8 28.0 27.0\nBactrian-X-LLamA (13B) 25.6 27.4 29.2 30.7 27.9 27.8\nTable 9: Zero-shot performance (% accuracy) of large language models based on the probability of the full\ngenerated answer, aggregated across education levels. “Average” means the average across all subject areas in\nIndoMMLU.\nD Model Artifacts\nModels (#parameters) Source\nXGLM (564M) facebook/xglm-564M\nXGLM (1.7B) facebook/xglm-1.7B\nXGLM (2.9B) facebook/xglm-2.9B\nXGLM (4.5B) facebook/xglm-4.5B\nXGLM (7.5B) facebook/xglm-7.5B\nFalcon (7B) tiiuae/falcon-7b\nFalcon (40B) tiiuae/falcon-40b\nBLOOMZ (560M) bigscience/bloomz-560m\nBLOOMZ (1.1B) bigscience/bloomz-1b1\nBLOOMZ (1.7B) bigscience/bloomz-1b7\nBLOOMZ (3B) bigscience/bloomz-3b\nBLOOMZ (7.1B) bigscience/bloomz-7b1\nmT0small(300M) bigscience/mt0-small\nmT0base(580M) bigscience/mt0-base\nmT0large(1.2B) bigscience/mt0-large\nmT0xl(3.7B) bigscience/mt0-xl\nmT0xxl(13B) bigscience/mt0-xxl\nLLamA (7B) decapoda-research/llama-7b-hf\nLLamA (13B) decapoda-research/llama-13b-hf\nLLamA (30B) decapoda-research/llama-30b-hf\nLLamA (65B) huggyllama/llama-65b\nBactrian-X-LLamA (7B)MBZUAI/bactrian-x-llama-7b-lora\nBactrian-X-LLamA (13B)MBZUAI/bactrian-x-llama-13b-lora\nTable 10: With the exception of GPT–3.5 (Ouyang et al., 2022), all the models used in this study were sourced from\nHuggingface (Wolf et al., 2020).\n12373\nE Full Results in Each Subject and Education Level in GPT-3.5, mT0, and BLOOMZ\n0 10 20 30 40 50 60 70 80\nScience (SD)\nSocial science (SD)\nCivic education (SD)\nIndonesian language (SD)\nSport (SD)\nArt (SD)\nBalinese (SD)\nMakassarese (SD)\nBanjarese (SD)\nLampungic (SD)\nMadurese (SD)\nSundanese (SD)\nJavanese (SD)\nDayak language (SD)\nMinangkabau culture (SD)\nChristian religion (SD)\nIslam religion (SD)\nHindu religion (SD)\nScience (SMP)\nSocial science (SMP)\nCivic education (SMP)\nIndonesian language (SMP)\nSport (SMP)\nArt (SMP)\nBalinese (SMP)\nMakassarese (SMP)\nBanjarese (SMP)\nLampungic (SMP)\nMadurese (SMP)\nSundanese (SMP)\nJavanese (SMP)\nMinangkabau culture (SMP)\nChristian religion (SMP)\nIslam religion (SMP)\nHindu religion (SMP)\nPhysics (SMA)\nChemistry (SMA)\nBiology (SMA)\nGeography (SMA)\nSociology (SMA)\nEconomy (SMA)\nHistory (SMA)\nCivic education (SMA)\nIndonesian language (SMA)\nSport (SMA)\nArt (SMA)\nBalinese (SMA)\nMakassarese (SMA)\nBanjarese (SMA)\nLampungic (SMA)\nMadurese (SMA)\nSundanese (SMA)\nJavanese (SMA)\nChristian religion (SMA)\nIslam religion (SMA)\nHindu religion (SMA)\nIndonesian language (UE)\nChemistry (UE)\nBiology (UE)\nGeography (UE)\nSociology (UE)\nEconomy (UE)\nHistory (UE)\nmT0xxl\nGPT-3.5\nBLOOMZ\nFigure 11: Performance (% accuracy) breakdown across the 64 tasks. “SD”, “SMP”, “SMA”, “UE” indicate primary\nschool, junior high school, senior high school, and university entrance tests, respectively. The red vertical line\ndenotes random performance.\n12374",
  "topic": "Indonesian",
  "concepts": [
    {
      "name": "Indonesian",
      "score": 0.9516388177871704
    },
    {
      "name": "Computer science",
      "score": 0.6739264130592346
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6679675579071045
    },
    {
      "name": "Task (project management)",
      "score": 0.6488267183303833
    },
    {
      "name": "Test (biology)",
      "score": 0.6142184138298035
    },
    {
      "name": "Natural language processing",
      "score": 0.5104240775108337
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4443928599357605
    },
    {
      "name": "Work (physics)",
      "score": 0.4443582594394684
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4263922870159149
    },
    {
      "name": "Mathematics education",
      "score": 0.4169744551181793
    },
    {
      "name": "Linguistics",
      "score": 0.29528847336769104
    },
    {
      "name": "Psychology",
      "score": 0.2562316060066223
    },
    {
      "name": "Geography",
      "score": 0.09749758243560791
    },
    {
      "name": "Engineering",
      "score": 0.09018278121948242
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Cartography",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}