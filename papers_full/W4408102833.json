{
  "title": "Automatic recognition of cross-language classic entities based on large language models",
  "url": "https://openalex.org/W4408102833",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2275677837",
      "name": "Qiankun Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152366482",
      "name": "Yutong Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096345423",
      "name": "Dongbo Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144990891",
      "name": "Shui-qing Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2946260552",
    "https://openalex.org/W4402670730",
    "https://openalex.org/W4403098592",
    "https://openalex.org/W6601375265",
    "https://openalex.org/W2123167375",
    "https://openalex.org/W4401023518",
    "https://openalex.org/W4402670253",
    "https://openalex.org/W4402671928",
    "https://openalex.org/W4385573120",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4389524317",
    "https://openalex.org/W6856398428",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2155524176",
    "https://openalex.org/W1584308190"
  ],
  "abstract": "Abstract Large language models (LLMs) hold immense potential for the intelligent processing of classical texts. They offer new approaches for digital research on classical literature resources, cross-linguistic understanding, text knowledge mining, and the promotion and preservation of cultural heritage. To explore the performance of named entity recognition (NER) tasks supported by LLMs, this study first fine-tuned four LLMs—Xunzi-Baichuan, Baichuan2-7B-Base, Xunzi-GLM, and ChatGLM3-6B—using supervised fine-tuning methods based on open-source models. Zero-shot, one-shot, and few-shot prompting methods were then employed to validate the performance of these models in the NER tasks. Finally, the applicability of fine-tuning LLMs in specific domains for NER tasks was examined using BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, precision, recall, and F1 scores as evaluation metrics for model performance and applicability. The experimental results indicated that fine-tuned LLMs achieved high scores across multiple metrics, demonstrating strong performance in text generation. In entity extraction, the Xunzi-Baichuan model performed optimally across several metrics and also exhibited generalization capabilities. In addition, we have open-sourced our models for community research. https://github.com/Xunzi-LLM-of-Chinese-classics/XunziALLM .",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6288459897041321
    },
    {
      "name": "Natural language processing",
      "score": 0.5077984929084778
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4022141695022583
    },
    {
      "name": "Linguistics",
      "score": 0.38185036182403564
    },
    {
      "name": "Philosophy",
      "score": 0.06050211191177368
    }
  ],
  "institutions": [],
  "cited_by": 1
}