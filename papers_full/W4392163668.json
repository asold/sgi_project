{
    "title": "Neural machine translation of clinical text: an empirical investigation into multilingual pre-trained language models and transfer-learning",
    "url": "https://openalex.org/W4392163668",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5047560778",
            "name": "Lifeng Han",
            "affiliations": [
                "University of Manchester"
            ]
        },
        {
            "id": "https://openalex.org/A5020735272",
            "name": "Serge Gladkoff",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5043109148",
            "name": "Gleb Erofeev",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5073719890",
            "name": "Irina Sorokina",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5093487462",
            "name": "Betty Galiano",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5005912060",
            "name": "Goran Nenadić",
            "affiliations": [
                "University of Manchester"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3044533399",
        "https://openalex.org/W2404369708",
        "https://openalex.org/W3211056521",
        "https://openalex.org/W2979250794",
        "https://openalex.org/W2095931818",
        "https://openalex.org/W3013605954",
        "https://openalex.org/W3165345393",
        "https://openalex.org/W3194921886",
        "https://openalex.org/W3107124456",
        "https://openalex.org/W3128580531",
        "https://openalex.org/W4210592931",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3144451117",
        "https://openalex.org/W2130413976",
        "https://openalex.org/W2095896451",
        "https://openalex.org/W2111063183",
        "https://openalex.org/W3131999495",
        "https://openalex.org/W2884488303",
        "https://openalex.org/W6674286886",
        "https://openalex.org/W6713634263",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W4214837599",
        "https://openalex.org/W6784447870",
        "https://openalex.org/W4389584441",
        "https://openalex.org/W3164871893",
        "https://openalex.org/W4229335055",
        "https://openalex.org/W4226536126",
        "https://openalex.org/W3093871477",
        "https://openalex.org/W3135877859",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Clinical text and documents contain very rich information and knowledge in healthcare, and their processing using state-of-the-art language technology becomes very important for building intelligent systems for supporting healthcare and social good. This processing includes creating language understanding models and translating resources into other natural languages to share domain-specific cross-lingual knowledge. In this work, we conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three sub-tasks including (1) clinical case (CC), (2) clinical terminology (CT), and (3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) outperformed the other two extra-large language models by a large margin in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method works well in our experimental setting using the WMT21fb model to accommodate a new language space Spanish that was not seen at the pre-training stage within WMT21fb itself, which deserves more exploitation for clinical knowledge transformation, e.g. to investigate into more languages. These research findings can shed some light on domain-specific machine translation development, especially in clinical and healthcare fields. Further research projects can be carried out based on our work to improve healthcare text analytics and knowledge transformation. Our data is openly available for research purposes at: https://github.com/HECTA-UoM/ClinicalNMT .",
    "full_text": "EDITED BY\nPatrick Ruch,\nGeneva School of Business Administration,\nSwitzerland\nREVIEWED BY\nMd Adnanul Islam,\nMonash University, Australia\nNona Naderi,\nUniversite Paris-Saclay, France\n*\nCORRESPONDENCE\nLifeng Han\nLifeng.Han@manchester.ac.uk\nSerge Gladkoff\nserge.gladkoff@logrusglobal.com\nGoran Nenadic\ng.nenadic@manchester.ac.uk\nRECEIVED 24 April 2023\nACCEPTED 12 January 2024\nPUBLISHED 26 February 2024\nCITATION\nHan L, Gladkoff S, Erofeev G, Sorokina I,\nGaliano B and Nenadic G (2024) Neural\nmachine translation of clinical text: an\nempirical investigation into multilingual\npre-trained language models and\ntransfer-learning.\nFront. Digit. Health 6:1211564.\ndoi: 10.3389/fdgth.2024.1211564\nCOPYRIGHT\n© 2024 Han, Gladkoff, Erofeev, Sorokina,\nGaliano and Nenadic. This is an open-access\narticle distributed under the terms of the\nCreative Commons Attribution License (CC\nBY). The use, distribution or reproduction in\nother forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nNeural machine translation of\nclinical text: an empirical\ninvestigation into multilingual\npre-trained language models and\ntransfer-learning\nLifeng Han\n1*, Serge Gladkoff\n2*, Gleb Erofeev\n2\n, Irina Sorokina\n2\n,\nBetty Galiano\n3\nand Goran Nenadic\n1*\n1Department of Computer Science, The University of Manchester, Manchester, United Kingom,2AI Lab,\nLogrus Global, Translation & Localization, Philadelphia, PA, United States,3Management Department,\nOcean Translations, Rosario, Argentina\nClinical text and documents contain very rich information and knowledge in\nhealthcare, and their processing using state-of-the-art language technology\nbecomes very important for building intelligent systems for supporting\nhealthcare and social good. This processing includes creating language\nunderstanding models and translating resources into other natural languages\nto share domain-speciﬁc cross-lingual knowledge. In this work, we conduct\ninvestigations on clinical text machine translation by examining multilingual\nneural network models using deep learning such as Transformer based\nstructures. Furthermore, to address the language resource imbalance issue, we\nalso carry out experiments using a transfer learning methodology based on\nmassive multilingual pre-trained language models (MMPLMs). The\nexperimental results on three sub-tasks including (1) clinical case (CC), (2)\nclinical terminology (CT), and (3) ontological concept (OC) show that our\nmodels achieved top-level performances in the ClinSpEn-2022 shared task on\nEnglish-Spanish clinical domain data. Furthermore, our expert-based human\nevaluations demonstrate that the small-sized pre-trained language model\n(PLM) outperformed the other two extra-large language models by a large\nmargin in the clinical domainﬁne-tuning, which ﬁnding was never reported in\nthe ﬁeld. Finally, the transfer learning method works well in our experimental\nsetting using the WMT21fb model to accommodate a new language space\nSpanish that was not seen at the pre-training stage within WMT21fb itself,\nwhich deserves more exploitation for clinical knowledge transformation, e.g.\nto investigate into more languages. These researchﬁndings can shed some\nlight on domain-speci ﬁc machine translation development, especially in\nclinical and healthcare ﬁelds. Further research projects can be carried out\nbased on our work to improve healthcare text analytics and knowledge\ntransformation. Our data is openly available for research purposes at:\nhttps://github.com/HECTA-UoM/ClinicalNMT.\nKEYWORDS\nNeural machine translation, clinical text translation, multilingual pre-trained language\nmodel, large language model, transfer learning, clinical knowledge transformation,\nSpanish-English translation\nTYPE Original Research\nPUBLISHED 26 February 2024\n| DOI 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 01 frontiersin.org\n1 Introduction\nHealthcare Text Analytics (HECTA) have gained more\nattention nowadays from researchers across different disciplines,\ndue to their impact on clinical treatment, decision-making,\nhospital operation, and their recently empowered capabilities.\nThese developments have much to do with the latest\ndevelopment of powerful language models (LMs), advanced\nmachine-learning (ML) technologies, and increasingly available\ndigital healthcare data from social media (1–3), and discharged\noutpatient letters from hospital settings (4–6).\nIntelligent healthcare systems have been deployed in some\nhospitals to support clinicians ’ diagnoses and decision-making\nregarding patients and problems (7, 8). Such usages include key\ninformation extraction (IE) from electronic health records\n(EHRs), normalisation to medical terminologies, knowledge\ngraph (KG) construction, and relation extraction (RE) between\nsymptoms (problems), diagnoses, treatments, and adverse drug\nevents (9, 10). Some of these digital healthcare systems can also\nhelp patients self-diagnose in situations where no General\nPractitioners (GPs) and professional doctors are available (11, 12).\nHowever, due to the language barriers and inequality of digital\nresources across languages, there is an urgent need for knowledge\ntransformation, such as from one human language to another\n(13, 14). Thus, to help address digital health inequalities, machine\ntranslation (MT) technologies can be of good use in this case.\nMT is one of the earliest artiﬁcial intelligence (AI) branches\ndating back to the 1950s, and it has gained a boom with other\nnatural language processing (NLP) tasks in recent years due to\nthe newly designed powerful learning model Transformers\n(15–18). Several attention mechanisms designed in Transformer\ndeep neural models are proven to be capable of better learning\nfrom a large amount of available digital data compared to\ntraditional statistical and neural network-based models (19–21).\nIn this work, we investigate the state-of-the-art Transformer\nbased Neural MT (NMT) models regarding clinical domain text\ntranslation, to facilitate digital healthcare and knowledge\ntransformation with the work ﬂow drawn in Figure 1 . Being\naware of some current development in the competition of\nlanguage model sizes in NLPﬁeld, we set up the following base\nmodels for comparison study: (1) a small-sized multilingual pre-\ntrained language model (s-MPLM) Marian, which was developed\nby researchers at the Adam Mickiewicz University in Poznan and\nat the NLP group in University of Edinburgh (22, 23); and (2) a\nmassive-sized multilingual pre-trained LM (MMPLM/xL-MPLM)\nNLLB, developed by Meta-AI covering more than 200 languages\n(13). In addition to this, we set up a third model to investigate\nthe possibility of transfer learning in the clinical domain MT: (3)\nthe WMT21fb model which is another MMPLM from Meta-AI\nbut with a limited amount of pre-trained language pairs\nincluding from English to Czech, German, Hausa, Icelandic,\nJapanese, Russian, and Chinese, and the opposite (24).\nThe testing language pairs of these translation models in our\nwork are English $Spanish. There aren\n’t other language pairs\nof openly available resources in the clinical domain MT as far as\nwe know. We use the international shared task challenge data\nfrom ClinSpEn2022 “clinical domain Spanish-English MT 2022”\nfor this purpose. 1 ClinSpEn2022 was a sub-task of the\nBioMedical MT track at WMT2022 ( 25). There are three\ntranslation tasks inside ClinSpEn2022 including (i) clinical cases\nreport; (ii) clinical terms, and (3) ontological concepts from the\nbiomedical domain.\nRegarding the evaluation of these LMs, we used the evaluation\nplatform offered by ClinSpEn2022 shared task including several\nautomatic metrics such as BLEU, METEOR, ROUGE, COMET.\nHowever, the automatic evaluation results did not give apparent\ndifferentiation between models on some tasks. Furthermore, there\nare issues like in-consistency regarding model ranking across\nautomatic metrics. To address these issues and give a high-\nquality evaluation, we carried out an expert-based human\nevaluation step on three models using outputs of Task one\n“clinical case report”.\nOur experimental investigation shows that (1) the extra-large\nMMPLM does not necessarily win the small-sized MPLM on\nclinical domain MT via ﬁne-tuning; (2) our transfer-learning\nmodel works successfully for clinical domain MT task on\nlanguage pairs that were not pre-trained upon but using ﬁne-\ntuning. The ﬁrst ﬁnding can shed some light on the research\nﬁeld that in clinical domain-speciﬁc MT, it is worthy to carry\nout more work on data cleaning and ﬁne-tuning rather than\nbuilding extra large LMs. Our second ﬁnding tells us the\ncapability of MMPLMs in generating a new language pair\nknowledge space for translating clinical domain text even though\nthis language pair was unseen in the pre-training stage with our\nexperimental settings. This can be useful to low-resource NLP,\nsuch as the work by (26, 27).\n2\nThe rest of this article is organised as below: Section2. surveys\nthe related work to ours including clinical domain MT and\nNLP, large LMs, and transfer learning. Section3. details the three\nLMs we deployed for comparison study. Section4. introduces the\nexperimental work we carried out and automatic evaluation\noutcomes. Section 5. follows up with expert-based human\nevaluation and the results. Finally, Section6. concludes our work\nwith discussion.\n2 Related work\nApplying NLP models to clinical healthcare has attracted much\nattention of many researchers, such as the work on disease status\nprediction using discharged summaries by Yang et al. ( 31),\ntemporal expressions and events extraction from clinical narratives\nusing combined methods of rules and machine learning by\nKovačević et al. ( 32), using knowledge-based and data-driven\n1https://codalab.lisn.upsaclay.fr/competitions/6696\n2This paper reports systematic investigation ﬁndings based upon the\npreliminary work from (28–30)\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 02 frontiersin.org\nmethods for de-identiﬁcation task in clinical narratives by Dehghan\net al. (33), systematic reviews on clinical text mining and healthcare\nby Spasic et al. (5) and Elbattah and Dequen (34), etc.\nHowever, using MT to help translate clinical text for\nknowledge transformation and help clinical decision-making is\nstill a rising topic (14), even though it has been proven to be\nuseful in the history for assistinghealth communication especially\nwith post-editing strategies (35). This is partial because of the\nsensitive domain and high risk in clinical settings (36). Some of\nthe recent progress on using MT for clinical text includes the\nwork by Soto et al. ( 37) which leverages SNOMED-CT terms\n(38) and relations for MT between Basque and Spanish\nlanguages, Mujjiga et al. ( 39) which applies NMT model to\nidentify semantic concepts in “abundant interchangeable words”\nin clinical domain and their experimental result shows NMT\nmodel can greatly improve the ef ﬁciency on extracting UMLS\n(40) concepts from a single document by using 30 milliseconds\nin comparison to traditional regular expression based methods\nwhich takes 3 seconds, and Finley et al. (41) which uses NMT to\nsimplify the typical multi-stage work ﬂow on clinical report\ndictation and even correct the errors from speech recognition.\nWith the prevalence of multilingual PLMs (MPLMs) developed\nfrom NLP ﬁelds, it becomes a current need to test their\nperformances in the clinical domain of NMT. MPLMs have been\nadopted by many NLP tasks since the ﬁrst emergence of the\nTransformer based learning structure (16). Among these, Marian\nis a small-sized MPLM led by Microsoft Translator based upon\nNenatus NMT ( 42) with around 7.6 million parameters ( 22).\nThen, different research and development teams have been\ncompeting on the size of their LMs in recent years, e.g. the\nmassive MPLMs (MMPLMs) WMT21fb and NLLB by Meta-AI\nwhich have the number of parameters set of 4.7 billion and 54\nbillion respectively (24, 13). To investigate the performances of\nthese different models with varied model sizes towards clinical\ndomain NMT with ﬁne-tuning, we set up all these three models\nas our base models. To the best of our knowledge, our work is\nthe ﬁrst one regarding the comparison between small-size and\nextra-large MPLMs in the clinical domain of NMT.\nVery close to the clinical domain, there has been a biomedical\ndomain MT challenge series together held with the Annual\nConference of MT (WMT), since 2016 (43, 44). The historical\nbiomedical MT task covered a corpus of biomedical terminologies,\nscientiﬁc abstracts from Medline, summaries of proposals for\nanimal experiments, etc. In 2022, it was theﬁr s tt i m et h a tt h i s\nBiomedical-MT shared task introduced clinical domain data for\nSpanish-English language pairs (25).\nAs the WMT21fb model does not include Spanish in its pre-\ntraining, we also examine the transfer learning technology into the\nclinical domain NMT towards Spanish-English using the\nWMT21fb model. Transfer-learning (45) has proved useful for text\nclassiﬁcation and relation extraction (46, 47), and low-resource MT\n(48) ﬁelds. However, to the best of our knowledge, we are theﬁrst\nto test clinical domain NMT via transfer learning using MMPLMs.\n3 Experimental designs\nIn this section, we introduce more details about the three\nMPLMs that we investigate in this work, i.e., Marian ( 22),\nWMT21fb (24), and NLLB (13).\n3.1 Multilingual Marian NMT\nFirstly, we draw a training diagram of the original Marian\nmodel on its pre-training steps in Figure 2 according to (22).\nThe pre-processing step includes tokenisation, true-casing, and\nByte-Pair Encoding (BPE) for sub-words. The shallow training is\nto learn a mid-phase translation model to produce temporary\ntarget outputs for back-translation. Then, the back-translation\nstep produces the same amount of input source sentences to\nenlarge the corpus. The deep-training stepﬁrst uses four left-to-\nright models which can be RNN ( 42) or Transformer ( 16)\nstructures, which is followed by four right-to-left models in the\nopposite direction. The ensemble-decoding step will generate the\nn-best hypothesis translations for each source input segment,\nwhich will be re-ranked using a re-scoring mechanism. Finally,\nin Marian NMT, there is an automatic post-editing step\nintegrated before producing the output. This step is also based\non an end-to-end neural structure by modelling the set(MT-\noutput, source sentence)!“post-edited output” as introduced by\nJunczys-Dowmunt and Grundkiewicz (49).\nThe Marian NMT model we deployed is from the Language\nTechnology Research Group at the University of Helsinki led by\nFIGURE 1\nIllustration of the Investigation Workﬂow.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 03 frontiersin.org\nTiedemann and Thottingal (50) which is based on the original\nMarian model but continuously trained on the multilingual\nOPUS corpus ( 51) to make the model available to broader\nlanguages. It includes Spanish $English (es $en) pre-trained\nmodels and has 7.6 million parameters forﬁne-tuning.3\n3.2 Extra-large multilingual WMT21fb and\nNLLB\nInstead of the optional RNN structure used in the Marian model,\nboth massive-sized multilingual PLMs (MMPLMs) WMT21fb and\nNLLB adopted Transformer as the main methodology. As shown in\nFigure 3, Transformer’sm a i nc o m p o n e n t sf o re n c o d e ri n c l u d e\nposition encoding, Multi-Head Attention, and Feed-Forward\nNetwork with layer normalisation at both two steps. The decoder\nuses Masked Multi-Head Attention to constrain the generation\nmodel only taking the already generated text into account.\nTo increase the model capacity without making the extra-large\nmodel too slow for training, inspired by the work from Lepikhin et al.\n(52), the WMT21fb model included “Sparsely Gated Mixture-of-\nExpert (MoE)” models into the FFN layer of Transformer, as shown\nin Figure 4. The MoE model will only pass a sub-set of model\nparameters into the next level, thus decreasing the computational\ncost. However, this dropout is done in a random manner.\nFurthermore, this structure design still needs language-speciﬁc\ntraining, such as English-to-other and other-to-English used by\nWMT21fb.\nTo further improve on this, the NLLB model designed a\nConditional MoE Routing layer inspired by Zhang et al. (53)t o\nask the MoE model to decide which tokens to dropout according\nto their capacity demanding or routing ef ﬁciency. This is\nachieved by a binary gate, which assigns weights to dense FNN\nFFNshared or MoE Gating, as in Figure 5. The Conditional MoE\nalso removes language-speciﬁc parameters for learning.\nIn summary, the WMT21fb and NLLB models share very\nsimilar learning structures, but most differently WMT21fb used\nlanguage-speciﬁc constrained learning. The WMT21fb model we\napplied is ‘wmt21-dense-24-wide.En-X’ (and X-En direction)\nwhich has 4.7 billion parameters 4 and contains the language\npairs English $ Chinese, Czech, German, Hausa, Icelandic,\nJapanese, and Russian. The full NLLB model includes 200+\nFIGURE 2\nMarian Pre-Trained NMT - Training Pipeline.\nFIGURE 3\nSeveral Attention based Transformer NMT structure (16).\n3https://huggingface.co/Helsinki-NLP 4https://github.com/facebookresearch/fairseq/tree/main/examples/wmt21\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 04 frontiersin.org\nlanguages and has 54.5 billion parameters. Due to the\ncomputational restriction, we applied the distilled model of\nNLLB, i.e. NLLB-distilled, which has 1.3 billion parameters.\nThe WMT21fb model does not have Spanish in the trained\nlanguage pairs, while NLLB includes Spanish as a high-resource\nlanguage. This is a perfect setting for us to examine the\ntransfer-learning technology on the clinical domain NMT by\nﬁne-tuning a translation model for the Spanish language on\nthe WMT21fb model and comparing the output with the\nNLLB model (Spanish version).\n4 Experimental settings and\nevaluations\n4.1 Domainﬁne-tuning corpus\nTo ﬁne-tune the three MPLMs for English$Spanish language\npair towards the clinical domain, we used the medical bilingual\ncorpus MeSpEn from Villegas et al. ( 54), which contains\nsentences, glossaries, and terminologies. We carried out data\ncleaning and extracted around 250K pairs of segments on this\nlanguage pair for domainﬁne-tuning of the three models. These\nFIGURE 4\noriginal dense Transformer (left) vs MoE Transformer (right) (13).\nFIGURE 5\nMoE vs Conditional MoE (13).\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 05 frontiersin.org\nextracted 250K pairs of segments are random from the original\nMeSpEn corpus and we divided them into a 9:1 ratio for training\nand development purposes. Because the WMT21fb pre-trained\nmodel did not include Spanish as one of the pre-trained\nlanguage model, we could not use , 2es . (to-Spanish)\nindicator for ﬁne-tuning. As a solution, we used, 2ru . as the\nindicator for this purpose (to-Spanish). This means a transfer\nlearning challenge to investigate if the extra-large multilingual\nPLM (xL-PLM) WMT21fb has created a semantic space to\naccommodate a new language pair for translation modelling\nusing the 250K size of corpus we extracted.\n4.2 Model parameter settings\nSome parameter settings for s-MPLM Marian model ﬁne-\ntuning are listed below. The last activation function for the\ngenerative model is a linear layer. Within the decoder and\nencoder, we used the Sigmoid Linear Units (SiLU) activation\nfunction. More detailed parameter and layer settings are\ndisplayed in Appendix Figure A1.\n learning rate ¼2e /C0 5\n batch size ¼128\n weight decay /C0 0:01\n training epochs ¼1\n encoder-decoder layers ¼6 þ6\nSome ﬁne-tuning parameters for NLLB-200-distilled (13) are listed\nbelow:\n batch size ¼24\n gradient accumulation steps¼8\n weight decay ¼0:01\n learning rate ¼2e /C0 5\n Activation function (encoder/decoder)¼ReLU\n number of training epochs¼1\n encoder-decoder layers ¼24 þ24\nThe ﬁne-tuning parameters for WMT21fb model are the same as\nthe NLLB-200-distilled, except for the batch size value which is\nset as 2. This is because the model is too large that we get out-\nof-memory (OOM) errors if we increase the batch size larger\nthan 2. More details on M2M-100 parameters and layer settings\nfor Conditional Generation Structure (55) we used for xL-MPLM\nWMT21fb and NLLB-200 can be found inAppendix Figure A2.\n4.3 Test sets and automatic evaluations\nThe evaluation corpus we used is from the ClinSpEn-2022\nshared task challenge data organised as part of the Biomedical MT\ntrack in WMT2022 ( 25). It has three sub-tasks: (1) EN !ES\ntranslation of 202 COVID19 clinical case reports; (2) ES!EN 19K\nclinical terms translation from biomedical literature and EHRs;\nand (3) EN!ES 2K ontological concept from biomedical ontology.\nThe automatic evaluation metrics used for testing include BLEU\n(HuggingFace) (56), ROUGE-L-F1 (57), METEOR (58), SACREBLEU\n(59), and COMET (60), hosted by the ClinSpEn-2022 platform.5 The\nmetric scores are reported inTable 1for three translation tasks. In the\ntable, the parameter‘plm.es’is a question mark asking if the Spanish\nlanguage was already included in the original off-the-shelf PLMs. For\nthis question, both Marian and NLLB have Spanish in their PLMs,\nwhile WMT21fb does not, which indicates that Clinical-WMT21fb\nis a transfer learning model for EN$ES language pair.\nFrom this automatic evaluation result,ﬁrstly, it is surprising\nthat the much smaller-sized Clinical-Marian model won most of\nthe scores across three tasks, indicated by italic font. Secondly,\nfor two xL-MPLMs, even though the transfer-learning model\nClinical-WMT21fb has a certain score gap to Clinical-NLLB on\nTask-1, it almost catches up with Clinical-NLLB for Task-2 and\n3 even winning one score, the COMET for Task-3 (0.9908 vs\n0.9180). This means the xL-MPLM has the capacity to create a\nmultilingual semantic space and the capability to generate a new\nlanguage model as long as there is enough ﬁne-tuning of the\ncorpus for this new language. Thirdly, there are issues with\nautomatic metrics. This includes the conﬁdence level on score\ndifference (signiﬁcance test), such as the very closely related\nTABLE 1 Automatic Evaluation of Three MPLMs using ClinSpEn-2022 Platform.‘plm.es’ means if the Spanish language is included in PLMs.\nMT ﬁne-tuning plm.es S ACREBLEU METEOR COMET BLEU ROUGE-L-F1\nTask-I: Clinical Cases (CC) EN!ES\nClinical-Marian Yes 38.18 0.6338 0.4237 0.3650 0.6271\nClnical-NLLB Yes 37.74 0.6273 0.4081 0.3601 0.6193\nClinical-WMT21fb No 34.30 0.5868 0.3448 0.3266 0.5927\nTask-II: Clinical Terms (CT) EN ES\nClinical-Marian Yes 26.87 0.5885 0.9791 0.2667 0.6720\nClinical-NLLB Yes 28.57 0.5873 1.0290 0.2844 0.6710\nClinical-WMT21fb No 24.39 0.5840 0.8584 0.2431 0.6699\nTask-III: Ontology Concept (OC) EN!ES\nClinical-Marian Yes 39.10 0.6262 0.9495 0.3675 0.7688\nClinical-NLLB Yes 41.63 0.6072 0.9180 0.3932 0.7477\nClinical-WMT21fb No 40.71 0.5686 0.9908 0.3859 0.7199\n5https://temu.bsc.es/clinspen/\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 06 frontiersin.org\nscores for Task-1 on theﬁrst two winner models. In addition, the\nwinner models change across Task-2 and 3 via different metrics.\nWe also observed that there are 4 percent of Russian tokens in\nthe EN ! ES output from Clinical-WMT21fb model. This\nindicates that the model keeps Russian tokens when it does not\nknow how to translate the English token into Spanish. This is\nvery interesting since the Russian tokens reserved in the text are\nnot-nonsense, instead, they are meaning correct tokens, just in a\nforeign language. This might be the reason why COMET\ngenerated higher score for Clinical-WMT21fb model than\nClinical-NLLB on Task-3 ‘ontological concept’, since COMET is\na neural metric that calculates the semantic similarity on an\nembedding space, ignoring the word surface form.\nTo improve the trustworthiness of our empirical investigation\nand generate more clear evaluation output across three models,\nwe carry out human expert-based evaluations in the next section.\n4.4 Comparisons\nTo compare our much smaller sized clinical-Marian model\nwith other existing work on this shared task data, such as Optum\n(61) and Huawei (62), we list the automatic evaluation scores in\nTable 2 where Optum attended all three sub-tasks, while Huawei\nonly attended Task II: Clinical Terminology (CT). From the\ncomparison scores using automatic metrics, we can see that much\nsmaller-sized Clinical-Marian wins some metrics in each of the\ntasks. In addition, Optum used their in-house clinical data as extra\ntraining resources in addition to WMT offered training set, while\nthe 250K training set we used for Clinical-Marian is extracted only\nusing WMT data. Huawei’s model only wins one metric (COMET)\nout ofﬁve metrics on Task-2 (CT), however, both Clinical-Marian\nand Optum wins two metrics out ofﬁve. So there is not much\nbetter performance from Huawei on this task even though they\nhave much more online resources and computational support.\n5 Human evaluation\nAs observed in the last section, there are two motivations for us\nto set up the expert-based human evaluation: (1) it is really\nsurprising that the much smaller-sized MPLM (s-MPLM)\nClinical-Marian wins the xL-MPLMs Clinical-NLLB and Clinical-\nWMT21fb; (2) to verify the hypothesis from automatic\nevaluation that Clinical-Marian really performs the best.\n5.1 Human evaluation setup\nTo achieve both qualitative and quantitative human evaluation,\nwe deployed a human-centric expert-based post-editing quality\nevaluation metric called HOPE by Gladkoff and Han (63) (it is\nalso called LOGIPEM invented from Logrus Global LLC, a\nlanguage service provider). HOPE evaluation metric has 8\npredeﬁned error types and each error type has corresponding\ndifferent levels of penalty points according to the severity level.\nThe sentence level and system level HOPE score is a\ncomprehensive score reﬂecting the overall quality of outputs.\nFirstly, we recruited 5 human evaluators who have\nbackgrounds as professional translators, linguists, and biomedical\nresearchers. For the evaluation data set, we take all the test set\noutput from Task-1 ‘clinical case’ reports since this is the only\ntask with full sentences. For the other two tasks on term and\nontology level translation, MT engines can perform relatively\ngood outcomes even without an effective encoder-decoder neural\nmodel, e.g. via a well prepared bilingual dictionary. We prepared\n100 strings for each set and delivered all the sets to 5\nprofessional evaluators.6 The tasks consisted of strings of medical\ncases going in order one by one, so the context of each case was\nclear to the evaluator.\nFirstly, each one of them was given threeﬁles for evaluation\nfrom different engines, and instructions were given on both the\nonline Perfectionist tool to be used for evaluation and the HOPE\nmetrics. To ensure the human evaluation quality, we have also\nasked the strictest reviewer/evaluator to validate the work from\nother evaluators. The strictest reviewer is one of our specialists\nfrom the language service provider industry and has our trust\naccording to their long term experiences in post-editing MT\noutputs and selecting MT engines in real world projects. The\nstrictest reviewer gave better distinctions among all three\nevaluated models, while the less-strict reviewers sometimes gave\nsimilar scores to these models without picking their errors strictly.\n5.2 Human evaluation output\nThe results of the evaluation can be seen in the online\nPerfectionist tool used, as downloaded from the tool in the form\nof the familiar Excel scorecards. They are tallied inFigure 6 and\nTable 3. The human evaluation result clearly shows which model is\nthe best with large score gap in-between, i.e. the Clinical-Marian\nTABLE 2 Model Comparisons on 3 Tasks between Clinical-Marian and\nOthers.\nModels S ACREBLEU METEOR COMET BLEU ROUGE\nTask-1: Translating Clinical Cases\nClinical-Marian 38.17 0.6337 0.4237 0.3650 0.6270\nOptum 38.12 0.6447 0.4425 0.3642 0.6285\nTask-2: Clinical Terminologies\nOptum 44.97 0.5880 1.1197 0.4396 0.7479\nHuawei 41.57 0.624 1.190 0.4132 0.721\nClinical-Marian 39.10 0.6261 0.9494 0.3674 0.7688\nTask-3: Translating Ontology Concepts\nOptum 44.97 0.5880 1.1197 0.4396 0.7479\nClinical-Marian 39.10 0.6261 0.9494 0.3674 0.7688\n6The 100 examples for evaluators were randomly selected from the\ntest dataset.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 07 frontiersin.org\nwith score 0.801625, followed by Clinical-NLLB and Clinical-\nWMT21fb with scores 0.768125 and 0.692429 respectively.\nTo compare the human evaluation outputs with the automatic\nmetric scores, we also added two metrics into the ﬁgure, i.e.\nMETEOR and ROUGE, and the average score of these two\nmetrics. The reason we choose these two is that they have a\nrelatively positive correlation to human judgements. For the\nother three metrics, ﬁrstly, BLEU shows NLLB as better for\nterms and concepts, which does not correspond to human\njudgement. More than that, BLEU shows WMT21fb concepts to\nbe better than the Marian Helsinki model, which is completely\nincorrect. Secondly, COMET score for the NLLB model is higher\nthan 1, which is clearly caused by the fact that this\nimplementation of COMET was not normalised by the Sigmoid\nfunction. Also, this COMET score for NLLB is higher than the\none for Marian Helsinki. Another error is that the COMET score\nfor clinical cases is much better than for both Marian and NLLB,\nwhich is completely impossible due to the presence of foreign\nlanguage tokens in WMT21fb output. Finally, when we see\nCOMET scores like 0.99 and 0.949 for Concepts, the score 0.42,\n0.40 and 0.34 for Cases look evidently out of whack. BLEU-HF\nscores for all content types are ridiculously low on the scale of\n[0, 1] for both Cases and especially for Terms.\nWe have someﬁndings from the comparisons.\n Most importantly, all human evaluators consistently showed\npositive correlation with preliminary human judgement of the\nMT output quality. Some of them were stricter than the\nothers, but all of them rated the worst model as the worst and\nthe best model as the best with only one exception. Results of\nhuman evaluation fully conﬁrm the initial hypothesis about\nthe quality of outputs of different engines, which is based on\ninitial holistic spot-check human evaluation.\n LOGIPEM/HOPE metric shows the difference in the quality\nmuch bigger than any of the automated metrics. Where the\nautomatic score shows 6 percent difference, human evaluation\ngives 14 percent. In other words, human linguists see the\nsigniﬁcant difference between the output quality of different\nengines very clearly. Even less trained evaluators show a\npositive correlation with the hypothesis.\n Even for those automatic metrics which correlate with human\njudgement, the score values do not seem to be representations\nof the uniform interval of [0, 1]. LOGIPEM/HOPE score will\nbe exactly 1 if the segments, in reviewer ’s opinion, do not\nhave to be edited, and LOGIPEM/HOPE score 0.8 means only\nabout 20% of work left to be done on the text with that score,\nsince LOGIPEM/HOPE scoring model is designed with\nproductivity assumptions in mind for various degrees of\nquality. COMET or ROUGE score 0.6 means that MT\ngenerated words different from those in the reference, this\nmeans that a perfect translation which is different from the\nreference would be rated much lower than 1. This is a huge\ndistortion of linearity, which is metric-speci ﬁc because all\nscores for different metrics live in their own ranges.\nAutomatic scores appear to live on some sort of non-uniform\nscale of their own, which is yet another reason why they are\nFIGURE 6\nComparison of Automatic Evaluations against Human Evaluation (HOPE).\nTABLE 3 Automatic Evaluations vs Human Evaluations (HOPE) on Three MPLMs\nMPLMs Auto. Metrics Average Diff. in scores\nMETEOR ROUGE Averge(M,R) HOPE Auto. HOPE\nClinical-Marian 0.6338 0.6271 0.6304 0.8016 6.45% 13.62%\nClnical-NLLB 0.6273 0.6193 0.6233 0.7681 1.13% 4.18%\nClinical-WMT21fb 0.5868 0.5927 0.5898 0.6924 5.38% 9.85%\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 08 frontiersin.org\nnot comparable to each other. The scale is compressed, and the\ndifference between samples becomes statistically insigniﬁcant.\n The margin of error for all three engines is about 6%, which is about\nthe same as the difference between mean of the measurements for\ndifferent engines. This means that the difference between\nmeasurement is statistically signiﬁcant, but a lot depends on the\nsubjectivity of the reviewer, and the difference between reviewers’\npositions may negate the difference in scores. However, even\ndespite the subjectivity of the reviewers, groups of measurements\nfor different engines appear to provide the statistically and\nvisually signiﬁcant difference.\n In general, human evaluators are to be trained / highly\nexperienced, and need to maintain a certain level of rigour.\nThe desired target quality should be stipulated quite clearly by\ncustomer speciﬁcations, as deﬁned in ISO 11669 and ASTM\nF2575. To avoid incorrect (inﬂated) scores and decrease Inter-\nRater Reliability (IRR), linguists must be tested prior to doing\nevaluations, or cross-validated.\n One evaluation task only takes 1 hour. There were 24 evaluation\ntasks in total, each task with 100 segments. It does not require\nsetting up any data processing, software development,\nreference “golden standard” data or model-trained evaluation\nmetric, it is clearly faster, more economical and reliable than\nresearch on whether automatic metric even pass the positive\ncorrelation test with human judgement (3 out of 5 did not in\nour case). While individual human measurements have\nvariance, they are all valid, all correlate with human\njudgement if done with minimal training and rigour.\n Automatic metrics cannot be comparable across different\nengines, different data sets, different languages and different\ndomains. On the contrary, human measurement is a golden\nuniversal ruler which provides the least common denominator\nbetween these scenarios. In other words, if Rouge is 0.67 for\nEn-Fr for medical text, and Rouge is 0.82 for En-De for\nautomotive text, we can’t compare these numbers. In contrast,\nLOGIPEM/HOPE score would mean one and the same thing\nacross the board.\nAll of the above conﬁrms the validity and interoperability of our\nhuman evaluation using LOGIPEM/HOPE metrics, which can be\nused as a single quick and easy validator of automatic metrics,\nultimate fast and easy way to carry out analytic quality\nmeasurement to compare the engines and evaluate the quality of\ntranslation and post-editing.\n5.3 Inter-rater-reliability\nTo measure the inter-rater-reliability (IRR) of the human\nevaluation we carried out, in Figure 7 , we summarise the\nevaluation output from ﬁve human evaluators on three models.\nThe summaries include the average scores for each model, the\nscore difference between these three models, and the average\nscores from the three models, from each person.\nIn this case we have continuous ratings (ranging from 0 to 1)\nrather than categorical ratings. Therefore, Cohen ’s Kappa or\nFleiss’ Kappa are not the most appropriate measures for this\nwork. The Intraclass Correlation Coef ﬁcient (ICC) which\nmeasures the reliability of ratings by comparing the variability of\ndifferent ratings of the same subject to the total variation across\nall ratings and all subjects would also not be appropriate here\nbecause there is a greater variation within the ratings of the same\nMT engine than between different MT engines.\nHowever, we can compute standard deviations of the\nevaluations by different reviewers for each engine as follows:\n Marian: approximately 0.101\n NLLB: approximately 0.100\n WMT21: approximately 0.125\nThese values represent the amount of variability in the ratings\ngiven by different reviewers for each engine. The con ﬁdence\nintervals for these measurements for conﬁdence level 80% are:\n Marian: approximately (0.759, 0.875)\n NLLB: approximately (0.729, 0.844)\n WMT21: approximately (0.658, 0.802)\nIn other words, with 80% conﬁdence:\n Marian: 0.817 + 0.058\n NLLB: 0.7865 + 0.0575\n WMT21: 0.73 + 0.072\nFIGURE 7\nSummary of Human Expert-Based Evaluations.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 09 frontiersin.org\nThis can be visualised inFigure 8. These intervals indeed overlap;\nhowever, Marian is reliably better than NLLB, and it is of course\nextremely surprising that WMT21fb rating is that high,\nconsidering that this result has been achieved with transfer\nlearning by ﬁne-tuning the engine without English-Spanish in\nthe original PLM training dataset! As we can see, for some\nreviewers who are quite tolerant to errors (e.g. Evaluator-1) the\nquality of all the engines is almost the same. The more proﬁcient\nand knowledgeable the reviewer is, the higher is the difference in\ntheir ratings.\n5.4 Error analysis\nWe list sampled error analyses on the outputs from theﬁne-\ntuned WMT21fb and NLLB models inFigures 9–11 for the three\ntasks on translations of sentences, terms, and concepts. The\npreferred translations are highlighted in green colour and“both\nsounds ok” is marked in orange.\nFrom the comparisons of sampled output sentences, we\ndiscovered that the most frequent errors in a ﬁne-grained\nanalysis include literal translations, oral vs written languages,\nFIGURE 9\nTask-1 Cases/Sentences EN-ES Translation Examples: clinic-WMT21fbvs clinic-NLLB.\nFIGURE 8\nConﬁdence Intervals of Three Models (M, N, W): Clinical-Marian, Clinical-NLLB, and Clinical-WMT21fb.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 10 frontiersin.org\ntranslation inconsistency, inaccuracy of terms, hallucination/made-\nup words, andgender-related errors such as feminine vs masculine,\nin addition to the standardﬂuency and adequacy that have been\ncommonly used by traditional MT researchers (64). For instance,\nin Figure 9, the ﬁrst two sentences (line 0 and 1) from the\nclinical-WMT21fb model are more written Spanish than the\nclinical-NLLB model which outputs are more oral Spanish.\nHowever, line 6 from the clinical-WMT21fb model includes the\nwords “fuertes” which means “strong” that is not as accurate as\n“severas/severe” from the other model. In addition,“de manana”\nin the same line is less natural than “matinal” from clinical-\nNLLB. Regarding gender-related issues, we can see the examples\nalso in line 6, where clinical-WMT21fb produced“el paciente” in\nmasculine while clinical-NLLB produced “la paciente ” in\nfeminine. However, the source did not say what gender is“the\npatient”. Regarding literal translation examples, we can see in\nFigure 11, line ont-19 shows that clinical-WMT21fb gives more\nliteral translation “Mal función vesical” than the preferred one\n“Función vesical deﬁciente” by clinical-NLLB when translating\n“Poor bladder function”. The neural model output hallucinations\ncan also be found inFigure 11, e.g. “Vejícula” does not exist and\nit is like a mix of “vejiga” and “vesicula” in Line ont_27;\nsimilarly, in Line ont_2, “multicística” is a mix of Spanish and\nEnglish, because the correct Spanish shall be“multiquística”.\nAs we mentioned in Section4., there are 4% Russian tokens in\nthe English-to-Spanish translation outputs from the Clinical-\nWMT21fb model which can be observed in Figures 9 and 11.\nHowever, they are meaningful tokens instead of nonsense, e.g.\nthe Russian tokens inFigure 9 from line_n 4 means“soon” and\nin Figure 11 means “type of” from ont_11.\n6 Discussions and conclusions\nTo boost the knowledge transformation for digital healthcare\nand make the most knowledge out of available clinical resources,\nwe explored the state-of-the-art neural language models\nregarding their performances in clinical machine translation. We\ninvestigated a smaller-sized multilingual pre-trained language\nmodel (s-MPLM) Marian from the Helsinki NLP group, in\ncomparison to two extra-large MPLM (xL-MPLM) NLLB and\nWMT21fb from Meta-AI. We also investigated the transfer-\nlearning possibility in clinical domain translation using xL-\nMPLM WMT21fb. We carried out data cleaning andﬁne-tuning\nFIGURE 10\nTask-2 Clinical Term ES-EN Translation Examples: clinic-WMT21fbvs clinic-NLLB.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 11 frontiersin.org\nin the clinical domain. We evaluated our work using both\nautomatic evaluation metrics and human expert-based evaluation\nusing the HOPE (63) framework.\nThe experiment also leads to very far-reaching conclusions\nabout MT models and their design, test, and applications:\n(1) The bigger model does not mean that the quality is better.\nThis premise proved to be false, evidently because\nresearchers need vast amounts of data to train very large\nmodels and very often such data is not clear enough. On the\ncontrary, when we clean the data very well forﬁne-tuning,\nwe can bring the model quality much higher in speci ﬁc\ndomains, e.g. clinical text. We reached the point when the\ndata quality was more important than the model’s size.\nOne key takeaway for researchers and practitioners from this\npoint is that if it is possible to get 250,000 clean segments in a\nnew low-resource language, it is capable ofﬁne-tuning large\nlanguage models (LLMs) and get a good enough engine in\nthis language. Then, the next step is to continue to get clean\ndata by post-editing translation output from that engine. This\nis a very important conclusion for“low resource languages”.\n(2) Automated metrics deliver an illusion of measurement– they\nare a good tool for iterative stochastic gradient descent during\ntraining, but they do not measure quality (only some sort of\nsimilarity), are not compatible when any of the underlying\nfactors change, provide results on a non-uniform scale even\non their interval of validity, in general are not sufﬁciently\nreliable, and may be misleading. We can’t rely on automatic\nmetrics alone. Instead, human translation quality validation\nis a must and such validation can deny and reverse the\nresults of automatic measurement.\nData availability statement\nThe original contributions presented in the study are included\nin the article/Supplementary Material, further inquiries can be\ndirected to the corresponding authors.\nEthics statement\nEthical review and approval was not required for this study in\naccordance with the national legislation and the institutional\nrequirements.\nFIGURE 11\nTask-3 Concept EN-ES Translation Examples: clinic-WMT21fbvs clinic-NLLB.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 12 frontiersin.org\nAuthor contributions\nLH drafted theﬁrst manuscript; SG, GE, IS carried out technical\nimplementation; BG carried out the leader role of human evaluation;\nSG and GN supervised the project; SG co-wrote the revised\nmanuscript especially on Human Evaluations. All authors\ncontributed to the article and approved the submitted version.\nFunding\nLH and GN are grateful for the support from the grant\n“Assembling the Data Jigsaw: Powering Robust Research on\nthe Causes, Determinants and Outcomes of MSK Disease”.T h e\nproject has been funded by the Nufﬁeld Foundation, but the views\nexpressed are those of the authors and not necessarily the\nFoundation. Visit www.nufﬁeldfoundation.org. LH and GN are also\nsupported by the grant“Integrating hospital outpatient letters into\nthe healthcare data space” (EP/V047949/1; funder: UKRI/EPSRC).\nAcknowledgments\nThe human evaluation of this work was carried out by Betty\nGaliano, Marta Martínez Albaladejo, Valeria López Expósito,\nCarlos Mateos, and Alfredo Madrid; We thanks our human\nevaluators for their volunteering and hard work. We also thank\nCristina Sánchez for assisting with double checking the sampled\nhuman evaluations.\nConﬂict of interest\nThe authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial\nrelationships that could be construed as a potential con ﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their\nafﬁliated organizations, or those of the publisher, the\neditors and the reviewers. Any product that may be\nevaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by\nthe publisher.\nReferences\n1. Griciūtė B, Han L, Li H, Nenadic G. Topic modelling of Swedish newspaper\narticles about coronoavirus: A Case Study using latent girichlet allocation method.\nIEEE 11th International Conference on Healthcare Informatics (ICHI); Houston, TX,\nUSA; 2023. (2023). p. 627–36. doi: 10.1109/ICHI57859.2023.00110\n2. Oyebode O, Ndulue C, Adib A, Mulchandani D, Suruliraj B, Orji FA, et al. Health,\npsychosocial, and social issues emanating from the COVID-19 pandemic based on\nsocial media comments: text mining, thematic analysis approach.JMIR Med Inform.\n(2021) 9:e22734. doi: 10.2196/22734\n3. Luo X, Gandhi P, Storey S, Huang K. A deep language model for symptom\nextraction from clinical text and its application to extract COVID-19 symptoms\nfrom social media. IEEE J Biomed Health Inform. (2022) 26:1737–48. doi: 10.1109/\nJBHI.2021.3123192\n4. Henry S, Buchan K, Filannino M, Stubbs A, Uzuner O. 2018 n2c2 shared task on\nadverse drug events and medication extraction in electronic health records.J Am Med\nInform Assoc. (2020) 27:3–12. doi: 10.1093/jamia/ocz166\n5. Spasic I, Nenadic G. Clinical text data in machine learning: systematic review.\nJMIR Med Inform. (2020) 8:e17984. doi: 10.2196/17984\n6. Percha B. Modern clinical text mining: a guide, review.Annu Rev Biomed Data\nSci. (2021) 4:165–87. doi: 10.1146/annurev-biodatasci-030421-030931\n7. Noor K, Roguski L, Bai X, Handy A, Klapaukh R, Folarin A, et al. Deployment of\na free-text analytics platform at a UK national health service research hospital:\ncogstack at University College London hospitals. JMIR Med Inform . (2022) 10:\ne38122. doi: 10.2196/38122\n8. Qian Z, Alaa AM, van der Schaar M. CPAS: the UK’s national machine learning-\nbased hospital capacity planning system for COVID-19. Mach Learn . (2021)\n110:15–35. doi: 10.1007/s10994-020-05921-4\n9. Wu Y, Han L, Antonini V, Nenadic G. On cross-domain pre-trained language\nmodels for clinical text mining: how do they perform on data-constrained ﬁne-\ntuning? arXiv [Preprint]. arXiv:2210.12770 (2022).\n10. Nguyen NTH, Miwa M, Ananiadou S.Span-Based Named Entity Recognition by\nGenerating, Compressing Information. arXiv [preprint]. (2023). doi: 10.48550/arXiv.\n2302.05392\n11. Wroge TJ, Özkanca Y, Demiroglu C, Si D, Atkins DC, Ghomi RH. Parkinson’s\ndisease diagnosis using machine learning and voice. In:2018 IEEE Signal Processing in\nMedicine and Biology Symposium (SPMB). IEEE (2018). p. 1–7.\n1 2 .Z h uZ ,X i n g m i n gZ ,T a oG ,D a nT ,L iJ ,C h e nX ,e ta l .C l a s s iﬁcation of\nCOVID-19 by compressed chest ct image through deep learning on a large\npatients cohort. Interdiscip Sci Comput Life Sci . (2021) 13:73 –82. doi: 10.1007/\ns12539-020-00408-1\n13. Costa-jussà MR, Cross J, Çelebi O, Elbayad M, Heaﬁeld K, Heffernan K, et al. No\nlanguage left behind: scaling human-centered machine translation.arXiv [Preprint]\narXiv:2207.04672 (2022).\n14. Khoong EC, Rodriguez JA. A research agenda for using machine translation in\nclinical medicine. J Gen Intern Med. (2022) 37:1275–7. doi: 10.1007/s11606-021-\n07164-y\n15. Weaver W. Translation. In: Machine Translation of Languages: Fourteen\nEssays (1955).\n16. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.\nAttention is all you need.Conf Neural Inf Process Syst. (2017) 30:6000–10.\n17. Devlin J, Chang M, Lee K, Toutanova K. BERT: pre-training of deep\nbidirectional transformers for language understanding.CoRR abs/1810.04805 (2018).\n18. Han L, Jones G, Smeaton A, Bolzoni P. Chinese character decomposition for\nneural MT with multi-word expressions. In: Proceedings of the 23rd Nordic\nConference on Computational Linguistics (NoDaLiDa) Reykjavik, Iceland (Online):\nLinköping University Electronic Press, Sweden (2021). p. 336–44.\n19. Han L. An investigation into multi-word expressions in machine translation\n(Ph.D. thesis). Dublin City University (2022).\n20. Kuang S, Li J, Branco A, Luo W, Xiong D. Attention focusing for neural machine\ntranslation by bridging source, target embeddings. In:Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers).\nMelbourne, Australia: Association for Computational Linguistics (2018). p. 1767–76.\n21. Han L, Kuang S. Incorporating Chinese radicals into neural machine translation:\ndeeper than character level. In: Proceedings of ESSLLI-2018. Association for Logic,\nLanguage, Information (FoLLI) (2018). p. 54–65.\n22. Junczys-Dowmunt M, Grundkiewicz R, Dwojak T, Hoang H, Hea ﬁeld K,\nNeckermann T, et al. Marian: fast neural machine translation in C++. In:\nProceedings of ACL 2018, System Demonstrations. Melbourne, Australia: Association\nfor Computational Linguistics (2018). p. 116–21.\n23. Junczys-Dowmunt M, Heaﬁeld K, Hoang H, Grundkiewicz R, Aue A. Marian:\ncost-effective high-quality neural machine translation in C++. In:Proceedings of the\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 13 frontiersin.org\n2nd Workshop on Neural Machine Translation and Generation. Melbourne, Australia:\nAssociation for Computational Linguistics (2018). p. 129–35.\n24. Tran C, Bhosale S, Cross J, Koehn P, Edunov S, Fan A. Facebook AI’s WMT21\nnews translation task submission. In:Proceedings of WMT(2021).\n25. Neves M, Jimeno Yepes A, Siu A, Roller R, Thomas P, Vicente Navarro M, et al.\nFindings of the WMT 2022 biomedical translation shared task: monolingual clinical\ncase reports. In: Proceedings of the Seventh Conference on Machine Translation\n(WMT). Abu Dhabi, United Arab Emirates (Hybrid): Association for\nComputational Linguistics (2022). p. 694–723.\n26. Almansor EH, Al-Ani A. A hybrid neural machine translation technique for\ntranslating low resource languages. In: Perner P, editor.Machine Learning and Data\nMining in Pattern Recognition . Cham: Springer International Publishing (2018).\np. 347–56.\n27. Islam MA, Anik MSH, Islam AAA. Towards achieving a delicate blending\nbetween rule-based translator and neural machine translator.Neural Comput Appl.\n(2021) 33:12141–67. doi: 10.1007/s00521-021-05895-x\n28. Han L, Erofeev G, Sorokina I, Gladkoff S, Nenadic G. Examining large pre-\ntrained language models for machine translation: what you don’t know about it. In:\nProceedings of the Seventh Conference on Machine Translation (WMT). Abu Dhabi,\nUnited Arab Emirates (Hybrid): Association for Computational Linguistics (2022).\np. 908–19.\n29. Han L, Erofeev G, Sorokina I, Gladkoff S, Nenadic G. Using massive multilingual\npre-trained language models towards real zero-shot neural machine translation in\nclinical domain. arXiv [preprint]. (2022). doi: 10.48550/arXiv.2210.06068\n30. Han L, Erofeev G, Sorokina I, Gladkoff S, Nenadic G. Investigating massive\nmultilingual pre-trained machine translation models for clinical domain via transfer\nlearning. In: Naumann T, Ben Abacha A, Bethard S, Roberts K, Rumshisky A,\neditors. Proceedings of the 5th Clinical Natural Language Processing Workshop .\nToronto, Canada: Association for Computational Linguistics (2023). p. 31–40.\n31. Yang H, Spasic I, Keane JA, Nenadic G. A text mining approach to the\nprediction of disease status from clinical discharge summaries. J Am Med Inform\nAssoc JAMIA. (2009) 16:596. doi: 10.1197/jamia.M3096\n32. Kovačević A, Dehghan A, Filannino M, Keane JA, Nenadic G. Combining rules\nand machine learning for extraction of temporal expressions and events from clinical\nnarratives. J Am Med Inform Assoc JAMIA. (2013) 20(5):859–66. doi: 10.1136/amiajnl-\n2013-001625\n33. Dehghan A, Kovacevic A, Karystianis G, Keane JA, Nenadic G. Combining\nknowledge-and data-driven methods for de-identi ﬁcation of clinical narratives.\nJ Biomed Inform. (2015) 58:S53. doi: 10.1016/j.jbi.2015.06.029\n34. Elbattah M, Dequen G. The role of text analytics in healthcare: a review\nof recent developments, applications. Healthinf. (2021) 5:825 –32. doi: 10.5220/\n0010414508250832\n35. Dew KN, Turner AM, Choi YK, Bosold A, Kirchhoff K. Development of\nmachine translation technology for assisting health communication: a systematic\nreview. J Biomed Inform. (2018) 85:56–67. doi: 10.1016/j.jbi.2018.07.018\n36. Randhawa G, Ferreyra M, Ahmed R, Ezzat O, Pottie K. Using machine\ntranslation in clinical practice.Can Fam Phys. (2013) 59:382–3.\n37. Soto X, Perez-De-Vinaspre O, Oronoz M, Labaka G. Leveraging SNOMED CT\nterms and relations for machine translation of clinical texts from basque to Spanish.\nIn: Proceedings of the Second Workshop on Multilingualism at the Intersection of\nKnowledge Bases and Machine Translation(2019). p. 8–\n18.\n38. Donnelly K. SNOMED-CT: the advanced terminology and coding system for\neHealth. Stud Health Technol Inform. (2006) 121:279.\n39. Mujjiga S, Krishna V, Chakravarthi K, Vijayananda J. Identifying semantics in\nclinical reports using neural machine translation. In: Proceedings of the AAAI\nConference on Artiﬁcial Intelligence (2019). Vol. 33, p. 9552–7.\n40. Bodenreider O. The uni ﬁed medical language system (UMLS): integrating\nbiomedical terminology. Nucleic Acids Res. (2004) 32:D267–70. doi: 10.1093/nar/\ngkh061\n41. Finley G, Salloum W, Sadoughi N, Edwards E, Robinson A, Axtmann N, et al.\nFrom dictations to clinical reports using machine translation. In:Proceedings of the\n2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 3 (Industry\nPapers). New Orleans - Louisiana: Association for Computational Linguistics\n(2018). p. 121–8.\n42. Sennrich R, Firat O, Cho K, Birch A, Haddow B, Hitschler J, et al. Nematus: a\ntoolkit for neural machine translation. In:Proceedings of the Demonstrations at the\n15th Conference of the European Chapter of the Association for Computational\nLinguistics. Valencia, Spain (2017).\n43. Bojar O, Chatterjee R, Federmann C, Graham Y, Haddow B, Huck M, et al.\nFindings of the 2016 conference on machine translation. In:Proceedings of the First\nConference on Machine Translation: Volume 2, Shared Task Papers (Berlin,\nGermany: Association for Computational Linguistics) (2016). p. 131–98.\n44. Yeganova L, Wiemann D, Neves M, Vezzani F, Siu A, Jauregi Unanue I, et al.\nFindings of the WMT 2021 biomedical translation shared task: summaries of animal\nexperiments as new test set. In: Proceedings of the Sixth Conference on Machine\nTranslation. Online: Association for Computational Linguistics (2021). p. 664–83.\n45. Alyafeai Z, AlShaibani MS, Ahmad I. A survey on transfer learning in natural\nlanguage processing. arXiv [Preprint] arXiv:2007.04239 (2020).\n46. Pomares-Quimbaya A, López-Úbeda P, Schulz S. Transfer learning for\nclassifying Spanish and English text by clinical specialties. In: Public Health and\nInformatics. IOS Press (2021). p. 377–81.\n47. Peng Y, Yan S, Lu Z. Transfer learning in biomedical natural language\nprocessing: an evaluation of BERT and ELMo on ten benchmarking datasets. In:\nDemner-Fushman D, Cohen KB, Ananiadou S, Tsujii J, editors.Proceedings of the\n18th BioNLP Workshop and Shared Task . Florence, Italy: Association for\nComputational Linguistics (2019). p. 58–65.\n48. Jiang H, Zhang C, Xin Z, Huang X, Li C, Tai Y. Transfer learning based on\nlexical constraint mechanism in low-resource machine translation. Comput Electr\nEng. (2022) 100:107856. doi: 10.1016/j.compeleceng.2022.107856\n49. Junczys-Dowmunt M, Grundkiewicz R. An exploration of neural sequence-to-\nsequence architectures for automatic post-editing. In: Proceedings of the Eighth\nInternational Joint Conference on Natural Language Processing (Volume 1: Long\nPapers). Taipei, Taiwan: Asian Federation of Natural Language Processing (2017).\np. 120–9.\n50. Tiedemann J, Thottingal S. OPUS-MT— building open translation services for\nthe world. In:Proceedings of the 22nd Annual Conferenec of the European Association\nfor Machine Translation (EAMT). Lisbon, Portugal (2020).\n51. Tiedemann J. Parallel data, tools and interfaces in opus. In:Lrec. Citeseer (2012).\nVol. 2012. p. 2214–8.\n52. Lepikhin D, Lee H, Xu Y, Chen D, Firat O, Huang Y, et al. Gshard: scaling giant\nmodels with conditional computation and automatic sharding. In: International\nConference on Learning Representations(2020).\n53. Zhang B, Bapna A, Sennrich R, Firat O. Share or not? learning to schedule\nlanguage-speciﬁc capacity for multilingual translation. In: Ninth International\nConference on Learning Representations 2021(2021).\n54. Villegas M, Intxaurrondo A, Gonzalez-Agirre A, Marimon M, Krallinger M. The\nMeSpEN resource for English-Spanish medical machine translation and\nterminologies: census of parallel corpora, glossaries and term translations. In:LREC\nMultilingualBIO: Multilingual Biomedical Text Processing(2018).\n55. Fan A, Bhosale S, Schwenk H, Ma Z, El-Kishky A, Goyal S, et al. Beyond english-\ncentric multilingual machine translation.J Mach Learn Res. (2021) 22(107):1–48.\n56. Papineni K, Roukos S, Ward T, Zhu WJ. BLEU: a method for automatic\nevaluation of machine translation. In:Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics . Philadelphia, Pennsylvania, USA:\nAssociation for Computational Linguistics (2002). p. 311–8.\n57. Lin CY. ROUGE: a package for automatic evaluation of summaries. In:Text\nSummarization Branches Out . Barcelona, Spain: Association for Computational\nLinguistics (2004). p. 74–81.\n58. Banerjee S, Lavie A. METEOR: an automatic metric for MT evaluation with\nimproved correlation with human judgments. In:Proceedings of the ACL(2005).\n59. Post M. A call for clarity in reporting BLEU scores. In:Proceedings of the Third\nConference on Machine Translation: Research Papers. Belgium, Brussels: Association\nfor Computational Linguistics (2018). p. 186–91.\n60. Rei R, Stewart C, Farinha AC, Lavie A. COMET: a neural framework for MT\nevaluation. In: Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP). Online: Association for Computational Linguistics\n(2020). p. 2685–702.\n61. Manchanda S, Bhagwat S. Optum ’s submission to WMT22 biomedical\ntranslation tasks. In: Proceedings of the Seventh Conference on Machine Translation\n(WMT). Abu Dhabi, United Arab Emirates (Hybrid): Association for\nComputational Linguistics (2022). p. 925–9.\n62. Wang W, Meng X, Yan S, Tian Y, Peng W. Huawei BabelTar NMT at WMT22\nbiomedical translation task: how we further improve domain-speci ﬁc NMT. In:\nProceedings of the Seventh Conference on Machine Translation (WMT). Abu Dhabi,\nUnited Arab Emirates (Hybrid): Association for Computational Linguistics (2022).\np. 930–5.\n63. Gladkoff S, Han L. HOPE: a task-oriented and human-centric evaluation\nframework using professional post-editing towards more effective MT evaluation.\nIn: Proceedings of the Thirteenth Language Resources and Evaluation Conference .\nMarseille, France: European Language Resources Association (2022). p. 13–21.\n64. Han L, Smeaton A, Jones G. Translation quality assessment: a brief survey on\nmanual and automatic methods. In: Bizzoni Y, Teich E, España-Bonet C, van\nGenabith J, editors. Proceedings for the First Workshop on Modelling Translation:\nTranslatology in the Digital Age. online: Association for Computational Linguistics\n(2021). p. 15–33.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 14 frontiersin.org\nAppendix\nFIGURE A1\nMarianNMT Fine-Tuning Parameters: Encoder and Decoder with 6þ6 Layers.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 15 frontiersin.org\nFIGURE A2\nM2M-100 Model Structure For Conditional Generation: Encoder and Decoder Parameters with 24þ24 Layers.\nHan et al. 10.3389/fdgth.2024.1211564\nFrontiers in Digital Health 16 frontiersin.org"
}