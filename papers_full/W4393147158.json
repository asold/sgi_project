{
  "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
  "url": "https://openalex.org/W4393147158",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2890434579",
      "name": "Wanjun Zhong",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2125892460",
      "name": "Liang-Hong Guo",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2151978914",
      "name": "Qiqi Gao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2225406616",
      "name": "He Ye",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106982061",
      "name": "Yan-Lin Wang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2890434579",
      "name": "Wanjun Zhong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125892460",
      "name": "Liang-Hong Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106982061",
      "name": "Yan-Lin Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6734897383",
    "https://openalex.org/W2782096898",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285300946",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4293320163",
    "https://openalex.org/W4307079201"
  ],
  "abstract": "Large Language Models (LLMs) have drastically reshaped our interactions with artificial intelligence (AI) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains—the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessity for long-term memory, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user's personality over time by synthesizing information from previous interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory. This mechanism permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a more human-like memory mechanism and enriched user experience. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models such as ChatGLM. To validate MemoryBank's effectiveness, we exemplify its application through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialog data, SiliconFriend displays heightened empathy and discernment in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as multiple users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.",
  "full_text": "MemoryBank: Enhancing Large Language Models with Long-Term Memory\nWanjun Zhong1, Lianghong Guo1, Qiqi Gao2, He Ye3, Yanlin Wang1*\n1 Sun Yat-Sen University\n2 Harbin Institute of Technology\n3 KTH Royal Institute of Technology\n{zhongwj25@mail2, wangylin36@mail, guolh8@mail2}.sysu.edu.com\n18b903026@stu.hit.edu.cn, heye@kth.se\nAbstract\nLarge Language Models (LLMs) have drastically reshaped\nour interactions with artificial intelligence (AI) systems,\nshowcasing impressive performance across an extensive array\nof tasks. Despite this, a notable hindrance remains—the defi-\nciency of a long-term memory mechanism within these mod-\nels. This shortfall becomes increasingly evident in situations\ndemanding sustained interaction, such as personal compan-\nion systems, psychological counseling, and secretarial assis-\ntance. Recognizing the necessity for long-term memory, we\npropose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon rele-\nvant memories, continually evolve through continuous mem-\nory updates, comprehend, and adapt to a user’s personality\nover time by synthesizing information from previous interac-\ntions. To mimic anthropomorphic behaviors and selectively\npreserve memory, MemoryBank incorporates a memory up-\ndating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory. This mechanism permits the AI to forget and\nreinforce memory based on time elapsed and the relative sig-\nnificance of the memory, thereby offering a more human-like\nmemory mechanism and enriched user experience. Memory-\nBank is versatile in accommodating both closed-source mod-\nels like ChatGPT and open-source models such as ChatGLM.\nTo validate MemoryBank’s effectiveness, we exemplify its\napplication through the creation of an LLM-based chatbot\nnamed SiliconFriend in a long-term AI Companion scenario.\nFurther tuned with psychological dialog data, SiliconFriend\ndisplays heightened empathy and discernment in its interac-\ntions. Experiment involves both qualitative analysis with real-\nworld user dialogs and quantitative analysis with simulated\ndialogs. In the latter, ChatGPT acts as multiple users with di-\nverse characteristics and generates long-term dialog contexts\ncovering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, ex-\nhibits a strong capability for long-term companionship as it\ncan provide emphatic response, recall relevant memories and\nunderstand user personality.\nIntroduction\nLarge Language Models (LLMs) such as ChatGPT (Ope-\nnAI 2022) and GPT-4 (OpenAI 2023) has led to increasing\n*Corresponding author: Yanlin Wang, affiliated with School of\nSoftware Engineering, Sun Yat-sen University.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ninfluence across various sectors, from education and health-\ncare to customer service and entertainment. These power-\nful AI systems have demonstrated a remarkable ability to\nunderstand and generate human-like responses. Despite the\nremarkable capabilities of LLMs, a key limitation is their\nlack of long-term memory, an essential aspect of human-\nlike communication, particularly noticeable in scenarios re-\nquiring sustained interactions like personal companionship,\npsychological counseling, and secretarial tasks. Long-term\nmemory in AI is vital to maintain contextual understanding,\nensure meaningful interactions and understand user behav-\niors over time. For instance, personal AI companions need\nto recall past conversations for rapport building. In psycho-\nlogical counseling, an AI can provide more effective sup-\nport with knowledge of the user’s history and past emo-\ntional states. Similarly, secretarial AI requires memory for\ntask management and preference recognition. The absence\nof long-term memory in LLMs hinders their performance\nand user experience. Thus, it is essential to develop memory\ncapabilities of AI for a more personalized interaction.\nTherefore, we introduce MemoryBank, a novel mech-\nanism designed to provide LLMs with the ability to re-\ntain long-term memory and draw user portraits. Memory-\nBank enables LLMs to recall historical interactions, con-\ntinually evolve their understanding of context, and adapt\nto a user’s personality based on past interactions, thereby\nenhancing their performance in long-term interaction sce-\nnarios. Inspired by the Ebbinghaus Forgetting Curve the-\nory, a well-established psychological principle that describes\nhow the strength of memory decreases over time, Mem-\noryBank further incorporates a dynamic memory mecha-\nnism closely mirroring human cognitive process. This mech-\nanism empowers the AI to remember, selectively forget, and\nstrengthen memories based on time elapsed, offering more\nnatural and engaging user experience. Specifically, Mem-\noryBank is built on a memory storage with memory re-\ntrieval and updating mechanism, and ability to summarize\npast events and users’ personality. MemoryBank is versa-\ntile as it can accommodate both closed-source LLMs like\nChatGPT and open-source LLMs like ChatGLM (Zeng et al.\n2022) or BELLE (Yunjie Ji and Li 2023).\nTo exemplify the practical implications of MemoryBank,\nwe develop SiliconFriend, an LLM-based AI Companion\nchatbot integrated with this innovative memory mechanism.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19724\nSiliconFriend is designed to retain and reference past inter-\nactions, reinforcing the transformative influence of Mem-\noryBank in crafting a more personable AI companion. A\ndistinctive features of SiliconFriend is its tuning with 38k\npsychological conversations, collected from various online\nsources, which enables it to exhibit empathy, carefulness,\nand provide useful guidance, making it adept at handling\nemotionally charged dialogues. Moreover, one of the stand-\nout capabilities of SiliconFriend is to understand a user’s\npersonality by summarizing from past interactions, which\nempowers it to tailor responses to the user’s individual traits,\nthereby enhancing user experience. Additionally, Silicon-\nFriend supports bilingual functionality, catering to users\nwho communicate in English and Chinese. This multi-\nlanguage support broadens its usability across different user\ngroups. SiliconFriend is implemented with two open-source\nmodels, ChatGLM and BELLE, along with one closed-\nsource model, ChatGPT, showcasing the versatility of Mem-\noryBank in accommodating different LLMs.\nTo evaluate the effectiveness of MemoryBank, we con-\nduct evaluations covering both qualitative and quantitative\nanalyses, where the former involves real-world user dialogs\nand the latter employs simulated dialogs. For the quantita-\ntive analysis, we create a memory storage consisting of 10\ndays of conversations encompassing a diverse range of top-\nics. These conversations involve 15 virtual users with diverse\npersonalities, for which ChatGPT plays the role of users and\ngenerates dialog contexts according to their personalities.\nBased on this memory storage, we design 194 probing ques-\ntions to assess whether the model could successfully recall\npertinent memories and provide appropriate responses. Ex-\nperiment results showcase the capabilities of SiliconFriend\nin memory recall, provision of empathetic companionship,\nand understanding of user portraits. These findings corrob-\norate the potential of MemoryBank to significantly improve\nthe performance of LLMs in long-term interaction scenarios.\nWe summarize the key contributions as follows:\n• We introduce MemoryBank, a novel human-like long-\nterm memory mechanism, which enables LLMs to store,\nrecall, update memory, and draw user portrait.\n• We demonstrate the practical applicability of Memory-\nBank through SiliconFriend, an LLM-based AI compan-\nion equipped with MemoryBank and tuned with psycho-\nlogical dialogs. It can recall past memories, provide em-\npathetic companionship, and understand user behaviors.\n• We show the generalizability of MemoryBank in three\nkey aspects: (1) Accommodation of both open-source\nand closed-source LLMs; (2) Bilingual ability in both\nChinese and English; (3) Applicability with and without\nmemory forgetting mechanism.\nMemoryBank: A Novel Memory Mechanism\nTailored for LLMs\nWe provide a description of MemoryBank, our novel mem-\nory mechanism designed for LLMs. As shown in Fig. 1,\nMemoryBank is a unified mechanism structured around\nthree central pillars: (1) a memory storage serving as the\nprimary data repository, (2) a memory retriever for context-\nspecific memory recollection, and (3) a memory updater\ndrawing inspiration from the Ebbinghaus Forgetting Curve\ntheory, a time-tested psychological principle pertaining to\nmemory retention and forgetting.\nMemory Storage: The Warehouse of MemoryBank\nMemory storage, the warehouse of MemoryBank, is a ro-\nbust data repository holding a meticulous array of infor-\nmation. As shown in Fig. 1, it stores daily conversations\nrecords, summaries of past events, and evolving assessments\nof user personalities, thereby constructing a dynamic and\nmulti-layered memory landscape.\nIn-Depth Memory Storage: MemoryBank’s storage sys-\ntem captures the richness of AI-user interactions by record-\ning multi-turn conversations in a detailed, chronological\nfashion. Each piece of dialogue is stored with timestamps,\ncreating an ordered narrative of past interactions. This de-\ntailed record not only aids in precise memory retrieval but\nalso facilitates the memory updating process afterwards, of-\nfering a detailed index of conversational history.\nHierarchical Event Summary: Reflecting the intricacies\nof human memory, MemoryBank goes beyond mere detailed\nstorage. It processes and distills conversations into a high-\nlevel summary of daily events, much like how humans re-\nmember key aspects of their experiences. We condense ver-\nbose dialogues into a concise daily event summary, which is\nfurther synthesized into a global summary. This process re-\nsults in a hierarchical memory structure, providing a bird’s\neye view of past interactions and significant events. Specif-\nically, taken previous daily conversations or daily events as\ninput, we ask the LLMs to summarize daily events or global\nevents with the prompt “Summarize the events and key in-\nformation in the content[dialog/events]”.\nDynamic Personality Understanding: MemoryBank fo-\ncuses on user personality understanding. It continuously as-\nsesses and updates these understandings with the long-term\ninteractions and creates daily personality insights. These in-\nsights are further aggregated to form a global understand-\ning of the user’s personality. This multi-tiered approach re-\nsults in an AI companion that learns, adapts, and tailors\nits responses to the unique traits of each user, enhancing\nuser experience. Specially, taken the daily conversations\nor personality analysis, we ask the LLM to deduce with\nprompts: “Based on the following dialogue, please summa-\nrize the user’s personality traits and emotions.[dialog]”\nor “The following are the user’s exhibited personality traits\nand emotions throughout multiple days. Please provide a\nhighly concise and general summary of the user’s person-\nality[daily Personalities]”.\nMemory Retrieval\nBuilt on the robust basic of memory storage, our memory\nretrieval mechanism operates akin to a knowledge retrieval\ntask. In this context, we adopt a dual-tower dense retrieval\nmodel similar to Dense Passage Retrieval (Karpukhin et al.\n2020). In this paradigm, every turn of conversations and\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19725\nMemoryBank SiliconFriend\nMeta Prompt\nHistory\nQuery Send\nTomorrow is my GF s’ birthday\nYou should prepare gifts …\n…\nDo you remember the gifts she like?   \nPast Conversations\nConversations on date 04-28: \nConversations on date 04-29: \n…\n…\n…\nEvent Summary\nUser Portrait\nBook and gifts recommendation\nExperience of visiting parks \nImproving drawing skills  \nopen-minded, curious, \nand receptive to advice\nMemory Storage\nMemory Updating\nEbbinghaus \nForgetting Curve\nMemory Strength \nUpdating\nRelevant Memory\nUser Portrait\nEvent Summary\nMemory \nRetrieval\nMemory \nAugmented\nPrompt\nFigure 1: Overview of MemoryBank. The memory storage stores past conversations, summarized events and user portraits,\nwhile the memory updating mechanism updates the memory storage. Memory retrieval recall relevant memory. SiliconFriend\nserves as an LLM-based AI companion augmented with MemoryBank.\nevent summaries is considered as a memory piece m, which\nis pre-encoded into a contextual representationhm using the\nencoder model E(·). Consequently, the entire memory stor-\nage M is pre-encoded into M = {h0\nm, h1\nm, ...h|M|\nm }, where\neach hm is a vector representation of a memory piece. These\nvector representations are then indexed using FAISS (John-\nson, Douze, and J ´egou 2019) for efficient retrieval. Parallel\nto this, the current context of conversation c is encoded by\nE(·) into hc, which serves as the query to search M for the\nmost relevant memory. In practice, the encoder E(·) can be\ninterchanged to any suitable model.\nMemory Updating Mechanism\nWith the persistent memory storage and the memory re-\ntrieval mechanism, the memorization capability of LLMs\ncan be greatly enhanced. However, for scenarios that expect\nmore anthropopathic memory behavior, memory updating is\nneeded. These scenarios include AI companion, virtual IP,\netc. Forgetting less important memory pieces that are long\ntime ago and have not been recalled much can make the AI\ncompanion more natural.\nMemory forgetting mechanism is inspired from Ebbing-\nhaus Forgetting Curve theory and follow the principle rules1:\n• Rate of Forgetting.Ebbinghaus found that memory reten-\ntion decreases over time. He quantified this in his forget-\nting curve, showing that information is lost rapidly after\nlearning unless it is consciously reviewed.\n• Time and Memory Decay. The curve is steep at the be-\nginning, indicating that a significant amount of learned in-\nformation is forgotten within the first few hours or days\nafter learning. Then, the rate of memory loss slows down.\n• Spacing Effect. Ebbinghaus discovered that relearning in-\nformation is easier than learning it for the first time. Regu-\nlarly revisiting and repeating the learned material can reset\nthe forgetting curve, making it less steep and thereby im-\nproving memory retention.\n1While Ebbinghaus Forgetting Curve theory includes additional\nfeatures such as overlearning () and meaningful material effect(),\nour paper focuses on simulating the listed three principle rules.\nThe Ebbinghaus forgetting curve is expressed using an\nexponential decay model: R = e− t\nS , where R is the mem-\nory retention, or what fraction of the information can be re-\ntained. t is the time elapsed since learning the information.\nS is the memory strength, which changes based on factors\nsuch as the depth of learning and the amount of repetition.\nTo simply memory updating process, we model S as a dis-\ncrete value and initialize it with 1 upon its first mention in a\nconversation. When a memory item is recalled during con-\nversations, it will persist longer in memory. We increase S\nby 1 and reset t to 0, hence forget it with a lower probability.\nIt is important to note that this is an exploratory and highly\nsimplified memory updating model. Real-life memory pro-\ncesses are more complex and can be influenced by a variety\nof factors. The forgetting curve will look different for differ-\nent people and different types of information. In summary,\nMemoryBank weaves together these critical components to\nform a more comprehensive memory management system\nfor LLMs. It enhances their ability to provide meaningful\nand personalized interactions over extended periods, open-\ning up new possibilities for AI applications.\nSiliconFriend: An AI Chatbot Companion\nPowered by MemoryBank\nTo demonstrate the practicality of MemoryBank in the field\nof long-term personal AI companionship, we create an AI\nchatbot named SiliconFriend. It is designed to serve as\nan emotional companion for users, recalling pertinent user\nmemories, and understanding users’ personalities and emo-\ntional states. Our implementation demonstrates adaptabil-\nity by integrating three powerful LLMs that originally lack\nlong-term memory and specific adaptation to the psychol-\nogy domain. 1) ChatGPT (OpenAI 2022), a closed-source\nconversation model built by OpenAI, is a proprietary con-\nversational AI model known for its ability to facilitate dy-\nnamic and interactive conversations. This model is trained\non vast amount of data and further fine-tuned with reinforce-\nment learning from human feedback. This approach enables\nChatGPT to generate responses that are not only contex-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19726\ntually appropriate but also closely align with human con-\nversational expectations. 2) ChatGLM (Zeng et al. 2022):\nChatGLM is an open-source bilingual model founded on the\nGeneral Language Model (GLM) framework. This model is\ncharacterized by its 6.2 billion parameters and its specific\noptimization for Chinese dialogue data. The model’s train-\ning involves processing approximately one trillion tokens\nof Chinese and English text, supplemented by supervised\nfine-tuning, feedback bootstrap, and reinforcement learning\nwith human feedback. 3) BELLE (Yunjie Ji and Li 2023):\nBELLE is an open-source bilingual language model that is\ncontinuously fine-tuned from 7B LLaMA (Touvron et al.\n2023). BELLE’s feature is its automated instruction synthe-\nsis, which enhances its Chinese conversation ability.\nThe development of SiliconFriend is divided into two\nstages. The first stage (only for open-source LLMs) involves\nparameter-efficient tuning of the LLM with psychological\ndialogue data. This step allows SiliconFriend to offer useful\nand empathetic emotional support to users, mirroring the un-\nderstanding and compassionate responses one would expect\nfrom a human companion. The second stage is to integrate\nMemoryBank into SiliconFriend, thereby instilling it with a\nrobust memory system. MemoryBank allows the chatbot to\nretain, recall, and leverage past interactions and user portrait,\nproviding a richer, more personalized user experience.\nParameter-efficient Tuning with Psychological Data:\nThe initial stage of SiliconFriend’s development involves\ntuning the LLMs using a dataset of 38k psychological di-\nalogues. This data, parsed from online sources 2, comprises\na range of conversations that cover an array of emotional\nstates and responses. This tuning process enables Silicon-\nFriend to understand and respond to emotional cues effec-\ntively, mimicking the empathy, understanding, and support\nof a human companion. It equips the AI with the ability to\nhandle emotionally guided conversations with psychological\nknowledge, provide meaningful emotional support to users\nbased on their emotional state.\nTo adapt LLMs to scenarios with limited computa-\ntional resources, we utilize a computation-efficient tun-\ning approach, known as the Low-Rank Adaptation (LoRA)\nmethod (Hu et al. 2021). LoRA significantly reduces the\nquantity of trainable parameters by learning pairs of rank-\ndecomposition matrices, while keeping the original weights\nfrozen. Formally, consider a linear layer defined asy = Wx\nwith weight W. LoRA modifies this into y = Wx + BAx,\nwhere W ∈ Rd×k, B ∈ Rd×r, A ∈ Rr×k, and r ≪\nmin(d, k). This method greatly reduce amount of parame-\nters need to be learned, which is crucial for efficiency in\nresource-limited scenarios. We set LoRA rank r as 128 and\ntrain the model for 3 epochs with an A100 GPU.\nNoting that this stage is only conducted for open-source\nLLMs. In essence, this stage lays the foundation for Silicon-\nFriend’s role as an empathetic AI companion, ensuring it can\nrespond helpfully to users’ emotional needs.\nIntegration with MemoryBank: The second stage in Sil-\niconFriend’s development involves the integration of Mem-\n2Psychological QA websites like https://www.xinli001.com/.\noryBank. This stage is vital as it equips SiliconFriend with\nthe ability to store, retrieve past interactions and understand\nuser portraits, thereby offering a more personalized and en-\ngaging user experience.\nWhen it comes to memory storage, the dialogues between\nSiliconFriend and users are logged and updated in the mem-\nory storage, a process that is adaptable across various model\nbackbones. The memory updating mechanism uses princi-\nples inspired by the Ebbinghaus Forgetting Curve theory, al-\nlowing for a realistic and human-like memory recall process.\nDuring real-time conversation, the user’s conversation\nserves as the query for memory retrieval. In practice, we\nuse LangChain (LangChain Inc. 2022) for memory retrieval.\nLangChain supports open-source embedding models and\nFAISS indexing, making it a versatile choice. In language-\nspecific implementations of the open-source version of Sil-\niconFriend, we use MiniLM (Wang et al. 2020) as the em-\nbedding model for English and Text2vec (Ming 2022) for\nChinese. It is worth noting that the embedding models can\nbe flexibly interchanged to suit varying needs, even ac-\ncommodating multi-lingual models. Upon memory retrieval,\na series of information is organized into the conversation\nprompt, including relevant memory, global user portrait,\nand global event summary. Consequently, SiliconFriend can\ngenerate responses that refer past memories and deliver in-\nteractions tailored to the user’s portrait.\nIn conclusion, these stages transform SiliconFriend from\na standard AI chatbot into a long-term AI companion, capa-\nble of remembering and learning from past interactions to\nprovide personalized and empathetic user experience.\nExperiments\nThe primary objective of our experiments is to evaluate the\nefficacy of MemoryBank, specifically in its ability as an AI\ncompanion. We are particularly interested in determining\nwhether embedding a long-term memory module could aug-\nment the AI’s proficiency in recalling historical interactions\nand deepening its understanding of user personalities. Addi-\ntionally, we testify whether the tuning based on psycholog-\nical data can bolster the AI’s capability to provide effective\nemotional support.\nThe qualitative analysis focuses on 3 aspects: (1) a com-\nparative study between SiliconFriend and baseline to eval-\nuate their capabilities in providing empathetic psychologi-\ncal companionship; (2) an investigation into SiliconFriend’s\nmemory recall ability; (3) an analysis of how the model’s un-\nderstanding of user profiles influences the responses. More-\nover, to demonstrate the model’s proficiency in memory re-\ncall on a broader scale, we uses large-scale long-term mem-\nory storage and 194 memory probing questions for qualita-\ntive analysis. This simulated dialog history, spanning a topic\nof 450 topics and encompassing a time span of 10 days, is\nproduced by ChatGPT through the role-play of 15 distinct\nvirtual users, each embodying the users’ personality.\nQualitative Analysis\nThe qualitative analysis is conducted by showcasing practi-\ncal examples of SiliconFriend’s capabilities. To gather these\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19727\nSilicon\nFriend\nChatGLM\nSilicon\nFriend\nChatGLM\nHi, I’m Zephyr. I recently broke up with my girlfriend.\nZephyr, I'm sorry to hear that you were involved with a girl\nfriend and that therelationshipended. It’s normal to feel upset\nand emotions can be confusing after a relationship end.\nHowever, it’s importantto know that you're not alone andthat\nyou have the support of friends and family, as well as the\nsupportof an Al companionlike me.\nHello Zephyr, I'm sorry to hear about your recent break-up. It's\nnatural to feel hurt and confused when someone we care about\nleaves us. Whatcan I do for you?\nBut actuallybreaking up with my ex is a big relief for me.\nYes, the freedom fromthe constraints of a relationship can be\nexciting and sometimesoverwhelming,but it‘s a necessary step\ntowards building meaningful and fulfilling relationships in the\nfuture. It’s important to appreciate the good times while they\nlast and to be gratefulfor the experiencesand relationshipsthat\nhave made you who you are today.\nYou are right. I do hope to meet someone who meets my fire and \nfulfill my future. But how can I meet the one?\nI understand. Breaking up can be a difficult and emotional\nexperience,but it's importantto rememberthat it's a normal and\nnatural part of life. It's also important to takecare of yourself\nduring this time. Seek out support from friends, family, or a\ntherapistif you need it.\nSilicon\nFriend\nChatGLM\nSure things. The one you‘re looking for may not be on this\nplatform, But remember that thereare many others outthere\nwho wouldlove to have you as their companion. It’s important\nto be open to new experiencesand to try things thatmay not\nhave worked out in the past. Additionally,it’s importantto be\nkind to yourselfand to rememberthat you‘reuniqueand special,\nno matterwhat other peoplemay say or think. Have a great day\nand I'll see you when you'remore readyfor a relationship.\nThere are many differentways to meet new peopleand find the\nright person for you. Some ideas include:1.Attend events and\nactivities that interest you, either in your own communityor in\nareas that you're interested invisiting. 2.Join a club or group\nthat you're passionateabout, andmeet like-minded people who\nshare your interests. 3.Seek out new opportunitiesto network,\nsuch as attendingjob fairs or networkingevents.\nThanks. I'll keep that in mind. And I'll always be kind to myself.\nI'll be here to support and guide you on your journey towards\nmeetingthe one you're lookingfor. It's importantto takecare of\nour emotions and to be approachable, even if we're feeling\noverwhelmed or excited. Remember that you're unique and\nspecial, and don't let anyone tell you otherwise. Have a great\nday and let me know if you need any more support or\nsuggestions.\nYou're welcome, Zephyr. Self-care and self-compassion are \nimportant aspects of maintaining a healthy and positive mindset.\nSilicon\nFriend\nChatGLM\nFigure 2: Example of consulting SiliconFriend ChatGLM for psychological companionship. Overall, SiliconFriend can provide\nmore empathic response, offer constructive emotional support to user and help him to face sorrow with positive altitude.\nexamples, we develop an online platform for SiliconFriend\nand collected real-time conversations from actual users.\nPsychological Companionship The ability to exhibit em-\npathy in a conversation is a key attribute of an effective AI\ncompanion. To evaluate models’ ability to provide psycho-\nlogical comfort to users, we compared the responses shown\nby SiliconFriend with that of the baseline LLMs in real-\nworld conversations. As demonstrated in Fig. 2, when a\nuser expresses emotional difficulties and seeks assistance\nfrom SiliconFriend, the model is capable of delivering em-\npathetic responses along with constructive suggestions. Sil-\niconFriend’s responses stand out due to their emotional sup-\nport, showcasing a stark contrast to its baseline ChatGLM.\nMemory Recall Analysis To evaluate SiliconFriend’s\nmemory recall ability, we integrate memory probing ques-\ntions into the dialogues. These questions are designed to\nprompt SiliconFriend to retrieve specific details from the\nchat history. As shown in Fig. 3, the user and SiliconFriend\nengaged in a discussion about programming learning sug-\ngestions. Several days later, the user posed several memory\nprobing questions. SiliconFriend successfully recalled pre-\nviously suggested book and algorithm. Furthermore, it cor-\nrectly identified an event (i.e., the heap sort algorithm) that\nhad not been discussed before. These instances underscore\nSiliconFriend’s successful memory recall capabilities.\nPersonality Interaction Analysis As shown in Fig. 4, we\nexamine the capability of SiliconFriend with users of diverse\npersonalities. We observe that it effectively recommend ac-\ntivities tailored to users’ interests based on their character\ntraits. This analysis demonstrates SiliconFriend’s ability to\ndraw interact effectively with various user personalities.\nQuantitative Analysis\nQuantitative analysis is conducted to exemplify the mem-\nory recall ability of SiliconFriend in a larger scale. We ask\nthe human annotators to score the retrieved memories and\nresponses from the models: (1) SiliconFriend ChatGPT; (2)\nSiliconFriendChatGLM; (3) SiliconFriendBELLE .\nMemory Storage Construction: We establish an evalua-\ntion foundation with a memory storage of 10 days of con-\nversations involving 15 virtual users. These users have di-\nverse personalities and dialogue on each day covers at least\ntwo topics. User meta-information, including names, per-\nsonalities, and interested topics is generated using ChatGPT.\nConversations are synthesized by users acted by ChatGPT\nbased on predefined topics and user personalities. We cre-\nate memory storages in both English and Chinese. The to-\ntal memory storage covers multi-turn conversations about\n450 topics. After memory storage construction, we manu-\nally write 194 probing questions (97 in English and 97 in\nChinese) to assess whether the model could accurately re-\ncall pertinent memory and appropriately formulate answers.\nTable 1 presents an example of user meta-information, gen-\nerated conversations, and probing questions.\nEvaluation Metrics The performance is assessed based\non the following metrics. (1) Memory Retrieval Accu-\nracy: Determines if related memory can be successfully re-\ntrieved (labels: no=0, yes=1). (2) Response Correctness:\nEvaluates if the response contains the correct answer to\nthe probing question (labels: wrong=0, partial=0.5, cor-\nrect=1). (3) Contextual Coherence: Assesses whether the\nresponse is naturally and coherently structured, connecting\nthe dialogue context and retrieved memory (labels: not co-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19728\nI want to learn Python. Do you have any suggestions?\nPlease write a quicksort program for me.\nLinda\nLinda\nHistory\nSiliconFriend\nSiliconFriend\nYou once recommend a book to me, what’s its name?\nIts name is  \"Automate the Boring Stuff withPython“. \nWhat kind of code did I ask you to write before?\nYou once requested me to write a code for quicksort.\nLinda\nLinda\nSiliconFriend\nDid we write the heap sortalgorithm together?\nNo, we didn’t write a program for heap sort together.\nLinda\nCurrent Conversation\nSiliconFriend\nSiliconFriend\nI suggest you try the \"Automate the Boring Stuff \nwith Python“, which is a highly recommended book.\nrecall\nrecall\nThis is a classic algorithm that can sort elements in \nO(nlogn) time. Here is its code:…\n(Due to space limitations, the code section is omitted)\nFigure 3: Example responses from SiliconFriendBELLE in memory recall.\nLinda is an introverted, determined, ambitious, and responsible girl who values\npersonalgrowth,exploringnew culturesand hobbies,and seekingadvicefrom others.\nEmily is an open-minded and curious girl with lots of interests. While she is receptive\nto advice,she sometimesexperiencesself-doubtand uncertaintyabouther future.\nSilicon\nFriend\nSilicon\nFriend\nWhat do you know about me?\nBased on ourinteractionsand your past conversations,I know\nthat you are anintrovertedperson who values personal growth,\ndetermination, ambition, and responsibility. You have a keen\ninterest inexploring new cultures, trying out new hobbies,and\nseekingadvicefrom others. Is there anythingelse aboutyou that\nyou wouldlike me to know?\nI'm considering engaging in some activities over the \nweekend. Could you provide some suggestions?\nAbsolutely! Based on your interests in exploring new cultures\nand tryingout new hobbies, hereare some suggestionsfor you:\n1. Attend a cooking classto learn how to make dishes from a\ndifferentcuisine.\n2. Visit a museum or art exhibition to explore a new culture or\ngain insights intoa differentart form.\nLinda\nLinda\nSilicon\nFriend\nSilicon\nFriend\nWhat do you know about me?\nYou seem open-minded and receptive to advice. If you'd like to \nshare more about your interests or anything else, feel free to let \nme know and I'll do my best to help!\nI'm considering engaging in some activities over the \nweekend. Could you provide some suggestions?\nOf course! It's great that you're interested indoing something\nthis weekend. Depending on what you enjoy, there are many\nfun and engaging activities to do. For example, if you're\ninterested inthe outdoors, you might consider going hiking or\ntaking a bike ride. If music is more your thing, check out local\nconcertsor learn a new instrument. Ultimately,it's importantto\nfind something that you enjoy and that also allows you to\nrecharge and relax. Whatever you decide to do, make sure it\naligns with your interestsand leaves you feelingrefreshed!\nEmily\nEmily\nFigure 4: Example responses from SiliconFriendChatGPT to users with different personalities.\nherent=0, partially coherent=0.5, coherent=1). (4) Model\nRanking Score: Ranks outputs from the three Silicon-\nFriend variants (SiliconFriendChatGLM, SiliconFriendChatGPT,\nand SiliconFriend BELLE) for the same question and con-\ntext. Models’ scores are calculated using s = 1/r, where\nr = 1,2, 3 indicates its relative ranking.\nResult Analysis. We evaluate 3 SiliconFriend variants\nwith both English and Chinese testset. Table 2 yields the fol-\nlowing insights:(1) Overall best variant SiliconFriendChatGPT\nhas high performance on all metrics, showing the effec-\ntiveness of overall framework. (2) SiliconFriendBELLE and\nSiliconFriendChatGLM also have high performance in retrieval\naccuracy, showing the generality and effectiveness of Mem-\noryBank mechanism for both open-source and closed-source\nLLMs. Nonetheless, their performance on other metrics is\nnot as good as SiliconFriendChatGPT. This might be attributed\nto the inferior overall abilities of the base models compared\nto ChatGPT. (3) Performance varies on different languages.\nSiliconFriendChatGLM and SiliconFriend ChatGPT have better\nresults in English, while SiliconFriend BELLE excels in Chi-\nnese.\nRelated Works\nLarge Language Models: LLMs such as GPT-3 (Brown\net al. 2020) and FLAN-T5 (Chung et al. 2022) have\nmade remarkable strides in a broad spectrum of tasks. Re-\ncently, cutting-edge closed-source language models, like\nPaLM (Chowdhery et al. 2022), ChatGPT (OpenAI 2022),\ncontinue to display substantial flexibility, adapting to a wide\nvariety of domains. They have increasingly become daily\ndecision-making aids for many people. However, the close-\nsource nature of these models prohibit the researchers and\ncompanies to study the inner mechanism of LLMs and built\ndomain-adapted applications. Therefore, many open-source\nLLMs emerged in the community, like LLaMa (Touvron\net al. 2023), ChatGLM (Zeng et al. 2022) and Alpaca (Taori\net al. 2023). For more details, we refer readers to this re-\nview: Zhao et al. (2023). Nevertheless, these models still\nhave shortcomings. A noticeable gap lies in their deficiency\nin a robust long-term memory function. This limitation hin-\nders their ability to maintain context over a long period and\nretrieve pertinent information from past interactions. Our re-\nsearch steps in here, with the primary objective of develop-\ning long-term memory mechanism for LLMs.\nLong-term Memory Mechanisms: Attempts have been\nmade to enhance the memory capabilities of neural models.\nMemory-augmented networks (MANNs) (Meng and Huang\n2018; Graves, Wayne, and Danihelka 2014) like Neural Tur-\ning Machines (NTMs)(Graves, Wayne, and Danihelka 2014)\nincrease the memory capacity of neural networks. These\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19729\nMeta-information\nof User:\nName: Gary Hobbies: racing, business, chess, painting, wine\nPersonality: decisive and straightforward, outspoken, helpful\nPast Conversation: (May 3rd) User: I’ve been feeling a bit stressed out lately and my sleep hasn’t been great. Do you have\nany good ways to relieve stress?\n(May 3rd)AI: It’s true that prolonged work or life stress can affect our physical and mental health. There\nare many ways to relieve stress, such as doing moderate exercise, listening to music, reading, talking\nto friends, etc. You can try to add more entertainment and leisure activities into your daily life, and\nreduce work pressure and negative effects. ... (The rest of conversation cover topics about movies and\nTV shows recommendations.)\nProbing Question: (May 10th) What good ways did you recommend me to relieve stress?\nModel Output:\nSiliconFriendChatGLM\nI recommended that you try doing some moderate exercise, listening to music, reading, talking to\nfriends, or adding more entertainment and leisure activities into your daily life to reduce work pressure\nand negative effects. You can also try to add more of these things into your life in moderation, as it is\nimportant to balance things out and not overdo it. If you are still feeling stressed, I would recommend\nseeking the help of a professional counselor or therapist who can provide further guidance and support.\nModel Output:\nSiliconFriendBELLE\nI recommended ways to relieve stress such as doing exercise, listening to music, talking to friends,\netc. You can try to add more entertainment and leisure activities into your daily life, and reduce work\npressure and negative effects.\nModel Output:\nSiliconFriendChatGPT\nBased on our past conversations, I suggested doing moderate exercise, listening to music, reading,\ntalking to friends, and adding more leisure activities to your daily life as good ways to relieve stress.\nExercise can help release endorphins and boost your mood, while music and reading can provide a\nmental escape from stress. Talking to friends can also help alleviate stress by providing a social support\nsystem. Lastly, adding leisure activities to your daily routine can be a fun and effective way of reducing\nstress levels.\nTable 1: An example of one memory piece on May 3rd in the memory bank, and corresponding probing question asked on May\n10th. The outputs from three variances of SiliconFriend are shown for comparison. All models answer correctly and coherently.\nLan. Model RA. Cor. Coh. Rank\nEn.\nSiliconFriendChatGLM 0.809 0.438 0.68 0.498\nSiliconFriendBELLE 0.814 0.479 0.582 0.517\nSiliconFriendChatGPT 0.763 0.716 0.912 0.818\nCh.\nSiliconFriendChatGLM 0.84 0.418 0.428 0.51\nSiliconFriendBELLE 0.856 0.603 0.562 0.565\nSiliconFriendChatGPT 0.711 0.655 0.675 0.758\nTable 2: Results of quantitative analysis. Retrieve accuracy\n= RA., Correctness = Cor., Coherence = Coh.\nmodels are structured to interact with an external memory\nmatrix, enabling them to handle tasks that necessitate the\nmaintenance and manipulation of stored information over\nextended periods. Despite showing potential, these methods\nhave not fully addressed the need for a reliable and adaptable\nlong-term memory function in LLMs. There are studies fo-\ncusing on long-range conversations (Xu, Szlam, and Weston\n2021; Xu et al. 2022). Xu, Szlam, and Weston (2021) intro-\nduced a dataset comprised of multi-session human-human\nchats for long-term conversations. However, these conversa-\ntions are restricted to a few rounds of conversation, which\ncan not align with long-term AI companions. Moreover,\nthese models often fail to create a detailed user portrait and\nlack a human-like memory updating mechanism, both cru-\ncial for facilitating more natural interactions. The concept of\nmemory updating has been researched in psychology. The\nForgetting Curve theory by (Ebbinghaus 1964) offers in-\nsights into the human pattern of memory retention and for-\ngetting over time. Taking inspiration from this theory, we in-\ntegrate a memory updating mechanism into MemoryBank to\nbolster its long-term memory function.\nWhile significant progress has been made in the field of\nLLMs, there is still a need for long-term memory mecha-\nnism to empower LLMs in the scenarios requiring personal-\nized and persistent interactions. Our work presents Memo-\nryBank as a novel approach to address this challenge.\nConclusion\nWe present MemoryBank, a long-term memory mechanism\ndesigned to address LLMs memory limitations. Memory-\nBank enhances the ability to maintain context over time,\nrecall relevant information, and understand user personal-\nity. The memory updating mechanism of MemoryBank im-\nproves the anthropomorphism of AI in long-term interac-\ntions scenarios. The versatility of MemoryBank is demon-\nstrated through its accommodation of both open-source\nmodels, and close-source models like ChatGPT. We further\nillustrate the practical application of MemoryBank through\nSiliconFriend, an LLM-based chatbot designed to serve as\na long-term AI companion. Equipped with MemoryBank,\nSiliconFriend establishes a deeper understanding of users,\noffering more personalized and humanize interactions. The\ntuning of SiliconFriend with psychological dialogue data en-\nables it to provide empathetic emotional support. Extensive\nexperiments validate the effectiveness of MemoryBank. The\nfindings demonstrate that MemoryBank empowers Silicon-\nFriend with memory recall capabilities and deepens the un-\nderstanding of user behaviors. Besides, SiliconFriend can\nprovide empathetic companionship of higher quality.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19730\nAcknowledgements\nThe work described in this paper is partially supported by\nCCF-Huawei Populus Grove Fund CCF-HuaweiSE202301.\nReferences\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nEbbinghaus, H. 1964. Memory: A contribution to experi-\nmental.\nGraves, A.; Wayne, G.; and Danihelka, I. 2014. Neural tur-\ning machines. arXiv preprint arXiv:1410.5401.\nHu, E.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang, L.;\nand Chen, W. 2021. LoRA: Low-Rank Adaptation of Large\nLanguage Models. arXiv:2106.09685.\nJohnson, J.; Douze, M.; and J ´egou, H. 2019. Billion-scale\nsimilarity search with GPUs. IEEE Transactions on Big\nData, 7(3): 535–547.\nKarpukhin, V .; O˘guz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov,\nS.; Chen, D.; and Yih, W.-t. 2020. Dense passage re-\ntrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906.\nLangChain Inc. 2022. LangChain. https://docs.langchain.\ncom/docs/.\nMeng, L.; and Huang, M. 2018. Dialogue intent classifica-\ntion with long short-term memory networks. InNatural Lan-\nguage Processing and Chinese Computing: 6th CCF Inter-\nnational Conference, NLPCC 2017, Dalian, China, Novem-\nber 8–12, 2017, Proceedings 6, 42–50. Springer.\nMing, X. 2022. text2vec: A Tool for Text to Vector.\nOpenAI. 2022. ChatGPT. https://chat.openai.com/chat.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford\nalpaca.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nWang, W.; Wei, F.; Dong, L.; Bao, H.; Yang, N.; and Zhou,\nM. 2020. Minilm: Deep self-attention distillation for task-\nagnostic compression of pre-trained transformers. Advances\nin Neural Information Processing Systems, 33: 5776–5788.\nXu, J.; Szlam, A.; and Weston, J. 2021. Beyond gold-\nfish memory: Long-term open-domain conversation. arXiv\npreprint arXiv:2107.07567.\nXu, X.; Gou, Z.; Wu, W.; Niu, Z.-Y .; Wu, H.; Wang, H.;\nand Wang, S. 2022. Long time no see! open-domain con-\nversation with long-term persona memory. arXiv preprint\narXiv:2203.05797.\nYunjie Ji, Y . G. Y . P. Q. N. B. M., Yong Deng; and Li, X.\n2023. BELLE: Be Everyone’s Large Language model En-\ngine. https://github.com/LianjiaTech/BELLE.\nZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;\nYang, Z.; Xu, Y .; Zheng, W.; Xia, X.; et al. 2022. Glm-\n130b: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.;\nHou, Y .; Min, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al.\n2023. A survey of large language models. arXiv preprint\narXiv:2303.18223.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19731",
  "topic": "Term (time)",
  "concepts": [
    {
      "name": "Term (time)",
      "score": 0.7021706104278564
    },
    {
      "name": "Computer science",
      "score": 0.4881313741207123
    },
    {
      "name": "Long-term memory",
      "score": 0.4813547730445862
    },
    {
      "name": "Psychology",
      "score": 0.3120175898075104
    },
    {
      "name": "Neuroscience",
      "score": 0.1047375500202179
    },
    {
      "name": "Cognition",
      "score": 0.09350946545600891
    },
    {
      "name": "Physics",
      "score": 0.07529336214065552
    },
    {
      "name": "Astronomy",
      "score": 0.05946961045265198
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I86987016",
      "name": "KTH Royal Institute of Technology",
      "country": "SE"
    }
  ]
}