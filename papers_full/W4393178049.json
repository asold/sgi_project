{
  "title": "Comprehensive Evaluation and Insights Into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation",
  "url": "https://openalex.org/W4393178049",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Karpurapu, Shanthi",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Myneni, Sravanthy",
      "affiliations": [
        "Illinois Institute of Technology"
      ]
    },
    {
      "id": null,
      "name": "Nettur, Unnati",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": null,
      "name": "Gajja, Likhit Sagar",
      "affiliations": [
        "BML Munjal University"
      ]
    },
    {
      "id": null,
      "name": "Burke, Dave",
      "affiliations": [
        "Inova Fairfax Hospital"
      ]
    },
    {
      "id": null,
      "name": "Stiehm, Tom",
      "affiliations": [
        "Inova Fairfax Hospital"
      ]
    },
    {
      "id": null,
      "name": "Payne, Jeffery",
      "affiliations": [
        "Inova Fairfax Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1987622710",
    "https://openalex.org/W3031657470",
    "https://openalex.org/W3162543111",
    "https://openalex.org/W4385627008",
    "https://openalex.org/W3182620217",
    "https://openalex.org/W4375840656",
    "https://openalex.org/W2781696492",
    "https://openalex.org/W3185198889",
    "https://openalex.org/W4214655501",
    "https://openalex.org/W3080457372",
    "https://openalex.org/W4387608638",
    "https://openalex.org/W4210447404",
    "https://openalex.org/W2060538844",
    "https://openalex.org/W2793776506",
    "https://openalex.org/W2607213010",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W2910453440",
    "https://openalex.org/W6740580305",
    "https://openalex.org/W4319163914",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4387898288",
    "https://openalex.org/W4385570371",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6854866820",
    "https://openalex.org/W6768817161"
  ],
  "abstract": "Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using LLMs.",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nComprehensive Evaluation and Insights into \nthe Use of Large Language Models in the \nAutomation of Behavior-Driven Development \nAcceptance Test Formulation \nShanthi Karpurapu1, Sravanthy Myneni2, Unnati Nettur3, Likhit Sagar Gajja4, Dave Burke5, \nTom Stiehm6, AND Jeffery Payne7 \n1Test Architect ‚Äì Digital Innovation, SQAC, Ashburn, VA 20147 USA \n2Department of Information Technology and Management, Illinois institute of technology, Chicago, IL 60616 USA \n3Department of Computer Science, Virginia Tech, Blacksburg, VA 24061 USA \n4Department of Computer Science, BML Munjal University, Haryana 122413 INDIA \n5COO, Coveros Inc, Fairfax, VA 22033 USA \n6CTO, Coveros Inc, Fairfax, VA 22033 USA \n7CEO, Coveros Inc, Fairfax, VA 22033 USA  \nCorresponding author: Shanthi Karpurapu (shanthi.karpurapu@gmail.com) \n \n \nABSTRACT Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration \namong developers, QA analysts, and stakeholders. In t his manuscript , we  propose a novel approach to \nenhance BDD practices using large language models (LLMs) to automa te acceptance test generation. Our \nstudy uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-\n2. The paper presents a detailed methodology that includes the dataset , prompt techniques, LLMs, and the \nevaluation process. The results demonstrate that GPT -3.5 and GPT -4 generate error -free BDD acceptance \ntests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy \nby incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation \naccuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. \nHowever, our study acknowledges that there are limitations to the proposed approach. We emphasize that \nthis approach can support collaborative BDD processes and create opportunities for future research into \nautomated BDD acceptance test generation using LLMs. \nINDEX TERMS agile software development, behavior driven development, large language model, \nmachine learning, natural language processing, prompt engineering, software testing, test cases, test \nautomation, zero-shot, few-shot.  \nI. INTRODUCTION \nBDD is a widely adopted, test-first Agile testing practice for \nspecifying system behavior through tests. It promotes \ncollaboration among software developers, quality assurance \nanalysts, and business stakeholders [1] [5] [35]. BDD utilizes \nthe Gherkin language  [2] [33], a semi -structured natural \nlanguage, to express the acceptance criteria of user stories in \nhuman-readable acceptance tests written as scenarios framed \nby the Given, When, Then keywords [3]. \nA simple scenario is provided in Table 1 to illustrate the use \nof the Gherkin language. In Table 1, The Given step \nestablishes the ini tial condition, ensuring the calculator is \nready for use. The When step represents the action ‚Äì entering \nthe numbers 5 and 7 and choosing addition. Finally, the Then \nstep defines the expected outcome ‚Äì the calculator displays the \nresult as \"12\". This Gherk in structure enhances clarity and \nenables non-technical professionals to understand scenarios. \n \nTABLE 1: BDD Scenario example  \n \nScenario: Performing Addition \n  Given I have opened the calculator application \n  When I enter ‚Äú5‚Äù into the calculator \n  And I add ‚Äú7‚Äù \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3391815\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 7 \n  Then the result should be ‚Äú12‚Äù \n \nDespite its effectiveness in translating software \nspecifications into behavior for validation, BDD experiences \nchallenges in manually crafting acceptance tests, leading to a \nnotable impact on efficiency and accuracy [4]. Ensuring high-\nquality BDD acceptance tests is crucial for successful \nimplementation [34]. The term \"high-quality BDD acceptance \ntests\" denotes adherence to Gherkin rules and best practices \noutlined in the methodology section  [6]. In addition , \nsignificant experience is required to craft good quality BDD \nacceptance tests. \nIn this manuscript we describe an  approach that optimizes \nBDD practices using LLMs, aiming to automate acceptance \ntest generation, reduce manual effort, and enhance \nproductivity. The suggested approach can also aid in \nstandardizing the BDD acceptance test formulation in Agile \nteams. Our study explores the automation of BDD acceptance \ntest possibilities using LLMs such as GPT-3.5, GPT-4, Llama-\n2-13B, and PaLM -2. Additionally, it  delves into the \nsignificance of prompt engineering for evaluating and \nselecting optimal prompts to ensure the generated scenarios \nadhere to best practices and avoid Gherkin syntax issues. \n. \nII. RELATED WORKS \nThis section provides an overview of our literature review on \nBDD acceptance test creation automation. Despite our  \nexhaustive search, we found no prior research in this field. \nTherefore, we broadened our literature review to explore the \nautomation of conventional test case generation methods. The \nterm 'conventional test case generation method' refers to test \ncases written with preconditions, test steps, expected results, \nand actual results.  We have reviewed approaches that used \nconventional based methods.  \nMathur et al. [7] proposed a method for generating test cases \nfrom conversation text. The context and topic are extracted \nfrom the conversation text using T5 and GPT-3 models. This \napproach eliminates the need for manual test case creation and \nenables the testing of complex systems. However, the method \nmay need to enhance its performance when generating \nnumerous test cases and exhibiting accuracy issues in the \noutput. \nZhuang et al.  [8] proposed a technique in 2018 for \ngenerating short test cases using an ensemble model. This \nmethod uses multiple rankers and a generation -based \napproach to produce the final response, which is selected \nbased on the ensemble module ranking provided. However, \nInterpreting the output of an ensemble model can be \nchallenging due to its complexity.  Additionally, creating an \nensemble model can be challenging and time-consuming, \nwhich may result in lower prediction accuracy compared to \nusing a single model. \nIn a different study conducted by Lafi et al. [9] proposed an \napproach to generate test cases automatically from \nrequirement specifications. This approach has a sequence of \nsteps: extracting the required information from the use case \ndescription obtained from the  use case diagram, then \ndeveloping the control flow graph and NLP table, and \ngenerating test paths as the next step. The test paths and NLP \ntable generate test cases, but there is a need to explore the \napplicability of the proposed approach to various case studies. \nThis approach does not address validating results obtained \nusing different test coverage criteria. \nGuo et al. [10] introduced a framework for generating test \ncases based on the generative adversarial network (GAN). \nThis study has shown that the W GAN-GP model effectively \nimproves test coverage. The results indicate that unit testing \nhas better test coverage than integration testing. However, the \nstudy highlights the potential for enhancing integration tests in \nthe future. \nUtting et al. [11] developed an approach for efficient test \nidentification and generation from the customer and test \nexecution traces, particularly in web services and API testing. \nThis approach utilizes clustering algorithms to improve testing \nprocedures. This study only presents preliminary results and \nhas yet to determine the method's robustness.  \nOur proposed approach uses LLMs like GPT -3.5, GPT-4, \nor PaLM -2 with optimized prompt design deviating from \nmachine learning -based conventional test case generation \nmethods [12] ‚Äì [15] mentioned in the literature [12]. In the \nconventional approach, teams typically write test cases in the \nlater stages of software development.  Our approach is \ndeveloped for BDD based agile methodology [40] and enables \nearly testing and validation. Therefore, teams formulate BDD \nacceptance tests for user stories before progressing from the \nsprint backlog to th e 'in progress' state, defining all \nspecifications upfront in a testable format. Additionally, using \nLLMs in the proposed approach has the advantage of being \nsimple, easily accessible to everyone, and use r-friendly. \nAdopting this proposed approach is relatively straightforward \nfor Agile teams, provided the organization's policies allow the \nuse of LLMs. \n \nIII. METHODOLOGY \nOur proposed methodology aims to comprehensively evaluate \ndifferent LLMs and prompt design techniques to gauge their \neffectiveness in generating BDD  acceptance tests. Figure 1 \nillustrates the procedural steps for automating BDD \nacceptance tests and validation. \nWe developed separate programs for each LLM and \nexecuted each program twice to evaluate both few -shot and \nzero-shot prompts. The implementation is written in Python, \ntailored individually for each model, and executed within the \nGoogle Colab framework [17]. The code developed for all the \nmodels is available in the GitHub repository [16]. \nThe initial step involves reading user story descriptions \nfrom a CSV file. Each user story description, accompanied by \nthe prompt (prompt is the task instructions to the model) and \nmodel parameters (temperature, top_p, max_tokens etc.), are \nsent in the request body to the model API endpoint. Iterating \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3391815\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 7 \nthrough all user stories, we store the model's BDD acceptance \ntests (scenarios) response as a .feature file. \nAfter the feature file generation phase, the generated feature \nfiles undergo validation using a tool that validates Gherkin \nsyntax, capturing syntax errors for further analysis. We \ncomprehensively assessed LLMs and prompt design \ntechniques for this proposed methodology that integrates \nautomated generation with rigorous validation.  \nFIGURE 1.  Behavior driven development acceptance tests generation  \nand validation process. \n \nThe following details outline the essential elements of our \nproposed methodology. \nA. DATASET \nFor this study, we curated a diverse set of approximately 50 \nreal-world user stories drawn from various sources. The data \nsources include user story data from Mendeley [18] and a blog \npost [19] that details user stories across different domains. \nThis compilation ensures a comprehensive representation of \nuser stories for robust examination in our study. \nB. PROMPT TECHNIQUES \nThe prompt technique involves creating clear, concise \ninstructions or queries that guide LLMs in task execution. \nThese prompts are essential because they enhance model \ncomprehension and performance by  providing explicit \nguidance, reducing ambiguity, and ensuring accurate \nresponses. Wei et al. (2022) suggested the effectiveness of \ninstruction tuning, which refines models through fine-tuning \nbased on datasets described with clear instructions, thereby \nenhancing zero-shot learning capabilities [20] [36] [37]. \nIn the context of few -shot learning, prompts are crucial. \nDespite the impressive zero-shot capabilities of LLMs, they \nmay face challenges with complex tasks. Therefore, few-shot \nprompting [39] involves incorporating examples within the \nprompt. This prompt technique enables in-context learning by \nguiding the model to better performance, conditioning it for \nsubsequent examples with minimal instances. \nAdhering to best practices in generating BDD acceptance \ntests can fully unlock the value of LLM. We evaluate d zero \nand few-shot prompt techniques [20] - [24] to determine which \nprompts better align with best practices. Best practices \nadoption may vary based on the software development \npractices followed by the agile teams. Therefore, as part of this \nstudy, we considered a subset of common best practices for \nevaluation, focusing solely on evaluating if the model can give \nan output based on the best practice instructions given in the \nprompt. \nTable 2 and 3 details the prompt techniques considered in \nthis study. The best practices are included in both the prompts \nas instructions from 1 to 6, as shown in Table 2 and 3. It is \nworth noting that zero and few -shot prompts have the same \ninstructions, and the few -shot prom pt includes additional \nBDD scenario examples for the model to follow while \nproducing the response. \n \nTABLE 2: Zero -shot prompt technique  instructions.  \n \nGenerate a feature file with 5 Gherkin Scenarios for {user_story} \nby following below {instructions}. \n{instructions}= \n1. Start the feature file with the 'Feature:' keyword. \n2. Provide a descriptive feature name to specify the context of the \nscenarios. \n3. Include steps in the Background if they are repeated at the \nbeginning of all scenarios in a feature. \n4. The background step is executed before every scenario. \n5. Use tags as annotations to group and organize scenarios and \nfeatures. \n6. Tags are written with the '@' symbol followed by a significant \ntext. \n{user_story} = \"As a user, I need a simple calculator for quick \nand accurate basic operations.\" \n \n \nTABLE 3: Few --shot prompt technique  instructions.  \n \nUser role: \nSame instructions from Zero -shot are given along with the \nexamples as written below \nAssistant role: \nFeature: Basic Calculator Operations \n  As a user, I need a simple calculator for quick and accurate basic \noperations. \n      Background: \n        Given I have opened the calculator application \n      @basicoperations \n      Scenario: Performing Addition \n        When I enter ‚Äú5‚Äù into the calculator \n        And I add ‚Äú7‚Äù \n        Then the result should be ‚Äú12‚Äù \n      @basicoperations \n      Scenario: Performing Subtraction \n   When I enter ‚Äú10‚Äù into the calculator \n        And I subtract ‚Äú3‚Äù \n        Then the result should be ‚Äú7‚Äù \n  # Continued.  ‚Ä¶‚Ä¶.. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3391815\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 7 \nC. LARGE LANGUAGE MODELS \nWe listed the LLMs considered for evaluation, as detailed in \nTable 4. The selected models include: \n \n  TABLE 4: Large Langue Model Details . \n \nModel Details \nGPT-3.5-\nTurbo \nThis autoregressive LLM utilizes the \nReinforcement Learning from Human Feedback \n(RLHF) mechanism. As the initial backbone \nmodel for ChatGPT, GPT -3.5 [38] demonstrates \nremarkable zero-shot performance across diverse \ntasks [25]. Our study employs the gpt -3.5-turbo-\n0613 model from OpenAI, known for its \nmaximum context length of 4K tokens. \nGPT-4-\nPreview \nThe most recent addition to OpenAI's LLMs, \nGPT-4, stands out as the first multimodal model in \nthe GPT series [26]. Recognized for its enhanced \ninstruction-following capabilities compared to \nGPT-3.5, GPT-4 is deemed more reliable. In our \nstudy, we utilize the GPT-4 version. \nPaLM-2 Developed by Google [27], PaLM-2 is an LLM \nthat employs a mixture of objective techniques It \ndemonstrates substantial performance \nimprovements over the original PaLM model [28]. \nIn our study, we utilize the chat-bison-001 model. \nLlama-2 ‚Äì \n13B \nDeveloped by Meta [29], Llama-2 stands out as an \nopen-source LLM. A notable advantage of Llama-\n2, distinguishing it from the previously mentioned \nLLMs, is its open -source nature, making it \naccessible for research and commercial purposes. \nOur study utilizes the Chat versions of the Llama-\n2-13B model, sourced from HuggingFace [30]. \n \nD. EVALUATION \nManual validation of feature files generated from LLM can be \nresource-intensive, demanding significant time and effort. In \naddition, the human-driven nature of the process introduces \nthe potential for biases and errors, emphasizing the need for \nrigorous checks and collaboration to mitigate these challenges. \nIn evaluating the quality of the generated feature files, we \nemployed the Gherkin-lint tool [31] [32] to overcome manual \nvalidation challenges, a robust instrument designed to detect \nsyntax violations. This tool meticulously evaluates adherence \nto predefined style guidelines tailored for  the Gherkin \nlanguage. This tool comprehensively evaluates and examines \nthe syntactical correctness of the feature files and ensures \nalignment with established conventions and best practices in \nGherkin-based specifications. This approach offers a detailed \nassessment of BDD acceptance tests  and enhances the \nrobustness and comprehensiveness of our evaluation process. \nIV. RESULTS \nAs the methodology section outlines, we generate BDD \nacceptance tests for all user stories, considering various LLMs \nand prompts. The subsequent sections present the findings \nfrom the conducted experiments. Visual representations have \nbeen employed to conv ey critical findings and offer \ncomprehensive insights into the experiments.  \nA. VALIDATION ACCURACY \nAs shown in equation (1), we calculated accuracy by dividing \nthe number of BDD feature files without syntax issues by the \ntotal number of generated BDD feature files. When employing \nthe few-shot prompt technique, we observe a high success rate \nin generating BDD feature files without syntax issues. GPT-\n3.5 and GPT-4 models exhibit higher accuracy with the few-\nshot prompt technique than other models, such as Llama -2-\n13B and PaLM -2 (Figure 2). Llama -2-13B achieves the \nsecond-highest accuracy with the few-shot prompt technique \n(Figure 2). \nùëâùëéùëôùëñùëëùëéùë°ùëñùëúùëõ ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ = \nùëÅùëú.ùëúùëì ùêπùëíùëéùë°ùë¢ùëüùëí ùêπùëñùëôùëíùë† ùë§/ùëú ùëÜùë¶ùëõùë°ùëéùë• ùê∏ùëüùëüùëúùëüùë†\nùëáùëúùë°ùëéùëô ùëÅùëú.ùëúùëì ùêπùëíùëéùë°ùë¢ùëüùëí ùêπùëñùëôùëíùë†  (1) \n \nFIGURE 2. BDD-based feature syntax validation accuracies across all \nuser stories with various prompt techniques and LLMs. \nB. SYNTAX ERRORS \nThe various syntax errors reported by the Gherkin-lint tool are \ncategorized as shown in Table 5. The aggregation of syntax \nerrors across all models reveals that the highest incidence of \nerrors occurs when employing the zero-shot technique (Figure \n3). Notably, the zero-shot approach contributes to 89% of the \ntotal syntax errors (Figure 3). \n \nTABLE 5: Syntax error type details  \n\\ \nError Type Error Details \ngherkin-\nkeywords-not-\nin-logical-\norder \nWhen writing scenario steps in some BDD \nfeature files generated by LLM, the tool reports \nan error if the specified order of scenario step \nkeywords (Given, When, Then)  is not \nfollowed‚Äîfor instance, a sequence like \"Given \nThen When\" is considered incorrect. \ngherkin-\nkeyword-not-\npresent-in-\nstep \nThe tool reports this error when scenario steps \ndo not commence with the keywords Given, \nWhen, Then, And, or But. For instance, \ninappropriately, tags are sometimes written at \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3391815\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 7 \nthe end of the file, such as @regression \n@download-files @experiments \n@attachments, violating the correct structure \nwhere tags should appear before the scenario \ndescription. \nmissing-tags The tool reports this error in the BDD feature \nfile when scenarios lack tags. While reporting \nthe lack of tags error is optional in the tool \nsettings, the configuration dictates treating \nmissing tags as an error, as the prompt technique \ninvolves generating tags as one of the \ninstructions. \nrestricted-\npatterns-\npresent \nThe presence of unnecessary special characters \nin the generated BDD feature files triggers the \nreporting of this error. Examples include: \nFeature description: \"-------------------------------\n----\" (matches a restricted pattern) \nBackground description: \"------------\" (matches \nrestricted pattern) \nScenario description: \"-------------------------\" \nFIGURE 3. Total syntax errors for zero and few-shot techniques. \n \nThe compilation of syntax errors by error type across all \nmodels indicates that the predominant contributors to errors in \nthe zero-shot prompt technique are the \"Restricted -Patterns-\nPresent\" and \"Gherkin-Keyword-Not-Present\" error types. In \nthe few-shot prompt technique, the most prominent error types \nare \"Restricted-Patterns-Present\" and \"Missing-Tags (Figure \n4).  \nThe examination of model performance concerning syntax \nerrors shown in Figure 5 is a vital aspect.GPT-3.5 and GPT-4 \nexhibit better performance by showcasing the fewest errors in \nthe few-shot technique. Additionally, the number of syntax \nissues observed in the zero -shot technique for GPT -3.5 and \nGPT-4 is comparable but slightly higher than that in the few- \nshot technique. GPT -3.5 and GPT -4 are the most effective \nmodels, with fewer syntax errors in generat ing BDD feature \nfiles as shown in Figure 5 . Conversely, L lama-2-13B \ncontributes the highest number of errors in the zero -shot \ntechnique, while PaLM-2 does so in the few-shot technique. \nFIGURE 4. Syntax error type distribution. \nFIGURE 5. No. of syntax errors for various LLMs. \n \nThis comparative analysis evaluates GPT -3.5, GPT -4, \nLlama-2-13B, and PaLM-2 language models based on their \nperformance in handling Gherkin syntax errors, including \ngherkin-keywords-not-in-logical-order, gherkin -keyword-\nnot-present-in-step, missing -tags, and restricted -patterns-\npresent. As shown in Table 6 & 7, GPT-3.5 and GPT -4 \ndisplayed limited occurrences of errors in zero-shot and few-\nshot evaluations. L lama-2-13B exhibited challenges during \nzero-shot evaluations, particularly in gherkin -keyword-not-\npresent-in-step (130 instances) and restricted-patterns-present \n(335 instances). PaLM-2 demonstrated a spectrum of errors, \nwith many instances involving missing -tags as (95 \noccurrences). Notably, few-shot evaluations across all models \nrevealed an overall lower number of errors compared to zero-\nshot evaluations. \n \nTABLE 6: Syntax error distribution for zero -shot. \n \nError Type GPT-\n3.5 \nGPT-\n4 \nPaLM-\n2 \nLlama-2-\n13B \ngherkin-\nkeywords-not-\nin-logical-order \n6 5 4 0 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3391815\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 7 \ngherkin-\nkeyword-not-\npresent-in-step \n1 0 58 130 \nrestricted-\npatterns-\npresent \n0 1 32 335 \nmissing-tags 0 0 95 0 \n \nTABLE 7: Syntax error distribution for few -shot. \n \nError Type GPT-\n3.5 \nGPT-\n4 \nPaLM-\n2 \nLlama-2-\n13B \ngherkin-\nkeywords-not-\nin-logical-order \n1 1 1 0 \ngherkin-\nkeyword-not-\npresent-in-step \n0 0 3 1 \nrestricted-\npatterns-\npresent \n0 0 4 28 \nmissing-tags 0 0 41 0 \n \nAlthough GPT -3.5 and GPT -4 models performed better \nwith the few-shot prompt technique, our proposed approach \nhad certain limitations within the current research scope. We \nhave manually validated some of the BDD acceptance tests \nand confirmed the validity o f those tests in terms of their \napplicability to user stories. In our present research scope, we \nhaven't explored the realms of test coverage and the validity of \ngenerated tests in terms of their applicability to user stories \nusing any formal validation process. However, we recognize \nthese areas as valuable aspects for future research. The \nfoundational findings from our research should instill \nconfidence and pave the way for a more in-depth examination \nof test coverage and validity in subsequent research in  this \nfield. Our primary aim was to validate the readiness of tests \nproduced by the LLM model, ensuring they are free from \nsyntax issues. Also, we employed a single syntax validation \ntool for validation. We assumed that the outcomes would \nlikely align across other syntax validator tools. \nFurthermore, in our current study, we couldn‚Äôt utilize any \nreal-time projects due to the lack of dataset, which can boost \nour confidence if explored with real -time project data. The \nBDD methodology emphasizes collaboration among \nstakeholders and developers for crafting acceptance tests. The \nsuggested method should facilitate this collaborative process, \nsignifying its role as a supportive tool. It implies that the \napproach should assist in the collaborative process and not be \nutilized in isolation, emphasizing the need for discussions on \nformulating acceptance tests from the proposed approach. \n \nV. CONCLUSION \nUtilizing OpenAI's GPT-3.5 and GPT-4 models and the few-\nshot prompt technique to create BDD acceptance tests from \nuser stories displays potential for improving productivity in \nagile software development and implementing standardized \nBDD practices. Incorporating the few -shot technique, \ninvolving instruction tuning with examples has demonstrated \nimproved performance compared to the zero -shot approach. \nThis advancement proves particularly beneficial in addressing \ncomplex natural language problems, such as the creation of \nBDD acceptance tests. The potential success of this approach \ncould inspire further research into utilizing LLMs for \nautomated BDD acceptance test generation to achieve \nenhanced test coverage. \nREFERENCES \n[1] G. Downs, \"Lean -agile acceptance test -driven development: B etter \nsoftware through collaboration by Ken Pugh\",  ACM SIGSOFT \nSoftware Engineering Notes, vol. 36, no. 4, pp. 34-34, 2011. \n[2] Gherkin Syntax, [Online] Available: https://cucumber.io/docs/. \n[3] Gherkin Syntax, [Online] Av ailable: \nhttps://docs.specflow.org/projects/specflow/en/latest/Gherkin/Gherki\nn-Reference.html/ \n[4] M. S. Farooq, U. Omer, A. Ramzan, M. A. Rasheed and Z. Atal, \n\"Behavior Driven Development: A Systematic Literature Review,\" \nin IEEE Access , vol. 11, pp. 88008 -88024, 2023, doi: \n10.1109/ACCESS.2023.3302356. \n[5] L. P. Binamungu, S. M. Embury and N. Konstantinou, \"Characterising \nthe quality of behaviour driven development specifications\",  Proc. \nAgile Processes Softw. Eng. Extreme Program. 21st Int. Conf. Agile \nSoftw. Develop. XP, pp. 87-102, Jun. 2020. \n[6] L. P. Binamungu, S. M. Embury and N. Konstantinou, \"Characterising \nthe quality of behaviour driven development specifications\",  Proc. \nAgile Processes Softw. Eng. Extreme Program. 21st Int. Conf. Agile \nSoftw. Develop. XP, pp. 87-102, Jun. 2020. \n[7] A. Mathur, S. Pradhan, P. Soni, D. Patel and R. Regunathan, \n\"Automated Test Case Generation Using T5 and GPT -3,\" 2023 9th \nInternational Conference on Advanced Computing and \nCommunication Systems (ICACCS) , Coimbatore, India, 2023, pp. \n1986-1992, doi: 10.1109/ICACCS57279.2023.10112971 \n[8] Zhuang, Y., Wang, X., Zhang, H., Xie , J., Zhu, X. (2018). An \nEnsemble Approach to Conversation Generation. In: Huang, X., Jiang, \nJ., Zhao, D., Feng, Y., Hong, Y. (eds) Natural Language Processing \nand Chinese Computing. NLPCC 2017. Lecture Notes in Computer \nScience(), vol 10619. Springer, Cham. https://doi.org/10.1007/978-3-\n319-73618-1_5. \n[9] M. Lafi, T. Alrawashed and A. M. Hammad, \"Automated Test Cases \nGeneration From Requirements Specification,\"  2021 International \nConference on Information Technology (ICIT) , Amman, Jordan, \n2021, pp. 852-857, doi: 10.1109/ICIT52682.2021.9491761. \n[10] X. Guo, H. Okamura and T. Dohi, \"Automated Software Test Data \nGeneration With Generative Adversarial Networks,\" in IEEE Access, \nvol. 10, pp. 20690 -20700, 2022, doi: \n10.1109/ACCESS.2022.3153347. \n[11] M. Utting, B. Legeard, F. D adeau, F. Tamagnan and F. Bouquet, \n\"Identifying and Generating Missing Tests using Machine Learning \non Execution Traces,\"  2020 IEEE International Conference On \nArtificial Intelligence Testing (AITest), Oxford, UK, 2020, pp. 83-90, \ndoi: 10.1109/AITEST49225.2020.00020.  \n[12] T. Potuzak and R. Lipka, \"Current Trends in Automated Test Case \nGeneration,\" 2023 18th Conference on Computer Science and \nIntelligence Systems (FedCSIS), Warsaw, Poland, 2023, pp. 627-636, \ndoi: 10.15439/2023F9829. \n[13] F. Y. B. Daragh and S. Malek,  \"Deep GUI: Black -box GUI Input \nGeneration with Deep Learning\", 2021 36th IEEE/ACM International \nConference on Automated Software Engineering (ASE), pp. 905-916, \nNovember 2021. \n[14] Vineeta, A. Singhal and A. Bansal, \"Generation of test oracles using \nneural network and decision tree model\", 2014 5th Int. Conf. - Conflu. \nNext Gener. Inf. Technol. Summit, pp. 313-318, September 2014. \n[15] J. Zhang, L. Zhang, M. Harman, D. Hao, Y. Jia and L. Zhang, \n\"Predictive Mutation Testing\",  IEEE Transactions on Software \nEngineering, vol. 45, no. 9, pp. 898-918, 2019. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3391815\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 7 \n[16] Github repository, [Online] Available  -\nhttps://github.com/karpurapus/BDDGPT-Automate-Tests.git. \n[17] Google colab infrastructure \n[18] Mendley-Requirement Data set (user stories), [Online] Available:  \n‚Äúhttps://data.mendeley.com/datasets/7zbk8zsd8y/1. \n[19] 45 User Story Examples to inspire your Agile Teams, [Online] \nAvailable: https://www.parabol.co/blog/user-story-examples/. \n[20] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, \nN., Dai, A. M., & Le, Q. V. (2021). Finetuned Language Models Are \nZero-Shot Learners. ArXiv. /abs/2109.01652. \n[21] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., \nLacroix, T., Rozi√®re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, \nA., Joulin, A., Grave, E.,  & Lample, G. (2023). LLaMA: Open and \nEfficient Foundation Language Models. ArXiv. /abs/2302.13971. \n[22] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., \nChild, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling \nLaws for Neural Language Models. ArXiv. /abs/2001.08361. \n[23] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, \nP., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., \nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, \nJ., Winter, C., Hesse, C., . . . Amodei, D. (2020). Language Models \nare Few-Shot Learners. ArXiv. /abs/2005.14165. \n[24] Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., \n& Zettlemoyer, L. (2022). Rethinking t he Role of Demonstrations: \nWhat Makes In-Context Learning Work? ArXiv. /abs/2202.12837. \n[25] Laskar, M. T., Bari, M. S., Rahman, M., Bhuiyan, M. A., Joty, S., & \nHuang, J. X. (2023). A Systematic Study and Comprehensive \nEvaluation of ChatGPT on Benchmark Dataset s. ArXiv. \n/abs/2305.18486. \n[26] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. \nL., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., \nBabuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, \nM., Belgum, J.,  Bello, I., . . . Zoph, B. (2023). GPT -4 Technical \nReport. ArXiv. /abs/2303.08774. \n[27] Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., \nShakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., \nShafey, L. E., Huang, Y., Mishr a, G., Moreira, E., Omernick, M., \nRobinson, K., Ruder, S., . . . Wu, Y. (2023). PaLM 2 Technical Report. \nArXiv. /abs/2305.10403. \n[28] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., \nRoberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., \nSchuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, \nP., Tay, Y., Shazeer, N., Prabhakaran, V., . . . Fiedel, N. (2022). PaLM: \nScaling Language Modeling with Pathways. ArXiv. /abs/2204.02311. \n[29] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, \nY., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., \nBlecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., \nFernandes, J., Fu, J., Fu, W., . . . Scialom, T. (2023). Llama 2: Open \nFoundation and Fine-Tuned Chat Models. ArXiv. /abs/2307.09288. \n[30] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., \nCistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., \nVon Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, \nS., . . . Rush, A. M. (2019). HuggingFace's Transformers: State-of-the-\nart Natural Language Processing. ArXiv. /abs/1910.03771. \n[31] Gherkin-lint, Github repository, [Online] Available: GitHub - gherkin-\nlint/gherkin-lint: A Gherkin linter/validator written in javascript. \n[32] Gherkin-lint, npm, [Online] Available: gherkin-lint - npm \n(npmjs.com). \n[33] Kamil Nicieja, Writing Great Specifications: Using Specification by \nExample and Gherkin , Manning, 2017. \n[34] O. Bezsmertnyi, N. Golian, V. Golian and I. Afanasieva, \"Behavior \nDriven Development Approach in the Modern Quality Control \nProcess,\" 2020 IEEE International Conference on Problems of \nInfocommunications. Science and Technology (PIC S&T), Kharkiv, \nUkraine, 2020, pp. 217 -220, doi: \n10.1109/PICST51311.2020.9467891. keywords: \n{Visualization;Automation;Process control;Quality \ncontrol;Documentation;Manuals;Best practices;behaviour driven \ndevelopment;test driven development;quality \ncontrol;gherkin;practice;technique;scenario;feature;keyword;step}, \n[35] H. M. Abushama, H. A. Alassam and F. A. Elhaj, \"The effect of Test-\nDriven Development and Behavior -Driven Development on Project \nSuccess Factors: A Systematic Literature Review Based Study,\" 2020 \nInternational Conference on Computer, Control, Electrical, and \nElectronics Engineering (ICCCEEE), Khartoum, Sudan, 2021, pp. 1 -\n9, doi: 10.1109/ICCCEEE49695.2021.9429593. keywords: {Digital \ncontrol;Systematics;Bibliographies;Customer \nsatisfaction;Software;Testing;Test Driven Develo pment;Behavior \nDriven Development;Test Last Development;Systematic Literature \nReview;Agile Testing;Projects Success Factors}, \n[36] Wei Wang, Vincent W. Zheng, Han Yu, and Chunyan Miao. 2019. A \nSurvey of Zero-Shot Learning: Settings, Methods, and Applications. \nACM Trans. Intell. Syst. Technol. 10, 2, Article 13 (March 2019), 37 \npages. https://doi.org/10.1145/3293318. \n[37] Xian, Y., Lampert, C. H., Schiele, B., & Akata, Z. (2017). Zero -Shot \nLearning -- A Comprehensive Evaluation of the Good, the Bad and the \nUgly. ArXiv. /abs/1707.00600. \n[38] Kalyan, Katikapalli S. \"A Survey of GPT -3 Family Large Language \nModels Including ChatGPT and GPT -4.\" ArXiv, (2023). Accessed \nFebruary 5, 2024. /abs/2310.12321. \n[39] Song, Yisheng, Ting Wang, Subrota K. Mondal, and Jyoti P. Sahoo. \n\"A Comprehensive Survey of Few -shot Learning: Evolution, \nApplications, Challenges, and Opportunities.\" ArXiv, (2022). \nAccessed February 5, 2024. /abs/2205.06743. \n[40] R. Khan , A. K. Srivastava and D. Pandey, \"Agile approach for \nSoftware Testing process,\" 2016 International Conference System \nModeling & Advancement in Research Trends (SMART), \nMoradabad, India, 2016, pp. 3 -6, doi: \n10.1109/SYSMART.2016.7894479. keywords: {Software ;Software \ntesting;Industries;Encoding;Adaptation models;Iterative \nmethods;Agile Testing;Software Engineering;Software \nTesting;Software Development}. \n \n \n \n \nSHANTHI KARPURAPU received the Bachelor of Technology degree in \nchemical engineering from Osmania University, Hyderabad, India  and the \nMasters technology degree in chemical engineering from Institute of \nChemical Technology, Mumbai, India.  \nShe has over a decade of experience leading, designing, and developing test \nautomation solutions for various platforms across healthcare, banking, and \nmanufacturing industries using Agile and Waterfall methodologies. She is \nexperienced in building reusable and extendable automation frameworks for \nweb applications, REST, SOAP, and microservices. She is a strong follower \nof the shift-left testing approach, a certified AWS Cloud practitioner, and a \nmachine learning specialist. She is passionate about utilizing AI -related \ntechnologies in software testing and the healthcare industry. \n \nSRAVANTHY MYNENI earned master of science in information \ntechnology and management from Illinois institute of technology, Chicago, \nIllinois in 2017 and Bachelor‚Äôs degree in computer science in 2013. She is  \ncurrently working as an engineer focused on data engineering and analysis. \nShe has 8+ years of experience in designing, building and deploying data  \n \ncentric solutions using Agile and Waterfall methodologies.  She is \nenthusiastic about data analysis, data en gineering and AI application to \nprovide solutions for real world problems. \n \nUNNATI NETTUR currently pursuing an undergraduate degree in \nComputer Science at Virginia Tech, Blacksburg, VA, USA. She possesses \nan avid curiosity about the constantly evolving field of technology and \nsoftware development, with a particular interest in  Artificial Intelligence. \nShe is passionate about gaining experience in building innovative and  \ncreative solutions for current issues in the field of software engineering. \n \nLIKHIT SAGAR GAJJA pursuing a Computer Science Bachelor‚Äôs degree \nat BML Munjal University, Haryana, INDIA. He is evident  in showing his \npassion for the dynamic field of technology and software development. His \nspecific interests include Artificial Intelligence, P rompt Engineering, and \nGame Designing technologies, highlighting his dedication to obtaining \nhands-on experience and developing innovative solutions for real -time \nissues in software engineering. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3391815\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2017 \n \nDAVE BURKE  earned his Master‚Äôs Degree in the Management of  \nInformation Technology from the University of Virginia and Bachelors in \nEconomics from Providence College. \nHe brings over 24 years of leadership and full life cycle software \ndevelopment experience to Coveros. His experience includes work across \nvarious ma rkets including: Telecommunications, Media, Logistics, \nTransportation, and Financial Services. Prior to Coveros, He was the COO \nfor Command Information/Digital Focus, an industry leader in agile \nsoftware development and held leadership roles at Verizon \nCommunications. Additionally, he served 7 years as an officer in the United \nStates Army. \n \nTOM STIEHM holds a B.S. degree in Computer Science from George \nMason University. He is a 20 year veteran of the Information Technology \nindustry. He has spent the past 10  years managing, designing and \nimplementing software products and applications using agile software \ndevelopment methods. Prior to Coveros, He held a variety of CTO and \narchitect positions at software development companies. He is a member of \nthe northern Virginia BEA users group and the northern Virginia Java users \ngroup. \n \nJEFFERY PAYNE holds a B.S. in Computer Science from Allegheny \nCollege and an M.S. in Computer Science from The College of William and \nMary.  He has led Coveros since its inception in 2008. Under his guidance, \nthe company has become a recognized market leader in secure agile \nsoftware development.  He is a popular keynote and featured speaker at \ntechnology conferences and has testified before Congress on technology  \nissues such as intellectual property rights and cyber -terrorism. Prior to \nCoveros, he was co -founder, CEO, and Chairman of the Board of Cigital, \nwhere he led the startup and growth of the company for 16 years.  He is a \nformer ACM National Lecturer and the co-founder of the Northern Virginia \nChapter of the IEEE Computer Society.   \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3391815\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Agile software development",
  "concepts": [
    {
      "name": "Agile software development",
      "score": 0.6461685299873352
    },
    {
      "name": "Test (biology)",
      "score": 0.5948900580406189
    },
    {
      "name": "Computer science",
      "score": 0.5923702716827393
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5895748734474182
    },
    {
      "name": "Process (computing)",
      "score": 0.4715571403503418
    },
    {
      "name": "Software engineering",
      "score": 0.36007076501846313
    },
    {
      "name": "Process management",
      "score": 0.3434804677963257
    },
    {
      "name": "Engineering",
      "score": 0.19145077466964722
    },
    {
      "name": "Programming language",
      "score": 0.14180275797843933
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": []
}