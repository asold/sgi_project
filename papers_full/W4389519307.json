{
  "title": "Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens",
  "url": "https://openalex.org/W4389519307",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4208472320",
      "name": "Byung-Doh Oh",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2097769359",
      "name": "William R. Schuler",
      "affiliations": [
        "The Ohio State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964531451",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W2997938465",
    "https://openalex.org/W4361766487",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3171953676",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W4309994492",
    "https://openalex.org/W4385573487",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W2139450036",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4385572280",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3083146265",
    "https://openalex.org/W2795342569",
    "https://openalex.org/W3033254023",
    "https://openalex.org/W2962941914",
    "https://openalex.org/W1951724000",
    "https://openalex.org/W4281758439",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W4214909510"
  ],
  "abstract": "Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a 'tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1915–1921\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTransformer-Based Language Model Surprisal Predicts\nHuman Reading Times Best with About Two Billion Training Tokens\nByung-Doh Oh\nDepartment of Linguistics\nThe Ohio State University\noh.531@osu.edu\nWilliam Schuler\nDepartment of Linguistics\nThe Ohio State University\nschuler.77@osu.edu\nAbstract\nRecent psycholinguistic studies have drawn\nconflicting conclusions about the relationship\nbetween the quality of a language model and\nthe ability of its surprisal estimates to predict\nhuman reading times, which has been specu-\nlated to be due to the large gap in both the\namount of training data and model capacity\nacross studies. The current work aims to consol-\nidate these findings by evaluating surprisal esti-\nmates from Transformer-based language model\nvariants that vary systematically in the amount\nof training data and model capacity on their\nability to predict human reading times. The\nresults show that surprisal estimates from most\nvariants with contemporary model capacities\nprovide the best fit after seeing about two bil-\nlion training tokens, after which they begin to\ndiverge from humanlike expectations. Addi-\ntionally, newly-trained smaller model variants\nreveal a ‘tipping point’ at convergence, after\nwhich the decrease in language model perplex-\nity begins to result in poorer fits to human read-\ning times. These results suggest that the mas-\nsive amount of training data is mainly respon-\nsible for the poorer fit achieved by surprisal\nfrom larger pre-trained language models, and\nthat a certain degree of model capacity is nec-\nessary for Transformer-based language models\nto capture humanlike expectations.\n1 Introduction\nThe predictability of upcoming linguistic material\nhas long been considered a crucial factor underly-\ning difficulty in human sentence processing (Hale,\n2001; Levy, 2008), and has received empirical\nsupport from numerous studies showing surprisal\n(Shannon, 1948) to be highly predictive of relevant\nbehavioral and neural measures (e.g. Demberg and\nKeller, 2008; Smith and Levy, 2013; Hale et al.,\n2018; Shain et al., 2020). Since language models\n(LMs) are trained to estimate a conditional proba-\nbility distribution of a word given its context, sur-\nprisal estimates calculated from them have often\nbeen evaluated on their ability to predict measures\nof processing difficulty.\nRecent studies in computational psycholinguis-\ntics have provided conflicting evidence with regard\nto the relationship between LM quality (i.e. next-\nword prediction accuracy) and goodness-of-fit to\nhuman reading times. Earlier work using newly-\ntrained LMs showed a negative relationship be-\ntween LM perplexity and predictive power of sur-\nprisal estimates (Goodkind and Bicknell, 2018;\nWilcox et al., 2020; Merkx and Frank, 2021),\nbut more recent work using large pre-trained\nTransformer-based LMs (e.g. GPT-2; Radford et al.,\n2019) show a robust positive relationship between\nthe two variables (Oh et al., 2022; Oh and Schuler,\n2023). While Oh and Schuler (2023) conjecture\nthat these studies capture two distinct regimes, it\nremains less clear where the reversal in this rela-\ntionship happens. The main challenge in answering\nthis question lies in the massive difference in terms\nof both the amount of training data and the model\ncapacity of LMs that were studied.\nThe current study aims to cover this conceptual\nmiddle ground by evaluating, on their ability to pre-\ndict human reading times, surprisal estimates from\nTransformer-based LM variants that vary system-\natically in the amount of training data and model\ncapacity. Results from regression analyses show\nthat surprisal from most LM variants with contem-\nporary model capacities make the biggest contri-\nbution to regression model fit after seeing about\ntwo billion tokens of training data, after which ad-\nditional training data result in surprisal estimates\nthat continue to diverge from humanlike expecta-\ntions. Additionally, surprisal estimates from newly-\ntrained smaller LM variants reveal a ‘tipping point’\nat convergence, after which the decrease in perplex-\nity begins to result in poorer fits to human read-\ning times. Taken together, these results suggest\nthat the vast amount of training data is mainly re-\nsponsible for the poorer fit achieved by surprisal\n1915\nfrom larger Transformer-based pre-trained LMs\n(Oh et al., 2022; Oh and Schuler, 2023), and that\na certain degree of model capacity is necessary\nfor Transformer-based LMs to capture humanlike\nexpectations that manifest in reading times.\n2 Experiment 1: Influence of Training\nData Size\nThe first experiment examines the influence of\ntraining data size on the predictive power of\nTransformer-based LM surprisal by evaluating LM\nvariants at various points in training on self-paced\nreading times from the Natural Stories Corpus\n(Futrell et al., 2021) and go-past eye-gaze durations\nfrom the Dundee Corpus (Kennedy et al., 2003).\n2.1 Response Data\nThe Natural Stories Corpus contains reading times\nfrom 181 subjects that read 10 naturalistic English\nstories consisting a total of 10,245 tokens. The data\npoints were filtered to remove those for sentence-\ninitial and final words, those from subjects who\nanswered three or fewer comprehension questions\ncorrectly, and those shorter than 100 ms or longer\nthan 3000 ms, which resulted in a total of 384,905\nobservations in the exploratory set. The Dundee\nCorpus contains eye-gaze durations from 10 sub-\njects that read 67 newspaper editorials consisting\na total of 51,501 tokens. The data points were fil-\ntered to exclude those for unfixated words, words\nfollowing saccades longer than four words, and\nsentence-, screen-, document-, and line-initial and\nfinal words, which resulted in a total of 98,115 ob-\nservations in the exploratory set.1 All observations\nwere log-transformed prior to model fitting.\n2.2 Predictors\nThis experiment evaluates surprisal estimates from\neight variants of Pythia LMs (Biderman et al.,\n2023), whose intermediate parameters were saved\nat various points during training. Pythia LMs\nare decoder-only autoregressive Transformer-based\nmodels2 whose variants differ primarily in their ca-\npacity. The model capacities of the Pythia variants\nare summarized in Table 1.\n1The held-out set of each corpus, which have a comparable\nnumber of observations, is reserved for statistical significance\ntesting and therefore was not analyzed in this work.\n2Technical details such as the parallelization of self-\nattention/feedforward computations and the separation of em-\nbedding/projection matrices differentiate Pythia LMs from\nother large language model families.\nModel #L #H dmodel #Parameters\nPythia 70M 6 8 512 ∼70M\nPythia 160M 12 12 768 ∼160M\nPythia 410M 24 16 1024 ∼410M\nPythia 1B 16 8 2048 ∼1B\nPythia 1.4B 24 16 2048 ∼1.4B\nPythia 2.8B 32 32 2560 ∼2.8B\nPythia 6.9B 32 32 4096 ∼6.9B\nPythia 12B 36 40 5120 ∼12B\nTable 1: Model capacities of Pythia variants whose\nsurprisal estimates were examined in this work. #L, #H,\nand dmodel refer to number of layers, number of attention\nheads per layer, and embedding size, respectively.\nCrucially for this experiment, all eight Pythia\nvariants were trained using identical batches of\ntraining examples that were presented in the same\norder. These training examples come from the Pile\n(Gao et al., 2020), which is a collection of English\nlanguage datasets that consist of around 300 bil-\nlion tokens. Batches of 1,024 examples with a\nsequence length of 2,048 (i.e. 2,097,152 tokens)\nwere used to train the eight variants for 143,000\nsteps, which amounts to about one epoch of the\nentire Pile dataset. Model parameters that were\nsaved during early training stages (i.e. after 1, 2,\n4, ..., 256, 512 steps) as well as after every 1,000\nsteps are publicly available.\nEach article of the Natural Stories Corpus and\neach article of the Dundee Corpus was tokenized\nby Pythia’s byte-pair encoding (BPE; Sennrich\net al., 2016) tokenizer and provided as input to each\nmodel variant. For each model variant, all publicly\navailable intermediate model weights were used to\ncalculate surprisal estimates on the two corpora. In\ncases where each story or article was longer than\na single context window of 2,048 tokens, surprisal\nestimates for the remaining tokens were calculated\nby using the second half of the previous context\nwindow as the first half of a new context window.\n2.3 Regression Modeling\nSubsequently, following previous work (Oh et al.,\n2022; Oh and Schuler, 2023), a ‘baseline’ linear\nmixed-effects (LME) model that contains baseline\npredictors for low-level cognitive processing, and\n‘full’ LME models that additionally contain each\nLM surprisal predictor, were fit to self-paced read-\ning times and go-past durations using lme4 (Bates\net al., 2015). These baseline predictors are word\nlength in characters and index of word position\nin each sentence (Natural Stories and Dundee), as\nwell as saccade length and whether or not the pre-\n1916\nFigure 1: Increase in regression model log-likelihood due to including each surprisal estimate from Pythia variants\nas a function of training steps (top) and perplexity (middle; the stars indicate the fully trained versions after 143,000\nsteps), as well as perplexity as a function of training steps (bottom) on the exploratory set of Natural Stories (left)\nand Dundee data (right).\nvious word was fixated (Dundee only). All pre-\ndictors were centered and scaled, 3 and the LME\nmodels included by-subject random slopes for all\nfixed effects and random intercepts for each subject.\nIn addition, a random intercept for each subject-\nsentence interaction was included for self-paced\nreading times collected from 181 subjects, and a\nrandom intercept for each sentence was included\nfor eye-gaze durations collected from a smaller\nnumber of 10 subjects. Once the regression mod-\nels were fit, the increase in regression model log-\nlikelihood (∆LL) was calculated for each regres-\nsion model by subtracting the log-likelihood of the\nbaseline regression model from that of a full re-\n3‘Spillover’ predictors were not included in the regression\nmodels to avoid convergence issues.\ngression model. Finally, the perplexity of each LM\nvariant was calculated on the two corpora.\n2.4 Results\nThe results in Figure 1 show that across both cor-\npora, surprisal from most LM variants made the\nbiggest contribution to regression model fit after\n1,000 training steps (i.e. after about two billion\ntokens).4 This seems to represent a ‘humanlike\n4Results from after 2,000 steps were selectively omitted\nfor clarity, as they were consistent with the general trend. As\npointed out by a reviewer, including a frequency-based pre-\ndictor in the regression models may change the exact location\nof this peak. However, this work avoids potential confounds\nintroduced by the corpus used for frequency estimation by\nevaluating surprisal estimates on their own following the pro-\ntocols of Oh and Schuler (2023).\n1917\nFigure 2: Increase in regression model log-likelihood due to including each surprisal estimate from newly-trained\nLM variants as a function of training steps (top) and perplexity (middle; the stars indicate the fully trained versions\nafter 10,000 steps), as well as perplexity as a function of training steps (bottom) on the exploratory set of Natural\nStories (left) and Dundee data (right). The variants are labeled using their number of layers, number of attention\nheads per layer, and embedding size, in that order.\noptimum,’ after which surprisal estimates begin to\ndiverge from humanlike expectations as training\ncontinues. At this point in training, there appears\nto be no systematic relationship between model ca-\npacity and predictive power of surprisal estimates.\nHowever, after all 143,000 training steps (i.e. after\nabout 300 billion tokens), the eight model variants\nshow a strictly monotonic and negative relation-\nship, which directly replicates the findings of Oh\nand Schuler (2023).5 Taken together, these results\nindicate that the vast amount of training data is re-\nsponsible for the poorer fit achieved by surprisal\n5The best-fitting line between log perplexity and ∆LL\nof these variants had a slope significantly greater than 0 at\np <0.05 level according to a one-tailedt-test on both corpora.\nfrom larger Transformer-based LMs.\n3 Experiment 2: Influence of Model\nCapacity\nThe second experiment further examines the re-\nlationship between model capacity and predic-\ntive power of surprisal estimates by evaluating\nTransformer-based LM variants smaller than the\nPythia variants at various points in training, follow-\ning similar procedures as Experiment 1.\n3.1 Procedures\nSurprisal estimates from eight smaller LM vari-\nants were evaluated at various points during train-\ning in this experiment. The largest of these vari-\n1918\nants has the same model capacity as the smallest\nPythia 70M variant, and the smaller variants were\ndesigned to have fewer layers and attention heads,\nas well as smaller embeddings. These variants\nwere trained closely following the training proce-\ndures of the Pythia variants, including the size and\norder of training batches. For computational ef-\nficiency, these variants were trained for the first\n10,000 training steps, based on the observation that\n∆LL on both corpora did not change substantially\nafter 8,000 steps for the smallest Pythia variant. 6\nThe predictive power of resulting surprisal esti-\nmates was evaluated following identical procedures\nas Experiment 1.\n3.2 Results\nThe results in Figure 2 show that surprisal from the\ntwo largest variants made the biggest contribution\nto regression model fit after 1,000 training steps on\nboth corpora, replicating the results of Experiment\n1. In contrast, the smaller variants such as the 2-\n2-128 and 2-3-192 variants seem to peak later at\naround 2,000 training steps and stabilize afterward.\nAfter all 10,000 training steps, the model variants\nshow a reversal in the relationship between LM\nperplexity and fit to reading times; the 2-3-192\nvariant seems to represent a ‘tipping point,’ after\nwhich the decrease in perplexity starts to result in\npoorer fits to human reading times. Additionally,\nvariants that are smaller than this yield surprisal\nestimates that are less predictive of reading times\nwhen sufficiently trained. These results suggest\nthat a certain degree of model capacity is necessary\nfor Transformer-based LMs to capture humanlike\nexpectations that manifest in reading times.\n4 Discussion and Conclusion\nThis work aims to consolidate conflicting findings\nabout the relationship between LM quality and\nthe predictive power of its surprisal estimates by\nsystematically manipulating the amount of train-\ning data and model capacity. Experimental re-\nsults show that surprisal from most contemporary\nTransformer-based LM variants provide the best\nfit to human reading times with about two billion\ntraining tokens, after which they begin to diverge\nfrom humanlike expectations. It is conjectured that\nearly training data up to about two billion tokens is\n6Refer to Appendix A for the model capacities of these\nvariants as well as further details on their training procedures.\nCode and trained weights are available at https://github.\ncom/byungdoh/slm_surprisal.\nhelpful for learning e.g. selectional preferences that\nalign well with humanlike prediction and process-\ning difficulty. However, as the models see more\ntraining data, they are able to achieve ‘superhuman’\nprediction, which makes their surprisal estimates\ndiverge more and more from human reading times\nas training continues. The words for which predic-\ntion by LMs improves with massive amounts of\ntraining data are likely to be open-class words like\nnouns and adjectives, whose reading times were\nidentifed as being most severely underpredicted by\ntheir surprisal estimates (Oh and Schuler, 2023).\nMoreover, at the end of training, these model\nvariants show a strictly monotonic and negative\nrelationship between perplexity and fit to human\nreading times. This directly replicates the find-\nings of Oh et al. (2022) and adds to a growing\nbody of research reporting an inverse correlation be-\ntween model size and regression model fit (Kurib-\nayashi et al., 2022; Shain et al., 2022; de Varda and\nMarelli, 2023). The current results demonstrate\nthat this relationship emerges with large amounts\nof training data and becomes stronger as training\ncontinues. The bottleneck posed by the limited\nmodel capacity of the smaller variants appears to\nprevent them from learning to make excessively\naccurate predictions that cause the divergence be-\ntween surprisal and human reading times. How-\never, newly-trained LM variants that are smaller\nthan those of contemporary standards reveal a ‘tip-\nping point’ at convergence, which indicates that a\ncertain amount of model capacity is necessary for\nLMs to correctly learn humanlike expectations.\nFinally, across both experiments, model capacity\ndoes not seem to modulate the relationship between\nperplexity and fit to human reading times, with data\npoints from different LM variants forming a con-\ntinuous curve between log perplexity and ∆LL.\nThis suggests that Transformer-based LMs of dif-\nferent capacities share a similar inductive bias that\ninitially improves the fit of surprisal estimates to\nhuman reading times but begins to have an adverse\neffect on it with large amounts of training data.\nAcknowledgments\nWe thank the reviewers and the area chair for their\nhelpful comments. This work was supported by\nthe National Science Foundation grant #1816891.\nAll views expressed are those of the authors and\ndo not necessarily reflect the views of the National\nScience Foundation.\n1919\nLimitations\nThe connection between conditional probabilities\nof Transformer-based language models and human\nsentence processing drawn in this work is based on\nlanguage model variants trained on English text and\ndata from human subjects that are native speakers\nof English. Therefore, the connection made in this\nwork may not generalize to other languages.\nEthics Statement\nExperiments presented in this work used datasets\nfrom previously published research (Futrell et al.,\n2021; Kennedy et al., 2003), in which the proce-\ndures for data collection and validation are outlined.\nAs this work focuses on studying the connection\nbetween conditional probabilities of language mod-\nels and human sentence processing, its potential\nnegative impacts on society seem to be minimal.\nReferences\nAlex Andonian, Quentin Anthony, Stella Biderman, Sid\nBlack, Preetham Gali, Leo Gao, Eric Hallahan, Josh\nLevy-Kramer, Connor Leahy, Lucas Nestler, Kip\nParker, Michael Pieler, Shivanshu Purohit, Tri Songz,\nPhil Wang, and Samuel Weinbach. 2021. GPT-NeoX:\nLarge scale autoregressive language modeling in Py-\nTorch.\nDouglas Bates, Martin Mächler, Ben Bolker, and Steve\nWalker. 2015. Fitting linear mixed-effects models\nusing lme4. Journal of Statistical Software, 67(1):1–\n48.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory\nAnthony, Herbie Bradley, Kyle O’Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, Aviya Skowron,\nLintang Sutawika, and Oskar van der Wal. 2023.\nPythia: A suite for analyzing large language mod-\nels across training and scaling. In Proceedings of the\n40th International Conference on Machine Learning,\nvolume 202, pages 2397–2430.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness.\narXiv preprint, arXiv:2205.14135.\nAndrea de Varda and Marco Marelli. 2023. Scaling\nin cognitive modelling: a multilingual approach to\nhuman reading times. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics, pages 139–149.\nVera Demberg and Frank Keller. 2008. Data from eye-\ntracking corpora as evidence for theories of syntactic\nprocessing complexity. Cognition, 109(2):193–210.\nRichard Futrell, Edward Gibson, Harry J. Tily, Idan\nBlank, Anastasia Vishnevetsky, Steven Piantadosi,\nand Evelina Fedorenko. 2021. The Natural Stories\ncorpus: A reading-time corpus of English texts con-\ntaining rare syntactic constructions. Language Re-\nsources and Evaluation, 55:63–77.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800GB dataset of diverse text for language modeling.\narXiv preprint, arXiv:2101.00027.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive power of word surprisal for reading times is a\nlinear function of language model quality. In Pro-\nceedings of the 8th Workshop on Cognitive Modeling\nand Computational Linguistics, pages 10–18.\nJohn Hale. 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Proceedings of the Second\nMeeting of the North American Chapter of the Asso-\nciation for Computational Linguistics on Language\nTechnologies, pages 1–8.\nJohn Hale, Chris Dyer, Adhiguna Kuncoro, and\nJonathan Brennan. 2018. Finding syntax in human\nencephalography with beam search. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, pages 2727–2736.\nAlan Kennedy, Robin Hill, and Joël Pynte. 2003. The\nDundee Corpus. In Proceedings of the 12th Euro-\npean Conference on Eye Movement.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the 3rd International Conference on Learning Rep-\nresentations, pages 1–15.\nTatsuki Kuribayashi, Yohei Oseki, Ana Brassard, and\nKentaro Inui. 2022. Context limitations make neural\nlanguage models more human-like. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 10421–10436.\nRoger Levy. 2008. Expectation-based syntactic compre-\nhension. Cognition, 106(3):1126–1177.\nDanny Merkx and Stefan L. Frank. 2021. Human sen-\ntence processing: Recurrence or attention? In Pro-\nceedings of the Workshop on Cognitive Modeling and\nComputational Linguistics, pages 12–22.\nByung-Doh Oh, Christian Clark, and William Schuler.\n2022. Comparison of structural parsers and neural\nlanguage models as surprisal estimators. Frontiers in\nArtificial Intelligence, 5:777963.\nByung-Doh Oh and William Schuler. 2023. Why does\nsurprisal from larger Transformer-based language\nmodels provide a poorer fit to human reading times?\nTransactions of the Association for Computational\nLinguistics, 11:336–350.\n1920\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nTechnical Report.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. ZeRO: Memory optimiza-\ntions toward training trillion parameter models. In\nProceedings of the International Conference for High\nPerformance Computing, Networking, Storage and\nAnalysis, pages 1–16.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1715–1725.\nCory Shain, Idan Asher Blank, Marten van Schijn-\ndel, William Schuler, and Evelina Fedorenko. 2020.\nfMRI reveals language-specific predictive coding dur-\ning naturalistic sentence comprehension. Neuropsy-\nchologia, 138:107307.\nCory Shain, Clara Meister, Tiago Pimentel, Ryan Cot-\nterell, and Roger Levy. 2022. Large-scale evidence\nfor logarithmic effects of word predictability on read-\ning time. PsyArXiv preprint.\nClaude Elwood Shannon. 1948. A mathematical theory\nof communication. Bell System Technical Journal,\n27:379–423.\nNathaniel J. Smith and Roger Levy. 2013. The effect\nof word predictability on reading time is logarithmic.\nCognition, 128:302–319.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger P. Levy. 2020. On the predictive\npower of neural language models for human real-\ntime comprehension behavior. In Proceedings of\nthe 42nd Annual Meeting of the Cognitive Science\nSociety, pages 1707–1713.\nA Model Capacities and Training\nProcedures of Smaller LM Variants\nThe eight LM variants that were trained as part\nof Experiment 2 are decoder-only autoregressive\nTransformer-based models that share the same ar-\nchitecture as the Pythia LM variants (Biderman\net al., 2023). Their model capacities are summa-\nrized in Table 2.\nThese variants were trained using the GPT-NeoX\nlibrary (Andonian et al., 2021) closely following\nthe training procedures of the Pythia LM variants.7\nIdentical training batches of 1,024 examples with a\nsequence length of 2,048 from the Pile (Gao et al.,\n7The only minor difference is that the FlashAttention (Dao\net al., 2022) implementation of scaled dot-product attention\ncould not be used during training due to a mismatch in GPU\nhardware specifications.\nModel #L #H dmodel #Parameters\nRepro 1-1-64 1 1 64 ∼6M\nRepro 1-2-128 1 2 128 ∼13M\nRepro 2-2-128 2 2 128 ∼13M\nRepro 2-3-192 2 3 192 ∼20M\nRepro 2-4-256 2 4 256 ∼27M\nRepro 3-4-256 3 4 256 ∼28M\nRepro 4-6-384 4 6 384 ∼46M\nRepro 6-8-512 6 8 512 ∼70M\nTable 2: Model capacities of newly-trained LM variants\nwhose surprisal estimates were examined in this work.\n#L, #H, and dmodel refer to number of layers, number of\nattention heads, and embedding size, respectively.\n2020) were provided to each variant in the same or-\nder as the Pythia variants. The variants were trained\nusing the Zero Redundancy Optimizer (ZeRO; Ra-\njbhandari et al., 2020) implementation of Adam\n(Kingma and Ba, 2015) with a learning rate of\n0.001. The learning rate was warmed up linearly\nover the first 1% of training steps (i.e. 1,430 steps)\nand were subsequently lowered to a minimum of\n0.0001 following a cosine annealing schedule over\nthe remainder of the 143,000 training steps. How-\never, for computational efficiency, training was\nstopped after the first 10,000 training steps. For\ncomparability with the Pythia variants, intermedi-\nate parameters were saved during early training\nstages (i.e. after 1, 2, 4, ..., 256, 512 steps) as well\nas after every 500 steps from step 1,000 onward.\n1921",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9045259952545166
    },
    {
      "name": "Language model",
      "score": 0.797310471534729
    },
    {
      "name": "Computer science",
      "score": 0.7367368340492249
    },
    {
      "name": "Transformer",
      "score": 0.7098182439804077
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49875497817993164
    },
    {
      "name": "Natural language processing",
      "score": 0.49809932708740234
    },
    {
      "name": "Machine learning",
      "score": 0.3471566140651703
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    }
  ]
}