{
  "title": "Learning to Generate Diverse Dance Motions with Transformer",
  "url": "https://openalex.org/W3068510429",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2753267045",
      "name": "Li Jiaman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4310698674",
      "name": "Yin, Yihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2365258233",
      "name": "Chu Hang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2073766686",
      "name": "Zhou, Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2419242279",
      "name": "Wang, Tingwu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748935333",
      "name": "Fidler, Sanja",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A875780830",
      "name": "Li, Hao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2728472980",
    "https://openalex.org/W2964203186",
    "https://openalex.org/W2947432162",
    "https://openalex.org/W2088563154",
    "https://openalex.org/W2901994722",
    "https://openalex.org/W2796347433",
    "https://openalex.org/W2899129842",
    "https://openalex.org/W3025978279",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2962916650",
    "https://openalex.org/W2068357230",
    "https://openalex.org/W2191779130",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963402313",
    "https://openalex.org/W2983796203",
    "https://openalex.org/W2469134594",
    "https://openalex.org/W2963389355",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W2989607414",
    "https://openalex.org/W2296629652",
    "https://openalex.org/W3036133893",
    "https://openalex.org/W2402172128"
  ],
  "abstract": "With the ongoing pandemic, virtual concerts and live events using digitized performances of musicians are getting traction on massive multiplayer online worlds. However, well choreographed dance movements are extremely complex to animate and would involve an expensive and tedious production process. In addition to the use of complex motion capture systems, it typically requires a collaborative effort between animators, dancers, and choreographers. We introduce a complete system for dance motion synthesis, which can generate complex and highly diverse dance sequences given an input music sequence. As motion capture data is limited for the range of dance motions and styles, we introduce a massive dance motion data set that is created from YouTube videos. We also present a novel two-stream motion transformer generative model, which can generate motion sequences with high flexibility. We also introduce new evaluation metrics for the quality of synthesized dance motions, and demonstrate that our system can outperform state-of-the-art methods. Our system provides high-quality animations suitable for large crowds for virtual concerts and can also be used as reference for professional animation pipelines. Most importantly, we show that vast online videos can be effective in training dance motion models.",
  "full_text": "Learning to Generate Diverse Dance Motions with Transformer\nJiaman Li1,2 Yihang Yin3 Hang Chu4,5 Yi Zhou1 Tingwu Wang4,5 Sanja Fidler4,5 and Hao Li6\n1University of Southern California 2USC Institute for Creative Technologies 3Beihang University\n4University of Toronto 5Vector Institute 6Pinscreen\nFigure 1:Given novel music (shown in the second row), our model generates diverse dance motions following beats (shown in the ﬁrst row).\nThe green box shows the initial pose for dance motion synthesis.\nAbstract\nWith the ongoing pandemic, virtual concerts and live events using digitized performances of musicians are getting traction\non massive multiplayer online worlds. However, well choreographed dance movements are extremely complex to animate and\nwould involve an expensive and tedious production process. In addition to the use of complex motion capture systems, it typically\nrequires a collaborative effort between animators, dancers, and choreographers. We introduce a complete system for dance\nmotion synthesis, which can generate complex and highly diverse dance sequences given an input music sequence. As motion\ncapture data is limited for the range of dance motions and styles, we introduce a massive dance motion data set that is created\nfrom YouTube videos. We also present a novel two-stream motion transformer generative model, which can generate motion\nsequences with high ﬂexibility. We also introduce new evaluation metrics for the quality of synthesized dance motions, and\ndemonstrate that our system can outperform state-of-the-art methods. Our system provides high-quality animations suitable for\nlarge crowds for virtual concerts and can also be used as reference for professional animation pipelines. Most importantly, we\nshow that vast online videos can be effective in training dance motion models.\nCCS Concepts\n•Computing methodologies→Motion Generation;\n1. Introduction\nDue to the ongoing COVID-19 pandemic, an entire global live\nevents industry is being shut down. Interactive Vtuber perfor-\nmances and virtual music concerts that take place in online gaming\nplatforms are becoming increasingly popular. Examples include the\nwidely popular holographic and VR concerts with Hatsune Miku,\nor the virtual rap event in Fortnite, performed by a digital avatar\nof Travis Scott, hosting tens of millions of viewers in real-time in a\nmassive multiplayer online setting. The ability to generate complex\nand believable dance motions through music alone could impact a\nbroad range of interactive and immersive applications in entertain-\nment. In existing production settings, dance animations are typi-\ncally generated through carefully choreographed performances and\nthey are often captured using complex motion capture systems. In-\ntensive manual labor is generally required for data clean up and\nanimation reﬁnement, which results in long and expensive produc-\ntion cycles.\nMore recently, data-driven motion synthesis methods [HSK16,\nLZX∗18] have been introduced to scale the production of anima-\ntion without the need of actual performers. However, most of these\ntechniques are based on a very limited diversity of motion, e.g. the\npopular CMU mocap dataset [LLC] in [LZX ∗18] containing only\ntwo kinds of dances with a duration of less than one hour. These se-\nquences are often very short and difﬁcult to expand. Furthermore,\narXiv:2008.08171v1  [cs.CV]  18 Aug 2020\n2 Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler & Hao Li / Learning to Generate Diverse Dance Motions with Transformer\nthe movements are often monotonous with repetitive patterns and\ncontain little variation and diversity. Therefore, models trained on\nthese mocap data show limited generalization capabilities for real-\nistic dance motions.\nWith recent advances in learning-based motion synthesis, sev-\neral deep learning-based methods have been introduced [HSK16,\nLZX∗18]. These techniques formulate the problem as a long-term\nprediction task [LZX∗18] or as an audio-to-pose sequence transla-\ntion problem [SDSKS18,AFP17,ACI ∗17]. These regression-based\nmethods fail to predict highly diverse and complex motions even\nusing advanced training strategies via RNN models [LZX ∗18] as\nthey are deterministic. In particular, these models are designed to\npredict the successors given the current status instead of generating\na novel motion according to a database distribution. These methods\nare not suitable for music-oriented dance motion synthesis which\nusually expects more than one possible motions given the same\nmusic. We further note that there is no benchmark or evaluation\nmetric other than visual inspection for all the above methods.\nIn this work, we introduce a novel system that can synthe-\nsize diverse dance motions by learning from a large-scale dataset\nwith a comprehensive set of highly diverse dance movements. To\nachieve this goal, we face three major challenges: (1) it is difﬁ-\ncult to physically collect a large-scale dance motion dataset with\nsufﬁcient diversity. Although motion/performance capture meth-\nods can provide high-precision data, they require dedicated devices,\nprofessional performers, and a tedious clean-up process; (2) exist-\ning regression-based models cannot handle the diversity of dance\nmovements; (3) long-term temporal consistency and motion co-\nherency have to be preserved in the generated sequence.\nTo address the above issues, (1) we take advantage that a large\nnumber of dance videos are available online. We download thou-\nsands of dance videos, and use cutting edge techniques for 2D\npose detection, tracking, and 3D pose estimation to recover the\ndance sequence. Our large-scale dance motion dataset consists of\n50 hours of s synchronized music and dance pose sequences. (2)\nUsing this dataset, we propose a conditional auto-regressive gener-\native model to learn the motion distribution, along with Transform-\ners [VSP∗17] as the main architecture for capturing extended time\ndependency. We formulate the output in each timestep as a cate-\ngorical distribution using the discrete pose representations inspired\nby the success of discrete audio representation used in WaveNet\narchitecture [ODZ ∗16]. The discrete pose representation enables\nus to model the next step’s pose distribution and sample diverse\nposes at inference. Our model not only outperforms previous mod-\nels for important evaluation metrics but also enables generating di-\nverse dances with new music, demonstrating better modeling and\ngeneralization capabilities. (3) Besides, we propose several evalu-\nation metrics from different perspectives to better judge whether\nthe motion synthesis is satisfactory or not. We ﬁrst use a Bullet-\nbased [C∗13] virtual humanoid to evaluate the feasibility of gener-\nated pose sequences. Then, inspired by the motion-beat analysis ap-\nproach in [KPS03], we introduce an effective and automatic metric\nfor evaluating whether the dance movements follow the beat prop-\nerly. In addition to physical plausibility and beat consistency, we\nalso provide a metric for dance variation to measure the diversity\nin the synthesized results.\nBy testing our model using different settings and comparing it\nwith two main baseline techniques, including acLSTM [LZX ∗18]\nand ChorRNN [CFCF16] in a non-audio setting, we show that our\nreal-time method can generate more diverse and realistic dance mo-\ntions than existing techniques. We also show that when compared\nwith LSTM architectures, our Transformer model is also more ef-\nﬁcient to train. Finally, we demonstrate the effectiveness of our\nmodel and proposed evaluation metrics using a perceptual study.\nOur main contributions include:\n1. An end-to-end real-time system for dance motion synthesis that\nuses highly complex and diverse motion data obtained from In-\nternet videos. We also introduce an efﬁcient and scalable data\ncollection pipeline.\n2. A novel two-stream motion transformer model with discrete\npose representation to model the motion distribution and to cap-\nture long-term dependencies, which can be conditioned on mu-\nsic for diverse dance motion synthesis.\n3. Several effective evaluation metrics to assess the quality of syn-\nthesized dance motions.\n2. Related Work\nDance motion synthesis is a highly interdisciplinary problem and\nwe review the most relevant work here.\n2.1. Motion Synthesis\nMotion synthesis has been an actively studied problem in both, the\ncomputer graphics and the computer vision communities. Typical\nmethods rely on representation learning techniques such as auto-\nencoders, to embed motions into a low-dimensional space. Convo-\nlutional auto-encoders have been used to learn valid motion rep-\nresentation termed as the motion manifolds [HSKJ15], such that\ncorrupted motion or missing-marker data can be easily recovered.\nHigh-level, user-friendly parameters for motion control have also\nbeen explored [HSK16]. In particular, they ﬁrst specify charac-\nter trajectory and foot contact information, then conditionally syn-\nthesize the human motion. A language-guided motion generation\nframework [LWC∗18] has been proposed to generate realistic mo-\ntions from natural language descriptions.\nMotion synthesis can be also viewed as a long-term human mo-\ntion prediction. A number of works use recurrent neural networks\nto address this problem. In the work of Martinez et al. [MBR17],\npractical strategies including adding residual blocks and introduc-\ning sampling during training to improve RNN learning. Auto-\nconditioned RNN [LZX ∗18] takes both ground truth and model\nprediction as input with a speciﬁc alternating interval to train\nthe sequential model, showing the potential of generating motion\nsequences with long, or even unlimited future duration. Quater-\nNet [PGA18] conducts extensive experiments to demonstrate the\neffectiveness of quaternion representation. Dance generation can be\nregarded as a special case of motion synthesis, while dance move-\nments are more complex compared to usual motions such as walk-\ning. It is also important to ensure the coordination between music\nand motions.\n2.2. Choreography Learning\nThe creative process of choreography requires rich dancing ex-\nperiences, which has inspired the development of data-driven ap-\nJiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler & Hao Li / Learning to Generate Diverse Dance Motions with Transformer 3\nproaches. A common approach of predicting dance movements\nfrom audio is to formulate the task as a translation problem.\nIn particular, this approach enforces a one-to-one mapping from\nmusic to dance, which does not generalize well beyond train-\ning songs. GrooveNet [AFP17] has been the ﬁrst work to exploit\nRNNs to model the correspondence between audio features and\nmotion capture data. Based on music content analysis with mo-\ntion connectivity constraints, a probabilistic framework [FG15]\nhas been proposed. It mainly focuses on exploring and identify-\ning the major contributing choreographic factors. Moreover, auto-\nencoder [ACI∗17] has been exploited for mapping music features\nto a latent space, and to generate dance pose sequences. Essen-\ntially, this formulation still falls into the audio to dance mapping\ncategory. Similar to previous work in choreography, Audio to Body\nDynamics [SDSKS18] uses time-delayed RNNs to learn a mapping\nfrom audio to hand key points. The common drawbacks of these\nworks lies in the difﬁculty of generating diverse pose sequences\ngiven audio information only. We argue that dance movements are\nmore complex, hence less suitable to such deterministic formula-\ntion. This is because the same music can induce various kinds of\ndance motions, and the aesthetics of dance is strongly tied to the\ndiversity of motions. Other work solve the task from a genera-\ntive modeling perspective, ChorRNN [LKL18b], which introduces\nmixture-density RNNs to generate pose sequences and is particu-\nlarly promising. Nevertheless, their approach is motion-only and\ndoes not take music information into account. In this paper, we\ntreat dance motion synthesis as a conditional generative task. This\nallows us to not only promote the diversity of generation, but also\ntake music as conditions to ensure the consistency with music fea-\ntures.\n3. Methods\nIn this section, we present the Two-Stream Motion Transformer\n(TSMT) model. It processes the pose sequence and music context\nseparately and then fuses the two streams together to predict the\nnext motion.\n3.1. Problem Formulation\nInspired by the recent success of the auto-regressive\nmodel [ODZ∗16], we formulate our problem as an auto-regressive\ngenerative model conditioned on both music and past motions for\nsynthesizing realistic and self-coherent dance motions. We denote\nthe sequence of 3D dance motions as X={x1, ...,xT }, and the\nsequence of audio features as A={α1, ...,αT }. T is the number of\nframes in the sequence. For each time step, xt ∈R3n and αt ∈Rm,\nwhere n is the number of body joints and m is the dimension of\naudio features. We model the joint conditional probability as\np(X) =\nT\n∏\nt=1\np(xt |αt , ...,α1, xt−1, ...,x1) (1)\nwhere the motion stream and the audio stream are both encoded by\nneural networks.\n3.2. Motion and Audio Representation\n3.2.1. Motion\nUnlike acLSTM [LZX ∗18] and ChorRNN [LKL18b] which rep-\nresent poses as deterministic coordinates or the distributions re-\nspectively, we represent the continuous value of joint coordi-\nnates as discrete categories. For each dimension of the 3D pose\nxt , we perform uniform discretization into 300 constant inter-\nvals and obtain 3 n 300-dimensional one-hot vectors. To reduce\nmemory cost, we then transform each one-hot vector into a DE -\ndimensional embedding vector with a shared learnable matrix of\nsize [DE , 300]. This converts the motion sequence into a tensor of\nsize [T, 3n, DE ]. We merge the latter two axes of motion embed-\nding and input it to a temporal-wise fully connected feed-forward\nlayer to obtain a sequence of vectors with DM channels. Follow-\ning Vaswani et al. [VSP∗17], we also compute a DM-dimensional\npositional embedding sequence with sine and cosine functions to\nencode the temporal information. We add the positional embedding\nsequence to the motion embedding sequence. This forms our ﬁnal\nmotion representation X′of size [T, DM].\n3.2.2. Audio\nFor the audio data at each time step, we directly use the continu-\nous 13-dimensional MFCC vector concatenated with its temporal\nderivatives into a 26-dimensional feature vector. We embed the bi-\nnary one-hot beat signal into a 30-dimensional vector, resulting dif-\nferent embedding vectors at beat and non-beat positions. Similar to\nmotion, we feed the audio representation into a 1D convolution and\nadd to the positional embedding. The output is denoted as A′.\n3.3. Two-Stream Motion Transformer (TSMT)\n3.3.1. Transformer\nWe adopt the Transformer [VSP ∗17] architecture, harnessing and\nexploiting its power in modeling sequential data. Transformer con-\nsists of multiple blocks, each block is further composed of multi-\nple heads of self-attention sub-layers as well as position-wise feed-\nforward sub-layers. Obtained from the previous step, our inputX′is\na matrix of size [T, DM]. We ﬁrst transform the input sequence into\nthree matrices, namely keys K=X′WK , queries Q=X′WQ, and val-\nues V=X′WV , where K, Q, V are [T, D] matrices. We split each ma-\ntrix into multiple heads where each head is [T, Di], with D=∑i Di.\nFor each head, we compute the scaled-dot attentional features as\nZi = Softmax(QiKT\ni√Di\n)Vi (2)\nWe concatenate all theZi to form Z with size [T, D], then feed it into\ntwo position-wise 1D convolution layers followed by layer normal-\nization [VSP∗17]. This forms one Transformer block. We use mul-\ntiple such blocks with residual connections between the blocks. We\nrefer readers to [VSP ∗17] for more details about the Transformer\narchitecture.\n3.3.2. TSMT\nThe essence of dance is the manifestation of musicality in physical\nforms. Musicality takes form of multiple components such as vocal,\nbass, snare, keyboard, hi hat, drum, and other sound effects. A key\nelement is carefully paying attention to different layers of music.\nTherefore, we use a pose-stream transformer to capture dance his-\ntory, an audio-stream transformer to extract music context, and fuse\nthese two streams together to predict the next pose as shown in Fig-\nure 2. We denote the output of pose transformer asZX ={zX\n1 , ...,zX\nT },\n4 Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler & Hao Li / Learning to Generate Diverse Dance Motions with Transformer\nAudio Transformer\nPose Transformer\nFusion\nAttention\nTranspose\nTranspose\nWkW\nq Wv\nConv1d\nConv1d\nLayer Norm\nHead (x, y, z)\nFoot (x, y, z)\nMFCC\nBinary Beat\nX'\nA'\nFigure 2:Overview of our TSMT model. Left shows an example time step of pose and audio embedding. Middle shows our Two-Stream\nMotion Transformer including a pose transformer and an audio transformer. Each time step is followed by a late fusion module between two\nstreams, which predicts the pose in the next time step. Right shows a detailed sub-layer composition inside a transformer block.\nDE Di DM block layer head\nPose 5 128 256 4 4 4\nAudio - 32 64 2 2 2\nTable 1:Model details. D E for embedding dimension. D i for key, query,\nvalue dimension of each head, and DM for overall feature dimension.\nand the output of audio transformer as ZA={zA\n1 , ...,zA\nT }. We com-\npute the discrete representation of the pose at next time step from\nZX and ZA and predict the ﬁnal pose as,\nlog p(xt ) =Softmax(WX zX\nt−1 +WAzA\nt ) (3)\nDuring training, we can efﬁciently compute all the time steps in\nparallel, with a mask applied to the attention to ensure each step\nonly attend to its past. During inference, we sample from the log-\nlikelihood at the new time step.\n3.4. Implementation Details\nThe model consists of multiple blocks with multiple layers, each\ncontaining the multi-head self-attention and position-wise feed-\nforward layers as described above. We list important model param-\neters in Table 1. To train the models, We use Adam optimizer with\nmini-batches of size 32. We set the initial learning rate as 10 −4\nwith 0.3 decay rate after 200 epochs. The global motion is inferred\nby the Global Path Predictor in Zhou et al. [ZLB∗20].\n4. Dataset\nWe collected large amounts of high-quality videos from various\ndance channels on Youtube, and extracted 3D pose sequences with\nsynchronized audios.\n4.1. Dataset Collection\nWe started by manually selecting popular YouTube dance studio\nchannels and downloading both videos and tags. Our next-step data\nprocessing pipeline could be divided into four stages: 1) Quick an-\nnotation of dancing segments from untrimmed YouTube videos;2)\n2D pose detection and pose tracking;3) Simple manual cleaning of\ncorrect tracking results; 4) 3D pose estimation from 2D and post-\nprocessing. The ﬁrst and third steps were introduced mainly be-\ncause the Tubers edited the dance videos by inserting non-dance\ncontents. If the video sources only contain clean dance clips, our\npipeline would be fully automatic without manual annotation.\n4.2. Video Statistics and Trimming\nWe downloaded all the videos from ﬁve popular street dance chan-\nnels and obtained 3809 videos in total. We ﬁltered out the irrelevant\ncontents e.g. dancer’s daily life and trimmed the dance segments\nfrom the original videos. For each video, we annotated the start and\nend time of dance performance as shown in Figure 3(a). The statis-\ntics of original and trimmed video segments are shown in Table 2\nand Figure 3(c)(d).\n4.3. 2D Pose Detection and Tracking\nWe use YOLO-V3 [RF18], SimplePose [XWW18] and Light-\nTrack [NH19] to detect humans, estimate 2D pose, and track human\npose sequences respectively. Since the videos often contain multi-\nple dancers and audiences, we kept top-5 largest human detection\nbounding boxes to reduce computation cost.\n4.4. Track Cleaning\nSince our collected online videos are completely unconstrained, we\nobserve two main issues: First, some tracks are audiences instead\nof dancers. Second, there are incorrect track id exchanges between\ndancers, which leads to the pose discontinuity. We performed man-\nual annotations on pose sequences to reduce these types of noises.\nWe ask volunteers to watch pose tracking visualization videos, and\nmark the correct start and end times for each track ids as shown in\nFigure 3(b). We notice that imperfect tracking usually happens dur-\ning group formation changes, where dancers occasionally occlude\neach other. However, this does not affect the overall data quantity\nand distribution because most correct tracks have sufﬁcient dura-\ntions. Figure 3(d) shows the statistics after this data cleaning step.\n4.5. 3D Pose Estimation and Jitter Removal\nWe applied VideoPose3d [PFGA19] to convert 2D pose sequences\ninto 3D with 17 joints. Upon examining the results, we observed\nfrequent motion jitters to be the main issue. Thus, we used Hodrick-\nPrescott (HP) ﬁlter [HP97] to remove these jitters. HP ﬁlter sepa-\nrates a time-series X={xt } into a trend component and a cyclical\nJiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler & Hao Li / Learning to Generate Diverse Dance Motions with Transformer 5\n(a) (b)\n(c) (d)\nFigure 3:Our YouTube-Dance3D dataset. Figure (a) shows examples of video trimming. Figure (b) shows an example of valid pose tracking\nannotation. Figure (c) shows the distribution of original video FPS and duration. Figure (d) shows the duration distribution for cropped\nvideos and tracked videos.\nVideo Source Video Min. Trim Seg. Trim Min. Track IDs Track Min.\nUrban Dance Camp 197 400 193 173 9.1 201\nMat 302 1865 462 241 8,7 248\nMovement Lifestyle 478 1454 217 144 11.9 185\nSnowglobe 1184 3568 873 654 10.2 727\nOne Million Dance 1648 6301 2075 1494 14.5 1641\nTotal 3809 13588 3820 2707 12.4 3002\nSource Min. Dancer Audio 3D Public\nChorRNN [LKL18b] MoCap 300 1 \u0017 \u0013 \u0017\nGrooveNet [AFP17] MoCap 24 1 \u0013 \u0013 \u0017\nRobotKinect [ACI∗17] MoCap - 4 \u0013 \u0013 \u0017\nMelodyDance [TJM18] MoCap 94 - \u0013 \u0013 \u0013\nMikuDance [YLX∗19] Game 600 - \u0013 \u0013 \u0017\nYT2D [LKL18a] YouTube 376 - \u0013 \u0017 \u0017\nDanceToMusic [LYL∗19] YouTube 4260 - \u0013 \u0017 \u0017\nOurs YouTube 3002 - \u0013 \u0013 \u0013\nTable 2:Left shows statistics of our dataset: the number of videos, duration in minutes, trimmed segments, duration after trimming, average number of track\nIDs per video, and total duration of all tracks. Right shows a comparison of our dataset to previous datasets.\ncomponent, i.e. xt = xtrdt + xcyc\nt . The components are determined\nby minimizing a quadratic loss function,\nmin\n{xtrdt }∑\nt\n[\nxcyc\nt\n]2 +λ\n[\nxtrd\nt −2xtrd\nt−1 +xtrd\nt−2\n]2\n(4)\nWe applied this ﬁlter to the 3D coordinates of each joint sep-\narately, with λ=1 which empirically produces better result on our\ndata. All poses are bicubic interpolated into 24-fps to ensure con-\nsistent framerates.\n4.6. Audio Processing\nMel-Frequency Cepstral Coefﬁcients (MFCC) are effective audio\nfeatures widely used in various audio related tasks [SDSKS18].\nWe use LibROSA [MRL∗15] to compute music features including\nMFCC and time intervals between beats. The audio are standard-\nized with 44.1Khz sample rate with EBU R128 loudness normal-\nization via ffmpeg-normalize, after which features are extracted at\n24-fps.\n4.7. Dataset Analysis\nCompared to existing dance datatsets, our dataset has not only\nlarger scale, but also higher quality and bigger variety. As given\nIn Table 2, the scale of YouTube-Dance3D exceeds the previous\nlargest YT2D [LKL18a] by more than a magnitude order and com-\nparable to a concurrent pose2D dataset introduced in [LYL ∗19].\nMoreover, our dataset possesses higher diversity due to the choice\nof urban dance, which by nature emphasizes choreography and\nvariation. This is in contrast to other datasets that contain dances\nwith obvious repetitive motion patterns e.g. Salsa, Tango, Ballet,\nand Waltz. Additionally, our data processing pipeline enables in-\nexpensive data collection, making future expansion possible given\nnew sources of dance video. We split data into approximately 80%\ntraining and 20% validation. For the convenience of model train-\ning, we divide data into segments of 20s, with 10s overlap between\ntwo consecutive segments. Each segment contains 480 frames. This\nresults in 9561 training segments and 2136 validation segments.\n5. Experiments\nWe ﬁrst describe evaluation metrics including physical plausibility,\nbeat consistency and generation diversity. Next, we show quantita-\ntive and qualitative results.\n5.1. Metrics\nAutomatic evaluation metric has been a known challenge in many\ngenerative tasks. Recently [YLX∗19, LYL∗19], motion generation\nevaluation receives more attention, where FID-based metrics have\n6 Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler & Hao Li / Learning to Generate Diverse Dance Motions with Transformer\nFigure 4:An example of detected music beats and motion beats.\nbeen explored. We share the similar insight on the aspect of eval-\nuating generation diversity. Moreover, we further introduce to use\na humanoid physics simulator to evaluate dance plausibility and a\nnew beat consistency metric.\n5.1.1. Physical Plausibility\nWe measure the ratio of implausible frames that can not be executed\nby the humanoid inside Bullet simulator. Concretely, we measure\ntwo types of pose invalidity: 1) Authenticity is the ratio of frames\nwhere none of the joints exceeded its rotation limit. This ensures\nthe pose is statically plausible, as can be performed by a normal\nhuman body. 2) Coherence is the ratio of frames where the angular\nvelocity of all joints stay within a realistic range. This ensures the\nmotion between poses is dynamically plausible, preventing abnor-\nmal behaviour such as twitching.\n5.1.2. Beat Consistency\nA good dancer knows to express their perception of beat by period-\nically changing their moves, in other words, to accompany musical\nbeats with their motion beats. We ﬁrst extract motion beats from\nposes. Then we measure the similarities between generated motion\nbeats and ground-truth motion beats. We extract motion beats using\nthe method of Kim et al. [KPS03], which detects zero-crossings of\nthe joint angular acceleration. For two beats to match, we allow a\nﬂexibility of 2 frames. We compute precision, recall, and F-score\nbetween two beat sequences.\n5.1.3. Diversity\nThe complexity of choreography reﬂects in the composition of di-\nverse body movements. We measure the diversity aspect of gener-\nated dances via four aspects. 1)FrÃl’chet Inception Distance (FID)\nThis refers to the default usage of FID [HRU ∗17], measuring the\ndifference between ground truth and generation feature distribu-\ntion. 2) Inter-sequence Diversity (A-seq-D) We generate a large\nnumber of pose sequences, from which pairs of sequences are ran-\ndomly selected. For each pair, we measure the L2 distance between\ntheir feature vectors. We use the average feature distance as the A-\nseq-D score. 3) Intra-sequence Diversity (I-seq-D) Within a pose\nsequence, we divide it into chunks, and compute the feature dis-\ntance among all possible pairs. This distance is averaged over all\npairs from all sequences as the I-seq-D score. 4) Same-music Di-\nversity (S-music-D) We generate multiple sequences given the same\nmusic, and compute the feature distances between these genera-\ntions. We average this over all music as the S-music-D score.\nWe obtain perceptual features of the dance with a dance style\nclassiﬁer. We ﬁrst divide our dataset into 5 categories based on the\nYouTube channel name, with balanced size between classes. Then\nwe train a 2-block transformer with classiﬁcation outputs, obtaining\nCoherence↑ Authenticity↑ FID↓ A-seq-D↑ I-seq-D↑\nGround-Truth 1 1 0 32.84 10.76\nacLSTM 0.9995 0.9998 3.94 12.99 3.55\nChorRNN-5 0.83 0.75 2.56 30.48 21.26\nDLSTM 0.94 0.88 2.47 29.07 12.06\nTSMT-noaudio 0.97 0.96 0.53 32.52 7.98\nTable 3:Comparing different methods at the non-audio setting.\n61.0% top-1 classiﬁcation accuracy and 71.5% top-2 classiﬁcation\naccuracy.\n5.2. Comparisons\nPrevious work mainly focus on motion synthesis without audio in-\nput. Therefore, we ﬁrst compare our model with them in non-audio\nsetting. Then we compare variations of audio-enabled models.\n5.2.1. acLSTM [LZX∗18]\nacLSTM [LZX∗18] is a widely-used model in dance motion syn-\nthesis from mocap data. It introduces an interval when ground-\ntruth and model samples are used in the training process. This ad-\ndress the motion freeze issue of standard teacher-forcing training,\nwhich enables generating unlimited sequence length by learning\nfrom small amount of data. The model is a 3-layer LSTM with\na hidden dimension of 1024. We follow the same settings as de-\nscribed in [LZX∗18] to train on our dataset. acLSTM is essentially\na deterministic model, we evaluate its generative ability by feeding\nit additional information of different initial pose sequences with a\nlength of 10.\n5.2.2. ChorRNN [CFCF16]\nChorRNN [CFCF16] is a mixture density model based on a 3-layer\nLSTM. In each time step, ChorRNN predicts a distribution of pose\ninstead of deterministic coordinates. Since their code is not public,\nwe re-implement their model and experiment with different number\nof mixtures.\n5.3. Results\n5.3.1. Quantitative Results\nWe ﬁrst report experimental results with the non-audio setting in\nTable 3. We generate for each model 1000 pose sequences for eval-\nuation. All model use a random initial ﬁrst pose to reduce the pos-\nsible noise brought by generation in the ﬁrst step. It can be seen\nthat for acLSTM [LZX∗18], although it has the highest Coherence\nand Authenticity scores, it is unable to generate diverse dances as\nshown in the FID and Diversity scores. For ChorRNN [LKL18b],\nalthough it has the highest intra-sequence diversity, many of its gen-\nerations are hardly valid human poses. This can be seen from its low\nCoherence and Autheticity scores. We further experiment different\nnumber of mixtures which consists of its most important param-\neters. From the results in Table 4, the high FID scores show that\nnone of the settings can generate both realistic and diverse dances.\nRegarding Coherence and Authenticity, our model scores compa-\nrably to acLSTM, while being able to generate more diverse mo-\ntions close to the ground truth distribution. In the audio-enabled\nsetting, We show baseline and ablation study results in Table 5.\nSingle Transformer model refers to the baseline model that directly\nJiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler & Hao Li / Learning to Generate Diverse Dance Motions with Transformer 7\nacLSTM [LZX∗18]\nChorRNN [LKL18b]\nOur TSMT-no-audio\nFigure 5:Qualitative comparison. Our model generates plausible, realistic, and diverse dances.\nconcatenates audio and pose data as input to a single-stream trans-\nformer. We also explore the effects of using different combinations\nof audio information for each model. We observe that all of our\nmodels have high Coherence and Authenticity scores. The beat-\nonly models achieve higher diversity scores. This is because of less\nconstraints imposed by the audio input.\n5.3.2. Qualitative Results\nWe show qualitative results for acLSTM, ChorRNN and our pro-\nposed TSMT-noaudio Model in Figure 5. All results are from a gen-\neration of 20 seconds at the non-audio setting, screen captured at\nthe same time interval. It can be seen that acLSTM tends to quickly\nfreeze to a generic static pose. ChorRNN generates invalid poses\nthat can hardly be performed by any dancer. Our model is able to\ngenerate valid and diverse poses, which is consistent with the quan-\ntitative evaluation metric scores as reported in Table 3.\n5.3.3. Computation Time\nWe test the computation time of our transformer model and base-\nlines models in the non-audio setting. We use a single GPU GTX\n1080 to perform the running time evaluation. For training time, we\nreport the average time per batch from our training log. For test-\ning time, we generate 10 sequences with each model, each with\n480 time steps. Results are shown in Table 6. It can be seen that our\ntransformer-based model outperforms LSTM-based model in terms\nof training efﬁciency, while remaining real-time (24-fps) in testing.\n5.4. Human Evaluation\nWe conduct human evaluation to verify whether our proposed auto-\nmatic metrics are consistent with human judgement. We make use\nof Amazon Mechanical Turk (AMT) and pay workers to perform\na crowd-sourced evaluation. We restrict to US-based workers who\nhave at least 85% acceptance score.\nCoherence↑ Authenticity↑ FID↓ A-seq-D↑ I-seq-D↑\nChorRNN-1 0.94 0.94 8.42 40.39 17.32\nChorRNN-5 0.83 0.75 2.56 30.48 21.26\nChorRNN-10 0.75 0.54 17.30 28.35 17.99\nChorRNN-20 0.74 0.55 17.29 24.88 16.66\nTable 4:Effect of mixture component number for ChorRNN [LKL18b]\n5.4.1. Physical Plausibility and Beat\nWe ﬁrst obtain Authenticity, Coherence, and Beat scores computed\nwith our automatic metrics. Then for each metric, we divide the\nscore into three levels of high, middle and low. We randomly sam-\nple 60 pairs from three different level combinations, namely high-\nmid, high-low, and mid-low. Then we resort to workers and ask\nthem to select the better one from each pair. Each test contains 20\nevaluation pairs and 3 validation pairs. The validation pairs con-\ntain a ground-truth and an artiﬁcially noised motion sequence. We\nuse this as a hidden test to ﬁlter out the workers who are inatten-\ntive or intentionally abusing. We take the answer from AMT work-\ners via majority-voting, compare with known answer decided by\nactual scores in each pair and calculate the consistency between\ntwo answers. We observe that Authenticity and Coherence have\nhigh consistency with human evaluations which verify the effec-\ntiveness of our physical plausibility metrics. However, the consis-\ntency for beats is relatively low for High-Mid and Mid-Low tests,\nwhile High-Low test has higher consistency. We believe that the\naverage person does not have high sensitivity for beats as machine\nmeasurements do. In other words, some of AMT workers might be\nincapable of distinguishing whether the dance follows the beat or\nnot when the differences are small. Further studies are needed in\nthe future for deeper beat analysis.\n8 Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler & Hao Li / Learning to Generate Diverse Dance Motions with Transformer\nCoherence↑ Authenticity↑ Beat↑ FID↓ A-seq-D↑ I-seq-D↑ S-music-D↑\nGround-Truth 1 1 1 0 32.84 10.76 0\nSingle Stream Transformer + Beat 0.97 0.92 0.447 0.56 41.95 11.09 17.30\nSingle Stream Transformer + MFCC 0.96 0.92 0.445 0.43 38.69 9.69 16.96\nSingle Stream Transformer + Beat + MFCC 0.97 0.93 0.439 1.27 38.86 10.88 16.00\nTwo Stream Transformer + Beat 0.97 0.93 0.451 1.43 40.90 9.49 18.11\nTwo Stream Transformer + MFCC 0.96 0.92 0.430 1.46 33.37 9.41 16.07\nTwo Stream Transformer + Beat + MFCC 0.97 0.93 0.449 0.21 36.44 10.02 16.16\nTable 5:Comparing different methods at the audio-enabled settings.\nacLSTM ChorRNN DLSTM Our TSMT\nTrain per Batch 2.40s 0.31s 0.39s 0.23s\nTest per Step 0.55ms 62.5ms 15.7ms 19.4ms\nTable 6:Training and testing time comparison.\nHigh-Low High-Mid Mid-Low Total\nAuthenticity 0.9 1.0 0.65 0.85\nCoherence 0.95 0.95 0.9 0.933\nBeat 0.65 0.55 0.4 0.533\nTable 7:AMT user study result to evaluate the consistency between auto-\nmatic metrics and human judgement.\n0% 20%40%60%80%100%TransformeracLSTMChorRNNDLSTMGT\n1585 7525100100\nGTDLSTMChorRNNacLSTM\nOursOursOursOurs\nFigure 6:AMT user study on overall generation quality.\n5.4.2. Overall Quality\nWe randomly select 200 pairs generated by acLSTM, ChorRNN,\nDiscrete-LSTM, and our TSMT model (non-audio setting) in dif-\nferent combinations, and ask workers to pick the more prefer-\nable one with better quality. A comparison between our result and\nground-truth is also included. Similarly, we use validation ques-\ntions to ﬁlter out noisy annotations. Figure 6 shows the pair-wise\ncomparison results. It can been seen that our model is obviously\nbetter than acLSTM and ChorRNN baselines, also superior than\nLSTM with discrete representation. However, there is still a gap\nbetween our synthetic motions and the ground truth, and the main\nreason is that our synthetic motion sequences are based on sam-\npling in each timestep, there is a chance that low probability pose\ngets sampled which introduce noise to the sequence generation. It\nis possible to eliminate this type of noise by applying some con-\nstraints during pose sampling.\n6. Discussion and Conclusion\nWe proposed a complete system for dance motion synthesis from\naudio input, and we have shown that we can handle highly diverse\ndance movements using widely available online dance videos as\ntraining. We have also introduced a new large-scale motion dataset,\nas well as new evaluation metrics in terms of quality, diversity and\nmusicality. Our proposed conditional generative model also outper-\nformed existing methods. We also conducted a study that indicates\nthe effectiveness of our model and proposed metrics.\n6.1. Limitation and Future Work\nSince our data is collected from videos, the number of joints de-\npends on 3D pose estimation method which usually does not take\nﬁnger animations into account. An interesting future direction is to\nextract ﬁnger joints and facial expressions as well from videos so\nthat we are able to produce expressive dance data. Futhermore, we\nare interested in using motion capture data to denoise and improve\nthe quality of 3D pose sequence extracted from online videos.\nWe have used audio representations such as MFCC features and\nbeat, but according to professional dancers and choreographers,\nthey tend to also follow additional musical layers including bass,\nlyrics, etc. The exploration for more complex audio features is\ntherefore particularly intriguing, as well as the analysis of inter-\nactions between dancers and crowds.\n7. Acknowledgements\nThis research was funded by in part by the ONR YIP grant N00014-\n17-S-FO14, the CONIX Research Center, a Semiconductor Re-\nsearch Corporation (SRC) program sponsored by DARPA, the An-\ndrew and Erna Viterbi Early Career Chair, the U.S. Army Research\nLaboratory (ARL) under contract number W911NF-14-D-0005,\nAdobe, and Sony. We thank Yajie Zhao, Mingming He for prof\nreading, Zhengfei Kuang for editing the demo, Marcel Ramos and\nKalle Bladin for the character rigging and rendering, and Pengda\nXiang for the data annotation.\nReferences\n[ACI∗17] A UGELLO A., C IPOLLA E., I NFANTINO I., M ANFRE A., P I-\nLATO G., V ELLA F.: Creative robot dance with variational encoder.\narXiv:1707.01489 (2017). 2, 3, 5\n[AFP17] A LEMI O., F RANÇOISE J., P ASQUIER P.: Groovenet: Real-\ntime music-driven dance movement generation using artiﬁcial neural net-\nworks. networks 8, 17 (2017), 26. 2, 3, 5\n[C∗13] C OUMANS E., ET AL .: Bullet physics library. Open source: bul-\nletphysics.org 15, 49 (2013), 5. 2\n[CFCF16] C RNKOVIC -FRIIS L., C RNKOVIC -FRIIS L.: Generative\nchoreography using deep learning. arXiv:1605.06921 (2016). 2, 6\n[FG15] F UKAYAMA S., G OTO M.: Music content driven automated\nchoreography with beat-wise motion connectivity constraints. Proceed-\nings of SMC (2015), 177–183. 3\nJiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler & Hao Li / Learning to Generate Diverse Dance Motions with Transformer 9\n[HP97] H ODRICK R. J., P RESCOTT E. C.: Postwar us business cycles:\nan empirical investigation.Journal of Money, credit, and Banking(1997),\n1–16. 4\n[HRU∗17] H EUSEL M., R AMSAUER H., U NTERTHINER T., N ESSLER\nB., H OCHREITER S.: Gans trained by a two time-scale update rule con-\nverge to a local nash equilibrium. In NIPS (2017). 6\n[HSK16] H OLDEN D., S AITO J., K OMURA T.: A deep learning frame-\nwork for character motion synthesis and editing. TOG 35, 4 (2016), 138.\n1, 2\n[HSKJ15] H OLDEN D., S AITO J., K OMURA T., J OYCE T.: Learning\nmotion manifolds with convolutional autoencoders. In SIGGRAPH Asia\n(2015), p. 18. 2\n[KPS03] K IM T.-H., PARK S. I., S HIN S. Y.: Rhythmic-motion synthesis\nbased on motion-beat analysis. In TOG (2003), vol. 22, pp. 392–401. 2,\n6\n[LKL18a] L EE J., K IM S., L EE K.: Listen to dance: Music-driven\nchoreography generation using autoregressive encoder-decoder network,\n2018. arXiv:arXiv:1811.00818. 5\n[LKL18b] L EE J., K IM S., L EE K.: Listen to dance:music-driven\nchoreography generation using autoregressive encoder-decoder network.\narXiv:1811.00818 (2018). 3, 5, 6, 7\n[LLC] LLC. M.: Cmu graphics lab motion capture database. http:\n//mocap.cs.cmu.edu. 1\n[LWC∗18] L IN A. S., W U L., C ORONA R., T AI K., H UANG Q.,\nMOONEY R. J.: Generating animated videos of human activities from\nnatural language descriptions. Learning 2018 (2018). 2\n[LYL∗19] L EE H.-Y., Y ANG X., L IU M.-Y., W ANG T.-C., L U Y.-D.,\nYANG M.-H., K AUTZ J.: Dancing to music. In NeurIPS (2019). 5\n[LZX∗18] L I Z., Z HOU Y., XIAO S., H E C., H UANG Z., L I H.: Auto-\nconditioned recurrent networks for extended complex human motion\nsynthesis. ICLR (2018). 1, 2, 3, 6, 7\n[MBR17] M ARTINEZ J., B LACK M. J., R OMERO J.: On human motion\nprediction using recurrent neural networks. In CVPR (2017). 2\n[MRL∗15] M CFEE B., R AFFEL C., L IANG D., E LLIS D. P., M CVICAR\nM., B ATTENBERG E., N IETO O.: librosa: Audio and music signal anal-\nysis in python. 5\n[NH19] N ING G., H UANG H.: Lighttrack: A generic framework for on-\nline top-down human pose tracking. arXiv:1905.02822 (2019). 4\n[ODZ∗16] O ORD A. V. D., D IELEMAN S., Z EN H., S IMONYAN\nK., V INYALS O., G RAVES A., K ALCHBRENNER N., S ENIOR A.,\nKAVUKCUOGLU K.: Wavenet: A generative model for raw audio.\narXiv:1609.03499 (2016). 2, 3\n[PFGA19] P AVLLO D., F EICHTENHOFER C., G RANGIER D., A ULI M.:\n3d human pose estimation in video with temporal convolutions and semi-\nsupervised training. CVPR (2019). 4\n[PGA18] P AVLLO D., G RANGIER D., A ULI M.: Quaternet: A\nquaternion-based recurrent model for human motion. In BMVC (2018).\n2\n[RF18] R EDMON J., F ARHADI A.: Yolov3: An incremental improve-\nment. arXiv:1804.02767 (2018). 4\n[SDSKS18] S HLIZERMAN E., D ERY L., S CHOEN H., K EMELMACHER -\nSHLIZERMAN I.: Audio to body dynamics. CVPR (2018). 2, 3, 5\n[TJM18] T ANG T., J IA J., M AO H.: Dance with melody: An lstm-\nautoencoder approach to music-oriented dance synthesis. In ACM Mul-\ntimedia (2018), pp. 1598–1606. 5\n[VSP∗17] V ASWANI A., S HAZEER N., P ARMAR N., U SZKOREIT J.,\nJONES L., G OMEZ A. N., K AISER Ł., P OLOSUKHIN I.: Attention is\nall you need. In NIPS (2017). 2, 3\n[XWW18] X IAO B., W U H., W EI Y.: Simple baselines for human pose\nestimation and tracking. In ECCV (2018). 4\n[YLX∗19] Y AN S., L I Z., X IONG Y., Y AN H., L IN D.: Convolu-\ntional sequence generation for skeleton-based action synthesis. In ICCV\n(2019). 5\n[ZLB∗20] Z HOU Y., LU J., B ARNES C., Y ANG J., X IANG S., ET AL .:\nGenerative tweening: Long-term inbetweening of 3d human motions.\narXiv preprint arXiv:2005.08891 (2020). 4",
  "topic": "Dance",
  "concepts": [
    {
      "name": "Dance",
      "score": 0.7271448373794556
    },
    {
      "name": "Transformer",
      "score": 0.5947425365447998
    },
    {
      "name": "Computer science",
      "score": 0.4307398200035095
    },
    {
      "name": "Engineering",
      "score": 0.24295631051063538
    },
    {
      "name": "Art",
      "score": 0.21792665123939514
    },
    {
      "name": "Visual arts",
      "score": 0.2150653600692749
    },
    {
      "name": "Electrical engineering",
      "score": 0.15866997838020325
    },
    {
      "name": "Voltage",
      "score": 0.056981950998306274
    }
  ],
  "institutions": []
}