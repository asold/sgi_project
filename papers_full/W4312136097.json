{
  "title": "polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics",
  "url": "https://openalex.org/W4312136097",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3096356045",
      "name": "Christopher Kuenneth",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2443284501",
      "name": "Rampi Ramprasad",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3100445615",
    "https://openalex.org/W3114168740",
    "https://openalex.org/W2754122850",
    "https://openalex.org/W2050445699",
    "https://openalex.org/W3142241411",
    "https://openalex.org/W3155755963",
    "https://openalex.org/W3095028680",
    "https://openalex.org/W3165831121",
    "https://openalex.org/W2984886967",
    "https://openalex.org/W4221142372",
    "https://openalex.org/W3024556032",
    "https://openalex.org/W3091459767",
    "https://openalex.org/W3199836559",
    "https://openalex.org/W3197690017",
    "https://openalex.org/W3112510144",
    "https://openalex.org/W2952832141",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2022476850",
    "https://openalex.org/W2315837940",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2415372084",
    "https://openalex.org/W1513260206",
    "https://openalex.org/W2791355014",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W3198212015",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W3010145447",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2904181725",
    "https://openalex.org/W2951450280",
    "https://openalex.org/W2883528235",
    "https://openalex.org/W2978032524",
    "https://openalex.org/W3026577685",
    "https://openalex.org/W2976224320",
    "https://openalex.org/W3002410303",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4289677850",
    "https://openalex.org/W4288253152",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W4287642541",
    "https://openalex.org/W2556522401",
    "https://openalex.org/W4288090629"
  ],
  "abstract": "<title>Abstract</title> Polymers are a vital part of everyday life. Their chemical universe is so large that it presents unprecedented opportunities as well as significant challenges to identify suitable application-specific candidates. We present a complete end-to-end machine-driven polymer informatics pipeline that can search this space for suitable candidates at unprecedented speed and accuracy. This pipeline includes a polymer chemical fingerprinting capability called polyBERT (inspired by Natural Language Processing concepts), and a multitask learning approach that maps the polyBERT fingerprints to a host of properties. polyBERT is a chemical linguist that treats the chemical structure of polymers as a chemical language. The present approach outstrips the best presently available concepts for polymer property prediction based on handcrafted fingerprint schemes in speed by two orders of magnitude while preserving accuracy, thus making it a strong candidate for deployment in scalable architectures including cloud infrastructures.",
  "full_text": "polyBERT: A chemical language model to enable\nfully machine-driven ultrafast polymer informatics\nChristopher Kuenneth \nGeorgia Institute of Technology\nRampi Ramprasad  (  rampi.ramprasad@mse.gatech.edu )\nGeorgia Institute of Technology https://orcid.org/0000-0003-4630-1565\nArticle\nKeywords:\nPosted Date: December 13th, 2022\nDOI: https://doi.org/10.21203/rs.3.rs-2116998/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: Yes there is potential Competing Interest. R.R. is the founder of the company\nMatmerize, Inc., that intends to provide polymer informatics services. A provisional patent on polyBERT\nhas been \u0000led by R.R. and C.K.\nVersion of Record: A version of this preprint was published at Nature Communications on July 11th,\n2023. See the published version at https://doi.org/10.1038/s41467-023-39868-6.\npolyBERT: A chemical language model to enable\nfully machine-driven ultrafast polymer informatics\nChristopher Kuenneth and Rampi Ramprasad∗\nSchool of Materials Science and Engineering, Georgia Institute of Technology, Atlanta,\nGeorgia 30332, USA\nE-mail: rampi.ramprasad@mse.gatech.edu\nAbstract\nPolymers are a vital part of everyday life. Their chemical universe is so large that it\npresents unprecedented opportunities as well as signiﬁcant challe nges to identify suit-\nable application-speciﬁc candidates. We present a complete end-to- end machine-driven\npolymer informatics pipeline that can search this space for suitable c andidates at un-\nprecedented speed and accuracy. This pipeline includes a polyme r chemical ﬁngerprint-\ning capability called polyBERT (inspired by Natural Language Processing concepts),\nand a multitask learning approach that maps the polyBERT ﬁngerprints to a host\nof properties. polyBERT is a chemical linguist that treats the chemi cal structure of\npolymers as a chemical language. The present approach outstrips the best presently\navailable concepts for polymer property prediction based on handcrafte d ﬁngerprint\nschemes in speed by two orders of magnitude while preserving accur acy, thus making\nit a strong candidate for deployment in scalable architectures incl uding cloud infras-\ntructures.\n1\nPolymers are an integral part of our everyday life and instru mental in the progress of tech-\nnologies for future innovations. 1 The sheer magnitude and diversity of the polymer chemical\nspace provide opportunities for crafting polymers that acc urately match application de-\nmands, yet also come with the challenge of eﬃciently and eﬀec tively browsing this gigantic\nspace. The nascent ﬁeld of polymer informatics 2–5 allows access to the depth of the polymer\nuniverse and demonstrates the potency of machine learning ( ML) models to overcome this\nchallenge. ML frameworks have enabled substantial progres s in the development of polymer\nproperty predictors 6–10 and solving inverse problems in which polymers that meet spe ciﬁc\nproperty requirements are either identiﬁed from candidate sets,11,12 or are freshly designed\nusing genetic 13,14 or generative 15–17 algorithms.\nAn essential step in polymer informatics pipelines is the con version of polymer chem-\nical structures to numerical representations that are ofte n called ﬁngerprints, features, or\ndescriptors (see blue boxes in Figure 1a). Past and current handcrafted ﬁngerprinting ap-\nproaches20–24 utilize cheminformatics tools that numerically encode key chemical and struc-\ntural features of polymers. Although such handcrafted ﬁnger prints build on invaluable intu-\nition and experience, they are tedious to develop, involve c omplex computations that often\nconsume most of the time during model training and inference , and lack generalization to\nall polymer chemical classes (i.e., new features may have to be added to the catalog of\nfeatures in an ad hoc manner). ML pipelines that use handcrafted ﬁngerprints are thus\nprone to errors during the exploration of new polymer chemic al classes. Also, handcrafted\nﬁngerprints present barriers for the development and deplo yment of fully machine-driven in-\nformatics pipelines, which are suited for scalability in cl oud computing and high-throughput\nenvironments.\nThe present contribution overcomes the previously mention ed limitations by replacing\nhandcrafted ﬁngerprints with fully machine-crafted “Tran sformer” ﬁngerprints (see right\npipeline of Figure 1a). Transformers 25 were recently developed in the ﬁeld of Natural Lan-\nguage Processing (NLP) and have swiftly become the gold stand ard in ML language mod-\n2\nPolymer\nProperties\nID Polymer 1 c1 Polymer 2 c2\n1 [*]CC[*] 40 [*]CC([*])C 60\n2 [*]CCO[*] 100 - 0\n3 [*]CC([*])C1CC1 30 [*]CC([*])C1CCC1 70\n4 ... ... ... ...\nID Tg  Tm  Td  E εb σb ...\n1 230 361 683 190 460 16 ...\n2 220 326 604 170 267 11 ...\n3 320 520 620 600 113 23 ...\n4 ... ... ... ... ... ... ...\nHandcrafted \n\u0000ngerprints\nPretraining\nO=C(C(C[*])(C)[*])OC\nMasking\n...\nPSMILES\nb Training polyBERT\nDeBERTa\nEmbedding\npolyBERT\nCanonicalization\nTokenization\nPreprocessing\n[*] CC  ([*]) (C)  C(=O)  OC\nFeed forward network\n[*] CC  ([*]) (C)  C(=O)  OCPSMILES\n[*] CC  ([*]) (C)  C(=O)  OC\n[*] CC  ([*]) (C)  C(=O)  OC\nTransformer encoder\nTransformer encoder\n0.3 \n... \n0.4\n0.2 \n... \n0.5\n0.8 \n... \n0.2\n0.3 \n... \n0.6\n0.2 \n... \n0.9\nBRICS Composition\npolyBERT\n\u0000ngerprints\nChem-\ninformatics\npolyBERT \n(Transformer)\na Pipelines\n~13 000\nsynthesized polymers \n100 million  \nhypothetical polymers \nFragments\nNew \npolymers\nBRICS Decomposition\nPolymer\nMultitask property\npredictors\nMultitask property\npredictors\nInference\nc Generate 100 million PSMILES \nFigure 1: Polymer informatics with polyBERT. a Prediction pipelines. The left pipeline\nshows the prediction using handcrafted ﬁngerprints from ch eminformatics tools, while the\nright pipeline (present work) portrays a fully end-to-end m achine-driven predictor using\npolyBERT. Property symbols are deﬁned in Table 1. ID1 and ID3 are copolymers, and ID2\nis a homopolymer. b polyBERT is a polymer chemical language linguist. polyBERT canon-\nicalizes, tokenizes, and masks polymer SMILES 18 (PSMILES) strings strings before passing\nthem to the DeBERTa model. A last dense layer with a softmax ac tivation function ﬁnds\nthe masked tokens. polyBERT ﬁngerprints (dashed arrow) are the averages over the token\ndimension (sentence average) of the last Transformer encod er. c 100 million hypothetical\nPSMILES strings. First, 13 766 known (i.e., previously synth esized) polymers are decom-\nposed to 4 424 fragments using the BRICS 19 method. Second, re-assembling the BRICS\nfragments in many diﬀerent ways generates 100 million hypot hetical polymers by randomly\nand enumeratively combining the fragments.\n3\neling. In this work, we envision simpliﬁed molecular-input line-entry system (SMILES) 18\nstrings that have been used to represent polymers as the “che mical language” of polymers.\nWe use millions of polymer SMILES (PSMILES) strings for trai ning a language model called\npolyBERT to become an expert – a linguist – of the polymer chem ical language. In combina-\ntion with multitask deep neural networks, 6,7 polyBERT enables a fully end-to-end machine-\ndriven polymer informatics pipeline that uses and unleashe s the true power of artiﬁcial\nintelligence methods. Multitask deep neural networks harn ess inherent correlations in multi-\nﬁdelity and multi-property data sets, scale eﬀortlessly in cloud computing environments, and\ngeneralize to multiple prediction tasks.\nRecent studies 26–28 demonstrated the beneﬁts of using Transformers in the molec ule\nchemical space. For example, Wang et al. 27 have trained a BERT model 29 (the most common\ngeneral language model) with a data set of molecule SMILES st rings. Using BERT’s latent\nspace representations of molecules as ﬁngerprints, the aut hors show that their approach out-\nperforms other ﬁngerprinting methods (including ﬁngerpri nts of an unsupervised recurrent\nneural network and a graph-based neural network). Similarl y, Schwaller et al. 30,31 have de-\nveloped a Transformer model to predict retrosynthesis path ways of molecules from reactants\nand reagents that outperforms known algorithms in the react ion prediction literature. No\npast study has applied Transformers to polymers.\nThis study has several critical and novel ingredients. First , we generate a data set\nof 100 million hypothetical polymers by enumeratively comb ining chemical fragments ex-\ntracted from a list of more than 13 000 synthesized polymers. Next, we train polyBERT,\na DeBERTa 32-based encoder-only Transformer, using this hypothetical polymer data set to\nbecome a polymer chemical linguist. During training, polyB ERT learns to translate input\nPSMILES strings to numerical representations that we use as polymer ﬁngerprints. Finally,\nwe map the polyBERT ﬁngerprints to about 3 dozen polymer prop erties using our mul-\ntitask ML framework to yield fully machine-driven ultrafas t polymer property predictors.\nFor benchmarking, the performance (both accuracy and speed ) of this new end-to-end prop-\n4\nerty prediction pipeline is compared with the state-of-the -art handcrafted Polymer Genome 8\n(PG) ﬁngerprint based pipeline pioneered previously. Using the ultrafast polyBRET polymer\ninformatics pipeline, we are in a position to predict the pro perties of the 100 million hypo-\nthetical polymers intending to ﬁnd property boundaries of t he polymer universe. This work\ncontributes to expediting the discovery, design, developm ent, and deployment of polymers\nby harnessing the true power of language, data, and artiﬁcia l intelligence models.\nResults\nData Sets Figure\n1c sketches the two-step process for fabricating 100 million hypothetical\nPSMILES strings. We use the Breaking Retrosynthetically In teresting Chemical Substruc-\ntures (BRICS) 19 method (as implemented in RDKit 33) to decompose previously synthesized\n13 766 polymers into 4 424 unique chemical fragments. Random and enumerative compo-\nsitions of these fragments yield 100 million hypothetical P SMILES strings that we ﬁrst\ncanonicalize (see Methods section) and then use for trainin g polyBERT. The hypothetical\nPSMILES strings are chemically valid polymers but, mostly, have never been synthesized\nbefore.\nOnce polyBERT has completed its unsupervised learning task using the 100 million\nhypothetical PSMILES strings, multitask supervised learn ing maps polyBERT polymer ﬁn-\ngerprints to multiple properties to produce property predi ctors. We use the property data\nset in Table 1 for training the property predictors. The data set contains 28 061 ( ≈80 %)\nhomopolymer and 7 456 ( ≈20 %) copolymer (total of 35 517) data points of 29 experimen-\ntal and computational polymer properties that pertain to 11 145 diﬀerent monomers and\n1 338 distinct copolymer chemistries, respectively. Each o f the 7 456 copolymer data points\ninvolves two distinct comonomers at various compositions. All data points in the data set\nhave been used in past studies 6,7,11,34–41 and were produced using computational methods\nor obtained from literature and other public sources. Suppl ementary Figures S3-S8 show\n5\nTable 1: Training data set for the property predictors. The p roperties are sorted into\ncategories, showed at the top of each block. The data set cont ains 29 properties (dielectric\nconstants kf are available at 9 diﬀerent frequencies f). HP and CP stand for homopolymer\nand copolymer, respectively.\nProperty Symbol Unit Source a Data range Data points\nHP CP All\nThermal\nGlass transition temp. Tg K Exp. [8e+01, 9e+02] 5 183 3 312 8 495\nMelting temp. Tm K Exp. [2e+02, 9e+02] 2 132 1 523 3 655\nDegradation temp. Td K Exp. [3e+02, 1e+03] 3 584 1 064 4 648\nThermodynamic & physical\nHeat capacity cp Jg− 1K− 1 Exp. [8e-01, 2e+00] 79 79\nAtomization energy Eat eV atom − 1 DFT [-7e+00, -5e+00] 390 390\nLimiting oxygen index Oi % Exp. [1e+01, 7e+01] 101 101\nCrystall. tendency (DFT) Xc % DFT [1e-01, 1e+02] 432 432\nCrystall. tendency (exp.) Xe % Exp. [1e+00, 1e+02] 111 111\nDensity ρ g cm − 3 Exp. [8e-01, 2e+00] 910 910\nElectronic\nBand gap (chain) Egc eV DFT [2e-02, 1e+01] 4 224 4 224\nBand gap (bulk) Egb eV DFT [4e-01, 1e+01] 597 597\nElectron aﬃnity Eea eV DFT [4e-01, 5e+00] 368 368\nIonization energy Ei eV DFT [4e+00, 1e+01] 370 370\nElectronic injection barrier Eib eV DFT [2e+00, 7e+00] 2 610 2 610\nCohesive energy density δ eV Exp. [2e+01, 3e+02] 294 294\nOptical & dielectric\nRefractive index (DFT) nc DFT [1e+00, 3e+00] 382 382\nRefractive index (exp.) ne Exp. [1e+00, 2e+00] 516 516\nDielec. constant (DFT) kc DFT [3e+00, 9e+00] 382 382\nDielec. constant at freq. fb kf Exp. [2e+00, 1e+01] 1 187 1 187\nMechanical\nYoung’s modulus E MPa Exp. [2e-02, 4e+03] 592 322 914\nTensile strength at yield σy MPa Exp. [3e-05, 1e+02] 216 78 294\nTensile strength at break σb MPa Exp. [5e-03, 2e+02] 663 318 981\nElongation at break ǫb Exp. [3e-01, 1e+03] 868 260 1128\nPermeability\nO2 gas permeability µO2 barrer Exp. [5e-06, 1e+03] 390 210 600\nCO2 gas permeability µCO2 barrer Exp. [1e-06, 5e+03] 286 119 405\nN2 gas permeability µN2 barrer Exp. [3e-05, 5e+02] 384 99 483\nH2 gas permeability µH2 barrer Exp. [2e-02, 5e+03] 240 46 286\nHe gas permeability µHe barrer Exp. [5e-02, 2e+03] 239 58 297\nCH4 gas permeability µCH4 barrer Exp. [4e-04, 2e+03] 331 47 378\n28 061 7 456 35 517\na Experiments (Exp.); density functional theory (DFT)\nb f ∈ { 1.78, 2, 3, 4, 5, 6, 7, 9, 15} is the log 10(frequency in Hz); e.g., k3 is the dielectric\nconstant at a frequency of 1 kHz.\n6\nhistograms for each property.\npolyBERT polyBERT iteratively ingests 100 million hypothetical PSM ILES strings to\nlearn the polymer chemical language, as sketched in Figure\n1b. polyBERT is a DeBERTa 32\nmodel (as implemented in Huggingface’s Transformer Python l ibrary42) with a supplemen-\ntary three-stage preprocessing unit for PSMILES strings. Fi rst, polyBERT transforms a\ninput PSMILES string into its canonical form (e.g., [*]CCOCCO[*] to [*]COC[*]) using the\ncanonicalize psmiles Python package developed in this work. Details can be found i n the\nMethods section. Second, polyBERT tokenizes canonical PSM ILES strings using the Senten-\ncePiece43 tokenizer. The tokens are frequent patterns in PSMILES stri ngs and determined in\na pretraining process of SentencePiece with the 100 million hypothetical PSMILES strings.\nThird, polyBERT masks 15 % (default parameter for masked lan guage models) of the tokens\nto create a self-supervised training task. In this training task, polyBERT is taught to predict\nthe masked tokens using the non-masked surrounding tokens b y adjusting the weights of the\nTransformer encoders (ﬁll-in-the-blanks task). We use 80 m illion PSMILES strings for train-\ning and 20 million PSMILES strings for validation. The valid ation F1-score is > 99. This\nexceptionally good F1-score indicates that polyBERT ﬁnds th e masked tokens in almost all\ncases. The total CO 2 emissions for training polyBERT on our hardware are estimat ed to be\n12.6 kgCO2eq (see CO 2 Emission section).\nThe training with 80 million PSMILES strings renders polyBE RT an expert polymer\nchemical linguist who knows grammatical and syntactical ru les of the polymer chemical\nlanguage. polyBERT learns patterns and relations of tokens via the multi-head self-attention\nmechanism and fully connected feed-forward network of the T ransformer encoders. 25 The\nattention mechanism instructs polyBERT to devote more focu s to a small but essential\npart of a PSMILES string. polyBERT’s learned latent spaces a fter each encoder block\nare numerical representations of the input PSMILES strings . The polyBERT ﬁngerprint\nis the average over the token dimension (sentence average) o f the last latent space (dotted\n7\nline in Figure 1b). We use the Python package SentenceTransformers 44 for extracting and\ncomputing polyBERT ﬁngerprints.\na polyBERT\nb Polymer Genome\n200 400 600 800\nTg (K)\n500 750 1000\nTd (K)\n2.5 5.0 7.5\nEgc (eV)\nHP\nCP\n[*]CC([*])CC\n[*]CC([*])CCC\n[*]CC([*])c1ccncc1\nFigure 2: Two-dimensional UMAP 45 plots for polyBERT and Polymer Genome ﬁngerprints\nand all homo- and copolymer chemistries in Table 1. The triangles (blue, orange, and green)\nin the ﬁrst column indicate ﬁngerprint positions in the UMAP sp aces of three selected\npolymers. The colored dots in columns two, three, and four in dicate property values of Tg,\nTd, and Egc, which stand for the glass transition temperature, degrada tion temperature,\nand band gap (chain), respectively. Light gray dots show pol ymers with unknown property\nvalues. The PSMILES strings [*]CC([*])CC, [*]CC([*])CCC, and [*]CC([*])c1ccncc1\ndenote poly(but-1-ene), poly(pent-1-ene), and poly(4-vi nylpyridine), respectively.\nFingerprints For acquiring analogies and juxtaposing chemical relevanc y, we compare\npolyBERT ﬁngerprints with the handcrafted Polymer Genome 8 (PG) ﬁngerprints that nu-\nmerically encode polymers at three diﬀerent length scales. A description of PG ﬁngerprints\ncan be found in the Methods section. The PG ﬁngerprint vector for the data set in this\nwork has 945 components and is sparsely populated (93.9 % zer os). The reason for this ultra\nsparsity is that many PG ﬁngerprint components count chemic al groups in polymers. 8 A\nﬁngerprint component of zero indicates that a chemical grou p is not present. In contrast,\npolyBERT ﬁngerprint vectors have 600 components and are ful ly dense (0 % zeros). Fully\n8\ndense and lower-dimensional ﬁngerprints are often advanta geous for ML models whose com-\nputation time scales superlinear ( O(ns), s > 1) with the data set size ( n) such as Gaussian\nprocess or kernel ridge techniques. Moreover, in the case of neural networks, sparse and\nhigh-dimensional input vectors can cause unnecessary high memory load that reduces train-\ning and inference speed. We note that the dimensionality of p olyBERT ﬁngerprints is a\nparameter that can be chosen arbitrarily to yield the best tr aining result. A summary of the\nkey ﬁgures can be found in Supplementary Table S2.\nFigure 2 shows uniform manifold approximation and projection (UMAP) 45 plots for all\nhomo- and copolymer chemistries in Table 1. The colored triangles in the ﬁrst column\nindicate the coordinates of three selected polymers for pol yBERT and PG ﬁngerprints. We\nobserve for both ﬁngerprint types that the orange and blue tri angles are very close, while the\ngreen triangle is separate. We also note that polymers corre sponding to the orange and blue\ntriangles, namely poly(but-1-ene) and poly(pent-1-ene), have similar chemistry (diﬀerent\nby only one carbon atom), but poly(4-vinylpyridine) repres ented by a green triangle, is\ndiﬀerent. This chemically intuitive positioning of ﬁngerp rints suggests the chemical relevancy\nof ﬁngerprint distances. The cosine ﬁngerprint distances r eported in Supplementary Figure\nS1 allow for the same conclusion.\nThe second, third, and fourth columns of Figure 2 display the same UMAP plots as in\nthe ﬁrst column. Colored dots indicate the property values o f Tg, Td, and Egc, while light\ngray dots show polymer ﬁngerprints with unknown property va lues. We observe localized\nclusters of similar color in each plot pertaining to polymer s of similar properties. Although\nthis ﬁnding is not surprising for the PG ﬁngerprint because i t relies on handcrafted chem-\nical features that purposely position similar polymers nex t to each other, it is remarkable\nfor polyBERT. With no chemical information and purely based o n training on a massive\namount of PSMILES strings, polyBERT has learned polymer ﬁng erprints that match chem-\nical intuition. This again shows that polyBERT ﬁngerprints have chemical pertinence and\ntheir distances measure polymer similarity (e.g., using th e cosine distance metric).\n9\n100 101 102 103 104\nNumber of PSMILES\n10− 1\n100\n101\n102\n103\nComputation time (s)\npolyBERT (CPU)\npolyBERT (GPU)\nPolymer Genome\nFigure 3: Computation time of polymer ﬁngerprints. The ﬁnger prints are computed on one\nCPU core (Intel(R) Xeon(R) CPU E5-2667), except for polyBERT (GPU) ﬁngerprints that\nare computed on one GPU (Quadro GP100). Computation times pe r PSMILES string, in the\norder of the legend, are 33.39, 0.76, and 163.59 ms/PSMILES ( computed for 10 4 PSMILES),\nrespectively.\nThe computation of polyBERT and PG ﬁngerprints scales nearl y linear with the number\nof PSMILES strings although their performance can be quite d iﬀerent, as shown in the log-\nlog scaled Figure 3. The computation of polyBERT (GPU) is over two orders of magni tude\n(215 times) faster than computing PG ﬁngerprints. polyBERT ﬁngerprints may be computed\non CPUs and GPUs. Because of the presently large eﬀorts in indus try to develop faster and\nbetter GPUs, we expect the computation of polyBERT ﬁngerprin t to become even faster in\nthe future. Time is extremely important for high-throughpu t polymer informatics pipelines\nthat identify polymers from large candidate sets. 11 With an estimate of 0.30 ms/PSMILES for\nthe multitask deep neural networks (see Property Predictio n section), the total time using the\npolyBERT-based pipeline to predict 29 polymer properties s ums to 1.06 ms/polymer/GPU.\nProperty Prediction For benchmarking the property prediction accuracy of polyB ERT\nand PG ﬁngerprints, we train multitask deep neural networks for each property category\ndeﬁned in Table 1. Multitask deep neural networks have demonstrated best-in -class results\n10\npolyBERT Polymer Genome\nTg\nTm\nTd\nThermal\n0.92 ± 0.01 0.92 ± 0.00\n0.84 ± 0.02 0.84 ± 0.02\n0.70 ± 0.03 0.72 ± 0.02\n0.82 ± 0.02 0.83 ± 0.01\na PB PG\n0.99 0.98\n0.97 0.97\n0.96 0.96\n0.97 0.97\nb\nmin mean max\n162.21 498.71 836.81\n275.11 592.19 873.36\n394.11 713.01 1185.81\nc\ncp\nEat\nOi\nXc\nXe\nρ\nThermo. & phys.\n0.61 ± 0.11 0.76 ± 0.10\n0.85 ± 0.02 0.94 ± 0.02\n0.57 ± 0.16 0.61 ± 0.16\n0.50 ± 0.11 0.47 ± 0.06\n0.44 ± 0.28 0.41 ± 0.32\n0.75 ± 0.03 0.81 ± 0.05\n0.62 ± 0.12 0.67 ± 0.12\n0.93 0.97\n0.97 0.98\n0.92 0.96\n0.90 0.90\n0.87 0.90\n0.95 0.96\n0.92 0.94\n0.87 1.28 2.33\n-6.69 -6.03 -4.93\n15.11 32.47 72.87\n3.90 41.35 100.50\n5.19 38.25 92.05\n0.88 1.30 2.07\nEgc\nEgb\nEea\nEi\nEib\nδ\nElectronic\n0.89 ± 0.02 0.90 ± 0.02\n0.93 ± 0.01 0.94 ± 0.01\n0.93 ± 0.03 0.93 ± 0.03\n0.82 ± 0.07 0.85 ± 0.03\n0.71 ± 0.05 0.72 ± 0.06\n0.57 ± 0.36 0.65 ± 0.28\n0.81 ± 0.09 0.83 ± 0.07\n0.98 0.98\n0.98 0.98\n0.99 0.99\n0.96 0.97\n0.94 0.93\n0.96 0.95\n0.97 0.97\n0.42 3.42 10.39\n0.78 3.06 10.59\n0.59 2.26 5.12\n3.81 5.82 10.63\n2.00 3.34 5.42\n32.15 132.43 293.57\nnc\nne\nkc\nkf\nOptical & diele.\n0.86 ± 0.06 0.89 ± 0.05\n0.76 ± 0.06 0.78 ± 0.02\n0.86 ± 0.06 0.84 ± 0.04\n0.91 ± 0.03 0.92 ± 0.02\n0.85 ± 0.05 0.86 ± 0.03\n0.98 0.99\n0.98 0.98\n0.97 0.98\n0.98 0.98\n0.97 0.98\n1.55 1.95 3.22\n1.33 1.63 2.21\n2.92 4.45 10.46\n2.10 3.45 9.86\nE\nσy\nσb\nǫb\nMechanical\n0.75 ± 0.07 0.75 ± 0.04\n0.80 ± 0.08 0.81 ± 0.08\n0.76 ± 0.05 0.79 ± 0.04\n0.62 ± 0.06 0.61 ± 0.07\n0.73 ± 0.07 0.74 ± 0.06\n0.94 0.94\n0.97 0.96\n0.94 0.94\n0.92 0.92\n0.94 0.94\n177.56 2016.50 3884.81\n5.94 67.52 123.94\n9.05 83.44 184.27\n0.85 14.79 565.19\nµO2\nµCO2\nµN2\nµH2\nµHe\nµCH4\nGas permeability\n0.96 ± 0.01 0.96 ± 0.01\n0.94 ± 0.02 0.94 ± 0.03\n0.96 ± 0.03 0.96 ± 0.03\n0.97 ± 0.01 0.97 ± 0.01\n0.95 ± 0.02 0.96 ± 0.02\n0.95 ± 0.03 0.96 ± 0.03\n0.95 ± 0.02 0.96 ± 0.02\n0.99 0.99\n0.98 0.98\n0.99 0.99\n0.99 1.00\n0.99 0.99\n0.99 0.99\n0.99 0.99\n-0.36 5.97 3047.37\n-0.40 25.41 21933.14\n-0.39 1.57 892.52\n-0.41 41.68 14659.80\n-0.34 34.99 6484.88\n-0.47 1.74 1683.41\nOverall 0.80 ± 0.06 0.81 ± 0.06 0.96 0.97\n0.5 0.6 0.7 0.8 0.9\nR2\nFigure 4: R2 performance values for polyBERT (PB) and Polymer Genome (PG ) ﬁngerprints.\na R2 averages of the ﬁve cross-validation validation data sets a long with standard deviations\n(1σ). b R2 values of the meta learner’s test data set. The category-ave raged R2 values are\nstated in the last rows of each block, while overall R2 values are given in the very last block.\nThe properties gas permeabilities ( µx) and elongation at break ( ǫb) are trained on log base\n10 scale ( x ↦→log10(x+1)). The R2 values are reported on this scale. c Minimum, mean, and\nmaximum of polyBERT-based property predictions for 100 mil lion hypothetical polymers.\nSymbols are deﬁned in Table 1.\n11\nfor polymer property predictions, 6,7,11 while being fast, scalable, and readily amenable if more\ndata points become available. Unlike single-task models, mu ltitask models simultaneously\npredict numerous properties (tasks) and harness inherent b ut hidden correlations in data\nto improve their performance. Such correlation exists, for instance, between Tg and Tm,\nbut the exact correlation varies across speciﬁc polymer che mistries. Multitask models learn\nand improve from these varying correlations in data. The tra ining protocol of the multitask\ndeep neural networks follows state-of-the-art methods inv olving ﬁve-fold cross-validation\nand a consolidating meta learner that forecasts the ﬁnal pro perty values based upon the\nensemble of cross-validation predictors. More details abo ut multitask deep neural networks\nare provided in the Methods section. Their training process is outlined in Supplementary\nFigure S2.\nFigure 4a shows the color-encoded ﬁve-fold cross-validation coeﬃc ient of determination\n(R2) averages across the ﬁve validation data sets for 29 polymer properties. Root-mean-\nsquare errors (RMSEs) can be found in Supplementary Table S1 . Overall, PG performs best\n(R2 = 0 .81) but is very closely followed by polyBERT ( R2 = 0 .80). This overall performance\norder of the ﬁngerprint types is persistent with the categor y averages and properties, except\nfor Xc, Xe, and ǫb, where polyBERT slightly outperforms PG ﬁngerprints. We no te that\npolyBERT and PG ﬁngerprints are both practical routes for po lymer featurization because\ntheir R2 values lie close together and are generally high. polyBERT ﬁ ngerprints have the\naccuracy of the handcrafted PG ﬁngerprints but are over two o rders of magnitude faster (see\nFigure 3).\nFigure 4b shows high R2 values for each meta learner (one for each category), sugges ting\nan exceptional prediction performance across all properti es. We train the meta learners on\nunseen 20 % of the data set and validate using 80 % of the data se t (also used for cross-\nvalidation). The reported validation R2 values thus only partly measure the generalization\nperformance with respect to the full data set. Meta learners can be conceived as taking\ndecisive roles in selecting the best values from the predict ions of the ﬁve cross-validation\n12\nmodels. We use the meta learners for all property prediction s in this work. Supplementary\nFigures S9-S14 show the meta learners’ parity plots.\nThe ultrafast and accurate polyBERT-based polymer informa tics pipeline allows us to\npredict all 29 properties of the 100 million hypothetical po lymers that were originally created\nto train polyBERT. Figure 4c shows the minimum, mean, and maximum for each property.\nHistograms are given in Supplementary Figures S17-S22. Given the vast size of our data set\nand consequent chemical space of the 100 million hypothetic al polymers, the minimum and\nmaximum values can be interpreted as potential boundaries o f the total polymer property\nspace. The data set with 100 million hypothetical polymers i ncluding the predictions of 29\nproperties is available for academic use. The total CO 2 emissions for predicting 29 properties\nof 100 million hypothetical polymers are estimated to be 5.5 kgCO2eq (see CO 2 Emission\nsection).\nOther Advantages of polyBERT: Beyond Speed and Accuracy\nThe feed-forward network (last layer in Figure\n1b), which predicts masked tokens during the\nself-supervised training of polyBERT, enables the mapping of numerical latent spaces (i.e.,\nﬁngerprints) to PSMILES strings. However, because we avera ge over the token dimension\nof the last latent space to compute ﬁngerprints, we cannot un ambiguously map the current\npolyBERT ﬁngerprints back to PSMILES strings. A modiﬁed fut ure version of polyBERT\nthat provides PSMILES strings encoding and ﬁngerprint deco ding could involve inserting\na dimensionality-reducing layer after the last Transforme r encoder. Fingerprint decoders\nare important elements of design informatics pipelines tha t invert the prediction pipeline\nto meet property speciﬁcations. We note that the current cho ice of computing polyBERT\nﬁngerprints as pooling averages stems from basic dimension ality reduction considerations\nthat require no modiﬁcation of the Transformer architectur e.\nA second advantage of the polyBERT approach is interpretabi lity. Analysing the chemical\nrelevancy of polyBERT ﬁngerprints (as discussed in the Finge rprints section) in greater\n13\ndetail can reveal chemical functions and interactions of st ructural parts of the polymers.\nAs shown for trained NLP Transformers, 46 deciphering and visualizing the attention layers\nof the Transformer encoders can reveal such information. Sa liency methods 47 may explain\nthe relationships between structural parts of the PSMILES s trings (inputs) and polymer\nproperties (outputs).\nDiscussion\nHere, we show a generalizable, ultrafast, and accurate polym er informatics pipeline that is\nseamlessly scalable on cloud hardware and suitable for high -throughput screening of huge\npolymer spaces. polyBERT, which is a Transformer-based NLP m odel modiﬁed for the poly-\nmer chemical language, is the critical element of our pipeli ne. After training on 100 million\nhypothetical polymers, the polyBERT-based informatics pi peline arrives at a representation\nof polymers and predicts polymer properties over two orders of magnitude faster but at the\nsame accuracy as the best pipeline based on handcrafted PG ﬁn gerprints.\nThe accurate prediction of 29 properties for 100 million hyp othetical polymers in a reason-\nable time demonstrates that polyBERT is an enabler to extens ive explorations of the polymer\nuniverse at scale. polyBERT paves the pathway for the discov ery of novel polymers 100 times\nfaster (and potentially even faster with newer GPU generati ons) than state-of-the-art infor-\nmatics approaches – but at the same accuracy as slower handcr afted ﬁngerprinting methods\n– by leveraging Transformer-based ML models originallydro developed for NLP. polyBERT\nﬁngerprints are dense and chemically pertinent numerical r epresentations of polymers that\nadequately measure polymer similarity. They can be used for any polymer informatics task\nthat requires numerical representations of polymers such a s property predictions (demon-\nstrated here), polymer structure predictions, ML-based sy nthesis assistants, etc. polyBERT\nﬁngerprints have a huge potential to accelerate past polyme r informatics pipelines by re-\nplacing the handcrafted ﬁngerprints with polyBERT ﬁngerpr ints. polyBERT may also be\n14\nused to directly design polymers based on ﬁngerprints (that can be related to properties)\nusing polyBERT’s decoder that has been trained during the se lf-supervised learning. This,\nhowever, requires retraining and structural updates to pol yBERT and is thus part of a future\nwork.\nMethods\nPSMILES Canonicalization The string representations of homopolymer repeat units in\nthis work are polymer SMILES (PSMILES) strings. PSMILES str ings follow the SMILES\n18\nsyntax deﬁnition but use two stars to indicate the two endpoi nts of the polymer repeat unit\n(e.g., [*]CC[*] for polyethylene). The raw PSMILES syntax is non-unique; i. e., the same\npolymer may be represented using many PSMILES strings; cano nicalization is a scheme to\nreduce the diﬀerent PSMILES strings of the same polymer to a s ingel unique canonicalized\nPSMILES string. polyBERT requires canonicalized PSMILES s trings because polyBERT\nﬁngerprints change with diﬀerent writings of PSMILES strin gs. In contrast, PG ﬁngerprints\nare invariant to the way of writing PSMILES strings and, thus , do not require canonicaliza-\ntion. Figure 5 shows three variances of PSMILES strings that leave the poly mer unchanged.\nThe translational variance of PSMILES strings allows to mov e the repeat unit window of\npolymers (cf., white and red box). The multiplicative varia nce permits to write polymers as\nmultiples of the repeat unit (e.g., two-fold repeat unit of Ny lon 6), while the permutational\nvariance stems from the SMILES syntax deﬁnition 18 and allows syntactical permutations of\nPSMILES strings that leave the polymer unchanged.\nFor this work, we developed the canonicalize psmiles Python package that ﬁnds the\ncanonical form of PSMILES strings in four steps; (i) it ﬁnds t he shortest PSMILES string\nby searching and removing repetition patterns, (ii) it conn ects the polymer endpoints to\ncreate a periodic PSMILES string, (iii) it canonicalizes th e periodic PSMILES string using\nRDKit33’s canonicalization routines, (iv) it breaks the periodic P SMILES string to create\n15\nN\nO\nTranslation\nN\nO\nNylon 6\n[*]NCCCCCC(=O)[*] [*]CCC(=O)NCCC[*]\n[*]NCCCCCC(=O)NCCCCCC(=O)[*]\nMultiplication:\nH\nH\n[*]NCCCCCC(NCCCCCC([*])=O)=O\nMultiplication and permutation: \nFigure 5: Translational, multiplicative, and permutationa l variances of PSMILES strings.\nThe gray and red boxes represent the smallest repeat unit of p oly(hexano-6-lactam) (Nylon\n6). The red box can be translated to match the black box. The da shed boxes show the\nsecond smallest repeat unit (two-fold repeat unit) of Nylon 6 .\nthe canonical PSMILES string. The canonicalize psmiles package is available at https:\n//github.com/Ramprasad-Group/canonicalize_psmiles.\nPolymer Fingerprinting Fingerprinting converts geometric and chemical informatio n of\npolymers (based upon the PSMILES string) to machine-readab le numerical representations\nin the form of vectors. These vectors are the polymer ﬁngerpr ints and can be used for prop-\nerty predictions, similarity searches, or other tasks that require numerical representations of\npolymers.\nWe compare the polyBERT ﬁngerprints, developed in this work , with the handcrafted\nPolymer Genome (PG) polymer ﬁngerprints. PG ﬁngerprints ca pture key features of poly-\nmers at three hierarchical length scales. 8,22 At the atomic scale (1 st level), PG ﬁngerprints\ntrack the occurrence of a ﬁxed set of atomic fragments (or mot ifs).23 The block scale\n(2nd level) uses the quantitative structure-property relation ship (QSPR) ﬁngerprints 20,36 for\ncapturing features on larger length-scales as implemented in the cheminformatics toolkit\nRDKit.33 The chain scale (3 rd level) ﬁngerprint components deal with “morphological de-\nscriptors” such as the ring distance or length of the largest side-chain.36 The PG ﬁn-\ngerprints are developed within the Ramprasad research grou p and used, for example, at\n16\nhttps://PolymerGenome.org. More details can be found in References 8,36.\nAs discussed recently, 6,11 we sum the composition-weighted polymer ﬁngerprints to com -\npute copolymer ﬁngerprints F = ∑ N\ni Fici, where N is the number of comonomers in the\ncopolymer, Fi the ith comonomer ﬁngerprint, and ci the fraction of the ith comonomer.\nThis approach renders copolymer ﬁngerprints invariant to t he order in which one may sort\nthe comonomers and satisﬁes the two main demands of uniquene ss and invariance to dif-\nferent (but equivalent) periodic unit speciﬁcations. Cont rary to homopolymer ﬁngerprints,\ncopolymer ﬁngerprints may not be interpretable (e.g., the c omposition-weighted sum of the\nﬁngerprint component “length of largest side-chain” of two homopolymers has no physical\nmeaning).\nMultitask Neural Networks Multitask deep neural networks simultaneously learn mul-\ntiple polymer properties to utilize inherent correlations of properties in data sets. The\ntraining protocol of the concatenation-conditioned multi task predictors follows state-of-the-\nart techniques involving ﬁve-fold cross-validation and a m eta learner that forecasts the ﬁnal\nproperty values based upon the ensemble of cross-validatio n predictors.\n6,7,11 Supplementary\nFigure S2 details this process. After shuﬄing, we split the dat a set into two parts and use\n80 % for the ﬁve cross-validation models and for validating t he meta learners. 20 % of the\ndata set is used for training the meta learners. We perform da ta set stratiﬁcation of all\nsplits based on the polymer properties. All parameters of th e neural networks, such as the\nnumber of layers, number of nodes, dropout rates, and activa tion functions, are optimized\nusing the Hyperband method 48 of the Python package KerasTuner. 49 The multitask deep\nneural networks are implemented using the Python API of Tenso rFlow.50\nCO2 Emission\nExperiments were conducted using a private infrastructure , which has an estimated carbon\neﬃciency of 0.432 kgCO 2eq kWh − 1. A cumulative of 31 hours of computation was performed\n17\non four Quadro-GP100-16GB (thermal design power of 235 W) for training polyBERT. Total\nemissions are estimated to be 12.6 kgCO 2eq. The total emissions for predicting 29 properties\nfor 100 million hypothetical polymers are estimated to be 5. 5 kgCO2eq. Estimations were\nconducted using the Machine Learning Impact calculator pre sented in Reference 51.\nData and Code Availability\nThe polyBERT code and data set of 100 million hypothetical po lymers with the predictions\nof 29 properties are available for academic use at\nhttps://github.com/Ramprasad-Group/\npolyBERT. The trained polyBERT model is available at https://huggingface.co/kuelumbus/\npolyBERT. The Python package for canonicalizing PSMILES strings is a vailable at https:\n//github.com/Ramprasad-Group/canonicalize_psmiles. polyBERT-based property pre-\ndictions will be made accessible through the polymer inform atics platform Polymer Genome\nat https://PolymerGenome.org.\nDeclaration of Interests\nR.R. is the founder of the company Matmerize, Inc., that inte nds to provide polymer infor-\nmatics services. A provisional patent on polyBERT has been ﬁ led by R.R. and C.K.\nAcknowledgement\nC.K. thanks the Alexander von Humboldt Foundation for ﬁnanci al support. We acknowledge\nfunding from the Oﬃce of Naval Research through a Multidiscip linary University Research\nInitiative grant (N00014-17-1-2656) and the National Scienc e Foundation (#1941029).\n18\nAuthor Contributions\nC. K. designed, trained and evaluated the machine learning m odels and drafted this paper.\nThe work was conceived and guided by R. R. All authors discusse d results and commented\non the manuscript.\nSupporting Information Available\nSupplementary Figures and Tables are available.\nReferences\n(1) Plastics Europe.\nhttps://plasticseurope.org/knowledge-hub/\nplastics-the-facts-2021/ .\n(2) Batra, R.; Song, L.; Ramprasad, R. Emerging materials in telligence ecosystems\npropelled by machine learning. Nature Reviews Materials 2021, 6, 655–678, DOI:\n10.1038/s41578-020-00255-y.\n(3) Chen, L.; Pilania, G.; Batra, R.; Huan, T. D.; Kim, C.; Kuen neth, C.; Ramprasad, R.\nPolymer informatics: Current status and critical next step s. Materials Science and\nEngineering: R: Reports 2021, 144, 100595, DOI: 10.1016/j.mser.2020.100595.\n(4) Audus, D. J.; de Pablo, J. J. Polymer Informatics: Opportuni ties and Challenges. ACS\nMacro Letters 2017, 6, 1078–1082, DOI: 10.1021/acsmacrolett.7b00228.\n(5) Adams, N.; Murray-Rust, P. Engineering Polymer Informat ics: Towards the Computer-\nAided Design of Polymers. Macromolecular Rapid Communications 2008, 29, 615–632,\nDOI: 10.1002/marc.200700832.\n19\n(6) Kuenneth, C.; Schertzer, W.; Ramprasad, R. Copolymer Inf ormatics with\nMultitask Deep Neural Networks. Macromolecules 2021, 54, 5957–5961, DOI:\n10.1021/acs.macromol.1c00728.\n(7) Kuenneth, C.; Rajan, A. C.; Tran, H.; Chen, L.; Kim, C.; Ramp rasad, R.\nPolymer informatics with multi-task learning. Patterns 2021, 2, 100238, DOI:\n10.1016/j.patter.2021.100238.\n(8) Doan Tran, H.; Kim, C.; Chen, L.; Chandrasekaran, A.; Batra , R.; Venkatram, S.;\nKamal, D.; Lightstone, J. P.; Gurnani, R.; Shetty, P.; Rampra sad, M.; Laws, J.; Shel-\nton, M.; Ramprasad, R. Machine-learning predictions of pol ymer properties with Poly-\nmer Genome. Journal of Applied Physics 2020, 128, 171104, DOI: 10.1063/5.0023759.\n(9) Chen, G.; Tao, L.; Li, Y. Predicting Polymers’ Glass Tran sition Temperature\nby a Chemical Language Processing Model. Polymers 2021, 13, 1898, DOI:\n10.3390/polym13111898.\n(10) Pilania, G.; Iverson, C. N.; Lookman, T.; Marrone, B. L. M achine-Learning-Based Pre-\ndictive Modeling of Glass Transition Temperatures: A Case o f Polyhydroxyalkanoate\nHomopolymers and Copolymers. Journal of Chemical Information and Modeling 2019,\n59, 5013–5025, DOI: 10.1021/acs.jcim.9b00807.\n(11) Kuenneth, C.; Lalonde, J.; Marrone, B. L.; Iverson, C. N.; Ramprasad, R.; Pi-\nlania, G. Bioplastic Design using Multitask Deep Neural Netwo rks. 2022, DOI:\n10.48550/arXiv.2203.12033.\n(12) Barnett, J. W.; Bilchak, C. R.; Wang, Y.; Benicewicz, B. C.; Murdock, L. A.;\nBereau, T.; Kumar, S. K. Designing exceptional gas-separat ion polymer membranes\nusing machine learning. Science Advances 2020, 6, DOI: 10.1126/sciadv.aaz4301.\n(13) Kim, C.; Batra, R.; Chen, L.; Tran, H.; Ramprasad, R. Poly mer design using genetic\n20\nalgorithm and machine learning. Computational Materials Science 2021, 186, 110067,\nDOI: 10.1016/j.commatsci.2020.110067.\n(14) Kern, J.; Chen, L.; Kim, C.; Ramprasad, R. Design of polym ers for energy storage\ncapacitors using machine learning and evolutionary algori thms. Journal of Materials\nScience 2021, 56, 19623–19635, DOI: 10.1007/s10853-021-06520-x.\n(15) Gurnani, R.; Kamal, D.; Tran, H.; Sahu, H.; Scharm, K.; Ashr af, U.; Ram-\nprasad, R. polyG2G: A Novel Machine Learning Algorithm Applied to the Genera-\ntive Design of Polymer Dielectrics. Chemistry of Materials 2021, 33, 7008–7016, DOI:\n10.1021/acs.chemmater.1c02061.\n(16) Batra, R.; Dai, H.; Huan, T. D.; Chen, L.; Kim, C.; Gutekuns t, W. R.; Song, L.;\nRamprasad, R. Polymers for Extreme Conditions Designed Usin g Syntax-Directed\nVariational Autoencoders. Chemistry of Materials 2020, 32, 10489–10500, DOI:\n10.1021/acs.chemmater.0c03332.\n(17) Wu, S.; Kondo, Y.; Kakimoto, M.-a.; Yang, B.; Yamada, H.; K uwajima, I.; Lam-\nbard, G.; Hongo, K.; Xu, Y.; Shiomi, J.; Schick, C.; Morikawa, J.; Y oshida, R.\nMachine-learning-assisted discovery of polymers with hig h thermal conductivity us-\ning a molecular design algorithm. npj Computational Materials 2019, 5, 66, DOI:\n10.1038/s41524-019-0203-2.\n(18) Weininger, D. SMILES, a chemical language and informat ion system. 1. Introduction\nto methodology and encoding rules. Journal of Chemical Information and Modeling\n1988, 28, 31–36, DOI: 10.1021/ci00057a005.\n(19) Degen, J.; Wegscheid-Gerlach, C.; Zaliani, A.; Rarey, M. On the Art of Compiling and\nUsing ’Drug-Like’ Chemical Fragment Spaces. ChemMedChem 2008, 3, 1503–1507,\nDOI: 10.1002/cmdc.200800178.\n21\n(20) Le, T.; Epa, V. C.; Burden, F. R.; Winkler, D. A. Quantitative Structure–Property\nRelationship Modeling of Diverse Materials Properties. Chemical Reviews 2012, 112,\n2889–2919, DOI: 10.1021/cr200066h.\n(21) Rogers, D.; Hahn, M. Extended-Connectivity Fingerprint s. Journal of Chemical Infor-\nmation and Modeling 2010, 50, 742–754, DOI: 10.1021/ci100050t.\n(22) Mannodi-Kanakkithodi, A.; Pilania, G.; Huan, T. D.; Look man, T.; Ramprasad, R.\nMachine Learning Strategy for Accelerated Design of Polymer Dielectrics. Scientiﬁc\nReports 2016, 6, 20952, DOI: 10.1038/srep20952.\n(23) Huan, T. D.; Mannodi-Kanakkithodi, A.; Ramprasad, R. Acce lerated materials prop-\nerty predictions and design using motif-based ﬁngerprints . Physical Review B 2015,\n92, 014106, DOI: 10.1103/PhysRevB.92.014106.\n(24) Moriwaki, H.; Tian, Y.-S.; Kawashita, N.; Takagi, T. Mordr ed: a molec-\nular descriptor calculator. Journal of Cheminformatics 2018, 10, 4, DOI:\n10.1186/s13321-018-0258-y.\n(25) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L. ; Gomez, A. N.; Kaiser, L.;\nPolosukhin, I. Attention Is All You Need. 2017, DOI: 10.48550/arXiv.1706.03762.\n(26) Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa: Large-Scale\nSelf-Supervised Pretraining for Molecular Property Predi ction. 2020, DOI:\n10.48550/arXiv.2010.09885.\n(27) Wang, S.; Guo, Y.; Wang, Y.; Sun, H.; Huang, J. SMILES-BERT. Pr oceedings\nof the 10th ACM International Conference on Bioinformatics , Computational Bi-\nology and Health Informatics. New York, NY, USA, 2019; pp 429–436, D OI:\n10.1145/3307339.3342186.\n22\n(28) Li, J.; Jiang, X. Mol-BERT: An Eﬀective Molecular Represent ation with BERT\nfor Molecular Property Prediction. Wireless Communications and Mobile Computing\n2021, 2021, 1–7, DOI: 10.1155/2021/7181815.\n(29) Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. BERT: Pre -training\nof Deep Bidirectional Transformers for Language Understand ing. 2018, DOI:\n10.48550/arXiv.1810.04805.\n(30) Schwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Hunter , C. A.; Bekas, C.; Lee, A. A.\nMolecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Predic-\ntion. ACS Central Science 2019, 5, 1572–1583, DOI: 10.1021/acscentsci.9b00576.\n(31) Schwaller, P.; Petraglia, R.; Zullo, V.; Nair, V. H.; Haeusel mann, R. A.; Pisoni, R.;\nBekas, C.; Iuliano, A.; Laino, T. Predicting retrosynthetic pathways using transformer-\nbased models and a hyper-graph exploration strategy. Chemical Science 2020, 11,\n3316–3325, DOI: 10.1039/C9SC05704H.\n(32) He, P.; Liu, X.; Gao, J.; Chen, W. DeBERTa: Decoding-enhance d BERT with Disen-\ntangled Attention. 2020, DOI: 10.48550/arXiv.2006.03654.\n(33) Landrum, G.; others, RDKit: Open-source cheminformati cs. 2006,\n(34) Jha, A.; Chandrasekaran, A.; Kim, C.; Ramprasad, R. Impact of dataset uncertainties\non machine learning model predictions: the example of polym er glass transition tem-\nperatures. Modelling and Simulation in Materials Science and Engineering 2019, 27,\n024002, DOI: 10.1088/1361-651X/aaf8ca.\n(35) Kim, C.; Chandrasekaran, A.; Jha, A.; Ramprasad, R. Active- learning and materials\ndesign: the example of high glass transition temperature po lymers. MRS Communica-\ntions 2019, 9, 860–866, DOI: 10.1557/mrc.2019.78.\n23\n(36) Kim, C.; Chandrasekaran, A.; Huan, T. D.; Das, D.; Rampras ad, R. Polymer Genome:\nA Data-Powered Polymer Informatics Platform for Property P redictions. The Journal\nof Physical Chemistry C 2018, 122, 17575–17585, DOI: 10.1021/acs.jpcc.8b02913.\n(37) Patra, A.; Batra, R.; Chandrasekaran, A.; Kim, C.; Huan, T. D.; Ram-\nprasad, R. A multi-ﬁdelity information-fusion approach to machine learn and pre-\ndict polymer bandgap. Computational Materials Science 2020, 172, 109286, DOI:\n10.1016/j.commatsci.2019.109286.\n(38) Chen, L.; Kim, C.; Batra, R.; Lightstone, J. P.; Wu, C.; Li , Z.; Deshmukh, A. A.;\nWang, Y.; Tran, H. D.; Vashishta, P.; Sotzing, G. A.; Cao, Y.; Ramp rasad, R.\nFrequency-dependent dielectric constant prediction of po lymers using machine learning.\nnpj Computational Materials 2020, 6, 61, DOI: 10.1038/s41524-020-0333-6.\n(39) Venkatram, S.; Kim, C.; Chandrasekaran, A.; Ramprasad, R. Critical Assessment of\nthe Hildebrand and Hansen Solubility Parameters for Polymers . Journal of Chemical\nInformation and Modeling 2019, 59, 4188–4194, DOI: 10.1021/acs.jcim.9b00656.\n(40) Zhu, G.; Kim, C.; Chandrasekarn, A.; Everett, J. D.; Rampr asad, R.; Lively, R. P.\nPolymer genome–based prediction of gas permeabilities in p olymers. Journal of Polymer\nEngineering 2020, 40, 451–457, DOI: 10.1515/polyeng-2019-0329.\n(41) PolyInfo. https://polymer.nims.go.jp/en/.\n(42) Wolf, T. et al. Transformers: State-of-the-Art Natural L anguage Processing. Pro-\nceedings of the 2020 Conference on Empirical Methods in Natur al Language Pro-\ncessing: System Demonstrations. Stroudsburg, PA, USA, 2020; p p 38–45, DOI:\n10.18653/v1/2020.emnlp-demos.6.\n(43) Kudo, T.; Richardson, J. SentencePiece: A simple and lan guage indepen-\ndent subword tokenizer and detokenizer for Neural Text Proce ssing. 2018, DOI:\n10.48550/arXiv.1808.06226.\n24\n(44) Reimers, N.; Gurevych, I. Sentence-BERT: Sentence Embe ddings using Siamese BERT-\nNetworks. 2019, DOI: 10.48550/arXiv.1908.10084.\n(45) McInnes, L.; Healy, J.; Melville, J. UMAP: Uniform Manifold App roximation and Pro-\njection for Dimension Reduction. 2018, DOI: 10.48550/arXiv.1802.03426.\n(46) Vig, J. A Multiscale Visualization of Attention in the Tran sformer Model. Pro-\nceedings of the 57th Annual Meeting of the Association for Comp utational Lin-\nguistics: System Demonstrations. Stroudsburg, PA, USA, 2019; pp 37–42, DOI:\n10.18653/v1/P19-3007.\n(47) Bastings, J.; Filippova, K. The elephant in the interpret ability room: Why\nuse attention as explanation when we have saliency methods? 2020, DOI:\n10.48550/arXiv.2010.05607.\n(48) Li, L.; Jamieson, K.; DeSalvo, G.; Rostamizadeh, A.; Talw alkar, A. Hyperband: A\nNovel Bandit-Based Approach to Hyperparameter Optimization. Journal of Machine\nLearning Research 2016, 18, 1–52, DOI: 10.48550/arXiv.1603.06560.\n(49) O’Malley, T.; Bursztein, E.; Long, J.; Chollet, F.; Jin, H.; Invernizzi, L. Keras Tuner.\n2019; https://github.com/keras-team/keras-tuner.\n(50) Martin, A. et al. TensorFlow: Large-Scale Machine Learni ng on Heterogeneous Systems.\n2015; https://www.tensorflow.org/.\n(51) Lacoste, A.; Luccioni, A.; Schmidt, V.; Dandres, T. Quanti fying the Carbon Emissions\nof Machine Learning. 2019, DOI: 10.48550/arXiv.1910.09700.\n25\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nPolyBERTSI.pdf",
  "topic": "Informatics",
  "concepts": [
    {
      "name": "Informatics",
      "score": 0.6757033467292786
    },
    {
      "name": "Ultrashort pulse",
      "score": 0.6351431012153625
    },
    {
      "name": "Polymer",
      "score": 0.5559172034263611
    },
    {
      "name": "Computer science",
      "score": 0.5070888996124268
    },
    {
      "name": "Nanotechnology",
      "score": 0.3438515067100525
    },
    {
      "name": "Data science",
      "score": 0.3376971483230591
    },
    {
      "name": "Materials science",
      "score": 0.2882040739059448
    },
    {
      "name": "Engineering",
      "score": 0.22240793704986572
    },
    {
      "name": "Physics",
      "score": 0.14718559384346008
    },
    {
      "name": "Electrical engineering",
      "score": 0.11438456177711487
    },
    {
      "name": "Optics",
      "score": 0.09669899940490723
    },
    {
      "name": "Laser",
      "score": 0.06169724464416504
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}