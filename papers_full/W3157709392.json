{
    "title": "On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning",
    "url": "https://openalex.org/W3157709392",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5101110247",
            "name": "Xisen Jin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5102984496",
            "name": "Francesco Barbieri",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5008962142",
            "name": "Brendan Kennedy",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5000967986",
            "name": "Aida Mostafazadeh Davani",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5113916937",
            "name": "Leonardo Neves",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5009408707",
            "name": "Xiang Ren",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2893425640",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W2963549959",
        "https://openalex.org/W2964235839",
        "https://openalex.org/W2963526187",
        "https://openalex.org/W2970550868",
        "https://openalex.org/W2950145857",
        "https://openalex.org/W2926555354",
        "https://openalex.org/W2964186069",
        "https://openalex.org/W2595653137",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3161695571",
        "https://openalex.org/W2963116854",
        "https://openalex.org/W2165698076"
    ],
    "abstract": "Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task and domain at hand, the effects of bias mitigation may not directly transfer to new tasks, requiring additional data collection and customized annotation of sensitive attributes, and re-evaluation of appropriate fairness metrics. We explore the feasibility and benefits of upstream bias mitigation (UBM) for reducing bias on downstream tasks, by first applying bias mitigation to an upstream model through fine-tuning and subsequently using it for downstream fine-tuning. We find, in extensive experiments across hate speech detection, toxicity detection, occupation prediction, and coreference resolution tasks over various bias factors, that the effects of UBM are indeed transferable to new downstream tasks or domains via fine-tuning, creating less biased downstream models than directly fine-tuning on the downstream task or transferring from a vanilla upstream model. Though challenges remain, we show that UBM promises more efficient and accessible bias mitigation in LM fine-tuning.",
    "full_text": "On Transferability of Bias Mitigation Effects in Language\nModel Fine-Tuning\nXisen JinÂ§, Francesco Barbieriâ€ , Brendan KennedyÂ§, Aida Mostafazadeh DavaniÂ§,\nLeonardo Nevesâ€ , Xiang RenÂ§\nÂ§University of Southern California\nâ€ Snap Inc.\n{xisenjin, mostafaz, btkenned, xiangren}@usc.edu\n{fbarbieri, lneves}@snap.com\nAbstract\nFine-tuned language models have been shown\nto exhibit biases against protected groups in\na host of modeling tasks such as text classi-\nï¬cation and coreference resolution. Previous\nworks focus on detecting these biases, reduc-\ning bias in data representations, and using aux-\niliary training objectives to mitigate bias dur-\ning ï¬ne-tuning. Although these techniques\nachieve bias reduction for the task and domain\nat hand, the effects of bias mitigation may not\ndirectly transfer to new tasks, requiring addi-\ntional data collection and customized annota-\ntion of sensitive attributes, and re-evaluation\nof appropriate fairness metrics. We explore\nthe feasibility and beneï¬ts of upstream bias\nmitigation (UBM) for reducing bias on down-\nstream tasks, by ï¬rst applying bias mitiga-\ntion to an upstream model through ï¬ne-tuning\nand subsequently using it for downstream ï¬ne-\ntuning. We ï¬nd, in extensive experiments\nacross hate speech detection, toxicity detec-\ntion, occupation prediction, and coreference\nresolution tasks over various bias factors, that\nthe effects of UBM are indeed transferable to\nnew downstream tasks or domains via ï¬ne-\ntuning, creating less biased downstream mod-\nels than directly ï¬ne-tuning on the down-\nstream task or transferring from a vanilla up-\nstream model. Though challenges remain, we\nshow that UBM promises more efï¬cient and\naccessible bias mitigation in LM ï¬ne-tuning.12\n1 Introduction\nThe practice of ï¬ne-tuning pretrained language\nmodels (PTLMs or LMs), such as BERT (Devlin\net al., 2019), has improved prediction performance\nin a wide range of NLP tasks. However, ï¬ne-\ntuned LMs may exhibit biases against certain pro-\ntected groups (e.g., gender and ethnic minorities),\n1Code and data: https://github.com/INK-U\nSC/Upstream-Bias-Mitigation\n2The work was partially done when Xisen Jin was an intern\nat Snap Inc.\n(a) Classifier bias mitigation(b) Representational bias mitigation\nModel\n(c) Removing sensitive attributes from data\nDe-bias\nDataset\nDataset.man.woman.doctor\nDe-bias\nModel\nModelâ€¦De-bias Upstream Model Model\nDataset ğ’Ÿ!\"\nDatasetğ’Ÿ!#\n(d) Upstream bias mitigation for fine-tuning\nDataset\nDatarepresentationsâ€¦Frozen Model\nFine-tuning\nDatasets ğ’Ÿ$\",ğ’Ÿ$#â€¦\nDe-bias\nModel\nFigure 1: Comparison between the focus of our\nstudy (d) and previous works (a,b,c). We study the\nviability of obtaining an upstream model that could re-\nduce bias in a number of downstream classiï¬ers when\nï¬ne-tuned.\nas models may learn to associate certain features\nwith positive or negative labels spuriously (Dixon\net al., 2018), or propagate bias encoded in PTLMs\nto downstream classiï¬ers (Caliskan et al., 2017;\nBolukbasi et al., 2016). Among many exam-\nples, Kurita et al. (2019) demonstrates gender-bias\nin the pronoun resolution task when models are\ntrained using BERT embeddings, and Kennedy\net al. (2020) shows that hate speech classiï¬ers ï¬ne-\ntuned from BERT result in more frequent false pos-\nitive predictions for certain group identiï¬er men-\ntions (e.g., â€œmuslimâ€, â€œblackâ€).\nApproaches for bias mitigation are mostly ap-\nplied during ï¬ne-tuning to reduce bias in a spe-\nciï¬c downstream task or dataset (Park et al., 2018;\nZhang et al., 2018; Beutel et al., 2017) (see Fig. 1\n(a)). For example, data augmentation approaches\nreduce the inï¬‚uence of spurious features in the\noriginal dataset (Dixon et al., 2018; Zhao et al.,\n2018; Park et al., 2018), and adversarial learn-\ning approaches generate debiased data represen-\ntations that are exclusive to the downstream model\n(Kumar et al., 2019; Zhang et al., 2018). These\ntechniques act on biases particular to the given\ndataset, domain, or task, and require new bias miti-\ngation when switching to a new downstream task\nor dataset. This can require auxiliary training ob-\njectives, the deï¬nition of task-speciï¬c fairness met-\narXiv:2010.12864v2  [cs.CL]  11 Apr 2021\nrics, the annotation of bias attributes (e.g., identi-\nfying African American Vernacular English), and\nthe collection of usersâ€™ demographic data. These\ndrawbacks make bias mitigation inaccessible to\nthe growing community, ï¬ne-tuning LMs to new\ndatasets and tasks.\nIn contrast, we investigate initially mitigating\nbias while ï¬ne-tuning an â€œupstreamâ€ model in\none or more upstream datasets, and subsequently\nachieving reduced bias when ï¬ne-tuning for down-\nstream applications (Fig. 1 (d)), so that bias miti-\ngation is no longer required in downstream train-\ning. Similar to transfer learning for enhancing pre-\ndictive performance in common setups (Pan and\nYang, 2010; Dai and Le, 2015), we suggest that\nLMs that undergo bias mitigation acquire induc-\ntive bias that is helpful for reducing harmful biases\nwhen ï¬ne-tuned on new domains and tasks. In four\ntasks with known bias factors â€” hate speech de-\ntection, toxicity detection, occupation prediction\nfrom short bios, and coreference resolution â€” we\nexplore whether upstream bias mitigation of a LM\nfollowed by downstream ï¬ne-tuning reduces bias\nfor the downstream model. Though previous work\nhas addressed biases in frozen PTLM or word em-\nbeddings (Bolukbasi et al., 2016; Zhou et al., 2019;\nBhardwaj et al., 2020; Liang et al., 2020; Ravfogel\net al., 2020), for example by measuring associa-\ntions between gender and occupations in an em-\nbedding space, they do not study their effect on\ndownstream classiï¬ers (Fig. 1 (b)), while some of\nthem study the effects while keeping the embed-\ndings frozen (Zhao et al., 2019; Kurita et al., 2019;\nProst et al., 2019). Bias in these frozen represen-\ntations can also be directly corrected by remov-\ning associations between feature and sensitive at-\ntributes (Elazar and Goldberg, 2018; Madras et al.,\n2018) (Fig. 1 (c)), but this does not allow predic-\ntions to be generated for new data.\nOur experiments address the following research\nquestions: (a) whether mitigating a single bias fac-\ntor in the upstream stage is maintained when ï¬ne-\ntuning on new examples from the same domain\nand task, (b) whether transfer is viable when the\ndownstream domains and tasks are different from\nthe upstream model, and (c) whether we can ad-\ndress multiple kinds of bias with a single upstream\nmodel. We perform these experiments under a\ngeneric transfer learning framework, noted as Up-\nstream Bias Mitigation (UBM) for Downstream\nFine-Tuning for convenience, which consists of\ntwo stages: ï¬rst, in the upstream bias mitigation\nstage, a LM is ï¬ne-tuned with bias mitigation objec-\ntives on one or several â€œupstreamâ€ tasks, and sub-\nsequently the classiï¬cation layer is re-initialized;\nthen, in the downstream ï¬ne-tuning stage the en-\ncoder from the upstream model, jointly with the\nnew classiï¬cation layer, are again ï¬ne-tuned on\na downstream task without additional bias miti-\ngation steps. Using six datasets with previously\nrecognized bias factors, our analysis show overall\npositive results for the questions above; still, there\nare challenges remaining to stabilize the results\nof bias mitigation in challenging setups, e.g., the\nmulti-bias factor setting.\nOur contributions are summarized as follows: (1)\nwe propose a new research direction for mitigating\nbias in ï¬ne-tuned models; (2) we perform extensive\nexperiments to study the viability of the upstream\nbias mitigation framework in various settings; (3)\nwe demonstrate the effectiveness of this research\ndirection, motivating further improvements, tests,\nand applications.\n2 Exploring the Transferability of Bias\nMitigation Effects\nWe consider biases against protected groups in clas-\nsiï¬ers ï¬ned-tuned from LMs. In our present analy-\nsis, bias is deï¬ned as disparate model performance\non different subsets of data which are associated\nwith different demographic groups (e.g., instances\nthat mention or are generated by different social\ngroups) (Blodgett et al., 2020). Our evaluation of\nbias aligns with the deï¬nition of equalized odds\nand equal opportunities (Hardt et al., 2016) in pre-\nvious works of fairness in machine learning.\nHere, we ï¬rst outline our experimental setup\nfor exploring the transferability of bias mitigation\neffects, in which we detail the process of apply-\ning UBM and pose three key research questions\n(section 2.1). We follow by introducing the bias\nfactors studied and the corresponding classiï¬cation\ntasks and datasets (section 2.2), and our evaluation\nprotocols and metrics (section 2.3).\n2.1 Experiment Setups of UBM\nOur goal is to evaluate the transferability of bias\nmitigation effects for one or multiple bias factors in\ndownstream ï¬ne-tuned models. We follow an Up-\nstream Bias Mitigation (UBM) for Downstream\nFine-Tuning procedure, pictured in Figure 2. First,\nin the Upstream Bias Mitigationphase, an upstream\nProvide Encoder   gs\nSettings\nOne bias factor\nFramework\nNo data access\nMultiple bias factors\n(train & debias)(train w/o debias)Upstream Model\nDownstream Model\nUpstream dataset ğ’Ÿ!w/ attribute labelsDownstream datasetsğ’Ÿ\"\nğ‘ƒğ’Ÿ!=ğ‘ƒ(ğ·\")\nğ‘ƒğ’Ÿ!â‰ ğ‘ƒ(ğ·\")\n ğ‘“!=â„!âˆ˜ğ‘”!\n(1) Upstream bias mitigation stage(2) Downstream fine-tuning stage\nğ’Ÿ! ğ’Ÿ\"\nğ‘“\"=â„\"âˆ˜ğ‘”\"\nFigure 2: Experiment setups to study Upstream Bias Mitigation (UBM) for Downstream Fine-Tuning . We\nconsider the settings with the same or different upstream and downstream domains and tasks, while addressing one\nor more bias factors (e.g., both dialect bias and gender bias). The framework consists of two stages: (1) an upstream\n(source) model fs = hs â—¦gs is trained with bias mitigation algorithms and (2) the encoder gs is transferred to the\ndownstream (target) model ft for ï¬ne-tuning.\n(source) model fs = hs â—¦gs, composed of a text\nencoder gs and a classiï¬er head hs, is trained on\none or more upstream datasets Ds with bias mitiga-\ntion algorithms. The encoder gs is to be transferred\nto downstream (target) domains and tasks while\nthe classiï¬er head hs is discarded. Then, in the\nDownstream Fine-Tuning phase, the downstream\nmodel ft = ht â—¦gt utilizes gs to initialize the en-\ncoder weights and is ï¬ne-tuned for prediction per-\nformance without bias mitigation approaches on\ndownstream datasets Dt.\nThis UBM process is applied in three settings,\nsummarized below, which each contribute to evalu-\nating the transferability of bias mitigation effects.\n1. Fine-Tuning on the Same Distribution. In\nthe simplest setting, we ï¬ne-tune the downstream\nmodel over new examples from the same data dis-\ntribution as the upstream model. In practice, each\ndataset is split into two halves, with one used for\nupstream bias mitigation and the other for down-\nstream ï¬ne-tuning.\n2. Cross-Domain and Cross-Task Fine-Tuning.\nSimilar to how LMs are ï¬ne-tuned for various tasks\nand domains, in a more practical setup, we test\nwhether transfer of bias mitigation effects is viable\nacross domains and tasks. To achieve this, we\napply bias mitigation while ï¬ne-tuning a LM on\none dataset and perform ï¬ne-tuning on another.\n3. Multiple Bias Factors. In the most challenging\nsetup, we train a single upstream model to address\nmultiple bias factors ( e.g., both dialect bias and\ngender bias). Such upstream models can be trained\nwith multi-task learning (i.e., jointly training over\nmultiple datasets with shared encoder gbut differ-\nent classiï¬er heads h) while mitigating multiple\nkinds of bias. Subsequently, the resulting upstream\nmodel is transferred to downstream models as be-\nDataset Prediction Task Bias\nGHC (Kennedy et al., 2018) Hate Group Identiï¬erStormfront (de Gibert et al., 2018) Hate Group Identiï¬erDWMW (Davidson et al., 2017) Toxicity AA VE DialectFDCL (Founta et al., 2018) Toxicity AA VE DialectBiasBios (De-Arteaga et al., 2019) Occupation Gender StereotypingOntoNotes 5.0 (Weischedel et al., 2013) Coreference Gender Stereotyping\nTable 1: Summary of tasks and bias included for study.\nfore. This is a key test of UBMâ€™s viability for\nwidespread application.\n2.2 Bias Factors and Datasets\nTo ensure our analysis holds true for a variety of\ndomains, tasks, and bias factors, we experiment\nwith three different bias factors studied in previ-\nous research along with six different datasets (also\nsummarized in Table 1), described below.\nGroup Identiï¬er Bias. This bias refers to higher\nfalse positive rates of hate speech predictions\nfor sentences containing speciï¬c group identiï¬ers,\nwhich is harmful to protected groups by misclassi-\nfying innocuous text (e.g., â€œI am a Muslimâ€) as hate\nspeech. We include two datasets for study, namely\nthe Gab Hate Corpus (GHC; Kennedy et al., 2018)\nand the Stormfront corpus (de Gibert et al., 2018).\nBoth datasets contain binary labels for hate and\nnon-hate instances, though with differences in the\nlabeling schemas and domains.\nAA VE Dialect Bias.Sap et al. (2019) show that\noffensive and hate speech classiï¬ers yield a higher\nfalse positive rate on text written in African Ameri-\ncan Vernacular English (AA VE). This bias brings\nsigniï¬cant harm to the communities that uses\nAA VE, for example, by leading to the dispropor-\ntionate removal of the text written in AA VE in\nsocial media platforms (Blodgett et al., 2020). We\ninclude two datasets for study: FDCL (Founta et al.,\n2018) and DWMW (Davidson et al., 2017). In both\ndatasets, we treat abusive, hateful and spam to-\ngether as harmful outcomes (i.e., false positives for\neach are harmful) to compute false positive rates.\nFollowing Sap et al. (2019), we use an off-the-shelf\nAA VE dialect predictor (Blodgett et al., 2016) to\nidentify examples written in AA VE.\nGender Stereotypical Bias. Zhao et al. (2018)\nsummarize a list of occupations that are prone to\nbe stereotyped in practice, leading to coreference\nresolutions models and occupation prediction mod-\nels having biases in performance in pro- and anti-\nstereotypical instances when trained on short bios.\nWe train the coreference resolution model on the\nOntoNotes 5.0 dataset (Weischedel et al., 2013)\nand the occupation classiï¬er on the BiasBios (De-\nArteaga et al., 2019) dataset.\n2.3 Evaluation Protocol and Metrics\nWe evaluate the overall performance of the mod-\nels on downstream tasks along with appropriate\nbias metrics for each bias factor, analyzed for\neach dataset and task in previous works. We ex-\npect UBM to minimally affect classiï¬cation perfor-\nmance while improving on bias metrics.\nClassiï¬cation Performance. We report in-domain\nF1 scores for GHC, Stormfront, OntoNotes 5.0, and\naccuracy scores for FDCL, DWMW and BiasBios.\nFollowing Zhang et al. (2018), for hate speech de-\ntection and toxicity detection datasets, we use the\nequal error rate (EER) threshold for prediction.\nGroup Identiï¬er Bias Metrics. To evaluate group\nidentiï¬er bias, we evaluate false positive rate (FPR)\ndifferences, noted as FPRD, between examples\nmentioning one of 25 group identiï¬ers provided\nby Kennedy et al. (2020) and the overall FPR. In\naddition, we followed Kennedy et al. (2020) in us-\ning a New York Times articles (NYT) corpus of\n25k non-hate sentences, each mentioning one of\n25 group identiï¬ers. This corpus speciï¬cally pro-\nvides an opportunity to measure FPRâ€”reported as\n(NYT Acc.), equivalent to 1âˆ’FPR. Additionally,\nfollowing the evaluation protocol of Dixon et al.\n(2018) and Zhang et al. (2020), we incorporate the\nIdentity Phrase Templates Test Sets (reported as\nIPTTS), which consists of 77khate and non-hate\nexamples mentioning group identiï¬ers, generated\nwith templates. Following these works, for IPTTS\nwe compute FPRD as âˆ‘\nz|FPRz âˆ’FPRoverall|,\nwhere FPRz is false positive rate on sentences with\nthe group identiï¬er z, and FPRoverall is the overall\nfalse positive rate.\nAA VE Dialect Bias Metrics. Given the sparsity\nof AA VE examples in the datasets and the noisy\noutputs of AA VE classiï¬er (Blodgett et al., 2016),\nwe expect the in-domain FPRD metrics to be noisy.\nTherefore, following Xia et al. (2020), we incor-\nporate the BROD (Blodgett et al., 2016) dataset,\nwhich is a large unlabeled collection of Twitter\nposts written in l. Since in practice only a small\nportion of texts are toxic or spam, we treat all exam-\nples from BROD asnormal, and report the accuracy\n(which equals 1âˆ’FPR) on the dataset.\nGender Stereotype Metrics. We employ the\nWinoBias (Zhao et al., 2018) dataset which pro-\nvides opportunities to evaluate models on pro-\nstereotypical and anti-stereotypical coreference ex-\namples. We report the differences in F1 (F1-Diff)\non two subsets of data. On occupation prediction,\nfollowing Ravfogel et al. (2020), we report mean\ndifferences of true positive rate (TPR) differences\nin predicting each occupation for men and women.\n3 Method\nHere, we detail the particular bias mitigation algo-\nrithms used for implementing UBM, as well as the\nother baselines used for verifying the transferability\nof bias mitigation effects.\n3.1 Implementations of UBM\nWe implement UBM with two different bias miti-\ngation algorithms in the upstream bias mitigation\nphase: explanation regularization (Kennedy et al.,\n2020), and adversarial de-biasing (Zhang et al.,\n2018; Madras et al., 2018; Xia et al., 2020), de-\nnoted here as UBMreg and UBMadv, respectively.\nUBM with Explanation Regularization. Expla-\nnation regularization reduces importance placed on\nspurious surface patterns ( i.e., words or phrases)\nduring upstream model training. We apply UBMreg\nto group identiï¬er and AA VE dialect bias, where\nthe set of spurious patterns are group identiï¬ers\nand the most frequent words, from statistics of the\ndataset, used by AA VE speakers; we ï¬nd expla-\nnation regularization not effective for gender bias.\nThe importance of a surface pattern wâˆˆW in the\ninput x, noted as Ï†(w,x) is measured as the model\nprediction change when it is removed. The model\nis trained by optimizing the main learning objective\nâ„“while penalizing importance attributed to patterns\nwâˆˆW that exist in the input x.\nmin\nf\nâ„“c + Î±\nâˆ‘\nwâˆˆxâˆ©W\n||Ï†(w,x)||2, (1)\nwhere Î±is a trade-off hyperparameter.\nUBM with Adversarial De-biasing. In UBMadv,\nthe upstream model is trained with adversarial de-\nbiasing techniques, so that sensitive attributes re-\nlated to bias ( e.g., the dialect of the sentence or\nthe gender referenced in the sentence) cannot be\npredicted from the hidden representations z given\nby the encoder g. During training, an adversar-\nial classiï¬er head hadv is built upon the encoder\nand trained to predict sensitive attributes, while\nthe encoder is optimized to prevent the adversarial\nclassiï¬er from success. Formally, the optimization\nobjective is written as,\nmin\ng,h\nmax\nhadv\nâ„“c + â„“adv(hadv â—¦g(x),a), (2)\nwhere anotes the ground truth sensitive attribute,\nand â„“adv is the cross entropy loss between the pre-\ndicted sensitive attribute and the ground truth sen-\nsitive attribute.\nAs mentioned in Sec. 2.1, upstream models\ncan be trained to mitigate multiple bias factors\nwith multi-task learning on multiple datasets. We\nseparately apply bias mitigation algorithms for\neach dataset (sharing the same encoder) and\nnote the algorithms applied in the subscript ( e.g.,\nUBMreg+adv).\n3.2 Other Baselines\nWe compare UBM with two families of methods.\nMethods without Bias Mitigation. Two types of\nmodels were evaluated that did not address bias.\nFirst, the Vanilla model is a downstream classiï¬er\ndirectly ï¬ne-tuned on downstream task from a LM\n(e.g., RoBERTa). Second, Van-Transfer is ï¬ne-\ntuned on upstream datasets without bias mitigation\nand ï¬ne-tuned on downstream datasets.\nDownstream Bias Mitigation. For reference, we\nshow the results of directly applying explanation\nregularization, noted as Expl. Reg., or adversarial\nde-biasing, noted as Adv. Learning, during down-\nstream ï¬ne-tuning. In most cases, mitigating bias\nin downstream classiï¬er should be the most effec-\ntive way to reduce bias, though this is not always\nfeasible in practice for reasons discussed above.\nWe also consider two simple baselines that could\nreduce bias in downstream models via heuristics.\nEmb. Zero zeros out the word embedding of spu-\nrious surface patterns (using the same word list as\nexplanation regularization) in PTLMs before ï¬ne-\ntuning. We also include Emb. Zero. Trans, which\nGHC B\nMetrics In-domainF1 (â†‘) In-domainFPRD (â†“) IPTTSFPRD (â†“) NYTAcc (â†‘)\nNon-Transfer (GHC . B)\nVanilla 37.91Â±2.5 35.64Â±2.2 21.50Â±2.8 68.55Â±20Expl. Reg.38.09Â±2.7 18.68Â±0.3 4.82Â±1.1 84.05Â±3.0\nGHC . Aâ†’GHC . B\nVan. Transfer42.41Â±1.0 37.44Â±1.5 17.67Â±2.1 75.35Â±4.2UBMReg 43.79Â±1.9 34.34Â±3.1 10.02Â±1.1 81.40Â±1.4\nTable 2: Same-domain and task UBM with a single\nbias factor. The source datasets are noted before arrow\n(â†’). All metrics except In-domain F1 measure bias.\nSee Table 6 in Appendix for complete results.\nzeros out embeddings of spurious surface patterns\nbefore ï¬ne-tuning from an upstream model. The\nmethod does not apply to cases where surface pat-\nterns related to bias (e.g., gendered pronouns) are\ncrucial for prediction, e.g., coreference resolution.\n4 Results\nIn this section, we present the results of UBM in\nthree settings following the order in Sec. 2.1: trans-\nferring to the same data distribution, transferring\nto different data distributions, transferring from an\nupstream model with bias mitigation for multiple\nbias factors. We follow these main analyses with\nan investigation of the impact of freezing encoder\nweights before downstream ï¬ne-tuning, and lastly\nwith a brief exploration of how UBMâ€™s positive\nresults are achieved.\nImplementation Details. In all experiments re-\nported on below, models are initially ï¬ne-tuned\nfrom RoBERTa-base. The upstream model is\ntrained for a ï¬xed number of epochs and the check-\npoint with the best prediction performance is trans-\nferred to the downstream model. See Appendix for\nmore implementation details. We use Ds â†’Dt as\nthe transfer notation, in which upstream and down-\nstream datasets are respectively represented in the\nleft and right-hand side of the arrow.\n4.1 UBM with the Same Data Distribution\nWe ï¬rst brieï¬‚y show the results when the down-\nstream model sees new, unseen samples from the\nsame data distribution as the upstream model. In\nthis controlled setting, we isolate and test the basic\nviability of UBM, which requires that information\nfrom the upstream model is retained during down-\nstream ï¬ne-tuning. GHC, Stormfront, FDCL and\nBiasBios were partitioned into two subsets with\nequal size, noted as subsets A and B of correspond-\ning datasets, to train the upstream and downstream\nmodels respectively.\nDownstream dataset GHC Stormfront DWMW OntoNotes 5.0\nMetrics In-domainF1 (â†‘) In-domainFPRD (â†“) IPTTSFPRD (â†“) NYTAcc (â†‘) In-domainF1 (â†‘) In-domainFPRD (â†“) IPTTSFPRD (â†“) NYTAcc (â†‘) In-domainAcc. (â†‘) BRODAcc. (â†‘) In-domainF1 (â†‘) WinobiasF1-Diff (â†“)\nNon-Transfer (GHC) Non-Transfer (Stormfront)Non-Transfer (DWMW)Non-Transfer (OntoNotes 5.0)\nVanilla 49.60Â±1.046.43Â±2.5 20.01Â±5.7 72.08Â±7.353.74Â±2.818.09Â±2.7 11.51Â±5.1 73.06Â±10 91.46Â±0.1 78.77Â±0.376.53Â±0.2 8.04Â±0.5Emb. Zero 43.76Â±0.7 38.31Â±2.0 11.95Â±2.783.21Â±5.249.97Â±0.6 18.80Â±2.0 8.20Â±0.3 70.15Â±4.490.59Â±0.1 62.37Â±0.4Expl. Reg. 43.37Â±1.829.29Â±1.2 4.2Â±1.6 81.22Â±11 51.53Â±1.113.43Â±1.5 3.80Â±0.4 83.73Â±8.091.38Â±0.1 76.61Â±1.5Adv. Learning 91.11Â±0.3 77.53Â±0.9\nStf.â†’GHC GHCâ†’Stf. FDCLâ†’DWMWBiasBiosâ†’OntoNotes 5.0\nVan-Transfer47.83Â±2.1 47.51Â±4.6 14.00Â±0.8 66.71Â±10.655.79Â±1.3 17.83Â±2.2 8.26Â±2.5 76.98Â±1.191.27Â±0.2 78.98Â±1.176.65Â±0.3 10.54Â±0.7Emb. Zero. Trans.44.51Â±0.540.92Â±4.212.91Â±0.280.11Â±1.252.98Â±0.616.35Â±0.98.04Â±2.1 81.11Â±1.991.53Â±0.081.01Â±0.9UBMReg 49.94Â±1.042.71Â±3.812.23Â±3.375.34Â±4.856.43Â±0.618.03Â±2.56.86Â±1.1 81.18Â±1.191.39Â±0.0 80.27Â±0.2UBMAdv 91.20Â±0.081.24Â±0.276.34Â±0.2 9.27Â±1.4\nTable 3: Cross-domain and task UBM with a single bias factor . The source datasets are noted before arrow\n(â†’). All metrics except In-domain F1 or In-domain Accuracy measures bias. The preferred outcomes for each\nmetric are marked with arrows. The main comparators of UBM are Vanilla, Van-Transfer, Emb. Zero , and\nEmb. Zero. Trans that do not perform downstream bias mitigation. We see UBM maintains in-domain prediction\nperformance while overall reduces bias. Results of Adv. Learning and UBM Adv on GHC, Stf. are not included\nbecause applying adversarial de-biasing to reduce group identiï¬er bias yields degenerated classiï¬ers.\nTable 2 presents the results for mitigating group\nidentiï¬er bias in the GHC. We see an overall bias\nreduction, via UBM, by comparing with Vanilla\ntraining and Van-Transfer. We include full results\nand discussions for this simple setting in Appendix.\n4.2 Cross-domain and Task UBM\nFollowing the result that UBM is effective in the\nsame-domain setting, we now move to analyzing\ncross-domain settings in greater depth. For hate\nspeech classiï¬cation, we perform transfer learning\nfrom GHC to Stormfront and from Stormfront to\nGHC; and for toxicity classiï¬cation, we perform\ntransfer learning from FDCL to DWMW. We also\nperform transfer learning from BiasBios (occupa-\ntion prediction) to OntoNotes 5.0 (coreference res-\nolution). Table 3 shows the results of cross-domain\nand task transfer learning and non-transfer base-\nlines. Our ï¬ndings are summarized below.\nUBM can reduce bias in different target\ndomains and tasks compared to ï¬ne-tuning\nwithout bias mitigation. The results of cross-\ndomain and task transfer learning (i.e., Stf.â†’GHC,\nGHCâ†’Stf., FDCLâ†’DWMW), show that transfer-\nring from a less biased upstream model (UBMReg\nand UBMAdv) leads to better downstream bias mit-\nigation compared to directly training without bias\nmitigation in the target domain (Vanilla). Mean-\nwhile, the in-domain classiï¬cation performance\nhas improved (on GHC and Stormfront) or been\npreserved (on DWMW). It is notable that directly\nmitigating bias (Expl. Reg., Adv. Learning) on\nDWMW is not effective, which is previously ob-\nserved by Xia et al. (2020), while transferring from\nFDCL is successful.\nThere are exceptions where UBM fails to reduce\nbias. We see the in-domain FPRD on Stormfront\ndoes not improve; however, as discussed in our\nmetrics section, the in-domain FPRD is computed\nover a much smaller set of examples compared to\nNYT and IPTTS datasets, and is thus less reliable.\nUBM does not reduce bias compared to Vanilla\ntraining on OntoNotes 5.0, but achieves less bias\ncompared to Van-Transfer. This result conï¬rms the\neffect of bias mitigation in upstream models, but\nthe transfer learning itself has increased the bias.\nComparison with Emb. Zero and Emb. Zero.\nTrans. We ï¬nd two alternative methods, Emb.\nZero and Emb. Zero Trans, also reduce bias on\nsome of the datasets. On GHC, Emb. Zero achieves\nan in-domain FPRD and IPTTS-FPRD lower than\nUBM. However, it comes with clear drop of in-\ndomain classiï¬cation performance.\n4.3 Mitigating Multiple Bias Factors\nHaving observed an overall positive effect\nof UBM across domains and tasks, next we present\nthe results of experiments on mitigating multiple\nbias factors with a single upstream model. This\ninvolves training an upstream model with multiple\nbias mitigation objectives across multiple datasets,\nfollowed by ï¬ne-tuning on a single dataset with-\nout bias mitigation. We test three combinations\nof datasets. First, a multi-task model is trained to\njointly mitigate group identiï¬er bias and AA VE\ndialect bias using GHC and FDCL (GHC + FDCL),\nand transferred to Stormfront and DWMW. Next, a\nmodel is similarly trained jointly on group identi-\nï¬er and AA VE biases on and Stormfront and FDCL\n(Stf. + FDCL) and transferred to GHC and DWMW.\nLastly, models were trained over source datasets\n3We ï¬nd UBMReg,Reg,Adv yield degenerated classiï¬ers\nfor OntoNotes (Test F1< 46.00) in 5 out of 6 runs. The result\nis from one successful run.\nBias Factor Group Identiï¬er Bias AA VE Dialect BiasGender Stereotypical Bias\nMetrics In-domain F1 (â†‘) In-domain FPRD (â†“) IPTTS FPRD (â†“) NYT Acc (â†‘) In-domain F1 (â†‘) BROD Acc. (â†‘) In-domain F1 (â†‘) Winobias F1-Diff (â†“)\nUpstream model Stormfront + FDCL\nDownstream model GHC DWMW\nVan-Transfer49.71Â±0.3â†—â†— 45.84Â±3.8â†˜â†˜ 12.43Â±2.5â†˜â†˜72.37Â±7.4â†—â†—91.64Â±0.2â†—â†— 81.12Â±0.1â†—â†—UBMReg,Reg 50.21Â±1.4â†—â†— 47.63Â±0.7â†’â†’ 12.29Â±2.7â†˜â†˜68.44Â±8.6â†’â†’91.66Â±0.2â†—â†— 80.05Â±0.1â†—â†—UBMReg,Adv 49.89Â±1.7â†—â†— 47.85Â±1.2â†’â†’ 21.25Â±2.0â†’â†’65.78Â±5.7â†’â†’91.55Â±0.2â†—â†— 81.14Â±1.5â†—â†—\nUpstream Model GHC + FDCL\nDownstream Model Stormfront DWMW\nVan-Transfer56.78Â±1.6â†—â†— 14.26Â±0.8â†˜â†˜ 11.04Â±0.7â†˜â†˜77.06Â±5.1â†—â†—91.65Â±0.1â†—â†— 80.98Â±0.4â†—â†—UBMReg,Reg 53.87Â±1.2â†—â†— 15.92Â±1.2â†˜â†˜ 8.40Â±1.4â†˜â†˜ 83.71Â±3.2â†—â†—91.79Â±0.4â†—â†— 81.36Â±0.8â†—â†—UBMReg,Adv 53.63Â±0.7â†’â†’ 15.52Â±2.2â†˜â†˜ 8.90Â±1.6â†˜â†˜ 84.87Â±1.1â†—â†—91.33Â±0.1â†’â†’ 81.09Â±0.4â†—â†—\nUpstream Model GHC + FDCL + BiasBios\nDownstream Model Stormfront DWMW OntoNotes 5.0\nVan-Transfer55.47Â±0.7â†—â†— 16.74Â±1.5â†˜â†˜ 12.19Â±0.7â†˜â†˜64.15Â±6.5â†’â†’91.58Â±0.1â†—â†— 80.74Â±0.3â†—â†— 73.64Â±0.3â†’â†’ 9.91Â±0.2â†’â†’UBMReg,Reg,Adv52.59Â±0.5â†’â†’ 21.17Â±2.0â†’â†’ 9.99Â±2.8â†˜â†˜ 74.58Â±4.9â†—â†—91.64Â±0.3â†—â†— 81.07Â±0.4â†—â†— 75.68â†’â†’ 4.933â†˜â†˜UBMReg,Adv,Adv52.85Â±0.9â†’â†’ 18.55Â±5.8â†’â†’ 13.15Â±3.7â†˜â†˜70.00Â±5.8â†’â†’91.50Â±0.1â†—â†— 81.08Â±0.3â†—â†— 76.01Â±0.4â†’â†’ 8.67Â±0.7â†’â†’\nTable 4: Dealing with multiple bias factors with a single upstream model with UBM.We test three combination\nof upstream datasets, namely Stormfront + FDCL, GHC + FDCL, and GHC + FDCL + BiasBios, in reducing two\nor three bias factors. â†—â†—and â†˜â†˜show whether the metrics has increased or decreased (both imply improvement)\ncompared to non-transfer Vanilla training in Table 3.\nMetricsIn-domainAcc. (â†‘) In-domainFPRD (â†“) IPTTSFPRD (â†“) NYTAcc. (â†‘) In-domainAcc. (â†‘) BRODAcc. (â†‘)\nStf.â†’GHC, UBMReg FDCLâ†’DWMW, UBMAdv\nFreeze 45.42 37.71 7.82 84.45 83.25 64.80â„“2-sp 49.31 47.03 14.24 71.88 91.38 79.95Fine-tune49.94 42.71 12.23 75.34 91.20 81.24\nGHCâ†’Stf.UBMReg\nFreeze 47.32 25.02 8.24 64.60â„“2-sp 55.80 19.75 6.72 80.42Fine-tune56.43 18.03 6.86 81.18\nTable 5: UBM while keeping the encoder frozen\n(Freeze), discouraging parameter changes ( â„“2-sp), or\nstandard ï¬ne-tuning (Fine-tune). We see weight freez-\ning and â„“2-sp overall do not improve over simple ï¬ne-\ntuning on Stf. â†’GHC and FDCL â†’DWMW.\nGHC, FDCL, BiasBios (GHC+FDCL+BiasBios)\nto mitigate all three bias factors, and transferred to\nStormfront, DWMW, and OntoNotes. The results\nare shown in Table 4.\nComparison to Single-Dataset Vanilla Base-\nlines. As a basic measure of bias mitigation\nsuccess, we compare multi-dataset modelsâ€™ re-\nsults with single-dataset Vanilla training and Van-\nTransfer. We see UBM with GHC + FDCL success-\nfully reduces both group identiï¬er bias and AA VE\ndialect bias in downstream models. UBM with\nGHC + FDCL + BiasBios also successfully reduces\ngroup identiï¬er bias in terms of IPTTS, FPRD\n(which is the most reliable metrics of bias given its\nlarge size), and AA VE bias. It also reduces gen-\nder stereotypical bias compared to Van-Transfer in\nsome experimental runs, but in an unstable manner,\ndemonstrated by the large variance of F1-Diff and\ndegenerated runs of UBMReg,Reg,Adv.\nResults of UBM on Stf. + FDCL are less promis-\ning. We ï¬nd UBMReg,Adv,Adv is not successful in\nreducing group identiï¬er bias. UBM Reg,Reg,Adv\ncould reduce bias on IPTTS-FPRD, but does not\nimprove other metrics. Notably, UBM on Stf. +\nFDCL clearly underperform UBM on Stf. only.\nUBMReg versus UBM Adv. Empirically, we\nï¬nd using explanation regularization on FDCL\n(UBMreg,reg, UBMreg,reg,adv) instead of adversar-\nial learning (UBMreg,adv, UBMreg,adv,adv) consis-\ntently improves bias mitigation performance on\nother bias factors.\nTakeaways. Our results show it is possible to\nreduce multiple bias factors via UBM. However,\nwe have shown that these effects are not automatic\nfor each new dataset added to upstream models for\nmulti-task bias mitigation.\n4.4 Freezing or Regularizing Model Weights\nIn the experiments above, we have shown that\nthe effect of mitigating bias is partially preserved\nwith simple ï¬ne-tuning. Next, we study whether\nfreezing the encoders or discouraging their weight\nchanges improves bias mitigation in the target\ndomain, as they intuitively try to retain effect\nof bias mitigation. However, we ï¬nd a counter-\nintuitive result: these approaches typically do not\nachieve reduced downstream bias, and in fact re-\nduce in-domain classiï¬cation performance. Table 5\nshows the results when we keep the weights frozen\n(Freeze), discouraging weights from changing with\nâ„“2-sp regularizer (Li et al., 2018, details in ap-\npendix), or standard ï¬ne-tuning (ï¬ne-tune). In Stf.\nâ†’GHC, freezing the weights contributed to reduc-\ning the bias, while â„“2-sp failed to help. In GHC\nâ†’Stf and FDCL â†’DWMW, freezing the weights\nand â„“2-sp both increased the bias. A possible rea-\nson is that by freezing the encoder, we reduce its\nexpressive power. As a result, the encoder is prone\n0 1 2 3 4\nEpoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25Gradient of (w, x)\n Van. Trans\nUBMreg\n0.00\n0.01\n0.02\n0.03\n0.04\n(w, x)\n(a) Gab A â†’Gab B\n0 1 2 3 4\nEpoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25Gradient of (w, x)\n Van. Trans\nUBMreg\n0.00\n0.01\n0.02\n0.03\n0.04\n(w, x)\n (b) Stf. A â†’Stf. B\n0 1 2 3 4\nEpoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25Gradient of (w, x)\n Van. Trans\nUBMreg\n0.00\n0.01\n0.02\n0.03\n0.04\n(w, x)\n(c) Gab â†’Stf.\n0 1 2 3 4\nEpoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25Gradient of (w, x)\n Van. Trans\nUBMreg\n0.00\n0.01\n0.02\n0.03\n0.04\n(w, x)\n (d) Stf. â†’Gab\nFigure 3: Gradient of importance attribution on group\nidentiï¬ers Ï†(w,x) over time (in solid lines) and the\ncorresponding values of Ï†(w,x) (in dash lines) dur-\ning downstream ï¬ne-tuning. The cross-marks show the\ngradient measured in the upstream model (before re-\ninitialization of the classiï¬er layer). UBM reg not only\nreduces importance attributed to group identiï¬ers, but\nalso the gradient norm of the importance.\nto capture simple but spurious correlations.\n4.5 Investigating Why UBM Reduces Bias\nWe attempt to interpret why ï¬ne-tuning from a de-\nbiased upstream model remains less biased during\nï¬ne-tuning from the perspective of gradient of im-\nportance attributed to words wrelated to bias fac-\ntors (e.g., group identiï¬ers) by the input occlusion\nalgorithm. A large importance attribution usually\ninduces bias. Figure 3 plots the importance attri-\nbution of group identiï¬ers Ï†(w,x) and the norm\nof its gradient w.r.t. parameters Î¸of the encoder g,\nnoted as ||âˆ‡Î¸Ï†(w,x)||2.\nUBM reduces the gradient of Ï†(w,x), so\nthat Ï†(w,x) is less likely to change at the be-\nginning of downstream ï¬ne-tuning . Fig. 3\nshows UBM has not only reduced value of im-\nportance attributed to spurious patterns, but also re-\nduced their gradients. The gradient norm is highly\nindicative about how the importance Ï†(w,x) will\nchange in the downstream model, because when the\nloss in Eq. 1 in the upstream model is minimized,\nthe gradient âˆ‡Î¸Ï†(w,x) has the same norm but the\nopposite direction as the main downstream classiï¬-\ncation objective âˆ‡Î¸â„“c. It implies that whether the\nupstream model converges at an optimum where\nboth objectives agree (i.e., gradients are small) can\nbe an important indicator of the success of UBM.\nThe ï¬gure further shows that the gradient and\nthe value ofÏ†(w,x) remain small for UBMreg over\nthe whole training process. We leave more study\ninto the training dynamics of UBM as future works.\n5 Related Works\nHere we review approaches that inform the present\nwork (techniques for bias mitigation) and are re-\nlated to the basic idea of UBM.\nMitigating bias in representations. Bias can be\nmitigated directly in representations of data. Zhang\net al. (2018); Beutel et al. (2017) proposed training\na classiï¬er together with an adversarial predictor\nfor sensitive attributes. Madras et al. (2018) further\nstudied re-usable de-biased representations by train-\ning a new downstream classiï¬er (potentially with a\ndifferent classiï¬cation task) using the learned repre-\nsentations. However, this practice relies on frozen\nrepresentations (rather than models themselves),\nwhich precludes the possibility of generating pre-\ndictions for new data.\nMitigating bias in pretrained models. Another\nline of work addresses bias in pretrained mod-\nels (e.g., word vectors, BERT, Zhou et al., 2019;\nMay et al., 2019; Bhardwaj et al., 2020; Liang\net al., 2020). Many such studies again focus on\nbias in frozen data representations, and do not study\ntheir effects on downstream classiï¬ers. Others al-\nternatively assess the propagation of bias from pre-\ntrained models to downstream classiï¬ers: Ravfogel\net al. (2020) study algorithms for mitigating bias\nin pretrained models by de-biasing the learned rep-\nresentations, which can subsequently be used in\nclassiï¬ers as frozen representations.\nTransferring learning of fairness and robust-\nness. A few previous works have studied related\nresearch problems, with signiï¬cant differences to\nour work. Though Schumann et al. (2019) theoreti-\ncally analyzes the transferability of fairness across\ndomains, it assumes simultaneous access of source\nand target domain data, which does not account\nfor transferring upstream bias mitigation to arbi-\ntrary downstream ï¬ne-tuned models. Shafahi et al.\n(2020) study transfer learning of robustness to ad-\nversarial attacks under ï¬ne-tuning, but do not seek\nto mitigate bias.\n6 Conclusion\nWe observe that the effects of bias mitigation are in-\ndeed transferable in ï¬ne-tuning LMs. Future works\nin ï¬ne-tuning LMs can use UBM in order to easily\napply the positive effects of bias mitigation meth-\nods to new domains and tasks without customized\nbias mitigation processes or access to sensitive user\ninformation. Though UBM does not rival directly\nmitigating bias on the downstream task, it is more\nefï¬cient and accessible. Future works can develop\nthe effectiveness of UBM beyond the default sce-\nnarios in this paper, and potentially apply it to tasks\nand settings beyond hate speech, toxicity classi-\nï¬cation, occupation prediction, and coreference\nresolution in English corpora.\nBroader Impact Statement\nOur analysis demonstrates the effectiveness of Up-\nstream Bias Mitigation for Downstream Fine-\nTuning. As we stated in the paper, the reduced\nefforts of downstream bias mitigation will facili-\ntate broader application of bias mitigation in the\ngrowing deep learning community.\nWhile we may expect to obtain an â€œoff-the-shelfâ€\nlanguage model that could reduce multiple kinds\nof bias with UBM, we emphasize that proper eval-\nuation of bias may still be required in downstream\nside, especially for guaranteed bias mitigation. Cur-\nrently, our initial analysis of UBM conï¬rms that\nbias mitigation effects are transferable, but does\nnot provide guarantees of bias mitigation or levels\nof bias mitigation in the direct setting. The ï¬nd-\nings in this analysis should identify the potential\nof UBM to the broader NLP and machine learning\ncommunities, which may be extended with new\napproaches within the UBM framework, or inter-\npretation techniques (as in Sec. 4.5).\nReferences\nAlex Beutel, J. Chen, Zhe Zhao, and Ed Huai hsin\nChi. 2017. Data decisions and theoretical impli-\ncations when adversarially learning fair representa-\ntions. ArXiv, abs/1707.00075.\nRishabh Bhardwaj, Navonil Majumder, and Soujanya\nPoria. 2020. Investigating gender bias in bert.\nArXiv, abs/2009.05021.\nSu Lin Blodgett, Solon Barocas, Hal DaumÃ© III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of â€œbiasâ€ in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5454â€“\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Lisa Green, and Brendan Oâ€™Connor.\n2016. Demographic dialectal variation in social\nmedia: A case study of African-American English.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1119â€“1130, Austin, Texas. Association for Compu-\ntational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai.\n2016. Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings.\nIn Advances in Neural Information Processing Sys-\ntems 29: Annual Conference on Neural Informa-\ntion Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pages 4349â€“4357.\nA. Caliskan, J. Bryson, and A. Narayanan. 2017. Se-\nmantics derived automatically from language cor-\npora contain human-like biases. Science, 356:183\nâ€“ 186.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised sequence learning. In Advances in Neu-\nral Information Processing Systems 28: Annual\nConference on Neural Information Processing Sys-\ntems 2015, December 7-12, 2015, Montreal, Quebec,\nCanada, pages 3079â€“3087.\nThomas Davidson, Dana Warmsley, Michael Macy,\nand Ingmar Weber. 2017. Automated hate speech\ndetection and the problem of offensive language.\nICWSM.\nMaria De-Arteaga, Alexey Romanov, Hanna Wal-\nlach, Jennifer Chayes, Christian Borgs, Alexandra\nChouldechova, Sahin Geyik, Krishnaram Kentha-\npadi, and Adam Tauman Kalai. 2019. Bias in bios:\nA case study of semantic representation bias in a\nhigh-stakes setting. In Proceedings of the Confer-\nence on Fairness, Accountability, and Transparency,\npages 120â€“128.\nOna de Gibert, Naiara Perez, Aitor GarcÃ­a-Pablos, and\nMontse Cuadros. 2018. Hate speech dataset from\na white supremacy forum. In Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2),\npages 11â€“20, Brussels, Belgium. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171â€“4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLucas Dixon, John Li, Jeffrey Scott Sorensen, Nithum\nThain, and L. Vasserman. 2018. Measuring and mit-\nigating unintended bias in text classiï¬cation. Pro-\nceedings of the 2018 AAAI/ACM Conference on AI,\nEthics, and Society.\nYanai Elazar and Yoav Goldberg. 2018. Adversarial\nremoval of demographic attributes from text data.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n11â€“21, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nAntigoni-Maria Founta, Constantinos Djouvas, De-\nspoina Chatzakou, I. Leontiadis, Jeremy Blackburn,\nG. Stringhini, Athena Vakali, M. Sirivianos, and\nNicolas Kourtellis. 2018. Large scale crowdsourc-\ning and characterization of twitter abusive behavior.\nICWSM.\nMoritz Hardt, Eric Price, and Nati Srebro. 2016. Equal-\nity of opportunity in supervised learning. In Ad-\nvances in Neural Information Processing Systems\n29: Annual Conference on Neural Information\nProcessing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pages 3315â€“3323.\nMandar Joshi, Omer Levy, Luke Zettlemoyer, and\nDaniel Weld. 2019. BERT for coreference reso-\nlution: Baselines and analysis. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5803â€“5808, Hong Kong,\nChina. Association for Computational Linguistics.\nBrendan Kennedy, Mohammad Atari, Aida M Da-\nvani, Leigh Yeh, Ali Omrani, Yehsong Kim, Kris\nCoombs Jr, Shreya Havaldar, Gwenyth Portillo-\nWightman, Elaine Gonzalez, et al. 2018. The gab\nhate corpus: A collection of 27k posts annotated for\nhate speech. PsyArXiv. July, 18.\nBrendan Kennedy, Xisen Jin, Aida Mostafazadeh Da-\nvani, Morteza Dehghani, and Xiang Ren. 2020. Con-\ntextualizing hate speech classiï¬ers with post-hoc ex-\nplanation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5435â€“5442, Online. Association for Computa-\ntional Linguistics.\nSachin Kumar, Shuly Wintner, Noah A. Smith, and\nYulia Tsvetkov. 2019. Topics to avoid: Demoting\nlatent confounds in text classiï¬cation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4153â€“4163, Hong\nKong, China. Association for Computational Lin-\nguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in contex-\ntualized word representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 166â€“172, Florence, Italy. Associ-\nation for Computational Linguistics.\nXuhong Li, Yves Grandvalet, and Franck Davoine.\n2018. Explicit inductive bias for transfer learning\nwith convolutional networks. In Proceedings of the\n35th International Conference on Machine Learning,\nICML 2018, StockholmsmÃ¤ssan, Stockholm, Sweden,\nJuly 10-15, 2018, volume 80 of Proceedings of Ma-\nchine Learning Research, pages 2830â€“2839. PMLR.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sen-\ntence representations. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5502â€“5515, Online. Association\nfor Computational Linguistics.\nDavid Madras, Elliot Creager, Toniann Pitassi, and\nRichard S. Zemel. 2018. Learning adversarially fair\nand transferable representations. In Proceedings\nof the 35th International Conference on Machine\nLearning, ICML 2018, StockholmsmÃ¤ssan, Stock-\nholm, Sweden, July 10-15, 2018 , volume 80 of\nProceedings of Machine Learning Research , pages\n3381â€“3390. PMLR.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 622â€“628, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nSinno Jialin Pan and Qiang Yang. 2010. A survey on\ntransfer learning. IEEE Transactions on Knowledge\nand Data Engineering, 22:1345â€“1359.\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-\nducing gender bias in abusive language detection.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2799â€“2804, Brussels, Belgium. Association\nfor Computational Linguistics.\nFlavien Prost, Nithum Thain, and Tolga Bolukbasi.\n2019. Debiasing embeddings for reduced gender\nbias in text classiï¬cation. In Proceedings of the First\nWorkshop on Gender Bias in Natural Language Pro-\ncessing, pages 69â€“75, Florence, Italy. Association\nfor Computational Linguistics.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out:\nGuarding protected attributes by iterative nullspace\nprojection. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7237â€“7256, Online. Association for Computa-\ntional Linguistics.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1668â€“1678, Florence,\nItaly. Association for Computational Linguistics.\nCandice Schumann, Xuezhi Wang, Alex Beutel,\nJ. Chen, Hai Qian, and Ed Huai hsin Chi. 2019.\nTransfer of machine learning fairness across do-\nmains. ArXiv, abs/1906.09688.\nAli Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghi-\nasi, Christoph Studer, David W. Jacobs, and Tom\nGoldstein. 2020. Adversarially robust transfer learn-\ning. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. Ontonotes release 5.0\nldc2013t19. Linguistic Data Consortium, Philadel-\nphia, PA, 23.\nMengzhou Xia, Anjalie Field, and Yulia Tsvetkov.\n2020. Demoting racial bias in hate speech detection.\nIn Proceedings of the Eighth International Work-\nshop on Natural Language Processing for Social Me-\ndia, pages 7â€“14, Online. Association for Computa-\ntional Linguistics.\nB. H. Zhang, B. Lemoine, and Margaret Mitchell. 2018.\nMitigating unwanted biases with adversarial learn-\ning. Proceedings of the 2018 AAAI/ACM Confer-\nence on AI, Ethics, and Society.\nGuanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Con-\nghui Zhu, and Tiejun Zhao. 2020. Demographics\nshould not be the reason of toxicity: Mitigating\ndiscrimination in text classiï¬cations with instance\nweighting. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4134â€“4145, Online. Association for Computa-\ntional Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\nGender bias in contextualized word embeddings. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 629â€“634,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15â€“20,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nPei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao Huang,\nMuhao Chen, Ryan Cotterell, and Kai-Wei Chang.\n2019. Examining gender bias in languages with\ngrammatical gender. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5276â€“5284, Hong Kong,\nChina. Association for Computational Linguistics.\nA Implementation Details\nA.1 Training Details\nWe use RoBERTa-base as our base model. In the\nbias mitigation phase, models for GHC, Stormfront,\nFDCL, DWMW, and BiasBios are trained with a\nlearning rate 1eâˆ’5, and the checkpoint with the\nbest validation F1 or accuracy score is provided to\nthe ï¬ne-tuning phase. We train on GHC, FDCL,\nDWMW, BiasBios for maximum 5 epochs and\nStormfront for maximum 10 epochs . The check-\npoint with the best validation in-domain classiï¬ca-\ntion performance is kept. In the ï¬ne-tuning phase,\nwe try the learning rate 1eâˆ’5 and 5eâˆ’6, and re-\nport the results with a higher validation in-domain\nclassiï¬cation performance. For the coreference res-\nolution model on OntoNotes 5.0, we adapt existing\ncode implementation4 (Joshi et al., 2019) to support\nloading RoBERTa-base as the base model. We use\nthe same hyperparameter settings as BERT-base in\nthe provided code implementation.\nTo report mean and standard deviation of perfor-\nmance are computed over 3 runs for most of the\nexperiments, with the same set of random seeds;\nfor GHC and Stf. experiments in Table 3, and\nUBMreg,reg,adv on OntoNotes 5.0, we run experi-\nments for 6 runs. Models except coreference reso-\nlution models on OntoNotes, are trained on a single\nGTX 2080 Ti GPU. Coreference resolution models\nare trained on a single Quadro RTX 6000 GPU.\nThe training time per iteration is consistent over\nexperiments in about 1.5 iteration per second, ex-\ncept the conference resolution. The training of\ncoreference resolution model on OntoNotes 5.0\ntakes around 8 hours. The largest dataset among\nother datasets, BiasBios, takes 2 hours to train.\nA.2 Details of Bias Mitigation Algorithms\nFor explanation regularization algorithm, we set\nthe regularization strength Î±as 0.03 for GHC and\nStormfront experiments, and 0.1 for FDCL and\nDWMW experiments. We regularize importance\nscore on 25 group identiï¬ers in (Kennedy et al.,\n2018) for GHC and Stormfront. These group iden-\ntiï¬ers the ones that have the largest coefï¬cient in\na bag-of-words linear classiï¬er. For FDCL, we\nextract 50 words with largest coefï¬cient in the bag-\nof-words linear classiï¬er with a AAE dialect prob-\nability higher than 60% (given by the off-the-shelf\nAAE dialect predictor (Blodgett et al., 2016)) on\n4 https://github.com/mandarjoshi90/co\nref\nits own. For adversarial de-biasing, the adversarial\nloss term has the same weight as the classiï¬cation\nloss term.\nA.3 Dataset Details\nGroup Identiï¬er Bias Experiments. We use a\nbalanced split of the GHC dataset, where training,\nvalidation, and the test set consist of 22,767, 1,586,\nand 1,344 examples. We use the union of â€œhuman\ndegrationâ€ and â€œcall for violenceâ€ has the hate la-\nbel, which results in around 9% of hate examples\nfor all the splits. Note that the split is different\nfrom Kennedy et al. (2020) where the test split has\na much higher ratio of hate examples. We use the\nsame split as Kennedy et al. (2020) for the Storm-\nfront dataset, with 7,896, 978, and 1,998 examples\nin training, validation, and test sets. The NYT cor-\npus contain 12.5knon-hate sentences for testing.\nAA VE Dialect Bias Experiments.We follow Sap\net al. (2019) for the split ratio (73/12/15) of the\nDWMW dataset, which results in 17,994, 2,974,\nand 3,718 examples in each split. For the FDCL\ndataset, as only tweet ids are provided and some of\nthe tweets are no longer available, the ï¬nal dataset\nconsists of 41191, 5149, 5149 examples for each\nsplit. We release the tweet ids used in each split.\nFollowing (Xia et al., 2020), we sample 20 k ex-\namples with an AA VE speaker probability (which\nis included in the dataset) greater than 80%. We\nmanually verify a subset of examples in BROD\nfollowing the protocols of (Sap et al., 2019) and\nfound 93% of sentences clearly non-toxic.\nGender Stereotypical Bias Experiments. For the\nBiasBios dataset, we use the same split as Ravfogel\net al. (2020) with 255,710 training examples (65%),\n39,359 validation examples (10%), and 98,344\n(25%) test examples. We use the ofï¬cial dataset\nsplit (Weischedel et al., 2013) for the OntoNotes\n5.0 dataset.\nWe use the same train/test splits between â€œtrans-\nferâ€ and â€œnon-transferâ€ setup. Two partitions of\ndatasets used for Same-distribution UBMexper-\niments have a random half of total examples for\ntrain/validation/test splits. For IPTTS/NYT/BROD,\nwe use the same test set across tables.\nA.4 Details of â„“2-sp Regularizer\nThe â„“2-sp regularizer (Li et al., 2018) we applied in\nSec. 4.4 penalizes the distance between the weights\nand the initial point of ï¬ne-tuning. Formally, let\nw0 be the initial weight of the encoder gt before\nï¬ne-tuning, and w be the current weight of gt. The\nMethod / Datasets GHC B Stormfront B FDCL B Biasbios B\nMetrics In-domainF1 (â†‘) In-domainFPRD (â†“) IPTTSFPRD (â†“) NYTAcc (â†‘) In-domainF1 (â†‘) In-domainFPRD (â†“) IPTTSFPRD (â†“) NYTAcc (â†‘) In-domainAcc. (â†‘) BRODAcc. (â†‘) In-domainF1 (â†‘) WinobiasF1-Diff (â†“)\nNon-Transfer (GHC B) Non-Transfer (Stf. B) Non-Transfer (FDCL. B)Non-Transfer (Biasbios B)\nVanilla 37.91Â±2.5 35.64Â±2.2 21.50Â±2.8 68.55Â±20 55.56Â±0.520.81Â±4.9 10.99Â±5.666.28Â±8.175.72Â±0.2 73.57Â±1.285.52Â±0.113.83Â±0.2Expl. Reg. 38.09Â±2.7 18.68Â±0.3 4.82Â±1.1 84.05Â±3.053.05Â±1.015.97Â±1.1 3.36Â±3.3 65.23Â±10 77.30Â±0.276.72Â±1.1Adv. Learning 75.28Â±0.277.12Â±1.285.07Â±0.0 9.61Â±0.5\nGHC Aâ†’GHC B Stf. Aâ†’Stf. B FDCL Aâ†’FDCL BBiasbios Aâ†’Biasbios B\nVan. Transfer42.41Â±1.0 37.44Â±1.5 17.67Â±2.1 75.35Â±4.258.43Â±1.2 17.96Â±3.6 11.58Â±4.874.12Â±4.276.33Â±0.670.35Â±2.485.81Â±0.2 12.59Â±0.5UBMReg 43.79Â±1.9 34.34Â±3.1 10.02Â±1.1 81.40Â±1.458.56Â±1.0 16.42Â±0.9 7.51Â±2.4 69.45Â±4.076.22Â±0.5 69.29Â±1.8UBMAdv 75.88Â±0.471.11Â±1.685.86Â±0.1 11.99Â±0.5\nTable 6: Same-domain and task UBM with a single bias factor. We partition a dataset to two subsets, noted as\nsplit A and split B. We train the upstream model with split A and ï¬ne-tune on split B. All metrics except In-domain\nF1 or In-domain Accuracy measures bias. The preferred outcomes for eachmetric are marked with arrows.\nMetrics In-domain F1 (â†‘) In-domain FPRD (â†“) IPTTS FPRD (â†“) NYT Acc (â†‘) In-domain F1 (â†‘) BROD Acc. (â†‘) In-domain F1 (â†‘) In-domain TPRD (â†“)\nUpstream model Stormfront A + FDCL A\nDownstream model Stormfront B FDCL B\nVan-Transfer57.58Â±2.7â†—â†— 13.97Â±2.0â†˜â†˜ 11.33Â±2.2â†’â†’75.72Â±7.5â†—â†—77.18Â±0.5â†—â†— 71.12Â±1.2â†’â†’UBMReg,Reg 56.72Â±1.7â†—â†— 17.91Â±1.0â†˜â†˜ 8.05Â±0.6â†˜â†˜ 77.40Â±0.3â†—â†—77.13Â±0.3â†—â†— 72.17Â±1.6â†’â†’UBMReg,Adv 55.63Â±2.5â†—â†— 17.14Â±0.5â†˜â†˜ 13.78Â±4.3â†’â†’70.37Â±10â†—â†— 76.64Â±0.6â†—â†— 76.55Â±0.6â†—â†—\nUpstream Model GHC A + FDCL A\nDownstream Model GHC B FDCL B\nVan-Transfer44.30Â±0.7â†—â†— 41.06Â±3.9â†’â†’ 19.75Â±6.9â†˜â†˜74.60Â±6.3â†—â†—77.34Â±0.4â†—â†— 72.96Â±1.5â†’â†’UBMReg,Reg 42.96Â±2.0â†—â†— 33.98Â±3.0â†˜â†˜ 9.30Â±2.1â†˜â†˜ 86.05Â±1.9â†—â†—76.21Â±0.4â†—â†— 73.10Â±1.4â†’â†’UBMReg,Adv 42.44Â±3.5â†—â†— 33.96Â±1.5â†˜â†˜ 16.68Â±1.7â†˜â†˜81.79Â±9.0â†—â†—76.94Â±0.4â†—â†— 76.55Â±0.7â†—â†—\nUpstream Model GHC A+ FDCL A+ BiasBios A\nDownstream Model GHC B FDCL B BiasBios B\nVan-Transfer42.80Â±3.3â†—â†— 37.83Â±10â†’â†’ 17.38Â±1.7â†˜â†˜72.23Â±13â†—â†— 77.30Â±0.2â†—â†— 72.91Â±1.1â†’â†’ 85.81Â±0.0â†—â†— 12.78Â±0.0â†˜â†˜UBMReg,Reg,Adv42.81Â±1.9â†—â†— 31.86Â±1.1â†˜â†˜ 9.61Â±1.8â†˜â†˜ 82.15Â±9.5â†—â†—76.95Â±0.1â†—â†— 72.93Â±0.6â†’â†’ 85.81Â±0.0â†—â†— 11.72Â±0.8â†˜â†˜UBMReg,Adv,Adv41.66Â±2.3â†—â†— 33.39Â±0.2â†˜â†˜ 10.00Â±0.8â†˜â†˜83.50Â±3.0â†—â†—77.03Â±0.3â†—â†— 75.87Â±0.8â†—â†— 85.79Â±0.1â†—â†— 12.43Â±0.5â†˜â†˜\nTable 7: Dealing with multiple bias factors with a single upstream model with UBM, where the domains\nand tasks are the same in the upstream and the downstream model . â†—â†—and â†˜â†˜show whether the metrics has\nincreased or decreased (both imply improvement) compared to non-transfer Vanilla training in Table 3.\nâ„“2-sp regularizer is written as â„¦(w) = Î²||w âˆ’\nw0||2\n2, appended to the learning objective. Î² is\na hyperparameter controlling the strength of the\nregularization. We reported results where Î² = 1.\nWe tried different values of Î² from 1eâˆ’6 to 100,\nincreasing Î²by 10 times each time, but we do not\nsee changes in the conclusion.\nB Complete Analysis of UBM over the\nSame Data Distribution\nTable 6 show the results of same-domain transfer\nwith a single bias factors. Table 7 further show the\nresults of addressing multiple bias factors in this\nsetup.\nOn GHC, Stormfront, and BiasBios, UBM over-\nall reduces bias compared to Vanilla and Vanilla-\nTransfer. We notice the NYT accuracy on Storm-\nfront in Stf. A â†’Stf. B setup is an exception.\nHowever, we see the bias is not reduced on Stf.\nB even when we directly run explanation regular-\nization in the target domain. We reason that the\nHalf-Stormfront dataset is small and the average\nlength of the sentences are quite different between\nStormfront and NYT, so that a model trained on\nMetrics In-domainF1 (â†‘) In-domainFPRD (â†“) IPTTSFPRD (â†“) NYTAcc (â†‘)\nStormfrontâ†’GHC\nExpl. Reg. 43.37Â±1.8 29.29Â±1.2 4.20Â±1.6 81.22Â±11Van-Transfer + Reg.45.25Â±2.2 29.91Â±1.9 5.01Â±1.3 86.15Â±2.9UBMReg+ Reg. 44.92Â±2.0 28.85Â±2.1 3.36Â±1.2 89.33Â±1.2\nGHCâ†’Stormfront\nExpl. Reg. 51.53Â±1.8 13.43Â±1.5 3.80Â±0.4 83.73Â±8.0Van-Transfer + Reg.52.18Â±1.3 13.12Â±1.1 4.35Â±0.3 80.54Â±2.0UBMReg+ Reg. 53.58Â±1.4 16.07Â±1.3 4.53Â±0.9 82.59Â±1.4\nTable 8: Applying both UBM and downstream bias-\nmitigation (UBMReg + Reg.), compared to down-\nstream bias mitigation only (Expl. Reg.) and down-\nstream bias mitigation over Van-Transfer model (Van-\nTransfer + Reg.).\nStormfront hardly generalizes to NYT.\nWe ï¬nd intriguing results on FDCL; From FDCL\nA â†’FDCL B in Table 6, we ï¬nd bias is not re-\nduced with UBM. However, as shown in Table 7,\nwhen the upstream model is trained jointly with\nother datasets to reduce multiple bias factors (Stf A\n+ FDCL A, GHC A + FDCL A, GHC A + FDCL\nA + BiasBios A), the bias is clearly reduced.\nC Applying UBM with Downstream Bias\nMitigation\nIn Table 8, we report the performance of perform-\ning both upstream and downstream bias mitiga-\ntion, compared with downstream bias mitigation\nonly, and downstream bias mitigation over a vanilla-\ntransferred model. We see UBM further reduced\nbias in the Stormfront â†’GHC setup, while fail to\nimprove in GHC â†’Stormfront. Compared to our\nprevious results in Tables 3 and 4, we see a clearer\ndirectionality of transfer of bias mitigation effects\nwhen downstream bias mitigation is also applied."
}