{
  "title": "An Innovative Approach Utilizing Binary-View Transformer for Speech Recognition Task",
  "url": "https://openalex.org/W4226407676",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5051225559",
      "name": "Muhammad Babar Kamal",
      "affiliations": [
        "COMSATS University Islamabad"
      ]
    },
    {
      "id": "https://openalex.org/A5030330846",
      "name": "Arfat Ahmad Khan",
      "affiliations": [
        "Suranaree University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5089948761",
      "name": "Faizan Ahmed Khan",
      "affiliations": [
        "COMSATS University Islamabad"
      ]
    },
    {
      "id": "https://openalex.org/A5044727888",
      "name": "Malik Muhammad Ali Shahid",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5056044175",
      "name": "Chitapong Wechtaisong",
      "affiliations": [
        "Suranaree University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5043194929",
      "name": "Muhammad Daud Kamal",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5102891059",
      "name": "Muhammad Ali",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5044429602",
      "name": "Peerapong Uthansakul",
      "affiliations": [
        "Suranaree University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6655832526",
    "https://openalex.org/W6678940646",
    "https://openalex.org/W2888641632",
    "https://openalex.org/W6745037961",
    "https://openalex.org/W6746451879",
    "https://openalex.org/W2783089003",
    "https://openalex.org/W2127566601",
    "https://openalex.org/W6665302887",
    "https://openalex.org/W6746700228",
    "https://openalex.org/W2522454276",
    "https://openalex.org/W2055920531",
    "https://openalex.org/W6741001162",
    "https://openalex.org/W2889351375",
    "https://openalex.org/W6732160842",
    "https://openalex.org/W6727253422",
    "https://openalex.org/W6755789009",
    "https://openalex.org/W2996989128",
    "https://openalex.org/W6750992012",
    "https://openalex.org/W6753640285",
    "https://openalex.org/W3006197249",
    "https://openalex.org/W2011055829",
    "https://openalex.org/W2041498927",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2953763392",
    "https://openalex.org/W6745181628",
    "https://openalex.org/W6734260513",
    "https://openalex.org/W2962822108",
    "https://openalex.org/W6694260854",
    "https://openalex.org/W2558580397",
    "https://openalex.org/W6748283985",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W2944961840",
    "https://openalex.org/W2799876173",
    "https://openalex.org/W2988012990",
    "https://openalex.org/W2952802413",
    "https://openalex.org/W6741103355",
    "https://openalex.org/W2797583228",
    "https://openalex.org/W2732599665",
    "https://openalex.org/W2122364000",
    "https://openalex.org/W2369522632",
    "https://openalex.org/W2519564115",
    "https://openalex.org/W2600687292"
  ],
  "abstract": "The deep learning advancements have greatly improved the performance of speech recognition systems, and most recent systems are based on the Recurrent Neural Network (RNN). Overall, the RNN works fine with the small sequence data, but suffers from the gradient vanishing problem in case of large sequence. The transformer networks have neutralized this issue and have shown state-of-the-art results on sequential or speech-related data. Generally, in speech recognition, the input audio is converted into an image using Mel-spectrogram to illustrate frequencies and intensities. The image is classified by the machine learning mechanism to generate a classification transcript. However, the audio frequency in the image has low resolution and causing inaccurate predictions. This paper presents a novel end-to-end binary view transformer-based architecture for speech recognition to cope with the frequency resolution problem. Firstly, the input audio signal is transformed into a 2D image using Mel-spectrogram. Secondly, the modified universal transformers utilize the multi-head attention to derive contextual information and derive different speech-related features. Moreover, a feed-forward neural network is also deployed for classification. The proposed system has generated robust results on Google's speech command dataset with an accuracy of 95.16% and with minimal loss. The binary-view transformer eradicates the eventuality of the over-fitting problem by deploying a multi-view mechanism to diversify the input data, and multi-head attention captures multiple contexts from the data's feature map.",
  "full_text": "This work is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided\nthe original work is properly cited.\nechT PressScienceComputers, Materials & Continua\nDOI: 10.32604/cmc.2022.024590\nArticle\nAn Innovative Approach Utilizing Binary-View Transformer for Speech\nRecognition Task\nMuhammad Babar Kamal1, Arfat Ahmad Khan2, Faizan Ahmed Khan3,\nMalik Muhammad Ali Shahid4, Chitapong Wechtaisong2,*, Muhammad Daud Kamal5,\nMuhammad Junaid Ali6 and Peerapong Uthansakul2\n1COMSATS University Islamabad, Islamabad Campus, 45550, Pakistan\n2Suranaree University of Technology, Nakhon Ratchasima, 30000, Thailand\n3COMSATS University Islamabad, Lahore Campus, 54000, Pakistan\n4COMSATS University Islamabad, Vehari Campus, 61100, Pakistan\n5National University of Sciences & Technology, Islamabad, 45550, Pakistan\n6Virtual University of Pakistan, Islamabad Campus, 45550, Pakistan\n*Corresponding Author: Chitapong Wechtaisong. Email: chitapong@g.sut.ac.th\nReceived: 23 October 2021; Accepted: 26 November 2021\nAbstract: The deep learning advancements have greatly improved the perfor-\nmance of speech recognition systems, and most recent systems are based on\nthe Recurrent Neural Network (RNN). Overall, the RNN works fine with\nthe small sequence data, but suffers from the gradient vanishing problem in\ncase of large sequence. The transformer networks have neutralized this issue\nand have shown state-of-the-art results on sequential or speech-related data.\nGenerally, in speech recognition, the input audio is converted into an image\nusing Mel-spectrogram to illustrate frequencies and intensities. The image\nis classified by the machine learning mechanism to generate a classification\ntranscript. However, the audio frequency in the image has low resolution\nand causing inaccurate predictions. This paper presents a novel end-to-end\nbinary view transformer-based architecture for speech recognition to cope\nwith the frequency resolution problem. Firstly, the input audio signal is\ntransformed into a 2D image using Mel-spectrogram. Secondly, the modified\nuniversal transformers utilize the multi-head attention to derive contextual\ninformation and derive different speech-related features. Moreover, a feed-\nforward neural network is also deployed for classification. The proposed\nsystem has generated robust results on Google’s speech command dataset with\nan accuracy of 95.16% and with minimal loss. The binary-view transformer\neradicates the eventuality of the over-fitting problem by deploying a multi-\nview mechanism to diversify the input data, and multi-head attention captures\nmultiple contexts from the data’s feature map.\nKeywords: Convolution neural network; multi-head attention; multi-view;\nRNN; self-attention; speech recognition; transformer\n5548 CMC, 2022, vol.72, no.3\n1 Introduction\nThe recent surge of Artificial Intelligence (AI) in modern technology has resulted in the\nwidespread adoption of Human-Computer-Interaction (HCI) applications. Big corporations in\ninformation technology like Google, Apple, Microsoft, and Amazon are relentlessly working to\nimprove the applicability and dynamics of HCI applications using speech recognition algorithms.\nThe importance of recognition systems underscores vast fields, including stakeholders from the\ndomain related to entertainment applications, utility applications and critical lifesaving appliances.\ne.g., Y ouTube [1] and Facebook [2] use speech recognition systems for captioning. Various robust\nvoice commands applications are proposed for devices that work in areas without internet services and\ncritical mission’s robots [3,4]. Moreover, robust design of micro-controller-based devices which works\nbased on speech commands are also proposed in literature [5]. Apple Siri, Amazon Alexa, Microsoft\nCortana, Y ouTube captions, and Google Assistant [6] deploy speech recognition systems which works\nbased on these designs. Google and Microsoft [7], use deep neural networks-based algorithms that\nconvert sound to text through speech recognition, process the text, and respond accordingly. Typically,\ndeep learning algorithms processes the 1D data as audio is recorded and represented s a 1D waveform\n[8]. The waveform of the 1D signal is represented in the sinusoidal time domain. In [9], the authors\nstudied the 2D representation of an audio signal called the spectrogram where the frequencies spectrum\nis derived from the time-frequency domain through Fourier transform.\nSpeech signals contains rich prominent features such as emotions and dialect. Studies have been\nconducted to compare the 1D audio waveform and 2D spectrogram, the spectrogram concluded\nthat the 1D signal does not contain frequency information vital for better speech recognition [10].\nStudies shows that 2D spectrogram performs better to extract features for speech recognition. Since\na spectrogram focuses on all the frequencies, the recognition system cannot properly differentiate [11]\nbetween relevant frequencies and noise. Fusion of mel-scale with spectrogram reduces noise which\nshows performance improvement in speech recognition. The mel-scale discards noise and amplifies\nthe desired spectrum of frequencies in the 2D spectrogram. The 2D transformation (mel-spectrogram)\nof audio signal deploy state-of-the-art image recognition algorithms in Neural Networks (N.N) for\nspeech recognition to improve the precision of the system by imitating the human speech perception\n[12]. The N.N algorithms [13] process raw input data by correlating hidden patterns to recognize\nsimilar clusters in data and classify it by continuously learning and enhancing the recognition system.\nRecurrent NNs (RNNs) [14–16], Convolutional NNs (CNNs) [17] and Attention are commonly used\nto develop speech recognition systems. RNN captures sequential prediction of data using recurrence\nunits to predict pattern for next likely scenario. RNN algorithms and their variants, i.e., Long-Short-\nTerm-Memory (LSTM), and Gated-Recurrent-Unit (GRU) allow the machine to process sequential\ndata models, such as speech recognition. LSTM has become popular in recurrent networks due to its\nsuccess in solving the vanishing gradient problem by retaining the long-term dependencies of data.\nHowever, the LSTM [18] fails to solve the vanishing gradient problem completely due to the\ncomplexity of the additional evaluation of memory cells. The RNN models are prone to over-fitting\ndue to the difficulty of applying dropout algorithms with LSTM probabilistic units. The sequential\nnature of models is inconsistent with the parallelization of processing [19]. RNN models require more\nresources and time to train due to the linearized natures of layers and random weights initialization.\nMany researchers have used CNN for audio classification to analyze visual imagery of audio by\nconvolving multiple filters on data to extract features for the neural network. Deep CNN convolves\nmultiple layers of filters on image to extract distinct features having depth depending on the number\nof layers. Deep networks improve algorithm’s ability by capturing unique properties using multiple\nCMC, 2022, vol.72, no.3 5549\nconvolution layers to retrieve a higher level of features. The feature-map produced from this process\nenhances the recognition system accuracy.\nHowever, these studies observe that the deeper layers of convolution tends to assimilate gener-\nal/abstract level information from the input data [20]. The deep CNN model tends to over-fit when\nthe labeled data for training is less. The deep networks of the convolution model are prone to gradient\nvanishing/exploding problems as the network deepens, causing less precision of the recognition model.\nTherefore, the researchers deploy attention mechanisms with an RNN model to obtain long-term\ndependencies by contextualizing the feature-map. The attention model uses probabilistic learning\nby giving weight to the important feature using the soft-max probabilistic function. Moreover, the\nattention-based models reduce the vanishing gradient problem by decreasing the number of features\nto process important and unique features for the recognition system [21]. In [22], the authors introduce\none of the attention mechanism variations, self-attention, to compute the representation of the same\nsequence relating to different positioning. Self-attention allows input sequences to interact with all\nneighboring values and find contextual and positional attention within the same sequence. In [23], the\nauthors observe the multi-view approach with a neural network algorithm to increase the efficiency\nof the architecture. The main objective of the paper is to improve existing speech recognition systems\nby building a precise method that can be implemented in any speech recognition application with a\nlightweight footprint.\nIn [24], the authors use Fourier transform to convert the waveform signal to alternative represen-\ntations characterized by a sinusoidal function. The paper uses Infrared spectroscopy through Fourier\ntransform for analysis of biological material. In [25], the Short-Time Fourier-Transform (STFT) is\nused to extract features from the signal of audio by slicing the signal into windows and performing\nFourier transform on each window to obtain meaningful information. Actually, Deep Learning\n(DL) [26] models extract intricate structures in data, and back-propagation algorithms show which\nparameters are used for calculating each layer representation. In fact, DL allows the computation of\nmultiple processing for the learning of data representation having many levels of abstractions. In [27],\nauthors elaborate the feature extraction in speech categorizing speech recognition to three stages. At\nfirst, the audio signal is divided into small chunks; secondly, the phoneme is extracted and processed,\nand lastly, the speech is categorized on word level. Music detection is discussed in [28], where the\nauthors use CNN with mel kernel to separate music content from speech and noise. The mel-scale is\nuseful for focusing on a specific type of frequency and minimizing the effect of noisy and unrelated\nparts.\nIn [29], an attention model is used for audio tagging of Google Audio Set [30]. The authors\ninvestigate Multi Instance-Learning (MIL) problem for weakly labeled audio set classification by\nintroducing the attention model for probabilistic learning, where attention is used with a fully\nconnected neural network for multi-label classification on audio. Multi-head attention is used in [31],\nwhere authors elaborate the implication to extract information from multiple representation sub-\nspaces at various positions by the ability of multi-head to attend to different interpretations within the\ndata jointly. The multi-head attention is useful for obtaining different contexts within the information\nwhich improve the efficiency of the model.\nIn this paper, we present a novel end-to-end binary view transformer-based architecture for\nspeech recognition to cope with the frequency resolution problem. Firstly, the input audio signal is\ntransformed into a 2D image using Mel-spectrogram. Secondly, the multi-view mechanism is used\nto enhance the frequency resolution in the image. In addition, the Modified universal transformers\nutilized the multi-head attention to derive contextual information and derive different speech-related\n5550 CMC, 2022, vol.72, no.3\nfeatures. A feed-forward neural network is also deployed for classification. The proposed system is\ndiscussed in details in the Section 5. Moreover, the proposed system has generated robust results on\nGoogle’s speech command dataset with an accuracy of 95.16% and with minimal loss. The binary-\nview transformer eradicates the eventuality of the over-fitting problem by deploying a multi-view\nmechanism to diversify the input data, and multi-head attention captures multiple contexts from the\ndata’s feature map.\nThe rest of the paper is organized as follows: The Section 2 contains the speech perception and\nrecognition by using AI, and the proposed system is discussed in the Section 3. The Section 4 includes\nthe experiment steps and testing. Furthermore, the Section 5 includes the experiment results and\ndiscussions. Finally, the Section 6 concludes the research work.\n2 Speech Perception and Recognition Using AI\nPerception is the ability to systematically receive information, identify essential data features and\nthen interpret that information, while recognition is the system’s ability to identify the classification of\ndata. To build a system using AI for the speech recognition, we need to have input data that is in the\nform of an audio signal. After pre-processing, the audio signal progresses to the speech recognition\nsystem, and the systems output will be a classification transcript of the audio. A microphone records\nthe audio signal with a bit depth of 16 (recorded signal in time domain having values of 2∗ 16).\nAudio is recorded at 16 kilohertz having a nitrous frequency of 8 kilohertz; the nitrous is a range of\ndistinguished lower frequency, which is interpretable and differentiable by the brain for speech because\nmost frequency changes happen at lower frequencies. The signal in the time domain is complicated to\ninterpret, as the human ear can sense the intensity of frequency. Moreover, we use a pre-processing\nstep to convert the signal into the frequency domain using Fourier transform, where the time-domain\nrepresentation of the signal is transformed into a time-frequency domain.\nThe power spectral density of the audio signal for different bands of frequencies are shown in\nthe Fig. 1, where the nitrous frequency range has most frequencies changes. We create a spectrogram\nby stacking periodogram adjacent to one another over time. The spectrogram is a colored 2D image\nrepresentation of the audio.\nFigure 1: Periodogram of audio frequencies and the 2D representation of audio signal using\nspectrogram\nFor speech recognition, the human brain amplifies some frequencies, while nullifying or reducing\nthe background noise by giving more importance to the lower band of frequencies. For example,\nCMC, 2022, vol.72, no.3 5551\nhumans can tell the difference between 40 and 100 hertz, but are unable to differentiate between 10,000\nand 12,000 hertz. This objective in computing is achieved through mel-scale; by applying mel-filter-\nbank on the frequencies, we can retrieve the lower frequencies efficiently, as shown in theFig. 2.\nFigure 2:Mel filter-banks and the frequencies from linear to logarithmic\n2.1 Convolutional Neural Network\nIn the field of machine learning, CNN is one of the most efficient algorithms for image\nrecognition. Since the inception of CNN, the field of machine learning is revolutionized, and state-\nof-the-art results are produced. In CNN, different filters are convolved over the image to compute\nessential features by using theEqs. (1)and (2), where filter B convolves over image A having k number\nof rows and columns. Convolution gives us a large pool of features in data that is passed to a N.N.,\nwhich helps to classify them into different classes. Many variants of CNN are produced over the years\nthat improve the performance of these models, e.g., Inception net [32], Resnet [33], and mobile net [34].\nConv\n(A,B) = A ∗ B (1)\nConv|x, y|=\nK∑\na=−k\nK∑\nb=−k\nA|a, b|B|x − a, y − b| (2)\n2.2 Recurrent Neural Network\nRNN algorithms allow the machine to process temporal sequences of variable lengths [14]. This\ntype of algorithm is useful in processing sequential data through sequential modelling, e.g., signal\nprocessing, NLP, and speech recognition. The RNN models produce hidden-vector as a function of the\nformer states and the next input as shown inEq. (3), where input vectorsA are sequentially processed\nby recurrenceFunction having w parameters on each time-stamp to produce a new state for the model.\nState\n(new) = Function(w)[State(old), A] (3)\nRecurrence models generate a sequential pattern of data that prevents parallelization for training data.\nThe sequential nature increases the computation time of the model and limits longer sequences from\nprocessing, which causes the gradient vanishing/exploding problem.\n5552 CMC, 2022, vol.72, no.3\n2.3 Attention Mechanism\nAttention is a deep learning mechanism that is mainly inspired by the natural perception ability of\nhumans as humans receive information in raw form from the senses and transmit it to the brain [29].\nThe brain opts for the relevant and useful information by ignoring background noises; this process\npolishes the data, making it easier to perceive. Moreover, the attention is a weighted probabilistic\nvector with the soft-max function used in a neural network, which was introduced to improve the\nsequential models (LSTMs, RNNs) to capture essential features in context vectors as shown inEq. (4),\nand Attention_weightis elaborated in theEq. (5).\nContext_vector =\nn∑\ny\nAttention_weighty ∗ Hidden_statey (4)\nAttention_weighty = exp(Hiddenstatey)\n∑k\na=0 Hidden_statea\n(5)\nThe attention mechanism extracts model dependencies while the effect of distance between input\nand output sequences is negated, which improves the model performance. Self-attention [35]i sa\nvariation of the attention mechanism that allows the vectors to interact with each other to discover\nthe important features, so that more attention can be given. Applying attention to sequential models\nimproves the accuracy, and state of the art results are achieved.\n2.4 Transformer\nIn transformer architecture, instead of sequential representation, the positional information\n(input data) is embedded in positional vector with input vectors that permit parallelization. Trans-\nformer architecture consists of two parts, i.e., encoder layers and decoder layers. In a transformer,\nattention mechanism is used for content-based memory retrieval, where decoder attends to content\nthat is encoded and decides which information needs to be extracted based on affinity or its position.\nThe input data is processed by the transformer in the form of pixel blocks [36] i.e., each row of image\nare embedded, and the positional information of data are encoded by using positional encoder into\nthe input embedding, which is subsequently passed to transformer for processing.\nThe positional information is extracted from the data by using positional encoderE, which is\nadded to the input embedding. The input embedding and positional encoder have same dimensiond,\nso that both can be summed. The positional encoding is extracted using sine function by using the\nEq. (6), and cosine function inEq. (7)alternatively. TheEq. (6)is used for odd values, andEq. (7)is\nused for even value as shown in theEq. (8)for n length input sequence. The sine and cosine functions\nare used to create a unique pattern of values for each position.\nE\np = sine\n(p\nfi\n)\n(6)\nEp = cosine\n(p\nfi\n)\n(7)\nCMC, 2022, vol.72, no.3 5553\nwhere p is the position to encode, andfi are the frequencies ofi numbers up tod/2 as shown in equation\nEq. (9).\nEp =\n⎧\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎨\n⎪⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎪⎪\n⎪⎩\nsine\n(p\nf1\n)\ncosine\n(p\nf1\n)\nsine\n(p\nf2\n)\ncosine\n(p\nf2\n)\n...\nsine\n(\np\nfd\n2\n)\nsine\n(\np\nfd\n2\n)\n(8)\nfi => 1\nλi\n:= 10000\n2i\nd (9)\nIn Transformer encoder layer, the embedded inputX ={ x1,x 2,x 3, ... xn} is fed into three fully\nconnected layers to create three embeddings, namely keys, query and value; these embeddings are\ncommonly used in the search retrieval system. During the search retrieval, the query is mapped against\nsome keys that are associated with search candidates; this presents best match searches (values). To\ncompute the attention value of input embeddingx\n1 against x2 as shown in theFig. 3,t r a n s f o r m e r\nself-attention; Firstly, theQ, K,a n dV are randomly initialized having same dimension as the input\nembedding. The inputx1 is matrix-multiplied withQ to produce Query embeddingQe, and embedding\nx2 is matrix-multiplied withK to produce Key embeddingKe, then the resultant matrixes dot product\n(weighted score matrixZ) is calculated. The scoresZ are then scaled-down as shown inEq. (10)for a\nstable gradient, wheredKe is the dimension of keys embedding.\nZs = Z√dKe\n(10)\nThe softmax function inEq. (11)is applied toZs ={ z1,z 2,z 3, ... zn} to calculate attention weights,\ngiving probability values between zero and one. TheFig. 3input embedding is a matrix which is\nmultiplied with theVe to produce value embedding.\nSoftmax(zi) = exp(zi)∑\ni exp(zi) (11)\nAttention (K, Q, V) = Softmax\n(QeKt\ne\n√dKe\n)\nVe (12)\n5554 CMC, 2022, vol.72, no.3\nFigure 3:Transformer: Encoding layer self-attention and the transformer layer and model\nThe Zs multiplies with theVe. This process is repeated for all the inputs neighborhood, andVe\nare then concatenated. The functionality is elaborated inEq. (12),w h e r eKe\nt is the transpose of keys\nembedding. The self-attention produces weighted embeddings for each input.\n3 Proposed System and Architecture\nThe architecture proposed in this paper is a novel binary-view transformer architecture, an end-\nto-end model for a speech recognition system, which is inspired by the human perception of speech\nand is articulated by carefully studying human physiology. The architecture consists of two primary\nmodules i.e., (i) Pre-processing and feature extraction module and (ii) Classification module.\nThree convolution layers are applied for the feature extraction on both inputs. The filter size is\n3 × 3, and numbers of filters are 32, 64, and 1, respectively. After each layer of convolution, batch\nnormalization is implemented with an activation function. Both of the inputs are then concatenated\nto add the extracted features in multi-view i.e., binary view model.\nOur system incorporates a modified universal transformer [19,22], where multi-head self-attention\nis used with four heads capturing four different contexts at the same time. The depth of the transformer\nis six, i.e., six encoding and six decoding transformer layers are implemented. The transformer is\ntuned to 25 percent dropout after each layer, and a high-performance gaussian-error linear unit [37]\nactivation function of the neural network is used. The adaptive computations time algorithm is then\nused with the aim of allowing the neural network to determine computation steps for getting inputs\nand computing outputs. The resultant vectors then proceed to global average pooling [38], where mean\nvalue for every feature map is computed, and soft-max then determines its probabilities. Lastly, the\nfeature map is passed to a dense layer, i.e., fully-connected layer; of 128 nodes and subsequently another\ndense layer of nodes equal to desire classes where the input is classified to its respective class. It is\nimportant to noted that the classification is vital by considering the fact that the internet data traffic\nis increasing with every passing day [39–42]. The working of system is shown in Algorithm 1.\nCMC, 2022, vol.72, no.3 5555\n4 Training and Experimentation\nFor training the proposed model, we use Google’s data-set of speech command [43,44]c r e a t e d\nby Google Brain, which has speech audio files in W A V format, having a total of 105,829 command\nutterance files. The data-set audio files have a length of 1 s, divided into 35 classes of words. The audio\nwas recorded in a 16-bits mono channel, and the command files are collected from 2618 different\nspeakers having a range of dialects.\nThe tool used for training the architecture is Google cloud service for machine learning, namely\nGoogle-colab, which uses a jupyter-notebook environment, and Tesla Graphics Processing Unit\n(GPU) K80 is provided by Google having 12 GB of GPU.\nWe trained different architecture for speech recognition of 35 classes, which includes our binary-\nview transformer model and the models introduced by paper [3], i.e., LSTM and attention-based recur-\nrent convolutional architectures. We also experimented with well-known convolutions architectures\nof resnet and inception net, where we modify our model by replacing the Transformer with Resnet\n(Fig. 4), proposed architecture (Fig. 5) and Inception net (Fig. 6). We then compute and Recurrent\ncompare their results. We then compute and Recurrent compare their results.\nFigure 4:Binary-view ResNet\n5556 CMC, 2022, vol.72, no.3\nFigure 5: System architecture of binary-view convolution the transformer, which captures multiple\nfeature contexts of each class, making recognition more rob\nFigure 6:Binary-view inception net\nCMC, 2022, vol.72, no.3 5557\n5 Results and Discussion\nInitially, the experiments of paper [3] were replicated, including the LSTM model and attention\nwith a recurrent convolutional network. The purpose of the experimentation results is to compare\nand demonstrate the efficiency and shortcomings of different neural network models for the speech\nrecognition. In terms of accuracy, we improved the validation accuracy by gradually decreasing the\nlearning rate over epochs and increasing filters in convolutions and introducing batch normalization.\nThe LSTM model [3] accuracy is recorded up to 93.9, and the attention based recurrent convolutional\nnetwork [3] accuracy is 94.2. The transformer architecture without multiple inputs gives 94.24%\naccuracy. The binary-view resnet model, binary view inception net model, binary-view convolutional\nmodel, and binary-view transformer model were executed on the dataset, where validation accuracy\nwas 94.91, 94.74, 95.05, and 95.16, respectively as shown inFig. 7. Moreover, the proposed transformer\nmodel produced state-of-the-art as well as a minimalistic number of parameters, i.e., 375,787. The\nFig. 8shows the comparison of training and validation accuracies.\nIn terms of loss, the binary-view transformer model validation loss is comparatively less, which is\n0.191. Single input transformer model produces 0.227 loss. The binary-view resnet model, binary-\nview inception net model, binary-view convolutional model losses, and attention based recurrent\nconvolutional network were 0.194, 0.192, 0.21, and 0.237, respectively as can be seen inFig. 9.T h e\ndecline of loss exhibits a better performance of the architecture and lower chance of the model being\nover-fitting with the aim of eradicating the gradient vanishing/exploding problem.\nTable 1: Results comparison of the proposed approach with existing studies\nModel Validation\nloss\nValidation\naccuracy\nTraining\naccuracy\nValidation\nprecision\nValidation\nrecall\nValidation\nF1-score\nTraining\nprecision\nTraining\nrecall\nTraining\nF1-score\nLSTM model [3] 0.242 82.11 98.84 - - - - - -\nAttention based\nrecurrent\nconvolution\nnetwork [3]\n0.237 84.35 98.68 - - - - - -\n(Continued)\n5558 CMC, 2022, vol.72, no.3\nTable 1:Continued\nModel Validation\nloss\nValidation\naccuracy\nTraining\naccuracy\nValidation\nprecision\nValidation\nrecall\nValidation\nF1-score\nTraining\nprecision\nTraining\nrecall\nTraining\nF1-score\nTransformer 0.227 93.91 94.51 87.2 88.89 87.10 92.10 91.89 92.33\nBinary-view\ninception\nNet model\n(Ours)\n0.192 94.2 99.48 90.93 89.10 91.10 97.45 96.20 97\nBinary-view\nResNet\nModel (Ours)\n0.194 94.24 99.55 91.10 90.23 91.93 96.7 95.8 97.12\nBinary-view\ninception\nNet model\n(Ours)\n0.21 94.74 99.48 92.14 92 92.5 98.4 98 98.2\nBinary-view\nTransformer\nmodel (Ours)\n0.191 95.05 99.05 93.34 92 93.4 97.6 96.8 97.2\nFigure 7:Validation accuracies of speech recognition models\nCMC, 2022, vol.72, no.3 5559\nFigure 8:Training and validation accuracies of implemented models\nFigure 9:Validation loss of speech recognition model\n5560 CMC, 2022, vol.72, no.3\n6 Conclusions\nThis research aimed to improve the speech recognition system. We analyzed human physiology\nfor speech perception. This research aimed to improve the speech recognition system. In addition,\nBinary-view transformer architecture produced state of the art results on Google’s speech command\ndataset [43,44]. Three aspects of recognition models, i.e., validation accuracy, precision, and loss,\nwere considered to determine the efficiency of binary-view transformer architecture. By introducing\na binary-view mechanism, similar data from different sources were processed, and the attention\nmechanism within the transformer increases efficiency, where the best validation accuracy of 95.16\nwas achieved. The proposed model decreased the eventuality of gradient vanishing/exploding problem\nby processing long-term dependencies. Whereas the confusion matrix showed better precision of the\nbinary-view transformer architecture compared to other models since the transformer used a multi-\nhead attention mechanism, which catches more contexts of the same data, which helped in improving\nmodel precision and the probability of model over-fitting diminish. Better precision on Google’s speech\ncommand dataset showed that our model performed better on different dialects because over 2000\nspeaker’s speech was precisely recognized. As shown inTab. 1, our model exhibited less loss of 0.191\ncompared to 0.237, 0.194, 0.192, and 0.21 of the attention based recurrent convolutional networks\n[3], binary-view resnet model, binary-view inception net model and binary-view convolutional model,\nrespectively. The binary-view transformer architecture has a lightweight footprint of 375,787 trainable\nparameters, which can be run locally on small systems.\nFunding Statement:This research was supported by Suranaree University of Technology, Thailand,\nGrant Number: BRO7-709-62-12-03.\nConflicts of Interest:The authors declare that they have no conflicts of interest to report regarding the\npresent study.\nReferences\n[1] L. Kaushik, A. Sangwan and J. H. L. Hansen, “Automatic sentiment extraction from Y ouTube videos,”\nin 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouch, Czech Republic,\npp. 239–244, 2013.\n[2] M. Aylett, Y . Vazquez-Alvarez and L. Baillie, “Evaluating speech synthesis in a mobile context: Audio\npresentation of Facebook, Twitter and RSS,” inProc. of the ITI 2013 35th Int. Conf. on Information\nTechnology Interfaces, Cavtat, Croatia, pp. 167–172, 2013.\n[3] D. C. D. Andrade, S. Leo, M. Viana and C. Bernkopf, “A neural attention model for speech command\nrecognition,” 2008. [Online]. Available: arXiv preprint arXiv:1808.08929.\n[4] B. McMahan and D. Rao, “Listening to the world improves speech command recognition,” inProc. of the\n32nd AAAI Conf. on Artificial Intelligence, New Orleans, LA, USA, pp. 209–215, 2018.\n[5] Y . Zhang, N. Suda, L. Lai and V . Chandra, “Hello edge: Keyword spotting on microcontrollers,” 2017.\n[Online]. Available: arXiv preprint arXiv:1711.07128.\n[6] M. H. Hoy, “Alexa, Siri, Cortana, and More: An introduction to voice assistants,”Medical Reference\nServices Quarterly, vol. 37, no. 1, pp. 81–88, 2018.\n[7] G. López, L. Quesada and L. A. Guerrero, “Alexavs. siri vs. cortana vs. google assistant: A comparison of\nspeech-based natural user interfaces,” inAdvances in Intelligent Systems and Computing,N e wY o r k ,U S A :\nSpringer, pp. 241–250, 2017.\n[8] M. Müller, D. Ellis, A. Klapuri and G. Richard, “Signal processing for music analysis,”IEEE Journal of\nSelected Topics in Signal Processing, vol. 5, pp. 1088–1110, 2011.\nCMC, 2022, vol.72, no.3 5561\n[9] P . Y enigalla, A. Kumar, S. Tripathi, C. Singh, S. Karet al.,“Speech emotion recognition using spectrogram\n& phoneme embedding,” inInterspeech, Hyberabad, India, pp. 3688–3692, 2018.\n[10] S. Dieleman and B. Schrauwen, “End-to-end learning for music audio,” in2014 IEEE Int. Conf. on\nAcoustics, Speech and Signal Processing (ICASSP), Florence, Italy, pp. 6964–6968, 2014.\n[11] J. Shen, R. Pang, R. Weiss, M. Schuster, N. Jaitlyet al.,“Naturaltts synthesis by conditioning wavenet\non mel spectrogram predictions,” in2018 IEEE Int. Conf. on Acoustics, Speech and Signal Processing\n(ICASSP), Alberta, Canada, pp. 4779–4783, 2018.\n[12] M. R. Schomers and F . Pulvermüller, “Is the sensorimotor cortex relevant for speech perception and\nunderstanding? An integrative review,”Frontiers in Human Neuroscience, vol. 10, pp. 435, 2016.\n[13] G. Dede and M. HüsnüSazlı, “Speech recognition with artificial neural networks,”Digital Signal Process-\ning, vol. 20, no. 3, pp. 763–768, 2010.\n[14] W . Feng, N. Guan, Y . Li, X. Zhang and Z. Luo, “Audio visual speech recognition with multimodal recurrent\nneural networks,” in2017 Int. Joint Conf. on Neural Networks (IJCNN), Ak, USA, pp. 681–688, 2017.\n[15] J. Gonzalez and W . Yu, “Non-linear system modeling using lstm neural networks,”IFAC-PapersOnLine,\nvol. 51, no. 13, pp. 485–489, 2018.\n[16] R. Dey and F . M. Salemt, “Gate-variants of gated recurrent unit (gru) neural networks,” in2017 IEEE 60th\nInt. Midwest Symp. on Circuits and Systems (MWSCAS), MA, USA, pp. 1597–1600, 2017.\n[17] S. Hershey, S. Chaudhuri, D. P . W . Ellis, J. F . Gemmeke, A. Jansenet al.,“CNN architectures for large-scale\naudio classification,” in2017 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),L A ,\nUSA, pp. 131–135, 2017.\n[18] Z. Cao, Y . Zhu, Z. Sun, M. Wang, Y . Zhenget al.,“Improving prediction accuracy in lstm network model\nfor aircraft testing flight data,” in2018 IEEE Int. Conf. on Smart Cloud (SmartCloud), NY , USA, pp. 7–12,\n2018.\n[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Joneset al.,“Attention is all you need,” inProc. of\nAdvances in Neural Information Processing Systems 30 (NIPS 2017), CA, USA, pp. 5998–6008, 2017.\n[ 2 0 ] M .G h a f o o r ,S .A .T a r i q ,T .Z i a ,I .A .T a j ,A .A b b a set al., “Fingerprint identification with shallow\nmultifeature view classifier,”IEEE Transactions on Cybernetics, vol. 51, no. 9, pp. 4515–4527, 2019.\n[21] H. Fan and J. Zhou, “Stacked latent attention for multimodal reasoning,” inProc. of the IEEE Conf. on\nComputer Vision and Pattern Recognition, PR, USA, pp. 1072–1080, 2018.\n[22] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit and Ł. Kaiser, “Universal transformers,” 2018. [Online].\nAvailable: arXiv preprint arXiv:1807.03819.\n[23] M. Junaid, M. Ghafoor, A. Hassan, S. Khalid, S. A. Tariqet al., “Multi-feature view-based shallow\nconvolutional neural network for road segmentation,”IEEE Access, vol. 8, pp. 36612–36623, 2020.\n[24] M. J. Baker, J. Trevisan, P . Bassan, R. Bhargava, H. J. Butleret al.,“Using Fourier transform ir spectroscopy\nto analyze biological materials,”Nature Protocols, vol. 9, no. 8, pp. 1771, 2014.\n[25] M. Krawczyk and T. Gerkmann, “Stft phase reconstruction in voiced speech for an improved single channel\nspeech enhancement,”IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12,\npp. 1931–1940, 2014.\n[26] Y . L. Cun, Y . Bengio and G. Hinton, “Deep learning,”Nature, vol. 521, no. 7553, pp. 436–444, 2015.\n[27] N. Dave, “Feature extraction methods lpc, plp and mfcc in speech recognition,”International Journal for\nAdvance Research in Engineering and Technology, vol. 1, no. 6, pp. 1–4, 2013.\n[28] B. Jang, W . Heo, J. Kim and O. Kwon, “Music detection from broadcast contents using convolutional neural\nnetworks with a mel-scale kernel,”EURASIP Journal on Audio, Speech, and Music Processing, vol. 11, no.\n1, pp. 11, 2019.\n[29] Q. Kong, Y . Xu, W . Wang and M. D. Plumbley, “Audio set classification with attention model: A\nprobabilistic perspective,” in2018 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),\nCalgary, Canada, pp. 316–320, 2018.\n[30] J. F . Gemmeke, D. P . Ellis, D. Freedman, A. Jansen, W . Lawrenceet al., “Audio set: An ontology and\nhuman-labeled dataset for audio events,”in2017 IEEE Int. Conf. on Acoustics, Speech and Signal Processing\n(ICASSP), LA, USA, pp. 776–780, 2017.\n5562 CMC, 2022, vol.72, no.3\n[31] J. Li, Z. Tu, B. Yang, M. R. Lyu and T. Zhang, “Multi-head attention with disagreement regularization,”\n2018. [Online]. Available: arXiv preprint arXiv:1810.10183.\n[32] C. Szegedy, S. Ioffe, V . Vanhoucke and A. A. Alemi, “Inception-v4, inception-resnet and the impact of\nresidual connections on learning,” inThirty-First AAAI Conf. on Artificial Intelligence, California, USA,\npp. 4278–4284, 2007.\n[33] Z. Wu, C. Shen and A. Van Den Hengel, “Wider or deeper: Revisiting the resnet model for visual\nrecognition,” Pattern Recognition, vol. 90, pp. 119–133, 2019.\n[34] Z. Qin, Z. Zhang, X. Chen, C. Wang and Y . Peng, “Fd-mobilenet: Improved mobilenet with a fast down\nsampling strategy,” in2018 25th IEEE Int. Conf. on Image Processing (ICIP), Athens, Greece, pp. 1363–\n1367, 2018.\n[35] A. A. Khan, C. Wechtaisong, F . A. Khan and N. Ahmad, “A Cost-efficient environment monitoring robotic\nvehivle for smart industries,”CMC-Computers, Materials & Continua, vol. 7, pp. 473–487, 2022.\n[36] N. Parmar, A. Vaswani, J. Uszkoreit, Ł. Kaiser, N. Shazeeret al.,“Image transformer,” 2018. [Online].\nAvailable: arXiv preprint arXiv:1802.05751.\n[37] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” 2016. [Online]. Available: arXiv\npreprint arXiv:1606.08415.\n[38] Z. Li, S. Wang, R. Fan, G. Cao and Y . Zhang, “Teeth category classification via seven-layer deep\nconvolutional neural network with max pooling and global average pooling,”International Journal of\nImaging Systems and Technology, vol. 29, no. 4, pp. 577–583, 2019.\n[39] A. A. Khan, P . Uthansakul, P . Duangmanee and M. Uthansakul, “Energy efficient design of massive\nMIMO by considering the effects of nonlinear amplifiers,”Energies, vol. 11, pp. 1045, 2018.\n[40] P . Uthansakul and A. A. Khan, “Enhancing the energy efficiency of mmWave massive MIMO by modifying\nthe RF circuit configuration,”Energies, vol. 12, pp. 4356, 2019.\n[41] P . Uthansakul and A. A. Khan, “On the energy efficiency of millimeter wave massive MIMO based on\nhybrid architecture,”Energies, vol. 12, pp. 2227, 2019.\n[42] A. A. Khan, P . Uthansakul and M. Uthansakul, “Energy efficient design of massive MIMO by incorporat-\ning with mutual coupling,”International Journal on Communication Antenna and Propagation,v o l .7 ,p p .\n198–207, 2017.\n[43] P . Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” 2018. [Online].\nAvailable: arXivpreprint. arXiv:1804.03209.\n[44] S. N. Atluri and S. Shen, “Global weak forms, weighted residuals, finite elements, boundary elements &\nlocal weak forms,” inThe Meshless Local Petrov-Galerkin (MLPG) Method, 1st ed., vol. 1. Henderson,\nNV , USA: Tech Science Press, pp. 15–64, 2004.",
  "topic": "Spectrogram",
  "concepts": [
    {
      "name": "Spectrogram",
      "score": 0.8206562995910645
    },
    {
      "name": "Computer science",
      "score": 0.7870756387710571
    },
    {
      "name": "Transformer",
      "score": 0.689856767654419
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6842687726020813
    },
    {
      "name": "Speech recognition",
      "score": 0.6306577920913696
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5487625002861023
    },
    {
      "name": "Binary classification",
      "score": 0.5077461004257202
    },
    {
      "name": "Binary number",
      "score": 0.4832364022731781
    },
    {
      "name": "Deep learning",
      "score": 0.4653843939304352
    },
    {
      "name": "Artificial neural network",
      "score": 0.4161611497402191
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4061979651451111
    },
    {
      "name": "Engineering",
      "score": 0.0853031575679779
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Support vector machine",
      "score": 0.0
    },
    {
      "name": "Arithmetic",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}