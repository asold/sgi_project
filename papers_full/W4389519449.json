{
  "title": "The Internal State of an LLM Knows When It’s Lying",
  "url": "https://openalex.org/W4389519449",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1988279427",
      "name": "Amos Azaria",
      "affiliations": [
        "Ariel University"
      ]
    },
    {
      "id": "https://openalex.org/A2120740404",
      "name": "Tom Mitchell",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3156158944",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4232704105",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4310419543",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W4310926773",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4385570703",
    "https://openalex.org/W4385495736",
    "https://openalex.org/W4313445468",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4322718421"
  ],
  "abstract": "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71% to 83% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 967–976\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nThe Internal State of an LLM Knows When It’s Lying\nAmos Azaria\nSchool of Computer Science,\nAriel University, Israel\nTom Mitchell\nMachine Learning Dept.,\nCarnegie Mellon University, Pittsburgh, PA\nAbstract\nWhile Large Language Models (LLMs) have\nshown exceptional performance in various\ntasks, one of their most prominent drawbacks\nis generating inaccurate or false information\nwith a confident tone. In this paper, we provide\nevidence that the LLM’s internal state can be\nused to reveal the truthfulness of statements.\nThis includes both statements provided to the\nLLM, and statements that the LLM itself gen-\nerates. Our approach is to train a classifier that\noutputs the probability that a statement is truth-\nful, based on the hidden layer activations of\nthe LLM as it reads or generates the statement.\nExperiments demonstrate that given a set of\ntest sentences, of which half are true and half\nfalse, our trained classifier achieves an average\nof 71% to 83% accuracy labeling which sen-\ntences are true versus false, depending on the\nLLM base model. Furthermore, we explore\nthe relationship between our classifier’s perfor-\nmance and approaches based on the probability\nassigned to the sentence by the LLM. We show\nthat while LLM-assigned sentence probability\nis related to sentence truthfulness, this prob-\nability is also dependent on sentence length\nand the frequencies of words in the sentence,\nresulting in our trained classifier providing a\nmore reliable approach to detecting truthful-\nness, highlighting its potential to enhance the\nreliability of LLM-generated content and its\npractical applicability in real-world scenarios.\n1 Introduction\nLarge Language Models (LLMs) have recently\ndemonstrated remarkable success in a broad range\nof tasks (Brown et al., 2020; Bommarito II and\nKatz, 2022; Driess et al., 2023; Bubeck et al., 2023).\nHowever, when composing a response, LLMs tend\nto hallucinate facts and provide inaccurate infor-\nmation (Ji et al., 2023). Furthermore, they seem to\nprovide this incorrect information using confident\nand compelling language. The combination of a\nbroad body of knowledge, along with the provision\nPluto is the\nsecond largest smallest\ndwarf planetin our solarsystem.\ncelestialbody in thesolar systemthat hasever beenclassifiedas a planet.\nonly most\nFigure 1: A tree diagram that demonstrates how gener-\nating words one at a time and committing to them may\nresult in generating inaccurate information.\nof confident but incorrect information, may cause\nsignificant harm, as people may accept the LLM as\na knowledgeable source, and fall for its confident\nand compelling language, even when providing\nfalse information.\nWe believe that in order to perform well, an\nLLM must have some internal notion as to whether\na sentence is true or false, as this information is\nrequired for generating (or predicting) following\ntokens. For example, consider an LLM generating\nthe following false information “The sun orbits the\nEarth.\" After stating this incorrect fact, the LLM\nis more likely to attempt to correct itself by saying\nthat this is a misconception from the past. But after\nstating a true fact, for example “The Earth orbits\nthe sun,\" it is more likely to focus on other planets\nthat orbit the sun. Therefore, we hypothesize that\nthe truth or falsehood of a statement should be\nrepresented by, and therefore extractable from, the\nLLM’s internal state.\nInterestingly, retrospectively “understanding”\nthat a statement that an LLM has just generated\nis false does not entail that the LLM will not gener-\nate it in the first place. We identify three reasons for\nsuch behavior. The first reason is that an LLM gen-\nerates a token at a time, and it “commits” to each\n967\ntoken generated. Therefore, even if maximizing the\nlikelihood of each token given the previous tokens,\nthe overall likelihood of the complete statement\nmay be low. For example, consider a statement\nabout Pluto. The statement begins with the com-\nmon words \"Pluto is the\", then, since Pluto used\nto be the smallest planet the word \"smallest\" may\nbe a very plausible choice. Once the sentence is\n\"Pluto is the smallest\", completing it correctly is\nvery challenging, and when prompted to complete\nthe sentence, GPT-4 (March 23rd version) com-\npletes it incorrectly: “Pluto is the smallest dwarf\nplanet in our solar system.” In fact, Pluto is the sec-\nond largest dwarf planet in our solar system (after\nEris). One plausible completion of the sentence\ncorrectly is “Pluto is the smallest celestial body\nin the solar system that has ever been classified\nas a planet.” (see Figure 1). Consider the follow-\ning additional example: “Tiztoutine is a town in\nAfrica located in the republic of Niger.” Indeed,\nTiztoutine is a town in Africa, and many countries’\nnames in Africa begin with “the republic of”. How-\never, Tiztoutine is located in Morocco, which is\nnot a republic, so once the LLM commits to “the\nrepublic of”, it cannot complete the sentence using\n“Morocco\", but completes it with “Niger\". In addi-\ntion, committing to a word at a time may lead the\nLLM to be required to complete a sentence that it\nsimply does not know how to complete. For exam-\nple, when describing a city, it may predict that the\nnext words should describe the city’s population.\nTherefore, it may include in a sentence \"Tiztou-\ntine has a population of\", but the population size is\nnot present in the dataset, so it must complete the\nsentence with a pure guess.\nThe second reason for an LLM to provide false\ninformation is that at times, there may be many\nways to complete a sentence correctly, but fewer\nways to complete it incorrectly. Therefore, it might\nbe that a single incorrect completion may have a\nhigher likelihood than any of the correct comple-\ntions (when considered separately).\nFinally, since it is common for an LLM to not\nuse the maximal probability for the next word, but\nto sample according to the distribution over the\nwords, it may sample words that result in false\ninformation.\nIn this paper we present our Statement Accuracy\nPrediction, based on Language Model Activations\n(SAPLMA). SAPLMA is a simple yet powerful\nmethod for detecting whether a statement generated\nby an LLM is truthful or not. Namely, we build a\nclassifier that receives as input the activation values\nof the hidden layers of an LLM. The classifier de-\ntermines for each statement generated by the LLM\nif it is true or false. Importantly, the classifier is\ntrained on out-of-distribution data, which allows us\nto focus specifically on whether the LLM has an\ninternal representation of a statement being true or\nfalse, regardless of the statement’s topic.\nIn order to train SAPLMA we created a dataset\nof true and false statements from 6 different top-\nics. Each statement is fed to the LLM, and its\nhidden layers’ values are recorded. The classifier\nis then trained to predict whether a statement is\ntrue or false only based on the hidden layer’s val-\nues. Importantly, our classifier is not tested on\nthe topics it is trained, but on a held-out topic.\nWe believe that this is an important measure, as\nit requires SAPLMA to extract the LLM’s inter-\nnal belief, rather than learning how information\nmust be aligned to be classified as true. We show\nthat SAPLMA, which leverages the LLM’s internal\nstates, results in a better performance than prompt-\ning the LLM to explicitly state whether a statement\nis true or false. Specifically, SAPLMA reaches ac-\ncuracy levels of between 60% to 80% on specific\ntopics, while few-shot prompting achieves only\nslightly above random performance, with no more\nthan a 56% accuracy level.\nOf course there will be some relationship be-\ntween the truth/falsehood of a sentence, and the\nprobability assigned to that sentence by a well-\ntrained LLM. But the probability assigned by an\nLLM to a given statement depends heavily on the\nfrequency of the tokens in the statement as well as\nits length. Therefore, sentence probabilities pro-\nvide only a weak signal of the truth/falsehood of the\nsentence. At minimum, they must be normalized\nto become a useful signal of the veracity of a state-\nment. As we discuss later, SAPLMA classifications\nof truth/falsehood significantly outperform simple\nsentence probability. In one test described later in\nmore detail, we show that SAPLMA performs well\non a set of LLM-generated sentences that contain\n50% true, and 50% false statements. We further dis-\ncuss the relationship between statement probability\nand veracity in the discussion section.\nSAPLMA employs a simple and relatively shal-\nlow feedforward neural network as its classifier,\nwhich requires very little computational power at\ninference time. Therefore, it can be computed\n968\nalongside the LLM’s output. We propose for\nSAPLMA to supplement an LLM presenting in-\nformation to users. If SAPLMA detects that the\nLLM “believes” that a statement that it has just\ngenerated is false, the LLM can mark it as such.\nThis could raise human trust in the LLM responses.\nAlternatively, the LLM may merely delete the in-\ncorrect statement and generate a new one instead.\nTo summarize, the contribution of this paper is\ntwofold.\n• The release of a dataset of true-false state-\nments along with a method for generating\nsuch information.\n• Demonstrating that an LLM might “know”\nwhen a statement that it has just generated\nis false, and proposing SAPLMA, a method\nfor extracting this information.\n2 Related Work\nIn this section we provide an overview of previ-\nous research on LLM hallucination, accuracy, and\nmethods for detecting false information, and we\ndiscuss datasets used to that end.\nMany works have focused on hallucination in\nmachine translation (Dale et al., 2022; Ji et al.,\n2023). For example, Dale et al. (Dale et al.,\n2022) consider hallucinations as translations that\nare detached from the source, hence they propose a\nmethod that evaluates the percentage of the source\ncontribution to a generated translation. If this con-\ntribution is low, they assume that the translation is\ndetached from the source and is thus considered to\nbe hallucinated. Their method improves detection\naccuracy hallucinations. The authors also propose\nto use multilingual embeddings, and compare the\nsimilarity between the embeddings of the source\nsentence and the target sentence. If this similarity\nis low, the target sentence is considered to be hal-\nlucinated. The authors show that the latter method\nworks better. However, their approach is very dif-\nferent than ours, as we do not assume any pair of\nsource and target sentences. In addition, while we\nalso use the internal states of the model, we do so\nby using the hidden states to descriminate between\nstatements that the LLM “believes” are true and\nthose that are false. Furtheremore, we focus on\ndetecting false statements rather than hallucination,\nas defined by their work.\nOther works have focused on hallucination in\ntext summarization (Pagnoni et al., 2021). Pagnoni\net al. propose a benchmark for factuality metrics of\ntext summarization. Their benchmark is developed\nby gathering summaries from several summariza-\ntion methods and requested humans to annotate\ntheir errors. The authors analyze these anotation\nand the proportion of the different factual error of\nthe summarization methods. We note that most\nworks that consider hallucination do so with rela-\ntion to a given input (e.g., a passage) that the model\noperates on. For example, a summarization model\nthat outputs information that does not appear in the\nprovided article, is considered hallucinate it, regard-\nless if the information is factually true. However,\nin this work we consider a different problem—-the\nveracity of the output of an LLM, without respect\nto a specific input.\nSome methods for reducing hallucination as-\nsume that the LLM is a black box (Peng et al.,\n2023). This approach uses different methods for\nprompting the LLM, possibly by posting multiple\nqueries for achieving better performance. Some\nmethods that can be used for detecting false state-\nments may include repeated queries and measuring\nthe discrepancy among them. We note that instead\nof asking the LLM to answer the same query mul-\ntiple times, it is possible to request the LLM to\nrephrase the query (without changing its meaning\nor any possible answer) and then asking it to an-\nswer each rephrased question.\nOther methods finetune the LLM, using human\nfeedback, reinforcement learning, or both (Bakker\net al., 2022; Ouyang et al., 2022). Ouyang et al.\npropose a method to improve LLM-generated con-\ntent using reinforcement learning from human feed-\nback. Their approach focuses on fine-tuning the\nLLM with a reward model based on human judg-\nments, aiming to encourage the generation of bet-\nter content. However, fine tuning, in general, may\ncause a model to not perform as well on other tasks\n(Kirkpatrick et al., 2017). In this paper, we take an\nintermediate approach, that is, we assume access to\nthe model parameters, but do not fine-tune or mod-\nify them. Another approach that can be applied to\nour settings is presented by (Burns et al., 2022),\nnamed Contrast-Consistent Search (CCS). How-\never, CCS requires rephrasing a statement into a\nquestion, evaluating the LLM on two different ver-\nsion of the prompt, and requires training data from\nthe same dataset (topic) as the test set. These limi-\ntations render it unsuitable for running in practice\non statements generated by an LLM. In addition,\nCCS increases the accuracy by only approximately\n969\n4% over the 0-shot LLM query, while our approach\ndemonstrates a nearly 20% increase over the 0-shot\nLLM\nA dataset commonly used for training and fine-\ntuning LLMs is the Wizard-of-Wikipedia (Dinan\net al., 2018). The Wizard-of-Wikipedia dataset in-\ncludes interactions between a human apprentice\nand a human wizard. The human wizard receives\nrelevant Wikipedia articles, which should be used\nto select a relevant sentence and compose the re-\nsponse. The goal is to replace the wizard with a\nlearned agent (such as an LLM). Another highly\nrelevant dataset is FEVER (Thorne et al., 2018,\n2019). The FEVER dataset is designed for devel-\noping models that receive as input a claim and a\npassage, and must determine whether the passage\nsupports the claim, refutes it, or does not provide\nenough information to support or refute it. While\nthe FEVER dataset is highly relevant, it does not\nprovide simple sentence that are clearly true or\nfalse independently of a provided passage. In ad-\ndition, the FEVER dataset is not partitioned into\ndifferent topics as the true-false dataset provided in\nthis paper.\nIn conclusion, while several approaches have\nbeen proposed to address the problem of halluci-\nnation and inaccuracy in automatically generated\ncontent, our work is unique in its focus on utilizing\nthe LLM’s hidden layer activations to determine\nthe veracity of generated statements. Our method\noffers the potential for more general applicability in\nreal-world scenarios, operating alongside an LLM,\nwithout the need for fine-tuning or task-specific\nmodifications.\n3 The True-False Dataset\nThe work presented in this paper requires a dataset\nof true and false statements. These statements must\nhave a clear true or false label, and must be based\non information present in the LLM’s training data.\nFurthermore, since our approach intends to reveal\nthat the hidden states of an LLM have a notion of\na statement being true or false, the dataset must\ncover several disjoint topics, such that a classifier\ncan be trained on the LLM’s activations of some\ntopics while being tested on another. Unfortunately,\nwe could not find any such dataset and therefore,\ncompose the true-false dataset.\nOur true-false dataset covers the following top-\nics: “Cities\", “Inventions\", “Chemical Elements\",\n“Animals\", “Companies\", and “Scientific Facts\".\nFor the first 5 topics, we used the following method\nto compose the dataset. We used a reliable source1\nthat included a table with several properties for\neach instance. For example, for the “chemical el-\nements\" we used a table that included, for each\nelement, its name, atomic number, symbol, stan-\ndard state, group block, and a unique property (e.g.,\nHydrogen, 1, H, Gas, Nonmetal, the most abun-\ndant element in the universe). For each element we\ncomposed true statement using the element name\nand one of its properties (e.g., “The atomic number\nof Hydrogen is 1”). Then, we randomly selected a\ndifferent row for composing a false statement (e.g.,\n“The atomic number of Hydrogen is 34”). If the\nvalue in the different row is identical to the value in\nthe current row, we resample a different row until\nwe obtain a value that is different. This process\nwas repeated for the all topics except the “Scien-\ntific Facts”. For the “Scientific Facts” topic, we\nasked ChatGPT (Feb 13 version) to provide “sci-\nentific facts that are well known to humans” (e.g.\n“The sky is often cloudy when it’s going to rain”).\nWe then asked ChatGPT to provide the opposite of\neach statement such that it becomes a false state-\nment (e.g., “The sky is often clear when it’s going\nto rain”). The statements provided by ChatGPT\nwere manually curated, and verified by two hu-\nman annotators. The classification of 48 facts were\nquestioned by at least one of the annotators; these\nfacts were removed from the dataset. The true-\nfalse dataset comprises 6,084 sentences, including\n1,458 sentences for “Cities\", 876 for “Inventions\",\n930 for “Chemical Elements\", 1,008 for “Animals\",\n1,200 for “Companies\", and 612 for “Scientific\nFacts\". The following are some examples of true\nstatements from the dataset:\n• Cities: “Oranjestad is a city in Aruba”\n• Inventions: “Grace Hopper invented the\nCOBOL programming language”\n• Animals: “The llama has a diet of herbivore”\n• Companies: “Meta Platforms has headquar-\nters in United States”\n• Scientific Facts: “The Earth’s tides are pri-\nmarily caused by the gravitational pull of the\nmoon”\nThe following are some examples of false state-\nments from the dataset:\n1Cities: Downloaded from simplemaps. Inventions: Ob-\ntained from Wikipedia list of inventors. Chemical Elements:\nDownloaded from pubchem ncbi nlm nih gov periodic-table.\nAnimals: Obtained from kids national geographics. Compa-\nnies: Forbes Global 2000 List 2022: The Top 200.\n970\n• Chemical Elements: “Indium is in the Lan-\nthanide group”\n• Animals: “The whale has a long, tubular\nsnout, large ears, and a powerful digging abil-\nity to locate and consume termites and ants.”\n• Scientific Facts: “Ice sinks in water due to its\nhigher density”\nOther candidates for topics to be added to the\ndataset include sports, celebrities, and movies. The\ntrue-false dataset is available at: azariaa.com/\nContent/Datasets/true-false-dataset.zip.\n4 SAPLMA\nIn this section, we present our Statement Accuracy\nPrediction, based on Language Model Activations\n(SAPLMA), a method designed to determine the\ntruthfulness of statements generated by an LLM\nusing the values in its hidden layers. Our general\nhypothesis is that the values in the hidden layers of\nan LLM contain information on whether the LLM\n“believes” that a statement is true or false. However,\nit is unclear which layer should be the best candi-\ndate for retaining such information. While the last\nhidden layer should contain such information, it\nis primarily focused on generating the next token.\nConversely, layers closer to the input are likely fo-\ncused on extracting lower-level information from\nthe input. Therefore, we use several hidden layers\nas candidates. We use two different LLMs: Face-\nbook OPT-6.7b (Zhang et al., 2022) and LLAMA2-\n7b (Roumeliotis et al., 2023); both composed of\n32 layers. For each LLM, we compose five differ-\nent models, each using activations from a different\nlayer. Namely, we use the last hidden layer, the\n28th layer (which is the 4th before last), the 24th\nlayer (which is the 8th before last), the 20th layer\n(which is the 12th before last), and the middle layer\n(which is the 16th layer). We note that each layer\nis composed of 4096 hidden units.\nSAPLMA’s classifier employs a feedforward\nneural network, featuring three hidden layers with\ndecreasing numbers of hidden units (256, 128, 64),\nall utilizing ReLU activations. The final layer is a\nsigmoid output. We use the Adam optimizer. We\ndo not fine-tune any of these hyper-parameters for\nthis task. The classifier is trained for 5 epochs.\nFor each topic in the true-false dataset, we train\nthe classifier using only the activation values ob-\ntained from all other topics and test its accuracy on\nthe current topic. This way, the classifier is required\nto determine which sentences the LLM “believes”\nare true and which it “believes” are false, in a gen-\neral setting, and not specifically with respect to the\ntopic being tested. To obtain more reliable results,\nwe train each classifier three times with different\ninitial random weights. This process is repeated for\neach topic, and we report the accuracy mean over\nthese three runs.\n5 Results\nWe compare the performance of SAPLMA against\nthree different baselines. The first is BERT, for\nwhich we train a classifier (with an identical archi-\ntecture to the one used by SAPLMA) on the BERT\nembeddings of each sentence. Our second baseline\nis a few shot-learner using OPT-6.7b. This base-\nline is an attempt to reveal whether the LLM itself\n“knows” whether a statement is true or false. Un-\nfortunately, any attempts to explicitly prompt the\nLLM in a ‘zero-shot” manner to determine whether\na statement is true or false completely failed with\naccuracy levels not going beyond 52%. Therefore,\nwe use a few shot-query instead, which provided\nthe LLM with truth values from the same topic\nit is being tested on. Note that this is very dif-\nferent from our methodology, but was necessary\nfor obtaining some results. Namely, we provided\nfew statements along with the ground-truth label,\nand then the statement in question. We recorded\nthe probability that the LLM assigned to the token\n“true” and to the token “false”. Unfortunately, the\nLLM had a tendency to assign higher values to the\n“true” token; therefore, we divided the probability\nassigned to the “true” token by the one assigned\nto the “false” token. Finally, we considered the\nLLM’s prediction “true” if the value was greater\nthan the average, and “false” otherwise. We tested\na 3-shot and a 5-shot version. The third baseline,\ngiven a statement ‘X’, we measure the probabilities\n(using OPT-6.7) of the sentences “It is true that X”,\nand “It is false that X”, and pick the higher proba-\nbility (considering only the probabilities for X, not\nthe added words). This normalizes the length and\nfrequency factors.\nTable 1 and Figure 2 present the accuracy of\nall the models tested using the OPT-6.7b LLM,\nfor each of the topics, along with the average accu-\nracy. As depicted by the table and figure, SAPLMA\nclearly outperforms BERT and Few-shot learning,\nwith BERT, 3-shot, and 5-shot learning achiev-\ning only slightly above a random guess (0.50). It\ncan also be observed that SAPLMA for OPT-6.7b\n971\nModel Cities Invent. Elements Animals Comp. FactsAverage\nlast-layer0.7796 0.5696 0.5760 0.6022 0.6925 0.64980.644928th-layer0.7732 0.5761 0.5907 0.5777 0.7247 0.66180.650724th-layer0.7963 0.67120.62110.5800 0.77580.68680.688620th-layer0.8125 0.72680.61970.6058 0.81220.68190.7098middle-layer0.7435 0.6400 0.5645 0.5800 0.7570 0.62370.6515\nBERT 0.5357 0.5537 0.5645 0.5228 0.5533 0.53020.54343-shot 0.5410 0.4799 0.5685 0.5650 0.5538 0.51640.53745-shot 0.5416 0.4799 0.5676 0.5643 0.5540 0.51480.5370It-is-true 0.523 0.5068 0.5688 0.4851 0.6883 0.5840.5593\nTable 1: Accuracy classifying truthfulness of externally\ngenerated sentences during reading. The table shows\naccuracy of all the models tested for each of the topics,\nand average accuracy using OPT-6.7b as the LLM.\nCitiesInvent.ElementsAnimalsComp.FactsAverage\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAccuracy\nSAPLMAIt-is-trueBERT3-shot5-shot\nFigure 2: A bar-chart comparing the accuracy of\nSAPLMA (20th-layer), BERT, 3-shot, 5-shot, and It-\nis-true on the 6 topics, and the average. SAPLMA con-\nsistently outperforms other models across all categories.\nSince the data is balanced, a random classifier should\nobtain an accuracy of 0.5.\nseems to work best when using the 20th layer (out\nof 32). Recall that each model was trained three\ntimes. We note that the standard deviation between\nthe accuracy among the different runs was very\nsmall; therefore, the differences in performance\nbetween the different layers seem consistent. How-\never, the optimal layer to be used for SAPLMA is\nvery likely to depend on the LLM.\nWe note that the average training accuracy for\nSAPLMA (using OPT-6.7b’s 20th layer) is 86.4%.\nWe believe that this relatively high value may indi-\ncate once again that the veracity of a statement is in-\nherit to the LLM. To demonstrate this, we run a test\nwhere the true/false labels are randomly permuted.\nThe average accuracy on the random training data\nis only 62.5%. This indicates that our model does\nnot have the capacity to completely over-fit the\ntraining data, and thus, must exploit structure and\npatterns that appear in the data.\nAdditionally, Table 2 presents the performance\nof SAPLMA using LLAMA2-7b. As expected,\nSAPLMA using LLAMA2-7b achieves much bet-\nter performance. Interestingly, for LLAMA2-7b,\nthe middle layer performs best.\nModel Cities Invent. Elements Animals Comp. FactsAverage\nlast-layer0.7574 0.6735 0.6814 0.7338 0.6736 0.74440.710728th-layer0.8146 0.7207 0.6767 0.7249 0.6894 0.76620.732124th-layer0.8722 0.7816 0.6849 0.7394 0.7094 0.78580.762220th-layer0.8820 0.8459 0.6950 0.7758 0.8319 0.80530.806016th-layer0.9223 0.8938 0.6939 0.7774 0.8658 0.82540.8298\nTable 2: Accuracy classifying truthfulness of externally\ngenerated sentences using SAPLMA with LLAMA2-7b.\nThe table shows accuracy of all the models tested for\neach of the topics, and the average accuracy.\nAs for the differences between the topics, we\nbelieve that these values depend very much on the\ntraining data of the LLM. That is, we believe that\nthe data used for training OPT-6.7b and LLAMA2-\n7b includes information or stories about many cities\nand companies, and not as much on chemical ele-\nments and animals (other than the very common\nones). Therefore, we conjecture that is the reason\nSAPLMA achieves high accuracy for the “cities”\ntopic (while trained on all the rest) and the “com-\npanies” topic, but achieves much lower accuracy\nwhen tested on the “animals” and “elements” top-\nics.\nIn addition to the topics from the true-false\ndataset, we also created a second data set of state-\nments generated by the LLM itself (the OPT-6.7b\nmodel). For generating statements, we provided\na true statement not present in the dataset, and al-\nlowed the LLM to generate a following statement.\nWe first filtered out any statements that were not\nfactual statements (e.g., “I’m not familiar with the\nJapanese versions of the games.”). All statements\nwere generated using the most probable next word\nat each step, i.e., we did not use sampling. This\nresulted in 245 labeled statements. The statements\nwere fact-checked and manually labeled by three\nhuman judges based on web-searches. The human\njudges had a very high average observed agree-\nment of 97.82%, and an average Cohen’s Kappa of\n0.9566. The majority determined the ground-truth\nlabel for each statement. 48.6% of the statements\nwere labeled as true, resulting in a balanced dataset.\nEach of the models was trained 14 times using\nthe same classifier described in Section 4. The\nmodels were trained on the entire true-false dataset\n(i.e., all topics, but not the generated sentences) and\ntested on the generated sentences.\nTable 3 presents the average accuracy of all mod-\nels on the sentences generated by the LLM. As\nanticipated, SAPLMA (using OPT-6.7b) clearly\noutperforms the baselines, which appear to be en-\ntirely ineffectual in this task, achieving an accuracy\n972\nModel Accuracy AUC\nlast-layer 0.6187 0.7587\n28th-layer 0.6362 0.7614\n24th-layer 0.6134 0.7435\n20th-layer 0.6029 0.7182\nmiddle-layer 0.5566 0.6610\nBERT 0.5115 0.5989\n3-shot 0.5041 0.4845\n5-shot 0.5125 0.4822\nTable 3: Accuracy classifying truthfulness of sentences\ngenerated by the LLM (OPT-6.7b) itself.\nnear 50%. However, the accuracy of SAPLMA on\nthese sentences is not as promising as the accuracy\nachieved when tested on some of the topics in the\ntrue-false dataset (i.e., the cities and companies).\nSince we expected the LLM to generate sentences\nthat are more aligned with the data it was trained\non, we did expect SAPLMA’s performance on the\ngenerated sentences to be closer to its performance\non topics such as cities and companies, which are\nlikely aligned with the data the LLM was trained\non. However, there may be also a counter-effect in\nplay: the sentences in the true-false dataset were\nmostly generated using a specific pattern (except\nthe scientific facts topic), such that each sentence is\nclearly either true or false. However, the sentences\ngenerated by the LLM where much more open,\nand their truth value may be less clearly defined\n(despite being agreed upon by the human judges).\nFor example, one of the sentences classified by all\nhuman judges as false is “Lima gets an average of\n1 hour of sunshine per day.” However, this sen-\ntence is true during the winter. Another example is\n“Brink is a river,” which was also classified as false\nby all three human judges; however, brink refers to\nriver bank (but is not a name of a specific river, and\ndoes not mean river). Indeed, SAPLMA classified\napproximately 70% of the sentences as true, and the\nAUC values seem more promising. This may hint\nthat any sentence that seems plausible is classified\nas true. Therefore, we evaluate the models using\n30% of the generated sentences for determining\nwhich threshold to use, i.e., any prediction above\nthe threshold is considered true. Importantly, we\ndo not use this validation set for any other goal. We\ntest the models on the remaining 70% of the gen-\nerated sentences. We do not evaluate the few shot\nmodels again, as our evaluation guaranteed that the\nnumber of positive predictions would match the\nnumber of negative predictions, which matches the\ndistribution in of the data.\nModel Avg Threshold Accuracy\nlast-layer 0.8687 0.7052\n28th-layer 0.8838 0.7134\n24th-layer 0.8801 0.6988\n20th-layer 0.9063 0.6587\nmiddle-layer 0.8123 0.650\nBERT 0.9403 0.5705\nTable 4: Accuracy classifying truthfulness of sentences\ngenerated by the LLM itself. Unlike the data in Table\n3, where a threshold of 0.5 on the classifier output was\nused to classify sentences as true or false, in this ta-\nble the results were obtained by estimating the optimal\nthreshold from a held-out validation data set (30% of\nthe original test-set).\nTable 4 presents the accuracy of all models when\nusing the optimal threshold from the validation set.\nClearly, SAPLMA performs better with a higher\nthreshold. This somewhat confirms our assumption\nthat the truth value of the sentences generated by\nthe LLM is more subjective than those that appear\nin the true-false dataset. We note that also the\nBERT embeddings perform better with a higher\nthreshold. The use of a higher threshold can also\nbe justified by the notion that it is better to delete\nor to mark as unsure a sentence that is actually true,\nthan to promote false information.\nAnother interesting observation is that the 20th-\nlayer no longer performs best for the statements\ngenerated by the LLM, but the 28th layer seems to\nperform best. This is somewhat perplexing and we\ndo not have a good guess as to why this might be\nhappening. Nevertheless, we stress that the differ-\nences between the accuracy levels of the 28th-layer\nand the others are statistically significant (using a\ntwo-tailed student t-test; p <0.05). In future work\nwe will consider fusing multiple layers together,\nand using the intermediate activation values out-\nputted by the LLM for all the words appearing in\nthe statement (rather than using only the LLM’s\noutput for the final word).\nWe also ran the statements generated by the\nOPT 6.7b model on GPT-4 (March 23rd version),\nprompting it to determine whether each statement\nwas true or false. Specifically, we provided the\nfollowing prompt “Copy the following statements\nand for each statement write true if it is true and\n973\nfalse if it is false:”, and fed it 30 statements at a\ntime. It achieved an accuracy level of 84.4%, and a\nCohen’s Kappa agreement level of 0.6883 with the\ntrue label.\n6 Discussion\nIn this work we explicitly do not consider models\nthat were trained or fine-tuned on data from the\nsame topic of the test-set. This is particularly im-\nportant for the sentences generated by the LLM, as\ntraining on a held-out set from them would allow\nthe classifier to learn which type of sentences gen-\nerated by the LLM are generally true, and which\nare false. While this information may be useful\nin practice, and its usage is likely to yield much\nhigher accuracy, it deviates from this paper’s focus.\nWe note that the probability of the entire sen-\ntence (computed by multiplying the conditional\nprobabilities of each word, given the previous\nwords) cannot be directly translated to a truth value\nfor the sentence, as many words are more common\nthan others. Therefore, while sentence probabilities\nmay be useful to determine which of two similar\nsentences is true, they cannot be used alone for the\ngeneral purpose of determining the truthfulness of\na given sentence.\nIn Table 5 we compare the probability assigned\nby the LLM and the sigmoid output from SAPLMA\non 14 statements, which do not appear in the true-\nfalse dataset. We use the 28-layer, as it proved\nto perform best on the statements generated by\nthe LLM, but we note that other layers provide\nvery similar results on this set of statements. As\ndepicted by the table, the probabilities provided\nby the LLM are highly susceptible to the syntax,\ni.e., the exact words and the statement’s length.\nThe first two sets of examples in the table illustrate\nhow sentence length highly affects the probabilities,\nbut not SPLMA. In the following examples the\nfalse statements are not necessarily shorter than the\ntrue statements, yet the probabilities remain highly\nunreliable, while SPLMA generally succeeds in\nmaking accurate predictions.\nThe statement “The Earth is flat” as well as “The\nEarth is flat like a pancake” probably appear several\ntimes in the LLM’s training data, therefore, it has a\nrelatively high probability; however, SAPLMA is\nnot baffled by it and is almost certain that both sen-\ntences are false. A basketeer is a rare word meaning\na basketball player. It seems that the LLM is not\nfamiliar with the phrase and assigns it a low proba-\nbility. However, while SAPLMA still classifies the\nstatement “Kevin Durant is basketeer” as false, it’s\nstill much more confident that Kevin Durant is not\na baseball player, in contrast to the probabilities.\nSince the statement “Kevin Duarnt is basketball\nplayer” has a typo, its probability is extremely low,\nbut SAPLMA still classifies the statement as true.\n“Jennifer Aniston is a female person” is an im-\nplicit truth, a fact that is universally acknowledged\nwithout needing to be explicitly stated; thus, it is\nunlikely to be mentioned in the LLM’s training data.\nTherefore, its probability is very low—much lower\nthan “Jennifer Aniston is not an actress”—despite\nhaving the same number of words. Nevertheless,\nSAPLMA classifies it correctly, albeit not with very\nhigh confidence.\nWhile we show that the probabilities cannot be\nused alone to determine the veracity of a statement,\nthey are not useless and do convey important infor-\nmation. Therefore, in future work we will consider\nproviding SAPLMA with the probabilities of the\ngenerated words; however, this information may be\nredundant, especially if SAPLMA uses the interme-\ndiate activation values for all the words appearing\nin the statement.\nStatement Label Probability SAPLMA(28th-layer)\nH2O is water, which is essential for humans True 6.64E-16 0.9032Humans don’t need water False 2.65E-10 0.0282\nThe sun is hot, and it radiates its heat to Earth True 1.01E-17 0.9620The sun protects Earth from heat False 2.03E-14 0.3751\nThe Earth is flat False 5.27E-07 0.0342The world is round and rotates True 2.96E-11 0.6191The Earth is flat like a pancake False 3.88E-10 0.0097\nKevin Durant is a basketball player True 2.89E-10 0.9883Kevin Durant is a baseball player False 4.56E-12 0.0001Kevin Durant is a basketeer True 5.78E-16 0.0469Kevin Duarnt is a basketball player True 1.52E-21 0.7105\nJennifer Aniston is an actress True 1.88E-10 0.9985Jennifer Aniston is not an actress False 1.14E-11 0.0831Jennifer Aniston is a female person True 2.78E-14 0.6433\nHarry Potter is real False 9.46E-09 0.0016Harry Potter is fictional True 1.53E-09 0.9256Harry Potter is an imaginary figure True 6.31E-14 0.8354\nTable 5: Comparison of the probability assigned by\nthe LLM and the sigmoid output from SAPLMA’s\n28th layer (using OPT-6.7b), color-coded for clarity.\nSAPLMA’s values are much better aligned with the\ntruth value.\n7 Conclusions & Future Work\nIn this paper, we tackle a fundamental problem\nassociated with LLMs, i.e., the generation of in-\ncorrect and false information. We have introduced\nSAPLMA, a method that leverages the hidden layer\nactivations of an LLM to predict the truthfulness\n974\nof generated statements. We demonstrated that\nSAPLMA outperforms few-shot prompting in de-\ntecting whether a statement is true or false, achiev-\ning accuracy levels between 60% to 80% on spe-\ncific topics when using OPT-6.7b, and between\n70% to 90% when using LLAMA2-7b. This is a\nsignificant improvement over the maximum 56%\naccuracy level achieved by few-shot prompting (for\nOPT-6.7b).\nOur findings suggest that LLMs possess an in-\nternal representation of statement accuracy, which\ncan be harnessed by SAPLMA to filter out incor-\nrect information before it reaches the user, and\nfurthermore that this representation of accuracy\nis very different from the probability assigned to\nthe sentence by the LLM. Using SAPLMA as a\nsupplement to LLMs may increase human trust\nin the generated responses and mitigate the risks\nassociated with the dissemination of false informa-\ntion. Furthermore, we have released the true-false\ndataset and proposed a method for generating such\ndata. This dataset, along with our methodology,\nprovides valuable resources for future research in\nimproving LLMs’ abilities to generate accurate and\nreliable information.\nIn future work we intend to apply our method\nto larger LLMs, and run experiments with humans,\nsuch that a control group will interact with an unfil-\ntered LLM, and the experimental group will inter-\nact with a system that augments SAPLMA and an\nLLM. We hope to demonstrate that humans trust\nand better understand the limitations of a system\nthat is able to review itself and mark statements that\nit is unsure about. We also intend to study how the\nactivations develop over time as additional words\nare generated, and consider multilingual input.\n8 Limitations\nThis paper focuses on detecting whether a state-\nment is true or false. However, in practice, it may\nbe more beneficial to detect if the LLM is positive\nthat a statement is correct or if it is unsure. The\nmost simple adjustment to the proposed method in\nthis paper is to lift the required threshold for classi-\nfying a statement as true above 0.5; however, the\nexact value would require some form of calibration\nof the model (Bella et al., 2010). Another option\nis to use multiple classifiers and to require all (or\na vast majority of) classifiers to output “true”, for\nthe statement to be considered true. Alternatively,\ndropout layers can be used for the same goal (Chen\nand Yi, 2021). The overall system can benefit from\nmultiple outcomes, such that if the LLM is not pos-\nitive whether the statement is true or false, it can be\nmarked for the user to treat with caution. However,\nif a statement is classified as false, the LLM can\ndelete it and generate a different statement instead.\nTo avoid regenerating the same statement again, the\nprobability of sampling the words that appear in the\ncurrent statement should be adjusted downward.\nOur work was only tested in English. However,\nwe believe that a multilingual LLM can be trained\non one language and applied on statements in an-\nother language. We will test this hypothesis in\nfuture work.\nIn our work, we collected the activation val-\nues when each sentence was generated separately.\nHowever, in practice, in an LLM generating longer\nresponses the activation values develop over time,\nso they may process both correct and incorrect in-\nformation. Therefore, the activation values would\nneed to be decoupled so that they can be tested\nwhether the most recent statement was true or false.\nOne approach might be to subtract the value of the\nactivations obtained after the previous statement\nfrom the current activation values (a discrete deriva-\ntive). Clearly, training must be performed using the\nsame approach.\n9 Ethical Impact\nOne of the primary ethical concerns of LLMs is\nthe generation of false information; yet, we believe\nthat SAPLMA could potentially reduce this issue.\nOn the other hand, it is important to acknowledge\nthat certain ethical issues, such as bias, may per-\nsist, potentially being transferred from the LLM to\nSAPLMA. Specifically, if the LLM exhibits bias\ntowards certain ethnic groups, SAPLMA may like-\nwise classify statements as true or false based on\nthese inherited biases from the original LLM. Nev-\nertheless, it may be possible to adapt the approach\npresented in this paper to bias mitigation.\n10 Acknowledgments\nThis work was supported, in part, by the Ministry\nof Science and Technology, Israel, and by the Israel\nInnovation Authority.\nReferences\nMichiel Bakker, Martin Chadwick, Hannah Sheahan,\nMichael Tessler, Lucy Campbell-Gillingham, Jan\n975\nBalaguer, Nat McAleese, Amelia Glaese, John\nAslanides, Matt Botvinick, et al. 2022. Fine-tuning\nlanguage models to find agreement among humans\nwith diverse preferences. Advances in Neural Infor-\nmation Processing Systems, 35:38176–38189.\nAntonio Bella, Cèsar Ferri, José Hernández-Orallo, and\nMaría José Ramírez-Quintana. 2010. Calibration of\nmachine learning models. In Handbook of Research\non Machine Learning Applications and Trends: Al-\ngorithms, Methods, and Techniques, pages 128–146.\nIGI Global.\nMichael Bommarito II and Daniel Martin Katz.\n2022. Gpt takes the bar exam. arXiv preprint\narXiv:2212.14402.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Stein-\nhardt. 2022. Discovering latent knowledge in lan-\nguage models without supervision. arXiv preprint\narXiv:2212.03827.\nYuanyuan Chen and Zhang Yi. 2021. Adaptive sparse\ndropout: Learning the certainty and uncertainty in\ndeep neural networks. Neurocomputing, 450:354–\n361.\nDavid Dale, Elena V oita, Loïc Barrault, and Marta R\nCosta-jussà. 2022. Detecting and mitigating halluci-\nnations in machine translation: Model internal work-\nings alone do well, sentence similarity even better.\narXiv preprint arXiv:2212.08597.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with frank: A benchmark for\nfactuality metrics. arXiv preprint arXiv:2104.13346.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, et al. 2023. Check your facts and\ntry again: Improving large language models with\nexternal knowledge and automated feedback. arXiv\npreprint arXiv:2302.12813.\nKonstantinos I Roumeliotis, Nikolaos D Tselikas, and\nDimitrios K Nasiopoulos. 2023. Llama 2: Early\nadopters’ utilization of meta’s new open-source pre-\ntrained model.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification. arXiv preprint arXiv:1803.05355.\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal. 2019.\nThe fever2. 0 shared task. In Proceedings of the Sec-\nond Workshop on Fact Extraction and VERification\n(FEVER), pages 1–6.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\n976",
  "topic": "Sentence",
  "concepts": [
    {
      "name": "Sentence",
      "score": 0.8474410772323608
    },
    {
      "name": "Classifier (UML)",
      "score": 0.8109230995178223
    },
    {
      "name": "Computer science",
      "score": 0.7601500749588013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5921320915222168
    },
    {
      "name": "Statement (logic)",
      "score": 0.5863734483718872
    },
    {
      "name": "Natural language processing",
      "score": 0.5235509276390076
    },
    {
      "name": "Speech recognition",
      "score": 0.37021559476852417
    },
    {
      "name": "Machine learning",
      "score": 0.3240225315093994
    },
    {
      "name": "Linguistics",
      "score": 0.09491822123527527
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I52170813",
      "name": "Ariel University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 86
}