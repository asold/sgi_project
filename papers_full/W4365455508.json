{
  "title": "Vision transformers for the prediction of mild cognitive impairment to Alzheimer’s disease progression using mid-sagittal sMRI",
  "url": "https://openalex.org/W4365455508",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2287912853",
      "name": "Gia Minh Hoang",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4226809885",
      "name": "Ue-Hwan Kim",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2199534373",
      "name": "Jae Gwan Kim",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2287912853",
      "name": "Gia Minh Hoang",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4226809885",
      "name": "Ue-Hwan Kim",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2199534373",
      "name": "Jae Gwan Kim",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2154521931",
    "https://openalex.org/W2135755473",
    "https://openalex.org/W2342702273",
    "https://openalex.org/W4214938808",
    "https://openalex.org/W3111737001",
    "https://openalex.org/W2905035821",
    "https://openalex.org/W2063486268",
    "https://openalex.org/W3167864582",
    "https://openalex.org/W1970225606",
    "https://openalex.org/W2160034813",
    "https://openalex.org/W2011481459",
    "https://openalex.org/W2165013497",
    "https://openalex.org/W2766959165",
    "https://openalex.org/W2977883299",
    "https://openalex.org/W3167900828",
    "https://openalex.org/W2142367912",
    "https://openalex.org/W2906155095",
    "https://openalex.org/W4312000402",
    "https://openalex.org/W2889669663",
    "https://openalex.org/W2327949797",
    "https://openalex.org/W2952992882",
    "https://openalex.org/W3005273529",
    "https://openalex.org/W4200002228",
    "https://openalex.org/W4211213171",
    "https://openalex.org/W2616247523",
    "https://openalex.org/W2996359303",
    "https://openalex.org/W4248681802",
    "https://openalex.org/W2146089088",
    "https://openalex.org/W3131390313",
    "https://openalex.org/W3158631207",
    "https://openalex.org/W3102564565",
    "https://openalex.org/W2077849766",
    "https://openalex.org/W4220739453",
    "https://openalex.org/W4226336089",
    "https://openalex.org/W3188404242",
    "https://openalex.org/W4386918813",
    "https://openalex.org/W2070210423",
    "https://openalex.org/W4281826907",
    "https://openalex.org/W4200463156",
    "https://openalex.org/W4294975187"
  ],
  "abstract": "Background Alzheimer’s disease (AD) is one of the most common causes of neurodegenerative disease affecting over 50 million people worldwide. However, most AD diagnosis occurs in the moderate to late stage, which means that the optimal time for treatment has already passed. Mild cognitive impairment (MCI) is an intermediate state between cognitively normal people and AD patients. Therefore, the accurate prediction in the conversion process of MCI to AD may allow patients to start preventive intervention to slow the progression of the disease. Nowadays, neuroimaging techniques have been developed and are used to determine AD-related structural biomarkers. Deep learning approaches have rapidly become a key methodology applied to these techniques to find biomarkers. Methods In this study, we aimed to investigate an MCI-to-AD prediction method using Vision Transformers (ViT) to structural magnetic resonance images (sMRI). The Alzheimer’s Disease Neuroimaging Initiative (ADNI) database containing 598 MCI subjects was used to predict MCI subjects’ progression to AD. There are three main objectives in our study: (i) to propose an MRI-based Vision Transformers approach for MCI to AD progression classification, (ii) to evaluate the performance of different ViT architectures to obtain the most advisable one, and (iii) to visualize the brain region mostly affect the prediction of deep learning approach to MCI progression. Results Our method achieved state-of-the-art classification performance in terms of accuracy (83.27%), specificity (85.07%), and sensitivity (81.48%) compared with a set of conventional methods. Next, we visualized the brain regions that mostly contribute to the prediction of MCI progression for interpretability of the proposed model. The discriminative pathological locations include the thalamus, medial frontal, and occipital—corroborating the reliability of our model. Conclusion In conclusion, our methods provide an effective and accurate technique for the prediction of MCI conversion to AD. The results obtained in this study outperform previous reports using the ADNI collection, and it suggests that sMRI-based ViT could be efficiently applied with a considerable potential benefit for AD patient management. The brain regions mostly contributing to prediction, in conjunction with the identified anatomical features, will support the building of a robust solution for other neurodegenerative diseases in future.",
  "full_text": "Frontiers in Aging Neuroscience 01 frontiersin.org\nVision transformers for the \nprediction of mild cognitive \nimpairment to Alzheimer’s disease \nprogression using mid-sagittal \nsMRI\nGia Minh Hoang 1, Ue-Hwan Kim 2* and Jae Gwan Kim 1*\n1 Department of Biomedical Science and Engineering, Gwangju Institute of Science and Technology, \nGwangju, Republic of Korea, 2 AI Graduate School, Gwangju Institute of Science and Technology, \nGwangju, Republic of Korea\nBackground: Alzheimer’s disease (AD) is one of the most common causes of \nneurodegenerative disease affecting over 50 million people worldwide. However, \nmost AD diagnosis occurs in the moderate to late stage, which means that the \noptimal time for treatment has already passed. Mild cognitive impairment (MCI) \nis an intermediate state between cognitively normal people and AD patients. \nTherefore, the accurate prediction in the conversion process of MCI to AD \nmay allow patients to start preventive intervention to slow the progression of \nthe disease. Nowadays, neuroimaging techniques have been developed and are \nused to determine AD-related structural biomarkers. Deep learning approaches \nhave rapidly become a key methodology applied to these techniques to find \nbiomarkers.\nMethods: In this study, we aimed to investigate an MCI-to-AD prediction method \nusing Vision Transformers (ViT) to structural magnetic resonance images (sMRI). \nThe Alzheimer’s Disease Neuroimaging Initiative (ADNI) database containing 598 \nMCI subjects was used to predict MCI subjects’ progression to AD. There are three \nmain objectives in our study: (i) to propose an MRI-based Vision Transformers \napproach for MCI to AD progression classification, (ii) to evaluate the performance \nof different ViT architectures to obtain the most advisable one, and (iii) to visualize \nthe brain region mostly affect the prediction of deep learning approach to MCI \nprogression.\nResults: Our method achieved state-of-the-art classification performance in \nterms of accuracy (83.27%), specificity (85.07%), and sensitivity (81.48%) compared \nwith a set of conventional methods. Next, we visualized the brain regions that \nmostly contribute to the prediction of MCI progression for interpretability of the \nproposed model. The discriminative pathological locations include the thalamus, \nmedial frontal, and occipital—corroborating the reliability of our model.\nConclusion: In conclusion, our methods provide an effective and accurate \ntechnique for the prediction of MCI conversion to AD. The results obtained in this \nstudy outperform previous reports using the ADNI collection, and it suggests that \nsMRI-based ViT could be efficiently applied with a considerable potential benefit \nfor AD patient management. The brain regions mostly contributing to prediction, \nin conjunction with the identified anatomical features, will support the building of \na robust solution for other neurodegenerative diseases in future.\nOPEN ACCESS\nEDITED BY\nZe Wang,  \nUniversity of Maryland, United States\nREVIEWED BY\nWoon-Man Kung,  \nChinese Culture University, Taiwan\nSaneera Hemantha Kulathilake,  \nRajarata University of Sri Lanka, Sri Lanka\n*CORRESPONDENCE\nUe-Hwan Kim  \n uehwan@gist.ac.kr  \nJae Gwan Kim  \n jaekim@gist.ac.kr\nSPECIALTY SECTION\nThis article was submitted to  \nAlzheimer's Disease and Related Dementias,  \na section of the journal  \nFrontiers in Aging Neuroscience\nRECEIVED 19 November 2022\nACCEPTED 22 March 2023\nPUBLISHED 13 April 2023\nCITATION\nHoang GM, Kim U-H and Kim JG (2023) Vision \ntransformers for the prediction of mild \ncognitive impairment to Alzheimer’s disease \nprogression using mid-sagittal sMRI.\nFront. Aging Neurosci. 15:1102869.\ndoi: 10.3389/fnagi.2023.1102869\nCOPYRIGHT\n© 2023 Hoang, Kim and Kim. This is an open-\naccess article distributed under the terms of \nthe Creative Commons Attribution License \n(CC BY). The use, distribution or reproduction \nin other forums is permitted, provided the \noriginal author(s) and the copyright owner(s) \nare credited and that the original publication in \nthis journal is cited, in accordance with \naccepted academic practice. No use, \ndistribution or reproduction is permitted which \ndoes not comply with these terms.\nTYPE Methods\nPUBLISHED 13 April 2023\nDOI 10.3389/fnagi.2023.1102869\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 02 frontiersin.org\nKEYWORDS\nAlzheimer’s disease, MCI conversion prediction, vision transformers, sMRI image \nanalysis, deep learning, Alzheimer’s disease diagnosis\n1. Introduction\nAlzheimer’s disease (AD) is one of the most common causes of \nneurodegenerative disease affecting over 50 million people \nworldwide. The structural changes of the brain can be one of the \nbiomarkers for identifying AD patients from normal elderly subjects \n(Chiba et al., 2009; Scheltens et al., 2016). Because of the accumulation \nof Aβ and the deposition of hyper-phosphorylated tau protein, the \nstructure in the brain begins to shrink, called brain atrophy, especially \nin specific regions such as the frontal, and hippocampus. Progression \nof atrophy is first manifest in the medial temporal lobe and then \nclosely followed by the hippocampus, amygdala, and para-\nhippocampus. Some studies have suggested that in AD patients, \nentorhinal volumes are already reduced by 20–30%, hippocampal \nvolumes by 15–25% and rates of hippocampal atrophy in mild AD \nare 3–5% per year. This cerebral atrophy can be visualized in life with \nMRI (best with a T1-weighted scan) (Johnson et al., 2012). However, \nmost AD diagnosis occurs in the moderate to late stage, which means \nthat the optimal time for treatment has already passed. Mild cognitive \nimpairment (MCI) is an intermediate state between cognitively \nnormal people and AD patients. It refers to mild impairment of \ncognitive and memory functions rather than dementia. People with \nMCI tend to convert to AD at a significantly higher rate than normal \npeople. Typically, there are two subtypes of MCI: non-convert MCI \n(MCINC), which will not develop to AD, and converted MCI \n(MCIC), which will progress to AD. Therefore, the accurate \nprediction in the conversion process of MCI to AD may allow \npatients to start preventive intervention to slow or stop the \nprogression of the disease.\nAs mentioned above, the accumulation of plaque and \nneurofibrillary tangles make several changes in brain structures. These \nchanges could be used as a biomarker for the classification of MCI \nprogression and are clearly analyzed by structural MRI (sMRI). Three \nplanes of view are there in sMRI known as the axial, sagittal, and \ncoronal planes (Vlaardingerbroek and Boer, 2003). The sagittal plane, \nespecially the mid-sagittal plane provides the most visible information \nfor the diagnosis such as the thalamus, frontal lobe, cerebellum, \ncorpus callosum, which is expected to be  the source site for AD \ntangles and senile plaque. The frontal lobe is in charge of cognitive \nfunction in humans and gives an idea about the prognosis of AD \n(Stuss et al., 1992). Thalamus is also related to episodic memory loss \nand attention dysfunction in AD ( Aggleton et al., 2016 ). Figure 1 \nrepresents a mid-sagittal plane view of an MRI scan taken from the \nADNI dataset showing the important section responsible for AD \nprogression. Thus, we  adopted the mid-sagittal plane for the \nassessment of AD for the proposed model.\nIn recent years, deep learning approaches and their variants \nhave been increasingly applied to AD diagnosis ( Islam and Zhang, \n2017; Maqsood et al., 2019 ; Mehmood et al., 2020 ; Lim et al., 2022). \nAD classification has been greatly improved by structural \nMRI-based approaches using the whole brain, image patches, and \nregions of interest (ROIs). In regional feature-based methods, \nrecent studies mainly relied on prior knowledge to determine ROIs. \nUsing an ROI-based level model, Zhang et al. (2011)  achieved an \naccuracy of 93.2% in AD classification, and 76.4% in MCI \nclassification. Liu et  al. (2016)  proposed a relationship-induced \nmulti-template learning method for the automatic diagnosis of \nAlzheimer’s disease based on multiple sets of regional gray matter \ndensity features with an accuracy of 93.06% in the AD classification \ntask and 79.25% in the progression MCI task. However, identify and \nsegmenting regions of interest (ROIs) was a time-consuming \nprocess that relied on the expertise of specialists, and the features \nextracted from these regions might not capture the intricate \nalterations that occur in the brain.\nTo overcome this limitation, image patch-level methods were used \nfor more effectively capturing the local structural changes in MRI \nscans. Zhu et al. (2021) have also proposed a method using image \npatch-level and multi-instance deep learning which achieves an \naccuracy of 90.2% for the AD classification task and 82.5% for the \nprogression MCI task. Liu et al. (2019) extracted 27 overlapping 3D \npatches of size 50 × 41 × 40 voxels covering the whole volume of the \nMR image (100 × 81 × 80 voxels) then fit them into their model and \nachieve an accuracy of 93.7% in AD classification.\nAlthough many studies have reported very high accuracy in AD \nclassification task based on the deep learning model with \nneuroimaging data, there is currently a lack of studies regarding the \nprediction of MCI converting to AD. The progression of MCI \nclassification has been challenging for not only computer-aid study \nbut also clinical study. There is no obvious difference in brain anatomy \nbetween progression and stable MCI patients. Therefore, study about \nthe progression of MCI classification using brain regions related to \ncognitive and sensory function is necessary.\nInspired by the success of transformers in Natural language \nprocessing (NLP), Dosovitskiy et  al. (2021) developed the vision \ntransformers (ViT) by formulating the image classification as a \nsequence prediction task for the patches. ViT and its variants have \nachieved SOTA performance on several datasets. Nowadays, \ntransformers are becoming one of the most popular methodologies of \ncomputer vision tasks, including classification, detection, and \nsegmentation. Coming up with many successes of vision transformers \nin the medical imaging field (Dai and Gao, 2021; Gao et al., 2021; Jun \net  al., 2021 ; Gheflati and Rivaz, 2022 ), we  hypothesized that \ntransformers also could advance the performance of \nMCI-to-AD progression.\nThe motivations of our proposed method are as follows:\nAbbreviations: AD, Alzheimer’s disease; MCI, mild cognitive impairment; CNN, \nconvolutional neural network; ViT, vision transformers; sMRI, structural magnetic \nresonance image; MCINC, non-convert MCI; MCIC, convert MCI; NLP, natural \nlanguage processing; PET, positron emission tomography; MLP, multi-layer \nperceptron.\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 03 frontiersin.org\n 1. Due to no obvious difference in brain atrophy among MCI \npatients, we hypothesize that the brain region responsible for \ncognitive and sensory function could be  used as a good \nbiomarker for computer-aid diagnosis in the progression of \nMCI. The mid-sagittal plane is used for MCI progression because \nit provides extremely good informative features of the mid-brain \nregion, including the thalamus, medial frontal lobe, etc.\n 2. Vision transformers are proposed due to their successes in many \nmedical classification studies before. We believe that ViT could \nreplace other CNN-based methods in medical applications.\nIn this study, we proposed the first study to explore the potential \nof vision transformers in the medical image classification of \nMCI-to-AD progression by mid-sagittal planes MRI scan. We make \nthe following contributions:\n 1. We apply vision transformers to sMRI classification of MCI \nprogression for the first time and achieve state-of-the-art \nperformance in respect of accuracy in comparison with \nrecent studies.\n 2. We visualize the region that our model mostly focused on to \nensure the interpretability and the reliability of our model. \nWe  found that the medial frontal and thalamus were strong \npredictors of MCI progression, in agreement with previous studies.\n2. Materials and methods\n2.1. Data sets and preprocessing\nThe sMRI scans used for this study are collected from the \nAlzheimer’s Disease Neuroimaging Initiative (ADNI 1) database, \n1 adni.loni.usc.edu/\nincluding ADNI-1, ADNI-2, and ADNIGO. The ADNI was initiated \nin 2003 by Principal Investigator Michael W . Weiner, MD to test \nwhether magnetic resonance imaging (MRI), positron emission \ntomography (PET), other biological markers, and clinical and \nneuropsychological tests can be  incorporated to measure the \ndevelopment of MCI and early AD. We included all participants with \na T1 weighted MRI scan at baseline from the ADNI1/GO/2: 258 MCI \npatients who progressed to AD within 36 months after the baseline \ntime (MCIC) and 340 MCI patients who did not convert (MCINC). \nTable  1 shows the demographic details of subjects accessed \nfrom ADNI.\nThe T1-weighted MRI scans were selected by following the steps, \nFigure 2. If there are multiple scans for a single session, we select the \nscan preferred by MAYOADIRL_MRI_IMAGEQC_12_08_15.csv, \nprovided by ADNI. If no preferred scan is identified, we choose the \nhigher-quality scan defined in MRIQUALITY .csv, also provided by \nADNI. If there is no information regarding quality control, then \nwe select the first scan for the visit.\nThe original sMRI data retrieved from the ADNI database are \npreprocessed to obtain improved image features for classification. In \nADNI data, the scanners have different scanning parameters such as \nflip angle, slice thickness, etc. The ADNI scan intensity values were \nnormalization by adjusting to have a zero mean and unit variance by \nsubtracting the average intensity values and dividing the deviation. To \nreduce global linear differences or make all images fit into each other \ngeometrically, we perform linear registration to the Colin27 templates \n(Holmes et al., 1998). Because the skull information is irrelevant for \nMCI progression prediction, skull-removing is applied for all images. \nRegistration and skull-stripping steps were performed by ‘FLIRT’ \nfunction with default parameter and ‘BET’ function with fractional \nintensity threshold (0.5) of the FSL toolbox (Jenkinson et al., 2012) \nrespectively. After preprocessing, the MR images have a size of \n181 × 217 × 181. Then we extract three middle slices of sagittal planes \n(middle ±3 voxels) to form mid-sagittal planes with the size of \n3 × 217 × 181. The mid-sagittal images then are resized to \n3 × 224 × 224 in order to fit the pre-trained model. The zero-padding \nwas applied to resize for keeping the same resolution. Figure 2 shows \nthe pipeline of our preprocessing process.\n2.2. Vision transformers architecture\nThis work is oriented towards the exploitation of Vision \nTransformers (ViT) approaches. Proposed by Dosovitskiy et al. (2021) \nthe ViT is an architecture for image classification that employs a \nTransformer-like architecture over patches of the image and can \noutperform common CNN architectures when trained on large \namounts of image data. The concept of vision transformers is \ndescribed as follows:\nA standard transformer receives an input as a 1D sequence of \ntoken embeddings; therefore in order to handle a 2D image, ViT \nTABLE 1 Demographics information of the obtained subjects.\nDiagnosis Num. of \nSubjects\nGender \n(Female/Male)\nAge \n(Mean ± Std)\nMCIC 258 103/155 74.11 ± 6.99\nMCINC 340 134/206 72.32 ± 7.48\nFIGURE 1\nSagittal plane view of MRI scan taken from ADNI dataset describing \nAD relevant section of the brain.\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 04 frontiersin.org\nreshapes the image IR HW C ××  into a sequence of flattened 2D \npatches IRP nP C ××()\n2\n, where HW,()  is the resolution of the \noriginal image, C  is the number of channels, P  is the resolution of \nimage patch and nH WP= / 2  is the number of patches. Vision \ntransformers flatten the patches and transform image patches to a D \ndimension vector with a trainable linear projection because vision \ntransformers use the same width across all layers. The output of this \nprojection is used as the patch embeddings.\nThe essential components of the standard transformer layers \ninclude Multi-Head Self Attention (MSA) and Multi-Layer Perceptron \n(MLP). The multi-head self-attention mechanism splits the input into \nmany small parts, then measure the scaled dot-product of each input \nin parallel, and splices all the attention outputs to have the final \noutputs of multi-head self-attention:\n \nAttentionQ KV Softmax QK\nd\nV\nT\nx\n,,() =\n\n\n\n\n\n\n−\n.\n \n(1)\n \nhead AttentionQ WK WV Wi i\nQ\ni K i V= ()\n− −−,,\n \n(2)\n MS AQ KV Concathead head Wii O,, ,,() =… ()  (3)\nThe multi-layer perceptron is added on top of the MSA layer. The \nMLP module contains linear layers separated by a Gaussian Error \nLinear Unit (GeLU) activation. Both MSA and MLP have skip-\nconnection-like residual networks and layer normalization.\n xM SA LN xxtt t′ −−= ()() +11  (4)\n \nxM LP LN xxtt t= ()( ) +′′\n \n(5)\nWhere xt−1 represent for the t −1  layer, LN represents the linear \nnormalization, xt  is the output of the t-th layer.\n2.3. Our methodology\nIn this study, we followed the original ViT implementation as \nmuch as possible to intuitively compare the benefits of transformers \nand access the extensible ViT model and its pre-trained almost \nimmediately. The structure of our model is shown in Figure 3.\nGiven original MR images IR BC hS CAoriginal original original\n ×× ×× , where \nSC Aoriginal original original××  (170 × 256 × 256 voxel) is sagittal, \ncoronal and axial spatial resolution, the number of channels is Ch , \nthe batch size is B. After registration and skull-stripping, the image \nwill become IRpreprocessed BC hS CAnewn ew new ×× ×× , where \nSC Anewn ew new××  (181 × 217 × 181 voxel) is the new sagittal, \ncoronal, axial spatial resolution after preprocessing. Before sending \nit to transformers, it is necessary to extract mid-sagittal slices from \nthe preprocessed images to 2D mid-sagittal slices images \nIRmids agittal\nBC Afinal final\n−\n×× × 3 , where CAfinal final×  is the final \nFIGURE 2\nT1-weighted MRI scans selection.\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 05 frontiersin.org\nspatial resolution. Here, we choose three mid-sagittal slices (mid \nslices ±3 voxels) of the sagittal plane to form three channels of a 2D \nimage. Then, the image sequences are constructed and fit to \nthe model.\n2.4. Experimental setting and evaluation \ncriteria\nOur total dataset has 2,681 images from 258 MCIC and 340 \nMCINC patients. We  shuffled the images randomly and all \nexperiments were performed by splitting data into 10% as test and \n90% as train data; 20% data from the train set is used as a validation \nset. We set SGD as the optimizer with a learning rate equal to 1 7e−\n. The model is trained for 200 epochs and an L2-Regularization \nvalue of 1 5e− . Our experiments are done on a machine with an \nIntel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz with 256GB \nRAM. The GPU used is 4x NVIDIA GeForce RTX 3090. The code \nis implemented using Pytorch ( Paszke et al., 2019 ) and PyCharm \n(PyCharm, n.d. ).\nThe evaluation criteria for the model are accuracy rate (ACC), \nsensitive rate (SEN), and specificity (SPEC), defined as follows:\n \nACC TP TN\nTP TN FP FN= +\n++ +  \n(6)\n \nSEN TP\nTP FN= +  \n(7)\n \nSPEC TN\nTN FP= +  \n(8)\nwhere TP , TN, FP , and FN denoted as true positive, true negative, \nfalse positive and false negative value, respectively.\n3. Results and analysis\n3.1. Classification performance on ADNI\nThe performance on MCI-to-AD progression prediction \nachieved by mid-sagittal MRI-based vision transformers is shown in \nA\nB\nFIGURE 3\nsMRI Image preprocessing. (A) Image preprocessing pipeline for extracting mid-sagittal plane and fit to the model. (B) Sagittal plane view of the image \nin original, registration, and skull-stripping stages.\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 06 frontiersin.org\nTable 2 and Figure 4. The tables include our results and those of the \nliterature. For each study, we  indicate the methodology, and the \nvalues of the performance measures. We provided three variants of \nour method: the small version (VIT-S), the base version (VIT-B), and \nthe large version (VIT-L) of vision transformers. Our method \nconsistently outperforms previous studies in three indicators of \nclassification performance including sensitivity (Sen), specificity \n(Spec), and accuracy (Acc). The VIT-S archives 83.27% in accuracy \nand 85.07% which is about a 1 and 4% improvement, respectively in \ncomparison with the other studies ( Eskildsen et  al., 2013 ; Basaia \net  al., 2019; Bae et  al., 2021 ; Zhang et  al., 2021; Zhu et  al., 2021; \nAshtari-Majlan et  al., 2022 ). VIT-B also shows a significant \nperformance enhancement in specificity with 82.22%. Although \nAshtari-Majlan et al. (2022) have shown higher specificity (99.70%), \ntheir results are lower in both accuracy and sensitive indicators. \nFigure 4 shows the confusion matrix of the VIT-S model which has \nthe best result among our methods with an AUC of 0.87. Finally, our \nproposed method is highly efficient compared with state-of-the-art \nMRI-based research.\n3.2. Pathological locations attentions by \ntransformers\nDefining the brain region most related to the deep learning model \nprediction is of great importance to computer-aided diagnosis. One of \nthe keys to the clinical diagnosis of AD and the progress of MCI to AD \nis to observe the structural change in the brain. As a prediction \napproach, we investigate the possible pathological region in the brain \nwhich is related to the prediction of our method. We use GRAD-CAM \n(Selvaraju et al., 2020 ) to investigate which brain region attention \nlayers observe and focus on in order to classify MCIC and MCINC \nclasses ( Figure  5). depicts several locations in mid-sagittal slices \nidentified by our proposed method. The marked locations are, \nrespectively, suggested by attention score using GRAD-CAM. In the \nleft panel, we compare the MCIC and MCINC classes in each marked \nlocation. The right panel shows the related brain regions of marked \nlocations. We find that three major regions that are most informative \nfor our model prediction are the thalamus, medial frontal, and \noccipital. We  observed brain atrophy in the medial frontal and \noccipital of MCIC class compared with these of MCINC. The right \npanel shows the related brain regions of marked locations. These \nmarked regions are consistent with many previous studies about AD \ndiagnosis (Lian et  al., 2020; Shao et  al., 2020; Bron et  al., 2021)  - \nattesting the reliability of the proposed model.\n3.3. Ablation experiment\nTo investigate the effect of design choices for transformers in \nMCI-to-AD classification, we conducted an ablation experiment. In \nthe experiment, we explore the impact of different models, the size \nof ViT, patch size, and pre-trained weight. We experimented in 3 \nversions of our method: small version (VIT-S), base version (VIT-B), \nand large version (VIT-L). All models were trained using pre-trained \nweights available in Pytorch-Image-Model implementation \n(Wightman et al., 2022 ), including ImageNet-1K, ImageNet-21K, \nand Facebook DINO pre-trained weight. Patch size was selected \nbetween 8, 16, and 32. The results are shown in Table 3. We observe \nthat the ViT small version with a patch size of 16 and pre-trained \nweight of ImageNet-1K gives the best classification performance. \nFrom our point of view, the patch size of 16 could catch the most \neffective and informative features of MR images. The proposed \nmodel makes predictions by extracting the brain regions with related \npatch sizes. With the larger patch size, the information collected by \nthe model becomes too generalized and loses a lot of details leading \nto underfitting. In contrast, too small an image patch size can destroy \nthe semantic information of the MRI scan.\n4. Discussion\nEffective and precise MCI-to-AD prediction is critical for early \nintervention and management of the disease. Therefore, many studies \nmake an effort to research and improve the performance of MCI \nprogression prediction. In this study, we investigated a comparative \nstudy focusing on the predictive performance of vision transformers \nbased on mid-sagittal slices sMRI data of the ADNI. Our proposed \nmethod outperformed the current state-of-the-art MRI-based studies \non MCI progression diagnosis (Basaia et al., 2019; Bae et al., 2021; Zhu \net al., 2021; Ashtari-Majlan et al., 2022) with an accuracy of 83.27%, \nspecificity of 85.07%, and sensitivity of 81.48%. These results imply \nthat using vision transformers equipped with attention power could \nachieve better classification performance compared with the current \nCNN architecture. The possible reason is that the attention mechanism \nin vision transformers could effectively enhance the difference in the \nbrain region between MCI convert and no convert classes.\nIn the ablation contribution, we conducted the variant of the \nmodel, patch size, and pre-trained weights to better understand the \nefficacy of the proposed method. For patch size, we observe that the \nproposed method could gather better informative features with a \npatch size of 16. The results also have shown that reducing the \nTABLE 2 Referential comparison between the proposed model with MRI-\nbased studies for MCI progression prediction (in %).\nStudy Method Acc Sen Spec\nBasaia et al. \n(2019)\n3D CNN architecture 75.1 74.8 75.3\nBae et al. (2021) CNN with ResNet backbone 82.4 81.08 –\nZhang et al. \n(2021)\nConnection-wise-attention-\nmodel-based densely \nconnected convolution \nneural network (CAM-CNN)\n78.79 75.16 82.42\nEskildsen et al. \n(2013)\nmRMR, linear discriminant \nanalysis (LDA)\n77.30 69.00 79.10\nAshtari-Majlan \net al. (2022)\nMulti-stream convolutional \nneural network\n79.90 75.55 99.70\nZhu et al. \n(2021)\nDual attention multi-\ninstance deep learning \nnetwork\n80.20 77.10 82.60\nVIT-S Vision transformers 83.27 85.07 81.48\nVIT-B Vision transformers 80.67 79.10 82.22\nVIT-L Vision transformers 72.86 74.63 71.11\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 07 frontiersin.org\ncomplexity of the model leads to better accuracy, where ViT-S gives \nus the best accuracy. We  assume that the reason is the poor \nperformance of the pure transformer with larger complexity due to \nthe small dataset. These also are proven by the better results of \nFacebook DINO pre-trained weight in the base version (VIT-B). \nDINO Facebook pre-trained is the weight trained by self-distillation \nwith no label’s methods (Caron et al., 2021). Through the distillation \ntrain approach, their model has worked efficiently even with a small \ndataset. In future work, we will explore the performance of the self-\ndistillation train approach in our proposed method.\nBesides, we  also observed the brain regions that affect our \nproposed method’s prediction. Identifying these regions will help \nfor future development of deep learning models to improve \nclassification performance as well as for doctors to find the regions \nof interest for diagnosis easily. We mark out three main regions \nthat have the highest attention score: the thalamus, medial frontal, \nand occipital. We also observed brain atrophy in these regions in \nMRI scans. Figure 6  have shown examples of MRI scans in both \nconvert (MCIC) and non-convert (MCINC). Specifically, the \nthalamus is the main relay of sensorimotor information in the \nbrain and is thought to be crucial for memory processing which is \nimpacted early in Alzheimer’s disease ( Aggleton and Brown, 1999 ; \nAggleton and Nelson, 2015 ). The medial frontal also plays an \nimportant role in numerous cognitive functions, including \nattention, and spatial or long-term memory ( Jobson et al., 2021 ; \nPark et al., 2021 ). The occipital is responsible for visual perception, \nincluding color, form, and motion where the volume is reduced \ndue to Alzheimer’s disease ( Holroyd et  al., 2000 ; Brewer and \nBarton, 2014 ). These results suggest informative regions for future \nfeature extraction to improve our proposed method by focusing \nmore attention on these locations. In addition, the three marked \nbrain regions which contribute critical information for the \nprediction of the method also give more useful clues for doctors in \nclinical diagnosis ( Figure 7).\nFIGURE 4\nOverview of sMRI-based vision transformers model in prediction of MCI-to-AD progression.\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 08 frontiersin.org\n5. Limitations and future work\nAlthough our proposed method achieves good performance in \nAD-related diagnosis, several limitations still need to improve in the \nfuture. We  summarize the limitations and potential solutions \nas follows.\nOur method is not actually a 3D scan model, only 3 slices of the \nmid-sagittal brain are extracted. Therefore, the global anatomical \ninformation in another brain region could be  missed during the \nprediction. In Alzheimer’s disease diagnosis, the hippocampus and \nmedia temporal lobe is critical regions. Missing the image slices \ncovering these regions could lead to misdiagnosing the disease. The \nsegmentation of these regions is needed in future work.\nMoreover, the 3DViT consumed a lot of computational time when \ntrained with 224 × 224 × 224 images. In future work, embedding only \nthe attention mechanism of vision transformer into convolution \nneutral network model could be one of the solutions to reduce the \ncomputational time when performing the model in the 3D scan. The \nautomatic segmentation of the hippocampus and media temporal lobe \nfrom a 3D scan could reduce the image size and keep the necessary \ninformation for disease diagnosis.\nAB\nFIGURE 5\nClassification result for the proposed model for MCINC and MCIC class. (A) Confusion matrix of the test dataset. (B) ROC curve (receiver operating \ncharacteristic curve).\nTABLE 3 Ablation study on the effectiveness of transformers.\nModel Patch size Pretrained data Accuracy Sensitivity Specificity\nVIT-B\n32\nImageNet-1k 76.21 77.04 75.37\nImageNet-21k 75.84 75.56 76.12\n16\nImageNet-1k 80.67 82.22 79.10\nImageNet-21k 76.95 75.56 78.36\nFacebook DINO 80.67 82.22 79.10\n8\nImageNet-1k 73.98 72.59 75.37\nImageNet-21k 72.86 71.11 74.63\nFacebook DINO 81.41 82.96 79.85\nVIT-S\n32\nImageNet-1k 73.61 71.85 75.37\nImageNet-21k 75.84 72.59 79.10\n16\nImageNet-1k 83.27 81.48 85.07\nImageNet-21k 76.58 76.30 76.87\nFacebook DINO 73.98 69.63 78.36\n8 Facebook DINO 77.32 71.85 82.84\nVIT-L\n32 ImageNet-21k 72.49 69.63 75.37\n16\nImageNet-1k 72.86 71.11 74.63\nImageNet-21k 71.38 69.63 73.13\n8\nImageNet-1k 74.35 71.11 77.61\nImageNet-21k 73.61 71.85 75.37\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 09 frontiersin.org\n6. Conclusion\nIn this work, we proposed a vision transformers model to advance \nthe classification accuracy in MCI-to-AD conversion prediction, \nwhich includes two major contributions:\n 1. Our proposed method is evaluated on 598 subjects from ADNI \ndatasets. As far as we know, we are the first study to develop a \nViT model for midsagittal sMRI for MCI to AD progression \nclassification. We achieved a classification accuracy of 83.27%, \nspecificity of 85.07%, and sensitivity of 81.48%. Compared with \nother MRI-based studies on the same datasets, the proposed \nmethod has demonstrated top-ranked classification accuracy.\n 2. We visualized the brain regions affected mostly to the \nperformance of our method. We  found that the thalamus, \nmedial frontal, and occipital regions of sMRI were the strongest \nfeatures in our proposed model. These results highlight the \npotential for early diagnosis and stratification of individuals \nwith MCI based on patterns of cortical atrophy, prior to \ninterventional clinical trials.\nFIGURE 6\nVisualization of the pathological sagittal brain region identified by proposed method on MCI-to-AD Progression Classification. The left panel shows the \ninformative locations suggested by attention scores and the comparison between MCIC and MCINC classes in each region. The right panel shows the \nrelated brain region, respectively, with marked locations.\nFIGURE 7\nExample of MCIC and MCINC MRI scans with region of interest (ROI).\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 10 frontiersin.org\nData availability statement\nThe raw data supporting the conclusions of this article will \nbe made available by the authors, without undue reservation.\nEthics statement\nWritten informed consent was obtained from the individual(s) for \nthe publication of any potentially identifiable images or data included \nin this article.\nAuthor contributions\nGH: experiments, data analysis, and manuscript writing. JK: \nconceptualization, designing experiments, supervision, review and \nrevision of the manuscript. U-HK: conceptualization, designing \nexperiments, supervision, fund acquisition, review and revision of the \nmanuscript. All authors contributed to the article and approved the \nsubmitted version.\nFunding\nThis work was supported by Healthcare AI Convergence Research \n& Development Program through the National IT Industry Promotion \nAgency of Korea (NIPA) funded by the Ministry of Science and ICT \n(No. S1601-20-1016); National Research Foundation of Korea \n(NRF-2022R1A2C3009749) and “GIST Research Institute (GRI) IIBR” \ngrant funded by the GIST in 2023.\nAcknowledgments\nWe thank Y oonho Oh (Theranostics by Electro Digital Technology \nLaboratory, Gwangju Institute of Science and Technology) for his help \nin server management.\nConflict of interest\nThe authors declare that the research was conducted in the \nabsence of any commercial or financial relationships that could \nbe construed as a potential conflict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors \nand do not necessarily represent those of their affiliated organizations, \nor those of the publisher, the editors and the reviewers. Any product \nthat may be evaluated in this article, or claim that may be made by its \nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAggleton, J. P ., and Brown, M. W . (1999). Episodic memory, amnesia, and the \nhippocampal-anterior thalamic axis. Behav. Brain Sci.  22, 425–444. doi: 10.1017/\nS0140525X99002034\nAggleton, J. P ., and Nelson, A. J. D. (2015). Why do lesions in the rodent anterior \nthalamic nuclei cause such severe spatial deficits? Neurosci. Biobehav. Rev. 54, 131–144. \ndoi: 10.1016/j.neubiorev.2014.08.013\nAggleton, J. P ., Pralus, A., Nelson, A. J. D., and Hornberger, M. (2016). Thalamic \npathology and memory loss in early Alzheimer’s disease: moving the focus from the \nmedial temporal lobe to Papez circuit. Brain 139, 1877–1890. doi: 10.1093/brain/\naww083\nAshtari-Majlan, M., Seifi, A., and Dehshibi, M. M. (2022). A multi-stream \nconvolutional neural network for classification of progressive MCI in Alzheimer’s \ndisease using structural MRI images. IEEE J. Biomed. Health Inform. 26, 3918–3926. doi: \n10.1109/JBHI.2022.3155705\nBae, J., Stocks, J., Heywood, A., Jung, Y ., Jenkins, L., Hill, V ., et al. (2021). Transfer \nlearning for predicting conversion from mild cognitive impairment to dementia of \nAlzheimer’s type based on a three-dimensional convolutional neural network. Neurobiol. \nAging 99, 53–64. doi: 10.1016/j.neurobiolaging.2020.12.005\nBasaia, S., Agosta, F ., Wagner, L., Canu, E., Magnani, G., Santangelo, R., et al. (2019). \nAutomated classification of Alzheimer’s disease and mild cognitive impairment using a \nsingle MRI and deep neural networks. Neuroimage Clin. 21:101645. doi: 10.1016/j.\nnicl.2018.101645\nBrewer, A., and Barton, B. (2014). Visual cortex in aging and Alzheimer’s disease: \nchanges in visual field maps and population receptive fields. Front. Psychol. 5:74. doi: \n10.3389/fpsyg.2014.00074\nBron, E. E., Klein, S., Papma, J. M., Jiskoot, L. C., Venkatraghavan, V ., Linders, J., et al. \n(2021). Cross-cohort generalizability of deep and conventional machine learning for \nMRI-based diagnosis and prediction of Alzheimer’s disease. NeuroImage Clinical. \n31:102712. doi: 10.1016/j.nicl.2021.102712\nCaron, M, Touvron, H, Misra, I, Jégou, H, Mairal, J, Bojanowski, P , et al. (2021) \nEmerging properties in self-supervised vision transformers. Available at: http://arxiv.\norg/abs/2104.14294\nChiba, T., Y amada, M., Sasabe, J., Terashita, K., Shimoda, M., Matsuoka, M., et al. \n(2009). Amyloid-β causes memory impairment by disturbing the JAK2/STAT3 axis in \nhippocampal neurons. Mol. Psychiatry 14, 206–222. doi: 10.1038/mp.2008.105\nDai, Y , and Gao, Y . (2021). TransMed: transformers advance multi-modal medical \nimage classification. Available at: http://arxiv.org/abs/2103.05940\nDosovitskiy, A, Beyer, L, Kolesnikov, A, Weissenborn, D, Zhai, X, Unterthiner, T, et al. \n(2021). An image is worth 16x16 words: transformers for image recognition at scale. \nAvailable at: http://arxiv.org/abs/2010.11929\nEskildsen, S. F ., Coupé, P ., García-Lorenzo, D., Fonov, V ., Pruessner, J. C., Collins, D. L., \net al. (2013). Prediction of Alzheimer’s disease in subjects with mild cognitive \nimpairment from the ADNI cohort using patterns of cortical thinning. NeuroImage 65, \n511–521. doi: 10.1016/j.neuroimage.2012.09.058\nGao, X, Qian, Y , and Gao, A. COVID-VIT: classification of COVID-19 from CT chest \nimages based on vision transformer models. (2021). Available at: http://arxiv.org/\nabs/2107.01682\nGheflati, B, and Rivaz, H. (2022) Vision transformer for classification of breast \nultrasound images. Available at: http://arxiv.org/abs/2110.14731\nHolmes, C. J., Hoge, R., Collins, L., Woods, R., Toga, A. W ., and Evans, A. C. (1998). \nEnhancement of MR images using registration for signal averaging. J. Comput. Assist. \nTomogr. 22, 324–333. doi: 10.1097/00004728-199803000-00032\nHolroyd, S., Shepherd, M. L., and Downs, J. H. (2000). Occipital atrophy is associated \nwith visual hallucinations in Alzheimer’s disease. J. Neuropsychiatry Clin. Neurosci. 12, \n25–28. doi: 10.1176/jnp.12.1.25\nIslam, J., and Zhang, Y . (2017). “ A novel deep learning based multi-class classification \nmethod for Alzheimer’s disease detection using brain MRI data” in Brain informatics. \neds. Y . Zeng, Y . He, J. H. Kotaleski, M. Martone, B. Xu and H. Peng, et al. (Cham: \nSpringer International Publishing), 213–222.\nJenkinson, M., Beckmann, C. F ., Behrens, T. E. J., Woolrich, M. W ., and Smith, S. M. \n(2012). FSL. Neuroimage. 62, 782–790. doi: 10.1016/j.neuroimage.2011.09.015\nJobson, D. D., Hase, Y ., Clarkson, A. N., and Kalaria, R. N. (2021). The role of the \nmedial prefrontal cortex in cognition, ageing and dementia. Brain Commun.  \n3:3:fcab125. doi: 10.1093/braincomms/fcab125\nJohnson, K. A., Fox, N. C., Sperling, R. A., and Klunk, W . E. (2012). Brain imaging in \nAlzheimer disease. Cold Spring Harb. Perspect. Med. 2:a006213. doi: 10.1101/cshperspect.\na006213\nJun, E, Jeong, S, Heo, DW , and Suk, HI. Medical transformer: universal brain encoder \nfor 3D MRI analysis. (2021) Available at: http://arxiv.org/abs/2104.13633\nHoang et al. 10.3389/fnagi.2023.1102869\nFrontiers in Aging Neuroscience 11 frontiersin.org\nLian, C., Liu, M., Zhang, J., and Shen, D. (2020). Hierarchical fully convolutional \nnetwork for joint atrophy localization and Alzheimer’s disease diagnosis using structural \nMRI. IEEE Trans. Pattern Anal. Mach. Intell.  42, 880–893. doi: 10.1109/\nTPAMI.2018.2889096\nLim, B. Y ., Lai, K. W ., Haiskin, K., Kulathilake, K. A. S. H., Ong, Z. C., Hum, Y . C., et al. \n(2022). Deep learning model for prediction of progressive mild cognitive impairment \nto Alzheimer’s disease using structural MRI. Front Aging Neurosci 14. doi: 10.3389/\nfnagi.2022.1027857\nLiu, M., Zhang, J., Adeli, E., and Shen, D. (2019). Joint classification and regression via \ndeep multi-task Multi-Channel learning for Alzheimer’s disease diagnosis. IEEE Trans. \nBiomed. Eng. 66, 1195–1206. doi: 10.1109/TBME.2018.2869989\nLiu, M., Zhang, D., and Shen, D. (2016). Relationship induced multi-template learning \nfor diagnosis of Alzheimer’s disease and mild cognitive impairment. IEEE Trans. Med. \nImaging 35, 1463–1474. doi: 10.1109/TMI.2016.2515021\nMaqsood, M., Nazir, F ., Khan, U., Aadil, F ., Jamal, H., Mehmood, I., et al. (2019). \nTransfer learning assisted classification and detection of Alzheimer’s disease stages using \n3D MRI scans. Sensors 19:2645. doi: 10.3390/s19112645\nMehmood, A., Maqsood, M., Bashir, M., and Shuyuan, Y . (2020). A deep Siamese \nconvolution neural network for multi-class classification of Alzheimer disease. Brain Sci. \n10:84. doi: 10.3390/brainsci10020084\nPark, M., Hoang, G. M., Nguyen, T., Lee, E., Jung, H. J., Choe, Y ., et al. (2021). Effects \nof transcranial ultrasound stimulation pulsed at 40 Hz on Aβ plaques and brain rhythms \nin 5×FAD mice. Transl Neurodegener. 10:48. doi: 10.1186/s40035-021-00274-x\nPaszke, A, Gross, S, Massa, F , Lerer, A, Bradbury, J, Chanan, G, et al. (2019). PyTorch: \nan imperative style, high-performance deep learning library. Available at: http://arxiv.\norg/abs/1912.01703\nPyCharm (n.d) The python IDE for professional developers by JetBrains. Available at: \nhttps://www.jetbrains.com/pycharm/\nScheltens, P ., Blennow, K., Breteler, M. M. B., de Strooper, B., Frisoni, G. B., \nSalloway, S., et al. (2016). Alzheimer’s disease. Lancet 388, 505–517. doi: 10.1016/\nS0140-6736(15)01124-1\nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. (2020). \nGrad-CAM: visual explanations from deep networks via gradient-based localization. \nInt. J. Comput. Vis. 128, 336–359. doi: 10.1007/s11263-019-01228-7\nShao, W ., Peng, Y ., Zu, C., Wang, M., and Zhang, D. (2020). Alzheimer’s Disease \nNeuroimaging Initiative. Hypergraph based multi-task feature selection for multimodal \nclassification of Alzheimer’s disease. Comput. Med. Imaging Graph. 80:101663. doi: \n10.1016/j.compmedimag.2019.101663\nStuss, D. T., Gow, C. A., and Hetherington, C. R. (1992). No longer gage: frontal lobe \ndysfunction and emotional changes. J. Consult. Clin. Psychol.  60, 349–359. doi: \n10.1037/0022-006X.60.3.349\nVlaardingerbroek, M. T., and Boer, J. A. (2003). Magnetic resonance imaging: Theory \nand practice. Berlin, Heidelberg: Springer Science & Business Media, 526.\nWightman, R, Soare, A, Arora, A, Ha, C, Reich, C, Raw, N, et al. Rwightman/\npytorch-image-models: v0.6.11 release. Available at: https://zenodo.org/\nrecord/7140899\nZhang, D., Wang, Y ., Zhou, L., Yuan, H., and Shen, D. (2011). Multimodal classification \nof Alzheimer’s disease and mild cognitive impairment. NeuroImage 55, 856–867. doi: \n10.1016/j.neuroimage.2011.01.008\nZhang, J., Zheng, B., Gao, A., Feng, X., Liang, D., and Long, X. (2021). A 3D densely \nconnected convolution neural network with connection-wise attention mechanism for \nAlzheimer’s disease classification. Magn. Reson. Imaging 78, 119–126. doi: 10.1016/j.\nmri.2021.02.001\nZhu, W ., Sun, L., Huang, J., Han, L., and Zhang, D. (2021). Dual attention multi-\ninstance deep learning for Alzheimer’s disease diagnosis with structural MRI. IEEE \nTrans. Med. Imaging 40, 2354–2366. doi: 10.1109/TMI.2021.3077079",
  "topic": "Neuroimaging",
  "concepts": [
    {
      "name": "Neuroimaging",
      "score": 0.7265938520431519
    },
    {
      "name": "Disease",
      "score": 0.5318512320518494
    },
    {
      "name": "Cognitive impairment",
      "score": 0.5238185524940491
    },
    {
      "name": "Cognition",
      "score": 0.5201318860054016
    },
    {
      "name": "Alzheimer's disease",
      "score": 0.46020621061325073
    },
    {
      "name": "Medicine",
      "score": 0.45773768424987793
    },
    {
      "name": "Magnetic resonance imaging",
      "score": 0.4313046336174011
    },
    {
      "name": "Neuroscience",
      "score": 0.36510246992111206
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34449321031570435
    },
    {
      "name": "Psychology",
      "score": 0.33716434240341187
    },
    {
      "name": "Computer science",
      "score": 0.2676393389701843
    },
    {
      "name": "Pathology",
      "score": 0.19191697239875793
    },
    {
      "name": "Radiology",
      "score": 0.11906668543815613
    }
  ]
}