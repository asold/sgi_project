{
    "title": "Image recoloring for color vision deficiency compensation using Swin transformer",
    "url": "https://openalex.org/W4390975897",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2215003454",
            "name": "Ligeng Chen",
            "affiliations": [
                "University of Yamanashi"
            ]
        },
        {
            "id": "https://openalex.org/A2324801929",
            "name": "Zhenyang Zhu",
            "affiliations": [
                "Takeda (Japan)",
                "University of Yamanashi"
            ]
        },
        {
            "id": "https://openalex.org/A3030494828",
            "name": "Wangkang Huang",
            "affiliations": [
                "University of Yamanashi"
            ]
        },
        {
            "id": "https://openalex.org/A2074869655",
            "name": "Kentaro Go",
            "affiliations": [
                "University of Yamanashi",
                "Takeda (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2660648769",
            "name": "Xiao-Diao Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116962689",
            "name": "Xiaoyang Mao",
            "affiliations": [
                "University of Yamanashi",
                "Takeda (Japan)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2108071170",
        "https://openalex.org/W1969818288",
        "https://openalex.org/W1991906508",
        "https://openalex.org/W2945076737",
        "https://openalex.org/W2971463489",
        "https://openalex.org/W3187818469",
        "https://openalex.org/W2918818563",
        "https://openalex.org/W2940961047",
        "https://openalex.org/W2944464059",
        "https://openalex.org/W3149340975",
        "https://openalex.org/W3157619262",
        "https://openalex.org/W4283265606",
        "https://openalex.org/W2552465644",
        "https://openalex.org/W3045518893",
        "https://openalex.org/W2983150043",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2106474058",
        "https://openalex.org/W2005137440",
        "https://openalex.org/W2101793629",
        "https://openalex.org/W2033101041",
        "https://openalex.org/W4390872431",
        "https://openalex.org/W2128086789",
        "https://openalex.org/W2133665775",
        "https://openalex.org/W2732026016",
        "https://openalex.org/W1965886316",
        "https://openalex.org/W2932029951",
        "https://openalex.org/W3159231306"
    ],
    "abstract": "Abstract People with color vision deficiency (CVD) have difficulty in distinguishing differences between colors. To compensate for the loss of color contrast experienced by CVD individuals, a lot of image recoloring approaches have been proposed. However, the state-of-the-art methods suffer from the failures of simultaneously enhancing color contrast and preserving naturalness of colors [without reducing the Quality of Vision (QOV)], high computational cost, etc. In this paper, we propose an image recoloring method using deep neural network, whose loss function takes into consideration the naturalness and contrast, and the network is trained in an unsupervised manner. Moreover, Swin transformer layer, which has long-range dependency mechanism, is adopted in the proposed method. At the same time, a dataset, which contains confusing color pairs to CVD individuals, is newly collected in this study. To evaluate the performance of the proposed method, quantitative and subjective experiments have been conducted. The experimental results showed that the proposed method is competitive to the state-of-the-art methods in contrast enhancement and naturalness preservation and has a real-time advantage. The code and model will be made available at https://github.com/Ligeng-c/CVD_swin .",
    "full_text": "ORIGINAL ARTICLE\nImage recoloring for color vision deficiency compensation using Swin\ntransformer\nLigeng Chen1 • Zhenyang Zhu2 • Wangkang Huang1 • Kentaro Go2 • Xiaodiao Chen3 • Xiaoyang Mao2\nReceived: 15 June 2023 / Accepted: 7 December 2023 / Published online: 18 January 2024\n/C211 The Author(s) 2024\nAbstract\nPeople with color vision deﬁciency (CVD) have difﬁculty in distinguishing differences between colors. To compensate for\nthe loss of color contrast experienced by CVD individuals, a lot of image recoloring approaches have been proposed.\nHowever, the state-of-the-art methods suffer from the failures of simultaneously enhancing color contrast and preserving\nnaturalness of colors [without reducing the Quality of Vision (QOV)], high computational cost, etc. In this paper, we\npropose an image recoloring method using deep neural network, whose loss function takes into consideration the natu-\nralness and contrast, and the network is trained in an unsupervised manner. Moreover, Swin transformer layer, which has\nlong-range dependency mechanism, is adopted in the proposed method. At the same time, a dataset, which contains\nconfusing color pairs to CVD individuals, is newly collected in this study. To evaluate the performance of the proposed\nmethod, quantitative and subjective experiments have been conducted. The experimental results showed that the proposed\nmethod is competitive to the state-of-the-art methods in contrast enhancement and naturalness preservation and has a real-\ntime advantage. The code and model will be made available at https://github.com/Ligeng-c/CVD_swin.\nKeywords Color vision deﬁciency /C1 Unsupervised /C1 Swin transformer /C1 Recoloring /C1 Deep neural network /C1\nContrast /C1 Naturalness\n1 Introduction\nThere are three kinds of cone cells in human retinas, the L,\nM, and S cones, and they are sensitive to long-, medium-,\nand short-wavelength visible light, respectively, enabling\npeople to perceive distinct colors. Anomalies in cone cells\nlead to color vision deﬁciency (CVD) in individuals.\nSpeciﬁcally, when an anomaly occurs in the L, M,o r S\ncones, CVD is classiﬁed as protanopia, deuteranopia, or\ntritanopia, respectively. It is estimated that approximately\n8% of men and 0.5 % of women in the world are affected by\nvarious degrees of CVD [ 1, 2]. People with CVD may\nexperience inconvenience in their lives, such as the\ninability to read red characters written on a black back-\nground, due to the reduced capability to discriminate col-\nors. CVD is an inherited deﬁciency for which there is\ncurrently no cure.\nOver the years, numerous image processing approaches\n[3–7] have been proposed to support people with CVD. In\n[8], Ribeiro et al. summarized CVD compensation studies\nconducted in the 2 decades before 2019. Further, Zhu et al.\n& Xiaoyang Mao\nmao@yamanashi.ac.jp\nLigeng Chen\nligengchen422@gmail.com\nZhenyang Zhu\nzzhu@yamanashi.ac.jp\nWangkang Huang\nhuang10669668281@gmail.com\nKentaro Go\ngo@yamanashi.ac.jp\nXiaodiao Chen\nxiaodiao@hdu.edu.cn\n1 Graduate School of Engineering, University of Yamanashi, 4-\n4-37 Takeda, Kofu, Yamanashi 400-8510, Japan\n2 Department of Computer Science and Engineering,\nUniversity of Yamanashi, 4-4-37 Takeda, Kofu,\nYamanashi 400-8510, Japan\n3 School of Computer Science, Hangzhou Dianzi University,\nNo.1, the No.2 Baiyang Street, Hangzhou 310018, Zhejiang,\nChina\n123\nNeural Computing and Applications (2024) 36:6051–6066\nhttps://doi.org/10.1007/s00521-023-09367-2(0123456789().,-volV)(0123456789().,- volV)\n[9] surveyed the latest image recoloring studies, as well as\nquantitative and subjective evaluation metrics for CVD\ncompensation methods. In recent years, image recoloring\napproaches have become the mainstream for CVD com-\npensation, aimed at enhancing the color contrast perceived\nby CVD individuals by changing a part of the colors in the\nimages. Considering CVD individuals may feel uncom-\nfortable if signiﬁcant changes are applied to the original\ncolor appearance, which can reduce quality of life (QOL),\nstate-of-the-art studies [ 10–16] further considered natural-\nness preservation, which put constraints on deviations from\nthe original image colors. The state-of-the-art studies\n[12–14] proposed recoloring images using optimization\nmodels, which take contradictory constraints, i.e., contrast\nenhancement and naturalness preservation, into a uniﬁed\nenergy function and achieve balanced recoloring results;\nhowever, the GPU-based implementation, which delivers\nthe best performance, still requires approximately 7 s to\nprocess an image sized 256 /C2 256 pixels [ 14]. In other\nwords, the poor time efﬁciency limits their usage in real\napplications.\nRecently, deep neural network (DNN)-based machine\nlearning techniques have achieved remarkable success in\nthe image processing and image synthesis ﬁelds. Speciﬁ-\ncally, Isola et al. proposed a Pix2Pix network [ 17]t o\ntransfer images from a source domain to a target domain in\nreal time, and it showed the potential of using a DNN to\nrecolor images in real time. In [ 18], Li et al. demonstrated\nthe feasibility of utilizing Pix2Pix to generate compensa-\ntion images for CVD by training the network using images\ngenerated with their own recoloring algorithms. Because\ntheir recoloring algorithm does not consider naturalness\npreservation for CVD and because the Pix2pix model used\nwas based on a convolutional neural network (CNN),\nwhich has a limited receptive ﬁeld, leading to a poor ability\nto learn explicit global and long-range information, the\neffectiveness of the resulting images is limited. Hu et al.\n[19] proposed a method to decompose an original image\ninto 27 basis images and then to use a CNN to predict the\ncoefﬁcients for blending the basis images into image pairs\nfor use with stereo displays, enabling content sharing\nbetween audiences with CVD and with normal vision.\nHowever, Hu et al. [ 19] were also subject to CNN limita-\ntions and did not consider naturalness preservation for\nCVD either.\nIn this paper, we propose a new DNN-based image\nrecoloring method for CVD compensation. The proposed\nmethod aims to enhance contrast and preserve naturalness\nsimultaneously in recolored images. The architecture of the\nproposed network is similar to that of U-Net [ 20], which is\ncomprised of an encoder and a decoder. To improve the\nperformance of U-Net, we replace its CNN layers with\nSwin transformer blocks [ 21]. Transformers, also known as\nself-attention (SA) mechanisms, have gained signiﬁcant\nattention in the ﬁelds of natural language processing and\ncomputer vision [ 22, 23]. The Swin transformer [ 21] has\nthe same performance as a standard transformer, but it\nrequires a much lower computational cost. To overcome\nthe limitation caused by using ground-truth images gener-\nated by existing methods, we design an unsupervised\ntraining manner to guide the training of the proposed net-\nwork; concretely, the training phase only requires input\nimages and is guided by a newly designed loss function\nthat considers both contrast enhancement and naturalness\npreservation. To enhance the generalization capability of\nthe trained model, we built an image dataset containing\ncolor pairs that are confusing to CVD individuals, and we\nuse these images as the input to the proposed model during\nthe training stage. The dataset consists of 131,000 color\nimages, including 31,000 photographs of natural scenes\nand 100,000 artiﬁcially created images. To evaluate the\nperformance of the proposed method, we conducted\nquantitative and subjective experiments. The experimental\nresults show that our method can generate recoloring\nresults competitive to those of state-of-the-art methods but\nexecuted in real time. Our contributions can be summa-\nrized as follows:\n• A novel DNN-based image recoloring method for CVD\ncompensation. By using the U-net structure of Swin\ntransformer blocks, the method succeeded in capturing\nboth local and long-range color dependency.\n• A novel unsupervised training approach guided by a\nnewly designed loss function considering both global\nand local contrast enhancement, as well as naturalness\npreservation.\n• A new dataset that can be used to train a DNN model in\nan unsupervised manner to generate CVD compensa-\ntion images. The database consists of photographs and\nartiﬁcial images with color pairs that are confusing to\nCVD individuals.\n2 Related work\n2.1 Conventional recoloring method for CVD\nWith the popularization of digital devices, recoloring\nalgorithms are dominating the approaches to CVD com-\npensation. As mentioned above, Ribeiro et al. [ 8] published\na survey paper regarding image processing approaches for\nCVD compensation, covering studies conducted in the\nmost recent 2 decades, including 59 papers on image\nrecoloring approaches. Further, Zhu et al. [ 9] conducted\nsurvey research focusing on image recoloring approaches,\nadding 17 studies on the topic. Early studies focused on\n6052 Neural Computing and Applications (2024) 36:6051–6066\n123\nenhancing color contrast, while preserving naturalness and\nenhancing contrast simultaneously became the trend in the\nmost recent decade.\nIaccarino et al. [ 4] enhanced the color contrast between\ntext and backgrounds on web pages by simulating recol-\nored colors falling in two different half-planes. Jefferson\net al. [ 24] developed an interface to allow CVD users to\nadjust a single parameter of a recoloring algorithm. To\nmaximize the contrast between colors on the gamut of\ndichromacy, Machado et al. [ 6] proposed ﬁrst projecting all\ncolors in the input image onto a two-dimensional plane,\nwhere the distribution of projected colors can be maximally\ndispersed, and then rotating the plane to match the CVD\ngamut. Lin et al. [ 7] proposed a color-warping method that\nutilizes the orientation of the eigenvectors of the CVD\nsimulation results for recoloring images in the opponent\ncolor space [ 25]. These studies did not consider naturalness\npreservation, thus changing the color appearance of the\noriginal image signiﬁcantly, which may make CVD users\nfeel uncomfortable. Therefore, later researchers focused on\ncolor appearance preservation, that is, naturalness preser-\nvation. Kuhn et al. [ 26] enhanced color contrast and pre-\nserved naturalness based on mass-spring optimization, but\nthe quality of the recoloring result depends on the quality\nof the quantization. In [ 27], Huang et al. proposed to\nextract key colors from an input image and to recolor them\nto enhance their distances, but not too far from the original\nimage. Hassan et al. [ 10] compensated for the contrast loss\nby increasing the blue component of confusing colors,\nbecause CVD individuals with red–green deﬁciency, which\nis most individuals, have difﬁculty discriminating red and\ngreen, but they can perceive blue well. To achieve sufﬁ-\ncient contrast enhancement, in [ 11], they extended their\nwork so users could exaggerate the blue channel. However,\ncontrast loss occurs in the recolored images due to the\nsaturation of the blue channels. Zhu et al. [ 14] proposed an\noptimization model by using an object function considering\nboth contrast enhancement and naturalness preservation,\nand they demonstrated that their method achieves the best\ncompensation effect among all existing methods through a\nsubjective experiment involving volunteers with CVD.\nHowever, their method is time-consuming. To reduce the\ncomputation time, they proposed to extract dominant colors\nfrom the original image and to recolor the extracted\ndominant colors using an optimization model. Finally, the\nrecolored dominant colors are propagated to the whole\nimage. Despite the acceleration, their method still requires\nmore than 7 s to recolor an image sized 256 /C2 256 pixels\non a desktop with an Intel(R) Core (TM) i7-9800X\nCPU@3.80GHz, 64-GB memory, and GeForce RTX 2080\nTi.\nTo accelerate processing, Wang et al. [ 15] proposed a\nfast recoloring method by computing a single optimal map\nfrom the 3D color space to the CVD color gamut. Huang\net al. [ 16] ﬁtted a curved surface to represent the gamut of\ndichromacy in Lab space and considered a luminance\nchannel in their compensation model. However, their\nmethods still require a few seconds to generate an image.\nEbelin et al. [ 28] introduced a real-time algorithm to\nenhance the visual experience for color vision-deﬁcient\nobservers by recoloring images. The proposed algorithm\nprioritizes temporal stability and luminance preservation.\nHowever, their method falls short in addressing the issue of\nnaturalness for individuals with CVD.\n2.2 Recoloring method for CVD individuals\nbased on DNNs\nBecause image recoloring can be regarded as transferring\nan image from a source domain to a target domain, it is\npossible to apply a DNN to image recoloring for CVD\ncompensation. For instance, Isola et al. [ 17] proposed a\nnetwork, Pix2Pix, based on conditional GANs (generative\nadversarial networks) to carry out the image-to-image\ntranslation task. A standard GAN is comprised of a gen-\nerator and a discriminator. Taking white noise as the input,\nthe generator aims to generate fake images whose\nappearance is similar to real images, while the discrimi-\nnator tries to distinguish real and fake images. Setting the\ninput image as a condition, Pix2Pix maps the image from\nthe source domain to the target domain. In [ 18], Li et al.\ntrained the Pix2Pix network with thousands of ground-truth\nimage pairs generated using their recoloring algorithm.\nHowever, their recoloring algorithm did not consider nat-\nuralness preservation; thus, the network has no ability to\npreserve naturalness. In [ 19], Hu et al. proposed a model\nfor sharing the same contents to normal vision and CVD\naudiences. For CVD audiences who wear stereo glasses,\nthe model generates a pair of contrast-enhanced images\nfrom the input image for left and right eyes, respectively.\nFor normal color vision audiences who do not wear stereo\nglasses, they perceive the blending of the paired images.\nFor generating the image pair, the original image is ﬁrst\ndecomposed into 27 basis images; then, their CNN-based\nmodel predicts a set of coefﬁcients; ﬁnally, the output\nimage pair is computed as the linear combination of the\nbasis images with the predicted coefﬁcients. Though the\nblending of the image pair, which synthesizes the image for\npeople with normal vision, has been constrained to be\nsimilar to the input image, Hu et al. [ 19] did not consider\nnaturalness preservation for CVD. Besides, both methods\nused a CNN as the backbone for feature extraction in their\nnetwork model, which has a limited receptive ﬁeld leading\nto a poor ability to learn explicit global and long-range\ninformation; hence, the effectiveness of the resulting ima-\nges is limited. Jiang et al. [ 29] proposed a method based on\nNeural Computing and Applications (2024) 36:6051–6066 6053\n123\nStyleGAN-ada[30] for generating compensation images\nacross various degrees through latent traversal. However,\nthe limitation lies in the fact that their method relies on\nlatent codes as input, making it challenging to apply in\nreal-time to real images.\n2.3 Vision transformer\nDriven by transformers’ success in the natural language\nprocessing domain [ 22], which process all tokens at once\nand calculate relationships between them, more researchers\nare paying attention to the use of transformer-based models\nin the computer vision domain. The pioneering vision\ntransformer (ViT) in [ 23] ﬂattens 2D image patches in a\nvector and then feeds it to the transformer. It achieved an\nimpressive performance in an image recognition task, but it\nrequires a large-scale training dataset, such as JFT-300 M.\nThe Swin transformer [ 21], an efﬁcient and effective\nhierarchical vision transformer, has linear computational\ncomplexity from adopting a shifted windows scheme. With\nthe hierarchical architecture, it has been adopted as a vision\nbackbone, and it achieved a state-of-the-art performance in\nvarious vision tasks, including image classiﬁcation, object\ndetection, and semantic segmentation. In this work, we\nadopt a Swin transformer block as a basic unit to build an\nencoder-decoder model for generating compensation ima-\nges for CVD people.\n3 Proposed method\n3.1 Overview\nAs illustrated in Fig. 1, given an input image I that contains\ncolor pairs confusing to CVD individuals, the proposed\nmodel can output a compensated image with enhanced\ncontrast but still natural to CVD individuals. The proposed\nmodel uses an auto-encoder framework whose structure is\nsimilar to that of U-Net [ 20]. It is comprised of two parts:\n(1) an encoder and (2) a decoder; the encoder extracts\nfeatures from the input image, while the decoder generates\na recolored result from the extracted features. Particularly,\nthe Swin transformer is adopted as a feature extractor.\nThe input image is ﬁrst fed to the encoder for hierar-\nchical feature extraction, and then the compensated image\nis generated by the decoder by referring to the different\nfeature layers of the encoder using skip connection. In [ 20],\nthe encoder adopted CNN layers for feature extraction. In\nthis study, we construct our encoder with patch partitioning\nand patch embedding and with a series of Swin transformer\nblocks and patch merging. The decoder consists of Swin\ntransformer blocks, up-sample layer patch expanding, and\nconvolution block layers.\nMore details of the proposed model will be introduced in\nSect. 3.2. In Sect. 3.3, we will introduce the unsupervised\ntraining approach, as well as the loss function, which aims\nused to guide mode training in an unsupervised manner.\n3.2 Network design\n3.2.1 Swin transformer block\nTaking the original image as the input, the patch partition\nof the encoder divides the input image into small patches.\nAs shown in Fig. 2, the area partitioned by the gray square\ndenotes a patch, and each patch can be regarded as a token.\nThe big squares in brown represent local windows, and\ntokens exchange information with all other tokens in the\nPatch EmbeddingPatch Embedding\nSwin Transformer \nBlock x8\nSwin Transformer \nBlock x8\nPatch MergingPatch Merging\nSwin Transformer \nBlock x4\nSwin Transformer \nBlock x4\nPatch MergingPatch Merging\nUpsample LayerUpsample Layer\nSwin Transformer \nBlock x4\nSwin Transformer \nBlock x4\nUpsample LayerUpsample Layer\nPatch ExpandingPatch Expanding\nCVD \nSimulation of \nInput Image\nSwin Transformer \nBlock x4\nSwin Transformer \nBlock x4\nSkip Connection\nSkip Connection\nSwin Transformer \nBlock x4\nSwin Transformer \nBlock x4\nCVD \nSimulation of \nOutput Image\nEncoder\nDecoder\nPatch PartitionPatch Partition Convolution Block Convolution Block \nFig. 1 Architecture of the proposed model: encoder, decoder, and\nskip connections\nLayer l Layer l+1\nA patch\nA local window to \nperform self-attention\nFig. 2 An illustration of the W-MSA and SW-MSA modules. In layer\nl (left), self-attention is computed within each regular window. In the\nnext layer l ? 1 (right), the window partitioning is shifted, resulting in\nnew windows. Within each new window, the self-attention compu-\ntation leads to connections with the previous windows\n6054 Neural Computing and Applications (2024) 36:6051–6066\n123\nsame window. The standard transformer contains only one\nbig window, which includes all tokens, and each token\nmust be compared with all other tokens using the multi-\nhead self-attention (MSA) module; thus, the standard\ntransformer is time-consuming. In [ 21], Liu et al. proposed\ndividing the whole image into several windows; thus, the\nMSA operation can be conducted within each window\n(Fig. 2[layer l]). Such a module is called window-based\nMSA (W-MSA). To enable a token to exchange informa-\ntion with tokens in other windows, when it proceeds to the\nnext layer, the windows are shifted, as illustrated in\nFig. 2[layer l þ 1]; that is, the Swin transformer block of\nthe next layer uses the shifted W-MSA (SW-MSA) module\nfrom the second layer. Because the number of token pairs\nin each layer is reduced signiﬁcantly, the computational\ncost of the Swin transformer is much smaller than that of a\nstandard transformer. Besides the W-MSA and SW-MSA\nmodules, a token must also proceed through the Layer-\nNorm (LN) module, residual connections, and multi-layer\nperceptron (MLP) modules. As illustrated in Fig. 3, the l\nlayer takes the output of the previous layer, z\nl/C0 1, as the\ninput, and the input proceeds through the LN, W-MSA,\nLN, and MLP modules in sequence, while the output of the\nl layer, z\nl, proceeds through the SW-MSA instead of the W-\nMSA module. Such a procedure can be represented using a\nformula as follows:\n^zl ¼ W-MSAðLNðzl/C0 1ÞÞ þ zl/C0 1\nzl ¼ MLPðLNð^zlÞÞ þ ^zl\n^zlþ1 ¼ SW-MSAðLNðzlÞÞ þ zl\nzlþ1 ¼ MLPðLNð^zlþ1ÞÞ þ ^zlþ1\nð1Þ\nwhere ^zl and zl denote the output features of the (S)W-MSA\nmodule and the MLP module for Swin transformer block l,\nrespectively.\n3.2.2 Encoder\nAs mentioned in Sect. 3.2.1, the input image is divided into\npatches, and each patch is regarded as a token. In this\nstudy, the input image I is supposed to consist of H /C2 W\npixels, and it is equally divided into non-overlapping pixel\npatches. Then, the patch embedding layer converts the\npixel patches into tokens, which is followed by a series of\nSwin transformer blocks. By going through the ﬁrst layer\nof Swin transformer blocks, feature representations can be\nobtained from the patch tokens. Then, the patch merging\nlayer reduces the feature resolution while increasing fea-\nture dimensions, which produces a hierarchical represen-\ntation. Then, the downsized feature representations are\nfurther processed by the other two layers of the Swin\ntransformer blocks and patch merging layers; as a result,\nhierarchical feature representations of the input image can\nbe obtained.\nIn this study, each pixel patch can be deﬁned as P 2\nR\n4/C24/C23, and each token converted by patch embedding can\nbe denoted as v 2 R1/C21/C2C(C is set to 96). In the patch\nmerging layer, the feature resolution is down-sampled to a\nquarter of the input size, and the output feature dimension\nis enlarged to two times the values of the input features. In\nsummary, the encoder has three layers of Swin transformer\nblocks, which have 8, 4, and 4 blocks, respectively. In\naddition, the input resolutions of three Swin transformer\nlayers are H=4 /C2 W=4, H=8 /C2 W=8, and H=16 /C2 W=16,\nand their dimensions are C, 2C, and 4C, respectively.\n3.2.3 Decoder\nGiven the hierarchical feature representations extracted by\nthe encoder, our decoder outputs a recolored image of the\nsame resolution as the input image for CVD individuals.\nThe structure of the decoder is shown in Fig. 1. In our\ndecoder, two layers of Swin transformer blocks are adopted\nto reconstruct the hierarchical features. The ﬁrst layer of\nSwin transformer blocks (at the lower part of the decoder\nshown in Fig. 1) takes the ﬁnal feature from the encoder as\nthe input; the output is then fed into an up-sampling layer,\nwhich enlarges the feature resolution and reduces the fea-\nture dimension. Before the enlarged feature is fed into the\nupper layer of Swin transformer blocks, it is combined with\nthe middle layer feature of the Swin transformer blocks in\nthe encoder through skip connections. The output feature of\nthe upper layer of Swin transformer blocks is further\nenlarged by the second up-sampling layer. Combined with\nthe token produced by the patch embedding layer in the\nencoder, the enlarged feature is fed into the patch\nexpanding layer, and the resulting image can be obtained\nthrough a ﬁnal CNN block, which reduces the number of\nchannels to three.\nIn summary, our decoder has two layers of Swin trans-\nformer blocks, both of which consist of four blocks,\nrespectively. In addition, the resolutions of the output\nfeature maps of the up-sampling layers are H=8 /C2 W=8 and\nFig. 3 The architecture of two Swin transformer blocks (notation\npresented with Eq. 1)\nNeural Computing and Applications (2024) 36:6051–6066 6055\n123\nH=4 /C2 W=4, and their dimensions are 2C and C,\nrespectively.\n3.3 Loss function\nIn this study, the training of the proposed model is guided\nby a loss function only; in other words, no ground-truth\nresults are used to train the model. Considering the two\ngoals of the proposed model, that is, contrast enhancement\nand naturalness preservation, we expect the contrast in the\nCVD simulation for the resulting image, O, can be\nenhanced to the same level as that in the input image, I,\nwhile changes to the color appearance are minimized. To\ndo so, we introduce contrast and naturalness terms into our\nloss function. At the same time, we further divide the\ncontrast terms into global and local, given that both can be\nreduced due to CVD.\nTo calculate the contrast loss, the distance between\nCVD-simulated colors on two pixels in O is compared with\nthat between the colors of the corresponding pixels in I.\nThe contrast loss is calculated in the lab color space,\nconsidering that lab color space is more consistent with the\nhuman color perception system, and it can be represented\nas follows:\nCLðx; yÞ¼j j bc\n0\nx /C0 bc0\ny j/C0j cx /C0 cyjj\nbc0\nx ¼ CVDðc0\nxÞ; bc0\ny ¼ CVDðc0\nyÞ\nð2Þ\nwhere cx; cy; c0\nx; c0\ny denote the colors of the homologous\npixels x, y in I, O, respectively; CVDð/C1Þ stands for the CVD\nsimulation for a color and j/C1j represents the L1 norm of a\nvector. We adopt the model proposed by [ 31] for CVD\nsimulation. Because the model in [ 31] can simulate varying\ndegrees of CVD, we adopt the most severe degree in this\nstudy for the dichromacy simulation.\nFor the loss of local contrast, each pixel x in the\nrecolored image O is required to be compared with a set of\nneighboring pixels of x. The local contrast loss term is\ndeﬁned as follows:\nL\nl ¼\nXN\nx¼1\nX\ny2xx\nCLðx; yÞ\njjxxjj ð3Þ\nwhere xx stands for a set of pixels in a small window,\nwhich is centered on pixel x, and xx represents the number\nof pixels in xx; N is the number of pixels in an image. In\nthis study, the window size for local contrast loss is set to\n11 /C2 11.\nFor global contrast, the contrast loss of any pixel pair\n\\x; y [ randomly selected from the image for evaluation\nis calculated as follows:\nL\ng ¼ 1\njjxjj\nX\n\\x;y [ 2x\nCLðx; yÞ ð4Þ\nwhere x stands for a set of randomly selected pixel pairs,\nand x is the number of selected pixel pairs. x is set to 3000\naccording to a validation experiment conducted in this\nstudy, which will be discussed in Sect. 5.3. As a result, the\nintegrated contrast loss term is deﬁned as follows:\nLc ¼ bLg þð 2 /C0 bÞLl ð5Þ\nwhere b is the weight of the global contrast, we set the\nb ¼ 1 in the proposed method, which will be discussed in\nSect. 6.4. To preserve the naturalness of the original image,\nthe recolored image is required to be as similar to the\noriginal image as possible. To measure the similarity\nbetween the recolored and original images, we adopt the\nstructural similarity (SSIM) metric [ 32], which differs from\nthe color difference metric used in [ 14], as SSIM is known\nto be able to measure the perceptual similarity of images.\nGiven the pixels x, y in the test and reference images, the\nSSIM metric is calculated among local windows X, Y\ncentered on x, y, respectively. The formula of SSIM is\nwritten as follows:\nSSIMðX; YÞ¼\nð2uxuy þ c1Þð2rxy þ c2Þ\nðu2\nx þ u2\ny þ c1Þðr2\nx þ r2\ny þ c2Þ ð6Þ\nwhere ux and rx are the mean value and standard deviation\nof the pixels in X, and uy and ry are the mean value and\nstandard deviation of the pixels in Y; rxy is the covariance\nof x and y, c1 and c2 are small values for avoiding zero\ndivision set to be 0.0001 and 0.0009, respectively. In this\nstudy, the naturalness loss term is deﬁned as follows:\nLn ¼ 1 /C0 SSIMðO; IÞð 7Þ\nFinally, the loss function for guiding training is deﬁned as\nfollows:\nL ¼ aLn þð 1 /C0 aÞLc ð8Þ\nwhere a denotes the parameter to control the weight of\nnaturalness preservation over the contrast enhancement in\nthe recolored image. The higher the value of a, the more\nnatural the generated image will be. In this study, to clarify\nthe effect of a, we trained various models with different a\nvalues, which will be elaborated in Sect. 4.\n4 Dataset collection\nTo improve the capability of the proposed model to\nenhance the contrast between CVD-indistinguishable color\npairs, in this study, we created a new dataset consisting of\n6056 Neural Computing and Applications (2024) 36:6051–6066\n123\npictures of natural scenes and artiﬁcial images containing\nCVD-confusing colors.\n4.1 Natural image\nWe ﬁrst collected images from the Places365-Standard\ndataset [ 33]. However, not all images in the dataset [ 33]\ncontain colors confusing to CVD individuals. To improve\nthe training efﬁciency, we ﬁlter out images in which CVD\nindividuals have no difﬁculty discriminating colors. To do\nso, we assess the degree of contrast loss in the CVD sim-\nulation of the original image using the color contrast-pre-\nserving ratio (CCPR) metric introduced in [ 34], which is\ncalculated as follows:\njj\\^x; ^y [ j\\x; y [ 2 U and D ð^x; ^yÞ [ sjj\njjUjj ð9Þ\nwhere U represents a set of pixel pairs in the original\nimage; for an arbitrary pixel pair \\x; y [ in U, the con-\ntrast D(x, y) is greater than a threshold s. \\x; y [\nand\\^x; ^y [ indicate a homologous pixel pair in I and the\nCVD-simulated result of I; jjUjj stands for the number of\npixel pairs in this U set. In this paper, the contrast D\nbetween a pixel pair refers to the distance in the Lab color\nspace, and the threshold s is empirically set to 6.\nThe lower the CCPR value, the severer the degree of\ncontrast loss. In our implementation, we adopted images\nwhose CCPR value is smaller than 0.8 in our dataset. As a\nresult, we collected 31,000 natural images from the Pla-\nces365-Standard dataset.\n4.2 Artificial image\nTo make the proposed network more general and to learn\nhow to recolor the most confusing colors, we synthesized\nartiﬁcial images to enrich the dataset. If the distance of a\ncolor pair in the Lab color space is greater than the\nthreshold C\nv, while the distance of its CVD simulation\nresult is smaller than the threshold Cv, then this color pair is\ncalled a CVD-confusing color pair. And the threshold Cv is\nempirically set to 2.3, same as [ 35]. To obtain confusing\ncolor pairs, we traversed all colors in the RGB color space\nat a step length of 8 for each channel and then transferred\nthe colors to the Lab color space for evaluation. Finally, we\ncollected about 770,000 CVD-confusing color pairs. Then,\nwe randomly selected 4 to 10 pairs of confusing colors and\nput them on a white canvas; ﬁnally, we ﬁlled the whole\nimage using the nearest-neighbor interpolation method.\nExamples of artiﬁcial images are shown in Fig. 4a, c, and\ntheir CVD simulation results are shown in Fig. 4b, d,\nrespectively. In total, we generated 100,000 artiﬁcial ima-\nges to enrich our training dataset.\n5 Implementation and validation\n5.1 Implementation details\nIn this study, our model was implemented and trained on a\nPC with an Intel(R) Core(TM) i7-9800X CPU@3.80GHz,\n64-GB memory, and GeForce RTX 2080 Ti. Before the\ninput image are fed into the neural network, it will be\nresized to 256 /C2 256, and the patch size is set to 4. For the\ntraining phase, the Adam optimizer was adopted, whose\nhyperparameters were set as the learning rate lr = 0.0002,\nb1 = 0.5, and b2 = 0.999. To explore the effect of the\nweight a for the naturalness term, we trained our model\nwith three different naturalness weights a = 0.25, 0.5, and\n0.75, and the obtained models were denoted as\nOur\n025; Our050; Our075;respectively. To shorten the time for\ntraining, we pretrained a model without a naturalness\nconstraint by setting a = 0, and the model is denoted as\nOur\n000. Then, we ﬁne-tuned the model Our 000 using three\nkinds of naturalness weights to obtain the models\nOur\n025; Our050; Our075; respectively.\nTo determine the best number of pixel pairs for global\ncontrast enhancement and to verify the effectiveness of the\nSwin transformer block layers, we conducted two experi-\nments on a subset of our dataset that consisted of 10,000\nartiﬁcial images and 1000 natural images, and we tested the\ntrained models on 20,000 natural images.\n5.2 Effect of Swin transformer block layer\ncompared with CNN\nTo validate the effect of the Swin transformer layer, we\ncompared the proposed model constructed using Swin\ntransformer layers with the model based on CNN layers,\nwhich was used in the existing DNN-based recoloring\nmethod [ 18, 19].\nFirst, for a fair comparison of the effect of the network\nstructure, we excluded the global contrast term while\ntraining the models. Table 1 shows the average contrast\nloss of 20,000 recolored images by the two models. It can\nbe observed that the average contrast loss of images\nrecolored by the Swin transformer network is smaller than\nthat by the CNN network. An example of the recoloring\nFig. 4 Examples of artiﬁcially generated images for our dataset. a and\nc are artiﬁcial images; b and d are the CVD simulations of a and c,\nrespectively\nNeural Computing and Applications (2024) 36:6051–6066 6057\n123\nresults by the two networks is shown in Fig. 5. The red\nsquare in the original image (Fig. 5a) is hard to distinguish\nfrom the brown background in the CVD simulation. The\nrecoloring results of the Swin transformer-based network\nand CNN-based network are shown in Fig. 5b, c, respec-\ntively. Squares in both recoloring results can be easily\ndistinguished. In Fig. 5a, the triangle has the same red\ncolor as the square; in Fig. 5b, the triangle and the square\nare both recolored to blue. It is important to avoid recol-\noring the same colors differently, as colors are usually\nlinked to some particular semantic meanings. However, in\nFig. 5c, the triangle and the square are recolored differ-\nently, demonstrating that the Swin transformer-based net-\nwork could model long-range dependency successfully,\nwhile the CNN-based network failed to do so.\n5.3 Number of pixel pairs for capturing global\ncontrast\nIn Zhu et al.’s study [ 14], to enhance the global contrast,\nthe contrast loss of all color pairs was considered, which\nled to a high computation cost. Given the color coherence\nwithin a particular area, we assume that a relatively small\nnumber of randomly selected pixel pairs can also work well\nto capture the global color contrast. To determine empiri-\ncally an appropriate value for the number of pixels in x,w e\nconducted an experiment in which we trained our Our\n000\nmodel with x being set to 0, 1000, 3000, 5000, and 7000,\nrespectively. To evaluate the performance of the model\ntrained with different values of x, we calculated the con-\ntrast loss of the output images using Eq. 5, and the result is\nshown in Fig. 6. The smaller the value, the better the\nrecoloring result. As shown in Fig. 6, the contrast loss\ndecreases when the global points increase from 0 to 3000.\nHowever, when the global points are greater than 3000, the\ncontrast loss increases with an increase in global points.\nThe model trained with the parameter x = 3000 obtained\nthe best score; thus, all remaining models in this paper are\ntrained with the parameter x = 3000\n6 Evaluation experiments\nWe conducted experiments to compare the results of the\nproposed method with two state-of-the-art recoloring\nmethods [ 11, 14]. According to a recent survey [ 9], these\ntwo methods have the best performance among all existing\nmethods considering both contrast enhancement and natu-\nralness preservation. For the two existing CNN-based\nmethods [ 18, 19], we already demonstrated in Sect. 5.2 that\nour Swin transformer-based structure could achieve better\nresults in contrast enhancement. These two methods do not\nconsider naturalness preservation; therefore, we excluded\nthem from further evaluation experiments.\nFigures 7 and 8 show the recoloring results of the pro-\nposed and existing methods for protan and deutan CVD,\nrespectively. In each ﬁgure there two examples; for each\nexample, the ﬁrst row shows the input images (Figs. 7a and\n8a), the recolored images of Hassan et al. [ 11] (Figs. 7b\nand 8b), the recoloring results of Zhu et al. [ 14] (Figs. 7c\nand 8c), and the proposed methods Our\n025,Our050,Our075\nwith different naturalness weights, i.e., a = 0.25 (Figs. 7d\nand 8d), a = 0.5 (Figs. 7e and 8e), and a = 0.75 (Figs. 7f\nand 8f), respectively. The corresponding CVD simulation\nimages are aligned in the second row to help readers with\nnormal vision understand intuitively how the original and\nrecolored images are perceived by CVD individuals. The\nFig. 5 An example showing that the proposed Swin transformer\nnetwork-based model outperforms the existing CNN network-based\none in modeling long-range dependency. a original image; b result of\nthe Swin transformer network; c result of the CNN network\n Fig. 6 Contrast loss with different numbers of global points\nTable 1 Contrast loss between of CNN network and transformer\nnetwork\nNetwork Net CNN NetTransformer\nContrast loss 4.66e -1 3.08e -1\n6058 Neural Computing and Applications (2024) 36:6051–6066\n123\nCVD simulation images are obtained using the CVD sim-\nulation model proposed in [ 31].\nFor Example 1 in Fig. 7, the ﬂowers in the input image\n(Fig. 7a) are almost indistinguishable from the ground by\nindividuals with protanopia. For contrast enhancement, the\nresult by Hassan et al. [ 11] (Fig. 7b) added too much blue\ncomponent, leading to an unnatural appearance. The\nrecolored image by Our\n075 (Fig. 7f) is similar to that by\nZhu et al. [ 14] (Fig. 7c), which preserved naturalness well\nwhile failing to enhance the contrast between the ﬂowers\nand the background. The proposed models Our\n025 (Fig. 7d)\nand Our 050 (Fig. 7e) succeeded in enhancing the contrast in\nrecolored images, where the ﬂowers are easily distin-\nguishable and their color appearance is natural. For\nExample 2 in Fig. 7, the sun in the original image (Fig. 7a)\nis almost unnoticeable from the perception of CVD indi-\nviduals. The result by Hassan et al. [ 11] (Fig. 7b) changes\nthe color of the sun to blue, which is still difﬁcult to dis-\ntinguish from the blue sky. It seems Our\n075 (Fig. 7f) put so\nmuch weight on naturalness that it failed to enhance the\ncontrast between the sun and the sky. The recoloring results\nby Zhu et al. [ 14] (Fig. 7c), Our 025 (Fig. 7d), and Our 050\n(Fig. 7e) succeeded in preserving naturalness and enhanc-\ning contrast, that is, making the sun distinguishable. For\nExample 1 in Fig. 8, the contrast between the lotus leaves\nand the ﬂowers in the input image (Fig. 8a) is weakened for\nthose with deuteranopia. The result by Hassan et al. [ 11]\nadded too much value to the blue channel, resulting in the\nloss of naturalness, while their method also failed to\nenhance the color contrast. For the recolored image by Zhu\net al. [ 14], there is almost no contrast enhancement effect.\nMeanwhile, in the recolored images by Our\n025, Our 050, and\nOur075, the ﬂowers and leaves are easily distinguished. For\nExample 2 in Fig. 8, the contrast between the ﬂower and\nthe background is almost unnoticeable in the simulation of\nthe original image (Fig. 8a). The result by Hassan et al.\n[11] (Fig. 8b) signiﬁcantly changed the colors of the\nﬂower, but it added too much blue component, leading to\nan unnatural appearance. Further, the recolored images by\nZhu et al. [ 14] (Fig. 8c) and Our 075 (Fig. 8f) failed to\nenhance the contrast. Meanwhile, the recoloring result of\nOur025 (Fig. 8d) and Our 050 (Fig. 8e) succeeded in\nenhancing the contrast. But Our 025 (Fig. 8d) changed the\ncolor too much, while Our 050 (Fig. 8e) succeeded in pre-\nserving naturalness, keeping the ﬂowers as same as the\noriginal one.\nFig. 7 Results for the existing methods and proposed method (Protan). a Input image. b Result of Hassen. c Result of Zhu. d Result of Our 025. e\nResult of Our 050. f Result of Our 075\nNeural Computing and Applications (2024) 36:6051–6066 6059\n123\n6.1 Quantitative evaluation\nTo assess the proposed methods objectively in terms of\nboth naturalness and contrast, we conducted a quantitative\nexperiment using total color contrast (TCC) and SSIM\nmetrics to compare state-of-the-art studies on contrast\nenhancement and naturalness preservation, respectively.\nThe TCC metric calculates the absolute contrast in an\nimage, and it is deﬁned as follows:\nTCC ¼\n1\nn1\nX\nði;jÞ2X1\nðjxi /C0 xjjÞ\nþ 1\nN /C3 n2\nXN\ni¼1\nX\nj2X2\nðjxi /C0 xjjÞ\nð10Þ\nwhere X1 denotes a set of randomly selected global pixel\npairs, and n1 stands for the number of pixel pairs in X1; X2\nrepresents a set of pixels located at the window centered at\nan arbitrary pixel xi, and n2 indicates the number of pixels\nin X2; ﬁnally, N represents the number of pixels in the\nwhole image. In our experiment, n1 was set to 20,000, and\nthe window size for Xi was set to 11 /C2 11.\nTo validate the effectiveness of compensating for pro-\ntanopia and deuteranopia, 10 natural scene, plant, artiﬁcial\nobject, and painting images were selected for each type of\ncolor vision deﬁciency in this study. Table 2 shows the\nTCC results. The larger the value, the better the contrast-\nenhancing effect. The top three scores are indicated in\nascending order by ***, **, and *, respectively. This shows\nthat for protan CVD, Hassan et al. [ 11] achieved the best\nscore, while the proposed models Our\n025 and Our 050\nachieved higher scores than the model proposed by Zhu\net al.[ 14]; for deutan CVD, the proposed model Our 025\nachieved the best score, and all proposed models outper-\nform that in [ 14].\nWe used chromatic difference(CD) metric as the metric\nfor evaluating naturalness preservation. CD is computed in\nthe CIE Lab color space, and it can be computed as:\nCDðiÞ¼\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nkðl\n0\ni /C0 liÞ2 þð a\n0\ni /C0 aiÞ2 þð b\n0\ni /C0 biÞ2\nq\nð11Þ\nFig. 8 Results for the existing methods and proposed method (Deutan). a Input image. b Result of Hassen. c Result of Zhu. d Result of Our 025. e\nResult of Our 050. f Result of Our 075\nTable 2 Result for the total color contrast metric (contrast)\nCVD Type Hassan Zhu Our 025 Our050 Our075\nProtan 0.725*** 0.668 0.711** 0.670* 0.636\nDeutan 0.879** 0.776 0.898*** 0.851* 0.792\n6060 Neural Computing and Applications (2024) 36:6051–6066\n123\nwhere i ranges over the pixels in the image; l\n0\ni; li; a\n0\ni; ai; b\n0\ni; bi\nare the L,a,b value of homologous pixel i in the test image\nand reference image. By setting k ¼ 0, same metric is\nadopted in the [ 14]. The same images used in the quanti-\ntative evaluation for contrast are used to evaluate the nat-\nuralness metric. The average scores are shown in Table 3.\nThe bigger the value, the better the result. The ﬁrst-, sec-\nond-, and third-place scores are indicated by ***, **, and *,\nrespectively. This shows that for protan, images recolored\nby Our\n050 and Our 075 achieved better scores than those by\n[11, 14]; for deutan, images recolored by Our 075 achieved a\nbetter score than those by [ 11, 14].\nThe result of the quantitative experiment will be further\ndiscussed in the Discussion section.\n6.2 Subjective evaluation\nTo evaluate subjectively the performance of the proposed\nmethod in comparison with state-of-the-art methods, we\ninvited 18 CVD volunteers aged from 18 to 50 years. First,\nwe used the Ishihara test and Farnsworth Panel D-15 test to\ndiagnose their CVD types, where 5 volunteers were diag-\nnosed with protan CVD and 13 with deutan CVD. CVD\nvolunteers were asked to evaluate recolored images\naccording to three aspects: contrast, naturalness, and pref-\nerence. The same images used in the quantitative experi-\nment were used in the subjective evaluation experiment. In\nthis study, all images were presented on an Iiyama ProLite\n23-inch display calibrated using the X-Rite i1Display 2\nunder ambient illuminance below 10 lux, and all partici-\npants were siting half a meter from the display. Given one\ninput image, the recoloring results produced by ﬁve dif-\nferent methods are presented. In other words, six images in\ntotal, including one input image and ﬁve recolored images,\nwere shown to the participants at a time. And the partici-\npants were notiﬁed which one is input image. We use the\nboxplot to display the score by all subjects on all images\nfor contrast, naturalness, and preference, respectively, and\nuse the solid square to present the median. We also con-\nducted the Mann Whitney Test to analyze the results. The\nresults are shown in Figs. 9 and 10. In the following\ndescriptions, if we say A is signiﬁcantly different from B, it\nmeans that A has a signiﬁcantly better score than B at a\nsigniﬁcance level of 5 %.\nContrast Participants were required to compare the\nrecolored images with the original image and to rate the\ndegree of contrast in the recolored image on a scale of 1 to\n5, where ‘‘1: contrast decreased signiﬁcantly,’’ ‘‘2: contrast\ndecreased slightly,’’ ‘‘3: almost the same with the original\nimage,’’ ‘‘4: contrast enhanced slightly,’’ and ‘‘5: contrast\nenhanced signiﬁcantly.’’ As shown in Fig. 9, for protan\nCVD, Our\n025, Zhu et al. [ 14], and Our 050 are in the top\nthree, and Our 025 is signiﬁcantly different from Zhu et al.\n[14], while Zhu et al. [ 14] is signiﬁcantly different from\nOur075. For deutan CVD,as shown in Fig. 10, Our 025,\nOur050, and Hassan et al. [ 11] are in the top three. Mean-\nwhile, Our 025 and Our 050 are signiﬁcantly different from\nZhu et al. [ 14], and Hassan et al. [ 11] is signiﬁcantly dif-\nferent from Our 075.\nNaturalness Similar to the contrast evaluation, all par-\nticipants were asked to compare the recolored images with\nthe original image and to rate the statement, ‘‘The color\nappearance of the recolored image is similar to the original\nimage,’’ on a scale of 1 to 5, where ‘‘1: totally disagree,’’\nFig. 9 The result of the subjective experiment. (Protan)\nFig. 10 The result of the subjective experiment. (Deutan)\nTable 3 Result for the CD metric (naturalness)\nCVD type Hassan Zhu Our 025 Our050 Our075\nProtan 0.923 0.992* 0.972 0.995** 0.999***\nDeutan 0.899 0.995** 0.963 0.983* 0.998***\nNeural Computing and Applications (2024) 36:6051–6066 6061\n123\n‘‘2: disagree,’’ ‘‘3: neutral,’’ ‘‘4: agree,’’ and ‘‘5: totally\nagree.’’ As shown in Fig. 9, for protan CVD, Our 075,\nOur050, and Zhu et al. [ 14] are in the top three.\nOur075,Our050 and Our 025 are all signiﬁcantly different\nfrom Hassan et al. [ 11]. Our 075 and Our 050 are signiﬁcantly\ndifferent from Zhu et al. [ 14], while Zhu et al. [ 14] is only\nsigniﬁcantly from Our 025. For deutan CVD, as shown in\nFig. 10, Our 075, Zhu et al. [ 14], and Our 050 are in the top\nthree. Our 075,Our050 and Our 025 are all signiﬁcantly dif-\nferent from Hassan et al. [ 11]. And Our 075 is signiﬁcantly\ndifferent from Zhu et al. [ 14]. Zhu et al. [ 14] is signiﬁcantly\nfrom Our 025 and Our 050. For the proposed models, which\nwere trained with different a values, that is, weights of\nnaturalness, the scores become higher with an increasing a.\nPreference As a comprehensive evaluation of both\ncontrast and naturalness, CVD participants were asked to\nevaluate recolored images according to their preference\nand to rate the statement, ‘‘The recolored image is prefer-\nable from the aspects of both contrast and naturalness,’’ on\na scale of 1 to 5, where ‘‘1: totally disagree,’’ ‘‘2: dis-\nagree,’’ ‘‘3: neutral,’’ ‘‘4: agree,’’ and ‘‘5: totally agree.’’ As\ncan be observed from Figs. 9 and 10, for protan CVD,\nOur\n075, Our 050, and Zhu et al. [ 14] are in the top three.\nOur075,Our050 and Our 025 are all signiﬁcantly different\nfrom Hassan et al. [ 11]. Our 075 and Our 050 are signiﬁcantly\ndifferent from Zhu et al. [ 14], while Zhu et al. [ 14] is only\nsigniﬁcantly from Our 025; for deutan CVD, Our 075, Zhu\net al. [ 14], and Our 050 are in the top three. Our 075,Our050\nand Our 025 are all signiﬁcantly different from Hassan et al.\n[11]. And Our 075 is signiﬁcantly different from Zhu et al.\n[14]. Zhu et al. [ 14] is signiﬁcantly from Our 025 and Our050.\nFrom the contrast and naturalness evaluation results, we\ncan see a tradeoff between the two, as shown in Fig. 11.\nNevertheless, all scores of the Our 050 model are higher than\n3.0, meaning it can improve the contrast but also maintain\nnaturalness for both protan and deutan users. Moreover, its\npreference score is also larger than 3.0 for both protan and\ndeutan cases, meaning the recolored images are preferred\nby all users. The result of the subjective experiment will be\nfurther considered in the Discussion section.\n6.3 Visualization experiment\nBesides the experiment with natural images, we also con-\nducted an evaluation experiment using images without the\nissue of naturalness preservation. In such a case, a recol-\noring method is usually required to enhance the perceiv-\nability of the information contained in the image, which is\nvisible to individuals with normal vision and almost\ninvisible to people with CVD. In this study, we took ﬁg-\nures from the Ishihara test as input images and compared\nthe recoloring results of the proposed model Our\n000 with\nthose of the methods in [ 11] and [ 14]. In [ 14], they set a\nsmaller weight, 0.01, for naturalness in the objective\nfunction when applying the model to the Ishihara images.\nWe used the same weight in our experiment. The results of\nthe two examples are shown in Figs. 12 and 13. The ﬁrst\nrow of each ﬁgure shows the input image (Figs. 12a, and\n13a) and the recoloring results of Hassan et al. [ 11]\n(Figs. 12b and 13b), Zhu et al. [ 14] (Figs. 12c and 13c),\nand Our\n000 (Figs. 12d and 13d), while the second row\nshows the CVD simulation results for the images in the ﬁrst\nrow.\nIndividuals with protanopia have difﬁculty recognizing\nthe ﬁgure ‘‘5’’ in Fig. 12a; given the recoloring result of\nFig. 11 The subjective experiment result of the proposed method.\na the result for Protan; b the result for Deutan\nFig. 12 Recoloring results of Ishihara images using existing methods\nand the proposed method (Protan). a original image. b result of\nHassan et al. [ 11] c result of Zhu et al. [ 14]. d Result of Our 000\n6062 Neural Computing and Applications (2024) 36:6051–6066\n123\n[11] (Fig. 12b), it is still very difﬁcult for people with\nprotanopia to recognize the correct ﬁgure, as depicted by\nthe simulation images. With the recoloring result of [ 14]\n(Fig. 12c) or Our 000 (Fig. 12d), CVD individuals can read\nthe ﬁgure ‘‘5’’ correctly. Individuals with deuteranopia\nhave difﬁculty recognizing the ﬁgure ‘‘97’’ in Fig. 13a;\ngiven the recoloring result of [ 11] (Fig. 13b) or [ 14]\n(Fig. 13c), it is still difﬁcult for people with deuteranopia\nto recognize the ﬁgure, as depicted by the simulation\nimages. With the recoloring result of Our\n000, though the\ncolor appearance is changed signiﬁcantly, CVD individuals\ndo not have any problem reading the ﬁgure. In this\nexperiment, 16 Ishihara images in total were selected and\npresented to the participants, together with the recoloring\nresults, and the evaluation results are shown in Table 4.\nEach value in Table 4 indicates the number of recolored\nimages that could be correctly identiﬁed by the participant,\nand the best score of each participant is in bold. In contrast\nto the recoloring results of [ 11] and [ 14], which only par-\ntially assisted CVD individuals in passing the Ishihara test,\nOur\n000 enabled CVD individuals to recognize all the ima-\nges. Hassan et al.’s [ 11] method may result in images with\na saturated blue channel, and it may fail to enhance the\ncontrast in the areas containing colors consisting of blue\ncomponents. Zhu et al. [ 14] recolored key colors and then\npropagated the results to all colors. If the color of the\nﬁgure does not take up a substantial proportion, their\nmethod will fail to enhance the contrast between the ﬁg-\nure and the background.\n6.4 Time efficiency evaluation\nTo evaluate the time efﬁciency, we run all the methods on\nthe same machine on which we implemented our model in\nthe experiment. On average, our method takes less than\n0.02s to process a 256 /C2 256 image, which is faster than\nthe 0.08s of Hassan et al.’s method [ 11] and the 7.16s of\nZhu et al.’s method [ 14].\n6.5 Different global contrast weight\nIn this study, we initially set the weights for global contrast\nand local contrast in a 1:1 ratio. We conducted an evalu-\nation of the model’s performance across varying ratios of\nglobal contrast to local contrast to determine the appro-\npriateness of this conﬁguration. To maintain the contrast\nand naturalness ratio as deﬁned in Eq. 5, we ensured that\nthe sum of the contrast weights equaled 2. Assuming the\nglobal weight is denoted as b, the weight of the local\ncontrast was calculated as 2 - b. And we adopt the Our\n050\nmodel, and the results are presented in Table 5.A s\nobserved, increasing the weight of global contrast leads to\nan increase in the contrast evaluation value but a decrease\nin the naturalness evaluation value. Therefore, for a bal-\nanced trade-off between contrast and naturalness, a setting\nof b ¼ 1 is deemed reasonable.\n6.6 Ablation study\nIn our study, there are global contrast(GC), local con-\ntrast(LC) and naturalness(N) components in the loss\nfunction Eq. 8. To test the impact of each component, we\nconducted an ablation study where we removed GC, LC, N\ncomponent from Eq. 8 respectively. And we use the a ¼\n0:5 in Eq. 8. The evaluation result is shown in Table 6.\nWhen focusing solely on the contrast component, we\nobserve that the contrast value is the best, but the natu-\nralness value is the lowest. When considering the contrast\ncomponent separately - global contrast alone yields a\nconsiderably high contrast value, whereas local contrast\nyields a signiﬁcantly lower contrast value. Remarkably, the\ncombination of both global and local contrast results in a\nsatisfactory contrast value, while preserving a high level of\nnaturalness.\nFig. 13 Recoloring results of Ishihara images using existing methods\nand the proposed method (Deutan). a original image. b result of\nHassan et al. [ 11] c result of Zhu et al. [ 14]. d Result of Our 000\nTable 4 The number of correctly recognized Ishihara test images\nCVD type Original Hassan Zhu Our 000\nProtan 1.4 7.4 11 16\nDeutan 1.8 7.6 11.2 16\nTable 5 Contrast evaluation value and naturalness evaluation value\nwith different global weight\nGlobal weight ( b) 0 0.5 1 1.5 2\nContrast 0.732 0.759 0.76 0.765 0.772\nNaturalness 0.999 0.996 0.996 0.992 0.988\nNeural Computing and Applications (2024) 36:6051–6066 6063\n123\n7 Discussion\nIn this study, the resolution of image is 256 /C2 256, so if we\nwant to deal with larger images, we can reduce the image\nto 256 /C2 256 ﬁrst, and then use the state-of-the-art super-\nresolution method (e.g., Wang et al. [ 36]) to enlarge the\nrecolored image. We proposed a DNN-based image\nrecoloring model, which was trained with different natu-\nralness weights, namely, Our\n025, Our 050, and Our 075.W e\nalso conducted qualitative, quantitative, and subjective\nexperiments to evaluate these models and compare them\nwith state-of-the-art studies. The results of quantitative and\nsubjective evaluation experiments show that with the\nincrease in the naturalness weight, the scores of naturalness\nand preference increased, while the contrast score\ndecreased. To further investigate how contrast and natu-\nralness impact the preference, we computed two correlation\ncoefﬁcients, one between contrast and preference and the\nother between naturalness and preference. Figures 14 and\n15 show the plots of all subjects, with the x-axis repre-\nsenting the coefﬁcient between contrast and preference and\nthe y-axis representing the coefﬁcient between naturalness\nand preference We can clearly see that for naturalness,\nthere is a high correlation with preference for almost all\nsubjects. However, for contrast, correlations vary largely\nby subject. First, this means that preserving naturalness is\ncrucial when considering user preferences. Nevertheless,\nthe requirements of CVD compensation may vary by\nsituation, and contrast enhancement may become the most\nimportant characteristic if recognizing the detailed infor-\nmation in an image is critical, such as in the visualization\nexperiment shown in Sect. 6.3. One fact is that the sub-\njective contrast experiment did not use such a task, and the\nimages presented are pictures or paintings of natural scenes\nand still life, where the color appearance is likely a more\ndominant factor in affecting subjects’ preferences. There-\nfore, pre-training a set of recoloring models with different\ncontrast and naturalness weights and then selecting an\nappropriate model according to the particular situation and\nuser preferences is a promising way to use our method in\nreal applications.\nFigure 16 shows an example where all methods failed to\nenhance the color contrast between the purple ball and blue\nballs. Our current models consider the relationships\nbetween sample pixels, and it may be possible to solve\nsuch a problem by considering the relationships between\nobjects. In the current study, contrasts in all areas of an\ninput image are treated with the same importance; how-\never, contrast enhancement to the RoI (region of interest)\nin the image should be much more important to CVD users.\nThus, incorporating some RoI prediction approach, such as\nthe salient detection (e.g., Zhao et al. [ 37]) technique, to the\nrecoloring model can be another important future work. By\nrelaxing the constraints on contrast enhancement in less-\nsalient areas, it becomes easier to restrict the contrast\nenhancement to salient areas while better preserving nat-\nuralness in less-salient areas.\n8 Conclusion\nIn this paper, we proposed a DNN-based image recoloring\nmethod for generating compensation images for CVD\nindividuals that considers contrast enhancement and natu-\nralness preservation. Considering long-range color depen-\ndency, we adopted Swin transformer blocks in the\nFig. 14 The correlation coefﬁcient of contrast and naturalness with\nthe preference of each CVD individual with protanopia\nFig. 15 The correlation coefﬁcient of contrast and naturalness with\nthe preference of each CVD individual with deuteranopia\nTable 6 Ablation study with different loss combination\nGC LC N Contrast value Naturalness value\nUUU 0.760 0.996\nUU 0.819 0.935\nUU 0.772 0.988\nUU 0.732 0.999\n6064 Neural Computing and Applications (2024) 36:6051–6066\n123\nproposed network. The evaluation experiment showed that\nthe proposed model achieved image qualities that are\ncompetitive with state-of-the-art studies while improving\ntime efﬁciency drastically. The proposed unsupervised\ntraining approach, as well as the dataset containing images\nwith confusing colors for CVD individuals, can be used to\ndevelop and evaluate other image synthesis technologies\nfor assisting CVD users. Currently, we set people with\ndichromacy as our compensation target; however, CVD\nindividuals’ perceptions differ from person to person, and\nthe recoloring results may be unsuitable for individuals\nwith lower degrees of CVD. Therefore, adapting the\nrecoloring model to the degree of CVD can be important\nfuture work. Additionally, we could expand our research\nfrom images to videos in the future. In videos, we can use a\nvisual tracking method described in [ 38] to follow objects\nand ensure their colors remain consistent from one frame to\nthe next.\nAuthor contributions CL, ZZ, and MX designed the algorithm and\nthe network architecture. CL was responsible for the implementation\nand for conducting the experiments. HW, GK, and CX contributed to\nthe design of the experiments and the analysis of the results. CL\nserved as the primary author of the paper. ZZ and CX reviewed the\npaper and provided valuable feedback, while MX played a crucial\nrole in enhancing the manuscript’s quality. The funding for this\nproject was provided by MX and ZZ.\nFunding Open Access funding provided by University of Yamanashi.\nThis work is supported by JSPS Grants-in-Aid for Scientiﬁc Research\n(Grant Nos. 20K20408, 22H00549, 22K21274, 23K16899). The\nauthors would like to thank all participants for evaluating the pro-\nposed method and their constructive comments.\nData availability All data generated or analyzed during this study are\nincluded in this published article.\nCode availability The code and model will be made available at\nhttps://github.com/Ligeng-c/CVD_swin.\nDeclarations\nConflict of interest The authors declare that they have no competing\ninterests\nEthics approval The experimental protocol was established, accord-\ning to the ethical guidelines of the Helsinki Declaration and was\napproved by the Ethics Committee of University of Yamanashi.\nConsent to participate Written informed consent was obtained from\nindividual or guardian participants.\nConsent for publication Written informed consent was obtained from\nindividual or guardian participants.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n1. Sharpe LT, Stockman A, Ja ¨ gle H, Nathans J (1999) Opsin genes,\ncone photopigments, color vision, and color blindness. Color Vis:\nFenes Percept: 3–51\n2. Hunt RWG (2005) The Reproduction of Colour. Wiley, New\nYork\n3. Wakita K, Shimamura K. Smartcolor: disambiguation framework\nfor the colorblind. In: Proceedings of the 7th international ACM\nSIGACCESS conference on computers and accessibility,\npp 158–165\n4. Iaccarino G, Malandrino D, Del Percio M, Scarano V. Efﬁcient\nedge-services for colorblind users. In: Proceedings of the 15th\ninternational conference on World Wide Web, pp 919–920\nFig. 16 The correlation coefﬁcient of contrast and naturalness with the preference of each CVD individual with deuteranopia\nNeural Computing and Applications (2024) 36:6051–6066 6065\n123\n5. Jefferson L, Harvey R. Accommodating color blind computer\nusers. In: Proceedings of the 8th international ACM SIGACCESS\nconference on computers and accessibility, pp 40–47\n6. Machado GM, Oliveira MM. Real-time temporal-coherent color\ncontrast enhancement for dichromats. In: Computer graphics\nforum, Wiley Online Library, vol. 29, pp 933–942\n7. Lin HY, Chen LQ, Wang ML (2019) Improving discrimination in\ncolor vision deﬁciency by image re-coloring. Sensors (Basel)\n19(10):2250. https://doi.org/10.3390/s19102250\n8. Ribeiro M, Gomes AJP (2019) Recoloring algorithms for color-\nblind people: a survey. Acm Comput Surv 52(4):1–37. https://doi.\norg/10.1145/3329118\n9. Zhu Z, Mao X (2021) Image recoloring for color vision deﬁ-\nciency compensation: a survey. Vis Comput 37(12):2999–3018.\nhttps://doi.org/10.1007/s00371-021-02240-0\n10. Hassan MF, Paramesran R (2017) Naturalness preserving image\nrecoloring method for people with red–green deﬁciency. Sign\nProcess: Image Commun 57:126–133\n11. Hassan MF (2019) Flexible color contrast enhancement method\nfor red–green deﬁciency. Multidimens Syst Sign Process\n30(4):1975–1989. https://doi.org/10.1007/s11045-019-00638-7\n12. Zhu Z, Toyoura M, Go K, Fujishiro I, Kashiwagi K, Mao X\n(2019) Naturalness- and information-preserving image recoloring\nfor red–green dichromats. Sign Process: Image Commun\n76:68–80. https://doi.org/10.1016/j.image.2019.04.004\n13. Zhu Z, Toyoura M, Go K, Fujishiro I, Kashiwagi K, Mao X\n(2019) Processing images for red–green dichromats compensa-\ntion via naturalness and information-preservation considered\nrecoloring. Vis Comput 35(6–8):1053–1066. https://doi.org/10.\n1007/s00371-019-01689-4\n14. Zhu Z, Toyoura M, Go K, Kashiwagi K, Fujishiro I, Wong T-T,\nMao X (2022) Personalized image recoloring for color vision\ndeﬁciency compensation. IEEE Trans Multimed 24:1721–1734.\nhttps://doi.org/10.1109/tmm.2021.3070108\n15. Wang X, Zhu Z, Chen X, Go K, Toyoura M, Mao X (2021) Fast\ncontrast and naturalness preserving image recolouring for\ndichromats. Comput Graph 98:19–28. https://doi.org/10.1016/j.\ncag.2021.04.027\n16. Huang WK, Zhu ZY, Chen LG, Go K, Chen XD, Mao XY (2022)\nImage recoloring for red–green dichromats with compensation\nrange-based naturalness preservation and reﬁned dichromacy\ngamut. Vis Comput 38(9–10):3405–3418. https://doi.org/10.\n1007/s00371-022-02549-4\n17. Isola P, Zhu J-Y, Zhou T, Efros AA. Image-to-image translation\nwith conditional adversarial networks. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition,\npp 1125–1134\n18. Li HS, Zhang L, Zhang XD, Zhang ML, Zhu GM, Shen PY, Li P,\nBennamoun M, Shah SAA (2020) Color vision deﬁciency data-\nsets and recoloring evaluation using gans. Multimed Tools Appl\n79(37–38):27583–27614. https://doi.org/10.1007/s11042-020-\n09299-2\n19. Xinghong H, Xueting L, Zhuming Z, Menghan X, Chengze L,\nTien-Tsin W (2019) Colorblind-shareable videos by synthesizing\ntemporal-coherent polynomial coefﬁcients. ACM Trans Graph\n38(6):1–12. https://doi.org/10.1145/3355089.3356534\n20. Ronneberger O, Fischer P, Brox T. U-net: convolutional networks\nfor biomedical image segmentation. In: International conference\non medical image computing and computer-assisted intervention,\nSpringer, pp 234–241\n21. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B\n(2021) Swin transformer: hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:.14030\n22. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser L, Polosukhin I (2017) Attention is all you need. Adv\nNeural Inf Process Syst 30\n23. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X,\nUnterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S\n(2020) An image is worth 16x16 words: transformers for image\nrecognition at scale. arXiv preprint arXiv:.11929\n24. Jefferson L, Harvey R (2007) An interface to support color blind\ncomputer users. In: Proceedings of the SIGCHI conference on\nhuman factors in computing systems, pp 1535–1538\n25. Judd DB (1966) Fundamental studies of color vision from 1860 to\n1960. Proc Natl Acad Sci U S A 55(6):1313–30. https://doi.org/\n10.1073/pnas.55.6.1313\n26. Kuhn GR, Oliveira MM, Fernandes LA (2008) An efﬁcient nat-\nuralness-preserving image-recoloring method for dichromats.\nIEEE Trans Vis Comput Graph 14(6):1747–54. https://doi.org/10.\n1109/TVCG.2008.112\n27. Huang HB, Tseng YC, Wu SI, Wang SJ (2007) Information\npreserving color transformation for protanopia and deuteranopia.\nIEEE Sign Process Lett 14(10):711–714. https://doi.org/10.1109/\nLsp.2007.898333\n28. Ebelin P, Crassin C, Denes G, Oskarsson M, A ˚stro¨ m K, Akenine-\nMo¨ ller T (2023) Luminance-preserving and temporally\nstable daltonization. In: EUROGRAPHICS 2023, the 44th annual\nconference of the European association for computer graphics.\nEurographics-European association for computer graphics\n29. Jiang S, Liu D, Li D, Xu C (2023) Personalized image generation\nfor color vision deﬁciency population. In: Proceedings of the\nIEEE/CVF international conference on computer vision,\npp 22571–22580\n30. Karras T, Aittala M, Hellsten J, Laine S, Lehtinen J, Aila T\n(2020) Training generative adversarial networks with limited\ndata. Adv Neural Inf Process Syst 33:12104–12114\n31. Machado GM, Oliveira MM, Fernandes LA (2009) A physio-\nlogically-based model for simulation of color vision deﬁciency.\nIEEE Trans Vis Comput Graph 15(6):1291–8. https://doi.org/10.\n1109/TVCG.2009.113\n32. Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Image\nquality assessment: from error visibility to structural similarity.\nIEEE Trans Image Process 13(4):600–12. https://doi.org/10.1109/\ntip.2003.819861\n33. Zhou B, Lapedriza A, Khosla A, Oliva A, Torralba A (2018)\nPlaces: a 10 million image database for scene recognition. IEEE\nTrans Patt Anal Mach Intell 40(6):1452–1464. https://doi.org/10.\n1109/TPAMI.2017.2723009\n34. Lu C, Xu L, Jia J. Contrast preserving decolorization. In: 2012\nIEEE international conference on computational photography\n(iccp), IEEE, pp 1–7\n35. Hu XH, Zhang ZM, Liu XT, Wong TS (2019) Deep visual\nsharing with colorblind. IEEE Trans Comput Imag 5(4):649–659.\nhttps://doi.org/10.1109/Tci.2019.2908291\n36. Wang X, Xie L, Dong C, Shan Y. Real-esrgan: training real-\nworld blind super-resolution with pure synthetic data. In: Pro-\nceedings of the IEEE/CVF international conference on computer\nvision, pp 1905–1914\n37. Zhao T, Wu X. Pyramid feature attention network for saliency\ndetection. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp 3085–3094\n38. Yang K, He Z, Pei W, Zhou Z, Li X, Yuan D, Zhang H (2021)\nSiamcorners: Siamese corner networks for visual tracking. IEEE\nTrans Multimed 24:1956–1967\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\n6066 Neural Computing and Applications (2024) 36:6051–6066\n123"
}