{
  "title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
  "url": "https://openalex.org/W4389523738",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2556632840",
      "name": "Alon Goldstein",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093134537",
      "name": "Miriam Havin",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A1972118651",
      "name": "Roi Reichart",
      "affiliations": [
        "Decision Sciences (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2285538401",
      "name": "Ariel Goldstein",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4327893302",
    "https://openalex.org/W4389520455",
    "https://openalex.org/W4214872993",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4380714623",
    "https://openalex.org/W3011489899",
    "https://openalex.org/W1832238961",
    "https://openalex.org/W4399281666",
    "https://openalex.org/W2075780421",
    "https://openalex.org/W2138921773",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2983144399",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2978368159",
    "https://openalex.org/W2081056419",
    "https://openalex.org/W2161080627",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W4220949944",
    "https://openalex.org/W2136094593",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W3125491424",
    "https://openalex.org/W4361855702",
    "https://openalex.org/W2148117181",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2972045671",
    "https://openalex.org/W3126893361",
    "https://openalex.org/W2145524985",
    "https://openalex.org/W2130158090"
  ],
  "abstract": "This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our findings reveal that the new-generation LLMs excel in solving stumpers and surpass human performance. However, humans exhibit superior skills in verifying solutions to the same problems. This research enhances our understanding of LLMs’ cognitive abilities and provides insights for enhancing their problem-solving potential across various domains.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11644–11653\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDecoding Stumpers: Large Language Models vs. Human Problem-Solvers\nAlon Goldstein1 Miriam Havin2 Roi Reichart3 Ariel Goldstein1245\n* Corresponding Author: alon@xoltar.com\n1 Xoltar Inc\n2 Department of Cognitive and Brain Sciences, Hebrew University, Jerusalem, Israel\n3 Faculty of Data and Decision Sciences, Technion\n4 The Hebrew University Business School, Jerusalem, Israel\n5 Google Research\nAbstract\nThis paper investigates the problem-solving capabilities\nof Large Language Models (LLMs) by evaluating their\nperformance on stumpers, unique single-step intuition\nproblems that pose challenges for human solvers but\nare easily verifiable. We compare the performance of\nfour state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-\n3.5-Turbo, GPT-4) to human participants. Our findings\nreveal that the new-generation LLMs excel in solving\nstumpers and surpass human performance. However,\nhumans exhibit superior skills in verifying solutions\nto the same problems. This research enhances our un-\nderstanding of LLMs’ cognitive abilities and provides\ninsights for enhancing their problem-solving potential\nacross various domains1\n1 Introduction\nSince their inception, Large Language Models\n(LLMs) have astonished the scientific community\nwith their ability to tackle complex tasks (Rad-\nford et al., 2019; Brown et al., 2020; Devlin et al.,\n2018). These emerging capabilities, along with\nshared principles with human cognition and the\nbrain, have motivated significant efforts to utilize\ndeep language models and, recently, LLMs for ex-\nplaining neural activity (Tikochinski et al., 2023;\nGoldstein et al., 2022b,a; Schwartz et al., 2019),\npredicting human behavior (Goldstein et al., 2022b;\nBrand et al., 2023), and even providing a theo-\nretical framework for the human mind (Richards\net al., 2019; Hasson et al., 2020). Recent advance-\nments, particularly the ability of LLMs to perform\ntasks requiring different skills such as mathemat-\nical calculations, analytical reasoning and use of\nworld knowledge, have led several papers to de-\nclare that LLMs possess what is termed in the\n1The data is available athttps://github.com/Alon-Go/\nStumpers-LLMs.\ncognitive literature System 2 capabilities (Matsuo,\n2020; Kojima et al., 2022). The dual-system model\nof the mind has arguably been the most prevalent\nmodel of thought and behavior in psychology, eco-\nnomics and social science in general (Goldstein and\nYoung, 2022; Evans and Stanovich, 2013; Chaiken\nand Trope, 1999; Gawronski and Creighton, 2013),\nespecially in addressing systematic limitations of\ncognitive and artificial systems. In simple terms,\nSystem 1 is associated with effortless, associative\nprocesses and is often thought of as compatible\nwith neural nets implementation, while System 2\nis related to effortful, serial, and often symbolic\nprocesses (Frankish, 2010; Evans, 2003). A fa-\nmous example where System 1’s heuristic hinders\na solution is described in Box 1.\nBox 1: The bat and the ball\n\"A bat and a ball cost 1.10 dollars in total.\nThe bat costs 1 dollar more than the ball.\nHow much does the ball cost?\"\nThe immediate but incorrect response is to\nassume that the ball costs 10 cents. How-\never, a symbolic-serial approach formula-\ntion of the problem “x + (x + 1) = 1.10”\nyields the correct solution of 0.05 dollars.\nWhile this type of questions (Cognitive Reflec-\ntive Test; CRT) are considered hard to solve, as they\ntend to elicit wrong responses (Frederick, 2005;\nToplak et al., 2011), people can be primed to solve\nthem correctly by insisting on a formalist approach\n(i.e., applying System 2 instead of System 1; (Alter\net al., 2007)). In contrast, problems that require\ninsight (i.e., have neither an intuitive/associative so-\nlution nor a symbolic one) are hard for humans and\noften elicit no response (i.e., humans are \"stuck\";\n(Bar-Hillel et al., 2018; Bar-Hillel, 2021)).\n11644\nConsider, for example, the riddle in Box 2:\nBox 2: Blood relatives\n\"Alex is Bobbie’s blood relative, and Bob-\nbie is Charlie’s blood relative, but Alex\nis not a blood relative of Charlie. How\ncome?\".\nAnswer: Alex and Charlie can be related to\nBobbie from different sides of the family,\ne.g., they could be his parents, uncles, etc.\nThis riddle typically challenges human intuition\n(System 1) because humans tend to consider Alex,\nBobbie, and Charlie as blood relatives. However,\na symbolic solution (System 2) is typically also\nnot available to humans who try to solve it, as\nthere is no clear algorithm to follow to reach a\nsolution. Facing this question, humans seem to be\nanchored (or stuck) in the framing according to\nwhich the three men are blood relatives and cannot\nescape it to generate an alternative framing of the\nproblem that would yield effective explanations of\nthe situation (Bar-Hillel, 2021).\nThe above-mentioned question is an example of\na stumper. A stumper is a one-step intuition riddle,\nthe solution to which is typically so elusive that it\ndoes not come to mind, at least initially - leaving\nthe responder stumped. Stumpers do not fall within\nthe System 1 or System 2 frameworks but are re-\nlated to creative thinking (Bar-Hillel et al., 2019).\nImportantly, once presented with a solution, people\ncan easily classify it as right or wrong - a simple\nsystem-2 task. In this paper, we demonstrate that\nrecent LLMs (e.g., GPT-3.5 and GPT-4) outper-\nform humans in solving stumpers but lag behind\nhumans in classifying solutions as right or wrong.\n2 Task\nA stumper is a single-step intuition riddle in which\na misleading cue sparks a visual or semantic repre-\nsentation that blocks the solution from coming to\nmind. Unlike other riddles, there is no need for fur-\nther computation or thinking, and once the obstacle\nis removed, the answer is clear. See examples in\nAppendix A.\nThe dataset used for our analysis consists of all\n76 stumpers curated in (Bar-Hillel, 2021). Each\nstumper is a textual description of a narrative or\nscenario that requires a unique solution. To en-\nhance the number of stumpers beyond this exhaus-\ntive list, we generated two similar riddles for each\nstumper, by asking GPT-3.5-Turbo to change the\nnames and wording of the original data-set. After\nthe generation, we manually approved or edited\neach stumper to reduce confusion and alternative\nsolutions as much as we could. This process re-\nsulted in a set of additional 152 stumpers. As the\nset of new stumpers was not validated with human\nparticipants and may differ from the original set, we\npresent the results for the original set in the body of\nthis paper and the detailed results in the appendix.\nThe data also includes correct and incorrect solu-\ntions, which allowed a comparative analysis of the\naccuracy and reasoning strategies of the responses\ngiven by models and human participants. A dataset\nsample can be found in Appendix A.\n3 Models and Experiments\nThe study involved four language models: Davinci-\n2, Davinci-3, GPT-3.5-turbo, and GPT-4. Addition-\nally, 81 human participants (F=48%; ages 20-54,\nm=28.52, sd=7.73) were recruited via an online\nsurvey participation platform (Prolific).\nWhen solving each stumper, both humans and mod-\nels were presented with a prompt. To normalize\nthe answers across conditions, models, and par-\nticipants, all prompts started with a standardized\ndefinition of a correct answer to a riddle:\nAn answer to a riddle is correct only if\nit is consistent with all the riddle’s clues,\nsensical, specific, logical, and fitting with\nthe context of the riddle.\nSee prompts examples in Appendix C.\n3.1 Answer Generation\nTo avoid learning, each participant was presented\nwith only one stumper and was asked to answer it\nor type \"IDK\" if they did not know the answer.\nEach model, in each prompt, was presented either\nwith only one stumper (\"naïve response\") or with\ntwo other pairs of riddles and their ground-truth\nanswer (\"prompted response\").\n3.2 Answer Verification\nAfter answering the riddle or typing \"IDK\", human\nparticipants were presented with the two possible\nsolutions and were asked to choose the correct one.\nThe models were presented with the same choice\nwithout their previous response in the prompt.\n11645\n3.3 Answer Verification - Models’ response\nTo further compare, the models were given a verifi-\ncation problem where their own answers replaced\none of the answers. For riddles to which the model\nknew (/did not know) the answer, their response\nwas used instead of the correct (/incorrect) ground\ntruth.\n4 Results and Observations\nWe replaced participants who reported knowing\ntheir riddle or finding the answer online. Two au-\nthors evaluated the responses unanimously, with\nonly one response being disagreed upon, leading\nto its exclusion.\nLLMs proficiency at solving stumpers Our re-\nsults are provided in Figure 1. Solving a stumper\nby chance has virtually zero probability, given the\ninfinitesimal likelihood of randomly arriving at the\ncorrect solution among countless potential answers.\nHuman participants in our sample have accurately\nsolved 38.15% of the stumpers, replicating the 35%\naccuracy found in (Bar-Hillel, 2021).\nImproved performance of the advanced models\nA two-way ANOV A was conducted to examine the\neffects of the models and the prompting: The chat\nmodels (GPT-3.5-Turbo and GPT-4; m=57.8% for\nthe original stumper, 43.4% for the enhanced data-\nset) have performed significantly better than the\nGPT-3 model (Davinci-2 and Davinci-3; m=29.6%\nfor the original stumper, 22.5% for the enhanced\ndata-set) [F(1,4)=686, p=0.00001]. The main effect\nof Prompt was not significant [F(1, 4) = 0.023, p =\n0.887], but the interaction was [F(1, 4) = 30.82, p\n= 0.005], indicating that the prompt has a positive\nimpact on the performance of the GPT-3 models\nand a negative impact on the performance of the\nchat models. See results for the original data-set\nin figure 1. See results for the enhanced set in\nAppendix B.\nAnswer Verification The answer verification\ntask was tested once with the ground-truth answers\n(Figure 2) and once with the model’s responses\nvs. ground truth (Figure 3). Different scores are\nreported for correct and incorrect responses.\nWhile humans performed perfectly at verifica-\ntion when knowing to generate the answer (100%)\nand above chance even when they failed to generate\n(63.8%), most models performed below the chance\nlevel (m=41%). A two-way ANOV A was con-\nducted to compare the models’ ability to choose the\nAnswer Generation\nDavinci-2Davinci-3\nGPT-3.5-Turbo\nGPT-4\n0.2\n0.3\n0.4\n0.5\n0.6\n0.28 0.26\n0.55\n0.58\n0.32 0.33\n0.51 0.51\nPercent Correct\nNaïvePromptHumans\nFigure 1: performance of the different models. Human\nperformance is presented in the dashed line.\ncorrect answer from the ground-truth. The models\ndid not show different performances [F(3,3)=2.43,\np=0.24], nor did the models’ previous success in\nsolving the stumper [F(1, 3) = 5.25, p = 0.106].\nFor the Model’s responses (Figure 3), a three-\nway ANOV A was conducted, with model type,\nthe correctness of the response, and the prompt\ntype. Here, too, no effects were found for the\nmodel type [F(3,10)=1.28 p>0.33] or the prompt\n[F(1,10)=0.13, p>0.7]. The model’s previous\nsuccess has significantly improved performance\n[F(1,10)=24.97, p=0.0005].\nAnswer Verification: Ground Truth\nDavinci-2Davinci-3\nGPT-3.5-Turbo\nGPT-4Humans\n0.2\n0.4\n0.6\n0.8\n1\n0.38\n0.7\n0.41 0.39\n1\n0.22\n0.38 0.41 0.39\n0.64\nPercent Correct\nSucceeded\nfailed\nFigure 2: Accuracy of the models and humans in choos-\ning the correct answer out of two alternatives.\n5 Discussion and Conclusions\nThis study compared the stumper-solving abilities\nof LLMs and humans. We found that while the\n11646\nAnswer Verification: Model’s response\nDavinci-2Davinci-3\nGPT-3.5-Turbo\nGPT-4\n0.2\n0.4\n0.6\n0.8\n1\n0.48\n0.85\n0.66 0.7\n0.2 0.2 0.24\n0.31\n0.52\n0.75\n0.54\n0.45\n0.29\n0.38\n0.47 0.41\nPercent Correct\nSucceeded, Naïvefailed, Naïve\nSucceeded, Promptfailed, Prompt\nFigure 3: models’ accuracy in choosing a solution be-\ntween their previous response and the ground truth.\nLLMs are better than humans at solving stumpers,\ntheir answer-verification abilities are limited and\nfall far from human performance. These findings\nprovide valuable insights into the capabilities and\nlimitations of LLMs, their relationship with human\ncognition, and the potential for utilizing LLMs as\na framework for cognitive capacities.\nThe results showed that LLMs, specifically the\nLLMs used in this study, demonstrated proficiency\nin solving stumpers that are obstructed by mislead-\ning representations. The models correctly solved\n26%-58% of the stumpers, outperforming human\nparticipants and surpassing the chance level (Fig-\nure 1). This suggests that LLMs possess the skills\nrequired for solving these types of questions.\nThe study revealed an improvement in perfor-\nmance for more advanced models. The chat mod-\nels, GPT-3.5-Turbo and GPT-4, which are fine-\ntuned with human feedback during training, outper-\nformed the GPT-3 models (Davinci-2 and Davinci-\n3). This indicates that advancements in model\ntraining contribute to enhanced problem-solving\nabilities. Prompting the models with additional\npairs of riddles and their ground-truth answers had\na positive impact on the performance of the GPT-\n3 models, further emphasizing the importance of\ncontext and prior knowledge in solving stumpers.\nDespite their ability to generate correct answers,\nthe models fell short in the task of answer verifi-\ncation compared to human participants (Figure 2).\nTo further stress this problem, we have asked the\nmodels to compare their correct responses against\na false response (Figure 3, full bars), demonstrat-\ning their inconsistency and inability to verify an-\nswers. Humans, however, demonstrated a higher\nproficiency in recognizing the correct answer, even\nwhen they were unable to solve the problem ini-\ntially (Figure 2). This suggests that humans possess\na verification capability, considered a System 2 pro-\ncess, which has not yet fully emerged in LLMs.\nInterestingly, the Davinci-3 model showed good\nperformance in recognizing correct answers cu-\nrated by the authors (70%; Figure 2) and by itself\n(85%; Figure 3).\nThe overall pattern of results suggests that GPT-\n3 aligns better with human capabilities, as its\nanswer-verification capabilities are better than its\nsolving capabilities. This pattern stands in contrast\nto GPT-3.5 and GPT-4, which solve stumpers better\nthan they verify their solutions. This finding indi-\ncates that for disciplines interested in using LLMs\nto model human behavior or cognition, Davinci-3\nis likely a more suitable model to employ. This is\nin line with (Hagendorff and Fabi, 2023), which\nshows how GPT-3 (but not GPT-4 and GPT-3.5)\nexhibits similar biases and errors as demonstrated\nby humans. Another reason to consider Davinci-3\nover GPT-4 and GPT-3.5 in modeling human be-\nhavior is the fact that the results acquired here, as\nwell as the psychological literature, suggest that it\nis easier for humans to classify a correct response\nthan generates it (Pintrich, 2002), a pattern of re-\nsult similar to Davinci-3 and not congruent with\nGPT-4 and GPT-3.5 performance. This is closely\nrelated to the literature showing that recognition is\nconsidered easier than recall, as the former requires\nonly identifying the presence of familiar informa-\ntion, whereas the latter demands retrieving specific\ninformation from memory without external cues\n(Roediger III and Karpicke, 2006).\nThe challenge of answer verification is closely\nrelated to the problem of text classification, which\nhas been found to be challenging for LLMs (Sun\net al., 2023). There is a significant discrepancy\nbetween the abilities to generate a correct answer\nand to classify a correct response. This has im-\nportant implications for estimating LLM capabili-\nties, as many NLP benchmarks are designed based\non the model’s ability to classify correct answers\n(Rajpurkar et al., 2016; Wang et al., 2018; Da-\ngan et al., 2005; Reddy et al., 2019; Clark et al.,\n2019). One possible implication is the necessity\nof including interaction-based measures (Collins\net al., 2023), based on continuous human-LLM in-\nteraction, when evaluating LLMs. Like in oral ex-\n11647\nams, the opportunity to react to the models’ output\nin tailored follow-up questions allows the evalua-\ntor a deeper probing into the models’ capabilities\n(Gharibyan, 2005; Davis and Karunathilake, 2005).\nFurthermore, the findings from this study can\ninform the development of benchmark tasks for\nevaluating the intelligence and human-like behav-\nior of LLMs. Stumpers provide a challenging do-\nmain that tests problem-solving, associative capac-\nities, and verification skills. By designing more\ncomprehensive benchmarks and evaluating LLMs’\nperformance on these tasks, we can gain a better\nunderstanding of their cognitive capabilities and\nidentify areas for improvement.\nIn conclusion, this study investigated the abil-\nity of large language models (LLMs) to solve\nstumpers, challenging riddles characterized by elu-\nsive solutions. Our findings demonstrate that\nLLMs, especially the advanced models GPT-3.5-\nTurbo and GPT-4, exhibit a remarkable proficiency\nin solving stumpers, surpassing human perfor-\nmance in some cases. These results highlight the\npotential of LLMs as powerful problem-solving\ntools and provide insights into the cognitive pro-\ncesses involved in solving complex puzzles. Our\nanalysis also uncovered that the human ability to\nverify solutions has not been fully developed yet\nin LLMs. Future research can build upon these\nfindings to explore the role of context, expand the\nvariety of stumpers, and investigate the generaliz-\nability of LLMs in different domains, contributing\nto developing more robust and human-like artificial\nintelligence.\n6 Limitations\nOur study on stumpers and large language models\n(LLMs) has several limitations to consider. Firstly,\nthe limited number of 76 validated stumper, even\nwith two more versions of each in the enhanced\ndataset, potentially restricting the representative-\nness and generalizability of our findings. Sec-\nondly, we focused exclusively on OpenAI models,\nlimiting the scope of comparison with other lan-\nguage models. Thirdly, subjective judgment was\ninvolved in evaluating correctness, leading to po-\ntential variations in interpretations. Lastly, prompt\nengineering techniques were underutilized, poten-\ntially limiting the models’ problem-solving poten-\ntial. Future research should address these limita-\ntions for more robust and comprehensive insights\ninto LLMs’ problem-solving abilities.\nReferences\nAdam L Alter, Daniel M Oppenheimer, Nicholas Ep-\nley, and Rebecca N Eyre. 2007. Overcoming in-\ntuition: metacognitive difficulty activates analytic\nreasoning. Journal of experimental psychology: Gen-\neral, 136(4):569.\nMaya Bar-Hillel. 2021. Stumpers: an annotated com-\npendium. Thinking & Reasoning, 27(4):536–566.\nMaya Bar-Hillel, Tom Noah, and Shane Frederick.\n2018. Learning psychology from riddles: The\ncase of stumpers. Judgment and Decision Making,\n13(1):112–122.\nMaya Bar-Hillel, Tom Noah, and Shane Frederick. 2019.\nSolving stumpers, crt and crat: Are the abilities re-\nlated? Judgment and Decision Making, 14(5):620–\n623.\nJames Brand, Ayelet Israeli, and Donald Ngwe. 2023.\nUsing gpt for market research. Available at SSRN\n4395751.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nShelly Chaiken and Yaacov Trope. 1999. Dual-process\ntheories in social psychology. Guilford Press.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL,\npages 2922–2932.\nKatherine M Collins, Albert Q Jiang, Simon Frieder,\nLionel Wong, Miri Zilka, Umang Bhatt, Thomas\nLukasiewicz, Yuhuai Wu, Joshua B Tenenbaum,\nWilliam Hart, et al. 2023. Evaluating language\nmodels for mathematics through interactions. arXiv\npreprint arXiv:2306.01694.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges. Evalu-\nating Predictive Uncertainty, Visual Object Classifi-\ncation, And Recognising Textual Entailment, pages\n177–190.\nMargery H Davis and Indika Karunathilake. 2005. The\nplace of the oral examination in today’s assessment\nsystems. Medical teacher, 27(4):294–297.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJonathan St BT Evans. 2003. In two minds: dual-\nprocess accounts of reasoning. Trends in cognitive\nsciences, 7(10):454–459.\n11648\nJonathan St BT Evans and Keith E Stanovich. 2013.\nDual-process theories of higher cognition: Advanc-\ning the debate. Perspectives on psychological sci-\nence, 8(3):223–241.\nKeith Frankish. 2010. Dual-process and dual-\nsystem theories of reasoning. Philosophy Compass,\n5(10):914–926.\nShane Frederick. 2005. Cognitive reflection and de-\ncision making. Journal of Economic perspectives ,\n19(4):25–42.\nBertram Gawronski and Laura A Creighton. 2013. Dual\nprocess theories 14. The Oxford handbook of social\ncognition, page 282.\nHasmik Gharibyan. 2005. Assessing students’ knowl-\nedge: oral exams vs. written tests. ACM SIGCSE\nBulletin, 37(3):143–147.\nAlon Goldstein and Benjamin D Young. 2022. The\nunconscious mind. In Mind, Cognition, and Neuro-\nscience, pages 344–363. Routledge.\nAriel Goldstein, Avigail Dabush, Bobbi Aubrey, Mari-\nano Schain, Samuel A Nastase, Zaid Zada, Eric Ham,\nZhuoqiao Hong, Amir Feder, Harshvardhan Gazula,\net al. 2022a. Brain embeddings with shared geome-\ntry to artificial contextual embeddings, as a code for\nrepresenting language in the human brain. BioRxiv,\npages 2022–03.\nAriel Goldstein, Zaid Zada, Eliav Buchnik, Mariano\nSchain, Amy Price, Bobbi Aubrey, Samuel A Nas-\ntase, Amir Feder, Dotan Emanuel, Alon Cohen, et al.\n2022b. Shared computational principles for language\nprocessing in humans and deep language models. Na-\nture neuroscience, 25(3):369–380.\nThilo Hagendorff and Sarah Fabi. 2023. Human-like\nintuitive behavior and reasoning biases emerged in\nlanguage models–and disappeared in gpt-4. arXiv\npreprint arXiv:2306.07622.\nUri Hasson, Samuel A Nastase, and Ariel Goldstein.\n2020. Direct fit to nature: An evolutionary perspec-\ntive on biological and artificial neural networks. Neu-\nron, 105(3):416–434.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nYutaka Matsuo. 2020. Special features of deep learning\nand symbol emergence. New Generation Computing,\n38:5–6.\nPaul R Pintrich. 2002. The role of metacognitive knowl-\nedge in learning, teaching, and assessing. Theory\ninto practice, 41(4):219–225.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249–266.\nBlake A Richards, Timothy P Lillicrap, Philippe Beau-\ndoin, Yoshua Bengio, Rafal Bogacz, Amelia Chris-\ntensen, Claudia Clopath, Rui Ponte Costa, Archy\nde Berker, Surya Ganguli, et al. 2019. A deep\nlearning framework for neuroscience. Nature neuro-\nscience, 22(11):1761–1770.\nHenry L Roediger III and Jeffrey D Karpicke. 2006.\nTest-enhanced learning: Taking memory tests im-\nproves long-term retention. Psychological science,\n17(3):249–255.\nDan Schwartz, Mariya Toneva, and Leila Wehbe. 2019.\nInducing brain-relevant bias in natural language pro-\ncessing models. Advances in neural information pro-\ncessing systems, 32.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shang-\nwei Guo, Tianwei Zhang, and Guoyin Wang. 2023.\nText classification via large language models. arXiv\npreprint arXiv:2305.08377.\nRefael Tikochinski, Ariel Goldstein, Yaara Yeshurun,\nUri Hasson, and Roi Reichart. 2023. Perspective\nchanges in human listeners are aligned with the con-\ntextual transformation of the word embedding space.\nCerebral Cortex, page bhad082.\nMaggie E Toplak, Richard F West, and Keith E\nStanovich. 2011. The cognitive reflection test as\na predictor of performance on heuristics-and-biases\ntasks. Memory & cognition, 39(7):1275–1289.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding.\n11649\nAppendix\nA Stumpers examples\n1 A father and son were involved in a traffic\naccident. The father was killed, and the son was\nrushed to hospital.\nThe surgeon walked into the operating room, and\nupon seeing the severely wounded boy cried out:\n“OMG, it is my son!”.\nHow could this be true?\nAnswer: The surgeon is the boy’s mother\n2 A very tall man was holding up a wine decanter\nway above his head.\nHe let go of it, and it dropped to the carpet he was\nstanding on.\nExplain briefly how not a single drop of wine was\nspilled.\nAnswer: The decanter was empty\n3 Farmer Joe eats two fresh eggs from his own\nfarm for breakfast every day.\nYet there are no chickens on his farm. Where does\nFarmer Joe get his eggs?\nAnswer: Famer Joe does not eat chicken\neggs, but a different animal’s egg, such as ducks.\n4 Marcy went from one bank of a river to the one\n20 meters across.\nThere are no bridges on the river.\nMarcy had no equipment, no devices, no special\nclothing, and she can’t even swim.\nShe relied on her own body only -and none of it\ngot wet!\nExplain briefly how she managed this.\nAnswer: The river was dry.\nB Enhanced data-set results\nFigure 4: Perfomrnace scores for the different models\non the enhanced data-set. The dashed line indicates\nhuman performance in the original set.\nAnswer Verification\nDavinci-2Davinci-3\nGPT-3.5-Turbo\nGPT-4\nOriginal Dataset\n0\n0.2\n0.4\n0.6\n0.8\n0.12\n0.72\n0.65 0.67\n0.52\n0.21\n0.42\n0.51 0.51 0.5\nPercent Correct\nSucceeded\nfailed\nFigure 5: Accuracy of the models in choosing the cor-\nrect answer out of two alternatives.\n11650\nC Prompt examples\nAnswer Generation, Naïve, GPT-3 models\nAn answer to a riddle is correct\nonly if it is consistent with\nall the riddle’s clues, sensical,\nspecific, logical, and fitting\nwith the context of the riddle.\n--\nRiddle:\nA father and son were involved in\na traffic accident. The father\nwas killed, and the son was\nrushed to hospital. The surgeon\nwalked into the operating room,\nand upon seeing the severely\nwounded boy cried out: “OMG, it\nis my son!”. How could this be\ntrue?\nAnswer:\nAnswer Generation, Prompt, GPT-3 models\nAn answer to a riddle is correct\nonly if it is consistent with\nall the riddle’s clues, sensical,\nspecific, logical, and fitting\nwith the context of the riddle.\n--\nRiddle:\nLong after the screen of Kim’s\nsmart phone had cracked. It\nwas still functioning just fine.\nBefore he could replace it, the\nphone accidentally fell into the\nfamily’s swimming pool. It was\nretrieved almost at once, but\n- alas - the phone was dead.\nYet no water had penetrated\nthe cracked screen, so all the\ncritical components remained\ncompletely dry. Explain briefly\nwhy the phone was dead.\nAnswer:\nthe pool was empty\n--\nRiddle:\nPolly bought a beautiful parrot.\nThe seller guaranteed that the\nbird repeats everything it hears.\nHowever, try as Polly might to\nteach it, her squawking parrot\nnever repeated a single word.\nThe seller did not lie. Explain\nbriefly.\nAnswer:\nThe parrot was deaf\n--\nRiddle:\nAn accountant says: ”That\nattorney is my brother”, and that\nis true - they really do have the\nsame parents. Yet the attorney\ndenies having any brothers - and\nthat is also true! How is that\npossible?\nAnswer:\n11651\nAnswer Generation, naïve, Chat models\n{’model’: ’gpt-3.5-turbo’,\n’messages’: [\n’role’: ’system’,\n’content’: \"An answer to a\nriddle is correct only if it is\nconsistent with all the riddle’s\nclues, sensical, specific,\nlogical, and fitting with the\ncontext of the riddle.\",\n’role’: ’user’,\n’content’: ’Riddle:\\nTwo\nItalians are sharing a pizza.\nThe older Italian is the brother\nof the younger Italian. But\nthe younger Italian is not the\nbrother of the older Italian.\nExplain briefly. ’,\n’role’: ’assistant’,\n’content’: ’Answer:\\n’}],\n’temperature’: 0.0,\n’frequency_penalty’: 1.0,\n’presence_penalty’: 0.5,\n’n’: 1,\n’max_tokens’: 120}\nAnswer Generation, prompt, Chat models\n{’model’: ’gpt-4’,\n’messages’: [\n{’role’: ’system’,\n’content’: \"An answer to a\nriddle is correct only if it is\nconsistent with all the riddle’s\nclues, sensical, specific,\nlogical, and fitting with the\ncontext of the riddle.\"},\n{’role’: ’user’,\n’content’: ’Riddle:\\nCindy\nrecycles everything: paper,\nglass, metal, plastic, etc. She\nalso brings her still useable\nstuff (books, housewares, etc.\n) to the donation bins at\nthe recycle center. Recently,\nshe brought a bag full of\nlarge (2 liter) bottles to\nthe recycling center. The\nvolunteer on duty could barely\nlift it. Explain briefly.\n\\n\\nAnswer:\\nThe bottles were\nfull. They were donations to the\nplant.\\n\\n\\n--\\nRiddle:\\nFred\nbought a used car from his\nneighbor next door. The neighbor\nclaimed that the car got 35 miles\nper gallon. Fred, an excellent\ndriver, could only get about\nhalf that much, in spite of\ndriving on the same roads as\nhis neighbor. Explain in a few\nwords. \\n\\nAnswer:\\nThe seller\nwas lying\\n\\n\\n--\\nRiddle:\\nTwo\nRussians were standing in line.\nThe taller one was the brother of\nthe shorter one, but the shorter\none was not the brother of the\ntaller one. Explain in a few\nwords how that is possible. ’},\n{’role’: ’assistant’, ’content’:\n’Answer:\\n’}], ’temperature’:\n0.0, ’frequency_penalty’: 1.0,\n’presence_penalty’: 0.5, ’n’: 1,\n’max_tokens’: 120}\n11652\nAnswer Verification, GPT-3 models\nAn answer to a riddle is\ncorrect only if it is consistent\nwith all the riddle’s clues, be\nsensical, specific, logical, and\nfitting with the context of the\nriddle.\nRiddle:\nFarmer Joe eats two fresh eggs\nfrom his own farm for breakfast\nevery day. Yet there are no\nchickens on his farm. Where does\nFarmer Joe get his eggs?\nAnswers:\n1. Famer Joe do not eat chicken\neggs, but a different animal’s\negg, such as ducks.\n2. Farmer Joe gets his eggs from\nthe grocery store.\nWhich of these answers is\ncorrect?\nAnswer Verification, Chat models\n{’model’: ’gpt-3.5-turbo’,\n’messages’: [\n{’role’: ’system’,\n’content’: \"An answer to a\nriddle is correct only if it is\nconsistent with all the riddle’s\nclues, be sensical, specific,\nlogical, and fitting with the\ncontext of the riddle.\"},\n{’role’: ’user’,\n’content’: \"Riddle:\\nAlex\nis Bobbie’s blood relative,\nand Bobbie is Charlie’s blood\nrelative, but Alex is not a\nblood relative of Charlie. How\ncome? \\n\\nAnswers:\\n1. Alex\nand Charlie could be Bobby’s\nparents, making them both Bobby’s\nblood relatives but not each\nother’s\\n\\n 2.Alex is Bobbie’s\nparent, and Bobbie is Charlie’s\nparent, but Alex is not a parent\nof Charlie.\\n\\nWhich of these\nanswers is correct?\"},\n{’role’: ’assistant’,\n’content’: ’The correct answer\nis number ’}],\n’temperature’: 0.0,\n’frequency_penalty’: 1.0,\n’presence_penalty’: 0.5,\n’n’: 1,\n’max_tokens’: 20}\n11653",
  "topic": "Intuition",
  "concepts": [
    {
      "name": "Intuition",
      "score": 0.6957494020462036
    },
    {
      "name": "Computer science",
      "score": 0.6499388813972473
    },
    {
      "name": "Decoding methods",
      "score": 0.5808308720588684
    },
    {
      "name": "Cognition",
      "score": 0.47677189111709595
    },
    {
      "name": "Verifiable secret sharing",
      "score": 0.43330106139183044
    },
    {
      "name": "Cognitive science",
      "score": 0.27445751428604126
    },
    {
      "name": "Programming language",
      "score": 0.270598828792572
    },
    {
      "name": "Psychology",
      "score": 0.203509122133255
    },
    {
      "name": "Algorithm",
      "score": 0.12731346487998962
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I197251160",
      "name": "Hebrew University of Jerusalem",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I4210133369",
      "name": "Decision Sciences (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}