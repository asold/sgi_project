{
  "title": "Character-Aware Neural Language Models",
  "url": "https://openalex.org/W1938755728",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2108608223",
      "name": "Kim, Yoon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222415834",
      "name": "Jernite, Yacine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227151425",
      "name": "Sontag, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222402211",
      "name": "Rush, Alexander M.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962853227",
    "https://openalex.org/W2142377809",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W196214544",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W1505680913",
    "https://openalex.org/W2997617958",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W1026270304",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W4249773576",
    "https://openalex.org/W2056250865",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2963682821",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1889624880",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W2403625461",
    "https://openalex.org/W2165395109",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2053306448",
    "https://openalex.org/W2105372662",
    "https://openalex.org/W4292012835",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2040711288",
    "https://openalex.org/W2963522696",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2101609803",
    "https://openalex.org/W1860935423",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W4297744104",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2131876387",
    "https://openalex.org/W2251640092",
    "https://openalex.org/W1951325712",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2949563612",
    "https://openalex.org/W2150355110"
  ],
  "abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.",
  "full_text": "Character-Aware Neural Language Models\nYoon Kim†\n†School of Engineering and Applied Sciences\nHarvard University\n{yoonkim,srush}@seas.harvard.edu\nYacine Jernite∗ David Sontag∗\n∗Courant Institute of Mathematical Sciences\nNew York University\n{jernite,dsontag}@cs.nyu.edu\nAlexander M. Rush†\nAbstract\nWe describe a simple neural language model that re-\nlies only on character-level inputs. Predictions are still\nmade at the word-level. Our model employs a con-\nvolutional neural network (CNN) and a highway net-\nwork over characters, whose output is given to a\nlong short-term memory (LSTM) recurrent neural net-\nwork language model (RNN-LM). On the English\nPenn Treebank the model is on par with the existing\nstate-of-the-art despite having 60% fewer parameters.\nOn languages with rich morphology (Arabic, Czech,\nFrench, German, Spanish, Russian), the model out-\nperforms word-level/morpheme-level LSTM baselines,\nagain with fewer parameters. The results suggest that on\nmany languages, character inputs are sufﬁcient for lan-\nguage modeling. Analysis of word representations ob-\ntained from the character composition part of the model\nreveals that the model is able to encode, from characters\nonly, both semantic and orthographic information.\nIntroduction\nLanguage modeling is a fundamental task in artiﬁcial intel-\nligence and natural language processing (NLP), with appli-\ncations in speech recognition, text generation, and machine\ntranslation. A language model is formalized as a probability\ndistribution over a sequence of strings (words), and tradi-\ntional methods usually involve making ann-th order Markov\nassumption and estimating n-gram probabilities via count-\ning and subsequent smoothing (Chen and Goodman 1998).\nThe count-based models are simple to train, but probabilities\nof rare n-grams can be poorly estimated due to data sparsity\n(despite smoothing techniques).\nNeural Language Models (NLM) address then-gram data\nsparsity issue through parameterization of words as vectors\n(word embeddings) and using them as inputs to a neural net-\nwork (Bengio, Ducharme, and Vincent 2003; Mikolov et al.\n2010). The parameters are learned as part of the training\nprocess. Word embeddings obtained through NLMs exhibit\nthe property whereby semantically close words are likewise\nclose in the induced vector space (as is the case with non-\nneural techniques such as Latent Semantic Analysis (Deer-\nwester, Dumais, and Harshman 1990)).\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nWhile NLMs have been shown to outperform count-based\nn-gram language models (Mikolov et al. 2011), they are\nblind to subword information (e.g. morphemes). For exam-\nple, they do not know, a priori, that eventful, eventfully, un-\neventful, and uneventfully should have structurally related\nembeddings in the vector space. Embeddings of rare words\ncan thus be poorly estimated, leading to high perplexities\nfor rare words (and words surrounding them). This is espe-\ncially problematic in morphologically rich languages with\nlong-tailed frequency distributions or domains with dynamic\nvocabularies (e.g. social media).\nIn this work, we propose a language model that lever-\nages subword information through a character-level con-\nvolutional neural network (CNN), whose output is used\nas an input to a recurrent neural network language model\n(RNN-LM). Unlike previous works that utilize subword in-\nformation via morphemes (Botha and Blunsom 2014; Lu-\nong, Socher, and Manning 2013), our model does not require\nmorphological tagging as a pre-processing step. And, unlike\nthe recent line of work which combines input word embed-\ndings with features from a character-level model (dos Santos\nand Zadrozny 2014; dos Santos and Guimaraes 2015), our\nmodel does not utilize word embeddings at all in the input\nlayer. Given that most of the parameters in NLMs are from\nthe word embeddings, the proposed model has signiﬁcantly\nfewer parameters than previous NLMs, making it attractive\nfor applications where model size may be an issue (e.g. cell\nphones).\nTo summarize, our contributions are as follows:\n• on English, we achieve results on par with the existing\nstate-of-the-art on the Penn Treebank (PTB), despite hav-\ning approximately 60% fewer parameters, and\n• on morphologically rich languages (Arabic, Czech,\nFrench, German, Spanish, and Russian), our model\noutperforms various baselines (Kneser-Ney, word-\nlevel/morpheme-level LSTM), again with fewer parame-\nters.\nWe have released all the code for the models described in\nthis paper.1\n1https://github.com/yoonkim/lstm-char-cnn\narXiv:1508.06615v4  [cs.CL]  1 Dec 2015\nModel\nThe architecture of our model, shown in Figure 1, is straight-\nforward. Whereas a conventional NLM takes word embed-\ndings as inputs, our model instead takes the output from\na single-layer character-level convolutional neural network\nwith max-over-time pooling.\nFor notation, we denote vectors with bold lower-case (e.g.\nxt,b), matrices with bold upper-case (e.g. W,Uo), scalars\nwith italic lower-case (e.g.x,b), and sets with cursive upper-\ncase (e.g. V,C) letters. For notational convenience we as-\nsume that words and characters have already been converted\ninto indices.\nRecurrent Neural Network\nA recurrent neural network (RNN) is a type of neural net-\nwork architecture particularly suited for modeling sequen-\ntial phenomena. At each time step t, an RNN takes the input\nvector xt ∈Rn and the hidden state vector ht−1 ∈Rm and\nproduces the next hidden state ht by applying the following\nrecursive operation:\nht = f(Wxt + Uht−1 + b) (1)\nHere W ∈Rm×n,U ∈Rm×m,b ∈Rm are parameters\nof an afﬁne transformation and f is an element-wise nonlin-\nearity. In theory the RNN can summarize all historical in-\nformation up to time twith the hidden state ht. In practice\nhowever, learning long-range dependencies with a vanilla\nRNN is difﬁcult due to vanishing/exploding gradients (Ben-\ngio, Simard, and Frasconi 1994), which occurs as a result of\nthe Jacobian’s multiplicativity with respect to time.\nLong short-term memory (LSTM) (Hochreiter and\nSchmidhuber 1997) addresses the problem of learning long\nrange dependencies by augmenting the RNN with a memory\ncell vector ct ∈Rn at each time step. Concretely, one step\nof an LSTM takes as input xt,ht−1,ct−1 and produces ht,\nct via the following intermediate calculations:\nit = σ(Wixt + Uiht−1 + bi)\nft = σ(Wf xt + Uf ht−1 + bf )\not = σ(Woxt + Uoht−1 + bo)\ngt = tanh(Wgxt + Ught−1 + bg)\nct = ft ⊙ct−1 + it ⊙gt\nht = ot ⊙tanh(ct)\n(2)\nHere σ(·) and tanh(·) are the element-wise sigmoid and hy-\nperbolic tangent functions, ⊙is the element-wise multipli-\ncation operator, and it, ft, ot are referred to as input, for-\nget, and output gates. At t = 1, h0 and c0 are initialized to\nzero vectors. Parameters of the LSTM are Wj,Uj,bj for\nj ∈{i,f,o,g }.\nMemory cells in the LSTM are additive with respect to\ntime, alleviating the gradient vanishing problem. Gradient\nexploding is still an issue, though in practice simple opti-\nmization strategies (such as gradient clipping) work well.\nLSTMs have been shown to outperform vanilla RNNs on\nmany tasks, including on language modeling (Sundermeyer,\nSchluter, and Ney 2012). It is easy to extend the RNN/LSTM\nto two (or more) layers by having another network whose\nFigure 1:Architecture of our language model applied to an exam-\nple sentence. Best viewed in color. Here the model takes absurdity\nas the current input and combines it with the history (as represented\nby the hidden state) to predict the next word,is. First layer performs\na lookup of character embeddings (of dimension four) and stacks\nthem to form the matrix Ck. Then convolution operations are ap-\nplied between Ck and multiple ﬁlter matrices. Note that in the\nabove example we have twelve ﬁlters—three ﬁlters of width two\n(blue), four ﬁlters of width three (yellow), and ﬁve ﬁlters of width\nfour (red). A max-over-time pooling operation is applied to obtain\na ﬁxed-dimensional representation of the word, which is given to\nthe highway network. The highway network’s output is used as the\ninput to a multi-layer LSTM. Finally, an afﬁne transformation fol-\nlowed by a softmax is applied over the hidden representation of\nthe LSTM to obtain the distribution over the next word. Cross en-\ntropy loss between the (predicted) distribution over next word and\nthe actual next word is minimized. Element-wise addition, multi-\nplication, and sigmoid operators are depicted in circles, and afﬁne\ntransformations (plus nonlinearities where appropriate) are repre-\nsented by solid arrows.\ninput at tis ht (from the ﬁrst network). Indeed, having mul-\ntiple layers is often crucial for obtaining competitive perfor-\nmance on various tasks (Pascanu et al. 2013).\nRecurrent Neural Network Language Model\nLet Vbe the ﬁxed size vocabulary of words. A language\nmodel speciﬁes a distribution over wt+1 (whose support is\nV) given the historical sequence w1:t = [w1,...,w t]. A re-\ncurrent neural network language model (RNN-LM) does this\nby applying an afﬁne transformation to the hidden layer fol-\nlowed by a softmax:\nPr(wt+1 = j|w1:t) = exp(ht ·pj + qj)∑\nj′∈Vexp(ht ·pj′\n+ qj′\n) (3)\nwhere pj is the j-th column of P ∈Rm×|V|(also referred to\nas the output embedding),2 and qj is a bias term. Similarly,\nfor a conventional RNN-LM which usually takes words as\ninputs, if wt = k, then the input to the RNN-LM at t is\nthe input embedding xk, the k-th column of the embedding\nmatrix X ∈Rn×|V|. Our model simply replaces the input\nembeddings X with the output from a character-level con-\nvolutional neural network, to be described below.\nIf we denote w1:T = [w1,··· ,wT ] to be the sequence of\nwords in the training corpus, training involves minimizing\nthe negative log-likelihood (NLL) of the sequence\nNLL = −\nT∑\nt=1\nlog Pr(wt|w1:t−1) (4)\nwhich is typically done by truncated backpropagation\nthrough time (Werbos 1990; Graves 2013).\nCharacter-level Convolutional Neural Network\nIn our model, the input at time t is an output from a\ncharacter-level convolutional neural network (CharCNN),\nwhich we describe in this section. CNNs (LeCun et al.\n1989) have achieved state-of-the-art results on computer vi-\nsion (Krizhevsky, Sutskever, and Hinton 2012) and have also\nbeen shown to be effective for various NLP tasks (Collobert\net al. 2011). Architectures employed for NLP applications\ndiffer in that they typically involve temporal rather than spa-\ntial convolutions.\nLet Cbe the vocabulary of characters, d be the dimen-\nsionality of character embeddings,3 and Q ∈Rd×|C|be the\nmatrix character embeddings. Suppose that word k ∈V is\nmade up of a sequence of characters [c1,...,c l], where lis\nthe length of wordk. Then the character-level representation\nof kis given by the matrix Ck ∈Rd×l, where the j-th col-\numn corresponds to the character embedding for cj (i.e. the\ncj-th column of Q).4\nWe apply a narrow convolution between Ck and a ﬁlter\n(or kernel) H ∈Rd×w of width w, after which we add a\nbias and apply a nonlinearity to obtain a feature map fk ∈\nRl−w+1. Speciﬁcally, the i-th element of fk is given by:\nfk[i] =tanh(⟨Ck[∗,i : i+ w−1],H⟩+ b) (5)\n2In our work, predictions are at the word-level, and hence we\nstill utilize word embeddings in the output layer.\n3Given that |C|is usually small, some authors work with one-\nhot representations of characters. However we found that using\nlower dimensional representations of characters (i.e. d <|C|) per-\nformed slightly better.\n4Two technical details warrant mention here: (1) we append\nstart-of-word and end-of-word characters to each word to better\nrepresent preﬁxes and sufﬁxes and hence Ck actually has l + 2\ncolumns; (2) for batch processing, we zero-padCk so that the num-\nber of columns is constant (equal to the max word length) for all\nwords in V.\nwhere Ck[∗,i : i+w−1] is the i-to-(i+w−1)-th column of\nCk and ⟨A,B⟩= Tr(ABT ) is the Frobenius inner product.\nFinally, we take the max-over-time\nyk = max\ni\nfk[i] (6)\nas the feature corresponding to the ﬁlter H (when applied to\nword k). The idea is to capture the most important feature—\nthe one with the highest value—for a given ﬁlter. A ﬁlter is\nessentially picking out a character n-gram, where the size of\nthe n-gram corresponds to the ﬁlter width.\nWe have described the process by which one feature is\nobtained from one ﬁlter matrix. Our CharCNN uses multiple\nﬁlters of varying widths to obtain the feature vector for k.\nSo if we have a total of h ﬁlters H1,..., Hh, then yk =\n[yk\n1 ,...,y k\nh] is the input representation of k. For many NLP\napplications his typically chosen to be in [100,1000].\nHighway Network\nWe could simply replace xk (the word embedding) with yk\nat each tin the RNN-LM, and as we show later, this simple\nmodel performs well on its own (Table 7). One could also\nhave a multilayer perceptron (MLP) over yk to model in-\nteractions between the character n-grams picked up by the\nﬁlters, but we found that this resulted in worse performance.\nInstead we obtained improvements by runningyk through\na highway network, recently proposed by Srivastava et al.\n(2015). Whereas one layer of an MLP applies an afﬁne trans-\nformation followed by a nonlinearity to obtain a new set of\nfeatures,\nz = g(Wy + b) (7)\none layer of a highway network does the following:\nz = t ⊙g(WHy + bH) + (1 −t) ⊙y (8)\nwhere gis a nonlinearity, t = σ(WT y + bT ) is called the\ntransform gate, and (1−t) is called thecarry gate. Similar to\nthe memory cells in LSTM networks, highway layers allow\nfor training of deep networks by adaptively carrying some\ndimensions of the input directly to the output.5 By construc-\ntion the dimensions of y and z have to match, and hence\nWT and WH are square matrices.\nExperimental Setup\nAs is standard in language modeling, we use perplexity\n(PPL) to evaluate the performance of our models. Perplex-\nity of a model over a sequence [w1,...,w T ] is given by\nPPL = exp\n(NLL\nT\n)\n(9)\nwhere NLLis calculated over the test set. We test the model\non corpora of varying languages and sizes (statistics avail-\nable in Table 1).\nWe conduct hyperparameter search, model introspection,\nand ablation studies on the English Penn Treebank (PTB)\n(Marcus, Santorini, and Marcinkiewicz 1993), utilizing the\n5Srivastava et al. (2015) recommend initializing bT to a neg-\native value, in order to militate the initial behavior towards carry.\nWe initialized bT to a small interval around −2.\nDATA-S DATA-L\n|V| |C| T |V| |C| T\nEnglish (EN) 10 k 51 1 m 60 k 197 20 m\nCzech (CS) 46 k 101 1 m 206 k 195 17 m\nGerman (DE) 37 k 74 1 m 339 k 260 51 m\nSpanish (ES) 27 k 72 1 m 152 k 222 56 m\nFrench (FR) 25 k 76 1 m 137 k 225 57 m\nRussian (RU) 62 k 62 1 m 497 k 111 25 m\nArabic (AR) 86 k 132 4 m – – –\nTable 1:Corpus statistics. |V|= word vocabulary size;|C|= char-\nacter vocabulary size; T = number of tokens in training set. The\nsmall English data is from the Penn Treebank and the Arabic data\nis from the News-Commentary corpus. The rest are from the 2013\nACL Workshop on Machine Translation. |C|is large because of\n(rarely occurring) special characters.\nstandard training (0-20), validation (21-22), and test (23-24)\nsplits along with pre-processing by Mikolov et al. (2010).\nWith approximately 1m tokens and |V|= 10k, this version\nhas been extensively used by the language modeling com-\nmunity and is publicly available.6\nWith the optimal hyperparameters tuned on PTB, we ap-\nply the model to various morphologically rich languages:\nCzech, German, French, Spanish, Russian, and Arabic. Non-\nArabic data comes from the 2013 ACL Workshop on Ma-\nchine Translation,7 and we use the same train/validation/test\nsplits as in Botha and Blunsom (2014). While the raw data\nare publicly available, we obtained the preprocessed ver-\nsions from the authors,8 whose morphological NLM serves\nas a baseline for our work. We train on both the small\ndatasets (D ATA-S) with 1m tokens per language, and the\nlarge datasets (D ATA-L) including the large English data\nwhich has a much bigger |V| than the PTB. Arabic data\ncomes from the News-Commentary corpus, 9 and we per-\nform our own preprocessing and train/validation/test splits.\nIn these datasets only singleton words were replaced with\n<unk> and hence we effectively use the full vocabulary. It\nis worth noting that the character model can utilize surface\nforms of OOV tokens (which were replaced with<unk>), but\nwe do not do this and stick to the preprocessed versions (de-\nspite disadvantaging the character models) for exact com-\nparison against prior work.\nOptimization\nThe models are trained by truncated backpropagation\nthrough time (Werbos 1990; Graves 2013). We backprop-\nagate for 35 time steps using stochastic gradient descent\nwhere the learning rate is initially set to 1.0 and halved if\nthe perplexity does not decrease by more than 1.0 on the\nvalidation set after an epoch. On D ATA-S we use a batch\nsize of 20 and on D ATA-L we use a batch size of 100 (for\n6http://www.ﬁt.vutbr.cz/∼imikolov/rnnlm/\n7http://www.statmt.org/wmt13/translation-task.html\n8http://bothameister.github.io/\n9http://opus.lingﬁl.uu.se/News-Commentary.php\nSmall Large\nCNN\nd 15 15\nw [1,2,3,4,5,6] [1 ,2,3,4,5,6,7]\nh [25 ·w] [ min{200,50 ·w}]\nf tanh tanh\nHighway l 1 2\ng ReLU ReLU\nLSTM l 2 2\nm 300 650\nTable 2: Architecture of the small and large models. d =\ndimensionality of character embeddings; w = ﬁlter widths;\nh = number of ﬁlter matrices, as a function of ﬁlter width\n(so the large model has ﬁlters of width [1, 2, 3, 4, 5, 6, 7] of\nsize [50, 100, 150, 200, 200, 200, 200] for a total of 1100 ﬁlters);\nf, g= nonlinearity functions; l = number of layers; m = number\nof hidden units.\ngreater efﬁciency). Gradients are averaged over each batch.\nWe train for25 epochs on non-Arabic and30 epochs on Ara-\nbic data (which was sufﬁcient for convergence), picking the\nbest performing model on the validation set. Parameters of\nthe model are randomly initialized over a uniform distribu-\ntion with support [−0.05,0.05].\nFor regularization we use dropout (Hinton et al. 2012)\nwith probability 0.5 on the LSTM input-to-hidden layers\n(except on the initial Highway to LSTM layer) and the\nhidden-to-output softmax layer. We further constrain the\nnorm of the gradients to be below 5, so that if the L2 norm\nof the gradient exceeds 5 then we renormalize it to have\n||·|| = 5 before updating. The gradient norm constraint\nwas crucial in training the model. These choices were largely\nguided by previous work of Zaremba et al. (2014) on word-\nlevel language modeling with LSTMs.\nFinally, in order to speed up training on D ATA-L we em-\nploy a hierarchical softmax (Morin and Bengio 2005)—a\ncommon strategy for training language models with very\nlarge |V|—instead of the usual softmax. We pick the number\nof clusters c = ⌈\n√\n|V|⌉and randomly split Vinto mutually\nexclusive and collectively exhaustive subsets V1,..., Vc of\n(approximately) equal size. 10 Then Pr(wt+1 = j|w1:t) be-\ncomes,\nPr(wt+1 = j|w1:t) = exp(ht ·sr + tr)∑c\nr′=1 exp(ht ·sr′\n+ tr′\n)\n× exp(ht ·pj\nr + qj\nr)∑\nj′∈Vr\nexp(ht ·pj′\nr + qj′\nr )\n(10)\nwhere ris the cluster index such that j ∈Vr. The ﬁrst term\nis simply the probability of picking clusterr, and the second\n10While Brown clustering/frequency-based clustering is com-\nmonly used in the literature (e.g. Botha and Blunsom (2014) use\nBrown clusering), we used random clusters as our implementation\nenjoys the best speed-up when the number of words in each clus-\nter is approximately equal. We found random clustering to work\nsurprisingly well.\nPPL Size\nLSTM-Word-Small 97.6 5 m\nLSTM-Char-Small 92.3 5 m\nLSTM-Word-Large 85.4 20 m\nLSTM-Char-Large 78.9 19 m\nKN-5 (Mikolov et al. 2012) 141.2 2 m\nRNN†(Mikolov et al. 2012) 124.7 6 m\nRNN-LDA†(Mikolov et al. 2012) 113.7 7 m\ngenCNN†(Wang et al. 2015) 116.4 8 m\nFOFE-FNNLM†(Zhang et al. 2015) 108.0 6 m\nDeep RNN (Pascanu et al. 2013) 107.5 6 m\nSum-Prod Net†(Cheng et al. 2014) 100.0 5 m\nLSTM-1†(Zaremba et al. 2014) 82.7 20 m\nLSTM-2†(Zaremba et al. 2014) 78.4 52 m\nTable 3:Performance of our model versus other neural language\nmodels on the English Penn Treebank test set. PPL refers to per-\nplexity (lower is better) and size refers to the approximate number\nof parameters in the model. KN-5 is a Kneser-Ney5-gram language\nmodel which serves as a non-neural baseline.†For these models the\nauthors did not explicitly state the number of parameters, and hence\nsizes shown here are estimates based on our understanding of their\npapers or private correspondence with the respective authors.\nterm is the probability of picking word jgiven that cluster r\nis picked. We found that hierarchical softmax was not nec-\nessary for models trained on DATA-S.\nResults\nEnglish Penn Treebank\nWe train two versions of our model to assess the trade-off\nbetween performance and size. Architecture of the small\n(LSTM-Char-Small) and large (LSTM-Char-Large) models\nis summarized in Table 2. As another baseline, we also\ntrain two comparable LSTM models that use word em-\nbeddings only (LSTM-Word-Small, LSTM-Word-Large).\nLSTM-Word-Small uses200 hidden units and LSTM-Word-\nLarge uses 650 hidden units. Word embedding sizes are\nalso 200 and 650 respectively. These were chosen to keep\nthe number of parameters similar to the corresponding\ncharacter-level model.\nAs can be seen from Table 3, our large model is on\npar with the existing state-of-the-art (Zaremba et al. 2014),\ndespite having approximately 60% fewer parameters. Our\nsmall model signiﬁcantly outperforms other NLMs of sim-\nilar size, even though it is penalized by the fact that the\ndataset already has OOV words replaced with <unk> (other\nmodels are purely word-level models). While lower perplex-\nities have been reported with model ensembles (Mikolov and\nZweig 2012), we do not include them here as they are not\ncomparable to the current work.\nOther Languages\nThe model’s performance on the English PTB is informative\nto the extent that it facilitates comparison against the large\nbody of existing work. However, English is relatively simple\nDATA-S\nCS DE ES FR RU AR\nBotha KN-4 545 366 241 274 396 323\nMLBL 465 296 200 225 304 –\nSmall\nWord 503 305 212 229 352 216\nMorph 414 278 197 216 290 230\nChar 401 260 182 189 278 196\nLarge\nWord 493 286 200 222 357 172\nMorph 398 263 177 196 271 148\nChar 371 239 165 184 261 148\nTable 4:Test set perplexities for DATA-S. First two rows are from\nBotha (2014) (except on Arabic where we trained our own KN- 4\nmodel) while the last six are from this paper. KN- 4 is a Kneser-\nNey 4-gram language model, and MLBL is the best performing\nmorphological logbilinear model from Botha (2014). Small/Large\nrefer to model size (see Table 2), and Word/Morph/Char are models\nwith words/morphemes/characters as inputs respectively.\nfrom a morphological standpoint, and thus our next set of\nresults (and arguably the main contribution of this paper)\nis focused on languages with richer morphology (Table 4,\nTable 5).\nWe compare our results against the morphological log-\nbilinear (MLBL) model from Botha and Blunsom (2014),\nwhose model also takes into account subword information\nthrough morpheme embeddings that are summed at the input\nand output layers. As comparison against the MLBL mod-\nels is confounded by our use of LSTMs—widely known\nto outperform their feed-forward/log-bilinear cousins—we\nalso train an LSTM version of the morphological NLM,\nwhere the input representation of a word given to the LSTM\nis a summation of the word’s morpheme embeddings. Con-\ncretely, suppose that Mis the set of morphemes in a lan-\nguage, M ∈ Rn×|M| is the matrix of morpheme embed-\ndings, and mj is the j-th column of M (i.e. a morpheme\nembedding). Given the input word k, we feed the following\nrepresentation to the LSTM:\nxk +\n∑\nj∈Mk\nmj (11)\nwhere xk is the word embedding (as in a word-level model)\nand Mk ⊂ Mis the set of morphemes for word k. The\nmorphemes are obtained by running an unsupervised mor-\nphological tagger as a preprocessing step. 11 We emphasize\nthat the word embedding itself (i.e.xk) is added on top of the\nmorpheme embeddings, as was done in Botha and Blunsom\n(2014). The morpheme embeddings are of size 200/650 for\nthe small/large models respectively. We further train word-\nlevel LSTM models as another baseline.\nOn D ATA-S it is clear from Table 4 that the character-\nlevel models outperform their word-level counterparts de-\n11We use Morfessor Cat-MAP (Creutz and Lagus 2007), as in\nBotha and Blunsom (2014).\nDATA-L\nCS DE ES FR RU EN\nBotha KN-4 862 463 219 243 390 291\nMLBL 643 404 203 227 300 273\nSmall\nWord 701 347 186 202 353 236\nMorph 615 331 189 209 331 233\nChar 578 305 169 190 313 216\nTable 5:Test set perplexities on DATA-L. First two rows are from\nBotha (2014), while the last three rows are from the small LSTM\nmodels described in the paper. KN-4 is a Kneser-Ney 4-gram lan-\nguage model, and MLBL is the best performing morphological log-\nbilinear model from Botha (2014). Word/Morph/Char are models\nwith words/morphemes/characters as inputs respectively.\nspite, again, being smaller.12 The character models also out-\nperform their morphological counterparts (both MLBL and\nLSTM architectures), although improvements over the mor-\nphological LSTMs are more measured. Note that the mor-\npheme models have strictly more parameters than the word\nmodels because word embeddings are used as part of the in-\nput.\nDue to memory constraints 13 we only train the small\nmodels on D ATA-L (Table 5). Interestingly we do not ob-\nserve signiﬁcant differences going from word to morpheme\nLSTMs on Spanish, French, and English. The character\nmodels again outperform the word/morpheme models. We\nalso observe signiﬁcant perplexity reductions even on En-\nglish when Vis large. We conclude this section by noting\nthat we used the same architecture for all languages and did\nnot perform any language-speciﬁc tuning of hyperparame-\nters.\nDiscussion\nLearned Word Representations\nWe explore the word representations learned by the models\non the PTB. Table 6 has the nearest neighbors of word rep-\nresentations learned from both the word-level and character-\nlevel models. For the character models we compare the rep-\nresentations obtained before and after highway layers.\nBefore the highway layers the representations seem to\nsolely rely on surface forms—for example the nearest neigh-\nbors of you are your, young, four, youth, which are close to\nyou in terms of edit distance. The highway layers however,\nseem to enable encoding of semantic features that are not\ndiscernable from orthography alone. After highway layers\nthe nearest neighbor of you is we, which is orthographically\ndistinct from you. Another example is while and though—\nthese words are far apart edit distance-wise yet the composi-\ntion model is able to place them near each other. The model\n12The difference in parameters is greater for non-PTB corpora\nas the size of the word model scales faster with |V|. For example,\non Arabic the small/large word models have35m/121m parameters\nwhile the corresponding character models have 29m/69m parame-\nters respectively.\n13All models were trained on GPUs with 2GB memory.\nFigure 2: Plot of character n-gram representations via PCA for\nEnglish. Colors correspond to: preﬁxes (red), sufﬁxes (blue), hy-\nphenated (orange), and all others (grey). Preﬁxes refer to character\nn-grams which start with the start-of-word character. Sufﬁxes like-\nwise refer to character n-grams which end with the end-of-word\ncharacter.\nalso makes some clear mistakes (e.g.his and hhs), highlight-\ning the limits of our approach, although this could be due to\nthe small dataset.\nThe learned representations of OOV words ( computer-\naided, misinformed) are positioned near words with the\nsame part-of-speech. The model is also able to correct for\nincorrect/non-standard spelling ( looooook), indicating po-\ntential applications for text normalization in noisy domains.\nLearned CharacterN-gram Representations\nAs discussed previously, each ﬁlter of the CharCNN is es-\nsentially learning to detect particular charactern-grams. Our\ninitial expectation was that each ﬁlter would learn to activate\non different morphemes and then build up semantic repre-\nsentations of words from the identiﬁed morphemes. How-\never, upon reviewing the character n-grams picked up by\nthe ﬁlters (i.e. those that maximized the value of the ﬁlter),\nwe found that they did not (in general) correspond to valid\nmorphemes.\nTo get a better intuition for what the character composi-\ntion model is learning, we plot the learned representations\nof all character n-grams (that occurred as part of at least two\nwords in V) via principal components analysis (Figure 2).\nWe feed each character n-gram into the CharCNN and use\nthe CharCNN’s output as the ﬁxed dimensional representa-\ntion for the corresponding character n-gram. As is appar-\nent from Figure 2, the model learns to differentiate between\npreﬁxes (red), sufﬁxes (blue), and others (grey). We also ﬁnd\nthat the representations are particularly sensitive to character\nn-grams containing hyphens (orange), presumably because\nthis is a strong signal of a word’s part-of-speech.\nHighway Layers\nWe quantitatively investigate the effect of highway network\nlayers via ablation studies (Table 7). We train a model with-\nout any highway layers, and ﬁnd that performance decreases\nsigniﬁcantly. As the difference in performance could be\ndue to the decrease in model size, we also train a model\nthat feeds yk (i.e. word representation from the CharCNN)\nIn V ocabulary Out-of-V ocabulary\nwhile his you richard trading computer-aided misinformed looooook\nLSTM-Word\nalthough your conservatives jonathan advertised – – –\nletting her we robert advertising – – –\nthough my guys neil turnover – – –\nminute their i nancy turnover – – –\nchile this your hard heading computer-guided informed look\nLSTM-Char whole hhs young rich training computerized performed cook\n(before highway) meanwhile is four richer reading disk-drive transformed looks\nwhite has youth richter leading computer inform shook\nmeanwhile hhs we eduard trade computer-guided informed look\nLSTM-Char whole this your gerard training computer-driven performed looks\n(after highway) though their doug edward traded computerized outperformed looked\nnevertheless your i carl trader computer transformed looking\nTable 6:Nearest neighbor words (based on cosine similarity) of word representations from the large word-level and character-level (before\nand after highway layers) models trained on the PTB. Last three words are OOV words, and therefore they do not have representations in the\nword-level model.\nLSTM-Char\nSmall Large\nNo Highway Layers 100.3 84 .6\nOne Highway Layer 92.3 79 .7\nTwo Highway Layers 90.1 78 .9\nOne MLP Layer 111.2 92 .6\nTable 7:Perplexity on the Penn Treebank for small/large models\ntrained with/without highway layers.\nthrough a one-layer multilayer perceptron (MLP) to use as\ninput into the LSTM. We ﬁnd that the MLP does poorly, al-\nthough this could be due to optimization issues.\nWe hypothesize that highway networks are especially\nwell-suited to work with CNNs, adaptively combining lo-\ncal features detected by the individual ﬁlters. CNNs have\nalready proven to be been successful for many NLP tasks\n(Collobert et al. 2011; Shen et al. 2014; Kalchbrenner,\nGrefenstette, and Blunsom 2014; Kim 2014; Zhang, Zhao,\nand LeCun 2015; Lei, Barzilay, and Jaakola 2015), and we\nposit that further gains could be achieved by employing\nhighway layers on top of existing CNN architectures.\nWe also anecdotally note that (1) having one to two high-\nway layers was important, but more highway layers gener-\nally resulted in similar performance (though this may de-\npend on the size of the datasets), (2) having more convolu-\ntional layers before max-pooling did not help, and (3) high-\nway layers did not improve models that only used word em-\nbeddings as inputs.\nEffect of Corpus/Vocab Sizes\nWe next study the effect of training corpus/vocabulary sizes\non the relative performance between the different models.\nWe take the German (DE) dataset from DATA-L and vary the\ntraining corpus/vocabulary sizes, calculating the perplex-\n|V|\n10 k 25 k 50 k 100 k\nT\n1 m 17% 16% 21% –\n5 m 8% 14% 16% 21%\n10 m 9% 9% 12% 15%\n25 m 9% 8% 9% 10%\nTable 8:Perplexity reductions by going from small word-level to\ncharacter-level models based on different corpus/vocabulary sizes\non German (D E). |V|is the vocabulary size and T is the number\nof tokens in the training set. The full vocabulary of the 1m dataset\nwas less than 100k and hence that scenario is unavailable.\nity reductions as a result of going from a small word-level\nmodel to a small character-level model. To vary the vocabu-\nlary size we take the most frequent kwords and replace the\nrest with <unk>. As with previous experiments the character\nmodel does not utilize surface forms of <unk> and simply\ntreats it as another token. Although Table 8 suggests that the\nperplexity reductions become less pronounced as the corpus\nsize increases, we nonetheless ﬁnd that the character-level\nmodel outperforms the word-level model in all scenarios.\nFurther Observations\nWe report on some further experiments and observations:\n• Combining word embeddings with the CharCNN’s out-\nput to form a combined representation of a word (to be\nused as input to the LSTM) resulted in slightly worse\nperformance (81 on PTB with a large model). This was\nsurprising, as improvements have been reported on part-\nof-speech tagging (dos Santos and Zadrozny 2014) and\nnamed entity recognition (dos Santos and Guimaraes\n2015) by concatenating word embeddings with the out-\nput from a character-level CNN. While this could be due\nto insufﬁcient experimentation on our part, 14 it suggests\nthat for some tasks, word embeddings are superﬂuous—\ncharacter inputs are good enough.\n• While our model requires additional convolution opera-\ntions over characters and is thus slower than a comparable\nword-level model which can perform a simple lookup at\nthe input layer, we found that the difference was manage-\nable with optimized GPU implementations—for example\non PTB the large character-level model trained at1500 to-\nkens/sec compared to the word-level model which trained\nat 3000 tokens/sec. For scoring, our model can have the\nsame running time as a pure word-level model, as the\nCharCNN’s outputs can be pre-computed for all words in\nV. This would, however, be at the expense of increased\nmodel size, and thus a trade-off can be made between\nrun-time speed and memory (e.g. one could restrict the\npre-computation to the most frequent words).\nRelated Work\nNeural Language Models (NLM) encompass a rich fam-\nily of neural network architectures for language modeling.\nSome example architectures include feed-forward (Bengio,\nDucharme, and Vincent 2003), recurrent (Mikolov et al.\n2010), sum-product (Cheng et al. 2014), log-bilinear (Mnih\nand Hinton 2007), and convolutional (Wang et al. 2015) net-\nworks.\nIn order to address the rare word problem, Alexandrescu\nand Kirchhoff (2006)—building on analogous work on\ncount-based n-gram language models by Bilmes and Kirch-\nhoff (2003)—represent a word as a set of shared factor em-\nbeddings. Their Factored Neural Language Model (FNLM)\ncan incorporate morphemes, word shape information (e.g.\ncapitalization) or any other annotation (e.g. part-of-speech\ntags) to represent words.\nA speciﬁc class of FNLMs leverages morphemic infor-\nmation by viewing a word as a function of its (learned)\nmorpheme embeddings (Luong, Socher, and Manning 2013;\nBotha and Blunsom 2014; Qui et al. 2014). For example Lu-\nong, Socher, and Manning (2013) apply a recursive neural\nnetwork over morpheme embeddings to obtain the embed-\nding for a single word. While such models have proved use-\nful, they require morphological tagging as a preprocessing\nstep.\nAnother direction of work has involved purely character-\nlevel NLMs, wherein both input and output are charac-\nters (Sutskever, Martens, and Hinton 2011; Graves 2013).\nCharacter-level models obviate the need for morphological\ntagging or manual feature engineering, and have the attrac-\ntive property of being able to generate novel words. How-\never they are generally outperformed by word-level models\n(Mikolov et al. 2012).\nOutside of language modeling, improvements have\nbeen reported on part-of-speech tagging (dos Santos and\nZadrozny 2014) and named entity recognition (dos Santos\n14We experimented with (1) concatenation, (2) tensor products,\n(3) averaging, and (4) adaptive weighting schemes whereby the\nmodel learns a convex combination of word embeddings and the\nCharCNN outputs.\nand Guimaraes 2015) by representing a word as a concatena-\ntion of its word embedding and an output from a character-\nlevel CNN, and using the combined representation as fea-\ntures in a Conditional Random Field (CRF). Zhang, Zhao,\nand LeCun (2015) do away with word embeddings com-\npletely and show that for text classiﬁcation, a deep CNN\nover characters performs well. Ballesteros, Dyer, and Smith\n(2015) use an RNN over characters only to train a transition-\nbased parser, obtaining improvements on many morpholog-\nically rich languages.\nFinally, Ling et al. (2015) apply a bi-directional LSTM\nover characters to use as inputs for language modeling and\npart-of-speech tagging. They show improvements on various\nlanguages (English, Portuguese, Catalan, German, Turkish).\nIt remains open as to which character composition model\n(i.e. CNN or LSTM) performs better.\nConclusion\nWe have introduced a neural language model that utilizes\nonly character-level inputs. Predictions are still made at the\nword-level. Despite having fewer parameters, our model\noutperforms baseline models that utilize word/morpheme\nembeddings in the input layer. Our work questions the ne-\ncessity of word embeddings (as inputs) for neural language\nmodeling.\nAnalysis of word representations obtained from the char-\nacter composition part of the model further indicates that\nthe model is able to encode, from characters only, rich se-\nmantic and orthographic features. Using the CharCNN and\nhighway layers for representation learning (e.g. as input into\nword2vec (Mikolov et al. 2013)) remains an avenue for fu-\nture work.\nInsofar as sequential processing of words as inputs is\nubiquitous in natural language processing, it would be in-\nteresting to see if the architecture introduced in this paper is\nviable for other tasks—for example, as an encoder/decoder\nin neural machine translation (Cho et al. 2014; Sutskever,\nVinyals, and Le 2014).\nAcknowledgments\nWe are especially grateful to Jan Botha for providing the\npreprocessed datasets and the model results.\nReferences\nAlexandrescu, A., and Kirchhoff, K. 2006. Factored Neural Lan-\nguage Models. In Proceedings of NAACL.\nBallesteros, M.; Dyer, C.; and Smith, N. A. 2015. Im-\nproved Transition-Based Parsing by Modeling Characters instead\nof Words with LSTMs. In Proceedings of EMNLP.\nBengio, Y .; Ducharme, R.; and Vincent, P. 2003. A Neural Prob-\nabilistic Language Model. Journal of Machine Learning Research\n3:1137–1155.\nBengio, Y .; Simard, P.; and Frasconi, P. 1994. Learning Long-term\nDependencies with Gradient Descent is Difﬁcult. IEEE Transac-\ntions on Neural Networks 5:157–166.\nBilmes, J., and Kirchhoff, K. 2003. Factored Language Models\nand Generalized Parallel Backoff. In Proceedings of NAACL.\nBotha, J., and Blunsom, P. 2014. Compositional Morphology for\nWord Representations and Language Modelling. In Proceedings\nof ICML.\nBotha, J. 2014. Probabilistic Modelling of Morphologically Rich\nLanguages. DPhil Dissertation, Oxford University.\nChen, S., and Goodman, J. 1998. An Empirical Study of Smooth-\ning Techniques for Language Modeling. Technical Report, Har-\nvard University.\nCheng, W. C.; Kok, S.; Pham, H. V .; Chieu, H. L.; and Chai, K. M.\n2014. Language Modeling with Sum-Product Networks. In Pro-\nceedings of INTERSPEECH.\nCho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning Phrase\nRepresentations using RNN Encoder-Decoder for Statistical Ma-\nchine Translation. In Proceedings of EMNLP.\nCollobert, R.; Weston, J.; Bottou, L.; Karlen, M.; Kavukcuoglu, K.;\nand Kuksa, P. 2011. Natural Language Processing (almost) from\nScratch. Journal of Machine Learning Research 12:2493–2537.\nCreutz, M., and Lagus, K. 2007. Unsupervised Models for Mor-\npheme Segmentation and Morphology Learning. InProceedings of\nthe ACM Transations on Speech and Language Processing.\nDeerwester, S.; Dumais, S.; and Harshman, R. 1990. Indexing by\nLatent Semantic Analysis. Journal of American Society of Infor-\nmation Science 41:391–407.\ndos Santos, C. N., and Guimaraes, V . 2015. Boosting Named Entity\nRecognition with Neural Character Embeddings. InProceedings of\nACL Named Entities Workshop.\ndos Santos, C. N., and Zadrozny, B. 2014. Learning Character-\nlevel Representations for Part-of-Speech Tagging. In Proceedings\nof ICML.\nGraves, A. 2013. Generating Sequences with Recurrent Neural\nNetworks. arXiv:1308.0850.\nHinton, G.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2012. Improving Neural Networks by Prevent-\ning Co-Adaptation of Feature Detectors. arxiv:1207.0580.\nHochreiter, S., and Schmidhuber, J. 1997. Long Short-Term Mem-\nory. Neural Computation 9:1735–1780.\nKalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A Con-\nvolutional Neural Network for Modelling Sentences. In Proceed-\nings of ACL.\nKim, Y . 2014. Convolutional Neural Networks for Sentence Clas-\nsiﬁcation. In Proceedings of EMNLP.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. 2012. ImageNet\nClassiﬁcation with Deep Convolutional Neural Networks. In Pro-\nceedings of NIPS.\nLeCun, Y .; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.;\nHubbard, W.; and Jackel, L. D. 1989. Handwritten Digit Recogni-\ntion with a Backpropagation Network. In Proceedings of NIPS.\nLei, T.; Barzilay, R.; and Jaakola, T. 2015. Molding CNNs for\nText: Non-linear, Non-consecutive Convolutions. In Proceedings\nof EMNLP.\nLing, W.; Lui, T.; Marujo, L.; Astudillo, R. F.; Amir, S.; Dyer, C.;\nBlack, A. W.; and Trancoso, I. 2015. Finding Function in Form:\nCompositional Character Models for Open V ocabulary Word Rep-\nresentation. In Proceedings of EMNLP.\nLuong, M.-T.; Socher, R.; and Manning, C. 2013. Better Word\nRepresentations with Recursive Neural Networks for Morphology.\nIn Proceedings of CoNLL.\nMarcus, M.; Santorini, B.; and Marcinkiewicz, M. 1993. Building\na Large Annotated Corpus of English: the Penn Treebank. Compu-\ntational Linguistics 19:331–330.\nMikolov, T., and Zweig, G. 2012. Context Dependent Recurrent\nNeural Network Language Model. In Proceedings of SLT.\nMikolov, T.; Karaﬁat, M.; Burget, L.; Cernocky, J.; and Khudanpur,\nS. 2010. Recurrent Neural Network Based Language Model. In\nProceedings of INTERSPEECH.\nMikolov, T.; Deoras, A.; Kombrink, S.; Burget, L.; and Cernocky,\nJ. 2011. Empirical Evaluation and Combination of Advanced Lan-\nguage Modeling Techniques. In Proceedings of INTERSPEECH.\nMikolov, T.; Sutskever, I.; Deoras, A.; Le, H.-S.; Kombrink, S.;\nand Cernocky, J. 2012. Subword Language Modeling with Neural\nNetworks. preprint: www.ﬁt.vutbr.cz/˜imikolov/rnnlm/char.pdf.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-\nﬁcient Estimation of Word Representations in Vector Space.\narXiv:1301.3781.\nMnih, A., and Hinton, G. 2007. Three New Graphical Models for\nStatistical Language Modelling. In Proceedings of ICML.\nMorin, F., and Bengio, Y . 2005. Hierarchical Probabilistic Neural\nNetwork Language Model. In Proceedings of AISTATS.\nPascanu, R.; Culcehre, C.; Cho, K.; and Bengio, Y . 2013. How to\nConstruct Deep Neural Networks. arXiv:1312.6026.\nQui, S.; Cui, Q.; Bian, J.; and Gao, B. 2014. Co-learning of Word\nRepresentations and Morpheme Representations. In Proceedings\nof COLING.\nShen, Y .; He, X.; Gao, J.; Deng, L.; and Mesnil, G. 2014. A Latent\nSemantic Model with Convolutional-pooling Structure for Infor-\nmation Retrieval. In Proceedings of CIKM.\nSrivastava, R. K.; Greff, K.; and Schmidhuber, J. 2015. Training\nVery Deep Networks. arXiv:1507.06228.\nSundermeyer, M.; Schluter, R.; and Ney, H. 2012. LSTM Neural\nNetworks for Language Modeling.\nSutskever, I.; Martens, J.; and Hinton, G. 2011. Generating Text\nwith Recurrent Neural Networks.\nSutskever, I.; Vinyals, O.; and Le, Q. 2014. Sequence to Sequence\nLearning with Neural Networks.\nWang, M.; Lu, Z.; Li, H.; Jiang, W.; and Liu, Q. 2015. genCNN:\nA Convolutional Architecture for Word Sequence Prediction. In\nProceedings of ACL.\nWerbos, P. 1990. Back-propagation Through Time: what it does\nand how to do it. In Proceedings of IEEE.\nZaremba, W.; Sutskever, I.; and Vinyals, O. 2014. Recurrent Neural\nNetwork Regularization. arXiv:1409.2329.\nZhang, S.; Jiang, H.; Xu, M.; Hou, J.; and Dai, L. 2015. The Fixed-\nSize Ordinally-Forgetting Encoding Method for Neural Network\nLanguage Models. In Proceedings of ACL.\nZhang, X.; Zhao, J.; and LeCun, Y . 2015. Character-level Convo-\nlutional Networks for Text Classiﬁcation. In Proceedings of NIPS.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8036916255950928
    },
    {
      "name": "Treebank",
      "score": 0.7505033016204834
    },
    {
      "name": "Morpheme",
      "score": 0.717494785785675
    },
    {
      "name": "Character (mathematics)",
      "score": 0.7036501169204712
    },
    {
      "name": "Natural language processing",
      "score": 0.651371955871582
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6492941379547119
    },
    {
      "name": "Language model",
      "score": 0.6254628896713257
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6171377301216125
    },
    {
      "name": "Word (group theory)",
      "score": 0.5458607077598572
    },
    {
      "name": "Convolutional neural network",
      "score": 0.504792332649231
    },
    {
      "name": "Artificial neural network",
      "score": 0.4324404299259186
    },
    {
      "name": "German",
      "score": 0.4307868182659149
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.41103583574295044
    },
    {
      "name": "Linguistics",
      "score": 0.35583174228668213
    },
    {
      "name": "Speech recognition",
      "score": 0.34444883465766907
    },
    {
      "name": "Parsing",
      "score": 0.11327105760574341
    },
    {
      "name": "Mathematics",
      "score": 0.07942402362823486
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36672615",
      "name": "Courant Institute of Mathematical Sciences",
      "country": "US"
    }
  ]
}