{
    "title": "Hybrid Transformer-Based Large Language Models for Word Sense Disambiguation in the Low-Resource Sesotho sa Leboa Language",
    "url": "https://openalex.org/W4408870195",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2503451115",
            "name": "Hlaudi Daniel Masethe",
            "affiliations": [
                "Tshwane University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2185218082",
            "name": "Mosima Anna  Masethe",
            "affiliations": [
                "Sefako Makgatho Health Sciences University"
            ]
        },
        {
            "id": "https://openalex.org/A2062012619",
            "name": "Sunday O. Ojo",
            "affiliations": [
                "Durban University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2643286278",
            "name": "Pius A. Owolawi",
            "affiliations": [
                "Tshwane University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A164943294",
            "name": "Fausto Giunchiglia",
            "affiliations": [
                "University of Trento"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4240560945",
        "https://openalex.org/W4320011320",
        "https://openalex.org/W2770028682",
        "https://openalex.org/W1667652016",
        "https://openalex.org/W6663777190",
        "https://openalex.org/W178598126",
        "https://openalex.org/W2573997883",
        "https://openalex.org/W2169855455",
        "https://openalex.org/W6675491295",
        "https://openalex.org/W6750677391",
        "https://openalex.org/W3165244476",
        "https://openalex.org/W4395688644",
        "https://openalex.org/W4390970565",
        "https://openalex.org/W4319718034",
        "https://openalex.org/W4389933731",
        "https://openalex.org/W4313342727",
        "https://openalex.org/W3214195598",
        "https://openalex.org/W4321089622",
        "https://openalex.org/W2336380850",
        "https://openalex.org/W2586919913",
        "https://openalex.org/W2509161097",
        "https://openalex.org/W4311414551",
        "https://openalex.org/W3164819020",
        "https://openalex.org/W3117718204",
        "https://openalex.org/W4389947148",
        "https://openalex.org/W3198659451",
        "https://openalex.org/W4387487230",
        "https://openalex.org/W4312393584",
        "https://openalex.org/W4390675039",
        "https://openalex.org/W4318570864",
        "https://openalex.org/W4391543666",
        "https://openalex.org/W4200568286",
        "https://openalex.org/W4224104888",
        "https://openalex.org/W4315643106",
        "https://openalex.org/W3162462834",
        "https://openalex.org/W4387408641",
        "https://openalex.org/W3200498707",
        "https://openalex.org/W4391929832",
        "https://openalex.org/W4391930131",
        "https://openalex.org/W4321066972",
        "https://openalex.org/W4385188297",
        "https://openalex.org/W4386960399",
        "https://openalex.org/W4392368764",
        "https://openalex.org/W2902736485",
        "https://openalex.org/W3117568220",
        "https://openalex.org/W2962962672",
        "https://openalex.org/W3000062312",
        "https://openalex.org/W4286255294",
        "https://openalex.org/W4388281480",
        "https://openalex.org/W2905014578",
        "https://openalex.org/W3102373121",
        "https://openalex.org/W3161669601",
        "https://openalex.org/W3008736151",
        "https://openalex.org/W3159804782",
        "https://openalex.org/W6870925914",
        "https://openalex.org/W3034549508",
        "https://openalex.org/W4399149859",
        "https://openalex.org/W4398157486",
        "https://openalex.org/W4243289491",
        "https://openalex.org/W3145831416"
    ],
    "abstract": "This study addresses a lexical ambiguity issue in Sesotho sa Leboa that arises from terms with various meanings, often known as homonyms or polysemous words. When compared to, for instance, European languages, this lexical ambiguity in Sesotho sa Leboa causes computational semantic problems in NLP when trying to identify the lexicon of a language. In other words, it is challenging to determine the proper lexical category and sense of words due to this ambiguity problem. In order to address the issue of polysemy in the Sesotho sa Leboa language, this study set out to create a word sense discrimination (WSD) scheme using a corpus-based hybrid transformer-based architecture and deep learning models. Additionally, the performance of baseline and improved machine learning models for a sequence-based natural language processing (NLP) task was assessed and compared. The baseline models included RNN-LSTM, BiGRU, LSTMLM, DeBERTa, and DistilBERT, with accuracies of 61%, 79%, 74%, 70%, and 64%, respectively. Among these, BiGRU emerged as the strongest performer, leveraging its bidirectional architecture to achieve the highest baseline accuracy. Transformer-based models, such as DeBERTa and DistilBERT, demonstrated moderate performance, with the latter prioritizing efficiency at the cost of accuracy. The enhanced results explored optimization techniques and hybrid model architectures to improve performance. BiGRU, optimized with ADAM, achieved an accuracy of 84%, while BiGRU with attention mechanisms further improved to 85%, showcasing the effectiveness of these enhancements. Hybrid models integrating BiGRU with transformer architectures demonstrated varying results. BiGRU + DeBERTa and BiGRU + ALBERT achieved the highest accuracies of 85% and 84%, respectively, highlighting the complementary strengths of bidirectional context modeling and advanced transformer-based contextual understanding. Conversely, the Hybrid BiGRU + RoBERTa model underperformed, with an accuracy of 70%, indicating potential mismatches in model synergy. These findings highlight how crucial hybridization and optimization are to reaching cutting-edge performance on NLP tasks. According to this study’s findings, the most promising approaches for fusing accuracy and efficiency are attention-based BiGRU and BiGRU–transformer hybrids, especially those that incorporate DeBERTa and ALBERT. To further improve speed, future research should concentrate on exploring task-specific optimizations and improving hybrid model integration.",
    "full_text": null
}