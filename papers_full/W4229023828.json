{
  "title": "ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models",
  "url": "https://openalex.org/W4229023828",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2129999472",
      "name": "Junyi Li",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2133263866",
      "name": "Tianyi Tang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2102511683",
      "name": "Zheng Gong",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2110507279",
      "name": "Lixin Yang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A3173862604",
      "name": "Zhuohao Yu",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2099607494",
      "name": "Zhipeng Chen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2104795587",
      "name": "Jingyuan Wang",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2098250721",
      "name": "Zhao Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2998696444",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3157374291",
    "https://openalex.org/W1912975988",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W1919586704",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1523363798",
    "https://openalex.org/W2165579771",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3037763555",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2037423277",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W3167432598",
    "https://openalex.org/W3173273620",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3187134297",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4298149550",
    "https://openalex.org/W235548781",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W4307979480",
    "https://openalex.org/W2788292930",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3177423701",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3037191812"
  ],
  "abstract": "Junyi Li, Tianyi Tang, Zheng Gong, Lixin Yang, Zhuohao Yu, Zhipeng Chen, Jingyuan Wang, Xin Zhao, Ji-Rong Wen. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3519 - 3539\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nElitePLM: An Empirical Study on General Language Ability Evaluation of\nPretrained Language Models\nJunyi Li1,5, Tianyi Tang1, Zheng Gong1, Lixin Yang2, Zhuohao Yu2, Zhipeng Chen2,\nJingyuan Wang3,4, Wayne Xin Zhao1,5∗\nand Ji-Rong Wen1,5\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2Renmin University of China 3Peng Cheng Laboratory\n4School of Computer Science and Engineering, Beihang University\n5Beijing Key Laboratory of Big Data Management and Analysis Methods\n{lijunyi,steven_tang}@ruc.edu.cn batmanfly@gmail.com\nAbstract\nNowadays, pretrained language models (PLMs)\nhave dominated the majority of NLP tasks.\nWhile, little research has been conducted on\nsystematically evaluating the language abilities\nof PLMs. In this paper, we present a large-scale\nempirical study on gen Eral language ab ility\nevaluation of PLMs (ElitePLM). In our study,\nwe design four evaluation dimensions, i.e.,\nmemory, comprehension, reasoning, and com-\nposition, to measure ten widely-used PLMs\nwithin five categories. Our empirical results\ndemonstrate that: (1) PLMs with varying train-\ning objectives and strategies are good at differ-\nent ability tests; (2) fine-tuning PLMs in down-\nstream tasks is usually sensitive to the data size\nand distribution; (3) PLMs have excellent trans-\nferability between similar tasks. Moreover, the\nprediction results of PLMs in our experiments\nare released as an open resource for more deep\nand detailed analysis on the language abilities\nof PLMs. This paper can guide the future\nwork to select, apply, and design PLMs for\nspecific tasks. We have made all the details\nof experiments publicly available at https:\n//github.com/RUCAIBox/ElitePLM.\n1 Introduction\nRecent years have featured a trend towards Trans-\nformer (Vaswani et al., 2017) based pretrained lan-\nguage models (PLMs) in natural language process-\ning (NLP) systems. By being pretrained on massive\nunlabeled text, PLMs can be directly fine-tuned on\ndownstream tasks, entirely removing the need for\ntask-specific architectures (Radford et al., 2018).\nThis paradigm has led to significant progress on\nmany challenging NLP tasks such as reading com-\nprehension (Devlin et al., 2019) and text genera-\ntion (Brown et al., 2020).\nWith rising new state-of-the-art results that ap-\nproach or surpass human performance on several\ntasks, it is a non-trivial research topic about how\n∗Corresponding author\nto systematically evaluate the language abilities of\nPLMs from a wide range of perspectives. Given a\nwide range of publicly released PLMs, it is partic-\nularly useful to derive principles or guidelines for\nselecting suitable PLMs for specific downstream\ntasks. However, existing works either target some\nsingle ability (Talmor et al., 2020; Zhou et al.,\n2020), or consider a simple mixture of multiple\n(small-scale) tasks that lack a comprehensive de-\nsign and test (Wang et al., 2019b; Liang Xu, 2020).\nThere has been no detailed and systematic analysis\nof PLM’s abilities in large-scale NLP tasks. To\nfill the gap of PLMs evaluation, we introduce the\ngenEral language ability evaluation (ElitePLM)\nfor empirically and systematically assessing the\ngeneral language abilities of PLMs.\nThe ideal goal behind PLMs is to create a human-\nlike machine learner where it can understand the\nlanguage and then perform any specific task re-\nlated to language. In cognitive science, Wechsler\nAdult Intelligence Scale (WAIS) (Kaufman and\nLichtenberger, 2005) is the most commonly used\nintelligence quotient (IQ) test for measuring the\nintelligence and cognitive ability of humans. This\ntest would assess the level of individuals on ver-\nbal comprehension, perceptual reasoning, working\nmemory, and processing speed. Thus, by imitat-\ning the intelligence test on humans, we design four\nevaluation dimensions in ElitePLM for measuring\nthe abilities of PLMs, including memory, compre-\nhension, reasoning, and composition. Following\nprevious works (Zhou et al., 2020; Wang et al.,\n2019b), for each ability in ElitePLM, we elabo-\nrate and select multiple representative tasks ( e.g.,\nquestion answering for the comprehension ability)\nand commonly-used benchmarks (e.g., GLUE and\nSQuAD) to quantitatively evaluate the performance\nof PLMs. These results can serve as numerical ex-\nplanations of PLMs at a specific ability.\nIn human intelligence tests, the background of\nparticipants ( e.g., gender, race, and occupation)\n3519\nshould be as much as diverse. Thus, in ElitePLM,\nwe select a diversity of PLMs to conduct general-\nized and meaningful comparisons. According to\ntraining objectives, PLMs can be divided into three\ntypes: bidirectional LMs (e.g., BERT (Devlin et al.,\n2019)) for natural language understanding (NLU),\nunidirectional LMs ( e.g., GPT (Radford et al.,\n2019)) for natural language generation (NLG), and\nhybrid LMs ( e.g., UniLM (Dong et al., 2019))\nfor combining these two paradigms. Furthermore,\nknowledge-enhanced LMs ( e.g., ERNIE (Zhang\net al., 2019)) and text-to-text LMs (e.g., T5 (Raffel\net al., 2020)) also emerge as important branches of\nPLMs. Considering the variety, we finally select\nten widely-used PLMs within the above five cate-\ngories and evaluate their abilities on four dimen-\nsions. We show the comparisons of these PLMs in\nTable 7 of Appendix A.\nFrom the ability test results, we have three salient\nfindings. First, PLMs with varying pretraining ob-\njectives and strategies are good at different kinds\nof downstream tasks. Specifically, we observe that\nbidirectional LMs like BERT and pretraining strate-\ngies like larger training batches in RoBERTa are\nhelpful for memorizing pretraining corpora; permu-\ntation language modeling in XLNet is beneficial\nfor modeling the bidirectional context in language\ncomprehension; inter-sentence coherence objective\nin ALBERT is suitable for sentence-level reasoning\ntasks; text-to-text LMs using denoising objective\nlike BART perform better in short text generation.\nSecond, when fine-tuning PLMs in downstream\ntasks, their performance is typically sensitive to the\ndata distribution in fine-tuning stage, which can be\naddressed by incorporating intermediate datasets or\ntasks to alleviate such a discrepancy. Third, PLMs\nhave excellent transferability between similar tasks,\nespecially reasoning tasks. This finding will inspire\nfuture researchers to leverage data-rich tasks for\nimproving data-scarce tasks. For more clarity, we\nillustrate the impact level of each factor for PLMs’\nabilities in Table 8 of Appendix A.\nBesides ElitePLM being an evaluation bench-\nmark of PLMs’ language ability, more importantly,\nthe predicted results of ElitePLM can be used as\nan open resource for more depth and granularity in\nanalyzing PLMs performance on each ability. For\nexample, we further analyze the comprehension\ntest results of PLMs across answer types in QA\ntasks. The analysis shows that PLMs are good at\nsimple single-token answers such as dates but more\nchallenged on intricate phrase answers. Moreover,\nby analyzing human test and Turing test results\non composition, we observe that summaries with\nhigh accuracy are more likely to pass the Turing\ntest while rich information is more important for\nstory generation. Overall, ElitePLM can act as an\nanalysis tool to gain more insight into PLMs. We\nshow the details of our used datasets and predicted\noutputs of PLMs in Appendix B.\nThis paper is intended to help establish sound\nprinciples for choosing, applying, interpreting and\ndesigning PLMs for NLP tasks in practical settings.\nWe have released the code and predicted results\nof each ability experiment, providing the research\nand industry community with off-the-shelf tools to\nevaluate and analyze their PLMs.\n2 ElitePLM\nIn this section, we will detail these four kinds of\nlanguage abilities, i.e., memory, comprehension,\nreasoning, and composition, in ElitePLM.\nMemory Ability. Memory is the most basic abil-\nity of humanity, involved in how much informa-\ntion we recall throughout our lives (Miyake and\nShah, 1999). By analogy, ElitePLM will measure\nhow much knowledge and language patterns PLMs\nhave memorized in pretraining, as assessed by tests\nof recalling words based on contexts. Based on\nthe memorized information, PLMs can effectively\nadapt to downstream tasks for understanding and\nreasoning about the similar context in a specific\ntext. On the other hand, efficiency is also a critical\naspect of memory ability for PLMs learning from\nnew data distribution in the fine-tuning stage. Thus,\nbesides recalling words, we also compare the mem-\nory efficiency of PLMs in terms of memorizing the\ngiven new information.\nComprehension Ability. Comprehension is an in-\ntricate and multifaceted ability. It typically consists\nof understanding a text’s vocabulary, background\nknowledge of a specific topic, and comprehension\nof its linguistic structures like grammar (Cain and\nOakhill, 2008). In particular, background (prior)\nknowledge is used to comprehend a special situa-\ntion, lesson, or text. For example, readers should\nbe aware of the background knowledge of dog be-\nhavior when reading a text about dog training. In\nElitePLM, we will assess PLMs’ comprehension\nability from three aspects, i.e., vocabulary, back-\nground knowledge, and linguistic structures.\n3520\nReasoning Ability. Based on the comprehension\nof a text, reasoning ability refers to the power of the\nprocesses and strategies used in drawing inferences,\nreaching conclusions, arriving at solutions, and\nmaking decisions (Kyllonen and Christal, 1990).\nIn ElitePLM, we mainly focus on three types of rea-\nsoning abilities. In detail, commonsense reasoning\nrequires PLMs to draw inferences using common-\nsense knowledge about the world, like the fact that\n“matches” plus “ logs” usually equals “ fire” (Sap\net al., 2020); Note that subtle differences exist be-\ntween commonsense knowledge and background\nknowledge in comprehension ability. Common-\nsense knowledge is broadly defined as the total\naccumulation of facts and information that a per-\nson has gained from previous experiences. Deduc-\ntive reasoning involves PLMs drawing conclusions\nfrom a set of given premises in the form of cate-\ngorical syllogisms (e.g., all x are y) or symbolic\nlogic (e.g., if p then q) (Johnson-Laird, 1999); Ab-\nductive reasoning involves reaching the most likely\nexplanation for a set of facts, such as a scientific\ntheory to explain a set of empirical findings (Wal-\nton, 2014).\nComposition Ability. In the literature (Connors,\n1997), composition is a highly intelligent and syn-\nthetic process where a writer assembles words and\nsentences to create a coherent and meaningful work\n(e.g., poem, music, and novel) from scratch, which\nclosely resembles to the text generation task in\nNLP (Berninger, 1999). Therefore, in ElitePLM,\nwe introduce several text generation tasks to eval-\nuate the composition ability of PLMs, including\nstory generation, text summarization, and question\ngeneration. Note that, story generation is a repre-\nsentative composition task which needs PLMs to\nnot only comprehend the given story background,\nbut also reason about and create reasonable and\ncoherent story endings (Fan et al., 2018). During\nthe composition process, PLMs should include a\ngood vocabulary, grammar, spelling, and punctua-\ntion knowledge, and deliberate the text structure.\n3 Experiments\nIn this section, we first set up baselines, and then\nreport the results and analysis on four ability tests.\n3.1 Models\nAs mentioned before, we compare the performance\nof ten publicly released PLMs from five categories:\n(1) Bidirectional LMs: BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019b), and ALBERT (Lan\net al., 2020); (2) Unidirectional LMs: GPT-2 (Rad-\nford et al., 2019); (3) Hybrid LMs: XLNet (Yang\net al., 2019) and UniLM (Dong et al., 2019); (4)\nKnowledge-enhanced LMs: ERNIE (Zhang et al.,\n2019); (5) Text-to-Text LMs: BART (Lewis et al.,\n2020), T5 (Raffel et al., 2020), and ProphetNet (Qi\net al., 2020). We implement these models and abil-\nity tests mostly on huggingface (Wolf et al., 2020),\nfairseq (Ott et al., 2019), and jiant (Phang et al.,\n2020). To reflect the true level of language abilities,\nwe adopt the best hyper-parameter values reported\nin their original papers for each PLM.\n3.2 Memory Tests\nDatasets and Metrics. The goal of memory tests\nis to assess how much knowledge and language\npatterns PLMs have memorized during pretraining.\nFor this purpose, we adopt two datasets for evalua-\ntion, i.e., LAMA (F. Petroni and Riedel, 2019) and\nEnglish Wikipedia (2,500M words). Specifically,\nLAMA is a knowledge probe corpus containing\na set of knowledge facts, where facts are either\nsubject-relation-object triples or question-answer\npairs. Each fact is converted into a cloze state-\nment where the subject or object entity is masked.\nWikipedia is one of the widely-used pretraining\ncorpora for our selected PLMs (except GPT-2 and\nT5). Therefore, to conduct a fair comparison, we\ncontinuously train GPT-2 and T5 on Wikipedia us-\ning their pretraining objectives. Similar to LAMA,\nwe randomly sample 100,000 texts from Wikipedia\nand then mask a proportion of 15% tokens follow-\ning BERT. By querying PLMs with the missing\ntokens on Wikipedia and LAMA, we can test the\nlanguage patterns and factual knowledge in PLMs’\nmemory. For metrics, we use Mean Precision at\nOne (P@1) of predicting missing tokens. For ef-\nficiency, we measure it as the performance w.r.t.\nthe number of training epochs: the more efficient a\nmodel is, the fewer epochs to achieve a reference\nperformance.\nResults and Analysis. To evaluate how much text\nPLMs have recalled in pretraining, we directly test\nPLMs using Wikipedia and LAMA without fine-\ntuning, similar to zero-shot learning. The results\non P@1 metric are shown in Table 1. Compared\nwith bidirectional and hybrid LMs (e.g., BERT and\nXLNet), GPT-2 uses auto-regressive self-attention\nwhere every token can only attend to the context to\n3521\nModels\nBidirectional Uni. Hybrid KE Text-to-Text\nBERT RoBERTa ALBERT GPT-2 XLNet UniLM ERNIE T5 BART ProphetNet\nVocab Size 28996 50265 30000 50257 32000 28996 28996 32100 50295 30522\nLAMA\nGoogle-RE 11.0 7.1 3.3 3.9 10.0 9.6 1.3 4.0 9.4 0.1\nT-REx 29.2 23.9 21.0 12.0 28.9 28.4 13.4 21.7 15.8 1.1\nConceptNet 19.1 21.6 20.0 6.4 19.5 18.3 13.0 17.1 7.7 0.3\nSQuAD 17.0 21.0 20.6 5.6 20.8 17.4 8.1 11.7 3.1 0.7\nWikipedia 70.9 71.1 63.9 42.7 68.7 71.5 45.7 65.0 47.8 31.3\nTable 1: Memory test results on LAMA and Wikipedia datasets (test set). These results are based on the LARGE\nversion of each PLM and more results can be found in the Appendix C. Bold and underlined fonts denote the best\nand the second best performance of a PLM (the same as below).\nRelation Template BERT RoBERTa GPT-2 BART T5\n<[X], place_of_death, [Y]>\n[X] died in [MASK]. 13.98 0.46 0.15 11.09 4.19\n[X] passed away in [MASK]. 13.46 0.46 0.62 3.54 1.51\n[X]’s place of death was [MASK]. 3.27 0.00 0.00 0.00 1.51\n<[X], place_of_birth, [Y]>\n[X] was born in [MASK]. 16.07 12.52 7.53 14.77 6.32\n[X] was born in the place of [MASK]. 2.83 1.29 0.00 0.00 1.39\n[X]’s place of birth was [MASK]. 12.16 1.87 0.00 0.00 3.12\nTable 2: The impact of template on eliciting PLMs’ stored knowledge.\n/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056\n/uni00000014/uni00000013\n/uni00000015/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000024/uni0000002f/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000025/uni00000024/uni00000035/uni00000037\n/uni0000003b/uni0000002f/uni00000031/uni00000048/uni00000057\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000015\n(a) Google-RE\n/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000024/uni0000002f/uni00000025/uni00000028/uni00000035/uni00000037\n/uni0000003b/uni0000002f/uni00000031/uni00000048/uni00000057\n/uni00000025/uni00000024/uni00000035/uni00000037\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000015 (b) T-REx\nFigure 1: Memory efficiency (P@1) of five PLMs on\nGoogle-RE and T-REx datasets.\nits left. This unidirectional training objective natu-\nrally limits the performance of GPT-2 in terms of\nmemorizing information. It has been previously re-\nported that PLMs can remember more information\nby scaling up the model size (Brown et al., 2020).\nHowever, in our tests, BART-large (400M) achieves\nworse results than RoBERTa-base (125M) with the\nsame training corpus and similar vocabulary sizes\n(50,295 vs 50,265). During pretraining, RoBERTa\nadopts bidirectional objectives and novel strategies\nlike larger training batches. It can be concluded\nthat, as opposed to model size, training objectives\nand strategies reflect the way that PLMs memo-\nrize information, making significant impacts on\nPLMs’ memory ability. Besides, we can clearly\nobserve that all PLMs achieve their best results\nin T-REx (created from Wikipedia triples) among\nLAMA, and perform relatively well on Wikipedia.\nThis implies that PLMs indeed remember a large\nproportion of knowledge and language patterns\nfrom pretraining corpora.\nTo test the memory efficiency, we fine-tune five\nmodels, BERT, ALBERT, GPT-2, BART, and XL-\nNet, for several epochs. As shown in Figure 1, to\nachieve a reference performance, the bidirectional\ntraining objective like BERT needs fewer epochs\nthan other kinds of objectives. This further im-\nplies that the bidirectional training objective is also\nhelpful to facilitate the memory efficiency since\nbidirectional language modeling can make PLMs\nmore quickly capture the language patterns.\nBased on the memory test results, we further\nanalyze how to effectively elicit the information\nfrom PLMs’ memory. LAMA hand-crafts tem-\nplates to test PLMs by filling the [MASK] token.\nTherefore, we conduct a pilot study on designing\ndifferent templates for two relations in Google-RE.\nTable 2 shows that different templates can result in\nsubstantial differences in eliciting PLMs’ memory.\nThe bidirectional LMs, e.g., BERT, show relatively\nadaptability to varying templates, further verifying\ntheir strength in memory ability. Therefore, with\nlarge-scale knowledge stored in PLMs, how to de-\nrive an effective and appropriate method to provoke\nthem is a key challenge.\n3522\nModels WNLI CoLA MNLI RTE QNLI SST-2 QQP STS-B MRPC Avg.\nAcc. Matt. M./MM. Acc. Acc. Acc. F1/Acc. P/S Corr. F1/Acc.\nBERTBASE 65.1 52.1 84.6/83.4 66.4 90.5 93.5 69.9/88.2 77.4/73.7 79.0/85.1 76.5\nBERTLARGE 65.1 60.5 86.7/85.9 70.1 92.7 94.9 72.1/89.3 87.6/86.5 85.4/89.3 80.5\nRoBERTaBASE 65.1 61.4 87.4/87.2 75.1 92.9 95.7 72.5/89.4 89.2/88.5 87.5/90.7 81.8\nRoBERTaLARGE 89.0 67.8 90.8/90.2 88.2 98.9 96.7 74.3/90.2 92.2/91.9 89.9/92.4 88.5\nALBERTXLARGE 65.8 58.2 35.6/36.5 62.5 94.2 95.1 71.7/88.9 87.6/86.6 69.8/80.3 72.7\nALBERTXXLARGE 64.4 64.7 89.7/89.6 70.4 95.3 96.0 70.7/88.4 91.3/90.6 68.1/80.4 80.6\nGPT-2SMALL 54.8 33.8 81.1/81.4 62.1 86.7 91.2 69.8/87.9 79.0/76.5 76.9/83.6 71.9\nGPT-2MEDIUM 54.1 50.5 84.8/84.5 63.6 91.2 92.1 71.4/88.6 84.3/82.7 80.0/85.5 75.8\nXLNetBASE 58.9 26.2 86.1/85.3 59.9 91.3 94.0 71.5/88.9 83.9/82.9 84.3/88.3 74.0\nXLNetLARGE 92.5 70.2 90.9 /90.9 88.5 99.0 97.1 74.7 /90.4 93.0 /92.6 90.5 /92.9 89.5\nUniLMBASE 65.1 49.0 83.0/82.2 60.3 88.7 92.3 70.7/88.4 82.3/81.4 84.3/88.7 76.2\nUniLMLARGE 65.1 61.1 87.0/85.9 70.9 92.7 94.5 71.5/89.2 86.6/85.3 85.2/89.1 80.5\nERNIEBASE 65.1 52.3 84.0/83.2 68.8 91.3 93.5 70.5/88.4 85.1/83.8 80.3/85.9 70.7\nT5BASE 78.8 51.1 87.1/86.2 80.1 93.7 95.2 72.6/89.4 89.4/88.6 87.5/90.7 82.7\nT5LARGE 85.6 61.2 89.9/89.6 87.2 94.8 96.3 73.9/89.9 89.9/89.2 89.8/92.4 86.4\nBARTBASE 65.1 52.8 85.1/84.3 69.5 92.6 94.4 72.5/89.7 87.6/86.6 86.1/89.5 79.5\nBARTLARGE 58.9 62.4 90.2/89.3 83.5 94.8 96.3 73.6/90.1 91.1/90.4 87.8/91.1 83.1\nProphetNetLARGE 52.1 24.2 81.3/80.8 51.3 93.2 93.6 70.6/88.1 73.5/72.3 69.7/80.8 69.2\nTable 3: Comprehension tests results on GLUE (test set). All results are scored by the GLUE evaluation server1.\n/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048/uni00000056\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013/uni00000030/uni00000044/uni00000057/uni00000057/uni0000004b/uni00000048/uni0000005a/uni00000056\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000025/uni00000024/uni00000035/uni00000037\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000015\n/uni00000038/uni00000051/uni0000004c/uni0000002f/uni00000030\n(a) CoLA\n/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048/uni00000056\n/uni00000018/uni00000013\n/uni00000018/uni00000018\n/uni00000019/uni00000013\n/uni00000019/uni00000018\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n (b) QNLI\nFigure 2: Few-shot results of four PLMs on CoLA and\nQNLI tasks.\n3.3 Comprehension Tests\nDatasets and Metrics. In comprehension tests, we\ntake into account three aspects of comprehension\nability, including vocabulary, background knowl-\nedge, and linguistic structures. Therefore, we em-\nploy five datasets for comprehension tests, i.e.,\nGLUE (Wang et al., 2019b), SuperGLUE (Wang\net al., 2019a), SQuAD v1.1 (Rajpurkar et al.,\n2016), SQuAD v2.0 (Rajpurkar et al., 2018), and\nRACE (Lai et al., 2017). Among these datasets,\nGLUE and SuperGLUE are two widely-used com-\nprehension benchmarks. Several tasks, like word\nsense disambiguation and coreference resolution,\ncan assess PLMs’ understanding of vocabulary\nmeaning and grammatical structure of a text. By\ncontrast, SQuAD v1.1&v2.0, and RACE are three\npopular question answering datasets. To answer the\n1https://gluebenchmark.com/\n/uni00000027/uni00000044/uni00000057/uni00000048/uni00000033/uni00000048/uni00000055/uni00000056/uni00000052/uni00000051/uni00000031/uni00000052/uni00000058/uni00000051/uni00000024/uni00000047/uni0000004d/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000039/uni00000048/uni00000055/uni00000045\n/uni00000024/uni00000051/uni00000056/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000029/uni00000014/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000035/uni00000052/uni00000025/uni00000028/uni00000035/uni00000037/uni00000044\n/uni00000024/uni0000002f/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000025/uni00000024/uni00000035/uni00000037\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051\nFigure 3: PLMs Performance on SQuAD v1.1&v2.0\nstratified by five types of answer.\nnatural language questions, PLMs should be aware\nof the background knowledge about some partic-\nular topic. For example, to answer the question\n“what can be used as rewards for dog training?”,\nthe background knowledge “dogs like bones” will\nbe helpful for PLMs to answer “bones”. For evalu-\nation, we report the corresponding metrics results\nfor each task, such as the Matthews corr. metric for\nCoLA.\nResults and Analysis. Table 3 presents the results\nof comprehension test in GLUE dataset (results in\nother four datasets can be found in Appendix D).\nThe last column in this table indicates the average\noverall performance across all tasks. Interestingly,\nthe models behaving well in memory tests ( e.g.,\nRoBERTa and XLNet) also present good results\nin many comprehension tasks. The results indi-\ncate that the improvement on memory ability is\nbeneficial for the performance of comprehen-\nsion ability, which is in line with our intuition.\nCompared with bidirectional language modeling in\n3523\nDatasets\nBidirectional Uni. Hybrid KE Text-to-Text\nBERT RoBERTa ALBERT GPT-2 XLNet UniLM ERNIE T5 BART ProphetNet\nCQA 55.9 72.2 80.0 60.8 62.9 62.3 54.1 69.8 75.8 21.3\nROCStories 90.2 97.4 97.1 59.9 93.8 86.9 84.7 91.4 91.7 82.2\nSW AG 86.3 89.9 90.7 79.7 86.8 83.1 80.2 73.7 87.9 70.1\nHellaSwag 47.3 85.2 90.1 60.4 79.7 46.7 44.5 79.1 76.6 26.4\nSM-A 89.4 93.0 92.5 88.7 83.7 89.3 88.7 92.7 82.9 85.5\nSM-B 85.8 92.3 92.3 73.4 88.7 86.4 87.7 88.2 67.9 78.0\nARCT 71.2 57.9 79.5 66.7 83.1 72.3 73.7 69.4 84.2 65.5\nTable 4: Reasoning tests results on seven datasets (test set). CQA is short for CommonsenseQA. SM-A and SM-B\ndenote the Task A and Task B of Sense Making, respectively. We report the results of LARGE version for each\nmodel in this table and more results can be found in the Appendix E.\nBERT, permutation language modeling (relying on\nall permutations of the factorization order) used in\nXLNet enables PLMs to learn more context for en-\nhancing PLMs’ understanding of text, which seems\nto be effective for good comprehension ability.\nAmong these tasks, we observe a significant per-\nformance drop in the linguistic acceptability task\n(CoLA) since it has different data distribution from\nthe pretraining corpora (Wang et al., 2021). This\nkind of sensitiveness to unfamiliar tasks is also re-\nflected in Figure 2, where the model performance\non CoLA shows a more volatile fluctuation (rang-\ning from 10 to 35) than QNLI (ranging from 15 to\n20). It indicates that the performance of PLMs is\nclosely related to the similarity of data distribu-\ntions in pretraining and fine-tuning. To solve this\nchallenge, it will be better to adopt intermediate\nfine-tuning, which involves first fine-tuning PLMs\non an intermediate dataset similar to the final target\ndataset and then transferring tuned PLMs to the\nfinal dataset.\nTo gain more insights into PLMs’ comprehen-\nsion ability, we choose four representative PLMs\n(i.e., BERT, RoBERTa, ALBERT, and BART) and\nhumans to analyze their performance across the\nanswer types of SQuAD v1.1&v2.0. The results\nin Figure 3 show that PLMs perform well on sim-\nple answers such as dates and persons. For these\ncategories of answers, there are usually only a few\nplausible candidates and most answers are single\ntokens. The models are more challenged on other\nintricate answer types (e.g., noun and verb phrases)\nbecause there are many more plausible candidates\nand multiple tokens. Thus, improving PLMs’ un-\nderstanding of intricate named entities during the\npretraining phase will possibly benefit PLMs’ com-\nprehension ability later.\n/uni00000035/uni00000032/uni00000026/uni00000036/uni00000036/uni00000030/uni00000010/uni00000024/uni00000024/uni00000035/uni00000026/uni00000037\n/uni00000037/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000057/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e\n/uni00000035/uni00000032/uni00000026/uni00000036/uni00000036/uni00000030/uni00000010/uni00000024/uni00000024/uni00000035/uni00000026/uni00000037\n/uni00000036/uni00000052/uni00000058/uni00000055/uni00000046/uni00000048/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e\n/uni0000001c/uni00000013/uni00000011/uni00000015/uni00000019/uni0000001c/uni00000011/uni0000001c/uni0000001a/uni00000015/uni00000011/uni00000014\n/uni0000001b/uni00000015/uni00000011/uni00000015/uni0000001b/uni0000001c/uni00000011/uni00000017/uni0000001a/uni00000013/uni00000011/uni0000001a\n/uni0000001a/uni00000019/uni00000011/uni00000014/uni00000018/uni0000001a/uni00000011/uni0000001c/uni0000001a/uni00000014/uni00000011/uni00000015\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013\n/uni0000001c/uni00000013\n(a) BERTLARGE\n/uni00000035/uni00000032/uni00000026/uni00000036/uni00000036/uni00000030/uni00000010/uni00000024/uni00000024/uni00000035/uni00000026/uni00000037\n/uni00000037/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000057/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e\n/uni00000035/uni00000032/uni00000026/uni00000036/uni00000036/uni00000030/uni00000010/uni00000024/uni00000024/uni00000035/uni00000026/uni00000037\n/uni00000036/uni00000052/uni00000058/uni00000055/uni00000046/uni00000048/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e\n/uni0000001c/uni00000014/uni00000011/uni00000017/uni00000019/uni0000001c/uni00000011/uni0000001a/uni00000019/uni00000018/uni00000011/uni00000016\n/uni0000001a/uni00000013/uni00000011/uni0000001a/uni0000001c/uni00000015/uni00000011/uni0000001a/uni00000018/uni00000017/uni00000011/uni00000014\n/uni0000001a/uni00000015/uni00000011/uni00000016/uni00000018/uni00000017/uni00000011/uni00000015/uni00000019/uni0000001c/uni00000011/uni00000017\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013\n/uni0000001c/uni00000013 (b) T5 LARGE\nFigure 4: Heatmaps of two-stage transfer learning.\n3.4 Reasoning Tests\nDatasets and Metrics. In reasoning tests, we\nmainly consider three forms of reasoning,i.e., com-\nmonsense reasoning, deductive reasoning, and ab-\nductive reasoning, focusing on commonsense uti-\nlization, conclusion induction, and reason deriva-\ntion, respectively. For evaluation, we choose six\nreasoning datasets, namely CommonsenseQA (Tal-\nmor et al., 2019), ROCStories (Mostafazadeh\net al., 2016), SWAG (Zellers et al., 2018), Hel-\nlaSwag (Zellers et al., 2019), Sense Making (Wang\net al., 2019c), and ARCT (Habernal et al., 2018).\nSpecifically, CommonsenseQA requires PLMs to\nreason about commonsense knowledge in human\nexperience of everyday life (Liu and Singh, 2004).\nROCStories, SWAG, HellaSwag, and Sense Mak-\ning Task A are concerned with deriving the con-\nclusions of stories and events, while Sense Making\nTask B and ARCT focus on identifying the reason\nbehind a statement. For evaluation, we report the\nAccuracy results for each dataset.\nResults and Analysis. Table 4 shows the model\nperformances in reasoning ability. It can be clearly\nobserved that performing well in comprehension\ntests, ALBERT and RoBERTa also achieve stronger\nperformance in almost all reasoning tasks. In pre-\n3524\nModels\nCNN/DailyMail GigaWord SQuAD WritingPrompts\nR-1 R-2 R-L R-1 R-2 R-L B-4 R-L ME B-4 R-L ME\nGPT-2 27.00 8.00 23.08 23.72 8.12 21.56 8.48 18.82 26.77 14.47 3.23 7.29\nUniLm 43.44 20.21 40.51 38.45 19.45 35.75 4.42 17.43 20.13 26.88 1.84 5.01\nT5 42.50 20.68 39.75 34.75 16.26 31.49 11.19 22.35 30.53 8.61 4.19 9.51\nBART 44.16 21.28 40.90 39.41 20.21 36.42 15.87 25.47 38.42 14.72 3.14 7.08\nProphetNet 44.20 21.17 41.30 39.51 20.42 36.69 14.20 23.97 35.99 19.31 2.59 7.19\nTable 5: Composition tests results on four datasets. R-1, R-2, R-L are short for ROUGE-1, ROUGE-2, ROUGE-L\nrespectively. B-4 and MT denote BLEU-4 and METEOR, respectively. We report the result of LARGE version for\neach model in this table and more results can be found in the Appendix F.\ntraining, ALBERT introduces an inter-sentence co-\nherence objective to capture the relationship among\nsentences, which is helpful for the sentence-level\nreasoning ability of PLMs. It has been found\nthat the next sentence prediction (NSP) loss in\nBERT might hurt the performance of PLMs in\nsentence-level tasks of downstream datasets (Liu\net al., 2019b). Interestingly, despite being the best\nin comprehension tests, XLNet does not perform\nas well as we expected in reasoning tests. We spec-\nulate that the permutation operation in XLNet dis-\nturbs the semantic relationship between sentences,\nthus leading to poor reasoning ability. To improve\nPLMs’ reasoning ability, it would be useful to\ndesign sentence-level reasoning objectives like\ninter-sentence coherence loss in ALBERT. More-\nover, despite incorporating knowledge, ERNIE still\nshows mediocre performance in knowledge-related\ndatasets such as CQA. A possible reason might be\nthat ERNIE only uses trained KB embeddings to\nenhance semantic representations but ignores the\nreasoning structure of KBs. This inspires us that\ndesigning appropriate and effective fusion methods\nto integrate knowledge is more important.\nTo further analyze the transferability of PLMs’\nreasoning ability, we conduct a two-stage study on\nthree task datasets, i.e., ROCStories, SM-A, and\nARCT. We first train PLMs on source tasks with\nfull data and then fine-tune PLMs on target tasks\nwith ten instances. In Figure 4, it can be observed\nthat PLMs have better reasoning transferability\nbetween similar tasks such as deductive reason-\ning tasks (ROCStories and SM-A). This shows that\nmodel performance on data-scarce reasoning tasks\ncan be improved by incorporating additional train-\ning on data-rich similar tasks (Wang et al., 2021).\n3.5 Composition Tests\nDatasets and Metrics. Composition is similar to\nthe text generation task, aiming at generating new\nModels\nGigaWord\nTT (%) Flu. Info. Acc. Overall\nGPT-2 26.09 3.11 2.79 2.64 4.87\nUniLM 50.34 4.02 3.49 3.45 6.73\nT5 53.67 3.95 3.45 3.46 6.68\nBART 51.10 4.01 3.46 3.49 6.73\nProphetNet 53.02 3.99 3.52 3.45 6.74\nGold 40.77 3.61 3.29 3.15 6.05\nModels\nWritingPrompts\nTT (%) Flu. Info. Rel. Overall\nGPT-2 45.70 3.42 3.17 3.20 5.87\nUniLM 1.20 1.32 1.88 2.03 2.74\nT5 34.40 3.01 2.80 3.09 5.18\nBART 45.20 3.37 3.16 3.39 5.96\nProphetNet 29.60 2.95 2.91 3.10 5.18\nGold 71.30 3.79 4.07 3.87 7.37\nTable 6: Turing test (TT) and human scores on the test\nset of GigaWord and WritingPrompts. Flu., Info., Acc.\nand Rel. denote fluency, informativeness, accuracy and\nrelevance respectively. We report the result ofLARGE\nversion for each model in this table and more results can\nbe found in the Appendix F.\ncontent from scratch. Therefore, we use four text\ngeneration benchmarks for composition tests, i.e.,\nWritingPrompts (Fan et al., 2018) on story genera-\ntion, CNN/Daily Mail (Hermann et al., 2015) and\nGigaWord (Rush et al., 2015) on text summariza-\ntion, and SQuAD v1.1 (Rajpurkar et al., 2016) on\nquestion generation. According to the length of\nthe target text, text summarization and question\ngeneration is short text generation, while story gen-\neration is long text generation. For evaluation, we\nadopt three automatic metrics, i.e., BLEU (Pap-\nineni et al., 2002), ROUGE (Lin, 2004), and ME-\nTEOR (Banerjee and Lavie, 2005). Besides, follow-\ning (Zou et al., 2021), we conduct human test from\nfive aspects, i.e., Fluency, Informativeness, Accu-\nracy, Relevance and Overall. The overall score is\nrated from 1 to 10, while the others are rated from\n1 to 5. Inspired by Turing (2009), we further de-\n3525\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000037/uni00000058/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000029/uni0000004f/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni0000002c/uni00000051/uni00000049/uni00000052/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000051/uni00000048/uni00000056/uni00000056\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n(a) GigaWord\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000037/uni00000058/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000029/uni0000004f/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni0000002c/uni00000051/uni00000049/uni00000052/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000051/uni00000048/uni00000056/uni00000056\n/uni00000035/uni00000048/uni0000004f/uni00000048/uni00000059/uni00000044/uni00000051/uni00000046/uni00000048 (b) WritingPrompts\nFigure 5: Impact factors of Turing Test.\nsign a Turing test to assess the generation ability of\nPLMs, where a human interrogator is requested to\ndistinguish whether the given text is generated by\na human. From the generated texts of each model\nand gold texts, we randomly select 500 texts scored\nby judges. More details of human test and Turing\ntest are shown in Appendix F.\nResults and Analysis. Table 5 and Table 6 present\nthe automatic evaluation and human evaluation re-\nsults on composition ability, respectively. We can\nobserve that ProphetNet and BART achieve great\nperformance on short text generation, while GPT-2\nand T5 show better results on long text generation.\nSpecifically, BART employs denoising objectives\nfor reconstructing the corrupted original text, and\nProphetNet adopts future n-gram prediction, which\nis flexible for modeling the semantic relations be-\ntween tokens and phrases in short texts. However,\nin long texts, a small ratio of masked tokens (i.e.,\n15%) might be not effective in capturing the com-\nplex long-range dependency. By comparison, the\nleft-to-right prediction objective in GPT-2 can be\nmore suitable to model the long-range semantic\ncontinuity in long texts, and T5 has the largest\nmodel size to achieve a strong composition abil-\nity. For composition ability, we conclude that the\ndenoising objective is helpful for short text com-\nposition, while the left-to-right objective is more\npowerful for long text composition. Besides, the\nmodel size is also an important factor in improving\nPLMs’ composition ability.\nTo further investigate what factors affect the pass\nrate of the Turing test, we deeply analyze the in-\ntermediate scoring results in the human test and\nTuring test. As shown in Figure 5, we calculate\nthe pass rate of the Turing test for each human\ntest metric across 1 to 5 scale. Moreover, we com-\npute the Pearson correlation coefficient between\nthe pass rate and each metric. In story genera-\ntion (WritingPrompts), the coefficients for Fluency,\nInformativeness, and Relevance are 96.63, 97.93,\n96.44, respectively. While, in text summarization\n(GigaWord), the coefficients for Fluency, Informa-\ntiveness, and Accuracy are 96.08, 97.67, 98.38,\nrespectively. From these analysis results, we can\nconclude that Informativeness is more important\nfor story generation, while Accuracy is more influ-\nential in text summarization. Besides, we compute\nthe text similarity between the generated texts from\ndifferent PLMs, which is shown in Appendix F.\n4 Discussion\nBased on the above four ability tests, we intend to\nprovide a guideline for helping researchers choose,\napply, interpret and design PLMs for NLP tasks.\nIn section 3.3, we observe that the improvement\nin memory ability is likely to be helpful for the\nperformance of comprehension ability. Hence, de-\nsigning PLMs with special objectives like bidirec-\ntional language modeling in BERT and strategies\nlike larger training batches in RoBERTa for larger\nmemory capacity will further benefit PLMs in the\ndownstream comprehension tasks. Besides, when\napplying PLMs to downstream tasks, the similarity\nof data distribution between pretraining and fine-\ntuning has a great impact on PLMs performance.\nPossible solutions such as introducing intermedi-\nate tasks or datasets can alleviate such a discrep-\nancy. Moreover, we further find some limitations\nin PLMs’ comprehension ability, where PLMs are\ngood at simple single-token answer types in QA\nsuch as dates but perform worse in complex phrase\nanswers.\nCompared to comprehension, reasoning in sec-\ntion 3.4 is much more intricate and usually in-\nvolves inferring the semantic relationships among\nmultiple sentences. Therefore, PLMs such as AL-\nBERT trained with sentence-level objectives can\nbe more suitable for conducting reasoning tasks.\nIntuitively, incorporating sentence-level objectives\nduring pretraining will help PLMs learn the corre-\nlation among different sentences. Note that PLMs\nhave better reasoning transferability between sim-\nilar tasks, thus data-scarce reasoning tasks can be\nimproved by first training on data-rich tasks.\nFor composition ability, PLMs with denoising\ntraining objectives perform much better on short\ntext composition, while PLMs with left-to-right\nobjectives or larger model size are more suitable\nfor long text composition. This might be because\n3526\nPLMs with different training objectives can finally\ncapture different ranges of semantic dependency\nbetween tokens and phrases. Moreover, to obtain a\nhigher pass rate of Turing test, different text gener-\nation tasks will be concerned with varying factors,\nsuch as informativeness is much more critical for\nstory generation.\n5 Related Work\nPretrained Language Models . Owing to the\ngreat achievements Transformer (Vaswani et al.,\n2017) has made, the paradigm of pretrained lan-\nguage models (PLMs) is thriving (Radford et al.,\n2019; Devlin et al., 2019; Liu et al., 2019b; Lewis\net al., 2020; Raffel et al., 2020). It is widely rec-\nognized that PLMs can learn massive knowledge\nfrom corpora (Li et al., 2021c), leading to signifi-\ncant progress in various language tasks (Li et al.,\n2021a,b). With such encouraging results in exten-\nsive NLP tasks, it is a non-trivial topic to system-\natically evaluate the abilities of PLMs, which can\nfurther deepen our understanding of PLMs and fa-\ncilitate their application to more fields.\nLanguage Model Evaluation. Many efforts have\nstudied the evaluation of language model perfor-\nmance. Liu et al. (2019a) evaluate BERT (De-\nvlin et al., 2019), GPT (Radford et al., 2018), and\nELMo (Peters et al., 2018) on a variety of linguis-\ntics tasks. Their findings indicate that the features\ngenerated by PLMs are sufficient for good perfor-\nmance on a board set of tasks but fall short on tasks\nrequiring fine-grained linguistic knowledge. Ten-\nney et al. (2019) evaluate similar models on a range\nof sub-sentence linguistic analysis tasks, showing\nthat PLMs encode both syntax and semantics into\nparameters. Zhou et al. (2020) also report that\nPLMs can learn rich knowledge but focus on eval-\nuating the commonsense. However, these studies\nonly look at one dimension of PLMs ability evalua-\ntion. Other work such GLUE (Wang et al., 2019b)\nand CLUE (Liang Xu, 2020) just consider a simple\nmixture of multiple tasks lacking comprehensive\nevaluation. To the best of our knowledge, this is\nthe first work to systematically evaluate PLMs by\ndefining various kinds of language abilities and\nperforming extensive comparison.\n6 Conclusion\nThis paper investigates the general language abil-\nity evaluation of pretrained language models. We\ndesign four kinds of language abilities of PLMs,\nincluding memory, comprehension, reasoning, and\ncomposition, and measure ten widely-used PLMs\nwithin five categories. For each language ability,\nwe select multiple representative tasks to quanti-\ntatively evaluate the performance of PLMs. Our\nexperimental results demonstrate that PLMs with\nvarying objectives and strategies are good at dif-\nferent ability tests. Note that our final predicted\noutputs of PLMs can also be reused as an open re-\nsource for more depth and granularity in analyzing\nPLMs’ language abilities. As a result, it is believed\nthat this study will benefit future work about choos-\ning or designing suitable PLMs for the target NLP\ntasks based on their properties.\nAcknowledgement\nThis work was partially supported by Beijing Natu-\nral Science Foundation under Grant No. 4222027,\nNational Natural Science Foundation of China un-\nder Grant No. 61872369 and 82161148011, Bei-\njing Outstanding Young Scientist Program under\nGrant No. BJJWZYJH012019100020098, and the\nOutstanding Innovative Talents Cultivation Funded\nPrograms 2021 of Renmin University of China. We\nare grateful to Amazon Web Services for providing\nefficient GPU computing resource support and tech-\nnical support for this NLP research project. Xin\nZhao is the corresponding author.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nan automatic metric for MT evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the Workshop on Intrinsic and Extrinsic Evalua-\ntion Measures for Machine Translation and/or Sum-\nmarization@ACL 2005, Ann Arbor, Michigan, USA,\nJune 29, 2005, pages 65–72. Association for Compu-\ntational Linguistics.\nVirginia W Berninger. 1999. Coordinating transcrip-\ntion and text generation in working memory during\ncomposing: Automatic and constructive processes.\nLearning Disability Quarterly, 22(2):99–112.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nKate Cain and Jane Oakhill. 2008. Children’s compre-\nhension problems in oral and written language: A\ncognitive perspective. Guilford Press.\n3527\nRobert Connors. 1997. Composition-rhetoric: Back-\ngrounds, theory, and pedagogy. University of Pitts-\nburgh Pre.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages\n13042–13054.\nA. H. Miller P. Lewis A. Bakhtin Y . Wu F. Petroni,\nT. Rocktäschel and S. Riedel. 2019. Language mod-\nels as knowledge bases? In In: Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 2019.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 889–898. Association for Computational Lin-\nguistics.\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych,\nand Benno Stein. 2018. The argument reasoning\ncomprehension task: Identification and reconstruc-\ntion of implicit warrants. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 1930–1940. Association for\nComputational Linguistics.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural Information\nProcessing Systems 28: Annual Conference on Neu-\nral Information Processing Systems 2015, December\n7-12, 2015, Montreal, Quebec, Canada, pages 1693–\n1701.\nPhilip N Johnson-Laird. 1999. Deductive reasoning.\nAnnual review of psychology, 50(1):109–135.\nAlan S Kaufman and Elizabeth O Lichtenberger. 2005.\nAssessing adolescent and adult intelligence . John\nWiley & Sons.\nPatrick C Kyllonen and Raymond E Christal. 1990. Rea-\nsoning ability is (little more than) working-memory\ncapacity?! Intelligence, 14(4):389–433.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard H. Hovy. 2017. RACE: large-scale read-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2017, Copenhagen, Denmark, September 9-11, 2017,\npages 785–794. Association for Computational Lin-\nguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nJunyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaox-\nuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu,\nWayne Xin Zhao, and Ji-Rong Wen. 2021a. Textbox:\nA unified, modularized, and extensible framework for\ntext generation. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing: System Demon-\nstrations, pages 30–39.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei,\nNicholas Jing Yuan, and Ji-Rong Wen. 2021b. Few-\nshot knowledge graph-to-text generation with pre-\ntrained language models. In Findings of the Associa-\ntion for Computational Linguistics: ACL/IJCNLP\n2021, Online Event, August 1-6, 2021 , volume\nACL/IJCNLP 2021 of Findings of ACL, pages 1558–\n1568. Association for Computational Linguistics.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong\nWen. 2021c. Pretrained language model for text gen-\neration: A survey. In Proceedings of the Thirtieth\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2021, Virtual Event / Montreal, Canada,\n19-27 August 2021, pages 4492–4499. ijcai.org.\nLu Li Hai Hu Chenjie Cao Weitang Liu Junyi Li Yudong\nLi Kai Sun Yechen Xu Yiming Cui Cong Yu Qianqian\nDong Yin Tian Dian Yu Bo Shi Jun Zeng Rongzhao\nWang Weijian Xie Yanting Li Yina Patterson Zuoyu\nTian Yiwen Zhang He Zhou Shaoweihua Liu Qipeng\nZhao Cong Yue Xinrui Zhang Zhengliang Yang Zhen-\nzhong Lan Liang Xu, Xuanwei Zhang. 2020. Clue:\nA chinese language understanding evaluation bench-\nmark. arXiv preprint arXiv:2004.05986.\n3528\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nHugo Liu and Push Singh. 2004. Conceptnet—a practi-\ncal commonsense reasoning tool-kit. BT technology\njournal, 22(4):211–226.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short\nPapers), pages 1073–1094. Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAkira Miyake and Priti Shah. 1999. Models of working\nmemory: Mechanisms of active maintenance and\nexecutive control. Cambridge University Press.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and evaluation framework for deeper under-\nstanding of commonsense stories. arXiv preprint\narXiv:1604.01696.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of NAACL-HLT\n2019: Demonstrations.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227–2237. Association for\nComputational Linguistics.\nJason Phang, Phil Yeres, Jesse Swanson, Haokun Liu,\nIan F. Tenney, Phu Mon Htut, Clara Vania, Alex\nWang, and Samuel R. Bowman. 2020. jiant 2.0:\nA software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan\nDuan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\n2020. Prophetnet: Predicting future n-gram for\nsequence-to-sequence pre-training. In Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing: Findings, EMNLP 2020,\nOnline Event, 16-20 November 2020 , pages 2401–\n2410. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\nfor squad. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 2: Short Papers, pages 784–789. Association\nfor Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nThe Association for Computational Linguistics.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2015, Lisbon, Portugal,\nSeptember 17-21, 2015, pages 379–389. The Associ-\nation for Computational Linguistics.\nMaarten Sap, Vered Shwartz, Antoine Bosselut, Yejin\nChoi, and Dan Roth. 2020. Commonsense reasoning\nfor natural language processing. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics: Tutorial Abstracts, ACL 2020,\nOnline, July 5, 2020, pages 27–33. Association for\nComputational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics - on what language\nmodel pre-training captures. Trans. Assoc. Comput.\nLinguistics, 8:743–758.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. In Proceedings of the 2019 Conference of\n3529\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4149–4158. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al.\n2019. What do you learn from context? probing for\nsentence structure in contextualized word representa-\ntions. arXiv preprint arXiv:1905.06316.\nAlan M Turing. 2009. Computing machinery and in-\ntelligence. In Parsing the turing test, pages 23–65.\nSpringer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nDouglas Walton. 2014. Abductive reasoning. Univer-\nsity of Alabama Press.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019a. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiao-\nnan Li, and Tian Gao. 2019c. Does it make sense?\nand why? A pilot study for sense making and ex-\nplanation. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4020–4026. Association\nfor Computational Linguistics.\nSinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,\nand Hao Ma. 2021. Entailment as few-shot learner.\narXiv preprint arXiv:2104.14690.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754–5764.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. SW AG: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 93–104. As-\nsociation for Computational Linguistics.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n4791–4800. Association for Computational Linguis-\ntics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 1441–1451. Association for Computa-\ntional Linguistics.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 9733–9740. AAAI Press.\nXu Zou, Da Yin, Qingyang Zhong, Hongxia Yang,\nZhilin Yang, and Jie Tang. 2021. Controllable gener-\nation from pre-trained language models via inverse\nprompting. arXiv preprint arXiv:2103.10685.\n3530\nSupplementary Material for ElitePLM\nWe give some experiment-related information\nas supplementary materials. The appendix is orga-\nnized into six sections:\n• Configurations and pretraining setting com-\nparisons for selected models are presented in\nAppendix A;\n• Data statistics of each test are presented in\nAppendix B;\n• Full results for memory tests are presented in\nAppendix C;\n• Full results for comprehension tests are pre-\nsented in Appendix D;\n• Full results for reasoning tests are presented\nin Appendix E; and\n• Full results for composition tests are presented\nin Appendix F.\nA Configurations of Pretrained Language\nModels\nThe selected ten PLMs within five categories and\nthe comparisons of these PLMs in configuration\nand pretraining setting have been shown in Table 7.\nThe effect extent of each factor for PLMs abilities\nin Table 8.\nB Data Statistics\nMemory Tests. The data statistics of LAMA and\nWikipedia of each model are presented in Table 9.\nDue to the differences of each PLM, we drop the\ndata that are not in the vocabulary.\nComprehension Tests. The data statistics of\nGLUE, SuperGLUE, SQuAD and RACE are pre-\nsented in Table 10.\nReasoning Tests. The data statistics for common-\nsense reasoning, deductive reasoning and abductive\nreasoning are presented in Table 11.\nComposition Tests. The data statistics for text\nsummarization, question generation and story gen-\neration are presented in Table 12. For the first three\ndatasets, we truncate the source text considering\nthe input length of PLMs during training. And\nfor WritingPrompts, we reconstruct the original\ndataset and discard examples where text contains\nmore than 512 tokens.\nC Memory Tests\nFull results on LAMA and Wikipedia datasets are\npresented in Table 13.\nD Comprehension Tests\nFull results on SuperGLUE, SQuAD and RACE\nare presented in Table 14 and Table 15.\nE Reasoning Tests\nFull results on CommonsenseQA, ROCStories,\nSW AG, HellaSwag, Sense Making, and ARCT are\npresented in Table 16.\nF Composition Tests\nFor automatic metrics, BLEU- n and ROUGE-n\ncompute the ratios of overlappingn-grams between\ngenerated and real text, while METEOR measures\nword-to-word matches based on WordNet between\ngenerated and real text. For the human test, Flu-\nency evaluates whether the text is well-formed and\nlogical to read; Informativeness measures whether\nthe text contains useful information; Accuracy tests\nwhether the text describes the given content ac-\ncurately; Relevance measures whether the text is\nrelevant to the given context; Overall evaluates the\noverall quality of the text.\nIn the human test, we ramdomly select 500 gen-\nerated texts for each PLM and 500 gold text. There-\nfore, there are 3000 texts totally. The judges are\nall PhD students which do not know about where\neach text comes from. Each text will be scored by\ntwo judges from the above five aspects, and the\nfinal score is the average of the two scores. In the\nTuring test, each text will also be distinguished by\ntwo judges. Only when two judges make the same\ndecisions that the text is generated by human, we\nwill consider the text is true.\nFull results on CNN/Daily-Mail, GigaWord,\nSQuAD, and WritingPrompts are presented in Ta-\nble 17. Turing test results are presented in Table 6.\nWe also show some summaries and stories gener-\nated by different PLMs in Table 19, Table 20, and\nTable 21.\n3531\nType Models\nConfigurations Pretraining Setting\nSize #Parameter Corpus Size\nBidirectional\nBERT base/large 110M/340M BooksCorpus, English Wikipedia 16GB\nRoBERTa base/large 125M/355M BooksCorpus, CC-News,\nWebText, Stories 160GB\nALBERT xlarge/xxlarge 60M/235M BERT Corpus 16GB\nUnidirectional GPT-2 small/medium 117M/345M WebText (removing Wikipedia) 40GB\nHybrid XLNet base/large 110M/340M BooksCorpus, English Wikipedia,\nGiga5, ClueWeb, Common Crawl 158GB\nUniLM base/large 110M/340M BERT Corpus 16GB\nKnowledge-\nEnhanced ERNIE base 114M English Wikipedia, Wikipedia 17GB\nText-to-Text\nT5 base/large 220M/770M Colossal Clean Crawled Corpus 745GB\nBART base/large 140M/400M RoBERTa Corpus 160GB\nProphetNet large 373M RoBERTa Corpus 160GB\nTable 7: Configurations and pretraining setting comparisons for our selected models.\nAbility MA DD MS PO PS\nMemory ⋆⋆ ⋆ ⋆ ⋆⋆⋆ ⋆⋆⋆\nComprehension ⋆⋆ ⋆⋆ ⋆ ⋆⋆ ⋆⋆⋆\nReasoning ⋆ ⋆⋆⋆ ⋆ ⋆⋆⋆ ⋆⋆⋆\nComposition ⋆ ⋆⋆⋆ ⋆⋆⋆ ⋆⋆⋆ ⋆\nTable 8: The impact extent of each factor for four language abilities of PLMs. MA, DD, MS, PO, and PS are short\nfor model architecture, data distribution, model size, pretraining objective, and pretraining strategy, respectively\nG-RE T-REx ConceptNet SQuAD Wikipedia\n#Origin 6,106 34,014 14,878 305 100,000\n#Relation 3 41 16 - -\nBERT / UniLM 5,527 34,014 11,658 305 85,836\nRoBERTa 4,618 29,500 12,505 286 85,862\nALBERT 5,469 33,636 12,389 291 86,533\nERNIE 1,900 9,071 11,649 173 -\nBART 4,618 29,500 12,505 286 85,862\nT5 4,256 25,850 10,905 230 78,069\nGPT-2 4,618 29,500 7,477 196 1,184\nXLNet 5,202 32,293 12,080 279 85,228\nProphetNet 5,527 34,014 12,506 305 87,516\nThe Predicted Outputs The predicted token of “[MASK]” in each template.\nTable 9: Statistics of datasets in memory tests, including LAMA and Wikipedia. #Origin and #Relation denote\nthe number of examples and relations in original dataset, and the number of each model denotes the number of\nexamples after selected. The predicted outputs is the intermediate result resources we provide.\n3532\nCorpus #Train #Valid #Test The Predicted Outputs\nGLUE\nCoLA 8,551 1,043 1,063 The predicted binary class whether a sentence is grammatical.\nSST-2 67,349 872 1,821 The predicted sentiment (positive/negative) of a sentence.\nMRPC 3,668 408 1,725 The predicted binary class whether two sentences are\nQQP 363,846 40,430 390,965 semantically equivalent.\nSTS-B 5,749 1,500 1,379 The predicted similarity score (1-5) of two sentences.\nMNLI\n-M. 392,702 9,815 9,796 The predicted relation (entailment/contradiction/neutral)\nMNLI\n-MM. 9,832 9,847 between two sentences.\nQNLI 104,743 5,463 5,463\nRTE 2,490 277 3,000 The predicted relation (entailment or not) between two sentences.\nWNLI 635 71 146\nSuper\nGLUE\nBoolQ 9,427 3,270 3,245 The predicted answer (yes/no) to the passage-based question.\nCB 250 57 250 The predicted relation (entailment/contradiction/neutral)\nbetween two sentences.\nCOPA 400 100 500 The predicted cause or effect of the premise from two choices.\nMultiRC 5,100 953 1,800 The predicted answer choice to the passage-based question.\nWic 6,000 638 1,400 The predicted binary class whether a word is used with the same\nsense in two sentences .\nWNLI 635 71 146 The predicted relation (entailment or not) between two sentences.\nWSC 554 104 146 The predicted noun phrase referrent of the pronoun from among\nthe provided choices.\nSQuAD v1.1 88,567 10,790 - The predicted answer span to the passage-based question.v2.0 131,924 12,165 -\nRACE\nall 25,137 1,389 1,407\nThe predicted answer choice to the passage-based question.\n87,866 4,887 4,934\nmiddle 6,409 368 362\n25,421 1,436 1,436\nhigh 18,728 1,021 1,045\n62,445 3,451 3,498\nTable 10: Statistics of datasets in comprehension tests including GLUE, SuperGLUE, SQuAD and RACE. #Train,\n#Valid and #Test denote the number of instances in train, valid and test set, respectively (the same as below).\nMNLI-M. and MNLI-MM. denote MNLI-match and MNLI-mismatch, respectively. SQuAD doesn’t have test set,\nand we utilize the valid set as the test set. The predicted outputs is the intermediate result resources we provide.\nReasoning Task Corpus #Train #Valid #Test The Predicted Outputs\nCom.sense CQA 9,741 1,221 1,140 The predicted answer choice to a commonsense question.\nDeductive\nROCS. 1,257 314 1,571 The predicted ending choice based on the context.\nSW AG 73,546 20,006 20,005 The predicted answer choice based the grounded situation.\nHellaS. 39,905 10,042 10,003\nSM-A 10,000 1,000 1,000 The predicted valid sentence between two sentences.\nAbductive SM-B 10,000 1,000 1,000 The predicted reason choice why the sentence is invalid.\nARCT 1,210 316 444 The predicted warrant choice that justifies reason and claim.\nTable 11: Statistics of datasets in reasoning tests, including commonsense reasoning, deductive reasoning and\nabductive reasoning. CQA is short for CommonsenseQA. SM-A and SM-B denote the Task A and Task B of Sense\nMaking, respectively. The Predicted outputs is the intermediate result resources we provide.\n3533\nTask Corpus #Train #Valid #Test #Input #Output The Predicted Outputs\nTS\nCNN/\nDailyMail 287,113 13,368 11,490 822.3 57.9 The generated summary given a news.\nGigaword 3,803,957 189,651 1,951 33.7 8.7 The generated headline given a paragraph and\ncorresponding Turing test and aspect scores.\nQG SQuAD 75,722 10,570 11,877 149.4 11.5 The generated question given a passage and\ncorresponding answer.\nSG Writing\nPrompts 67,765 3,952 3,784 30.2 281.2 The generated story given a prompt and\ncorresponding Turing test and aspect scores.\nTable 12: Statistics of datasets in composition tests, including text summarization (TG), question generation (QG)\nand story generation (SG). #Input and #Output denote the average number of tokens in the input text and output text.\nThe Predicted outputs is the intermediate results and human evaluation resources we provide.\nModels Vocab Size LAMA-G LAMA-T LAMA-C LAMA-S Wikipedia Average\nBERTBASE 28996 10.3 27.5 15.3 12.8 66.8 41.6\nBERTLARGE 28996 11.0 29.2 19.1 17.0 70.9 45.0\nRoBERTaBASE 50265 7.5 19.9 17.9 13.3 66.9 40.8\nRoBERTaLARGE 50265 7.1 23.9 21.6 21.0 71.1 44.8\nALBERTXLARGE 30000 2.9 19.6 16.8 14.4 64.3 38.9\nALBERTXXLARGE 30000 3.3 21.0 20.0 20.6 63.9 40.1\nGPT-2SMALL 50257 1.3 6.8 4.0 3.0 36.0 19.9\nGPT-2MEDIUM 50257 3.9 12.0 6.4 5.6 42.7 24.8\nXLNetBASE 32000 0.0 0.0 2.8 0.0 64.6 32.7\nXLNetLARGE 32000 0.0 0.0 5.5 0.4 68.7 35.1\nUniLMBASE 28996 8.5 27.6 15.4 11.8 66.9 41.4\nUniLMLARGE 28996 9.6 28.4 18.3 17.4 71.5 46.4\nERNIEBASE 28996 1.3 13.4 13.0 8.1 - -\nT5BASE 32100 5.5 20.0 13.2 9.6 60.5 36.3\nT5LARGE 32100 4.0 21.7 17.1 11.7 65.0 39.3\nBARTBASE 50295 5.7 11.7 9.5 4.2 47.9 27.8\nBARTLARGE 50295 9.4 15.8 7.7 3.1 47.8 28.4\nProphetNetLARGE 30522 0.1 1.1 0.3 0.7 31.3 15.9\nTable 13: Memory tests results on LAMA and Wikipedia datasets (test set). We report accuracy score for each\ndataset. Average is computed by averaging the scores of LAMA and Wikipedia (the score of LAMA is averaged\namong four dataset first). LAMA-G, LAMA-T, LAMA-C and LAMA-S denote the LAMA corpus Google-RE,\nT-REx, ConceptNet and SQuAD, respectively.\n3534\nModel WSC CB RTE COPA Wic BoolQ MultiRC Avg\nAcc. F1/Acc. Acc. Acc. Acc. Acc. F1/EM\nBERTBASE 60.6 78.7/80.4 66.4 65.0 69.9 74.6 68.1/16.9 65.5\nBERTLARGE 63.5 89.0/92.9 70.1 73.0 72.7 75.6 69.4/22.6 70.3\nRoBERTaBASE 71.1 89.1/91.1 75.1 78.0 67.2 81.1 72.6/31.9 73.6\nRoBERTaLARGE 75.0 95.0/96.4 88.2 84.0 72.7 85.4 81.7/47.2 80.8\nALBERTXLARGE 63.5 81.1/85.7 62.5 75.0 66.5 62.2 63.6/12.4 64.4\nALBERTXXLARGE 64.4 87.6/92.9 70.4 91.0 74.3 62.2 85.1/54.0 74.6\nGPT-2SMALL 54.8 64.0/76.8 62.1 62.0 64.1 68.2 67.3/19.5 60.7\nGPT-2MEDIUM 61.5 84.4/82.1 63.6 63.0 67.2 73.9 71.5/29.2 66.1\nXLNetBASE 64.4 91.0/91.1 59.9 65.0 67.9 76.9 72.5/29.6 68.0\nXLNetLARGE 65.3 87.6/92.9 88.5 82.0 69.7 84.7 79.0/41.6 77.3\nUniLMBASE 63.5 74.7/82.1 60.3 67.0 68.5 73.3 67.9/20.5 65.0\nUniLMLARGE 65.4 86.5/87.5 70.9 76.0 72.3 82.3 75.7/36.3 72.8\nERNIEBASE 65.4 81.6/82.1 68.8 64.0 70.8 74.4 68.7/21.3 67.2\nT5BASE 79.8 86.2/94.0 80.1 71.2 68.3 81.4 79.7/43.1 76.0\nT5LARGE 84.6 91.6/94.8 87.2 83.4 69.3 85.4 83.3/50.7 81.4\nBARTBASE 64.4 86.6/85.7 69.5 70.0 65.7 75.7 74.2/31.7 69.2\nBARTLARGE 65.4 97.4/96.4 83.5 86.0 70.4 85.1 82.9/50.6 79.2\nProphetNetLARGE 63.5 94.7 /92.9 51.3 61.0 60.7 67.4 64.7/17.2 62.7\nTable 14: Comprehension tests results on SuperGLUE (valid set). Avg column is computed by averaging the scores\nof tasks to its left (the scores for CB and MultiRC are first averaged).\nModels\nSQuAD v1.1 SQuAD v2.0 RACE\nEM F1 EM F1 RACE RACE-M RACE-H\nBERTBASE 80.8 88.5 72.8 76.0 65.0 71.7 62.3\nBERTLARGE 84.1 90.9 78.7 81.9 72.0 76.6 70.1\nRoBERTaBASE 86.1 92.3 80.3 83.4 72.8 72.6 26.6\nRoBERTaLARGE 88.9 94.6 86.5 89.4 83.2 86.5 81.3\nALBERTXLARGE 86.1 92.5 83.1 86.1 78.1 76.7 79.8\nALBERTXXLARGE 88.3 94.1 85.1 88.1 87.4 85.9 87.1\nGPT-2SMALL 63.6 75.1 57.1 61.5 61.2 62.9 58.2\nGPT-2MEDIUM 70.3 80.8 61.5 66.0 62.2 65.0 61.4\nXLNetBASE 12.8 14.7 78.5 81.3 71.3 72.8 67.5\nXLNetLARGE 89.7 95.1 87.9 90.6 85.4 88.6 84.0\nUniLMBASE 82.8 89.9 74.9 78.0 59.0 64.1 50.3\nUniLMLARGE 86.5 92.7 80.5 83.4 70.3 70.0 66.4\nERNIEBASE - - - - - 67.8 -\nT5BASE 85.4 92.1 77.6 81.3 70.6 74.4 68.4\nT5LARGE 86.7 93.8 - - 80.4 82.6 77.8\nBARTBASE 84.6 91.0 76.0 79.2 70.1 72.4 63.2\nBARTLARGE 88.8 94.6 86.1 89.2 82.2 82.5 79.6\nProphetNetLARGE - - - - - 74.1 -\nTable 15: Comprehension tests results on SQuAD and RACE (test set).\n3535\nModel CQA ROCStories SW AG HellaSwag SM-A SM-B ARCT\nBERTBASE 53.0 88.1 81.6 40.5 87.3 80.1 65.1\nBERTLARGE 55.9 90.2 86.3 47.3 89.4 85.8 71.2\nRoBERTaBASE 72.1 93.3 82.6 61.0 89.3 87.5 46.1\nRoBERTaLARGE 72.2 97.4 89.9 85.2 93.0 92.3 57.9\nALBERTXLARGE 66.2 90.4 84.6 75.9 87.9 89.4 56.1\nALBERTXXLARGE 80.0 97.1 90.7 90.1 92.5 92.3 79.5\nGPT-2SMALL 47.8 58.8 48.1 39.9 84.2 74.7 66.0\nGPT-2MEDIUM 60.8 59.9 79.7 60.4 88.7 73.4 66.7\nXLNetBASE 53.8 92.0 80.4 55.1 81.6 85.4 80.2\nXLNetLARGE 62.9 93.8 86.8 79.7 83.7 88.7 83.1\nUniLMBASE 47.6 80.6 77.0 36.3 86.2 83.6 48.4\nUniLMLARGE 62.3 86.9 83.1 46.7 89.3 86.4 72.3\nERNIEBASE 54.1 84.7 - - 88.7 - 73.7\nT5BASE 61.9 88.2 65.8 55.2 89.2 82.9 63.3\nT5LARGE 69.8 91.4 73.7 79.1 92.7 88.2 69.4\nBARTBASE 61.0 88.9 81.2 53.4 72.0 67.9 71.8\nBARTLARGE 75.8 91.7 87.9 76.6 82.9 67.9 84.2\nProphetNetLARGE 21.3 82.2 70.1 26.4 85.5 78.0 65.5\nTable 16: Reasoning tests results on seven datasets (test set). We report accuracy score for each dataset. CQA is\nshort for CommonsenseQA. SM-A and SM-B denote the Task A and Task B of Sense Making, respectively.\nModels\nCNN-DailyMail GigaWord SQuAD WritingPrompts\nR-1 R-2 R-L R-1 R-2 R-L B-4 R-L ME B-4 R-L ME\nGPT-2SMALL 24.60 7.21 21.06 25.25 9.03 23.20 5.13 14.83 21.06 11.58 3.80 8.18\nGPT-2MEDIUM 22.95 5.99 22.08 23.72 8.12 21.56 8.48 18.82 26.77 14.47 3.23 7.29\nUniLMBASE 17.83 0.11 5.50 16.64 6.11 15.12 4.47 17.65 20.30 27.71 2.35 5.47\nUniLMLARGE 43.44 20.21 40.51 38.45 19.45 35.75 4.42 17.43 20.13 26.88 1.84 5.01\nT5BASE 42.05 20.34 39.40 33.13 15.60 30.18 11.18 21.82 29.93 6.04 4.61 9.81\nT5LARGE 42.50 20.68 39.75 34.75 16.26 31.49 11.19 22.35 30.53 8.61 4.19 9.51\nBARTBASE 36.36 20.87 33.32 38.65 19.43 35.82 14.44 24.11 36.92 11.91 3.57 7.69\nBARTLARGE 44.16 21.28 40.90 39.41 20.21 36.42 15.87 25.47 38.42 14.72 3.14 7.08\nProphetNetLARGE 44.20 21.17 41.30 39.51 20.42 36.69 14.20 23.97 35.99 19.31 2.59 7.19\nTable 17: Composition tests results on four datasets. R-1, R-2, R-L are short for ROUGE-1, ROUGE-2, ROUGE-L\nrespectively. B-4 and MT denote BLEU-4 and METEOR, respectively.\nModels TT (%) Fluency Informativeness Accuracy Coherence Overall\nGPT-2MEDIUM 45.7 3.42 3.17 3.20 3.23 5.87\nUniLMLARGE 1.2 1.32 1.88 2.03 1.71 2.74\nT5LARGE 34.4 3.01 2.80 3.09 2.87 5.18\nBARTLARGE 45.2 3.37 3.16 3.39 3.22 5.96\nProphetNetLARGE 29.6 2.95 2.91 3.10 2.89 5.18\nGold 71.3 3.79 4.07 3.87 3.80 7.37\nTable 18: Turing test and human scores on the test set of WritingPrompts. TT is short for Turing Test and represents\nthe percentage of makers who believe the text is written by human. Coherence evaluates how content is coherent\nconsidering both intra- and inter-sentence correlation of a paragraph. Rated from 1 to 5.\n3536\nPassage japan ’s nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join\nforces in supercomputer sales .\nGPT-2MEDIUM unk computer to jointly sell supercomputers\nUniLMLARGE nec [UNK] to join forces in supercomputer sales\nT5LARGE nc and unk computer to join forces in supercomputer sales\nBARTLARGE nec and unk computer to join forces in supercomputer sales\nProphetNetLARGE nec unk computer to join forces in supercomputer sales\nGold nec UNK in computer sales tie-up\nPassage the sri lankan government on wednesday announced the closure of government schools with immediate\neffect as a military campaign against tamil separatists escalated in the north of the country .\nGPT-2MEDIUM closed as military campaign escalates\nUniLMLARGE sri lankan gov t shuts down schools in north\nT5LARGE sri lanka closes schools as fighting escalates\nBARTLARGE sri lanka closes government schools\nProphetNetLARGE sri lanka closes schools as military escalates\nGold sri lanka closes schools as war escalates\nTable 19: Test set examples of GigaWord dataset for each model.\n3537\nPrompts [ WP ] You have a very shitty type of precognition .\nGPT-2MEDIUM <newline>“ I’m sorry, ” I said. “ I don’t know what you’re talking about. ” I looked around the\nroom, trying to find the source of the noise. “ What do you mean? ” I asked. “ It’s not like I’ve never\nheard it before. ” He looked at me, confused. “ No, ” he said, “ it’s never happened to me before.\nI ’d be surprised if it ever did. ” “ What’s that supposed to mean? You’re telling me that I can’t\nremember anything before? ” “ No. ” he replied. “ That’s exactly what it means. You’ve always been\nprecognitions, and you ’ll never be able to remember anything. ”\nUniLMLARGE i’m not sure what i’ve done wrong. i’d rather not be able to do it. i don’t know what i have done wrong,\ni don’t know what it has done. i have been told that i have n’ve been wrong. <newline ><new york.\n<york ><yoline >i’ll never forget that day. «new >«line >the day i was born. the day that i was raised.\nthe one that i never forget. the time that i first met her. the moment that i met her, the time when i first\nsaw her. <nowline >my life is a blur. the blur is the blur. i am not sure if i have ever been wrong, or\nif i had ever been right. i ca n’re sure if it was the blur or the blur, but i do know it was. <tim. <the\nblur <new newline. <i am a blur, the blur and the blur are the same. the same, the same is the same\nthing. the difference is the difference between the two. the two are the difference. the differences are\nthe differences. the different things. the things... the changes. the change. the changing. the thing.. is\nthe change, the change is the thing that is the different. the new thing. <.. <this is the story of a man\nwho was born in the blur of the two, the one where i was n’s. the other where i am. the first time i met.\nthe second time i saw her, and the first where i never forgot. the story is the one in the middle of the\nblur where i met the woman i loved. the woman who i loved, the woman that i loved the woman. the\nman who i love the woman with the most. the person who i hate the most, the person that i hate most.\n<cline >* * * <new story ><nbsp >i am the blur ; i am i. <pline »<bsp. i was a blur ; <new line ><i was\nthe one who was the first to be born. <first time. <np ><do nbs. <do ibs. i did n’i do it, i did not know\nit had happened. <line i woman i <i i i ve, the <blur ve\nT5LARGE <newline>It’s been a long time since I’ve been able to use my powers. I haven’t had a chance to use\nthem in a while, but I do have a few things I can do to help. The first thing I do is go to the bathroom,\nand I have to wash my hands. It is a pain, I know it. But I do it anyways, because I have the ability to\nsee things. And I can see them. They can see me in the mirror, they can see my eyes, their eyes. My\nhands, my feet, the way they move, how they feel. How I feel, what they do to me. What I do to them?\nHow do they know I am? I can hear them in my ears, a scream, or a cry. A sigh of relief, as I look at\nthe mirror and see them in front of me, like a mirror. You’re gon na see me? ”\nBARTLARGE I’ve always been a bit of a weirdo. <newline>I’m not sure why, but I always knew I was going to die at\nsome point in my life. I don’t know if it’s because I was born in the wrong year, or if it was because my\nparents died when I was very young. I just knew that I would die. I’ve never been able to explain it\nto anyone, but it was always there in the back of my mind. It’s not like I could do anything about it.\nI couldn’t even do anything to stop it. It was just always there. I never knew what it was, or what it\nwould do to me. I didn’t want to know. I wanted it to be over. But it never was. It just kept getting\nworse and worse. I can’t explain it, I just know that I ’ll die.\nProphetNetLARGE it’s been a long time since i’ve had a precognition, but i don’t think i’ll ever be able to get rid of it.\n<newline >it started when i was a kid. i was told that i had a very shitty type of pre - lucid dreaming. i\nhad no idea what it was until i was in my late teens. it was like i could see the future in front of me. i\nknew exactly what i was going to do when i saw the future. i could feel it in my bones. i can feel it. i\nknow exactly what’s going to happen when i look at the future, and i know it will be the best thing\nthat’s ever happened to me. the future is going to be better than i ever thought it would be. i’m not\ngoing to let it happen again.\nGold Just looking at the guy I already knew it . He was in for a seven on the Bristol scale and it was only\nhours away . <newline><newline>Looking at his carefree face from below as he was slowly transported\ntowards me on the escalator , I wondered , not for the first time , if I should warn him . But as always I\nasked myself : How could I tell him ? How could I tell anyone ? <newline><newline>Tell people that I\ncould see the future . That I could see their fate ? That I could feel the forces that were at work , often\nwithout their knowledge and beyond their control , inside their own bodies ? That I could see shape of\nthings to come ? That I was a coproscognitive ? <newline><newline>Nobody would believe me . Not\nuntil after a few predictions had been proven to be correct at least and few humored me for long enough\nto realize the truth of my predictions . <newline><newline>And this guy did n’t have a few predictions\nto confirm the authenticy of my gift . He needed to know and believe me about the very next event I\ncould foresee and there was no way to convince him in time . <newline><newline>And thus I stayed\nsilent again as the escalator carried us past one another leaving him to his fate . I was going up and\nhe was going down . <newline><newline>I could only hope for him that he would be close to the\nright place two hours and thirteen minutes from now , because I could tell it would be a fast and quite\nexplosive one and without my word he would not have much advanced warning telling him to run for\nsalvation . <newline><newline>I put him out of my mind I had my own future to worry about and I did\nn’t need my gift to tell me about it . I started looking for the signs pointing the way to closest lavatory .\nTable 20: Test set examples of WritingPrompts dataset for each model.\n3538\nPrompts [ WP ] “ Before we allow humankind to live among us in the stars , tell us the faults of your kind . ”\nGPT-2MEDIUM <newline>“ I’m sorry, sir, but I don’t think we’re ready for this. ” I said. “ We’ve been here for\nthousands of years, and we’ve never seen anything like this before. ” He looked at me with a look of\nhorror on his face. “ What do you mean? ” I asked. He looked down at his hands, and then back up at\nme. “ It’s like this, ” he said, “ You can’t see it, but it’s there. It ’ll be here for a long time, and it ’ll kill\nus all. ” <newline>I looked at him, and he looked back at me, and both of us looked at the stars.\nUniLMLARGE ‘ ‘ we allow humanskind to be among us, tell them the faults.’<newline ><new new >‘ ‘ humanskind,” i\nsaid, ‘ ‘ you are the one who has been chosen to be our leader.’‘ ‘ humankind, you are our leader,’‘ i said.\n<new york. <york ><yoline >‘’humanskind.’* <newrk >* <yo >«new ><>humanskind : <new name\n><nowline >humans : <now name >humans. <now names ><the name of the universe. <the names of\nthe stars. «line >* humanskind * <now named ><first name >the universe, <new names >the stars and\nthe stars <new stars. the names <new planets. <first names >* * humans * <firstline >the name <new\nspecies ><humanline »<humans ><last name >* humankind *. <last names >humans * * <lastline\n>humankind. <name »* humanity * <name <><* humans. * <* human * «»humans, <now known as\nhumans. the name. <.. <* humanity. <human name >... * * * humanity <new humans >*.. humans *. *.\nhumans.. ’. <line. <humans.’s.. human.’the humans. they were the humans, the humans of the galaxy.\n<: «<humans : humans. humans. humans, humans. humankind <new galaxy ><: / / www. reddit. com /\nr / writingprompts / comments / 2jclq / comments _ 2jflq _ comments / 1xxxfxxgxxcxxbxxdxxkxxqxx\n<new _ ><_ _ <new i am humankind : the humankind of the galactic system. <_ «_ >i am the human\nrace. <tv ><tline >i was the human. <pline >it was a long time since i was human. i was a human.. i\nam a human race,..kind <. <’humans human «race humans <* <human of * <the «* i humans. new..\nT5LARGE Before we allow humankind to live amongst the stars, tell us the faults of your kind. ” newline>I don’t\nknow, I’m not a scientist, but I do have a degree in astronomy, and I do know a thing or two about\nscience. I know that a lot of people think that science is a good thing, that it’s a great thing. But, if you\nthink about it, you’re a fucking shithole. You’ve got a bunch of crazies, all of them. So, what do you\nthink?? Do you know what? I mean, they ’ll tell you. And, of course, we ’d like to know what you\nthink of us.\nBARTLARGE “ Before we allow humankind to live among us in the stars, tell us the faults of your kind. ” <new-\nline><newlines>“ Well, first of all, they aren’t very smart. They don’t know how to read. They’re not\nvery good at math. They haven’t learned how to write yet. They are also very lazy. They spend most of\ntheir time staring at their screens. They can’t even get up to go to the bathroom. They just sit there and\nstare at the screen. They also have a tendency to stare at their phones for hours at a time. I’m not sure\nwhy they do that, but I guess it’s because they’re bored. ”\nProphetNetLARGE ‘ ‘ before we allow humankind to live among us in the stars, tell us the faults of our kind.” <newline >‘\n‘ i’m sorry, sir, but we don’t have the technology to do that. we’re too afraid of the consequences of our\nactions, and we’ve spent too much time trying to find a way to stop them.’cause they’re just too stupid\nto do anything about it. we have to do something about it, or we’ll never be able to get out of here. we\nneed to find some way to get them out of there, and if they do, then we’d have to go back to earth and\nstart all over again. and if that’s the case, then i’d like to thank you for your time, and i hope to see you\nagain soon,”\nGold Tell us your faults ? Really ? This was the question - the shibboleth - that unlocked the cosmos ?\n<newline><newline>The Masters could have picked a scientist to answer but they feared she might\nmask ignorance . They could have picked from our global leaders bit they feared that they would mask\ndeceit . They could have picked a holy man but feared he would mask violence , oppression , hate ,\nintolerance ... the list of disqualifying sins was almost too long to enumerate . <newline><newline>So\nthey picked Josh Thornton , a 45 year old MBA in human resources . <newline><newline>“ Our\ngreatest weakness ? Well , I think we work a little too hard and , as a race , we might be a bit of a\nperfectionist .\nTable 21: Test set examples of WritingPrompts dataset for each model.\n3539",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.7763415575027466
    },
    {
      "name": "Computer science",
      "score": 0.599998414516449
    },
    {
      "name": "Computational linguistics",
      "score": 0.5851738452911377
    },
    {
      "name": "Natural language processing",
      "score": 0.4706662893295288
    },
    {
      "name": "Linguistics",
      "score": 0.4606840908527374
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4550245404243469
    },
    {
      "name": "Cognitive science",
      "score": 0.43640270829200745
    },
    {
      "name": "Language model",
      "score": 0.4276791512966156
    },
    {
      "name": "Philosophy",
      "score": 0.2888101637363434
    },
    {
      "name": "Psychology",
      "score": 0.24058237671852112
    },
    {
      "name": "Biology",
      "score": 0.05722007155418396
    },
    {
      "name": "Ecology",
      "score": 0.05228281021118164
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210096250",
      "name": "Beijing Institute of Big Data Research",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    }
  ],
  "cited_by": 4
}