{
    "title": "LightVA: Lightweight Visual Analytics With LLM Agent-Based Task Planning and Execution",
    "url": "https://openalex.org/W4404371576",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2134751210",
            "name": "Yuheng Zhao",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2097441002",
            "name": "Junjie Wang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5114172227",
            "name": "Linbing Xiang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2109300607",
            "name": "Xiao-wen Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A3195601143",
            "name": "Zifei Guo",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2033870557",
            "name": "Cagatay Turkay",
            "affiliations": [
                "University of Warwick"
            ]
        },
        {
            "id": "https://openalex.org/A2023856972",
            "name": "Yu Zhang",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2117897572",
            "name": "Siming Chen",
            "affiliations": [
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2141115795",
        "https://openalex.org/W4298326929",
        "https://openalex.org/W2488113179",
        "https://openalex.org/W4298326528",
        "https://openalex.org/W2965346190",
        "https://openalex.org/W2969478830",
        "https://openalex.org/W2735080104",
        "https://openalex.org/W4389520747",
        "https://openalex.org/W6860393137",
        "https://openalex.org/W6853847959",
        "https://openalex.org/W4389524334",
        "https://openalex.org/W4385565383",
        "https://openalex.org/W4389524090",
        "https://openalex.org/W6855917509",
        "https://openalex.org/W6853844158",
        "https://openalex.org/W4392405493",
        "https://openalex.org/W3183318857",
        "https://openalex.org/W3177093812",
        "https://openalex.org/W4381950923",
        "https://openalex.org/W1961845056",
        "https://openalex.org/W2428096022",
        "https://openalex.org/W2964101465",
        "https://openalex.org/W2886887279",
        "https://openalex.org/W3169230937",
        "https://openalex.org/W3080485811",
        "https://openalex.org/W4386825362",
        "https://openalex.org/W3203626883",
        "https://openalex.org/W6810623102",
        "https://openalex.org/W2753171348",
        "https://openalex.org/W3189585406",
        "https://openalex.org/W3030168100",
        "https://openalex.org/W2888611489",
        "https://openalex.org/W4323022566",
        "https://openalex.org/W3187846328",
        "https://openalex.org/W4380998028",
        "https://openalex.org/W4226018502",
        "https://openalex.org/W3031617143",
        "https://openalex.org/W1988772517",
        "https://openalex.org/W2795915595",
        "https://openalex.org/W4237375617",
        "https://openalex.org/W1991207010",
        "https://openalex.org/W2151122362",
        "https://openalex.org/W6802962953",
        "https://openalex.org/W4302276022",
        "https://openalex.org/W4392026410",
        "https://openalex.org/W6861040085",
        "https://openalex.org/W4388145405",
        "https://openalex.org/W6852607646",
        "https://openalex.org/W4389523621",
        "https://openalex.org/W4393161130",
        "https://openalex.org/W6809646742",
        "https://openalex.org/W6853465110",
        "https://openalex.org/W4393160302",
        "https://openalex.org/W3203321135",
        "https://openalex.org/W4391614548",
        "https://openalex.org/W2106281550",
        "https://openalex.org/W4226518299",
        "https://openalex.org/W2946535156",
        "https://openalex.org/W2516678343",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6850886341",
        "https://openalex.org/W4389519928",
        "https://openalex.org/W4389523830",
        "https://openalex.org/W6855330075",
        "https://openalex.org/W4402401751",
        "https://openalex.org/W4402402086",
        "https://openalex.org/W6857691370",
        "https://openalex.org/W4396828482",
        "https://openalex.org/W4396832280",
        "https://openalex.org/W2961269636",
        "https://openalex.org/W2982550137"
    ],
    "abstract": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.",
    "full_text": "arXiv:2411.05651v2  [cs.HC]  21 Jun 2025\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1\nLightV A: Lightweight Visual Analytics with LLM\nAgent-Based Task Planning and Execution\nYuheng Zhao, Junjie Wang, Linbin Xiang, Xiaowen Zhang, Zifei Guo,\nCagatay Turkay, Yu Zhang and Siming Chen\nAbstract—Visual analytics (V A) requires analysts to iteratively\npropose analysis tasks based on observations and execute tasks\nby creating visualizations and interactive exploration to gain\ninsights. This process demands skills in programming, data pro-\ncessing, and visualization tools, highlighting the need for a more\nintelligent, streamlined V A approach. Large language models\n(LLMs) have recently been developed as agents to handle various\ntasks with dynamic planning and tool-using capabilities, offering\nthe potential to enhance the efficiency and versatility of V A. We\npropose LightV A, a lightweight V A framework that supports\ntask decomposition, data analysis, and interactive exploration\nthrough human-agent collaboration. Our method is designed to\nhelp users progressively translate high-level analytical goals into\nlow-level tasks, producing visualizations and deriving insights.\nSpecifically, we introduce an LLM agent-based task planning\nand execution strategy, employing a recursive process involving\na planner, executor, and controller. The planner is responsible\nfor recommending and decomposing tasks, the executor handles\ntask execution, including data analysis, visualization generation\nand multi-view composition, and the controller coordinates the\ninteraction between the planner and executor. Building on the\nframework, we develop a system with a hybrid user interface\nthat includes a task flow diagram for monitoring and managing\nthe task planning process, a visualization panel for interactive\ndata exploration, and a chat view for guiding the model through\nnatural language instructions. We examine the effectiveness of\nour method through a usage scenario and an expert study.\nIndex Terms—Visual Analytics, Task Planning, Large Lan-\nguage Model Agent, Mixed-Initiative Interaction\nI. I NTRODUCTION\nVisual analytics (V A) deciphers complex datasets with data\nmining and interactive visualizations [1], [2]. However, build-\ning and using a V A system can be a costly endeavor that\nencompasses several main stages: goal understanding, task\ndecomposition, data modeling, and visualization creation to\ndiscover insights. A key challenge is that this process is\niterative, requiring continual refinement based on evolving\nneeds [3]. Different tasks necessitate various data analysis\nand visualization methods to form a V A system. While using\nthe system, tasks may evolve based on the insights gained,\nnecessitating ongoing iterations until the analytical goals are\nachieved [4]. Consider a scenario where the goal is to identify\nhigh-risk events from social media data. Users may first\nneed an overview from different perspectives, such as the\nYuheng Zhao, Junjie Wang, Linbin Xiang, Xiaowen Zhang, Zifei Guo,\nSiming Chen are with School of Data Science, Fudan University. E-mail:\n{yuhengzhao, simingchen}@fudan.edu.cn. Siming Chen is the corresponding\nauthor.\nCagatay Turkay is with the Centre for Interdisciplinary Methodologies,\nUniversity of Warwick. E-mail: Cagatay.Turkay@warwick.ac.uk.\nYu Zhang is with Department of Computer Science, University of Oxford.\nE-mail: yuzhang94@outlook.com.\nManuscript received April 19, 2005; revised August 26, 2015.\ndistribution of keywords over time or changes in sentiment for\nrisk analysis. If outliers are identified, users may need further\ndetails, such as spatial distribution or entity relationships.\nIn this process, the task space is broad and fluid, requiring\nefficient task planning and method implementation.\nRecent research focuses on data-driven or natural language-\nbased visual data exploration, with an emphasis on automatic\nvisualization generation [5], [6] or insight mining [7], [8].\nLarge Language Models (LLMs) present a potential for data\nanalysis, supporting dynamic task planning and lower devel-\nopment costs across various scenarios. The reasoning abilities\nthat enable autonomous planning and execution of analytical\ntasks [9]–[11], while code generation capability support creat-\ning insightful visualizations efficiently [12]–[14]. Additionally,\ntheir broad knowledge base makes LLMs versatile tools ca-\npable of adapting to diverse data analysis contexts [15], [16].\nLEV A [17] integrates LLMs in V A systems to recommend\ninsights for a given task but still cannot generate visualization\nand data modeling methods adapted to tasks. There is a\nlack of approaches supporting task planning, V A methods\nimplementation, and interactive analysis with human-agent\ncollaboration.\nThis paper introduces LightVA, a lightweight V A framework\nwith agent-based task planning. The term “lightweight” refers\nto the framework’s focus on reducing the cost of development\nand using V A systems. Using LLM agents to aid the task\nplanning and execution process. The framework builds upon\nmulti-level relationships, translating high-level goals to low-\nlevel tasks and deriving insights through data mining and\ninteractive visualizations. Specifically, the framework employs\na recursive process that includes a planner, executor, and\ncontroller, which dynamically accommodates task complexity.\nThe planner is responsible for task decomposition, the executor\nhandles task execution, including visualization generation and\ndata analysis, and the controller orchestrates the executor and\nplanner and manages whether tasks continue to be decom-\nposed. We develop a system based on the framework that\nprovides a chat view to support communication among users\nand agents, a task flow view to visualize manage the process of\ntask planning, a visualization panel to show single view and\nmultiple linked views connected to the task flow. Our main\ncontributions are as follows:\n• We propose a lightweight V A framework using LLM agent-\nbased task planning and execution. This approach enables\nadaptive, efficient analysis through human-agent collabora-\ntion, supporting users in task decomposition, visualization,\nand insight discovery.\n• We develop a system that embodies our framework, sup-\nporting users to analyze data with the assistance of agents\nand communicate through the hybrid user interface.\n• We demonstrate the effectiveness of the system through a\nusage scenario and an expert study.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 2\nII. R ELATED WORK\nOur research is related to prior studies on visualization\nrecommendations, task-driven data exploration, and LLM ap-\nplication in data exploration.\nA. Visualization Recommendation\nVisualization authoring typically requires users to have\nprofessional visualization knowledge and programming ability.\nFor example, tools like Tableau support creating visualizations\nand multiple linked views with shelf-configuration design,\noffering robust visualization creation features. However, while\nTableau excels as an authoring tool, it offers limited support\nfor automatic task decomposition and V A method implemen-\ntation to further assist with analysis. A corpus of research\nhas been proposed on automatic visualization recommenda-\ntions [18]–[20]. Rule-based methods, such as V oyager [21]\nand CompassQL [22], utilize the visualization principles to\nconstruct visual mapping and allow users to choose their\ninterested data properties and visual encoding to create vi-\nsualizations. For machine learning methods, Data2Vis [23]\nintroduces an end-to-end trainable neural translation model for\nautomatically generating visualizations from given datasets.\nVizML [24] learned visualization design choices from a corpus\nof data-visualization pairs. Table2Charts [25] recommends\nvisualizations by learning patterns between tables and visual-\nizations. ChartSeer [26] employs deep learning to recommend\nvisualizations based on users’ interactions. Unlike end-to-end\ndeep learning methods that directly learn from datasets to\ngenerate visualizations, knowledge graph-based approaches,\nsuch as AdaVis [27], KG4VIS [28], Lodestar [29], leverage\nstructured information about data and relationships to recom-\nmend visualizations.\nIn addition, there are some works that consider multi-\nview generation. Qu and Hullman [30] proposes coordination\nprinciples to keep consistency. Sun et al. [31] investigate\ndifferent linking techniques based on data relationships. Dz-\niban [32] is a visualization API using anchored recommen-\ndation and extending Draco [33] to reason about multiple\nviews. DMiner [34] investigated the design rules of the single\nviews and view-wise relationships from online notebooks to\nrecommend multiple-view dashboards. MultiVision [35] and\nDashBot [5] recommend dashboards given an input dataset\nin an end-to-end manner using deep learning models. Shi\net al. [36] optimize multi-view layouts by predicting the\nsimilarity of visual elements using Transformer-based models.\nPrevious work has provided a solid research foundation for\nthe principles between data, visualization, and multi-views.\nBuilding on this, we further study integrating LLM-agent\nto recommend visualizations that align with high-level goals\nand evolving tasks throughout the V A pipeline, which require\nsignificant human effort.\nB. Task-Driven Visual Data Exploration\nIn addition to data attributes when recommending visualiza-\ntions, some visualization recommendation systems are task-\ndriven recommendation systems that consider one or more\nanalytic tasks (e.g., correlate, analyze trend). Some scholars\nstudy the recommendation of analysis methods in exploratory\ndata analysis (EDA) within notebooks. EDAssistant [37] rec-\nommends code by analyzing associations between APIs in a\nlarge notebook collection. ATENA [38] shapes EDA into a\nMarkov Decision Process (MDP) model using a deep rein-\nforcement learning architecture to effectively optimize note-\nbook generation. Furthermore, visual analytics incorporates\nvisualization techniques into the EDA process, expanding the\ntask space. Casner [39] presents one of the earliest examples\nof visualization systems that suggest charts based on a user’s\ntask (e.g., finding direct flight routes or a table to see flight\ninformation). Saket et al. [40] conducted a study to assess\nthe effectiveness of five canonical visualizations on ten low-\nlevel analytic tasks [41] and developed a recommendation\nengine based on their study’s findings. Gotz and Wen [42]\npresent a prototype system that observes interaction patterns\n(e.g., repeatedly changing filters or swapping attributes) to\ninfer analytic tasks such as comparison or trend analysis\nand correspondingly recommends visualizations such as small\nmultiples or line charts. VizAssist [43] enables its users to\nspecify their data objectives in terms of analytic tasks (e.g.,\ncorrelate, compare) and considers these tasks in combination\nwith existing perceptual guidelines as input to a genetic\nalgorithm for recommending visualizations. Foresight [8] uses\ntasks like distributions, outliers, and correlations to guide in-\nsight discovery and grouping recommendations. TaskVis [44]\nrecommends visualizations under specific tasks through an-\nswer set programming.\nIn addition to visual generation based on a single task,\nMedley [45] recommends multi-view collections based on\nseveral analytic intents, and views and widgets can be selected\nto compose a variety of dashboards. However, the analytic\nintents and visualization combinations are often chosen from\npreset options, which limits exploration flexibility. Different\nfrom them, we study dynamic task planning based on the goal\nand findings and leverage human-agent collaboration.\nC. Large Language Model in Data Exploration\nLLM-based tools have been proposed for data explo-\nration and visualization tasks. For visualization generation\nand recommendations, LLM4Vis [14] and ChartGPT [46]\nutilize LLMs to choose appropriate visualizations from natural\nlanguage instructions. Li et al. [47] evaluate the capability\nof GPT-3.5 to generate visualization specifications, demon-\nstrating its superiority over previous machine learning-based\napproaches. NL2Rigel [48] showcases the LLM’s ability to\nconvert instructions into comprehensive data visualizations\nand tables. For analytical task translation and automation,\nHassan et al. [49] and Data-Copilot [16] concentrate on\nconverting analytical goals and ambiguous queries into ac-\ntionable data analysis tasks. Ma et al. [50] and JarviX [12]\nintroduce systems that automate the data exploration process\nby identifying suitable analysis intents and generating insights.\nText2Analysis [51] offers a framework for categorizing data\nanalysis tasks, establishing a structured approach to tackling\ncommon analytical challenges ranging from basic operations\nto forecasting and chart generation. However, the tasks in these\nstudies are generally straightforward and focused, with limited\nexploration into the decomposition of more complex tasks.\nWhen using LLMs to solve complex tasks where multi-\nstep reasoning is demanded, the performance of directly using\nLLMs tends to decrease. Recently, prior methods have utilized\nLLMs with input-output prompting, CoT [52], ToT [53] or\nGoT [54] to perform complex task planning and execution.\nThese methods proved that LLMs are good at task planning\nbut require appropriate prompting techniques. Another way is\nto integrate LLMs in the interface, allowing chaining multiple\nprompts to address a much wider range of human tasks. Wu et\nal. [55] introduce the chaining of AI models, where a complex\ntask is divided into multiple steps. Talk2Data [56] presents a\nnatural language interface that enables users to explore visual\ndata through question decomposition. However, the linked\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 3\nvisualization and more advanced data analysis methods remain\nlimited. To enhance this capability, we leverage LLMs and\npropose an agent-based autonomous task-planning strategy for\nadaptive V A system construction and exploration.\nIII. L IGHT VA FRAMEWORK\nThe pipeline of V A involves two stages: development and\nanalysis. Thus, in LightV A, we aim to reduce the efforts\nfor both developers and analysts. In the following, we will\nexamine the challenges and derive the design requirements\nfor integrating LLM-based agents into the user’s workflow.\nFinally, we introduce the conceptual framework of agent-based\nV A workflow.\nA. Challenges\nThrough a review of visual analytics literature, we identify\nand dissect specific challenges that intensify the effort users\nmust exert:\nC1 Accommodating diverse analysis requirements: Data\nAnalysts often face the immense challenge of navigating\na vast exploration space, where the analytical process\nis dynamic and iterative [4]. Forming hypotheses and\nvalidating them through the continuous proposal of new\ntasks and insights is complex. They have to figure out\nthe connections between tasks and insights in their mind\nand try to propose tasks in the next few steps until they\nachieve the goal.\nC2 Developing visualization and data mining tools is time-\nconsuming: Analysts often lack proficiency in developing\nand managing V A systems, which makes it challenging for\nthem to select and apply appropriate data modeling and\nvisualization techniques [45]. Additionally, when tasks\ninvolve multiple visualizations, these must be integrated\ninto a linked view to enable more effective interactive\nexploration [57]. However, this process requires substan-\ntial knowledge of both visualization principles and coding\nskills, resulting in inefficiencies and delays [58].\nB. Design Requirements\nBased on these challenges, we have settled on a set of\ntargeted design requirements.\nR1 Adaptive task planning: Task proposals need to be\ntailored to users’ analysis goals and the given dataset. This\nincludes exploring in depth and breadth, with automatic\nplanning and execution reducing user efforts. Moreover,\nas exploration results emerge, new tasks should contextu-\nally link to previous exploration results, adapting to the\nswitching between tasks. (C1)\nR2 Flexible visualization generation: The generation of\nvisualizations should flexibly handle different tasks and\ndata. This encompasses accurate data identification, trans-\nformation, and selection of visualization types, as well\nas adding highlights based on discovered insights to re-\nduce cognitive load. Furthermore, generated views should\nsupport interaction, allowing for more immersive user\nanalysis. (C2)\nR3 Automatic insight generation: To reduce development\ncosts, the system should efficiently complete code-based\ndata analyses for given tasks, providing visualizations and\nsuggesting findings to facilitate hypothesis forming and\nvalidation. To lessen the cognitive burden, important parts\nof insights should be highlighted in rich text format in the\noutput results. (C2)\nR eport\ninsights\nV alidateSelect\nPr opose\ntask (subtask)\nR ecommend and Decompose tasks\nData modeling\nVisualization\nGenerate  \nV A codes\nUse V A \nS ystem\nDev elop V A \nS ystem\nT ask planning and execution\n R efinement\n Interact\nFoundation\nGoal Data\nFig. 1. The illustration of conceptual framework in LightV A. The agent\ncreates the V A system for analysis by task planning and execution, while the\nuser uses the created V A system and refines agent outputs.\nR4 Multiple view composition: As new visualizations are\nadded, older ones may become less relevant. To display\nthe most recent results to the user, the visualization panel\nneeds to be updated. However, it is crucial to avoid\ndiscarding previous results as they may be relevant to new\ninsights. Users should be allowed to merge visualizations\nof interest even if they are not the latest. Within the\nmerged views, users can focus their analysis on a smaller\nscope through interaction, reducing cognitive load among\nlarge sets. (C2)\nR5 Intuitive analysis process: The relationship between\ntasks, visualization, insights, and analysis progress should\nbe presented in a more intuitive form. The system should\nsupport interactions between human and agent to help\nusers understand the agent’s task planning and execution.\n(C1, C2)\nC. Conceptual Framework\nBased on the challenges and requirements discussed, we\npropose LightV A, a lightweight V A framework with agent-\nbased task planning. The “lightweight” refers to the light ex-\npectations in developing V A systems and using the developed\nsystems for analysis. We design the framework to involve a\nrecursive task-solving process in which goals and data are\ninputs, and insights are outputs. The intermediate results are\ntasks, subtasks, visualization, data modeling (Fig. 1). The\nLLM agents are integrated to support goal understanding\nand task decomposition (R1), data modeling and visualization\ncodes generation (R2, R3), and linked view generation for\ninteractive exploration (R4). Meanwhile, the users can monitor\nthe process, guide the agent, and refine the agent’s output\nthrough direct manipulations and natural languages (R5).\n1) Defining Primitives: According to the above descriptions\nof the framework, some key primitives need to be defined.\nGoal and Data: We define a goal as an overall description\nor a vague utterance that expresses the high-level purpose of\nthe analysis. For instance, “to analyze the influence factor\non a car’s fuel efficiency”. The goal directs the focus of the\nanalysis, and the data supplies the analysis evidence to meet\nthe goal.\nTask: Tasks are specific aspects derived from the goal\nand make the goal executable. We define them using four\nattributes:\ntask := ⟨type , data variables , method, progress⟩ (1)\nThe type indicates the nature or category of the analytical\noperation to be performed, such as finding extreme, outlier,\nchange point, trend analysis, etc. The data variables are the\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 4\nobjects upon which the tasks will be applied. The method\ndefines how the task will be solved, including data modeling\nand visualization methods, and the solved result is insight.\nIf the task is complex, which means it covers multiple data\nvariables and needs to use multiple methods to solve, the task\ncan be decomposed into subtasks. To depict whether the task\nis completed, we define progress, which means the task is\nsolved when its subtasks are solved as well. Given that the\ntasks can be complex and range from low to high abstraction,\nwe describe them using natural language.\nInsight: This denotes the expected or targeted knowledge or\nunderstanding that the task aims to achieve. We define insight\nusing four attributes:\ninsight := ⟨type , parameters, data variables , data values ⟩ (2)\nThe insight type can be discoveries, patterns, trends, or\nanomalies identified through analysis [59]. For example, if\nthe task type is “compare”, the insight type could be “differ-\nence” [7]. The parameters specific features of the insight, such\nas “increasing” or “decreasing” of “trend”. The data variables\nfor insights can be the original data columns or transformed\nvariables, while the data variables for tasks are the original\ndata columns. For example, if the task involves analyzing\nvehicle weight and fuel efficiency (MPG), the task’s data\nvariables would include “Weight in lbs” and “MPG”. If the\ninsight includes a derived metric such as “MPG per pound,”\nthis would be considered a transformed variable specific to\nthe insight. The data values refer to particular values of data\nvariables. For example, the maximum MPG value is “48”.\nVisualization: Refers to the visual representation required\nfor the task that best conveys the data and insights.\nvisualization := ⟨type , encoding, interaction, coordination⟩ (3)\nOur framework generates visualizations using Vega-Lite\ngrammar [60], which supports the above four aspects. The\ntypes of interactions are, e.g., filtering, zooming, and hovering\nsupported by the visualization. The coordination describes\nhow this visualization interacts or synchronizes with other\nvisualizations, such as a brush, to filter each other.\n2) Workflow of Framework: Our framework involves build-\ning a task flow , represented as a directed compound graph\nG = (V,E), where each node v ∈V represents a task and edge\ne ∈ E represents the connection from one task to another task.\nSince the process of exploration involves both enlightening\nand in-depth thinking, we define two types of task edges:\nrecommendation and decomposition. And we define the\nsolving of the task as execution. The difference between\nrecommendation and decomposition is that recommendation is\n“goal-oriented”, and decomposition is “task-specific”. The rec-\nommendation aims to broaden the exploration scope, providing\nheuristic suggestions. The decomposition aims to ensure a de-\ntailed plan with clear logic for solving the task. The following\nsections will introduce how humans and agents collaborated\nto complete the analysis in our framework (Fig. 2).\nStage1: Task recommendation. Initially, the user uploads\nthe data and inputs a goal ( H1), and then the agent interprets\ndata ( A1) and transforms the goal into specific, actionable\ntasks ( A2). The user can provide feedback and accept or\nmodify recommended tasks to align with their analysis needs\n(H2). Specifically, the interaction process involves two stages:\n• Initial stage: At the begining, agent should first interpret\nthe data and propose tasks that make the goal executable\nby mapping it to the data. The principle is to identify data\nvariables and task type . For instance, for a goal of “find\nInterpret \ndata\nSet \ndata & goal\nGoal\nSelect or \nPropose tasks \nRecommend \ntasks \nDecompose \ntask\nEvaluate \ntask\nExamine and modify \ndecomposition plan\nData\nTask\nTask\nData modeling\nInsight\nSummarized \ninsight\nVisualization\nVisual analytics \nsystem\nT ask r ecommendation1  T ask decomposition3 T ask ex ecution2\nLLM Agent-based Task Planning and Execution for Visual Analytics\nMerge \nvisualizations\nInteractive \nexploration\nGenerate \ncodes\nReport \ninsight\nH4H3H2H1 H5\nA4A3A2A1 A5 A6\nSubtask\nSubtask\nSubtask\nFig. 2. The workflow of the LLM agent-based task planning and execution\nfor visual analytics. The collaboration between users and the AI agent is\ncharacterized by three stages: task recommendation, task execution, and task\ndecomposition. The user proposes goals, selects tasks, merges visualizations,\nand engages in interactive exploration (H1-H5). The agent interprets data,\nrecommends tasks, generates codes, reports insights, evaluates tasks, and\ndecomposes tasks (A1-A6).\nhigh-risk events in a city” , useful data variables could in-\nclude time, space, text, and sentiment. The agent also needs\nto distinguish the purpose of the analysis, e.g., “finding\noutliers”. Thus, different combinations of data variables and\ntask types can form different tasks under a goal.\n• Historical context stage: As the analysis continues to\naccumulate in the recommendation process, the agent should\nrecommend tasks considering previous tasks and the overall\ngoal. First, when a task is completed, the agent evaluates if\nthe overall goal has been achieved. If not, it should recom-\nmend new tasks aligned with the goal. Second, previously\nunexplored tasks should be re-evaluated and proposed again\nif they are still relevant to the goal.\nStage2: Task execution. In this stage, the agent gener-\nates visualization and modeling codes ( A3) and executes the\ncodes to report and summarize insights ( A4). Having multiple\nvisualizations and modeling approaches, users could select\nvisualizations to merge a linked view ( H3) and interactively\nexplore it (H4). It comprises the following two-step approach:\n• Visualization and insight generation : When the user se-\nlects an agent-proposed task or proposes a task themselves,\nthe agent writes codes to complete the analysis. The agent\nneeds to choose appropriate data modeling and visualization\nmethods for each task and run the codes to complete the\nanalysis. The agent then needs to generate the insights into\na structured format. Finally, the agent should summarize\ninsights obtained from the decomposition of subtasks, pro-\nviding a comprehensive overview of the analysis.\n• Multi-view linking : Users can initiate coordination by\nselecting multiple visualizations they prefer while the agent\ngenerates codes. We currently do not automatically combine\nvisualizations from subtasks because subtasks decomposed\nfrom a main task often share common variables and visu-\nalization types. For example, several subtasks might use\nlatitude and longitude to create maps exploring different\nspatial patterns. The linked view is often used to conduct\nmulti-variate association analysis with different visualiza-\ntions. Thus, allowing users to choose their visualizations\ncan provide a more flexible analysis.\nStage3: Task decomposition. We design the decomposition\nfollowing a “on-demand” strategy. In other words, execution\nis prioritized, and decomposition is considered only if the task\nis not completed. This approach aims to ensure users can\nsee initial results quickly in an interactive environment. In\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 5\nLogic operator\nAND DOWN\nUser Input\nGoal\nTask 1\nExecutor\nMultiple \nVisualizations\nInsight\nSummary Insight\nAnalysis History\nDecomposer\nTask 2\nTask 3\nSubTask 1 TPA (SubTask 1 ,  k+1)\nTPA (SubTask 2 , k+1)\nTPA (SubTask 3 , k+1)\nSubTask 2\nSubTask 3\nDataset\nRecommender\nCompleted?\nLogicLogic\nFinished?\nInteractive \nVisualization\nT ask R ecommendation T ask DecompositionT ask Execution\nReturn \nReturn \nReturn \nReturn \nReturn \nController\nVisual Consistency\nColor mapping\nInteraction linking\nLayout organizing\nfield, scale\nfield, type\norder, relation\nTask Flow \nnodes, edgesnodes, edges\nData Lens\ndata subset\nChat Panel\nnodes, edges task, logic\nInsight Styling\nfiled, values, type, \nparameter\na\nb\nc\nf\nde\nFig. 3. The agent-based system architecture. The pipeline starts from the goal and data with an agent-based task planning strategy, including recommendation,\nexecution, and decomposition. Specific components and interactions are labeled (a)-(f), where (a) is the initial stage to recommend tasks from the goal, (b)\nrecommends tasks with historical contexts, and (c) is the execution of tasks generating visualization and insights. After that, a series of optimizations for\nvisual consistency are performed (d). Historical records are managed, and the progress of tasks is updated in a timely manner (e). According to the analysis\nresults, the agent evaluates whether the task needs to be decomposed (f).\nthis stage, the agent is responsible for evaluating tasks ( A5)\nand proposing a decomposition plan ( A6), while the users can\nexamine and modify agent output to override the agents ( H5).\n• Results assessment: Based on the initial execution’s insights\nand the complexity of the task, the agent assesses the need\nfor further analysis. According to the definition of task, the\nagent should verify the selection of data variables and the\nrationality of data modeling and visualization methods. The\nevaluation should be explained to make users understand the\nmotivation of decomposition.\n• Sub-task generation : If decomposition is necessary, agent\nshould formulate a plan outlining subtasks. Each subtask\nshould have appropriate data variables and methods address-\ning distinct aspects of the main task. These subtasks should\nhave an execution order, e.g., in parallel or sequentially.\nIV. L IGHT VA SYSTEM\nGuided by the design requirements, we propose a pipeline\nof LLM agent-based task planning and an interface to support\ninteractive visual data exploration with assistance.\nA. Agent-based Task Planning\nBased on the conceptual framework previously introduced,\nwe propose an agent-based task-planning pipeline Fig. 3. In\nthis process, we have three kinds of modules. The planner has\ntwo types: recommender and decomposer, which are responsi-\nble for task recommendation and decomposition respectively.\nThe executor handles task execution, including visualization\ngeneration and data analysis, and the controller is the recursive\nalgorithm that bridges the executor and planner (See the\nalgorithm in the appendix). When introducing each stage’s\nstrategy, we provide the prompt templates that describe input\nand output, instructions to LLMs, and indicators to describe\nthe output format. The prompt examples and outputs are\navailable in Appendix A.\n1) Task recommendation: Task planning begins with a\nrecommendation approach based on the given goal and dataset.\nThe recommender aims to transform the goal into actionable\ntasks. The implementation follows the step-by-step guidelines\nwith two stages: initial stage and historical context stage\n(Prompt template 1).\nIn the initial stage, the input is the goal and the data (Fig. 3-\na) and the objective is to convert the goal into actionable tasks.\nBased on the framework (Section III-C2), we guide the model\nwith three principles. First, identify data variables relevant to\nthe goal. Second, propose several task type to refine the goal\nfrom different aspects. Third, draft descriptions of tasks by\ncombining these data variables and task types.\nIn the historical context stage (Fig. 3-b), the generated\ntasks should be divided into two types: linking goals with\ndiscoveries to propose new tasks and tasks that might have\nbeen forgotten by the user needs to review. To achieve this\ngoal, the model might need to consider the following steps.\nFirst, evaluate completed tasks to check if the overall goal\nis achieved. Second, propose new tasks based on the current\ncontext and previous tasks. Third, propose previously unex-\nplored tasks if they remain relevant. In addition to guidelines,\nwe should provide examples of output to guide models.\nPrompt Template 1(Task Recommendation).\n———————————————————-Initial stage\nInput: {goal, data}\nInstruction: You need to come up with a short plan based\non the understanding of the data to help accomplish the\ngoal. Please recommend n exploratory tasks, including task\ndescription, type, and data variables. For task type, you may\nconsider the trend, correlation, category, distribution, etc, to\nexplore the goal from different aspects. For data variables,\nlist the original column names.\nIndicator: {An example in JSON format}\nOutput: {new tasks}\n—————————————————Historical context\nInput: {goal, data, explored and unexplored tasks}\nInstruction: You need to supplement some new tasks for\nexplored tasks by considering the results of tasks already\nexplored if needed. Second, it is recommended that previous\nunexplored tasks be revisited if they are suitable for analysis\nat this stage by considering the explored tasks.\nIndicator: {An example in JSON format}\nOutput: {new tasks, existing tasks to review}\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 6\n2) Task Execution: After tasks are proposed and confirmed\nby the user, the executor will solve tasks by generating\ncodes (Fig. 3-c). We then adopt the “decompose on-demand”\nstrategy, which means we first execute the task and then\ndecompose the task if the results are unsatisfactory. This\napproach allows users to obtain preliminary results quickly\nand enables a more flexible way to address both simple and\ncomplex tasks, avoiding unnecessary time cost that comes\nfrom always decomposing tasks in advance.\nTo execute a task, the input for the executor includes a goal,\ndata, and task description, and the output includes visualiza-\ntions and insights for the selected task (Prompt template 2). In\nthe first round, two code snippets are generated: one for data\nanalysis and the other for visualization. In the second round,\nstructured insights are generated by data analysis results.\nPrompt Template 2(Task Execution).\n—————————————————-Code generation\nInput: {task, data summary, code template}\nInstruction: You need to write Python codes to analyze\nthe data to solve this task. After finishing the data analysis,\nyou should continue to use Altair to generate an interactive\nvisualization. Add a brush or click function, a tooltip, and\na legend if different colors are used.\nIndicator: {A code template}\n1 import altair as alt\n2 import pandas as pd\n3 def plot(data: pd.DataFrame):\n4 # Data preprocessing\n5 <codes>\n6 # Chart generation\n7 chart = alt.Chart().mark bar().encode()\n8 return chart\nOutput:{insight, visualization}\n————————————-Structured insight generation\nInput: {task, data, codes}\nInstruction: Run the codes to report an important insight\nfor this task: task. You should output insight, including text,\ninsight type, parameters, data variables, and data values.\n{Definitions of insight attributes.}\nIndicator: {A few examples in JSON format}\nOutput: {insight}\nVisualization generation: For each task, the model needs to\nimplement data analysis and visualization methods, where one\ntask corresponds to one visualization, and the visualizations\nwill be combined into multiple views when multi-variables\nassociation analysis is required, which we will introduce later.\nWe use Vega-Lite via Altair [60] to generate visualizations.\nVega-Lite is a high-level grammar that can support the gener-\nation of a variety of visualization types in a low-code way, and\nits declarative structure allows users to modify visualizations\neasily. As Altair only supports basic data transformation, we\nleverage Python’s flexible libraries, allowing for more complex\nanalysis, such as regression and clustering. However, this\nmay introduce inconsistencies and errors between analysis and\nvisualization code snippets. To address this, we provide a code\nscaffold in the prompt to let LLMs fill the empty to enhance\nconsistency and improve output stability.\nStructured insight generation: After generating visual-\nization and modeling codes, the executor needs to execute\nthem to structure the results into insights. As the agent’s\ncoding environment1 does not support Altair, we use the local\n1https://platform.openai.com/docs/assistants/tools/code-interpreter, Last ac-\ncessed March 2024. Code Interpreter allows Assistants to write and run Python\ncode in a sandboxed execution environment.\nenvironment to execute the visualization part of the code. The\nagent runs the data analysis part, interprets the results, and\nderives insights. We provide a format example according to\nour definition of insight in the framework to guide model\noutput. This includes the insight description, the insight type,\nand the data variables and values. These attributes highlight\nkey elements in the insight description to improve readability.\nLinked-view generation : For multi-variable association\nanalysis, we enable the generation of multiple linked views\n(Fig. 3-d). While LLMs can add interactivity and set basic col-\nors and layouts within the code, maintaining visual consistency\nacross views, such as avoiding duplicate colors and ensuring\nneat layouts, requires implementing additional constraints.\n• Interaction linking: To enable interaction linking among\ncharts, we instruct the model to modify and combine the\nchart codes following several guidelines. First, ensure both\ncharts contain common key columns with consistent data\nformats for the selection fields. Next, define the selection\nmechanisms. Common interaction methods include a brush\non the time axis for the line chart, dual brushes for the\nscatter plot, and click interactions for the bar chart. Then,\nthe transform filter should be applied to the other charts.\n• Layout organization: When generating new visualizations,\nwe set the charts to the same size and arrange them\nsequentially. If the user selects charts to merge multiple\nviews, we allow selecting up to six charts to avoid excessive\ncognitive load. The LLMs should output the layout no more\nthan three charts in a row.\n• Color mapping: We built a data-to-color mapping rule\nwithin the task space based on Qu and Hullman’s guidelines\n[30], where one color corresponds to one data dimension\nwithout reuse. For example, the same field should use the\nsame quantitative color scale across different views, while\ndifferent fields should use non-overlapping hues or palettes\nto avoid confusion.\nTask progress calculation: In the analysis process, each\nnode has a progress attribute that quantifies the task comple-\ntion degree. When the task is executed, the agent evaluates\nthe task’s completion status. If the task remains incomplete\nand needs further in-depth analysis, the progress is set to\n0%; otherwise, it is set to 100%. Upon determining a task\nas incomplete, the agent suggests a decomposition plan. If the\nfurther decomposition is denied by the user, the progress is\nset to 100%. Each recently executed task is represented as a\nleaf node, initiating a bottom-up refresh of progress values for\nnon-leaf nodes across the tree (Fig. 3-e). The process for the\nmain task is the average progress of two child tasks, which\nare user-confirmed nodes generated by task decomposition or\nrecommendation. This hierarchical update process accurately\nreflects the analysis status and ensures progress values account\nfor both the agent’s assessments and user input.\n3) Task Decomposition: Upon the execution of a task, the\ndecomposer evaluates the quality of completion and decides\nwhether to decompose and how (Prompt template 3). To\nevaluate if the task is completed, the input includes tasks,\ncodes, and insight, while the output is a score ranging from\n0 to 10 and an explanation. Then, if decomposition is needed\n(i.e., its score is below a certain threshold, like 8), a detailed\ndecomposition plan with the appropriate logic operators will\nbe proposed (Fig. 3-f). Conversely, if no further decomposition\nis required, the controller calls the recommender to propose\nnew exploratory tasks. According to the framework, the eval-\nuation process can be guided by the following guidelines. The\nfirst is to determine if the initial solution adequately segments\nthe data, ensuring that the analysis covers all relevant subsets.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 7\nFig. 4. The LightV A system comprises four views. Users can communicate with LLMs and control the planning process in (A) Chat view by selecting\nthe tasks or setting the decomposition plan. The generated visualization and insights from LLMs are updated in (B) Visualization view. Task flow view (C)\nvisualizes the task planning structure and allows users to control the analysis process. When a task is completed, users check the data exploration situation\nin the Data table with table lens (D).\nSecond, assess if the task requires further data mining or\nvisualization methods (e.g., regression, clustering) to extract\ndeeper insights.\nPrompt Template 3(Task Decomposition).\n———————————————————–Is complete\nInput: {task, codes, insight}\nInstruction: You need to judge whether the task requires\nfurther analysis. The complexity of the task can be con-\nsidered from data, data mining, and visualization methods.\nIf the task appears incomplete, it needs to be decomposed\nfurther. Please rate the task from 1-10. For example, if the\ninitial solution adequately segments the data or if the task\nrequires advanced statistical analysis. You should output a\nscore and explanation for your evaluation.\nIndicator: {A output template in JSON format}\nOutput: {score, explanation}\n———————————————————–Decompose\nInput: {task, insight, score, explanation}\nInstruction: This task requires further analysis. According\nto the score and explanation, please generate no more than\nn subtasks and indicate the methods they use respectively\nbased on this. There are two operators AND and DOWN in\nthe execution order to connect tasks.\nIndicator: {An example in JSON format}\nOutput: {subtasks, execution order}\nIf a task requires decomposition, a depth-first decomposition\nprocess is employed. This process is guided by two factors.\nOne is a predefined maximum decomposition step depth\n(kmax), managing the depth ( k) of decomposition (decompose\nwhen k < kmax). Another factor is the completeness of sub-\ntasks. Here, we define each task that can be decomposed into\nsubtasks with two types of logic.\n• AND: Subtasks are independent and executed in parallel\norder with multiple agents.\n• DOWN: This is a particular AND case. Subtasks are depen-\ndent and will be executed in serial order with a single agent.\nWe use an example to illustrate the decomposition process.\nIf the task of analyzing vehicle weight and fuel efficiency is\ncompleted with a score of 6/10, the agent might suggest the\nfollowing decompositions: T1: Segment the data into weight\ncategories (light, medium, heavy) and analyze the fuel effi-\nciency within each segment. T2: Within each weight category,\nconduct a clustering analysis to identify patterns or groupings\nthat could further explain variations in fuel efficiency. T3:\nPerform a multiple regression analysis to control for additional\nvariables like engine size and vehicle age. The execution logic\nwould be (T1 DOWN T2 AND T3).\nAccording to the logic, the agent executes each subtask\nand summarizes the subtasks’ insights to formulate an overall\ninsight for the decomposed task. No further decomposition\nis required if subtasks are all completed or up to the max\nsteps. Then, considering the goal and analysis results, the\nagent might propose revisiting previously proposed tasks or\nrecommending new exploration tasks.\nB. LightVA Interface\nThe agent-based interface includes several views to enable\nuser-controlled visual exploration, as shown in Fig. 4. To\nenable this, we provide four views: Chat view , Visualization\nview, Task flow view, Data view. The design of the interaction\nfollows the design considerations of Section III-B and provides\nthree modes of interaction.\nChat view: The exploration begins with the Chat\nview (Fig. 4A), an LLM-based chat box that facilitates direct\ncommunication between users and agents through natural\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 8\nlanguage interaction. The interaction between the user and\nagent can switch between delegate, guide, and discuss. In\nthis interface, the user uploads data and inputs goals. The\nagent then responds with its understanding of the dataset and\nsuggests new tasks in the form of buttons. Users can bookmark\ntasks of interest, and tasks that are not of interest will not\nbe counted in the progress of exploration. The user can then\nexecute a task by clicking on it. Additionally, users can enter\ntheir own tasks in the chat box. After submitting a task,\nthe agent will provide a visualization of the production and\ninsights obtained from the calculations in the Visualization\nview (Fig. 4B). If the task needs to be decomposed, the task\ndecomposition plan will appear. The user can modify the\nlogical relationship between the task and the execution plan by\nswitching the text button. The agent will analyze the task based\non the user’s instructions and return the results of multiple\nsubtasks. During the exploration process, users can ask various\nquestions and discuss them with LLM in the dialog box, such\nas the problem analysis method, understanding of the answer,\nand explanation for the recommendation.\nVisualization view: This view presents visualizations and\ninsights in the form of cards. Each card contains the task serial\nnumber, the task content, the interactive visualizations, and\nthe insights in rich text format. Users can modify the Vega-\nLite JSON codes and insight text. To address the issue of\ncognitive load for the user, we allow the user to select which\nvisualization cards they want to view, and these selected cards\nare displayed in the main view while the remaining cards are\nmoved to the candidate set below. Additionally, the user can\nmerge the selected cards to create an interactive linked view.\nMoreover, users can modify and export the generated codes,\nwhich providing flexibility for those who need to customize\ntheir analyses further.\nTask flow view: The task flow view updates as goals and\ntasks are added, providing the user with a clear analysis of\nstatus and progress (Fig. 4C). Each node is a rectangular box\nshowing the task id, task type, and percent progress. The\noriginal task text appears when the cursor is over a node.\nClicking on a node will update the visualization layout in\nthe Visualization view . Hovering over the edge can see the\ndifferences in data and task type between the associated nodes.\nIn accordance with design considerations, we allow users to\nchoose unexplored tasks to be executed and delegate them to\nthe agent. Additionally, users can remove pending tasks from\nthe flow to reduce workload. A clear exploration structure may\ninspire user’s ideas.\nData table view: In addition to task visualization, we\nprovide a data lens visualization in Data table view (Fig. 4D)\nto guide users in the large exploration space. When a task\nis being completed, the user can observe the exploration of\nthe accumulated data usage frequency in table lens mode. The\ncolor of the table cell represents the relative frequency of each\ncell being explored. This observation may inspire the user to\ndiscover regions of interest to propose new questions.\nC. Error Handling\nWe refer to errors as issues that cause the system to crash or\nbecome unresponsive. Due to the inherent unpredictability of\nLLM outputs, errors may occur if the outputs cannot be parsed\ncorrectly, causing the system to crash or become unresponsive.\nTo ensure the system operates correctly and prevents workflow\ndisruptions, we implement an error-handling mechanism.\nTo develop an effective error-handling mechanism, we con-\nducted a test using two datasets in expert evaluation with 20\nagent-opposed tasks, employing both GPT-3.5-turbo and GPT-\n4-turbo. Each task was tested with code generation and insight\nannotation, utilizing two different prompting techniques. This\nresulted in a total of 160 initial tests. We classified the\nerrors in the test into five categories: (1) Unfamiliar dataset,\n(2) Data binding issues, (3) Serialization issues, (4) Data\ntransformation issues, (5) Syntax errors . More details about\nthe test can be found in Appendix B. To address these\ntypes of errors effectively, we implemented specific error-\nhandling strategies in three ways: before model generation\nwith prompting techniques, within the system after generation,\nor delegated to users.\nPrompting techniques: (1) Few-shot prompting [61]: We\nprovide examples to assist the model in better grasping the\nrequirements of the outputs. For example, when making insight\nannotations, examples can explain the insight components of\nthe model. (2) Chain-of-thoughts [52]: We guide the model\nthrough a thought plan with step-by-step instructions, which\nis helpful in reasoning processes, such as task execution.\nWithin system handling: (1) Self-reflection [62]: We allow\nLLMs to examine and correct their actions and outputs. For\nexample, the model should debug codes based on the observa-\ntion of the error. Based on our test results, the self-correction\nhelped reduce around 40% initially identified errors, such as\nsyntax errors, spelling mistakes, and logical inconsistencies.\n(2) Catching and feedback: Common syntax errors such as\nmatching quotes and parentheses, can be solved by rule-based\nsolutions. We classify these common errors to highlight which\nsteps in the data analysis process the LLM made mistakes\nrather than merely pointing out low-level errors.\nUser-side handling: Once errors are identified, the system\nnotifies the user, and the user may edit the codes. Additionally,\nwe maintain the analysis history, allowing for a rollback to\nthe previous step if an error occurs during the execution of\nthe current task. Users can choose a new task and remove the\nfailed task from the task flow.\nV. U SAGE SCENARIO : EVENT ANALYSIS\nTo examine the effectiveness of our framework, we demon-\nstrate the LightV A with IEEE V AST Challenge 2021 Mini-\nChallenge 3 2 as a usage scenario. The challenge’s goal is to\ndetect events that happened in Abila City during the evening\nof January 23, 2014. The provided data include microblog\nrecords and emergency dispatch records from a call center.\nOur motivation for this scenario stems from two reasons.\nFirstly, it incorporates a representative blend of data types\nand corresponding visualizations, encompassing text, spatial,\nand temporal data, which is a typical V A scenario. Secondly,\nthe V AST challenge has the ground truth to support an\nobjective evaluation of our LightV A. We use the OpenAI GPT-\n4 model in our work. For a more detailed demonstration of\nthis scenario, a video is added to the supplemental materials.\nInitialization: In the beginning, we upload a dataset .csv\nfile and a map outline data .json file at the Chat view. Then,\nwe type a goal, “find some high-risk events in this city”.\nThe dataset is introduced briefly, and the system proposes\nfour initial exploration tasks based on our goal: sentiment\nanalysis, keyword analysis, tags analysis, and spatial anal-\nysis (Fig. 4-a1). As the four tasks are aligned with our\nobjectives, we bookmark them all in our progress. We then\nprioritize the exploration of the first task - sentiment analysis.\nUpon submission, the agent generates a time varying card at\nthe Visualization view (Fig. 4-B). The generated insight for\n2https://vast-challenge.github.io/2021/MC3.html\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 9\nFig. 5. A linked view for event analysis. The timeline, histograms, and map\nare selected by the user to generate a linked view. The charts can be brushed\nor clicked to filter each other (a). The polarity, tags, and message can be found\nin the tooltip (b) of each view to support detailed analysis. By interactively\nexploring the linked views (c, d), we find some dangerous events, such as\n“Hit and run”, “Standoff”, “Fire”.\nsentiment analysis indicates that the microblog records has\nexperienced a drop in average sentiment, especially with a\nminimum polarity in 19:20, indicating potentially dangerous\nevents (Fig. 4-b1).\nTask recommendation: The agent not only generates the\nvisualization and findings but also evaluates them. For this\nsentiment analysis task, the agent gives a score of 8/10 and\nexplains that the generated results have basically completed the\ntask, but further statistical analysis can be performed (Fig. 4-\na3). As we prefer to start with a broad overview, we decide\nnot to decompose. The agent thus reminds us to revisit\nour previous collection tasks in time. Thus, we choose the\nsecond task ( keyword analysis ), which involves generating\na preliminary keyword visualization. The generated finding\nsummarizes the highly frequent words such as “fire”, “police”,\nand “POK”. In addition to the agent’s finding, clicking over\nthe stacked histogram for task 11 shows the time range of\neach word. For example, the “shooting” from 19:40 until 19:50\nand “abilafire” from 19:00 until 21:30. These findings suggest\nhigh-risk incidents like shootings and fires at first glance.\nThe agent then recommends spatial analysis . From the Data\ntable with lens, we can notice that there is indeed no latitude\nand longitude explored so far, which indicates that agent\nrecommendation can pay attention to the data coverage (Fig. 4-\nD). After execution, the system generates a map with shapes\nand messages, and the decomposition structure is updated on\nthe Task flow (Fig. 4-c1). We observe a higher concentration\nof microblogs in two specific locations, which might indicate\nincidents affecting public safety in those areas (Fig. 4-b2).\nTask decomposition: Based on the assessment of tasks\nand preliminary results, the agent recommends further de-\ncomposition with three subtasks: clustering analysis, hotspots\ndetection, and prediction modeling, with an AND and DOWN\nlogic (Fig. 4-a3). To conduct a retrospective analysis, we\nchoose the first two subtasks. The cluster analysis combines\nlocation and polarity to divide messages on the map into five\ncategories. We find that two places on the left and right sides of\nthe map have a distinct sentiment distribution. The second sub-\ntask, through analyzing message quantity, highlights hotspots,\nvisually indicating areas with dense messages.\nLinked-view analysis: Based on the suggestion of the\nagent, we can form the analytical logic from multi-variables to\nconcrete details. To find events more clearly, we select three\ntasks from the task flow view. The tasks include sentiment\nevolution charts, tag evolution charts, and a map to form\nan interactive multi-view (Fig. 5). Through interaction on\nline chart (Fig. 5-a), we observe the histogram and map and\ndiscover that around 19:20, a “hit-and-run” event occurs: a\n“black van” first collides with a small car and then a cyclist,\nsparking a lot of discussions (Fig. 5-b, c). Additionally, we\nfind that the fire occurred near the hit-and-run site. Later, we\nfind that the black van heads west by 19:44, and a shooting\noccurs with the police, marking the period with the highest\ndiscussion intensity. Finally, at 21:17, the van guys surrender.\nBy observing the table and task flow, we find that the progress\nof the goal reached 100%, and the data is basically covered,\n“type” “latitude” and “longitude” were explored frequently.\nTo conclude, in this scenario, we find significant events such\nas Fire, Hit and Run, and Stand-off without manually coding\nand designing. With task planning, we generate eight views\nwith auto-summarized findings based on statistical analysis\nand a linked view to solve the goal rapidly. Compared to our\nsubmission to the V AST Challenge [63], where we spent 2\npeople, 2 days, and 6 hours on data preprocessing (including\ntagging, categorizing, and removing spam messages) plus 2\npeople, 7 days, and 6 hours on interface construction and\nanalysis, totaling around 108 hours. In LightV A, we upload\nthe preprocessed data. Further construction and analysis take\nroughly 1 hour. This represents an efficiency increase of\napproximately 5 times, as the current effort is 25 out of 108\nhours. Moreover, it is worth noting that data wrangling still\nrequire human involvement, especially in complex scenarios.\nOne potential avenue would be to integrate agent-assisted\ndata annotation and cleaning into LightV A’s workflow. In\naddition, there may be a lack of finesse and aesthetic appeal\nin interface visualizations and interactions when compared to\nmanual designs. Despite this, the overall cost-effectiveness has\nbeen enhanced. To address this concern, we offer support for\nexporting the code, which can be further modified as needed.\nVI. E XPERT STUDY\nTo examine the effectiveness of LightV A in V A system con-\nstruction and task recommendation, we invited two V A experts\n(denoted as E1 and E2) and a domain expert (denoted as E3)\nto participate in the study. These experts have experience in\ndata analysis and are familiar with using V A systems.\nParticipants background: E1 is a V A expert specializing\nin V A system development, topics around digital humanities,\ntext-based data, road data for autonomous driving, spatiotem-\nporal datasets, and tabular data. E1 often uses D3.js and Vue.js\nto develop V A systems. E2 is a V A expert with experience in\nautonomous driving and social media, as well as work expe-\nrience in business analysis. In daily work, E2 chooses to use\ntools like Tableau within the company to analyze quantitative\ndata. The frequency of using data analysis and visualization\ntools is about weekly. E3 is a domain expert analyzing data for\nfast-moving consumer goods and supply chains. E3 frequently\nuses Excel and Power BI and occasionally uses Python for in-\ndepth analysis. The use of data analysis and visualization tools\noccurs on a daily basis.\nProcedure: The study consists of three sessions. First,\nwe spent 15 minutes to know the experts’ backgrounds in\ndata analysis and introduced our work by showing the video\ndemonstration. Then, in the exploration phase, the experts\nspend about 30 minutes exploring the system using the think-\naloud method [64]. Considering E3’s daily work requirements\nand aligning with their domain expertise, we opted for a\ndataset that mirrors their routine tasks. Due to data confi-\ndentiality concerns, we substituted the original dataset with\na comparable one related to sales for E3’s use. Meanwhile,\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 10\nFig. 6. The illustrations of the expert study using our framework with\ncars dataset. Two V A experts show different analysis preferences during the\nexploration process. (A) E1 focuses on the creation of V A, progressively\ndecomposing the goal and employing statistical methods to complete the\nexploration. (B) E2 has a clear goal during the analysis, and continuously\noptimizes the goal with task decomposition, and leverages the linked-view\ninteractions to find a satisfactory answer.\nthe automotive dataset was designated for exploration by E1\nand E2, fitting their respective areas of expertise. During\nthe system usage stage, we observed and recorded how they\ninteracted with the LightV A system. In the final stage of the\ninterview, we spent approximately 30 minutes gathering the\nexperts’ feedback on their usage experience of the system.\nA. Visual Analytics Expert Evaluation: Cars Dataset\nIn this study, we evaluate LightV A with E1 and E2 using\na well-known Auto MPG dataset 3. The dataset has 406\nrecords and 9 attributes, including brand, model, performance\nindicators (such as horsepower and cylinders), and the year of\nmanufacture and place of origin.\nDuring the exploration process, E1 is more concerned with\nthe construction of the V A system. At the beginning, E1\nsets the analytical goal to identify the factors influencing\nfuel efficiency. E1 chooses one of the initially proposed tasks\nfor execution. After completing that task, the agent presents\na detailed decomposition plan. E1 then follows the agent’s\nguidance to continue selecting subsequent sub-tasks for fur-\nther decomposition and in-depth analysis. In the generated\nvisualization (Fig. 6-a1), E1 discovers the differences in the\ndistribution of the number of vehicles produced by different\norigins across various performance ranges in the bar chart on\n3http://archive.ics.uci.edu/ml/datasets/Auto+MPG\nthe right, by utilizing the area brushing feature on the scatter\nplot. E1 also discovers the performance differences between\ncars with different numbers of cylinders through the filter\nfunctionality between linked view (Fig. 6-a2).\nAnother expert E2, demonstrates a focused interest in\nthe data analysis, pursuing a clear analytical goal that was\nrefined throughout the analysis process. Initially, E2’s goal\nwas to identify vehicles that excel both in fuel efficiency\nand performance (Fig. 6-b1). However, recommended insights\nrevealed a strong negative correlation between fuel efficiency\n(measured in MPG) and the majority of performance metrics,\nindicating a trade-off between high fuel efficiency and superior\nperformance capabilities. This insight led E2 to quickly adjust\nthe analytical goal towards prioritizing fuel efficiency while en-\nsuring performance was not significantly compromised (Fig. 6-\nb2). Based on the plan proposed in task decomposition, E2\nexplored the relationship between various vehicle performance\nmetrics and fuel efficiency with five charts. Finally, E2 com-\nbined these charts to generate linked view (Fig. 6-b3). By\nfiltering points across views, E2 found several vehicle models\nmeeting the revised goal.\nB. Domain Expert Evaluation: Sales Dataset\nTo further evaluate LightV A, we conducted another study\nwith a domain expert. E3 was interested in a sales dataset for\na large store that provides detailed transaction information 4.\nThe dataset contains 3,312 records with 21 fields, e.g., sales,\ndiscount, profit, and category.\nAt the beginning, E3 mentioned the high-time sensitivity of\nsales data for products in real-world scenarios. Therefore, in\nthe initial tasks, E3 opted to explore the temporal trends of\nproduct sales volumes. In the histogram and insight (Fig. 7-\na1) suggested by the agent, E3 found that the overall sales of\nthe item differ significantly from month to month. In the next\nstep of the decomposition plan, E3 categorized the products\nto explore the temporal trends of different categories (Fig. 7-\na2). It was found that the temporal trends of the different\ncategories were broadly similar, but the total sales volume\nof the technology category in November was the highest.\nMoreover, following the agent’s decomposition into another\ntask: “Analyze sales trends of top-selling and bottom-selling\nproducts”, E3 noted that in the domain scenario, “finding\nout how well products are selling can optimize inventory\nto prevent stockouts and excess inventory. ” Therefore, E3\nselected a task previously proposed but not executed, recom-\nmended for review by the agent, to rank the sales volumes of\ndifferent categories and subcategories (Fig. 7-a3). Based on\nthe generated visualization, E3 discovered that the “phone”\nsubcategory within the “Technology” category has the best\nsales performance.\nAfter finding the answer to the previous task, E3 was\ninterested in “analyze the correlation between sales and\nprofit. ”Upon receiving the specific scatter plot (Fig. 7-b1), E3\ndiscovered that high sales do not always mean higher profit.\nThe expert speculated that this may be due to the impact\nof discounts, as items with high sales often have significant\ndiscounts. E3 confirmed this hypothesis through interaction\nwith and analysis of the scatter plots of sales and profit, with\nand without discount (Fig. 7-b2). To find the optimal discount\nrate for products, E3 chose to analyze the relationship between\ndiscount and profit across different product categories. Accord-\ning to Fig. 7-b3, the differences between product categories are\nminor, with all reaching maximum profit at a 10% discount.\n4https://www.kaggle.com/datasets/addhyay/superstore-dataset\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 11\nC. Results and Analysis\nThis section discusses the expert’s exploration process and\nfeedback on the system construction and task planning.\nExploration process analysis: The exploration behaviors\nof the three experts differ significantly. V A expert E1 created\nthe most visualization charts, with half including interactive\nfeatures, emphasizing the generation of visualizations and\nthe exploratory use of linked views. Furthermore, E1 delved\ndeeper into task decomposition to employ more complex\nstatistical methods, such as linear regression and polynomial\nregression. E2, with profound data analysis experience, did\nnot strictly follow the agent’s recommendations during the\nexploration process. Instead, E2 refined and improved the\nanalysis goals and exploration directions based on the results\nof previous tasks, proposing new questions and flexibly em-\nploying interactive V A to complete the goal. E3, as a domain\nexpert, possessed an understanding of the characteristics of\ncertain data attributes within the dataset, such as the relation-\nships between discounts, sales, and profit. E3’s analysis was\ngrounded in real-world scenarios, selecting different analysis\nmethods as needed to integrate data insights into practical\napplications. This suggests our system supports a degree of\nflexibility, responding effectively to users’ individual needs.\nFeedback on system construction: E2 and E3 both ex-\npressed that interactive analysis through linked views fa-\ncilitates a better understanding of the data. From the per-\nspective of a V A expert, E1 commented that “basic charts\nand interactions can be generated, which are sufficient for\nsimple data analysis problems, but modifications might be\nnecessary for more complex issues. ” E2 and E3 agreed that\nthe system can effectively generate insights, enhancing the\naccuracy of analysis. E2 suggested that generating insights\nbased on awaring of user behavior would enhance the execu-\ntion of tasks. Additionally, E3 noted the significant reduction\nin manual effort due to AI generation, stating, “Previously,\nusing PowerBI required a lot of time and operations, but now\nit only takes a sentence. ”\nFeedback on task planning: E3 observed that “the pro-\nposed tasks are quite good, indicating the system has a certain\nunderstanding of the dataset, and the language used is very\nstandard. ”E1 valued the system’s ability to decompose tasks\nas “the most useful part, ” which can provide deeper analysis\nand reveal certain characteristics of the data, especially for\nthose lacking domain-specific expertise. However, domain\nexpert E3 cautioned that the decomposed tasks are not always\nreliable, stating, “The process of task decomposition needs\nto be explained.” Regarding task recommendations, E1 seeks\nmore targeted suggestions that adhere closely to instructions\nwithout becoming too divergent. In contrast, E3 emphasizes\nthat approaching goals from different perspectives can yield\nvaluable insights and strategies in their work scenarios. These\ndiffering viewpoints highlight the need for the planning algo-\nrithm to be adaptable to user preferences.\nVII. D ISCUSSION\nThis section outlines the limitations of our work and discuss\nthe implications learned from the research with the future\ndirections.\nGeneralizability of the framework: Our framework is\ngeneralizable in two aspects. First, our conceptual framework\nunifies the development and usage of V A systems, based on the\nconnections between goals, tasks, visualizations, and insights.\nSecondly, compared with existing visualization software, e.g.,\nFig. 7. The example results of the domain expert study with a superstore sales\ndataset. E3 implements analysis from overview to details and beyond with\ntwo rounds of task decomposition. (A) The top-selling products and specific\ncategories are found for inventory formulation. (B) The system effectively\npointed out the impact of discounts on sales and profits, promoting hypothesis\ngeneration and verification.\nTableau (with AI version, Ask Data) 5, LightV A offers several\ndistinct advantages. While Tableau supports natural language\ninterfaces, task recommendation, and single visualization gen-\neration, it lacks support for task decomposition, multi-view\nvisualization generation, and iterative human-in-the-loop task\ncompletion evaluation, which are the focuses of LightV A.\nMeanwhile, there are some limitations to our framework.\nFirst, the framework may require more detailed guidelines for\ncomplex and specific domains, particularly in task customiza-\ntion, visualization methods, and model selection. A more\ncomprehensive grammar for tasks and insights would improve\nmodel output evaluation and refinement. This necessitates\nextensive research and documentation, categorizing different\ntasks and domains to enhance the framework’s applicability.\nSecond, functionality in the development of V A systems needs\nto be improved, particularly features that facilitate authoring\nand version iteration. Third, the relatively small number of\nparticipants in the expert study limits the generalizability of the\nresults. Conducting a larger crowdsourced user study would\nbe beneficial in verifying the quality of the automated output\nfurther and investigating the factors that contribute to users’\nperceived quality.\nThe performance of LLMs in V A tasks: While LLM\nexhibits potentials, some of the limitations of LLM requires\ncareful handling.\n• Output stability and accuracy: The output of LLMs can be\nunstable. During our evaluation, we encountered instances\nwhere the model failed to follow instructions accurately,\nleading to parsing and execution failures in the generated\ncode. In order to address this problem, we propose an error-\nhandling mechanism. Although this mechanism helps to\navoid errors making system crashes, additional LLM errors\nabout incorrect facts should be detected and corrected. In\nthe future, we can incorporate LLMs’ self-reflection to solve\nhallucination [65] and provide insights on errors so that users\ncan work together to fix these errors.\n• Response speed : From our test, GPT-3.5-turbo completes\ntasks in significantly less time than GPT-4-turbo. To increase\nthe running speed, we use a multi-agent parallel computing\nstrategy and error handling for time exceeding a certain\n5https://help.tableau.com/current/pro/desktop/en-us/ask data.htm, last ac-\ncessed March 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 12\nthreshold. A future direction could be to utilize caching\nmechanisms such as GPTCache [66] to explore acceleration\nstrategies in data analysis scenarios.\n• Problem-solving ability: Our test results indicate that LLMs\ncan struggle with complex tasks, such as predictive model-\ning. This highlights the need for a broader evaluation of\nmodel capabilities across various complex domain-specific\nproblems to derive guidelines or evaluation metrics for task\nplanning and execution. Additionally, we found that LLMs\nmay struggle with processing issues like missing values due\nto unfamiliarity with data. From the implementation side,\nwe should provide additional materials or tools and teach\nagents to process data effectively [67].\n• Domain knowledge: In some scenarios, LLMs may not have\nsufficient domain knowledge. To address this limitation,\nfuture research can focus on combining retrieval augmented\ngeneration (RAG) and fine-tuning for LLMs to solve specific\ndomain tasks [68].\nInjecting visualization design knowledge: Design knowl-\nedge is important in designing V A systems. In our imple-\nmentation, we constrain color, interaction, and layout. These\nconstraints are not exhaustive and we only consider them\nas preliminary. Further research can include more design\nguidelines for the LLM with prompting techniques or multi-\nmodal models [69] to perceive and evaluate the effectiveness.\nBesides, a memory module [70] can be integrated to make\nthe agent evolve. In addition, interactions other than natural\nlanguage can be designed to facilitate user input of intentions,\nsuch as sketching [71] or generating widgets [72].\nAutomation and personalization: From the user study, we\nfound that users from different backgrounds have different\nneeds. Some users have a clear idea of what they want and\nneed the agent to follow orders in detail, while others prefer\nthe agent to take the lead in a more automated way. This\ndivergence in user preferences highlights the balance between\nuser agency (the control they maintain over decisions) and the\nautomation of processes. In future studies, we could explore\nhow different levels of agency and automation affect task\nperformance and user satisfaction [32]. Additionally, users\nmay require different levels of assistance in terms of breadth\nand depth. We could conduct further qualitative experiments\nto test the differences in decision-making paths when experts\nin visualization are assisted by agents [73], [74], comparing\nthese paths visually using graph algorithms. This could help\nus build a preference knowledge base for different user types,\nenabling the model to simulate and adapt to users’ desired\npaths, providing more personalized recommendations.\nVIII. C ONCLUSION\nIn this paper, we aim to reduce the complexities and\ntechnical demands of carrying out visual analytics. Therefore,\nWe introduce LightV A, a lightweight visual analytics frame-\nwork that supports task planning, insight analysis, and linked\nvisualization generation based on human-agent collaboration.\nOur framework utilizes LLM agents for task planning and\nexecution. The framework employs a recursive approach in\nwhich the agents recommend tasks, break down complex tasks\ninto subtasks, and generate visualization and data modeling\ncodes to solve tasks. We develop a system that embodies our\nproposed framework, supporting users to analyze data based\non the communication with LLM agents and use the task-\ndriven generated V A system. A usage scenario and an expert\nstudy suggest that LightV A not only reduced the manual effort\nrequired but also provided new opportunities to leverage LLMs\nto facilitate visual data exploration.\nACKNOWLEDGMENTS\nWe thank anonymous reviewers for their constructive com-\nments. This work is supported by the Natural Science Foun-\ndation of China (NSFC No.62472099).\nREFERENCES\n[1] J. J. Thomas and K. A. Cook, Illuminating the Path: An R&D Agenda\nfor Visual Analytics . National Visualization and Analytics Ctr, 2005,\npp. 69–104.\n[2] D. Keim, G. Andrienko, J.-D. Fekete, C. G ¨org, J. Kohlhammer, and\nG. Melanc ¸on, “Visual analytics: Definition, process, and challenges,”\nin Information Visualization: Human-Centered Issues and Perspectives ,\n2008, vol. 4950, pp. 154–175.\n[3] A. Wu, D. Deng, F. Cheng, Y . Wu, S. Liu, and H. Qu, “In defence\nof visual analytics systems: Replies to critics,” IEEE Transactions on\nVisualization and Computer Graphics , vol. 29, no. 1, pp. 1026–1036,\n2023.\n[4] D. Ceneda, T. Gschwandtner, T. May, S. Miksch, H.-J. Schulz, M. Streit,\nand C. Tominski, “Characterizing guidance in visual analytics,” IEEE\ntransactions on visualization and computer graphics , vol. 23, no. 1, pp.\n111–120, 2016.\n[5] D. Deng, A. Wu, H. Qu, and Y . Wu, “DashBot: Insight-driven dashboard\ngeneration based on deep reinforcement learning,” IEEE Transactions on\nVisualization and Computer Graphics, vol. 29, no. 1, pp. 690–700, 2023.\n[6] B. Yu and C. T. Silva, “FlowSense: A natural language interface for\nvisual data exploration within a dataflow system,” IEEE Transactions on\nVisualization and Computer Graphics , vol. 26, no. 1, pp. 1–11, 2019.\n[7] Y . Wang, Z. Sun, H. Zhang, W. Cui, K. Xu, X. Ma, and D. Zhang,\n“DataShot: Automatic generation of fact sheets from tabular data,” IEEE\nTransactions on Visualization and Computer Graphics , vol. 26, no. 1,\npp. 895–905, 2020.\n[8] C. Demiralp, P. J. Haas, S. Parthasarathy, and T. Pedapati, “Foresight:\nRecommending visual insights,” Proceedings of the VLDB Endowment\nInternational Conference on Very Large Data Bases , vol. 10, no. 12,\n2017.\n[9] S. Hao, Y . Gu, H. Ma, J. Hong, Z. Wang, D. Wang, and Z. Hu,\n“Reasoning with language model is planning with world model,” in\nProceedings of the Conference on Empirical Methods in Natural Lan-\nguage Processing, H. Bouamor, J. Pino, and K. Bali, Eds., Dec. 2023,\npp. 8154–8173.\n[10] S. Sharan, F. Pittaluga, M. Chandraker et al., “LLM-Assist: Enhancing\nclosed-loop planning with language-based reasoning,” arXiv preprint\narXiv:2401.00125, 2023.\n[11] J. Yang, A. Prabhakar, K. Narasimhan, and S. Yao, “InterCode: Stan-\ndardizing and benchmarking interactive coding with execution feed-\nback,” Advances in Neural Information Processing Systems , vol. 36, pp.\n23 826–23 854, 2023.\n[12] S.-C. Liu, S. Wang, T. Chang, W. Lin, C.-W. Hsiung, Y .-C. Hsieh, Y .-P.\nCheng, S.-H. Luo, and J. Zhang, “JarviX: A llm no code platform for\ntabular data analysis and optimization,” inProceedings of the Conference\non Empirical Methods in Natural Language Processing: Industry Track ,\n2023, pp. 622–630.\n[13] V . Dibia, “LIDA: A tool for automatic generation of grammar-agnostic\nvisualizations and infographics using large language models,” in Pro-\nceedings of the 61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 3: System Demonstrations) , D. Bollegala,\nR. Huang, and A. Ritter, Eds., Jul. 2023, pp. 113–126.\n[14] L. Wang, S. Zhang, Y . Wang, E.-P. Lim, and Y . Wang, “LLM4Vis:\nExplainable visualization recommendation using chatgpt,”arXiv preprint\narXiv:2310.07652, 2023.\n[15] R. Mao, G. Chen, X. Zhang, F. Guerin, and E. Cambria, “GPTE-\nval: A survey on assessments of chatgpt and gpt-4,” arXiv preprint\narXiv:2308.12488, 2023.\n[16] W. Zhang, Y . Shen, W. Lu, and Y . Zhuang, “Data-Copilot: Bridging\nbillions of data and humans with autonomous workflow,” arXiv preprint\narXiv:2306.07209, 2023.\n[17] Y . Zhao, Y . Zhang, Y . Zhang, X. Zhao, J. Wang, Z. Shao, C. Turkay,\nand S. Chen, “LEV A: Using large language models to enhance visual\nanalytics,” IEEE Transactions on Visualization and Computer Graphics,\npp. 1–17, 2024.\n[18] A. Wu, Y . Wang, X. Shu, D. Moritz, W. Cui, H. Zhang, D. Zhang,\nand H. Qu, “AI4VIS: Survey on artificial intelligence approaches for\ndata visualization,” IEEE Transactions on Visualization and Computer\nGraphics, vol. 28, no. 12, pp. 5049–5070, 2021.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 13\n[19] Q. Wang, Z. Chen, Y . Wang, and H. Qu, “A survey on ML4VIS: Apply-\ning machine learning advances to data visualization,” IEEE transactions\non visualization and computer graphics, vol. 28, no. 12, pp. 5134–5153,\n2021.\n[20] P. Ren, Y . Wang, and F. Zhao, “Re-understanding of data storytelling\ntools from a narrative perspective,” Visual Intelligence, vol. 1, no. 1,\np. 11, 2023.\n[21] K. Wongsuphasawat, D. Moritz, A. Anand, J. Mackinlay, B. Howe,\nand J. Heer, “V oyager: Exploratory analysis via faceted browsing of\nvisualization recommendations,” IEEE Transactions on Visualization\nand Computer Graphics , vol. 22, no. 1, pp. 649–658, 2015.\n[22] K. Wongsuphasawat, D. Moritz, A. Anand, J. Mackinlay, B. Howe, and\nJ. Heer, “Towards a general-purpose query language for visualization\nrecommendation,” in Proceedings of the Workshop on Human-In-the-\nLoop Data Analytics , 2016, pp. 1–6.\n[23] V . Dibia and C ¸ . Demiralp, “Data2Vis: Automatic generation of data\nvisualizations using sequence to sequence recurrent neural networks,”\nIEEE computer graphics and applications , vol. 39, no. 5, pp. 33–46,\n2019.\n[24] K. Hu, M. A. Bakker, S. Li, T. Kraska, and C. Hidalgo, “VizML:\nA machine learning approach to visualization recommendation,” in\nProceedings of the SIGCHI Conference on Human Factors in Computing\nSystems, 2019, pp. 1–12.\n[25] M. Zhou, Q. Li, X. He, Y . Li, Y . Liu, W. Ji, S. Han, Y . Chen, D. Jiang,\nand D. Zhang, “Table2Charts: Recommending charts by learning shared\ntable representations,” in Proceedings of the ACM SIGKDD Conference\non Knowledge Discovery & Data Mining , 2021, pp. 2389–2399.\n[26] J. Zhao, M. Fan, and M. Feng, “ChartSeer: Interactive steering ex-\nploratory visual analysis with machine intelligence,” IEEE Transactions\non Visualization and Computer Graphics, vol. 28, no. 3, pp. 1500–1513,\n2020.\n[27] S. Zhang, Y . Wang, H. Li, and H. Qu, “AdaVis: Adaptive and explainable\nvisualization recommendation for tabular data,” IEEE Transactions on\nVisualization and Computer Graphics , 2023.\n[28] H. Li, Y . Wang, S. Zhang, Y . Song, and H. Qu, “KG4Vis: A knowledge\ngraph-based approach for visualization recommendation,” IEEE Trans-\nactions on Visualization and Computer Graphics , vol. 28, no. 1, pp.\n195–205, 2021.\n[29] D. Raghunandan, Z. Cui, K. Krishnan, S. Tirfe, S. Shi, T. D. Shrestha,\nL. Battle, and N. Elmqvist, “Lodestar: Supporting independent learning\nand rapid experimentation through data-driven analysis recommenda-\ntions,” arXiv preprint arXiv:2204.07876 , 2022.\n[30] Z. Qu and J. Hullman, “Keeping multiple views consistent: Constraints,\nvalidations, and exceptions in visualization authoring,” IEEE Transac-\ntions on Visualization and Computer Graphics , vol. 24, no. 1, pp. 468–\n477, 2017.\n[31] M. Sun, A. Namburi, D. Koop, J. Zhao, T. Li, and H. Chung, “Towards\nsystematic design considerations for visualizing cross-view data rela-\ntionships,” IEEE Transactions on Visualization and Computer Graphics,\nvol. 28, no. 12, pp. 4741–4756, 2021.\n[32] H. Lin, D. Moritz, and J. Heer, “Dziban: Balancing agency & automation\nin visualization design via anchored recommendations,” in Proceedings\nof the SIGCHI Conference on Human Factors in Computing Systems ,\nser. CHI ’20, 2020, pp. 1–12.\n[33] D. Moritz, C. Wang, G. L. Nelson, H. Lin, A. M. Smith, B. Howe,\nand J. Heer, “Formalizing visualization design knowledge as constraints:\nActionable and extensible models in draco,” IEEE Transactions on\nVisualization and Computer Graphics, vol. 25, no. 1, pp. 438–448, 2018.\n[34] Y . Lin, H. Li, A. Wu, Y . Wang, and H. Qu, “DMiner: Dashboard design\nmining and recommendation,” IEEE Transactions on Visualization and\nComputer Graphics, pp. 1–15, 2023.\n[35] A. Wu, Y . Wang, M. Zhou, X. He, H. Zhang, H. Qu, and D. Zhang,\n“MultiVision: Designing analytical dashboards with deep learning based\nrecommendation,” IEEE Transactions on Visualization and Computer\nGraphics, vol. 28, no. 1, pp. 162–172, 2021.\n[36] D. Shi, W. Cui, D. Huang, H. Zhang, and N. Cao, “Reverse-engineering\ninformation presentations: Recovering hierarchical grouping from lay-\nouts of visual elements,” Visual Intelligence, vol. 1, no. 1, p. 9, 2023.\n[37] X. Li, Y . Zhang, J. Leung, C. Sun, and J. Zhao, “EDAssistant: Supporting\nexploratory data analysis in computational notebooks with in situ\ncode search and recommendation,” ACM Transactions on Interactive\nIntelligent Systems, vol. 13, no. 1, pp. 1–27, 2023.\n[38] O. Bar El, T. Milo, and A. Somech, “Automatically generating data\nexploration sessions using deep reinforcement learning,” in Proceedings\nof the 2020 ACM SIGMOD international conference on management of\ndata, 2020, pp. 1527–1537.\n[39] S. M. Casner, “Task-analytic approach to the automated design of\ngraphic presentations,” ACM Transactions on Graphics (ToG) , vol. 10,\nno. 2, pp. 111–151, 1991.\n[40] B. Saket, A. Endert, and C ¸ . Demiralp, “Task-based effectiveness of\nbasic visualizations,” IEEE Transactions on Visualization and Computer\nGraphics, vol. 25, no. 7, pp. 2505–2512, 2018.\n[41] R. Amar, J. Eagan, and J. Stasko, “Low-level components of analytic ac-\ntivity in information visualization,” in Proceddings of IEEE Symposium\non Information Visualization . IEEE, 2005, pp. 111–117.\n[42] D. Gotz and Z. Wen, “Behavior-driven visualization recommendation,”\nin Proceedings of the 14th international conference on Intelligent user\ninterfaces, ser. IUI ’09, 2009, pp. 315–324.\n[43] F. Bouali, A. Guettala, and G. Venturini, “VizAssist: an interactive user\nassistant for visual data mining,” The Visual Computer , vol. 32, pp.\n1447–1463, 2016.\n[44] L. Shen, E. Shen, Z. Tai, Y . Song, and J. Wang, “TaskVis: Task-oriented\nvisualization recommendation,” in EuroVis, 2021.\n[45] A. Pandey, A. Srinivasan, and V . Setlur, “Medley: Intent-based recom-\nmendations to support dashboard composition,” IEEE Transactions on\nVisualization and Computer Graphics , vol. 29, no. 1, pp. 1135–1145,\n2022.\n[46] Y . Tian, W. Cui, D. Deng, X. Yi, Y . Yang, H. Zhang, and Y . Wu,\n“ChartGPT: Leveraging llms to generate charts from abstract natural\nlanguage,” IEEE Transactions on Visualization and Computer Graphics,\npp. 1–15, 2024.\n[47] G. Li, X. Wang, G. Aodeng, S. Zheng, Y . Zhang, C. Ou, S. Wang, and\nC. H. Liu, “Visualization generation with large language models: An\nevaluation,” arXiv preprint arXiv:2401.11255 , 2024.\n[48] Y . Huang, Y . Zhou, R. Chen, C. Pan, X. Shu, D. Weng, and Y . Wu,\n“Interactive table synthesis with natural language,” IEEE Transactions\non Visualization and Computer Graphics , 2023.\n[49] M. M. Hassan, A. Knipper, and S. K. K. Santu, “ChatGPT as your\npersonal data scientist,” arXiv preprint arXiv:2305.13657 , 2023.\n[50] P. Ma, R. Ding, S. Wang, S. Han, and D. Zhang, “Demonstration of\nInsightPilot: An llm-empowered automated data exploration system,”\narXiv preprint arXiv:2304.00477 , 2023.\n[51] X. He, M. Zhou, X. Xu, X. Ma, R. Ding, L. Du, Y . Gao, R. Jia,\nX. Chen, S. Han et al., “Text2Analysis: A benchmark of table question\nanswering with advanced data analysis and unclear queries,” arXiv\npreprint arXiv:2312.13671, 2023.\n[52] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi,\nQ. V . Le, and D. Zhou, “Chain-of-Thought prompting elicits reasoning\nin large language models,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 24 824–24 837, 2022.\n[53] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y . Cao, and\nK. Narasimhan, “Tree of Thoughts: Deliberate problem solving with\nlarge language models,” Advances in Neural Information Processing\nSystems, vol. 36, pp. 11 809–11 822, 2023.\n[54] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi,\nJ. Gajda, T. Lehmann, M. Podstawski, H. Niewiadomski, P. Nyczyk\net al. , “Graph of thoughts: Solving elaborate problems with large\nlanguage models,” arXiv preprint arXiv:2308.09687 , 2023.\n[55] T. Wu, M. Terry, and C. J. Cai, “AI Chains: Transparent and controllable\nhuman-ai interaction by chaining large language model prompts,” in\nProceedings of the SIGCHI Conference on human factors in computing\nsystems, ser. CHI ’22, 2022, pp. 1–22.\n[56] Y . Guo, D. Shi, M. Guo, Y . Wu, N. Cao, and Q. Chen, “Talk2Data: A\nnatural language interface for exploratory visual analysis via question\ndecomposition,” ACM Transactions on Interactive Intelligent Systems ,\n2021.\n[57] M. Q. Wang Baldonado, A. Woodruff, and A. Kuchinsky, “Guidelines\nfor using multiple views in information visualization,” in Proceedings\nof the Working Conference on Advanced Visual Interfaces , ser. A VI ’00,\n2000, pp. 110–119.\n[58] S. Khan, P. H. Nguyen, A. Abdul-Rahman, E. Freeman, C. Turkay,\nand M. Chen, “Rapid development of a data visualization service in\nan emergency response,” IEEE Transactions on Services Computing ,\nvol. 15, no. 3, pp. 1251–1264, 2022.\n[59] R. Ding, S. Han, Y . Xu, H. Zhang, and D. Zhang, “QuickInsights: Quick\nand automatic discovery of insights from multi-dimensional data,” in\nProceedings of the International Conference on Management of Data ,\nser. SIGMOD ’19, 2019, pp. 317–332.\n[60] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer, “Vega-\nLite: A grammar of interactive graphics,” IEEE Transactions on Visual-\nization and Computer Graphics , vol. 23, no. 1, p. 341–350, jan 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 14\n[61] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, “Language models are few-shot learners,” in Advances\nin Neural Information Processing Systems , H. Larochelle, M. Ranzato,\nR. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33, 2020, pp. 1877–1901.\n[62] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao,\n“Reflexion: language agents with verbal reinforcement learning,” in\nProceedings of the 37th International Conference on Neural Information\nProcessing Systems, ser. NIPS ’23, 2024.\n[63] L. Peng, Y . Zhao, Y . Hou, Q. Wang, S. Shen, X. Lai, J. Gao, J. Dong,\nZ. Lin, and S. Chen, “Mixed-initiative visual exploration of social media\ntext and events,” inProceedings of the IEEE Conference on Visualization\nand Visual Analytics , 2021.\n[64] M. Van Someren, Y . F. Barnard, and J. Sandberg, “The think aloud\nmethod: A practical approach to modelling cognitive,” London: Aca-\ndemicPress, vol. 11, no. 6, 1994.\n[65] Z. Ji, T. Yu, Y . Xu, N. Lee, E. Ishii, and P. Fung, “Towards mitigating\nllm hallucination via self reflection,” in Findings of the Association for\nComputational Linguistics: EMNLP 2023 , 2023, pp. 1827–1843.\n[66] F. Bang, “GPTCache: An open-source semantic cache for llm applica-\ntions enabling faster answers and cost savings,” in Proceedings of the\n3rd Workshop for Natural Language Processing Open Source Software ,\n2023, pp. 212–218.\n[67] Y . Qin, S. Liang, Y . Ye, K. Zhu, L. Yan, Y . Lu, Y . Lin, X. Cong, X. Tang,\nB. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu,\nand M. Sun, “ToolLLM: Facilitating large language models to master\n16000+ real-world apis,” arxiv.2307.16789, 2023.\n[68] L. Gao, J. Lu, Z. Shao, Z. Lin, S. Yue, C. Ieong, Y . Sun, R. J.\nZauner, Z. Wei, and S. Chen, “Fine-tuned large language model for\nvisualization system: A study on self-regulated learning in education,”\nIEEE Transactions on Visualization and Computer Graphics , pp. 1–11,\n2024.\n[69] X. Zeng, H. Lin, Y . Ye, and W. Zeng, “Advancing multimodal large lan-\nguage models in chart question answering with visualization-referenced\ninstruction tuning,” IEEE Transactions on Visualization and Computer\nGraphics, pp. 1–11, 2024.\n[70] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\nzalez, “MemGPT: Towards llms as operating systems,” arXiv preprint\narXiv:2310.08560, 2023.\n[71] D. Masson, S. Malacria, G. Casiez, and D. V ogel, “DirectGPT: A\ndirect manipulation interface to interact with large language models,”\nin Proceedings of the 2024 CHI Conference on Human Factors in\nComputing Systems, ser. CHI ’24, 2024.\n[72] P. Vaithilingam, E. L. Glassman, J. P. Inala, and C. Wang, “DynaVis:\nDynamically synthesized ui widgets for visualization editing,” in Pro-\nceedings of the CHI Conference on Human Factors in Computing\nSystems, ser. CHI ’24, 2024.\n[73] L. Battle and J. Heer, “Characterizing exploratory visual analysis: A\nliterature review and evaluation of analytic provenance in tableau,”\nComputer graphics forum , vol. 38, no. 3, pp. 145–159, 2019.\n[74] Y . Liu, T. Althoff, and J. Heer, “Paths Explored, Paths Omitted, Paths\nObscured: Decision points & selective reporting in end-to-end data\nanalysis,” in Proceedings of the SIGCHI Conference on Human Factors\nin Computing Systems , ser. CHI ’20, 2020, pp. 1–14.\nAPPENDIX A\nIMPLEMENTATION DETAILS\nThis appendix introduces prompt examples and a history of\nanalysis during the analysis process.\nA. Task Planning and Execution Algorithm\nHere is the task planning and execution algorithm we\nintroduce in Section IV-A.\nAlgorithm 1 T ask Planning and Execution\n1: function TPE(Goal G, Task T , Data D, History H,\nDecomposition depth k = 1)\n2: if T is empty then\n3: Tnew ← Recommender(G,D)\n4: H ← Update(H,Tnew) ▷ Add new tasks to history\n5: return Tnew,H\n6: else\n7: if k > kmax then return {},False\n8: result ← Executor(T,D)\n9: completed ← Decomposer(T,result) ▷ Evaluate if\nthe task is completed and output score and expla-\nnation\n10: if completed is False then\n11: subtasks,logic ← Decomposer(T,D)\n12: for each Tsub in subtasks do\n13: result[Tsub],H ← TPE(G,Tsub,D,H,k +1)\n14: completed ← logic(result)\n15: return completed ,H\n16: else\n17: Tnew ← Recommender(G,T,D,H)\n18: H ← Update(H,Tnew) ▷ Add new tasks to his-\ntory and update progress of tasks and goal\n19: return Tnew,H\n20: return completed ,H\nThe algorithm aims to achieve a specified goal through\nrecursive task decomposition and execution. Given a Goal G,\nTask T , Data D, History H, and Decomposition depth k, the\nalgorithm initially checks if the task is empty. If so, it calls\na task recommender to generate a new list of tasks, which\nis then updated in the history. If a task T is already defined,\nthe algorithm checks whether the current decomposition depth\nexceeds a maximum depth kmax; if it does, it returns an empty\nresult and a failure flag.\nIf the depth is within limits, the algorithm executes the cur-\nrent task using an executor and then assesses task completion\nwith a decomposer. If the task is incomplete, the decomposer\nfurther breaks down the current task into a list of subtasks,\nwhich are then recursively processed by the algorithm. The\nresults are combined based on a logical structure to determine\noverall task completion. The algorithm updates the history\nwith any newly recommended tasks and finalizes completion\nstatus accordingly.\nB. Prompt Examples\nThis section provides examples of prompts and output\nresults when using LLM in multiple stages, including task\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 15\nrecommendation, visualization generation, insight generation,\nis completed, decompose planner, merge visualization and\nsummarize insights.\nOur prompt template introduced in Section IV-A consists of\nfour abstract components:\n• Input: The data or context provided to the model that is\nnecessary for executing the task. Example: A summary of\nthe data, the task description, and a code template.\n• Instruction: Detailed directions that guide the model on\nhow to perform the task. Example: Instructions to write\nPython code for data analysis and visualization using Altair,\nincluding adding interactive features like brush functions,\ntooltips, and legends.\n• Indicator: Examples or formats that illustrate what the\nexpected output should look like. Example: A partial code\nsnippet or formatted output that helps the model understand\nthe structure and format of the final result.\n• Output: The result generated by the model based on the\ninput and instructions provided. Example: The generated\nPython code, the resulting visualization, and a JSON object\nsummarizing the insights.\n1) Task Recommendation: This stage’s prompt is designed\nto recommend new and previously unexplored tasks based on\nthe goal and the analysis history.\nPrompt Template 4(Propose initial tasks).\nYour need to come up with a short plan to help a user\naccomplish the goal: {goal}. Please recommend {n 0} ex-\nploratory tasks. For task type, you may consider the trend,\ncorrelation, category, distribution, etc., to explore the goal\nfrom different aspects. Do not use advanced modeling\nmethods. At the end of your answer, you should write the\ndata summary and tasks beginning with ```json and ending\nwith ```,. Summary no more than 100 words. Each task is\na sentence. For data variables, the task may not correspond\nto the data, so you need to go to the dataset and find the\ncorresponding data variables, and even if transformation is\nrequired, you need to write the original column name. There\nmay be more than one column of data so it’s an array. No\nneed to say Here is the requested JSON format listing the\ntasks:. Do not add other sentences after this json data.\n1 {\n2 \"tasks\": [\"\",\"\",\"\"],\n3 \"data variables\":[],\n4 \"task type\": [\"\",\"\",\"\"]\n5 }\nPrompt Template 5(Propose new tasks).\nBased on the goal: “My goal is to analyze the fuel effi-\nciency”, previously explored tasks and unexplored tasks,\n(Extracted from analysis history)\nplease recommend 3 new tasks. The dataset named cars.csv\nhas already been uploaded for you.\nFirst, supplement some new tasks for explored tasks by\nconsidering the results of tasks already explored if needed.\nYou just need to record the id in source in your json output,\nif no just leave it none. Every new task has a source.\nSecond, it is recommended that previous unexplored tasks\nbe revisited if they are suitable for analysis at this stage by\nconsidering the explored tasks. You just need to record the\nid in tasks to review in your json output, if no just leave it\nnone.\nYou also need to label a task type for new tasks.\nAt the end of your answer, you should write the tasks\nbeginning with ```json and ending with ```, and the json\ndata includes a list of new tasks and their source, the\nreviewed tasks and insight type will need to be analyzed.\nLike this:\n1 {\n2 \"tasks\": [\"\",\"\",\"\"],\n3 \"source\": [\"\",\"\",\"\"],\n4 \"tasks to review\":[],\n5 \"data variables\":[],\n6 \"task type\": [\"\",\"\",\"\"]\n7 }\nEach task is a sentence.\nAn output example of recommendation.\n```json\n1 {\n2 \"tasks\": [\n3 \"Determine the optimal number of cylinders that\nbalance fuel efficiency and vehicle weight.\",\n4 \"Investigate the impact of year of manufacture on\nthe relationship between weight and fuel\nefficiency.\",\n5 \"Perform a cluster analysis to identify groups of\ncars with similar weight and fuel efficiency\nprofiles.\"\n6 ],\n7 \"source\": [\n8 \"9\",\n9 \"3\",\n10 \"3\"\n11 ],\n12 \"tasks to review\": [\n13 \"4\",\n14 \"5\"\n15 ],\n16 \"task type\": [\n17 \"Optimization\",\n18 \"Historical Analysis\",\n19 \"Cluster Analysis\"\n20 ],\n21 \"data variables\": [\n22 [\"cylinders\", \"MPG\", \"Weight\\ in\\ lbs\"],\n23 [\"year\", \"Weight\\ in\\ lbs\", \"MPG\"],\n24 [\"Weight\\ in\\ lbs\", \"MPG\"]\n25 ]\n26 }\n```\n2) Visualization Generation: The following prompt is the\nvisualization generation part of task execution. By inputting\nthe task and data introduction, the agent generates the visual\nand data analysis code using Python.\nPrompt Template 6(Visualization Generation).\nFor the given task “Evaluate the relationship between vehi-\ncle weight and fuel efficiency.”, data summary,\n(Add a data introduction) you need to write Python codes to\nanalyze the data and solve this task. The dataset cars.csv has\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 16\nbeen read into a dataframe by pandas.read csv(). Just use\nthe variable ‘data’ to refer to this dataframe and do not read\nanother data. After finishing the insight analysis, you should\ncontinue to use Altair to generate the visualization. Ensure\nthat each generated visualization has a brush function that\nallows you to select a subset of the data. Please ensure\nthat only one view is generated. No combination of views\nis required. Please give me the whole code, including data\nanalysis and Altair, in a function called ‘plot(data)’. You\nonly need to give the function definition of plot(data), not\nexecute it.\nplease write the codes in this structure: ```python ```, there\nis an example:\n```python\n1 import altair as alt\n2 import pandas as pd\n3\n4 def plot(data: pd.DataFrame):\n5 # Data preprocessing\n6 <codes>\n7 # Chart generation\n8 chart = alt.Chart().mark bar().encode().properties\n()\n9 return chart\n```\nAn output example of visualization generation.\n```python\n1 import altair as alt\n2 import pandas as pd\n3\n4 def plot(data: pd.DataFrame):\n5 # Ensure correct data types\n6 data = data.astype({\n7 ’Weight_in_lbs’ : float,\n8 ’Miles_per_Gallon’ : float\n9 }).dropna(subset=[’Weight_in_lbs’, ’\nMiles_per_Gallon’]) # Remove rows with\nmissing values\n10\n11 # Create selection features\n12 click = alt.selection multi(fields=[’Name’])\n13 brush = alt.selection(type=’interval’, encodings=[’x\n’])\n14\n15 # Base chart for scatter plot\n16 scatter = alt.Chart(data).mark circle(size=60).encode\n(\n17 x=alt.X(’Weight_in_lbs’, axis=alt.Axis(title=’\nVehicle Weight in lbs’)),\n18 y=alt.Y(’Miles_per_Gallon’, axis=alt.Axis(title\n=’Fuel Efficiency (MPG)’)),\n19 color=alt.condition(brush, ’Origin:N’, alt.value\n(’lightgray’)),\n20 opacity=alt.condition(click, alt.value(0.9), alt.\nvalue(0.2)),\n21 tooltip=[’Name:N’, ’Weight_in_lbs’, ’\nMiles_per_Gallon’, ’Year:T’, ’Origin:N’]\n22 ).add selection(\n23 brush\n24 ).add selection(\n25 click\n26 ).properties(\n27 width=600,\n28 height=400\n29 )\n30\n31 # Base chart for histogram\n32 histogram = alt.Chart(data).mark bar().encode(\n33 x=alt.X(’count()’, axis=alt.Axis(title=’Number\nof Cars’)),\n34 y=alt.Y(’Origin:N’, axis=alt.Axis(title=’\nCountry of Origin’)),\n35 color=alt.Color(’Origin:N’, legend=alt.Legend(\ntitle=\"Origin\")),\n36 opacity=alt.condition(click, alt.value(0.9), alt.\nvalue(0.2)),\n37 ).transform filter(\n38 brush\n39 ).add selection(\n40 click\n41 ).properties(\n42 width=600\n43 )\n44\n45 return alt.vconcat(scatter, histogram)\n```\nIn actual analysis, challenges may arise, such as overly large\ndatasets, unclear meanings of data attribute names, or highly\ndomain-specific data, which may prevent an LLM from fully\nunderstanding the uploaded dataset. Summarizing the dataset\nin natural language and using this as a prompt can help the\nLLM perform the analysis task more effectively.\nThe data introduction includes a natural language descrip-\ntion of the column names of each variable in the data, the\ntypes of variables and some statistical indicators. The given\noutput json format template and some existing descriptions of\nthe data (or not) are passed to LLM as prompts and let it\ngenerate data introduction according to the given format.\nThis is an example that we provide for the cars dataset.\n1 class DataProperty {\n2 column: string; // Column Name\n3 description: string; // Natural language\ndescription\n4 dtype: string; // Data type of the column\n5 std: number | null; // Standard deviation, null if\nnot applicable\n6 samples: string[]; // Sample values from the\ncolumn\n7 nunique: number; // Number of unique values in the\ncolumn\n8 min: string; // Minimum value in the column\n9 max: string; // Maximum value in the column\n10 }\n1 {\n2 \"columns\": [\n3 \"Name\",\n4 \"Miles per Gallon\",\n5 \"Cylinders\",\n6 \"Displacement\",\n7 \"Horsepower\",\n8 \"Weight in lbs\",\n9 \"Acceleration\",\n10 \"Year\",\n11 \"Origin\"\n12 ],\n13 \"properties\": [\n14 {\n15 \"column\": \"Name\",\n16 \"description\": \"The make and model of the car.\"\n,\n17 \"properties\": {\n18 \"dtype\": \"object\",\n19 \"std\": null,\n20 \"samples\": [\n21 \"ford galaxie 500\",\n22 \"oldsmobile delta 88royale\",\n23 \"ford mustang ii\"\n24 ],\n25 \"nunique\": 311,\n26 \"min\": \"amc ambassador brougham\",\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 17\n27 \"max\": \"vw rabbit custom\"\n28 }\n29 },\n30 {\n31 \"column\": \"Miles per Gallon\",\n32 \"description\": \"Fuel efficiency of the car,\nmeasured in miles per gallon (MPG).\",\n33 \"properties\": {\n34 \"dtype\": \"number\",\n35 \"std\": 7.8159843125657815,\n36 \"samples\": [\n37 19.8,\n38 18.0,\n39 16.0\n40 ],\n41 \"nunique\": 129,\n42 \"min\": 9.0,\n43 \"max\": 46.6\n44 }\n45 }\n46 ]\n47 }\n3) Insight Generation: The following prompt is the insight\ngeneration part of task execution. Based on the python codes\nin visualization generation,the agent generate the structured\ninsight with examples.\nPrompt Template 7(Insight Generation).\nPlease provide insight for this task: “Evaluate the relation-\nship between vehicle weight and fuel efficiency.” with the\ninitial visualization and analysis codes. Based on previous\ndata transformation process, you need to conduct insight\nanalysis further to report an important insight. Before gen-\nerating natural language insight text, you need to specify the\ncolumn names (data variables) and data values in the data\ninvolved in the insight, as well as the insight type. Improve\ninsight text based on these. Note that if data variables are\nused in insight text, use the original column names directly.\nHere is an example for your output:\n1 {\n2 \"text\": \"There is a correlation between A and B,\nindicating xx generally exhibit lower B for xx in\nC.\",\n3 \"type\": \"Correlation\",\n4 \"parameters\": \"negative\",\n5 \"data variables\": [‘A’, ‘B’],\n6 \"data values\": {\n7 ‘A’: ‘2000 to 5000lbs’,\n8 ‘B’: ‘10 to 35MPG’,\n9 ‘C’: ‘xx’\n10 }\n11 }\nAn output example of insight generation.\n1 {\n2 \"text\": \"There is a strong negative correlation between\nWeight in lbs and Miles per Gallon, indicating\nheavier cars generally exhibit lower\nMiles per Gallon.\",\n3 \"type\": \"Correlation\",\n4 \"parameters\": \"negative\",\n5 \"data variables\": [\n6 \"Weight in lbs\",\n7 \"Miles per Gallon\"\n8 ],\n9 \"data values\": {\n10 \"Weight in lbs\": \"1613 to 5140lbs\",\n11 \"Miles per Gallon\": \"9 to 46.6 MPG\"\n12 }\n13 }\n4) Merge Visualization: For the different selected views,\ngiven generates altair and vega-lite codes for these views,\nmerges these views and adds interactive functions between\nviews according to the given interaction rules.\nPrompt Template 8(Merge visualization).\nThere are some Altair codes:\n(Add visualization codes)\nfor creating visualizations, please combine them into one\nchart and add interactive features. Do not change the\nnumber of charts in the codes list. Just use the variable\n’data’ to refer to this dataframe and do not read another\ndata. You should continue use altair to generate linked\nvisualization using given charts. I want these charts can\nbe brushed to filter each other. Please give me the whole\ncodes including data analysis and chart part in a function\n‘plot(data)’. You only need to give the function definition\nof plot(data), not execute it. please write the codes in this\nstructure:```python ```”\nAn example of generated multiple linked-view.\n1 import altair as alt\n2 import pandas as pd\n3\n4 def plot(data: pd.DataFrame):\n5 # Data Analysis part for the first chart\n6 data[’issue_date’] = pd.to datetime(data[’\nissue_date’])\n7 # Calculate total quantity of trading activity\nper day\n8 trading activity = data.groupby(’issue_date’).agg(\ntotal quantity=(’quantity’, ’sum’)).\nreset index()\n9 # Calculate percentage change in total\nquantity from the previous day\n10 trading activity[’quantity_change’] =\ntrading activity[’total_quantity’].pct change\n()\n11\n12 # Data Analysis part for the second chart\n13 # Group by issue_date and calculate\ndescriptive statistics for ’amount’\n14 stats = data.groupby(’issue_date’)[’amount’].\ndescribe().reset index()\n15 stats = stats.melt(id vars=[’issue_date’],\nvalue vars=[’min’, ’25%’, ’50%’, ’75%’, ’max\n’],\n16 var name=’statistic’, value name=’\namount’)\n17\n18 # Create a shared selection for interactivity\n19 shared brush = alt.selection interval(encodings=[’x’\n])\n20\n21 # Create the first chart: Trading Activity\nOver Time\n22 chart1 = alt.Chart(trading activity).mark line().\nencode(\n23 x=alt.X(’issue_date:T’, axis=alt.Axis(title=’\nIssue Date’)),\n24 y=alt.Y(’total_quantity:Q’, axis=alt.Axis(title\n=’Total Quantity’)),\n25 tooltip=[’issue_date:T’, ’total_quantity:Q’]\n26 ).properties(\n27 title=’Trading Activity Over Time’,\n28 width=600,\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 18\n29 height=400\n30 ).add selection(\n31 shared brush\n32 )\n33\n34 # Create the second chart: Distribution of\nTransaction Amounts\n35 bar chart = alt.Chart(stats).mark bar().encode(\n36 x=alt.X(’statistic:N’, axis=alt.Axis(title=’\nStatistic’)),\n37 y=alt.Y(’amount:Q’, axis=alt.Axis(title=’Amount\n’)),\n38 color=alt.Color(’issue_date:T’, legend=None),\n39 tooltip=[’issue_date:T’, ’statistic’, ’amount\n’]\n40 ).properties(\n41 title=’Distribution of Transaction Amounts’,\n42 width=200,\n43 height=400\n44 ).transform filter(\n45 shared brush\n46 )\n47\n48 # Combine the charts using hconcat\n49 combined chart = alt.hconcat(chart1, bar chart).\nresolve scale(color=’shared’)\n50\n51 return combined chart\n5) Is Completed: Based on the task content and the insights\ngenerated in the previous step, the agent scores the task accord-\ning to the degree of completion and gives explanations.\nPrompt Template 9(Is completed).\nFor the given task: “Evaluate the relationship between\nvehicle weight and fuel efficiency.”, and some derived\ninsights: “There is a strong negative correlation between\nWeight in lbs and Miles per Gallon, indicating heavier\ncars generally exhibit lower Miles per Gallon.” and codes\nto solve this task.\nYou need to judge whether the task requires further analysis.\nThe complexity of the task can be considered from data,\ndata mining, and visualization methods. If the task appears\nincomplete, it needs to be decomposed further. Please rate\nthe task from 1-10. For example, if the initial solution\nadequately segments the data or if the task requires ad-\nvanced statistical analysis. You should output a score and\nexplanation of no more than 20 words for your evaluation.\nYou should write the result beginning with ```json and\nending with ```, and the json data includes a score and\nan explanation. The output structure is like:\n1 {\n2 \"score\": \"\",\n3 \"explanation\": \"\"\n4 }\nThe output is:\n1 {\n2 \"score\": 7/10,\n3 \"explanation\": \"Metadata extracted and relationship\nidentified, but further segmentation and in−depth\nanalysis could reveal more insights.\"\n4 }\n6) Whether Decompose: The agent determines whether the\ntask needs to be decomposed based on the task’s completion\nscore and explanation. If so, it returns the decomposition plan,\notherwise it returns empty.\nPrompt Template 10(Decompose Planner).\nFor the given goal: “My goal is to analyze the fuel effi-\nciency,” and task: “Evaluate the relationship between vehicle\nweight and fuel efficiency.”, the generated derived some\ninsights: “There is a strong negative correlation between\nWeight in lbs and Miles per Gallon, indicating heavier\ncars generally exhibit lower Miles per Gallon.” and visu-\nalizations, discriminating whether this task requires further\nanalysis, such as using more detailed data segmentation or\nadvanced statistical methods.\nThe completion score of this task is 7/10, with the reasons:\n“Metadata extracted and relationship identified, but further\nsegmentation and in-depth analysis could reveal more in-\nsights.”. If needed, please generate no more than 3 subtasks\nand indicate the methods they use respectively based on\nthis. You should write the tasks begin with ```json and end\nwith ```, and the json data includes a list of tasks like this:\n1 {\n2 \"tasks\": [\"\",\"\",\"\"],\n3 \"execution order\": \"(Task 1 AND Task 2 AND Task 3\n)\",\n4 \"methods\": [\"\",\"\",\"\"],\n5 \"data variables\":[[\"\",\"\"],[],[]],\n6 \"task type\":[\"\",\"\",\"\"]\n7 }\nFor data variables, the task may not correspond to the data,\nso you need to go to the data set and find the corresponding\ndata variables, and even if transformation is required, you\nneed to write the original column. There are two operators\n“AND” and “DOWN” in the execution order to connect\ntasks. “AND” indicates that all subtasks need to complete\nsuccessfully but there is no requirement for execution order,\n“DOWN” indicates that subtasks need to be executed step\nby step in order. Each task is a sentence no more than\n20 words, methods include data segmentation, statistical\nmethods. If do not need to decompose, output {“tasks”:\n‘null’}. No need to say Here is the requested JSON format\nlisting the tasks:.\nAn output example of whether decompose.\n1 {\n2 \"tasks\": [\"Segment data by Origin to compare regional\ndifferences in weight and fuel efficiency\ncorrelations.\",\"Apply linear regression to model the\nrelationship between weight and fuel efficiency.\",\"\nInvestigate impact of Cylinders on fuel efficiency\nwithin different weight categories.\"],\n3 \"execution order\": \"(Task 1 AND Task 2 AND Task 3)\",\n4 \"methods\": [\"Data segmentation\",\"Regression analysis\",\"\nSegmentation analysis\"],\n5 \"data variables\":[[\"Weight in lbs\", \"Miles per Gallon\",\n\"Origin\"],[\"Weight in lbs\",\"Miles per Gallon\"],[\"\nCylinders\", \"Miles per Gallon\", \"Weight Category\"]]\n,\n6 \"task type\":[\"Analysis\",\"Modeling\",\"Investigation\"]\n7 }\nAnother two examples of whether decompose. In both exam-\nples, the ‘tasks’ represent three subtasks decomposed from the\nprevious task, and the execution order is ‘AND’ and ‘DOWN’,\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 19\nwhich means that the decomposition tasks are executed in\nthe following logic and order: the first subtask is executed\nin parallel with the second subtask, i.e., the decomposition\ntask is triggered again when either subtask fails, and then the\nthird subtask is executed in serial order.\n1 {\n2 \"tasks\": [\"Segment data by Origin to compare regional\ndifferences in weight and fuel efficiency.\", \"Apply\nlinear regression to model the relationship between\nweight and fuel efficiency.\", \"Investigate impact of\nCylinders on fuel efficiency within different\nweight categories.\"],\n3 \"execution order\": \"(Task 1 AND Task 2 DOWN Task 3)\"\n4 }\n1 {\n2 \"tasks\": [\"Investigate polynomial relationships between\nweight and MPG for each cylinder category.\", \"Model\ninteraction terms between cylinders, horsepower, and\ndisplacement to assess combined effects on MPG.\", \"\nPerform residual analysis to evaluate model fit and\ndiscover potential outlier influence.\"],\n3 \"execution order\": \"(Task 1 AND Task 2 DOWN Task 3)\"\n4 }\n7) Merge Visualization: For the different selected views,\nthe given generates Altair and Vega-Lite codes for these views,\nmerges these views, and adds interactive functions between\nviews according to the given interaction rules.\nPrompt Template 11(Merge visualization).\nThere are some Altair codes:\n(Add visualization codes)\nfor creating visualizations, please combine them into one\nchart and add interactive features. Do not change the\nnumber of charts in the codes list. Just use the variable\n‘data’ to refer to this dataframe and do not read other\ndata. You should continue to use Altair to generate linked\nvisualization using the given charts. I want these charts can\nbe brushed to filter each other. First, you should ensure\nboth charts contain common key columns with consistent\ndata formats for the selection fields. Second, you should\nadd selections based on the chart type, such as a brush\non the time axis for the line chart, dual brushes for the\nscatter plot, and click interactions for the bar chart. Third,\nthe layout should be no more than three charts in a row.\nPlease give me the whole code, including the data analysis\nand chart part, in a function ‘plot(data)’. You only need to\ngive the function definition of the plot(data), not execute\nit. please write the codes in this structure: ```python ```”\n8) Summarize Insights: Given a batch of the same task,\ndecompose it into subtasks and their insights and execution\norders, and summarize their insights.\nPrompt Template 12(Summarize Insights).\nNow the decomposed sub-tasks for the task “Evaluate the\nrelationship between vehicle weight and fuel efficiency.”\nare executed. The decomposition logic is “(Task 1 AND\nTask 2 AND Task 3)”. You need to give a summary\ninsight for the source task based on insight from each\nsubtask. Here is the results for each sub-task: “There is\na strong negative correlation between Weight in lbs and\nMiles per Gallon, indicating heavier cars generally exhibit\nlower Miles per Gallon.”. Your output should be begin with\n```json and end with ```, like this:\n1 {\"summary insight\":\"\"}.\nThe output example of merge visualization.\n1 {\"summary insight\":\"Vehicle weight negatively impacts\nfuel efficiency, with the greatest effects seen in\nheavier, USA−made cars. A linear model quantified\nthis decrease as −0.0077 MPG per pound. Additionally\n, increasing engine cylinders reduces MPG across all\nweight categories, more so in heavier vehicles.\"}.\nC. Task Flow\nAnalysis history stores the path of user historical explo-\nration. It is a task-driven directed graph flow. Each node\nin the graph represents a task and the edges represent the\nrelationship between tasks. There are three types of nodes:\ngoal, task, subtask, and the edges include Decomposition,\nenlightened, and review. Subtask refers to the node obtained by\ntask decomposition. The edge type connected from the parent\ntask node to the child subtask node is decomposition. The\nedge of recommendation represents the agent recommending\nnew tasks, and the edge of review represents the agent rec-\nommending review of nodes that have been proposed but not\nexplored. We provide an analysis history example.\n1 {\n2 \"nodes\": [\n3 {\n4 \"id\": 1,\n5 \"group\": 1,\n6 \"task\": \"My goal is to analyze influencing factors\nof fuel efficiency\\n\",\n7 \"type\": \"goal\",\n8 \"variables\": null,\n9 \"task type\": null,\n10 \"visualization\": null,\n11 \"insight\": null,\n12 \"refined insight\": null,\n13 \"summary insight\": null,\n14 \"chat text\": \"The dataset contains 406entries of\nvehicle data, including name, MPG, engine\nattributes, weight, acceleration, year, and\norigin. It has some missing values and\nrequires date−time conversion for the ’Year’\ncolumn.\",\n15 \"error message\": null,\n16 \"execution order list\": null,\n17 \"progress\": 0.5,\n18 \"codes\": null\n19 },\n20 {\n21 \"id\": 2,\n22 \"group\": 1,\n23 \"task\": \"Analyze the distribution of fuel\nefficiency across the dataset.\",\n24 \"type\": \"task\",\n25 \"variables\": [\n26 \"Miles per Gallon\"\n27 ],\n28 \"task type\": \"distribution\",\n29 \"visualization\": null,\n30 \"insight\": null,\n31 \"refined insight\": null,\n32 \"summary insight\": null,\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 20\n33 \"chat text\": null,\n34 \"error message\": null,\n35 \"execution order list\": null,\n36 \"progress\": 0.0,\n37 \"codes\": null\n38 },\n39 {\n40 \"id\": 3,\n41 \"group\": 0,\n42 \"task\": \"Evaluate the relationship between vehicle\nweight and fuel efficiency.\",\n43 \"type\": \"task\",\n44 \"variables\": [\n45 \"Weight in lbs\",\n46 \"Miles per Gallon\"\n47 ],\n48 \"task type\": \"correlation\",\n49 \"visualization\": {\n50 \"$schema\": \"https://vega.github.io/schema/vega−\nlite/v5.15.1.json\",\n51 \"config\": {\n52 \"view\": {\n53 \"continuousHeight\": 300,\n54 \"continuousWidth\": 300\n55 }\n56 },\n57 \"data\": {\n58 \"url\": \"http://127.0.0.1:5000/data/data 507d1\neff−14b2−426d−b87a−b8cf18c9c741.json\"\n59 },\n60 \"height\": 400,\n61 \"layer\": [\n62 {\n63 \"encoding\": {\n64 \"color\": {\n65 \"field\": \"Origin\",\n66 \"legend\": {\n67 \"title\": \"Origin\"\n68 },\n69 \"type\": \"nominal\"\n70 },\n71 \"opacity\": {\n72 \"condition\": {\n73 \"param\": \"param 3\",\n74 \"value\": 0.9\n75 },\n76 \"value\": 0.2\n77 },\n78 \"tooltip\": [\n79 {\n80 \"field\": \"Name\",\n81 \"type\": \"nominal\"\n82 },\n83 {\n84 \"field\": \"Weight in lbs\",\n85 \"type\": \"quantitative\"\n86 },\n87 {\n88 \"field\": \"Miles per Gallon\",\n89 \"type\": \"quantitative\"\n90 },\n91 {\n92 \"field\": \"Year\",\n93 \"type\": \"temporal\"\n94 },\n95 {\n96 \"field\": \"Origin\",\n97 \"type\": \"nominal\"\n98 }\n99 ],\n100 \"x\": {\n101 \"axis\": {\n102 \"title\": \"Vehicle Weight (lbs)\"\n103 },\n104 \"field\": \"Weight in lbs\",\n105 \"type\": \"quantitative\"\n106 },\n107 \"y\": {\n108 \"axis\": {\n109 \"title\": \"Fuel Efficiency (MPG)\n\"\n110 },\n111 \"field\": \"Miles per Gallon\",\n112 \"type\": \"quantitative\"\n113 }\n114 },\n115 \"mark\": {\n116 \"size\": 60,\n117 \"type\": \"circle\"\n118 },\n119 \"name\": \"view 3\"\n120 },\n121 {\n122 \"encoding\": {\n123 \"color\": {\n124 \"field\": \"Origin\",\n125 \"legend\": {\n126 \"title\": \"Origin\"\n127 },\n128 \"type\": \"nominal\"\n129 },\n130 \"opacity\": {\n131 \"condition\": {\n132 \"param\": \"param 3\",\n133 \"value\": 0.9\n134 },\n135 \"value\": 0.2\n136 },\n137 \"size\": {\n138 \"condition\": {\n139 \"test\": {\n140 \"not\": {\n141 \"param\": \"param 3\"\n142 }\n143 },\n144 \"value\": 30\n145 },\n146 \"value\": 60\n147 },\n148 \"tooltip\": [\n149 {\n150 \"field\": \"Name\",\n151 \"type\": \"nominal\"\n152 },\n153 {\n154 \"field\": \"Weight in lbs\",\n155 \"type\": \"quantitative\"\n156 },\n157 {\n158 \"field\": \"Miles per Gallon\",\n159 \"type\": \"quantitative\"\n160 },\n161 {\n162 \"field\": \"Year\",\n163 \"type\": \"temporal\"\n164 },\n165 {\n166 \"field\": \"Origin\",\n167 \"type\": \"nominal\"\n168 }\n169 ],\n170 \"x\": {\n171 \"axis\": {\n172 \"title\": \"Vehicle Weight (lbs)\"\n173 },\n174 \"field\": \"Weight in lbs\",\n175 \"type\": \"quantitative\"\n176 },\n177 \"y\": {\n178 \"axis\": {\n179 \"title\": \"Fuel Efficiency (MPG)\n\"\n180 },\n181 \"field\": \"Miles per Gallon\",\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 21\n182 \"type\": \"quantitative\"\n183 }\n184 },\n185 \"mark\": {\n186 \"size\": 60,\n187 \"type\": \"circle\"\n188 }\n189 }\n190 ],\n191 \"params\": [\n192 {\n193 \"name\": \"param 3\",\n194 \"select\": {\n195 \"encodings\": [\n196 \"x\"\n197 ],\n198 \"type\": \"interval\"\n199 },\n200 \"views\": [\n201 \"view 3\"\n202 ]\n203 },\n204 {\n205 \"name\": \"param 4\",\n206 \"select\": {\n207 \"fields\": [\n208 \"Name\"\n209 ],\n210 \"type\": \"point\"\n211 },\n212 \"views\": [\n213 \"view 3\"\n214 ]\n215 }\n216 ],\n217 \"resolve\": {\n218 \"scale\": {\n219 \"color\": \"independent\",\n220 \"size\": \"independent\"\n221 }\n222 },\n223 \"width\": 600\n224 },\n225 \"insight\": {\n226 \"text\": \"There is a strong negative correlation\nbetween Weight in lbs and\nMiles per Gallon, indicating heavier cars\ngenerally exhibit lower Miles per Gallon\n.\",\n227 \"type\": \"Correlation\",\n228 \"parameters\": \"negative\",\n229 \"data variables\": [\n230 \"Weight in lbs\",\n231 \"Miles per Gallon\"\n232 ],\n233 \"data values\": {\n234 \"Weight in lbs\": \"1613 to 5140lbs\",\n235 \"Miles per Gallon\": \"9 to 46.6 MPG\"\n236 }\n237 },\n238 \"refined insight\": null,\n239 \"summary insight\": \"Vehicle weight negatively\nimpacts fuel efficiency, with the greatest\neffects seen in heavier, USA−made cars. A\nlinear model quantified this decrease as −0.00\n77 MPG per pound. Additionally, increasing\nengine cylinders reduces MPG across all weight\ncategories, more so in heavier vehicles.\",\n240 \"chat text\": null,\n241 \"error message\": \"null\",\n242 \"execution order list\": [\n243 \"AND\",\n244 \"DOWN\"\n245 ],\n246 \"progress\": 1,\n247 \"codes\": \"import altair as alt\\nimport pandas as\npd\\n\\ndef plot(data: pd.DataFrame):\\n #\nEnsure correct data types\\n data = data.\nastype({\\n ’Weight in lbs’: float,\\n ’\nMiles per Gallon’: float\\n }).dropna(subset=[\n’Weight in lbs’, ’Miles per Gallon’]) #\nRemove rows with missing values\\n\\n # Create\nselection features\\n click = alt.\nselection multi(fields=[’Name’])\\n brush =\nalt.selection(type=’interval’, encodings=[’x’]\n)\\n\\n # Base chart for scatter plot\\n scatter\n= alt.Chart(data).mark circle(size=60).\nencode(\\n x=alt.X(’Weight in lbs’, axis=alt.\nAxis(title=’Vehicle Weight in lbs’)),\\n y=alt\n.Y(’Miles per Gallon’, axis=alt.Axis(title=’\nFuel Efficiency (MPG)’)),\\n color=alt.\ncondition(brush, ’Origin:N’, alt.value(’\nlightgray’)),\\n opacity=alt.condition(click,\nalt.value(0.9), alt.value(0.2)),\\n tooltip=[’\nName:N’, ’Weight in lbs’, ’Miles per Gallon’\n, ’Year:T’, ’Origin:N’]\\n ).add selection(\\n\nbrush\\n ).add selection(\\n click\\n ).\nproperties(\\n width=600,\\n height=400\\n )\\n\\n\n# Base chart for histogram\\n histogram = alt\n.Chart(data).mark bar().encode(\\n x=alt.X(’\ncount()’, axis=alt.Axis(title=’Number of Cars\n’)),\\n y=alt.Y(’Origin:N’, axis=alt.Axis(\ntitle=’Country of Origin’)),\\n color=alt.\nColor(’Origin:N’, legend=alt.Legend(title=\\\"\nOrigin\\\")),\\n opacity=alt.condition(click,\nalt.value(0.9), alt.value(0.2)),\\n ).\ntransform filter(\\n brush\\n ).add selection\n(\\n click\\n ).properties(\\n width=600\\n )\\n\\n\nreturn alt.vconcat(scatter, histogram)\"\n248 },\n249 {\n250 \"id\": 7,\n251 \"group\": 0,\n252 \"task\": \"Segment data by Origin to compare\nregional differences in weight and fuel\nefficiency correlations.\",\n253 \"type\": \"sub task\",\n254 \"variables\": [\n255 \"Weight in lbs\",\n256 \"Miles per Gallon\",\n257 \"Origin\"\n258 ],\n259 \"task type\": \"Analysis\",\n260 \"visualization\": null,\n261 \"insight\": null,\n262 \"refined insight\": null,\n263 \"summary insight\": null,\n264 \"chat text\": null,\n265 \"error message\": null,\n266 \"execution order list\": null,\n267 \"progress\": 0.0,\n268 \"codes\": null\n269 },\n270 {\n271 \"id\": 8,\n272 \"group\": 0,\n273 \"task\": \"Apply linear regression to model the\nrelationship between weight and fuel\nefficiency.\",\n274 \"type\": \"sub task\",\n275 \"variables\": [\n276 \"Weight in lbs\",\n277 \"Miles per Gallon\"\n278 ],\n279 \"task type\": \"Modeling\",\n280 \"visualization\": null,\n281 \"insight\": null,\n282 \"refined insight\": null,\n283 \"summary insight\": null,\n284 \"chat text\": null,\n285 \"error message\": null,\n286 \"execution order list\": null,\n287 \"progress\": 0.0,\n288 \"codes\": null\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 22\n289 }\n290\n291 ],\n292 \"edges\":[\n293 {\n294 \"source\": 1,\n295 \"target\": 2,\n296 \"weight\": 1.0,\n297 \"type\": \"Enlightenment\",\n298 \"labels\": [],\n299 \"differences\": {\n300 \"changes\": {\n301 \"variables change\": {\n302 \"original\": [],\n303 \"new\": [\n304 \"Miles per Gallon\"\n305 ]\n306 },\n307 \"type change\": {\n308 \"original\": [\n309 \"goal\"\n310 ],\n311 \"new\": [\n312 \"task\"\n313 ]\n314 },\n315 \"insight change\": {\n316 \"original\": [],\n317 \"new\": [\n318 \"DISTRIBUTION\"\n319 ]\n320 }\n321 },\n322 \"costs\": {\n323 \"variables cost\": 1.0,\n324 \"type cost\": 1.0,\n325 \"insight cost\": 1.0,\n326 \"total difference cost\": 1.0\n327 }\n328 }\n329 },\n330 {\n331 \"source\": 1,\n332 \"target\": 3,\n333 \"weight\": 1.0,\n334 \"type\": \"Enlightenment\",\n335 \"labels\": [],\n336 \"differences\": {\n337 \"changes\": {\n338 \"variables change\": {\n339 \"original\": [],\n340 \"new\": [\n341 \"Weight in lbs\",\n342 \"Miles per Gallon\"\n343 ]\n344 },\n345 \"type change\": {\n346 \"original\": [\n347 \"goal\"\n348 ],\n349 \"new\": [\n350 \"task\"\n351 ]\n352 },\n353 \"insight change\": {\n354 \"original\": [],\n355 \"new\": [\n356 \"CORRELATION\",\n357 \"CORRELATION\"\n358 ]\n359 }\n360 },\n361 \"costs\": {\n362 \"variables cost\": 1.0,\n363 \"type cost\": 1.0,\n364 \"insight cost\": 1.0,\n365 \"total difference cost\": 1.0\n366 }\n367 }\n368 },\n369 {\n370 \"source\": 3,\n371 \"target\": 4,\n372 \"weight\": 0.2760920571391645,\n373 \"type\": \"Decomposition\",\n374 \"labels\": [],\n375 \"differences\": {\n376 \"changes\": {\n377 \"variables change\": {\n378 \"original\": [\n379 \"Weight in lbs\",\n380 \"Miles per Gallon\"\n381 ],\n382 \"new\": [\n383 \"Weight in lbs\",\n384 \"Miles per Gallon\",\n385 \"Origin\"\n386 ]\n387 },\n388 \"type change\": {\n389 \"original\": [\n390 \"task\"\n391 ],\n392 \"new\": [\n393 \"sub task\"\n394 ]\n395 },\n396 \"insight change\": {\n397 \"original\": [\n398 \"CORRELATION\",\n399 \"CORRELATION\"\n400 ],\n401 \"new\": [\n402 \"CORRELATION\",\n403 \"ANALYSIS\"\n404 ]\n405 }\n406 },\n407 \"costs\": {\n408 \"variables cost\": 0.18350341907227408,\n409 \"type cost\": 1.0,\n410 \"insight cost\": 0.29289321881345254,\n411 \"total difference cost\": 0.276092057139164\n5\n412 }\n413 }\n414 },\n415 {\n416 \"source\": 3,\n417 \"target\": 5,\n418 \"weight\": 0.20000000000000018,\n419 \"type\": \"Decomposition\",\n420 \"labels\": [],\n421 \"differences\": {\n422 \"changes\": {\n423 \"variables change\": {\n424 \"original\": [\n425 \"Weight in lbs\",\n426 \"Miles per Gallon\"\n427 ],\n428 \"new\": [\n429 \"Weight in lbs\",\n430 \"Miles per Gallon\"\n431 ]\n432 },\n433 \"type change\": {\n434 \"original\": [\n435 \"task\"\n436 ],\n437 \"new\": [\n438 \"sub task\"\n439 ]\n440 },\n441 \"insight change\": {\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 23\n442 \"original\": [\n443 \"CORRELATION\",\n444 \"CORRELATION\"\n445 ],\n446 \"new\": [\n447 \"REGRESSION ANALYSIS\",\n448 \"MODELING\"\n449 ]\n450 }\n451 },\n452 \"costs\": {\n453 \"variables cost\": 2.220446049250313e−16,\n454 \"type cost\": 1.0,\n455 \"insight cost\": 1.0,\n456 \"total difference cost\": 0.200000000000000\n18\n457 }\n458 }\n459 },\n460 {\n461 \"source\": 5,\n462 \"target\": 2,\n463 \"weight\": 0.0,\n464 \"type\": \"Review\",\n465 \"labels\": [],\n466 \"differences\": \"Node(s) not found\"\n467 }\n468 ]\n469 }\nAPPENDIX B\nLLM P ERFORMANCE TEST\nThis appendix analyzes the performance of GPT-3.5-turbo\nand GPT-4-turbo on data analysis and visualization tasks. We\ntested 160 tasks across two datasets, focusing on error types\nand model efficiency. The results highlight common error\ncategories and compare the models’ performance in terms of\nerror rates and time costs.\nA. Error Analysis\nTo enhance the robustness of our system, we conducted\ntests focusing on 2 datasets, with 20 tasks per dataset, uti-\nlizing GPT-3.5-turbo and GPT-4-turbo. Each task was tested\nwith data analysis code generation and insight annotation.\nBoth of these two steps tested two prompts to support the\nchoice of better results. This setting resulted in a total of\n2 (datasets) × 10 (tasks per dataset) × 2 (models) × 2\n(codes generation prompts) × 2 (insight annotation prompts)\n= 160 initial results.\nTo improve error handling, we allow the model to self-\ndebug again while we prompt the model with cached error\nmessages, yielding an overall 204 experimental results. In the\nfirst round, out of the 160 tasks, 44 errors were identified.\nUpon a second verification, 18 of these errors were resolved,\nwhile 26 remained erroneous.\n1) Error Types: We classified these 70 (44 +26) errors we\nfound into five categories, as illustrated in Fig. 8.\n• Unfamiliar Dataset: These errors highlight the model’s\nlack of understanding regarding the structure and methods\napplicable to the given dataset. The model attempts to use\nnon-existent attributes or incorrect methods, indicating a\ngap in its knowledge about the dataset’s schema and the\noperations that can be performed on it.\nFig. 8. Distribution of error types encountered across various attempts,\ncategorized by whether the issue was resolved in a second attempt or remained\nunsolved. Error types include issues related to unfamiliar datasets, syntax,\nserialization, data transformation, and data binding.\n– ‘DataFrame’ object has no attribute\n‘reset src index’\n– Cannot reset index inplace on a Series to create\na DataFrame\n• Data Binding Issues: These errors occur when the model\nincorrectly references variable names that do not exist in the\ngiven context. This suggests that the model has difficulty\nmaintaining consistency in variable naming and ensuring\nthat all referenced variables are defined and accessible\nwithin the code.\n– name ‘actuator line’ is not defined\n– name ‘hp preusline’ is not defined\n• Serialization Issues: These errors arise when the model\nattempts to use data types that are not compatible with JSON\nserialization, which is required for certain operations such\nas visualization with Altair. These errors indicate that the\nmodel does not adequately understand the requirements for\ndata serialization and the need to use JSON-compatible data\ntypes.\n– Object of type Interval is not JSON serializable\n– Object of type Period is not JSON serializable\n• Data Transformation Issues: These errors indicate the\nmodel’s difficulty in converting data from one type to\nanother and identifying correct data types. The first example\ndemonstrates a failure to convert a string of concatenated\nnames into a numeric format, while the second highlights the\nchallenge of correctly identifying and spelling field names.\n– Could not convert ... to numeric\n– Unable to determine data type for the field\n\"Origin\"\n• Syntax Errors: Syntax errors occur when the model gener-\nates code that does not adhere to the correct syntax rules\nof the programming language. These errors suggest that\nthe model may overlook essential syntax elements such as\nmatching parentheses or correctly structuring the code.\n– invalid syntax (<string>, line 37)\n2) Comparison of Models: Based on the analysis of error\ntype distribution with model comparison. we found that:\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 24\nGPT-3.5-turbo has lower overall error counts : In terms\nof these code generation task types, the overall error rate ratio\nof GPT-3.5-turbo and GPT-4-turbo is 1:2. This indicates that\nGPT-4-turbo commits more errors overall compared to GPT-\n3.5-turbo, as shown by the larger counts of unsolved errors.\nRepair Ability of GPT-4-turbo : Despite the higher error\nrate, GPT-4-turbo shows significant repair ability in the sec-\nond round, solving many errors that were initially unsolved\n(Fig. 9). This repair ability is evident across all error types,\nparticularly in Unfamiliar Dataset and Data Binding Issues .\nFig. 9. Distribution of error types across two models (GPT-4-turbo and GPT-\n3.5-turbo), showing the count of errors resolved in a second attempt or left\nunsolved.\n3) Comparison of Tasks: The bar chart illustrates the rela-\ntionship between various task types and specific error types\nencountered in data analysis and visualization using large\nmodels. The tasks for the tests are shown in Table I.\nFor Correlation Analysis, the primary errors are Data Trans-\nformation Issues and Data Binding Issues , with occasional\nSyntax Errors. In Trend Analysis, the most frequent issues are\nData Transformation Issues, indicating challenges in convert-\ning data formats or structures, and Data Binding Issues, which\npoint to problems in linking data with visualization elements.\nSegmentation Analysis also suffers from Data Transforma-\ntion Issues and Data Binding Issues. For Predictive Modeling,\nsignificant errors include Serialization Issues, suggesting diffi-\nculties in data serialization processes, along with Data Binding\nIssues.\nVariance Analysis encounters substantial Unfamiliar\nDataset issues, highlighting challenges in working with\nnew or unexpected data, and occasional Serialization Issues .\nCluster Analysis is predominantly affected by Unfamiliar\nDataset issues, reflecting problems with novel dataset, and\nsome Data Transformation Issues .\nAggregation Analysis shows minimal errors with an even\ndistribution across different types, while Distribution Analysis\nis mainly affected by Data Binding Issues . Margin Analysis\nexhibits few errors, with Unfamiliar Dataset and Data Binding\nIssues being the most frequent.\nOverall, Unfamiliar Dataset and Data Binding Issues are the\nmost prevalent errors across various task types, underscoring\nthe challenges in handling novel data and effectively linking\ndata with visualization components.\nFig. 10. Distribution of error types across various analytical tasks, highlight-\ning the susceptibility of each task type (e.g., correlation, trend, segmentation)\nto specific errors like syntax issues, data binding problems, and unfamiliar\ndatasets.\nHere’s two generated codes and errors for Trend Analysis\nand For Correlation Analysis:\n• Trend Analysis has the highest occurrence of Data Bind-\ning Issues , suggesting frequent difficulties in linking data\nattributes correctly for trend-related tasks.\n– Line 21: name ’top slowing’ is not defined\n1 import altair as alt\n2 import pandas as pd\n3\n4 def plot(data: pd.DataFrame):\n5 # Data Analysis: Calculate Total Sales and Total\nProfit for each Product\n6 product summary = data.groupby(’Product Name’).agg(\n7 Total Sales=pd.NamedAgg(column=’Sales’, aggfunc=’\nsum’),\n8 Total Profit=pd.NamedAgg(column=’Profit’, aggfunc=\n’sum’),\n9 Total Quantity=pd.NamedAgg(column=’Quantity’,\naggfunc=’sum’)\n10 ).reset index()\n11\n12 # Sort by Total Sales to find Top Selling and\nBottom Selling Products\n13 top selling = product summary.sort values(by=’\nTotal_Sales’, ascending=False).head()\n14 bottom selling = product summary.sort values(by=’\nTotal_Sales’, ascending=True).head()\n15\n16 # Adding annotations: profit for top and bottom\nselling products\n17 top selling[’Annotation’] = ’Top Selling’\n18 bottom selling[’Annotation’] = ’Bottom Selling’\n19\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 25\nTABLE I\nTASKS ANALYSIS : THE TASKS ARE DERIVED FROM THE DATASETS USED IN THE EVALUATION AND PROPOSED BY THE AGENT . THE “Cars” DATASET IS\nANALYZED IN THE FIRST TEN TASKS , WHILE THE “Superstore” DATASET IS ANALYZED IN THE SUBSEQUENT TEN TASKS . THE TYPES OF TASKS ARE\nLABELED BY THE LLM.\ndataset task type\nCars Analyze the distribution of fuel efficiency across the dataset. Distribution Analysis\nCars Evaluate the relationship between vehicle weight and fuel efficiency. Correlation Analysis\nCars Compare the average horsepower for vehicles of different origins. Comparative Analysis\nCars Examine how vehicle characteristics have changed over time. Trend Analysis\nCars Assess the distribution of engine cylinders in the dataset. Distribution Analysis\nCars Segment data by Origin to compare regional differences in weight and\nfuel efficiency correlations.\nSegmentation Analysis\nCars Apply linear regression to model the relationship between weight and fuel\nefficiency.\nPredictive Modeling\nCars Investigate the impact of Cylinders on fuel efficiency within different\nweight categories.\nCorrelation Analysis\nCars Analyze the variance in MPG within each cylinder count across weight\ncategories.\nVariance Analysis\nCars Perform a cluster analysis to identify groups of cars with similar weight\nand fuel efficiency profiles.\nCluster Analysis\nSuperstore Analyze the correlation between sales and profit to understand how they\nare related at a product or category level.\nCorrelation Analysis\nSuperstore Aggregate sales by category and sub-category to identify which product\ngroups are performing best.\nAggregation Analysis\nSuperstore Examine the distribution of sales and profit to determine common ranges\nand any outliers or anomalies in the data.\nDistribution and Outlier Analysis\nSuperstore Compare average discount rates across different segments (Consumer,\nCorporate, Home Office) to assess the impact of discounts on sales and\nprofit.\nCorrelation Analysis\nSuperstore Segment monthly sales data by product categories. Segmentation Analysis\nSuperstore Analyze sales trends of top-selling and bottom-selling products. Trend Analysis\nSuperstore Perform cluster analysis to group similar products based on sales trends\nand profitability.\nCluster Analysis\nSuperstore Investigate the impact of promotions or discounts on monthly sales trends\nby product category.\nCorrelation Analysis\nSuperstore Determine the product groups with the highest profit margin within each\ncategory.\nMargin Analysis\nSuperstore Analyze the impact of seasonal trends on profit margins by category and\nsub-category.\nTrend Analysis\n20 # Combine the data for top and bottom selling\nfor visualization\n21 annotated data = pd.concat([top slowing,\nbottom selling], axis=0)\n22\n23 # Chart part\n24 brush = alt.selection interval(encodings=[’x’, ’y’],\nname=’Brush select’)\n25 chart = alt.Chart(annotated data).mark bar().encode(\n26 x=alt.X(’Product Name:N’, sort=None, title=’\nProduct Name’),\n27 y=alt.Y(’Total_Sales:Q’, title=’Total Sales’),\n28 color=alt.Color(‘Annotation:N’, legend=alt.\nLegend(title=‘‘Product Category\")),\n29 tooltip=[‘Product Name’ , ‘Total Sales’, ‘\nTotal_Profit’, ‘Total Quantity’]\n30 ).add_selection(\n31 brush\n32 ).transform_filter(\n33 brush\n34 )\n35 return chart\n• Correlation Analysis frequently encounters Syntax Errors\nand Data Transformation Issues , indicating common prob-\nlems with code syntax and data type handling in these tasks.\n– invalid syntax (<string>, line 37)\n1 import altair as alt\n2 import pandas as pd\n3\n4 def plot(data: pd.DataFrame):\n5\n6 # Data Analysis part\n7 # Perform linear regression using Altair’s\ntransform_regression\n8 regression = alt.transform regression(\n9 ‘Weight in lbs’, ‘Miles_per_Gallon’, method\n=‘‘linear\"\n10 )\n11\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 26\n12 # Chart part\n13 brush = alt.selection_interval(encodings=[’x\n’, ’y’], name=’brush’)\n14\n15 # Points for the scatter plot - brushable\n16 points = alt.Chart(data).mark_point(color=’\nblue’).encode(\n17 x=’Weight_in_lbs:Q’,\n18 y=’Miles_per_Gallon:Q’,\n19 tooltip=[’Name’, ’Weight_in_lbs’, ’\nMiles_per_Gallon’]\n20 ).add_selection(\n21 brush\n22 ).transform_filter(\n23 brush\n24 )\n25\n26 # Line for linear regression - highlighted by\nbrush\n27 trendline = alt.Chart(data).\ntransform_regression(\n28 ’Weight_in_lbs’, ’Miles_per_Gallon’,\nmethod=’linear’\n29 ).mark_line(color=’red’).encode(\n30 x=’Weight_in_lbs:Q’,\n31 y=’Miles_per_Gallon:Q’\n32 ).transform_filter(\n33 brush\n34 )\n35\n36 # Combining both the points and trend line\n37 chart = points + trend aoxnli\n38\n39 return chart\nB. Visualization Annotations\nTo further assess the model’s ability to generate visualiza-\ntions, such as highlighting insights on visualizations, which\nis more difficult than drawing an overview, we tested these\ntwo different situations. By comparing the error rate with or\nwithout annotation requirements, we find that adding annota-\ntions does increase the probability of errors, especially making\nerrors from unfamiliar datasets and data binding.\nFig. 11. Error rates analysis by visualization prompts (annotated or non-\nannotated).\nSpecifically, for data binding issues, there are 12 errors\nwith annotations compared to 4 without, indicating that the\ncomplexity introduced by annotations increases the likelihood\nof errors. Similarly, for unfamiliar datasets, the model makes\n19 errors with annotations but only 5 without, suggesting\nthat annotations make it harder for the model to handle\nunfamiliar data. On the other hand, for data transformation\nissues, the model performs better with annotations, making\nonly 2 errors compared to 8 without. Serialization issues\nshow an equal distribution of 7 errors for both annotated\nand unannotated cases, while syntax errors are slightly higher\nwith annotations (4 errors) compared to without (1 error).\nResults highlight the model’s difficulty in managing additional\nannotation complexity.\nC. Insight Generation\nAmong all 160 tasks, after self-debugging, 134 successfully\ngenerated insight text content that provided some level of\nanalysis of the tasks. However, among these, 18 did not\ngenerate the insight in the required JSON format.\nWe conducted a statistical analysis to compare the success\nrates of generating each key-value pair in the standardized\ninsight dictionaries.\nTo generate the insights, we employed two different ap-\nproaches. The first approach involved generating the insight\nin JSON format first, while the second approach generated\nthe natural language text first. The following are the results of\nthese two different methods.\nFig. 12. The insight attributes generation success rates (e.g., text, type,\nparameters) based on different prompts, with Answer first or Natural Language\n(NL) first.\nD. Time Cost\nTo evaluate the efficiency of the models in terms of process-\ning time, we measured the time costs for both code generation\nand insight generation tasks. From the point of view of time,\nGPT-3.5-turbo is significantly less than 4, which is 1/4 of the\ntime of GPT-4-turbo.\nCode generation time: GPT-3.5-turbo demonstrates con-\nsistent performance with average times of 5.05 seconds for\nthe cars dataset and 5.58 seconds for the superstore dataset,\nand relatively low variances of 1.32 and 2.34, respectively.\nIn contrast, GPT-4-turbo has significantly higher average code\ngeneration times of 20.79 seconds for the cars dataset and\n24.77 seconds for the superstore dataset, with much larger\nvariances of 22.65 and 61.71, respectively, indicating greater\nvariability and potentially less optimization.\nInsight generation time: GPT-3.5-turbo again shows stable\nperformance with average times of 3.36 seconds for the cars\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 27\nFig. 13. Time cost analysis for code and insight generation across different\ndatasets (cars, superstore) and models (GPT-3.5-turbo, GPT-4-turbo), illustrat-\ning the variations in generation time based on model and dataset combination.\ndataset and 3.94 seconds for the superstore dataset, and low\nvariances of 0.44 and 0.70, respectively. On the other hand,\nGPT-4-turbo has much higher average insight generation times\nof 21.39 seconds for the cars dataset and 32.43 seconds for the\nsuperstore dataset, with significantly larger variances of 216.26\nand 558.72, respectively, suggesting substantial variability and\na need for further optimization.\nOverall, GPT-3.5-turbo demonstrates more efficient and\nconsistent performance in both code and insight generation\ntasks across different datasets. GPT-4-turbo, while potentially\nmore powerful, shows higher time costs but greater variability,\nsuggesting it may be more sensitive to the complexity of tasks\nand datasets."
}