{
  "title": "WenLan 2.0: Make AI Imagine via a Multimodal Foundation Model.",
  "url": "https://openalex.org/W3210084825",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2904126237",
      "name": "Nanyi Fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134475493",
      "name": "Zhiwu Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104839185",
      "name": "Yizhao Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146282143",
      "name": "Guoxing Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152485585",
      "name": "Yuqi Huo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097020016",
      "name": "Jingyuan Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096803518",
      "name": "Haoyu Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163477531",
      "name": "Ruihua Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103988188",
      "name": "Xin Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1960361089",
      "name": "Tao Xiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2070768212",
      "name": "Hao Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2768477045",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3177934633",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W3171007011",
    "https://openalex.org/W2981586349",
    "https://openalex.org/W3170928047",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W3170943566",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2134873765",
    "https://openalex.org/W3101821705",
    "https://openalex.org/W2607508964",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2101295242",
    "https://openalex.org/W1980038761",
    "https://openalex.org/W2998388430",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2999659281",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W3105577662",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W3176254219",
    "https://openalex.org/W3156643189",
    "https://openalex.org/W2017561954",
    "https://openalex.org/W3134582802",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3035588244",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W3035454331",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2123046721"
  ],
  "abstract": "The fundamental goal of artificial intelligence (AI) is to mimic the core cognitive activities of human including perception, memory, and reasoning. Although tremendous success has been achieved in various AI research fields (e.g., computer vision and natural language processing), the majority of existing works only focus on acquiring single cognitive ability (e.g., image classification, reading comprehension, or visual commonsense reasoning). To overcome this limitation and take a solid step to artificial general intelligence (AGI), we develop a novel foundation model pre-trained with huge multimodal (visual and textual) data, which is able to be quickly adapted for a broad class of downstream cognitive tasks. Such a model is fundamentally different from the multimodal foundation models recently proposed in the literature that typically make strong semantic correlation assumption and expect exact alignment between image and text modalities in their pre-training data, which is often hard to satisfy in practice thus limiting their generalization abilities. To resolve this issue, we propose to pre-train our foundation model by self-supervised learning with weak semantic correlation data crawled from the Internet and show that state-of-the-art results can be obtained on a wide range of downstream tasks (both single-modal and cross-modal). Particularly, with novel model-interpretability tools developed in this work, we demonstrate that strong imagination ability (even with hints of commonsense) is now possessed by our foundation model. We believe our work makes a transformative stride towards AGI and will have broad impact on various AI+ fields (e.g., neuroscience and healthcare).",
  "full_text": "Towards artiﬁcial general intelligence via a multimodal\nfoundation model\nNanyi Fei1,2,3, Zhiwu Lu1,2, Yizhao Gao1,2, Guoxing Yang1,2, Yuqi Huo2,3, Jingyuan Wen1,2,\nHaoyu Lu1,2, Ruihua Song1,2, Xin Gao4, Tao Xiang5, Hao Sun1,2 and Ji-Rong Wen1,2,3\n1Gaoling School of Artiﬁcial Intelligence, Renmin University of China, Beijing, China\n2Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\n3School of Information, Renmin University of China, Beijing, China\n4Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and\nTechnology, Thuwal, Saudi Arabia\n5Department of Electrical and Electronic Engineering, University of Surrey, Guildford, United Kingdom\nAbstract— The fundamental goal of artiﬁcial intelligence (AI) is to mimic the core cognitive activities of human.\nDespite tremendous success in the AI research, most of existing methods have only single-cognitive ability. To overcome\nthis limitation and take a solid step towards artiﬁcial general intelligence (AGI), we develop a foundation model pre-\ntrained with huge multimodal data, which can be quickly adapted for various downstream cognitive tasks. To achieve\nthis goal, we propose to pre-train our foundation model by self-supervised learning with weak semantic correlation\ndata crawled from the Internet and show that promising results can be obtained on a wide range of downstream tasks.\nParticularly, with the developed model-interpretability tools, we demonstrate that strong imagination ability is now\npossessed by our foundation model. We believe that our work makes a transformative stride towards AGI, from our\ncommon practice of “weak or narrow AI” to that of “strong or generalized AI”.\nIntroduction\nScience ﬁctions and sci-ﬁ ﬁlms, that describe highly intelligent computer minds, robots, or even human-shaped ones,\ncan be said to understand or have primitive cognitive abilities analogous to human intelligence. Since this form of\nhuman-level artiﬁcial intelligence (AI) is too far from reality hitherto, researchers change to set a less ambitious goal of\nachieving artiﬁcial general intelligence (AGI). Despite not being precisely deﬁned, AGI is broadly agreed to have several\nkey features [1] including: (1) matching or exceeding human performance across a broad class of cognitive tasks (e.g.,\nperception, reading comprehension, and reasoning) in a variety of contexts and environments; (2) possessing the ability\nto handle problems quite diﬀerent from those anticipated by its creators; and (3) being able to generalize/transfer the\nlearned knowledge from one context to others. As we can imagine, devising and obtaining an AGI system would not\nonly accelerate the AI research itself, but also beneﬁt a wide range of AI+ ﬁelds including neuroscience, healthcare,\nand biomedicine.\nIn recent years, deep learning [2] has achieved tremendous successes in various AI research areas such as computer\nvision (CV) and natural language processing (NLP). For example, deep residual networks (ResNets) [3] have already\nsurpassed human performance on image classiﬁcation. The language model RoBERTa [4] has also outperformed\nhuman on several natural language understanding tasks of the GLUE benchmark [5]. Relation networks [6] devised by\nDeepMind have achieved super-human performance on a relational reasoning dataset. However, most of existing AI\nadvances only focus on approaching or exceeding human intelligence on single cognitive ability (e.g., image classiﬁcation,\nlanguage understanding, or relational reasoning). To overcome such a limitation and take a solid step to AGI, we\ndevelop a foundation model pre-trained with huge multimodal (visual and textual) data such that it can be quickly\nadapted for a broad class of downstream cognitive tasks.\nOur motivations are two-fold: (1) Foundation models [7] (also well-known as pre-trained models) are established\nbecause they are exactly designed to be adapted (e.g., ﬁnetuned) to various downstream cognitive tasks by pre-training\non broad data at scale. Importantly, foundation models are closely related to two breakthroughs of MIT Technology\nReview’s “10 Breakthrough Technologies 2021” [8]: GPT-3 [9] (a pre-trained language model) and multi-skilled AI.\n(2) Our choice of learning from huge multimodal data is inspired by the fact that most human intelligent behaviors\nare exhibited in a multimodal context using visual-textual content as the primary carrier of knowledge and means of\n1\narXiv:2110.14378v2  [cs.AI]  8 Jun 2022\nmodeling byimage-to-text “translation”\nstrong semantic correlation\nmodeling byimage-text matching\nweak semantic correlation\nimage-textretrievaltext-to-imagegenerationvisual question answering...\nimage encoder\ntext encoder\nperceptionmemoryreasoning...\noccipital lobe\ntemporal lobe\na b\n?language\nvision\ncognitiveabilities\nhuman brain\n[101...1]contrastivelearningadaptation\nBriVL (foundation model)\n[101...1]\nHappy birthday! Make a wish.\nhugeweb datalimitedmanually-annotateddata\ncloser to AGI?\ntext\nimage\nThis is a fruitcakewith a candle.\nBriVL (ours)most other models\nunknownmechanism\nFig. 1: Overarching concept of our BriVL model with weak training data assumption. a . Comparison\nbetween the human brain and our multimodal foundation model BriVL ( Bridging-Vision-and-Language) for coping\nwith both vision and language information. b. Comparison between modeling weak semantic correlation data and\nmodeling strong semantic correlation data.\ncommunication (see Fig. 1a). Indeed, researchers have reported that a subset of neurons in the human medial temporal\nlobe can be selectively activated by representations of a speciﬁc object/scene across diﬀerent sensory modalities (e.g.,\npictures, written names, and spoken names) [10, 11]. Although the mechanism of cross-modal alignment in our brain\nis unknown, this still suggests that human brain neurons are able to process multimodal information and encode\nconcepts into invariant representations. Overall, we believe that pre-training a large-scale multimodal foundation\nmodel is indeed a potential approach to achieving AGI.\nMultimodal (visual and textual) foundation models [12, 13] typically take image-text pairs as input and model\nthe correlation between two diﬀerent modalities in their pre-training data. Although existing multimodal foundation\nmodels have shown promising results on fast learning/transfer and cross-modal understanding tasks, the majority of\nthem [12, 14–18] make the assumption of strong semantic correlation over the input image-text pairs (e.g., image-\ncaption pairs) and expect exact matches between the objects/regions in an image and the words in a piece of text\n(see Fig. 1b). This seriously limits these models’ generalization abilities because the strong semantic correlation\nassumption is often invalid in the real world and multimodal data following this assumption is limited (e.g., only\nmillions of image-caption pairs are collected by years of human annotation). This situation becomes worse when latest\nmultimodal foundation models [12, 17, 19–21] often employ object detectors to obtain meaningful image regions and\nadopt a single-tower network architecture for better modeling the ﬁne-grained region-word matching (i.e., taking the\nconcatenation of image regions and text words as input). These two common practices (i.e., object detectors and the\nsingle-tower architecture) are both computationally costly and thus unsuited for real-world applications. Particularly,\nas for the latter, given a query in cross-modal retrieval (text-to-image or image-to-text), all possible query-candidate\npairs need to be fed into the model to compute matching scores, resulting in large latency in retrieval.\nTo address the above issues, we develop a large-scale multimodal foundation model dubbed Bridging-Vision-and-\nLanguage (BriVL) by self-supervised learning [22–25] from huge multimodal data. Firstly, to build our pre-training\ndata collection, we choose to exploit weak semantic correlation data (see Fig. 1b) available on the Internet without any\nneed of human annotation (i.e., we crawl a total of 650 million image-text pairs from the web). Importantly, such huge\nweak semantic correlation data contains complicated/abstract human emotions and thoughts. Therefore, comparing\nto modeling strong semantic correlation data by direct image-to-text “translation” in previous works [12, 17, 19–\n21], modeling weak semantic correlation data by image-text matching would help us obtain a more cognitive model.\nSecondly, to design our network architecture, since there do not necessarily exist ﬁne-grained region-word matches\nbetween image and text modalities, we drop the time-consuming object detectors and adopt a simple two-tower\narchitecture (instead of the single-tower one), which encodes image and text inputs using two separate encoders (see\nFig. 1a). Note that the two-tower architecture has a clear advantage in eﬃciency during inference, as the embeddings of\ncandidates can be computed and indexed before querying, meeting the latency requirement of real-world applications.\nThirdly, with the advancement of large-scale distributed training techniques [26, 27] and self-supervised learning [22–\n25], learning from huge unannotated multimodal data becomes possible. Speciﬁcally, to model the weak image-text\ncorrelation and learn a uniﬁed semantic space where global-level image/text embeddings are aligned, we devise a\ncross-modal contrastive learning (CL) algorithm, where CL is a special form of self-supervised learning that is initially\n2\ndeveloped in single-modality (i.e., images) [28–31] with the learning objective of keeping the positive samples close\nand pushing away the negative ones. The proposed network and algorithm designs are detailed in Methods.\nAlthough OpenAI CLIP [13] and Google ALIGN [32] are most closely related to our BriVL, there exist two main\ndiﬀerences between BriVL and these two latest models: (1) We follow the weak semantic correlation assumption\nand construct a huge dataset crawled from the Internet, and only pornographic/sensitive data are ﬁltered out in our\ncollected dataset. In contrast, CLIP only keeps image-text pairs with high word frequency (i.e., the long-tail concepts\nare discarded), while ALIGN also ﬁlters its pre-training dataset by some rules (e.g., excluding texts shared by more\nthan 10 images, excluding texts with extremely low word frequency, and excluding texts that are too long or too\nshort). Our dataset thus preserves a data distribution closer to that of the real world. (2) Inspired by the single-modal\ncontrastive learning (CL) algorithm MoCo [29], our BriVL model adopts a momentum mechanism to dynamically\nmaintain queues of negative samples across diﬀerent training batches. In this way, we have a large negative sample size\n(crucial for CL) while using a relatively small batch size (to reduce GPU memory footprint). On the contrary, both\nCLIP and ALIGN use negative samples within each training batch, requiring a large batch size (i.e., a great demand\nfor GPU memories/resources). More technical diﬀerences can be found in Methods.\nWe conduct extensive experiments on various downstream cognitive tasks (e.g., news classiﬁcation in single-modal\nand visual question answering in cross-modal) and show that our foundation model BriVL achieves promising results,\ndemonstrating its cross-modal understanding ability and cross-domain learning/transfer ability. Although our BriVL is\nonly pre-trained with an image-text matching learning objective, its strong generalization ability has already satisﬁed\nsome of the key features that an AGI system should have. Importantly, with a couple of model-interpretability tools\ndeveloped in this work, we manage to visually reveal how a multimodal foundation model reasonably and logically\nimagines when words or sentences are told, showing that our BriVL exhibits strong imagination ability. A closer\nexamination reveals that the possession of strong imagination is mainly due to the fact that our BriVL leverages weak\nsemantic correlation data in large-scale multimodal pre-training. Overall, these ﬁndings indicate that pre-training a\nmultimodal (visual and textual) foundation model can make a giant stride towards AGI. With more sensory modalities\nexploited for multimodal pre-training and further exploration on more advancing foundation models, we believe that\nwe are approaching AGI and our work will have a broad impact on a variety of AI+ ﬁelds including neuroscience,\nhealthcare, and biomedicine.\nResults\nOur BriVL model has been pre-trained based on a huge weak semantic correlation dataset collected from public web\nsources. The resulting model possesses excellent imagination ability, evidenced by Neural Network Visualization,\nText-to-Image Generation and multiple downstream tasks (i.e., remote sensing scene classiﬁcation, news classiﬁcation,\ncross-modal retrieval, and visual question answering), which are discussed in detail in this section.\nPre-Training Data Collection. We construct a huge web-crawled multi-source image-text dataset called weak\nsemantic correlation dataset (WSCD) as our pre-training data collection. WSCD collects Chinese image-text pairs\nfrom multiple sources on the web, including news, encyclopedia, and social media. Concretely, images from these data\nsources, together with their corresponding/surrounding text descriptions, are used to form image-text pairs. Since the\nobtained image-text pairs are crawled from the web, the image and the text of each pair are expected to be weakly\ncorrelated. For example, an image from social media that contains people having a good time with friends tends\nto have a simple title of “What a nice day!”, without any ﬁner-grained description of the image content. Note that\nwe only ﬁlter out the pornographic/sensitive data from WSCD, without any form of editing/modiﬁcation to the raw\ndata to preserve the natural data distribution. Totally, WSCD has around 650 million image-text pairs covering a\nwide range of topics such as sports, lifestyle, and movie posters. Since WSCD is based on Chinese, English texts of\nall experiments in this section are translated into Chinese for our BriVL. Furthermore, we pre-train our BriVL on\nan English dataset and show results on English tasks in Supplementary Note Fig. S3, indicating that our foundation\nmodel also provides a feasible solution closer to AGI beyond speciﬁc languages.\nNeural Network Visualization. Humans have the ability (or even instinct) that scenes, e.g., in the context of\nimages, come into our minds when we hear words or descriptive sentences. As for our BriVL, once pre-trained on such\na vast amount of loosely aligned image-text pairs (see Methods), we are fascinated by what exactly it would imagine\nwhen texts are given. Instead of examining it indirectly through downstream tasks, we extend Feature Visualization\n(FeaVis) [33] to see the visual responses (i.e., imagination) of BriVL to semantic inputs directly. FeaVis is an algorithm\ndesigned only to visualize the features of convolutional neural networks (CNNs). However, given a large-scale cross-\nmodal foundation model like our BriVL, we can visualize any text input by using the joint image-text embedding\nspace as the bridge. Concretely, we ﬁrst input a piece of text and obtain its text embedding through the text encoder\nof BriVL. Next, we randomly initialize a noisy image and also get an image embedding through the image encoder.\n3\n“nature” “time” “science” “dream”\n“Every cloud hasa silver lining.”“Let life be beautifullike summer flowers.”“The sun goes down belowthemountains, and the YellowRiver flows into the sea.”“A few peach flowersstart to blossomoutsidethe bamboo grove.”\n“mountains with forests”“mountains with stones” “mountains with waterfall”“mountains with snow”\na\nb\nc\n activating LLP-456with “mountains”\nactivating LLP-108with “mountains”\nactivating LLP-678with “mountains”\nactivating LLP-456with “forest”\nactivating LLP-678with “forest”\nactivating LLP-108with “forest”\nd\nFig. 2: Network and neuron visualizations of our BriVL’s imagination. a . Visualizations of the ﬁnal embed-\nding layer of BriVL w.r.t. high-level concepts. b. Visualizations of the ﬁnal embedding layer of BriVL w.r.t. free-form\ntext inputs. c. Visualizations of the ﬁnal embedding layer of BriVL with semantic restrictions related to “mountains\nwith”. d. Visualizations for diﬀerent neurons of BriVL with semantic restrictions “forest” and “mountains”.\nSince the input image is randomly initialized, its embedding does not match that of the input text. We thus deﬁne\nthe objective of matching the two embeddings and back-propagate the resultant gradients to update the input image.\nNote that we do not use any additional module or data for visualization, while the pre-trained BriVL is frozen during\nthe whole process. The ﬁnally obtained image thus depicts a clear picture of what BriVL imagines about the input\ntext. The visualizations of diﬀerent semantic inputs are shown in Fig. 2. Note that the input texts are originally in\nChinese and translated into English for illustration purpose.\nWe ﬁrst present the imagination ability of BriVL to high-level concepts in Fig. 2a. It can be seen that, even though\nthese concepts are rather abstract, the visualizations are able to show concrete embodiment of these concepts (e.g.,\n“nature”: plants like grass; “time”: a clock; “science”: a face with glasses and a conical ﬂask; “dream”: cloud, a\nbridge leading to a door, and the dream-like atmosphere). This ability to generalize an abstract concept to a series\nof more concrete objects is a sign of learned common sense and an indication of the eﬀectiveness of our multimodal\npre-training using only weak semantic correlation data (which expose the model with abstract concepts).\nIn Fig. 2b, we show the imagination of sentences. The visualization of “Every cloud has a silver lining.” not\nonly embodies the sunlight behind dark clouds literally, but also seems to show a dangerous situation on the sea (the\nship-like object and the waves on the left), expressing the implicit meaning of this sentence. In the visualization of\n“Let life be beautiful like summer ﬂowers.”, we can see a ﬂower shrub. The next two text inputs describing more\ncomplicated scenes are both from ancient Chinese poems written with completely diﬀerent grammar from most other\ntexts in the dataset. It seems that BriVL also understands them well: for “A few peach ﬂowers start to blossom\noutside the bamboo grove.”, there are bamboos and pink ﬂowers; for “The sun goes down below the mountains, and\nthe Yellow River ﬂows into the sea.”, we can see mountains with trees hiding the sunset, and a small boat on the river.\nOverall, we ﬁnd that BriVL possesses strong capability of imagination given a complicated sentence as prompt.\nIn Fig. 2c, a few similar text inputs containing a shared prompt are used for network visualization. For “mountains\nwith forests”, there is more green area in the image; for “mountains with stones”, the image is more rocky; for\n“mountains with snow”, the ground turns into white/blue around the trees in the center; for “mountains with waterfall”,\nwe can see blue water falling down with even vapor visible. These imagination results indicate that our model is capable\nof linking speciﬁc objects with more general visual context.\n4\na\nb\nc\n“nature” “surfing” “Every cloud hasa silver lining.”“winter campus”\n“nature” “surfing” “winter campus” “Every cloud hasa silver lining.” “The caterpillar grows upby eating leaves.” “The caterpillar begins tospit out silk and form acocoon to wrap itself up.”\n“The caterpillar breaks outof the cocoon and turnsinto a beautiful butterfly.” “The butterfly fluttershappily among the flowers.”\n“cyberpunk-styled city”“castle in the clouds”“blazing sea”“cosmic sky”\n“ruins of doom”“palace on the moon”“colorful dream”“high-dimensional space”\n“horrible nightmare”“futuristic building”\n“glowing forest”“neural network”\nd\nFig. 3: Text-to-image generation examples of clearer imagination. a . Generation examples of VQGAN\ninversion with CLIP (w/ ResNet-50x4). b. Generation examples of VQGAN inversion with our BriVL. c. A series of\ngeneration examples by VQGAN inversion with our BriVL. d. More generation examples by VQGAN inversion with\nour BriVL, where concepts/scenes are rarely seen by us humans (e.g., “blazing sea” and “glowing forest”) or even do\nnot exist in real life (e.g., “cyberpunk-styled city” and “castle in the clouds”). Note that VQGAN is pre-trained on\nILSVRC-2012. BriVL, CLIP and VQGAN are all frozen during text-to-image generation.\nWe also present the neuron visualization results with semantic constraints in Fig. 2d. Concretely, in addition to\nthe image-text matching loss described above, we select neurons (i.e., channels) in the feature map of the last layer\nbefore the pooling layer (LLP, short for “Last Layer before Pooling”) in our image encoder and maximize the value of\neach neuron. Since each text input may contain many semantic contents, we can see what it is equivalent to activating\none neuron under certain semantic constraint. Three neurons LLP-108, LLP-456, and LLP-678 (the number means\nthe position of each channel in the feature map) are selected for neuron visualization. The two columns in Fig. 2d\nshow the visualizations with text inputs “forest” and “mountains”, respectively. We can clearly see that even with the\nsame semantic constraint, activating diﬀerent neurons leads to diﬀerent imagination results, indicating that each text\ninput has rich semantics with diﬀerent aspects being captured by diﬀerent neurons.\nText-to-Image Generation. Network/neuron visualizations of the imagination are straightforward but sometimes\ncan be hard to interpret. Here, another visualization/interpretability method is developed to make the imagined visual\ncontents of our BriVL better understood by us human. Speciﬁcally, we utilize VQGAN [34] to generate images under\nthe guidance of our BriVL and contrast them with those generated with CLIP [13]. A VQGAN pre-trained on the\nILSVRC-2012 [35] dataset is excellent in generating photorealistic images given a sequence of tokens. Each of such\ntoken is a vector from the pre-trained token set (i.e., codebook) of VQGAN. We ﬁrst randomly sample a sequence\nof tokens, and obtain a generated image from the pre-trained VQGAN. Next, we input the generated image into the\nimage encoder of CLIP/BriVL and also input a piece of text into the text encoder. Finally, we deﬁne the objective\n5\nof matching the image and text embeddings, and back-propagate the resultant gradients to update the initial token\nsequence. Like network/neuron visualization, both VQGAN and CLIP/BriVL are frozen during the generation process.\nThe generated examples are presented in Fig. 3.\nIn Fig. 3a and Fig. 3b, we select four text inputs and show the results obtained by CLIP and our BriVL, respectively.\nCLIP and BriVL both understand the texts well; however, we also observe two major diﬀerences. Firstly, cartoon-\nstyled elements tend to appear in the generated images of CLIP, while images generated by our BriVL are more real\nand natural. Secondly, CLIP tends to simply put elements together while BriVL-generated images are more coherent\nglobally. The ﬁrst diﬀerence may be due to the diﬀerences in the training data used by CLIP and BriVL. The images\nin our training data are crawled from the Internet (most are real photos), while there may be a fair amount of cartoon\nimages in the training data of CLIP. The second diﬀerence lies in the fact that CLIP uses image-text pairs with strong\nsemantic correlation (by word ﬁltering) while we use weakly correlated data. This means that during multimodal\npre-training, CLIP is more likely to learn the correspondence between objects (in images) and words (in texts) while\nBriVL is trying to understand each image with the given text as a whole.\nIn Fig. 3c, we consider a signiﬁcantly more challenging task where a series of images should be generated according\nto multiple coherent sentences. Although each image in Fig. 3c is generated independently, we can observe that all\nfour generated images are visually coherent and of the same style. This ﬁnding demonstrates another advantage of our\nBriVL model: although the environment and background in an image are hard to explicitly mention in the associated\ntext, they are not neglected in our large-scale multimodal pre-training.\nWe present more text-to-image generation examples obtained by VQGAN inversion with our BriVL in Fig. 3d.\nSpeciﬁcally, we choose concepts/scenes rarely seen by us humans (e.g., “blazing sea” and “glowing forest”) or even\nthose not existing in real life (e.g., “cyberpunk-styled city” and “castle in the clouds”). We ﬁnd that our proposed\nmodel can generate images quite consistent with our imagination about the input concepts/scenes, indicating its strong\ngeneralization/imagination ability. This also provides evidence that the superior performance of BriVL is not due to\noverﬁtting the pre-training data since the text inputs here correspond to concepts/scenes that even do not exist in real\nlife. In addition, these generation examples again demonstrate the advantage of pre-training BriVL on weak semantic\ncorrelation data (otherwise the ﬁne-grained region-word matching would harm the imagination ability of BriVL).\nRemote Sensing Scene Classiﬁcation. To show the cross-domain knowledge transfer ability and the out-of-\ndomain imagination ability of our pre-trained BriVL, we conduct zero-shot experiments on two remote sensing scene\nclassiﬁcation benchmarks. The ﬁrst dataset is UC Merced Land-Use (UCM) [36], which has 21 classes and 100 images\nfor each class. The size of each image in UCM is 256 ×256. The second dataset is AID [37], which has 30 classes\nand 10,000 images in total. The size of each image in AID is 600 ×600. AID is a multi-source dataset, which makes\nit more challenging for scene classiﬁcation than the single-source UCM. Concretely, images of each class in AID are\nextracted from diﬀerent countries and regions around the world, and also at diﬀerent times and seasons of the year\nunder diﬀerent imaging conditions. This leads to larger intra-class data diversity in AID. For each dataset, we ﬁrst\nobtain class embeddings by inputting class names into the text encoder of CLIP/BriVL. Then for each test image, we\nobtain its image embedding via the image encoder of CLIP/BriVL, and compute its cosine similarity with each class\nembedding to predict the class that it belongs to. Note that since the class names of these two datasets are all English,\nwe need to translate them into Chinese to ﬁt our BriVL (but the original class names are directly used for CLIP).\nIn the ﬁeld of zero-shot learning (ZSL) [38], datasets typically follow the split of unseen and seen classes. Conven-\ntional ZSL models are thus trained with seen class data and evaluated on unseen class data. Although we do not need\nto train on seen classes, we still follow the common practice and split each dataset with diﬀerent unseen/seen class\nratios (the seen classes are simply not used). Under the split settings where the number of seen classes are not zero,\nwe randomly sample 25 splits and report the standard deviations in brackets along with average accuracy.\nThe zero-shot classiﬁcation results on UCM are shown in the table of Fig. 4a. Our BriVL is compared to a strong\nbaseline ZSSC [39] specially designed for zero-shot remote sensing scene classiﬁcation, and also CLIP with diﬀerent\nCNN backbones. We can see that large-scale cross-modal foundation models achieve far higher rates compared with\nZSSC, indicating their strong cross-domain knowledge transfer abilities. Moreover, our classiﬁcation rates are also\nhigher than those of all CLIP models with diﬀerent CNNs, which is impressive considering the loss in English-to-\nChinese translation and also cultural diﬀerences (CLIP is trained on English data while we use data crawled from\nChinese Internet). Results on another dataset AID are shown in the table of Fig. 4b. Since we did not ﬁnd methods\nconducting ZSL experiments on AID, we only make comparisons with CLIP variations. As we have mentioned, AID is\nmore challenging than UCM, which is also reﬂected by the much worse performance of CLIP variations on AID than\non UCM. However, our BriVL achieves similar performance on the two datasets when evaluated over all data, and the\ngap between BriVL and CLIP is larger on AID than that on UCM. This means that BriVL has stronger generalization\nability and can cope with more complicated situations.\nFurthermore, we deploy the aforementioned network visualization technique to clarify the visual responses of our\nBriVL to remote sensing related concepts. Concretely, we select one class “baseball ﬁeld”, and add the prompt “viewed\n6\nUnseen/Seen Class RatiosMethod21 / 05 / 168 / 1311 / 1014 / 7ZSSC / 58.7 (0.9)35.4 (1.0)19.6 (0.5)15.1 (0.2)CLIP w/ ResNet-50 50.1971.98 (0.10)64.66 (0.09)59.87 (0.06)57.14 (0.05)CLIP w/ ResNet-101 54.8176.84 (0.11)70.52 (0.09)63.61 (0.07)62.01 (0.07)CLIP w/ ResNet-50x456.6776.02 (0.09)71.53 (0.07)64.44 (0.07)63.77 (0.05)BriVL 58.4382.41 (0.08)72.91 (0.07)69.16 (0.05)65.06 (0.06)\nUnseen/Seen Class RatiosMethod30 / 08 / 2212 / 1816 / 1420 / 10CLIP w/ ResNet-50 46.0165.99 (0.08)59.15 (0.05)54.44 (0.05)51.72 (0.04)CLIP w/ ResNet-101 48.0568.71 (0.07)64.39 (0.06)57.75 (0.06)54.54 (0.05)CLIP w/ ResNet-50x450.9669.32 (0.08)64.30 (0.05)59.53 (0.06)56.35 (0.04)BriVL 58.1276.73 (0.09)71.25 (0.07)67.52 (0.06)64.19 (0.04)\nan example of the remotesensing scene “baseball field”\nvisualization of “baseball fieldviewed from above”with BriVL\na c\nb\nFig. 4: Zero-shot remote sensing scene classiﬁcation results. a. Zero-shot accuracies (%) on UCM with diﬀerent\nunseen/seen class ratios. b. Zero-shot accuracies (%) on AID with diﬀerent unseen/seen class ratios. c. Visualizations\nof “baseball ﬁeld”. For a and b, we report standard deviations in brackets over 25 random splits. Highest results in a\nand b are highlighted in bold.\nfrom above” to the class name as the text input. The imagined visual content of our BriVL is shown in Fig. 4c along\nwith one example of this class. We can see that remote sensing scenes are very diﬀerent from traditional photos,\nmainly in the perspective of cameras. Despite this, we can observe from BriVL’s imagination that there is a small\nsector-shaped area (marked with red lines) in “baseball ﬁeld viewed from above”. This provides direct explanation to\nthe impressive performance of our BriVL on remote sensing scene classiﬁcation. In addition, we search the keyword\n“baseball ﬁeld” in our pre-training dataset WSCD and ﬁnd that most of the related images are taken in a normal\ncamera perspective. Given that there is hardly any remote sensing data in our WSCD, this ﬁnding suggests that\nBriVL has somehow learned to generalize transformation of perspectives to unseen domains during pre-training. This\nagain shows the strong imagination ability and even hints of common sense reasoning ability of our BriVL.\nNews Classiﬁcation. To demonstrate how large-scale multimodal learning can beneﬁt single-modal skills and also\nimprove the imagination ability on single-modal tasks, we conduct zero-shot experiments on two Chinese news classiﬁ-\ncation datasets. The ﬁrst dataset is Toutiao News [40], which has 15 classes and a total of around 380K samples. The\nsecond dataset is THUCNews [41], which has 14 classes and around 840K samples in total. Since the contents in these\ntwo datasets are all texts, we only need the text encoder of our BriVL. Concretely, we ﬁrst obtain class embeddings\nby inputting class names into the text encoder. Further, for each piece of news, we only use its title to obtain its\nembedding via the text encoder. Finally, we compute the cosine similarities between each title embedding and class\nembeddings to make predictions.\nThe following methods are chosen for comparison: (1) RoBERTa-base [42]: it is an oﬀ-the-shelf Chinese language\nmodel pre-trained by the original authors on a large Chinese dataset with a total of 5.4B words. (2) RoBERTa-base\n(ﬁnetune): we ﬁnetune the pre-trained RoBERTa-base on a subset of our WSCD dataset (i.e., only the text data\nof 22M image-text pairs is used). (3) BriVL w/ RoBERTa-base: it is a small version of our standard BriVL as we\nreduce the CNN from EﬃcientNet-B7 [43] to EﬃcientNet-B5 and also the text backbone from RoBERTa-large to\nRoBERTa-base. We pre-train this small version with the aforementioned 22M image-text pairs. (4) RoBERTa-large:\nit is the larger version of RoBERTa-base and is also pre-trained by the original authors. Its pre-training data is the\nsame as that of RoBERTa-base. (5) BriVL w/ RoBERTa-large: our standard BriVL pre-trained on the whole WSCD.\nThe zero-shot classiﬁcation results on Toutiao News and THUCNews are shown in Fig. 5a. It can be seen that:\n(1) The results of RoBERTa-base are lower than those of RoBERTa-large, which is expected since the latter has more\nparameters and a larger model capacity. (2) On both datasets, RoBERTa-base (ﬁnetune) has limited performance\ngains over RoBERTa-base, while BriVL w/ RoBERTa-base outperforms RoBERTa-base by large margins. This clearly\nindicates the advantage of cross-modal learning over single-modal learning, given that the ﬁnetuning data of RoBERTa-\nbase (ﬁnetune) and BriVL w/ RoBERTa-base is both from the 22M subset of WSCD. (3) When it comes to RoBERTa-\nlarge, our BriVL w/ RoBERTa-large also leads to much better results than RoBERTa-large.\n7\nMethodToutiaoNewsTHUCNewsRoBERTa-base 36.51 26.61RoBERTa-base (finetune)38.14 28.69BriVLw/ RoBERTa-base52.01 47.53RoBERTa-large 42.55 49.62BriVLw/ RoBERTa-large62.38 58.83\nBriVL w/ RoBERTa-large\nBriVL w/ RoBERTa-large\nRoBERTa-large\nRoBERTa-large\na\nb\nc\nFig. 5: Zero-shot news classiﬁcation results. a . Zero-shot news classiﬁcation results (%) on two Chinese news\ndatasets. b. Zero-shot accuracy gain/loss (%) of BriVL w/ RoBERTa-large comparing to RoBERTa-large on each\ncategory of Toutiao News. c. Top-30 phrase retrieval results of “sports” (top) and “automobile” (bottom) using\nRoBERTa-large and BriVL w/ RoBERTa-large, respectively. The candidate phrase list is obtained from Jieba, which\nconsists of 347,728 Chinese phrases. We translate the results into English for presentation clarity. Highest results in\na are highlighted in bold.\nMoreover, in Fig. 5b, we present the performance gain/loss of our BriVL w/ RoBERTa-large comparing to\nRoBERTa-large on each category of Toutiao News. We can observe that the performance of BriVL decreases only\non 5 categories but increases on the other 10, validating that the single-modal imagination/association ability can be\nimproved by multimodal learning. Further, in Fig. 5c, we show top-30 phrase retrieval results of the category names\n“sports” and “automobile” using these two models to take a closer look. Concretely, we use a Chinese phrase list from\nJieba [44] as the candidate list, which contains 347,728 phrases. Then we obtain the text embeddings of all candi-\ndates using RoBERTa-large and BriVL w/ RoBERTa-large, respectively. For each model and each category name, we\ncompute the category name embedding and retrieve top-30 phrases by comparing it with all candidate embeddings\nusing the cosine similarity. Finally, we visualize the results with the UMAP algorithm [45]. For “sports”, we can see\nthat our BriVL relates it to phrases with a higher variety than RoBERTa-large does. However, for “automobile”, the\nretrieved top-30 phrases of our BriVL are more monotonous.\nCross-Modal Retrieval. Here we conduct experiments on the cross-modal retrieval downstream task, which is\nexactly what we train our BriVL to do. Since our BriVL is pre-trained with Chinese data, we choose the only available\nmultimodal Chinese dataset AIC-ICC [46] for performance evaluation. AIC-ICC is originally designed for image\ncaptioning, which was ﬁrst released in AI Challenger 2017, a competition organized by Sinovation Ventures, Sogou,\nand Toutiao (ByteDance). The training set of AIC-ICC has 300K images and the validation set has 30K images.\nEach image has 5 Chinese captions. Since the test set is not released, we take the ﬁrst 10K images along with their\ncorresponding 50K pieces of texts from the validation set for testing.\nThe cross-modal retrieval results on AIC-ICC are shown in the table of Fig. 6a. The method “BriVL (direct\ntraining)” means that we directly train a randomly-initialized BriVL model on the training set of AIC-ICC rather\nthan using the pre-trained BriVL. Moreover, the results of three “BriVL (pre-train & ﬁnetune)” variations are all\nobtained by ﬁnetuning our pre-trained BriVL on the training set of AIC-ICC with diﬀerent ﬁnetuning strategies.\nWe only consider two ﬁnetuning factors: whether to ﬁx all the batch normalization (BN) layers in the CNN (i.e.,\nEﬃcientNet-B7), and how many blocks should be unﬁxed in the CNN. We perform both image-to-text and text-to-\n8\nImage-to-Text RetrievalText-to-Image RetrievalMethodFix BN# Unfixed BlocksRecall@1Recall@5Recall@10Recall@1Recall@5Recall@10Recall@SUMBriVL(direct training)no 4 36.0359.4869.7128.6654.3365.26317.47BriVL(pre-train & finetune)no 2 43.0565.1174.5732.4957.5367.99342.74BriVL(pre-train & finetune)no 4 44.4967.1475.6333.6758.7668.80 352.49BriVL(pre-train & finetune)yes4 45.6168.0176.3134.0658.8669.09355.94\nQuestion TypeMethodFix BN# Unfixed BlocksWhatWhereWhenWhoWhyHowOverallBriVL(direct training)no 4 70.5171.9981.8877.0578.3668.6272.16BriVL(pre-train & finetune)no 2 79.8981.7187.7884.4882.6676.3180.67BriVL(pre-train & finetune)no 4 79.4181.6687.3184.4683.1174.4480.16BriVL(pre-train & finetune)yes4 77.7980.50 87.9984.4482.4672.8778.96\nWhy is the train blurry?A.Moving fast.B.Bad weather.C.It's raining.D.It's nighttime.BriVL (direct training): CBriVL (pre-train & finetune): A\nWhere was this taken?A.In a zoo.B.In a field.C.On the street.D.At the park.BriVL (direct training): ABriVL (pre-train & finetune): B\nWhat are the boats doing?A.Floating.B.Sailing.C.Getting cleaned.D.Not moving.BriVL (direct training): ABriVL (pre-train & finetune): D\nWhy is the traffic stopped?A.Car accident.B.Traffic.C.Red light.D.Parade.BriVL (direct training): BBriVL (pre-train & finetune): C\na\nb\nc\nFig. 6: Cross-modal retrieval and visual question answering (VQA) results. a . Cross-modal retrieval results\n(%) on the Chinese dataset AIC-ICC. b. VQA results on Visual7W. Overall accuracies (%) along with results on each\nquestion type are reported. The dataset is translated into Chinese. c. VQA examples of our BriVL model regarding\nwhether it is pre-trained to validate the strong imagination ability of our pre-trained BriVL. Highest results in a and\nb are highlighted in bold.\nimage retrieval for evaluation, and report the results with Recall@ k (k = 1,5,10) as well as Recall@SUM (i.e., the\nsummation of six Recall@ k metrics).\nWe can observe from the table of Fig. 6a that image-to-text retrieval results are generally higher than text-to-\nimage ones, which is expected because like us humans, describing a given image is easier than imagining a picture\nfrom a sentence. We can also see that three “BriVL (pre-train & ﬁnetune)” variations achieve far better results than\n“BriVL (direct training)” for all evaluation metrics, indicating the usefulness of large-scale multimodal pre-training.\nIn addition, using pre-trained models like our BriVL is more beneﬁcial to image-to-text retrieval than to text-to-image\nretrieval, which may be due to the fact that image-to-text retrieval is an easier task. From the performance of three\n“BriVL (pre-train & ﬁnetune)” variations, we ﬁnd that diﬀerent ﬁnetuning strategies do aﬀect the ﬁnal results, which\nshould be kept in mind when we ﬁnetune pre-trained models for diﬀerent downstream tasks.\nVisual Question Answering. We consider another multimodal downstream task called visual question answering\n(VQA) [47] to further validate the strong imagination ability of our pre-trained BriVL on the Visual7W dataset [48].\nVisual7W has 47.3K images from MSCOCO [49] and each image comes with a question and four answer candidates,\nwhere only one is the correct answer. The whole dataset can be divided into “Telling” questions and “Pointing” ones.\nSince “Pointing” questions rely on the bounding boxes of objects in images, we only conduct experiments on the\n“Telling” part, which can be further divided into six question types: “What”, “Where”, “When”, “Who”, “Why”,\nand “How”. We randomly make the training and test splits with 70% and 30%, respectively. Since Visual7W is an\nEnglish dataset, we translate all of the questions and answer candidates into Chinese.\nIn the table of Fig. 6b, we report the overall accuracies on the test set, as well as the results on each question type.\nSimilar to the situation on the cross-modal retrieval task, three “BriVL (pre-train & ﬁnetune)” variations achieve\nmuch better results than “BriVL (direct training)” for all question types, again indicating the usefulness of large-scale\npre-training on downstream tasks. We also notice that the best ﬁnetuning strategy for cross-modal retrieval (i.e., ﬁxing\nBN and keeping 4 blocks of the CNN unﬁxed) is no longer the best for VQA. In addition, although the strategy of\nnot ﬁxing BN and keeping 2 blocks unﬁxed obtains the best overall result, it does not achieve the best for all question\n9\ntypes. This is expected as diﬀerent tasks require diﬀerent ﬁnetuning strategies.\nFurthermore, we present four VQA examples in Fig. 6c. From these examples, we see our pre-trained BriVL clearly\nshowing the strong imagination ability and even hints of common sense as it knows that the train in the picture looks\nblurry because it is moving fast, the picture of horses was taken in a ﬁeld rather than in a zoo, the boats being tied\nto the dock are simply not moving instead of ﬂoating, and the traﬃc is stopped because of the red light instead of\ntraﬃc jam. We believe that this is achieved by pre-training with our weak semantic correlation data: the texts are not\ndetailed descriptions of their corresponding images, and thus our BriVL has to ﬁgure out the complicated connections\nhidden among this weak correlation during pre-training. With large pre-training data as much as 650 million, our\nBriVL ﬁnally succeeds in acquiring the ability of reasonably and logically imagining/associating, and also manages to\nlearn some common sense.\nDiscussion\nWe have developed a large-scale multimodal foundation model called BriVL, which is eﬃciently trained on weak\nsemantic correlation dataset (WSCD) consisting of 650 million image-text pairs. We have identiﬁed the direct evidence\nof the aligned image-text embedding space by neural network visualizations and text-to-image generation. In addition,\nwe have visually revealed how a multimodal foundation model understands language and how it makes imagination or\nassociation about words and sentences. Moreover, extensive experiments on other downstream tasks show the cross-\ndomain learning/transfer ability of our BriVL and the advantage of multimodal learning over single-modal learning.\nParticularly, our BriVL appears to acquire abilities in imagination and reasoning. Last but not least, we believe that\nall of these advantages are mainly due to the weak semantic correlation assumption followed by our BriVL. That is,\nby eﬀectively fusing the complex human emotions and thoughts from those weakly correlated image-text pairs, our\nBriVL is made more cognitive and general (i.e., much closer to AGI).\nWe believe that the solid step we take towards AGI would have a broad impact not only on the AI development\ncommunity but also on a wide range of AI+ ﬁelds. For the AI research ﬁeld itself, based on our GPU-resource-\nsaving multimodal pre-training framework, researchers could easily extend our BriVL to a larger capacity with more\nmodalities, leading to more general foundation models. Moreover, with the help of large-scale multimodal foundation\nmodels, it would also be much easier for researchers to explore novel tasks (especially those without abundant human-\nannotated samples). For AI+ ﬁelds, such foundation models could be quickly adapted to speciﬁc working context or\nenvironment, thanks to their strong generalization abilities. For example, in healthcare, multimodal foundation models\ncould make full use of case data in multi-modality (e.g., computed tomography data, and blood routine examination\ndata) to improve the diagnosing accuracy. Moreover, in neuroscience, multimodal foundation models could even help\nﬁnd out the mechanism of how multimodal data connect and fuse since artiﬁcial neural networks are much simpler to\nexamine than real neural systems in human brains.\nNevertheless, multimodal foundation models still face potential risks and challenges. Since the performance of\nfoundation models is based on the data that they are pre-trained on, it is likely that the models learn prejudices and\nstereotypes about certain issues, which should be carefully handled before model training and monitored/addressed\nin downstream applications. Moreover, as foundation models master more and more skills, creators of these models\nshould be aware of model misuse by ill-intentioned people (e.g., manipulating or generating fake contents), which would\nhave a negative inﬂuence on the society. In addition, on the evolution challenges of foundation models academically, it\nis of grand challenge for (1) developing model-interpretability tools deeper into the foundation models, (2) constructing\nhuge pre-training datasets with more modalities, as well as (3) applying foundation models to various downstream\ntasks with more eﬀective adaptation/ﬁnetuning techniques.\nOur understanding of what BriVL (or any large-scale multimodal foundation model) has learned and what it is\ncapable of has only just started. There is still much room for further study to better understand the foundation\nmodel and develop more novel use cases. For instance, since the image can be regarded as a universally-understood\n“language”, soliciting an even larger dataset containing multiple languages could result in a language translation model\nobtained as a by-product of multimodal pre-training. Moreover, additional modalities (e.g., videos and audios) can be\nalso explored to pre-train a more intelligent model, taking us even closer to AGI.\nMethods\nArchitecture Overview. The notion of pre-training a large-scale machine learning model and then using it for\ndownstream tasks ﬁrst appeared in natural language processing (NLP). As shown in Supplementary Note Fig. S1c,\nlarge-scale NLP models like GPT [50], BERT [51], and their variants, take Transformers [52] as text encoders to encode\ninput texts into text embeddings, and then design pre-training objectives on top of these embeddings (e.g., generative\n10\nloss and masked language modeling loss). In computer vision (CV), large-scale pre-training also becomes popular.\nModels like BiT [53] and ViT [54] use convolutional neural networks (CNNs) or Transformers as image encoders to\nobtain embeddings of input images. Similarly, pre-training losses are computed using the image embeddings (e.g.,\nimage classiﬁcation loss and masked image patch prediction loss). However, these models are single-modal and thus\nonly beneﬁt downstream tasks in one modality. For multimodal (e.g., image, text, and audio) pre-training [12–18, 32],\nexisting works can be divided into two groups according to their network architectures: single-tower models (e.g.,\nUNITER [17], OSCAR [12], and M6 [18]) and two-tower ones (e.g., CLIP [13] and ALIGN [32]). Our BriVL can also\nbe categorized into the two-tower models since we use separate image and text encoders. But note that we actually\nadopt two additional momentum encoders to help with the pre-training process (i.e., to dynamically maintain negative\nsample queues across training batches), resulting in a four-tower pre-training architecture.\nThe pre-training goal of our BriVL is to learn two encoders that can embed image and text inputs into the same\nsemantic space for eﬀective image-text retrieval. To enforce the image and text encoders to learn better representations\nin the same embedding space, we introduce cross-modal contrastive learning with the InfoNCE loss [23] into our BriVL.\nSpeciﬁcally, our learning objective is to ﬁnd the corresponding image embedding from a batch of them for a given text\nembedding and vice versa. By maximizing the cosine similarity of the image and text embeddings for each ground-\ntruth pair while minimizing the cosine similarities of the embeddings from negative pairs, we jointly train the image\nand text encoders to learn an aligned cross-modal embedding space.\nFormally, for the image-text retrieval task, we denote the training set as D= {(x(i)\ni ,x(t)\ni )|i = 1,··· ,N}, where\n(x(i)\ni ,x(t)\ni ) is a matched image-text pair, and N is the size of D. Our BriVL leverages contrastive learning by applying\nMoCo [29] into the cross-modal scenario, as illustrated in Supplementary Note Fig. S1a. Each image x(i)\ni (or each text\nx(t)\ni ) is encoded by the image encoder f(i) (or the text encoder f(t)) to obtain its d-dimensional embedding z(i)\ni (or\nz(t)\ni ). The image encoder (see Supplementary Note Fig. S1b) contains a CNN backbone, a successive self-attention\n(SA) block, and a multi-layer perceptron (MLP). A sequence of patch embeddings are ﬁrst obtained by applying multi-\nscale patch pooling (MSPP) to the feature map from CNN. They are then fused/encoded by the SA block. The text\nencoder, on the other hand, is stacked by several SA blocks such as BERT [51] and RoBERTa [4]. A two-layer MLP\nblock with a ReLU [55] activation layer is used for mapping each encoder’s representation into the joint cross-modal\nembedding space. The parameters of f(i) and f(t) are denoted as θ(i) and θ(t), respectively.\nImage Encoder. To obtain better performance in the image-text retrieval task, most previous methods [19–21, 56]\nutilize a bottom-up attention mechanism [57] with object features extracted by the Faster R-CNN detector [58].\nHowever, extracting region/object features with a heavy detector is computationally expensive, e.g., a Faster R-CNN\ndetector typically costs 0.064s (15.6 fps) to extract ﬁne-grained region information from an image of moderate size.\nMeanwhile, the image-text retrieval would be inevitably limited by the detector’s performance, which is not adaptable\nto real-world applications. In this paper, we thus introduce a simple yet eﬀective module named Multi-Scale Patch\nPooling (MSPP) to address this problem.\nFor each input image x(i), we ﬁrst split it into multiple patches at diﬀerent scales and record the patch coordinates.\nIn all experiments, we take two scales as 1 ×1 and 6 ×6, resulting in a total of 37 patches. Next, we project each set\nof patch coordinates onto the feature map that is obtained by the CNN backbone (e.g., EﬃcientNet [43]) and generate\na sequence of 37 region feature maps. Finally, we apply average pooling to each region feature map and obtain a\nsequence of patch features S ∈Rc×Np , where each column corresponds to a patch, Np is the number of patches (i.e.,\nNp = 37 in this paper), and c is the number of channels in the feature map.\nTo better capture the relationship of image patch features, we deploy a self-attention (SA) block containing multiple\nTransformer [52] encoder layers. Each Transformer encoder layer consists of a multi-head attention (MultiHeadAttn)\nlayer and a feed forward network (FFN) layer:\nS′= LayerNorm(S + MultiHeadAttn(S)), (1)\nS = LayerNorm(S′+ FFN(S′)). (2)\nWe then fuse the extracted patch features by applying an average pooling layer:\nr(i) = 1\nNp\nNp∑\nj=1\nSj ∈Rc, (3)\nwhere Sj is the j-th column of S. A two-layer MLP block with a ReLU activation layer is adopted to project r(i) to\nthe joint cross-modal embedding space, resulting in the ﬁnal d-dimensional image embedding z(i) ∈Rd.\n11\nText Encoder. Given a sentence x(t), we ﬁrst tokenize it to obtain a sequence of tokens T = {tj|j = 1,...,l }, where\nl denotes the length of the sentence (e.g., the number of words) and tj denotes the j-th token of T. A pre-trained\nTransformer encoder (e.g., RoBERTa [42]) is then used to map text tokens to a sequence of feature vectors (each\nfeature vector corresponds to a word). Similarly, to better capture the relationship between words, we use the same\nself-attention mechanism as in the image encoder to extract the text representation r(t). A two-layer MLP block with\na ReLU activation layer is also used for mapping the text representation r(t) to the joint cross-modal embedding space,\nresulting in the ﬁnal d-dimensional text embedding z(t) ∈Rd.\nContrastive Loss. The cross-modal contrastive loss in our BriVL is deﬁned based on MoCo [29], which provides\na mechanism of building dynamic sample queues for contrastive learning. Since the two negative queues used in our\nBriVL decouple the queue size from the mini-batch size, we can have a much larger negative sample size than the\nmini-batch size (thus GPU-resource-saving).\nTo maintain large queues of samples coming from diﬀerent mini-batches and address the problem that sample\nfeatures are extracted by encoders with very diﬀerent parameters, we need two more smoothly updated encoders, that\nis, momentum encoders. The parameters θ(i)\nm (or θ(t)\nm ) of the momentum image encoder f(i)\nm (or the momentum text\nencoder f(t)\nm ) are updated in each training iteration with a momentum hyper-parameter m:\nθ(i)\nm = m·θ(i)\nm + (1 −m) ·θ(i), (4)\nθ(t)\nm = m·θ(t)\nm + (1 −m) ·θ(t). (5)\nFurther, we maintain two negative sample queues Q(i) and Q(t), which contain Nq image negatives and Nq text\nnegatives for contrastive learning, respectively. In each pre-training iteration with the batch size Nb, all Nb image\nnegatives and Nb text negatives are separately pushed into these two queues. Meanwhile, there are Nb earliest samples\nbeing popped out of each queue. Concretely, at iteration t, the image and text negatives from the current data batch\n(B(i)\nt ,B(t)\nt ) are computed by respectively forwarding the momentum encoders f(i)\nm and f(t)\nm :\nN(i)\nt =\n{\nf(i)\nm (x(i)\ni )|x(i)\ni ∈B(i)\nt\n}\n, (6)\nN(t)\nt =\n{\nf(t)\nm (x(t)\ni )|x(t)\ni ∈B(t)\nt\n}\n, (7)\nwhere |B(i)\nt |= |B(t)\nt |= Nb. The obtained N(i)\nt and N(t)\nt are then pushed into Q(i) and Q(t), respectively. Note that\nalthough we generally call N(i)\nt (or N(t)\nt ) image negatives (or text negatives), there is still one sample being positive\nto each text (or image). Here, we denote the positive image sample (or text sample) for the j-th input text x(t)\nj (or\nthe j-th input image x(i)\nj ) of the current mini-batch as:\np(i)\nj = f(i)\nm (x(i)\nj ) ∈N(i)\nt , (8)\np(t)\nj = f(t)\nm (x(t)\nj ) ∈N(t)\nt . (9)\nWith the two negative queues, the loss function in each training iteration is thus computed as follows. For each\ninput image x(i)\ni , we deﬁne the contrastive loss between its image embedding z(i)\ni and all positive/negative texts in the\nqueue Q(t) as an InfoNCE loss [23]:\nLi2t = −1\nNb\nNb∑\ni\nlog\nexp\n(\nz(i)\ni ·p(t)\ni /τ\n)\nexp\n(\nz(i)\ni ·p(t)\ni /τ\n)\n+ ∑\nn(t)\nexp\n(\nz(i)\ni ·n(t)/τ\n), (10)\nwhere n(t) ∈Q(t) \\{p(t)\ni }denotes a text negative for each image, τ is the temperature hyper-parameter, and the vector\nsimilarity is measured by dot product ( ·). Similarly, for each input text x(t)\ni , the InfoNCE loss is given by:\nLt2i = −1\nNb\nNb∑\ni\nlog\nexp\n(\nz(t)\ni ·p(i)\ni /τ\n)\nexp\n(\nz(t)\ni ·p(i)\ni /τ\n)\n+ ∑\nn(i)\nexp\n(\nz(t)\ni ·n(i)/τ\n), (11)\nwhere n(i) ∈Q(i) \\{p(i)\ni }denotes an image negative for each text.\nThe total loss function for pre-training our BriVL is then deﬁned as:\nLtotal = Li2t + Lt2i. (12)\nIn the test/evaluation stage, given each query image (or text), the cross-modal retrieval results are obtained simply\nby the dot product deﬁned over the outputs of the text (or image) encoder.\n12\nAlgorithm 1 Neural Network Visualization\nInput: The pre-trained image and text encoders f(i) and f(t) of our BriVL\nA piece of text x(t)\nA randomly initialized image x(i)\nA learning rate parameter λ\nOutput: The updated input image\n1: Obtain the text embedding z(t) = f(t)(x(t));\n2: for all iteration = 1, 2, ···, MaxIteration do\n3: Obtain the image embedding z(i) = f(i)(x(i));\n4: Compute Lvis with Eq. (13);\n5: Compute the gradients ∇x(i) Lvis;\n6: Update x(i) using gradient descent with λ;\n7: end for\n8: return the updated input image.\nImplementation Details. Over the input images, we adopt random graying and random color jittering for data\naugmentation. All images are resized to 600 ×600 pixels. We adopt EﬃcientNet-B7 [43] as the CNN backbone in\nthe image encoder and RoBERTa-Large [42] as the basis Transformer in the text encoder. For both image and text\nencoders, the self-attention block consists of 4 Transformer encoder layers and the MLP block has two fully-connected\nlayers with a ReLU activation layer. The ﬁnal embedding size of the joint cross-modal space is 2,560. We select the\nhyper-parameters heuristically for pre-training our BriVL model due to the computational constraint: the temperature\nhyper-parameter τ = 0.07, momentum m= 0.99, and the queue size Nq = 13,440. We adopt the Adam optimizer [59],\nwith the weight decay 1e-5 and the learning rate 1e-4. We use a mini-batch size of 192 for each of the 14 machines (each\nmachine has 8 NVIDIA A100 GPUs), resulting in a total batch size of 2,688 (far smaller thanNq). The resource-saving\nadvantages of such batch setting are shown by the ablation study results in Supplementary Note Fig. S2. We also\ndeploy the latest distributed-training framework DeepSpeed [26] to accelerate the pre-training process and save the\nGPU memories. With 112 NVIDIA A100 GPUs in total, it takes about 10 days to pre-train our BriVL model over\nour WSCD of 650 million image-text pairs.\nDiﬀerences from CLIP/ALIGN. We have stated two main diﬀerences between our BriVL and CLIP/ALIGN\nin the Introduction section. Below we give more detailed diﬀerences technically. (1) We adopt a four-tower network\narchitecture (see Supplementary Note Fig. S1a) for pre-training. By extending the original single-modal contrastive\nlearning (CL) algorithm MoCo [29], we introduce momentum encoders and negative sample queues for multimodal\npre-training in a more GPU-resource-saving way. In contrast, both CLIP and ALIGN employ the standard two-tower\narchitecture, which requires large batch size (thus enough negative samples) to be eﬀective, taking up a mass of GPU\nmemories. (2) We additionally devise a multi-scale patch pooling (MSPP) module (see Supplementary Note Fig. S1b)\nto capture ﬁne-grained image region representations without using object detectors. While CLIP and ALIGN only\nconsider global-level image embeddings, which impedes their ability to learn ﬁne-grained/local image features.\nFormalization of Neural Network Visualization. Neural network visualization is developed to directly show\nthe visual response/imagination of BriVL w.r.t. the semantic input. Formally, given the pre-trained image and text\nencoders f(i) and f(t) of BriVL, we ﬁrst input a piece of text x(t) and obtain its text embedding z(t) = f(t)(x(t)) ∈Rd.\nIn the mean time, we randomly initialize a noisy image x(i) ∈R600×600×3, which contains all the learnable parameters\nthroughout the entire visualization process. Further, we obtain the image embedding z(i) = f(i)(x(i)) ∈Rd and deﬁne\nthe learning objective by matching the two embeddings:\nLvis = −cos\n(\nz(i),z(t)\n)\n, (13)\nwhere cos(·,·) computes the cosine similarity between two vectors. With the resultant gradients, we are able to update\nthe input image x(i) by back-propagation. After repeating the above updating step with multiple iterations, we ﬁnally\nobtain an image x(i), which can be regarded as BriVL’s response/imagination about the input text. The algorithm\nfor neural network visualization is summarized in Algorithm 1.\nFormalization of Text-to-Image Generation. To make BriVL’s response/imagination on input texts better\nunderstood, we further adopt VQGAN [34] to help generate more photo-realistic images. The reason of utilizing\nVQGAN instead of other GANs [60] is as follows. Although classic GANs are able to generate high quality images\n13\nAlgorithm 2 Text-to-Image Generation\nInput: The pre-trained image and text encoders f(i) and f(t) of our BriVL\nThe codebook Cand the CNN generator g of the pre-trained VQGAN\nA piece of text x(t)\nA randomly initialized collection of codebook entries U\nA learning rate parameter λ\nOutput: The image generated with the updated U\n1: Obtain the text embedding z(t) = f(t)(x(t));\n2: for all iteration = 1, 2, ···, MaxIteration do\n3: Generate an image x(i) = g(U);\n4: Obtain the image embedding z(i) = f(i)(x(i));\n5: Compute Lt2i with Eq. (14);\n6: Compute the gradients ∇ULt2i;\n7: Obtain U′by updating U using gradient descent with λ;\n8: Obtain U by performing element-wise quantization on U′with Eq. (15);\n9: end for\n10: return the image generated with the updated U.\nunder speciﬁc domains (e.g., natural sceneries or human faces), they tend to fail when complex scenarios are involved.\nIn contrast, VQGAN alleviates this problem and performs better under complex scenarios by combining VQVAE [61]\nand GAN. For our text-to-image generation, we only need a codebook Cand a CNN generator g of the VQGAN\npre-trained on ILSVRC-2012 [35]. The pre-trained codebook C = {ck ∈ Rdc |k = 1 ,2,··· ,Nc}is a collection of\ntokens/codes, where dc is the dimension of each code and Nc is the number of codes in the codebook ( dc = 256 and\nNc = 1,024 in our case). The pre-trained CNN generator gtakes a spatial collection of codebook entries U ∈Rh×w×dc\nas input to generate an image ( U can also be regarded as a sequence of hw codes, h = w = 16 in our case), where\neach element uij ∈Rdc (i = 1,2,··· ,h and j = 1,2,··· ,w) must come from the codebook C(i.e., uij ∈C). With\nthe pre-trained image and text encoders f(i) and f(t) of BriVL, we ﬁrst input a piece of text x(t) and obtain its\ntext embedding z(t) = f(t)(x(t)) ∈Rd. Meanwhile, we randomly initialize an input code collection U, which is the\nonly parameter matrix to be learned. Afterwards, we generate an image from the generator x(i) = g(U) and further\nobtain its image embedding z(i) = f(i)(x(i)) ∈Rd. The learning objective is to maximize the similarity between two\nembeddings:\nLt2i = −cos\n(\nz(i),z(t)\n)\n. (14)\nAfter updating the input U and obtaining U′by back-propagation, we need to perform an element-wise quantization\nof each spatial code u′\nij ∈Rdc in U′onto its closest codebook entry ck:\nuij = arg min\nck∈C\n∥u′\nij −ck∥. (15)\nBy repeating the above updating step with multiple iterations, we ﬁnally obtain an image x(i) generated with the\nupdated U. The algorithm for text-to-image generation is summarized in Algorithm 2.\nNeural Network Visualization vs. Text-to-Image Generation. The intrinsic diﬀerence between neural net-\nwork visualization and text-to-image generation lies in that they produce images following diﬀerent data distributions.\nNot utilizing extra modules or data, neural network visualization exhibits BriVL’s primitive visual understanding of a\ngiven piece of text. However, the VQGAN [34] used for text-to-image generation is pre-trained on ILSVRC-2012 [35]\n(i.e., the classic ImageNet dataset), which generates images following the data distribution of ImageNet and thus being\nmore photo-realistic. Due to such an intrinsic diﬀerence, we present the visualization results of these two tasks for\ndiﬀerent purposes in this paper. Speciﬁcally, neural network visualization allows us to see what exactly a pre-trained\nmulti-modal foundation model imagines about semantic concepts and sentences, while text-to-image generation is used\nto generate images matched with given texts in a more human-friendly way.\nData availability\nThe availability of datasets used in this study is detailed as follows:\n(1). Two remote sensing scene classiﬁcation datasets: UC Merced Land-Use (UCM,http://weegee.vision.ucmerced.\nedu/datasets/landuse.html) and AID ( https://captain-whu.github.io/AID/).\n14\n(2). Two news classiﬁcation datasets: THUCNews ( http://thuctc.thunlp.org/) and Toutiao News ( https://\ngithub.com/aceimnorstuvwxz/toutiao-text-classfication-dataset ).\n(3). The Chinese cross-modal retrieval dataset AIC-ICC is available at https://github.com/neilfei/brivl-nmi.\n(4). The VQA dataset Visual7W is available at http://ai.stanford.edu/~yukez/visual7w/.\n(5). The dataset used for pre-training our model is available at https://resource.wudaoai.cn/home.\nPlease note that the available datasets (1) – (4) are suﬃcient for ﬁnetuning our pre-trained BriVL model in order to\ninterpret, verify and extend our research.\nCode availability\nThe pre-trained BriVL and its inference code are available at https://github.com/neilfei/brivl-nmi under Cre-\native Commons Attribution-Non Commercial-No Derivatives 4.0 International Licence (CC BY-NC-ND).\nReferences\n[1] Goertzel, B. Artiﬁcial general intelligence: Concept, state of the art, and future prospects. Journal of Artiﬁcial\nGeneral Intelligence 5, 1–48 (2014).\n[2] LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015).\n[3] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In IEEE Conference on\nComputer Vision and Pattern Recognition , 770–778 (2016).\n[4] Liu, Y. et al. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[5] Wang, A. et al. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations (2019).\n[6] Santoro, A. et al. A simple neural network module for relational reasoning. In Advances in Neural Information\nProcessing Systems, 4967–4976 (2017).\n[7] Bommasani, R. et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258\n(2021).\n[8] Editors of MIT Technology Review. 10 breakthrough technologies 2021. https://www.technologyreview.com/\n2021/02/24/1014369/10-breakthrough-technologies-2021/ (2021).\n[9] Brown, T. B. et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems,\n1877–1901 (2020).\n[10] Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C. & Fried, I. Invariant visual representation by single neurons\nin the human brain. Nature 435, 1102–1107 (2005).\n[11] Quian Quiroga, R., Kraskov, A., Koch, C. & Fried, I. Explicit encoding of multimodal percepts by single neurons\nin the human brain. Current Biology 19, 1308–1313 (2009).\n[12] Li, X. et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on\nComputer Vision, 121–137 (2020).\n[13] Radford, A. et al. Learning transferable visual models from natural language supervision. In International\nConference on Machine Learning, 8748–8763 (2021).\n[14] Li, L. H., Yatskar, M., Yin, D., Hsieh, C. & Chang, K. VisualBERT: A simple and performant baseline for vision\nand language. arXiv preprint arXiv:1908.03557 (2019).\n[15] Li, G., Duan, N., Fang, Y., Gong, M. & Jiang, D. Unicoder-VL: A universal encoder for vision and language by\ncross-modal pre-training. In AAAI Conference on Artiﬁcial Intelligence , 11336–11344 (2020).\n15\n[16] Su, W. et al. VL-BERT: Pre-training of generic visual-linguistic representations. In International Conference on\nLearning Representations (2020).\n[17] Chen, Y.-C. et al. UNITER: Universal image-text representation learning. In European Conference on Computer\nVision, 104–120 (2020).\n[18] Lin, J. et al. M6: A chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823 (2021).\n[19] Wu, Y., Wang, S., Song, G. & Huang, Q. Learning fragment self-attention embeddings for image-text matching.\nIn ACM International Conference on Multimedia , 2088–2096 (2019).\n[20] Chen, H. et al. Imram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval.\nIn IEEE Conference on Computer Vision and Pattern Recognition , 12655–12663 (2020).\n[21] Diao, H., Zhang, Y., Ma, L. & Lu, H. Similarity reasoning and ﬁltration for image-text matching. In AAAI\nConference on Artiﬁcial Intelligence , 1218–1226 (2021).\n[22] Wu, Z., Xiong, Y., Yu, S. X. & Lin, D. Unsupervised feature learning via non-parametric instance discrimination.\nIn IEEE Conference on Computer Vision and Pattern Recognition , 3733–3742 (2018).\n[23] Oord, A. v. d., Li, Y. & Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748 (2018).\n[24] Hjelm, R. D. et al. Learning deep representations by mutual information estimation and maximization. In\nInternational Conference on Learning Representations (2019).\n[25] Zhuang, C., Zhai, A. L. & Yamins, D. Local aggregation for unsupervised learning of visual embeddings. In\nInternational Conference on Computer Vision , 6002–6012 (2019).\n[26] Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S. & He, Y. ZeRO-inﬁnity: breaking the GPU memory wall for\nextreme scale deep learning. In International Conference for High Performance Computing, Networking, Storage\nand Analysis (2021).\n[27] Riquelme, C. et al. Scaling vision with sparse mixture of experts. In Advances in Neural Information Processing\nSystems, 8583–8595 (2021).\n[28] Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple framework for contrastive learning of visual repre-\nsentations. In International Conference on Machine Learning , 1597–1607 (2020).\n[29] He, K., Fan, H., Wu, Y., Xie, S. & Girshick, R. Momentum contrast for unsupervised visual representation\nlearning. In IEEE Conference on Computer Vision and Pattern Recognition , 9729–9738 (2020).\n[30] Grill, J.-B. et al. Bootstrap your own latent - a new approach to self-supervised learning. In Advances in Neural\nInformation Processing Systems, 21271–21284 (2020).\n[31] Chen, X. & He, K. Exploring simple siamese representation learning. In IEEE Conference on Computer Vision\nand Pattern Recognition, 15750–15758 (2021).\n[32] Jia, C. et al. Scaling up visual and vision-language representation learning with noisy text supervision. In\nInternational Conference on Machine Learning , 4904–4916 (2021).\n[33] Olah, C., Mordvintsev, A. & Schubert, L. Feature visualization. Distill (2017). https://doi.org/10.23915/\ndistill.00007.\n[34] Esser, P., Rombach, R. & Ommer, B. Taming transformers for high-resolution image synthesis. In IEEE Confer-\nence on Computer Vision and Pattern Recognition , 12873–12883 (2021).\n[35] Russakovsky, O. et al. ImageNet large scale visual recognition challenge. International Journal of Computer\nVision 115, 211–252 (2015).\n[36] Yang, Y. & Newsam, S. D. Bag-of-visual-words and spatial extensions for land-use classiﬁcation. In International\nSymposium on Advances in Geographic Information Systems , 270–279 (2010).\n[37] Xia, G.-S. et al. AID: A benchmark data set for performance evaluation of aerial scene classiﬁcation. IEEE\nTransactions on Geoscience and Remote Sensing 55, 3965–3981 (2017).\n16\n[38] Guan, J. et al. Zero and few shot learning with semantic feature synthesis and competitive learning. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 43, 2510–2523 (2021).\n[39] Li, A., Lu, Z., Wang, L., Xiang, T. & Wen, J. Zero-shot scene classiﬁcation for high spatial resolution remote\nsensing images. IEEE Transactions on Geoscience and Remote Sensing 55, 4157–4167 (2017).\n[40] Contributors of Toutiao News. Toutiao text classiﬁcation dataset. https://github.com/aceimnorstuvwxz/\ntoutiao-text-classfication-dataset (2021).\n[41] Li, J., Sun, M. & Zhang, X. A comparison and semi-quantitative analysis of words and character-bigrams as\nfeatures in chinese text categorization. In Proc. 21st International Conference on Computational Linguistics and\n44th Annual Meeting of the Association for Computational Linguistics , 545–552 (2006).\n[42] Cui, Y. et al. Revisiting pre-trained models for chinese natural language processing. In Conference on Empirical\nMethods in Natural Language Processing: Findings , 657–668 (2020).\n[43] Tan, M. & Le, Q. EﬃcientNet: Rethinking model scaling for convolutional neural networks. In International\nConference on Machine Learning, 6105–6114 (2019).\n[44] Sun, J. “Jieba” Chinese text segmentation. https://github.com/fxsjy/jieba (2020).\n[45] McInnes, L., Healy, J., Saul, N. & Großberger, L. UMAP: Uniform manifold approximation and projection.\nJournal of Open Source Software 3, 861 (2018).\n[46] Wu, J. et al. AI challenger: A large-scale dataset for going deeper in image understanding. arXiv preprint\narXiv:1711.06475 (2017).\n[47] Niu, Y. et al. Counterfactual VQA: A cause-eﬀect look at language bias. In IEEE Conference on Computer\nVision and Pattern Recognition, 12700–12710 (2021).\n[48] Zhu, Y., Groth, O., Bernstein, M. S. & Fei-Fei, L. Visual7W: Grounded question answering in images. In IEEE\nConference on Computer Vision and Pattern Recognition , 4995–5004 (2016).\n[49] Lin, T.-Y. et al. Microsoft coco: Common objects in context. In European Conference on Computer Vision ,\n740–755 (2014).\n[50] Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language understanding by generative\npre-training. OpenAI Blog (2018).\n[51] Devlin, J., Chang, M., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional transformers for language\nunderstanding. In Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, 4171–4186 (2019).\n[52] Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing Systems , 5998–6008\n(2017).\n[53] Kolesnikov, A. et al. Big Transfer (BiT): General visual representation learning. In European Conference on\nComputer Vision, 491–507 (2020).\n[54] Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. InInternational\nConference on Learning Representations (2021).\n[55] Nair, V. & Hinton, G. E. Rectiﬁed linear units improve restricted boltzmann machines. InInternational Conference\non Machine Learning, 807–814 (2010).\n[56] Wei, X., Zhang, T., Li, Y., Zhang, Y. & Wu, F. Multi-modality cross attention network for image and sentence\nmatching. In IEEE Conference on Computer Vision and Pattern Recognition , 10941–10950 (2020).\n[57] Anderson, P. et al. Bottom-up and top-down attention for image captioning and visual question answering. In\nIEEE Conference on Computer Vision and Pattern Recognition , 6077–6086 (2018).\n[58] Ren, S., He, K., Girshick, R. B. & Sun, J. Faster R-CNN: Towards real-time object detection with region proposal\nnetworks. In Advances in Neural Information Processing Systems , 91–99 (2015).\n17\n[59] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning\nRepresentations (2015).\n[60] Goodfellow, I. J. et al. Generative adversarial nets. In Advances in Neural Information Processing Systems ,\n2672–2680 (2014).\n[61] van den Oord, A., Vinyals, O. & Kavukcuoglu, K. Neural discrete representation learning. In Advances in Neural\nInformation Processing Systems, 6306–6315 (2017).\n[62] Young, P., Lai, A., Hodosh, M. & Hockenmaier, J. From image descriptions to visual denotations: New simi-\nlarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational\nLinguistics 2, 67–78 (2014).\n[63] Ordonez, V., Kulkarni, G. & Berg, T. L. Im2text: Describing images using 1 million captioned photographs. In\nAdvances in Neural Information Processing Systems , 1143–1151 (2011).\n[64] Sharma, P., Ding, N., Goodman, S. & Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-\ntext dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics, 2556–2565 (2018).\nAcknowledgements\nZ.L. acknowledges National Natural Science Foundation of China (61976220). J.R.W. acknowledges National Natural\nScience Foundation of China (61832017), Beijing Outstanding Young Scientist Program (BJJWZYJH012019100020098),\nand Large-Scale Pre-Training Program 468 of Beijing Academy of Artiﬁcial Intelligence (BAAI). N.F. acknowledges the\nOutstanding Innovative Talents Cultivation Funded Programs 2021 of Renmin Univertity of China. We acknowledge\nthe WenLan Data Group for helping us collect the pre-training dataset.\nAuthor contributions\nZ.L. contributed the original idea, model design, and experimental analysis. Z.L. and N.F. wrote the majority of the\nmanuscript. N.F., Y.G. and Y.H. contributed the source code. The experiments were conducted by N.F., G.Y., J.W.,\nH.L. and Y.G. The review and editing of the manuscript were carried out by H.S., T.X., X.G., R.S. and J.R.W. The\nentire project was supervised by J.R.W.\nCorresponding authors\nCorrespondence to Zhiwu Lu ( luzhiwu@ruc.edu.cn), Hao Sun ( haosun@ruc.edu.cn) or Ji-Rong Wen ( jrwen@ruc.\nedu.cn).\nCompeting interests\nThe authors declare no competing interests.\n18\nTowards artiﬁcial general intelligence via a multimodal\nfoundation model – supplementary note\n⋱\n⋱\nHavingapicnicatthepark.Relaxingweekend!\nMomentumImageEncoder\npush\n𝑁\"\n𝑁#\nImageEncoder𝑁\"\nMomentumTextEncoder\npush\n𝑁\" 𝑁#\n⋯\n⋯ ⋯⋯\n⋯ ⋯⋯\nTextEncoder𝑁\"\n⋯ ⋯\n⋯⋯⋮⋮ ⋮ ⋯\n⋯⋯⋮⋮ ⋮⋱\n𝑁#\n𝑁\"\n⋯\n⋯⋯⋮⋮ ⋮ ⋯\n⋯⋯⋮⋮ ⋮⋱ 𝑁\"\n𝑁#\n𝐿\"#$\n𝐿$#\"\n𝐿$%$&'\nHavingapicnicatthepark.Relaxingweekend!Havingapicnicatthepark.Relaxingweekend!\npre-training data:a total of 650 million image-text pairs\nimage negatives\ntext negatives\ncross-modal CL\ncross-modal CL\nMomentumImage EncoderText Encoder:Transformer\ntext image\nImage Encoder Text EncoderImage Encoder:CNN/Transformer MomentumText EncoderImage EncoderText EncoderPair Encoder:Co-Transformer\nimage-text pair imagetext image text\ntext embedding\npre-training loss\nimage embedding\npre-training loss\npair embedding\npre-training loss\nimage embeddingtext embeddingcontrastive loss contrastive loss\nimage embedding text embeddinga queue oftext negativesa queue ofimage negatives\nGPT, BERTBiT, ViTUNITER, OSCAR, M6CLIP , ALIGN BriVL\n1x1 6x6\na sequence of patch coordinates\n...\n⋯\n...feature map\nCNN\n•project the patch coordinates onto the feature map•multi-scale patch-wise pooling\na sequence ofpatch features\nSelf-AttentionLayersMLPimageembeddingaveragepooling\na b\nc\nFig. S1: Illustrations of our algorithm design, our network design, and existing pre-training architectures.\na. The schematic illustration of the proposed BriVL model for large-scale multimodal pre-training. b. The detailed\nnetwork architecture of our image encoder in BriVL. c. Illustration of existing pre-training architectures. From\nleft to right: single-modal textual pre-training (e.g., GPT [50] and BERT [51]); single-modal visual pre-training (e.g.,\nBiT [53] and ViT [54]); single-tower multimodal pre-training (e.g., UNITER [17], OSCAR [12], and M6 [18]); two-tower\nmultimodal pre-training (e.g., CLIP [13] and ALIGN [32]); our BriVL.\nArchitecture Overview\nIn Fig. S1a, we present the schematic illustration of our proposed BriVL model for large-scale multimodal pre-training.\nIn Fig. S1b, we show the network details of our image encoder in BriVL. In Fig. S1c, existing architectures for pre-\ntraining are illustrated. Please see Methods of the main manuscript for more information.\nAblation Study\nTo show the contributions of the self-attention (SA) layers used in both encoders and the cross-modal contrastive\nloss based on MoCo [29], we conduct experiments by whether using SA layers and adopting alternative contrastive\nlosses. Since it is too costly to conduct the ablation study on our full WSCD dataset (of 650M image-text pairs),\n19\nImage-to-Text RetrievalText-to-Image RetrievalMethodw/ SA Layers?ContrastiveAlgorithmRecall@1Recall@5Recall@10Recall@1Recall@5Recall@10Recall@SUMBriVLw/ SimCLRyesSimCLR-based27.5047.32 55.5027.10 46.84 55.19 259.45BriVLw/o SAno MoCo-based28.56 47.9855.83 28.02 46.86 54.58 261.83BriVL yesMoCo-based29.82 49.13 56.47 29.28 48.12 55.84268.66\nOnOctober30,inFullerton,California,USA,firefightersworkonthefirescene.Inrecentdays,wildfiresinCalifornia,theUnitedStates,havecontinuedtoraging,andalargenumberofresidentshavebeenforcedtoevacuate.\nCherishyourbusytime,beingbusyisthemostpreciousmedicineintheworld.Busypeoplearehappy,becausethereissomethingtodo,lifeisvaluable.\nPeoplearelikegrassandmustards,lifeiseasytobreak,andcherishlifeisprecious,notwasteful.\nKeepinganoptimisticlifeisthemood,andpeoplelivethementality.Lifeisprecious. Insteadofworryingtoomuchaboutcomplicatedandtrivialmatters,itisbettertotreatyourselfandrelaxyourlife.\n“Knowyourselfandtheenemy,ahundredbattleswillneverbelost.”Thisisthetruthdrawnfrompractice.Nomatterwhatkindofindustryyouarein,theimportantprerequisiteforsuccessisto“knowyourself”.\nDrinkingteaoftenwhenthereisnothingwrongwithitcanhelpyoutocultivateyourbodyandimproveyoursex,anditcanalsoreducethebody’scholesterolandtriglyceridecontent.\nTop1 Top2 Top3 Top4 Top5Query Retrieval Results\na\nb\nFig. S2: Quantitative and qualitative results with 22M training data. a . Ablative results (%) of BriVL on\nthe 11K test set. b. Cross-modal retrieval examples of our standard BriVL model (note that images were taken from\nthe Pexels website for illustration). Texts are translated into English for representation clarity. Highest results are\nhighlighted in bold.\nwe only use 22M image-text pairs for training and another 11K for evaluation. We provide the same 9 machines\n(each has 8 NVIDIA A100 GPUs) for all ablation experiments. The compared methods are as follows: (1) BriVL w/\nBYOL: We replace the MoCo-based cross-modal contrastive algorithm in our standard BriVL model with BYOL [30],\nwhich does not need any negative samples and only focuses on matching the positive ones. (2) BriVL w/ SimCLR:\nWe replace the MoCo-based cross-modal contrastive algorithm with SimCLR [28], which does not have momentum\nencoders or negative sample queues, and computes the InfoNCE loss [23] within each batch. In other words, the\nmini-batch size decides the number of negative samples for each positive image-text pair. Note that CLIP [13] and\nALIGN [32] both adopt SimCLR-based contrastive loss. (3) BriVL w/o SA: We discard the SA layers from both image\nand text encoders comparing to our standard BriVL. (4) BriVL: Our standard model with SA layers and MoCo-based\ncross-modal contrastive algorithm. All methods are trained for 6 epochs.\nThe ablative results are shown in the table of Fig. S2a. Note that we do not report the results of BriVL w/ BYOL\nbecause no matter how we try (e.g., tuning the hyper-parameters and implementing in diﬀerent ways), the model\nalways collapses. One possible reason is that BYOL only works under single-modal scenarios and it is essential to\ninclude negative samples under multimodal scenarios. Moreover, since SimCLR has neither momentum encoders nor\nnegative sample queues, its mini-batch size Nb can be larger than that of standard BriVL with the same computational\nresources. Concretely, the total batch size is 2,160 for SimCLR and 1,728 for standard BriVL which additionally\nmaintains two negative sample queues with the size of 10,368 (since momentum encoders and negative sample queues\ndo not produce gradients, they take up little GPU memory). However, we can then see from the table that BriVL\nw/ SimCLR performs worse than our standard BriVL (based on MoCo) for all 7 evaluation metrics. This indicates\nthe importance of large number of negative samples in multimodal pre-training and the advantage of our standard\nBriVL when large batch size is not feasible. Our model design thus hopefully helps those researchers with limited\nGPU resources for multimodal pre-training. Besides, comparing to our standard BriVL, discarding the SA layers (i.e.,\nBriVL w/o SA) also leads to performance drop, which validates the eﬀectiveness of this module.\nFurthermore, in Fig. S2b, we present two cross-modal retrieval examples with our standard BriVL model trained\non the 22M data. The query image for text retrieval and the candidate images for image retrieval are all taken from\nthe Pexels website ( https://www.pexels.com/), while the texts are picked from the 11K test set. We can observe\nthat the returned top-5 texts for the query image containing a cup of tea are all philosophical sentences, validating\nthe eﬀectiveness of training BriVL over weak semantic correlation data. Further, for the text query in the second row,\nour BriVL also does a great job in ﬁnding the matched images.\n20\nQuestion TypeMethodWhatWhereWhenWhoWhyHowOverallBriVL-en(direct training)76.7477.1378.1276.2176.4577.5576.91BriVL-en(pre-train & zero-shot)52.4053.2453.1153.2952.3452.9852.75BriVL-en(pre-train & finetune)81.2081.6981.3680.4580.6481.7581.26\nMethodBLEU@4METEORCIDErSPICEROUGE-LBriVL-en(direct training)18.8714.4437.0612.7144.01BriVL-en(pre-train & finetune)20.1821.9044.4715.5445.85\nBriVL-en(directtraining):•Aredvehicleisdrivingdownadirtroad.BriVL-en(pre-train&finetune):•Awhitecarisparkedinthedesertwithalotofsand.\nBriVL-en(directtraining):•Adogisrunningonthegrasswithaballinitsmouth.BriVL-en(pre-train&finetune):•Adogiscatchingaball.\nBriVL-en(directtraining):•Adogisrunningacrossthewater.BriVL-en(pre-train&finetune):•Adogrunningonabeach.\na b\nc\nFig. S3: Results on two English downstream tasks. a . Visual question answering results on Visual7W. Overall\naccuracies (%) along with results on each question type are reported. b. Image captioning results (%) on the test\nsplit of Flickr30K. c. Image captioning examples of our BriVL-en model regarding whether it is pre-trained. Highest\nresults are highlighted in bold.\nResults on English Tasks\nIn this section, we pre-train our proposed model on a well-known English dataset and name it as BriVL-en. Concretely,\nthe pre-training English dataset consists of 4 publicly-available image captioning datasets. (1) MSCOCO [49]:\nWe only use the training split of MSCOCO, which contains 113,287 images (each image has 5 text captions). (2)\nFlickr30K [62]: We use the training set of Flickr30K, which contains 29,783 images (each image also has 5 text\ncaptions). (3) SBU Captioned Photo Dataset (SBU) [63]: We collect SBU by the provided urls and obtain\naround 867K image-text pairs. We use the whole SBU dataset. (4) Conceptual Captions (CC) [64]: We also\ncollect and use the whole CC dataset, which contains around 3M image-text pairs. As a result, the total size of the\npre-training English dataset is around 4M.\nIn the next two subsections, we conduct experiments on two downstream tasks (i.e., visual question answering and\nimage captioning) to show the potential use of our English model BriVL-en. Importantly, the obtained similar results\nindicate that our model indeed provides a feasible solution closer to AGI beyond speciﬁc languages.\nVisual Question Answering. We ﬁrst conduct visual question answering (VQA) experiments on the Visual7W\ndataset [48] with three BriVL-en variations. The dataset split is the same as that for Chinese VQA experiments in\nthe main paper, but this time we do not need to translate the texts. In the table of Fig. S3a, we report the overall\naccuracies on the test set of Visual7W, as well as the results on each question type. We can make the following\nobservations: (1) “BriVL-en (pre-train & zero-shot)” performs much worse than “BriVL-en (direct training)”. This\nis mainly because the VQA task has a large domain gap to our pre-training task and the data distributions are also\ntotally diﬀerent. (2) “BriVL-en (pre-train & ﬁnetune)” outperforms “BriVL-en (direct training)” by large margins\nfor all evaluation metrics, indicating the eﬀectiveness of the pre-trained model and also the importance of ﬁnetuning\nwhen the data distribution of the downstream task is diﬀerent with that of the pre-training data.\nImage Captioning. Image captioning aims to generate descriptions for given images. In the table of Fig. S3b, we\nreport the results on the test split of Flickr30K [62] (1K images) w.r.t. ﬁve commonly-used evaluation metrics in the\nﬁeld of image captioning. Since an additional Transformer [52] decoder is needed to generate texts, we cannot conduct\nzero-shot experiments. We can see that “BriVL-en (pre-train & ﬁnetune)” outperforms “BriVL-en (direct training)”\nfor all metrics, again validating the eﬀectiveness of pre-training.\nFurthermore, we present three image captioning examples in Fig. S3c. For the ﬁrst image, “BriVL-en (pre-train\n& ﬁnetune)” points out that the car is white instead of red, and is parked in the desert rather than driving down a\ndirt road. Actually it is hard to tell whether the car is driving or parked. But as we see a man standing out there,\nit is highly possible that the car is parked (which is commonsensical). For the second image, “BriVL-en (pre-train\n& ﬁnetune)” knows that the dog is catching the ball rather than “running on the grass with a ball in its mouth”.\nHere, our pre-trained BriVL-en describes the action of the dog running towards the ball as “catching a ball”, which is\nimpressive. For the third image, despite the reﬂection under the dog, “BriVL-en (pre-train & ﬁnetune)” sees that the\ndog is running on a beach rather than across the water. This also shows the hint of common sense because running\nacross the water is illogical and the dog is most likely to be on a beach considering the distant waves.\n21\nImage Sources\nExcept the images that are owned by us, all others used in both the main manuscript and the supplementary note\nare taken from the Pexels website ( https://www.pexels.com), which provides free stock photos and allows users to\ndownload for free use (see its license page “ https://www.pexels.com/license/” for more information). We list all\nimages that are taken from the public (i.e., the Pexels website) in Table S1.\nTable S1: The sources of images used in this work.\nImage Source (URL)\n1 The cake image in Fig. 1b. https://www.pexels.com/photo/a-close-up-shot-of-a-cake-\nwith-a-candle-on-top-8015277/\n2 The “baseball ﬁeld” image at the\ntop of Fig. 4c.\nhttps://www.pexels.com/photo/city-road-landscape-\nﬂying-9739479/\n3 The ﬁrst image (from left) in Fig.\n6c.\nhttps://www.pexels.com/photo/selective-focus-\nphotography-of-train-610683/\n4 The second image (from left) in Fig.\n6c.\nhttps://www.pexels.com/photo/photo-of-horses-grazing-\nin-grass-ﬁeld-2050425/\n5 The third image (from left) in Fig.\n6c.\nhttps://images.pexels.com/photos/752882/pexels-photo-\n752882.jpeg\n6 The fourth image (from left) in Fig.\n6c.\nhttps://www.pexels.com/photo/vehicles-stop-on-red-light-\n771184/\n7 The picnic image in Fig. S1a and\nFig. S1b.\nhttps://www.pexels.com/photo/food-platters-on-picnic-\nblanket-5076436/\n8 The upper-left middle image in Fig.\nS1a.\nhttps://www.pexels.com/photo/white-and-blue-ﬂoral-\ntable-lamp-1793037/\n9 The upper-left back image in Fig.\nS1a.\nhttps://www.pexels.com/photo/green-christmas-tree-with-\nbaubles-6139342/\n10 The tea cup image in Fig. S2b. https://www.pexels.com/photo/teacup-with-tea-905485/\n11 The ﬁrst image (from left) in the\nsecond row of Fig. S2b.\nhttps://www.pexels.com/photo/bushﬁre-4070651/\n12 The second image (from left) in the\nsecond row of Fig. S2b.\nhttps://www.pexels.com/photo/yellow-plane-ﬂying-over-a-\nforest-ﬁre-4902033/\n13 The third image (from left) in the\nsecond row of Fig. S2b.\nhttps://www.pexels.com/photo/photo-of-burning-forest-\n4621457/\n14 The fourth image (from left) in the\nsecond row of Fig. S2b.\nhttps://www.pexels.com/photo/forest-ﬁre-4070727/\n15 The ﬁfth image (from left) in the\nsecond row of Fig. S2b.\nhttps://www.pexels.com/photo/blazing-ﬁre-in-the-forest-\n4636324/\n16 The left image in Fig. S3c. https://www.pexels.com/photo/car-on-a-dessert-4318822/\n17 The middle image in Fig. S3c. https://www.pexels.com/photo/tilt-shot-photo-of-dog-\nchasing-the-ball-1562983/\n18 The right image in Fig. S3c. https://www.pexels.com/photo/dog-running-at-the-beach-\n2906033/\n22",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7505772113800049
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6225801110267639
    },
    {
      "name": "Interpretability",
      "score": 0.5549420714378357
    },
    {
      "name": "Human–computer interaction",
      "score": 0.32530349493026733
    }
  ]
}