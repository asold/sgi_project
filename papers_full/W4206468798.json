{
  "title": "DropDim: A Regularization Method for Transformer Networks",
  "url": "https://openalex.org/W4206468798",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1979946437",
      "name": "Zhang Hao",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2166610048",
      "name": "Qu Dan",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A4208868746",
      "name": "Shao Keji",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2754898016",
      "name": "Yang Xukui",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2327501763",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2605131327",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2973048981",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6754484989",
    "https://openalex.org/W1936750108",
    "https://openalex.org/W6714142977",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W6766279884",
    "https://openalex.org/W6762945437",
    "https://openalex.org/W3098985395",
    "https://openalex.org/W3198860978",
    "https://openalex.org/W6734807902",
    "https://openalex.org/W6688167117",
    "https://openalex.org/W3160201895",
    "https://openalex.org/W3172862365",
    "https://openalex.org/W6783973398",
    "https://openalex.org/W6898634591",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W6631362777",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3037217258",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6748215858",
    "https://openalex.org/W2407080277",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2785350307",
    "https://openalex.org/W3149497593",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4293714597",
    "https://openalex.org/W2594978815",
    "https://openalex.org/W3089659770",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2952634764",
    "https://openalex.org/W2963744496",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W2212703438"
  ],
  "abstract": "We introduceDropDim, a structured dropout method designed for regularizing\\nthe self-attention mechanism, which is a key component of the transformer. In\\ncontrast to the general dropout method, which randomly drops neurons, DropDim\\ndrops part of the embedding dimensions. In this way, the semantic information\\ncan be completely discarded. Thus, the excessive coadapting between different\\nembedding dimensions can be broken, and the self-attention is forced to encode\\nmeaningful featureswith a certain number of embedding dimensions erased.\\nExperiments on a wide range of tasks executed on the MUST-C English-Germany\\ndataset show that DropDim can effectively improve model performance, reduce\\nover-fitting, and show complementary effects with other regularization methods.\\nWhen combined with label smoothing, the WER can be reduced from 19.1% to 15.1%\\non the ASR task, and the BLEU value can be increased from26.90 to 28.38 on the\\nMT task. On the ST task, the model can reach a BLEU score of 22.99, an increase\\nby 1.86 BLEU points compared to the strong baseline.\\n",
  "full_text": "IEEE SIGNAL PROCESSING LETTERS, VOL. 29, 2022 1\nDropDim: A Regularization Method for\nTransformer Networks\nHao Zhang, Dan Qu, Keji Shao, and Xukui Yang\nAbstract—We introduce DropDim, a structured dropout\nmethod designed for regularizing the self-attention mechanism,\nwhich is a key component of the transformer. In contrast to\nthe general dropout method, which randomly drops neurons,\nDropDim drops part of the embedding dimensions. In this way,\nthe semantic information can be completely discarded. Thus, the\nexcessive co-adapting between different embedding dimensions\ncan be broken, and the self-attention is forced to encode mean-\ningful features with a certain number of embedding dimensions\nerased. Experiments on a wide range of tasks executed on\nthe MUST-C English-Germany dataset show that DropDim can\neffectively improve model performance, reduce over-ﬁtting, and\nshow complementary effects with other regularization methods.\nWhen combined with label smoothing, the WER can be reduced\nfrom 19.1% to 15.1% on the ASR task, and the BLEU value can\nbe increased from 26.90 to 28.38 on the MT task. On the ST\ntask, the model can reach a BLEU score of 22.99, an increase by\n1.86 BLEU points compared to the strong baseline.\nIndex Terms —End-to-end, Transformer, Regularization,\ndropout,\nI. I NTRODUCTION\nT\nHE end-to-end (E2E) neural network has achieved great\nsuccess in sequence-to-sequence problems such as Au-\ntomatic Speech Recognition (ASR) [1], Machine Translation\n(MT) [2], and Speech Translation (ST) [3]. In this paradigm,\na single neural network is used to directly map the input\nto the target, removing the independent part of the cascade\nmethod. Transformer [4] is currently the most popular E2E\nstructure and has achieved state of the art performance in\nmany sequences modeling tasks [5], [6]. This ability is partly\nbecause it makes few assumptions about structural information\nof data, which makes the transformer a universal and ﬂexible\narchitecture [7]. As a side effect, the lack of structural bias\nmakes the transformer prone to overﬁtting for small-scale data.\nDropout [8] is a widely used regularization strategy, which\nmultiplies each neuron with a sample of a Bernoulli random\nvariable during training, with probability pof dropping/zeroing\nout the neuron. The ﬁnal model can be understood as the\nManuscript received September 21, 2021; revised December 20, 2021;\naccepted December 25, 2021. Date of publication January 5, 2022; date of\ncurrent version February 1, 2022. This work was supported by the National\nNatural Science Foundation of China under Grants 61673395 and 62171470.\nThe associate editor coordinating the review of this manuscript and approving\nit for publication was Dr. Odette Scharenborg. (Corresponding authors: Dan\nQu; Xukui Yang.)\nHao Zhang, Dan Qu, Xukui Yang are with the School of the\nInformation Engineering University, Zhengzhou 450000, China.(e-\nmail:haozhang0126@163.com;qudanqudan@sina.com;gzyangxk@163.com).\nKeji Shao is with Jiangnan Institute of Computing Technology. (e-\nmail:shaokjppl@163.com).\nDigital Object Identiﬁer 10.1109/LSP.2022.3140693\naverage of multiple models. Several variations of dropout have\nalso been proposed, such as dropblock [9], spatial dropout\n[10], and zoneout [11]. While most transformer-based models\nstill use the regular dropout [12], [13].\nDropAttention is recommended in [14]. By randomly setting\nattention weights to zero, the ﬁnal model was encouraged to\nuse the full context of the input sequences for prediction, rather\nthan relying solely on a small piece of features. And consid-\nering that the multi-head attention mechanism is dominated\nby a small number of attention heads [15], Zhou et al. [16]\nrecommends DropHead, which randomly discards part of the\nattention heads during the training.\nThe above methods directly process the attention head. In\nthis paper, we recommend another simple structured dropout\nmethod from the perspective of self-attention layer input,\ncalled DropDim. The key motivation behind DropDim is\nthat each embedding dimension contains certain semantic\ninformation in transformer. In other words, the basic unit is a\nvector instead of a single neuron [14]. Thus, the independence\nassumption in dropout needs to be relaxed. We achieve this\nby dropping the embedding dimension. In this way, semantic\ninformation can be completely removed. The excessive co-\nadapting between different embedding dimensions can be\nbroken, and the self-attention is forced to encode meaningful\nfeatures with a certain number of embedding dimensions\nerased.\nThere exists some structured dropout like the one suggested\nby us [17]–[21].The most similar to our method is cutoff\n[22], which is a set of simple yet efﬁcient data augmentation\nstrategies, including token cutoff, feature cutoff, and span\ncutoff. However, the motivation and application location are\nvery different. Cutoff processes the input sentence to generate\na restricted perspective, which helps enrich empirical obser-\nvations and better cover the data space. While we process the\ninput of the attention mechanism to ensure that self-attention\nlayer can encode more meaningful features without utilizing\nthe information from the removed embedding dimensions at\nall.\nExperiments on a wide range of tasks executed on the\nMUST-C English-Germany dataset [23] show that DropDim\ncan effectively improve model performance, reduce over-\nﬁtting, and show complementary effects with other regulariza-\ntion methods. When combined with label smoothing [24], the\nWER can be reduced from 19.1% to 15.1% on the ASR task,\nand the BLEU value can be increased from 26.90 to 28.38 on\nthe MT task. On the ST task, the model can reach a BLEU\nscore of 22.99, an increase by 1.86 BLEU points compared to\nthe strong baseline.\narXiv:2304.10321v1  [cs.CL]  20 Apr 2023\nIEEE SIGNAL PROCESSING LETTERS, VOL. 29, 2022 2\nFig. 1. Schematic illustration of the proposed DropDim, including\nDropDim(random) and DropDim(span). The blue area indicates that the\ncorresponding element is dropped. Similar to dropout we do not apply\nDropDim during inference.\nII. M ETHOD\nDropDim is a simple regularization method similar to\ndropout. Its main difference from dropout is that it drops\nembedding dimensions with a certain probability instead of\nrandomly dropping independent units.\nh = DropDim(x + sub block(x)) (1)\nWhere x denotes the input of transformer encoder or decoder\nsub-block. Unless otherwise speciﬁed, DropDim is applied to\nall sub-blocks.\nIn detail, we propose two structured dropout methods:\nDropDim(random) and DropDim(span), which are shown in\nFig. 1.\n1) DropDim(random) means dropping independent embed-\nding dimension. Algorithm 1 describes its pseudo code.\n2) DropDim(span) means dropping span of the embedding\ndimension. Let αbe the pre-deﬁned value that determined\nthe max length of the consecutive drop. Then, the span\nlength l is chosen from the uniform distribution on the\ninterval [0, α] and the starting index s for the span is\nrandomly sampled as: s∈{0,1,...,D −l}with D is the\nsize of embedding dimension. Afterward, the embeddings\nbetween the s-th and (s+ l−1)-th positions are dropped.\nIII. E XPERIMENTAL SETUP\nA. Datasets\nTo prove the effectiveness and generalization of DropDim,\nwe conduct experiments on three types of sequence-to-\nsequence tasks: ASR, MT, and ST. MUST-C is a speech\ntranslation corpus, including triplets of speech, transcription,\nand translation, which can perform different tasks ﬂexibly. It is\nbuilt from English TED talks, covering 8 language pairs. We\nconduct experiments on the English-German language pair.\nAlgorithm 1 DropDim\n1) Input: Transformer sub block output h as a T × D\nmatrix, drop rate p, mode\n2) if mode== Inference then\n3) return h\n4) end if\n5) Randomly sample ξ: ξi ∼Bernoulli(p), i⩽ D\n6) For each element ξi, create a mask vector Mi ∈RT with\nall elements equal to ξi.\n7) Apply the mask: h = h ×M\n8) Normalization: h = h ×count(M)/count ones(M)\nThere are 234K utterances totaling 408 hours in this parallel\ncorpus, which is divided into training set (400 hours, 229,703\npronunciations), development set (3 hours, 1423 pronuncia-\ntions), and tst-COMMON1 set (5 hours, 2641 pronunciations).\nB. Preprocessing and Evaluation\nAll the acoustic features used in this paper are 80-\ndimensional MFCC extracted using Kaldi [25] with global\ncepstral mean and variance normalization. The text data in\nthe source language is normalized with lowercase conversion,\ntokenization, and punctuation removal. We apply punctuation\nnormalization instead of punctuation removal for the text data\nin the target language. The processing of the text is realized\nby Moses scripts 2. We apply BPE [26] on the combination of\nsource and target text to obtain shared subword units, and the\nvocabulary size is set to 8K.\nThe case-insensitive BLEU [27] calculated by the multi-\nbleu.pl script3 is used to evaluate translation tasks (MT, ST).\nMoreover, we use Word Error Rate (WER) to evaluate the\nASR system.\nC. Training Settings\nThe models involved in this paper all adopt the Transformer\nstructure. We conduct experiments based on Espnet [28] and\nuse PyTorch-lightning [29] to organize our code. For both\nencoder and decoder in the MT model, the number of layers is\n6, the number of attention heads is 4, the embedding dimension\nis 256, and the ﬁlter size of the feed-forward neural network is\n2048. The attention and residual dropout rates are 0.0 and 0.1,\nrespectively. For the ST and ASR model, the hyperparameters\nof encoder and decoder are 12 and 6, respectively. Other\nparameters remain the same with MT model. We train our\nmodels with Adam optimizer [30] on 4 NVIDIA V100 GPUs.\nIV. E XPERIMENTAL RESULTS\nA. Main Results\nWe ﬁrst experimentally verify the effectiveness of DropDim\nfor ASR, MT, and ST tasks, and compare it with other\nmethods. The results are summarized in Table I.\n1MuST-C is a multilingual dataset and this testset is the commonly shared\nutterances between the languages.\n2https://www.statmt.org/moses/\n3https://github.com/moses-smt/mosesdecoder/scripts/generic/multi-\nbleu.perl\nIEEE SIGNAL PROCESSING LETTERS, VOL. 29, 2022 3\nTABLE I\nMODEL PERFORMANCE UNDER DIFFERENT METHODS . *: T HE\nAPPLICATION LOCATION OF DROPOUT HERE IS THE SAME AS DROP DIM.\nModels ASR(WER%↓) MT(BLEU↑) ST(BLEU↑)\nTransformer(base) 19.1 26.90 21.13\n+dropout* 18.7 27.10 21.54\n+DropAttention 18.3 27.38 21.34\n+Drophead 18.1 27.51 21.57\n+label smoothing(0.1) 16.8 27.88 21.31\n+Dropdim(random) 17.8 27.22 21.85\n+label smoothing(0.1) 15.3 28.32 22.59\n+Dropdim(span) 17.7 27.42 21.97\n+label smoothing(0.1) 15.1 28.38 22.99\nTABLE II\nEXPERIMENTS ON DIFFERENT DATASETS AND LANGUAGE PAIRS IN ST\nTASK . LABEL SMOOTHING IS USED IN ALL METHODS BY DEFAULT .\nModels Librispeech En-Fr MUST-C En-Fr\nbase 16.08 33.15\n+Dropdim(random) 17.12 34.46\n+Dropdim(span) 17.34 34.57\nDropDim vs Dropout. The original transformer model did\nnot apply dropout to the output of the residual connection.\nFor the convenience of discussion, we deﬁne a transformer\nbaseline in which DropDim in Equation 1 is replaced with\ndropout. In all three tasks, DropDim and dropout share a\nsimilar trend and DropDim has a large gain compared to the\ndropout results. This shows evidence that the DropDim is a\nmore effective regularizer compared to dropout.\nComparison with DropAttention and Drophead. We\ncompare with two popular structured dropout methods de-\nsigned for self-attention mechanisms. All three methods can\nimprove model performance. Speciﬁcally, on speech-related\ntasks (ASR, ST), DropDim achiceves better results. We believe\nthis is because compared to DropAttention and Drophead,\nDropDim processes the input of the self-attention mechanism,\nwhich can completely drop semantic information and pro-\nduce a stronger regularization effect. However, compared with\nspeech, the text is coarse-grained, too strong regularization will\nhurt the MT model.\nIn the above comparison, DropAttention, Drophead, and\ndropout* do not use label smoothing. The same trend can be\nobserved when label smoothing is added, and the results were\nnot listed due to the limited number of pages.\nSpan vs Random. DropDim(span) delivers better results on\nall three tasks. This is due to the fact that in the case of high\ndimensions (256 in this article), it may not be that a single\nembedding dimension determines a semantics, but several ad-\njacent embedded dimensions jointly express a semantics. And\nsemantic information can be removed more effectively with\ncontinuous drop, which brings greater challenges to model\ntraining. We leave the speciﬁc details for future research.\nIntegration with other regularization techniques. Label\nsmoothing [24] is a common regularization technique. The\nresults in Table I show that both label smoothing and DropDim\ncan improve model performance. Moreover, the performance\ncan be further improved when combining DropDim and label\nsmoothing. This shows that the overﬁtting problem is a direc-\ntion that still needs to be studied, and there is no method that\nFig. 2. The effect of different hyperparameters on model performance.\nTABLE III\nBLEU SCORE OF MODELS WHEN DROP DIM IS APPLIED ON DIFFERENT\nPART DURING TRAINING .\ntask Part\nencoder decoder all\nASR(WER%↓) 16.9 16.2 15.1\nMT(BLEU↑) 27.82 27.71 28.38\nST(BLEU↑) 22.66 22.13 22.99\ncan completely solve it when used alone.\nResults under different langugae pair and datasets.\nIn order to further proves the effectiveness and generality\nof Dropdim, we performed additional experiments on the\nLibrispeech En-Fr [31] and MUST-C En-Fr datasets in ST\ntask. The results are listed in Table II. The experimental\nresults showed the same trend as on MUST-C En-De datasets.\nThe speech in Librispeech En-Fr was recorded on-site, which\ncontains more noise, resulting the transcription and translation\nare not well aligned with the original audio. So, the model\nperformance on this dataset is lower than on MUST-C En-Fr.\nB. Ablation Studies\nEffect of Hyperparameters. The hyperparameters of\nDropdim determine the number of embedding dimensions\nthat are dropped. The inﬂuence of hyperparameters on model\nperformance is summarized in Fig. 2. It can be observed\nthat determining a sweet point of the drop ratio is critical to\nthe generalization ability of the resulting model. Speciﬁcally,\nwhen DropDim (random) is used, the best drop ratio for ASR,\nMT, and ST are 0.01, 0.05, and 0.10, respectively. While using\nDropDim (span), the best max length ( α) is 10, 40, and 30,\nrespectively. Dropping too many embedding dimensions will\nbring a very small improvement to the model and even hurt\nthe performance. This may be because the model unable to\nextract knowledge from the input for learning.\nEffect on Different Part. In this section, we study the effect\nof DropDim on different parts of the model. Speciﬁcally, we\napply DropDim separately on the encoder, decoder, and the\nwhole model. The results are shown in Table III. We can see\nthat the proposed approach is more effective when used on the\nwhole model because of the strong regularization effect.\nC. Analysis\nDropDim drops more semantic information. We ﬁrst\ntrain the model normally on ST task and then apply dropout\nIEEE SIGNAL PROCESSING LETTERS, VOL. 29, 2022 4\nFig. 3. The performance of the model using dropout or DropDim during\ntesting.\nFig. 4. Model performance with or without DropDim under different amounts\nof data. Left: No data augmentation is used. Right: Speed Perturbs of 0.9, 1.0,\n1.1 are used for data augmentation.\nor DropDim when testing. For a fair comparison, DropDim\n(random) is used here because with the same drop rate, the\nsame number of neurons are dropped. The difference is that\nneurons are randomly dropped when dropout is used while\nusing DropDim (random) will drops the entire embedding\ndimension. It can be seen that as the drop ratio increases,\nthe yellow curve in Fig. 3 drops rapidly, indicating that\nDropDim can remove semantic information more effectively\nthan dropout.\nResults under different amounts of data. We test the\neffectiveness of DropDim under different amounts of data on\nthe ST task. To simulate different data resource scenarios, we\nrandomly select 10-hour, 50-hour, and 100-hour speech train-\ning data from the MUST-C English-German dataset, which\noriginally contains 400-hour speech. Then the model is trained\nwith or without DropDim. In addition, Speed Perturb [32] is a\ncommonly used data augmentation method under low resource\nconditions. We also study the combination of DropDim and\nSpeed Perturb. The results are shown in Fig. 4. The blue curve\nin the ﬁgure continues to be higher than the yellow curve,\nshowing that DropDim can continuously improve the model\nperformance under different amounts of data. Moreover, the\ncombination with Speed Perturb can produce better results,\nindicating that the two methods are complementary.\nD. Visualization\nAttention Visualization. We further analyze how DropDim\nimprove the model performance by visualizing the encoder-\nencoder and encoder-decoder attention maps. Speciﬁcally, the\nencoder-encoder attention is extracted from the last layer of\nthe encoder, and the encoder-decoder attention is extracted\nfrom the last layer of the decoder. Fig. 5 shows an example.\nThe attention in the ST model tends to be smooth between\ninput frames. However, after applying DropDim, both types of\nFig. 5. The visualization of attention of different modules under different\nstrategies. The horizontal coordinates represent the sequence of speech frames.\nIn the attention map of Encoder-Encoder, the vertical coordinates represent\nthe sequence of speech frames. In the attention map of Encoder-Decoder, the\nvertical coordinates represent the text token sequence.\nattention become more concentrated, indicating that DropDim\ncan not only help the model to remove interference but also\nmake the model better align with the target.\nTraining Process Visualization. It can be seen from the\nnetwork training curve in Fig. 6 that when DropDim is used,\nthe network can underﬁt the training loss, which is in sharp\ncontrast with the usual situation where networks tend to over-\nﬁt to the training data. This is consistent with specaugment\n[33], which converts over-ﬁtting problems into under-ﬁtting\nproblems, and improves model performance through longer\ntraining time.\nThis also reveals to us that the combination of Dropdim\nand specaugment is not suitable. Because although this com-\nbination can improve performance, the training time will be\nfurther increased. Dropdim is more suitable to be combined\nwith Speed Perturbs, because the latter improves the model\nperformance by increasing the amount of data, which is\ndifferent from Dropdim and specaugment.\nFig. 6. Training process under different strategies.\nV. C ONCLUSION\nIn this paper, we introduce DropDim, a simple regulariza-\ntion method for training transformer models. We have veriﬁed\nthe effectiveness of DropDim on a wide range of tasks, show-\ning that it can continuously improve the model performance.\nIEEE SIGNAL PROCESSING LETTERS, VOL. 29, 2022 5\nExperimental analysis show that DropDim is also competitive\nunder low-resource conditions and is complementary to other\ndata augmentation methods.\nREFERENCES\n[1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A\nneural network for large vocabulary conversational speech recognition,”\nin Proc. ICASSP, pp. 4960–4964.\n[2] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\nwith neural networks,” in Proc. Neurips, vol. 27, pp. 3104–3112.\n[3] R. J. Weiss, J. Chorowski, N. Jaitly, Y . Wu, and Z. Chen, “Sequence-\nto-sequence models can directly translate foreign speech,” in Proc.\nINTERPSEECH, pp. 2625–2629.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc.\nNeurips, vol. 30, pp. 5998–6008.\n[5] L. Dong, S. Xu, and B. Xu, “Speech-transformer: A no-recurrence\nsequence-to-sequence model for speech recognition,” in Proc. ICASSP,\npp. 5884–5888.\n[6] M. A. D. Gangi, M. Negri, and M. Turchi, “Adapting transformer to\nend-to-end spoken language translation.” in Proc. INTERPSEECH, pp.\n1133–1137.\n[7] T. Lin, Y . Wang, X. Liu, and X. Qiu, “A survey of transformers.”\n[8] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: a simple way to prevent neural networks from overﬁt-\nting,” vol. 15, no. 1, pp. 1929–1958.\n[9] G. Ghiasi, T.-Y . Lin, and Q. V . Le, “Dropblock: A regularization method\nfor convolutional networks,” in Proc. Neurips, vol. 31, pp. 10 727–\n10 737.\n[10] J. Tompson, R. Goroshin, A. Jain, Y . LeCun, and C. Bregler, “Efﬁcient\nobject localization using convolutional networks,” in Proc. CVPR, pp.\n648–656.\n[11] D. Krueger, T. Maharaj, J. Kram ´ar, M. Pezeshki, N. Ballas, N. R.\nKe, A. Goyal, Y . Bengio, A. C. Courville, and C. J. Pal, “Zoneout:\nRegularizing rnns by randomly preserving hidden activations.” in Proc.\nICLR(poster).\n[12] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A\nframework for self-supervised learning of speech representations,” in\nProc. Neurips, vol. 33, pp. 12 449–12 460.\n[13] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, “Transformer transducer: A streamable speech recognition\nmodel with transformer encoders and rnn-t loss,” in Proc. ICASSP, pp.\n7829–7833.\n[14] L. Zehui, P. Liu, L. Huang, J. Fu, J. Chen, X. Qiu, and X. Huang,\n“Dropattention: A regularization method for fully-connected self-\nattention networks.”\n[15] P. Michel, O. Levy, and G. Neubig, “Are sixteen heads really better than\none,” in Proc. Neurips, vol. 32, pp. 14 014–14 024.\n[16] W. Zhou, T. Ge, F. Wei, M. Zhou, and K. Xu, “Scheduled drophead:\nA regularization method for transformer models,” in Proc. EMNLP, pp.\n1971–1980.\n[17] Z. Y . W. W. Wang, H., “Specaugment++: A hidden space data augmenta-\ntion method for acoustic scene classiﬁcation,” in Proc. INTERPSEECH,\npp. 551–555.\n[18] L. J. e. a. Xie Z, Wang S I, “Data noising as smoothing in neural network\nlanguage models,” in Proc. ICLR.\n[19] Y . Gal and Z. Ghahramani, “A theoretically grounded application of\ndropout in recurrent neural networks,” in Proc. Neurips, vol. 29, pp.\n1019–1027.\n[20] S. Dalmia, Y . Liu, S. Ronanki, and K. Kirchhoff, “Transformer-\ntransducers for code-switched speech recognition,” in Proc. ICASSP.\nIEEE, pp. 5859–5863.\n[21] S. Dalmia, B. Yan, V . Raunak, F. Metze, and S. Watanabe, “Searchable\nhidden intermediates for end-to-end models of decomposable sequence\ntasks,” in Proc. NAACL, pp. 1882–1896.\n[22] D. Shen, M. Zheng, Y . Shen, Y . Qu, and W. Chen, “A simple but tough-\nto-beat data augmentation approach for natural language understanding\nand generation.”\n[23] M. A. D. Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi,\n“Must-c: a multilingual speech translation corpus,” in Proc. NAACL, pp.\n2012–2017.\n[24] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking\nthe inception architecture for computer vision,” in Proc. CVPR, pp.\n2818–2826.\n[25] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,\nM. Hannemann, P. Motlicek, Y . Qian, P. Schwarz, J. Silovsky, G. Stem-\nmer, and K. Vesely, “The kaldi speech recognition toolkit,” in In ASRU\nworkshop.\n[26] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of\nrare words with subword units,” in Proc. ACL, vol. 1, pp. 1715–1725.\n[27] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\nautomatic evaluation of machine translation,” in Proc. ACL, pp. 311–\n318.\n[28] H. Inaguma, S. Kiyono, K. Duh, S. Karita, N. Yalta, T. Hayashi, and\nS. Watanabe, “Espnet-st: All-in-one speech translation toolkit,” in Proc.\nACL, pp. 302–311.\n[29] W. Falcon, “Pytorch lightning,” vol. 3. [Online]. Available: https:\n//github.com/PyTorchLightning/pytorch-lightning\n[30] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimiza-\ntion,” in Proc. ICLR.\n[31] A. C. Kocabiyikoglu, L. Besacier, and O. Kraif, “Augmenting librispeech\nwith french translations: A multimodal corpus for direct speech transla-\ntion evaluation,” arXiv preprint arXiv:1802.03142, 2018.\n[32] T. Ko, V . Peddinti, D. Povey, and S. Khudanpur, “Audio augmentation\nfor speech recognition.” in Proc. INTERPSEECH, pp. 3586–3589.\n[33] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, “Specaugment: A simple data augmentation method for\nautomatic speech recognition.” inProc. INTERPSEECH, pp. 2613–2617.",
  "topic": "Regularization (linguistics)",
  "concepts": [
    {
      "name": "Regularization (linguistics)",
      "score": 0.7621278762817383
    },
    {
      "name": "Transformer",
      "score": 0.7477877736091614
    },
    {
      "name": "Computer science",
      "score": 0.7092198133468628
    },
    {
      "name": "Embedding",
      "score": 0.704156756401062
    },
    {
      "name": "Smoothing",
      "score": 0.6739832162857056
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5099347829818726
    },
    {
      "name": "ENCODE",
      "score": 0.43803995847702026
    },
    {
      "name": "Speech recognition",
      "score": 0.36644649505615234
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33636659383773804
    },
    {
      "name": "Computer vision",
      "score": 0.11771714687347412
    },
    {
      "name": "Voltage",
      "score": 0.09424489736557007
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I169689159",
      "name": "PLA Information Engineering University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210090176",
      "name": "Institute of Computing Technology",
      "country": "CN"
    }
  ],
  "cited_by": 13
}