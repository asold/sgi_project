{
    "title": "Zero-shot Text Classification With Generative Language Models",
    "url": "https://openalex.org/W2995335514",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4227412475",
            "name": "Puri, Raul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2988481184",
            "name": "Catanzaro, Bryan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2122223050",
        "https://openalex.org/W1552847225",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2118781169",
        "https://openalex.org/W2963112338",
        "https://openalex.org/W2987878148",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2798754355",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963866663",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2734377693",
        "https://openalex.org/W2951775809",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W2399033357",
        "https://openalex.org/W2888541716",
        "https://openalex.org/W2917041815",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2798664956",
        "https://openalex.org/W2809324505",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2963026102"
    ],
    "abstract": "This work investigates the use of natural language to enable zero-shot model adaptation to new tasks. We use text and metadata from social commenting platforms as a source for a simple pretraining task. We then provide the language model with natural language descriptions of classification tasks as input and train it to generate the correct answer in natural language via a language modeling objective. This allows the model to generalize to new classification tasks without the need for multiple multitask classification heads. We show the zero-shot performance of these generative language models, trained with weak supervision, on six benchmark text classification datasets from the torchtext library. Despite no access to training data, we achieve up to a 45% absolute improvement in classification accuracy over random or majority class baselines. These results show that natural language can serve as simple and powerful descriptors for task adaptation. We believe this points the way to new metalearning strategies for text problems.",
    "full_text": "Zero-shot Text Classiﬁcation With Generative\nLanguage Models\nRaul Puri\nNVIDIA\nraulp@nvidia.com\nBryan Catanzaro\nNVIDIA\nbcatanzaro@nvidia.com\nAbstract\nThis work investigates the use of natural language to enable zero-shot model adap-\ntation to new tasks. We use text and metadata from social commenting platforms\nas a source for a simple pretraining task. We then provide the language model\nwith natural language descriptions of classiﬁcation tasks as input and train it to\ngenerate the correct answer in natural language via a language modeling objective.\nThis allows the model to generalize to new classiﬁcation tasks without the need\nfor multiple multitask classiﬁcation heads. We show the zero-shot performance of\nthese generative language models, trained with weak supervision, on six benchmark\ntext classiﬁcation datasets from the torchtext library. Despite no access to training\ndata, we achieve up to a 45% absolute improvement in classiﬁcation accuracy over\nrandom or majority class baselines. These results show that natural language can\nserve as simple and powerful descriptors for task adaptation. We believe this points\nthe way to new metalearning strategies for text problems.\n1 Method\nOur method reformulates text classiﬁcation problems as multiple choice question answering. To\nenable our model to generalize to new classiﬁcation tasks, we provide the model with a multiple\nchoice question description containing each class in natural language, and train it to generate the\ncorrect answer, also in natural language, from the provided description. To better prepare our model\nto handle a wide variety of class descriptors, we utilize a pretrained GPT-2 (Radford et al., 2019)\ntransformer model and ﬁnetune it on the task of multiple choice title prediction for the OpenWebText\ndataset (Peterson et al., 2019). This pretraining task trains the model to use common sense reasoning\nto select the most probable title or description of the text data from a provided list of rich natural\nlanguage descriptions or classes, similar to the problem formulation of text classiﬁcation. The wide\nvariety of titles available in the pretraining dataset help simulate numerous automatically generated\nN-way text classiﬁcation tasks to enable meta-learning. In initial studies we found that the diverse\nlanguage found in title prediction was necessary to adapt to new tasks, and other pretraining tasks\nsuch as WebText subreddit prediction did not transfer at all.\nFor a given document, we randomly sample a number of titles t ∈[2,15] with one title being the\ncorrect title. Half of the time we replace a single title with “none of the above”, and occasionally\n(p= 1/t) we choose to replace the correct title with “none of the above”. We prepend all selected titles\nto the document in the form of a multiple choice question, and train the model to generate the answer,\nsimilar to generative Question Answering (McCann et al., 2018). Example input representations for\ntitle prediction can be found in Table 1.\nThe model is optimized by computing a next token prediction language modeling loss,∑\nt L(wt,P( ˆwt|w[1,t−1])), that optimizes over the entire concatenated input w = [question,\nreference_text,output_answer] and the questions are generated according to a grammar. The\ninput representation utilizes type tokens to segment the question, reference text, and answer. To\n3rd Workshop on Meta-Learning at NeurIPS 2019, Vancouver, Canada.\narXiv:1912.10165v1  [cs.CL]  10 Dec 2019\nFigure 1: Comparison between existing multitask classiﬁers and our method. (a) Multitask classiﬁers\nhave the model featurize text and send it to one of N task heads. (b) In our method, one of N task\ndescriptors is prepended to the text and the model generates the answer in natural language.\nDataset Question Text Answer\nTitle Prediction\nPretraining\nWhich of these choices best de-\nscribes the following document?\n: “ A pool For All Bodies ” , “\nLawmakers say they’d take pay\ncut, but they can’t ” , “ Raiders’\nGareon Conley faces civil suit\n” , “ Proliﬁc cybercriminal sus-\npected of spreading ransomware\narrested by Polish Police [Eu-\nropol] ”\nStory highlights Members of\nCongress also preparing for po-\ntential sharp cuts in federal\nspending\\n\\nBut lawmakers will\nnot see any change to their an-\nnual salary of $174,000...\nLawmakers\nsay they’d\ntake pay cut,\nbut they can’t\nAGNews Zero-\nshot Classiﬁca-\ntion\nHow is the text best described? : “\nScience & Technology ” , “ Busi-\nness ” , “ Sports ” , or “ World\nNews ”\nAn Entertaining Holiday Pick \\n\nHastings, a multimedia retailer,\ntrims losses and raises full-year\nguidance.\nBusiness\nTable 1: Example inputs for pretraining and downstream tasks. The descriptor questions are concate-\nnated to the text samples and the language model generates the remaining output answer text. Class\ndescriptors for the 5 other downstream classiﬁcation tasks can be found in appendix A.4.1\nencode positional information, the input uses learned positional embeddings that reset to position 0 at\nthe start of the answer. This is described in more detail in the appendix section A.1.\nFor our analysis of zero shot classiﬁcation we examine the performance of our model at various\nsizes on several of the torchtext classiﬁcation datasets. When transferring the model we provide\nall the given dataset’s classes (typically ranging from 2-15 classes) to the model in the multiple\nchoice question format and prompt it to generate out the correct class. Furthermore, we ensure that\ndownstream tasks do not contain \"none of the above\" options. We use greedy autoregressive decoding\nto generate our output text. Example inputs for each of our downstream tasks are shown in Table 1.\n1.1 Dataset\nWe build upon prior work collecting large language modeling datasets from the internet. Namely, we\nextend the OpenWebText corpus (Peterson et al., 2019) by annotating the documents with subreddits\nand titles in natural language. The OpenWebText dataset is collected by scraping outbound weblinks\nfrom reddit that have more than 3 karma score. We annotate each outbound weblink with the title of\nthe Reddit post, and the subreddit that the link was posted in. Weblinks can appear in multiple posts\nacross different subreddits, so for a given link we aggregate a list of all it’s related subreddits and\n2\ntitles. Detailed dataset statistics can be found in appendix section A.3. To create training data we\nsample a random document, multiple titles including one of the documents corresponding titles, and\narrange the input as described in the previous section. We evaluate the trained model on the DBPedia,\nAGNews, Yahoo Answers, SST-2, Amazon-2, and Yelp-2 text classiﬁcation datasets (Socher et al.,\n2013; Lehmann et al., 2015). The classes and class descriptors used for each of these tasks can\nbe found in appendix section A.4.1. To experiment with different class descriptions and model\narchitectures, we create a small validation set of 2000 random training set examples for each of the\ndownstream tasks. We evaluate our design choices on these validation sets before reporting ﬁnal\naccuracies on the entire test set.\n2 Related Work\nZero and few shot learning have been the subject of many studies. Some works have looked at\nmeta-learning for machine translation in low resource languages (Gu et al., 2018), iteratively guiding\npolicies with language (Co-Reyes et al., 2018) for instruction following (Branavan et al., 2009; Chen\nand Mooney, 2011), and generating WikiSQL-style structured queries from natural language queries\n(Huang et al., 2018). Radford et al. (2018, 2019) show that large scale language models can be used\nin a multitask zero shot capacity by allowing the model to generate output text in an autoregressive\nmanner given a prompt with the task description. They demonstrate that larger transformer language\nmodels perform better than smaller models in zero shot settings. However, their models are never\nexplicitly trained for zero shot text classiﬁcation. To perform classiﬁcation, the authors propose\nappending a prompt token to the text and restricting the output vocabulary to the tokens of possible\nanswers. This effectively turns the output vocabulary into a pretrained task-speciﬁc classiﬁcation\nhead. Unlike our approach their work requires manual intervention and does not take advantage of\ntask descriptors to modulate output behavior. The Multitask Question Answering Network (McCann\net al., 2018) study also investigates zero shot performance of multitask generative language models\nprompted with descriptor questions. However, they only analyze zero shot classiﬁcation performance\nbetween tasks of identical domains (SST-2 and Amazon-2) that are trained with supervised learning\nand identical prompts. Using identical prompts and supervised learning prevents a true analysis of\nthe model’s ability to adapt to unseen task descriptors.\nRecent work in meta-learning has centered around gradient based meta learning strategies such\nas Model Agnostic Meta-Learning or MAML (Finn et al., 2017). However, parallel work such as\nMemory Augmented Neural Networks (Santoro et al., 2016) and Simple Neural Attentive Learners\n(Mishra et al., 2017) demonstrate the effectiveness of architecture based meta-learning. This is\nsimilar to our work except that our models receive weak supervision in the form of class labels and a\nquestion in natural language instead of similar class examples. We show throughout this work that\nmelding techniques from NLP and architecture based meta-learning allows our model to adapt to new\nlanguage classiﬁcation tasks.\nLastly, similar to our work, concurrent research investigates models capable of handling tasks with\ndifferent class counts and output mappings. Bansal et al. (2019) combine prototypical networks\nand MAML to adapt to NLP tasks with different numbers of labels. Raffel et al. (2019) propose a\nuniﬁed multitask language model that uses weakly-supervised task labels to generate task outputs\nwith natural language. By doing so, the resulting model is capable of performing a diverse set\nof tasks including classiﬁcation, natural language inference, question answering, and abstractive\nsummarization. Furthermore, the authors demonstrate the viability of this approach by scaling the\nmodel to 11 billion parameters and achieving state of the art accuracy. However, neither of these\nworks examine the ability of a uniﬁed model to adapt to new task descriptors in a zero-shot fashion.\n3 Results\nTo test the ability of our pretrained models to adapt to new tasks and tasks descriptions, we transfer\nthe models to 6 classiﬁcation tasks. We provide three baselines the ﬁrst two of which are designed\nto expose dataset bias: random guessing, majority class (mode of the training dataset), and directly\nﬁnetuning a 355 million parameter classiﬁcation model on the downstream tasks. In our experiments\nwe investigate the effect of two components of the pretraining process on downstream task perfor-\nmance: model scale and data scale. Table 2 shows that increasing model size leads to improved\nperformance on downstream tasks. In some scenarios smaller models are barely able to perform better\n3\nModel SST-2 AGNews DBPedia Yahoo Amazon-2 Yelp-2 Average\nRandom Guess~ 50.6 27.4 7.27 10.2 52.9 50.4 33.1\nMajority Class~ 49.9 25.3 7.6 9.9 49.3 49.2 31.9\n117M All Data 51.8 / 0 40.2 / .00 39.6 / .25 26.1 / .97 50.3 / .001 50.1 / 0 43.0 / .202\n355M 1/4 Data 61.7 / 0 68.3 / .51 52.5 / .03 52.2 / .64 64.5 / .001 58.5 / 0 59.6 / .197\n355M All Data 62.5 / 0 65.5 / .01 44.8 / .62 49.5 / .30 80.2 / 0 74.7 / 0 62.9 / .176\n355M Finetuned~ 93.23 94.87 99.0 72.79 97.115 94.479 91.91\nSOTA 96.8* 95.51* 99.38* 76.26** 97.6* 98.45* 94\nTable 2: Zero shot transfer results. Seperated by a slash, each column contains test accuracies and\n(when applicable) the percentage of out of vocabulary test answers. Provided baseline models include\nrandom guessing~, majority class~, and ﬁnetuning~ baselines. State of the art results held by *XLNet\n(Yang et al., 2019) and **DRNN (Wang, 2018).\nthan random. For DBPedia the 355M GPT-2 model leads to a 45.2% absolute accuracy improvement\nover random. In tasks with several classes such as DBPedia, AGnews, and Yahoo Answers the model\nperforms noticeably better than random; however, they struggle to break past 50% and no task comes\nclose to achieving either ﬁnetuned or SOTA accuracies. Contextualizing these results with the results\nof the binary classiﬁcation tasks like SST-2, Amazon-2, and Yelp-2 we hypothesize that the model\ncan narrow down unlikely classes, but struggles to choose between the two most plausible options\ndue to its lack of formal supervision.\nThese results also show that restricting the size of the dataset and available document-title pairs leads\nto a reduction in overall task performance averaged across all tasks. This highlights the need for\npretraining across a diverse set of tasks and language. Table 2 demonstrates that the robustness of\nour generative model is also similarly dictated by model and pretraining dataset size. Although rare\nacross all pretrained models, the out of distribution answers (generated answers that are not valid\nclasses) diminish with larger pretrained models and data. The most common out of vocab answer\nis an empty string where the model decides to immediately predict the end of text token. Other out\nof vocab answers are typically rearrangements of valid answer tokens. These are rare with greedy\ndecoding, but become more frequent when using other sampling methods such as top-k (Fan et al.,\n2018) or top-p nucleus sampling (Holtzman et al., 2019). In the case of Yahoo Answers the model\ncan combine two categories such as \"Education & Reference\" with \"Science & Mathematics\" to\noutput \"Education & Mathematics\". We perform further studies examining the relationship between\nquestion descriptions, tokenization, accuracy, and out of vocabulary answers in appendix section\nA.4.2. These studies showcase the model’s ability to adapt to different descriptions, but expose issues\nwith controllability. Nevertheless, with this model the practitioner’s burden is shifted away from\ndesigning effective zero-shot multitask architectures, to data problem design.\n4 Conclusion and Future Work\nIn this work, we present a novel pretraining method for zero shot language classiﬁcation through\na generative language model classiﬁer. By generating classiﬁcations through natural language, the\nmodel eliminates the need for multiple task-speciﬁc classiﬁcation heads, making the model far more\ngeneral and ﬂexible. Increasing model and data scale further demonstrates that the capabilities of\nrecent transformer language models are sufﬁcient to extract meaningful feature representations that\nallow us to better generalize and adapt to new tasks. These results highlight the potential of natural\nlanguage as learning and adaptation signals in future applications.\nCurrently this work is employed for zero-shot classiﬁcation. Future extensions should investigate\nthe ability of gradient based metalearning to adapt to task descriptors, either through K-shot support-\nbased learning or by taking gradient steps on the task descriptors themselves as in Metz et al. (2018).\nAdditionally, future work could extend the text classiﬁcation task to other language problems such\nas question answer or instruction following. Applying this technique in other settings will require\naddressing its current limitations with respect to controllability, available data and task diversity.\n4\nReferences\nBansal, T., R. Jha, and A. McCallum\n2019. Learning to few-shot learn across diverse natural language classiﬁcation tasks.arXiv preprint\narXiv:1911.03863.\nBranavan, S. R., H. Chen, L. S. Zettlemoyer, and R. Barzilay\n2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint\nConference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference\non Natural Language Processing of the AFNLP: Volume 1-Volume 1, Pp. 82–90. Association for\nComputational Linguistics.\nChen, D. L. and R. J. Mooney\n2011. Learning to interpret natural language navigation instructions from observations. In Twenty-\nFifth AAAI Conference on Artiﬁcial Intelligence.\nCo-Reyes, J. D., A. Gupta, S. Sanjeev, N. Altieri, J. DeNero, P. Abbeel, and S. Levine\n2018. Guiding policies with language via meta-learning. CoRR, abs/1811.07882.\nFan, A., M. Lewis, and Y . N. Dauphin\n2018. Hierarchical neural story generation. CoRR, abs/1805.04833.\nFinn, C., P. Abbeel, and S. Levine\n2017. Model-agnostic meta-learning for fast adaptation of deep networks. CoRR, abs/1703.03400.\nGu, J., Y . Wang, Y . Chen, K. Cho, and V . O. K. Li\n2018. Meta-learning for low-resource neural machine translation. CoRR, abs/1808.08437.\nHoltzman, A., J. Buys, M. Forbes, and Y . Choi\n2019. The curious case of neural text degeneration. CoRR, abs/1904.09751.\nHuang, P., C. Wang, R. Singh, W. Yih, and X. He\n2018. Natural language to structured query generation via meta-learning. CoRR, abs/1803.02400.\nKingma, D. P. and J. Ba\n2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\nLehmann, J., R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey,\nP. Van Kleef, S. Auer, et al.\n2015. Dbpedia–a large-scale, multilingual knowledge base extracted from wikipedia. Semantic\nWeb, 6(2):167–195.\nLoshchilov, I. and F. Hutter\n2019. Decoupled weight decay regularization. In International Conference on Learning Represen-\ntations.\nMcCann, B., N. S. Keskar, C. Xiong, and R. Socher\n2018. The natural language decathlon: Multitask learning as question answering. CoRR,\nabs/1806.08730.\nMetz, L., N. Maheswaranathan, B. Cheung, and J. Sohl-Dickstein\n2018. Meta-learning update rules for unsupervised representation learning. arXiv preprint\narXiv:1804.00222.\nMicikevicius, P., S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston,\nO. Kuchaiev, G. Venkatesh, and H. Wu\n2017. Mixed precision training. CoRR, abs/1710.03740.\nMishra, N., M. Rohaninejad, X. Chen, and P. Abbeel\n2017. Meta-learning with temporal convolutions. CoRR, abs/1707.03141.\nPeterson, J., S. Meylan, and D. Bourgin\n2019. Open clone of openai’s unreleased webtext dataset scraper.\n5\nRadford, A., K. Narasimhan, T. Salimans, and I. Sutskever\n2018. Improving language understanding by generative pre-training.\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever\n2019. Better language models and their implications.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu\n2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\nSantoro, A., S. Bartunov, M. Botvinick, D. Wierstra, and T. P. Lillicrap\n2016. One-shot learning with memory-augmented neural networks. CoRR, abs/1605.06065.\nSocher, R., A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts\n2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceed-\nings of the 2013 conference on empirical methods in natural language processing, Pp. 1631–1642.\nSrivastava, N., G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov\n2014. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine\nLearning Research, 15:1929–1958.\nWang, B.\n2018. Disconnected recurrent neural networks for text categorization. In Proceedings of the\n56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\nPp. 2311–2320.\nYang, Z., Z. Dai, Y . Yang, J. G. Carbonell, R. Salakhutdinov, and Q. V . Le\n2019. Xlnet: Generalized autoregressive pretraining for language understanding. CoRR,\nabs/1906.08237.\n6\nA Appendix\nA.1 Input Representation and Training Details\nA.1.1 Input Tokens\nTo form the input representation, the question, text, and answer tokens are concatenated together.\nEach set of tokens has a <|endoftext|> token appended to the end, and has a special prompt\ntoken prepended to the set. The special tokens for the three ﬁelds are respectively <|question|>,\n<|text|>, and <|answer|>. In addition to prompt tokens, each segment of the input also has unique\ntype token embeddings added. There are three different type tokens total, one for each segment of\nthe input. Lastly, to encode positional information in our input representation we utilize two sets\nof position embeddings. One range of position ids up to and including the <|answer|> prompt\ntoken, and another set of ids starting from 0 at the beginning of the answer tokens. These ranges are\ndepicted by the colored gradient in the ﬁgure above. This helps the transformer distinguish between\nthe context and the generated output.\nA.1.2 Multiple Choice Format\nWe maintain a list of approximately 25 multiple choice question formats as shown below. At training\nand evaluation time we randomly sample a question format and ﬁll the brackets with the desired\nclasses. We format the classes as a comma separated list with double quotation marks to help segment\nthe answers from the rest of the question text. We ensure that spaces are put between the answers and\nthe quotation marks to avoid any unwanted byte pair merges: “ class1 ” , “ class2 ” , or “\nclass3 ”. Examples of this formatting can be seen in Table 1.\n• To which category does the following document belong? : {}\n• To which category does the following text belong? : {}\n• To which category does the text belong? : {}\n• To which category does the article belong? : {}\n• How would you describe the following document? : as {}\n• How would you describe the text? : as {}\n• How would you describe the following text? : as {}\n• Which best describes the text? : {}\n• Which best describes the document? : {}\n• Which best describes the following document? : {}\n• Which best describes the following text? : {}\n• The following document is _ ? : {}\n• The following text is _ ? : {}\n• The text is _ ? : {}\n• The document is _ ? : {}\n• How is the text best described? : {}\n• How is the document best described? : {}\n7\n• How is the following text best described? : {}\n• How is the following document best described? : {}\n• Which of these choices best describes the text? : {}\n• Which of these options best describes the text? : {}\n• Which of these choices best describes the document? : {}\n• Which of these options best describes the document? : {}\n• Which of these categories best describes the following document? : {}\n• Which of these choices best describes the following document? : {}\n• Which of these options best describes the following text? : {}\nA.2 Training Hyperparameters\nTo train our model we follow a procedure largely based on the training procedures described in\nRadford et al. (2019) with a few differences. All training is performed with a maximum sequence\nlength of 512 tokens. In the full dataset training setting we utilize a learning rate of 4 ×10−5 and\na batch size of 128. When training with a quarter of the dataset we then used a learning rate of\n3 ×10−5 and a batch size of 32. Our learning rate has a warmup period over 1% of the total training\niterations before decaying according to a single cycle cosine decay schedule over 10 epochs. We\nutilize an Adam optimizer (Kingma and Ba, 2014) with decoupled weight decay (Loshchilov and\nHutter, 2019) λ = 0.01. All our models are trained efﬁciently on V100 GPUs by utilizing mixed\nprecision training with dynamic loss scaling (Micikevicius et al., 2017). Additionally, we use global\ngradient norm clipping of 1.0 to improve the stability of training large models. Lastly, we utilize\nattention and hidden state dropout (Srivastava et al., 2014) values of 0.1.\nA.3 Training Data Statistics\nWe provide class frequency statistics shown below to highlight the diversity of the dataset used for\npretraining.\nFigure 2: Subreddit Class Distribution. The number of times a subreddit occurs (frequency) is\npresented on the x-axis. The y-axis corresponds to the number of subreddits that appear at a certain\nfrequency.\nThe data is distributed according to a power law distribution clustered around <1000 samples per\nsubreddit, with a long tail reaching up to 245000 samples for a given subreddit. Zooming into the\ndistribution (shown below) we ﬁnd that there are approximately 9400 subreddits with 20 or more\n8\nsamples out of 50700 subreddit. Out of the 9400 subreddit two thirds have fewer than 100 samples.\nThis level of diversity is ideal for a meta learning or domain adaptation dataset.\nFigure 3: Enlarged Subreddit Class Distribution.\nLastly we show the most common subreddits along with their subreddit frequency in Table A.3. We\nﬁnd that half of the top ﬁfteen subreddits are politically related. This skew may lead to possible biases\nin the training process. A plausible explanation for this bias can be found in the way the dataset\nis collected. Since we heuristically ﬁlter for reputable outbound links it is likely that we choose\nsubreddits where people post outside news.\nSubreddit Frequency\nr/politics 245308\nr/worldnews 122884\nr/The_Donald 80042\nr/todayilearned 59892\nr/news 59166\nr/technology 54860\nr/science 46452\nr/Conservative 30823\nr/POLITIC 28310\nr/conspiracy 28293\nr/india 27892\nr/environment 26816\nr/atheism 25999\nr/programming 24020\nr/Libertarian 23711\nTable 3: Subreddit Frequency.\n9\nA.4 Downstream Task Setup\nA.4.1 Class Descriptors\nListed below are the class descriptions used for each classiﬁcation task.\nDataset Classes\nSST-2 Positive Sentiment, Negative Sentiment\nAGNews Science & Technology, Business, Sports , World News\nDBPedia Company, Mean Of Transportation, Film, Ofﬁce Holder, Written Work, Animal, Natural\nPlace, Artist, Plant, Athlete, Album, Building, Village, Educational Institution\nYahoo Answers Family & Relationships, Business & Finance, Health, Society & Culture, Education\n& Reference, Entertainment & Music, Science & Mathematics, Computers & Internet,\nSports, Politics & Government\nYelp-2 Positive polarity, Negative polarity\nAmazon-2 Positive polarity, Negative polarity\nA.4.2 Descriptor Selection\nThe ability of our model to adapt to new tasks and its behavior for a given input is controlled by\nthe input descriptor questions it receives. In this section we investigate the impact that question\nformulation has on downstream task performance. Speciﬁcally, we modify the provided class\ndescriptions for several tasks and observe the effects this has on the 355 million parameter model’s\ndownstream task performance:\n• For binary classiﬁcation tasks like SST-2, Amazon-2, Yelp-2 we move away\nfrom Positive Sentiment and Negative Sentiment, or Positive polarity and\nNegative polarity. Instead we simply use positiveand negativeas in McCann et al.\n(2018).\n• For DBPedia we revert to the original class descriptions provided by the dataset and remove\nall whitespace (eg. Mean Of Transportationbecomes MeanOfTransportation).\n• For AGNews we also revert to the original class descriptions and change World Newsto\nWorldand Science & Technologyto Sci/Tech.\nTable A.4.2 shows that the choice of class description has a signiﬁcant impact on performance. In the\nworst case poor class descriptions can lead to an absolute 27% drop in accuracy and 44% increase in\nout of vocabulary answers. In the cases of binary classiﬁcation tasks and AGNews we hypothesize\nperformance is negatively impacted by incomplete task descriptions: positiveand Worlddo not\nexplicitly convey positive sentiment or World News. Empirical observations in Figure 4 show that\nthe model either selects plausibly overlapping categories in the case of AGNews, or responds with a\ncompletely out of vocabulary answer as in the case of sentiment analysis. For DBPedia and AGNews,\nconcatenating words together drastically changes the resulting bytepair tokenization despite the\ndescriptions still being human readable. This changes the semantic understanding that the model\nreceives and as a result the model completely avoids selecting it. In some cases the model may not\nhave ever trained the subword embeddings corresponding to those tokens. This section highlights that\nour language modeling technique, while general, is subject to errors arising from problem formulation\nand requires careful control to craft questions that elicit desired effects. Remedying these issues will\nbe a goal of future work.\nDescriptor Set SST-2 AGNews DBPedia Amazon-2 Yelp-2\nGood Descriptors 63.22 / 0 69.04 / .478 53.85 / .056 81.22 / .056 74.35 / 0\nBad Descriptors 35.91 / 44.3 62.61 / 0 44.99 / .050 64.3 / 22.1 68.02 / 23.4\nTable 4: Validation Set Accuracy/Out of V ocabulary Answer Percentages. We compare performance\non the validation set with two different sets of descriptors: one deemed good and one deemed bad.\nWe showcase the importance of selecting appropriate descriptors for a task.\n10\nFigure 4: Confusion matrices for several classiﬁcation tasks. The left column corresponds to the ﬁrst\nrow in Table A.4.2, and the right column corresponds to the second row. The color represents the\nprediction frequency with green being the highest, red the lowest, and yellow in the middle.\n11"
}