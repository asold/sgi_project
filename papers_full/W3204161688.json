{
  "title": "Scene Transformer: A unified architecture for predicting multiple agent trajectories",
  "url": "https://openalex.org/W3204161688",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221498878",
      "name": "Ngiam, Jiquan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287291493",
      "name": "Caine, Benjamin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2930782950",
      "name": "Vasudevan Vijay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1474586170",
      "name": "Zhang Zhengdong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287291496",
      "name": "Chiang, Hao-Tien Lewis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3125069701",
      "name": "Ling, Jeffrey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222258272",
      "name": "Roelofs, Rebecca",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2755949831",
      "name": "Bewley, Alex",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1972157489",
      "name": "Liu Chenxi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287291501",
      "name": "Venugopal, Ashish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4264265029",
      "name": "Weiss, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222868362",
      "name": "Sapp, Ben",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123157796",
      "name": "Chen, Zhifeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2927011765",
      "name": "Shlens, Jonathon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3106944564",
    "https://openalex.org/W3081405043",
    "https://openalex.org/W3107552218",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W3204875639",
    "https://openalex.org/W3132535424",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3169575318",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3139491754",
    "https://openalex.org/W2949269657",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3160050461",
    "https://openalex.org/W2794787653",
    "https://openalex.org/W1504992226",
    "https://openalex.org/W2983397630",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W3035339264",
    "https://openalex.org/W3181350748",
    "https://openalex.org/W3062588417",
    "https://openalex.org/W2963858432",
    "https://openalex.org/W3099312963",
    "https://openalex.org/W3034722190",
    "https://openalex.org/W2950268548",
    "https://openalex.org/W2055207897",
    "https://openalex.org/W3028769608",
    "https://openalex.org/W3108486966",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3156216502",
    "https://openalex.org/W3165444337",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970116586",
    "https://openalex.org/W2955189650",
    "https://openalex.org/W3073032883",
    "https://openalex.org/W2424778531",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W2606722458",
    "https://openalex.org/W3090789587",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W3135588948",
    "https://openalex.org/W3032731803",
    "https://openalex.org/W2532516272",
    "https://openalex.org/W2953212265",
    "https://openalex.org/W3129901004",
    "https://openalex.org/W3133707750",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3208318476",
    "https://openalex.org/W3124838366",
    "https://openalex.org/W3012478636",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2982745079",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2154844948",
    "https://openalex.org/W3132577852",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W3097237405",
    "https://openalex.org/W3048966129",
    "https://openalex.org/W2968008415",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3205301818",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3106547440",
    "https://openalex.org/W2980087597",
    "https://openalex.org/W2148361676",
    "https://openalex.org/W2953494151",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2967177252",
    "https://openalex.org/W2940129212",
    "https://openalex.org/W2607296803"
  ],
  "abstract": "Predicting the motion of multiple agents is necessary for planning in dynamic environments. This task is challenging for autonomous driving since agents (e.g. vehicles and pedestrians) and their associated behaviors may be diverse and influence one another. Most prior work have focused on predicting independent futures for each agent based on all past motion, and planning against these independent predictions. However, planning against independent predictions can make it challenging to represent the future interaction possibilities between different agents, leading to sub-optimal planning. In this work, we formulate a model for predicting the behavior of all agents jointly, producing consistent futures that account for interactions between agents. Inspired by recent language modeling approaches, we use a masking strategy as the query to our model, enabling one to invoke a single model to predict agent behavior in many ways, such as potentially conditioned on the goal or full future trajectory of the autonomous vehicle or the behavior of other agents in the environment. Our model architecture employs attention to combine features across road elements, agent interactions, and time steps. We evaluate our approach on autonomous driving datasets for both marginal and joint motion prediction, and achieve state of the art performance across two popular datasets. Through combining a scene-centric approach, agent permutation equivariant model, and a sequence masking strategy, we show that our model can unify a variety of motion prediction tasks from joint motion predictions to conditioned prediction.",
  "full_text": "Published as a conference paper at ICLR 2022\nSCENE TRANSFORMER : A UNIFIED ARCHITECTURE\nFOR PREDICTING MULTIPLE AGENT TRAJECTORIES\nJiquan Ngiam∗,1, Benjamin Caine∗,1, Vijay Vasudevan∗,1,\nZhengdong Zhang1, Hao-Tien Lewis Chiang2, Jeffrey Ling2,\nRebecca Roelofs1, Alex Bewley1, Chenxi Liu2, Ashish Venugopal2,\nDavid Weiss2, Ben Sapp2, Zhifeng Chen1, Jonathon Shlens1\n1Google Research, Brain Team,2 Waymo\n{jngiam,bencaine,vrv}@google.com\nABSTRACT\nPredicting the motion of multiple agents is necessary for planning in dynamic\nenvironments. This task is challenging for autonomous driving since agents (e.g.,\nvehicles and pedestrians) and their associated behaviors may be diverse and inﬂu-\nence one another. Most prior work have focused on predicting independent futures\nfor each agent based on all past motion, and planning against these independent\npredictions. However, planning against independent predictions can make it chal-\nlenging to represent the future interaction possibilities between different agents,\nleading to sub-optimal planning. In this work, we formulate a model for predicting\nthe behavior of all agents jointly, producing consistent futures that account for\ninteractions between agents. Inspired by recent language modeling approaches, we\nuse a masking strategy as the query to our model, enabling one to invoke a single\nmodel to predict agent behavior in many ways, such as potentially conditioned on\nthe goal or full future trajectory of the autonomous vehicle or the behavior of other\nagents in the environment. Our model architecture employs attention to combine\nfeatures across road elements, agent interactions, and time steps. We evaluate\nour approach on autonomous driving datasets for both marginal and joint motion\nprediction, and achieve state of the art performance across two popular datasets.\nThrough combining a scene-centric approach, agent permutation equivariant model,\nand a sequence masking strategy, we show that our model can unify a variety of\nmotion prediction tasks from joint motion predictions to conditioned prediction.\n1 I NTRODUCTION\nMotion planning in a dense real-world urban environment is a mission-critical problem for deploying\nautonomous driving technology. Autonomous driving is traditionally considered too difﬁcult for a\nsingle end-to-end learned system (Thrun et al., 2006). Thus, researchers have opted to split the task\ninto sequential sub-tasks (Zeng et al., 2019): (i) perception, (ii) motion prediction, and (iii) planning.\nPerception is the task of detecting and tracking objects in the scene from sensors such as LiDARs\nand cameras. Motion prediction involves predicting the futures actions of other agents in the scene.\nFinally, planning involves creating a motion plan that navigates through dynamic environments.\nDividing the larger problem into sub-tasks achieves optimal performance when each sub-task is truly\nindependent. However, such a strategy breaks down when the assumption of independence does not\nhold. For instance, the sub-tasks of motion prediction and planning are not truly independent—the\nautonomous vehicle’s actions may signiﬁcantly impact the behaviors of other agents. Similarly, the\nbehaviors of other agents may dramatically change what is a good plan. The goal of this work is\nto take a step in the direction of unifying motion prediction and planning by developing a model\nthat can exploit varying forms of conditioning information, such as the A V’s goal, and produce joint\nconsistent predictions about the future for all agents simultaneously.\nWhile the motion prediction task has traditionally been formulated around per-agent independent\npredictions, recent datasets (Ettinger et al., 2021; Zhan et al., 2019) have introduced interaction\nprediction tasks that enable us to study joint future prediction (Figure 1). These interaction prediction\n1\narXiv:2106.08417v3  [cs.CV]  4 Mar 2022\nPublished as a conference paper at ICLR 2022\nmarginal prediction joint prediction\nFigure 1: Joint prediction provides consistent motion prediction. Illustration of differences\nbetween marginal and joint motion prediction. Each color represents a distinct prediction. Left:\nMarginal prediction for bottom center vehicle. Scores indicate likelihood of trajectory. Note that the\nprediction is independent of other vehicle trajectories. Right: Joint prediction for three vehicles of\ninterest. Scores indicate likelihood of entire scene consisting of trajectories of all three vehicles.\ntasks require models to predict the joint futures of multiple agents: models are expected to produce\nfuture predictions for all agents such that the agents futures are consistent 1 with one another.\nA naive approach to producing joint futures is to consider the exponential number of combinations of\nmarginal agent predictions. Many of the combinations are not consistent, especially when agents\nhave overlapping trajectories. We present a uniﬁed model that naturally captures the interactions\nbetween agents, and can be trained as a joint model to produce scene-level consistent predictions\nacross all agents (Figure 1, right). Our model uses a scene-centric representation for all agents (Lee\net al., 2017; Hong et al., 2019; Casas et al., 2020a; Salzmann et al., 2020) to allow scaling to large\nnumbers of agents in dense environments. We employ a simple variant of self-attention (Vaswani\net al., 2017) in which the attention mechanism is efﬁciently factorized across the agent-time axes. The\nresulting architecture simply alternates attention between dimensions representing time and agents\nacross the scene, resulting in a computationally-efﬁcient, uniform, and scalable architecture.\nWe ﬁnd that the resulting model, termed Scene Transformer, achieves superior performance on both\nindependent (marginal) and interactive (joint) prediction benchmarks.\nMoreover, we demonstrate a novel formulation of the task using a masked sequence model, inspired by\nrecent advances in language modeling (Brown et al., 2020; Devlin et al., 2019), to allow conditioning\nof the model on the autonomous vehicle (A V) goal state or full trajectory. In this reformulation, a\nsingle model can naturally perform tasks such as motion prediction, conditional motion prediction,\nand goal-conditioned prediction simply by changing which data is visible at inference time.\nWe hope that our uniﬁed architecture and ﬂexible problem formulation opens up new research\ndirections for further combining motion prediction and planning. In summary, our key contributions\nin this work are:\n• A novel, scene-centric approach that allows us to gracefully switch training the model to\nproduce either marginal (independent) and joint agent predictions in a single feed-forward\npass. Our model achieves state-of-the-art on both marginal and joint prediction tasks on both\nthe Argoverse and the Waymo Open Motion Dataset.\n• A permutation equivariant Transformer-based architecture factored over agents, time, and\nroad graph elements that exploits the inherent symmetries of the problem. The resulting\narchitecture is efﬁcient and integrates the world state in a uniﬁed way.\n• A masked sequence modeling approach that enables us to condition on hypothetical agent fu-\ntures at inference time, enabling conditional motion prediction or goal conditioned prediction.\n2 R ELATED WORK\nMotion prediction architectures. Motion prediction models have ﬂourished in recent years, due\nto the rise in interest in self-driving applications and the release of related datasets and bench-\n1Marginal agent predictions may conﬂict with each other (have overlaps), while consistent joint predictions\nshould have predictions where agents respect each other’s behaviors (avoid overlaps) within the same future.\n2\nPublished as a conference paper at ICLR 2022\nmarks (Kesten et al., 2019; Chang et al., 2019; Caesar et al., 2020; Ettinger et al., 2021). Successful\nmodels must take into account the history of agent motion, and the elements of the road graph (e.g.,\nlanes, stop lines, trafﬁc light dynamic state). Furthermore, such models must learn the relationships\nbetween these agents in the context of the road graph environment.\nOne class of models draws heavily upon the computer vision literature, rendering inputs as a multi-\nchannel rasterized top-down image (Cui et al., 2019; Chai et al., 2019; Lee et al., 2017; Hong et al.,\n2019; Casas et al., 2020a; Salzmann et al., 2020; Zhao et al., 2019). In this approach, relationships\nbetween scene elements are captured via convolutional deep architectures. However, the localized\nstructure of the receptive ﬁeld makes capturing spatially-distant interactions challenging. A popular\nalternative is to use an entity-centric approach. With this approach, agent state history is typically\nencoded via sequence modeling techniques like RNNs (Mercat et al., 2020; Khandelwal et al., 2020;\nLee et al., 2017; Alahi et al., 2016; Rhinehart et al., 2019) or temporal convolutions (Liang et al.,\n2020). Road elements are approximated with basic primitives (e.g. piecewise-linear segments)\nwhich encode pose information and semantic type. Modeling relationships between entities is often\npresented as an information aggregation process, and models employ pooling (Zhao et al., 2020; Gao\net al., 2020; Lee et al., 2017; Alahi et al., 2016; Gupta et al., 2018), soft-attention (Mercat et al., 2020;\nZhao et al., 2020; Salzmann et al., 2020) as well as graph neural networks (Casas et al., 2020a; Liang\net al., 2020; Khandelwal et al., 2020).\nLike our proposed method, several recent models use Transformers (Vaswani et al., 2017), composed\nof multi-headed attention layers. Transformers are a popular state-of-the-art choice for sequence\nmodeling in natural language processing (Brown et al., 2020; Devlin et al., 2019), and have recently\nshown promise in core computer vision tasks such as detection (Bello et al., 2019; Carion et al., 2020;\nSrinivas et al., 2021), tracking (Hung et al., 2020) and classiﬁcation (Ramachandran et al., 2019;\nVaswani et al., 2021; Dosovitskiy et al., 2021; Bello, 2013; Bello et al., 2019). For motion modeling,\nrecent work has employed variations of self-attention and Transformers for modeling different axes:\ntemporal trajectory encoding and decoding (Yu et al., 2020; Giuliari et al., 2020; Yuan et al., 2021),\nencoding relationships between agents (Li et al., 2020; Park et al., 2020; Yuan et al., 2021; Yu et al.,\n2020; Mercat et al., 2020; Bhat et al., 2020), and encoding relationships with road elements. When\napplying self-attention over multiple axes, past work used independent self-attention for each axis (Yu\net al., 2020), or ﬂattened two axes together into one joint self-attention layer (Yuan et al., 2021) – by\ncomparison, our method proposes axis-factored attention to model relationships between time steps,\nagents, and road graph elements in a uniﬁed way.\nScene-centric versus agent-centric representations. Another key design choice is the frame of\nreference in which the representation is encoded. Some models do a majority of modeling in a\nglobal, scene-level coordinate frame, such as work that employs a rasterized top-down image (Cui\net al., 2019; Chai et al., 2019; Lee et al., 2017; Hong et al., 2019; Casas et al., 2020a; Salzmann\net al., 2020). This can lead to a more efﬁcient model due to a single shared representation of world\nstate in a common coordinate frame, but comes with the potential sacriﬁce of pose-invariance. On\nthe other hand, models that reason in the agent-coordinate frame (Mercat et al., 2020; Zhao et al.,\n2020; Khandelwal et al., 2020) are intrinsically pose-invariant, but scale linearly with the number\nof agents, or quadratically with the number of pairwise interactions between agents. Many works\nemploy a mix of a top-down raster representation for road representation fused with a per-agent\nrepresentations (Rhinehart et al., 2019; Tang & Salakhutdinov, 2019; Lee et al., 2017). Similar to our\nown work, LaneGCN (Liang et al., 2020) is agent-centric yet representations are in a global frame – to\nthe best of our knowledge, this is the only other work to do so. This enables efﬁcient reasoning while\ncapturing arbitrarily distant interactions and high-ﬁdelity state representations without rasterization.\nRepresenting multi-agent futures. A common way to represent agent futures is via a weighted set\nof trajectories per agent (Alahi et al., 2016; Biktairov et al., 2020; Buhet et al., 2020; Casas et al.,\n2020a;a; Chai et al., 2019; Cui et al., 2019; Gao et al., 2020; Hong et al., 2019; Lee et al., 2017;\nMarchetti et al., 2020; Mercat et al., 2020; Salzmann et al., 2020; Zhao et al., 2020; Chandra et al.,\n2018). This representation is encouraged by benchmarks which primarily focus on per-agent distance\nerror metrics (Caesar et al., 2020; Chang et al., 2019; Zhan et al., 2019). We argue in this work that\nmodeling joint futures in a multi-agent environment (Figure 1, right) is an important concept that\nhas been minimally explored in prior work. Some prior work consider a factorized pairwise joint\ndistribution, where a subset of agent futures are conditioned on other agents – informally, modeling\nP(X) and P(Y |X) for agents X and Y (Khandelwal et al., 2020; Tolstaya et al., 2021; Salzmann\net al., 2020). To generalize joint prediction to arbitrary multi-agent settings, other work (Tang\n3\nPublished as a conference paper at ICLR 2022\nSelf-attention (across time)\nAgent Features[A, T, D]\nAgent Features[A + 1, T + 1, D]\nAttention-based Encoder\nTile and MLP[F, A + 1, T + 1, D]\nAgent Features[A + 1, T + 1, D]\nAgent Features[F, A, T, 7]\nAttention-based Decoder\nSelf-attention (across time)\nRoad\ngraph\n[G, T, D]\nSelf-attention (across agents)\nSlice out extra agent/time dimension\nSelf-attention (across agents)\nAdd extra agent/time dimension\nCross-attention (agents to roadgraph)\nSelf-attention (across time)\nSelf-attention (across agents)\nCross-attention (agents to roadgraph)\nSelf-attention (across time)\nSelf-attention (across agents)\nx 3\nx 2\nMLP\nFigure 2: Single model architecture for multiple motion prediction tasks.Left: Different masking\nstrategies deﬁne distinct tasks. The left column represents current time and the top row represents\nthe agent indicating the autonomous vehicle (A V). A single model can be trained for data associated\nwith motion prediction, conditional motion prediction, and goal-directed prediction, by matching\nthe masking strategy to each prediction task. Right: Attention-based encoder-decoder architecture\nfor joint scene modeling. Architecture employs factored attention along the time and agent axes to\nexploit the dependencies in the data, and cross-attention to inject side information.\n& Salakhutdinov, 2019; Rhinehart et al., 2019; Casas et al., 2020b; Suo et al., 2021; Yeh et al.,\n2019) iteratively roll out samples per-agent, where each agent is conditioned on previously sampled\ntrajectory steps. In contrast, our model directly decodes a set of k distinct joint futures with associated\nlikelihoods.\n3 M ETHODS\nThe Scene Transformer model has three stages: (i) Embed the agents and the road graph into a high\ndimensional space, (ii) Employ an attention-based network to encode the interactions between agents\nand the road graph, (iii) Decode multiple futures using an attention-based network. The model takes\nas input a feature for every agent at every time step, and also predicts an output for every agent\nat every time step. We employ an associated mask, where every agent time step has an associated\nindicator of 1 (hidden) or 0 (visible), indicating whether the input feature is hidden (i.e. removed)\nfrom the model. This approach mirrors the approach of masked-language models such as BERT\n(Devlin et al., 2019). The approach is ﬂexible, enabling us to simultaneously train a single model for\nmotion prediction (MP) (Cui et al., 2019; Chai et al., 2019; Lee et al., 2017; Hong et al., 2019; Casas\net al., 2020a; Salzmann et al., 2020; Casas et al., 2020a; Liang et al., 2020; Khandelwal et al., 2020),\nconditional motion prediction (CMP) (Khandelwal et al., 2020; Tolstaya et al., 2021; Salzmann et al.,\n2020) and goal-conditioned prediction (GCP) (Deo & Trivedi, 2020) simply by changing what data\nis shown to the model (Figure 2, left). We summarize the key contributions below, and reserve details\nfor the Appendix.\nMulti-task representation. The key representation in the model is a 3-dimensional tensor of A\nagents with D feature dimensions across T time steps. At every layer within the architecture, we aim\nto maintain a representation of shape [A, T, D], or when decoding, [F, A, T, D] across F potential\nfutures. Each task (MP, CMP, GCP) can be formulated as a query with a speciﬁc masking strategy by\nsetting the indicator mask to 0, thus providing that data to the model (Figure 2, left). The goal of the\nmodel is to impute the features for each shaded region corresponding to subsets of time and agents in\nthe scenario that are masked.\n3.1 S CENE -CENTRIC REPRESENTATION FOR AGENTS AND ROAD GRAPHS .\nWe use a scene-centric embedding where we use an agent of interest’s position as the origin 2,\nand encode all roadgraph and agents with respect to it. This is contrast to approaches which use\nan agent-centric representation, where the representations are computed separately for each agent,\ntreating each agent in turn as the origin.\n2For WOMD, we center the scene with respect to the autonomous vehicle (A V). For Argoverse, we center\nthe scene with respect to the agent that needs to be predicted. Both are centered around what would be the last\nvisible time step in a motion prediction setup for all tasks.\n4\nPublished as a conference paper at ICLR 2022\nIn detail, we ﬁrst generate a feature for every agent time step if that time step is visible. Second, we\ngenerate a set of features for the static road graph, road elements static in space and time, learning one\nfeature vector per polyline (with signs being polylines of length 1) using a PointNet (Qi et al., 2017).\nLast, we generate a set of features for the dynamic road graph, which are road elements static in\nspace but dynamic in time (e.g. trafﬁc lights), also one feature vector per object. All three categories\nhave xyz position information, which we preprocess to center and rotate around the agent of interest\nand then encode with sinusoidal position embeddings (Vaswani et al., 2017).\n3.2 E NCODING TRANSFORMER\nWe focus on a simple encoder-decoder attention-based architecture which maintains a representation\nof [A, T, D] throughout (Figure 2, right). We summarize the architecture brieﬂy, but reserve details\nfor the Appendix and Table 5. The majority of layers are a form of the Transformer layer (Vaswani\net al., 2017) (Table 6). Attention layers are parameterized as matrices representing the query Q, key\nK, and value V , whose output y = softmax (QKT )V√dimk\n. Each matrix is computed as a learned linear\ntransformation of the underlying representation x, e.g. Q = Wqx. Each attention layer is followed\nby a feed-forward layer of the same hidden dimension, and a skip connection addition of the result\nwith the input to the whole Transformer layer. All layers of the encoder and decoder employ a D\nfeature dimension. The ﬁnal layer after the decoder is a 2-layer MLP that predicts 7 outputs. The\nﬁrst 6 outputs correspond to the 3-dimensional position of an agent at a given time step in absolute\ncoordinates (e.g. meters) with respect to the agent of interest, and the corresponding uncertainty\nparameterized by a Laplace distribution (Meyer & Thakurdesai, 2020). The remaining dimension\npredicts the heading.\nEfﬁcient factorized self-attention. The bulk of the computation is performed with a Transformer\n(Vaswani et al., 2017) (Table 6). One naive approach to use the Transformer would be to perform\nattention directly on the entire set of agent and time step features (i.e., attention acrossAT dimensions).\nHowever, this approach is computationally expensive, and also suffers from an identity symmetry\nchallenge: since we do not add any speciﬁc agent identity indicator, two agents of the same type\nwith the same masked future time-step will have the same input representation to the transformer,\nresulting in the same output. Thus, we design a factorized attention based on the time and agents\naxes (for related ideas, see Wang et al. (2020); Szegedy et al. (2016); Ho et al. (2019)).\nApplying attention only across time allows the model to learn smooth trajectories independent of\nthe identity of the agent. Likewise, applying attention only across agents allows the model to learn\nmulti-agent interactions independent of the speciﬁc time step. Finally, in order to capture both time\nand agent dependencies, the model simply alternates attention across agents and time in subsequent\nlayers (Figure 2, right panel). The model is also permutation equivariant to the ordering of the agents\nat input, since the attention operation is permutation equivariant.\nCross-attention. In order to exploit side information, which in our case is a road graph, we use\ncross-attention to enable the agent features to be updated by attending to the road graph. Concretely,\nwe calculate the queries from the agents, but the keys and values come from the embeddings of the\nroad graph. The road graph embeddings are ﬁnal after the per-polyline PointNet, and therefore not\nupdated during these attention layers. This requires that the model learn interactions between the\nroad structure and agents that are independent of the speciﬁc time step or agent. We highlight that the\nroad graph representation is also permutation-equivariant and shared across all agents in the scene,\nwhereas prior approaches have often used a per-agent road graph representation.\n3.3 P REDICTING PROBABILITIES FOR EACH FUTURES .\nOur model also needs to predict a probability score for each future (in the joint model) or trajectory\n(in the marginal model). In order to do so, we need a feature representation that summarizes the scene\nand each agent. After the ﬁrst set of factorized self-attention layers, we compute the mean of the\nagent features tensor across the agent and time dimension separately, and add these as an additional\nartiﬁcial agent and time making our internal representation [A + 1, T+ 1, D] (Figure 2, left panel).\nThis artiﬁcial agent and time step propagates through the network, and provides the model with extra\ncapacity for representing each agent, that is not tied to any timestep. At the ﬁnal layer, we slice out\nthe artiﬁcial agent and time step to obtain summary features for each agent (the additional time per\n5\nPublished as a conference paper at ICLR 2022\nagent), and for the scene (the ‘corner’ feature that is both additional time and agent). This feature is\nthen processed by a 2-layer MLP producing a single logit value that we use with a softmax classiﬁer\nfor a permutation equivariant estimate of probabilities for each futures.\n3.4 J OINT AND MARGINAL LOSS FORMULATION .\nThe output of our model is a tensor of shape[F, A, T,7] representing the location and heading of each\nagent at the given time step. Because the model uses a scene-centric representation for the locations\nthrough positional embeddings, the model is able to predict all agents simultaneously in a single\nfeed-forward pass. This design also makes it possible to have a straight-forward switch between joint\nfuture predictions and marginal future predictions.\nTo perform joint future prediction, we treat each future (in the ﬁrst dimension) to be coherent futures\nacross all agents. Thus, we aggregate the displacement loss across all agents 3 and time steps to\nbuild a loss tensor of shape [F]. We only back-propagate the loss through the individual future\nthat most closely matches the ground-truth in terms of displacement loss (Gupta et al., 2018; Yeh\net al., 2019). For marginal future predictions, each agent is treated independently. After computing\nthe displacement loss of shape [F, A], we do not aggregate across agents. Instead, we select the\nfuture with minimum loss for each agent separately, and back-propagate the error correspondingly\n(Appendix, Figure 8).\nEvaluation metrics for motion prediction. We evaluate the quality of k weighted trajectory hy-\npotheses using the standard evaluation metrics: minADE, minFDE, miss rate, and mAP. Each\nevaluation metric attempts to measure how close the top k trajectories are to ground truth observation.\nA simple and common distance-based metric is to measure the L2 norm between a given trajectory\nand the ground truth (Alahi et al., 2016; Pellegrini et al., 2009). minADE reports the L2 norm of the\ntrajectory with the minimal distance. minFDE likewise reports the L2 norm of the trajectory with the\nsmallest distance only evaluated at the ﬁnal location of the trajectory. We additionally report the miss\nrate (MR) and mean average precision (mAP) to capture how well a model predicts all of the future\ntrajectories of agents probabilistically (Yeh et al., 2019; Chang et al., 2019; Ettinger et al., 2021). For\njoint future evaluation settings, we measure the scene-level equivalents (minSADE, minSFDE, and\nSMR) that evaluate the prediction of the best single consistent future (Casas et al., 2020b).\n4 R ESULTS\nWe evaluate the Scene Transformer on motion prediction tasks from the Argoverse dataset (Chang\net al., 2019) and Waymo Open Motion Dataset (WOMD) (Ettinger et al., 2021). The Argoverse dataset\nconsists of 324,000 run segments (each 5 seconds in length) from 290 km of roadway containing 11.7\nmillion agent tracks in total, and focuses on single agent (marginal) motion prediction. The WOMD\ndataset consists of 104,000 run segments (each 20 seconds in length) from 1,750 km of roadway\ncontaining 7.64 million unique agent tracks. Importantly, the WOMD has two tasks, each with their\nown set of evaluation metrics: a marginal motion prediction challenge that evaluates the quality of\nthe motion predictions independently for each agent (up to 8 per scene), and a joint motion prediction\nchallenge that evaluates the quality of a model’s joint predictions of exactly 2 agents per scene. We\ntrain each model on a Cloud TPU (Jouppi et al., 2017) See Appendix for all training details.\nFirst, in Section 4.1, we focus on the marginal prediction task and show that Scene Transformer\nachieves competitive results on both Argoverse (Chang et al., 2019) and the WOMD (Ettinger et al.,\n2021). In Section 4.2, we focus on the joint prediction task and train the Scene Transformer with\nour joint loss formulation (see Section 3.4). We show that with a single switch of the loss formula,\nwe can achieve superior joint motion prediction performance. In Section 4.3 we discuss factorized\nversus non-factorized attention. Finally, in Section 4.4 we show how our masked sequence model\nformulation allows us to train a multi-task model capable of motion prediction, conditional motion\nprediction, and goal-conditioned prediction. In Appendix B.1 we discuss the trade-off between\nmarginal and joint models.\n3Each dataset identiﬁes a subset of agents to be predicted. We only include this subset in our loss calculation.\nFor Argoverse, this is 1 agent; for WOMD this is 2-8 agents.\n6\nPublished as a conference paper at ICLR 2022\nMethod minADE ↓ minFDE ↓ MR ↓\nJean (Mercat et al., 2020) 0.97 1.42 0.13\nWIMP (Khandelwal et al., 2020) 0.90 1.42 0.17\nTNT (Zhao et al., 2020) 0.94 1.54 0.13\nLaneGCN (Liang et al., 2020) 0.87 1.36 0.16\nTPCN (Ye et al., 2021) 0.85 1.35 0.16\nmmTransformer (Liu et al., 2021b) 0.84 1.34 0.15\nHOME (Gilles et al., 2021) 0.94 1.45 0.10\nOurs (marginal) 0.80 1.23 0.13\nTable 1: Marginal predictive performance on Argoverse motion prediction. Results reported on\ntest split for vehicles (Chang et al., 2019). minADE, minFDE reported for k = 6predictions (Alahi\net al., 2016; Pellegrini et al., 2009); Miss Rate (MR) (Chang et al., 2019) within 2 meters of the target.\nAll results are reported for t = 3seconds.\nMotion Prediction minADE↓ minFDE↓ MR↓ mAP↑\nveh ped cyc veh ped cyc veh ped cyc veh ped cyc\nvalid\nLSTM baseline (Ettinger et al., 2021)1.34 0.63 1.26 2.85 1.35 2.68 0.25 0.13 0.29 0.23 0.23 0.20\nOurs (marginal) 1.17 0.59 1.15 2.51 1.26 2.44 0.20 0.12 0.24 0.26 0.27 0.20\ntest\nLSTM baseline (Ettinger et al., 2021)1.34 0.64 1.29 2.83 1.35 2.68 0.24 0.13 0.29 0.24 0.22 0.19\nReCoAt (Huang et al., 2021) 1.69 0.69 1.47 3.96 1.51 3.30 0.40 0.20 0.37 0.18 0.25 0.17\nSimpleCNNOnRaster (Konev et al., 2021)1.47 0.71 1.39 3.18 1.52 2.89 0.27 0.16 0.31 0.19 0.18 0.14\nDenseTNT (Gu et al., 2021) 1.35 0.85 2.17 3.35 1.40 2.94 0.20 0.13 0.23 0.28 0.28 0.21\nOurs (marginal) 1.17 0.60 1.17 2.48 1.25 2.43 0.19 0.12 0.22 0.27 0.23 0.20\nTable 2: Marginal predictive performance on Waymo Open Motion Dataset motion prediction.\nResults presented on the standard splits of the validation and test datasets (Ettinger et al., 2021)\nevaluated with traditional marginal metrics for t = 8seconds. minADE, minFDE reported for k = 6\npredictions (Alahi et al., 2016; Pellegrini et al., 2009); Miss Rate (MR) (Chang et al., 2019) within 2\nmeters of the target. See Appendix for t = 3or 5 seconds. We include the challenge winner results in\nthis table (Waymo, 2021).\n4.1 M ARGINAL MOTION PREDICTION\nWe ﬁrst evaluate the performance of Scene Transformer trained and evaluated as a traditional marginal,\nper-agent motion prediction model. This is analogous to the problem illustrated in Figure 1 (left).\nFor all results until Section 4.4, we use a masking strategy that provides the model with all agents as\ninput, but with their futures hidden. We also mask out future trafﬁc light information.\nArgoverse. We evaluate on the popular Argoverse (Chang et al., 2019) benchmark to demonstrate the\nefﬁcacy of our architecture. During training and evaluation, the model is only required to predict the\nfuture of the single agent of interest. Our best Argoverse model usesD = 512feature dimensions and\nlabel smoothing for trajectory classiﬁcation. Our model achieves state-of-the-art results compared to\npublished, prior work 4 in terms of minADE and minFDE (Table 1).\nWaymo Open Motion Dataset (WOMD). We next evaluate the performance of our model with\nD = 256on the recently released WOMD (Ettinger et al., 2021) for the marginal motion prediction\ntask. This task is a standard motion prediction task where up to 8 agents per scene are selected to\nhave their top 6 motion predictions evaluated independently. Our model trained with the marginal\nloss achieves state-of-the-art results on the minADE, minFDE, and miss rate metrics (Table 2).\n4.2 J OINT MOTION PREDICTION\nTo evaluate the effectiveness of Scene Transformer when trained with a joint loss formulation (Section\n3.4), we evaluate our model on the Interaction Prediction challenge in WOMD (Ettinger et al., 2021).\nThis task measures the performance of the model at predicting the joint future trajectories of two\n4We exclude comparing to public leaderboard entries that have not been published since their details are not\navailable, but note that our results are competitive on the leaderboard as of the submission date.\n7\nPublished as a conference paper at ICLR 2022\nInteraction Prediction minSADE↓ minSFDE↓ SMR↓ mAP↑\nveh ped cyc veh ped cyc veh ped cyc veh ped cyc\nvalid\nLSTM baseline (Ettinger et al., 2021)2.42 2.73 3.16 6.07 4.20 6.46 0.66 1.00 0.83 0.07 0.06 0.02\nOurs (marginal-as-joint) 2.04 1.62 2.28 4.94 3.81 5.67 0.54 0.63 0.72 0.11 0.05 0.03\nOurs (joint, MP-only) 1.72 1.38 1.96 3.98 3.11 4.75 0.49 0.60 0.73 0.11 0.05 0.03\nOurs (joint, multi-task) 1.72 1.39 1.94 3.99 3.15 4.69 0.49 0.62 0.71 0.11 0.06 0.04\ntest\nLSTM baseline (Ettinger et al., 2021)2.46 2.47 2.96 6.22 4.30 6.26 0.67 0.89 0.89 0.06 0.03 0.03\nHeatIRm4 (Mo et al., 2021)2.93 1.77 2.74 7.20 4.06 6.69 0.80 0.80 0.91 0.07 0.05 0.00\nOurs (marginal-as-joint) 2.08 1.62 2.24 5.04 3.87 5.41 0.55 0.64 0.73 0.08 0.05 0.03\nOurs (joint, MP-only) 1.76 1.38 1.95 4.08 3.19 4.65 0.50 0.62 0.70 0.10 0.05 0.04\nOurs (joint, multi-task) 1.74 1.41 1.95 4.06 3.26 4.68 0.50 0.64 0.71 0.13 0.04 0.03\nTable 3: Joint predictive performance on Waymo Open Motion Dataset motion prediction .\nResults presented on the interactive splits of the validation and test datasets (Ettinger et al., 2021)\nevaluated with scene-leveljoint metrics for t = 8seconds. minSADE, minSFDE for k = 6predictions\n(Alahi et al., 2016; Pellegrini et al., 2009); Miss Rate (MR) (Chang et al., 2019) within 2 meters of\nthe target. “S” indicates a scene-level joint metric. See Appendix for t = 3or 5 seconds. We include\nthe challenge winner results in this table (Waymo, 2021).\ninteracting agents (Figure 1, right), and employs joint variants of the common minADE, minFDE,\nand Miss Rate (MR) metrics denoted as minSADE, minSFDE, SMR. Note that the “ S” indicates\n“scene-level” metrics. These metrics aim to measure the quality and consistency of the two agents\njoint prediction - for example, the joint variant of Miss Rate (SMR) only records a \"hit\" if both\ninteracting agent’s predicted trajectories are within the threshold of their respective ground truths.\nWe ﬁnd that for the Interaction Prediction challenge our joint model’s joint predictions easily\noutperforms the WOMD provided baseline as well as a marginal version of the model converted into\na joint prediction 5 into joint predictions. (Table 3). This shows that beyond the strength of our overall\narchitecture and approach, that explicitly training a model as a joint model signiﬁcantly improves joint\nperformance on joint metrics. A notable observation is that even though the Interaction Prediction\ntask only requires predicting the joint trajectories of two agents, our method is fully general and\npredicts joint consistent futures of all agents.\n4.3 F ACTORIZED AGENTS SELF -ATTENTION\nFactorized self-attention confers two beneﬁts to our model: (a) it is more efﬁcient since the attention\nis over a smaller set, and (b) it provides an implicit identity to each agent during the attention across\ntime. We ran an experiment where we replaced each axis-factorized attention layer (each pair of time\nand agent factorized layer) with a non-axis factored attention layer. This increased the computational\ncost of the model and performed worse on the Argoverse validation dataset: the factorized version\nachieved a minADE of 0.609 with the factored version, and 0.639 with the non-factorized version.\n4.4 A DVANTAGES OF A MASKED SEQUENCE MODELING STRATEGY\nOur model is formulated as a masked sequence model (Devlin et al., 2019), where at training and\ninference time we specify which agent timesteps to mask from the model. This formulation allows\nus to select which information about any agent at any timestep to supply to the model, and measure\nhow the model exploits or responds to this additional information. We can express several motion\nprediction tasks at inference time in mask space (Figure 2), in effect providing a multi-task model.\nWe can use this unique capability to query the models for counterfactuals. What would the model\npredict given a subset of agents full trajectories (conditional motion prediction), or given a subset of\nagents ﬁnal goals (goal-conditioned motion prediction)? This feature could be particularly useful\nfor autonomous vehicle planning to predict what various future rollouts of the scene would look like\ngiven a desired goal for the autonomous vehicle (Figure 3).\n5Note that the output of a joint model can be directly used in a marginal evaluation. However, converting the\noutput of a marginal model into a joint evaluation is nuanced because there lacks an association of futures across\nagents (Figure 1). We employ a simple heuristic to convert the outputs of a marginal model for joint evaluation:\nwe take the top 6 pairs of trajectories from the combination of both agent’s trajectories for 36 total pairs, and\nretain the top 6 pairs with the highest product of probabilities.\n8\nPublished as a conference paper at ICLR 2022\nScenario A Scenario B Scenario C\nFigure 3: Goal-conditioned prediction navigates A V to selected goal positions.Rectangles indi-\ncate vehicles on the road. Lines indicate the road graph (RG) colored by the type of lane marker.\nCircles indicate predicted trajectory for each agent. Star indicates selected goal for A V . Each column\nrepresents a scenario in which the A V is directed to perform one of two actions taking into account\nthe dynamics of other agents. (A) A V instructed to either change lanes or remain in the same lane.\n(B) A V instructed to stop to allow oncoming vehicle to turn left, or adjust to the side in a narrow\npassage so that the oncoming vehicle has enough space to pass. (C) A V instructed to either proceed\nstraight or turn right at the intersection.\nIn the previous results, we used a preﬁx-mask during training and inference, which shows the ﬁrst\nfew steps and predicts the remaining timesteps for all agents. For this experiment, we employ\ntwo masking strategies to test on the interactive split of the WOMD, namely \"conditional motion\nprediction\" (CMP) where we show one of the two interacting agents’ full trajectory, and \"goal-\nconditioned motion prediction\" (GCP) where we show the autonomous vehicle’s desired goal state\n(Figure 2). We train a model using each strategy (including MP masking) 1/3 of the time, and\nevaluate the model on each all tasks. We ﬁnd that across the 44k evaluation segments, the multi-task\nmodel matches the performance of our MP-only trained joint model on both joint (Table 3) and\nmarginal metrics (see also Appendix, Table 11); multi-tasking training does not signiﬁcantly degrade\nstandard motion prediction performance. We qualitatively examine the performance of the multi-task\nmodel in a GCP setting, and observe that the joint motion predictions for the A V and non-A V agents\nﬂexibly adapt to selected goal points for the A V (Figure 3).\nWe note that dataset average metrics like min(S)ADE do not capture forecasting subtleties (Ivanovic\n& Pavone, 2021) that are critical for measuring progress in the ﬁeld. Unfortunately, the Waymo\nMotion Open Dataset Interactive Prediction set does not contain pre-deﬁned metadata necessary to\nevaluate speciﬁc interesting slices using different models. In Appendix C we provide some initial\nanalysis of slices that we could compute on the dataset, which illustrate how joint and marginal model\npredictions behavior differently as the scene a) scales with more agents in complicated scenes, b)\ndiffers in the average speed of agents. We also show that joint prediction models demonstratejoint\nconsistent futures via lower inter-prediction overlap compared to marginal models.\n5 D ISCUSSION\nWe propose a uniﬁed architecture for autonomous driving that is able to model the complex interac-\ntions of agents in the environment. Our approach enables a single model to perform motion prediction,\nconditional motion prediction, and goal-conditioned prediction. In the ﬁeld of autonomous driving,\nelaborations of this problem formulation may result in learning models for planning systems that\nquantitatively improve upon existing systems (Buehler et al., 2009; Montemerlo et al., 2008; Ziegler\net al., 2014; Zeng et al., 2019; Liu et al., 2021a). Likewise, such modeling efforts may be used to\ndirectly go after issues of identifying interacting agents in an environment, and potentially provide an\nimportant task for identifying causal relationships (Arjovsky et al., 2019; Schölkopf et al., 2021).\n9\nPublished as a conference paper at ICLR 2022\nREFERENCES\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike\nSchuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent\nVanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,\nMartin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. URL http://tensorflow.org/. Software available from\ntensorﬂow.org.\nAlexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio\nSavarese. Social LSTM: Human trajectory prediction in crowded spaces. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pp. 961–971, 2016.\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\narXiv preprint arXiv:1907.02893, 2019.\nIrwan Bello. LambdaNetworks: Modeling long-range interactions without attention. In International\nConference on Learning Representations (ICLR), 2013.\nIrwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented\nconvolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 3286–3295, 2019.\nManoj Bhat, Jonathan Francis, and Jean Oh. Trajformer: Trajectory prediction with local self-attentive\ncontexts for autonomous driving. arXiv preprint arXiv:2011.14910, 2020.\nYuriy Biktairov, Maxim Stebelev, Irina Rudenko, Oleh Shliazhko, and Boris Yangel. Prank: motion\nprediction based on ranking. In Advances in neural information processing systems, 2020.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In Advances in Neural Information Processing Systems, 2020.\nMartin Buehler, Karl Iagnemma, and Sanjiv Singh. The DARPA urban challenge: autonomous\nvehicles in city trafﬁc, volume 56. springer, 2009.\nThibault Buhet, Emilie Wirbel, and Xavier Perrotton. Plop: Probabilistic polynomial objects trajectory\nplanning for autonomous driving. In Conference on Robot Learning, 2020.\nHolger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush\nKrishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for\nautonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 11621–11631, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer\nVision, pp. 213–229. Springer, 2020.\nSergio Casas, Cole Gulino, Renjie Liao, and Raquel Urtasun. Spagnn: Spatially-aware graph\nneural networks for relational behavior forecasting from sensor data. In 2020 IEEE International\nConference on Robotics and Automation, ICRA 2020, Paris, France, May 31 - August 31, 2020, pp.\n9491–9497. IEEE, 2020a.\nSergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie Liao, and Raquel Urtasun. Implicit\nlatent variable model for scene-consistent motion forecasting. In Proceedings of the European\nConference on Computer Vision (ECCV). Springer, 2020b.\nYuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple\nprobabilistic anchor trajectory hypotheses for behavior prediction. In Conference on Robot\nLearning, 2019.\n10\nPublished as a conference paper at ICLR 2022\nRohan Chandra, Uttaran Bhattacharya, Aniket Bera, and Dinesh Manocha. Traphic: Trajectory\nprediction in dense and heterogeneous trafﬁc using weighted interactions. CoRR, abs/1812.04767,\n2018. URL http://arxiv.org/abs/1812.04767.\nMing-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett,\nDe Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking and forecasting\nwith rich maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 8748–8757, 2019.\nHenggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo\nHuang, Jeff Schneider, and Nemanja Djuric. Multimodal trajectory predictions for autonomous\ndriving using deep convolutional networks. In 2019 International Conference on Robotics and\nAutomation (ICRA), pp. 2090–2096. IEEE, 2019.\nNachiket Deo and Mohan M. Trivedi. Trajectory forecasts in unknown environments conditioned\non grid-based plans. CoRR, abs/2001.00735, 2020. URL http://arxiv.org/abs/2001.\n00735.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Conference of the North American\nChapter of the Association for Computational Linguistics, 2019.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. In International Conference\non Learning Representations (ICLR), 2021.\nScott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning\nChai, Ben Sapp, Charles Qi, Yin Zhou, et al. Large scale interactive motion forecasting for\nautonomous driving: The waymo open motion dataset. arXiv preprint arXiv:2104.10133, 2021.\nJiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid.\nVectorNet: Encoding hd maps and agent dynamics from vectorized representation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\nThomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde.\nHOME: heatmap output for future motion estimation. CoRR, abs/2105.10968, 2021. URL\nhttps://arxiv.org/abs/2105.10968.\nFrancesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso. Transformer networks for\ntrajectory forecasting. In International Conference on Pattern Recognition, 2020.\nXavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and\nstatistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010.\nJunru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-end trajectory prediction from dense goal\nsets. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\nAgrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social GAN: Socially\nacceptable trajectories with generative adversarial networks. In CVPR, 2018.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv preprint arXiv:1912.12180, 2019.\nJoey Hong, Benjamin Sapp, and James Philbin. Rules of the road: Predicting driving behavior with a\nconvolutional model of semantic interactions. In CVPR, 2019.\nZhiyu Huang, Xiaoyu Mo, and Chen Lv. Recoat: A deep learning framework with attention\nmechanism for multi-modal motion prediction. In Workshop on Autonomous Driving, CVPR, 2021.\nWei-Chih Hung, Henrik Kretzschmar, Tsung-Yi Lin, Yuning Chai, Ruichi Yu, Ming-Hsuan Yang,\nand Drago Anguelov. Soda: Multi-object tracking with soft data association. arXiv preprint\narXiv:2008.07725, 2020.\n11\nPublished as a conference paper at ICLR 2022\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning, pp. 448–456.\nPMLR, 2015.\nBoris Ivanovic and Marco Pavone. Rethinking trajectory forecasting evaluation. arXiv preprint\narXiv:2107.10297, 2021.\nNorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa,\nSarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of\na tensor processing unit. In Proceedings of the 44th annual international symposium on computer\narchitecture, pp. 1–12, 2017.\nR. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira, M. Yuan, B. Low, A. Jain,\nP. Ondruska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova, C. Tao, L. Platinsky, W. Jiang, and\nV . Shet. Lyft level 5 perception dataset 2020. https://level5.lyft.com/dataset/,\n2019.\nSiddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew Hartnett, and Deva Ramanan. What-if\nmotion prediction for autonomous driving. arXiv preprint arXiv:2008.10587, 2020.\nStepan Konev, Kirill Brodt, and Artsiom Sanakoyeu. Motioncnn: A strong baseline for motion\nprediction in autonomous driving. In Workshop on Autonomous Driving, CVPR, 2021.\nNamhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan\nChandraker. Desire: Distant future prediction in dynamic scenes with interacting agents. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 336–345,\n2017.\nLingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng, Mengye Ren, Sean Segal, and Raquel Urta-\nsun. End-to-end contextual perception and prediction with interaction transformer. In IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), 2020.\nMing Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun. Learning\nlane graph representations for motion forecasting. In European Conference on Computer Vision\n(ECCV), 2020.\nJerry Liu, Wenyuan Zeng, Raquel Urtasun, and Ersin Yumer. Deep structured reactive planning.\narXiv preprint arXiv:2101.06832, 2021a.\nYicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, and Bolei Zhou. Multimodal motion\nprediction with stacked transformers, 2021b.\nFrancesco Marchetti, Federico Becattini, Lorenzo Seidenari, and Alberto Del Bimbo. Mantra:\nMemory augmented networks for multiple trajectory prediction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 7143–7152, 2020.\nJean Mercat, Thomas Gilles, Nicole Zoghby, Guillaume Sandou, Dominique Beauvois, and Guillermo\nGil. Multi-head attention for joint multi-modal vehicle motion forecasting. In IEEE International\nConference on Robotics and Automation, 2020.\nGregory P. Meyer and Niranjan Thakurdesai. Learning an uncertainty-aware object detector for\nautonomous driving. In IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pp. 10521–10527, 2020.\nXiaoyu Mo, Zhiyu Huang, and Chen Lv. Multi-modal interactive agent trajectory prediction using\nheterogeneous edge-enhanced graph attention network. In Workshop on Autonomous Driving,\nCVPR, 2021.\nMichael Montemerlo, Jan Becker, Suhrid Bhat, Hendrik Dahlkamp, Dmitri Dolgov, Scott Ettinger,\nDirk Haehnel, Tim Hilden, Gabe Hoffmann, Burkhard Huhnke, et al. Junior: The stanford entry in\nthe urban challenge. Journal of ﬁeld Robotics, 25(9):569–597, 2008.\n12\nPublished as a conference paper at ICLR 2022\nSeong Hyeon Park, Gyubok Lee, Jimin Seo, Manoj Bhat, Minseok Kang, Jonathan Francis, Ashwin\nJadhav, Paul Pu Liang, and Louis-Philippe Morency. Diverse and admissible trajectory forecasting\nthrough multimodal context understanding. In European Conference on Computer Vision, pp.\n282–298. Springer, 2020.\nStefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc Van Gool. You’ll never walk alone:\nModeling social behavior for multi-target tracking. In 2009 IEEE 12th International Conference\non Computer Vision, pp. 261–268. IEEE, 2009.\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets\nfor 3d classiﬁcation and segmentation. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 652–660, 2017.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. InAdvances in Neural Information Processing Systems,\nvolume 32, 2019.\nNicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction con-\nditioned on goals in visual multi-agent settings. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 2821–2830, 2019.\nTim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-\nfeasible trajectory forecasting with heterogeneous data. arXiv preprint arXiv:2001.03093, 2020.\nBernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,\nAnirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the\nIEEE, 2021.\nAravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. In CVPR, 2021.\nSimon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. Trafﬁcsim: Learning to simulate\nrealistic multi-agent behaviors. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2021.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking\nthe inception architecture for computer vision. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 2818–2826, 2016.\nCharlie Tang and Russ R Salakhutdinov. Multiple futures prediction. In NeurIPS, 2019.\nSebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel,\nPhilip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, et al. Stanley: The robot that won\nthe darpa grand challenge. Journal of ﬁeld Robotics, 23(9):661–692, 2006.\nEkaterina Tolstaya, Reza Mahjourian, Carlton Downey, Balakrishnan Vadarajan, Benjamin Sapp,\nand Dragomir Anguelov. Identifying driver interactions via conditional behavior prediction. 2021\nIEEE International Conference on Robotics and Automation (ICRA), 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\nAshish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efﬁcient visual backbones. In CVPR, 2021.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In European Conference on\nComputer Vision, pp. 108–126. Springer, 2020.\nWaymo. Waymo open dataset challenge 2021 winners. https://waymo.com/open/\nchallenges#winners, 2021. Accessed: 2021-10-01.\nMaosheng Ye, Tongyi Cao, and Qifeng Chen. Tpcn: Temporal point cloud networks for motion\nforecasting. arXiv preprint arXiv:2103.03067, 2021.\n13\nPublished as a conference paper at ICLR 2022\nRaymond A Yeh, Alexander G Schwing, Jonathan Huang, and Kevin Murphy. Diverse generation for\nmulti-agent sports games. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 4610–4619, 2019.\nCunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi. Spatio-temporal graph transformer\nnetworks for pedestrian trajectory prediction. In European Conference on Computer Vision, pp.\n507–523. Springer, 2020.\nYe Yuan, Xinshuo Weng, Yanglan Ou, and Kris Kitani. Agentformer: Agent-aware transformers for\nsocio-temporal multi-agent forecasting. arXiv preprint arXiv:2103.14023, 2021.\nWenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun.\nEnd-to-end interpretable neural motion planner. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 8660–8669, 2019.\nWei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian Naumann, Julius Küm-\nmerle, Hendrik Königshof, Christoph Stiller, Arnaud de La Fortelle, and Masayoshi Tomizuka.\nINTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in\nInteractive Driving Scenarios with Semantic Maps. arXiv:1910.03088 [cs, eess], 2019.\nHang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Benjamin Sapp, Balakrishnan Varadarajan, Yue Shen,\nYi Shen, Yuning Chai, Cordelia Schmid, et al. Tnt: Target-driven trajectory prediction. arXiv\npreprint arXiv:2008.08294, 2020.\nTianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi, Chris Baker, Yibiao Zhao, Yizhou Wang,\nand Ying Nian Wu. Multi-agent tensor fusion for contextual trajectory prediction. In CVPR, pp.\n12126–12134, 2019.\nJulius Ziegler, Philipp Bender, Thao Dang, and Christoph Stiller. Trajectory planning for bertha—a\nlocal, continuous method. In 2014 IEEE intelligent vehicles symposium proceedings, pp. 450–457.\nIEEE, 2014.\n14\nPublished as a conference paper at ICLR 2022\nAppendix\nA A RCHITECTURE AND TRAINING DETAILS\nOverview. Table 5 provides additional details about the Scene Transformer architecture and training.\nThe bulk of the computation and parameters reside in the Transformer layers. Table 6 lists all of\nthe operations and learned parameters in our implementation of a Transformer. MLPs are employed\nto embed the original data (e.g, layers A, B, C), build F futures from the encoding (layer T), and\npredict the ﬁnal position, headings and uncertainties of all agents (layers Z1 and Z2). The resulting\nattention-based architecture using D = 256feature dimensions contains 15,296,136 parameters.\nNearly all settings used for both WOMD and Argoverse are identical, except that for Argoverse we\nuse D = 512and label smoothing to get our best marginal results. The resulting model is trained on\na TPU custom hardware accelerator (Jouppi et al., 2017) and converges in about 3 days of training.\nDecoding multiple futures. In order to allow the decoder to output F distinct futures, we perform\nthe following operations. The decoder receives as input a tensor of shape [A, T, D] corresponding to\nA agents across T time steps and D feature dimensions. The following series of simple operations\nrestructures the representation to predict F futures. First, the representation is tiled F times to\ngenerate [F, A, T, D]. We append a one-hot encoding to the ﬁnal dimension where a 1 indicates\nwhich the identity of each future, resulting in a tensor of shape [F, A, T, D+ F]. The one-hot\nencoding allows the network to learn an embedding for each of the F futures. For computational\nsimplicity, the resulting representation is propagated through a small MLP to produce a return a\ntensor of the shape [F, A, T, D].\nPadding and Hidden Masks. Padding and hidden masks are important to get right in the implemen-\ntation of such a model. In particular, we need to ensure that the masks do not convey additional future\ninformation to the model (e.g., if the model knows which timesteps an agent is visible or occluded\nbased on the padding in the data, it may take advantage of this and not be able to generalize). We\nuse the concept of padding to indicate the positions where the input is absent or provided. This is\ndistinct from the hidden mask of shape [A, T] that is used for task speciﬁcation. The hidden mask is\nused to query the model to inform it on which locations to predict, while the padding tells us which\nlocations have inputs and ground-truth to compute a loss over. All padded positions are set to be\nhidden during preprocessing, regardless of the masking scheme, so the model tries to predict their\nvalues. All layers in our implementation are padding aware, including normalization layers (like\nbatch normalization used in our encoding MLPs) and attention operations. Our attention operations\nset the attention weights to 0 for padded elements. If after providing our task speciﬁc hidden mask,\nno non-padded (valid) time steps exist for an agent, the whole agent is set to be padded. This prevents\nagent slots with no valid data from being used in the model.\nPredicting uncertainties. For each predicted variate such as position or heading, we additionally\npredict a corresponding uncertainty. In early experiments we found that predicting a paired uncertainty\nimproves the predictive performance of the original variate and provides a useful signal for interpreting\nthe ﬁdelity of the predicted value. We employ a loss function that predicts a parameterization of the\nuncertainty corresponding to a Laplace distribution (Meyer & Thakurdesai, 2020).\nLoss in lateral and longitudinal coordinates. We use a Laplace distribution parameterization for\nthe loss, with a diagonal covariance such that each axis is independent. To enable the model to learn\nmeaningful uncertainties, we rotate the scene per box prediction such that each prediction’s associated\nground-truth is axis aligned. This formulation results in uncertainty estimates that correspond to the\nagent’s lateral and longitudinal coordinates.\nHeading representation: Our output format is 7 dimensional, where one of the dimensions, heading,\nis represented in radians. To supervise the loss on the heading dimension, we employ a now standard\nmethod of \"wrapping\" the angle difference between the predicted and groundtruth heading for each\nagent; see Figure 4.\nMarginal predictions from a joint model. The model has the added feature of being easily adapted\nfor making marginal predictions for each agent. To produce per-trajectory scores for each agents\npredictions, we attach an artiﬁcial extra time step to the end of the [A, T, D] agent feature matrix\nto give us a matrix of shape [A, T+ 1, D]. This additional feature provides the model with extra\n15\nPublished as a conference paper at ICLR 2022\ndef WrapAngleRadians(angles_rad, min_val=-np.pi, max_val=np.pi):\nmax_min_diff = max_val - min_val\nreturn min_val + tf.math.floormod(angles_rad + max_val, max_min_diff)\nheading_diff = pred_heading - gt_heading\nheading_error = WrapAngleRadians(heading_diff, min_val=-PI, max_val=PI)\nheading_loss = HuberLoss(heading_error)\nFigure 4: Pseudo-code for heading loss component.\nWith Augmentation? Vehicle minADE @8s Ped minADE @8s Cyclist minADE @ 8s\nNo 1.30 0.69 1.29\nYes 1.18 0.59 1.15\nTable 4: WOMD validation set performance with and without data augmentation during training.\ncapacity for representing each agent, that is not tied to any timestep. This additional feature can be\nused to predict which of the k trajectories most closely matches the ground truth for that agent.\nData augmentation. We use two methods for data augmentation to aid generalization and combat\noverﬁtting on both datasets. First, we use agent dropout, where we artiﬁcially remove non-predicted\nagents with some probability. We found a probability of 0.1 worked well for both datasets. We also\nfound it beneﬁcial to randomly rotate the entire scene between [−π\n2 , π\n2 ] after centering to the agent of\ninterest. On Argoverse, this agent of interest is the agent the task designates to be predicted, where as\non the WOMD we always center around the autonomous vehicle (A V), even though for some tasks\nit is not one of the predicted agents. Lastly, each Argoverse scene contains many agents that the\nmodel is not required to predict; we employ these contextual agents as additional target training data\nif the contextual agents moved by at least 6m. On WOMD, Table 4 shows data augmentation on the\nstandard validation set does improve minADE modestly, though does not account for the majority of\nthe beneﬁt of the model.\nArgoverse classiﬁcation label smoothing: Our best model on Argoverse uses D = 512feature\ndimensions, but naively scaling the model this way leads to severe overﬁtting on the classiﬁcation\nsubtask. During training of our best model we employ label smoothing (0.1 + 1/6 for negatives and\n0.9 + 1/6 for positive target).\nWOMD redundant trajectory combination: The AP evaluation metric on the WOMD expects that\nexactly one of the trajectories is given a high probability and the other trajectories a low probability.\nFor example, the future prediction for a stationary agent is expected to have only one future with\na high probability score, with a low score for the rest – even though the trajectories may all be the\nsame, in this case, stationary. Our best model combines redundant trajectories together if they are\nclose in spatial location (less than 3.2m) at the ﬁnal timestep.\nEmbedding of agents and road graph. To generate input features, we use sinusoidal positional\nembeddings (Vaswani et al., 2017) to embed the time (for agents and dynamic roadgraph) and\nxyz-coordinates separately into a D dimensional features per dimension. We encode the type of\neach object using a one-hot encoding (e.g. object type, lane type, etc), and concatenate any other\nfeatures provided in the data set such as yaw, width, length, height, and velocity. Dynamic road\ngraphs have a second one-hot encoding indicating state, like the trafﬁc light state. Lastly, for agents\nand the dynamic road graph, we add a binary indicator on whether the agent is hidden at that time\nstep. If the agent or dynamic road graph element is hidden, all input features (e.g. position, type,\nvelocity, state, etc) are set to 0 before encoding except the time embedding, which are linearly spaced\nvalues at the dataset’s update rate starting at 0, and the hidden indicator.\nFor agents and the dynamic road graph, we use a 2 layer MLP with a hidden and output dimension of\nD to produce a ﬁnal feature per agent or object and per time step. For the static road graph, we must\nreduce a point cloud of up to 20,000 road graph points, each belonging to a smaller set of polylines,\nto a single vector per polyline. Because some lanes can be extremely long, we break up any polyline\nlonger than 20 points into a new set of smaller polylines. Then, we apply a small PointNet (Qi et al.,\n16\nPublished as a conference paper at ICLR 2022\n2017) architecture with a 2 layer MLP with a hidden and output dimension of D to each point, and\nuse max pooling per polyline to get a ﬁnal feature per element.\nInference latency. Scene Transformer produces a prediction for every agent in the scene in a single\npass. Its inference speed depends on the number of agents in the scene. Our preliminary proﬁling of\ninference speed ranged from 52 ms (32 agents) to 175 ms (128 agents) on the Waymo Open Motion\nDataset (128 agents, 91 timesteps for each agent, up to 1400 roadgraph elements). This was measured\non an Nvidia V100 using a standard TensorFlow inference graph in ﬂoat32 without optimization\ntricks, optimization tuning, etc. This is in line with the expected linear scaling of the factorized\nattention modules.\n17\nPublished as a conference paper at ICLR 2022\nMeta-Arch Name Input Operation Queries Keys/Values Across Atten Matrix Output Size # Param\nEncoder\nA Agents MLP + BN – – – – [A,T,D ] 334080\nB Dyna RG MLP + BN – – – – [GD,T,D ] 337408\nC Static RG MLP + BN – – – – [GS,T,D ] 270592\nD A ,B,C Transformer Agents Agents Time [A,T,T ] [ A,T,D ] 789824\nE D Transformer Agents Agents Agents [T,A,A ] [ A,T,D ] 789824\nF E Transformer Agents Agents Time [A,T,T ] [ A,T,D ] 789824\nG F Transformer Agents Agents Agents [T,A,A ] [ A,T,D ] 789824\nH G Transformer Agents Agents Time [A,T,T ] [ A,T,D ] 789824\nI H Transformer Agents Agents Agents [T,A,A ] [ A,T,D ] 789824\nJ I Transformer Agents Static RG Time [T,A,G S] [ A,T,D ] 789824\nK J Transformer Agents Dyna RG Time [T,A,G D] [ A,T,D ] 789824\nL K Transformer Agents Agents Time [A,T,T ] [ A,T,D ] 789824\nM L Transformer Agents Agents Agents [T,A,A ] [ A,T,D ] 789824\nN M Transformer Agents Static RG Time [T,A,G S] [ A,T,D ] 789824\nO N Transformer Agents Dyna RG Time [T,A,G D] [ A,T,D ] 789824\nP O Transformer Agents Agents Time [A,T,T ] [ A,T,D ] 789824\nQ P Transformer Agents Agents Agents [T,A,A ] [ A,T,D ] 789824\nDecoder\nR Q Tile – – – – [F,A,T,D ] 0\nS R Concat – – – – [F,A,T,D +F] 0\nT S MLP – – – – [F,A,T,D ] 68096\nU T Transformer Agents Agents Time [A,T,T ] [ F,A,T,D ] 789824\nV U Transformer Agents Agents Agents [T,A,A ] [ F,A,T,D ] 789824\nW V Transformer Agents Agents Time [A,T,T ] [ F,A,T,D ] 789824\nX W Transformer Agents Agents Agents [T,A,A ] [ F,A,T,D ] 789824\nY X Layer Norm – – – – [F,A,T,D ] 512\nZ1 Y MLP + BN – – – – [F,A,T, 6] 66817\nZ2 Y MLP – – – – [F,A,T, 7] 1799\nOptimizer Adam ( α= 1e−4, β1 = 0.9, β2 = 0.999)\nLearning Rate Schedule Total epochs: 150; Linear ramp-up: 0.1 epochs\nBatch size 64\nGradient Clipping (norm) 5.0\nWeight initialization Glorot & Bengio (2010)\nWeight decay None\nPosition Embeddings Min Timescale: 4; Max Timescale: 256\nTemporal Embeddings Min Timescale: 6; Max Timescale: 80\nFuture classiﬁcation weight 0.1\nPosition classiﬁcation weight 1.0\nLaplace Target Scale 1.0\nTable 5: Scene Transformer architecture and training details. The network receives as input of\nA agents across T time steps and K features. K is the total number of input features (e.g. 3-D\nposition, velocity, object type, bounding box size). A subset of these inputs are masked. GS and GD\nis the maximum number of road graph (RG) elements andD is the total number of features. MLP and\nBN denote multilayer perception and batch normalization (Ioffe & Szegedy, 2015), respectively. The\noutput of the network is Z1 and Z2. Z1 corresponds to predicting the logits for classifying which one\nof the F futures is most likely. Z2 corresponds to the predicted xyz-coordinates with their associated\nuncertainties, and a single value for heading. In our model D = 256, K = 7and F = 6for a total of\n15,296,136 parameters for both datasets. For the Waymo Open Motion DatasetGs = 1400, Gd = 16,\nT = 91, A = 128, and for Argoverse Gs = 256, Gd = 0, T = 50, and A = 64. All layers employ\nReLU nonlinearities.\n18\nPublished as a conference paper at ICLR 2022\nName Input Operation Parameter Sizes Output Size # Param\nX X o Layer Norm [D], [D] [ A, T, D] 512\nK X Afﬁne Projection [D, H,D\nH], [H, D\nH] [ A, T, H,D\nH] 65792\nV X Afﬁne Projection [D, H,D\nH], [H, D\nH] [ A, T, H,D\nH] 65792\nQo X Afﬁne Projection [D, H,D\nH], [H, D\nH] [ A, T, H,D\nH] 65792\nQ Q o Rescale [D\nH] [ A, T, H,D\nH] 64\nY1 Q, K, V softmax(Q KT)V – [A, T, H,D\nH] 0\nY2 Y1 Afﬁne Projection [H, D\nH, D], [D] [ A, T, D] 65792\nS F 1, Xo Sum – [A, T, D] 0\nF1 Y2 MLP [D, kD], [kD] [ A, T, kD] 263168\nF2 S MLP [kD, D], [D] [ A, T, D] 262400\nZ F 2 Layer Norm [D], [D] [ A, T, D] 512\nTotal 789824\nTable 6: Transformer architecture. The network receives as input Xo and outputs Z. All MLP’s\nemploy a ReLU nonlinearity. D is the number of feature dimensions; H is the number of attention\nheads. In our model D=256, H=4 and k=4.\n19\nPublished as a conference paper at ICLR 2022\nB A DDITIONAL MOTION PREDICTION RESULTS\nMotion Prediction minADE↓ minFDE↓ MR↓ mAP↑\nveh ped cyc veh ped cyc veh ped cyc veh ped cyc\nvalid\nbaseline (Ettinger et al., 2021)0.39 0.19 0.41 0.65 0.36 0.73 0.14 0.07 0.25 0.33 0.33 0.27\nours (marginal) 0.33 0.20 0.39 0.57 0.33 0.67 0.11 0.07 0.21 0.38 0.33 0.28\nours (joint-as-marginal) 0.42 0.28 0.50 0.78 0.51 0.94 0.19 0.21 0.32 0.34 0.25 0.23\nours (multi-task joint-as-marginal)0.43 0.51 0.51 0.80 0.52 0.93 0.20 0.21 0.32 0.33 0.24 0.23\ntest\nbaseline (Ettinger et al., 2021)0.39 0.20 0.41 0.65 0.36 0.74 0.14 0.07 0.25 0.34 0.32 0.24\nours (marginal) 0.32 0.20 0.38 0.56 0.33 0.67 0.11 0.07 0.21 0.38 0.32 0.28\nours (joint-as-marginal) 0.42 0.28 0.49 0.78 0.53 0.92 0.19 0.21 0.32 0.33 0.26 0.24\nours (multi-task joint-as-marginal)0.44 0.29 0.50 0.80 0.53 0.93 0.20 0.22 0.32 0.32 0.24 0.24\nTable 7: Marginal predictive performance on Waymo Open Motion Dataset motion prediction\nfor t = 3seconds. Please see Table 2 for details.\nInteraction Prediction minSADE↓ minSFDE↓ SMR↓ mAP↑\nveh ped cyc veh ped cyc veh ped cyc veh ped cyc\nvalid\nbaseline (Ettinger et al., 2021)0.58 0.43 0.60 1.13 0.86 1.20 0.45 0.47 0.61 0.15 0.13 0.06\nours (marginal-as-joint)0.45 0.37 0.52 0.91 0.75 1.04 0.35 0.42 0.52 0.20 0.12 0.09\nours (joint) 0.41 0.34 0.47 0.81 0.65 0.92 0.29 0.38 0.49 0.26 0.14 0.11\nours (multi-task joint)0.40 0.34 0.47 0.80 0.65 0.91 0.28 0.38 0.49 0.27 0.17 0.13\ntest\nbaseline (Ettinger et al., 2021)0.58 0.42 0.61 1.14 0.85 1.21 0.45 0.47 0.61 0.16 0.11 0.05\nours (marginal-as-joint)0.45 0.36 0.53 0.93 0.74 1.06 0.36 0.40 0.55 0.18 0.11 0.07\nours (joint) 0.41 0.33 0.48 0.82 0.64 0.94 0.29 0.36 0.50 0.18 0.12 0.07\nours (multi-task joint)0.41 0.34 0.48 0.81 0.65 0.94 0.29 0.37 0.51 0.26 0.15 0.10\nTable 8: Joint predictive performance on Waymo Open Motion Dataset motion prediction for\nt = 3seconds. Please see Table 3 for details.\nMotion Prediction minADE↓ minFDE↓ MR↓ mAP↑\nveh ped cyc veh ped cyc veh ped cyc veh ped cyc\nvalid\nbaseline (Ettinger et al., 2021)0.74 0.37 0.75 1.36 0.73 1.43 0.17 0.10 0.25 0.29 0.27 0.26\nours (marginal) 0.65 0.35 0.69 1.23 0.67 1.30 0.15 0.10 0.22 0.33 0.26 0.25\nours (joint-as-marginal) 0.83 0.51 0.93 1.68 1.08 1.91 0.23 0.26 0.33 0.26 0.19 0.18\nours (multi-task joint-as-marginal)0.84 0.52 0.92 1.72 1.08 1.87 0.24 0.26 0.33 0.26 0.18 0.18\ntest\nbaseline (Ettinger et al., 2021)0.74 0.37 0.76 1.35 0.73 1.43 0.17 0.10 0.25 0.29 0.26 0.23\nours (marginal) 0.63 0.35 0.68 1.20 0.67 1.31 0.11 0.10 0.22 0.33 0.26 0.23\nours (joint-as-marginal) 0.83 0.52 0.91 1.68 1.10 1.88 0.23 0.26 0.33 0.26 0.19 0.22\nours (multi-task joint-as-marginal)0.85 0.53 0.93 1.72 1.10 1.91 0.24 0.27 0.34 0.26 0.19 0.21\nTable 9: Marginal predictive performance on Waymo Open Motion Dataset motion prediction\nfor t = 5seconds. Please see Table 2 for details.\n20\nPublished as a conference paper at ICLR 2022\nInteraction Prediction minSADE↓ minSFDE↓ SMR↓ mAP↑\nveh ped cyc veh ped cyc veh ped cyc veh ped cyc\nvalid\nbaseline (Ettinger et al., 2021)1.19 0.90 1.25 2.64 1.98 2.82 0.55 0.57 0.70 0.13 0.09 0.04\nours (marginal-as-joint)0.96 0.78 1.08 2.15 1.75 2.46 0.44 0.51 0.60 0.13 0.09 0.06\nours (joint) 0.84 0.68 0.95 1.81 1.45 2.09 0.38 0.48 0.59 0.18 0.08 0.07\nours (multi-task joint)0.83 0.68 0.95 1.79 1.45 2.08 0.38 0.48 0.59 0.19 0.10 0.06\ntest\nbaseline (Ettinger et al., 2021)1.21 0.89 1.26 2.70 1.96 2.80 0.56 0.59 0.69 0.13 0.07 0.03\nours (marginal-as-joint)0.96 0.77 1.07 2.19 1.74 2.45 0.46 0.51 0.62 0.13 0.09 0.06\nours (joint) 0.85 0.67 0.96 1.85 1.44 2.10 0.39 0.50 0.58 0.17 0.10 0.07\nours (multi-task joint)0.84 0.68 0.97 1.82 1.46 2.09 0.39 0.49 0.59 0.20 0.09 0.08\nTable 10: Joint predictive performance on Waymo Open Motion Dataset motion prediction for\nt = 5seconds. Please see Table 3 for details.\nMotion Prediction minADE↓ minFDE↓ MR↓ mAP↑\nveh ped cyc veh ped cyc veh ped cyc veh ped cyc\nvalid\nLSTM baseline (Ettinger et al., 2021)1.34 0.63 1.26 2.85 1.35 2.68 0.25 0.13 0.29 0.23 0.23 0.20\nours (marginal) 1.17 0.59 1.15 2.51 1.26 2.44 0.20 0.12 0.24 0.26 0.27 0.20\nours (joint-as-marginal) 1.53 0.90 1.63 3.48 2.09 3.77 0.28 0.30 0.37 0.20 0.16 0.13\nours (multi-task joint-as-marginal)1.56 0.89 1.60 3.56 2.06 3.68 0.29 0.29 0.37 0.19 0.15 0.15\ntest\nLSTM baseline (Ettinger et al., 2021)1.34 0.64 1.29 2.83 1.35 2.68 0.24 0.13 0.29 0.24 0.22 0.19\nours (marginal) 1.17 0.60 1.17 2.48 1.25 2.43 0.19 0.12 0.22 0.27 0.23 0.20\nours (joint-as-marginal) 1.52 0.91 1.61 3.43 2.09 3.68 0.28 0.30 0.37 0.19 0.16 0.19\nours (multi-task joint-as-marginal)1.55 0.91 1.64 3.50 2.08 3.75 0.28 0.29 0.38 0.19 0.16 0.19\nTable 11: Marginal predictive performance on Waymo Open Motion Dataset motion prediction\nfor t = 8seconds. Please see Table 2 for details. We additionally include \"joint-as-marginal\" results\nfor our standard MP masked joint model, and our multi-task joint model. These are joint models\nevaluated as if they were marginal models (with no changes in the outputs).\nInteraction Prediction minSADE↓ minSFDE↓ SMR↓ mAP↑\nveh ped cyc veh ped cyc veh ped cyc veh ped cyc\nvalid\nLSTM baseline (Ettinger et al., 2021)2.42 2.73 3.16 6.07 4.20 6.46 0.66 1.00 0.83 0.07 0.06 0.02\nours (marginal-as-joint) 2.04 1.62 2.28 4.94 3.81 5.67 0.54 0.63 0.72 0.11 0.05 0.03\nours (joint, mp-only) 1.72 1.38 1.96 3.98 3.11 4.75 0.49 0.60 0.73 0.11 0.05 0.03\nours (joint, multi-task) 1.72 1.39 1.94 3.99 3.15 4.69 0.49 0.62 0.71 0.11 0.06 0.04\ntest\nLSTM baseline (Ettinger et al., 2021)2.46 2.47 2.96 6.22 4.30 6.26 0.67 0.89 0.89 0.06 0.03 0.03\nours (marginal-as-joint) 2.08 1.62 2.24 5.04 3.87 5.41 0.55 0.64 0.73 0.08 0.05 0.03\nours (joint, mp-only) 1.76 1.38 1.95 4.08 3.19 4.65 0.50 0.62 0.70 0.10 0.05 0.04\nours (joint, multi-task) 1.74 1.41 1.95 4.06 3.26 4.68 0.50 0.64 0.71 0.13 0.04 0.03\nTable 12: Joint predictive performance on Waymo Open Motion Dataset motion prediction for\nt = 8seconds. These results are the same as Table 3 but is included here for completeness with the\nother Appendix tables.\n21\nPublished as a conference paper at ICLR 2022\nB.1 U NDERSTANDING THE TRADE -OFFS BETWEEN THE JOINT AND MARGINAL MODELS\nFigure 5: Quantitative comparison of the marginal and joint prediction models. Left: The\nmarginal minADE broken down as a function of number of predicted agents. When there are more\nagents, producing internally consistent predictions is more challenging, and hence the joint model\nperforms slightly worse. Right: Overlap rate between pairs of agent predictions. The joint model\nproduces internally consistent predictions with lower inter-prediction overlap rates.\nWhile the joint mp-only model performs better on the interactive prediction task than the marginal\nmodel when evaluated with joint metrics , it performs worse on the motion prediction task when\nevaluated with marginal metrics (see Table 11 \"ours (joint)\"). This is because the joint model is\ntrained to produce predictions that are internally consistent between agents, while the marginal model\ndoes not, which is a strictly harder task. In this section, we examine this quality difference and the\ninternal consistency of the predictions from both models.\nIn the WOMD dataset, each example has a different number of agents to be predicted. By slicing\nthe marginal minADE results based on the number of agents to be predicted (Figure 5, left), we ﬁnd\nthat the joint model performs worse as there are more predicted agents, while the marginal model\nperforms the same. This is expected: when there are more agents, the joint model has a more difﬁcult\ntask since it needs to produce internally consistent predictions. One may expect a joint model to need\nan exponential number of trajectory combinations to perform competitively with the marginal model,\nbut we are encouraged to ﬁnd that this is not actually the case. We believe this is due to the fact that\nwhen many agents are interacting, the number of realistic scenarios is actually heavily constrained by\ninteractions - most combinations of marginal predictions don’t actually make sense together.\nHowever, when more agents are to be predicted, the possibility of interactions are higher and we\nwould expect that the joint model is able to capture these interactions through internally consistent\npredictions. We measure the models’ internal consistency by selecting the best trajectory and\nmeasuring the inter-prediction overlap rate (details below). We ﬁnd that the joint model has a\nconsistently lower inter-prediction overlap rate, showing that it is able to capture the interactions\nbetween agents. The ability to model interactions enables the model to be suitable for conditional\nmotion prediction, and goal conditioned prediction tasks, which we discuss in Section 4.4.\nMeasuring agent inter-prediction overlap. We measure predicted agent overlap on the best set of\ntrajectory produced by the model. For the joint model, this corresponds to the joint prediction that\nhas the higher probability score. For the marginal model, we take the top scoring trajectory for each\nagent to obtain the best set. For every predicted agent in the trajectory, we determine if it has an\noverlap with any other predicted agent by comparing the rotated upright 3D bounding boxes using\nour predicted xyz and heading. The inter-prediction overlap rate is the number of predicted agents\nthat are involved in some overlap, divided by the total number of predicted agents. We count two\nagents as overlapping if the intersection over the union (IoU) exceeds 0.01.\n22\nPublished as a conference paper at ICLR 2022\nC S LICING RESULTS\nFigure 6: We show marginal minADE on a per scene basis broken down by different scene level\nstatistics. Interestingly, we show that both the marginal and joint models become out of distribution\nabove 20 m/s, where there is minimal training data.\n23\nPublished as a conference paper at ICLR 2022\nFigure 7: Analysis of the multi-task model for goal-conditioned motion prediction. Left: Cumu-\nlative probability of the A V minADE in goal conditioned prediction (blue) and motion prediction\n(black) masking strategies. Right: A V minADE for goal conditioned prediction (blue) and motion\nprediction (black) as a function of A V speed (averaged over the ground truth trajectory).\n24\nPublished as a conference paper at ICLR 2022\nD L OSS IMPLEMENTATION\nThe Scene Transformer model is both agent permutation equivariant and scene-centric. These\nproperties allow us to easily switch between a marginal vs joint loss formulation (Figure 8). In\nthe marginal formulation, we reduce the loss of the best future for every agent separately; while in\nthe joint formulation, we reduce the loss of the best future for all agents jointly. In practice, this\ndetermines when the reduce_min operation is performed.\nOur ﬁnal loss composes the regression loss on the trajectory and the classiﬁcation loss of the best\ntrajectory. A weighted linear combination of the loss terms is used to combine these two losses\ntogether. The classiﬁcation loss weight was set to be 0.1, while the regression losses have weight 1.0.\nThe weights were determined using the hold-out validation set.\n# agent_predictions and agent_gt are\n# [F, A, T, 7] Tensors with [x, y, z], uncertainty terms, and yaw.\n# Get the KL Divergence of the predictions vs ground truth.\n# Use LaplaceKL method from (Meyer & Thakurdesai, 2020)\nloss = LaplaceKL(agent_predictions, agent_gt)\n# Now reduce across all timesteps and values to produce\n# a tensor of shape [F, A]\nloss = tf.reduce_sum(loss, axis=[2, 3])\n# The marginal loss, we only apply the loss to the best trajectory\n# per agent (so min across the future dimension).\nmarginal_loss = tf.reduce_min(loss, axis=0)\n# Then sum over the agent dimension\nmarginal_loss = tf.reduce_sum(marginal_loss)\n# The joint loss, we sum over all agents to get a loss value\n# per future.\njoint_loss = tf.reduce_sum(loss, axis=1)\n# Then only apply the loss to the best future prediction\njoint_loss = tf.reduce_min(joint_loss)\nFigure 8: Pseudo-code in TensorFlow (Abadi et al., 2015) demonstrating the joint versus marginal\nloss formulation.\n25",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7902418375015259
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5631431341171265
    },
    {
      "name": "Autonomous agent",
      "score": 0.5137099027633667
    },
    {
      "name": "Motion planning",
      "score": 0.4664819836616516
    },
    {
      "name": "Machine learning",
      "score": 0.46619051694869995
    },
    {
      "name": "Motion (physics)",
      "score": 0.44213730096817017
    },
    {
      "name": "Architecture",
      "score": 0.43272674083709717
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.43062618374824524
    },
    {
      "name": "Time horizon",
      "score": 0.4173867404460907
    },
    {
      "name": "Futures contract",
      "score": 0.41691142320632935
    },
    {
      "name": "Transformer",
      "score": 0.4102104902267456
    },
    {
      "name": "Robot",
      "score": 0.20489242672920227
    },
    {
      "name": "Engineering",
      "score": 0.11609292030334473
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Financial economics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}