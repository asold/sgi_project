{
  "title": "MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models",
  "url": "https://openalex.org/W4389520172",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2224899096",
      "name": "Yifan Wei",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2171923244",
      "name": "Yisong Su",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2112883951",
      "name": "Huanhuan Ma",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2099601725",
      "name": "Xiaoyan Yu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2575536559",
      "name": "Fangyu Lei",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2107248586",
      "name": "Yuanzhe Zhang",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2028683064",
      "name": "Jun Zhao",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2072929885",
      "name": "Kang Liu",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence",
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4302305823",
    "https://openalex.org/W4389523675",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4285199586",
    "https://openalex.org/W4387724050",
    "https://openalex.org/W4385571244",
    "https://openalex.org/W4385565354",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4296415390",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4385571357",
    "https://openalex.org/W3211777899",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4281485769",
    "https://openalex.org/W3175987672",
    "https://openalex.org/W4385571836",
    "https://openalex.org/W4375869991",
    "https://openalex.org/W4288725442",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3195376057",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning. However, research on the temporal sensitivity of LLMs has been insufficiently emphasized. To fill this gap, this paper constructs Multiple Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors (scope factor, order factor, counterfactual factor) with total 2,853 samples for evaluating the time comprehension and reasoning abilities of LLMs. This paper tests current mainstream LLMs with different parameter sizes, ranging from billions to hundreds of billions. The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions. Furthermore, this paper undertakes a preliminary investigation into potential improvement strategies by devising specific prompts and leveraging external tools. These approaches serve as valuable baselines or references for future research endeavors.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1434–1447\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMenatQA: A New Dataset for Testing the Temporal Comprehension and\nReasoning Abilities of Large Language Models\nYifan Wei1,2, Yisong Su1,3, Huanhuan Ma2, Xiaoyan Yu1,4, Fangyu Lei1,2,\nYuanzhe Zhang1,2, Jun Zhao1,2, Kang Liu1,2,5\n1The Laboratory of Cognition and Decision Intelligence for Complex Systems, CASIA\n2University of Chinese Academy of Sciences, 3Fuzhou University\n4Beijing Institute of Technology,5Shanghai Artificial Intelligence Laboratory\n{weiyifan2021,mahuanhuan2021,leifangyu2022}@ia.ac.cn, 221020042@fzu.edu.cn\n{xiaoyan.yu,yzzhang,jzhao,kliu}@nlpr.ia.ac.cn\nAbstract\nLarge language models (LLMs) have shown\nnearly saturated performance on many natural\nlanguage processing (NLP) tasks. As a result,\nit is natural for people to believe that LLMs\nhave also mastered abilities such as time un-\nderstanding and reasoning. However, research\non the temporal sensitivity of LLMs has been\ninsufficiently emphasized. To fill this gap, this\npaper constructs Multiple S ensitive F actors\nTime QA ( MenatQA), which encompasses\nthree temporal factors (scope factor, order fac-\ntor, counterfactual factor) with total 2,853 sam-\nples for evaluating the time comprehension and\nreasoning abilities of LLMs. This paper tests\ncurrent mainstream LLMs with different param-\neter sizes, ranging from billions to hundreds of\nbillions. The results show most LLMs fall be-\nhind smaller temporal reasoning models with\ndifferent degree on these factors. In specific,\nLLMs show a significant vulnerability to tem-\nporal biases and depend heavily on the tempo-\nral information provided in questions. Further-\nmore, this paper undertakes a preliminary inves-\ntigation into potential improvement strategies\nby devising specific prompts and leveraging\nexternal tools. These approaches serve as valu-\nable baselines or references for future research\nendeavors.\n1 Introduction\nRecent Large Language Models (LLMs; Zeng et al.\n2022; Touvron et al. 2023; Zhang et al. 2022) such\nas GPT-4 (OpenAI, 2023) pretrained on a vast\namount of text corpus have achieved nearly sat-\nurated performance on most Natural Language Pro-\ncessing (NLP) tasks. Meanwhile, plenty of works\nhave evaluated the reasoning abilities of LLMs on\nseveral tasks, such as numerical reasoning (Chen\net al., 2022), logical reasoning (Saparov and He,\n2022), counterfactual reasoning (Li et al., 2023),\nand multi-hop reasoning (Lei et al., 2023; Ma et al.,\n2023). However, the temporal reasoning ability of\nLLMs, which refers to the capacity of a model to\nContext: [1] Twitter was created by Jack Dorsey, Noah Glass, \nBiz Stone, and Evan Williams in March 2006 and launched in \nJuly of that year. [2] On October 16, 2008, Evan Williams be-\ncame the CEO, and Dorsey became the chairman of the com-\npany. [3] Jack Dorsey rejoined Twitter in March 2011 as Exe-\ncutive Chief of Product Development. [4] In June 2020, Twitter \nannounced that Patrick Pichette would succeed Omid Korde-\nstani as chairman. [5] In November 2021, Jack Dorsey stepped \ndown as CEO and was replaced by Parag Agrawal, the chief \ntechnology officer. [6] On October 27, 2022, business magnate \nElon Musk acquired Twitter for US$44 billion, gaining control of \nthe platform. [7] On May 12, 2023, Musk announced that he will \nresign as CEO of Twitter in approximately six weeks.\n————————————————————————————————\nMultiple Sensitive Factors Time QA\nScope Factor: \nWho was the CEO of Twitter from May 2013 to 2020 ? \nOrder Factor: \nShuffle the order of the sentences [1-7] in the context .\nCounterfactual Factor: \nWho was the CEO of Twitter from March 2011 to July 2022 , if \nJack Dorsey stepped down as CEO in November 2022 ? \n————————————————————————————\nAnswer: Jack Dorsey \nFigure 1: Yellow front indicates the time specifiers of\nevents in the context. Scope Factor refers to the time\nspecifiers would be different between the question and\nthe given context. Order Factor is where the complete\nevents in the context are shuffled in chronological order.\nCounterfactual Factor is a question with hypothetical\npropositions.\ncapture the temporal scope and interval of events\nin a given context, is yet seldomly explored. This\nability is particularly important and necessary in\nmany downstream tasks, such as Question Answer-\ning (QA), due to the inconsistency of answers in\nreal events across time ranges. For example, as\nshown in Figure 1, given the context “From March\n2011 to November 2021, Jack Dorsey rejoined Twit-\nter as CEO, and In November 2021 Jack Dorsey\nstepped down as CEO and was replaced by Chief\nTechnology Officer Parag Agrawal”, the answer to\nthe question “Who was the CEO of Twitter from\nyear Ato year B?” could be either “Jack Dorsey”\nor “Parag Agrawal”, depending on the time pe-\n1434\nriod ([year A , year B]) in the question.\nTo verify the temporal reasoning ability of mod-\nels, a few datasets have been proposed. Situat-\nedQA (Zhang and Choi, 2021) focused on how\nanswers vary according to different extra-linguistic\ncontexts, such as, when questions are asked. Re-\nalTime QA (Kasai et al., 2022) was proposed to\nanswer questions where real-time news was served\nas the contexts. Recently, TimeQA (Chen et al.,\n2021) was proposed for time-sensitive question an-\nswering , particularly for the temporalscope factor.\nIn TimeQA, the time specifiers would be inconsis-\ntent between the question and the given context.\nAs shown in Figure 1, a time specifier in “Who\nwas the CEO of Twitter from May 2013 to 2020?”\nis 2020, but the time specifier of the correct an-\nswer in the context is November 2021. As a result,\nthe system needs more powerful abilities of time\nunderstanding and temporal reasoning.\nNevertheless, there are more temporal reason-\ning abilities (factors) that need to be verified but\nare usually neglected, besides the identified tem-\nporal scope factor in TimeQA. The first is order\nfactor. For example, “[On October 16, 2008],\nEvan Williams became the CEO, and Dorsey be-\ncame the chairman of the company. Jack Dorsey re-\njoined Twitter in [March 2011] as Executive Chief\nof Product Development” . In this example, the\nchronological sequence of events is laid out by the\ngiven time specifiers, illuminating the progression\nof roles within the company. Consequently, recog-\nnizing the chronological order of events is a funda-\nmental ability and typically assessed in evaluations\nconcerning time understanding and reasoning.\nThe second is counterfactual factor. Questions\nwith temporal assumptions greatly escalate the dif-\nficulty of temporal reasoning. Answering such\nquestions may require additional information or\ncounterfactual thinking of models. For example,\n“Who was the CEO of Twitter from March 2011 to\nJuly 2022, if Jack Dorsey stepped down as CEO\nin November 2022?” . LLMs should be able to\nunderstand that “Jack Dorsey was still the CEO\nof Twitter from March 2011 to November 2022”.\nObviously, answering such types of questions is\nanother form to test temporal reasoning ability.\nTo facilitate the development of research around\nthe aforementioned problems, this paper proposes\na new dataset, Multiple Sensitive Factors Time QA\n(MenatQA), which encompasses the above three\ntemporal sensitivity factors and is used to evaluate\nthe temporal reasoning ability of the LLMs. In de-\ntail, the MenatQA dataset contains 2,853 samples,\nwhich are partitioned into 1,448 samples for the\nscope type, 857 samples for the order type, and\n548 samples for the counterfactual type, respec-\ntively.\nBased on the proposed MenatQA, serveral main-\nstream models are evaluated, including the SOTA\ntemporal reasoning models (BigBird (Zaheer et al.,\n2020) and FiD (Izacard and Grave, 2020)), and cur-\nrent typical large language models such as LLAMA\n(Touvron et al., 2023), OPT (Zhang et al., 2022)\nand GPT-3.5 (gpt-3.5-turbo; Brown et al. 2020).\nThe experimental results demonstrate the majority\nof LLMs perform poorly on our MenatQA dataset.\nIt indicates a potential deficiency in LLMs’ com-\nprehension of temporal concepts. Moreover, to\nenhance the temporal reasoning ability of LLMs,\nespecially for aforementioned scope factor, order\nfactor, and counterfactual factor, this paper pro-\nposes some preliminary investigations, such as de-\nsigning specific prompts and tool learning. These\napproaches will serve as baselines in MenatQA and\ncan be used as a benchmark for future research.\nOur main contributions are summarized as fol-\nlows:\n• We present a new dataset named Multiple\nSensitive Factors Time QA (MenatQA). This\nis the first dataset containing multiple time-\nsensitive factors that can be used as an evalua-\ntion benchmark for assessing the time under-\nstanding and reasoning abilities of LLMs.\n• We evaluate the performance of current LLMs\non three temporal factors, revealing their high\nsusceptibility to temporal biases and their re-\nliance on specific temporal information given\nin questions for reasoning about time.\n• We provide preliminary investigations to op-\ntimize temporal reasoning ability of LLMs,\nwhich can be used as baseline to inspire the\nfuture research.\n2 The MenatQA Dataset\n2.1 Dataset Construction\nData collection. We construct MenatQA based on\nTimeQA (Chen et al., 2021) dataset. Only time\nquestions that are accompanied with a golden con-\ntext and a detailed time scope of event are col-\nlected. To extract the relevant time scope, correct\n1435\nType Questions Answerable Unanswerable Doc-Token #Question-Token\nScope 1448 1316 132 227 16\nOrder 857 669 188 264 24\nCounterfactual 548 462 86 227 30\nTotal 2853 2447 406 - -\nTable 1: The dataset provides statistics for different types of factors. #Doc-Token and #Question-Token represent\nthe average number of tokens within the document and question, respectively. This paper counts the number of\ntokens using GPT-2 tokenizer, which is the same tokenizer as ChatGPT.\nanswers, annotated paragraphs, and golden context\nfrom documents, we develop a script that utilizes\nJSON syntax for accurate identification.\nData annotation. We represent the time factor\nas three types: scope factor, order factor, counter-\nfactual factor. The detailed information about the\nannotation can be found in the Appendix A.2.\n• The definition of the scope factor refers to\nthe time scopes that are relevant to the ques-\ntion (e.g., “From 2011 to 2021”). Specially,\nthe scope type includes two types of ques-\ntions: extraction and reasoning. The extrac-\ntion questions originate from TimeQA1, and\nthe reasoning questions can be obtained by\nadding more fine-grained information such as\nmonths (e.g., “From March 2011 to November\n2021”), narrowing the time range (e.g.,“From\n2013 to 2020”), or expanding the time range\n(e.g., “From 2008 to 2021” ). In detail, the\nreasoning type questions are addressed using\nOpenAI’s text-davinci-003 API, employing\nfew-shot learning to alter the temporal inter-\nvals mentioned in the questions. Subsequently,\nwe provide both the original and altered ques-\ntions to the three annotators, requesting them\nto provide answers to the altered questions\nbased on the contextual information.\n• The order factor pertains to the chronologi-\ncal sequence of events in the context. Typ-\nically, the descriptive information on each\nWikipedia page is written in chronological\norder, as shown in the context in Figure 1.\nWe asked three annotators to read the context,\nidentify different events based on time, and\nthen shuffle the order of these events in the\ncontext.\n• The counterfactual factor refers to hypotheti-\ncal propositions about time, where the assump-\n1We adopt the Easy-Mode version of TimeQA, which only\ninvolves extraction type questions.\ntion goes beyond the context and requires\nimagination to connect the context and the\nhypothetical question (Li et al., 2023; Tang\net al., 2023). Counterfactual questions consist\nof a question (“Who was the CEO of Twitter\nfrom March 2011 to July 2022?”), alongside\na premise that contradicts the given context\n(“If Jack Dorsey stepped down as CEO in\nNovember 2022” ). Based on this premise,\nan imaginary consequence of the counterfac-\ntual question yields “Jack Dorsey”, as shown\nin Figure 1. Inspired by previous work on\nconstructing counterfactual samples (Li et al.,\n2022), we ask the annotators to imagine a tem-\nporal hypothesis that contradicts the context\n(e.g., changes in years). Then constructing a\n“if” question based on the hypothesis, while\nproviding the correct answer. To ensure the\ndiversity of phrasing, annotators are free to\ngenerate various phrasing of the assumption,\nand there is no restriction on the position of\nthe assumption.\n2.2 Dataset Statistics\nKey statistics. The MenatQA dataset contains\n2853 time-sensitive factor samples, which are parti-\ntioned into the scope type, order type and counter-\nfactual type corresponding to 1448, 857 and 548\nsamples. The main statistical data for factors are\nshown in Table 1. To address the issue of potential\nillusory outputs in LLMs, introducing unanswer-\nable questions serves as an effective means to as-\nsess their understanding of temporal knowledge. In\nMenatQA, we find that there are only 85.7% of the\nquestions are answerable questions, while 14.2%\nare unanswerable questions.\nSpecially, the scope type includes two types of\nquestions: reasoning and extraction, with 450 and\n998 samples, respectively. The extraction type\nrefers to questions where the corresponding time\nspecifier can be directly found in the context, while\nthe reasoning type refers to questions where there\n1436\nFigure 2: Statistics on the types of time-sensitive factors\nin the MenatQA dataset.\nis a discrepancy between the time in the context\nand the question. The proportion of time factor\ntypes is shown in Figure 2. These statistics indicate\nthat MenatQA exhibits rich diversity in question\ndistribution. The average length of questions in\nMenatQA is 20.71 words, while the context con-\nsists on average of 238.16 words, demonstrating\ntheir rich vocabulary. For more detailed statistical\ndata, please refer to Appendix A.1.\n3 The Performance of LLMs on\nMenatQA\n3.1 Task Definition\nWe focus on time-sensitive question answering\ntasks. The input of these tasks is formulated as\n(c, q) for free-form generation tasks, where c is the\ncontext and q is the question. The desired output is\neither a span from the context or \"unanswerable\"\ntext.\n3.2 Baselines\nIn this section, we introduce the temporal reason-\ning models and currently popular large language\nmodels. These serve as the main evaluation back-\nbone for MenatQA, enabling us to assess the per-\nformance of mainstream large language models on\nthree types of temporal factors.\nThe baselines in our experiments include: Big-\nBird (Zaheer et al., 2020) and FiD 2 (Izacard and\nGrave, 2020), ChatGLM (6B) (Zeng et al., 2022),\nBLOOM (7.1B) (Scao et al., 2022), GPT-J (6B)\n(Wang and Komatsuzaki, 2021), GPT-NEOX (20B)\n(Black et al., 2022), OPT (6.7B and 13B) (Zhang\n2Especially, We use the versions of BigBird and FiD\nthat have been fine-tuned on the Natural Questions (NQ;\nKwiatkowski et al. 2019) dataset.\net al., 2022), LLAMA (7B and 13B) (Touvron\net al., 2023), ChatGPT (gpt-3.5-turbo; Brown et al.\n2020). The detailed information about models can\nbe found in the Appendix A.3.6.\n3.3 Results and Analysis\nIn this section, we identify weaknesses in LLMs\nwith respect to three temporal factors by analyzing\nthe differences among various models.\nTo validate the impact of various time-sensitive\nfactors that were overlooked in previous works on\nthe temporal reasoning ability of large language\nmodels, we test the performance of aforementioned\nLLMs under three time factors on MenatQA, as\nshown in Table 2. In order to further comprehen-\nsively analyze the susceptibility of large language\nmodels to temporal biases, we compare the perfor-\nmance of LLMs on extraction and reasoning ques-\ntions in the scope factor of MenatQA, as shown in\nTable 3. Based on the results, we can find that:\nFirstly, analyzing the results in Table 2, it can be\nobserved that LLMs display varying sensitivities\ntowards different time factors. Notably, the coun-\nterfactual and scope factors exert the most signifi-\ncant impact on LLMs, as shown by the highlighted\nsections with yellow background in the table. Ad-\nditionally, not all LLMs outperform FiD on every\ntype of factor. For instance, when evaluating the\nperformance of GPT-3.5-turbo on the counterfac-\ntual factor, it fails to surpass FiD, with F1 and\nEM scores of 34.69 and 27.66, respectively. These\nscores are significantly lower than the correspond-\ning results achieved by FiD (F1: 45.79, EM: 34.03).\nBesides, none of the other LLMs demonstrate supe-\nriority over FiD across all temporal factors, except\nfor LLama-13B. In conclusion, LLMs still have\nlimitations in effectively processing implicit tem-\nporal information, as indicated by their inadequate\nperformance and sensitivity to different temporal\nfactors. Therefore, more research is needed to en-\nhance the temporal understanding and reasoning\ncapabilities of LLMs.\nSecondly, in extraction type questions, the major-\nity of LLMs (i.e., ChatGLM-6B, Bloom-7B1, OPT-\nSeries and GPT-Series) cannot achieve satisfactory\noutcomes when compared with temporal reasoning\nmodels (i.e., BigBird and Fid), as shown in Table\n3. The weakness of LLMs in temporal reasoning is\nmore prominent in reasoning type questions, where\nall LLMs exhibit varying degrees of performance\ndecline compared to extraction type questions. This\n1437\nMethod\nMenatQA\nw/scope\nMenatQA\nw/order\nMenatQA\nw/counterfactual\nMenatQA\nw/all factors\nF1 EM F1 EM F1 EM F1 EM\nTemporal Reasoning Models\nBigBird(NQ) 35.31 23.72 41.84 29.62 40.84 28.22 35.81 24.22\nFiD(NQ) 41.03 30.63 45.85 33.73 45.79 34.03 36.12 26.32\nLarge Language Models\nChatGLM-6B 21.49 6.11 23.99 6.01 19.20 3.61 13.77 3.81\nBloom-7B1 25.69 13.33 32.34 16.94 29.34 12.62 24.32 9.62\nGPT-J-6B 34.56 24.95 39.40 27.86 45.64 33.07 37.84 25.85\nGPT-Neox-20B 36.23 25.55 41.61 27.86 45.73 30.56 37.81 24.55\nOPT-6.7B 29.14 19.84 35.79 24.35 32.77 20.54 27.27 16.13\nOPT-13B 37.30 26.85 45.17 32.06 42.32 25.95 35.93 21.94\nLLama-7B 45.71 34.47 51.93 37.17 49.78 33.97 37.53 23.45\nLLama-13B 52.13 41.58 64.45 52.40 53.56 41.48 39.80 28.66\nGPT-3.5-turbo 47.34 37.78 51.20 38.38 34.69 27.66 27.99 24.45\nTable 2: The performance of models on each time-sensitive factor in the MenatQA dataset. Bold scores indicate\nsuperior performance compared to FiD. The factor with the most significant impact (lowest performance) on\nindividual model is highlighted with yellow as background color.\nMethod\nScope factor in MenatQA\nExtraction Reasoning\nF1 EM F1 EM\nBigBird(NQ) 43.38 30.13 35.31 23.72\nFiD(NQ) 48.77 36.73 41.03 30.63\nLLMs\nChatGLM-6B 24.97 6.21 21.49 6.11\nBloom-7B1 31.90 17.33 25.69 13.33\nGPT-J-6B 38.86 27.86 34.56 24.95\nGPT-Neox-20B 43.71 30.76 36.23 25.55\nOPT-6.7B 35.50 24.65 29.14 19.84\nOPT-13B 45.10 32.46 37.30 26.85\nLLama-7B 57.09 43.09 45.71 34.47\nLLama-13B 65.55 53.41 52.13 41.58\nGPT-3.5-turbo 52.52 39.08 47.34 37.78\nTable 3: The performance of LLMs on extraction and\nreasoning questions in the scope factor of MenatQA.\nBold Scores indicate a higher performance than FiD.\nfinding proves that LLMs are highly susceptible\nto temporal biases, and their ability to reason\nabout time relies on the specific temporal infor-\nmation provided in the question.\nFinally, larger parameter sizes generally lead\nto a stronger temporal reasoning ability in the\nsame series of LLMs. (i.e., LLama-7B & LLama-\n13B; and OPT-6.7B & OPT-13B). This conclusion\nis consistent with previous works (Zhong et al.,\n2021; Wei et al., 2022) that LLMs with a larger\nnumber of parameters tend to exhibit better perfor-\nmance.\n4 Simple Investigations for Improvement\nIn order to handle the three types of time factors\nin MenatQA, this paper proposes scope prompt-\ning, counterfactual prompting and rerank prompt-\ning methods under the zero-shot settings. Since\nthe scope prompting method is not universal (e.g.,\nit causes the EM score of GPT-3.5-turbo to drop\nfrom 37.78 to 31.36, as shown in Table 4), this\npaper explores tool learning and designs a time\ncomparison tool specifically to address the scope\nfactor questions.\n4.1 Specific Prompts for Three Temporal\nFactors\nBase Prompt To evaluate the temporal reasoning\nperformance of LLMs in the zero-shot setting, this\npaper uses the Base Prompt:\nScope Prompt Following the way humans answer\ntime scope questions, we first identify the start and\n1438\nend time specifiers of the events in the context, and\nthen compare the time in the question with the time\ninterval of the corresponding event, so as to achieve\ntemporal reasoning by comparing two time scopes.\nThe scope prompting template is as follows:\nCounterfactual Prompt In this paper, we propose\nto transform the context to a narrator’s statement\nand the question to enquire about the narrator’s\nopinion in this statement (Zhou et al., 2023). Our\nmethod is motivated by our own cognitive process\nfor answering different types of questions. The\ncounterfactual prompting template is as follows:\nRerank Prompt In real-world scenarios, numer-\nical information such as years often appears in\ndifferent sentences in the text. For example, the\nrecording of events is usually in chronological or-\nder, and the time specifier is used to distinguish\ndifferent events. Therefore, we use the year infor-\nmation in the sentences to reorder the chronological\nsequence of multiple events. The rerank prompting\ntemplate is as follows:\nIn all of the above prompting templates, where c\ndenotes the context, h represents the hypothetical\nscenarios, and q represents the main question of the\noriginal question. Specially, the instruction setting\nin the counterfactual prompt is consistent with the\nbase prompt.\n4.2 Tool Learning for Temporal Scope Factor\nTools provide domain-specific knowledge and ca-\npabilities. By leveraging tools to address the weak-\nnesses of LLMs in tasks that go beyond the realm\nof pure natural language, such as arithmetic calcu-\nlation (Wei et al., 2023) and table-based question\nanswering (Lei et al., 2022), we can effectively\nbridge the gap between language understanding\nand task-specific requirements, enabling LLMs to\nexcel in a wider range of applications beyond tradi-\ntional NLP tasks.\nTime Comparison Tool This paper follows the\nREACT (Yao et al., 2022), which prompts an LLM\nto generate reasoning texts that break down com-\nplex problems into intermediate steps, and action\ntexts that allocate NLP tools for solving these steps.\nOne example is that a LLM can make a decision\nbased on real-time problems to call a search engine\nand gather the latest internet information that is\nnot present in the pre-training corpus, and return\nit to the user. Inspired by the efficacy of reasoning\nand acting with LLMs and NLP tools, we explore\nthe integration of time comparison tool with LLMs.\nIn our setting, we build our time comparison tool\nbased on the langchain 3 framework. By compar-\ning whether the event mentioned in the question\nfalls within the temporal scope corresponding to\nthe events in the context, this approach helps LLMs\nunderstand temporal scope knowledge, as shown\nin Figure 8.\n5 Experimental Results\nAs shown in Table 4 and Table 5, our observations\nindicate that utilizing special prompting methods\nand a tool learning method for three temporal fac-\ntors can enhance the performance of LLMs.4\nEffect of the Scope Prompt We present results\nin Table 4, where the section with a yellow back-\nground represents the effect of the scope prompt-\ning method. The scope prompting method im-\nproves performance over LLama-7B and LLama-\n13B (+1.10 and +1.41 on EM metrics). However,\n3https://github.com/hwchase17/langchain\n4We select LLMs with billions (LLama-7B), tens of bil-\nlions (LLama-13B), hundreds of billions (GPT-3.5-turbo) pa-\nrameter scales as baselines for the proposed solutions in this\npaper.\n1439\nMethod\nMenatQA\nw/scope\nMenatQA\nw/order\nMenatQA\nw/counterfactual\nMenatQA\nw/all factors\nF1 EM F1 EM F1 EM F1 EM\nLLama-7B 45.71 34.47 51.93 37.17 49.78 33.97 37.53 23.45\nLLama-7B + Prompts 46.59 35.57 53.57 38.34 56.21 42.48 46.34 34.07\nLLama-13B 52.13 41.58 64.45 52.40 53.56 41.48 39.80 28.66\nLLama-13B + Prompts 51.72 42.99 64.47 52.19 60.55 49.90 48.37 38.98\nGPT-3.5-turbo 47.34 37.78 51.20 38.38 34.69 27.66 27.99 24.45\nGPT-3.5-turbo + Prompts 42.41 31.36 51.66 38.61 42.69 33.87 36.72 30.76\nTable 4: The effect of the various prompting methods, where the scope prompt, order prompt, and counterfactual\nprompt are represented by the background colors, Blue , Green and Red , respectively. Notably, the Orange\nbackground color is used to indicate the simultaneous use of the scope prompt, order prompt and counterfactual\nprompt.\nMethod\nMenatQA\nScope Factor All Factors\nF1 EM F1 EM\nLLama-7B 45.71 (0.00) 34.47 (0.00) 37.53 (0.00) 23.45 (0.00)\nLLama-7B + prompts 46.59 (0.88) 35.57 (1.10) 46.34 (8.81) 34.07 (10.62)\nLLama-7B + tool + prompts 46.90 (1.19) 35.37 (0.90) 44.67 (7.14) 32.67 (9.22)\nLLama-13B 52.13 (0.00) 41.58 (0.00) 39.80 (0.00) 28.66 (0.00)\nLLama-13B + prompts 51.72 (-0.41) 42.99 (1.41) 48.37 (8.57) 39.98 (10.32)\nLLama-13B + tool + prompts 65.85 (13.72) 55.03 (13.45) 61.06 (21.26) 51.80 (23.14)\nGPT-3.5-turbo 47.34 (0.00) 37.78 (0.00) 27.99 (0.00) 24.45 (0.00)\nGPT-3.5-turbo + prompts 42.41 (-4.93) 31.36 (-6.42) 36.72 (8.73) 30.76 (6.31)\nGPT-3.5-turbo + tool + prompts 47.71 (0.37) 37.58 (-0.20) 38.65 (10.66) 32.87 (8.42)\nTable 5: The table shows a comparison between the time comparison tool and the scope prompt on the scope factor\nand all factors. In brackets, the differences from scores compared to the original LLMs.\nit does not do as well on GPT-3.5-turbo, which\nsignificantly reduces the EM score (-6.42).\nEffect of the Counterfactual Prompt Based on\nthe results in Table 4, we can find that the counter-\nfactual prompt exhibits the greatest improvement\nin LLMs compared to the other two methods, with\nan average increase of 7.71 in EM score. This\nindicates that transforming counterfactual events\ninto the perspective of others can effectively assist\nLLMs in achieving counterfactual temporal associ-\nations and reasoning.\nEffect of the Rerank Prompt Compared to the\nhighlighted sections with yellow background in\nTable 4, it can be observed that the use of the rerank\nprompt exhibits only a minor improvement in the\norder factor, possibly due to the loss of information\nin the sorted context. We conduct an evaluation\nof the quality of the reordered context, and the\nresults reveal that LLMs are not inclined to output\nevery word in the context verbatim but rather tend\nto reorganize their language output, as shown in\nA.4.\nEffect of the Time Comparsion Tool One the\none hand, the experimental results in Table 5 indi-\ncate that the time comparison tool has stronger ro-\nbustness compared to the scope prompting method,\nwith similar performance on LLama-7B, and the\ntime comparison tool does not cause drastically\nperformance degradation on GPT-3.5-turbo, unlike\nthe scope prompting method. Besides, the time\ncomparison tool significantly improved the perfor-\nmance on LLama-13B, these results demonstrate\nthat the tool is more suitable for LLMs with larger\nparameters to address time scope questions com-\npared to the scope prompting method. On the other\nhand, the performance difference between LLama-\n7B and LLama-13B shows that LLMs with larger\nparameter sizes have a stronger capacity for utiliz-\n1440\ning tools. However, the performance of GPT-3.5-\nturbo do not improve, possibly due to its incorrect\nunderstanding of the temporal feedback provided\nby the tool and the limited impact of the scope\nfactor (e.g., EM metrics from 39.08 to 37.78), as\nshown in Table 3.\n6 Related Work\nThere have been plenty of works to tackle the tem-\nporal reasoning task. Zhang and Choi (2021) was\nintroduced to tackle open-domain time-sensitive\nquestion answering, with a particular emphasis\non analyzing how answers differ based on extra-\nlinguistic factors , such as the time of inquiry. Kasai\net al. (2022) extended time question answering to\nscenarios where real-time news serves as context,\nand requested the model to retrieve the latest tempo-\nral evidence to answer the question. StreamingQA\n(Liska et al., 2022) introduced the first QA dataset\nand task for studying adaptation to new informa-\ntion over time in open and close-book settings with\ntemporally non-overlapping training and evalua-\ntion sets. TimeQA (Chen et al., 2021) built the first\ndataset to investigate whether existing models can\nunderstand time-sensitive facts.\nThere are a few major differences between the\naforementioned works and MenatQA : 1) MenatQA\nencompasses various temporal factors, such as the\nscope factor, order factor, and counterfactual fac-\ntor, involving a significant amount of reasoning\nabout implicit temporal information. This aspect of\ntemporal reasoning ability, which is neglected by\nprevious works, is the most important. 2) MenatQA\nis not only the first dataset designed specifically for\nevaluating the time understanding and reasoning ca-\npabilities of LLMs, but also provides some simple\noptimization methods and baseline comparisons,\nwhich offer valuable references for evaluating the\ntime reasoning of LLMs in the future. 3) Consid-\nering the existence of hallucinations in generative\nmodels, we introduce unanswerable types to pe-\nnalize the illusory outputs of LLMs in MenatQA.\nThese unanswerable type questions are impossible\nfor humans to answer as well, and enable a genuine\nassessment of whether LLMs truly grasp temporal\nknowledge.\nOne concurrent work (published on 15 Jun 2023)\nsimilar to ours is (Tan et al., 2023), which proposed\na comprehensive probing dataset TEMPREASON\nto evaluate the temporal reasoning capability of lan-\nguage models. They also proposed a temporal span\nextraction and time-sensitive reinforcement learn-\ning framework to improve the temporal reasoning\ncapability of large language models. However, they\nonly evaluated three models, T5-Large (780M),\nFlan-T5-Large (780M), and GPT-3.5-turbo (175B),\nand mainly focused on using fine-tuning to improve\nthe time reasoning ability of T5-Large and Flan-T5-\nLarge. Besides, the fine-tuning based improvement\nmethods are not applicable to large language mod-\nels, such as OPT-175B. Our work aims to evaluate\nthe time reasoning capability of current mainstream\nLLMs on three time-sensitive factors, and conducts\npreliminary investigations to improve the current\nLLMs on different time factors by designing vari-\nous specific prompts and tool learning.\n7 Conclusion\nIn this paper, we propose a question answering\ndataset named Multiple Sensitive Factors Time\nQA (MenatQA). It is the first dataset containing\nmultiple time-sensitive factors that can be used as\nan evaluation benchmark for assessing the time un-\nderstanding and reasoning abilities of LLMs. We\nfind that most LLMs fall behind smaller temporal\nreasoning models with different degree on three\nfactors. Moreover, the parameter size of LLMs\nsubstantially influences their capacity for tempo-\nral reasoning. LLMs also demonstrate a signifi-\ncant vulnerability to temporal biases and depend\nheavily on the precise temporal information pro-\nvided in questions when reasoning about time. Fi-\nnally, we conduct some preliminary investigations\ninto improving the current LLMs’ performance on\nthe three temporal factors by utilizing prompting\nmethod and tool learning method, which could be\npotential avenues for future research.5\nLimitations\nThe 2853 samples in MenatQA can only be used as\na test set for evaluating LLMs, and the data size is\nnot sufficient for fine-tuning the models. However,\nthis limitation can be mitigated by utilizing previ-\nous temporal reasoning datasets. The improvement\nsolutions proposed in this paper, including the time\ncomparison tool, scope prompt, rerank prompt, and\ncounterfactual prompt, cannot be used as a com-\nplete and mature framework for LLMs. Instead,\nthey represent a preliminary investigation aimed\n5The dataset and code are released in https://github.\ncom/weiyifan1023/MenatQA\n1441\nat improving the LLMs’ performance in time rea-\nsoning. Due to hardware limitations, we do not\nevaluate LLMs that require loading weights with\na scale of more than 20B in the tens of billions\nparameter range.\nAcknowledgements\nThis work was supported by the National Key\nR&D Program of China (2022ZD0160503) and\nthe National Natural Science Foundation of China\n(No.62276264). This research was also supported\nby Meituan. We thank Kang Liu, Yuanzhe Zhang,\nYifan Wei and Xiaoyan Yu for helpful discussions\nof the methods used in this paper and the conclu-\nsions we reach from our experiments. Additionally,\nwe thank Jun Zhao, Yisong Su, Huanhuan Ma and\nFangyu Lei for feedback on writing and presen-\ntation of results. This work was conducted while\nXiaoyan Yu and Yisong Su were an intern student\nat Institute of automation, Chinese academy of sci-\nence.\nReferences\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nWenhu Chen, Xinyi Wang, and William Yang Wang.\n2021. A dataset for answering time-sensitive ques-\ntions. arXiv preprint arXiv:2108.06314.\nGautier Izacard and Edouard Grave. 2020. Leverag-\ning passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. Realtime qa: What’s the answer right now?\narXiv preprint arXiv:2207.13332.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453–\n466.\nFangyu Lei, Shizhu He, Xiang Li, Jun Zhao, and Kang\nLiu. 2022. Answering numerical reasoning ques-\ntions in table-text hybrid contents with graph-based\nencoder and tree-based decoder. arXiv preprint\narXiv:2209.07692.\nFangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming\nHuang, Jun Zhao, and Kang Liu. 2023. S3HQA: A\nthree-stage approach for multi-hop text-table hybrid\nquestion answering. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 1731–\n1740, Toronto, Canada. Association for Computa-\ntional Linguistics.\nJiaxuan Li, Lang Yu, and Allyson Ettinger. 2023. Coun-\nterfactual reasoning: Testing language models’ un-\nderstanding of hypothetical scenarios. arXiv preprint\narXiv:2305.16572.\nMoxin Li, Fuli Feng, Hanwang Zhang, Xiangnan He,\nFengbin Zhu, and Tat-Seng Chua. 2022. Learning\nto imagine: Integrating counterfactual thinking in\nneural discrete reasoning. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 57–69.\nAdam Liska, Tomas Kocisky, Elena Gribovskaya, Tay-\nfun Terzi, Eren Sezener, Devang Agrawal, D’Autume\nCyprien De Masson, Tim Scholtes, Manzil Zaheer,\nSusannah Young, et al. 2022. Streamingqa: A bench-\nmark for adaptation to new knowledge over time in\nquestion answering models. In International Con-\nference on Machine Learning, pages 13604–13622.\nPMLR.\nHuanhuan Ma, Weizhi Xu, Yifan Wei, Liuji Chen, Liang\nWang, Qiang Liu, Shu Wu, and Liang Wang. 2023.\nEx-fever: A dataset for multi-hop explainable fact\nverification.\nOpenAI. 2023. Gpt-4 technical report.\nAbulhair Saparov and He He. 2022. Language models\nare greedy reasoners: A systematic formal analysis of\nchain-of-thought. arXiv preprint arXiv:2210.01240.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nQingyu Tan, Hwee Tou Ng, and Lidong Bing. 2023.\nTowards benchmarking and improving the temporal\nreasoning capability of large language models.\n1442\nTianyi Tang, Yushuo Chen, Yifan Du, Junyi Li,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Learn-\ning to imagine: Visually-augmented natural language\ngeneration. arXiv preprint arXiv:2305.16944.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nBen Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6\nbillion parameter autoregressive language model.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nYifan Wei, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, and\nKang Liu. 2023. Multi-view graph representation\nlearning for answering hybrid numerical reasoning\nquestion. arXiv preprint arXiv:2305.03458.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in neural information\nprocessing systems, 33:17283–17297.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nMichael JQ Zhang and Eunsol Choi. 2021. Situatedqa:\nIncorporating extra-linguistic contexts into qa. arXiv\npreprint arXiv:2109.06157.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nRuiqi Zhong, Dhruba Ghosh, Dan Klein, and Jacob\nSteinhardt. 2021. Are larger pretrained language\nmodels uniformly better? comparing performance at\nthe instance level. arXiv preprint arXiv:2105.06020.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023. Context-faithful prompt-\ning for large language models. arXiv preprint\narXiv:2303.11315.\nA Appendix\nA.1 Data statistics\nIn MenatQA, the order factor can be combined\nwith other counterfactual and scope factors. Specif-\nically, the scope factor type can be further classified\ninto granularity operation, contraction operation,\nand expansion operation, as shown in section 2.1.\nWe calculated the proportions of different question\ntypes under ordered and unordered contexts, as\nshown in Figure 3 and Figure 4. Additionally, we\nalso calculated the proportions of answerable and\nunanswerable question types, and the results are\nshown in Figure 5 and Figure 6.\nFigure 3: order statistic\nFigure 4: disorder statistic\nA.2 Data annotation\nWe recruit college students majoring in English-\nrelated fields and adopt the quality control ap-\nproaches of annotator training and two-round vali-\ndation to ensure the quality of MenatQA.\nConsidering the input length limitation of LLMs,\nwe set the maximum number of documents in\n1443\nFigure 5: answerable statistic\nFigure 6: unanswerable statistic\nTimeQA to 5, which already includes the gold ev-\nidence. Any other documents exceeding the max-\nimum number will be filtered out. In our Closed\nBook QA setting, there is no need to set up a re-\ntriever to search for relevant documents. We en-\nsure that all answers come from the context which\nprovide based on the gold paragraph field given\nin TimeQA’s annotation documents. Taking into\naccount the phenomenon of knowledge conflicts ,\nwe restrict the temporal scope of the questions to\nbefore 2021. This measure ensures that the con-\ntext from Wikipedia pages appears in the pretrain-\ning corpus of LLMs, thereby aligning the parame-\nter knowledge of LLMs with the external context\nknowledge.\nWe generate scope factor type data using the\nprompt shown in Figure 7, scope factor can be\nfurther divided into three operations: granularity,\ncontraction, and expansion.\nA.3 Experimental Setup\nA.3.1 Time Comparison Tool\nThe workflow diagram for using the time compari-\nson tool is shown in Figure 8. In this paper, we used\nLLama-7B, LLama-13B, and GPT-3.5-turbo as the\nLLMs, and version 0.0.166 of the Langchain frame-\nwork was used to implement the Time Comparison\nTool.\nA.3.2 Zero-Shot Setting\nAll of our experiments were conducted under the\nzero-shot setting based on the base prompt, as\nshown in Base Prompt, and all the LLMs used\nin our experiments can be downloaded from the\nofficial website of Hugging Face.\nA.3.3 Extraction and Reasoning Questions\nIn section 3.3, to validate the sensitivity of LLMs to\nvarious time factors, only reasoning type questions\nwere used in the scope factor, and extraction type\nquestions were excluded. Sepcifically, extraction\ntype questions originate from the TimeQA easy\nmode version, where the time points mentioned\nin the questions are explicitly present in the con-\ntext. On the other hand, reasoning type questions\ninvolve time points that cannot be directly found\nin the context and require inference to obtain the\nanswers. Moreover, reasoning type questions can\nbe further classified into granularity questions, con-\ntraction questions, and expansion questions. These\ncategories align with the classification in previ-\nous works, and therefore, no further discussion\nis needed.\nA.3.4 Baselines Setting\nBased on the Table 2, we choose the best LLMs\nwith parameter sizes at the billion scale (e.g.,\nLLama-7B), tens of billions scale (e.g., LLama-\n13B), and hundreds of billions scale (e.g., GPT-3.5-\nturbo) as baseline models to evaluate the effective-\nness of our proposed enhancement methods (e.g.,\nTime Comparison Tool). To ensure that the pre-\ndictions are consistent, we used the GPT-3.5-turbo-\n0301 version of ChatGPT.\nA.3.5 Parameter Setting\nWe use InstructGPT (gpt-3.5-turbo) as the frozen\nLLM, with temperature set to 0.0 and nucleus\nsampling set to 1 and n represents the number of\nchat completion options generated for each input\nprompt, which is set to 1. The hyperparameter set-\ntings for other LLMs are the same as above. We\nselected the EM metric as our primary evaluation\nmetric to measure the performance of LLMs, and\nreport performance averaged over 3 runs.\n1444\nFigure 7: few-shot construction prompt\nA.3.6 Baseline models\nThe models used in this paper are as follows:\n• BigBird and FiD use 12 layers of encoder\nand decoder with 12 attention heads based on\nHugginFace Transformer.\n• ChatGLM (6B), ChatGLM-6B is an open\nbilingual language model based on General\nLanguage Model (GLM) framework, with 6.2\nbillion parameters. ChatGLM-6B uses tech-\nnology similar to ChatGPT, optimized for Chi-\nnese QA and dialogue.\n• BLOOM (7.1B), BLOOM model is a large\ndecoder-only language model pretrained for\naround 350 billion tokens with an architecture\nsimilar to GPT-3.\n• GPT-J (6B), an auto-regressive text genera-\ntion model trained on the Pile with 6 billion\nparameters.\n• GPT-NEOX (20B), a 20 billion parameter\nauto-regressive language model trained on the\nPile.\n• OPT (6.7B and 13B), a suite of decoder-only\npre-trained transformers ranging from 125M\nto 175B parameters.\n• LLAMA (7B and 13B) , a collection of foun-\ndation language models ranging from 7B to\n65B parameters, and it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLAMA-13B outperforms GPT-3\n(175B) on most benchmarks.\n• ChatGPT (gpt-3.5-turbo), the most capable\nand cost effective model in the GPT-3.5 fam-\nily is gpt-3.5-turbo which is optimized for\nchat but works well for traditional comple-\ntions tasks as well, and openai recommends\nusing gpt-3.5-turbo over the other GPT-3.5\nmodels because of its lower cost.\nA.4 Case Study\nCompared to other LLMs, GPT-3.5-turbo tends to\nclassify questions that require temporal reasoning\n(where the time period is not directly mentioned\nin the context) as unanswerable, leading to incor-\nrect outputs of \"unanswerable\". On the other hand,\nother LLMs such as LLama are lacking in their abil-\nity to reject answering, making them more likely\nto classify the questions as answerable. Therefore,\nthe impact of the counterfactual type on LLama is\nsmaller compared to GPT-3.5-turbo.\nHere are the possible reasons we have inferred:\n1) LLMs exhibit performance degradation due to\nscope factors, as the time involved in the questions\ndoes not appear directly in the context and requires\nmodel inference to derive the answer. This type is\nmore challenging compared to extractive questions,\nas shown in Table 3. 2) To the best of our knowl-\nedge, we are the first to introduce counterfactual\n1445\nFigure 8: The overall process of the Time Comparison Tool. The Time Comparison Tool is used to determine\nwhether the events in the question belong to the corresponding time range of the events in the context. Specially,\nScope Q refers to the blue temporal information involved in the question, and the timeline represents the events that\nappear in the context and their corresponding time ranges.\nquestions related to time, which LLMs have not\nencountered in their pretraining corpus, resulting\nin poor performance on these counterfactual ques-\ntions. 3) Furthermore, in the order type, since the\ntime relevant to the question appears directly in\nthe context, LLMs can directly extract the answer\nfrom the context, resulting in the least performance\ndegradation on this factor.\nA.5 Sample Introduction\nIn this work, unanswerable questions refer to ques-\ntions that cannot be inferred from the temporal in-\nformation in the context within a given time period.\nAs shown in Figure 1, an example of unanswer-\nable question is \"who was the CEO of Twitter from\n2005 to March 2006?\". In cases like this, where\nthe time period mentioned in the question is not\naddressed in the context, it is not possible to infer\nand provide an answer. Therefore, the appropriate\noutput would be marked as \"unanswerable.\" The\nsample results of the rerank prompt are shown in\nFigure 9.\n1446\nFigure 9: Rerank case on MenatQA using rerank prompt.\n1447",
  "topic": "Comprehension",
  "concepts": [
    {
      "name": "Comprehension",
      "score": 0.7154876589775085
    },
    {
      "name": "Counterfactual thinking",
      "score": 0.6740242838859558
    },
    {
      "name": "Mainstream",
      "score": 0.5044564008712769
    },
    {
      "name": "Computer science",
      "score": 0.4766186773777008
    },
    {
      "name": "Vulnerability (computing)",
      "score": 0.464127779006958
    },
    {
      "name": "Cognitive psychology",
      "score": 0.397086501121521
    },
    {
      "name": "Psychology",
      "score": 0.27445030212402344
    },
    {
      "name": "Social psychology",
      "score": 0.16304343938827515
    },
    {
      "name": "Political science",
      "score": 0.1297174096107483
    },
    {
      "name": "Computer security",
      "score": 0.08328157663345337
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I80947539",
      "name": "Fuzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ],
  "cited_by": 8
}