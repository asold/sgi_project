{
  "title": "SceneFormer: Indoor Scene Generation with Transformers",
  "url": "https://openalex.org/W3112776202",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2267141901",
      "name": "Wang Xinpeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226814851",
      "name": "Yeshwanth, Chandan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214023352",
      "name": "Nießner, Matthias",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250673545",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3034727889",
    "https://openalex.org/W2766913266",
    "https://openalex.org/W2963601843",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W2960202457",
    "https://openalex.org/W2162559028",
    "https://openalex.org/W3016025127",
    "https://openalex.org/W2594830175",
    "https://openalex.org/W2118714046",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035599387",
    "https://openalex.org/W2594030426",
    "https://openalex.org/W2810181048",
    "https://openalex.org/W2994701915",
    "https://openalex.org/W2251645943",
    "https://openalex.org/W2557465155",
    "https://openalex.org/W2509413994",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2149846167",
    "https://openalex.org/W2979672901",
    "https://openalex.org/W2888393442",
    "https://openalex.org/W2250711463",
    "https://openalex.org/W2902539442",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W3035108870",
    "https://openalex.org/W2964334375",
    "https://openalex.org/W2964015378"
  ],
  "abstract": "We address the task of indoor scene generation by generating a sequence of objects, along with their locations and orientations conditioned on a room layout. Large-scale indoor scene datasets allow us to extract patterns from user-designed indoor scenes, and generate new scenes based on these patterns. Existing methods rely on the 2D or 3D appearance of these scenes in addition to object positions, and make assumptions about the possible relations between objects. In contrast, we do not use any appearance information, and implicitly learn object relations using the self-attention mechanism of transformers. We show that our model design leads to faster scene generation with similar or improved levels of realism compared to previous methods. Our method is also flexible, as it can be conditioned not only on the room layout but also on text descriptions of the room, using only the cross-attention mechanism of transformers. Our user study shows that our generated scenes are preferred to the state-of-the-art FastSynth scenes 53.9% and 56.7% of the time for bedroom and living room scenes, respectively. At the same time, we generate a scene in 1.48 seconds on average, 20% faster than FastSynth.",
  "full_text": "SceneFormer: Indoor Scene Generation with Transformers\nXinpeng Wang∗ Chandan Yeshwanth∗ Matthias Nießner\nTechnical University of Munich\nFigure 1: 3D scenes generated by our method: the input to our model is a room layout (e.g., room ﬂoor, locations of doors\nand windows), from which we populate the room with CAD objects to generate a full 3D scene. We predict a sequence of\nobject locations, leveraging the self-attention mechanism of transformers to obtain realistic 3D scene arrangements.\nAbstract\nWe address the task of indoor scene generation by gener-\nating a sequence of objects, along with their locations and\norientations conditioned on a room layout. Large-scale in-\ndoor scene datasets allow us to extract patterns from user-\ndesigned indoor scenes, and generate new scenes based on\nthese patterns. Existing methods rely on the 2D or 3D ap-\npearance of these scenes in addition to object positions, and\nmake assumptions about the possible relations between ob-\njects. In contrast, we do not use any appearance informa-\ntion, and implicitly learn object relations using the self-\nattention mechanism of transformers. We show that our\nmodel design leads to faster scene generation with similar\nor improved levels of realism compared to previous meth-\nods. Our method is also ﬂexible, as it can be conditioned not\nonly on the room layout but also on text descriptions of the\nroom, using only the cross-attention mechanism of trans-\nformers. Our user study shows that our generated scenes\nare preferred to the state-of-the-art FastSynth scenes 53.9%\nand 56.7% of the time for bedroom and living room scenes,\nrespectively. At the same time, we generate a scene in 1.48\nseconds on average, 20% faster than FastSynth.\n∗ Equal contribution\n1. Introduction\nGenerating realistic 3D indoor scenes has a wide range\nof real-world applications for 3D content creation. For in-\nstance, real estate and interior furnishing companies can\nquickly visualize a furnished room and its contents without\nrequiring the rearrangement of any physical objects. Such a\nroom can be presented through augmented or virtual reality\nplatforms such as a headset, allowing a user to walk through\ntheir future home and interactively modify it.\nWe address the task of scene generation from a room lay-\nout by generating a set of objects and their arrangements in\nthe room. Each object is generated with a predicted class\ncategory, a 3D location, its angular orientation, and a 3D\nsize. Once this sequence is generated, the most relevant\nCAD model for each object is retrieved from a database\nand placed in the scene at the predicted location. The rele-\nvance of a CAD model can be predicted based only on size,\na shape descriptor [35], or other heuristics such as texture.\nCAD model selection reduces object collisions, accommo-\ndates special object properties such as symmetry and can\nensure style-consistency across objects.\nExisting methods operate on internal representations of\nthe scene such as 2D images [33], graphs [32], and ma-\ntrices [35]. Several of these works generate objects in an\nautoregressive manner - the properties of the (n+ 1)th ob-\narXiv:2012.09793v2  [cs.CV]  2 Apr 2021\nject are conditioned on the properties of the ﬁrst nobjects.\nWe adopt a similar autoregressive prediction. The extrac-\ntion of object and scene patterns is enabled by large object\nand scene datasets such as ModelNet [34], ShapeNet [8]\nand other human-created scene datasets with synthetic ob-\njects [29]. Some existing methods require object relations\nto be annotated, and assume a ﬁxed set of possible relations\n[36, 32], instead of operating on the raw scene data. In con-\ntrast, we operate directly on the raw locations and orienta-\ntions of the objects without any additional information. We\nthus avoid any bias introduced by the manual selection of\nrelations, or the heuristics used to create these relations.\nTransformers [31] perform well on a variety of natu-\nral language processing tasks by treating a sentence as a\nsequence of words, and recently on images [20] and 3D\nmeshes [23] as well. Based on the idea that a scene can be\ntreated as asequence of objects, we proposeSceneFormer, a\nseries of transformers that autoregressively predict the class\ncategory, location, orientation and the size of each object\nin a scene. We show that such a model generates realistic\nand diverse scenes, while requiring little domain knowledge\nor data preparation. In addition, we do not use any visual\ninformation such as a 2D rendering of the scene either as\ninput or as an internal representation.\nWe use the cross-attention mechanism of the Trans-\nformer decoder to build conditional models. Conditioning\ndiffers from a translation task – in translation the output is\nexpected to be aligned with the input as closely as possible\naccording to a given metric, while in a conditioning task the\ninput only guides the scene generation. We expect the out-\nput to mostly adhere to the input, but it can contain some\ninformation not indicated by the input. We condition sep-\narately on two kinds of user inputs. The ﬁrst is the room\nlayout, including the positions of doors and windows. Our\nlayout-conditioned scenes are preferred over the state-of-\nthe-art FastSynth 53.9% of the time for bedroom scenes,\nand 56.7% for living room scenes. The second type of in-\nput is a text description of the room, such as “There is a\nroom with a bed and a wardrobe. There is a table lamp next\nto the wardrobe. ”. Our text-conditioned model outperforms\nbaselines in terms of category and relation accuracy.\nIn summary, our main contributions are:\n• We represent an indoor scene as a sequence of object\nproperties, converting scene generation to a sequence\ngeneration task.\n• We leverage the self-attention of transformers to im-\nplicitly learn relations between objects in a scene,\neliminating the need for manually-annotated relations.\n• We generate complex scenes conditioned on room lay-\nout or text descriptions by leveraging discretized ob-\nject coordinates to predict their 3D locations.\n2. Related Work\nGraph-based 3D Scene Generation.A natural represen-\ntation of a scene is a graph [36, 20, 26], where each object\nis a node, and an edge is a relation between objects (e.g.,\n‘is next to‘, ‘is on top of‘). Features of the room such as\nwalls, doors and windows can be represented as nodes of\nthe graph [32]. This gives a simple method for conditioning\n– when the model is autoregressive on the graph and is able\nto generate one node at a time, the input is initialized with\nthe required object nodes and then repeatedly expanded us-\ning the model. Such a representation lends itself well to\nprocessing with graph convolutional networks [17].\nA scene can also be represented as a matrix of objects in\neach category [35] or a hierarchy of objects [18]. Next, an\nappropriate encoder-decoder or other model is used to gen-\nerate an output scene in the same format. Other methods\nplace objects with a Bayesian model [19], or model object\nproperties with probability distributions learned from the\ndata [14, 22]. Yet other works condition on full 3D scans\n[15], or focus on ﬁne-grained and smaller objects such as a\ntable with multiple objects on it [12]. However, these meth-\nods require complex optimization and post-processing steps\nto obtain realistic scenes. This may be in the form of a dis-\ncriminative loss, linear programming or other heuristics that\nare speciﬁc to the task. In contrast, we use only the cross-\nentropy loss for classiﬁcation that is both conceptually sim-\npler and easier to optimize.\nScene Generation from an Image.A scene can be rep-\nresented by a top down view of the objects, walls, and ﬂoor\n[27, 33]. [27] and [33] predict the walls and ﬂoors as binary\nimages, and object properties by their continuous or dis-\ncrete values. This can be used to represent arbitrary room\nshapes and sizes, by normalizing the room dimensions to a\nknown maximum dimension along each axis. Image repre-\nsentations can take advantage of modern CNN architectures\nsuch as ResNet [13].\nText-Conditioned Scene Generation. Several previous\nworks have addressed the task of text-conditional genera-\ntion or text-to-scene translation. Text inputs have been used\nto create detailed partial scenes [21, 6], such as a table with\nseveral small objects on it, which are then inserted into the\nlarger scene. User input is required for reﬁnement at ev-\nery step, making it semi-automatic. Similarly, SceneSeer\n[7] and related methods [4, 28] rely on interactive user in-\nputs. Other related works [3, 5] generate simple scenes with\nfew objects in them, by inferring rules from a smaller-scale\nhuman-annotated dataset. Text2Scene [30] solves a similar\ntask in 2D, by iteratively placing objects into an 2D im-\nage and then ensuring consistency. Intelligent Home 3D [9]\ntackles the related task of generating the full room layout\nFigure 2: Layout-conditioned SceneFormer: We take as input the room layout describing the room shape and locations of\ndoors and windows. The SceneFormer model sequentially generates the properties of the next object and inserts the object\ninto the existing scene. The ﬁnal output scene is shown on the right.\nof a house from text, and proposes a new dataset for this\ntask. Our method differs from these in that it can generate\nhigh quality complex scenes with a large number of objects,\nwithout requiring user input. However, our model is still\nﬂexible enough to accept user inputs if desired.\n3. Method\nAn overview of our scene generation approach is shown\nin Fig. 2. We ﬁrst discuss our data preparation, and then\nthe SceneFormer model and its layout-conditioned and text-\nconditioned variants.\n3.1. Data Preparation\nWe treat each scene as a sequence of objects, ordered\nby the frequency fci of their class categories ci in the\ntrain set. This ordering of objects is required to produce\na unique representation of each scene, up to the ordering\nwithin objects of the same class. Then the location of an\nobject is normalized by the maximum room size, and quan-\ntized into the range [0,255] to give the new coordinates of\nthe object (x,y,z ). Similarly the dimensions of each ob-\nject; length, width and height or (l,w,h ) are scaled and\nquantized. The orientation of the object in the ﬂoor plane\nof the room θ is quantized in the range [0,359]. Hence,\nfor each scene with objects {oi}we obtain 8 sequences\n({ci},{xi},{yi},{zi},{θi},{li},{wi},{hi}). We then\nadd startand stoptokens to each sequence, indicating the\nbeginning and end, and ﬁnally pad the sequence to the max-\nimum length present in the train set.\n3.2. Transformer for Scene Generation\nOur model architecture is shown in Fig. 3. We use the\ntransformer decoder [31] to generate the sequences of ob-\nject properties. For each of the four properties, we train\na separate model to predict the corresponding token of the\ncurrent object. Object properties are predicted in an au-\ntoregressive manner - each object’s properties are predicted\nconditioned on the previously predicted objects.\nThe distribution over the sequence {ci}is factorized as\np({ci}; Mc) =\nN∏\nn=1\np(cn|o<n; Mc)\nwhere the category model Mc expresses the distribution\nover the category cn of a single object. The factorization\nof the orientation sequence is\np({θi}; Mθ) =\nN∏\nn=1\np(θn|c≤n,o<n; Mθ)\nThe location and dimension models Mloc and Mdim are\nconditioned on tokens generated so far, for example the fac-\ntorization for {yi}is\np({yi}; Mloc) =\nN∏\nn=1\np(yn|c≤n,θ≤n,x≤n,o<n; Mloc)\nEach model is conditioned on the output of the previous\nmodels. The category model is conditioned on all previous\nobjects. The orientation model is conditioned on the cate-\ngory of the current object, as well as all other properties of\nall previous objects and so on.\nWe ﬁnd empirically that a single transformer with com-\nparable model capacity is difﬁcult to optimize, and tends to\nproduce unrealistic scenes, since learning a sequence com-\nposed of different features is hard. Further, conditioning on\nthe object dimensions does not improve performance, hence\nFigure 3: Layout-conditioned SceneFormer model. Start tokens are shown in green, tokens for existing objects in gray and\ntokens for new objects in yellow. Stop tokens and padding are omitted. All models take 3 kinds of sequences as input -\ncategory, orientation and location. Their respective outputs, except in the case of the dimension model, are appended to the\nexisting sequence before passing it on. A model with N output tokens is run N times, producing one token from each step.\nnone of our models, except the dimension model, take the\ndimension sequence as input. Intuitively, a prior over object\ndimensions is learned and the model infers likely object di-\nmensions of previous objects based on their categories and\nlocations.\nSimilarly, swapping the order of location and orientation\nmodels led to unrealistic object locations. Learning loca-\ntions is the most difﬁcult task of the 4 properties considered\nand the location model beneﬁts from more inputs.\nSequence Representation Each model takes multiple se-\nquences as input. Since the location (x,y,z ) and dimen-\nsion (l,w,h ) of each object are 3-dimensional, the input\nsequences for location and dimension are obtained by con-\ncatenating tuples of (xi,yi,zi)i and (li,wi,hi)i. Therefore,\nthe other input sequences should be repeated 3 times. In or-\nder to condition on properties of the current object during\ntraining, the corresponding input sequence is shifted to the\nleft by one token. For example, as shown in Fig. 3, the\ncategory input sequence for the orientation model is shifted\ntowards the left by one token, so that the orientation is gen-\nerated conditioned on the category of the current object.\nEmbedding We use learned position and value embed-\ndings [10] of the input sequences for each model. The po-\nsition embedding indicates the position in the sequence the\nobject belongs to. The value embedding indicates the to-\nken’s value. For the location and dimension models, we\nadd another embedding layer to indicate whether the token\nis an x,y or zcoordinate for location sequence, and whether\nthe token is l,w or hfor the dimension sequence. Then we\ncombine the embeddings of all sequences by addition.\nThe output embedding is converted to N logits with a\nlinear layer, where N is the number of possible quantized\nvalues. Each network is trained independently with a cross\nentropy loss.\nInference During inference, properties of objects are gen-\nerated in the order of class category, orientation, location\nand dimension. Once a new token is generated, the cor-\nresponding sequence is appended with the new token and\ngiven as input to the next model. The location and dimen-\nsion models are run three times each, to obtain three differ-\nent output tokens (x,y,z ) and (l,w,h ) respectively.\nWe use probabilistic nucleus sampling (top-p sampling)\non the category model outputs with p = 0.9, and pick the\ntoken with the maximum score from the other 3 models. If\nany model outputs a stop token, the sequence is terminated.\n3.3. Room-Layout Conditioned Scene Generation\nWe generate an indoor scene conditioned on a room lay-\nout, deﬁning the ﬂoor, windows and doors (walls are as-\nsumed to lie at the edges of the ﬂoor). The ﬂoor is repre-\nsented as a binary image and encoded by a series of residual\nblocks [13], shown in Fig. 3. We use binary images of size\n512 ×512 and obtain feature maps of size 16 ×16 ×E,\nwhere Eis embedding dimension of the transformer model.\nA discrete 2D coordinate embedding is added to this feature\nmap, which is then ﬂattened to obtain a 256 ×Esequence.\nThe SceneFormer decoder then performs cross-attention on\nthe embedded sequence.\nThe locations of doors and windows are inserted as ob-\njects at the beginning of the input sequence. Hence, in-\nference starts with a sequence consisting of the start token\nalong with tokens for doors and windows. Such leading to-\nkens have also been used in works such as CTRL [16]. We\nexperimented with doors and windows as multiple channels\nin the ﬂoor plan image, but locations were not recognized\nprecisely enough, causing collisions.\nWe do not use an additional loss to enforce that the gen-\nerated objects are within the input ﬂoor region.\n3.4. Text-Conditioned Scene Generation\nIn scene generation from text, a room is described by\na list of sentences. We use the ﬁrst 3 sentences, tokenize\nthem and pad the token sequence to a maximum length of\n40 tokens. We then embed each word with an embedding\nfunction. We experiment with GloVe [24], ELMo [25], and\nBERT [11]. We obtain ﬁxed-size word embeddings with di-\nmension d(100,1024 and 768 respectively) using the Flair\nlibrary [1], and then a 2-layer MLP to convert from d to\nE dimensions, where E is the dimension of the Scene-\nFormer embedding. For the text-conditional model, we use\ndecoders only for the category and location models, since\nour sentences only describe object classes and their spatial\nrelations. The decoders for orientation and dimension mod-\nels are replaced by encoders without cross-attention. We\ndo not use an additional loss to align the transformer output\nwith the input text; this relation is learned implicitly.\n3.5. Object Insertion\nFor each generated object, we ﬁnd the CAD model that\nis closest in size using the L2-norm over the dimensions of\nthe object. If this causes a collision with 3D IoU of up to\n0.05, we reselect the next CAD model, and repeat this upto\n20 times. If none of the models ﬁt in the predicted location,\nwe resample the object category. This heuristic is important\nfor large objects placed in rooms with little space left.\n3.6. Data and Training Details\nWe use bedrooms and living rooms from a human-\ncreated scene dataset, referred to as GT (ground truth) [29]\nand ﬁlter the bad samples as done in earlier works [32, 27]\nto obtain a total of 6351 bedrooms and 1099 living rooms,\nwhich are split 80:20 into training and validation sets. We\nuse 50 object categories for bedrooms and 39 object cat-\negories for living rooms. Rooms are augmented with ro-\ntations from the set (0,90,180,270) degrees, and object’s\nlocation jitter is sampled uniformly from (0,0.5). We train\nwith a learning rate of3e−4 and apply cosine annealing with\nrestarts after 40k iterations, for a maximum of 2000 epochs,\nusing the Adam optimizer with a 0.001 weight decay and\na batch size of 128. All experiments are run on a single\nNvidia RTX 2080 Ti. Training takes ≈4 hours for each\nmodel.\nTo generate textual scene descriptions, we use a heuristic\nmethod. We ﬁrst extractrelations between the objects in the\nscene, following [2]. All related objects within a distance of\n2.5mare retained. In addition, objects can only be related to\nan object that appears earlier in the sequence. Then, we use\na set of rules to generate sentences from these ﬁltered rela-\ntions. The ﬁrst sentence mentions up to the ﬁrst 3 objects in\nthe room’s category sequence{ci}. We then iterate over all\nobjects except the ﬁrst in the sequence, and with probability\n0.3 describe an object oi using its relation to another object\noj,j < iwhich has already been described. Each relation\nis a tuple (oi,rel,o j), where relis the relation type.\n4. Results\n4.1. Qualitative Layout-conditioned Generation\nScene Synthesis Comparison Figure 13 shows scenes\ngenerated by our model compared to the scenes from Deep-\nSynth [33], PlanIT [32] and FastSynth [27]. Our approach\ngenerates more complex scenes in terms of object categories\nand object relations. Previous works perform image-based\nscene generation and therefore can only generate objects on\nthe ﬂoor, or objects supported by a plane such as laptop. In\ncontrast, we are able to generate objects on the walls (e.g.,\nair conditioner, TV), and on the ceiling (e.g., chandelier).\nObject relations are also learned - the television is placed\nopposite the bed or sofa, curtains are placed on windows,\nbed stands are placed on either side of the bed, etc..\nScene Completion We show the effectiveness of our\nmethod on the scene completion task in Fig. 6. Given an\nincomplete scene with a few objects, our model can add rel-\nevant and missing objects to complete the scene. As a result\nof training on sorted sequences, the model ﬁrst generates\nlarge and frequent objects before generating small and in-\nfrequent objects in a greedy manner. Thus, stopping gener-\nation at an intermediate stage results in a realistic scene, and\na user can potentially interactively choose how complete the\nscene must be.\nScene Diversity and Memorization Various generated\nscenes conditioned on the same input room shape are shown\nin Fig. 4. We are able to generate different sets of objects\nand object orientations that are consistent with each other.\nTo show our model does not simply memorize the training\nsamples, we compare the generated scene with the nearest\nneighbor in the training set based on two different methods:\nroom shape and object class, as shown in Fig. 9. This indi-\ncates our ability to generate novel scenes.\nObject Category HeatmapFigs. 7 and 8 show heatmaps of\nrelative locations between objects, demonstrating that our\nmodel effectively captures object relations from the data.\n4.2. Qualitative Text-conditioned Scene Generation\nText-conditioned scenes generated by our model and the\ncorresponding descriptions are shown in Fig. 16. Generated\nFigure 4: Scene diversity: a ground truth scene for a room layout is shown at the bottom right. The other 7 scenes are\ngenerated conditioned on its room layout. Note that tables and chairs are learned to generate (or not generate) together in\ncombination. Location diversity is also obtained from rotation and jitter augmentations.\nFigure 5: Bedroom and living room scenes generated by our method in comparison with state of the art. Methods are ordered\nfrom left to right in increasing realism from the perceptual study.\nFigure 6: Scene completion sequence, showing the iterative process of adding objects; the leftmost is the input to the model,\nand the others are obtained sequentially by adding objects.\nscenes largely respect the input sentence in terms of object\ncategories and relations, and have additional objects to im-\nprove the completeness of the scene.\n4.3. Quantitative Layout-conditioned Generation\nPerceptual Study We conducted perceptual studies, fol-\nlowing those done by earlier works [32, 27]. In each study,\nwe compare a scene generated by our method with a scene\nfrom one of DeepSynth [27], FastSynth [27], PlanIT [32] or\nfrom the GT dataset [29]. In each study, users are shown\n50 pairs of images - one from ours and one from the other\nmethod. The user must select the more realistic scene. We\nuse 5 pairs of images as vigilance tests. The responses of\nusers who do not pass all the tests are discarded. Each study\nwas taken by 30 users through Amazon Mechanical Turk.\nTab. 1 shows the results of this study. Our scenes are con-\nsistently preferred over other methods, ranging from 53.5%\nup to 65% preference over other methods.\ndouble bed/stand double bed/tv desk/chairGT\nOurs\nFigure 7: Object location heatmap for pairs of objects in\nbedroom scenes, ground truth and ours. We learn to capture\ncommon patterns in object relations.\nplant/sofa sofa/coffee table sofa/tv\nGT\nOurs\nFigure 8: Object location heatmap for pairs of objects in\nliving room scenes, ground truth and ours.\nFigure 9: Closest scenes from the training set to a generated\nscene from our approach, based on room shape and object\nclass, showing the ability to generate novel scenes.\nAblation Studies We evaluate our design choices through\nablation studies shown in Tab. 2, using the accuracy of the\nnext token over a ﬁxed validation set of bedrooms. We\ncompare the Single model setting (1 transformer with 1 in-\nput sequence) against the Multiple model setting (multiple\ntransformers with parallel input sequences), and the effect\nof using rotation and jitter augmentation. The accuracy of\nthe multiple model setting is the average of all four mod-\nScene Type DeepSynth FastSynth PlanIT GT\nBedroom 55.4±6.3 53.9±6.5 53.5±5.1 50.2±6.6\nLiving 56.6±4.0 56.7±6.1 65.0±7.6 48.5±6.6\nTable 1: Perceptual user study of our scenes vs. GT and\nother methods (top row). Metric is the percentage of im-\nage pairs where our scene is preferred. Separate studies are\nconducted for bedroom and living room scenes.\nels. Augmentation gives small improvements in accuracy,\nwhile using multiple transformers leads to an improvement\nof 14.5 in accuracy. This is seen in the generated scenes as\nwell in Fig. 11.\nModel Accuracy\nSingle-no augmentation 33.1\nSingle-rotation 35.4\nSingle-rotation & jitter 37.0\nMultiple-rotation & jitter 51.5\nTable 2: Next token prediction accuracy for ablations.\nTiming. We evaluate the inference time of our layout-\nconditioned model compared with state-of-the-art in Tab. 3.\nOur model is an order of magnitude faster than PlanIT and\nDeep Synth, and 20% faster than Fast & Flexible.\nMethod Time (s)\nDeepSynth [33] 240\nFast & Flexible [27] 1.85\nPlanIT [32] 11.7\nOurs 1.48\nTable 3: Comparison of inference time.\n4.4. Quantitative Text-conditional Generation\nCategory Accuracy. We measure the fraction of objects\nmentioned in the scene description that are present in the\ngenerated scene, shown in Tab. 4. We compare against 2\nbaselines: Uniform, by sampling uniformly from all cate-\ngories and the stop token, and GT by sampling from the\nground truth object category distribution, with the stop to-\nken having the average frequency of all objects.\nModel Bedroom Living Room\nUniform 23.70 27.89\nGT 38.64 44.11\nOurs (GloVe) 58.10 59.35\nOurs (ELMo) 76.59 63.31\nOurs (BERT) 84.83 68.81\nTable 4: Category accuracy of text-conditioned scene gen-\neration (percentage).\n(a) There is a wardrobe cabinet, a double bed\nand a desk. There is an ofﬁce chair next to the\ndesk.\n(b) The room contains a bunker bed and a\nwardrobe cabinet.\n(c) The room has a wardrobe cabinet, a stand\nand a double bed.\n(d) In the room we see a heater and a\nwardrobe cabinet. There is a stand to the right\nof the wardrobe cabinet.\n(e) In the room we see two ottomans. The\nwardrobe cabinet is to the left of the second\nottoman.\n(f) This room has a wall lamp, a heater and a\nwardrobe cabinet. There is a double bed to the\nleft of the heater.\nFigure 10: Text-conditioned generated bedroom scenes shown with their corresponding text inputs. Since the room layout is\nnot given as input, objects are placed within a layout prior learned from the ground truth scenes. Important relations such as\ntable-chair, bed-television and sofa-television are captured by the model.\nFigure 11: Scenes generated by a single transformer vs.\nmultiple transformer models.\nSpatial Accuracy.We compute the relation accuracy as the\nfraction of generated relations that were present in the input\nscene description, shown in Tab. 5. We compare with the\nGT baseline where the probability of every pair of objects\nbeing related is computed over the training set.\nModel Bedroom Living Room\nGT 10.67 7.35\nOurs (GloVe) 35.41 32.65\nOurs (ELMo) 39.74 37.50\nOurs (BERT) 46.42 38.46\nTable 5: Relation accuracy of text-conditioned scene gener-\nation (percentage).\nPerceptual Study.We perform a perceptual study, showing\n40 users 34 pairs of text and images each, asking them to\nanswer two questions for each pair: how realistic the gener-\nated scene is, and how closely the generated scene matches\nthe input text, responding on a scale of 1 (poor) to 7 (good).\nOur bedroom scenes obtained a realism score of4.61±1.84,\nand a match score of 4.38 ±1.73, showing that our text-\nconditional model generates realistic scenes while capturing\nthe objects and relations mentioned in the text description.\n5. Conclusion and Future Work\nWe presented SceneFormer, which leverages a combi-\nnation of transformer models to generates realistic indoor\nscenes. SceneFormer enables ﬂexible learning from data,\nimplicitly learning object relations, and performing fast in-\nference. This can enable interactive scene generation from\npartial scenes. Our model can serve as a general frame-\nwork for scene generation: a different task can be solved\nby changing the set of object properties or conditioning in-\nputs. Future work can consider the 3D mesh of each object\nto obtain global style consistency. Our model could also\nbe used for 3D reconstruction of an indoor scene given its\n3D scan as a prior. Since we perform room layout and text\nconditioning separately, it would be interesting to see scene\ngeneration jointly conditioned on both inputs.\n6. Acknowledgement\nThis work was supported by a TUM-IAS Rudolf\nM¨oßbauer Fellowship, the ERC Starting Grant Scan2CAD\n(804724), and the German Research Foundation (DFG)\nGrant Making Machine Learning on Static and Dynamic 3D\nData Practical.\nReferences\n[1] Alan Akbik, Duncan Blythe, and Roland V ollgraf. Contex-\ntual string embeddings for sequence labeling. In COLING\n2018, 27th International Conference on Computational Lin-\nguistics, pages 1638–1649, 2018. 5\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Andrea Vedaldi,\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm, edi-\ntors, Computer Vision – ECCV 2020, pages 213–229, Cham,\n2020. Springer International Publishing. 5\n[3] Angel Chang, Will Monroe, Manolis Savva, Christopher\nPotts, and Christopher D. Manning. Text to 3D scene gener-\nation with rich lexical grounding. In Proceedings of the 53rd\nAnnual Meeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers) , pages\n53–62, Beijing, China, July 2015. Association for Computa-\ntional Linguistics. 2\n[4] Angel Chang, Manolis Savva, and Christopher D Manning.\nInteractive learning of spatial knowledge for text to 3d scene\ngeneration. In Proceedings of the Workshop on Interactive\nLanguage Learning, Visualization, and Interfaces, pages 14–\n21, 2014. 2\n[5] Angel Chang, Manolis Savva, and Christopher D Manning.\nLearning spatial knowledge for text to 3d scene generation.\nIn Proceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), pages 2028–\n2038, 2014. 2\n[6] Angel Chang, Manolis Savva, and Christopher D Manning.\nSemantic parsing for text to 3d scene generation. InProceed-\nings of the ACL 2014 Workshop on Semantic Parsing, pages\n17–21, 2014. 2\n[7] Angel X Chang, Mihail Eric, Manolis Savva, and Christo-\npher D Manning. Sceneseer: 3d scene design with natural\nlanguage. arXiv preprint arXiv:1703.00050, 2017. 2\n[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al. Shapenet:\nAn information-rich 3d model repository. arXiv preprint\narXiv:1512.03012, 2015. 2\n[9] Qi Chen, Qi Wu, Rui Tang, Yuhan Wang, Shuai Wang, and\nMingkui Tan. Intelligent home 3d: Automatic 3d-house de-\nsign from linguistic descriptions only. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12625–12634, 2020. 2\n[10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 4\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, Minneapolis, Minnesota, June 2019. Associa-\ntion for Computational Linguistics. 5\n[12] Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas\nFunkhouser, and Pat Hanrahan. Example-based synthesis\nof 3d object arrangements. ACM Transactions on Graphics\n(TOG), 31(6):1–11, 2012. 2\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 2, 4\n[14] Paul Henderson, Kartic Subr, and Vittorio Ferrari. Automatic\ngeneration of constrained furniture layouts. arXiv preprint\narXiv:1711.10939, 2017. 2\n[15] Z Sadeghipour Kermani, Zicheng Liao, Ping Tan, and H\nZhang. Learning 3d scene synthesis from annotated rgb-d\nimages. In Computer Graphics Forum , volume 35, pages\n197–206. Wiley Online Library, 2016. 2\n[16] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. Ctrl: A condi-\ntional transformer language model for controllable genera-\ntion. arXiv preprint arXiv:1909.05858, 2019. 5\n[17] Thomas N. Kipf and Max Welling. Semi-supervised classi-\nﬁcation with graph convolutional networks. In International\nConference on Learning Representations (ICLR), 2017. 2\n[18] Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri,\nOwais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen,\nDaniel Cohen-Or, and Hao Zhang. Grains: Generative re-\ncursive autoencoders for indoor scenes. ACM Transactions\non Graphics (TOG), 38(2):1–16, 2019. 2\n[19] Yuan Liang, Song-Hai Zhang, and Ralph Robert Martin. Au-\ntomatic data-driven room design generation. InInternational\nWorkshop on Next Generation Computer Animation Tech-\nniques, pages 133–148. Springer, 2017. 2\n[20] Andrew Luo, Zhoutong Zhang, Jiajun Wu, and Joshua B\nTenenbaum. End-to-end optimization of scene layout. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3754–3763, 2020. 2\n[21] Rui Ma, Akshay Gadi Patil, Matthew Fisher, Manyi Li,\nS¨oren Pirk, Binh-Son Hua, Sai-Kit Yeung, Xin Tong,\nLeonidas Guibas, and Hao Zhang. Language-driven synthe-\nsis of 3d scenes from scene databases. ACM Transactions on\nGraphics (TOG), 37(6):1–16, 2018. 2\n[22] Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala,\nand Vladlen Koltun. Interactive furniture layout using in-\nterior design guidelines. ACM transactions on graphics\n(TOG), 30(4):1–10, 2011. 2\n[23] Charlie Nash, Yaroslav Ganin, SM Eslami, and Peter W\nBattaglia. Polygen: An autoregressive generative model of\n3d meshes. arXiv preprint arXiv:2002.10880, 2020. 2\n[24] Jeffrey Pennington, Richard Socher, and Christopher D Man-\nning. Glove: Global vectors for word representation. In\nProceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP) , pages 1532–1543,\n2014. 5\n[25] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representations. In Proceedings\nof the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages 2227–\n2237, New Orleans, Louisiana, June 2018. Association for\nComputational Linguistics. 5\n[26] Pulak Purkait, Christopher Zach, and Ian Reid. Learn-\ning to generate new indoor scenes. arXiv preprint\narXiv:1912.04554, 2019. 2\n[27] Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and ﬂexi-\nble indoor scene synthesis via deep convolutional generative\nmodels. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 6182–6190,\n2019. 2, 5, 6, 7\n[28] Manolis Savva, Angel X Chang, and Maneesh Agrawala.\nScenesuggest: Context-driven 3d scene design. arXiv\npreprint arXiv:1703.00061, 2017. 2\n[29] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-\nlis Savva, and Thomas Funkhouser. Semantic scene com-\npletion from a single depth image. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1746–1754, 2017. 2, 5, 6\n[30] Fuwen Tan, Song Feng, and Vicente Ordonez. Text2scene:\nGenerating compositional scenes from textual descriptions.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6710–6719, 2019. 2\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 2,\n3\n[32] Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, An-\ngel X Chang, and Daniel Ritchie. Planit: Planning and in-\nstantiating indoor scenes with relation graph and spatial prior\nnetworks. ACM Transactions on Graphics (TOG), 38(4):1–\n15, 2019. 1, 2, 5, 6, 7\n[33] Kai Wang, Manolis Savva, Angel X Chang, and Daniel\nRitchie. Deep convolutional priors for indoor scene syn-\nthesis. ACM Transactions on Graphics (TOG), 37(4):1–14,\n2018. 1, 2, 5, 7\n[34] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d\nshapenets: A deep representation for volumetric shapes. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1912–1920, 2015. 2\n[35] Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo,\nAlexander Huth, Etienne V ouga, and Qixing Huang. Deep\ngenerative modeling for scene synthesis via hybrid represen-\ntations. ACM Transactions on Graphics (TOG), 39(2):1–21,\n2020. 1, 2\n[36] Yang Zhou, Zachary While, and Evangelos Kalogerakis.\nScenegraphnet: Neural message passing for 3d indoor scene\naugmentation. In Proceedings of the IEEE International\nConference on Computer Vision, pages 7384–7392, 2019. 2\nAppendix\nA. Model Details\nFurther details on our model are given below.\nTransformers We use these common hyperparameters\nacross all 4 models, unless otherwise speciﬁed.\n• Maximum number of objects in the scene: 50\n• Embedding dimension: Dimension of input and output\nof the transformer, 256 (1024 in location model)\n• Dimension of transformer activations: 256 (1024 loca-\ntion model)\n• Number of transformer attention heads: 8\n• Number of transformer blocks: 8\nResNet for Shape ConditioningWe use a ResNet with\n• 1 input channel\n• 3 blocks, having 3, 3, and 4 layers respectively\n• output dimension 256\nB. Shape-conditioned Model\nB.1. Results\nAdditional bedroom and living room scenes generated\nby our shape-conditioned model are shown in Fig. 12, along\nwith the input room shape. Our scenes are compared\nwith those from DeepSynth, conditioned on the same room\nshapes in Fig. 13.\nB.2. Object Category Heatmaps\nAdditional heatmaps comparing our generated scenes\nwith those from the ground truth are shown in Fig. 14. Our\nmodel captures the GT distribution well, while adding novel\nrelations in some cases. This is crucical for generating di-\nverse scenes and avoiding overﬁtting.\nB.3. Survey Interface\nThe web interface used to conduct the survey on user\npreference between our scenes, and scenes from Deep-\nSynth/PlanIT/FastSynth or from the dataset, is shown in\nFig. 15.\nC. Text-conditioned Model\nC.1. Scene Description Generation\nWe describe the method used to generate scene descrip-\ntions.\nRelation Generation First, we generate a set of relations\nbetween objects in the scene. Given a sequence of objects\n{o1,o2,... }, relations rij are generated such that i < j.\nBased on the bounding boxes of oi and oj, the relation type\nis classiﬁed as one of these -on, above, surrounding, inside,\nright of, left of, behind, in front of . At the same time, the\ndistance between the bounding box centers is computed.\nDescription Generation The ﬁrst 2 or 3 objects in the\nsequence are described in the ﬁrst sentence, this choice is\nmade uniformly. We use a ﬁxed set of starting phrases for\nthe sentence – The room has, In the room there are, The\nroom contains, This room has, There are, In the room we\nsee, and then add a list of the ﬁrst 2 or 3 objects. Repeated\nobjects are mentioned as the count followed by the object\ncategory.\nThe remaining sentences describe objects in relation to\nany object that has already been described. We iterate\nthrough objects that have not been described, and choose\nto describe any of them with a probability of 0.7. Then,\nwe pick one of the relations of this object to an object that\nhas already been described, such that the distance between\nthem is less than the threshold of 2.5 meters. This choice of\nrelation is done uniformly.\nSimilar to the earlier procedure, ordinal preﬁxes such as\nsecond and third are added when the new object is not the\nﬁrst in its category to be described. In addition, objects of\nthe same category are not related together; we reject sen-\ntences such as “There is a second table next to the ﬁrst ta-\nble”.\nNext, we choose the appropriate article for each object\n(a, an, the) , and choose a sentence template based on the\nrelation between the objects, such as – There is a table next\nto the chair , There is a sofa to the right of the wardrobe\ncabinet. These sentences are appended together to form the\nﬁnal scene description.\nC.2. Results\nAdditional text-conditioned scenes are shown in Fig. 16.\nSince the room shape is not given as input, objects are\nplaced within a room-shape prior learnt from the ground\ntruth scenes.\nC.3. Survey Interface\nThe web interface used to conduct the survey of gener-\nated scene quality is shown in Fig. 17 and Fig. 18. The\nintroduction explains the task to the user, and each scene\nhas 2 questions, with answers in the range of 1–7.\nFigure 12: Bedrooms and living room scenes generated by our method. In each pair, the left image is the input with the shape\nof the room and locations of doors and windows. The right image is the generated scene. All objects are generated within\nthe boundaries of the room, such that they do not overlap doors and windows. In addition, meaningful relations between\ndifferent object categories are learnt by the model.\nFigure 13: Bedroom and living room scenes generated by DeepSynth and our method, conditioned on the same room. Our\nscenes have more complexity and completeness in terms of object placement.\nFigure 14: Heatmap of object pair locations in ground truth and generated scenes. Top: pairs of object categories in bedroom\nscenes, bottom: living room scenes.\nFigure 15: A representative survey question from the survey for shape-conditioned images. Each question contains a pair of\nimages. One is from our method, the other is from the dataset or generated by DeepSynth. The user has to choose the one\nwhich looks more realistic. Here we use the top down view of the scene as done in earlier works.\n(a) The room has an ottoman and a wardrobe\ncabinet.\n(b) In the room there are a sofa chair, a\nwardrobe cabinet and a double bed. The\ndouble bed is behind the sofa chair.\n(c) The room has a shelving and two wardrobe\ncabinets.\n(d) This room has a wardrobe cabinet and a\ndouble bed. There is a desk to the left of the\ndouble bed.\n(e) The room has two stands and a double bed.\nThe double bed is to the right of the second\nstand. There is a dresser to the right of the\ndouble bed.\n(f) In the room we see a wardrobe cabinet, a\ndouble bed and a television. There is a dresser\nto the left of the television.\nFigure 16: Text-conditioned bedroom scenes shown with the corresponding text inputs. Important relations such as table-\nchair, bed-television and sofa-television are captured by the model.\nFigure 17: Introduction to the perceptual study of text-conditional samples\nFigure 18: A representative question from the perceptual study on text-conditional samples",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7736493349075317
    },
    {
      "name": "Transformer",
      "score": 0.7045817375183105
    },
    {
      "name": "Bedroom",
      "score": 0.6399281024932861
    },
    {
      "name": "Living room",
      "score": 0.6078097820281982
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5837809443473816
    },
    {
      "name": "Computer vision",
      "score": 0.5821317434310913
    },
    {
      "name": "Object (grammar)",
      "score": 0.45386600494384766
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.3360505700111389
    },
    {
      "name": "Architectural engineering",
      "score": 0.11174684762954712
    },
    {
      "name": "Geography",
      "score": 0.09460633993148804
    },
    {
      "name": "Engineering",
      "score": 0.08543896675109863
    },
    {
      "name": "Electrical engineering",
      "score": 0.06306740641593933
    },
    {
      "name": "Voltage",
      "score": 0.06242212653160095
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": []
}