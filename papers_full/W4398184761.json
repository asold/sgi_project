{
    "title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
    "url": "https://openalex.org/W4398184761",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2782587677",
            "name": "Stefan Dernbach",
            "affiliations": [
                "Pacific Northwest National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2150169738",
            "name": "Khushbu Agarwal",
            "affiliations": [
                "Pacific Northwest National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2183642834",
            "name": "Alejandro Zuniga",
            "affiliations": [
                "Pacific Northwest National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A1296545733",
            "name": "Michael Henry",
            "affiliations": [
                "Pacific Northwest National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2153547168",
            "name": "Sutanay Choudhury",
            "affiliations": [
                "Pacific Northwest National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2782587677",
            "name": "Stefan Dernbach",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2150169738",
            "name": "Khushbu Agarwal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2183642834",
            "name": "Alejandro Zuniga",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1296545733",
            "name": "Michael Henry",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2153547168",
            "name": "Sutanay Choudhury",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2159583324",
        "https://openalex.org/W3034862985",
        "https://openalex.org/W4307003748",
        "https://openalex.org/W4226079124",
        "https://openalex.org/W4389519864",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4387559176",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4389520747",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4389421988",
        "https://openalex.org/W4387800694",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4386977995",
        "https://openalex.org/W4380993239",
        "https://openalex.org/W4389520779",
        "https://openalex.org/W4387158126",
        "https://openalex.org/W4293718192",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W4384643740",
        "https://openalex.org/W4384918448"
    ],
    "abstract": "Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no – such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the models’ capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.",
    "full_text": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph\nAlignment via Neighborhood Partitioning and Generative Subgraph Encoding\nStefan Dernbach*, Khushbu Agarwal*,\nAlejandro Zuniga, Michael Henry, Sutanay Choudhury\nPacific Northwest National Lab\n902 Battelle Boulevard\nRichland, Washington 99354, USA\nAbstract\nIntegrating large language models (LLMs) with knowledge\ngraphs derived from domain-specific data represents an im-\nportant advancement towards more powerful and factual rea-\nsoning. As these models grow more capable, it is crucial to\nenable them to perform multi-step inferences over real-world\nknowledge graphs while minimizing hallucination. While\nlarge language models excel at conversation and text gener-\nation, their ability to reason over domain-specialized graphs\nof interconnected entities remains limited. For example, can\nwe query a LLM to identify the optimal contact in a profes-\nsional network for a specific goal, based on relationships and\nattributes in a private database? The answer is no – such ca-\npabilities lie beyond current methods. However, this question\nunderscores a critical technical gap that must be addressed.\nMany high-value applications in areas such as science, secu-\nrity, and e-commerce rely on proprietary knowledge graphs\nencoding unique structures, relationships, and logical con-\nstraints. We introduce a fine-tuning framework for developing\nGraph-aligned LAnguage Models (GLAM) that transforms a\nknowledge graph into an alternate text representation with la-\nbeled question-answer pairs. We demonstrate that grounding\nthe models in specific graph-based knowledge expands the\nmodels’ capacity for structure-based reasoning. Our method-\nology leverages the large-language model’s generative capa-\nbilities to create the dataset and proposes an efficient alternate\nto retrieval-augmented generation styled methods.\nIntroduction\nLarge language models (LLMs) have recently demonstrated\ndisruptive potential with their ability to generate text and an-\nswer questions with human-like language proficiency. How-\never, their reasoning remains limited by a reliance solely\non textual training data, lacking integration with structured\nknowledge graphs encoding intricate real-world constraints\nand relationships. Bridging this divide by aligning LLMs\nwith multi-relational graphs can enable grounded, factual\ninferences vital for applications driven by graph-structured\ndata.\nPast work on LLM-graph integration has predominantly\nfocused on harnessing LLM knowledge to improve graph\n*These authors contributed equally.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nneural network performance on tasks like node classifica-\ntion and link prediction (Jin et al. 2023). The alternate di-\nrection of augmenting or “fine-tuning” LLMs to reason over\ngraphs has remained relatively unexplored. For instance, ex-\nisting techniques still treat knowledge bases as external re-\ntrievable stores (Lewis et al. 2020), rather than integrating\nthem into model parameters. Using the LLM as an encoder\nto transform text-based node and edge labels in a graph, and\nthen fusing the LLM and GNN-derived representations has\nbeen the dominant approach for diverse applications rang-\ning from product recommendation (Choudhary et al. 2022)\nto biomedical question-answering in a multiple-choice set-\nting (Yasunaga et al. 2022).\nOur work is the first study on incorporating domain-\nspecific knowledge graphs directly into LLM represen-\ntations via fine-tuning, targeting accuracy improvements\non open ended question answering(QA), a more complex\ntask than the multiple choice setting explored in previous\nworks. By encoding both schema and entities within spe-\ncialized graphs like those in biomedical repositories, rec-\nommendation systems and social networks, we can enhance\nmulti-hop reasoning grounded by real-world constraints.\nThis addresses the challenge of factual hallucinations in\nfree-form reasoning, while retaining versatile text handling\nstrengths (Touvron et al. 2023; Nori et al. 2023).\nProblem Definition\nOur work targets natural language question answering (QA)\non graph data. We define fLLM : V → Vas a functional\nrepresentation of a large language model that accepts a se-\nquence of high-dimensional discrete tokens from a vocab-\nulary V as input and produces an output sequence drawn\nfrom the same space. Given a natural language question Q\n(also referred to as a prompt), fLLM (·) tokenizes Q into\na sequence of tokens from V and returns an answer A =\nfLLM (·)(Q).\nNext, we introduce a graph dataset G = (V, E), where V\nis the set of vertices and E is the set of edges. Importantly,\nwe assume that G was not included in the training data for\nfLLM (·). Figure 1 describes real-world use cases that moti-\nvate these graph QA workloads, such as social or profes-\nsional network-based recommendations or patient-specific\nclinical hypothesis generation.\nOur goal is to introduce a new function fGLM that uti-\nAAAI Spring Symposium Series (SSS-24)\n82\nFigure 1: Motivating examples for aligning foundational models with domain-specific knowledge graphs. The left figure demon-\nstrates a query where a LLM needs to be integrated with a knowledge graph derived from a social network. The right figure\ndemonstrates a need where a LLM needs to be integrated with a patient-profiles to disease network extracted from an electronic\nhealthcare records database.\nlizes information from G to answer question Q. Formally,\nA = fGLM (G, Q). In this paper, we systematically explore\nfollowing three query classes, in both open ended question\nanswering and multiple choice setting:\n1. Fact recall: Evaluates GLM’s ability to recall domain\nfacts seen during training (e.g. answering ”What are pos-\nsible treatments for diabetes?” after seeing ”Diabetes is\ntreated with insulin and metformin”).\n2. Inverse fact recall: Assesses handling of relationship\ndirectionality, which recent work shows standard LMs\nstruggle with (”A is B” does not imply ”B is A”)\n(Berglund et al. 2023). This is a key facet of graphs not\npreviously explored for LLM-graph models.\n3. Chain-of-Reasoning: Complex queries such as Figure\n1 (left) that necessitate appropriately employing graph\nstructure knowledge.\nTechnical Approach and Related Work\nExploring the intersection of large language models and\nknowledge graphs has strong significant interest over the\npast few years. We begin by outlining key design paradigms\nfrom literature for answering complex reasoning queries on\nknowledge graphs by aligning graph data with large lan-\nguage models (LLMs) and refer the reader to a collection\nof excellent survey articles for a detailed overview of this\nemerging sub-field (Pan et al. 2023; Liu et al. 2023; Jin et al.\n2023). Any approach must address two questions: 1) how to\nencode graph G into the LLM’s knowledge representation,\nand 2) how query Q is executed.\nDelegation to a GNN A common approach uses a graph\nneural network as encoder. Given a natural language query\nQ this requires extracting entities and relations from the\nquery and integrate GNN and LLM representations. Such\nintegration can be done via learning a joint-model coupling\nthe LLM and GNN representations (Saxena, Tripathi, and\nTalukdar 2020; Yasunaga et al. 2022) or using a soft prompt-\ning approach that inserts a GNN-derived vector embedding\ninto the LLM prompt (Tian et al. 2023).\nRetrieval Augmented Generation Retrieval augmented\ngeneration (RAG) approaches follow a similar path of im-\nplementation. The difference here is that instead of dele-\ngating to a GNN, an external graph database (Jiang et al.\n2023) or a vector database (Tian et al. 2023) containing node\nand/or relation embeddings is queried . In both approaches,\nthe LLM is used as an routing interface to a native graph\ndatabase or machine-learning model, and the answers from\nthe appropriate graph-based component is fed back to user\nwith LLM serving as generative layer that produces the final\nanswer.\nFew-Shot PromptingIn this approach subgraphs relevant\nto Q are extracted and inserted into the prompt with exam-\nples (Fatemi, Halcrow, and Perozzi 2023). While promising,\nthis approach faces potential drawbacks requiring encoding\nfull graphs in LLM prompts or performing multi-hop sub-\ngraph retrievals for each question.\nMotivation for Fine-Tuning Irrespective of their dif-\nferences, all of the above approaches potentially face a\nfundamental limitation - they cannot contextually integrate\nsymbolic constraints to shape intermediate reasoning. Ap-\nproaches retrieving embeddings or triples solely based on\nthe initial query overlook multifaceted execution where dy-\nnamic routing with mixture-of-experts (Shazeer et al. 2017;\nZhou et al. 2022), planning (Hao et al. 2023; Yao et al.\n2023) and heuristics search (Sprueill et al. 2023; Sun et al.\n2023) steps modify information needs in every reasoning\n83\nstep. Fixed retrieval precludes dynamically folding graph\nstructure into each decision point.\nIn contrast, fine-tuning instills domain knowledge into\nmodel parameters and representations a priori, rather than\ntreating the graph as an external add-on. By encoding con-\nstraints and dependencies directly into the knowledge sub-\nstrate, fine-tuning allows contextual graph influence at each\nstep of modeled cognition. Rather than acting as a static\nlook-up, the graph becomes an integral inference component\n- shaping complex reasoning in a tighter, more fine-grained\nfashion.\nOur Approach and Contributions We introduce an algo-\nrithm to iteratively partition and encode the neighborhood\nsubgraph around each node into textual sentences for fine-\ntuning data. This transforms graph structure into a format\nthat large language models can ingest and fine-tune. We\nexplore encoding strategies on two graphs: 1) UMLS - a\nbiomedical knowledge base, and 2) DBLP - an academic\npublication network.\nOur work makes the following contributions.\n1. Our neighborhood partitioning and encoding scheme ac-\ncommodate real-world graph properties like skewed size\ndistributions and sparsity. Our approach opens up fu-\nture experimental possibilities where encoding is tuned\nfor LLMs by setting context size limits based on cost-\naccuracy tradeoffs.\n2. We propose and assess five encoding approaches lever-\naging the LLM’s innate summarization and text gen-\neration strengths. For example, we evaluate neighbor-\nhood summaries produced by the LLM. Encouragingly,\nour results align with similar methods from concurrent\nwork (Fatemi, Halcrow, and Perozzi 2023), confirming\nthe promise of this direction.\n3. We developed a new domain question answering dataset\nbased on two above graphs with a suite of evaluation\ntasks capturing link prediction to multi-hop reasoning\nqueries. The code and datasets will be released as open\nsource upon acceptance.\nMethods\nTask Definition We propose methods for transforming\na knowledge graph into a corresponding text-based fine-\ntuning dataset for language models. Our goal is to produce\npairs of (context, question-answer) (Ouyang et al. 2022; Wei\net al. 2021) that require neural-graph reasoning to answer\nopen domain questions involving relational or multi-hop rea-\nsoning.\nWe begin with describing a generic algorithm (Algorithm\n1) that encodes a node’s k-hop neighborhood into such a\ncontext and QA-pair through a composition of multiple op-\nerator functions. We discuss the implementation of these op-\nerators in finer detail in the later half of the section.\nOptimal Generation of Subgraph Contexts\nFor every node v ∈ V (G), we transform the k-hop\nneighborhood of v into a set of pairs of the form:\nAlgorithm 1: Fine-tuning dataset generation.\nRequire: Graph G with nodes set V and edges set E,\ncontext subgraph node limit Nmax\nFine-tuning dataset D ← ∅\nfor each v ∈ V (G) do\nGcontext(v, k) =faggr (G, v, k)\npartitions = fpartition(Gcontext(v, k), Nmax)\nfor each gsub ∈ partitions do\nXcontext = fenc(gsub)\nXqa = fqa(gsub)\nappend(D, concat([Xcontext, Xqa])\nend\nend\nreturn D\n(fenc(G, v), fqa(G, v)). Algorithm 1 describes a step-by-\nstep in which we iterate over every node in the graph\nand encode it’s k-hop neighborhood subgraph, denoted as\nGcontext(v, k) into the alternate text-based representation.\n1. We retrieve the k-hop neighborhood subgraph as\nGcontext(v, k) using a query function denoted as\nfaggr (·).\n2. fenc encodes Gcontext(v, k) or its partitioned subgraph\ninto text.\n3. fqa(G, v) generates QA pairs requiring reasoning on\nGcontext(v, k). Same subgraph is used to drive the inputs\nfor fenc(G, v) and fqa(G, v).\n4. The concatenated output of fenc(G, v) and fqa(G, v) is\na text sequence of discrete tokens Xv drawn from V, the\nvocabulary of the LLM functionfLLM (·) mentioned pre-\nviously.\n5. Any LLM function fLLM (·) needs to operate within\na maximum token limit constraint (denoted as Tmax).\nWe partition Gcontext(v, k) to respect LLM token limits\nTmax such that len(Xv) < Tmax.\nWe introduce a hyperparameter Nmax to partition\nGcontext(v, k) into subgraphs within node count Nmax.\nThis prevents tokenized sequence lengths from exceeding\nTmax. Choosing an optimal Nmax is key because degree\ndistributions in Gcontext(v, k) can be highly skewed. Given\ncost constraints associated with Tmax, we want to pick\nNmax and encoding strategies that maximize context lengths\nfor the LLM’s capabilities.\nNeighborhood Encoding Functions\nThe purpose of a neighborhood encoding function is to\ntranslate the neighborhood subgraph Gcontext(v, k) cen-\ntered around a node v into a textual representation that can\nbe effectively processed by a large language model (LLM).\nThis process is crucial for enabling the LLM to perform\nhigher-order reasoning and answer complex questions about\nthe graph.\nThere are two main factors that influence the choice of a\nneighborhood encoding function:\n84\nFigure 2: Illustration of Graph Encodings in GLaM: Top left box shows ”Encoding via triples”, where each line represents an\nedge mapped to one training sample. The bottom left box shows graph encoding when given a node and relation, all relevant\nentities are collated into single training sample. The bottom right box shows when all relations/edges corresponding to a node\nare coalesced into single training sample. and top right box demonstrates the impact of summarization on the training sample.\nSummarizing helps to 1) map unwieldy node labels into human interpretable form, 2) reduce redundant terms, and 3) reduce\noverfitting to frequent node and edge labels. Collectively this leads to better semantic alignment betweeen the knowledge graph\nand LLM’s vocabulary and improves resulting model performance in all graph tasks.\n1. Communicating Graph Structure and Higher-Order\nReasoning Requirements to the LLM: The encoding\nfunction should effectively capture the structural rela-\ntionships between nodes in the subgraph, as well as any\nhigher-order logical dependencies that may exist. This\ncan be achieved by incorporating information about the\nedges and their types, as well as the relationships be-\ntween multiple nodes.\n2. Semantic Alignment with the LLM’s Internal Knowl-\nedge Representation: The encoding should represent the\nnodes and relations in the graph in a way that is consis-\ntent with how the LLM stores and interprets information.\nThis can involve using natural language labels for nodes\nand edges, or generating descriptive labels using a node’s\nneighborhood when node labels are not recognizable to a\nLLM (such as an academic network), while ensuring that\nthe encoded representation preserves the semantic mean-\ning of the graph elements.\nEncoding via Triples: A simple approach to neighbor-\nhood encoding is to translate the edge data into (source, re-\nlation, target) triples. This provides the LLM with basic in-\nformation about the relationships between nodes, but it is\nlimited to representing only single edges per training sam-\nple and has limited context size.\nEncoding via Adjacency List/Relational GroupsTo en-\nable the LLM to perform more complex reasoning tasks, we\nupdate the neighborhood encoding to include information\nabout multiple nodes in the subgraph. We experiment with\ntwo different options: including the entire adjacency list of\nthe central node v, and by partitioning the neighbors into\nsubsets based on their relation types. We observe that more\nsophisticated approaches, such as sampling techniques are\nrelevant for large neighbor lists but are not implemented in\ncurrent work.\nEncoding via Summarization Next, we focus on the se-\nmantic alignment objective and use prompting methods to\nrewrite the encoding from above methods into more coher-\nent representations (Figure 2).\n• The promptimg allows us to map unwieldy node labels to\nhuman understandable terms: For example,“Insulin hu-\nman, rDNA origin” is mapped by LLM to “Insulin ther-\napy from recombinant DNA” allowing for better inter-\npretation during fine-tuning.\n• It reduces redundant text from similarly labeled nodes:\n“Diabetes mellitus, Type 1 diabetes mellitus, Type 2 dia-\nbetes mellitus” is mapped to “diabetes, including Type 1\nand Type 2 diabetes.”\n• Introduces additional knowledge/synonyms into training:\n“Hypoinsulinaemia” is mapped to “low insulin levels\n(hypoinsulinaemia),” and “rDNA” is expanded to “re-\ncombinant DNA.”\n• Prompt-based rewriting also reduces overfitting to only a\nfew relation labels by mapping them to different phrases.\nExamples of such overfitting was observed with the “may\ntreat” relationship, where the high number of occurrence\nof this phrase in a specific pattern causes the LLM to\ngenerate answers incorrectly filled with too many occur-\nrences of the “may treat” phrase.\n85\nEncoding via Node Descriptors The previous encoding\nstep leveraged the LLM’s understanding of specific entities\n(such as “rDNA”) to rewrite with maximal semantic align-\nment. However, training on new graph data can include un-\nfamiliar terms to the LLM, i.e. words or phrases that appear\nrarely or do not occur at all in initial training. A common ex-\nample of this problem involves encoding the names of peo-\nple not common to standard LLM training datasets. Also,\nwe do not want to map a person based on their name, but\naccount for their profile attributes or k-hop connectivity in\nthe network. We generalize this need by transforming the\nk-hop context subgraph (G context(v, k)) into a set of text-\nbased node descriptors by leveraging the LLM’s zero-shot\ncapabilities. Typically, this is a step where an alternate im-\nplementation would have retrieved a GNN representation.\nFor example, to expand on the information about authors in\nthe DBLP dataset, we prompt the LLM to extract the topic\nareas of paper abstracts and construct a list of topics the au-\nthor has published on from their paper history.\nGenerating Question-Answer Pairs Finally, given a text\ncontext generated from a subgraph Gcontext(v, k), we gen-\nerate a set of question-answer pairs via prompting the text\ncontext for different tasks (fact recall, inverse fact recall,\nmulti-hop question answering). Each of the questions are\nalso mapped into two style of answers: 1) open-domain\nquestion-answering, and 2) multiple choice questions. For\nexample, given a (head, relation, tail) triple as the subgraph\ncontext, its multiple choice answer candidates are generated\nby including one of the tail entities and a random selection\nof other nodes in the graph to form a set of possible answers\nto the question.\nExperiments\nIn this section, we address following research questions\n(RQ) through experimental analysis:\n1. RQ-1 Does finetuning using graph encoding improve an\nLLM’s ability to recall the facts?\n2. RQ-2 Does finetuning an LLM with graph encoding im-\nprove its ability to answer open-domain natural language\nquestions through performing multi-hop reasoning on the\ngraph domain?\n3. RQ-3 Which strategies for encoding the subgraph con-\ntext yields maximal semantic alignment of the original\nLLM and the target graph?\nDatasets\nWe present the results of training GLaMs on two graph\ndatasets, DBLP (Tang et al. 2008) and UMLS (Bodenrei-\nder 2004), with diverse applications and coverage in LLMs\nto demonstrate the response improvement over the baseline\nlanguage models.\nUnified Medical Language System 1 (UMLS) (Boden-\nreider 2004) is a medical knowledge graph. We use a pro-\ncessed version of the knowledge graph from Yasunaga et\nal. (Yasunaga et al. 2022) consisting of 297, 927 concepts,\n1https://www.nlm.nih.gov/research/umls\n98 relation types and 1, 212, 586 edges that capture relation-\nships across a breadth of medical concepts. For GLaM train-\ning, we select a subgraph that captures relationships between\ndifferent diseases, symptoms and medications. This results\nin a reduction to 4 different relation types: “cause of”, “may\ncause”, “risk factor of”, and “may treat” totalling 126, 149\ntriples.\nDBLP2 (Tang et al. 2008) is a citation graph dataset ex-\ntracted from DBLP, ACM, MAG, and other sources. The\ndataset includes paper citations, abstracts, authors, publica-\ntion years, venues, and titles. For training the GLaM we fo-\ncus on the set papers containing titles, abstracts, venues, and\n2 or more authors, leading to 19, 577 unique papers.\nTraining and Inference Setup\nFor both UMLS and DBLP, the extracted natural language\nquestions and answers were split into 70% training (fact\nrecall) and 30% test (multi-hop reasoning). We used Mi-\ncrosoft Deepspeed framework (Rasley et al. 2020) for super-\nvised prompt and response fine-tuning. A grid-search was\nperformed over training hyper-parameters using Llama-7b-\nchat-hf as the base model for training GLaM. A learning\nrate of 1e−5 and a cosine learning rate scheduler were used\nwith the fused Adam optimizer with bfloat16 precision. The\nmaximum sequence length was set to 256 and a maximum\nof 4 training epochs were used for all models. A cluster of\n8 A100 GPUs with 80GB of GPU memory each were used\nfor training with a per-device batch size of 4 questions re-\nsulting in a total training batch size of 32. We use Llama-\n2-7b-chat-hf and Llama-2-13b-chat-hf (Touvron et al. 2023)\nmodels from Hugging Face as the baseline models for train-\ning. Training the 7b model on UMLS takes approximately 9\nminutes and 16 minutes for the 13b model. For DBLP, train-\ning time is approximately 11 and 21 minutes respectively.\nEvaluation Tasks\nFact recall: This task is equivalent to question answering\ntasks in language models and test GLaM’s ability to remem-\nber domain level facts seen during training. For example,\ngiven a training sentence such as ‘‘Diabetes is treated with\ninsulin and metformin” (from UMLS), the model is queried\nfor “What are possible treatment of diabetes?”. Similiarly,\nfor the DBLP dataset given a sentence such as “[Students\nlearn CS in different ways: insights from an empirical study]\nwas written by Anders Berglund.”, the model is queried with\n“[Students learn CS in different ways: insights from an em-\npirical study] was written by whom?” The UMLS question\nset for fact recall contains 7710 questions and the DBLP set\ncontains 13, 704.\nInverse Fact Recall: This task is equivalent to reverse\nquestion answering tasks (Berglund et al. 2023) in language\nmodels and test GLaM ’s ability to infer reverse relation-\nships from the domain level facts seen during training. For\nexample, given the above training statement, the model is\nqueries for “Which disease can be treated with insulin?”\nThere are 11130 questions in the UMLS reverse fact recall\nquestion set and 13704 in the DBLP set.\n2https://www.aminer.org/citation\n86\nFact Recall Reverse Recall Multi-hop Reasoning\nP R F1 P R F1 P R F1\nLlama 7B Chat 0.594 0. 631 0. 608 0. 382 0. 519 0. 439 0. 595 0. 631 0. 609\nGLaM (Triples) 0.683 0. 597 0. 627 0.431 0.533 0.474 0.677 0. 589 0. 621\nGLaM (Relational Grouping) 0.679 0. 678 0. 673 0. 403 0.537 0.459 0. 662 0. 663 0. 657\nGLaM (LLM Summarization) 0.724 0. 725 0. 720 0.386 0. 527 0. 445 0.689 0. 696 0. 688\nLlama 13B Chat 0.699 0. 623 0. 652 0. 396 0. 529 0. 451 0. 695 0. 623 0. 650\nGLaM 13B (LLM Summarization) 0.708 0. 730 0. 714 0. 395 0. 534 0. 453 0. 675 0. 697 0. 681\nTable 1: UMLS Results comparing the baseline Llama LLM with three versions of a refined GLaM on questions generated from\nthe UMLS knowledge graph. Each version corresponds to an encoding strategy described in the Methods section. Precision (P),\nRecall (R), and F1 scores are reported using Bert scores.\nFact Recall Reverse Recall Multi-hop Reasoning\nP R F1 P R F1 P R F1\nLlama 7B Chat 0.174 0. 177 0. 175 0. 168 0. 173 0. 170 0. 168 0. 171 0. 169\nGLaM (Triples) 0.105 0. 103 0. 104 0. 103 0. 102 0. 102 0. 100 0. 099 0. 099\nGLaM (Relational Grouping) 0.259 0. 261 0. 259 0. 259 0. 264 0. 260 0. 256 0. 259 0. 257\nGLaM (Adjacency List) 0.255 0. 258 0. 255 0. 247 0. 252 0. 249 0. 251 0. 253 0. 251\nGLaM (Node Descriptors) 0.313 0. 312 0. 312 0. 309 0. 314 0. 311 0. 318 0. 316 0. 316\nGLaM -7B 0.424 0. 426 0. 424 0. 401 0. 407 0. 402 0. 409 0. 410 0. 408\nLlama 13B Chat 0.150 0. 155 0. 152 0. 144 0. 151 0. 147 0. 153 0. 159 0. 155\nGLaM -13B 0.446 0. 446 0. 445 0. 381 0. 385 0. 382 0. 398 0. 398 0. 397\nTable 2: DBLP Results comparing the baseline Llama LLM with five versions of GLaM on questions generated from the DBLP\ncitation graph. GLaM-7B/13B represents a combination of strategies: aggregation of node descriptors, utilizing adjacency lists\nas context and performing summarization. Precision (P), Recall (R), and F1 scores are reported using Bert scores.\nUMLS DBLP\nFact Reverse Multi-hop Fact Reverse Multi-hop\nRecall Fact Recall Reasoning Recall Fact Recall Reasoning\nLlama 7B Chat 61.23 57.21 61. 1 35.26 36.19 27. 99\nGLaM -7B-MC 100 59.71 91. 93 78.62 75.68 73. 34\nTable 3: Multiple Choice Results Comparison of LLM and GLaM accuracy for fact recall, reverse fact recall, and fact inference\non the UMLS and DBLP graphs.\nMulti-hop Reasoning: This task mirrors the link predic-\ntion task in a GNN setting and tests the GLaM’s ability to\ninfer new facts (graph edges) by reasoning over facts seen\nduring training. The UMLS question set for multi-hop rea-\nsoning contains 3347 questions and the DBLP set contains\n5873. A common style of question we explore for DBLP\nis that of recommending authors to collaborate with. Using\nthe DBLP question referred to in the fact recall task as ex-\nample, a multi-hop reasoning question would ask: “Anders\nBerglund would like to write a paper titled [Students learn\nCS in different ways: insights from an empirical study] to\npublish in Proceedings of Australasian computing educa-\ntion. Who should they work with and why?”\nMultiple Choice: Each evaluation task: fact recall, in-\nverse fact recall, and multi-hop reasoning, are reformatted\nas multiple choice questions. Question includes the correct\nanswer and four additional incorrect options randomly se-\nlected from the graphs respectively. Note that this is a much\neasier task than open ended question answering setting, re-\nquiring models to only pick the most likely answer out of\ngiven options.\nEvaluation Metrics\nTo account for the inherent text variability in LLM or GLaM\ngenerated answers, we use the BERTScore metric (Zhang\net al. 2019) for open-ended domain QA setting and accuracy\nfor multiple choice questions.\nBert Score: Compares text similarity between the\nmodel generated response to the expected response. The\nmicrosoft/deberta-xlarge-mnli model (He et al. 2020) is used\nfor calculating BertScore for it’s strong performance in nat-\nural language understanding (NLU) tasks. We report preci-\nsion (P), recall (R), and F1 scores across the evaluation set.\nAccuracy: We use the standard accuracy measure to eval-\nuate a model’s ability to identify the correct answer out of 5\npossible choices in a multiple choice setting.\n87\nResults\nResults for training GLaM are presented in Table 1 and Ta-\nble 2. We discuss the results on the individual datasets and\nthen provide overall conclusions.\nUMLS graph experiment results are given in Table 1. For\nboth fact recall and inference, using LLM based summa-\nrization encoding to rewrite the statements exhibits the best\nperformance across precision, recall and F1 scores. How-\never, for reverse fact recall, using the simpler training ap-\nproaches leads to a slight improvement in scores. All fine-\ntuned GLaM versions outperform the baseline LLM show-\ning that even naive training strategies offer some improve-\nment over the baseline LLM. While the 13b version of\nLlama outperforms its 7b counterpart, once trained, there is\nnegligible difference between the 13b and 7b GLaM .\nDBLP citation graph experimental results are given in\nTable 2. The GLaM version with complete adjacency and\nLLM based summarization achieves the best results across\nall tasks. Unsurprisingly, the untrained LLM did only mod-\nerately better than random guessing for the multiple choice\ntask because of the number of unfamiliar names in the\ndataset. There is also a general trend of improved per-\nformance as neighborhood information is collated into the\ntraining, with the exception of adding the venue of the publi-\ncation not having a noticeable affect,likely due to title being\nsufficient to capture a publications context. There is a slight\nimprovement of the 13b version of GLaM over the 7b ver-\nsion for fact recall but the 7b version slightly outperforms\nthe larger GLaM on the reverse fact recall and fact inference\ntasks. This combined with similar findings on UMLS indi-\ncate that the smaller LLM is sufficient for fact retention and\ninference when fine-tuned for the domain.\nMultiple Choice results for both UMLS and DBLP are\nprovided in Table 3. Across all tasks, GLaM outperforms\nthe unrefined LLM, with the smallest difference being on\nthe reverse facts for UMLS where GLaM noticeably does\nnot learn to infer the inverse relationships from training. For\nUMLS fact recall GLaM demonstrates 100% accuracy on\nrecalling the answers to the training set and similarly per-\nforms extremely well on the multi-hop reasoning questions.\nWe hypothesize that the even larger gap between LLM and\nGLaM on the multiple choice results compared to the differ-\nence on the open ended question results comes from GLaM\nlearning to differentiate the good answers from poor ones\neven if it does not explicitly know the correct answer.\nGraph Aligned Language Models Significantly Im-\nprove Domain Knowledge Retrieval Tasks. Large lan-\nguage models are extraordinary tools for general knowledge\nbut can not produce answers to many domain specific ques-\ntions modeled in complex networks. This is evidenced by\nGLaM outperforming LLM across all domain level tasks,\nincluding simple fact retrieval questions.\nIncreasing the Node Neighborhood Context During\nTraining Improves Inference Performance. Both the\nUMLS (Table 1) and DBLP (Table 2) cases demonstrate that\nincorporating multiple edges into each training instance im-\nproves the language models recall and reasoning. This is ev-\nident as GLaM training evolves from single triple samples,\nto relations with multiple targets, and further to include ad-\nditional neighborhood information such as the topic areas an\nauthor publishes in.\nNode Context Summarization Using LLM Improves\nLearning. Using a LLM to rewrite or summarize statements\nproduced from the node neighborhood encoding improves\nGLaM’s fact recall and multi-hop reasoning as shown on\nTable 1. The LLM summarized version for the UMLS graph\nencoding outperforms other GLaM versions even if the same\ninformation is present in training. We postulate that variation\nin word choice, mapping of node labels to more interpretable\nnames helps significantly improve the learning process.\nConclusions and Future Work\nWe demonstrate an effective approach to integrate domain-\nspecific knowledge graphs into large language models via\nfine-tuning. Empirically, this technique yields significant\ngains in multi-hop reasoning ability over the base LLM. Our\nproposed fine-tuning method encodes graph structure and\nit’s semantic knowledge into the LLM, by maximally lever-\naging the original LLM’s strengths - textual understanding,\ncommonsense knowledge and generative capabilities.\nIn particular, quantitative experiments verify F1 score im-\nprovements of 18% on fact recall and 13% on complex in-\nference queries requiring multi-hop reasoning on the UMLS\ndomain for which the LLM already has some knowledge,\nand 142% and 141% respectively on DBLP’s social net-\nwork structure which represents novel information for the\nLLM. Given the importance of directionality of relation-\nships in a graph, we also measure the improvement of re-\ncalling inverse facts by the resulting model. Overall, our ex-\nperiments while preliminary in nature, confirm that integra-\ntion via fine-tuning instills more reliable reasoning capacity\nbased on graphs containing specialized entities and relation-\nships, and it enables tighter coupling of structured symbolic\nknowledge with learned representations. Evaluating the ef-\nfectiveness of the partitioning and encoding schemes across\na wider range of larger-scale graphs with highly uneven con-\nnectivity distributions are candidate for future work.\nReferences\nBerglund, L.; Tong, M.; Kaufmann, M.; Balesni, M.; Stick-\nland, A. C.; Korbak, T.; and Evans, O. 2023. The Reversal\nCurse: LLMs trained on” A is B” fail to learn” B is A”.arXiv\npreprint arXiv:2309.12288.\nBodenreider, O. 2004. The unified medical language system\n(UMLS): integrating biomedical terminology. Nucleic acids\nresearch, 32(suppl\n1): D267–D270.\nChoudhary, N.; Rao, N.; Subbian, K.; and Reddy, C. K.\n2022. Graph-based Multilingual Language Model: Lever-\naging Product Relations for Search Relevance. In Proceed-\nings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, 2789–2799.\nFatemi, B.; Halcrow, J.; and Perozzi, B. 2023. Talk like a\ngraph: Encoding graphs for large language models. arXiv\npreprint arXiv:2310.04560.\nHao, S.; Gu, Y .; Ma, H.; Hong, J. J.; Wang, Z.; Wang, D. Z.;\nand Hu, Z. 2023. Reasoning with language model is plan-\nning with world model. arXiv preprint arXiv:2305.14992.\n88\nHe, P.; Liu, X.; Gao, J.; and Chen, W. 2020. Deberta:\nDecoding-enhanced bert with disentangled attention. arXiv\npreprint arXiv:2006.03654.\nJiang, J.; Zhou, K.; Dong, Z.; Ye, K.; Zhao, W. X.; and Wen,\nJ.-R. 2023. Structgpt: A general framework for large lan-\nguage model to reason over structured data. arXiv preprint\narXiv:2305.09645.\nJin, B.; Liu, G.; Han, C.; Jiang, M.; Ji, H.; and Han, J. 2023.\nLarge Language Models on Graphs: A Comprehensive Sur-\nvey. arXiv preprint arXiv:2312.02783.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .;\nGoyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨aschel,\nT.; et al. 2020. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in Neural Infor-\nmation Processing Systems, 33: 9459–9474.\nLiu, J.; Yang, C.; Lu, Z.; Chen, J.; Li, Y .; Zhang, M.; Bai,\nT.; Fang, Y .; Sun, L.; Yu, P. S.; et al. 2023. Towards graph\nfoundation models: A survey and beyond. arXiv preprint\narXiv:2310.11829.\nNori, H.; King, N.; McKinney, S. M.; Carignan, D.; and\nHorvitz, E. 2023. Capabilities of gpt-4 on medical challenge\nproblems. arXiv preprint arXiv:2303.13375.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nPan, S.; Luo, L.; Wang, Y .; Chen, C.; Wang, J.; and Wu,\nX. 2023. Unifying Large Language Models and Knowledge\nGraphs: A Roadmap. arXiv preprint arXiv:2306.08302.\nRasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y . 2020.\nDeepspeed: System optimizations enable training deep\nlearning models with over 100 billion parameters. In Pro-\nceedings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining, 3505–3506.\nSaxena, A.; Tripathi, A.; and Talukdar, P. 2020. Improving\nmulti-hop question answering over knowledge graphs using\nknowledge base embeddings. In Proceedings of the 58th\nannual meeting of the association for computational linguis-\ntics, 4498–4507.\nShazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;\nHinton, G.; and Dean, J. 2017. Outrageously large neu-\nral networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538.\nSprueill, H. W.; Edwards, C.; Olarte, M. V .; Sanyal, U.;\nJi, H.; and Choudhury, S. 2023. Monte Carlo Thought\nSearch: Large Language Model Querying for Complex\nScientific Reasoning in Catalyst Design. arXiv preprint\narXiv:2310.14420.\nSun, J.; Xu, C.; Tang, L.; Wang, S.; Lin, C.; Gong, Y .; Shum,\nH.-Y .; and Guo, J. 2023. Think-on-graph: Deep and respon-\nsible reasoning of large language model with knowledge\ngraph. arXiv preprint arXiv:2307.07697.\nTang, J.; Zhang, J.; Yao, L.; Li, J.; Zhang, L.; and Su, Z.\n2008. ArnetMiner: Extraction and Mining of Academic So-\ncial Networks. In KDD’08, 990–998.\nTian, Y .; Song, H.; Wang, Z.; Wang, H.; Hu, Z.; Wang,\nF.; Chawla, N. V .; and Xu, P. 2023. Graph neural\nprompting with large language models. arXiv preprint\narXiv:2309.15427.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2021. Finetuned\nlanguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\nY .; and Narasimhan, K. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv preprint\narXiv:2305.10601.\nYasunaga, M.; Bosselut, A.; Ren, H.; Zhang, X.; Manning,\nC. D.; Liang, P. S.; and Leskovec, J. 2022. Deep bidirec-\ntional language-knowledge graph pretraining. Advances in\nNeural Information Processing Systems, 35: 37309–37323.\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi,\nY . 2019. Bertscore: Evaluating text generation with bert.\narXiv preprint arXiv:1904.09675.\nZhou, Y .; Lei, T.; Liu, H.; Du, N.; Huang, Y .; Zhao, V .; Dai,\nA. M.; Le, Q. V .; Laudon, J.; et al. 2022. Mixture-of-experts\nwith expert choice routing. Advances in Neural Information\nProcessing Systems, 35: 7103–7114.\n89"
}