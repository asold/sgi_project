{
  "title": "ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise â€¨Perspective for Medical Image Segmentation",
  "url": "https://openalex.org/W4285600307",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2117740219",
      "name": "Huimin Huang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3211498844",
      "name": "Shiao Xie",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2146947757",
      "name": "Lanfen LIN",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2132501694",
      "name": "Yutaro Iwamoto",
      "affiliations": [
        "Ritsumeikan University"
      ]
    },
    {
      "id": "https://openalex.org/A4202017852",
      "name": "Xian-Hua Han",
      "affiliations": [
        "Yamaguchi University"
      ]
    },
    {
      "id": "https://openalex.org/A2149509190",
      "name": "Yen Wei Chen",
      "affiliations": [
        "Ritsumeikan University"
      ]
    },
    {
      "id": "https://openalex.org/A2015213433",
      "name": "Ruofeng Tong",
      "affiliations": [
        "Zhejiang Lab",
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4225581272",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2395611524",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2804047627",
    "https://openalex.org/W2928133111",
    "https://openalex.org/W3199613405",
    "https://openalex.org/W2949846184",
    "https://openalex.org/W4285707640",
    "https://openalex.org/W2592905743",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3034907931",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3097510987",
    "https://openalex.org/W2996290406",
    "https://openalex.org/W3160284783",
    "https://openalex.org/W3017153481",
    "https://openalex.org/W4287324101"
  ],
  "abstract": "Recently, a variety of vision transformers have been developed as their capability of modeling long-range dependency. In current transformer-based backbones for medical image segmentation, convolutional layers were replaced with pure transformers, or transformers were added to the deepest encoder to learn global context. However, there are mainly two challenges in a scale-wise perspective: (1) intra-scale problem: the existing methods lacked in extracting local-global cues in each scale, which may impact the signal propagation of small objects; (2) inter-scale problem: the existing methods failed to explore distinctive information from multiple scales, which may hinder the representation learning from objects with widely variable size, shape and location. To address these limitations, we propose a novel backbone, namely ScaleFormer, with two appealing designs: (1) A scale-wise intra-scale transformer is designed to couple the CNN-based local features with the transformer-based global cues in each scale, where the row-wise and column-wise global dependencies can be extracted by a lightweight Dual-Axis MSA. (2) A simple and effective spatial-aware inter-scale transformer is designed to interact among consensual regions in multiple scales, which can highlight the cross-scale dependency and resolve the complex scale variations. Experimental results on different benchmarks demonstrate that our Scale-Former outperforms the current state-of-the-art methods. The code is publicly available at: https://github.com/ZJUGiveLab/ScaleFormer.",
  "full_text": " \n \nScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise  \nPerspective for Medical Image Segmentation \n \nHuimin Huang1, * Shiao Xie1, * Lanfen Lin1, Yutaro Iwamoto2, \nXian-Hua Han3, * Yen-Wei Chen2, and Ruofeng Tong1,4 \n1 Zhejiang University \n2 Ritsumeikan University \n3 Yamaguchi University \n4 Zhejiang Lab \n \nAbstract \nRecently, a variety of vision transformers have been \ndeveloped as their capability of modeling long -\nrange dependency. In current transformer -based \nbackbones for medical image segmentat ion, convo-\nlutional layers were replaced with pure transformers, \nor transformers were added to the deepest encoder \nto learn global context. However, there are mainly \ntwo challenges in a scale-wise perspective: (1) intra-\nscale problem: the existing methods l acked in ex-\ntracting local -global cues in each scale, which may \nimpact the signal propagation of small objects; (2) \ninter-scale problem: the existing methods failed to \nexplore distinctive information from multiple scales, \nwhich may hinder the representation  learning from \nobjects with widely variable size, shape and location. \nTo address these limitations, we propose a novel \nbackbone, namely ScaleFormer, with two appealing \ndesigns: (1) A scale -wise intra -scale transformer is \ndesigned to couple the CNN -based local features \nwith the transformer-based global cues in each scale, \nwhere the row -wise and column -wise global de-\npendencies can b e extracted by a lightweight Dual -\nAxis MSA. (2) A simple and effective spatial-aware \ninter-scale transformer is designed to interact among \nconsensual regions in multiple scales, which can \nhighlight the cross-scale dependency and resolve the \ncomplex scale variations. Experimental results on \ndifferent benchmarks demonstrate that our Scale-\nFormer outperforms the current state-of-the-art \nmethods. The code is publicly available at : \nhttps://github.com/ZJUGiveLab/ScaleFormer . \n1 Introduction \nIn the past decades, Convolutional Neural Networks (CNNs) \nhave greatly promoted the developmen t of dense prediction \ntasks [Shelhamer et al., 2015], and a variety of CNN-based \n                                                \n*\tCorresponding Authors:  Lanfen Lin (llf@zju.edu.cn), Yen-\nWei Chen (chen@is.ritsumei.ac.jp) .  \n \nFigure 1: Comparison of current transformer -based backbones ((a) \nand (b)) with our ScaleFormer (c).  \nsegmentation models have been developed  [Ronneberger et \nal., 2015; Chen et al., 2017; Huang et al., 2021]. Despite its \ngreat success, CNNs failed to model explicit long -range rela-\ntions beyond local regions (shown in Fig .2.(b) and (f)), since \nthe effective receptive field of a networkâ€™s units is severely \nlimited [Luo et al., 2016]. 1 \n    Recently, the transformer architecture has been introduced \nto visual tasks and shown to be an efficient way of modeling \nglobal context [Dosovitskiy  et al., 2020]. This largely attrib-\nutes to the multi -head self -attention (MSA) and Multilayer \nPerceptron (MLP)  structure. Fig.1. (a ) illustra tes one of the \ntransformer-based architectures for medical image segmen-\ntation, e.g. SwinUNet [ Cao et al ., 2021] and Missformer  \n[Huang et al., 2021]. They typically utilized a stack of trans-\nformer blocks to model global representations. However, \nthese pure transformer- based architectures often failed to \nachieve satisfactory performance, since lack of spatial induc-\ntive-bias in modelling local information. As shown in Fig.2(c) \nand (g), SwinUNet activated the background extensively and \nwas difficult to distinguish objects from background. Another \npopular architecture, e.g. TransUNet [Chen et al., 2021] and \nAFTer-UNet [Yan et al. , 2022], in Fig.1 (b), used a CNN -\nbased encoder to explore detailed high- resolution spatial in-\nformation, and added several transformers into the deepest \nlayer to learn global context. However, this kind of deepest  \nHuimin Huang and Shiao Xie are co-first authors. \n \nProceedings of the Thirty-First International Joint Conference on Artiï¬cial Intelligence (IJCAI-22)\n964\n \n \n  \nFigure 2: Visualization of feature maps from different layers of four methods. The CNN-based ResUNet (in (b), (f)) limits in modeling long-\nrange relations and thus fails to activate on objects beyond local regions (e.g. unactivated liver, gallbladder and pancreas). The pure trans-\nformer, SwinUNet (in (c), (g)), activates backgroun d extensively as a result it is difficult to distinguish objects from background. TransUNet \n(in (d), (h)) stack s transformers in the deepest encoder of UNet, but it still has problems accurately localizing small objects (e.g. blurred \ngallbladder and pancreas with small size). Our ScaleFormer (in (a), (e)) improves the transformer-based backbone from a scale-wise perspec-\ntive, which can recognize objects in various sizes and long -range scenarios. (e.g. long -distant large-size liver and small-size pancreas). \n \nhybrid transformer structure still led to blurred and inaccurate \nactivation on the salient objects in Fig.2 .(d) and (h). \nThe observed unsatisfactory activations of current trans-\nformer-based backbones may be imputed to  two main chal-\nlenges in a scale -wise perspective. (i) Intra-scale problem : \nAs we know, both contextual local -level details and posi-\ntional global -level cues are essential complements in each \nscale, especially for small objects, whose fine signals in the \nshallow layer (in Fig. 2(b)-(d)) may be gradually diluted dur-\ning downsampling (in Fig.2 (f)-(h)). But the integration of in-\ntra-scale local features with global representations remains a \nfundamental challenge.   (ii) Inter-scale problem : As shown \nin Fig.2, the objects in medial image often have widely vari-\nable size, shape and location.  The existing methods were in-\ncapable of extracting sufficient information from multiple \nscales generated by hierarchical encoder, which failed to no-\ntice the complete salient objects. To address these challenges, \nwe propose a novel backbone for medial image segmentation \ntask (in Fig.1. (c)), namely ScaleFormer, which is motivated \nby the following two aspects: \n Firstly, considering both the spatial difference and global \ndistribution of objects are essential characteristics, a scale-\nwise intra-scale transformer is designed to couple the CNN -\nbased local features with the transformer- based global repre-\nsentations in each scale.  In this way, we are able to highlight \nboth detailed spatial information (e.g. contextual cues) and \nlong-range dependencies (e.g. positional cues). However, the \nquadratic computational cost of previous transformers be-\ncomes the main obstacle in such intra -scale structure, espe-\ncially for the shallow scales with high -resolution feature \nmaps. To achieve this , we also design a lightweight and uni-\nversal Dual-Axis MSA to suppress less-relevant information \nand recognize salient parts with fast speed, which is realized \nby capturing row-wise and column-wise global dependencies.  \nSecondly, as many previous studies d emonstrated, multi -\nscale features explore distinctive information and are capable \nof resolving the complex scale variations. In this inter- scale \ncircumstance, for a specific object (e.g. liver), instead of cal-\nculating the redundant relationships with long-distant objects \n(e.g. spleen and aorta) at various scales, we focus on the long-\nrange cross -scale dependency of the object itself and the \nnearby objects (e.g. liver and gallbladder), which can effec-\ntively capture the scale difference (e.g. shape variation)  of \neach object, and achieve promising performance  with satis-\nfactory boundary. Inspired by this, we design a spatial-aware \ninter-scale transformer considering that there is a spatial cor-\nrespondence among patches in various scales through down -\nsampling operation. In this way, we focus on the cross -scale \nsimilarities among these spatial -aware patches, which effi-\nciently learns the mutual information in a lightweight manner. \nThe ability of ScaleFormer in capturing local -global cues \nis shown in Fig.2 (a), (e). By  extracting the intra -scale local \ndetails and global semantics, ScaleFormer can focus on the \nsmall object in both shallow and deep layers. Additionally, \nbenefiting from the inter-scale interactions, ScaleFormer also \nattends full object extent with in various sizes and long-range \nscenarios. The major contributions of this work are four-fold: \n(i) We analyze the intra -scale and inter-scale problems faced \nby the current transformer -based backbones, and propose a \nnovel backbone in a scale -wise perspective, terme d as Scale-\nFormer, to improve the segmentation quality of medical im-\nage segmentation. (ii) We propose a scale -wise intra -scale \ntransformer to associate CNN-based local features with intra-\nscale transformer -based global cues in each scale. Accord-\ningly, we design a lightweight and universal Dual-Axis MSA \nto capture row -wise and column -wise global dependencies. \n(iii) We propose a spatial -aware inter -scale transformer to \ninteract among consensual regions in multiple scales, which \nis capable of capturing cross-scale dependency and resolving \ncomplex scale variations in a simple and effective manner. \n(iv) We conduct extensive experiments on three public da-\ntasets, which shows ScaleFormer surpasses SOTA methods.  \n2 Related Work \nVision transformers. In the most current research, ViT [Do-\nsovitskiy et al. 2020] introduced transformer into visual tasks \nand achieved competitive performance . Motivated by the \nsuccess of ViT, SegFormer [Xie et al., 2021], SETER [Zheng \net al., 2020], DETR [Carion et al., 2020] have been proposed \nfor semantic segmentation and object detection tasks , which \nachieved comparative results with CNN -based methods. To \nreduce the computational cost of self-attention operation in \nViT, CMT [Guo et al., 2021] utilized depth-wise convolution \nto downsample key and value to extract important  infor-\nmation; while  Swin transformer [Liu et al . 2021] restricted \nProceedings of the Thirty-First International Joint Conference on Artiï¬cial Intelligence (IJCAI-22)\n965\n \n \n \nFigure 3: An overview of our ScaleFormer. First, the input image passes the CNN blocks to generate local -level details, which are sent to the \nintra-scale transformers to capture the global -level cues. The inter -scale transformer then takes the enhanced representation from each scale \nas the inputs and interacts among different scales to model the mutual information of objects. Finally, the reinforced output  is further com-\nbined with the same-scale CNN features, and then send to the corresponding decoder block for the final prediction.     \nthe attention in local region s. Different from existing meth-\nods, we design a novel backbone that couples local CNN \nblocks and global Transformer.  Such a structure not only in-\nherits sufficient structural  prior from CNN but also embeds \nglobal representations  from transformer block in each scale . \nMulti-scale architectures for medical image segmentation . \nMulti-scale fusion has been p roved to improve the segmen-\ntation performance in CNN-based methods. PSPNet [Zhao et \nal., 2017] exploited the capability of global context infor-\nmation by aggregating different region -based features . \nUNet++ [Zhou et al.,2020] and UNet 3+ [Huang et al., 2020] \nhave been proposed to bridge the semantic gap between shal-\nlow encoder layers and deep decoder layers  and made full \nuse of multi -scale information  through skip connection . To \nlearn more extensive and rich context information, MS-Dual-\nGuided network [Sinha et al., 2020] applied attention in both \nspatial and channel dimension of feature map s on different \nscales. Considering the multi -scale cues, we further propose \na simple and  effective spatial -aware inter-scale transformer, \nwhich capture s long-distance relat ionships  among scales. \n3 ScaleFormer \n3.1 Overview \nThe proposed ScaleFormer learns intra -scale local -global \ncues as well as modeling inter -scale dependencies as shown \nin Fig.3. In special, we utilize ResUNet backbone to extract \nlocal features in a hierarchi cal manner. The encoder part has \nfive CNN blocks comprised of basic resnet-34 blocks, where \nthe channel number increases with network depth while the \nresolution of feature maps decreases.  When the CNN goes \ndeeper, different fine -grained features are extracted in vari-\nous stages. Considering that transformer is  powerful at  cap-\nturing long -distance relationships,  the scale-wise intra-scale \ntransformer (in Sec.3.2)  is coupled with the corresponding \nCNN block to highlight both detailed spatial information and \nlong-range dependencies in each scale .  T o learn mutual re-\ngions at different scales, the spatial-aware inter-scale trans-\nformer (in Sec.3.3)  further interacts among multiple scales. \nFinally, the enhanced representation is aggregated with the \nlocal-level CNN features in the same scale, and then send s to \nthe decoder block for the final prediction.  In the following, \nwe will describe each step of  ScaleFormer in detail.  \n3.2 Intra-Scale Transformer Block  \nAs shown in Fig.3, the initial transformer only receives input  \nfrom the CNN branch of same stage, whereas the input of \nother intra- scale transformer blocks incorporates global in-\nformation from previous transformer stages to aggregate \nfine-grained details and coarse semantics information. Let i \nindexes the down -sampling layer along transformer branch, \nğ‘\n# refers to the total number of transformers. The stack of \nfeature maps ğ‘‹#%&'(\n)  can be represented as:  \nğ‘‹#%&'(\n) = ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ (ğ‘‹1''\n) ),\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tğ‘– = 1\nğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ ([ğ‘‹1''\n) , ğ·ğ‘œğ‘¤ğ‘›(ğ‘‹#%&'(\n):; )]), ğ‘– = 2, â‹¯ , ğ‘#\n\t\t\t(1) \nwhere function ğ·ğ‘œğ‘¤ğ‘›(âˆ™)  indicates the patch embedding \nmechanism, which is realized by a convolution operation \nwith stride of 2 and followed by a batch normalization and a \nReLU activation function. [Â·] is the concatenation operation. \nğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ (âˆ™) is the intra-scale transformer block, which consists \nof a lightweight Dual -Axis MSA module, and an enhanced \nMLP. We will describe these parts in the following.  \nDual-Axis MSA. In the Fig.4, we illustrate the previous two \nMSA architecture and our proposed method. In original MSA \n[Dosovitskiy  et al ., 2020] (shown in Fig.4.(a)), the input \nğ‘‹ âˆˆ â„\nBÃ—DÃ—E  is linearly transformed into query ğ‘„, key ğ¾ and \nvalue ğ‘‰ with the same shape of ğ»ğ‘ŠÃ—ğ¶. The computational \ncost of the original MSA is ğ‘‚(ğ» Mğ‘ŠMğ¶). To achieve lower \ncost, some works [Guo et al., 2021] used a spatial reduction  \nProceedings of the Thirty-First International Joint Conference on Artiï¬cial Intelligence (IJCAI-22)\n966\n \n \n  \nFigure 4: Comparison of previous MSAs (a) and (b)  with the pro-\nposed Dual-axis MSA (c). \nmechanism by applying a  reduction ratio ğ‘… to the key ğ¾ and \nvalue ğ‘‰ (shown in Fig.4.(b)) and the computational cost is \ndecreased to ğ‘‚(ğ»Mğ‘ŠMğ¶ ğ‘…M). However, it still requires large \nreduction ratio ğ‘… when applied to high -resolution feature \nmap, which degrades the horizontal and vertical information.  \nDifferent from the previous MSAs, we design a light-\nweight and universal Dual -Axis MSA mechanism, which r e-\nplaces the full- image relation ( ğ»ğ‘ŠÃ—ğ»ğ‘Š) with a row -wise \n(ğ»Ã—ğ») and column- wise (ğ‘ŠÃ—ğ‘Š) relation, as illustrat ed in-\nFig.4.(c). To adaptively learn the horizontal  and vertical rep-\nresentation [Chi et al., 2020], we first apply four distinctive \ndepth-wise convolutions on ğ‘„ and ğ¾ to obtain row -wise en-\nhanced ğ‘„%, ğ¾% and column -wise enhanced ğ‘„1, ğ¾1. We then \npass these into average pooling operation in two directions, \nwhich estimates global and compact conditions ( ğ‘„%O, ğ¾%O, \nğ‘„1O, ğ¾1O) over the whole feature map. The process can be \nformulated as: \nğ´ ğ‘„, ğ¾, ğ‘‰= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ ğ‘„%Oğ¾%O\nU\nğ‘‘W\nXYZ:Z)([\t\\]^\nğ‘‰ ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ ğ‘„1Oğ¾1O\nU\nğ‘‘W\nEY_`a':Z)([\t\\]^\n\t(2)\t\nwhen applying the row-wise MSA, the value ğ‘‰ is reshaped \ninto ğ»Ã—ğ‘Šğ¶ ; while using the column -wise MSA, ğ‘‰ is re-\nshaped into ğ»ğ¶Ã—ğ‘Š. In this way, the complexity of our Dual-\nAxis MSA is ğ‘‚(ğ»Mğ‘Šğ¶ + ğ»ğ‘ŠMğ¶). Compared with Axial At-\ntention [Ho et al.,2019] that requires consecutive two blocks \nto capture long-term relations, our method learns global con-\nditions using a single block. Then, we utilize an enhanced \nMLP [Huang et al., 2021] with recursive skip connection be-\nfore the depth-wise convolution for feature delivery.  \n3.3 Spatial-Aware Inter-Scale Transformer Block  \nMulti-scale cues have been proven to b e powerful in resolv-\ning complex scale variations, which are essential for medical \nimage segmentation. Hence, we aim to design a transformer -\nbased block to interact among different scales based on the  \nhierarchical structure of ScaleFormer. The input sequence of \nenhanced intra-scale tokens is first rearranged into 2D lattice. \nInstead of calculating the dependencies on a long sequence \nthat directly concatenated from multi -scale tokens (flattened \nfrom the complete 2D feature map of each scale) , we take  \n  \nFigure 5: An illustration of spatial -aware inter-scale transformer. \nfull advantage of the spatial correspondence among patches \nin different scales through down- sampling operation,  which \nis called Patch Matching. As an example, Fig.5.(a) illustrates \nhow to find the spatial-aware patches on three successive fea-\nture maps from i-th to (i+2)-th scale. Note that we first regu-\nlarize feature maps into the same channel dimension ğ¶. Let \ndenote ğ‘d\n) as the j-th patch in the i-th scale, and the corre-\nsponding down -sampeled patches are ğ‘d\n)e; and ğ‘d\n)eM sharing \nthe bounding box with the same color (i.e. blue). In this way, \nwe focus on the most related patches, and thus maintain the \nspatial correspondence as well as reduce the redundancy.  \n    Then we apply Scale Fusion to concatenate spatial corre-\nsponding inter-scale patches, which can be formulated as:  \nğ‘“ğ‘™ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘› ğ‘d\n; , â‹¯ ,ğ‘“ğ‘™ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘› ğ‘d\nh \t â†’ ğ‘ƒd\n1&#\t\t\t\t\t\t\t\t\t\t(3) \nwhere ğ‘“ğ‘™ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘›(âˆ™) operation rearrange the patch ğ‘d\n) into the  \nshape of ğ»ğ‘Š 2M) Ã—ğ¶ and [ Â·] represents the concatenation \noperation. Because the sequence of tokens lacks horizontal  \nand vertical information, in this part, we utilize the original \nMSA and MLP proposed by [Dosovitskiy et al., 2020] in our \nTransformer block, instead of using our Dual -Axis MSA \n(shown in eq.(2)). The spatial-aware inter-scale dependencies \namong patches can obtained as follows: \nğ‘ƒd\n1&# = ğ‘€ğ‘†ğ´ ğ¿ğ‘(ğ‘ƒd\n1&#) + ğ‘ƒd\n1&#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4) \nğ‘ƒd\n1&# = ğ‘€ğ¿ğ‘ƒ ğ¿ğ‘(ğ‘ƒd\n1&#) + ğ‘ƒd\n1&#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5) \nwhere ğ¿ğ‘(âˆ™) denotes the layer normalization operator. Then \nwe utilize Scale Division to reverse the enhanced sequence \nback to patches according to the order of concatenation:  \nğ‘…ğ‘’ğ‘ â„ğ‘ğ‘ğ‘’ ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡ ğ‘ƒd\n1&# â†’ ğ‘d\n;, â‹¯ , ğ‘d\nh\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(6) \nwhere ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡(âˆ™) is an inverse operation of previous concatena-\ntion operation [Â·]. In the reverse procedure of patch matching,  \nProceedings of the Thirty-First International Joint Conference on Artiï¬cial Intelligence (IJCAI-22)\n967\n \n \npatch reversion assembles all patches in the same scale  into \n a feature map. Then we concatenate the global -level trans-\nformer feature and the local -level CNN feature as skip con-\nnection, which further sends to the decoder stage. \nDiscussion. In this section, we analyze the successive opera-\ntions of the intra-scale and inter-scale transformers. The local \nfeatures firstly passed through intra -scale transformer to  ex-\ntract the contextual information, which learns the entire pixel-\nwise dependency in each scale. Then, the enhanced feature \nmaps are fed into inter -scale transformer, which explores the \ncorresponding patch -wise dependencies among multiple \nscales. In this way, even though two pixels (ğ‘¥)\n&, ğ‘¥d\ns) in differ-\nent scales ( ğ‘–  and ğ‘—) and irrelevant  patches ( ğ‘  and ğ‘ ), in \nFig5.(c), their relationship  can be also captured via first find-\ning the intra -scale dependencies of ğ‘¥)\n& and ğ‘¥)\ns, then calculat-\ning the inter- scale dependencies of ğ‘¥)\ns  and ğ‘¥d\ns . These two \nconsecutive steps realize the goal of capturing pixel -wise de-\npendencies in full scales with less computational cost.  \n4 Experiments \n4.1 Dataset and Evaluation  \nWe perform experiments on three public medical image da-\ntasets:  \nSynapse. The multi-organ segmentation challenge [Landman \net al ., 2015]  consists 30 abdominal CT scans. Follo wing \n[Chen et al., 2021], we split 18 cases for training and remain-\ning 12 cases for testing. We reported the Dice Coefficient \n(DSC) and Hausdorff Distance (HD) on 8 different organs. \nMoNuSeg. The Multi -Organ Nucleus  Segmentation Chal-\nlenge [Kumar et al., 2017] contains 30 images for training, \nand 14 images for testing. Following [Wang et al., 2021], we \nutilized DSC and Intersection over Union (IoU) metrics. \nACDC. The automated cardiac diagnosis challenge [Bernard \net al., 2018] contains 100 MRI scans with three organs, left \nventricle (LV), right ventricle (RV) and myocardium (MYO). \nFollowing [ Chen et al., 2021], we reported the DSC with a \nrandom split of 70 training cases, 10 validation cases and 20 \ntesting cases. We split the dataset randomly because the pre-\nvious division was unavailable.  \n4.2 Implementation Details  \nWe utilized data augmentations to avoid overfitting, includ-\ning random rotation and flipping. Note that our ScaleFormer \ndid not require pretrain on any large datasets. Here, we item-\nized the batch siz e (bs), learning rate (lr), maximum  training \nepochs (ep), optimizer (opt) for three datasets:  \nâ€¢ Synapse:    bs=8; lr=3e -3; ep=600; opt=SGD;  \nâ€¢ ACDC:      bs=8; lr=3e -3; ep=200; opt=SGD;  \nâ€¢ MoNuSeg: bs=4; lr=1e -3; ep=200; opt=Adam;  \nAll models were trained with mome ntum 0.9 and weight de-\ncay 1e-4. For fair comparison, we used the same settings and \ncombined cross entropy loss and dice loss for all experiments.  \n4.3 Comparison with the State -of-the-Arts \nTo demonstrate the effectiveness of ScaleFormer, we first \ncompare it with 11 state -of- the-art methods on the Synapse \ndataset, including V -Net [Milletari et al., 2016], DARR [Fu \net al., 2020], U-Net, Att-UNet [Schlemper et al., 2019], ViT, \nTransUnet, SwinUNet, AFTer -UNet, MISSFormer. Both \nTransUnet and SwinUNet are pretrained on the ImageNet. \nHere, we abbreviate ResNet-50 as â€œR50â€, which is utilized as \nencoder and combined with U -Net, Att-UNet and ViT.  \nExperimental results are reported in Table 1 and the best  \nresults are highlighted in bold. As shown, our ScaleFormer \nwithout inter -scale former (w/o inter) outperform s all the \nother methods in both regional measures DSC (82. 08%) and \nboundary-aware measure HD (18.03mm). In particular, our \nScaleFormer (w/o inter) surpasse s other transformer- based \nmethods, including SwinUNet an d MISSFormer with pure \ntransformers, as well as TransUNet and AFTer -UNet that \nstack several transformers in the deep layers of encoder. It \nindicates that the structure combined with CNN and intra -\nscale transformer ha s the ability of capturing local -global \nclues.  Additionally, an increment of (DSC: +0.78% and HD: \n-1.22mm) is achieved when utilizing inter- scale transformer, \nwhich achieves the new SOTA on synapse dataset.  \nMethod DSC(â†‘) HD(â†“) Aorta Gallbladdr Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach \nV-Net 68.81 - 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98 \nDARR 69.77 - 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96 \nR50 U-Net 74.68 36.87 87.74 63.66 80.60 78.19 93.74 56.90 85.87 74.16 \nU-Net 76.85 39.70 89.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58 \nR50 Att-UNet 75.57 36.97 55.92 63.91 79.20 72.71 93.56 49.37 87.19 74.95 \nAtt-UNet 77.77 36.02 89.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75 \nR50 ViT 71.29 32.87 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95 \nTransUnet 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62 \nSwinUNet 79.12 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60 \nAFTer-UNet 81.02 - 90.91 64.81 87.90 85.30 92.20 63.54 90.99 72.48 \nMISSFormer 81.96 18.20 86.99 68.65 85.21 82.00 94.41 65.67 91.92 80.81 \nScaleFormer(w/o inter) 82.08 18.03 88.29 76.48 83.10 80.59 94.88 63.90 89.29 80.12 \nScaleFormer 82.86 16.81 88.73 74.97 86.36 83.31 95.12 64.85 89.40 80.14 \n \nTable 1: Comparison to state -of-the-art (SOTA) methods on Synapse dataset.  \n \n \nProceedings of the Thirty-First International Joint Conference on Artiï¬cial Intelligence (IJCAI-22)\n968\n \n \nTo demonstrate the generality of ScaleFormer, we conduct \nexperiments on another two public datasets. Additionally, we \ncompare it with UNet++, MRUNet [Ibtehaz et al .,2020], \nMedT [Valanarasu et al.,2021] and UCTransNet [Wang et al., \n2021], which are four SOTA methods on MoNuSeg dataset. \nTable 2 presents DSC and IoU metrics on MoNuSeg, and Ta-\nble 3 reports DSC on our divided ACDC dataset (*) with re-\ntrained TransUnet and SwinUnet using their original settings. \nOur ScaleFormer achieves the best result for capturing more \nuseful representations, which demonstrate s the superior gen-\neralization and robu stness of ScaleFormer.  \nFig.6 show s the qualitative comparison of classic UNet, \nTransUNet, SwinUNet and our ScaleFormer on Synapse da-\ntaset. It can be observed that ScaleFormer not only accurately \nlocalizes organs but also produces coherent boundaries, even \nin small object circumstances.  \n4.4 Ablation Study \nAblation of intra-scale transformers on different stages.  \nWe investigate how intra-scale transformers affect perfor-\nmance among different scales on synapse dataset, which is \nvaried from Stage6 (deepest layer with size of 7Ã—7) to Stage2 \n(shallowest layer with size of 112 Ã—112). As shown in Fig.7, \nthe accuracy is  improved with stacking the transformer \nblocks, which achieves the highest results (DSC: 82.08%, \nHD:18.03mm) when extracting intra -scale local -global cues \non four consecutive stages (Stage6543). However, applying \nthe transformer on the Stage2 may  affect the genuine charac-\nteristics and increase the computational cost (required two \nGPUs). Therefore, in our experiments, we introduce the intra-\nscale transformers on Stage3-Stage6, which provides the best \nresults within a considerable computational complexity. \nFigure 6: Qualitative results of different models on Synapse dataset. \nFigure 7: Ablation of intra-scale transformers on different stages  \n \nFigure 8: Ablation of the input for intra -scale transformer block.  \nAblation of the input  for intra-scale transformer block. \nBased on the architecture with four -stage intra -scale trans - \nformers, we present the ablation of its input, i.e. the same -\nstage CNN and the previous -stage transformer (Pre -Trans). \nAs shown in Fig.8 that only using pre-trans in a staking man-\nner achieves inferior results than using CNN feature (DSC: -\n0.80%, HD: +3.05mm), which means that staking transform  \ners may dilute detailed signals by extracti ng global infor-\nmation. By combining two inputs, the best results (DSC: \n82.08%, HD: 18.03mm)  are achieved for aggregating fine -\ngrained details and coarse -grained semantics. \nEffectiveness of  our Dual-Axis MSA.  To verify the capa-\nbility of Dual -Axis MSA in ca pturing global cues, we com-\npare it with CMT [Guo et al., 2021], PVT [Wang et al. 2021], \nand Axial attention [Ho et al., 2019] in Fig.9(b). We find that \nour method achieves a better accuracy (higher DSC and lower \nHD) than these methods with less computation cost and fewer \nparameters. It indicates the superiority of Dual-Axis MSA via \nsplitting the full -image dependency in the row -wise and the \ncolumn-wise manner.  \nAblation of inter-scale transformers on various stages.  \nAdditionally, we conduct several experiments to assess the  \nMethods DSC(â†‘) IOU(â†‘) \nUNet 73.97 59.42 \nUNet++ 75.28 60.89 \nAttUNet 76.20 62.64 \nMRUNet 77.54 63.80 \nMedT 79.24 65.73 \nTransUNet 79.20 65.68 \nSwinUNet 78.49 64.72 \nUCTransNet 79.87 66.68 \nScaleFormer(w/o inter) 79.59 66.27 \nScaleFormer 80.06 66.87 \nTable 2: Comparison to SOTA methods on MoNuSeg dataset.  \nMethods DSC(â†‘) RV Myo LV \nR50 U-Net 87.55 87.10 80.63 94.92 \nR50 Att-UNet 86.75 87.58 79.20 93.47 \nR50 ViT 87.57 86.07 81.88 94.75 \nTransUNet 89.71 88.86 84.53 95.73 \nSwinUNet 90.00 88.55 85.62 95.83 \nTransUNet* 89.63 86.70 86.96 95.33 \nSwinUNet* 89.16 87.04 86.22 94.24 \nScaleFormer(w/o inter) 89.74 86.54 87.66 95.01 \nScaleFormer 90.17 87.33 88.16 95.04 \nTable 3: Comparison to SOTA methods on ACDC dataset.  \nProceedings of the Thirty-First International Joint Conference on Artiï¬cial Intelligence (IJCAI-22)\n969\n \n \n \nFigure 9: (a) Ablation of inter-scale transformers on various stages; \n(b) Comparison of  Dual-Axis MSA with CMT, PVT and Axial at-\ntention on Params, DSC and HD.  \n \nFigure 10: Visualization of the feature map  of various methods. \n \ninfluence of inter -scale transformers on various stages. As \nshown in Fig.9 (a), by learning interactive information on \ndeeper stages, an increment from 81.37% to 82.86% is \nachieved, where 0.67% and 0.82% improvements are pro-\nduced by adding Stage5 and Stage6. It indicat es that the \nlearned mutual relations can enhance segmentation quality.  \n4.5 Interpretation of ScaleFormer  \nWe visualize the feature map from the final decoder in Fig.10. \nAs shown, ResUNet (in Fig.10.(f)) has the capability of fo-\ncusing on the local areas but it fails to activate larger regions; \nwhile TransUNet (in Fig.10.(e)) and SwinUNet (in Fig.10.(d)) \ncan globally model the long -term relationships but decline \nthe important detailed local features. Impressively, our Scale-\nFormer (w/o inter) (in Fig.10.(b)) no t only inherits sufficient \nstructural prior from CNN but also embeds the global repre-\nsentations from transformer block , which help s to find the \ntiny objects. Considering the multi -scale cues, ScaleFormer \n(in Fig.10.(c)) has more complete attention areas on  objects \nwith widely variable size, shape and location. \n5 Conclusion \nIn this paper, we proposed a novel transformer -based back-\nbone, namely ScaleFormer, from a scale -wise perspective  to \nimprove the medical image segmentation quality even for the \nsmall obje cts. In each scale, a scale -wise intra -scale trans-\nformer with a lightweight Dual-Axis MSA is designed to  \ncombine the CNN -based local features and transformer-\nbased global features; while among multiple scales, a simple \nand effective spatial-aware inter -scale transformer was pro-\nposed to interact across scales. Experiments showed  that \nScaleFormer outperformed both CNN -based and trans-\nformer-based segmentation networks on three datasets .  \nAcknowledgements \nThis work was supported in part by Zhejiang Provincial N at-\nural Science Foundation of China under the Grant No. \nLZ22F020012, Major Scientific Research Project of \nZhejiang Lab under the Grant No. 2020ND8AD01, and in \npart by the Grant-in Aid for Scientific Research from the Jap-\nanese Ministry for Education, Science , Culture and Sports \n(MEXT) under the Grant No. 20KK0234, No. 21H03470 and \nNo. 20K21821. \nReferences \n[Shelhamer et al., 2015] Evan Shelhamer , Jonathan \nLong ,Trevor Darrell  . Fully Convolutional Networks for \nSemantic Segmentation . The IEEE Conference on Com-\nputer Vision and Pattern Recognition  (CVPR), 2015.   \n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fischer, \nThomas Brox . U-Net: Convolutional Networks for Bio-\nmedical Image Segmentation . Medical Image Computing \nand Computer-Assisted Intervention ( MICCAI), 2015.  \n[Chen et al., 2017] Liang -Chieh Chen, George Papandreou, \nFlorian Schroff,  et al. Rethinking atrous convolution for \nsemantic image segmentation . arXiv preprint \narXiv:1706.05587, 2017.  \n [Huang et al., 2020] Huimin Huang, Han Zheng, Lanfen Lin, \net al. Medical Image Segmentation with Deep Atlas Prior. \nIEEE Trans. Medical Imaging 40.12: 3519 -3530, 2021.  \n[Luo et al., 2016] Wenjie Luo, Yujia Li, Raquel Urtasun, et \nal. Understanding the effective receptive field in deep con-\nvolutional neural networks . In NIPS, 2016. â€¨\t\n[Dosovitskiy et al ., 2020] Alexey Dosovitskiy , Lucas \nBeyer, Alexander Kolesnikov,  et al. An image is worth \n16x16 words: Transformers for image recognition at \nscale. arXiv preprint arXiv:2010.11929 , 2020. \n.[Cao et al ., 2021] Hu Cao , Yueyue Wang , Joy Chen ,et al . \nSwin-Unet: Unet -like Pure Transformer for Medical Im-\nage Segmentation. arXiv:2105.05537, 2021.  \n[Huang et al., 2021] Xiaohong Huang , Zhifang Deng, Dan-\ndan Li , et al. MISSFormer: An Effective Medical Image \nSegmentation Transformer.   arXiv:2109.07162, 2021. \n[Chen et al., 2021] Jieneng Chen, Yongyi Lu, Qihang Yu, et \nal. Transunet: Transformers make strong encoders for \nmedical image segmentation.  arXiv:102.04306, 2021.  \n[Guo et al.,  2021] Jianyuan Guo , Kai Han, Han Wu, Chang \nXu, et al. Cmt: Convolutional neural networks meet vision \ntransformers. arXiv:2107.06263, 2021. \n[Zhao et al., 2017] Hengshuang Zhao, Jianping Shi, Xiaojuan \nQi, et al. Pyramid scene parsing network . The IEEE Con-\nference on  Computer Vision and Pattern Recognition  \n(CVPR), 2017. \n[Zhou et al. ,2020] Zongwei Zhou, Md Mahfuzur Rahman \nSiddiquee, Nima Tajbakhsh , et al. UNet++: Redesigning \nProceedings of the Thirty-First International Joint Conference on Artiï¬cial Intelligence (IJCAI-22)\n970\n \n \nSkip Connections to Exploit Multiscale Features in Image \nSegmentation. IEEE Trans . Medical Imaging 39.6:1856 -\n1867, 2020. \n[Huang et al ., 2020] Huimin Huang , Lanfen Lin , Ruofeng \nTong, et al. Unet 3+: A full-scale connected unet for med-\nical image segmentation. ICASSP 2020-2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal \nProcessing (ICASSP), 2020.  \n[Sinha et al., 2020] Ashish Sinha and Jose Dolz. Multi-scale \nself-guided attention for medi cal image segmenta-\ntion. IEEE Journal of Biomedical and Health Informat-\nics 25.1: 121-130, 2020.  \n[Zheng et al., 2020] Sixiao Zheng , Jiachen Lu , Hengshuang \nZhao, et al. Rethinking semantic segmentation from a se-\nquence-to-sequence perspective with transformers.  The \nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2021. \n[Xie et al., 2021] Enze Xie, Wenhai Wang, Zhiding Yu, et al. \nSegFormer: Simple and Efficient Design for Semantic \nSegmentation with Transformers. arXiv:2105.15203, \n2021. \n[Carion et al., 2020] Nicolas Carion, Francisco Massa, Ga-\nbriel Synnaeve, et al . End-to-end object detection with \ntransformers. European Conference on Computer Vision  \n(ECCV). Springer, Cham, 2020.  \n[Wang et al. 2021] Wenhai Wang, Enze Xie, et al. Pyramid \nvision transformer: A versatile backbone for dense pre-\ndiction without convolutions.  arXiv:2102.12122, 2021.  \n[Ho et al., 2019] Jonathan Ho, Nal Kalchbrenner, Dirk Weis-\nsenborn, et al. Axial attention in multidimensional trans-\nformers. arXiv:1912.12180, 2019.  \n[Liu et al. ,2021]Ze Liu , Yutong Lin , Yue Cao , et al . Swin \ntransformer: Hierarchical vision transformer using \nshifted windows. arXiv:2103.14030, 2021.  \n[Milletari et al., 2016] Fausto Milletari , Nassir Navab, et al. \nV-Net: Fully Convolutional Neural Networks for Volu-\nmetric Medical Image Segmentation . 2016 Fourth Inter-\nnational Conference on 3D Vision (3DV) IEEE, 2016.   \n[Fu et al ., 2020] Shuhao Fu , Yongyi Lu , Yan Wang , e t al. \nDomain adaptive relational reasoning for 3d multi -organ \nsegmentation. Medical Image Computing and Computer -\nAssisted Intervention (MICCAI). Springer, Cham, 2020.  \n[Schlemper et al., 2019] Jo Schlemper , Ozan Oktay, Michiel \nSchaap, et al. Attention gated networks: Learning to lev-\nerage salient regions in medical images. Medical Image \nAnalysis 53: 197-207, 2019. \n[Valanarasu et al., 2021] Jeya Maria Jose Valanarasu, Poojan \nOza, Ilker Hacihaliloglu, et al. Medical transformer: \nGated axial -attention for medical image segmentation. \narXiv preprint arXiv:2102.10662 , 2021. \n[Ibtehaz et al .,2020] Nabil Ibtehaz  and M. Sohel Rahman.  \nMultiResUNet: Rethinking the U -Net architecture for  \nmultimodal biomedical image segmentation . Neural Net-\nworks 121: 74-87, 2020.  \n[Wang et al., 2021] Haonan Wang, Peng Cao, Jiaqi Wang, et \nal. UCTransNet: Rethinking the skip connections in U-Net \nfrom a channel -wise perspective with transformer.  \narXiv:2109.04335, 2021.  \n[Kumar et al., 2017] Neeraj Kumar, Ruchika Verma, Sanuj \nSharma, et al . A dataset and a technique for generalized \nnuclear segmentation for computational pathology. IEEE \nTrans. Medical Imaging 36.7: 1550-1560, 2017. \n[Landman et al., 2015] Bennett Landman, Zhoubing Xu, Juan \nEugenio Igelsias, et al. Segmentation Outside the Cranial \nVault Challenge.  In MICCAI: Multi Atlas Labeling Be-\nyond Cranial Vault-Workshop Challenge, 2015  \n[Bernard et al., 2018] Olivier Bernard, Alain Lalande, Clem-\nent Zotti, et al. Deep learning techniques for automatic \nMRI cardiac multi-structures segmentation and diagnosis: \nIs the problem solved? . IEEE Trans. M edical Imag-\ning 37.11: 2514-2525, 2018.  \n [Chi et al., 2020] Lu Chi, Zehuan Yuan, Yadong Mu, et al.  \nNon-local neural networks with grouped bilinear att en-\ntional transforms. The IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2020. \nProceedings of the Thirty-First International Joint Conference on Artiï¬cial Intelligence (IJCAI-22)\n971",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7282543182373047
    },
    {
      "name": "Transformer",
      "score": 0.6830042004585266
    },
    {
      "name": "Encoder",
      "score": 0.6227631568908691
    },
    {
      "name": "Segmentation",
      "score": 0.597594141960144
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5478198528289795
    },
    {
      "name": "Image segmentation",
      "score": 0.43207815289497375
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4320336580276489
    },
    {
      "name": "Engineering",
      "score": 0.10179471969604492
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I135768898",
      "name": "Ritsumeikan University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I173915773",
      "name": "Yamaguchi University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210123185",
      "name": "Zhejiang Lab",
      "country": "CN"
    }
  ]
}