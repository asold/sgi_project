{
  "title": "Crosslingual Content Scoring in Five Languages Using Machine-Translation and Multilingual Transformer Models",
  "url": "https://openalex.org/W4388287898",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5072701486",
      "name": "Andrea Horbach",
      "affiliations": [
        "University of Hagen"
      ]
    },
    {
      "id": "https://openalex.org/A5317838346",
      "name": "Deleted Author ID",
      "affiliations": [
        "University of Hagen"
      ]
    },
    {
      "id": "https://openalex.org/A5071668280",
      "name": "Ronja Laarmann-Quante",
      "affiliations": [
        "Ruhr University Bochum"
      ]
    },
    {
      "id": "https://openalex.org/A5081915723",
      "name": "Yuning Ding",
      "affiliations": [
        "University of Hagen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4289521457",
    "https://openalex.org/W4295714264",
    "https://openalex.org/W4205184268",
    "https://openalex.org/W4389520692",
    "https://openalex.org/W2134227585",
    "https://openalex.org/W1537000426",
    "https://openalex.org/W2183293350",
    "https://openalex.org/W4287891043",
    "https://openalex.org/W4385572572",
    "https://openalex.org/W6691177557",
    "https://openalex.org/W3039023360",
    "https://openalex.org/W3010002614",
    "https://openalex.org/W2053154970",
    "https://openalex.org/W2014552602",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3114233412",
    "https://openalex.org/W4381956458",
    "https://openalex.org/W2898796614",
    "https://openalex.org/W3209653474",
    "https://openalex.org/W2250817048",
    "https://openalex.org/W2983404993",
    "https://openalex.org/W2805251832",
    "https://openalex.org/W2930638636",
    "https://openalex.org/W4285248172",
    "https://openalex.org/W2251033195",
    "https://openalex.org/W2966714320",
    "https://openalex.org/W2970052781",
    "https://openalex.org/W6802855695",
    "https://openalex.org/W6732019713",
    "https://openalex.org/W2172200076",
    "https://openalex.org/W2025595753",
    "https://openalex.org/W3031374736",
    "https://openalex.org/W2289293839",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W4230616807",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2171068337",
    "https://openalex.org/W4226379858",
    "https://openalex.org/W4226106469",
    "https://openalex.org/W2105845572",
    "https://openalex.org/W2148861942",
    "https://openalex.org/W2963050514",
    "https://openalex.org/W2566702382",
    "https://openalex.org/W2902677179",
    "https://openalex.org/W4322625259",
    "https://openalex.org/W4200387115",
    "https://openalex.org/W1848614604"
  ],
  "abstract": "Abstract This paper investigates crosslingual content scoring, a scenario where scoring models trained on learner data in one language are applied to data in a different language. We analyze data in five different languages (Chinese, English, French, German and Spanish) collected for three prompts of the established English ASAP content scoring dataset. We cross the language barrier by means of both shallow and deep learning crosslingual classification models using both machine translation and multilingual transformer models. We find that a combination of machine translation and multilingual models outperforms each method individually - our best results are reached when combining the available data in different languages, i.e. first training a model on the large English ASAP dataset before fine-tuning on smaller amounts of training data in the target language.",
  "full_text": "Int J Artif Intell Educ (2024) 34:1294–1320\nhttps://doi.org/10.1007/s40593-023-00370-1\nARTICLE\nCrosslingual Content Scoring in Five Languages Using\nMachine-Translation and Multilingual Transformer Models\nAndrea Horbach 1,2 · Joey Pehlke 1 · Ronja Laarmann-Quante 3 · Yuning Ding 1\nAccepted: 15 August 2023 / Published online: 3 November 2023\n© The Author(s) 2023\nAbstract\nThis paper investigates crosslingual content scoring, a scenario where scoring models\ntrained on learner data in one language are applied to data in a different language.\nWe analyze data in ﬁve different languages (Chinese, English, French, German and\nSpanish) collected for three prompts of the established English ASAP content scoring\ndataset. We cross the language barrier by means of both shallow and deep learning\ncrosslingual classiﬁcation models using both machine translation and multilingual\ntransformer models. We ﬁnd that a combination of machine translation and multilin-\ngual models outperforms each method individually - our best results are reached when\ncombining the available data in different languages, i.e. ﬁrst training a model on the\nlarge English ASAP dataset before ﬁne-tuning on smaller amounts of training data in\nthe target language.\nKeywords Automated content scoring · Automated short-answer scoring ·\nCrosslingual methods · Domain transfer\nRonja Laarmann-Quante and Y uning Ding contributed equally to this work.\nB Andrea Horbach\nhorbach@uni-hildesheim.de\nJoey Pehlke\njoey.pehlke@fernuni-hagen.de\nRonja Laarmann-Quante\nronja.laarmann-quante@rub.de\nY uning Ding\nyuning.ding@fernuni-hagen.de\n1 CA TALPA - Center of Advanced Technology for Assisted Learning and Predictive Analytics,\nFernUniversität in Hagen, Universitätsstraße 47, 58097 Hagen, Germany\n2 Hildesheim University, Institut für Informationswissenschaft und Sprachtechnologie,\nUniversitätsplatz 1, 31141 Hildesheim, Germany\n3 Ruhr University Bochum, Faculty of Philology, Department of Linguistics, Universitätsstraße 150,\n44801 Bochum, Germany\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1295\nIntroduction\nIn this paper, we investigate crosslingual methods for automated content scoring, a\nscenario where scoring models trained on data in a source language are applied to\ndata in a different target language. In content scoring in general, students answer to\na prompt in written form, typically with one or a few sentences. These answers are\nscored on the basis of their content rather than their form. It is only important that the\nstudent understood the right concepts, while form aspects, such as orthography and\ngrammar, are not considered for the ﬁnal score. In a real-life educational setting, such\na procedure can discriminate against non-native speaking students who might concep-\ntually understand the topic in question, but are unable to express their understanding\nin the language of instruction. One solution to this problem could be that students are\nallowed to answer a question in a language they are proﬁcient in. As only the content\nmatters, the form, including the language, is unimportant. This setting, however, would\nrequire that a teacher scoring an item is also proﬁcient in the language used by the\nstudent, which would still restrict the available language options. We therefore inves-\ntigate in this paper the feasibility of crosslingual automated scoring, where training\ndata in one language is used to predict scores on test items in a different language.\nBesides fostering educational equality, crosslingual scoring can also help to over-\ncome data sparsity, as available data in one language can be used to score the same item\nin a new language without the need to collect and humanly annotate large amounts of\nnew training data in the target language.\nOur work builds on earlier work by Horbach et al. ( 2018) who collected ASAP\nde ,a\nGerman version of the established English ASAP dataset (henceforth called ASAP orig )\nand conducted crosslingual scoring experiments with the help of automatic machine\ntranslation ﬁnding a substantial performance loss compared to monolingual scoring.\nWe extend this work in several respects: First, we extend the crosslingual\nsetup to include a broader variety of languages. We collected new versions of\nthe ASAP dataset in Spanish and French through crowd-sourcing studies and\ninclude a recently collected Chinese version (Ding et al., 2020). While German\nand English are closely related Germanic languages for which machine translation\nusually works very well, including those further languages allows us to examine\nthe inﬂuence of language proximity on crosslingual scoring performance. Second,\nprevious work (Horbach et al., 2018) has speculated that part of the performance\ndrop reported in a crosslingual setting could be attributed rather to the data col-\nlection setup than to the language difference, as ASAP\norig was collected from\nhigh school students while ASAP de was acquired through crowdsourcing. To fur-\nther address this question, we also collected a crowdsourced ASAP version in\nEnglish, ASAP en . For each of these setups (ASAP es ,A S A P fr ,A S A Pen ), we col-\nlected about 300 learner answers for each of the three individual prompts already used\nin ASAP\nde . These datasets were manually double-annotated by trained annotators.\nFigure 1 shows as an example the English version of the prompt material from prompt\n1. Table 1 presents example answers from the different language-speciﬁc datasets in\nresponse to that prompt. Note that the shown answers from different datasets are con-\nceptually very similar and thus all received the same score. We take this as a hint\n123\n1296 Int J Artif Intell Educ (2024) 34:1294–1320\nFig. 1 Prompt 1 from the original English ASAP dataset\nthat the annotators’ scoring, who followed the scoring instructions published with\nASAPorig , is indeed consistent across datasets.\nThird, we methodologically extend existing crosslingual work to include recent\ndevelopments in neural content scoring as well as crosslingual text-classiﬁcation. We\nfollow Horbach et al. ( 2018) in using machine translation to either automatically\ntranslate training data into the language of the test data or vice versa as a baseline\ncondition. As machine translation has the risk of introducing noise into the data,\nespecially in the case of learner data with misspellings and other language problems,\nwe investigate ways to circumvent the machine translation step through the use of\na multilingual BERT classiﬁcation model (Devlin et al., 2019) capable of dealing\nTable 1 Example answers in different languages for prompt 1\nDataset Example Answer\nASAPorig Some additional information that I would need is the amount of vinegar they poured.\nASAPen you need to know what amount of vinegar to put on each sample.\nASAPde Eventuell sollte man wissen, wie viel Essig in die Behälter gegeben werden soll.\nASAPes Cantidad de vinagre usada en cada experimento.\nASAP fr la quantité de vinaigre on ajouté dans les echantillon.\nASAPzh\nAll answers have been scored with 1 point (out of a range between 0 and 3)\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1297\nFig. 2 Visualization of the crosslingual scoring process at the example of English as source and German as\ntarget language\nwith multilingual input. We use such a model as another experimental condition and\nexplore ways how the two approaches, machine translation and the use of multilingual\ntransformer models, can be combined. (See Fig. 2 for a visualization of the different\nvariants of the scoring process.) In a domain adaptation approach, we also investigate\nhow smaller amounts of in-domain data from the same language as the test data can\nbe used to ﬁne-tune a model trained on larger amounts of out-of-domain data, i.e. in\nour case the large amounts of English answers from the original dataset (but always\nusing training data from the same prompt as the test data).\nOur paper thus makes the following contributions:\n We compare and discuss ﬁve additional language variants of the ASAP dataset,\nthree of which we have newly collected.\n1\n We present crosslingual experiments methodologically covering approaches based\non (a) machine translation, (b) multilingual transformer models and (c) a combi-\nnation thereof, showing that the latter works best in most setups.\n We show that results can be further improved in a two-step training process, where\na model is ﬁrst trained on large amounts of data in a source language and then\nﬁne-tuned on the smaller available amounts of target language training data.\nThe remainder of the paper is organized as follows. We discuss related work in\n“Related Work”. In “ Datasets”, we discuss and analyze the multilingual short-answer\ndatasets used in our study. “ Experimental Studies ” presents two experimental stud-\nies. The ﬁrst one uses monolingual scoring for comparison. The second one uses\ncrosslingual scoring with machine translation or multilingual transformer models or\nboth to cross the language boundary. We further analyze the datasets and classiﬁcation\nresults in “ Further Analyses” before concluding in “ Conclusion and Future Work ” and\ndiscussing ethical considerations in “ Ethical Considerations ”.\nRelated Work\nIn the following, we ﬁrst describe options how NLP tasks in general can be placed in\na crosslingual setting. We then review the automated scoring literature with respect to\nmulti- and crosslingual setups.\n1 The data is publicly available under https://github.com/andreahorbach/CrosslingualScoring/ .\n123\n1298 Int J Artif Intell Educ (2024) 34:1294–1320\nCrosslingual Methods\nMost NLP applications have been developed in a monolingual setting, which makes\nsense given that any machine learning task works best if the training data is as similar\nas possible to the actual test data at application time. However, given that the acqui-\nsition of new training data is a time-consuming task, crosslingual methods have been\ninvestigated as one way of performing domain transfer as the language of the data\nis one domain aspect in which two data sets can differ. In our paper, we mean by\ncrosslingual NLP tasks any NLP task with training data in one source language and\ntest data in a different target language.\nThere are several ways how the language boundary can be crossed, the most straight-\nforward one being by means of machine translation. In information retrieval, for\nexample, queries have been automatically translated from English to Spanish and\nvice-versa in a crosslingual information retrieval setup (Ballesteros and Croft, 1996).\nAnother approach for crosslingual text-classiﬁcation consists in translating a learnt\nn-gram model such as Shi et al. ( 2010). They accounted for the fact that individ-\nual words can have multiple translations by learning feature translation probabilities.\nAvoiding machine translation, Prettenhofer and Stein ( 2010) proposed a structural\ncorrespondence learning method for text classiﬁcation, where large amounts of unla-\nbeled data and some bilingual word correspondence lists are used to learn structural\ncorrespondences between two languages, i.e. basically to learn a task-speciﬁc machine\ntranslation model.\nIn the context of recent neural network advances, crosslingual word-embeddings\nhave become popular, for example in work by Klementiev et al. ( 2012), who learn\nrepresentations of words in different languages in the same vector space, such that dog,\nchien, Hund, perro and\nare represented by similar feature vectors. Finally, large pre-\ntrained language models such as BERT (Devlin et al., 2019), which we employ in our\nwork, have also been trained on documents in a multitude of languages making them\nmultilingual. Such approaches thus seemingly remove the need to translate training\nor test data.\nCrosslingual Automated Scoring\nThe existence of available data in more than one language is a necessary pre-requisite\nfor multi- and crosslingual scoring. However, the main body of work in the area\nof automated scoring concerns monolingual setups in English. This holds for both\nautomated content scoring, where only conceptual correctness is scored (Horbach and\nZesch, 2019), and essay scoring, where both content and linguistic form are evaluated\n(Klebanov and Madnani, 2021). We therefore ﬁrst provide pointers to approaches on\nlanguages other than English, before discussing the few instances of truly multi- or\ncrosslingual scoring approaches that have been developed so far.\nOur work is an instance of automated content scoring, with an ample body of work\nfor monolingual English scenarios and a fair number of established English-language\ndatasets (e.g., Bailey and Meurers ( 2008); Basu et al. ( 2013); Dzikovska et al. ( 2013);\nMohler and Mihalcea ( 2009). In contrast, approaches and datasets for languages other\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1299\nthan English are much scarcer, especially when it comes to approaches with datasets\nthat are freely available for research 2. They include languages such as Arabic (Abdul\nSalam et al., 2022; Ouahrani and Bennouar, year), German (Meurers et al., 2011; Pado\nand Kiefer, 2015; Sawatzki et al., 2021), Hebrew (Ariely et al., 2023), Indonesian\n(Herwanto et al., 2018; Wijaya, 2021), Japanese (Funayama et al., 2023), Portuguese\n(Galhardi et al., 2018; Gomes et al., 2021), Punjabi (Walia et al., 2019), Swedish\n(Weegar and Idestam-Almquist, 2023) or Turkish (Çınar et al., 2020)3.\nThe English ASAP-SAS dataset 4 used as one of the datasets in this paper has\nreceived substantial attention in the content scoring community (e.g. Heilman and\nMadnani ( 2015); Higgins et al. ( 2014); Kumar et al. ( 2019)). Crosslingual content\nscoring approaches instead are rare, with Horbach et al. ( 2018) as one of the ﬁrst\ninstances. Recently, Schlippe and Sawatzki ( 2022) proposed a method for crosslin-\ngual scoring, unfortunately on datasets which are not publicly available. Similar to\nour approach, they also make use of multilingual transformer models. The work by\nCamus and Filighera ( 2020), while not presenting genuine crosslingual experiments,\ninvestigates the inﬂuence of translating whole content scoring datasets automatically\ninto a foreign language. Note that for content scoring in general, most approaches\nfall either into the instance-based or the similarity-based algorithmic design (see\nHorbach and Zesch ( 2019)). The majority of current approaches belong to the ﬁrst\ncategory where a classiﬁer learns from a set of labeled learner answers properties of\na correct or incorrect answer. An alternative is the similarity-based approach where\nlearner answers are compared to one or several reference answers to determine their\nlabel. Both approaches have been found to reach similar performances Bexte et al.\n(2022, 2023). In this paper, we stick to the instance-based approach, as we extend\nprior, also instance-based, cross-lingual approaches. A similarity-based cross-lingual\nscenario opens up a vast additional parameter space - Do we ﬁne-tune a monolingual\nor cross-lingual similarity metric? Should we translate the answers used to train the\nmetric, the reference answers or the learner answers to be scored? A thorough inves-\ntigation of these additional questions goes beyond the scope of this paper and will be\ntackled in future work.\nThe situation regarding data availability is similar for the essay scoring task. An\nabundance of work has been published for the English ASAP-AES dataset\n5, a de-\nfacto standard in automated essay scoring. Work with publicly available datasets in\nother languages mostly comes from a foreign-language learning perspective and pro-\nvides data for CEFR classiﬁcation, such as the Swedish SWELL corpus (V olodina\net al., 2016), the German Falko corpus (Lüdeling et al., 2008) and the Portuguese\nCOPLE2 corpus (Mendes et al., 2016). While most of these datasets are monolingual,\nthe Merlin corpus (Boyd et al., 2014) is an exception in that it covers three languages,\n2 We acknowledge that the public release of educational data is always problematic and often requires\nsubstantial additional work in terms of anonymization, even more so in longer essays, which might contain\npersonal information, rather than in shorter content scoring answers. See Megyesi et al. ( 2021)a sa n\nexemplary in-depth description of a thorough anonymization effort.\n3 See also https://catalpa-cl.github.io/EduScoringDatasets/ for an overview of educational scoring datasets.\n4 https://www.kaggle.com/c/asap-sas/\n5 https://www.kaggle.com/c/asap-aes\n123\n1300 Int J Artif Intell Educ (2024) 34:1294–1320\nCzech, German and Italian, and has thus allowed for crosslingual CEFR classiﬁca-\ntion experiments (V ajjala and Rama, 2018). They use a set of essay scoring features\nthat generalize across languages, such as features based on part-of-speech tags and\nUniversal Dependency relations. Such features are suitable for the domain of essay\nscoring and especially proﬁciency classiﬁcation, where linguistic form is important,\nbut less so for content scoring.\nAn important research branch also concerns large-scale assessment. This scoring\nbranch is not discussed in detail here because these data sets can typically not be made\npublic. Yet PISA (Peña-López et al., 2012) and similar international standardized\nassessments of educational attainment would be ideal deatasets for crosslingual scor-\ning studies as data often comes in a wide variety of languages targeting exactly the\nsame prompts per language. While there is research targeting the (semi-)automatic\nassessment of such data, e.g. by means of clustering approaches (Andersen et al.,\n2023; Zehner et al., 2016) for the German PISA subset, there has - to the best of our\nknowledge - not been a systematic investigation of crosslingual scoring for this data.\nDatasets\nIn this section, we describe the data sets used in our study. We ﬁrst discuss requirements\nfor crosslingual data sets. Then we describe the data collection and annotation process.\nIn order to assess how similar the new datasets are to each other and to the original\nEnglish ASAP data, we analyze and compare them regarding answer length, label\ndistribution and linguistic variance in the data.\nRequirements for Crosslingual Data\nWhen working on crosslingual data, language should be the only or, at least, the\nmain factor where data sets differ from each other. Horbach et al. ( 2018) propose\na set of requirements for crosslingual data sets that includes independence of the\nprompts of the respective culture, language or curriculum. Especially when extending\nan already existing dataset, the guidelines used to score this original dataset should be\navailable to and applicable by new annotators. Following this rationale, we extend the\nmultilingual ASAP data sets already used in their study by integrating a Chinese data\nset by Ding et al. ( 2020) and three new data set variants: English, French and Spanish.\nAs mentioned above, although the original ASAP data is in English, we collected\nnew English answers via crowd-sourcing in order to keep the population providing\nthe answers similar across all languages in the multilingual dataset and have a way\nto compare ecologically valid educational data with crowd-sourced data in the same\nlanguage.\nData Collection\nPrompts 1, 2 and 10 in ASAP are science-related tasks, which do not have a strong\ncultural background, and are therefore considered as appropriate to be transferred to\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1301\nother languages and learner populations. In addition, earlier work found that annotators\nwere unable to apply the annotation guidelines successfully for the other prompts\n(Horbach et al., 2018).\nTherefore, we manually translated the original prompt material and collected\nanswers in all languages for these three prompts.\nThe French data was collected by distributing the prompts via different platforms\nand groups (e.g. Facebook). The majority of answers were obtained from Amazon\nMechanical Turk (Nguimkeng, 2021). In total, 1,070 answers were collected, of which\n672 were usable (manually eliminating nonsense answers or those given in another\nlanguage). The participants came from many different countries according to their\nself-report, predominantly from France and the USA, followed by Brazil, England,\nIndia and Italy. The new English data, as well as the Spanish data was collected via\nAmazon Mechanical Turk only, with the majority of participants coming from the\nUS and (for Spanish) from V enezuela. To determine payment for crowd-workers, we\ntested how long it takes on average to read the instructions and answer the questions\nand calculated per-item payments in a way that crowd-workers would earn the same\nas a student assistant at university. Those participants in the French data collection not\nrecruited via AMT were unpaid volunteers.\nFor the Chinese data, Ding et al. ( 2020) collected 314 answers per prompt from\nhigh school students in grades 9-12, which is a comparable population in age and\neducational background to the population in the original ASAP-SAS data set. The\nanswers are manually transcribed from handwriting into digital form. The German\ndataset by Horbach et al. ( 2018) consists of 301 crowdsourced answers per prompt.\nTable2 shows key statistics for the different data sets. Apart from the original ASAP\ndata set, all other data sets come with comparable amounts of answers per prompt.\nDataset Annotation\nFor the German, French, Spanish and Chinese datasets, two native speakers of the\nrespective language scored the answers based on the original scoring guidelines.\nThe newly collected English dataset was scored by two native speakers of German\nwith advanced English language skills. As deﬁned in the original scoring guidelines,\nTable 2 Key statistics for the datasets used in our study\nDataset Language Prompt 1 Prompt 2 Prompt 10\n# answ. IAA # answ. IAA # answ. IAA\nASAPorig English 2,229 .94 1,704 .92 2,186 .88\nASAPen English 330 .83 328 .65 330 .69\nASAPde German 301 .85 301 .67 301 .58\nASAPes Spanish 325 .84 297 .66 393 .61\nASAP fr French 274 .65 187 .91 211 .70\nASAPzh Chinese 314 .72 314 .70 314 .69\nInter-annotator agreement (IAA) is measured in QWK\n123\n1302 Int J Artif Intell Educ (2024) 34:1294–1320\nTable 3 Label distribution for\nthe original ASAP dataset and\nthe different language versions\nprompts 1 and 2 were rated on a scale from 0 to 3 points and prompt 10 on a scale\nfrom 0 to 2 points. All annotators were ﬁrst trained on a set of answers from the orig-\ninal English ASAP-SAS dataset before they scored the new data. The inter-annotator\nagreement measured in quadratically weighted kappa is shown in Table 2.\n6 The ﬁnal\ngold standard was created in that the annotators discussed and agreed on a ﬁnal label.\nNote that for the original English data, IAA only refers to the training set consisting\nof about 75% of the answers per prompt. IAA results for the other datasets are lower\nthan for the original ASAP data, although Higgins et al. ( 2014) have already wondered\nwhether these extremely high reported agreement values are really the result of com-\npletely independent ratings, while Shermis ( 2014) described kappa values between\n.62 and .85 as “a typical range for human rater performance in statewide high-stakes\ntesting programs” (p. 58).\nDataset Analysis\nWe further analyze properties of our data sets assuming that any differences between\ndatasets can be due either to the difference in language or to the difference in learner\npopulation. By including also a crowdsourced version of the English data we aim\nat decoupling those two inﬂuence factors. We investigate label distribution, answer\nlength and linguistic diversity operationalized by type-token-ratio.\nLabel Distribution\nWe evaluate the relative frequency for each label per prompt and dataset, comparing the\noriginal ASAP dataset and the different additional versions for different languages (see\nTable 3). We see that all crowd-sourced datasets have a higher amount of low-scoring\nanswers than the original ASAP data and attribute that to the lower intrinsic motivation\nin crowd-workers compared to the high school students involved in creating the original\n6 For Spanish, invalid answers were not ﬁltered out beforehand but as part of the annotation process. In this\ncase, inter-annotator agreement is reported only for answers which both annotators saw as valid answers.\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1303\nFig. 3 Distribution of answer length in characters for all answers with a certain score (left part) and for\nall answers of a certain prompt (right part). We show results on the original datasets (upper part) and their\nEnglish translations (lower part)\nASAP data and to the fact that the tasks were probably aligned with the science classes\nof the students, even though we tried to select prompts that were not particularly\ncurriculum-dependent. These high amounts of low-scoring answers also lead to more\nimbalanced datasets, which bear the risk of being harder to score automatically. It\nseems that the prompts have different difﬁculty for individual populations, e.g. prompt\n10 seems to be particularly difﬁcult for Chinese respondents, while prompt 1 has\nhigh amounts of low-scoring answers among the crowdsourced English and German\npopulation.\nAnswer Length\nWe measure answer length in characters and report for every dataset the distribution\nof lengths for all answers per score across all prompts (Fig. 3, left part) and for all\nanswers per prompt across all scores (Fig. 3, right part).\n7 We observe that across\nall datasets, better answers, i.e. answers with a higher score, tend to be longer than\nanswers with a lower score. We also see differences between prompts, with prompt\n2 eliciting the longest answers. Effects from data collection contexts are visible in\n7 In Fig. 3, extreme outlier answers with a length of > 1,000 characters have been removed prior to plotting\nin order to allow for a more concise visualization. This affected 23 answers in the original datasets and 22\nanswers in the translated datasets. The longest answer was 4,493 characters long.\n123\n1304 Int J Artif Intell Educ (2024) 34:1294–1320\nthat English crowd-worker answers are shorter than answers from the original ASAP\ndataset. To factor out the presumed language inﬂuence in the length comparison (most\npronounced for the Chinese dataset), we also compare a version of the dataset with\nall data automatically translated to English (Fig. 3, lower part) using DeepL (as we\ndid later in the crosslingual scoring experiments using machine translation). One can\nsee that even when translated to English, the answers in the Chinese dataset tend to\nbe shorter than those in the other datasets. By inspecting the data, we found that the\nChinese answers tend to appear in the form of bullet points. Instead of writing e.g.\nAfter reading this procedure, I have noticed that I would need to know the Ph of the\nvinegar in ASAP\norig , answers like (1)PH of the vinegar sampleare often found in the\nChinese dataset. Across all datasets, we observe that crowdsourced answers tend to\nbe shorter than the original data.\nLinguistic Diversity\nV ariance in learner answers has been identiﬁed as one parameter inﬂuencing the\nscoreability of datasets (Horbach and Zesch, 2019). One example of such variance\nis linguistic diversity. Intuitively, a very repetitive data set is much easier to score than\none with a broad range of different words and word combinations.\nWe measure linguistic diversity by type-token-ratio (TTR), i.e. the number of unique\nwords of a text divided by the overall number of words. We use TTR of unlemmatized\nword-forms treating each dataset as an individual text. Type-token ratio is known to be\ninﬂuenced by text length, so that a comparison between TTR values is only possible\nif texts of the same length are compared. To overcome this problem, we use a method\nsimilar to the moving average TTR (Covington and McFall, 2010) and randomly draw\nsubsamples of a ﬁxed size from the datasets and report averages over the individual\nvalues.\nFor Chinese, TTR could either be calculated on the token level or on the character\nlevel (Cui et al., 2022). Here, we rely on a character-based representation because,\nin our content scoring scenario, the accuracy of tokenization is lower, since most of\nthe answers are not complete sentences. From the results in Fig. 4 we see that most\ndatasets fall into the same TTR range after translation to English with the exception\nof Chinese displaying a lower TTR ratio than the other datasets indicating that there\nseems to be slightly less lexical variance in the Chinese data.\nExperimental Studies\nIn the following, we ﬁrst describe our experimental setup, followed by three experi-\nmental studies. In Study 1, we establish baselines for each dataset by cross-validating\non every dataset separately, either in its original version or translated into the other lan-\nguages. In Study 2, we score crosslingually by translating training or test data into the\nrespective other language or by using an inherently multilingual transformer model\n(either with or without additional translating datasets). In Study 3, we explore the\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1305\nFig. 4 Type-token-ratio for the six datasets computed for answers from a certain prompt (a) on the original\nversion of each dataset and (b) on the version translated to English\neffects of pretraining a model on a larger amount of English data and then ﬁne-tuning\non a smaller amount of data in the target language.\nExperimental Setup\nClassiﬁers and Features\nFor the shallow learning baseline we use a logistic regression classiﬁer provided\nthrough sklearn (Pedregosa et al., 2011)8. For all European languages, we use word\nuni- to trigrams and character bi- to 5-grams as features. In the case of Chinese, tok-\nenization of words can be a non-trivial problem as the Chinese language does not\nseparate words by whitespaces. Therefore, we follow Ding et al. ( 2020) in using only\ncharacter uni- to 7-grams.\nIn the case of deep learning, we used a standard transformer architecture with a pre-\ntrained multilingual BERT model ( bert-base-multilingual-uncased) with a sequence\nclassiﬁcation head, as shown in Fig. 5, and, for comparison, also a monolingual\nEnglish BERT model ( bert-base-unased) as monolingual models are known to exhibit\na stronger performance (Artetxe et al., 2023). We ﬁne-tuned for six epochs using an\nAdamW optimizer and CrossEntropy loss.\nNote that our main interest lies in comparing different crosslingual scoring models,\nnot ﬁnding an optimal hyperparameter setup. We therefore hold meta-parameters ﬁxed\nand also chose a ﬁxed number of epochs instead of using validation data to select the\nbest epoch.\n8 We also experimented with other shallow learning algorithms and found that logistic regression showed\non average the best results.\n123\n1306 Int J Artif Intell Educ (2024) 34:1294–1320\nFig. 5 Our neural architecture, using a multilingual BERT model with a sequence classiﬁcation head\nDatasplit and Evaluation Metric\nFor the ASAP orig dataset, we use the datasplit provided in the original data. Due to\nthe small size of the other datasets, we use 10-fold cross-validation for these datasets.\nWe always train separate models for each of the three prompts. Again, we do not use\na separate validation dataset for deep learning.\nWe evaluate our experiments using quadratically weighted kappa (QWK) (Cohen,\n1960).\nExperimental Study 1: Within-Dataset Baselines\nIn our ﬁrst experimental study, we establish baseline scoring performance per dataset,\ni.e. we train and test on each dataset individually as described in the previous section.\nTable 4 Performance in QWK for monolingual baseline experiments\nDataset LR M-BERT\n121 0 a v g . 121 0 a v g .\nASAPorig .700 .628 .674 .667 .791 .808 .720 .773\nASAPorig _300 .671 .424 .677 .591 .653 .618 .702 .658\nASAPen .692 .503 .629 .608 .677 .400 .617 .565\nASAPde .804 .570 .592 .655 .783 .668 .554 .668\nASAPes .786 .614 .631 .677 .805 .573 .661 .680\nASAP fr .718 .740 .667 .708 .734 .760 .685 .726\nASAPzh .616 .353 .529 .499 .666 .398 .587 .550\navg. .712 .547 .628 .629 .730 .604 .647 .660\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1307\nScoring each Dataset in its Original Language\nTable 4 shows the performance per language and prompt in both the shallow learn-\ning and deep learning condition. The original ASAP dataset (ASAP orig ) reaches the\nhighest scoring performance, which is not surprising given that this dataset contains\na much higher amount of training data. Therefore, we also down-sampled the dataset\nto 300 instances per prompt roughly matching the training sizes of the other data sets\n(ASAP\norig _300). In this scenario, results are more on par with other datasets, suggest-\ning that also the other datasets might beneﬁt from more training data than available\nin the monolingual setup - a scenario we will test later in Study 3 when combining\ndifferent data sets as training data.\nWe further observe for most datasets that prompt 2 is the hardest among the three\nprompts to score with the exception of ASAP\norig and ASAP fr while prompt 1 is on\naverage and across both shallow and neural approaches the easiest. This is reﬂected in\nsubstantially lower inter-annotator agreement for all but these two datasets for prompts\n2 and 10 compared to prompt 1, hinting at a generally higher difﬁculty by most of\nthe annotators to score these two prompts accurately. We also see that in most cases\nthe BERT model outperforms logistic regression (the highest performance per dataset\naveraged across the three prompts is marked in bold in the table). Performance on\nChinese data is slightly worse than for the other datasets despite high inter-annotator\nagreements. The values that we ﬁnd in our experiment, however, are comparable to\nthose reported by Ding et al. ( 2020) for Chinese and Horbach et al. ( 2018) for English\nand German.\nComparison between Monolingual and Multilingual BERT Models\nWe wanted to keep the pre-trained transformer model ﬁxed across experiments and\ntherefore chose to use the same multilingual BERT model for all of our experiments.\nHowever, monolingual models are known to exhibit a higher performance. Thus, in\norder to assess the potential performance drop from using an inherently multilingual\npre-trained model in cases where a monolingual model could be used, we compare\nthe two alternatives at the example of the English data.\nTable 5 repeats the monolingual baseline results from the previous experiment,\nwhich used a multilingual transformer model for the three English datasets (on the\nright) and for comparison the same experiments using a monolingual English trans-\nTable 5 Performance in QWK for monolingual baseline experiments comparing a monolingual transformer\nmodel (left) to a multilingual transformer model (right) for English\nDataset BERT M-BERT\n1 2 10 avg. 1 2 10 avg.\nASAPorig .850 .792 .737 .793 .791 .808 .720 .773\nASAPorig _300 .662 .560 .715 .646 .653 .618 .702 .658\nASAPen .698 .383 .642 .574 .677 .400 .617 .565\navg. .737 .578 .698 .671 .730 .604 .647 .660\n123\n1308 Int J Artif Intell Educ (2024) 34:1294–1320\nTable 6 Performance in QWK for monolingual baseline experiments using the logistic regression model\n(left) and the neural model (right)\nTraining and test data (rows) have been translated into a different language (columns). Results are averaged\nacross three prompts. Cells marked in blue do not use machine translation, i.e., they are the baseline results\nfrom Table 4 and report absolute values. All other cells report the delta from that baseline (i.e. from the\nbaseline value in the same row) with green values indicating an improvement over the baseline and red\nvalues a performance drop\nformer model. Results are mixed, but we see a tendency that the monolingual BERT\nmodel indeed has a slight advantage over the multilingual model (.671 for the mono-\nlingual versus .660 for the multilingual model). As the improvements are rather small\nand in order to use the same model in all experiments, we use the M-BERT model\nfrom here on.\nScoring Translated Datasets\nAs a second monolingual baseline, we use DeepL\n9 to translate both the test and\nthe training data into the same other language, using each of the other languages\nrepresented in our datasets as the target language. I.e. we still score in a monolingual\nsetting but both training and test data have been machine-translated.\nResults are shown in Table 6, with values highlighted in blue on the main diagonal\nrepresenting the untranslated baseline condition. In most cases, results produced on\ntranslated datasets are in the same ballpark as the original data, sometimes even slightly\nhigher especially in the case of Chinese as the original language. This points at a\nnormalizing effect that machine translation can have, for example on spelling errors.\nIt can also mean that a multilingual model does not perform equally well for all\nlanguage pairs (Pires et al., 2019).\nWe manually checked a number of misspelled sentences and indeed found that a\nspelling variant of a word, such as vinagar instead of vinegar or expirement instead of\nexperiment were often translated correctly. Only occasionally such a misspelled word\nwas not translated at all as if it were a named entity. We also encountered examples of\nnon-words spelling errors being translated to a different word in the target language\n9 www.deepl.com\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1309\nthan the correct form, such as English vinager being translated to German Wein(wine).\nThese probing support the idea of a normalization by translation, thus the translation\nwould be able to provide more informative embeddings or ngrams than the original\nversion.\nThe case is somewhat different for Chinese as potential misspellings in Chinese\nwere already removed by force through the digitization process (i.e. when typing the\nhandwritten answers, a non-existing character had to be entered as an existing one), so\nthat non-word errors in Chinese are rare and real-word errors often led to problematic\ntranslations. For example, some Chinese answers contain the compound word\n,\nwhich combines two real words (rinse) and (wash) in an unusual order. The\nintended compound word could be (wash up). This miss-spelled compound was\ntranslated into shabu-shabu, because the character is somehow related to\n(shabu-shabu, a Japanese hotpot) by machine translation.\nWhen looking at the average over all datasets translated into the same language\n(the last line of Table 6), we see that values are quite similar per machine learning\nmethod, indicating that both linear regression and M-BERT are able to handle data in\ndifferent languages and that differences we saw in Table 4 can be mainly attributed to\nthe dataset, not the language.\nCrosslingual Scoring within ASAP orig\nOur ultimate goal is to score cross-lingually data from one dataset with a model trained\non a different dataset in a different language. In the previous step, we have established\nmonolingual baseline performance per dataset and have also established that there is\nhardly any loss from applying machine translation to a data set. Next, we investigate\nthe inﬂuence of applying a multilingual transformer model on training and test data\nin different languages but from the same dataset in order to separate dataset from\nlanguage effects. Therefore, as an additional third baseline, we translate either the\ntraining ( MT -train) or the test data section ( MT -test) from the ASAP\norig set into\neach of the other four languages and then train a model on one language and apply it\nto another language (where one of those languages is always English).\nFrom the results in Table 7 we see that there is a performance loss of up to 12\npercentage points for German, Spanish and French compared to the English baseline\nwhere both training and test data are the original untranslated ASAP dataset. For\nChinese, performance goes down by over 50%. We expect that these values are an upper\nbound for the performance we can expect in true crosslingual setups without machine-\nTable 7 Performance in QWK\nfor experiments on ASAP orig\nwhere either the training or the\ntest data has been translated to a\ndifferent target language\nLanguage MT-train MT-test\nen .77 .77\nde .72 .65\nes .69 .65\nfr .66 .66\nzh .33 .35\n123\n1310 Int J Artif Intell Educ (2024) 34:1294–1320\ntranslation (at least when one language is English) where the effect of different datasets\nadd to that of different languages.\nExperimental Study 2: Crosslingual Scoring using Machine Translation\nand Pretrained Multilingual Models\nThe monolingual baselines established in Study 1 can serve as an upper bound for the\nperformance we hope to reach using crosslingual methods. We compare it against two\nways of how to cross the language barrier. One is by means of machine translation to\ntranslate either train or test data into the respective other language. As a second method,\nsimilar to the previous study, we use the M-BERT model in a zero-shot fashion so\nthat neither test nor training data is translated but rather both will be represented in a\nshared embedding space.\nTable 8 presents these crosslingual results grouped by the test data in different\nlanguages. In every block, we train on the different available training data (one training\ndata set per line) and apply the model to the same test data making results directly\ncomparable. We compare the different methods per column: We present results for\nlogistic regression with translated training data ( LR MT -train) or test data ( LR MT -\ntest) and the same for the neural model ( M-BERT MT -trainand M-BERT MT -test).\nFinally, M-BERT zero-shot relies solely on multilingual representations without any\nmachine translation.\nWhen comparing the three methods in Table 8, logistic regression with MT, M-\nBERT with MT and M-BERT in a zero-shot setting, we see that the M-BERT setting\nalmost always outperforms logistic regression and that machine translation is ben-\neﬁcial (comparing M-BERT MT with zero-shot). The best crosslingual method per\nblock (highlighted in bold print) is often M-BERT with MT with no clear tendency\nwhether translating training or test data is more beneﬁcial. For all languages, the best\nsetup among those with 300 training instances, i.e. excluding ASAP\norig , is lower than\nthe monolingual baseline with the smallest distance between monolingual baseline\nand best crosslingual setup for French (.726 for monolingual vs .698 for crosslin-\ngual when trained on Spanish data) and the worst performance gap for Chinese (.550\nvs .457). Also the zero-shot setup sometimes yields surprisingly good results, for\nexample for the transfer from German or Spanish to French. On the basis of the mono-\nlingual comparison between a monolingual and a multilingual transformer model from\nExperimental Study 1, we assume that replacing the multilingual BERT model with\nmonolingual versions for the respective variants would beneﬁt both the monolingual\nbaseline as well as the MT\ntrain and MT test variants as both could be scored with a\nmonolingual model.\nOverall, we observe that the transfer often works well for highly related languages,\nsuch as Franch and Spanish, while for Chinese, which is in our setup phylogenetically\nfarthest from the other languages, the transfer does not nearly work as well.\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1311\nTable 8 Performance in QWK for crosslingual experiments either using Logistic Regression (LR) or a mul-\ntilingual BERT model (M-BERT) either with machine translation (MT) or relying only on the multilingual\ntransformer (zero-shot)\nCells in blue are monolingual baseline results for comparison. The best crosslingual results per target\nlanguage are printed in bold\n123\n1312 Int J Artif Intell Educ (2024) 34:1294–1320\nExperimental Study 3: Pretraining on Larger Amounts of Data in another\nLanguage\nSo far, we examined domain transfer scenarios where only data in another language\nbut no target language data is available for training and compare it to a within-domain\napproach with (only) data in the target language. But one could imagine a real-life\nscenario where larger amounts of data might be available in a majority language (such\nas the original English ASAP data set) and only limited amounts of data in a new\napplication language. To simulate such a use case, we further pre-train the pre-trained\nM-BERT model on the original ASAP dataset in English and then ﬁne-tune on a\nspeciﬁc language using cross-validation (similar to the monolingual experiments in\nExperimental Study 1) in order to see whether we beneﬁt from English pretraining.\nI.e. each model in Study 3 has been trained on about 2000 English answers and about\n270 target language answers.\nWe compare several conditions. In the pretrain zero-shot condition, we pretrain on\nthe original (untranslated) English ASAP dataset before ﬁne-tuning on the (untrans-\nlated) target language dataset. In pretrain MT -train we translate the original ASAP\ndata into the respective target language before pretraining. In pretrain MT -test,w e\ntranslate the in-domain data (for both ﬁne-tuning and testing, i.e. the whole cross-\nvalidation procedure) from the target language into English.\nFor comparison, we report three baseline values: monolingual CV without pre-\ntraining, i.e. results from Study 1, as well as model performance when only training\non ASAP\norig without further ﬁne-tuning on the in-domain data in two versions. One\nwhere the original ASAP data has been translated into the respective language and\none where the test data has been translated to English, i.e. results from Study 2. (We\ndo not show the zero-shot condition from Study 2 as another baseline as it was clearly\noutperformed by the two variants using machine translation.) As our method in this\nstudy is essentially always a combination of two baseline setups, we expect results to\nbe above the maximum of these baselines.\nTable 9 shows results for different test data (per line) and different experimental\nconditions (per column), the three baselines on the left followed by three options to\ncombine both test data sets in the right part of the table.\nWe see that in all cases results outperform all three baselines. The improvement\nover the monolingual baseline indicates that we indeed beneﬁt from more training\ndata than the approximately 300 training instances in each dataset, even if it is not\nin the target language. The improvement over the ASAP\norig MT baselines shows the\nimportance of in-domain training data, given that a modest increase in training data\nsize (adding less than 300 instances to an existing dataset of 2000 answers) results in\na relatively large performance gain.\nWhen comparing the three experimental conditions, there is no clear winner and the\nzero-shot condition performs surprisingly well. We take that as an indicator that ﬁne-\ntuning on the target language can compensate for a pre-training in a non-matching\nlanguage. (Note that the three identical results in the ﬁrst line for ASAP\nen result\nfrom source and target language both being English, thus translating any data set\nhas no effect.) Overall, results in this study are for Spanish and French in the same\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1313\nTable 9 Performance in QWK for an M-BERT model pretrained on the original ASAP data and ﬁnetuned\non each of the other datasets (right part) either training or test data is translated or neither (zero-shot)\nTest Data baselines pretrained\nmonolingual MT train MTtest zero-shot MT train MTtest\nASAPen .608 .599 .599 .700 .700 .700\nASAPde .668 .623 .554 .735 .730 .727\nASAPes .680 .689 .634 .755 .776 .743\nASAP fr .726 .595 .593 .783 .777 .780\nASAPzh .550 .378 .432 .626 .651 .629\nThis is compared to three baselines (left part): monolingual cross-validation within the same dataset and\ntraining on ASAP orig with either translated training or test data\nregion as monolingual scoring performance on the full original English ASAP dataset\n(with much less additional annotation effort in the new target language), and at least\nsubstantially improved over the baselines in case of crowd-sourced English, German\nand Chinese.\nFurther Analyses\nWe saw in our experiments that crosslingual scoring with training and test data from\ndifferent datasets yields a substantial performance loss compared to monolingual scor-\ning where only one dataset is used, even if the data has been translated into a different\nlanguage. This gap only becomes smaller, when in addition to the data from a dif-\nferent dataset also genuine target-language data is used. We therefore conclude that\npart of the drop in performance is due to the nature of the different datasets, i.e. that\ndifferent user populations think or at least write differently about the same prompt.\nIn the following, we thus provide additional analyses intended to shed light on those\ndifferences. We do this in two ways: by comparing the semantic similarity between\nanswers across data sets and by examining high-frequency lexical material per data\nset.\nSimilarity between Datasets\nAs a ﬁrst analysis step, we measure the similarity between datasets. We operationalize\nthis by measuring pairwise similarity between answers for the same prompt.\nWe are ﬁrst and foremost interested in semantic similarity, not similarity on the sur-\nface. We measure semantic similarity by encoding every answer with a multilingual\nSBERT model ( distiluse-base-multilingual-cased-v1) and calculate its cosine similar-\nity with every other answer from either the same or a different dataset (only within the\nsame prompt). For each answer in dataset A, we record the maximum similarity to any\nanswer in dataset B, with A and B being either different or the same dataset. The idea\nis that if different user populations answer a prompt in conceptually different ways,\nwe would see a low maximum answer similarity between these datasets. Likewise, we\n123\n1314 Int J Artif Intell Educ (2024) 34:1294–1320\nexpect the answer similarities to be higher within a dataset. The reason why we look\nat maximum similarity only is that we want to see if a scoring model would have had\nthe chance to see a similar answer in the training data or not.\nTable 10 presents the results. Every histogram shows the distribution of the max-\nimum similarity of every answer in the dataset speciﬁed in the row label with the\nanswers in the dataset speciﬁed in the column label (measured within the same prompt).\nThe diagonal from the upper left to the lower right corner contains the results for the\nsimilarities within one dataset. We see that ASAP\norig 300 is rather homogeneous in that\nmost answers have a matching answer with a very high similarity, i.e. the histogram\nhas a clear peak towards the right. Also when compared to answers in the other datasets\n(see ﬁrst row), ASAP\norig 300 mostly has matching answers with high similarities, with\nthe Chinese dataset behaving a bit differently. For the Chinese dataset itself (see last\nrow), we see that the answers are rather similar to each other within this dataset but not\nso much when the answers are compared to any of the other datasets. This indicates\nthat either the Chinese data is indeed rather dissimilar to the other datasets, which is\nin line with the observation that Chinese was hardest to score in a crosslingual setting,\nor that the multilingual SBERT model is not able to capture semantic similarity across\nlanguages for Chinese. Most of the other histograms for same-dataset vs. different-\nTable 10 Distribution of maximum similarity between an answer from the dataset speciﬁed in the row label\nand any answer from the dataset speciﬁed in the column label\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1315\nFig. 6 Top 10 word-level trigrams in different datasets (character-level trigrams for Chinese). (The x-axes\nare not on the same scale because of the different size of data sets.)\ndataset comparisons look rather similar to each other, indicating that the conceptual\ndifferences between datasets are not more pronounced than the conceptual differences\none ﬁnds within a dataset.\nMost Frequent Terms and N-Grams\nTo investigate the idea further that different learner or crowd worker populations simply\ntalk about different concepts when answering the same questions we inspect the most\nfrequent n-grams per dataset and prompt as an easy way to gain insight into which\nconcepts are frequently mentioned.\nWe focus on content words and thus exclude n-grams that consist only of function\nwords, such as ‘and for the’. As for some of the previous analyses, we do this in\ntwo variants: We ﬁrst analyze each datasets in its original language. Next, we re-run\nthe analyses on all datasets translated into English. In doing so, we want to make\nthe analyses more accessible to readers unfamiliar with those languages, but also\nsee potentially detrimental effects of machine translation when one term might be\nconsistently translated to English as a term not occurring in the English data set.\nFigures 6 and 7 show exemplarily the most frequent 3-grams for Prompt 1 in\nall languages. We see that terms like vinegar, sample or experiment can be found\nfrequently in all datasets. Many answers in German contain the factor of time, since\nterms at the beginningand at the endare only listed as top 10 in this dataset. For answers\nin German and Chinese, unlike other languages, common terms for the formulation\nof answers into a whole sentence, repeating phrases from the question, such as need\nto know or I would need are rare. This conﬁrms our ﬁrst observation that answers in\nGerman and Chinese are often in bullet points. It is interesting to see that the terms PH\nvalue and acid rain can only be found in the top 10 list in Chinese whereas answers\n123\n1316 Int J Artif Intell Educ (2024) 34:1294–1320\nFig. 7 Top 10 word-level 3-grams in different datasets translated to English\nin other languages rather talk about the amount of vinegar and it remains an open\nquestion whether such variation reﬂects cultural differences or different approaches\nto a reading comprehension task as the term acid rain appeared only in the title of the\nprompt but not in the task itself. This supports our assumption that Chinese data is\nindeed conceptually different from other datasets.\nConclusion and Future Work\nIn this paper, we have presented new datasets from three languages addressing the\nsame content scoring prompts as a starting point to explore the ﬁeld of crosslingual\ncontent scoring. We used both machine translation and multilingual transformer mod-\nels as well as a combination of the two to assess the feasibility of using training data\nin one language to score test data in another. We found in Study 1 that in concor-\ndance with earlier ﬁndings machine translation by itself does not introduce noise that\nmakes automated scoring harder. Study 2 revealed that truly crosslingual experiments\nwith training data in a source language and test data in a different target language\nunsurprisingly work not as well as a monolingual baseline and that the extent of the\nperformance decrease depends on the language pair considered. Closely-related lan-\nguages (such as the Spanish-French pair) are often easier than pairs that are further\napart (such as Chinese paired with any other language in our study). When comparing\ndifferent methods, a semantic representation works better than one relying on surface\ninformation such as n-grams. The best performance could be achieved when using a\nmultilingual transformer model but still translating either the train or test data into the\nrespective other languages. In Study 3, we explored a scenario where a transformer\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1317\nmodel was iteratively trained on both larger amounts of English data and smaller\namounts of in-domain target language data. This setup yielded overall the best results\ncoming close to performance on large in-domain setups in English.\nWe postulated that differences in behaviour between datasets can have two reasons:\nlanguage differences and differences between learner populations. One aspect of such\npopulation effects is the difference between real learners, such as high school students,\nand paid crowd workers. In addition, other aspects we could not address here might\nplay a role, such as the cultural background of a learner population. To understand\nthese inﬂuences better, more research, ideally under more controlled conditions would\nbe necessary.\nEthical Considerations\nThe beneﬁts and risks of automated scoring in general have been extensively debated in\nthe literature (see, for example Loukina et al. ( 2019)). One important argument in favor\nof the use of automated scoring is that normally all learner answers are scored by the\nsame algorithm. If we use crosslingual scoring where one scoring model per language\nis trained, learners will be scored by different models depending on their choice of\nlanguage and thus be subjected to different model biases. A more advisable scenario\nwould thus be the automatic translation of all learner answers into the same language\nin which a model has been trained, but again with the caveat that machine translation\ncan have different quality for different source languages and learners writing in a less-\nresourced minority language might be discriminated against by the automated scoring\nprocess. Similarly, multilingual transformer models work differently well for different\nlanguages and language combination. Thus we would consider crosslingual scoring\nat the moment not suitable for high-stakes assessment.\nAcknowledgements This work was partially conducted at “CA TALPA - Center of Advanced Technology\nfor Assisted Learning and Predictive Analytics” of the FernUniversität in Hagen, Germany, and partially\nwithin the KI-Starter project “Explaining AI Predictions of Semantic Relationships” funded by the Ministry\nof Culture and Science, Nordrhein-Westfalen, Germany.\nFunding Open Access funding enabled and organized by Projekt DEAL.\nData Availability The data used in this paper is publicly available under https://github.com/andreahorbach/\nCrosslingualScoring/.\nDeclarations\nConﬂicts of Interest The authors have no relevant ﬁnancial or non-ﬁnancial interests to disclose.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included\nin the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted\n123\n1318 Int J Artif Intell Educ (2024) 34:1294–1320\nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the\ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nAbdul Salam, M., El-Fatah, M. A., & Hassan, N. F. (2022). Automatic grading for arabic short answer\nquestions using optimized deep learning model. Plos one, 17(8), e0272269.\nAndersen, N., Zehner, F., & Goldhammer, F. (2023). Semi-automatic coding of open-ended text responses\nin large-scale assessments. Journal of Computer Assisted Learning, 39(3), 841–854.\nAriely, M., Nazaretsky, T., & Alexandron, G. (2023). Machine learning and hebrew nlp for automated assess-\nment of open-ended questions in biology. International journal of artiﬁcial intelligence in education,\n33(1), 1–34.\nArtetxe, M., Goswami, V ., Bhosale, S., Fan, A., & Zettlemoyer, L. (2023). Revisiting machine translation\nfor cross-lingual classiﬁcation. arXiv preprint. arXiv:2305.14240\nBailey, S., & Meurers, D. (2008). Diagnosing meaning errors in short answers to reading comprehension\nquestions. In Proceedings of the third workshop on innovative use of NLP for building educational\napplications (pp. 107– 115)\nBallesteros, L., & Croft, B. (1996). Dictionary methods for cross-lingual information retrieval. International\nconference on database and expert systems applications (pp. 791–801)\nBasu, S., Jacobs, C., & V anderwende, L. (2013). Powergrading: a clustering approach to amplify human\neffort for short answer grading. Transactions of the Association for Computational Linguistics, 1 ,\n391–402.\nBexte, M., Horbach, A., & Zesch, T. (2022). Similarity-based content scoringhow to make s-bert keep up\nwith bert. In Proceedings of the 17th workshop on innovative use of nlp for building educational\napplications (bea 2022) (pp. 118–123)\nBexte, M., Horbach, A., & Zesch, T. (2023). Similarity-based content scoring- a more classroom-suitable\nalternative to instance-based scoring? Findings of the association for computational linguistics: Acl\n2023 (pp. 1892–1903). Toronto, Canada: Association for Computational Linguistics. Retrieved from\nhttps://aclanthology.org/2023.ﬁndings-acl.119\nBoyd, A., Hana, J., Nicolas, L., Meurers, D., Wisniewski, K., Abel, A., & V ettori, C. (2014). The MERLIN\ncorpus: Learner language and the CEFR. LREC (pp. 1281–1288)\nCamus, L., & Filighera, A. (2020). Investigating transformers for automatic short answer grading. Interna-\ntional conference on artiﬁcial intelligence in education (pp. 43–48)\nÇınar, A., Ince, E., Gezer, M., & Yılmaz, Ö. (2020). Machine learning algorithm for grading open-ended\nphysics questions in Turkish. Education and information technologies, 25(5), 3821–3844.\nCohen, J. (1960). A coefﬁcient of agreement for nominal scales. Educational and Psychological Measure-\nment, 20(1), 37–46. Retrieved from http://epm.sagepub.com/content/20/1/37.short\nCovington, M. A., & McFall, J. D. (2010). Cutting the Gordian knot: The moving-average type-token ratio\n(MA TTR).Journal of quantitative linguistics, 17(2), 94–100.\nCui, Y ., Zhu, J., Yang, L., Fang, X., Chen, X., Wang, Y ., & Yang, E. (2022). CTAP for chinese: a lin-\nguistic complexity feature automatic calculation platform. In Proceedings of the thirteenth Language\nResources and Evaluation Conference (pp. 5525–5538)\nDevlin, J., Chang, M. -W., Lee, K., & Toutanova, K. (2019). Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the 2019 conference of the north american\nchapter of the association for computational linguistics: Human language technologies, volume 1\n(long and short papers) (pp. 4171–4186)\nDing, Y ., Horbach, A., Wang, H., Song, X., & Zesch, T. (2020). Chinese Content Scoring: Open-Access\nDatasets and Features on Different Segmentation Levels. In Proceedings of the 1st conference of the\nAsia-Paciﬁc Chapter of the Association for Computational Linguistics and the 10th international joint\nconference on Natural Language Processing (AACL-IJCNLP 2020)\nDzikovska, M. O., Nielsen, R. D., Brew, C., Leacock, C., Giampiccolo, D., Bentivogli, L., & Dang, H. T.\n(2013). Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment\nchallenge (Tech. Rep.). North Texas State University Denton\n123\nInt J Artif Intell Educ (2024) 34:1294–1320 1319\nFunayama, H., Asazuma, Y ., Matsubayashi, Y ., Mizumoto, T., & Inui, K. (2023). Reducing the cost: Cross-\nprompt pre-netuning for short answer scoring. International conference on artiﬁcial intelligence in\neducation (pp. 78–89)\nGalhardi, L., Barbosa, C. R., de Souza, R. C. T., & Brancher, J. D. (2018). Portuguese automatic short\nanswer grading. Brazilian Symposium on Computers in Education (Simpósio Brasileiro de Informática\nna Educação-SBIE) (vol. 29, pp. 1373)\nGomes Rocha, F., Rodriguez, G., Andrade, E. E. F., Guimarães, A., Gonçalves, V ., & Sabino, R. F. (2021).\nSupervised machine learning for automatic assessment of free-text answers. Advances in soft comput-\ning: 20th mexican international conference on artiﬁcial intelligence, micai 2021, Mexico City, Mexico,\nOctober 25–30, 2021, proceedings, part ii 20 (pp. 3–12)\nHeilman, M., & Madnani, N. (2015). The impact of training data on automated short answer scoring\nperformance. In Proceedings of the tenth workshop on innovative use of NLP for building educational\napplications (pp. 81–85)\nHerwanto, G. B., Sari, Y ., Prastowo, B. N., Bustoni, I. A., & Hidayatulloh, I. (2018). Ukara: A fast and\nsimple automatic short answer scoring system for Bahasa Indonesia. ICEAP , 2019(2), 48–53.\nHiggins, D., Brew, C., Heilman, M., Ziai, R., Chen, L., Cahill, A., others (2014). Is getting the right answer\njust about choosing the right words? the role of syntactically-informed features in short answer scoring.\narXiv preprint. arXiv:1403.0801\nHorbach, A., Stennmanns, S., & Zesch, T. (2018). Cross-lingual Content Scoring. In Proceedings of the\nthirteenth workshop on innovative use of NLP for building educational applications (pp. 410–419).\nNew Orleans, LA, USA: Association for Computational Linguistics. Retrieved from http://www.\naclweb.org/anthology/W18-0550\nHorbach, A., & Zesch, T. (2019). The inﬂuence of variance in learner answers on automatic content scoring.\nFrontiers in education (vol. 4, pp. 28)\nKlebanov, B. B., & Madnani, N. (2021). Automated essay scoring. Synthesis Lectures on Human Language\nTechnologies, 14(5), 1–314.\nKlementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations of words.\nProceedings of COLING 2012 (pp. 1459–1474)\nKumar, Y ., Aggarwal, S., Mahata, D., Shah, R. R., Kumaraguru, P ., & Zimmermann, R. (2019). Get it scored\nusing autosas — An automated system for scoring short answers. Proceedings of the AAAI conference\non artiﬁcial intelligence (pp. 9662–9669)\nLüdeling, A., Doolittle, S., Hirschmann, H., Schmidt, K., & Walter, M. (2008). Das Lernerkorpus Falko.\nDeutsch als Fremdsprache, 45(2), 67.\nLoukina, A., Madnani, N., & Zechner, K. (2019). The many dimensions of algorithmic fairness in educa-\ntional applications. In Proceedings of the fourteenth workshop on innovative use of NLP for building\neducational applications (pp. 1–10)\nMegyesi, B., Rudebeck, L., & V olodina, E. (2021). SweLL pseudonymization guidelines\nMendes, A., Antunes, S., Jansseen, M., & Gonçalves, A. (2016). The COPLE2 corpus: a learner corpus for\nPortuguese. In Proceedings of the tenth language resources and evaluation conference–LREC’16(pp.\n3207–3214)\nMeurers, D., Ziai, R., Ott, N., & Kopp, J. (2011). Evaluating answers to reading comprehension questions\nin context: Results for German and the role of information structure. In Proceedings of the TextInfer\n2011 workshop on textual entailment (pp. 1–9)\nMohler, M., & Mihalcea, R. (2009). Text-to-text semantic similarity for automatic short answer grading. In\nProceedings of the 12th conference of the European Chapter of the Association for Computational Lin-\nguistics (pp. 567–575). Stroudsburg, PA, USA: Association for Computational Linguistics. Retrieved\nfrom http://dl.acm.org/citation.cfm?id=1609067.1609130\nNguimkeng, P . J. (2021). Cross-lingual content scoring with a focus on French (Bachelor’s Thesis). Uni-\nversity of Duisburg-Essen\nOuahrani, L., & Bennouar, D. (2020). AR-ASAG an Arabic dataset for automatic short answer grading\nevaluation. In Proceedings of the 12th Language Resources and Evaluation Conference (pp. 2634–\n2643)\nPado, U., & Kiefer, C. (2015). Short answer grading: When sorting helps and when it doesn’t. In Proceedings\nof the fourth workshop on NLP for computer-assisted language learning (pp. 42–50)\nPedregosa, F., V aroquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., & Duchesnay, E. (2011).\nScikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830.\n123\n1320 Int J Artif Intell Educ (2024) 34:1294–1320\nPeña-López, I., et al. (2012). PISA 2012 assessment and analytical framework. Mathematics, reading,\nscience, problem solving and ﬁnancial literacy\nPires, T., Schlinger, E., & Garrette, D. (2019). How multilingual is multilingual bert? In Proceedings of the\n57th annual meeting of the association for computational linguistics\nPrettenhofer, P ., & Stein, B. (2010). Cross-language text classiﬁcation using structural correspondence\nlearning. In Proceedings of the 48th annual meeting of the association for computational linguistics\n(pp. 1118–1127)\nSawatzki, J., Schlippe, T., Benner-Wickner, M. (2021). Deep learning techniques for automatic short answer\ngrading: Predicting scores for english and german answers. International conference on artiﬁcial\nintelligence in education technology (pp. 65–75)\nSchlippe, T., & Sawatzki, J. (2022). Cross-lingual automatic short answer grading. Artiﬁcial intelligence in\neducation: Emerging technologies, models and applications (pp. 117–129). Springer\nShermis, M. D. (2014). State-of-the-art automated essay scoring: Competition, results, and future directions\nfrom a United States demonstration. Assessing Writing, 20, 53–76.\nShi, L., Mihalcea, R., Tian, M. (2010). Cross language text classiﬁcation by model translation and semi-\nsupervised learning. In Proceedings of the 2010 conference on empirical methods in Natural Language\nProcessing (pp. 1057–1067)\nV ajjala, S., & Rama, T. (2018). Experiments with universal CEFR classiﬁcation. In Proceedings of the\nthirteenth workshop on innovative use of NLP for building educational applications (pp. 147–153)\nV olodina, E., Pilán, I., Alfter, D., et al. (2016). Classiﬁcation of Swedish learner essays by CEFR levels.\nCALL communities and culture-short papers from EUROCALL, 2016, 456–461.\nWalia, T. S., Josan, G. S., & Singh, A. (2019). An efﬁcient automated answer scoring system for punjabi\nlanguage. Egyptian Informatics Journal, 20(2), 89–96.\nWeegar, R., & Idestam-Almquist, P . (2023). Reducing workload in short answer grading using machine\nlearning. International Journal of Artiﬁcial Intelligence in Education, 1–27\nWijaya, M.C. (2021). Automatic short answer grading system in indonesian language using bert machine\nlearning. Revue d’Intelligence Artiﬁcielle, 35(6)\nZehner, F., Sälzer, C., & Goldhammer, F. (2016). Automatic coding of short text responses via clustering\nin educational assessment. Educational and psychological measurement, 76(2), 280–303.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8370333313941956
    },
    {
      "name": "Machine translation",
      "score": 0.8027371168136597
    },
    {
      "name": "Natural language processing",
      "score": 0.707993745803833
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6755489110946655
    },
    {
      "name": "Transformer",
      "score": 0.6754318475723267
    },
    {
      "name": "German",
      "score": 0.5536580085754395
    },
    {
      "name": "Training set",
      "score": 0.4723026752471924
    },
    {
      "name": "Linguistics",
      "score": 0.14039182662963867
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I120691247",
      "name": "University of Hagen",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I904495901",
      "name": "Ruhr University Bochum",
      "country": "DE"
    }
  ],
  "cited_by": 7
}