{
    "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
    "url": "https://openalex.org/W4389524481",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2106483471",
            "name": "Yuxin Jiang",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2152431215",
            "name": "Chunkit Chan",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2098011449",
            "name": "Mingyang Chen",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2009336559",
            "name": "Wei Wang",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2996834813",
        "https://openalex.org/W4386841414",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W2964016283",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W2963303354",
        "https://openalex.org/W4387725212",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4283816018",
        "https://openalex.org/W3196731672",
        "https://openalex.org/W4378770815",
        "https://openalex.org/W4367000491",
        "https://openalex.org/W4365211517",
        "https://openalex.org/W4306808908",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4385570578",
        "https://openalex.org/W2963785012",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W2996610304",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W4378499145",
        "https://openalex.org/W3034957837",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3119373805",
        "https://openalex.org/W3178659068",
        "https://openalex.org/W4288348717",
        "https://openalex.org/W4385573468",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W4386510385",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4367628242",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4226408727",
        "https://openalex.org/W3174136778",
        "https://openalex.org/W2603766943",
        "https://openalex.org/W3035317912",
        "https://openalex.org/W4365601026",
        "https://openalex.org/W4389523788",
        "https://openalex.org/W4379539933",
        "https://openalex.org/W4226278401"
    ],
    "abstract": "The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher models to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any “feedback”-identifying challenging instructions where the student model's performance falls short-to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify “hard” instructions and generate new “hard” instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3134–3154\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLion: Adversarial Distillation of Proprietary Large Language Models\nYuxin Jiang1,2 Chunkit Chan2* Mingyang Chen1,2* Wei Wang1,2\n1The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China\n2The Hong Kong University of Science and Technology, Hong Kong SAR, China\n{yjiangcm, ckchancc, mchenbt}@connect.ust.hk, weiwcs@ust.hk\nAbstract\nThe practice of transferring knowledge from a\nsophisticated, proprietary large language model\n(LLM) to a compact, open-source LLM has gar-\nnered considerable attention. Previous works\nhave focused on a unidirectional knowledge\ndistillation way by aligning the responses of\nthe student model with those of the teacher\nmodels to a set of instructions. Nevertheless,\nthey overlooked the possibility of incorporat-\ning any “feedback”—identifying challenging\ninstructions where the student model’s perfor-\nmance falls short—to boost the student model’s\nproficiency iteratively. To this end, we propose\na novel adversarial distillation framework for\na more efficient knowledge transfer. Leverag-\ning the versatile role adaptability of LLMs, we\nprompt the teacher model to identify “hard” in-\nstructions and generate new “hard” instructions\nfor the student model, creating a three-stage ad-\nversarial loop of imitation, discrimination, and\ngeneration. By applying this adversarial frame-\nwork, we successfully transfer knowledge from\nChatGPT to a student model (named Lion),\nusing a mere 70k training data. Our results\nshow that Lion-13B not only achieves compara-\nble open-ended generation capabilities to Chat-\nGPT but surpasses conventional state-of-the-art\n(SOTA) instruction-tuned models like Vicuna-\n13B by 55.4% in challenging zero-shot rea-\nsoning benchmarks such as BIG-Bench Hard\n(BBH) and 16.7% on AGIEval.1\n1 Introduction\nLarge language models (LLMs) capable of follow-\ning natural language instructions have exhibited\ntremendous success in generalizing zero-shot to\nnew tasks (Mishra et al., 2022; Wei et al., 2022a).\nDue to various concerns, the most advanced LLMs,\nsuch as ChatGPT (OpenAI, 2022) and GPT-4 (Ope-\nnAI, 2023) that boasting billions of parameters, are\n∗The two authors have equal contributions.\n1Code and model can be found at https://github.com/\nYJiangcm/Lion.\nFigure 1: An illustration of the distinction between our\napproach and earlier ones. Previous methods facilitate\na one-way knowledge transfer from the teacher to the\nstudent (solid arrow). Our approach, however, incorpo-\nrates an innovative step (dashed arrow) that completes a\nloop: it enables the feedback”—identifying the student\nmodel’s weaknesses—to be relayed back to the teacher,\nin order to foster tailored learning.\ntypically proprietary, comprising both the model\nparameter and the training data. To foster increased\ntransparency regarding their intricate operational\nmechanics, a surge in research efforts focusing on\nknowledge distillation from a proprietary “teacher”\nLLM to an open-source “student” LLM. This is typ-\nically accomplished by aligning the responses of\nthe student model with those of the teacher model\nto a set of instructions, which can be manually or\nautomatically generated (Wang et al., 2022; Taori\net al., 2023; Chiang et al., 2023; Xu et al., 2023).\nHowever, previous works employ a unidirec-\ntional approach to knowledge transfer (solid arrow\nin Figure 1), where the teacher imparts knowledge\nto the student without considering any “feedback”.\n3134\nTo better illustrate this using a tangible classroom\nscenario, the “feedback” refers to identifying the\n“hard” examples or problems where the student’s\nperformance falls short. This feedback guarantees\nthat the teacher can provide bespoke training that\ncenters on “hard” examples, thereby paving the\nway for more effective and tailored learning experi-\nences for the student.\nInspired by adversarial knowledge distillation\n(AKD), which aims to iteratively improve the stu-\ndent model’s performance by learning from gener-\nated hard samples (Fang et al., 2019; Micaelli and\nStorkey, 2019a; Heo et al., 2019), we propose an\nadversarial framework for distilling a proprietary\nLLM into a compact student model. Nevertheless,\nthese AKD methodologies necessitate accessibility\nto the weights or gradients of the teacher model,\nwhich cannot be directly adapted to our setting.\nTo circumvent this problem, we leverage the un-\nparalleled role adaptability of LLMs, which can\nbe effectively employed through a diverse range\nof prompts (Sanh et al., 2022). In particular, we\nprompt the proprietary teacher LLM to serve as a\n“referee” to discriminate hard instructions where\nthere exists a significant performance discrepancy\nbetween the teacher’s and student’s responses, and\nserve as a “generator” to produce new instructions\nthat emulate the data distributions corresponding\nto the discriminated hard instructions. Our frame-\nwork, as depicted in Figure 2, consists of three\nstages in an iteration: 1) an imitation stage to align\nthe student’s response with the teacher’s response;\n2) a discrimination stage to identify hard instruc-\ntions; 3) A generation stage to produce new hard\ninstructions for escalating the challenges presented\nto the student model. In essence, our adversarial\nframework forms a positive feedback loop that effi-\nciently bootstraps the student model’s proficiency.\nTo verify the efficiency and efficacy of our\nmethod, we apply our AKD framework to transfer\nthe knowledge of ChatGPT 2 onto an open-source\nfoundation LLM, known as LLaMA (Touvron et al.,\n2023). We select Alpaca’s training data (generated\nfrom only 175 manually selected seed instructions)\nas the initial training instructions and execute three\niterations of AKD, resulting in a total of 70K data\nthat our model is trained on. We’ve christened our\nmodel as Lion, drawing inspiration from the art\nof “distillation”. By conducting extensive exper-\n2We access ChatGPT using the OpenAI API (gpt-3.5-turbo\nmodel).\niments on open-ended generation and reasoning\ndatasets, which include a total of 40 sub-tasks, our\nLion-13B showcases superior performance surpass-\ning instruction-tuned baseline models such as Vi-\ncuna (Chiang et al., 2023). Our main contributions\nare as follows:\n• Our work is the first attempt to adopt the idea\nof adversarial knowledge distillation to large\nlanguage models.\n• Our proposed framework demonstrates im-\npressive efficiency and efficacy. With instruc-\ntion tuning performed on 70k data without\nany human annotation, our Lion-13B approxi-\nmates ChatGPT’s capabilities on open-ended\ngeneration dataset and largely outperforms the\ncurrent SOTA model Vicuna-13B on reason-\ning tasks.\n• The versatility of our framework allows for\nbroad application: it is not exclusive to Chat-\nGPT but can be conveniently adapted to suit a\nvariety of other proprietary LLMs.\n2 Related Work\n2.1 Instruction-Following Language Models\nWith the impressive ability of instruction-following\nlarge language models such as ChatGPT (Ope-\nnAI, 2022) and GPT-4 (OpenAI, 2023), the tech-\nniques of instruction tuning (Wei et al., 2022b)\nhave attracted a lot of attention (Wei et al., 2022c;\nBubeck et al., 2023; Bang et al., 2023; Chan et al.,\n2023a). The early research of instruction tuning\naims to enhance the generalization ability of lan-\nguage models, allowing these models to perform\nnew tasks by comprehending task descriptions with-\nout relying on a few examplars. By fine-tuning\nthese instruction-following language models (e.g.,\nT5 (Raffel et al., 2020), FLAN (Aribandi et al.,\n2022), T0 (Sanh et al., 2022), and ExT5 (Aribandi\net al., 2022)) on multi-task datasets in the form\nof natural language phrased as instructions, these\nmodels have been shown to perform well on unseen\ntasks with the instructions.\nHowever, these models are only fine-tuned on\nsimple task-specific instructions, and it is challeng-\ning to comprehend the sophisticated and diverse\nintent of users in real-world scenarios. Therefore,\nInstructGPT (Wei et al., 2022b), ChatGPT (Ope-\nnAI, 2022), and GPT-4 (OpenAI, 2023) trained\non the diverse forms and abundant task types of\n3135\nhuman-crafted instructions annotated by a consid-\nerable number of annotators. Since these instruc-\ntions were not open-sourced, recent works such as\nAlpaca (Taori et al., 2023), Vicuna (Chiang et al.,\n2023), and WizardLM (Xu et al., 2023) investi-\ngate how to generate high-quality instructions and\nfine-tune the open-source large language model\nLLaMA (Touvron et al., 2023) with them to ap-\nproach the performance of ChatGPT.\n2.2 Knowledge Distillation\nKnowledge Distillation (KD) (Hinton et al., 2015;\nRadosavovic et al., 2018; Chen et al., 2019) repre-\nsents a crucial strategy within the sphere of model\ncompression and acceleration, wherein a compact\nstudent model is instructed to emulate the perfor-\nmance traits of a more cumbersome teacher model.\nIn practical contexts, the availability of training\ndata is often constrained due to concerns regard-\ning privacy, legality, security, or confidentiality. To\naddress the absence of training data, data-free KD\nmethods were proposed to align the student model\nto the teacher model, capitalizing on either related\nproxy data (Orekondy et al., 2019; Papernot et al.,\n2017) or synthetic data generated by learnable\ngenerators (e.g., Generative Adversarial Network\n(GAN)) (Addepalli et al., 2020; Fang et al., 2019;\nMicaelli and Storkey, 2019b) or teacher model in-\nversions (Yin et al., 2020; Chawla et al., 2021; Fang\net al., 2022). Nevertheless, these KD methodolo-\ngies necessitate the accessibility to the weights or\ngradients of the teacher model. Consequently, an\nalternative line of research, commonly denoted as\ndata-free model extraction (or stealing), endeavors\nto bridge this gap by employing zero-order estima-\ntion methodologies to approximate the authentic\ngradients of the teacher model to guide the up-\ndate of the optimized generators (Kariyappa et al.,\n2021; Truong et al., 2021). However, adapting\nthese methods to our distillation task presents two\nmain hurdles. First, these techniques are primarily\ndesigned for image-based classification tasks, as-\nsuming access to a continuous softmax vector from\nthe teacher model. Estimating zero-order gradients\nbecomes problematic in our case, as responses are\ntypically sequence-oriented. Second, developing\nan effective instruction generator capable of pro-\nducing diverse, high-quality instructions that mir-\nror the teacher model’s training data distribution\nproves more challenging than in the image domain.\n3 Methodology\nHarnessing the learned knowledge of a sophisti-\ncated teacher model T(x; θT ) where the parame-\nter θT is inaccessible, our goal is to craft a more\nlightweight student model S(x; θS). Ideally, a stu-\ndent model is optimal if the expectation of model\ndiscrepancy (which indicates the prediction differ-\nences between teacher T and student S) on the\nuniform data distribution is minimized. Inspired by\nthe success of adversarial knowledge distillation\n(AKD) (Fang et al., 2019; Micaelli and Storkey,\n2019a; Heo et al., 2019), we turn to optimize an\nupper bound of the expectation —the expectation\nof the model discrepancy on “hard samples”, where\nthe teacher T and the student Shave a relatively\nlarge performance gap. These “hard samples” are\ninclined to dominate the expectation of the model\ndiscrepancy. Thus, the overall expected model\ndiscrepancy can be effectively and efficiently re-\nduced by optimizing the student model Son these\n“hard samples”. The underlying rationale is rather\nstraightforward and can be analogized to a real-\nworld educational scenario: continuously concen-\ntrating on the “hard” knowledge that the student\nfinds challenging to grasp is the most effective man-\nner of enhancing a student’s proficiency.\nHowever, in the process of training the student\nmodel S, hard samples will be mastered by the\nstudent and converted into easy samples. Hence we\nneed a mechanism to continuously generate hard\nsamples, which can be achieved by an adversarial\nframework.\nThe whole framework of ourAdversarial Knowl-\nedge Distillation is depicted in Figure 2, which\ncontains three stages in an iteration: 1) an imi-\ntation stage to align the student’s response with\nthe teacher’s response; 2) a discrimination stage\nto identify hard samples; 3) A generation stage to\nproduce new hard samples for escalating the chal-\nlenges presented to the student model.\n3.1 Initilization\nAs shown in Figure 2, four roles and two data pools\nare established in our framework, and we will com-\nprehensively illustrate their functions later. We\ninitialize our student model Susing a foundation\nLLM such as LLaMA (Touvron et al., 2023). We\ninitialize our teacher model T, referee R, and gen-\nerator Gby using the same proprietary LLM such\nas ChatGPT (OpenAI, 2022). The multiple roles\nthat this proprietary LLM serves are accomplished\n3136\nFigure 2: The overview of our adversarial distillation framework, where we craft a compact Student LLM Sbased\non a superior proprietary LLM that serves three roles: the Teacher T, the Referee R, and the Generator G. From\nleft to right, there are three stages in an iteration: 1) Imitation; 2) Discrimination; 3) Generation.\nthrough the use of varied prompt templates. We\nstart the iteration from a given initial Train Pool\nXA = {xA\ni }i∈[1,NA], where xA\ni is the i-th instruc-\ntion in XA, and NA is the number of samples in\nXA. The Cache Pool XB is initialized as identical\nto XA, consisting of instructions to evaluate the\nperformance of Sand T.\n3.2 Imitation Stage\nTo impart the knowledge of the teacher to the stu-\ndent, we construct the instruction-response data\n{xA\ni ,T(xA\ni )}i∈[1,NA] by forward propagating in-\nstructions in the Train PoolXA through the teacher\nT. The prompt template used for model inference\nis shown in Table 10. Like the imitation training\nof previous work (Taori et al., 2023; Chiang et al.,\n2023), we fine-tune our student model Sto align\nthe response of the teacher model, by optimizing\nthe autoregressive language modeling objective.\n3.3 Discrimination Stage\nFigure 2 demonstrates that the discrimination stage\nstarts from the Cache Pool, denoted as XB. Even\nthough this pool begins with the same initializa-\ntion as the Train Pool, their uses diverge. The Train\nPool is rejuvenated by replacing its existing instruc-\ntions with freshly generated instructions, whereas\nthe Cache Pool is enriched by incorporating these\ngenerated instructions. As a result, the growing\nstorage capacity of the Cache Pool provides a more\nextensive space for evaluating the performance gap\nbetween teacher T and student S. This allows for\nmore thorough detection of hard instructions.\nIn the discrimination stage, we ask the propri-\netary LLM to serve as a “referee”, which quantifies\nthe performance gap between T and S. Specifi-\ncally, we feed each instruction xB\ni in the Cache\nPool XB through both the teacher T and student\nSto generate the outputs T(xB\ni ) and S(xB\ni ), re-\nspectively. Then we ask the referee Rto quan-\ntitatively measure the quality difference between\nteacher’s response T(xB\ni ) and student’s response\nS(xB\ni ), conditioned on xB\ni :\ndi = R(T(xB\ni ),S(xB\ni ) |xB\ni ) (1)\nThe above process is conducted by using the\nprompt template (as shown in Table 11) inspired\nby (Chiang et al., 2023), which requires the LLM\nto consider the helpfulness, relevance, accuracy,\nand level of detail of two responses and output two\nscores. To mitigate the positional bias (Wang et al.,\n2023) of the LLM referee, we conduct two runs by\nexchanging the positions of the teacher’s response\nand the student’s response and compute the final\nscore as the average of the two runs. Then di is\ncalculated as the difference between the teacher’s\nscore and the student’s score. By setting a threshold\nτ (1.0 used in our experiments), we discriminate\nhard instructions as those instructions with di ≥τ,\nand the others are identified as easy ones. Fig-\nure 3b provides a clear and intuitive demonstration\nof which kinds of instructions are discriminated\nas hard in the first iteration. Compared with the\ninstructions in the Cache Pool (Figure 3a), the dis-\n3137\n(a) Instructions of the Cache Pool\nin the first iteration.\n(b) Identified hard instructions in\nthe first iteration.\n(c) Generated hard instructions in\nthe first iteration.\nFigure 3: The top 20 most common root verbs (inner circle) and their top 4 direct noun objects (outer circle) in the\ninstructions.\ntribution of the identified hard instructions is quite\ndifferent, focusing more on complex tasks such as\nmath, coding, etc.\n3.4 Generation Stage\nAfter carefully discerning the hard instructions, the\ngeneration stage aims to produce samples that mir-\nror the data distributions corresponding to these\nchallenging directives. This process is achieved\nby employing the proprietary LLM as a generator,\ndenoted as G, leveraging its exceptional prowess\nin content creation. Inspired by (Xu et al., 2023),\nwe randomly sample an instruction from the hard\ninstructions and prompt the generator Gto generate\na new instruction. The newly generated instruction\nis required to pertain to the same domain and match\nthe task type of the sampled instruction. The tem-\nplate utilized for this prompt is exhibited in Table\n12. As shown in Figure 3c, the distribution of the\nnewly generated hard instructions appears to be\ncomparable to that of the previously identified hard\ninstructions. To mitigate the issue of catastrophic\nforgetting and to augment the diversity of the gen-\nerated instructions, we also randomly sample an\ninstruction from the easy instructions and prompt\nthe generator Gto generate a new instruction that\nbelongs to the same domain as the sampled one,\nbut exhibit a more long-tailed distribution. The\ntemplate we use to prompt this process is displayed\nin Table 13.\nIn each iteration, we define N as the total count\nof newly generated instructions and maintain a 1:1\nratio rbetween the generated hard instructions and\nthe generated easy instructions. To promote diver-\nsity, a new instruction will be deemed valid only\nif its ROUGE-L overlap with any existing instruc-\ntions in the Cache Pool is below 0.7. Finally, as\naforementioned in Section 3.3, we proceed to reju-\nvenate the Train Pool, replacing its existing instruc-\ntions with freshly generated ones. Concurrently,\nwe enrich the Cache Pool by incorporating these\nnewly generated instructions.\n3.5 Min-Max Game Interpretation\nOur adversarial knowledge distillation framework\ncan be interpreted as a dynamic min-max game:\nin the imitation stage, we fine-tune our student to\nminimize the model discrepancy between itself and\nthe teacher on hard samples; in the discrimination\nand generation stage, we craft new hard samples\nto maximize the model discrepancy, based on the\nlearning progress of the student model. This dialec-\ntic framework propels the student model towards\nuncovering otherwise hidden knowledge, paving\nthe way to complete understanding. As the training\nprogresses through several iterations, the system\nshould ideally achieve equilibrium. This is the\npoint where the student model has mastered all\nthe hard samples and the referee Rcan no longer\ndistinguish between the student Sand teacher T\nmodels. At this juncture, Sbecomes functionally\nindistinguishable from T.\n4 Experiments Setting\n4.1 Datasets\nIn our experiments, we implemented a compre-\nhensive LLM evaluation protocol that considers a\ndiverse range of abilities, such as writing, coding,\ncommonsense, math, and logical reasoning. The\ndatasets we utilized can be classified into two main\ncategories: open-ended generation and reasoning.\n3138\n4.1.1 Open-ended Generation Datasets\nVicuna-Instructions (Chiang et al., 2023) is a\nset of 80 questions spanning 9 distinct task cate-\ngories. This dataset has gained extensive usage\nin evaluating the capabilities of LLMs. Within\nour work, we examine LLMs’ performance on this\ndataset in two different settings:\n• Setting1: Following Vicuna (Chiang et al.,\n2023), we leverage GPT-4 to automatically\nassess the quality of responses (rated on a\nscale of 1 to 10) between a reference model\n(ChatGPT) and a candidate model. Subse-\nquently, we calculate the candidate model’s\nperformance as the percentage of the total\nscore it achieves compared to the reference\nmodel.\n• Setting2: A recent work (Wang et al., 2023)\npointed out that a systematic bias may exist in\nthe above-mentioned GPT-4 automatic evalua-\ntion. To mitigate this, they propose two strate-\ngies, namely Multiple Evidence Calibration\nand Balanced Position Calibration, to obtain\ncloser alignment with human judgments.\n4.1.2 Reasoning Datasets\nAGIEval (Zhong et al., 2023) is a well-known\nbenchmark that quantifies the reasoning capabil-\nity of foundation models in the context of human-\ncentric standardized exams, including college en-\ntrance exams, math competitions, lawyer qualifi-\ncation tests, etc. We choose all English multiple-\nchoice questions (8 tasks, 2,546 samples) among\nAGIEval for our experiments. The data statistics\nare shown in Table 6.\nBIG-Bench Hard (BBH) (Suzgun et al., 2022)\nconsists of a suite of challenging tasks from BIG-\nBench (Srivastava et al., 2022), designed to assess\nthe capabilities and limitations of large language\nmodels. These are the tasks on which prior lan-\nguage models underperform the average human\nrater. We choose all tasks that can be formatted\ninto multiple-choice questions (23 tasks, 5,511 sam-\nples) among BBH for our experiments. The data\nstatistics are shown in Table 7.\nSetting We evaluate reasoning capabilities under\na zero-shot setting without any exemplars and with-\nout Chain-of-Thought (CoT). For both AGIEval\nand BBH, we use the prompt format and parsing\nfollowing (Zhong et al., 2023; Mukherjee et al.,\n2023). Given the free-form response from the gen-\nerative models, only the first capital character in\nthe response is considered to compare with the\ngold answer (exact match). The result we report is\naccuracy (%).\n4.2 Baselines\nWe select five superior LLMs as baselines, includ-\ning LLaMA (Touvron et al., 2023), Alpaca (Taori\net al., 2023), WizardLM (Xu et al., 2023), Vi-\ncuna (Chiang et al., 2023), and ChatGPT (Ope-\nnAI, 2022). It is worth noting that Vicuna has con-\nsistently ranked as the top open-source language\nmodel on multiple leaderboards, such as Chatbot\nArena3. Therefore, we will conduct a comprehen-\nsive comparison with Vicuna. See detailed descrip-\ntions of these baselines in Appendix B.\n4.3 Implementation Details\nTraining Details Our student model is initialized\nusing the pre-trained LLaMA. The Train Pool and\nCache Pool are initialized with the 52K automat-\nically generated instructions from Alpaca (Taori\net al., 2023). The total number of iterations is set\nto 3, with 6K newly generated instructions added\nat each iteration. This results in a total of 70K\ndata that our model is trained on in order to make\na fair comparison with current SOTA baselines,\nincluding WizardLM and Vicuna. The training hy-\nperparameters are listed in Appendix C.\nInference Details To draw inferences from Lion\nand ChatGPT, we calibrated the temperature to 0.7\nand set the maximum generation length at 1024.\nAll other parameters adhere to their default settings.\nFor LLaMA, Alpaca, WizardLM, and Vicuna, we\nconfigured their inference parameters in line with\nthe specifications given in their respective original\npapers. When engaging with the gpt-3.5-turbo API\nfor various roles, we employ an array of hyper-\nparameters, the specifics of which can be located\nin Appendix C.\n5 Experimental Results\n5.1 Results for Open-ended Generation\nTable 1 shows the performance comparison of var-\nious models against ChatGPT as the reference\nmodel, where GPT-4 is used as a referee/rater.\nOur Lion-7B and Lion-13B remarkably outperform\ntheir counterparts under two evaluation settings.\n3https://chat.lmsys.org/?arena\n3139\nModel Setting1 Setting2 Avg.\nLLaMA-7B 58.46 59.12 58.79\nAlpaca-7B 69.29 67.20 68.25\nWizardLM-7B 89.29 86.67 87.98\nVicuna-7B 87.79 89.96 88.88\nLion-7B 94.74 92.88 93.81\nLLaMA-13B 69.23 68.21 68.72\nAlpaca-13B 76.87 74.69 75.78\nVicuna-13B 92.25 92.97 92.61\nLion-13B 96.57 100.18 98.38\nTable 1: Relative response quality (%) against ChatGPT\n(assessed by GPT-4) on Vicuna-Instructions.\nFigure 4: Relative response quality against ChatGPT on\ndiverse task categories of Vicuna-Instructions.\nNoticeably, Lion-13B shows an 8-point improve-\nment over Vicuna-13B on aggregate, achieving\n98.38% capabilities of ChatGPT.\nTo comprehensively compare with other baseline\nmodels on the capability to generate high-quality\nresponses on various types of instruction, the rel-\native response quality (Setting2) among different\ntask categories is depicted in Figure 4. Our model\nimpressively and slightly surpasses ChatGPT in the\ngeneric, knowledge, common-sense, and counter-\nfactual task categories. Furthermore, for the two\ndifficulty task categories described in the previ-\nous study (Chiang et al., 2023; Xu et al., 2023),\nour model significantly outperforms other baseline\nmodels with at least 32.32% relative score in the\nmath task category while exceeding most of the\nbaseline in the coding generation task category.\n5.2 Results for Reasoning\nAGIEval Results Table 2 presents the standard\nzero-shot performance comparison between Lion\nand baseline models on the AGIEval benchmark for\nmultiple-choice English questions. Lion demon-\nstrates significantly stronger performance com-\npared to Vicuna, surpassing it in most task cate-\ngories and achieving an average relative improve-\nment of over 16%. However, Lion-13B still signifi-\ncantly lags behind ChatGPT, only retaining 72.5%\nof its reasoning capability.\nBIG-Bench Hard Results Table 3 displays the\nzero-shot performance comparison between Lion\nand baseline models on BIG-Bench Hard with stan-\ndard zero-shot prompting. Similar to AGIEval,\nVicuna exhibits poor performance on sophisticated\nreasoning tasks within this benchmark, while Lion\nsubstantially surpasses Vicuna by around 50% on\naverage. Particularly, Lion demonstrates signifi-\ncant performance enhancements of over 100% on\ntasks involving data understanding, semantic un-\nderstanding (Disambiguation QA and Snarks), log-\nical and geometric reasoning (Logical Deduction\nand Geometric Shapes), and position reasoning\n(Tracking Shuffled Objects). Despite achieving an\naverage ability of nearly 74% compared to Chat-\nGPT on BBH, Lion-13B surpasses ChatGPT in\nseveral tasks, including Movie Recommendation,\nSnarks (identifying sarcastic sentences from two\nnearly-identical ones), and Tracking Shuffled Ob-\njects. This demonstrates the effectiveness of our\nmethod.\n6 Analyses\n6.1 Ablation Studies\nThe thresholdτ for distinguishing between hard\nand easy instructions We systematically ex-\nplored τ ranging from 0.0 to 2.0 and documented\nits influence on average performance across three\ndatasets. Table 4 reveals an optimal range of τ\nbetween 1.0 and 1.5 for all datasets. Notably, ele-\nvating τ from 0.0 to 1.0 consistently enhances per-\nformance across all datasets, indicating effective\ndifferentiation between hard and easy instructions.\nHowever, a continuous increase from 1.0 to 2.0\ngradually degrades performance due to decreased\ndiversity in hard instructions. The ablation results\ndemonstrate that our method is not quite sensitive\nto a large value of τ.\nThe ratiorof generated hard and easy instruc-\ntions We change the ratio of generated hard in-\nstructions to generated easy instructions from 1:0\n(all hard) to 0:1 (all easy) and investigate its impact\non average performance across three datasets. It\ncan be seen from Table 5 that higher ratios of hard\nto easy instructions generally lead to improved per-\nformance, with a balanced ratio of 1:1 yielding the\n3140\nTask Human ChatGPT Vicuna-7B Lion-7B Vicuna-13B Lion-13BAvg Top\nAQuA-RAT 85.0 100.0 31.9 23.2 18.5 (-20.3%) 20.1 26.0 (29.4%)\nLogiQA 86.0 95.0 35.0 21.4 31.8 (48.6%) 29.8 31.3 (5.0%)\nLSAT-AR 56.0 91.0 24.4 22.2 17.4 (-21.6%) 20.4 23.0 (12.7%)\nLSAT-LR 56.0 91.0 52.6 18.6 28.2 (51.6%) 32.6 32.6 (0.0%)\nLSAT-RC 56.0 91.0 65.4 21.9 29.4 (34.2%) 32.7 40.9 (25.1%)\nSAT-Math 66.0 94.0 42.7 21.4 20.9 (-2.3%) 28.6 29.4 (2.8%)\nSAT-English 66.0 94.0 81.1 25.7 36.4 (41.6%) 44.2 53.9 (21.9%)\nSAT-English (w/o Psg.) 66.0 94.0 44.2 26.2 27.7 (5.7%) 26.2 36.2 (38.2%)\nAverage 67.1 93.8 47.2 22.6 26.3 (16.4%) 29.3 34.2 (16.7%)\nTable 2: Zero-shot performance comparison of ChatGPT, Vicuna, and Lion on AGIEval (multiple-choice English\nquestions). We report the performance of Human, ChatGPT, and Vicuna from (Mukherjee et al., 2023). Performance\nimprovements obtained by Lion over Vicuna are shown in parenthesis.\nTask ChatGPT Vicuna-7B Lion-7B Vicuna-13B Lion-13B\nBoolean Expressions 82.8 39.2 55.2 (40.8%) 40.8 65.6 (60.8%)\nCausal Judgement 57.2 39.7 50.3 (26.7%) 42.2 43.9 (4.0%)\nDate Understanding 42.8 8.6 34.0 (295.3%) 10.0 40.4 (304.0%)\nDisambiguation QA 57.2 15.2 35.6 (134.2%) 18.4 44.8 (143.5%)\nFormal Fallacies 53.6 40.0 46.0 (15.0%) 47.2 52.4 (11.0%)\nGeometric Shapes 25.6 3.6 8.8 (144.4%) 3.6 8.8 (144.4%)\nHyperbaton 69.2 42.8 51.6 (20.6%) 44.0 56.8 (29.1%)\nLogical Deduction (5 objects) 38.8 4.8 19.6 (308.3%) 4.8 20.8 (333.3%)\nLogical Deduction (7 objects) 39.6 1.2 14.4 (1100.0%) 1.2 21.2 (1666.7%)\nLogical Deduction (3 objects) 60.4 19.6 40.4 (106.1%) 16.8 38.0 (126.2%)\nMovie Recommendation 55.4 24.4 26.8 (9.8%) 43.4 57.6 (32.7%)\nNavigate 55.6 43.6 49.2 (12.8%) 46.4 45.2 (-2.6%)\nPenguins in a Table 45.9 17.5 24.7 (41.1%) 15.1 26.7 (76.8%)\nReasoning about Colored Objects 47.6 14.0 15.2 (8.6%) 12.0 17.6 (46.7%)\nRuin Names 56.0 12.2 14.4 (18.0%) 15.7 29.2 (86.0%)\nSalient Translation Error Detection 40.8 2.0 12.0 (500.0%) 2.0 12.4 (520.0%)\nSnarks 59.0 28.0 56.2 (100.7%) 28.1 61.2 (117.8%)\nSports Understanding 79.6 40.4 48.4 (19.8%) 48.4 51.6 (6.6%)\nTemporal Sequences 35.6 21.2 24.4 (15.1%) 16.0 10.4 (-35.0%)\nTracking Shuffled Objects (5 objects) 18.4 6.4 14.4 (125.0%) 9.2 24.8 (169.6%)\nTracking Shuffled Objects (7 objects) 15.2 4.0 13.6 (240.0%) 5.6 13.2 (135.7%)\nTracking Shuffled Objects (3 objects) 31.6 26.8 34.0 (26.9%) 23.2 34.4 (48.3%)\nWeb of Lies 56.0 49.4 47.2 (-4.5%) 41.2 54.8 (33.0%)\nAverage 48.9 21.9 32.0 (45.9%) 23.3 36.2 (55.4%)\nTable 3: Zero-shot performance comparison of ChatGPT, Vicuna, and Lion on BIGBench Hard (multiple-choice\nquestions) without CoT. We report the performance of ChatGPT and Vicuna from (Mukherjee et al., 2023).\nPerformance improvements obtained by Lion over Vicuna are shown in parenthesis.\nhighest average scores.\n6.2 The Learning Dynamics of Lion\nIn Figure 5, we delve into the learning dynamics\nof Lion by visualizing its performance on AGIEval\nand BBH throughout the training iterations. The\nresults clearly demonstrate that our adversarial\nknowledge distillation framework consistently en-\nhances the performance of the student model as the\niterations progress. Notably, the most significant\nimprovement in capability occurs in the first itera-\ntion, suggesting the usefulness of the identification\nof challenging example patterns (refer Figure 3b).\n6.3 Case Studies\nTo clearly compare the generated response quality\nbetween our model and other baselines, we provide\nnine case studies sampled from Vicuna-instruction,\nAGIEval, and BBH in Appendix E. Table 14 show-\ncases the responses of various models to a math\ninstruction. It can be seen that only Lion and Chat-\nGPT provide the correct answer and follow the\ncorrect problem-solving steps. A counterfactual\ncase is shown in Table 15, where ChatGPT pro-\nvides a relevant answer that considers the potential\nimpacts of Newton focusing on biology instead of\nphysics, but it lacked details and depth. Lion, on\n3141\nThreshold τ Vicuna-Instructions (Avg.) AGIEval (Avg.) BBH (Avg.)\n0.0 89.58 22.4 26.5\n0.5 92.16 23.5 29.8\n1.0 93.81 26.3 32.0\n1.5 94.09 25.7 31.6\n2.0 92.23 24.6 31.3\nTable 4: Ablation study of the threshold τ for Lion-7B.\nRatio r Vicuna-Instructions (Avg.) AGIEval (Avg.) BBH (Avg.)\n1:0 89.60 24.3 30.8\n2:1 92.95 25.7 33.1\n1:1 93.81 26.3 32.0\n1:2 91.77 23.9 29.6\n0:1 90.02 22.1 24.3\nTable 5: Ablation study of the ratio rfor Lion-7B.\n52k 58k 62k 70k\nTraining Data Size\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38Accuracy (%)\nAGIEval (Lion-7B)\nAGIEval (Lion-13B)\nBBH (Lion-7B)\nBBH (Lion-13B)\nFigure 5: Performance of Lion-7B and Lion-13B on\nAGIEval and BBH through the training iterations.\nthe other hand, offered a more detailed and engag-\ning response that explored different possibilities\nsuch as the development of biophysics or discov-\nering new principles that could be applied to both\nfields. Lion’s response also considered the potential\nimplications of Newton’s work on motion, force,\ngravity, and thermodynamics in biology, providing\na more comprehensive answer.\n7 Conclusion\nThis paper presents an innovative adversarial\nknowledge distillation framework for distilling a\nproprietary LLM into a compact, open-source stu-\ndent model. While previous methodologies have\nconcentrated on unidirectional knowledge transfer,\nour approach seeks to integrate “feedback” into\nthe learning process. Leveraging the versatile role\nadaptability of LLMs, we prompt the proprietary\nmodel to identify “hard” instructions and generate\nnew “hard” instructions for the student model, cre-\nating a three-stage adversarial loop of imitation,\ndiscrimination, and generation. This approach al-\nlows us to refine the student model’s performance\niteratively, efficiently bootstrapping its proficiency.\nWe aspire that our model, named Lion, may serve\nas a baseline to reflect the performance of ChatGPT,\nespecially the open-source instruction-following\nlanguage model baseline for our community.\nLimitations and Discussions\nThe Model Capability We have identified that\nLion is subject to certain constraints: 1) A recent\nstudy (Gudibande et al., 2023) asserts that “model\nimitation is a false promise” since imitation models\nare adept at mimicking ChatGPT’s style but fall\nshort in improving LMs across more challenging\ntasks. While Lion still lags behind its teacher model\nChatGPT in handling intricate reasoning tasks (as\nshown in our experiments), it demonstrates promis-\ning improvements compared to previous imitation\nmodels. Therefore, our adversarial knowledge dis-\ntillation framework may provide a more effective\nway for knowledge transfer. 2) Since our training\ndata doesn’t encompass dialogues, Lion struggles\nto manage multi-turn conversations. 3) Due to com-\nputational resource constraints, Lion’s maximum\nsequence length is limited to 1024. Consequently,\nit faces challenges when dealing with long doc-\numents. Despite these limitations, we envision\nLion serving as an accessible springboard for fu-\nture research endeavors aimed at addressing these\nlimitations.\nThe Training Process To train a single student\nmodel, we request the gpt-3.5-turbo API around\n450k times, a number that is roughly 70% of\nthe WizardLM’s usage of 624k (Xu et al., 2023).\n3142\nNonetheless, this utilization incurs a considerable\nexpense, nearing $900. In contrast to methods\nlike Alpaca (Taori et al., 2023) and WizardLM\n(Xu et al., 2023), which only fine-tune the student\nmodel once, our adversarial knowledge distillation\nmethod employs iterative parametric updates to the\nstudent model. While this iterative approach in-\nevitably leads to slower iteration speed, it offers\nadditional benefits. Finally, different from tradi-\ntional adversarial knowledge distillation where the\nweights of the generator are iteratively updated, we\nuse a black-box and parameter-frozen LLM (Chat-\nGPT in our paper) to serve the role. Therefore,\nthe quality of the LLM is quite essential in the\ngeneration of new instructions.\nThe Evaluation Metrics Though automated\nevaluations leveraging GPT-4 have showcased\npromising prospects in appraising chatbot perfor-\nmance, the technique is yet to reach a level of\nmaturity and accuracy, especially considering the\npropensity of large language models to generate\nnon-existent or “hallucinated” information. Eval-\nuating the efficacy of LLM across various tasks\npresents a considerable challenge since different\ntasks require quite different expertise (Wang et al.,\n2022). Therefore, the creation of a comprehensive,\nstandardized evaluation system for chatbots is a pre-\nvailing research challenge that demands additional\nexploration and study.\nEthics Statement\nInherited Biases It is important to consider that\nthe behavior of our distilled student models may ex-\nhibit potential toxicity, biases, or privacy issues (Li\net al., 2023a,b) inherited from the larger teacher\nLLM. We anticipate that the advancements made\nin reducing anti-social behaviors in LLMs can also\nbe utilized to enhance student language models.\nLicense and Legality Based on Stanford Al-\npaca’s guidelines (Taori et al., 2023), we have deter-\nmined that the weights of Lion will be exclusively\nlicensed for research purposes in the future. Uti-\nlizing Lion’s weights alongside LLaMA’s original\nweights must adhere to Meta’s LLaMA License\nAgreement. Users are responsible for acquiring\nand utilizing LLaMA in accordance with the li-\ncense agreement.\nSafety Unlike ChatGPT (OpenAI, 2022), Lion\ndoes not rely on human feedback to mitigate unde-\nsired behaviors. Instead, Lion learns to avoid such\nbehaviors by imitating ChatGPT. However, it is\nimportant to acknowledge the potential risks asso-\nciated with using Lion for malicious purposes, espe-\ncially upon releasing its weights in the future. For\nfuture work, we aim to incorporate the technique\nof Reinforcement Learning from Human Feedback\n(RLHF) (Ouyang et al., 2022) to enhance access\ncontrol. Additionally, Meta has implemented an\naccess application process that can help regulate\nthe distribution of LLaMA models and minimize\nthe potential risks associated with their usage, pro-\nviding an alternative option.\nAcknowledgements\nW. Wang was also affiliated with Guangzhou Mu-\nnicipal Key Laboratory of Materials Informatics,\nThe Hong Kong University of Science and Tech-\nnology (Guangzhou), China. He was supported by\nHKUST(GZ) Grant G0101000028, GZU-HKUST\nJoint Research Collaboration Grant GZU22EG04,\nCCF-HuaweiDBC202302, and Guangzhou Mu-\nnicipal Science and Technology Project (No.\n2023A03J0003).\n3143\nReferences\nSravanti Addepalli, Gaurav Kumar Nayak, Anirban\nChakraborty, and Venkatesh Babu Radhakrishnan.\n2020. Degan: Data-enriching gan for retrieving rep-\nresentative samples from a trained classifier. In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, pages 3130–3137.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-\nglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,\nJai Prakash Gupta, Kai Hui, Sebastian Ruder, and\nDonald Metzler. 2022. Ext5: Towards extreme multi-\ntask scaling for transfer learning. In The Tenth In-\nternational Conference on Learning Representations,\nICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. CoRR, abs/2302.04023.\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohannes Gehrke, Eric Horvitz, Ece Kamar, Peter\nLee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg,\nHarsha Nori, Hamid Palangi, Marco Túlio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general\nintelligence: Early experiments with GPT-4. CoRR,\nabs/2303.12712.\nChunkit Chan and Tsz Ho Chan. 2023. Discourse-aware\nprompt for argument impact classification. In Pro-\nceedings of the 15th International Conference on\nMachine Learning and Computing, ICMLC 2023,\nZhuhai, China, February 17-20, 2023 , pages 165–\n171. ACM.\nChunkit Chan, Jiayang Cheng, Weiqi Wang, Yuxin\nJiang, Tianqing Fang, Xin Liu, and Yangqiu Song.\n2023a. Chatgpt evaluation on sentence level rela-\ntions: A focus on temporal, causal, and discourse\nrelations. CoRR, abs/2304.14827.\nChunkit Chan, Xin Liu, Tsz Ho Chan, Jiayang\nCheng, Yangqiu Song, Ginny Y . Wong, and Si-\nmon See. 2023b. Self-consistent narrative prompts\non abductive natural language inference. CoRR,\nabs/2309.08303.\nChunkit Chan, Xin Liu, Jiayang Cheng, Zihan Li,\nYangqiu Song, Ginny Y . Wong, and Simon See.\n2023c. Discoprompt: Path prediction prompt tun-\ning for implicit discourse relation recognition. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 35–57. Association for Computational\nLinguistics.\nAkshay Chawla, Hongxu Yin, Pavlo Molchanov, and\nJose Alvarez. 2021. Data-free knowledge distillation\nfor object detection. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vi-\nsion, pages 3289–3298.\nYen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu,\nand Jingjing Liu. 2019. Distilling knowledge\nlearned in bert for text generation. arXiv preprint\narXiv:1911.03829.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nGongfan Fang, Kanya Mo, Xinchao Wang, Jie Song,\nShitao Bei, Haofei Zhang, and Mingli Song. 2022.\nUp to 100x faster data-free knowledge distillation.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 6597–6604.\nGongfan Fang, Jie Song, Chengchao Shen, Xinchao\nWang, Da Chen, and Mingli Song. 2019. Data-free\nadversarial distillation. CoRR, abs/1912.11006.\nGoogle. 2023. Bard.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang\nGeng, Hao Liu, Pieter Abbeel, Sergey Levine, and\nDawn Song. 2023. The false promise of imitating\nproprietary llms. CoRR, abs/2305.15717.\nByeongho Heo, Minsik Lee, Sangdoo Yun, and\nJin Young Choi. 2019. Knowledge distillation with\nadversarial samples supporting decision boundary. In\nThe Thirty-Third AAAI Conference on Artificial Intel-\nligence, AAAI 2019, The Thirty-First Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2019, The Ninth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2019, Hon-\nolulu, Hawaii, USA, January 27 - February 1, 2019,\npages 3771–3778. AAAI Press.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nYuxin Jiang, Linhan Zhang, and Wei Wang. 2022. Im-\nproved universal sentence embeddings with prompt-\nbased contrastive learning and energy-based learning.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 3021–3035.\nAssociation for Computational Linguistics.\nCheng Jiayang, Lin Qiu, Tsz Ho Chan, Tianqing Fang,\nWeiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng\nGuo, Hongming Zhang, Yangqiu Song, Yue Zhang,\nand Zheng Zhang. 2023. Storyanalogy: Deriv-\ning story-level analogies from large language mod-\nels to unlock analogical understanding. CoRR,\nabs/2310.12874.\n3144\nSanjay Kariyappa, Atul Prakash, and Moinuddin K\nQureshi. 2021. Maze: Data-free model stealing at-\ntack using zeroth-order gradient estimation. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13814–13823.\nHaoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiao-\njin Zhang, Qi Hu, Chunkit Chan, and Yangqiu Song.\n2023a. Privacy in large language models: Attacks,\ndefenses and future directions.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and\nYangqiu Song. 2023b. Multi-step jailbreaking pri-\nvacy attacks on chatgpt. CoRR, abs/2304.05197.\nPaul Micaelli and Amos J. Storkey. 2019a. Zero-shot\nknowledge transfer via adversarial belief matching.\nIn Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada , pages 9547–\n9557.\nPaul Micaelli and Amos J Storkey. 2019b. Zero-shot\nknowledge transfer via adversarial belief matching.\nAdvances in Neural Information Processing Systems,\n32.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 3470–3487. Association for Com-\nputational Linguistics.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-\nhar, Sahaj Agarwal, Hamid Palangi, and Ahmed Has-\nsan Awadallah. 2023. Orca: Progressive learning\nfrom complex explanation traces of GPT-4. CoRR,\nabs/2306.02707.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nTB OpenAI. 2022. Chatgpt: Optimizing language mod-\nels for dialogue. OpenAI.\nTribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.\n2019. Knockoff nets: Stealing functionality of black-\nbox models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 4954–4963.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155.\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow,\nSomesh Jha, Z Berkay Celik, and Ananthram Swami.\n2017. Practical black-box attacks against machine\nlearning. In Proceedings of the 2017 ACM on Asia\nconference on computer and communications secu-\nrity, pages 506–519.\nIlija Radosavovic, Piotr Dollár, Ross Girshick, Georgia\nGkioxari, and Kaiming He. 2018. Data distillation:\nTowards omni-supervised learning. In Proceedings\nof the IEEE conference on computer vision and pat-\ntern recognition, pages 4119–4128.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V . Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. 2022. Challenging\nbig-bench tasks and whether chain-of-thought can\nsolve them. CoRR, abs/2210.09261.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\n3145\nJean-Baptiste Truong, Pratyush Maini, Robert J Walls,\nand Nicolas Papernot. 2021. Data-free model extrac-\ntion. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages\n4771–4780.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023. Large language models are not fair evaluators.\nCoRR, abs/2305.17926.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. CoRR,\nabs/2212.10560.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022b. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022c. Emer-\ngent abilities of large language models. CoRR,\nabs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022d. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large language\nmodels to follow complex instructions.\nHongxu Yin, Pavlo Molchanov, Jose M Alvarez,\nZhizhong Li, Arun Mallya, Derek Hoiem, Niraj K\nJha, and Jan Kautz. 2020. Dreaming to distill: Data-\nfree knowledge transfer via deepinversion. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8715–8724.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\nShuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,\nand Nan Duan. 2023. Agieval: A human-centric\nbenchmark for evaluating foundation models. CoRR,\nabs/2304.06364.\nA Data Statistics\nTable 6 and Table 7 show the data statistics of\nAGIEval and BIG-Bench Hard, respectively.\nTask # Examples # Choices\nAQuA-RAT 254 5\nLogiQA 651 4\nLSAT-AR 230 5\nLSAT-LR 510 5\nLSAT-RC 269 5\nSAT-Math 220 4\nSAT-English 206 4\nSAT-English (w/o Psg.) 206 4\nTable 6: Statistics of AGIEval dataset.\nTask # Examples # Choices\nBoolean Expressions 250 2\nCausal Judgement 187 2\nDate Understanding 250 6\nDisambiguation QA 250 4\nFormal Fallacies 250 2\nGeometric Shapes 250 11\nHyperbaton 250 2\nLogical Deduction (5 objects) 250 5\nLogical Deduction (7 objects) 250 7\nLogical Deduction (3 objects) 250 3\nMovie Recommendation 250 5\nNavigate 250 2\nPenguins in a Table 146 5\nReasoning about Colored Objects 250 18\nRuin Names 250 11\nSalient Translation Error Detection 250 6\nSnarks 178 2\nSports Understanding 250 2\nTemporal Sequences 250 4\nTracking Shuffled Objects (5 objects) 250 5\nTracking Shuffled Objects (7 objects) 250 7\nTracking Shuffled Objects (3 objects) 250 3\nWeb of Lies 250 2\nTable 7: Statistics of BIG-Bench Hard dataset.\nB Baselines\n• LLaMA (Touvron et al., 2023) is a collection\nof foundation language models ranging from\n7B to 65B parameters. It is trained on trillions\nof tokens from publicly available datasets\nand is demonstrated to outperform larger-size\nLLMs such as GPT-3 (175B) across a multi-\ntude of benchmarks. We use the official code\nfrom LLaMA 4.\n• Alpaca (Taori et al., 2023) is a project initi-\nated by Stanford University with the objec-\ntive of developing and disseminating an open-\nsource model that adeptly follows instructions.\nIt is based on LLaMA and fine-tuned on 52K\ninstruction-following examples generated by\n4https://github.com/facebookresearch/llama\n3146\nquerying OpenAI’s text-davinci-003 model.\nOn the self-instruct evaluation set, Alpaca\nmirrors text-davinci-003, but is notably more\ncompact and cost-effective to reproduce. We\nuse the official code from Alpaca 5.\n• WizardLM (Xu et al., 2023) employs LLMs\ninstead of humans to automatically mass-\nproduce open-domain instructions of various\ndifficulty levels, to improve the performance\nof LLMs. It uses an Evol-Instruct method\nto bootstrap the 52k instruction-following ex-\namples of Alapca into a larger set of 250k\nmore intricate instructions. Out of this larger\nset, 70k examples were selected to fine-tune\nLLaMA. We use WizardLM-7B-V1.0 from\nthe official code 6.\n• Vicuna (Chiang et al., 2023), a superior open-\nsource chatbot, excels in generating fluid\nand captivating responses to user queries.\nIt is based on LLaMA and fine-tuned on\n70K user-shared conversations collected from\nShareGPT, a platform designed for sharing\ninteractions with ChatGPT. Its impressive ca-\npabilities make it one of the leading open\ninstruction-following models today. Vicuna\nachieves competitive performance against pro-\nprietary models such as ChatGPT and Bard\n(Google, 2023). We use Vicuna-7B-V1.1 and\nVicuna-13B-V1.1 from FastChat 7.\n• ChatGPT (OpenAI, 2022), a product of Ope-\nnAI, is an advanced AI chatbot renowned for\nits ability to interact with users in an authenti-\ncally human and engaging manner. The chat-\nbot is built on powerful LLMs such as GPT-\n3.5 and GPT-4, which are trained on a vast\ncorpus of internet text data. ChatGPT un-\ndergoes fine-tuning via both supervised and\nreinforcement learning techniques, with the\nhuman trainers providing necessary feedback\nand direction.\nC Implementation Details\nTraining Hyperparameters The training pro-\ncess is conducted on 8 A100 GPUs. During each\niteration of adversarial knowledge distillation, the\nhyperparameters for training are shown in Table 8.\n5https://github.com/tatsu-lab/stanford_alpaca\n6https://github.com/nlpxucan/WizardLM\n7https://github.com/lm-sys/FastChat\nHyperparameter Lion-7B Lion-13B\nBatch size 128 128\nLearning rate 2e-5 2e-5\nEpoches 3 3\nMax length 1024 1024\nOptimizer AdamW AdamW\nScheduler cosine cosine\nWeight decay 0 0\nWarmup ratio 0.03 0.03\nTable 8: Training hyperparameters.\nQuerying the gpt-3.5-turbo API We use dif-\nferent sets of hyperparameters when querying the\ngpt-3.5-turbo API for different roles (Teacher, Ref-\neree, Generator). These hyperparameters are found\nto work well and we listed them in Table 9.\nRole temperature top_p beam_size (n) max_tokens\nTeacher 0.7 1.0 1 1024\nReferee 0.2 1.0 1 512\nGenerator 1.0 1.0 1 512\nTable 9: Hyperparameters for querying OpenAI gpt-\n3.5-turbo API under different roles.\nD Prompt Templates for Our Adversarial\nDistillation Framework\nFine-tuning an LLM (i.e. ChatGPT) is costly and\nintricate, human-tailored prompt templates are uti-\nlized to solve various tasks (Wei et al., 2022d; Chan\net al., 2023b,c; Jiang et al., 2022; Jiayang et al.,\n2023; Chan and Chan, 2023). The prompt template\nof the Teacher for generating responses is shown\nin Table 10. The prompt template of the Referee\nfor comparing the quality of two responses gener-\nated by two AI assistants is shown in Table 11. The\nprompt templates of the Generator for generating\nnew hard instructions and new easy instructions are\nshown in Table 12 and Table 13, respectively.\nE Case Studies\nHere we show 3 cases in Table 14, 15, and 16 to\nclearly compare the open-ended generation perfor-\nmance among various models including our Lion-\n13B, LLaMA-13B, Alpaca-13B, Vicuna-13B, and\nChatGPT.\nBesides, we show 6 cases in Table 17, 18, 19, 20,\n21, and 22 to clearly compare the reasoning capabil-\nity among various models including our Lion-13B,\nVicuna-13B, and ChatGPT. We utilize✓ and ✗ to\ndenote whether the response is correct or incorrect,\nrespectively.\n3147\nsystem content You are a helpful assistant that generates a response to a given task instruction.\nuser content\n### Instruction:\n{instruction}\n### Response:\nTable 10: Prompt template of gpt-3.5-turbo for generating responses. Note that the original instruction in Alpaca is\ncomposed of an instruction prompt and an instance input. For example, the instruction prompt is “write an abstract\nabout the following method”, and the instance input is “knowledge distillation”. For a better adaption to real-world\nscenarios, we concatenate the instruction prompt and the instruction prompt into one instruction using a line break.\nsystem content You are a helpful and precise assistant for checking the quality of the answer.\nuser content\n[Instruction]\n{instruction}\n[The Start of Assistant 1’s Answer]\n{answer_1}\n[The End of Assistant 1’s Answer]\n[The Start of Assistant 2’s Answer]\n{answer_2}\n[The End of Assistant 2’s Answer]\n[System]\nWe would like to request your feedback on the performance of two AI assistants in\nresponse to the user instruction and input displayed above.\nPlease rate the helpfulness, relevance, accuracy, and level of detail of\ntheir responses. Each assistant receives an overall score on a scale of 1 to 10,\nwhere a higher score indicates better overall performance.\nPlease first provide a comprehensive explanation of your evaluation, avoiding any\npotential bias and ensuring that the order in which the responses were presented\ndoes not affect your judgment. Then, output two lines indicating the scores for\nAssistant 1 and 2, respectively.\nOutput with the following format:\nEvaluation evidence: <your evaluation explanation here>\nScore of the Assistant 1: <score>\nScore of the Assistant 2: <score>\nTable 11: Prompt template of gpt-3.5-turbo for comparing the quality of two responses generated by two AI\nassistants.\nsystem content You are a helpful assistant.\nuser content\nI want you to act as an Instruction Creator.\nYour goal is to draw inspiration from the #Given Instruction# to create a brand new\ninstruction.\nThis new instruction should belong to the same domain and the same task type as the\n#Given Instruction#.\nThe LENGTH and difficulty level of the #Created Instruction# should be similar to\nthat of the #Given Instruction#.\nThe #Created Instruction# must be reasonable and must be understood and responded\nto by humans.\n’#Given Instruction#’, ’#Created Instruction#’, ’given instruction’ and ’created\ninstruction’ are not allowed to appear in #Created Instruction#.\n#Given Instruction#:\n{instruction}\n#Created Instruction#:\nTable 12: Prompt template of gpt-3.5-turbo for generating new hard instructions.\n3148\nsystem content You are a helpful assistant.\nuser content\nI want you to act as an Instruction Creator.\nYour goal is to draw inspiration from the #Given Instruction# to create a brand new\ninstruction.\nThis new instruction should belong to the same domain as the #Given Instruction#\nbut be even more rare.\nThe LENGTH and difficulty level of the #Created Instruction# should be similar to\nthat of the #Given Instruction#.\nThe #Created Instruction# must be reasonable and must be understood and responded\nto by humans.\n’#Given Instruction#’, ’#Created Instruction#’, ’given instruction’ and ’created\ninstruction’ are not allowed to appear in #Created Instruction#.\n#Given Instruction#:\n{instruction}\n#Created Instruction#:\nTable 13: Prompt template of gpt-3.5-turbo for generating new easy instructions.\n3149\nInstruction: If the endpoints of a line segment are(2,−2) and (10,4), what is the length of the segment?\nLion\nTo find the length of the line segment with endpoints (2,−2) and (10,4), we can use the distance\nformula. The distance formula is derived from the Pythagorean theorem and is given as:\nd=\n√\n((x2 − x1)2 + (y2 − y1)2)\nwhere (x1,y1) and (x2,y2) are the coordinates of the endpoints of the line segment.\nSubstituting the given values, we get:\nd=\n√\n((10 − 2)2 + (4− (−2))2)\nSimplifying this expression, we get:\nd=\n√\n((8)2 + 62)\nd=\n√\n(64 + 36)\nd=\n√\n(100)\nd= 10\nTherefore, the length of the line segment with endpoints (2,−2) and (10,4) is 10 units. ✓\nLLaMA The length of the line segment is 10 units. ✓\nAlpaca The length of the line segment is 8 units. ✗\nVicuna\nThe length of the line segment is the distance between the two points. To find the length, you can use the\nPythagorean theorem, which states that the sum of the squares of the distances from a point to the two\nendpoints is equal to the square of the distance between the two endpoints.\nIn this case, the distance from the point (2,−2) to the point (10,4) is given by the formula:\n√\n((x2 − x1)2 + (y2 − y1)2)\n=\n√\n((10 − 2)2 + (4− (−2))2)\n=\n√\n(182 + 42)\n=\n√\n(324 + 16)\n=\n√\n(340)\n= 17.973\nSo the length of the line segment is 17.973. ✗\nChatGPT\nTo find the length of a line segment with two endpoints (x1,y1) and (x2,y2), we use the distance\nformula:\ndistance=\n√\n((x2 − x1)2 + (y2 − y1)2)\nPlugging in the coordinates of the given endpoints, we get:\ndistance=\n√\n((10 − 2)2 + (4− (−2))2)\ndistance=\n√\n(82 + 62)\ndistance=\n√\n(64 + 36)\ndistance=\n√\n(100)\ndistance= 10\nTherefore, the length of the line segment is 10 units. ✓\nTable 14: A math case from Vicuna-Instructions.\n3150\nInstruction: What if Isaac Newton had focused on biology instead of physics?\nLion\nIsaac Newton is renowned for his contributions to physics, particularly his laws of motion and universal\ngravitation. However, what if Newton had instead chosen to focus on biology?\nNewton’s work in physics was heavily influenced by his fascination with the natural world and\nhis desire to understand how it worked. Similarly, had he chosen to study biology, he may have made\nsignificant contributions to the field. Newton’s mathematical abilities and his interest in the natural world\nwould have made him a formidable biologist.\nOne area where Newton may have made a significant impact in biology is in the field of tax-\nonomy. Newton was a keen observer of nature and had a particular interest in plants. He spent much of\nhis time studying and cataloguing the various species of plants around him. He even developed a system\nfor classifying plants based on their physical characteristics.\nIn addition to his work in taxonomy, Newton may have also made important contributions to\nthe field of physiology. He was interested in the workings of the human body and may have studied the\nmechanics of muscles and the circulatory system.\nOverall, while Isaac Newton is most well-known for his work in physics, his interests and\nabilities would have made him a formidable biologist as well. Who knows what discoveries he might\nhave made had he chosen to focus on the natural world in a different way.\nLLaMA I’m not sure. If he did so, he wouldn’t have discovered the law of gravity.\nAlpaca\nIf Isaac Newton had focused on biology instead of physics, he might have made important contributions\nto the field of biology, such as developing a scientific theory of evolution or explaining the processes of\ncell growth and development. He might also have studied and discovered new principles of genetics and\ndeveloped methods to study and treat diseases.\nVicuna\nIsaac Newton is widely regarded as one of the most influential scientists in history for his work in\nphysics, particularly for his development of the laws of motion and universal gravitation. However, it is\nworth noting that Newton also made significant contributions to other fields, including mathematics,\noptics, and natural philosophy.\nIf Newton had decided to focus on biology instead of physics, it is possible that he would\nhave made significant contributions to the field. However, it is difficult to say exactly what those\ncontributions would have been without knowing more about his specific interests and areas of focus.\nNewton was a prolific scientist and wrote extensively on a wide range of topics, including nat-\nural philosophy, mathematics, and theology. He also conducted extensive research in optics, including\nthe study of light and color, and made important contributions to the field of mathematics, including the\ndevelopment of calculus.\nIt is possible that if Newton had focused on biology, he would have made significant contribu-\ntions to the understanding of living organisms and their behavior. However, it is also possible that his\nwork in physics and mathematics would have had a greater impact on the world.\nChatGPT\nIf Isaac Newton had focused on biology instead of physics, the scientific world may be vastly different\ntoday. His advancements in mathematics, such as calculus, would have certainly been useful in the field\nof biology. Newton’s curiosity, analytical mind, and dedication to scientific inquiry would have been\nequally effective in biology as they were in physics. He may have made significant contributions to the\nfield of genetics or evolutionary theory.\nOn the other hand, his contributions to physics were so groundbreaking that the subject may\nhave been hindered without his contributions. It’s difficult to say exactly what would have happened, but\nit’s likely that the course of scientific history would have been different.\nTable 15: A counterfactual case from Vicuna-Instructions.\n3151\nInstruction: How can I develop my critical thinking skills?\nLion\nDeveloping critical thinking skills is an important aspect of personal growth and professional development. Critical thinking enables\nus to analyze and evaluate information, ideas, and arguments objectively, make informed decisions, and solve complex problems. Here\nare some strategies that can help you develop your critical thinking skills:\n1. Read widely and deeply: Reading widely exposes you to different perspectives and ideas, while reading deeply helps\nyou to understand and analyze complex concepts. Choose books that are challenging and outside your comfort zone, and take notes as\nyou read.\n2. Ask questions: Asking questions is an essential part of critical thinking. Start by asking ¨whyänd ¨how¨questions to gain\na deeper understanding of a topic. Then, ask ¨what if¨questions to explore possible outcomes and consequences.\n3. Practice analysis and evaluation: Critical thinking involves analyzing and evaluating information and arguments. Start\nby breaking down complex ideas into smaller parts, and then evaluate each part based on evidence, reasoning, and credibility.\n4. Seek diverse perspectives: Surround yourself with people who have different backgrounds, experiences, and perspec-\ntives. Listen to their ideas and opinions, and challenge your own assumptions and biases.\n5. Practice decision-making: Critical thinking is essential in decision-making. Practice making decisions by weighing the\npros and cons, considering potential outcomes, and seeking advice from others.\n6. Take courses and workshops: Enrolling in courses and workshops that focus on critical thinking, problem-solving, and\ndecision-making can provide you with the tools and techniques to develop your critical thinking skills.\nBy implementing these strategies, you can develop your critical thinking skills and become better equipped to analyze,\nevaluate, and make informed decisions in your personal and professional life.\nLLaMA\nCritical thinking is a vital skill to have, but it can be difficult to develop. Here are some ways you can start developing your critical\nthinking skills:\n1. Analyze and interpret: When you’re presented with information, try to analyze and interpret it on your own terms. When you start\nto analyze and interpret information on your own, it will become easier to do so over time.\n2. Assess your information: When you receive new information, try to assess whether it’s valid or reliable. This is important for\nevaluating and forming your own opinions.\n3. Use your intuition: Use your intuition to complement your critical thinking skills. When you pay attention to your gut feelings,\nthey’ll usually tell you when you need to double-check your information.\n4. Learn from your mistakes: When you make a mistake, try not to beat yourself up about it. Instead, learn from your mistakes and use\nthem to your advantage.\nAlpaca\nDeveloping critical thinking skills involves actively questioning assumptions, weighing evidence, and thinking analytically. It also\ninvolves being open-minded and considering various perspectives, as well as thinking logically and systematically. Practicing\nproblem-solving and creative thinking, working through scenarios and dilemmas, and engaging in thoughtful conversations are all\nways to improve critical thinking. Additionally, reading, reflecting, and writing can help increase your capacity for critical thinking.\nVicuna\nDeveloping your critical thinking skills can help you to better analyze information, make more informed decisions, and solve complex\nproblems. Here are some tips for improving your critical thinking skills:\n1. Practice active listening: Pay attention to what others are saying and try to understand their perspective.\n2. Ask questions: Be curious and ask questions to clarify information and better understand a situation.\n3. Analyze information: Take time to carefully analyze information and consider different viewpoints.\n4. Seek out diverse viewpoints: Expose yourself to a variety of viewpoints and ideas to challenge your own assumptions and biases.\n5. Practice logic and reasoning: Use logic and reasoning to evaluate arguments and make decisions.\n6. Learn from your mistakes: Be open to learning from your mistakes and using them as opportunities for growth.\n7. Read and watch a variety of sources: Read and watch a variety of sources to expand your knowledge and expose yourself to different\nperspectives.\n8. Engage in debates and discussions: Engage in debates and discussions with others to practice critical thinking and develop your\nskills.\n9. Take breaks and reflect: Take breaks and reflect on your thinking process to improve your critical thinking skills.\n10. Practice mindfulness: Mindfulness can help you to be more aware of your thoughts and biases, which can help you to think more\ncritically.\nChatGPT\nSure, here are a few practical tips to develop critical thinking skills:\n1. Ask questions: Don’t simply accept information at face value. Ask questions that allow you to evaluate the reason, perspective and\nlogic behind the information.\n2. Analyze: Take the time to analyze and evaluate the information that you receive. This will help you to identify as-\nsumptions, biases, strengths and weaknesses of an argument.\n3. Consider different perspectives: When analyzing information, try to consider different perspectives to gain a wider un-\nderstanding of any topic.\n4. Research and read extensively: Read a wide range of sources to get a better understanding of different viewpoints and\nopinions.\n5. Practice logic and reasoning: Practice thinking through problems logically by breaking them down into smaller parts.\n6. Establish a point of view: Consider developing an opinion or thesis statement, and then collect evidence to support that\nposition.\n7. Reflect on your own thinking: Consider how you arrived at a conclusion, and whether you have any biases or assump-\ntions that may have influenced your thinking.\nWith time, and practicing these skills consistently, you can become a better critical thinker.\nTable 16: A generic case from Vicuna-Instructions.\n3152\nInstruction: Q: If 120 is reduced to 96, what is the reduction percent? Answer Choices: (A)30% (B)40% (C)20%\n(D)10% (E)5% A: Among A through E, the answer is\nLion\nTo calculate the reduction percent, we need to divide the difference between the original number and the\nreduced number by the original number and then multiply by 100.\nHere, the original number is 120 and the reduced number is 96. So, the difference between\nthe two numbers is 120 - 96 = 24.\nTo calculate the percentage reduction, we divide the difference by the original number and\nthen multiply by 100:\n(24 / 120) x 100 = 20%\nTherefore, the correct answer is option (C) 20%.\nOption (A) 30%, option (B) 40%, option (D) 10%, and option (E) 5% are all incorrect. ✓\nVicuna A. 30% ✗\nChatGPT\nTo calculate the reduction percentage, we need to find the difference between the original value and the\nreduced value, and then divide it by the original value.\nThe difference between 120 and 96 is 120 - 96 = 24.\nNow, divide the difference by the original value and multiply by 100 to get the percentage re-\nduction:\n(24 / 120) * 100 = 20%\nTherefore, the correct answer is (C) 20%. ✓\nTable 17: An AQuA-RAT case from AGIEval.\nInstruction: Q: not ( True ) and ( True ) is? Answer Choices: (A) True (B) False A: Among A through B, the\nanswer is\nLion\nThe expression \"not (True) and (True)\" can be simplified as \"(not True) and (True)\" or \"False and True\".\nThe \"and\" operator has a higher precedence than the \"not\" operator, so the first expression is evaluated as\n\"False and True\", which is false.\nTherefore, the correct answer is option (B) False. ✓\nVicuna\n(B) False\n### Explanation:\nThis statement is false.\nIf the given statement is \"not (True)\" and \"True\", it would be equivalent to \"not (True) and\nTrue\", which would be true.\n(This is because negation of \"True\" is \"False\", and \"True\" and \"False\" evaluate to \"True\".)\nTherefore, the correct answer is (A) True. ✗\nChatGPT (A) True. ✗\nTable 18: A boolean expressions case from BIGBench Hard.\nInstruction: Q: If you follow these instructions, do you return to the starting point? Take 8 steps. Turn around.\nTake 8 steps. Answer Choices: (A) Yes (B) No A: Among A through B, the answer is\nLion\nAccording to the instructions given, if you follow them, you will return to the starting point. Therefore,\nthe correct answer is Option A, \"Yes\". By taking 8 steps and turning around, you will end up back where\nyou started. It is important to follow instructions carefully to achieve the desired outcome. In this case,\nfollowing these instructions will ensure that you return to the starting point. ✓\nVicuna (A) Yes ✓\nChatGPT (B) No ✗\nTable 19: A navigate case from BIGBench Hard.\n3153\nInstruction: Q: Today is Christmas Eve of 1937. What is the date tomorrow in MM/DD/YYYY? Answer Choices:\n(A) 12/11/1937 (B) 12/25/1937 (C) 01/04/1938 (D) 12/04/1937 (E) 12/25/2006 (F) 07/25/1937 A: Among A\nthrough F, the answer is\nLion\nThe correct answer is (B) 12/25/1937.\nTo arrive at this answer, we need to consider the fact that Christmas Eve is the evening before\nChristmas Day. Therefore, the date of Christmas Day is one day after Christmas Eve. In this case, the\ndate of Christmas Eve is 12/24/1937, and the date of Christmas Day is 12/25/1937. Therefore, the date\ntomorrow in MM/DD/YYYY format is 12/25/1937.\nOption (A) is incorrect because it gives the date as 12/11/1937, which is incorrect.\nOption (C) is incorrect because it gives the date as 01/04/1938, which is incorrect.\nOption (D) is incorrect because it gives the date as 12/04/1937, which is incorrect.\nOption (E) is incorrect because it gives the date as 12/25/2006, which is incorrect.\nOption (F) is incorrect because it gives the date as 07/25/1937, which is incorrect. ✓\nVicuna\n(A) 12/11/1937\n### Explanation:\nThe date tomorrow, on Christmas Eve of 1937, would be 12/11/1937. ✗\nChatGPT (C) 01/04/1938. ✗\nTable 20: A data understanding case from BIGBench Hard.\nInstruction: Q: Which statement is sarcastic? Answer Choices: (A) Hey just be happy then you won’t be depressed\nanymore (B) Hey just be happy that you won’t be depressed anymore A: Among A through B, the answer is\nLion\nThe correct answer is (B). The statement \"Hey just be happy that you won’t be depressed anymore\" is\nsarcastic because it implies that being happy is an easy and straightforward solution to depression, which\nis a complex mental health issue that requires professional help and support. The use of the word \"just\"\nalso adds a dismissive tone to the statement, suggesting that the speaker doesn’t take depression seriously.\nIt’s important to avoid using sarcasm when talking about mental health issues, as it can be hurtful and\ninvalidating to those who are struggling. Instead, it’s important to offer compassion and support and to\nencourage individuals to seek professional help if needed. ✗\nVicuna (B) Hey just be happy that you won’t be depressed anymore ✗\nChatGPT The sarcastic statement is (A) \"Hey just be happy then you won’t be depressed anymore.\" ✓\nTable 21: A snarks case from BIGBench Hard.\nInstruction: Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers\nto), or state that it is ambiguous. Sentence: The scientist collaborated with the artist, and he shared a story. Answer\nChoices: (A) The scientist shared a story (B) The artist shared a story (C) Ambiguous A: Among A through C, the\nanswer is\nLion In the given sentence, the antecedent of the pronoun \"he\" is ambiguous. \"He\" could refer to either the\nscientist or the artist, as both of them shared a story. Therefore, option (C) is the correct answer. ✓\nVicuna\n(C) Ambiguous\n### Explanation:\nThe pronoun \"he\" is ambiguous, as it could refer to either the scientist or the artist. Without\nmore context, it is impossible to determine which person is being referred to. Therefore, the answer is\n\"C - Ambiguous.\" ✓\nChatGPT\n(C) Ambiguous.\nIn the given sentence, the antecedent of the pronoun \"he\" is unclear. It could refer to either\nthe scientist or the artist because there is no clear indication of which person shared the story. Therefore,\nthe sentence is ambiguous. ✓\nTable 22: A disambiguation QA case from BIGBench Hard.\n3154"
}