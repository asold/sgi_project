{
  "title": "KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations",
  "url": "https://openalex.org/W3104108820",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A100167829",
      "name": "Fabio Massimo Zanzotto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2712222932",
      "name": "Andrea Santilli",
      "affiliations": [
        "University of Rome Tor Vergata"
      ]
    },
    {
      "id": "https://openalex.org/A3098811994",
      "name": "Leonardo Ranaldi",
      "affiliations": [
        "University of Rome Tor Vergata",
        "Marconi University"
      ]
    },
    {
      "id": "https://openalex.org/A2786487805",
      "name": "Dario Onorati",
      "affiliations": [
        "University of Rome Tor Vergata"
      ]
    },
    {
      "id": "https://openalex.org/A3105320616",
      "name": "Pierfrancesco Tommasino",
      "affiliations": [
        "University of Rome Tor Vergata"
      ]
    },
    {
      "id": "https://openalex.org/A540360537",
      "name": "Francesca Fallucchi",
      "affiliations": [
        "Marconi University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W1563088657",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W2912206855",
    "https://openalex.org/W2137607259",
    "https://openalex.org/W2963580443",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W1528620860",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W2979473749",
    "https://openalex.org/W4300822525",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2766005220",
    "https://openalex.org/W1889268436",
    "https://openalex.org/W2118373646",
    "https://openalex.org/W2969624041",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2792643794",
    "https://openalex.org/W2962790223",
    "https://openalex.org/W2237235759",
    "https://openalex.org/W1879966306",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2193700427",
    "https://openalex.org/W2157306293",
    "https://openalex.org/W2891488835",
    "https://openalex.org/W2120814856",
    "https://openalex.org/W3117738520",
    "https://openalex.org/W2104518905",
    "https://openalex.org/W1423339008",
    "https://openalex.org/W3016169217",
    "https://openalex.org/W2891582949",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2510766756",
    "https://openalex.org/W224064951",
    "https://openalex.org/W2952060378",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2965647350",
    "https://openalex.org/W2965215977",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3097609957",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963090765",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2051840895",
    "https://openalex.org/W2735807231",
    "https://openalex.org/W2970534725",
    "https://openalex.org/W1608322251",
    "https://openalex.org/W2100693535",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2952846557",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2805436580",
    "https://openalex.org/W2131297983",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1505640402",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2962998327",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2964289395",
    "https://openalex.org/W1971844566",
    "https://openalex.org/W2251939518"
  ],
  "abstract": "Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 256–267,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n256\nKERMIT: Complementing Transformer Architectures\nwith Encoders of Explicit Syntactic Interpretations\nFabio Massimo Zanzotto(∗)\nAndrea Santilli(∗)\nLeonardo Ranaldi(‡,∗)\n(∗) ART Group\nDepartment of Enterprise Engineering\nUniversity of Rome Tor Vergata\nViale del Politecnico, 1, 00133 Rome, Italy\nfabio.massimo.zanzotto@uniroma2.it\nDario Onorati(∗)\nPierfrancesco Tommasino(∗)\nFrancesca Fallucchi(‡)\n(‡) Department of Innovation\nand Information Engineering\nGuglielmo Marconi University\nVia Plinio 44, 00193 Rome, Italy\nf.fallucchi@unimarconi.it\nAbstract\nSyntactic parsers have dominated natural lan-\nguage understanding for decades. Yet, their\nsyntactic interpretations are losing centrality\nin downstream tasks due to the success of\nlarge-scale textual representation learners. In\nthis paper, we propose KERMIT (Kernel-\ninspired Encoder with Recursive Mechanism\nfor Interpretable Trees) to embed symbolic\nsyntactic parse trees into artiﬁcial neural net-\nworks and to visualize how syntax is used in\ninference. We experimented with KERMIT\npaired with two state-of-the-art transformer-\nbased universal sentence encoders (BERT and\nXLNet) and we showed that KERMIT can in-\ndeed boost their performance by effectively\nembedding human-coded universal syntactic\nrepresentations in neural networks.\n1 Introduction\nUniversal sentence embeddings (Conneau et al.,\n2018), which are task-independent, distributed sen-\ntence representations, are redesigning the way lin-\nguistic models in natural language processing are\ndeﬁned. These embeddings are usually created\nfrom scratch over large corpora without human\nsupervision (Cho et al., 2014; Kiros et al., 2015;\nConneau et al., 2017; Subramanian et al., 2018;\nCer et al., 2018) or are crafted with compositional\ndistributional semantics methods (Clark and Pul-\nman, 2007; Mitchell and Lapata, 2008; Baroni and\nZamparelli, 2010; Zanzotto et al., 2010).\nTraditional task-independent, symbolic, human-\ndeﬁned syntactic interpretations for sentences,\nwhich may be referred to as universal syntactic\ninterpretations, are losing their centrality in lan-\nguage understanding systems due to the success of\ntransformer-based neural networks (Vaswani et al.,\n2017) that have boosted performances on a wide\nvariety of linguistic tasks (Devlin et al., 2018; Liu\net al., 2019; Yang et al., 2019).\nThere is evidence that universal sentence embed-\ndings store bits of universal syntactic interpreta-\ntions. Even if not explicitly designed for encoding\nsyntax, these embeddings implicitly capture syntac-\ntic relations among words with different strategies.\nTransformers (Devlin et al., 2018; Liu et al., 2019;\nYang et al., 2019; Dai et al., 2019) seem to capture\nsyntactic relations among words by “focusing the\nattention”. Yet, to be sure that syntax is encoded,\nmany syntactic probes (Conneau et al., 2018) for\nneural networks have been designed to test for spe-\nciﬁc phenomena (Kovaleva et al., 2019; Jawahar\net al., 2019; Hewitt and Manning, 2019; Ettinger,\n2019; Goldberg, 2019) or for full syntactic trees\n(Hewitt and Manning, 2019; Mare ˇcek and Rosa,\n2019). Indeed, some syntax is correctly encoded in\nthese universal sentence embeddings.\nHowever, universal sentence embeddings encode\nsyntax in a way that is opaque and not so univer-\nsal. Firstly, and perhaps surprisingly, task-adapted\nuniversal sentence embeddings encode syntax bet-\nter than general universal sentence embeddings\n(Jawahar et al., 2019). Secondly, even if these em-\nbeddings contains syntactic information and may\nbe “just another way in which traditional syntactic\nmodels are encoded” (Fodor and Pylyshyn, 1988),\nthere is no clear view on how this information is\nencoded and, hence, on how syntactic information\nis holistically (Chalmers, 1992) used in inference.\nThen, it is difﬁcult to envisage ways to symboli-\ncally control the behavior of neural networks.\nIn this paper, we investigate whether explicit\nuniversal syntactic interpretations can be used to\nimprove state-of-the-art universal sentence embed-\ndings and to create neural network architectures\nwhere syntax decisions are less obscure and, thus,\nsyntactically explainable. For this purpose we pro-\npose KERMIT, a Kernel-inspired Encoder with\na Recursive Mechanism for Interpretable Trees,\nand KERMITviz . KERMIT is a lightweight en-\n257\nFigure 1: The KERMIT+Transformer architecture, for-\nward and interpretation pass. During the forward pass\nparse trees are passed inactive (black and white trees).\nDuring the interpretation pass activations are back-\npropagated and heat parse trees are produced (colored\ntrees).\ncoder for embedding syntax parse trees in universal-\nsyntax-encoding vectors by explicitly embedding\nsubtrees in the representation space. KERMITviz\nis a visualizer to inspect how syntax is used in tak-\ning ﬁnal decisions in speciﬁc tasks. We showed\nthat KERMIT can effectively embed different syn-\ntactic information and KERMIT viz can explain\nKERMIT’s decisions. Furthermore, paired with\nuniversal sentence embeddings, KERMIT outper-\nforms state-of-the-art models - BERT (Devlin et al.,\n2018) and XLNet (Yang et al., 2019) - in three dif-\nferent downstream tasks, albeit ﬁndings in Kuncoro\net al. (2020), showing that traditional syntactic in-\nformation is not represented in universal sentence\nembeddings.\n2 Background and Related Work\nEmbedding symbolic syntactic or structured infor-\nmation within neural networks is a very active re-\nsearch ﬁeld given the impression that using pre-\nexisting syntactic knowledge in neural networks\ncan be beneﬁcial for many tasks. Initial attempts\nhave tried to recursively encode structures in dis-\ntributed representations to use them inside neu-\nral networks (Pollack, 1990; Goller and Kuechler,\n1996). More recently, Socher et al. (2011) have\ndeﬁned the notion of Recursive Neural Networks\n(RecNN) that are Recurrent Neural Networks ap-\nplied to binary trees. Initially, these RecNNs have\nbeen used to parse sentences and not to include pre-\nexisting syntax in a ﬁnal task (Socher et al., 2011).\nThen, these RecNNs have been used to encode pre-\nexisting syntax in the speciﬁc task of Sentiment\nAnalysis (Socher et al., 2012, 2013). With the rise\nof Long Short-Term Memories (LSTMs), Tai et al.\n(2015); Zhu et al. (2015) and Zhang et al. (2016)\nindependently proposed TreeLSTM as an adapted\nversion of LTSM that may use syntactic informa-\ntion. In TreeLSTM, the LSTM is applied following\nthe structure of a binary tree instead of following an\ninput sequence. In semantic relatedness and in sen-\ntiment classiﬁcation, TreeLSTM has outperformed\nRecNN (Tai et al., 2015) by using pre-existing syn-\ntactic information. TreeLSTM has also been used\nto induce task-speciﬁc trees while learning a novel\ntask (Choi et al., 2018). Moreover, Munkhdalai\nand Yu (2017) have specialized LSTM for binary\nand n-ry trees with their Neural Tree Indexers and\nStrubell et al. (2018) have encoded syntactic in-\nformation by using multi-head attention within a\ntransformer architecture.\nHowever, there is a major problem with the meth-\nods for embedding syntactic structures in neural\nnetworks, it is unclear which parts of the parse trees\nare represented, and how. Hence, the behavior of\nneural networks that use these embeddings is ob-\nscure. It is then difﬁcult to understand what kind\nof syntactic knowledge is encoded in the different\nlayers and how this syntactic knowledge is used.\nSome initial attempts to clarify which syntac-\ntic parts are encoded in embedding vectors exist.\nZhang et al. (2018) have encoded parse trees by\nmeans of paths connecting the root of parse trees\nwith words. Yet, these attempts are still far from\ncompletely representing parse trees.\nFor a long time, structural kernel functions have\nbeen the way to exploit syntactic information in\nlearning but these functions cannot be used within\nneural networks. Kernel machines (Cristianini and\nShawe-Taylor, 2000) exploit these, generally recur-\nsive, structural kernel functions that deﬁne a sim-\nilarity measure between two trees counting com-\nmon substructures. Hence, these structural kernel\nfunctions are built over a clear, although hidden,\nspace of substructures. Structural kernels have been\ndeﬁned for both constituency-based (Collins and\nDuffy, 2002; Moschitti, 2006) and dependency-\nbased parse trees (Culotta and Sorensen, 2004). As\nunderlying spaces are well deﬁned, it is even possi-\nble to extract back substructures that are relevant in\n258\neach decision (Pighin and Moschitti, 2010). How-\never, these structural kernel functions are generally\nrecursive algorithms that hide the real underlying\nspace of features. Thus, structures are never repre-\nsented as vectors in the target representation spaces\nas these spaces are generally huge. It is generally\nimpossible then to use these clear spaces in learn-\ning with neural networks.\nIn the ﬁeld of structural kernels, distributed tree\nkernels (Zanzotto and Dell’Arciprete, 2012) have\nopened an interesting possibility. To reduce the\ncomputational cost of tree kernels, these distributed\ntree kernels embed the huge space of substruc-\ntures in a smaller space. This embedding is ob-\ntained by using recursive functions, which are lin-\near with respect to the tree size. Hence, structures\nare represented in a smaller vector in an embedded\nspace that represents the original space of struc-\ntures. Hence, DTKs open an interesting path to\ninclude clear syntactic information in neural net-\nwork architectures (Zanzotto and Ferrone, 2017;\nSantilli and Zanzotto, 2018).\n3 The model\nThis section introduces our Kernel-inspired En-\ncoder with a Recursive Mechanism for Inter-\npretable Trees (KERMIT) (Sec.3.2) along its vi-\nsualizer KERMIT viz (Sec.3.3). KERMIT is a\nlightweight encoder for universal syntactic inter-\npretations which can be used in combination with\ntransformer-based networks such as BERT (Devlin\net al., 2018) and XLNet (Yang et al., 2019) (Fig.\n1). Some preliminary notations are given in Sec-\ntion 3.1.\n3.1 Preliminary notation\nThis section ﬁxes the notation for parse trees, ran-\ndom vectors and operations on random vectors as\nthese are core representations in our model to deal\nwith universal syntactic interpretations.\nParse trees Tand parse subtreesτare recursively\nrepresented as trees t = ( r,[t1,...,t k]) where r\nis the label representing the root of the tree and\n[t1,...,t k] is the list of child trees ti. Leaves tare\nrepresented as trees t = (r,[]) with an empty list\nof children or directly as t= r.\nOn parse trees T, our model KERMIT requires\nthe deﬁnition of three sets of subtrees: the set\nN(T), the set S(T) and the set of S(T). The\nlast two sets are deﬁned according to subtrees we\nwant to model in the embeddings of the universal\nsyntactic interpretations. We use subtrees deﬁned\nin Collins and Duffy (2002). The set N(T) con-\ntains all the complete subtrees of T. Given a tree\nT and rone of its nodes, a complete subtree of T\nfrom ris the subtree rooted in rthat reaches the\nleaves, for example (see the parse tree in Fig. 1):\nN\n(\nNP\nthe chef\n)\n=\nNP\nthe chef\n,\nA\nthe\n,\nN\nchef\nThe set S(T) contains all the valid subtrees of\nT = ( r,[t1,...,t k]) as follows (r,[]) is in S(T)\nand each (r,[τ1,...,τ k]) where τi ∈S(ti) are in\nS(T), for example:\nS\n(\nNP\nthe tasty soup\n)\n=\nNP\nthe tasty soup\nNP\nA J\ntasty\nN\nsoup\nNP\nA\nthe\nJ N\nsoup\nNP\nA\nthe\nJ N\nNP\nA J\ntasty\nN\nNP\nA J N\nsoup\n]\nFinally, the set S(T) is the union of the sets S(t)\nfor all the trees t∈N(T), that is:\nS(T) =\n⋃\nt∈N(T)\nS(t)\nand it contains the subtrees used during training\nand inference.\nFinally, to build the untrained KERMIT en-\ncoder, we use the properties of random vectors\ndrawn from a multivariate Gaussian distribution\nv ∼ N(0, 1√\ndI). These vectors guarantee that\nuT v ≈0 if u ̸= v and uT u ≈1. This prop-\nerty is extremely important for interpretability. To\ncompose vectors, we use the shufﬂed circular con-\nvolution u ⊗v. If these vectors are drawn from\na multivariate Gaussian distribution, the function\nguarantees that (u ⊗v)T u ≈0, (u ⊗v)T v ≈0\nand (u⊗v) ̸= (v⊗u). This operation is a circular\nconvolution ⋆(as for Holographic Reduced Repre-\nsentations (Plate, 1995)) with a permutation matrix\nΦ: u ⊗v = u ∗Φv. This operation is extremely\nimportant for soundly composing node vectors.\n3.2 The encoder for parse trees and its\nsub-network\nKERMIT is a lightweight neural layer that allows\nthe encoding and use of universal syntactic inter-\npretations in neural networks architectures. This\nlayer has two main components. The ﬁrst compo-\nnent is the KERMIT encoder that actually encodes\nparse trees T in embedding vectors:\ny = D(T) (1)\n259\nwhich corresponds to the gray arrow and the gray\nbox in the KERMIT side of Fig. 1. The second\ncomponent is a multi-layer perceptron that exploits\nthese embedding vectors:\nz = mlp(y) (2)\nwhich corresponds to green area in the KERMIT\nside of Fig. 1.\nThe KERMIT encoder Din Eq. 1 stems from\ntree kernels (Collins and Duffy, 2002) and dis-\ntributed tree kernels (Zanzotto and Dell’Arciprete,\n2012). It makes it possible to represent parse trees\nin vector spaces Rd that embed huge spaces of\nsubtrees Rn where nis the huge number of differ-\nent subtrees. Each tree T is represented by using\nthe set of its valid subtrees S(T). The encoder is\nbased on an embedding layer for tree node labels\nxr = Wor ∈Rd and on a recursive encoding\nfunction based on the shufﬂed circular convolu-\ntion ⊗introduced by Zanzotto and Dell’Arciprete\n(2012). The embedding layer xr = Wor ∈Rd\nis an untrained encoding function that maps one-\nhot vectors r of tree node labels rto random vec-\ntors drawn from the previously introduced multi-\nvariate Gaussian distribution N(0, 1√\ndI). Hence,\nWo ∈Rm×d is a matrix of mcolumns where mis\nthe cardinality of the set of node labels and each col-\numn w(i) of the matrix Wo is w(i) ∼N(0, 1√\ndI).\nThe function D(T) is deﬁned as a the sum of re-\ncursive function Υ(t) on parse trees:\ny = D(T) =\n∑\nt∈N(T)\nΥ(t)\nwhere N(T) is the previously deﬁned set of com-\nplete subtrees of T. Then, Υ(t) is deﬁned as:\nΥ(t)=\n\n\n\n√\nλWor if t= (r,[])√\nλ(Wor + Wor ⊗Υ(t1) ⊗...⊗Υ(tk))\nif t= (r,[t1,...,t k])\nwhere 0 < λ≤1 is a decaying factor penalizing\nlarge subtrees (Collins and Duffy, 2002; Zanzotto\nand Dell’Arciprete, 2012). By implementingD(T)\nwith a dynamic algorithm, its computational cost\nis linear with respect to the nodes of the tree Tand\nthe cost of the basic function ⊗is dlog dwhere d\nis the size of the representation space Rd. In fact,\nthe circular convolution can be computed with Fast\nFourier Transformation.\nGiven its nature, the tree neural encoder has\na nice interpretation as a very simple embedding\nlayer, that is, WΥ ∈Rd×n that embeds the space\nof subtrees in a smaller space Rd. This is in line\nwith the Johnson-Lindenstrauss Transformation\n(Johnson and Lindenstrauss, 1984). Hence, D(T)\ncan be seen as the following:\ny = D(T) = WΥx (3)\nwhere x is the vector representing the set of sub-\ntrees S(T), that is, the sum of\n√\nλkxt where xt\nis one-hot vector representing t ∈S(T), λis the\ndecaying factor for penalizing large trees and kis\nthe number of nodes of the tree t. It is possible\nand easy to show that columns wi of WΥ encode\nsubtrees tas follows:\nw(i)=Γ(t(i))=\n\n\n\nWor if t(i) = (r,[])\nWor ⊗Γ(t(i)\n1 ) ⊗...⊗Γ(t(i)\nk )\nif t= (r,[t(i)\n1 ,...,t (i)\nk ])\nfor example:\nΓ(\nVP\nV NP\nA J\ntasty\nN\nsoup\n) =\nWoeV P⊗ (WoeN ⊗ WoeNP\n⊗(WoeA ⊗ (WoeJ ⊗ Woetasty)\n⊗(WoeN ⊗ Woesoup)))\nwhere\n√\nλ8 is the decay factor applied to the sample\nsubtree with 8 nodes.\nGiven the properties of the vectors E(r) ∼\nN(0, 1√\ndI) and the properties of the shufﬂed cir-\ncular convolution ⊗, it is possible to empiri-\ncally demonstrate that Γ(ti)T Γ(ti) ≈ 1 and\nΓ(ti)T Γ(tj) ≈ 0 (Plate, 1995; Zanzotto and\nDell’Arciprete, 2012). Hence, this property can\nbe used to interpret the behavior of the decision in\nthe neural network.\n3.3 Visualizing Neural Network Activation\non Syntactic Trees\nThe deﬁnitions of the KERMIT encoder make it\npossibile to devise KERMITviz , which offers pre-\ndiction interpretability (Jacovi et al., 2018) in the\ncontext of textual classiﬁcation. We propose a clear\ncausal relation for explaining (Lipton, 2016) clas-\nsiﬁcation decisions where syntax is important by\ndeﬁning heat parse trees and calculating the rele-\nvance of single subtrees with layer-wise relevance\npropagation (LRP) (Bach et al., 2015). LRP has\nalready been used in the context of explaining de-\ncisions in natural language processing tasks (Croce\net al., 2019b,a).\nHeat parse trees(HPTs), similarly to “heat trees”\nin biology (Foster et al., 2017), are heatmaps over\n260\nparse trees (see the colored tree in Fig. 1). The\nunderlying representation is an active tree t, that\nis a tree where each node t = ( r,vr,[t1,..., tk])\nhas an activation value vr ∈R associated. HPTs\nare graphical visualizations of active trees twhere\ncolors and sizes of nodes rdepend on their activa-\ntion values vr. In this way, HPTs highlight parts of\nparse trees relevant in ﬁnal decisions.\nTo draw HPTs, we compute activation valuevr\nof nodes r in active tree t by using Layer-wise\nRelevance Propagation (LRP) (Bach et al., 2015)\nand the property in Eq. 3 of the KERMIT encoder\nD. LRP is a framework which explains decisions of\na generic neural network using local redistribution\nrules that propagate back decisions to activation\nvalues of initial features. In our case, this is used\nas a sort of inverted function of the multi-layer\nperceptron in Eq. 2, that is:\nyLRP = mlp−1\nLRP (z)\nThe property in Eq. 3 enables the activation of each\nsubtree t∈T to be computed back by transposing\nthe matrix WΥ, that is:\nxLRP = WΥT yLRP\nTo make the computation feasible, WΥT is pro-\nduced on-the-ﬂy for each tree T. Finally, activa-\ntion values vr of nodes r ∈T are computed by\nsumming up values x(i)\nLRP if r∈t(i).\n4 Experiments\nWe aim to investigate whether KERMIT can be\nused to create neural network architectures where\nuniversal syntactic interpretationsare useful: (1) to\nimprove state-of-the-art universal sentence embed-\ndings, especially in computationally light environ-\nments, and (2) to syntactically explain decisions.\nThe rest of the section describes the experimen-\ntal set-up, the quantitative experimental results of\nKERMIT and discusses how KERMITviz can be\nused to explain inferences made by neural networks\nover examples.\n4.1 Experimental Set-up\nThis section describes the general experimental\nset-up of our experiments, the speciﬁc conﬁgu-\nrations adopted in the completely universal and\ntask-speciﬁc settings, the used computational archi-\ntecture and the datasets.\nThe general experimental settings are described\nhereafter. Firstly, the core of our method KERMIT\nencoder has been tested on a distributed represen-\ntation space Rd with d = 4000 with the penaliz-\ning factor λset to λ = 0.4 as this has been con-\nsidered a common value in previous works (Mos-\nchitti, 2006). Secondly, constituency parse trees\nfor KERMIT have been obtained by using Stan-\nford’s CoreNLP probabilistic context-free gram-\nmar parser (Manning et al., 2014). Thirdly, the fol-\nlowing transformer sub-networks have been used:\n(1) BERTBASE , used in the uncased setting with\nthe pre-trained English model; (2) BERT LARGE ,\nused with the same settings of BERT BASE ; and,\n(3) XLNet base cased. All the models were imple-\nmented using Huggingface’s transformers library\n(Wolf et al., 2019). The input text for BERT and\nXLNet has been preprocessed and tokenized as\nspeciﬁed in respectively in Devlin et al. (2018)\nYang et al. (2019). Fourthly, as the experiments\nare text classiﬁcation tasks, the decoder layer of\nour KERMIT+Tranformer architecture is a fully\nconnected layer with the softmax activation func-\ntion applied to the concatenation of the KERMIT\noutput and the ﬁnal [CLS] token representation\nof the selected transformer model. Finally, the\noptimizer used to train the whole architecture is\nAdamW (Loshchilov and Hutter, 2019) with the\nlearning rate set to 3e−5.\nIn the completely universal setting, KERMIT\nis composed only by the ﬁrst lightweight encoder\nlayer (grey layer in Figure 1) (KERMIT ENC ). In\nthis setting, we used BERT BASE and XLNet. To\nstudy universality, transformers’ weights are ﬁxed\nin order to avoid the representation drifting to-\nward the data distribution of the task. More-\nover, we also experimented with BERT BASE -\nReverse and BERT BASE -Random to understand\nwhether syntactic or structural information is im-\nportant for the speciﬁc task. In fact, BERT BASE -\nReverse is BERTBASE with a reversed text as in-\nput and BERT BASE -Random is BERT BASE with\na randomly shufﬂed text as input. Compar-\ning BERT BASE with BERT BASE -Reverse and\nBERTBASE -Random is in itself an extremely im-\nportant test as it offers also a way to determine if\nsyntactic information is useful for a speciﬁc task.\nThe KERMIT+Tranformer is trained with a batch\nsize of 125 for 50 epochs. In addition, each exper-\niment has been repeated 5 times with 5 different\nﬁxed seeds to assess the statistical signiﬁcance of\nexperimental results. This setting is designed to\nasses whether universal syntactic interpretations\n261\nModel AGNews Yelp Review DBPedia Yelp Polarity\nXLNet 79.11(±0.12)⋆ 46.26(±0.13)⋆ 92.46(±0.09)⋆ 81.99(±0.15)⋆\nBERTBASE 82.88(±0.09)⋄ 42.90(±0.05)⋄ 97.11(±0.27)⋄ 79.21(±0.50)⋄\nBERTBASE -Reverse 79.72(±0.11) 38 .14(±0.09) 90 .46(±0.09) 72 .23(±0.50)\nBERTBASE -Random 80.39(±0.20) 38 .15(±0.08) 91 .55(±0.20) 71 .02(±0.50)\nKERMITENC 25.23(±0.14) 49 .58(±0.10) 69 .10(±0.06) 85 .91(±0.03)\nKERMITENC +XLNet 77.88(±0.12)⋆ 53.72(±0.14)⋆ 94.51(±0.05)⋆ 88.99(±0.17)⋆\nKERMITENC +BERTBASE 77.02(±0.13)⋄ 52.02(±0.06)⋄ 97.73(±0.16)⋄ 87.58(±0.17)⋄\nTable 1: Universal Setting - Average accuracy and standard deviation on four text classiﬁcation tasks. Results de-\nrive from 5 runs and⋆and ⋄indicate a statistically signiﬁcant difference between two results with a 95% conﬁdence\nlevel with the sign test.\nadd different information with respect to univer-\nsal sentence embeddings and whether universal\nsyntactic interpretations are a viable solution to in-\ncrease the performance of neural networks in light\ncomputational systems.\nIn the task-adapted setting, we used two dif-\nferent architecture of BERT, BERT BASE and\nBERTLARGE , and we trained different layers of\nthese architectures. In this way, BERT may adapt\nthe universal sentence embeddings to include task-\nspeciﬁc information which is the speciﬁc lexicon\nthat may drive syntactic analysis. For the KER-\nMIT side of the architecture, we used two different\nmulti-layer perceptrons: (1) a funnel MLP with two\nlinear layers that brings the 4,000 units of the KER-\nMIT encoder down to 200 units with an intermedi-\nate level of 300 units (KERMIT⊿); (2) a diamond\nMLP with four linear layers forming a diamond\nshape: 4,000 units, 5,000 units, 8,000 units, 5,000\nunits and, ﬁnally 4,000 units (KERMIT ⋄). Both\nKERMIT⊿ and KERMIT ⋄ have ReLU (Agarap,\n2018) activation functions and dropout (Srivastava\net al., 2014) set to 0.25 for each layer. Due to\nthe computational demand of these architectures\nand these experiments, we used the heavy system\nand we trained the overall model in two settings:\na one-epoch training session and a normal train-\ning session. In the one-epoch training session, we\ntrained the architecture with 1 epoch (Komatsuzaki,\n2019) to avoid overﬁtting and to guarantee the pos-\nsibility of having a relatively light computational\nburden. In the normal training session, we trained\nthe architecture for 5 epochs. The batch size for\nthese two settings was 32.\nWe experimented with two hardware systems: a\nlight system and a heavy system. The light system\nis an affordable old desktop consisting of a 4 Cores\nIntel Xeon E3-1230 CPU with 62 Gb of RAM and\n1 Nvidia 1070 GPU with 8Gb of onboard memory.\nThe heavy system is a more expensive, dedicated\nserver consisting of an IBM PowerPC 32 Cores\nCPU with 256 Gb of RAM and 2 Nvidia V100\nGPUs with 32Gb of on board memory each.\nTo verify our model, we experimented with four\nclassiﬁcation tasks 1 (Zhang et al., 2015) which\nshould be sensitive to syntactic information. The\ntasks include: (1) AGNews, a news classiﬁcation\ntask with 4 target classes; (2) DBPedia, a classi-\nﬁcation task over wikipedia with 14 classes; (3)\nYelp Polarity, a binary sentiment classiﬁcation task\nof Yelp reviews; and (4) Yelp Review, a sentiment\nclassiﬁcation task with 5 classes. Given the com-\nputational constraints of the light system setting,\nwe created a smaller version of the original train-\ning datasets by randomly sampling 11% of the ex-\namples and keeping the datasets balanced as the\noriginal versions.\nFor reproducibility, the source code of our exper-\niments is publically available2.\n4.2 Results and Discussion\nResults from the completely universal experimen-\ntal setting suggest that universal syntactic interpre-\ntations complement syntax in universal sentence\nembeddings. This conclusion is derived from the\nfollowing observations of Table 1, which reports\nresults in terms of the accuracy of the different\nmodels based on the different datasets. All these\nexperiments were carried out on the light system.\nFirstly, syntactic or structural information seems\nto be relevant in three out of four tasks. Syntac-\ntic information in AGNews seems to be irrelevant\nas there is a small difference in results between\nBERTBASE , on the one side, with 82.88(±0.09)\nand BERTBASE -Reverse with 79.72(±0.11) and\n1http://goo.gl/JyCnZq\n2The code is available at https://github.com/\nART-Group-it/KERMIT\n262\nSetting: 1 Epoch learning\n4 6 8 10 12\n90\n91\n92\n93\nLayer\nAccuracy\nAgNews\n4 6 8 10 12\n53\n54\n55\n56\n57\nLayer\nYelp Review\n4 6 8 10 12\n89\n90\n91\n92\nLayer\nYelp Polarity\nBERTBASE\nKERMITENC +BERTBASE\nKERMIT⊿+BERTBASE\nKERMIT⋄+BERTBASE\nSetting: 5 Epoch learning (compared with the best of 1-epoch learning setting)\n4 6 8 10 12\n90\n91\n92\n93\nLayer\nAccuracy\nAgNews\n4 6 8 10 12\n52\n53\n54\n55\n56\n57\nLayer\nYelp Review\n4 6 8 10 12\n89\n90\n91\n92\nLayer\nYelp Polarity\nBERTBASE\nKERMITENC +BERTBASE\nKERMIT⊿+BERTBASE\nKERMIT⋄+BERTBASE\nBest of 1-Epoch\nFigure 2: Comparison between KERMIT+BERT and BERT when training layers in BERT: Accuracy vs. Learned\nLayers in two different learning conﬁgurations - 1-Epoch and 5-Epoch training\nBERTBASE -Random with 80.39(±0.20), on the\nother. This small difference suggests that the order\nof words in the text is not particularly relevant and\nthe classiﬁcation is made on the lexical level. This\njustiﬁes the very poor result from KERMITENC in\nthis dataset, that is 25.23(±0.14).\nSecondly, KERMIT ENC alone outperforms\nBERTBASE and XLNet in two cases where syn-\ntactic information is relevant, that is, 49.58(±0.1)\nvs. 42.90(±0.05) and 46.26(±0.13) in Yelp\nReview and 85.91(±0.03) vs. 79.21(±0.5)\nand 81.99(±0.15) in Yelp polarity. Hence,\nKERMITENC provides a good model for includ-\ning universal syntactic interpretations in a neu-\nral network architecture. However, KERMITENC\nperformed worse with respect to XLNet and\nBERTBASE in DBPedia even if syntactic infor-\nmation seems to be useful. This may be justi-\nﬁed as both XLNet and BERTBASE are trained on\nWikipedia, thus universal sentence embeddings are\nalready adapted to the speciﬁc dataset.\nThirdly, in the three cases where syntactic infor-\nmation is relevant (Yelp Review, Yelp Polarity and\nDBPedia), the complete KERMIT+Transformer\noutperforms the model that is based only on the\nrelated Transformer, and the difference is statisti-\ncally signiﬁcant: 53.72( ±0.14) vs. 46.26( ±0.13)\nin Yelp Review, 94.51(±0.05) vs. 92.46( ±0.09)\nin DBPedia and 88.99( ±0.17) vs. 81.99( ±0.15)\nin Yelp Polarity for XLNet and 52.02( ±0.06)\nvs. 42.90( ±0.05) in Yelp Review, 97.73( ±0.16)\nvs. 97.11( ±0.27) in DBPedia and 87.58( ±0.17)\nvs. 79.21(±0.50) in Yelp Polarity for BERTBASE .\nEven in DBPedia where transformers’ embeddings\nare pretrained, KERMIT+Transformer outperforms\nthe model based only on the related transformer.\nThis last observation is a very important indi-\ncation and, together with the other observations,\nconﬁrms that universal sentence embeddings en-\ncode different syntactic information with respect\nto that deﬁned in universal syntactic interpreta-\ntions. Moreover, our KERMIT encoder allows neu-\nral networks to positively use universal syntactic\ninterpretations. Hence, using universal syntactic\ninterpretations is a viable solution also when only\nlight computational systems are available.\nExperiments in the task adapted setting: (1)\nshow that universal syntactic interpretation is still\nuseful even when universal sentence embeddings\nare adapted to the speciﬁc task; (2) conﬁrm the con-\nclusions of Jawahar et al. (2019) that universal sen-\ntence embeddings better capture syntactic phenom-\nena when the middle layers of BERT are learned\nover the task. The results of these experiments are\nplotted in Figure 2 where system accuracy is plot-\nted against the number of BERT’s learned layers\nstarting from the output layer. In fact, it seems\nthat different BERT’s layers encode different in-\n263\nYelp Review Rating: 4. Yelp Review Rating: 3\nKERMITviz\nBERTviz\nFigure 3: KERMIT viz vs. BERTviz: Comparing interpretations over KERMIT and over BERT on two sample\nsentences of Yelp Review where the wordbut is correlated or not with the ﬁnal polarity.\nformation Jawahar et al. (2019). Hence, learning\ndifferent layers in a speciﬁc setting means adapting\nthat kind of information. We experimented with\ntwo sub-settings: (1) a computationally lighter set-\nting where training is done only for 1 epoch; (2) a\nmore expensive setting where training is done for\n5 epochs.\nOur results in the task adapted setting conﬁrms\nthat BERT adapts universal sentence embeddings\nto include a better syntactic model when its weights\nin different layers are trained over the speciﬁc cor-\npus. Moreover, as shown in Jawahar et al. (2019),\nlayers in the middle cover better syntactic phenom-\nena. In fact, when BERT learns up to the 8th layer,\nBERT’s accuracy seems to come closer to the best\nmodel including universal syntactic interpretations\n(see Figure 2) . This suggests that more syntax is\nencoded in BERT.\nAll these experiments were performed also using\nBERTLARGE in place of BERTBASE , but in all the\nexperiments results were worse compared to the\nbase version, therefore not reported in the paper.\nWhen syntax matters, that is, in Yelp Review and\nin Yelp Polarity, KERMIT is able to exploituniver-\nsal syntactic interpretation to compensate for miss-\ning syntactic information in the task-adapted sen-\ntence embeddings of a trained BERT. In fact, KER-\nMIT+BERT outperforms a trained BERT BASE in\nboth the 1-epoch and 5-epoch settings for any num-\nber of trained layers (see Figure 2). In the 1-\nepoch setting, KERMIT⋄+BERTBASE outperforms\nBERTBASE and all the other conﬁgurations. In the\n5-epoch setting, KERMITENC +BERTBASE is the\nbest model.\nMoreover, KERMIT-based models behave better\nwith less training. In fact, KERMIT-based models\nlearned in the 1-epoch setting, outperform models\nlearned in the 5-epoch setting. Plots in Figure 2\nreport the best 1-epoch setting model in the plots\nof the 5-setting model. This can be linked to the\nfact that KERMIT with more parameters overﬁts\non training. In fact, KERMITENC +BERTBASE out-\nperforms the funnel and diamond KERMIT-based\nsystems. KERMITENC has fewer parameters than\nKERMIT⊿ and KERMIT⋄.\nFinally, we explored the interpretative power of\nKERMITviz comparing it with the transformer vi-\nsualizer BERTviz (Vig, 2019). We focused on two\nexamples of Yelp Reviews where the coordinating\nconjunction but plays an important role (see Fig.\n3): (1) “Unique food, great atmosphere, pricey\nbut worth a trip for special occasions. ”; (2) “The\nboba drink was terrible, but the shaved ice was\ngood. ”. The two sentences have 4 and 3 as ratings,\nrespectively. In fact, the but in the ﬁrst sentence\nintroduces a coordinated sentence that does not\n264\nchange the rating. On the contrary, the but in the\nsecond sentence introduces a coordinated sentence\nbut the shaved ice was good that radically changes\nthe polarity. In the case of BERTviz, this causal\nrelationship is extremely difﬁcult to grasp from the\nvisual representation. In fact, BERTviz is a good\nvisualization mechanism for seeing how models as-\nsign weights to different input elements (Bahdanau\net al., 2015; Belinkov and Glass, 2019), but it is\nextremely obscure in explaining causal relations\nin classiﬁcation predictions (Wiegreffe and Pinter,\n2019). Instead, KERMITviz with its tree heat maps\nshow exactly that the but and the related syntac-\ntic structure is irrelevant in the ﬁrst sentence and\nextremely relevant in the second. Hence, our heat\nparse trees can be useful to draw the causal relation\nbetween the decision and the information used.\n5 Conclusions\nUniversal syntactic interpretationsare valuable lan-\nguage interpretations, which have been developed\nin years of study. In this paper, we introduced\nKERMIT to show that these interpretations can\nbe effectively used in combination with univer-\nsal sentence embeddings produced from scratch.\nMoreover, KERMITviz allows us to explain how\nsyntactic information is used in classiﬁcation de-\ncisions within networks combining KERMIT, on\nthe one side, and BERT or XLNet on the other.\nWe also showed that KERMIT can be easily used\nin situations where training large transformers is\nextremely difﬁcult.\nAs KERMIT has a clear description of the used\nsyntactic subtrees and gives the possibility of vi-\nsualizing how syntactic information is exploited\nduring inference, it opens the possibility of devis-\ning models to include explicit syntactic inference\nrules in the training process.\nFinally, KERMIT is in the line of research of\nHuman-in-the-Loop Artiﬁcial Intelligence (Zan-\nzotto, 2019), since it gives the opportunity to track\nhow human knowledge is used by learning algo-\nrithms.\nReferences\nAbien Fred Agarap. 2018. Deep Learning using Recti-\nﬁed Linear Units (ReLU). CoRR, abs/1803.0.\nSebastian Bach, Alexander Binder, Gr ´egoire Mon-\ntavon, Frederick Klauschen, Klaus Robert M ¨uller,\nand Wojciech Samek. 2015. On pixel-wise explana-\ntions for non-linear classiﬁer decisions by layer-wise\nrelevance propagation. PLoS ONE, 10(7):1–46.\nDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by\njointly learning to align and translate. 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015 - Conference Track Proceedings , pages\n1–15.\nMarco Baroni and Roberto Zamparelli. 2010. Nouns\nare Vectors, Adjectives are Matrices: Representing\nAdjective-Noun Constructions in Semantic Space.\nIn Proceedings of the 2010 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1183–1193, Cambridge, MA. Association for Com-\nputational Linguistics.\nYonatan Belinkov and James Glass. 2019. Analysis\nMethods in Neural Language Processing: A Survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nDaniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-C ´espedes, Steve Yuan, Chris Tar,\nYun Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal sentence encoder for English.\nEMNLP 2018 - Conference on Empirical Methods in\nNatural Language Processing: System Demonstra-\ntions, Proceedings, pages 169–174.\nDavid J Chalmers. 1992. Syntactic Transformations\non Distributed Representations. In Noel Sharkey,\neditor, Connectionist Natural Language Processing:\nReadings from Connection Science , pages 46–55.\nSpringer Netherlands, Dordrecht.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations using{RNN}Encoder{–\n}Decoder for Statistical Machine Translation. In\nProceedings of the 2014 Conference on Empir-\nical Methods in Natural Language Processing\n({EMNLP}), pages 1724–1734, Doha, Qatar. Asso-\nciation for Computational Linguistics.\nJihun Choi, Kang Min Yoo, and Sang Goo Lee. 2018.\nLearning to compose task-speciﬁc tree structures.\nIn 32nd AAAI Conference on Artiﬁcial Intelligence,\nAAAI 2018, pages 5094–5101.\nStephen Clark and Stephen Pulman. 2007. Combin-\ning Symbolic and Distributional Models of Meaning.\nIn Proceedings of the AAAI Spring Symposium on\nQuantum Interaction, Stanford, CA, 2007, pages 52–\n55.\n265\nMichael Collins and Nigel Duffy. 2002. New Ranking\nAlgorithms for Parsing and Tagging: Kernels over\nDiscrete Structures, and the V oted Perceptron. In\nProceedings of {ACL}02.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In EMNLP 2017 -\nConference on Empirical Methods in Natural Lan-\nguage Processing, Proceedings, pages 670–680.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. ACL\n2018 - 56th Annual Meeting of the Association for\nComputational Linguistics, Proceedings of the Con-\nference (Long Papers), 1:2126–2136.\nNello Cristianini and John Shawe-Taylor. 2000. An In-\ntroduction to Support Vector Machines and Other\nKernel-based Learning Methods . Cambridge Uni-\nversity Press.\nDanilo Croce, Daniele Rossini, and Roberto Basili.\n2019a. Auditing Deep Learning processes through\nKernel-based Explanatory Models. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4037–4046, Hong\nKong, China. Association for Computational Lin-\nguistics.\nDanilo Croce, Daniele Rossini, and Roberto Basili.\n2019b. Neural embeddings: Accurate and readable\ninferences based on semantic kernels. Natural Lan-\nguage Engineering, 25(4):519–541.\nAron Culotta and Jeffrey Sorensen. 2004. Dependency\ntree kernels for relation extraction. In Proceedings\nof the 42nd Annual Meeting on Association for Com-\nputational Linguistics - ACL ’04, pages 423–es, Mor-\nristown, NJ, USA. Association for Computational\nLinguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive Language Models Be-\nyond a Fixed-Length Context. CoRR, abs/1901.0.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. {BERT:}Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. CoRR, abs/1810.0.\nAllyson Ettinger. 2019. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models.\nJerry A. Fodor and Zenon W. Pylyshyn. 1988. Connec-\ntionism and cognitive architecture: A critical analy-\nsis. Cognition, 28(1-2):3–71.\nZachary S L Foster, Thomas J Sharpton, and Niklaus J\nGr¨unwald. 2017. Metacoder: An {R}package for\nvisualization and manipulation of community taxo-\nnomic diversity data. PLoS Computational Biology,\n13(2).\nYoav Goldberg. 2019. Assessing BERT’s Syntactic\nAbilities.\nChristoph Goller and Andreas Kuechler. 1996. Learn-\ning task-dependent distributed representations by\nbackpropagation through structure. In IEEE Interna-\ntional Conference on Neural Networks - Conference\nProceedings, volume 1, pages 347–352. IEEE.\nJohn Hewitt and Christopher D Manning. 2019. {A}\nStructural Probe for Finding Syntax in Word Repre-\nsentations. In Proceedings of the 2019 Conference\nof the North {A}merican Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAlon Jacovi, Oren Sar Shalom, and Yoav Goldberg.\n2018. Understanding Convolutional Neural Net-\nworks for Text Classiﬁcation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n56–65, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\nGanesh Jawahar, , Beno ˆıt Sagot, , and Djam ´e Seddah.\n2019. What Does BERT Learn about the Struc-\nture of Language? In Proceedings of the Confer-\nence of the Association for Computational Linguis-\ntics, pages 3651–3657. Association for Computa-\ntional Linguistics (ACL).\nW Johnson and J Lindenstrauss. 1984. Extensions of\nLipschitz mappings into a Hilbert space. Contemp.\nMath., 26:189–206.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S Zemel, Antonio Torralba, Raquel Urtasun,\nand Sanja Fidler. 2015. Skip-Thought Vectors. In\nProceedings of the 28th International Conference on\nNeural Information Processing Systems - Volume 2 ,\nNIPS’15, page 3294–3302, Cambridge, MA, USA.\nMIT Press.\nAran Komatsuzaki. 2019. One Epoch Is All You Need.\npages 1–13.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the Dark Secrets\nof BERT.\nAdhiguna Kuncoro, Lingpeng Kong, Daniel Fried,\nDani Yogatama, Laura Rimell, Chris Dyer, and Phil\nBlunsom. 2020. Syntactic Structure Distillation Pre-\ntraining For Bidirectional Encoders.\nZachary C. Lipton. 2016. The Mythos of Model In-\nterpretability. ICML Workshop on Human Inter-\npretability in Machine Learning, 61(Whi):36–43.\n266\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: {A}Robustly Optimized {BERT}Pre-\ntraining Approach. CoRR, abs/1907.1.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. 7th International Con-\nference on Learning Representations, ICLR 2019.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The {S}tanford {C}ore{NLP}Natural Lan-\nguage Processing Toolkit. In Proceedings of 52nd\nAnnual Meeting of the Association for Computa-\ntional Linguistics: System Demonstrations , pages\n55–60, Baltimore, Maryland. Association for Com-\nputational Linguistics.\nDavid Mare ˇcek and Rudolf Rosa. 2019. Extract-\ning Syntactic Trees from Transformer Encoder Self-\nAttentions. pages 347–349.\nJeff Mitchell and Mirella Lapata. 2008. Vector-based\nModels of Semantic Composition. In Proceedings\nof ACL-08: HLT , pages 236–244, Columbus, Ohio.\nAssociation for Computational Linguistics.\nAlessandro Moschitti. 2006. Making Tree Kernels\npractical for Natural Language Learning. In Pro-\nceedings of EACL’06. Trento, Italy.\nTsendsuren Munkhdalai and Hong Yu. 2017. Neural\nTree Indexers for Text Understanding. In Proceed-\nings of the conference of the Association for Com-\nputational Linguistics, volume 1, pages 11–21. NIH\nPublic Access.\nDaniele Pighin and Alessandro Moschitti. 2010. On\nReverse Feature Engineering of Syntactic Tree Ker-\nnels. In Conference on Natural Language Learning\n(CoNLL-2010), Uppsala, Sweden.\nT A Plate. 1995. Holographic reduced representations.\nIEEE Transactions on Neural Networks , 6(3):623–\n641.\nJordan B. Pollack. 1990. Recursive distributed repre-\nsentations. Artiﬁcial Intelligence, 46(1-2):77–105.\nAndrea Santilli and Fabio Massimo Zanzotto. 2018.\nSyntNN at SemEval-2018 Task 2: is Syntax Useful\nfor Emoji Prediction? Embedding Syntactic Trees\nin Multi Layer Perceptrons. In Proceedings of The\n12th International Workshop on Semantic Evalua-\ntion, pages 477–481, New Orleans, Louisiana. As-\nsociation for Computational Linguistics (ACL).\nRichard Socher, Brody Huval, Christopher D Manning,\nand Andrew Y Ng. 2012. Semantic composition-\nality through recursive matrix-vector spaces. In\nEMNLP-CoNLL 2012 - 2012 Joint Conference on\nEmpirical Methods in Natural Language Process-\ning and Computational Natural Language Learning,\nProceedings of the Conference, pages 1201–1211.\nRichard Socher, Cliff Chiung Yu Lin, Andrew Y Ng,\nand Christopher D Manning. 2011. Parsing natu-\nral scenes and natural language with recursive neu-\nral networks. In Proceedings of the 28th Inter-\nnational Conference on Machine Learning, ICML\n2011, pages 129–136.\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In EMNLP 2013 - 2013 Conference on Empir-\nical Methods in Natural Language Processing, Pro-\nceedings of the Conference, pages 1631–1642. Asso-\nciation for Computational Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A Simple Way to Prevent Neural Networks\nfrom Overﬁtting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5027–5038.\nSandeep Subramanian, Adam Trischler, Yoshua Ben-\ngio, and Christopher J Pal. 2018. Learning gen-\neral purpose distributed sentence representations\nvia large scale multitask learning. In 6th Inter-\nnational Conference on Learning Representations,\nICLR 2018 - Conference Track Proceedings.\nKai Sheng Tai, Richard Socher, and Christopher D\nManning. 2015. Improved Semantic Representa-\ntions From Tree-Structured Long Short-Term Mem-\nory Networks. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), volume 1, pages 1556–1566, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NIPS.\nJesse Vig. 2019. A multiscale visualization of atten-\ntion in the transformer model. ACL 2019 - 57th An-\nnual Meeting of the Association for Computational\nLinguistics, Proceedings of System Demonstrations ,\npages 37–42.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention\nis not not Explanation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 11–20, Stroudsburg, PA,\nUSA. Association for Computational Linguistics.\n267\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. ArXiv, abs/1910.0.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In 33rd Conference on\nNeural Information Processing Systems (NeurIPS\n2019), volume abs/1906.0, pages 5754–5764.\nFabio Massimo Zanzotto. 2019. Viewpoint: Human-in-\nthe-loop Artiﬁcial Intelligence. J. Artif. Intell. Res.,\n64:243–252.\nFabio Massimo Zanzotto and Lorenzo Dell’Arciprete.\n2012. Distributed tree kernels. In Proceedings\nof the 29th International Conference on Machine\nLearning, ICML 2012, volume 1, pages 193–200.\nFabio Massimo Zanzotto and Lorenzo Ferrone. 2017.\nCan we explain natural language inference decisions\ntaken with neural networks? Inference rules in dis-\ntributed representations. In Proceedings of the Inter-\nnational Joint Conference on Neural Networks, vol-\nume 2017-May, pages 3680–3687.\nFabio Massimo Zanzotto, Ioannis Korkontzelos,\nFrancesca Fallucchi, and Suresh Manandhar. 2010.\nEstimating linear models for compositional distri-\nbutional semantics. In Coling 2010 - 23rd Inter-\nnational Conference on Computational Linguistics,\nProceedings of the Conference , volume 2, pages\n1263–1271.\nRichong Zhang, Zhiyuan Hu, Hongyu Guo, and Yongyi\nMao. 2018. Syntax encoding with application in\nauthorship attribution. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 2742–2753. Associa-\ntion for Computational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level Convolutional Networks for Text\nClassiﬁcation. In C Cortes, N D Lawrence, D D\nLee, M Sugiyama, and R Garnett, editors, Advances\nin Neural Information Processing Systems 28, pages\n649–657. Curran Associates, Inc.\nXingxing Zhang, Liang Lu, and Mirella Lapata. 2016.\nTop-down tree long short-term memory networks.\nIn 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL HLT 2016 -\nProceedings of the Conference, pages 310–320.\nXiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.\n2015. Long short-term memory over recursive struc-\ntures. In 32nd International Conference on Machine\nLearning, ICML 2015, volume 2, pages 1604–1612.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7462915778160095
    },
    {
      "name": "Transformer",
      "score": 0.6987507343292236
    },
    {
      "name": "Encoder",
      "score": 0.4976785480976105
    },
    {
      "name": "Natural language processing",
      "score": 0.44061994552612305
    },
    {
      "name": "Programming language",
      "score": 0.41386714577674866
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33342882990837097
    },
    {
      "name": "Electrical engineering",
      "score": 0.09600377082824707
    },
    {
      "name": "Voltage",
      "score": 0.09496414661407471
    },
    {
      "name": "Engineering",
      "score": 0.0898813009262085
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I116067653",
      "name": "University of Rome Tor Vergata",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I2801039369",
      "name": "Marconi University",
      "country": "IT"
    }
  ],
  "cited_by": 44
}