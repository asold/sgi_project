{
  "title": "Cost-effective Distillation of Large Language Models",
  "url": "https://openalex.org/W4385571682",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2110451824",
      "name": "Sayantan Dasgupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2188741563",
      "name": "Trevor Cohn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096893938",
      "name": "Timothy Baldwin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4297943940",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3016339201",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W1618905105",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3156323190",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W4297816198",
    "https://openalex.org/W4285202066",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4230471307",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2005245664",
    "https://openalex.org/W3176120057",
    "https://openalex.org/W4287725215",
    "https://openalex.org/W2251939518"
  ],
  "abstract": "Knowledge distillation (KD) involves training a small \"student\" model to replicate the strong performance of a high-capacity \"teacher\" model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task- or architecture-specific and lack generalizability. Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets. Here we propose an approach for improving KD through a novel distillation loss agnostic to the task and model architecture. We successfully apply our method to the distillation of the BERT-base and achieve highly competitive results from the distilled student across a range of GLUE tasks, especially for tasks with smaller datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 7346–7354\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCost-effective Distillation of Large Language Models\nSayantan Dasgupta, Trevor Cohn∗ and Timothy Baldwin\nSchool of Computing & Information Systems\nUniversity of Melbourne\nsayandg@umich.edu, {trevor.cohn, tbaldwin}@unimelb.edu.au\nAbstract\nKnowledge distillation (KD) involves train-\ning a small “student” model to replicate\nthe strong performance of a high-capacity\n“teacher” model, enabling efficient deploy-\nment in resource-constrained settings. Top-\nperforming methods tend to be task- or\narchitecture-specific and lack generalizability.\nSeveral existing approaches require pretraining\nof the teacher on task-specific datasets, which\ncan be costly for large and unstable for small\ndatasets. Here we propose an approach for im-\nproving KD through a novel distillation loss\nagnostic to the task and model architecture. We\nsuccessfully apply our method to the distilla-\ntion of the BERT-base and achieve highly com-\npetitive results from the distilled student across\na range of GLUE tasks, especially for tasks\nwith smaller datasets.1\n1 Introduction\nAn unfortunate problem affecting large language\nmodels, such as BERT (Devlin et al., 2018) or GPT\n(Radford et al., 2019), is their high compute costs,\nas a consequence of their complex architectures and\nvast numbers of parameters. This is particularly ap-\nparent in initial (pre)training, but also impacts the\ncost of fine-tuning to specific tasks, and the practi-\ncality of their deployment on resource-constrained\nedge devices (Sun et al., 2020). Knowledge distilla-\ntion (KD; Hinton et al. (2014)) attempts to mitigate\nthese concerns through learning a small “student”\nmodel to replicate the behaviour of a larger, un-\nwieldy “teacher”. The idea is that much of the\nperformance of the teacher can be captured by the\nstudent, despite it having many fewer parameters,\nand thereby better portability.\nSeveral distillation methods have been proposed\nfor large language models, including DistilBert\n(Sanh et al., 2019), which distills the 12-layer\n∗Now at Google DeepMind.\n1Code available at https://github.com/Sayan21/MAKD\nBERT transformer (Devlin et al., 2018) into a\n6 layer student model with only a small loss in\nthe performance on downstream tasks. Broadly,\nexisting KD approaches are either architecture-\nspecific or agnostic. The former group includes\nJiao et al. (2020) and Sun et al. (2019a) which\nincorporate a loss term to encourage matching\nhidden between teacher and student, and thus re-\nquiring aligned teacher and student architectures.\nApproaches like Turc et al. (2019), on the other\nhand are architecture-agnostic, treating the teacher\nmodel as a black box using only the logits from\nlanguage modelling heads for distillation it into a\nsmaller LM. There are numerous advantages to the\narchitecture-agnostic approach: (1) it is possible to\ndistill a teacher model into a different student ar-\nchitecture, e.g. Tang et al. (2019) distills the BERT\ntransformer into a simple single-layer Bi-LSTM;\nand (2) it frees the student to use different inference\ntechniques, e.g., to better handle long sequences\n(Xiong et al., 2021; Vyas et al., 2020).\nWhile the training of large language models in-\ncurs substantial compute resources – for instance\nthe training cost of GPT3 (Brown et al., 2020) was\nestimated at $4.6 million using Nvidia Tesla V100\nGPUs (Sharir et al., 2020). the cost of pretraining\na given model is incurred only once. On the other\nhand, practitioners apply models to specific tasks,\noften involving fine-tuning of the LLM on their\ntask-specific datasets, after which fine-tuned LLMs\nare then distilled into smaller LLMs for faster infer-\nence on real-time applications. This process incurs\nmore modest compute costs, however, given the\nmyriad of different applications, the process is re-\npeated many times, meaning the aggregate cost can\nbe significant, rivaling the cost of pre-training. 2\nIf we consider the per-instance training cost, fine-\ntuning is as costly as pre-training. Arguably this\n2Witness the explosion of BERT fine-tuning papers in the\nliterature, and OpenAI’s claim that GPT3 is being used in 300\napplications: https://openai.com/blog/gpt-3-apps.\n7346\n(a) Standard KD Approach\n(b) Our Approach\nFigure 1: Comparison of our KD approach against stan-\ndard KD. The red font signifies a computationally in-\ntensive step. DLM represents the large generic corpora\nsuch as Wiki or BookCorpus, whereas DT represents\nthe smaller task-specific corpora. The steps in the dotted\nbox are performed by practitioners, whereas the rest are\non-off performed by the authors of LLMs.\nis less of an issue for small datasets, as the fine-\ntuning costs will be also be small, however in this\nsetting fine-tuning can be unstable (Zhang et al.,\n2020) because there are not enough data points to\nreliably tune the parameters.\nIn this paper, we propose an architecture-\nagnostic approach for LLM distillation to eliminate\nthe fine-tuning step. The standard KD for an LLM\nis shown in Figure 1(a), whereas our approach cor-\nresponds to Figure 1(b). The boxes with red-font\nstand for computationally expensive steps. The\nboxes in the dotted line are replicated by the prac-\ntitioners and contribute to the major cost, whereas\nthe boxes outside represent a one-off cost and can\nbe ignored as such. We show the derivation of\nour approach along with its convergence properties,\nand then we describe our training strategy. We fi-\nnally demonstrate the effectiveness of our approach\nbased on distilling BERT models evaluated against\nthe GLUE benchmark (Wang et al., 2018).\n2 Methodology\nWe follow the empirical risk management frame-\nwork for deriving our KD approach. For simplicity,\nwe assume temperature τ = 1from the original def-\ninition in (Hinton et al., 2014). Let us assume that\nfor a problem over a domain X,Y , the Bayesian\noptimal probabilities are p0(x) = E[Y|X = x].\nThen the ideal KD loss is a loss between the stu-\ndent probabilities f(X) and p0(X) is l(f,p0), and\nthe optimal student is\nf0 = arg min\nf∈F\nEX[l(f(X),p0(X))] , (1)\nnote we use f,p and f(x),p(x) interchangeably.\nGiven that we do not know p0, the best we can\ndo is to train a teacher from a function classFover\nsome loss to find an estimate ˆp. We replace p0(X)\nin the loss by the empirical distribution, ˆp(X), to\narrive at the KD loss EX[l(f(X),ˆp(X))]. This\nis the KD loss defined over the entire population\nof X,Y . Given a training set Dof ndata points\n{xi,yi}n\ni=1, we can estimate it as\nED[l(f(X),ˆp(X))] = 1\nn\nN∑\ni=1\nl(f(xi),ˆp(xi)) .\nThis is the typical KD loss used in Hinton et al.\n(2014), also known as Vanilla KD. The loss\nl(f,ˆp) is usually Kullbach-Liebler (KL) diver-\ngence DKL(ˆp∥f) for τ = 1 , or the squared\ndifference of the logits. If the student ob-\ntained from optimizing the KD loss is ˆf =\narg minf∈F\n∑N\ni=1 l(f(xi),ˆp(xi)), then with high\nprobability (Dao et al., 2020) it satisfies\n∥ˆf−f0∥2\nn = O\n(1\nn + ∥ˆp−p0∥2\nn + δn(F,p0)2\n)\n,\n(2)\nwhere ∥·∥ stands for the L2 norm of the differ-\nence between the parameters of the two classifica-\ntion functions. δn(F,p0)2 is the local Rademacher\nradius of the class of function F, and is usu-\nally convex when Fis the family of neural net-\nwork or kernel functions (Dao et al., 2020). It\nis specific to the classification function class of\nthe teacher and is a constant when the teacher is\nfixed. The student error ∥f −f0∥n thus depends\non the second order teacher error ∥ˆp−p0∥2\nn =\n1\nn\n∑n\ni=1 ∥ˆp(xi) −p0(xi)∥2\n2.\n2.1 Taylor Series Expansion of the Loss\nLet us first start with a scalar p∈[0,1]. If L(p) is\na convex loss on p, then the following inequality\nholds (Böhning and Lindsay, 1988),\nL(p) ≤L(ˆp) + (p−ˆp)dL(p)\ndp\n⏐⏐⏐\np=ˆp\n+ 1\n2(p−ˆp)2C\n(3)\nwhere C = arg maxp\nd2L(p)\ndp2 is the maximum\ncurvature of the loss w.r.t. the entire domain of\np. For example, for a binary cross entropy loss\n7347\nMethod Pre-training ( DLM) Task-specific ( DT ) Architecture-agnostic\nDistilBERT (Sanh et al., 2019) BERT-base (truncated) + KD Fine-tuning No\nPatient-KD (Sun et al., 2019a) BERT-base (truncated) Patient-KD No\nStudentBERT (Turc et al., 2019) LM pretraining Vanilla KD Yes\nTinyBERT4 (Jiao et al., 2020) KD with loss between attention\nmatrices & hidden layers\nKD with data augmentation\nw.r.t fine-tuned BERT-base\nNo\nMobileBERT (Sun et al., 2020) KD with layer transfer loss Fine-tuning No\nEnhanced KD (ours) LM pretraining KD with Taylor series Yes\nTable 1: Detail of the two stages performed during KD under different approaches\nL(p) =−ylog(p) −(1 −y) log(1−p),\nC = arg max\np\n(y\np2 + 1 −y\n(1 −p)2\n)\n. (4)\nObserve that C →∞ as p→0 or p→1.\nNow, when p∈[0,1]K is a vector of probabili-\nties for Kclasses, we can extend the result to\nL(p) ≤L(ˆp)+\n⣨\np−ˆp,dL(p)\ndp\n⏐⏐⏐\np=ˆp\n⟩\n+ 1\n2∥p−ˆp∥2\n2C\nwith Cnow being the maximum value of the deter-\nminant of the Hessian, which is equivalent to the\ncurvature of the loss. This is also similar to the in-\nequalities for a β-smooth convex function (Bubeck\net al., 2015, §3.2). However, the constant βis not\nreally informative, unlike our case where we can\nconnect Cto the curvature of the loss,\nC = arg max\np\ndet\n⏐⏐⏐d2L(p)\ndp2\n⏐⏐⏐. (5)\nComing back to KD, if we assume the teacher\nprobabilities are p∈[0,1]K and the student prob-\nabilities are f ∈[0,1]K, then the vanilla KD loss\nis defined as l(f,p). As long as l(f,p) is convex\nw.r.t. to p, the following inequality holds,\nl(f,p0) ≤l(f,ˆp) +⟨p0 −ˆp,∇ˆpl(f,ˆp)⟩\n+ 1\n2∥p0 −ˆp∥2\n2C(f)\nNow we replace the derivatives with the partial\nderivatives as ∇ˆpl(f,ˆp) = ∂l(p)\n∂p\n⏐⏐⏐\np=ˆp\n. The maxi-\nmum curvature will be a function of the student\nprobabilities f,\nC(f) = arg max\np\ndet\n⏐⏐⏐∂2l(f,p)\n∂p2\n⏐⏐⏐. (6)\nRecall that l(f,p0) is the ideal KD loss, as de-\nfined in Equation (1). Although we cannot estimate\nit, we can now obtain an upper bound on it and\nminimize this upper bound in our algorithm.\nThe most common KD loss used in the literature\nis the KL divergence between the student and the\nteacher probabilities DKL(ˆp∥f), when we keep\nτ = 1. For KL divergence l(f,ˆp) =∑ ˆplog(ˆp/f)\nthe first order derivative is,\n∇ˆpl(f,ˆp) = 1 + log ˆp−log f\nand C(f) = arg maxp ∇2\nˆpl(f,ˆp) will not contain\nany term involving f. This means we can exclude\nthis term from KD. Removing the constant terms,\nthe loss function becomes,\nl(f,p0) ≤l(f,ˆp) +⟨p0 −ˆp,−log(f)⟩ (7)\nAs we do not have knowledge of p0, we cannot\ncompute the loss directly. But we can take an unbi-\nased estimate of p0 as yfrom the training data D,\nenabling the computation of the Taylor series term.\nAs such, our KD loss is,\nLKD = Ex,y∼D[l(f,ˆp)] +⟨y−ˆp,−log(f)⟩\n≥Ex,y∼D[l(f,p0)] (8)\nFollowing Mackey et al. (2018), an\nO(n−1/(2k+2)) estimate of the teacher p with k\nNeyman orthogonal factors gives us an O(1/√n)\nestimation of the student f. For Vanilla KD (i.e.\nk = 0), we see in Equation (2) that an O(1/√n)\nestimation of the student must have a O(1/√n)\nestimation of the teacher p, which is a more\nconservative requirement. The Taylor series term\nsatisfies the condition of the first-order orthogonal\nterm ( k = 1). That means now a O(1/n1/4)\nestimate of teacher error ∥p−p0∥n is enough to\ngive us an O(1/√n) bound of the student error\n∥f −f0∥n. O(1/n1/4) is a weaker convergence\nguarantee than O(1/√n). This simply means now\nwe can train a good student even from a weaker\nestimate of the teacher.\n7348\nFinally, combining this with the explicit classi-\nfication loss Lclass for the student, the overall loss\nfunction for some λ∈[0,1] is\nL= λLclass + (1−λ)LKD (9)\n3 Training Strategy\nExisting methods generally rely on a two-stage\napproach: (1) pre-train the student model on the\nentire or a truncated part of the same dataset as\nthe teacher (DLM ), and (2) perform fine-tuning or\nKD on a task-specific dataset ( DT ). This avoids\nthe costly fine-tuning of BERT on task-specific\ndatasets. For example, Turc et al. (2019) and Sun\net al. (2019a) perform simple pretraining of the\nstudent model on DLM , while Sanh et al. (2019)\nand Jiao et al. (2020) perform KD on DLM . While\nSanh et al. (2019) and Sun et al. (2020) only per-\nform output layer fine-tuning on the task-specific\ndataset, others perform KD on DT . The details of\nthe different stages of training are summarized in\nTable 1.\nTo test our method, we choose to perform KD\non BERT language models (DLM ) from Hugging-\nface (Wolf et al., 2020) and perform KD using\nonly the task-specific dataset DT . We do not use\na fine-tuned teacher on the task-specific dataset.\nFine-tuning of BERT is not only expensive but\nmay be unstable for small datasets (Zhang et al.,\n2020). While the teachers without fine-tuning will\nbe weak, as described in Section 2.1, our proposed\napproach is designed to be robust to this.\n4 Experiments\nWe use datasets from GLUE (Wang et al., 2018) for\nour experiments, specifically: SST-2 (Socher et al.,\n2013) for sentiment classification; MRPC (Dolan\nand Brockett, 2005), QQP, and STS-B for para-\nphrase similarity matching (Conneau and Kiela,\n2018); and MNLI (Williams et al., 2018), QNLI\n(Rajpurkar et al., 2016), and RTE (Wang et al.,\n2018) for natural language inference. We use KL\ndivergence loss and the first Taylor series term (see\nEquation 8). For datasets with real-valued outputs,\nwe can use Platt scaling (Platt et al., 1999) with a\nsigmoid function centered at the mean to convert it\nto a probability. For example, for STSB the output\nis a real number between 0 and 5, which we con-\nvert the target tinto a probability via Platt Scaling\np= 1/(1 + exp(−(t−2.5))).\nThe teacher model is BERT-base (Devlin et al.,\n2018), with 109 million parameters across 12 lay-\ners, and 768d hidden states. We conduct exper-\niments for three student models as listed in Ta-\nble 2. We take our baseline results for Vanilla\nKD from the corresponding student model in Turc\net al. (2019). We present results for our method\nbased on: (a) a 4-layer student model, which we\ncompare with the 4-layer TinyBERT model (Jiao\net al., 2020) and MobileBERT (Sun et al., 2020);3\nand (b) a 6-layer student model, which we simi-\nlarly compare against 6-layer TinyBert and Distil-\nBERT (Sanh et al., 2019) models. We constrain\nall experiments to run on a single RTX-3090 GPU\nwith 24GB RAM. The benchmark TinyBERT, Mo-\nbileBERT, and Distilbert models were downloaded\nfrom the Huggingface repository (Wolf et al., 2020)\nand used without further modification. We present\nthe results of 6-layer TinyBERT from Zhou et al.\n(2022).\nThe only hyper-parameter we optimize with our\nmethod is λ, in the range [0,1] at a step-size of 0.1,\nwith a fixed temperature of τ = 1and learning rate\nof η= 5×10−5 (for the Adam optimizer).\nIn the results in Table 2, we register improve-\nments in the GLUE metrics using the modified loss\nfor all our student architectures against the base-\nline of Vanilla KD (Turc et al., 2019). Relative to\nthe other KD methods, we get consistently better\nresults for smaller datasets like MRPC, RTE, and\nSTSB, but are slightly below the best KD mod-\nels for the larger datasets, noting that these are all\narchitecture-specific and rely on additional fine-\ntuning or data augmentation. The effect of dataset\nsize follows from the theory in Equation (2), which\nshows that the teacher error typically follows the\nsample complexity ∥p0 −ˆp∥n ∈O(n−1/(2k+2)),\nwith k = 0 being the best case (Mackey et al.,\n2018). The difference between p0 and ˆpis large for\nsmaller n, and this teacher error in turn reflects in\nthe student error in Vanilla KD. I.e., our technique\nfor expanding the loss makes a large difference for\nsmaller n.\nTinyBERT is overall the strongest performer for\nlarger datasets (>10K samples) but achieves this\nusing expensive task-specific fine-tuning and data\naugmentation. Data augmentation helps single-\nsentence tasks more than paired tasks because it is\ndifficult to align the extra data in a pair according to\n3MobileBERT uses a 6-layer architecture, but has similar\n#parameters as our 4-layer model.\n7349\nTask # of P(M) QQP MNLI (m/mm) SST-2 QNLI MRPC RTE STSB\n# of Training Samples (in K) 363.8 392.7 67.3 104.7 3.7 2.5 5.7\nBERT base 109 87.9 84.6/84.9 93.0 91.2 90.4 71.4 89.8\nVanilla KD (2 x 128) 4 62.2 70.2/70.3 83.2 81.5 71.1 57.2 73.6\nOur method (2 x 128) 4 64.4 71.7/70.5 83.4 81.6 72.1 62.1 76.2\nVanilla KD (4 x 312) 15 66.5 75.4/74.9 87.6 84.8 83.2 62.6 77.1\nMobileBERTTINY 15 68.9 81.5/81.6 91.7 89.5 87.9 65.1 80.1\nTinyBERT†\n4 (4 x 312) 15 71.3 82.5/81.8 91.9 87.7 86.4 66.6 80.4\nOur method (4 x 312) 15 68.8 80.6/80.1 89.9 86.5 88.1 66.7 82.2\nVanilla KD (6 x 768) 66 70.7 82.8/82.2 91.0 88.9 86.8 65.3 81.0\nDistilBERT (6 x 768) 66 70.1 82.6/81.3 92.5 88.9 86.9 58.4 81.3\nTinyBERT†\n6 (6 x 368) 66 71.6 84.6/83.2 93.1 90.4 87.3 66.8 83.7\nOur method (6 x 768) 66 71.4 82.8/82.5 91.6 89.3 89.0 67.5 84.0\nTable 2: Results for different student models on the GLUE test dataset, with result blocks grouping models of the\nsame architecture and parameter base. The Vanilla KD results are of the corresponding student model from (Turc\net al., 2019). The numbers in parenthesis are the number of layers and hidden states, respectively. The second\ncolumn indicates the number of parameters (millions). The scores mentioned are F1 score for QQP & MRPC,\nPearson’s correlation for STSB, and accuracy for the rest of the datasets. † TinyBERT uses additional unlabelled\ndata compared to the other methods, conferring an advantage.\nthe task. This is why TinyBert performs better than\neven BERT-base for SST2. We achieved the best re-\nsults over tasks with small datasets, which is where\ntask-specific KD is more difficult. The simplicity\nof our approach also makes it compatible with KD\nfor more complex tasks like machine translation\n(Wang et al., 2021). A fairer comparison would be\nagainst the results of TinyBert without data aug-\nmentation, but those results were not reported in\ntheir publication.\n5 Conclusion\nWe have proposed a general approach to improve\nKD on language models. We constrain the experi-\nments on BERT mainly due lack of benchmarks on\nother LLMs as well as resource limitations. But any\nLLM distillation will show a similar trend. Existing\nKD methods are highly customized to the specifics\nof the teacher model, and require additional pre-\ntraining, fine-tuning, or data augmentation. Our\napproach is much simpler and agnostic to both ar-\nchitecture and task. We ran our experiments on\nan RTX3090 GPU with 24GB RAM which cost\nonly $0.11 an hour, which is considerably cheap\ncompared to other approaches that include teacher\nfine-tuning. We showed that our method is partic-\nularly effective on small datasets, and competitive\nwith other KD methods which are much more com-\nputationally intensive and tailored to the teacher.\nA possible reason could be since the fine-tuning\nof BERT on small datasets like MRPC, STSB, or\nRTE can be unstable (Zhang et al., 2020), eliminat-\ning it makes the KD more robust and improves the\nresults. All other methods such as TinyBert (Jiao\net al., 2020) or PatientKD (Sun et al., 2019b) use\nfine-tuned teachers. DistilBert (Sanh et al., 2019)\ndoes not use a fine-tuned teacher, but it is only\nlimited to students with a hidden state of 784 due\nto the cosine loss it uses and lacks generalization\nacross architectures.\n6 Ethical Issues\nAs we distill the knowledge from an existing model\n(here BERT-base), our approach does not introduce\nany extra ethical concerns during knowledge dis-\ntillation. However, if a bias is already present in\nthe teacher model, it might get transferred to the\nstudent model (Hooker et al., 2020). This is not\nspecific to our algorithm but is a common risk for\nall types of knowledge distillation.\n7 Limitations\nA key limitation of our experiments is that we only\nconsider English corpora. The exclusive use of\nEnglish datasets is unlikely to have a substantive\neffect on distillation performance, and we would\nexpect the results to transfer to other languages and\ndatasets, however, languages with rich morphology\nmay present modeling challenges arising from to-\nkenization, that is, with many small word-pieces,\nlanguage modeling (and its distillation) is likely\nto be a considerably harder task. As it stands, our\n7350\nwork follows the standard evaluation protocols in\npeer benchmarks e.g., Jiao et al. (2020), Sanh et al.\n(2019), and Turc et al. (2019).\nWe only use BERT-base (Devlin et al., 2018)\nas our teacher model and benchmark against stu-\ndents that use it as a teacher model. For larger\nteacher models such as BERT-large or GPT2 (Rad-\nford et al., 2019), the inference time as well as\nmemory requirement would be much higher, and\nwould necessitate larger GPU clusters. This is a\nconsequence of the cost of the forward pass with\nthe teacher model, rather than our distillation algo-\nrithm, which has a much lighter footprint. We ar-\ngue that the result from one transformer-based pre-\ntrained language model should generalize well to\nother transformer-based pre-trained models. Thus\nour results are representative, despite our smaller-\nscale evaluation protocol.\nAnother shortcoming of transformer models,\nin general, is their scalability to long text. In\nthis setting, model-agnostic knowledge distillation,\nlike our technique, enjoys a distinctive advantage.\nWe can incorporate techniques like Beltagy et al.\n(2020) or Xiong et al. (2021) to speed up attention\nin the student model enabling it to scale to long\ntexts, even when paired with a different architec-\nture for the teacher. Jiao et al. (2020) and Sanh\net al. (2019) rely on specific model internals during\ndistillation, and therefore the student model has to\nbe similar to the teacher.\nReferences\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nDankmar Böhning and Bruce G Lindsay. 1988. Mono-\ntonicity of quadratic-approximation algorithms. An-\nnals of the Institute of Statistical Mathematics ,\n40(4):641–663.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck et al. 2015. Convex optimization: Al-\ngorithms and complexity. Foundations and Trends®\nin Machine Learning, 8(3-4):231–357.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nTri Dao, Govinda M Kamath, Vasilis Syrgkanis, and\nLester Mackey. 2020. Knowledge distillation as semi-\nparametric inference. In International Conference on\nLearning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third International Workshop on Paraphrasing\n(IWP2005).\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\nDistilling the knowledge in a neural network. In\nNIPS 2014 Deep Learning Workshop.\nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy\nBengio, and Emily Denton. 2020. Characteris-\ning bias in compressed models. arXiv preprint\narXiv:2010.03058.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language\nunderstanding. In Findings of the Association for\nComputational Linguistics: EMNLP 2020.\nLester Mackey, Vasilis Syrgkanis, and Ilias Zadik. 2018.\nOrthogonal machine learning: Power and limitations.\nIn International Conference on Machine Learning,\npages 3375–3383. PMLR.\nJohn Platt et al. 1999. Probabilistic outputs for support\nvector machines and comparisons to regularized like-\nlihood methods. Advances in large margin classifiers,\n10(3):61–74.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108.\nOr Sharir, Barak Peleg, and Yoav Shoham. 2020. The\ncost of training nlp models: A concise overview.\narXiv preprint arXiv:2004.08900.\n7351\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019a.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323–4332, Hong Kong, China. Association for Com-\nputational Linguistics.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019b.\nPatient knowledge distillation for bert model com-\npression. arXiv arXiv:1908.09355.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspecific knowledge from BERT into simple neural\nnetworks. arXiv preprint arXiv:1903.12136.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nApoorv Vyas, Angelos Katharopoulos, and François\nFleuret. 2020. Fast transformers with clustered at-\ntention. Advances in Neural Information Processing\nSystems, 33:21665–21674.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nFusheng Wang, Jianhao Yan, Fandong Meng, and Jie\nZhou. 2021. Selective knowledge distillation for neu-\nral machine translation. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 6456–6466, Online. Association\nfor Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty,\nMingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.\n2021. Nyströmformer: A nyström-based algorithm\nfor approximating self-attention. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, pages\n14138–14148.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Wein-\nberger, and Yoav Artzi. 2020. Revisiting few-sample\nBERT fine-tuning. arXiv preprint arXiv:2006.05987.\nWangchunshu Zhou, Canwen Xu, and Julian McAuley.\n2022. BERT learns to teach: Knowledge distillation\nwith meta learning. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7037–\n7049, Dublin, Ireland. Association for Computational\nLinguistics.\n7352\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6 (Limitations)\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 7 (Ethics)\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3, 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3, 4\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe only used open-source artifacts. We will open-source our own code and models too using MIT\nlicense.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nOur distilled encoder-only models for general-purpose NLP do not violate the intended use of the\nartifacts.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. We only used standard anonymized datasets.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4\nC □\u0013 Did you run computational experiments?\nSection 3, 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7353\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nOnly the single-run result is provided because the experiments are too computationally intensive. It\nwill waste energy and cause unnecessary CO2 emissions.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3, 4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n7354",
  "topic": "Generalizability theory",
  "concepts": [
    {
      "name": "Generalizability theory",
      "score": 0.872477650642395
    },
    {
      "name": "Distillation",
      "score": 0.8423174619674683
    },
    {
      "name": "Computer science",
      "score": 0.8064131736755371
    },
    {
      "name": "Task (project management)",
      "score": 0.7394525408744812
    },
    {
      "name": "Software deployment",
      "score": 0.6024250388145447
    },
    {
      "name": "Replicate",
      "score": 0.5466265678405762
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4869128465652466
    },
    {
      "name": "Architecture",
      "score": 0.47587886452674866
    },
    {
      "name": "Machine learning",
      "score": 0.4740065038204193
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4739527702331543
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4463229179382324
    },
    {
      "name": "Software engineering",
      "score": 0.16420483589172363
    },
    {
      "name": "Engineering",
      "score": 0.07561665773391724
    },
    {
      "name": "Systems engineering",
      "score": 0.06429117918014526
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": []
}