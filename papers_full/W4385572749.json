{
  "title": "M2D2: A Massively Multi-Domain Language Modeling Dataset",
  "url": "https://openalex.org/W4385572749",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5073571802",
      "name": "Machel Reid",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A5077994189",
      "name": "Victor W. Zhong",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5075783850",
      "name": "Suchin Gururangan",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5067919401",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1848279521",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2586087033",
    "https://openalex.org/W3146885639",
    "https://openalex.org/W4226218363",
    "https://openalex.org/W4286974574",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3114610051",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W2963326042",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "We present M2D2, a fine-grained, massively multi-domain corpus for studying domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and spans 145 domains extracted from Wikipedia and Semantic Scholar. Using ontologies derived from Wikipedia and ArXiv categories, we organize the domains in each data source into 22 groups. This two-level hierarchy enables the study of relationships between domains and their effects on in- and out-of-domain performance after adaptation. We also present a number of insights into the nature of effective domain adaptation in LMs, as examples of the new types of studies M2D2 enables. To improve in-domain performance, we show the benefits of adapting the LM along a domain hierarchy; adapting to smaller amounts of fine-grained domain-specific data can lead to larger in-domain performance gains than larger amounts of weakly relevant data. We further demonstrate a trade-off between in-domain specialization and out-of-domain generalization within and across ontologies, as well as a strong correlation between out-of-domain performance and lexical overlap between domains.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 964–975\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nM2D2: A Massively Multi-Domain Language Modeling Dataset\nMachel Reid1∗, Victor Zhong2, Suchin Gururangan2, Luke Zettlemoyer2\n1The University of Tokyo,2University of Washington\nmachelreid@google.com, {vzhong,sg01,lsz}@cs.washington.edu\nAbstract\nWe present M2D2, a ﬁne-grained, massively\nmulti-domain corpus for studying domain\nadaptation in language models (LMs). M2D2\nconsists of 8.5B tokens and spans 145 do-\nmains extracted from Wikipedia and Seman-\ntic Scholar. Using ontologies derived from\nWikipedia and ArXiv categories, we organize\nthe domains in each data source into 22 groups.\nThis two-level hierarchy enables the study of\nrelationships between domains and their ef-\nfects on in- and out-of-domain performance af-\nter adaptation. We also present a number of in-\nsights into the nature of effective domain adap-\ntation in LMs, as examples of the new types of\nstudies M2D2 enables. To improve in-domain\nperformance, we show the beneﬁts of adapt-\ning the LM along a domain hierarchy; adapt-\ning to smaller amounts of ﬁne-grained domain-\nspeciﬁc data can lead to larger in-domain per-\nformance gains than larger amounts of weakly\nrelevant data. We further demonstrate a trade-\noff between in-domain specialization and out-\nof-domain generalization within and across on-\ntologies, as well as a strong correlation be-\ntween out-of-domain performance and lexical\noverlap between domains.1\n1 Introduction\nEven though they can contain a wide variety of\ndifferent types of domains, the texts that make up\nthe corpora used to train and evaluate language\nmodels (LMs) are often treated as if they are all\nthe same. This makes it challenging to characterize\nLM performance under diverse data distributions\nand understand how to effectively adapt LMs to\nnew ones. To address these challenges, we develop\nM2D2 , a Massively Multi-Domain Dataset, with\n145 subdomains and a human-curated hierarchy for\nstudying ﬁne-grained domain adaptation.\n∗Currently at Google Research\n1We release our dataset publicly at https://github.\ncom/machelreid/m2d2.\nSociety and social sciences\nHuman activites\nTechnology and applied sciences\nCulture and the arts\nHistory and events\nNatural and physical sciences Philosophy and thinking\nHealth and fitness\nReligion and belief systems\nGeneral referece\nMathematics and logic\nSociety\nHuman activities\nImpact of human activity\nAgriculture\nEngineering\nComputing\nTransport\nCulture and Humanities\nSports and Recreation\nVisual arts\nPerforming arts\nGames and Toys\nMass media\nBy period\nBy continent\nBy region\nBiology\nPhysical sciences\nNature\nEarth sciences\nPhilosophy\nThinking\nPublic health\nSelf care\nExercise\nHealth science\nHuman medicine\nNutrition\nAllah\nMajor beliefs of the world\nBelief systems\nFurther research tools and topics\nReference works\nMathematics\nLogic\nFields of mathematics\nFigure 1: Visualization of the two-level ﬁne-grained do-\nmain hierarchy in the Wikipedia portion of M2D2.\nPrior work on domain transfer focuses on a small\nnumber of broad domains (typically 4-20; Guru-\nrangan et al., 2020; Gao et al., 2021; Gururangan\net al., 2021). In contrast, domains in M2D2 are\nﬁne-grained and organized into a hierarchy derived\nfrom human-curated ontologies in Wikipedia (Fig-\nure 1) and Semantic Scholar (Figure 2). Unlike\nprior work, the ﬁne granularity of M2D2 enables\nthe study of transfer to naturally occurring data-\nscarce domains recognized by human curators (e.g.\nPhilosophy, Public Health, Transport). This hier-\narchy enables the study of domain transfer at vary-\ning levels of topic granularity. For instance, how\nshould we combine widely available internet text\n(entire corpus), text on computer science (coarse\ndomain), and scarce corpus on machine learning\n(ﬁne domain) to improve performance in the ma-\nchine learning domain? To the best of our knowl-\nedge, M2D2 is the ﬁrst dataset that combines ﬁne\ndomain granularity with human-curated domain\nhierarchy in a massively multi-domain setting.\n964\nastro-ph \nmath \ncs \ncond-mat Medicine \nphysics s t a t \nq - b i o \nP h i l o s o p h y \nA r t \nastro-ph \nastro-ph.CO \na s t r o - p h . G A \na s t r o - p h . S R \na s t r o - p h . H E \na s t r o - p h . E P \nastro-ph.IM \nmath.MP \nm a t h . C O \nm a t h . A G \nm a t h . P R \nm a t h . A P \nm a t h . I T \nm a t h . D G \nma \nt h .OC \nma \nt h .D S \nm a t h . N T \nma \nt h .N A \nm a t h . R T \nm a t h . G T \nm a t h . F A \nm a t h . G R \nm a t h . S T \nm a t h . Q A \nma t h .A T \nm a t h . C A \nm a t h . R A \nma t h .C V \nm a t h . O A \nm a t h . L O \nm a t h . A C \nm a t h . S G \nm a t h . M G \nm a t h . S P \nm a t h . C T \nm a t h . K T \nm a t h . G N \ncs.LG \ncs .C V \ncs . I T \ncs.AI \ncs . D S \ncs . C R \ncs . C L \ncs . N I \ncs . D C \ncs . L O \nc s . S I \ncs . S Y \ncs . D M \ncs . C C \ncs . R O \ncs . G T \ncs . S E \ncs .N E \ncs . N A \ncs . C Y \ncs . D B \ncs .I R \ncs . P L \ncs . H C \ncs . C G \ncs . F L \ncs . M A \ncs .C E \ncs . S D \ncs .D L \ncs . P F \ncs . M M \ncs . G R \ncs . E T \ncs . S C \ncond-mat \nco n d - m a t . s t a t - m e ch \nco n d - m a t . m e s - h a l l \nco n d - m a t . s t r - e l \ncond-mat.mtrl-sci \nco n d- ma \nt .s o f t \nco n d - m a t . s u p r - co n \nco n d - m a t . d i s - n n \nco \nn d - m a t . q u a n t - g a s \nco \nn d - m a t . o t h e r \nMedicine \np h y s i cs . o p t i cs \np h y s i cs . ﬂ u - d y n \np h y s i cs . s o c- p h \np h y s i cs . co m p - p h \np h y s i cs . a t o m - p h \np h y s i cs . ch e m - p h \np h y s i cs . b i o - p h \np h y s i cs . i n s - d e t \np h y s i cs . p l a s m - p h \np h y s i cs . d a t a - a n \np h y s i cs . g e n - p h \nph y s i cs .a pp- ph \np h y s i cs . cl a s s - p h \np h y s i cs . g e o - p h \np h y s i cs . s p a ce - p h \np h y s i cs . a o - p h \np h y s i cs . h i s t - p h \np h y s i cs . a cc- p h \np h y s i cs . m e d - p h \np h y s i cs . e d - p h \ns t a t .M L \ns t a t . T H \ns t a t . M E \ns t a t . A P \ns t a t . C O \nq - b i o \nq - b i o . P E \nq - b i o . N C \nq - b i o . Q M \nq - b i o . B M \nq-bio.MN \nq - b i o . G N \nq - b i o . C B \nP h i l o s o p h y \nA r t \nFigure 2: Visualization of the hierarchies contained\nwithin the S2ORC portion of M2D2.\nUsing M2D2 , we investigate the following ques-\ntions, as examples of the broad classes of new ques-\ntions that can be asked: (1) how well do coarse\nand ﬁne domains transfer to each other across the\nhierarchy? (2) which features and aspects of a\ndomain are important for transfer? (3) how im-\nportant is domain speciﬁcity versus breadth? We\nperform preliminary experiments analyzing trans-\nfer between similar domains, disparate domains,\nand hierarchically related domains. Moreover, we\nexplore how to select source domains to improve\ntransfer performance.\nWe present baseline experiments using a\nGPT2 (Radford et al., 2019) language model. We\nﬁnd that (1) more speciﬁc data is often more im-\nportant for performance than larger, less-speciﬁc\ndata, shown by our comparison of coarse-grained,\nﬁne-grained and coarse-to-ﬁne adaptation compari-\nson (in which coarse-to-ﬁne performed best) , (2)\nvocabulary overlap is a surprising good indicator\nfor transfer, and (3) data source provenance infor-\nmation is often a better predictor than ontology\nwhen predicting transferability, perhaps indicating\na more multi-faceted deﬁnition of domain could be\ndeveloped in future work.\nGiven the importance of ﬁne granularity do-\nmains in language modeling, we hope that M2D2\nwill encourage the community to further study do-\nmain transfer: how do we identify hierarchical ﬁne-\ngrained domains in naturally occurring text, and\nhow do we leverage this ﬁne-grained domain hier-\narchy to improve domain transfer.\n2 M2D2\nM2D2 consists of a large quantity of ﬁne-grain\ndomains. Unlike prior work that deﬁnes the do-\nmain of a corpus using its source (e.g. the web text\ndomain; Chronopoulou et al., 2021), we derive do-\nmains from a human-curated Wikipedia and arXiv\nontologies. In this section, we describe how M2D2\nis collected and organized.\n2.1 Domain Organization\nOne of the unique properties of M2D2 is its hi-\nerarchical nature, enabling the study of transfer at\ndifferent levels of domain granularity. We assume a\nparticular corpus to have L0, . . . , LK levels of hier-\narchy, where L0 refers to the lowest or most coarse-\ngrained/broad level (i.e. the whole dataset), andLK\nrefers to the highest or most ﬁne-grained/speciﬁc\nlevel. A given level of hierarchy Li contains Ni\ndomains Di\nNi ,\nLi = [Di\n0, . . . ,Di\nj, . . . ,Di\nNi ] (1)\nDi\nj is composed of multiple subdomains\n{Di+1\n0 , . . . ,Di+1\nNi+1 }, which are represented in the\nnext level of the hierarchy Li+1. Similarly, we\nassume that a given subdomain is contained within\na larger domain.\nFor the rest of the paper, we use L1 and L2 to\nrepresent the two levels of a K level hierarchy that\nwe consider in this paper.\n2.2 Dataset Collection\nWe collect M2D2 from two resources, Wikipedia\nand Semantic Scholar. This allows us to explore\ndomain adaptation in a massively multi-domain set-\nting among domains of varying granularity, while\nalso allowing us to test whether our ﬁndings hold\nacross different data sources.\nSemantic Scholar We use the S2ORC corpus\n(Lo et al., 2020), a large corpus of English aca-\ndemic papers annotated with extensive metadata.\nUsing this corpus, which is already categorized\ninto L1-domains representing broader ﬁelds of aca-\ndemic research (e.g. Computer Science, Physics),\nwe extract L2-domains by ﬁnding a given paper’s\nrespective arXiv2 category (e.g. “Computation and\nLanguage” ∈ Computer Science).\n2https://arxiv.org\n965\nL1 (Abbrv) Size #L2 #Tokens Examples of L2 domains\nHealth and ﬁtness(HEAL) 761.2MB 7 116M Exercise, Health Science\nHistory and events(HIST) 1.4GB 4 226M Regions, Periods\nSociety and social sciences(SOCI) 2.3GB 3 379M Society, social sciences\nTechnology and applied sciences(TECH) 1.9GB 5 297M Agriculture, Computing\nCulture and the arts(CULT) 2.0GB 8 289M Games and Toys, The arts and entertainment\nNatural and physical sciences(NATU) 1.2GB 5 189M Physical sciences, Earth sciences\nHuman activites(HUMA) 2.1GB 3 343M Impact of human activity\nMathematics and logic(MATH) 332.3MB 4 52M Mathematics, Logic\nGeneral reference(GENE) 385.3MB 3 60M Research tools and topics, Reference works\nReligion and belief systems(RELI) 428.0MB 4 64M Major beliefs of the world, Belief systems\nPhilosophy and thinking(PHIL) 1.0GB 3 165M Philosophy, Thinking\nMathematics(math) 4.5GB 26 1.4B Topology, Number Theory\nQuantitative Biology(q-bio) 1.9GB 3 336M Biomolecules, Cell Behavior\nPhysics 4.1GB 12 737M General Physics, Biological Physics\nNonlinear Sciences(nlin) 730MB 5 134M Self-Organizing Systems, Chaotic Dynamics\nCondensed Matter(cm) 3.8GB 10 688M Materials Science, Quantum Gases\nEconomics(econ) 67MB 3 11M Econometrics, General Econometrics, Theory\nComputer Science(cs) 4.5GB 23 1.1B Machine Learning, Databases, Graphics\nStatistics(stat) 2.4GB 4 450M Applications, Methodology\nAstrophysics(astro-ph) 4.0GB 7 728M Earth/Planetary, Cosmology\nArt† 575MB 1 98M —\nPhilosophy†\n(phil) 919MB 1 156M —\nAverage±s.d. 1.9G±1.7G 6.6±6.2 373M±347M —\nTotal 41GB 145 8.5B —\nTable 1: Dataset statistics for M2D2. We list L1 domains, with their corresponding sizes, number of L2 domains,\nnumber of tokens, and examples of L2 domains. †These domains did not have any subdomains in the arXiv\nontology.\nWikipedia We crawl the Wikipedia ontology,3\nwhich lists major categories contained within\nWikipedia. Within these major categories or L1-\ndomains, we then proceed to look up the category\npages within a given L1-domain, and gather respec-\ntive L2-domains. This procedure yields a hierar-\nchy of domains contained within Wikipedia. We\nthen download the Wikipedia data dump, which\nwe clean using wikiextractor4 and assign a\ngiven page to its respective domain.\n2.3 Unique Properties\nM2D2 has the following major unique proper-\nties when compared to previous domain adapta-\ntion datasets. First, it is massively multi-domain:\nwe have 145 L2 domains grouped into 22 L1 do-\nmains, which allows us to test domain adaptation\nfor language modeling on a variety of axes (such\nas hierarchy, subject matter, and ontology) that\nwould be more difﬁcult with more coarse-grained\ndatasets. Second, M2D2 is hierarchical: this al-\n3https://en.wikipedia.org/wiki/\nWikipedia:Contents/Categories\n4https://github.com/attardi/\nwikiextractor\nlows us to also test the performance of domain\nspeciﬁcity versus domain breadth in more ﬂexible\nadaptation settings.\nWe describe dataset statistics in Table 1, includ-\ning dataset size (measured in MB/GB), token count\n(measured by whitespace tokenization), and the\nnumber of L2 domains within each L1 domain.\nM2D2 contains 8.5B tokens, with an average of\n373 million tokens per L1 domain. Demonstrat-\ning the hierarchical nature of M2D2 , we also list\nexamples of L2 domains contained within the L1\ndomains (e.g. Computing ∈ Technology and Ap-\nplied Sciences, Topology ∈ Mathematics) which\nare also graphically shown in Figures 1 and 2).\n2.4 Dataset Splits\nWe split each domain into the respective train, val-\nidation, and test sets. To prevent data leakage be-\ntween the domains when pages belong to two or\nmore domains, we construct validation and test sets\nfrom pages that are not contained within any other\ndomains on the same level of hierarchy. For ex-\nample, the page for “Biotechnology” overlaps in\ndomain with both Biology ∈ Natural and Physi-\ncal Sciencesand Engineering ∈ Technology and\n966\nLanguage Model \n(GPT2)\nTechnology\nand \nApplied Sciences\nComputing\nLanguage Model \n(GPT2) Computing\nL1 \nAdaptation\nL2 \nAdaptation\nL1-to-L2 \nAdaptation\nLanguage Model \n(GPT2)\nTest Data\nADAPTATION\nMETHODS\nZERO \nSHOT \nFigure 3: The types of domain adaptation that we con-\nsider in this work: L1, L2, and L1-to-L2 adaptation.\nHere, we use “Technology and Applied Sciences” to il-\nlustrate our L1 domain and “Computing” to illustrate\nour L2 domain. Bold arrows refer to adaptation steps,\nand dotted lines refer to an evaluation phase.\nApplied Sciencesso this would not be included in\nany evaluation set due to the potential for direct\nleakage. However, the page for “Computer” is only\nin Computing ∈ Technology and Applied Sciences\nand therefore could be included in an evaluation\nset. We include at least 1 million tokens in the\nvalidation and test sets, respectively. This enables\nus to have a precise evaluation set of texts that only\nbelong to a single ﬁne-grained domain.\n3 Experiments\nAs examples of the types of new studies M2D2\nenables, we explore a number of key questions\nabout the nature of effective domain adaptation in\nlanguage models. For example, how does one best\nspecialize a language model to a domain, given an\nontology? How well can adapted models be applied\nout-of-domain, within and across ontologies? What\nfeatures of target domains are predictive of out-of-\ndomain transfer?\nIn this section, we present a set of experiments\nthat begin to answer these questions. First, we\nstudy the impact of adapting to the L1 and L2 do-\nmains of our dataset on in-domain (§3.2) and out-\nof-domain (§3.3) language modeling performance.\nThen, we perform an analysis of lexical features\nin domains that are predictive of out-of-domain\nperformance (§3.4).\n3.1 Experimental setup\nIn all experiments, we use the 112M GPT2 model\n(Radford et al., 2019) as the baseline model. Our\nimplementation is based on HuggingFace Trans-\nformers (Wolf et al., 2020) and PyTorch (Paszke\net al., 2019). All adaptation techniques are per-\nformed using Adam (Kingma and Ba, 2015),\ndropout value of 0.2 (Srivastava et al., 2014), using\na learning rate of 5e-5 and a batch size of 64000\ntokens. We train all models for a maximum of 1\nmillion iterations and perform early stopping over\nthe validation set. All experiments are run on 8\nNVIDIA V100 GPUs.\nWhen adapting our GPT2 model to domains in\nM2D2, we use one of three settings:\nL1 Adaptation We continue training on a given\nL1 domain (e.g. Computer Science).\nL2 Adaptation We continue training on a given\nL2 domain (e.g. Machine Learning).\nL1-to-L2 Adaptation Given a L2 domain\n(e.g. Machine Learning), we ﬁrst perform L1 adap-\ntation on its corresponding L1 domain (e.g. Com-\nputer Science), and then we further perform L2\nadaptation. This setting similar to multi-stage adap-\ntive pretraining approaches used for supervised\ntasks (Gururangan et al., 2020).\nFor all techniques, we evaluate test perplexity on\nL2 domains validation sets. Due to the large quan-\ntity of L2 domains, we aggregate L2 results by their\ncorresponding L1. For each ontology, we report\nthe average and standard deviation (averages.d.) of\nperplexities across L2 domains in each L1.\n3.2 In-Domain Results\nThe ﬁrst set of experiments in this study consid-\ners the impact of adapting the language model to\ndifferent levels of the M2D2 ontologies. We only\nconsider in-domain perplexity, or the perplexity of\nmodel on the domain it is adapted to.\nAdaptation improves in-domain performance\ndespite pretraining. Table 2 shows test-set per-\nplexities on L2 domains, averaged across each L1\ndomain, after performing each adaptation tech-\nnique (see Appendix on full results). First, we\nobserve that all proposed adaptation techniques\nimprove performance over the base GPT-2 model.\nThis highlights the effectiveness of adaptation in\nimproving in-domain performance, even when con-\nsidering domains that the language model has likely\n967\nWiki HEAL HIST SOCI TECH CULT HUMA MATH GENE RELI PHIL NATU Avg\nGPT2 23.1 27.5 24.5 27.8 27.5 28.9 26.6 25.9 26.3 26.2 26.7 26.5\nL1 18.1 2.5 20.90.5 19.70.8 22.30.8 21.22.3 23.01.4 18.35.2 21.60.8 19.80.5 21.80.6 20.83.2 20.7\nL2 17.5 2.7 17.81.9 17.50.7 21.81.0 21.72.6 22.40.9 17.85.2 20.81.0 18.30.4 21.00.4 21.71.6 19.8\nL1-to-L216.82.7 16.72.1 15.40.5 21.40.9 20.62.6 22.00.8 17.15.0 19.61.1 16.90.5 20.50.4 20.31.5 18.8\nS2ORC Math Econ CS CM Physics Art Phil Stat Q-Bio Nlin Astro-Ph Avg\nGPT2 26.1 2.8 28.22.7 26.82.9 29.71.2 32.72.1 35.10.0 32.90.0 22.77.3 30.11.3 25.51.4 31.61.5 29.2\nL1 9.2 3.4 15.92.2 15.44.0 12.51.0 17.11.7 27.70.0 24.40.0 11.03.5 22.62.2 9.82.4 15.53.2 16.5\nL2 8.0 3.2 13.42.1 15.16.7 12.01.3 16.51.3 27.70.0 24.40.0 10.22.5 21.01.3 9.62.1 14.02.3 15.7\nL1-to-L27.53.2 12.52.2 14.05.9 11.51.0 16.11.6 27.70.0 24.40.0 9.33.3 20.31.0 9.22.1 12.92.3 15.0\nTable 2: In-domain test perplexities, aggregated to each L1 domain. We look at the impact of L1 vs L2 vs L1-to-\nL2 ﬁnetuning settings when compared to simply ﬁnetuning on L1. L2 Adaptation is usually more effective than\nL1 Adaptation, emphasizing the importance of ﬁne-grained domains, with a coarse-to-ﬁne setup using L1-to-L2\nAdaptation is most effective. This ﬁnding is statistically signiﬁcant ( p <0.05; measured using the Kolmogorov-\nSmirnov test).\nWiki HEAL HIST SOCI TECH CULT HUMA MATH GENE RELI PHIL NATU Avg\nL1 23.6 3.5 23.22.0 22.42.2 22.42.3 22.32.2 22.72.0 25.13.4 24.22.3 24.72.8 23.62.7 23.33.2 23.3\nL2 26.1 3.8 26.13.9 25.72.7 26.13.5 27.03.7 25.63.6 28.96.9 25.12.4 26.32.9 24.12.6 26.33.7 26.1\nL1-to-L2 25.53.8 25.93.8 25.22.6 26.03.3 27.03.7 25.13.6 28.57.0 24.52.4 26.22.9 23.22.6 25.23.7 25.7\nS2ORC Math Econ CS CM Physics Art Phil Stat Q-Bio Nlin Astro-Ph Avg\nL1 32.0 17.2 28.810.9 23.110.1 24.914.0 22.810.6 26.83.3 25.73.9 23.411.5 23.211.3 23.812.9 26.212.8 25.5\nL2 36.0 21.9 33.411.1 32.118.7 32.717.3 25.412.1 26.83.3 25.73.9 32.724.7 33.219.6 34.822.4 27.211.4 30.9\nL1-to-L2 36.824.8 31.912.6 31.022.0 30.218.2 24.211.4 26.83.3 25.73.9 30.423.0 32.123.4 36.530.8 27.515.1 30.3\nTable 3: Out-of-domain test perplexities, aggregated to each L1 domain. We look at the impact of L1 vs L2 vs L1-\nto-L2 ﬁnetuning settings when compared to simply ﬁnetuning on L1. We can see that L2 Adaptation and L1-to-L2\nAdaptation are generally less performant in out-of-domain settings that L1 Adapted models, given their in-domain\nspeciﬁcation. The comparison between L1 versus L2 is statistically signiﬁcant p <0.01.\nbeen exposed to during pretraining (as is the case\nwith Wikipedia; L1 adaptation results in a 5.8 de-\ncrease in perplexity). For domains which the lan-\nguage model is less likely to have been exposed to\nduring pretraining, this is more pronounced (as is\nthe case with S2ORC; L1 adaptation results in a\n12.7 decease in perplexity).\nSpeciﬁcity and hierarchy is more important\nthan broad coverage in adaptation. Next, we\nobserve that in most cases, adapting to L2 do-\nmains is more beneﬁcial to in-domain perfor-\nmance than adapting to L1 domains. Adaptation\nto ﬁner-grained domains better specializes a lan-\nguage model, even though these domains are much\nsmaller than their L1 counterparts. Finally, we\nobserve that using L1-to-L2 adaptation further ben-\neﬁts in-domain performance over L2 adaptation\nin all cases. Our results suggest that adapting to\nsmaller amounts of domain-speciﬁc data leads to\nmore effective in-domain specialization than adapt-\ning to large quantities of data that may be more\nweakly domain-relevant. Moreover, the best results\nmay be achieved by organizing the target domain\ninto subsets of broader and ﬁne-grained data, and\nadapting along this hierarchy. However, this ap-\nproach has increased memory and computational\nrequirements relative to solely relying on L1 Adap-\ntation.\n3.3 Out-of-Domain Results\nWe also study the effects of our adaptation tech-\nniques on out-of-domain performance, by perform-\ning zero-shot inference with adapted models on\ndomains (e.g. Art) other than the ones they are\nadapted to (e.g. Machine Learning). We ﬁrst trans-\nfer models between domains in the same ontology\n(e.g. Wikipedia → Wikipedia), and then across on-\ntologies (e.g. Wikipedia → S2ORC).\nL2 Adaptation decreases out-of-domain perfor-\nmance. We show out-of-domain performance for\neach adaptation technique in Table 3. We show that\nconversely to L2 and L1-to-L2 adaptation which\nsigniﬁcantly improved in-domain performance, this\ncomes with the tradeoff at less performance in both\n968\nDomain NATU TECH SOCI HEAL HIST RELI CULT GENE MATH HUMA PHIL Avg\nNATU — 25.5 22.1 20.0 24.5 23.5 25.7 23.7 21.0 25.6 23.2 23.3\nTECH 23.6 — 20.8 19.4 23.1 22.7 23.5 22.6 21.7 24.5 22.4 22.5\nSOCI 23.8 24.2 — 19.8 22.3 21.7 23.4 22.0 22.6 24.1 22.0 22.4\nHEAL 24.3 25.6 21.6 — 24.4 23.7 25.2 24.0 25.0 26.2 23.9 23.9\nHIST 24.7 25.3 20.7 21.8 — 21.4 24.2 22.8 24.0 23.9 22.6 23.0\nRELI 26.3 28.2 21.9 22.8 24.0 — 25.8 24.4 26.0 26.3 24.0 24.5\nCULT 23.7 24.3 20.6 20.1 23.0 22.1 — 22.5 22.8 24.4 22.2 22.5\nGENE 25.4 26.4 21.8 22.1 24.2 23.2 25.4 — 24.5 26.2 23.3 24.1\nMATH 26.3 26.7 23.7 23.1 26.4 25.0 27.1 25.2 — 28.3 24.4 25.0\nHUMA 23.9 24.0 20.1 20.7 22.0 21.3 24.1 22.3 23.2 — 22.3 22.5\nPHIL 25.1 25.7 21.8 21.3 24.4 22.9 24.7 23.5 20.9 26.0 — 23.5\nAvg 24.4 25.3 21.3 20.8 23.6 22.5 24.6 23.1 22.7 25.3 22.9 23.3\nTable 4: Out-of-domain transfer performance between all L1 domains (using abbreviations from Table 1) in the\nWikipedia portion of M2D2. For each domain, we use the ﬁrst four letters to refer to itself. The x-axis shows\nevaluation domains, and the y-axis shows training domains.\nDomain math econ cs cm physics Art Philosophy stat q-bio nlin astro-ph Avg\nmath — 25.0 22.0 21.2 35.8 66.1 57.2 19.6 38.8 13.6 43.2 32.0\necon 18.5 — 23.8 24.6 35.1 48.1 43.9 15.3 33.7 20.0 37.5 28.8\ncs 12.6 17.6 — 18.0 24.5 43.2 40.1 13.9 26.6 14.0 28.6 23.1\ncm 13.6 20.7 21.6 — 17.9 55.9 50.0 16.2 26.4 13.2 25.8 24.9\nphysics 14.1 20.5 21.0 14.1 — 46.2 41.9 15.8 24.8 13.3 22.1 22.8\nArt 22.9 25.8 25.9 27.5 31.1 — 29.0 21.3 29.1 22.6 31.7 26.8\nPhilosophy 20.8 24.7 23.4 26.2 31.2 30.4 — 20.3 28.1 21.5 31.4 25.7\nstat 12.7 14.0 18.2 17.9 24.8 47.0 43.2 — 26.6 14.8 27.0 23.4\nq-bio 13.7 18.1 19.2 14.6 20.9 48.1 42.9 13.6 — 14.3 26.8 23.2\nnlin 11.0 19.7 20.7 13.3 22.3 51.7 45.8 15.7 25.9 — 25.9 23.8\nastro-ph 16.6 23.9 25.2 17.1 23.6 54.4 48.1 17.8 30.9 15.2 — 26.2\nAvg 15.1 20.5 21.5 18.8 25.8 47.1 42.4 16.4 28.5 15.7 28.7 25.5\nTable 5: Out-of-domain transfer performance between all L1 domains in the S2ORC portion of M2D2. “GPT2”\nrefers to the zero-shot performance of the LM on our dataset.\nL2 and L1-to-L2 settings when compared to L1\nAdaptation.\nSpeciﬁc adaptation transfers better to related\ncategories across ontology. Although the two\ndata sources in M2D2 differ considerably in style\nand content, their ontological categories partially\noverlap. For example, Mathematics and Art appear\nin both Wikipedia and Semantic Scholar. Is it pos-\nsible to transfer between corresponding categories\nacross ontologies?\nTo answer this question, we ﬁrst manually align\nL1 domains from Wikipedia and Semantic Scholar\nwith similar ontological categories (e.g., group-\ning Mathematics from Wikipedia and Mathematics\nfrom S2ORC). We then apply a model adapted to\nan L1 domain in a source ontology onto its cor-\nresponding L1 domain in a target ontology. We\ncompare this cross-ontology performance with two\nbaselines: 1) the average out-of-domain perfor-\nmance of other L1 adapted models in the target\nontology and 2) the in-domain performance of a\nmodel adapted to the target L1 domain.\nOur results are displayed in Table 6. We observe\nthat while L1 adapted models are effective at trans-\nferring to other domains within an ontology, they\nare less effective at transferring to corresponding\ndomains outside an ontology. Surprisingly, in all\ncases, transferring outside an ontology performs\neven worse than using the base GPT-2 model with\nno additional adaptation. Moreover, the average\nout-of-domain performance of L1 adapted models\ngenerally outperforms cross-ontology performance,\nindicating properties shared within an ontology\n(e.g. style) could be transferred.\nSummary Our investigations into the out-of-\ndomain performance of adapted language models\n969\nS2ORC Mathematics Computer Science Art Philosophy Physics\nS2ORC (in-domain) 9.2 15.4 27.7 24.4 17.1\nWiki (in-domain) 19.6 26.8 35.3 33.4 29.6\nS2ORC (out-of-domain) 15.1 21.5 47.1 42.4 25.8\nWiki MATH TECH CULT PHIL NATU\nWiki (in-domain) 18.3 22.3 21.2 21.8 20.8\nS2ORC (in-domain) 29.6 29.5 26.8 27.0 31.5\nWiki (out-of-domain) 22.7 25.3 24.6 22.9 22.9\nTable 6: Transfer performance between corresponding domains(Math↔Mathematics and Logic(Math), Computer\nScience↔Technology and Applied Sciences, Art↔Culture and the Arts, etc..) in both ontologies. It can be seen\nthat provenance is a stronger indicator of transfer performance on M2D2 than ontological correspondence.\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nOverlap by POS/Entity\n20\n22\n24\n26\n28\n30\n32\n34\n36Micro averaged PPL over all L2 domains\nNoun, Pearson (r = 0.79, p < 0.01)\nAdj, Pearson (r = 0.77, p < 0.01)\nVerb, Pearson (r = 0.85, p < 0.01)\nEntities, Pearson (r = 0.78, p < 0.01)\nFigure 4: The relationship between overlap and transfer\nperformance over all Wikipedia L2 domains. Entities,\nverbs, nouns and adjectives are all strongly correlated\nwith performance across domains.\nreveals a tradeoff between specialization and gener-\nalization. The more ﬁne-grained the specialization\nof the language model, the less one can expect it to\nbe applicable outside of the domain it was trained\non. This effect size increases as we move outside\nthe ontology: models trained on one ontology are\nnot useful in other ontologies, despite being trained\non similar categories of data. These ﬁndings lead us\nto believe that domain adaptation should be studied\nfrom a multi-faceted perspective to exploit speciﬁc\naspects of domain (e.g. style, content). Future\nwork may look at reducing the tradeoff between\nhighly domain specialized models and out of do-\nmain performance, perhaps through ensembling or\nother approaches.\n3.4 Lexical indicators of out-of-domain\nperformance\nLooking closer at the out-of-domain performance\nof L1 models, we see intuitive relationships be-\ntween subject similarity and zero-shot out-of-\ndomain transfer performance (Table 4). For ex-\nample, Society and Human Activitiesdomains tend\nto transfer well to each other, whereasReligion and\nMathematics do not transfer as well. These ﬁndings\nsuggest that out-of-domain transfer is correlated\nwith content overlap. In this section, we present\nsome basic lexical indicators of out-of-domain per-\nformance which support this hypothesis.\nVocabulary overlap strongly correlates with\ntransfer regardless of part-of-speech. Figure 4\nshows the correlation of vocabulary overlap a given\npart-of-speech tag (VERB, NOUN, ADJ) or enti-\nties and average out-of-domain performance on\nM2D2 . We compute this by taking the top- k\n(k = 1000) most common words for a given do-\nmain which correspond to a given POS tag. For ev-\nery given domain, we then calculate the intersection\nof shared most common words corresponding to the\npart-of-speech tag with the entirety of M2D2 and\nplot them against the L2-domain-averaged perplex-\nity over the entire dataset. We use spacy (Honni-\nbal and Montani, 2017) for both entity recognition\nand POS tagging. We ﬁnd that vocabulary over-\nlap is a strong predictor of transfer performance\nregardless of part-of-speech, perhaps indicating its\nrelevance in transfer between ﬁne-grained domains.\nRelated domains mostly transfer domain-\nspeciﬁc tokens. We analyse domain adaptation\nat a token-level to characterize what different adap-\ntation settings transfer. Speciﬁcally, we measure\nwhich tokens are most impacted in terms of per-\nword perplexity when we ﬁnetune on a domain-\nspeciﬁc corpus. We do this by taking the difference\nbetween the softmax-normalized probability of pre-\n970\nTransfer Domain-speciﬁc General Examples\nDistant L1 25.7% 74.3% Blockchain, Alexa\nEasy L1 12.3% 87.7% the, cache\nZero-shot 23.4% 76.6% renewals, Markov\nL1-to-L2 31.6% 68.4% lambda DCS, Tacotron\nTable 7: Average percentage of tokens transferred in-\ndomain and out of domain. Examples are taken from\nPhilosophy→Computer Science, Statistics→Computer\nScience, GPT2 →Computer Science, and Computer\nScience→Computation and Language.\ndicting a given word in a given domain when com-\nparing two models adapted to different corpora.\nWe compare S2ORC adapted models in four set-\ntings: two best-transferred domains (a proxy for\nsimilar domains; easy transfers), two worst trans-\nferred L1 domains (a proxy for distant domains;\ndifﬁcult transfers), L1-to-L2 Adaptation (hierarchi-\ncal domain transfer), and no adaptation (zero-shot\nperformance of the base LM). We show the distri-\nbution between domain-speciﬁc (terms that appear\nless than 0.00001% of the time in any other domain)\nand non-domain-speciﬁc terms in Table 7 that ap-\npear in the top 1000 most adapted words. Finally,\nwe show representative samples of tokens with the\ngreatest change after adaptation. We ﬁnd that the\nmost changed tokens between easy transfers (e.g.\nStatistics and Computer Science) are non-domain-\nspeciﬁc words (such as the) but harder transfers\ninclude words that are more domain speciﬁc (such\nas Blockchain).\nSummary Our preliminary analyses suggest that\nsimple lexical characteristics of domains are strong\nindicators of how well an adapted model may gen-\neralize. Developing computationally inexpensive\nindicators of transfer (as lexical overlap is), is im-\nportant for domain transfer to ﬁnd the best out of\na large set of candidate corpora to perform adap-\ntation to a target domain. This would allow one\nto approximately ﬁnd the best corpus, without the\ncomputational overhead of adapting to all candi-\ndate corpora.\n4 Related Work\nDomain Adaptation Techniques (Gururangan\net al., 2020) show that pretrained language mod-\nels can be adapted to new domains by con-\ntinued pre-training on domain-speciﬁc corpora.\nChronopoulou et al. (2021); Gururangan et al.\n(2021) build upon this work by using hierarchi-\ncally constructed domain speciﬁc adapters/experts\n(Houlsby et al., 2019). Another line of work in\ndomain generalization is to simply scale the model\npre-training on a corpus containing different do-\nmains (e.g. GitHub, PubMed) such as done with\nGPT-J (Wang and Komatsuzaki, 2021) and the Pile\n(Gao et al., 2021). Dery et al. (2021) also look to\nbridge these approaches by learning a task/domain\nspeciﬁc mixture of tasks. Overall, however, much\nof this work (Daumé III, 2007; Ruder et al., 2017;\nRuder and Plank, 2018; Gururangan et al., 2020;\nRamponi and Plank, 2020; Gururangan et al., 2021;\nChronopoulou et al., 2021) ﬁts in a paradigm in\nwhich a base model is trained further on domain-\nspeciﬁc corpora and then testing on tasks within\nthat domain (e.g. abstract sentence role classiﬁca-\ntion (Bird et al., 2008) for the scientiﬁc domain).\nM2D2 is complementary to these works in pro-\nviding a testbed for ﬁne-grained and hierarhical\nadaptation across a large quantity of domains.\nDomain Adaptation Datasets One approach to-\nward improved pre-trained language models in-\ncludes building large-scale pre-training datasets\nthat contain a diverse set of domains, such as the\nPile (Gao et al., 2021). Overall, this emphasis has\nlead to improved performance in various domains,\nespecially with large-scale pre-trained language\nmodels, such as GPT-J (Wang and Komatsuzaki,\n2021). Another line of work has been in docu-\nmenting large-scale web-crawled datasets, so prac-\ntitioners and researchers can be more informed and\nmindful of the data used (Dodge et al., 2021). Our\nwork extends this thread with a massively multi-\ndomain corpus with a manually curated ontology\nthat can be used to study ﬁne-grained and hierar-\nchical domain transfer.\n5 Conclusion\nWe developed M2D2 , a new massively multi-\ndomain language modeling dataset for studying\ndomain adaptation in language models. M2D2\nconsists of 145 ﬁne-grained domains (curated from\nWikipedia and Semantic Scholar) that are hierar-\nchically organized using domain-speciﬁc ontolo-\ngies. Using M2D2, we ﬁnd that domain precision\nis more important than data quantity to improve\nin-domain performance, a tradeoff between spe-\ncialization and out-of-domain generalization. We\nrelease M2D2 publicly to spur further research\non building effective language models on highly\nheterogeneous data.\n971\n6 Limitations\nIn this work, we only consider adaptation tech-\nniques that assume domains are monolithic and\nnon-overlapping. Future work may instead ex-\nplore modeling the data as a mixture of domains,\nwhich may improve out-of-domain performance.\nIn addition, M2D2 only covers two data sources\n(Wikipedia and Semantic Scholar). Future work\ncould expand this corpus with ontologies from\nother data sources, such as Reddit, which have\na ﬁne-grained and hierarchical domains. More-\nover, data sourced from the web may contain hate\nspeech and other harmful content, which may be re-\nproduced by language models adapted to such data.\nThe data sources we use adhere to research-friendly\ndata licenses, but training models on web-curated\ndata while maintaining the rights of authors as data\nsubjects and creators remains an open problem.\nAcknowledgements\nWe thank Nikita Haduong, Jungo Kasai, Sophia\nSerrano, Wenya Wang for their feedback and proof-\nreading comments. We thank Jesse Dodge, Alexan-\ndra Chronopoulou, and Matthew Peters for sharing\ncode for their work on domain adaptation. We\nalso thank Sebastian Ruder for useful discussions.\nMR is grateful to the Masason Foundation for their\nsupport.\nReferences\nSteven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,\nMark Joseph, Min-Yen Kan, Dongwon Lee, Brett\nPowley, Dragomir Radev, and Yee Fan Tan. 2008.\nThe ACL Anthology reference corpus: A reference\ndataset for bibliographic research in computational\nlinguistics. In Proceedings of the Sixth Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC’08), Marrakech, Morocco. European\nLanguage Resources Association (ELRA).\nAlexandra Chronopoulou, Matthew E. Peters, and\nJesse Dodge. 2021. Efﬁcient hierarchical domain\nadaptation for pretrained language models.\nHal Daumé III. 2007. Frustratingly easy domain adap-\ntation. In Proceedings of the 45th Annual Meeting of\nthe Association of Computational Linguistics, pages\n256–263, Prague, Czech Republic. Association for\nComputational Linguistics.\nLucio M. Dery, Paul Michel, Ameet Talwalkar, and\nGraham Neubig. 2021. Should we be pre-training?\nan argument for end-task aware training as an alter-\nnative. arXiv preprint arXiv: Arxiv-2109.07437.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1286–1305, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The pile: An\n800gb dataset of diverse text for language modeling.\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2021. Demix\nlayers: Disentangling domains for modular lan-\nguage modeling. arXiv preprint arXiv:2108.05036.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Confer-\nence on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, volume 97 of\nProceedings of Machine Learning Research, pages\n2790–2799. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. As-\nsociation for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\n972\nlearning library. In Advances in Neural Informa-\ntion Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 8024–8035.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlan Ramponi and Barbara Plank. 2020. Neural unsu-\npervised domain adaptation in NLP—A survey. In\nProceedings of the 28th International Conference\non Computational Linguistics, pages 6838–6855,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nSebastian Ruder, Parsa Ghaffari, and John G. Breslin.\n2017. Knowledge adaptation: Teaching to adapt.\narXiv preprint arXiv: Arxiv-1702.02052.\nSebastian Ruder and Barbara Plank. 2018. Strong base-\nlines for neural semi-supervised learning under do-\nmain shift. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1044–1054, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\n973\nA Appendix\nA.1 Hyperparameters\nComputing Infrastructure 8 V olta 16GB GPUs\nHyperparameter Assignment\narchitecture GPT-2\ntokens per sample 1024\nbatch size 64000\nnumber of workers 8\nlearning rate 5e–5\nclip norm 0.1\nnumber of steps 1,000,000\nsave interval updates 1,000\nvalidation interval 1,000\nnumber of warmup steps 10,000\nlearning rate scheduler polynomial decay\nlearning rate optimizer Adam\nAdam beta weights (0.9, 0.99)\nAdam epsilon 1e-6\nweight decay 0.1\nTable 8: Hyperparameters for ﬁnetuning in all settings.\nA.2 Licenses\nOur data sources have open licenses. Wikipedia has a Creative Commons Attribution-ShareAlike 3.0\nUnported License and a S2ORC has a Creative Commons Attribution-NonCommercial 4.0 International\n(CC BY-NC 4.0).\nA.3 More examples of most transferred tokens\nWe give more examples of tokens transferred from the L1 S2ORC Computer Science (given its assumed\nfamiliarity to our audience) domain in the following table:\nTransfer Example Tokens\nComputer Science→Computation and Language lambda DCS, perplexity, Artetxe, Tacotron,\nSwayamdipta, Transformer, parallel, Socher,\nGigaword, Lapata\nComputer Science→Machine Learning criterion, Ganchev, Ioffe, labeling, autoencoder, Hin-\nton, hyperparameters\nComputer Science→Art Atheist, heroism, intellectuals, horrors, witchcraft,\nmourning, apostles\nComputer Science→Technology and Applied Sci-\nences\nSunderland, accounting, inventory, Libyan, bishop,\nravaged, trafﬁc\nTable 9: More examples of most transferred tokens\nA.4 All domains\nWe list all domains contained within dataset in Table 10.\n974\nS2ORC\ncs.CE, cs.IT, cs.CG, cs.SI, cond-mat.quant-gas, math.SG, cs.SC, cs.CY , econ.GN, math.CO, cs.AR,\ncs.MS, cs.DC, q-bio.TO, cs.GR, physics.acc-ph, physics.geo-ph, math.RT, math.HO, cs.RO, q-bio.SC,\nmath.QA, cs.NI, math.CA, cs.DS, astro-ph.GA, physics.atom-ph, math.CT, cs.CV , cond-mat.mtrl-sci,\nmath.CV , math.AC, cond-mat.str-el, physics.comp-ph, cs.CC, math.FA, cond-mat.dis-nn, econ.TH,\nphysics.gen-ph, physics.data-an, astro-ph.IM, q-bio.CB, math.LO, physics.ins-det, q-bio.BM, cs.LO,\nmath.GR, physics.optics, cs.GT, math.AG, cs.NE, cs.SY , physics.bio-ph, physics.ﬂu-dyn, cs.CL, math.MG,\ncs.AI, math.OC, nlin.CG, math.IT, stat.OT, math.OA, cond-mat.soft, Art, cs.GL, cs.PF, math.ST,\nphysics.ao-ph, physics.plasm-ph, math.RA, physics.hist-ph, cs.PL, cs.MA, physics.chem-ph, physics.soc-\nph, physics.med-ph, physics.ed-ph, stat.AP, stat.CO, math.DS, cs.DB, nlin.SI, q-bio.GN, physics.atm-\nclus, nlin.CD, astro-ph.CO, cs.CR, cond-mat.supr-con, cs.LG, math.KT, stat.ML, nlin.PS, q-bio.MN,\ncs.IR, math.GT, cs.SD, math.NA, cond-mat.other, math.NT, cs.FL, physics.pop-ph, cond-mat.stat-mech,\nmath.GN, cs.DL, astro-ph.EP, q-bio.QM, cs.ET, q-bio.PE, cs.OH, Philosophy, physics.space-ph, econ.EM,\nphysics.class-ph, cs.DM, cond-mat.mes-hall, stat.TH, cs.SE, astro-ph.HE, math.MP, nlin.AO, math.AP,\nq-bio.NC, q-bio.OT, astro-ph.SR, math.DG, math.AT, cs.MM, stat.ME, cs.OS, math.SP, physics.app-ph,\ncs.NA, math.PR, math.GM, cs.HC\nWikipedia\nCulture and Humanities, Games and Toys, Mass media, Performing arts, Sports and Recreation, The\narts and Entertainment, Visual arts, Further research tools and topics, Reference works, Exercise, Health\nscience, Human medicine, Nutrition, Public health, Self care, By continent, By period, By region, Human\nactivities, Impact of human activity, Fields of mathematics, Logic, Mathematics, Biology, Earth sciences,\nNature, Physical sciences, Philosophy, Thinking, Allah, Belief systems, Major beliefs of the world, Social\nsciences, Society, Agriculture, Computing, Engineering, Transport\nTable 10: All domains contained within M2D2\n975",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8197400569915771
    },
    {
      "name": "Hierarchy",
      "score": 0.7310368418693542
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.7246975302696228
    },
    {
      "name": "Domain adaptation",
      "score": 0.602138340473175
    },
    {
      "name": "Generalization",
      "score": 0.4560605585575104
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.45257917046546936
    },
    {
      "name": "Business domain",
      "score": 0.4404883086681366
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4102681875228882
    },
    {
      "name": "Natural language processing",
      "score": 0.3993265926837921
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3305668234825134
    },
    {
      "name": "Mathematics",
      "score": 0.07596537470817566
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Business process",
      "score": 0.0
    },
    {
      "name": "Geochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Market economy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Compatibility (geochemistry)",
      "score": 0.0
    },
    {
      "name": "Business architecture",
      "score": 0.0
    }
  ]
}