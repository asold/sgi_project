{
  "title": "Emergent Modularity in Pre-trained Transformers",
  "url": "https://openalex.org/W4385572213",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2104573188",
      "name": "Zhengyan Zhang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2143372570",
      "name": "Zhi-yuan Zeng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2131512363",
      "name": "Yankai Lin",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2881708739",
      "name": "Chaojun Xiao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2102849134",
      "name": "Xiao-Zhi Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1998709255",
      "name": "Xu Han",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2506913441",
      "name": "Ruobing Xie",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4285134706",
    "https://openalex.org/W4292945941",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4286856923",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2964118342",
    "https://openalex.org/W4280525617",
    "https://openalex.org/W4300886482",
    "https://openalex.org/W2947012833",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4214872993",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4289828103",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W3174088532",
    "https://openalex.org/W4298079254",
    "https://openalex.org/W3147874613",
    "https://openalex.org/W3101738797",
    "https://openalex.org/W2951539960",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3207139774",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2045396155",
    "https://openalex.org/W3127433878",
    "https://openalex.org/W3152884768",
    "https://openalex.org/W3099264534",
    "https://openalex.org/W4385573694",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2021947606",
    "https://openalex.org/W4304697563",
    "https://openalex.org/W4212828284",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3024936740",
    "https://openalex.org/W4225899846",
    "https://openalex.org/W2145438170",
    "https://openalex.org/W3171589458",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W4287890953",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3196986263",
    "https://openalex.org/W1999653836",
    "https://openalex.org/W4307868887",
    "https://openalex.org/W4281643269",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W4299959846",
    "https://openalex.org/W2403749440",
    "https://openalex.org/W117096852",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W4287124167",
    "https://openalex.org/W4286008945"
  ],
  "abstract": "Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao, Xiaozhi Wang, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Jie Zhou. Findings of the Association for Computational Linguistics: ACL 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 4066–4083\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nEmergent Modularity in Pre-trained Transformers\nZhengyan Zhang1∗, Zhiyuan Zeng1∗, Yankai Lin2,3, Chaojun Xiao1, Xiaozhi Wang1\nXu Han1, Zhiyuan Liu1,4,5†, Ruobing Xie6, Maosong Sun1,4†, Jie Zhou6\n1NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing\n2Gaoling School of Artificial Intelligence, Renmin University of China, Beijing\n3 Beijing Key Laboratory of Big Data Management and Analysis Methods\n4International Innovation Center of Tsinghua University, Shanghai\n5Quan Cheng Laboratory 6 Pattern Recognition Center, WeChat AI, Tencent Inc\n{zy-z19,zengzy20}@mails.tsinghua.edu.cn {liuzy,sms}@tsinghua.edu.cn\nAbstract\nThis work examines the presence of modular-\nity in pre-trained Transformers, a feature com-\nmonly found in human brains and thought to\nbe vital for general intelligence. In analogy\nto human brains, we consider two main char-\nacteristics of modularity: (1) functional spe-\ncialization of neurons : we evaluate whether\neach neuron is mainly specialized in a cer-\ntain function, and find that the answer is yes.\n(2) function-based neuron grouping : we ex-\nplore finding a structure that groups neurons\ninto modules by function, and each module\nworks for its corresponding function. Given\nthe enormous amount of possible structures,\nwe focus on Mixture-of-Experts as a promis-\ning candidate, which partitions neurons into\nexperts and usually activates different experts\nfor different inputs. Experimental results show\nthat there are functional experts, where clus-\ntered are the neurons specialized in a certain\nfunction. Moreover, perturbing the activations\nof functional experts significantly affects the\ncorresponding function. Finally, we study how\nmodularity emerges during pre-training, and\nfind that the modular structure is stabilized at\nthe early stage, which is faster than neuron sta-\nbilization. It suggests that Transformers first\nconstruct the modular structure and then learn\nfine-grained neuron functions. Our code and\ndata are available at https://github.com/\nTHUNLP/modularity-analysis.\n1 Introduction\nRecently, pre-trained Transformers have shown the\npotential to achieve general intelligence (Brown\net al., 2021; Fei et al., 2022; Reed et al., 2022;\nOpenAI, 2023; Bubeck et al., 2023), which encour-\nages researchers to explore the analogy between\n∗Equal contribution\n†Corresponding authors\nTransformers and human brains (Toneva and We-\nhbe, 2019; Caucheteux et al., 2021; Caucheteux\nand King, 2022; Goldstein et al., 2022). These\nworks have shown that the behaviors of Transform-\ners resemble those of human brains. Do the internal\nstructures of Transformers also mirror those of hu-\nman brains in order to achieve similar behaviors?\nNeuroscientists have found that the structure of\nneuron organization in human brains follows a mod-\nular pattern (Bullmore and Sporns, 2009; Meunier\net al., 2010), which has two main characteristics.\n(1) Functional specialization of neurons : each\nneuron is mainly specialized in a certain function.\n(2) Function-based neuron grouping : neurons\nwith the same function are clustered in a local re-\ngion, and each function relies on a specific region.\nIn this work, we wonder whether Transformers also\norganize neurons in a modular way.\n(Q1) Are neurons functionally specialized?\nThe functional specialization of neurons is the ba-\nsis of modularity. To this end, we propose a novel\nframework to analyze the functionality of neurons\nin Transformers. In this framework, we study three\nrepresentative functions by a unified method, in-\ncluding semantic function (Scarlini et al., 2019a;\nSuau et al., 2020), knowledge function (Jiang et al.,\n2020; Dai et al., 2022), and task function (Wang\net al., 2022b). Experimental results show that the\nneurons in pre-trained Transformers become much\nmore specialized than those in randomly-initialized\nones after self-supervised learning on large-scale\ncorpora. Moreover, in pre-trained Transformers,\nthere are several groups of neurons, each of which\nexcels in a specific function.\n(Q2) Is there a modular structure of neurons?\nIn analogy to human brains, a modular structure\nshould group neurons with the same function to-\ngether as modules, and each module plays a cru-\ncial role in a specific function. Since there are\n4066\nthousands of neurons in a Transformer, it is im-\npractical to iterate over all possible structures of\nneurons. We consider the grouping of Mixture-of-\nExperts (MoE) (Jacobs et al., 1991; Fedus et al.,\n2022) as a promising candidate, which gracefully\npartitions neurons into experts and is widely used\nin Transformers. Moreover, most MoE models\nare sparsely activated, which is similar to human\nbrains. Specifically, we study two types of MoE\nmodels, pre-partitioned MoE (pre-MoE) and post-\npartitioned MoE (post-MoE). Pre-MoE refers to the\nmodel architectures that expand feedforward lay-\ners by MoE to improve model capacity before pre-\ntraining (Fedus et al., 2022). Post-MoE refers to the\nmodels that are converted from vanilla Transform-\ners to their MoE version by partitioning feedfor-\nward layers into experts after pre-training (Zhang\net al., 2022b). It provides a way to group neurons\nwithout changes in parameters and forward pass.\nBy studying the function distribution on experts,\nwe find that both pre-MoE and post-MoE Trans-\nformers have a strong tendency to distribute the\nneurons excelling in a certain function concentrat-\nedly into some experts. Moreover, perturbing the\nactivations of 3% of experts by function leads to\nperformance close to random guessing, which is\nmore significant than perturbing the same amount\nof individual neurons specialized in the function.\nTherefore, the MoE structure indeed reflects the\nmodularity of pre-trained Transformers.\n(Q3) How does modularity emerge during pre-\ntraining? By analyzing the pre-training process,\nwe find that the functions of expert networks are\nstabilized to a large extent at the early stage (around\n15% of the total training steps) for both pre-MoE\nand post-MoE Transformers, which is faster than\nthe neuron stabilization (around 75% of the total\nsteps). Our findings provide evidence for a coarse-\nto-fine mechanism of pre-training, which first con-\nstructs the coarse modular structure and then learns\nfine-grained neuron functions.\n2 Related Work\nInterpreting Pre-trained Transformers. Al-\nthough pre-trained Transformers have achieved\ngreat success in the field of NLP (Min et al., 2021;\nBommasani et al., 2021), their inner working mech-\nanism is still a black box. Researchers explore inter-\npreting the pre-trained Transformers (Rogers et al.,\n2020) from different perspectives, such as hidden\nrepresentations (Liu et al., 2019), attention matri-\nces (V oita et al., 2019; Clark et al., 2019b), and\noutput distributions (Petroni et al., 2019). Among\nthem, studying the behavior of a single neuron is an\nimportant branch (Radford et al., 2017; Sajjad et al.,\n2022; Bills et al., 2023), which is most related to\nour work. From the neurons in feedforward layers,\nresearchers have found various encoded informa-\ntion such as concepts, facts, and task abilities (Suau\net al., 2020; Dai et al., 2022; Wang et al., 2022b).\nIn this work, we study how these neurons are orga-\nnized to form a modular structure, which is a new\ninterpretation perspective.\nModularity of Neural Networks. Modular-\nity is a widespread property in complex systems,\nboth artificial (Ballard, 1987; Baldwin et al., 2000)\nand biological (V on Dassow and Munro, 1999;\nLorenz et al., 2011; Clune et al., 2013). Previ-\nous work mainly focuses on incorporating explic-\nitly designed modules into neural networks (An-\ndreas et al., 2016; Kirsch et al., 2018; Goyal et al.,\n2021). Recently, Hod et al. (2021); Csordás et al.\n(2021); Dobs et al. (2022) study whether standard\nneural networks become modular by themselves,\nand have discovered some naturally-emerging mod-\nular structures of CNNs and LSTMs. Compared\nto previous work, we extend the modular analysis\nto pre-trained Transformers, which are expected to\nbe more complex w.r.t. architecture and to contain\nmore knowledge.\nTransformers with MoE. Sparse MoE is usu-\nally used to enlarge the model capacity of Trans-\nformers while keeping the computational effi-\nciency (Lepikhin et al., 2021; Lewis et al., 2021;\nFedus et al., 2022). Specifically, for a given input,\nMoE conditionally selects a subset of experts to\nprocess the input and then combines the outputs of\nthese experts to generate the final output. Beyond\ncomputation efficiency, MoE is also used to im-\nplement modular Transformers (Gururangan et al.,\n2022; Zhang et al., 2022a; Pfeiffer et al., 2022;\nWang et al., 2022a). These works explicitly design\nextra constraints during pre-training to ensure the\nmodularization of expert networks. However, it is\nstill unclear whether modular experts can emerge\nnaturally in Transformers during pre-training.\n3 Functionality Evaluation\nIn this section, we first introduce the definition of\nneurons and how to evaluate the functionality of\nneurons and experts. Then, we briefly introduce the\nevaluation setups, including the pre-trained models.\n4067\nNeurons in Transformer.Following Dai et al.\n(2022); Wang et al. (2022b), we study the neu-\nrons in the feedforward networks (FFNs), which\naccount for about two-thirds of Transformer param-\neters. Previous work has shown that there is rich\ninformation in FFNs (Suau et al., 2020; Geva et al.,\n2021, 2022; Dai et al., 2022; Wang et al., 2022b).\nSpecifically, the FFN is a two-layer MLP and\ncomputes the output by FFN(x) =WOσ(WIx +\nbI) +bO, where WI ∈Rdff ×d,WO ∈Rd×dff\nare the weight matrices, bI ∈Rdff ,bO ∈Rd are\nthe bias vectors, dis the dimensions of input and\noutput, dff is the dimensions of intermediate hid-\nden states, and σ is the activation function. For\nsimplicity, we discard the bias terms in the follow-\ning part. For fine-grained analysis, we dissect the\nFFN into neurons and rewrite the FFN equation as\nFFN(x) =\ndff∑\ni=1\nσ(WI\ni,: · x)WO\n:,i, (1)\nwhere WI\ni,: and WO\n:,i are the i-th row and column\nof WI and WO, respectively. The FFN output\nis the sum of the outputs of all neurons. From\nthis perspective, we define a neuron ni as a row\nvector WI\ni,: and a column vector WO\n:,i . The neuron\nactivation of ni is σ(WI\ni,: ·x). The number of\nneurons in an FFN is equal to the intermediate\nhidden dimension dff .\nSparse Mixture-of-experts in Transformer is a\nvariant of FFN (Lepikhin et al., 2021), which sig-\nnificantly increases the model capacity by adding\nmore parameters and keeps the computational cost\naffordable. In MoE layers, each expert is an FFN,\nand the output of the MoE layer is the weighted\nsum of the outputs of all experts. We can also define\nneurons of MoE as Equation 1. The neuron-based\nform of MoE is provided in Appendix A.1.\nPredictivity for Functions. To comprehensively\nstudy the functions of neurons and experts, we\ncover three typical functions. For each function,\nwe construct various sub-functions, each of which\nis a fine-grained version of the function. There are\n576 sub-functions in total. Here we introduce the\nthree functions and their sub-functions.\nSemantic Function. Semantic function refers to\nthe ability to understand the meaning of input texts.\nIn this work, we focus on how neurons capture\nthe patterns of word senses. We use a large-scale\ndataset with word-sense annotations, OneSec (Scar-\nlini et al., 2019b), to construct binary classification\ndata for semantic sub-functions. In OneSec, each\nsentence has a keyword whose sense is annotated\nbased on Wikipedia1. We first filter out the key-\nwords that have more than one sense in the dataset\nand then randomly select 100 sentences for each\nsense. For each sense pair of a word, we construct\na binary classification dataset by labeling the sen-\ntences with one sense as positive and the sentences\nwith the other sense as negative. Finally, we have\n529 semantic sub-functions, each of which is a bi-\nnary classification problem to distinguish the two\nsenses of a word. For experiments on Switch Trans-\nformer in Section 6, we randomly sample 100 se-\nmantic sub-functions for evaluation.\nKnowledge Function. Knowledge function\nrefers to the ability to memorize factual knowledge.\nIn this work, we focus on the factual triples, which\nare used to construct knowledge graphs. We define\na knowledge sub-function as a binary classification\nto identify whether a triple is correct. Specifically,\nwe sample several triples from Wikidata as positive\ninstances and randomly replace their head or tail\nentities to construct negative instances. We group\nthese instances according to their relations and\neach relation has its corresponding knowledge sub-\nfunction. There are 39 knowledge sub-functions\nfrom T-REx (Elsahar et al., 2018; Elazar et al.,\n2021) and each sub-function has 400 instances.\nTask Function. Task function refers to the ability\nto perform downstream tasks. Previous work has\nshown that training a small part of parameters in\npre-trained Transformers can achieve comparable\nperformance to full-parameter fine-tuning (Lester\net al., 2021; Hu et al., 2022) so that Transformers\nare supposed to learn amounts of task knowledge\nfrom pre-training. In this work, we use several\nclassification datasets. from GLUE (Wang et al.,\n2018), including SST-2 (Socher et al., 2013), QQP2,\nMNLI (Williams et al., 2018), CoLA (Warstadt\net al., 2018), MRPC (Dolan and Brockett, 2005),\nRTE (Dagan et al., 2006), QNLI (Rajpurkar et al.,\n2016). There are 8 task sub-functions in total be-\ncause MNLI is split into two binary classification\ntasks. To stimulate these sub-functions, we adopt\nthe input templates provided by (Raffel et al., 2020)\nto improve neuron predictivity. For each label class,\nwe randomly sample 1000 instances for evaluation\nif the number of instances under this class is larger\nthan 1000.\nAdmittedly, coarse is our function classifica-\n1https://en.wikipedia.org/\n2https://data.quora.com/\nFirst-Quora-Dataset-Release-Question-Pairs\n4068\ntion. It does not cover all functions learned by\npre-trained Transformers, and there are interac-\ntions between each pair of functions so there is\nunavoidable overlap. However, we focus on a uni-\nfied framework and concrete evaluation approach,\nand they can be easily generalized to other ways\nof function classification, meaning that our con-\ntribution is independent of function classification.\nUsing this way of classification is simply due to its\ntypicality.\nTo evaluate the ability of a neuron to capture the\npattern of a sub-function, we compute the predic-\ntivity of the neuron activations for the sub-function.\nFollowing Suau et al. (2020); Wang et al. (2022b),\nwe focus on the sub-functions that can be formu-\nlated as a binary classification problem.\nWe denote the dataset of a sub-function as\nD = {(si,yi)}|D|\ni=1, where si is the input se-\nquence and yi ∈ {0,1}is the label. Since the\ncomputation of FFNs is word-wise, we define\nthe activation of the neuron nj of a sequence si\nas aij = maxxk∈si σ(WI\nj,: ·xk), where si =\n{x1,x2,..., xl}is the hidden states of si and l\nis the length of si. Then, we have the pairs of\nneuron activations and labels, Aj = {(aij,yi)}|D|\ni=1.\nBased on Aj, we compute the average precision\n(AP) (Zhu, 2004) of the neuron activations as the\npredictivity of the neuron nj. For each expert, we\ncompute the average AP of all neurons in the expert\nas the predictivity of the expert for the sub-function.\nPlease refer to Appendix A.2 for more details.\nEvaluation Setups. We evaluate two kinds\nof MoE models, pre-partitioned MoE and post-\npartitioned MoE. (1) Pre-partitioned MoE ex-\npands feedforward layers by MoE before pre-\ntraining. Switch Transformer (Fedus et al., 2022)\nis a representative pre-MoE model. The architec-\nture of Switch Transformer is similar to T5 (Raffel\net al., 2020) except that Switch Transformer re-\nplaces FFNs in the even Transformer layer with\nMoE layers. (2) Post-partitioned MoE converts\na pre-trained vanilla Transformer into its MoE\nmodel by partitioning FFNs into experts. Since\nZhang et al. (2022b) show that vanilla Transform-\ners have implicit MoE structures by discovering\nthe inner correlation among neurons, we use the\nsame method to MoEfy vanilla Transformers. Note\nthat we only adopt MoEfication to provide a neu-\nron grouping. We do not change any architecture,\nparameters, forward function (computing all neu-\nrons and then summing them up), etc, making the\nMoE-fied T5 identical to the original T5.\nDue to the computational cost, we consider the\nSwitch Transformer with 16 experts and use the\nsame number of experts for the post-MoE models.\nSince the functionality evaluation does not involve\ndecoding, we only compute the neuron and expert\npredictivities of the encoders. Besides, we focus on\nthe neurons in MoE layers for Switch Transformer\nto facilitate the modular analysis in the following\nsections.\n4 Functional Specialization of Neurons\nIn this section, we analyze the functional special-\nization of neurons. First, we compare the neu-\nron predictivities of pre-trained models with those\nof randomly-initialized models at the layer level,\nwhich will present a general picture of neuron func-\ntions. Second, we study how functions distribute\non neurons in each layer.\nNeuron predictivities of different layers. At\neach layer, we compute the best predictivity of neu-\nrons for each sub-function and then calculate the\naverage best predictivity among all sub-functions\nin each function. For comparisons, we also eval-\nuate randomly-initialized models. We report the\nresults in the left part of Figure 1.\nFrom this figure, we have two observations.\n(1) The average best predictivities of pre-trained\nneurons are significantly higher than those of\nrandomly-initialized neurons, indicating that the\nneurons have learned these functions from pre-\ntraining and the neurons with top-ranked predictiv-\nities indeed excel in corresponding sub-functions.\n(2) The best predictivity of the task function in-\ncreases with the layer number while the best pre-\ndictivities of the semantic and knowledge functions\nvary little across layers. It suggests that the task\nfunction may be more difficult than the semantic\nand knowledge functions so the higher layers are\nmore suitable for learning the task function. Note\nthat we evaluate the frozen pre-trained models on\nthe tasks without fine-tuning or prompt-tuning and\nthe pre-training data of Switch Transformer do not\ncontain the tasks. Hence, we can exclude the possi-\nbility of optimization artifacts (Durrani et al., 2021),\nwhich may make the higher layers more related to\nthe task function.\nDistribution in each layer. We first identify\nthe neurons with the top predictivity rankings for\neach sub-function as sub-functional neurons in\neach layer and then compute the overlap between\n4069\nSemantic\n<latexit sha1_base64=\"nTJwjYoNfd7YGWk5Jg7/56Gz/bs=\">AAACI3icbVDLSsNAFJ34tj5adekmWARXJRFFl0U3Ln21FppSJtPbOjgzCTM3agn5Erf6Ne7EjQs/RXDSZmFbDwwczn2cOyeMBTfoeV/O3PzC4tLyymppbX1js1zZ2m6aKNEMGiwSkW6F1IDgChrIUUAr1kBlKOAufDjP63ePoA2P1C0OY+hIOlC8zxlFK3Ur5QDhGdMbkFQhZ1m3UvVq3gjuLPELUiUFLruVn6AXsUSCQiaoMW3fi7GTUm23CchKQWIgpuyBDqBtqaISTCcdHZ65+1bpuf1I26fQHal/J1IqjRnK0HZKivdmupaL/9XaCfZPOylXcYKg2NionwgXIzdPwe1xDQzF0BLKNLe3uuyeasrQZjXhEkr7BwVPLJI2ol4aXGdpkBuGYXqd5Xn50+nMkuZhzT+qHV8dVetnRXIrZJfskQPikxNSJxfkkjQIIwl5Ia/kzXlz3p0P53PcOucUMztkAs73L7MAphU=</latexit>\nKnowledge\n<latexit sha1_base64=\"TQxrrtf1GGMk+a+tOiQIJsCd1So=\">AAACJHicbVDLSsNAFJ34rO+oSzfBIrgqiSi6FN0IblSsCk0pk8ltHZxHmLmplpA/cWu/xp24cOOfCE5qF74ODBzOuZdz5ySZ4BbD8M2bmJyanpmtzc0vLC4tr/ira1dW54ZBk2mhzU1CLQiuoIkcBdxkBqhMBFwnd8eVf90HY7lWlzjIoC1pT/EuZxSd1PH9GOEBi1Ol7wWkPSg7fj1shCMEf0k0JnUyxlnH/4hTzXIJCpmg1raiMMN2QQ1yJqCcj3MLGWV3tActRxWVYNvF6PIy2HJKGnS1cU9hMFK/bxRUWjuQiZuUFG/tb68S//NaOXYP2gVXWY6g2FdQNxcB6qCqIUi5AYZi4AhlhrtbA3ZLDWXoyvqRkkj3BwX3TEtJVVrEF2URV4FJUlyUVV/R73b+kqudRrTb2DvfrR8ejZurkQ2ySbZJRPbJITkhZ6RJGOmTR/JEht7Qe/ZevNev0QlvvLNOfsB7/wSUp6aL</latexit>\nTask\n<latexit sha1_base64=\"400J+XfkSfO33mkbANDTQbOf/Go=\">AAACHXicbVBNS8NAEN3U7++qRy/BIngqiVT0WPTiUcWq0BTZbKft0v0IuxO1hPwMr/prvIlX8ccIbmoOtjow8Hhvhjfz4kRwi0Hw6VVmZufmFxaXlldW19Y3qptb11anhkGLaaHNbUwtCK6ghRwF3CYGqIwF3MTD00K/uQdjuVZXOEqgI2lf8R5nFB3VjhAeMbuidpjfVWtBPRiX/xeEJaiRss7vql9RV7NUgkImqLXtMEiwk1GDnAnIl6PUQkLZkPah7aCiEmwnG5+c+3uO6fo9bVwr9Mfs742MSmtHMnaTkuLATmsF+Z/WTrF33Mm4SlIExX6MeqnwUfvF/36XG2AoRg5QZri71WcDaihDl9KESyzdDwoemJaSqm4WXeZZVBjGcXaZF3mF0+n8BdcH9bBRP7xo1JonZXKLZIfskn0SkiPSJGfknLQII5o8kWfy4r14r96b9/4zWvHKnW0yUd7HN85wpBs=</latexit>\nDistribution Similarity\n<latexit sha1_base64=\"NoPDhpRSaCsjqITQb9DjI3ms/Ow=\">AAACMnicbVDLSgNBEJz1/TbqUZDBIHgKuxLRo6gHj76iQjaEmUlHB2dml5leNSx782u86s/oTbz6C4KzMQdfDQ1FVTdFFU+VdBiGz8HQ8Mjo2PjE5NT0zOzcfGVh8cwlmRXQEIlK7AVnDpQ00ECJCi5SC0xzBef8eq/Uz2/AOpmYU+yl0NLs0siuFAw91a6sxAh3mO97Jyt5VpL0RGqpmJXYK9qValgL+0P/gmgAqmQwh+3KR9xJRKbBoFDMuWYUptjKmUUpFBRTceYgZeKaXULTQ8M0uFbez1HQNc90aDexfg3SPvv9I2fauZ7m/lIzvHK/tZL8T2tm2N1u5dKkGYIRX0bdTFFMaFkK7UgLAlXPAyZ8cCmouGKWCfTV/XDh2mcwcCsSrZnp5PFxkcelIef5cVH2Ff1u5y8426hF9drmUb26sztoboIsk1WyTiKyRXbIATkkDSLIPXkgj+QpeApegtfg7et0KBj8LJEfE7x/AqT8rL4=</latexit>\n(a) Switch Transformer\n<latexit sha1_base64=\"coici4VCDuPUzL9aaJHeradIHR4=\">AAACMXicbVDLSiNBFK32PT6jLgehMAi6Cd2SYVyKblz6igrpEG5XbkxhVXVTdVsNTa/8mtmOX+NO3PoNA1Mds/B1oOBwzr2cuifJlHQUhk/BxOTU9Mzs3I/5hcWl5ZXa6tqFS3MrsCVSldqrBBwqabBFkhReZRZBJwovk5vDyr+8Retkas5pmGFHw7WRfSmAvNStbcSE91Rsww4/u5MkBvzcgnH91Gq0ZbdWDxvhCPwricakzsY47tb+xb1U5BoNCQXOtaMwo04BlqRQWM7HucMMxA1cY9tTAxpdpxidUfItr/S4j/bPEB+p7zcK0M4NdeInNdDAffYq8TuvnVN/r1NIk+WERrwF9XPFKeVVJ7wnLQpSQ09AWOn/ysUALAjyzX1ISbS/weCdSLUG0yvi07KIq8AkKU7Lqq/ocztfycVuI2o2fp006/sH4+bm2E+2ybZZxH6zfXbEjlmLCfbA/rC/7DF4DJ6C5+DlbXQiGO+ssw8IXv8DLFGrZA==</latexit>\n(b) T5\n<latexit sha1_base64=\"5UBweJgqeV2wPvP0iLOauF3We0w=\">AAACH3icbVDLSgMxFM34rO+qSzfBIuimzEhFl6IblypWhc4gSXqrwSQzJnfUMsx3uLVf407c+jGCmdqFrwOBwzn3cm4Oz5R0GIbvwdj4xOTUdG1mdm5+YXGpvrxy7tLcCmiLVKX2kjMHShpoo0QFl5kFprmCC357WPkX92CdTM0Z9jNINLs2sicFQy8lMcIjFpt8i57tlFf1RtgMh6B/STQiDTLC8VX9I+6mItdgUCjmXCcKM0wKZlEKBeVsnDvImLhl19Dx1DANLimGR5d0wytd2kutfwbpUP2+UTDtXF9zP6kZ3rjfXiX+53Vy7O0lhTRZjmDEV1AvVxRTWjVAu9KCQNX3hAkr/a1U3DDLBPqefqRw7f9g4EGkWjPTLeLTsoirQM6L07LqK/rdzl9yvt2MWs2dk1Zj/2DUXI2skXWySSKyS/bJETkmbSLIHXkiz2QQDIKX4DV4+xodC0Y7q+QHgvdPwk6j+A==</latexit>\nFigure 1: Left part: average predictivity of each function of each layer for the pre-trained models and their randomly-\ninitialized counterparts of (a) Switch Transformer and (b) T5. Right part: average distribution similarity between\ndifferent functions of different layers for the same pre-trained models. All of the similarities of random-initialized\nmodels are around 0.01, including the diagonal elements, which are significantly lower than those of the pre-trained\nmodels.\nthe two sets of sub-functional neurons. Formally,\nassuming that we identify the top k neurons for\neach sub-function, the overlap score is defined as∑\nni∈N1 I(ni∈N2)\nk , where N1 and N2 are the sets of\nneurons for the two considered sub-functions. If\nthe overlap score is high, there is a group of neu-\nrons that are good at both sub-functions. In the\nexperiments, we consider the neurons with the top\n1% predictivities for each sub-function. Since there\nare hundreds of sub-functions, it is impossible to\ndisplay all of them in a figure and we compute the\naverage overlap score between two functions to\nmeasure the distribution similarity between differ-\nent functions. Note that we omit the self-overlap\nscores, which are always equal to 1. The results\nare reported in the right part of Figure 1.\nFrom this figure, we observe that: (1) In the pre-\ntrained models, the distribution similarity of the\nsame function is significantly larger than that of\ndifferent functions, which indicates that there are\nsome groups of neurons, each of which is good at\na certain function. And, it is not mainly caused\nby the similarity that naturally exists between sub-\nfunctions of the same function because the results\nof randomly-initialized models do not show such\na phenomenon as shown in Appendix A.3. Hence,\nwe conclude that there are some emergent groups\nof functionally-specialized neurons after pre-\ntraining. (2) One neuron may be capable of mul-\ntiple sub-functions even from different functions.\nFor example, the average overlap score between\nthe knowledge function and task function is also\nsignificantly higher than that of random models so\nthere are some neurons good at both knowledge\nand task sub-functions.\n5 Finding Modular Structure of Neurons\nSince MoE is a promising candidate for the modu-\nlar structure of Transformers, we analyze the MoE\nstructure in this section. First, we verify whether\nthe neurons specialized in a certain function are\nconcentrated in some experts, called functional ex-\nperts. Second, we perturb the activations of experts\ncorresponding to a certain function. Third, we train\nthe model on different datasets, retaining only func-\ntional experts by removing non-functional experts.\nThe last two experiments verify the importance of\nthe experts for their corresponding functions.\nDistribution on experts. If experts were not\nfunctionally specialized, the sub-functional neu-\nrons would be randomly distributed among experts.\nHence, we conduct statistical hypothesis testing to\nevaluate whether the neuron distribution on experts\nis significantly different from random. Assume\nthat there are N neurons in a layer, nE neurons in\neach expert, ksub-functional neurons for each sub-\nfunction, and Msub-functions in a certain function.\nThe null hypothesis is that the sub-functional neu-\nrons of each sub-function are independently and\nrandomly distributed on experts, i.e., the number\nof sub-functional neurons in each expert follows a\nhypergeometric distribution with parameters N, K,\n4070\nModel Pre-trained Partitioning Semantics Knowledge Task\nProp. Degree Prop. Degree Prop. Degree\nSwitch Transformer\n/enc-37Random 0.002 0.037 0.001 0.024 0.001 0.018\n/enc-33Random 0.226 1.038 0.052 0.652 0.003 0.066\n/enc-33Pre-MoE 0.354 1.490 0.260 1.560 0.219 1.604\nT5\n/enc-37Random 0.016 0.257 0.001 0.031 0.001 0.017\n/enc-33Random 0.252 1.203 0.061 1.031 0.007 0.221\n/enc-33Post-MoE 0.338 2.000 0.214 2.686 0.120 3.276\nTable 1: Proportion of functional experts to all experts and their modularization degree. Higher modularization\ndegrees mean that functional experts have more sub-functional neurons than the expectation of uniform distribution.\nand nE. The sum of the numbers of sub-functional\nneurons on all sub-functions in an expert is denoted\nby ri.3 The alternative hypothesis is that an expert\nhas a larger ri than expected by chance.\nIn the experiments, we also treat the neurons\nwith the highest 1% predictivities for each sub-\nfunction as its sub-functional neurons. For each\nfunction, we compute the p-value of the sum of\nthe hypergeometric distribution for each expert and\nreject the null hypothesis if the p-value is less than\n0.001. We also conduct the same experiment on\nrandom partitioning, where the neurons are ran-\ndomly partitioned into expert-sized clusters, and\nrandomly-initialized counterparts. We regard the\nexperts that reject the null hypothesis as functional\nexperts and report the proportion of functional ex-\nperts to all experts in each function. And, we also\nconsider the modularization degree, which is de-\nfined as the relative ratio of functional neurons in\nthe expert compared to a uniform distribution. The\nproportion of functional experts and the modular-\nization degree are computed by\nProp. = Ef\nE , Degree = ri\nnE\n/( Mk\nN ), (2)\nwhere Ef is the number of functional experts, E\nis the number of experts, ri\nnE\nis the proportion of\nfunctional neurons in the expert, and Mk\nN is the\nproportion expectation of functional neurons under\na uniform distribution. The overall degree is 0 if\nno functional expert exists, and otherwise is the\naverage degree among all functional experts.\nThe results are shown in Table 1. From this\ntable, we have three observations. (1) Since the\nsub-function distributions of pre-trained models\nare not independent, the proportion of functional\n3We do not find a general form for the distribution of the\nsum of independent hypergeometric distributions. Since Kis\nsignificantly smaller than N, we approximate the hypergeo-\nmetric distribution with a binomial distribution.\n(b) Unseen Datasets\n<latexit sha1_base64=\"GAL2XLI1QrU9bmsTX/qY9XuEGJo=\">AAACLnicbVBNSwMxEM36/W3Vm16CRdBL2ZWKHkU9eFSxKnRLSdKpBpPsksyqZVnw13jVXyN4EK/+CcFs7cGvgcDjvZl5mcdTJR2G4UswNDwyOjY+MTk1PTM7N19ZWDxzSWYFNESiEnvBmQMlDTRQooKL1ALTXME5v94v9fMbsE4m5hR7KbQ0uzSyKwVDT7UryzHCHebrfIM2jAMw9ICh34euaFeqYS3sF/0LogGokkEdtSsfcScRmQaDQjHnmlGYYitnFqVQUEzFmYOUiWt2CU0PDdPgWnn/hoKueaZDu4n1zyDts98ncqad62nuOzXDK/dbK8n/tGaG3Z1WLk2aIRjxZdTNFMWEloHQjrQgUPU8YMJK/1cqrphlAn1sP1y49jcYuBWJ1sx08vikyOPSkPP8pCjzin6n8xecbdaiem3ruF7d3RskN0FWyCpZJxHZJrvkkByRBhHknjyQR/IUPAXPwWvw9tU6FAxmlsiPCt4/AVEPqek=</latexit>\n(a) Seen Datasets\n<latexit sha1_base64=\"o2ve9ulwaIBSaxYtxK9OKyhjJ8k=\">AAACLHicbVBNTxsxEPXSL6BfgV6QuFiNKtFLtIuoyhFBDxxTICRSNopmnUliYXtX9iwQrba/plfya3pBqFf+BRLekENJOpKlp/dm5nlekinpKAxvg5UXL1+9frO6tv723fsPH2sbm+cuza3AlkhVajsJOFTSYIskKexkFkEnCtvJxVGlty/ROpmaM5pk2NMwMnIoBZCn+rWtmPCaih34yk8RDf8B5LeRK/u1etgIZ8WXQTQHdTavZr/2EA9SkWs0JBQ4143CjHoFWJJCYbke5w4zEBcwwq6HBjS6XjG7oORfPDPgw9T6Z4jP2H8nCtDOTXTiOzXQ2C1qFfk/rZvTcL9XSJPlhEY8GQ1zxSnlVRx8IC0KUhMPQFjp/8rFGCwI8qE9c0m0v8HglUi1BjMo4pOyiCvDJClOyiqvaDGdZXC+24j2Gt9+7tUPDufJrbJt9pntsIh9ZwfsmDVZiwn2i/1mN2waTIM/wV3w96l1JZjPfGLPKrh/BHLOqPE=</latexit>\nFigure 2: Perturbation performance of T5. For “Ran-\ndom”, we randomly perturb neurons. For “SST-2”,\n“MRPC”, “CoLA”, “QQP”, we are guided by the neuron\npredictivities on each dataset and perturb the top-ranked\nneurons. For “Avg”, we sum the predictivities on all\ndatasets above and also perturb the top-ranked neurons.\nFor “MoE”, we consider the experts with top-ranked\nsums of the predictivities. The seen datasets are the four\ndatasets above. The unseen datasets include five other\ndatasets.\nexperts of pre-trained models with random parti-\ntioning is higher than that of random-initialized\ncounterparts. (2) However, the proportion of func-\ntional experts of pre-trained models with MoE par-\ntitioning is still higher than that with random par-\ntitioning. Moreover, the modularization degree\nof the functional experts in MoE structures is sig-\nnificantly higher than that in random partitioning.\nIt indicates that the experts of both pre-MoE and\npost-MoE are more likely to intensively include\nneurons excelling in a certain function. (3) We\nfurther compare the predictivities of the functional\nexperts and non-functional experts and find that\nthe functional experts have significantly higher pre-\ndictivities than the non-functional experts in their\ncorresponding functions. It indicates that our quan-\ntification for expert predictivities is consistent with\nthe concept of functional experts. More details are\nin Appendix A.4.\nPerturbation analysis. Furthermore, we con-\nduct perturbation experiments, which are widely\nused to analyze both biological and artificial neural\n4071\nnetworks (Cowley et al., 2022; Wang et al., 2022b),\nto evaluate the causal effect of experts on model per-\nformance. Since T5 is a dense model and Switch\nTransformer is a sparse model, we use different\nperturbation methods for them. Specifically, the\npre-MoE models only select one expert at each\nlayer, so we choose to perturb the selection func-\ntion. For the post-MoE models, which select all\nexperts at each layer, we perturb the activation val-\nues of the targeting experts. Due to the difference\nin experiment setups, the perturbation performance\nis also different and their perturbation results are\nnot comparable.\nFor T5, we perturb all neuron activations of the\ntarget experts by adding random noises to them\nand evaluate the perturbed models on the down-\nstream tasks. We rank experts according to their\nsum of the predictivities for several downstream\ndatasets and perturb the top-ranked experts. We\nregard the datasets used in computing the sum\nof the predictivities as seen datasets, including\nSST-2 (Socher et al., 2013), MRPC (Dolan and\nBrockett, 2005), CoLA (Warstadt et al., 2018), and\nQQP. To evaluate the generalization ability of the\nexperts, we also perturb them and evaluate the\nperturbed models on unseen datasets, including\nMNLI (Williams et al., 2018), QNLI (Rajpurkar\net al., 2016), CB (De Marneffe et al., 2019), Mul-\ntiRC (Khashabi et al., 2018), and BoolQ (Clark\net al., 2019a). We compute the average perfor-\nmance of the perturbed models on the seen and\nunseen datasets, respectively. For comparisons, we\nalso conduct neuron-level perturbation and keep\nthe proportion of perturbed neurons equal to that\nof expert-level perturbation. There are three kinds\nof neuron-level perturbations: (1) perturb the neu-\nrons that have top-ranked predictivities for a cer-\ntain dataset, (2) perturb the neurons that have top-\nranked sums of the predictivities for seen datasets,\nand (3) perturb the neurons randomly. We perturb\nthe neurons in the last four layers where the task\nfunction is mainly located as shown in Figure 1.\nFor fine-grained perturbation percentage, we parti-\ntion FFNs into 96 experts, which is different from\nthat of the other experiments.\nWe report the average accuracy of the perturbed\nmodels on the downstream tasks in Figure 2. From\nthis figure, we have three observations. (1) The\nexperts with high predictivities are very impor-\ntant for the model performance. For example,\nperturbing 10% of the experts by function in the\nOriginal No Function Function\n7-task Avg. 73.23 72.78 74.45\nTable 2: Perturbation performance of Switch Trans-\nformer on GLUE tasks.\nlast four layers (around 3% of the total experts\nin the model) decreases the average accuracy by\nnearly 30% and makes the model perform as ran-\ndom guessing. (2) For neuron-level perturbations,\n“Avg” perturbation achieves a larger performance\ndrop than single-dataset perturbation, which is ex-\npected because intuitively it perturbs the neurons\nwith high overall predictivities. (3) Perturbing\nexperts leads to a more significant performance\ndrop than perturbing individual neuronson both\nseen and unseen datasets when the perturbation pro-\nportion is higher than 6%. It suggests neurons in\nthe experts cooperate instead of working indepen-\ndently so perturbing them will influence the coop-\neration and lead to a significant performance drop.\nWe conclude single neurons can not perform a func-\ntion well in lack of the cooperation with modules\ndespite their high overall predictivity.\nFor Switch Transformer, in the last four layers,\nwe perturb the selection probabilities of experts.\nWe constraint the model to select from a subset\nof experts by setting the selection probabilities of\nthe other experts to 0 and fine-tune the perturbed\nmodels on the downstream tasks with the template\nin Gao et al. (2021). There are two kinds of per-\nturbations: (1) No Function: we only select the\nnon-functional experts in a certain task, which is\nequivalent to setting the activations of the func-\ntional experts to 0. (2) Function: we only select\nthe functional experts in a certain task. Since the\nnumber of functional experts is smaller than that of\nnon-functional experts, we randomly select a subset\nof non-functional experts for No Function to make\nthe size of the subset equal to that of the functional\nexperts. Besides, we also report the performance of\nthe original model without any perturbation. More\ndetails are in Appendix A.2. From Table 2, we\nobserve that avoiding the functional experts leads\nto an overall performance drop. Furthermore, only\nselecting the functional experts even achieves a\nhigher performance than the original model.\nIn summary, we observe that the specialized neu-\nrons tend to be located concentratedly in some ex-\nperts based on function and the functional experts\nplay an important role when the model performs\nrelated functions. Hence, it is reasonable to study\n4072\n(a) Switch Transformer\n<latexit sha1_base64=\"coici4VCDuPUzL9aaJHeradIHR4=\">AAACMXicbVDLSiNBFK32PT6jLgehMAi6Cd2SYVyKblz6igrpEG5XbkxhVXVTdVsNTa/8mtmOX+NO3PoNA1Mds/B1oOBwzr2cuifJlHQUhk/BxOTU9Mzs3I/5hcWl5ZXa6tqFS3MrsCVSldqrBBwqabBFkhReZRZBJwovk5vDyr+8Retkas5pmGFHw7WRfSmAvNStbcSE91Rsww4/u5MkBvzcgnH91Gq0ZbdWDxvhCPwricakzsY47tb+xb1U5BoNCQXOtaMwo04BlqRQWM7HucMMxA1cY9tTAxpdpxidUfItr/S4j/bPEB+p7zcK0M4NdeInNdDAffYq8TuvnVN/r1NIk+WERrwF9XPFKeVVJ7wnLQpSQ09AWOn/ysUALAjyzX1ISbS/weCdSLUG0yvi07KIq8AkKU7Lqq/ocztfycVuI2o2fp006/sH4+bm2E+2ybZZxH6zfXbEjlmLCfbA/rC/7DF4DJ6C5+DlbXQiGO+ssw8IXv8DLFGrZA==</latexit>\n(b) T5\n<latexit sha1_base64=\"5UBweJgqeV2wPvP0iLOauF3We0w=\">AAACH3icbVDLSgMxFM34rO+qSzfBIuimzEhFl6IblypWhc4gSXqrwSQzJnfUMsx3uLVf407c+jGCmdqFrwOBwzn3cm4Oz5R0GIbvwdj4xOTUdG1mdm5+YXGpvrxy7tLcCmiLVKX2kjMHShpoo0QFl5kFprmCC357WPkX92CdTM0Z9jNINLs2sicFQy8lMcIjFpt8i57tlFf1RtgMh6B/STQiDTLC8VX9I+6mItdgUCjmXCcKM0wKZlEKBeVsnDvImLhl19Dx1DANLimGR5d0wytd2kutfwbpUP2+UTDtXF9zP6kZ3rjfXiX+53Vy7O0lhTRZjmDEV1AvVxRTWjVAu9KCQNX3hAkr/a1U3DDLBPqefqRw7f9g4EGkWjPTLeLTsoirQM6L07LqK/rdzl9yvt2MWs2dk1Zj/2DUXI2skXWySSKyS/bJETkmbSLIHXkiz2QQDIKX4DV4+xodC0Y7q+QHgvdPwk6j+A==</latexit>\n(a) Switch Transformer\n<latexit sha1_base64=\"coici4VCDuPUzL9aaJHeradIHR4=\">AAACMXicbVDLSiNBFK32PT6jLgehMAi6Cd2SYVyKblz6igrpEG5XbkxhVXVTdVsNTa/8mtmOX+NO3PoNA1Mds/B1oOBwzr2cuifJlHQUhk/BxOTU9Mzs3I/5hcWl5ZXa6tqFS3MrsCVSldqrBBwqabBFkhReZRZBJwovk5vDyr+8Retkas5pmGFHw7WRfSmAvNStbcSE91Rsww4/u5MkBvzcgnH91Gq0ZbdWDxvhCPwricakzsY47tb+xb1U5BoNCQXOtaMwo04BlqRQWM7HucMMxA1cY9tTAxpdpxidUfItr/S4j/bPEB+p7zcK0M4NdeInNdDAffYq8TuvnVN/r1NIk+WERrwF9XPFKeVVJ7wnLQpSQ09AWOn/ysUALAjyzX1ISbS/weCdSLUG0yvi07KIq8AkKU7Lqq/ocztfycVuI2o2fp006/sH4+bm2E+2ybZZxH6zfXbEjlmLCfbA/rC/7DF4DJ6C5+DlbXQiGO+ssw8IXv8DLFGrZA==</latexit>\n(b) T5\n<latexit sha1_base64=\"5UBweJgqeV2wPvP0iLOauF3We0w=\">AAACH3icbVDLSgMxFM34rO+qSzfBIuimzEhFl6IblypWhc4gSXqrwSQzJnfUMsx3uLVf407c+jGCmdqFrwOBwzn3cm4Oz5R0GIbvwdj4xOTUdG1mdm5+YXGpvrxy7tLcCmiLVKX2kjMHShpoo0QFl5kFprmCC357WPkX92CdTM0Z9jNINLs2sicFQy8lMcIjFpt8i57tlFf1RtgMh6B/STQiDTLC8VX9I+6mItdgUCjmXCcKM0wKZlEKBeVsnDvImLhl19Dx1DANLimGR5d0wytd2kutfwbpUP2+UTDtXF9zP6kZ3rjfXiX+53Vy7O0lhTRZjmDEV1AvVxRTWjVAu9KCQNX3hAkr/a1U3DDLBPqefqRw7f9g4EGkWjPTLeLTsoirQM6L07LqK/rdzl9yvt2MWs2dk1Zj/2DUXI2skXWySSKyS/bJETkmbSLIHXkiz2QQDIKX4DV4+xodC0Y7q+QHgvdPwk6j+A==</latexit>\nFigure 3: Changing curves of the proportion of functional experts and their modularization degree. We also mark\nthe value of the last checkpoint with random partitioning on the curve by points with different shapes.\nthe modularity of Transformers by MoE.\n6 Emergence of Modularity\nIn this section, we study the emergence of modular-\nity during pre-training. To this end, we pre-train the\nbase version of T5 and Switch Transformer from\nscratch. We use the MoEfication partitioning of the\nlast checkpoint as the MoE structure of T5. More\ndetails of pre-training are in Appendix A.2.\nEmergence Patterns of Functional Experts.\nWe first study the changing curves of the propor-\ntion of functional experts and their modularization\ndegree during pre-training, i.e., we apply the same\nanalysis in Section 5 to each checkpoint. The re-\nsults are shown in Figure 3. We have the follow-\ning observations. (1) The proportion of functional\nexperts and their modularization degree quickly\nachieves a high point, and then keeps relatively sta-\nble till the end. It indicates that functional experts\nemerge at the early stage of pre-training. (2) The\nproportion of functional experts in the Switch\nTransformer fluctuates significantly at about 20K\nsteps and its stabilization is slower than that of T5.\nIt suggests that the emergence of the modular struc-\nture in Switch Transformer is surprisingly more\ndifficult than T5. The reason may be that Switch\nTransform omits the gradients of unselected ex-\nperts, which causes the optimization to be harder\nthan that of T5 (Du et al., 2022; Zoph et al., 2022).\nStabilization of Experts and Neurons. Even\nthough clear is the changing curve of the number\nand modularization degree of functional experts\nfrom a global perspective, we still do not know how\nthe predictivities of neurons and experts changes.\nThere are two kinds of predictivity dynamics.\nThe first is the changing of the absolute predictivi-\nties, and the second is the relative order changing of\npredictivities among all experts or neurons. While\nit is straightforward to study the absolute predic-\ntivities, it is difficult to have a consistent analysis\nstandard due to different scales for different func-\ntions and layers. Hence, we focus on the relative\norder changing of predictivities. Intuitively, some\nexperts or neurons may excel in a sub-function at a\ncertain stage, and maintain this relative dominance\nas pre-training continues. With this in mind, we\nstudy the stabilization of predictivity rankings.\nTo study the stabilization of predictivity rank-\nings, we quantify the similarity between a layer\nof two model checkpoints w.r.t. a particular sub-\nfunction, which is either at the expert level or at the\nneuron level. Specifically, for a sub-function, we\ndefine such a similarity as Spearman’s rank correla-\ntions (Spearman, 1961) between the predictivities\nof experts or neurons in the considered layer of the\ntwo checkpoints. In this way, we measure to what\nextent the predictivities of the two checkpoints is\npositively correlated. We measure the similarity\nbetween two adjacent (saved) checkpoints as sta-\nbilization score, which reflects the trend toward\nstabilization. Higher similarity indicates a lower\nchanging pace and thus a higher degree of stabi-\nlization. For each function, we show the curve of\naverage stabilization score among all sub-functions\nin it and across all layers, both at the expert level\nand neuron level. To facilitate our analysis, we also\nmeasure it on random partitioning.\nWe report the result in Figure 4. From this fig-\nure, we have four observations. (1) During the\npre-training, both experts and neurons are increas-\ningly stabilized. (2) Experts are stabilized to a large\nextent at the early stage of pre-training. It takes\naround 15% of the total training steps for the ex-\npert predictivities to achieve a stabilization score of\n0.9. (3) Expert stabilization is notably faster than\nboth neuron stabilization (around 75% of the to-\ntal training steps) and the stabilization for random\npartitioning4. In conclusion, we see strong evi-\ndence that coarse-to-fine is the inner mechanism\nof pre-training. Transformer first learns a modular\n4While the numbers of experts and neurons are different,\nthe correlation scores are comparable (De Winter et al., 2016).\n4073\n(a)\n<latexit sha1_base64=\"IiQcAAYHuIDgHl1FpQD455bTZiw=\">AAACHHicbVBNS8NAEN34WetX1aOXYBHqpSSi6FH04rEWW8WmlM122i7ubsLuRC0h/8Kr/TXexKvgjxHctD1odWDg8d4Mb+aFseAGPe/TmZtfWFxaLqwUV9fWNzZLW9tNEyWaQYNFItK3ITUguIIGchRwG2ugMhRwE95f5PrNA2jDI3WNwxjakvYV73FG0VJ3AcITphV6kHVKZa/qjcv9C/wpKJNp1Tqlr6AbsUSCQiaoMS3fi7GdUo2cCciKQWIgpuye9qFloaISTDsdX5y5+5bpur1I21bojtmfGymVxgxlaCclxYGZ1XLyP62VYO+0nXIVJwiKTYx6iXAxcvP33S7XwFAMLaBMc3urywZUU4Y2pF8uobQ/KHhkkZRUddOgnqVBbhiGaT3L8/Jn0/kLmodV/6h6fHVUPjufJlcgu2SPVIhPTsgZuSQ10iCMKPJMXsjIGTmvzpvzPhmdc6Y7O+RXOR/fKXSjMA==</latexit>\n(b)\n<latexit sha1_base64=\"roBzL38q0iLY5OiUBSulvSE7c0s=\">AAACHHicbVDLSgNBEJz1bXxFPXpZDIJewq4oehS9eFQxD8wGmZl0dHBmdpnpVcOyf+FVv8abeBX8GMHZJAeT2NBQVHVT3cUSKSwGwbc3NT0zOze/sFhaWl5ZXSuvb9RtnBoONR7L2DQZtSCFhhoKlNBMDFDFJDTYw1mhNx7BWBHra+wl0Fb0Touu4BQddRMhPGO2y/by23IlqAb98idBOAQVMqyL2/JP1Il5qkAjl9TaVhgk2M6oQcEl5KUotZBQ/kDvoOWgpgpsO+tfnPs7jun43di41uj32b8bGVXW9hRzk4rivR3XCvI/rZVi97idCZ2kCJoPjLqp9DH2i/f9jjDAUfYcoNwId6vP76mhHF1IIy5MuR80PPFYKao7WXSVZ1FhyFh2lRd5hePpTIL6fjU8qB5eHlROTofJLZAtsk12SUiOyAk5JxekRjjR5IW8kjfvzXv3PrzPweiUN9zZJCPlff0CKyWjMQ==</latexit>\nSemantic\n<latexit sha1_base64=\"nTJwjYoNfd7YGWk5Jg7/56Gz/bs=\">AAACI3icbVDLSsNAFJ34tj5adekmWARXJRFFl0U3Ln21FppSJtPbOjgzCTM3agn5Erf6Ne7EjQs/RXDSZmFbDwwczn2cOyeMBTfoeV/O3PzC4tLyymppbX1js1zZ2m6aKNEMGiwSkW6F1IDgChrIUUAr1kBlKOAufDjP63ePoA2P1C0OY+hIOlC8zxlFK3Ur5QDhGdMbkFQhZ1m3UvVq3gjuLPELUiUFLruVn6AXsUSCQiaoMW3fi7GTUm23CchKQWIgpuyBDqBtqaISTCcdHZ65+1bpuf1I26fQHal/J1IqjRnK0HZKivdmupaL/9XaCfZPOylXcYKg2NionwgXIzdPwe1xDQzF0BLKNLe3uuyeasrQZjXhEkr7BwVPLJI2ol4aXGdpkBuGYXqd5Xn50+nMkuZhzT+qHV8dVetnRXIrZJfskQPikxNSJxfkkjQIIwl5Ia/kzXlz3p0P53PcOucUMztkAs73L7MAphU=</latexit>\nKnowledge\n<latexit sha1_base64=\"TQxrrtf1GGMk+a+tOiQIJsCd1So=\">AAACJHicbVDLSsNAFJ34rO+oSzfBIrgqiSi6FN0IblSsCk0pk8ltHZxHmLmplpA/cWu/xp24cOOfCE5qF74ODBzOuZdz5ySZ4BbD8M2bmJyanpmtzc0vLC4tr/ira1dW54ZBk2mhzU1CLQiuoIkcBdxkBqhMBFwnd8eVf90HY7lWlzjIoC1pT/EuZxSd1PH9GOEBi1Ol7wWkPSg7fj1shCMEf0k0JnUyxlnH/4hTzXIJCpmg1raiMMN2QQ1yJqCcj3MLGWV3tActRxWVYNvF6PIy2HJKGnS1cU9hMFK/bxRUWjuQiZuUFG/tb68S//NaOXYP2gVXWY6g2FdQNxcB6qCqIUi5AYZi4AhlhrtbA3ZLDWXoyvqRkkj3BwX3TEtJVVrEF2URV4FJUlyUVV/R73b+kqudRrTb2DvfrR8ejZurkQ2ySbZJRPbJITkhZ6RJGOmTR/JEht7Qe/ZevNev0QlvvLNOfsB7/wSUp6aL</latexit>\nTask\n<latexit sha1_base64=\"400J+XfkSfO33mkbANDTQbOf/Go=\">AAACHXicbVBNS8NAEN3U7++qRy/BIngqiVT0WPTiUcWq0BTZbKft0v0IuxO1hPwMr/prvIlX8ccIbmoOtjow8Hhvhjfz4kRwi0Hw6VVmZufmFxaXlldW19Y3qptb11anhkGLaaHNbUwtCK6ghRwF3CYGqIwF3MTD00K/uQdjuVZXOEqgI2lf8R5nFB3VjhAeMbuidpjfVWtBPRiX/xeEJaiRss7vql9RV7NUgkImqLXtMEiwk1GDnAnIl6PUQkLZkPah7aCiEmwnG5+c+3uO6fo9bVwr9Mfs742MSmtHMnaTkuLATmsF+Z/WTrF33Mm4SlIExX6MeqnwUfvF/36XG2AoRg5QZri71WcDaihDl9KESyzdDwoemJaSqm4WXeZZVBjGcXaZF3mF0+n8BdcH9bBRP7xo1JonZXKLZIfskn0SkiPSJGfknLQII5o8kWfy4r14r96b9/4zWvHKnW0yUd7HN85wpBs=</latexit>\nFigure 4: Spearman’s rank correlation between the functionality distributions of two adjacent checkpoints.\nstructure, where the structure becomes stable at the\nearly stage, and then there is a fine-grained process\nto learn neuron functions.\n7 Discussion\nEfficient Pre-training. Large-scale pre-training\nof Transformers is very expensive (Brown et al.,\n2021). It is promising to use MoE to reduce the\ncomputational cost by activating only a small part\nof the experts. Our findings have shown the emer-\ngent modularity of experts, which demonstrates\nthe reasonableness of the MoE structure. How-\never, as shown in Section 6, Switch Transformer\nis more unstable than T5 on the modular structure\nat the beginning. It suggests that we should begin\nwith a dense model and gradually make it sparse\ninstead of directly training a sparse model from\nscratch, which has been explored in some prelimi-\nnary works (Nie et al., 2021; Hazimeh et al., 2021).\nModel Fusion. Considering there are many\npre-trained models on different corpora and even\nmodalities, researchers have started to explore how\nto fuse them to aggregate different knowledge to-\ngether. Compared with model ensembling, model\nfusion has the potential to be more efficient because\nit does not compute all of the models. Much of the\ncurrent research in model fusion focuses on weight\naveraging and achieves some promising results (Li\net al., 2022; Matena and Raffel, 2021). However,\nweight averaging requires two models having the\nsame architecture, which is not always the case.\nIn this work, we discover the modular structure\nof pre-trained Transformers, which may facilitate\nthe model fusion based on module combinations,\nwhich gets rid of the architecture constraint.\nConnection between Brains and Pre-trained\nTransformers. Building an artificial brain that\ncorresponds to the human brain is an impor-\ntant neuroscience problem, e.g., the Blue Brain\nproject (Markram, 2006). Currently pre-trained\nTransformers show strong power for predict-\ning brain signals (Toneva and Wehbe, 2019;\nCaucheteux et al., 2021), but more fine-grained\nconnections between the two are still not clear. In\nanalogy to brain regions, we present the modular\nstructure of pre-trained Transformers. It will be\ninteresting to explore the connection between brain\nregions and the Transformer modules in the future.\n8 Conclusion\nIn this paper, we study the modularity of pre-\ntrained Transformers and validate the emergence of\nmodularity. We also study the pre-training process\nto understand the emergence of modularity and find\nthe coarse-to-fine mechanism of pre-training. We\nexpect our evaluation framework and findings will\nfacilitate and inspire future research in this area.\nLimitations\nThe major limitations of our work are as follows:\n(1) We show that the neuron structure of MoE\nreveals the presence of modularity in pre-trained\nTransformers. However, the MoE structure is not\nthe only possible modular structure. To better un-\nderstand the modular structure of Transformers,\nwe need to explore more types of structures. For\nexample, the number of neurons in each module\ncould be different, and the modular structure could\n4074\nbe hierarchical, where modules are grouped into\nlarger modules. (2) We study three typical func-\ntions for language processing: semantic function,\nknowledge function, and task function. There are\nmany other functions that could be studied, such\nas the syntactic function, discourse function, etc.\nMoreover, our categorization of functions may be\nnot suitable for pre-trained Transformers because\nthere are some overlaps between studied functions.\nA new Transformer-based function categorization\nmay be needed. (3) We transform T5 into its MoE\nversion to study its modular structure while not all\ndense pre-trained Transformers can be studied in\nthis way because the adopted MoEfication tech-\nnique (Zhang et al., 2022b) can only transform\nReLU-based Transformers. Studying the modular-\nity of other dense pre-trained Transformers, such\nas BERT, is also important for future research.\nAcknowledgements\nThis work is supported by the National Key R&D\nProgram of China (No. 2020AAA0106502) and\nInstitute Guo Qiang at Tsinghua University.\nAuthor Contributions Zhengyan Zhang and\nZhiyuan Zeng wrote the code and conducted the\nexperiments. Zhengyan Zhang and Zhiyuan Zeng\nwrote the initial draft. Yankai Lin, Chaojun Xiao,\nXiaozhi Wang, Xu Han, and Zhiyuan Liu, Ruobing\nXie significantly edited and improved the paper.\nMaosong Sun and Jie Zhou provided valuable ad-\nvice to the research.\nReferences\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Neural module networks. In Pro-\nceedings of CVPR, pages 39–48.\nCarliss Young Baldwin, Kim B Clark, Kim B Clark,\net al. 2000. Design rules: The power of modularity,\nvolume 1. MIT press.\nDana H. Ballard. 1987. Modular learning in neural\nnetworks. In Proceedings of AAAI, pages 279–284.\nSteven Bills, Nick Cammarata, Dan Moss-\ning, Henk Tillman, Leo Gao, Gabriel Goh,\nIlya Sutskever, Jan Leike, Jeff Wu, and\nWilliam Saunders. 2023. Language mod-\nels can explain neurons in language models.\nhttps://openaipublic.blob.core.windows.\nnet/neuron-explainer/paper/index.html.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, et al.\n2021. On the opportunities and risks of foundation\nmodels. arxiv preprint arXiv:2108.07258.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2021. Language models are Few-Shot learners. In\nProceedings of NeurIPS, pages 1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohannes Gehrke, Eric Horvitz, Ece Kamar, Peter\nLee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg,\nHarsha Nori, Hamid Palangi, Marco Túlio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general\nintelligence: Early experiments with GPT-4. arxiv\npreprint arXiv:2303.12712.\nEd Bullmore and Olaf Sporns. 2009. Complex brain\nnetworks: graph theoretical analysis of structural and\nfunctional systems. Nature reviews neuroscience,\n10(3):186–198.\nCharlotte Caucheteux, Alexandre Gramfort, and Jean-\nRemi King. 2021. Disentangling syntax and seman-\ntics in the brain with deep networks. In Proceedings\nof ICML, volume 139, pages 1336–1348.\nCharlotte Caucheteux and Jean-Rémi King. 2022.\nBrains and algorithms partially converge in natu-\nral language processing. Communications biology,\n5(1):1–10.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019a. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof NAACL-HLT 2019.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019b. What does BERT\nlook at? an analysis of bert’s attention. In Proceed-\nings of BlackboxNLP, pages 276–286.\nJeff Clune, Jean-Baptiste Mouret, and Hod Lipson. 2013.\nThe evolutionary origins of modularity. Proceed-\nings of the Royal Society b: Biological sciences ,\n280(1755):20122863.\nBenjamin R. Cowley, Adam J. Calhoun, Nivedita Ran-\ngarajan, Jonathan W. Pillow, and Mala Murthy. 2022.\nOne-to-one mapping between deep network units and\nreal neurons uncovers a visual population code for\nsocial behavior. bioRxiv.\nRóbert Csordás, Sjoerd van Steenkiste, and Jürgen\nSchmidhuber. 2021. Are neural nets modular? in-\nspecting functional modularity through differentiable\nweight masks. In Proceedings of ICLR.\n4075\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evaluat-\ning predictive uncertainty, visual object classification,\nand recognising tectual entailment, pages 177–190.\nSpringer.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons\nin pretrained transformers. In Proceedings of ACL,\npages 8493–8502.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The CommitmentBank: Inves-\ntigating projection in naturally occurring discourse.\nJoost CF De Winter, Samuel D Gosling, and Jeff Potter.\n2016. Comparing the pearson and spearman corre-\nlation coefficients across distributions and sample\nsizes: A tutorial using simulations and empirical data.\nPsychological methods, 21(3):273.\nKatharina Dobs, Julio Martinez, Alexander JE Kell,\nand Nancy Kanwisher. 2022. Brain-like functional\nspecialization emerges spontaneously in deep neural\nnetworks. Science advances, 8(11):eabl8913.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the International Workshop on\nParaphrasing.\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret\nZoph, Liam Fedus, Maarten P. Bosma, Zongwei\nZhou, Tao Wang, Yu Emma Wang, Kellie Webster,\nMarie Pellat, Kevin Robinson, Kathleen S. Meier-\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\nQuoc V . Le, Yonghui Wu, Zhifeng Chen, and Claire\nCui. 2022. Glam: Efficient scaling of language mod-\nels with mixture-of-experts. In Proceedings of ICML,\npages 5547–5569.\nNadir Durrani, Hassan Sajjad, and Fahim Dalvi. 2021.\nHow transfer learning impacts linguistic knowledge\nin deep NLP models? In Findings of ACL/IJCNLP\n2021, pages 4947–4957.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, Eduard H. Hovy, Hinrich Schütze, and\nYoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Trans.\nAssoc. Comput. Linguistics, 9:1012–1031.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-REx: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings LREC.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Journal of\nMachine Learning Research, 23(120):1–39.\nNanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi\nHuo, Jingyuan Wen, Haoyu Lu, Ruihua Song, Xin\nGao, Tao Xiang, et al. 2022. Towards artificial gen-\neral intelligence via a multimodal foundation model.\nNature Communications, 13(1):1–13.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of ACL/IJCNLP , pages\n3816–3830.\nMor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav\nGoldberg. 2022. Transformer feed-forward layers\nbuild predictions by promoting concepts in the vo-\ncabulary space. arXiv preprint arXiv:2203.14680.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of EMNLP, pages\n5484–5495.\nAriel Goldstein, Avigail Dabush, Bobbi Aubrey, Mari-\nano Schain, Samuel A Nastase, Zaid Zada, Eric Ham,\nZhuoqiao Hong, Amir Feder, Harshvardhan Gazula,\net al. 2022. Brain embeddings with shared geome-\ntry to artificial contextual embeddings, as a code for\nrepresenting language in the human brain. bioRxiv.\nAnirudh Goyal, Alex Lamb, Jordan Hoffmann, Sha-\ngun Sodhani, Sergey Levine, Yoshua Bengio, and\nBernhard Schölkopf. 2021. Recurrent independent\nmechanisms. In Proceedings of ICLR.\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah\nSmith, and Luke Zettlemoyer. 2022. Demix layers:\nDisentangling domains for modular language mod-\neling. In Proceedings of NAACL-HLT, pages 5557–\n5576.\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdh-\nery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul\nMazumder, Lichan Hong, and Ed H. Chi. 2021.\nDselect-k: Differentiable selection in the mixture\nof experts with applications to multi-task learning. In\nProceedings of NeurIPS, pages 29335–29347.\nShlomi Hod, Stephen Casper, Daniel Filan, Cody Wild,\nAndrew Critch, and Stuart Russell. 2021. Detecting\nmodularity in deep neural networks. arXiv preprint\narXiv:2110.08058.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In Proceedings of ICLR.\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan,\nand Geoffrey E Hinton. 1991. Adaptive mixtures of\nlocal experts. Neural Comput., 3(1):79–87.\n4076\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics ,\n8:423–438.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nLouis Kirsch, Julius Kunze, and David Barber. 2018.\nModular networks: Learning to decompose neural\ncomputation. In Proceedings of NeurIPS , pages\n2414–2423.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. In Proceedings of\nICLR.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of EMNLP , pages 3045–\n3059.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. BASE layers:\nSimplifying training of large, sparse models. In Pro-\nceedings of ICM, volume 139, pages 6265–6274.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike\nLewis, Tim Althoff, Noah A. Smith, and Luke Zettle-\nmoyer. 2022. Branch-train-merge: Embarrassingly\nparallel training of expert language models. arxiv\npreprint arXiv:2208.03306.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of NAACL-HLT ,\npages 1073–1094.\nDirk M Lorenz, Alice Jeng, and Michael W Deem. 2011.\nThe emergence of modularity in biological systems.\nPhysics of life reviews, 8(2):129–160.\nHenry Markram. 2006. The blue brain project. In\nProceedings of SC.\nMichael Matena and Colin Raffel. 2021. Merging mod-\nels with fisher-weighted averaging. arxiv preprint\narXiv:2111.09832.\nDavid Meunier, Renaud Lambiotte, and Edward T Bull-\nmore. 2010. Modular and hierarchically modular\norganization of brain networks. Frontiers in neuro-\nscience, 4:200.\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heintz, and Dan Roth. 2021.\nRecent advances in natural language processing via\nlarge pre-trained language models: A survey. arxiv\npreprint arXiv:2111.01243, abs/2111.01243.\nXiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma,\nJilong Xue, Youshan Miao, Zichao Yang, Zhi Yang,\nand Bin Cui. 2021. Dense-to-sparse gate for mixture-\nof-experts. arxiv preprint arXiv:2112.14397.\nOpenAI. 2023. GPT-4 technical report. arxiv preprint\narXiv:2303.08774.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language models\nas knowledge bases? In Proceedings of EMNLP-\nIJCNLP, pages 2463–2473.\nJonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian Li,\nJames Cross, Sebastian Riedel, and Mikel Artetxe.\n2022. Lifting the curse of multilinguality by pre-\ntraining modular transformers. In Proceedings of the\nNAACL-HLT, pages 3479–3495.\nAlec Radford, Rafal Józefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arxiv preprint arXiv:1704.01444.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified Text-to-Text\ntransformer. J. Mach. Learn. Res., 21:140:1–140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nEMNLP, pages 2383–2392. Association for Compu-\ntational Linguistics.\nScott E. Reed, Konrad Zolna, Emilio Parisotto,\nSergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky,\nJackie Kay, Jost Tobias Springenberg, Tom Eccles,\nJake Bruce, Ali Razavi, Ashley Edwards, Nicolas\nHeess, Yutian Chen, Raia Hadsell, Oriol Vinyals,\nMahyar Bordbar, and Nando de Freitas. 2022. A\ngeneralist agent. arXiv preprint arXiv:2205.06175.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow BERT works. Trans. Assoc. Comput. Linguis-\ntics, 8:842–866.\nHassan Sajjad, Nadir Durrani, and Fahim Dalvi. 2022.\nNeuron-level interpretation of deep NLP models: A\nsurvey. TACL, 10:1285–1303.\n4077\nBianca Scarlini, Tommaso Pasini, and Roberto Navigli.\n2019a. Just “OneSeC” for producing multilingual\nsense-annotated data. In Proceedings of ACL, pages\n699–709.\nBianca Scarlini, Tommaso Pasini, and Roberto Nav-\nigli. 2019b. Just \"onesec\" for producing multilingual\nsense-annotated data. In Proceedings of ACL, pages\n699–709.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of EMNLP, pages 1631–1642.\nCharles Spearman. 1961. The proof and measurement\nof association between two things.\nXavier Suau, Luca Zappella, and Nicholas Apostoloff.\n2020. Finding experts in transformer models. arXiv\npreprint arXiv:2005.07647.\nMariya Toneva and Leila Wehbe. 2019. Interpreting and\nimproving natural-language processing (in machines)\nwith natural language-processing (in the brain). In\nProceedings of NeurIPS, pages 14928–14938.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting,\nthe rest can be pruned. In Proceedings of ACL, pages\n5797–5808.\nGeorge V on Dassow and Ed Munro. 1999. Modularity\nin animal development and evolution: elements of\na conceptual framework for evodevo. Journal of\nExperimental Zoology, 285(4):307–325.\nAlex Wang, Amapreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nEMNLP, pages 353–355.\nWenhui Wang, Hangbo Bao, Li Dong, Johan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subho-\njit Som, and Furu Wei. 2022a. Image as a foreign\nlanguage: Beit pretraining for all vision and vision-\nlanguage tasks. arXiv preprint arXiv:2208.10442.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,\nZhiyuan Liu, and Juanzi Li. 2022b. Finding skill\nneurons in pre-trained transformer-based language\nmodels. In Proceedings of EMNLP.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint 1805.12471.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In Pro-\nceedings of NAACL-HLT.\nFan Zhang, Duyu Tang, Yong Dai, Cong Zhou,\nShuangzhi Wu, and Shuming Shi. 2022a. Skillnet-\nnlu: A sparsely activated model for general-purpose\nnatural language understanding. arXiv preprint\n2203.03312.\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,\nMaosong Sun, and Jie Zhou. 2022b. MoEfication:\nTransformer feed-forward layers are mixtures of ex-\nperts. In Findings of ACL.\nMu Zhu. 2004. Recall, precision and average precision.\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yan-\nping Huang, Jeff Dean, Noam Shazeer, and William\nFedus. 2022. Designing effective sparse expert mod-\nels. arxiv preprint arXiv:2202.08906.\n4078\nA Appendix\nA.1 Neurons of MoE\nIn MoE layers, each expert is an FFN, and the out-\nput of the MoE layer is the weighted sum of the out-\nputs of all experts, MoE(x) = ∑E\ni=1 αiFFNi(x),\nwhere αi is the weight of the i-th expert and FFNi\nis the i-th expert. αi is computed by a gating net-\nwork. Note that the weights of unselected experts\nare zero, which makes the MoE layer sparse. We\ncan also rewrite the MoE layer into a neuron-based\nform,\nMoE(x) =\nE∑\ni=1\nαi\n∑\nj\nσ(WI\ni,j,: · x)WO\ni,:,j\n=\n∑\ni,j\nσ(WI\ni,j,: · x)αiWO\ni,:,j,\n(3)\nwhere WI\ni,j,: and WO\ni,:,j are the j-th row and col-\numn of WI\ni and WO\ni in the i-th expert, respec-\ntively. The gating weight αi is non-negative and\ncan be viewed as the scaling factor ofWO\ni,:,j. Corre-\nspondingly, we define a neuron ni,j as a row vector\nWI\ni,j,: and a column vector WO\ni,:,j, and the neuron\nactivation of ni,j is σ(WI\ni,j,: ·x).\nA.2 Experimental Details\nCalculation of AP. AP is the weighted average\nof precision at different recall levels, which is a\ncommon metric for evaluating the performance of\nbinary classification models. Since AP only rep-\nresents the positive correlation, we compute the\nAPs of both neuron activations and their opposite\nvalues, −ai, and take the maximum as the final AP.\nThe final AP ranges from 0.5 to 1, where 0.5 means\nthe neuron is useless for the sub-function and 1\nmeans the neuron is perfect for the sub-function.\nRandomly-initialized models. In Section 4, the\nevaluation of randomly-initialized models is con-\nducted 3 times and we report the average results.\nPerturbation analysis on T5. To match the\nmagnitude of neuron activations of T5, we set the\nvariance of the Gaussian noise to be 4. The pertur-\nbation analysis is conducted 5 times and we report\nthe average results.\nPerturbation analysis on Switch Transformer.\nIn this experiment, the predictivity and thus the\nfunctional experts are calculated based on the\ntraining set. Since Switch Transformer has not\nbeen trained on the considered datasets during pre-\ntraining, we train it on 128 randomly-picked train-\ning instances. We tune WO in the last four years\nOriginal No Function Function\nCoLA 72.11 ( ±0.01) 72.96 ( ±0.01) 73.14 (±0.01)\nMNLI 53.94 ( ±0.03) 50.94 ( ±0.02) 56.24 (±0.03)\nMRPC 75.83 ( ±0.02) 75.18 ( ±0.02) 77.24 (±0.02)\nRTE 65.66 (±0.04) 65.30 ( ±0.04) 65.43 ( ±0.03)\nQNLI 75.16 ( ±0.02) 76.19 ( ±0.02) 78.34 (±0.02)\nQQP 77.14 ( ±0.01) 76.80 ( ±0.01) 78.26 (±0.01)\nSST-2 92.79 (±0.01) 92.10 ( ±0.00) 92.53 ( ±0.01)\n7-task Avg. 73.23 72.78 74.45\nTable 3: Perturbation performance of Switch Trans-\nformer on GLUE tasks. For each dataset, we report the\nmean and standard deviation (std) accuracy over eight\nrandom seeds, using the mean ±std.\nas it takes the neuron activations as inputs, plus the\nrouters in MoE layers. The learning rate and batch\nsize are always 2E-4 and 16 respectively and we\ntake Adam (Kingma and Ba, 2015) as the optimizer.\nThe epoch is 100, 300, 50, 100, 300, 150, and 200\nfor CoLA, MNLI, MRPC, RTE, QNLI, QQP, and\nSST-2. We run each experiment with 8 different\nseeds and report the average best performance on\nthe validation set. The result for each dataset is\nreported in Table 3.\nHyper-parameters of pre-training. The pre-\ntraining corpus is OpenWebText (Radford et al.,\n2019), which contains 40GB of web text. We use\nthe same pre-training task as the official T5 and\nSwitch Transformer, which is masked language\nmodeling. The total number of training steps is\n200K and we save the model every 5K steps. We\nuse the same hyper-parameters for pre-training to\navoid the effect of hyper-parameters on our analy-\nsis, and it is also a common practice when compar-\ning dense and sparse T5s (Zoph et al., 2022). The\nlearning rate is 1e-4. The batch size is 512. The\nmax lengths of encoder inputs and decoder inputs\nare 512 and 256, respectively. We use 8 NVIDIA\nA100 GPUs for pre-training. The total pre-training\ntime is around 3 days.\nExperiments on random partioning. In Sec-\ntion 5 and Section 6, we do the hypothesis test-\ning on random partitioning, and we also calcu-\nlate Spearman’s rank correlation between adjacent\ncheckpoints on random partitioning. These random\nexperiments are done 1000 times and we report the\naverage results.\nA.3 Function Distribution in Each Layer\nFollowing Section 4, we report the distribution sim-\nilarity of the randomly-initialized models in Fig-\nure 5, which is significantly different from that of\nthe pre-trained models.\n4079\n(c) Switch Transformer\n<latexit sha1_base64=\"7cZKi+qgi/cbLC6AGwNMDcg5Adg=\">AAACMXicbVDLSgMxFM34tr6qLkUIFkE3ZUYquhTduNTaWqFTSia9tcEkMyR31DLMyq9xq1/jTtz6DYKZ2oWvA4HDOfdyck+USGHR91+8icmp6ZnZufnSwuLS8kp5de3Sxqnh0OSxjM1VxCxIoaGJAiVcJQaYiiS0opuTwm/dgrEi1g0cJtBR7FqLvuAMndQtb4YI95jt8F16cSeQD2jDMG37sVFg8m654lf9EehfEoxJhYxx1i1/hL2Ypwo0csmsbQd+gp2MGRRcQl4KUwsJ4zfsGtqOaqbAdrLRGTnddkqPumj3NNKR+n0jY8raoYrcpGI4sL+9QvzPa6fYP+xkQicpguZfQf1UUoxp0QntCQMc5dARxo1wf6V8wAzj6Jr7kRIpd4OGOx4rxXQvC+t5FhaBUZTV86Kv4Hc7f8nlXjWoVffPa5Wj43Fzc2SDbJEdEpADckROyRlpEk4eyCN5Is/es/fivXpvX6MT3nhnnfyA9/4JL9mrZg==</latexit>\n(d) Random Switch Transformer\n<latexit sha1_base64=\"X+zdb9csFiTT3qXIHppb8DTOA3w=\">AAACOHicbVC7bhNBFJ0NjxjziCElzQiDZBprN3IUSgsaSmP8iOS1rNnZa3uUeaxm7sZYq/0BviZt/Cd06RAtPRKztguScKSRjs65V2fuSTIpHIbhj+DgwcNHjw9rT+pPnz1/cdR4+WrkTG45DLmRxp4nzIEUGoYoUMJ5ZoGpRMI4ufhU+eNLsE4YPcB1BlPFFlrMBWfopVnjbYzwDYtW+p72mU6Nol9XAvmSDizTbm6sAlvOGs2wHW5B75NoT5pkj96s8SdODc8VaOSSOTeJwgynBbMouISyHucOMsYv2AImnmqmwE2L7TUlfeeVlPpo/zTSrfrvRsGUc2uV+EnFcOnuepX4P2+S4/zDtBA6yxE03wXNc0nR0KoamgoLHOXaE8at8H+lfMks4+gLvJWSKH+DhhU3SvnWirhfFnEVmCRFv6z6iu62c5+MTtpRp336pdPsftw3VyOvyRvSIhE5I13ymfTIkHDynVyRa7IJNsFN8DP4tRs9CPY7x+QWgt9/Ab61ri4=</latexit>\n(b) Random T5\n<latexit sha1_base64=\"+BDqxRWRspkP8+I+SPPBydHvCrc=\">AAACKHicbVBNTxsxFPTSUijQsv249WIRIaWXaBclgiMqlx4hIoCUjSLbeSFW/LGy30LT1f4XruXXcENc+z+Q6g05FMJIlkYz72meh+dKekySh2jlzdvVd2vr7zc2tz583I4/fT7ztnACesIq6y4486CkgR5KVHCRO2CaKzjn06PaP78C56U1pzjLYaDZpZFjKRgGaRh/zRB+Ydnk32mXmZHV9LRTDeNG0krmoMskXZAGWeB4GD9mIysKDQaFYt730yTHQckcSqGg2sgKDzkTU3YJ/UAN0+AH5fz6iu4GZUTH1oVnkM7V/zdKpr2faR4mNcOJf+nV4mtev8DxwaCUJi8QjHgKGheKoqV1FXQkHQhUs0CYcDLcSsWEOSYwFPYshevwBwPXwmodWiqzblVmdSDnZbeq+0pftrNMzvZaabvVOWk3Dn8smlsn38gOaZKU7JND8pMckx4R5De5IX/IbXQb3UX30cPT6Eq02PlCniH6+w+jI6bw</latexit>\n(a) T5\n<latexit sha1_base64=\"27r966ahhL/H+ha6Lxy680b+OuU=\">AAACH3icbVBNSwMxFMz6WetX1aOXYBH0UnalokfRi0ctrQrdRbLpqwaT7Jq8Vcuyv8Or/TXexKs/RjBbe1DrQGCYeY95mTiVwqLvf3hT0zOzc/OVheri0vLKam1t/cImmeHQ4YlMzFXMLEihoYMCJVylBpiKJVzGdyelf/kAxopEt3GQQqTYjRZ9wRk6KQoRnjDfYbu0vV9c1+p+wx+BTpJgTOpkjLPr2mfYS3imQCOXzNpu4KcY5cyg4BKKaphZSBm/YzfQdVQzBTbKR0cXdNspPdpPjHsa6Uj9uZEzZe1AxW5SMby1f71S/M/rZtg/jHKh0wxB8++gfiYpJrRsgPaEAY5y4AjjRrhbKb9lhnF0Pf1KiZX7g4ZHnijFdC8PW0UeloFxnLeKsq/gbzuT5GKvETQb++fN+tHxuLkK2SRbZIcE5IAckVNyRjqEk3vyTF7I0Bt6r96b9/49OuWNdzbIL3gfX8Cao/c=</latexit>\nFigure 5: Distribution similarity between different sub-functions. We report the average similarity between functions.\nWe consider the pre-trained models and their randomly-initialized counterparts.\nModel Partitioning Semantics Knowledge Task\nProp. Degree Prop. Degree Prop. Degree\nSwitch Random 0.001 0.022 0.001 0.022 0.001 0.022\nTransformer Pre-MoE 0.138 1.968 0.101 2.028 0.124 2.029\nT5 Random 0.001 0.021 0.001 0.021 0.001 0.021\nPost-MoE 0.030 2.330 0.038 2.985 0.039 3.522\nTable 4: Proportion of sub-functional experts identified by hypothesis testing and their modularization degree. The\nresult is averaged within each function.\nModel Semantics Knowledge Task\nSwitch Transformer 0.957 0.795 0.734\nT5 0.912 0.894 0.931\nTable 5: Average AP calculated based on B =\n{(bi,fi)}E\ni=1, where bi is the average predictivity across\nall sub-functions for expert ei, and fi is whether ei is a\nfunctional expert or not.\nA.4 Predictivity of Functional Experts\nWe quantify the predictivity of the expert for sub-\nfunctions in Section 3 and define functional experts\nin Section 5, and different are the technical details\nof quantification and definition. Hence, we conduct\nan experiment to check their consistency. For a\nfunction, we calculate bi as the average predictivity\nacross all sub-functions for each expert ei, and we\nalso denote fi ∈{0,1}as whether ei is a functional\nexpert or not. Now we have B = {(bi,fi)}E\ni=1,\nbased on which we compute the AP. A high AP\nindicates a high consistency. We report the average\nAP across all layers in Table 5.\nThe average AP is quite high. Therefore, we are\nconfident that the quantification for expert predic-\ntivity is consistent with the concept of functional\nexperts.\nA.5 Sub-Functioanl Experts\nSimilar to the concept of functional experts dis-\ncussed in Section 4, we can also define so-called\nsub-functional experts. Basically, for each sub-\nfunction, we conduct statistical hypothesis testing\non its sub-functional neurons. We similarly calcu-\nlate the proportion of sub-functional experts and\ntheir modularization degree. We report the aver-\nage result within each function. The result of the\nSwitch Transformer and T5 used in Section 4 is\nreported in Table 4. The changing curve of the\nSwitch Transformer and T5 trained by us is re-\nported in Figure 6.\nA.6 Organization of Sub-Functions\nWe further study how the model organizes sub-\nfunctions into their functional experts 5 and how\nthe organization changes during the pre-training.\nFrom the perspective of sub-functions, it is basi-\ncally how a sub-function shares functional experts\nwith others.\nFor a function, we can list all the sub-functions\nwithin it denoted as w1,w2,...,w M . The similar-\nity score between each pair of sub-functions can\nbe seen as a matrix S, where Si,j is the similarity\nscore between wi and wj for all 1 ≤i,j ≤M.\n5Strictly speaking, we did not define functional experts for\na sub-function. In this context, the concept of “functional ex-\nperts” is used to refer to the experts that have high predictivity\nfor a sub-function.\n4080\n(a) Switch Transformer\n<latexit sha1_base64=\"coici4VCDuPUzL9aaJHeradIHR4=\">AAACMXicbVDLSiNBFK32PT6jLgehMAi6Cd2SYVyKblz6igrpEG5XbkxhVXVTdVsNTa/8mtmOX+NO3PoNA1Mds/B1oOBwzr2cuifJlHQUhk/BxOTU9Mzs3I/5hcWl5ZXa6tqFS3MrsCVSldqrBBwqabBFkhReZRZBJwovk5vDyr+8Retkas5pmGFHw7WRfSmAvNStbcSE91Rsww4/u5MkBvzcgnH91Gq0ZbdWDxvhCPwricakzsY47tb+xb1U5BoNCQXOtaMwo04BlqRQWM7HucMMxA1cY9tTAxpdpxidUfItr/S4j/bPEB+p7zcK0M4NdeInNdDAffYq8TuvnVN/r1NIk+WERrwF9XPFKeVVJ7wnLQpSQ09AWOn/ysUALAjyzX1ISbS/weCdSLUG0yvi07KIq8AkKU7Lqq/ocztfycVuI2o2fp006/sH4+bm2E+2ybZZxH6zfXbEjlmLCfbA/rC/7DF4DJ6C5+DlbXQiGO+ssw8IXv8DLFGrZA==</latexit>\n(b) T5\n<latexit sha1_base64=\"5UBweJgqeV2wPvP0iLOauF3We0w=\">AAACH3icbVDLSgMxFM34rO+qSzfBIuimzEhFl6IblypWhc4gSXqrwSQzJnfUMsx3uLVf407c+jGCmdqFrwOBwzn3cm4Oz5R0GIbvwdj4xOTUdG1mdm5+YXGpvrxy7tLcCmiLVKX2kjMHShpoo0QFl5kFprmCC357WPkX92CdTM0Z9jNINLs2sicFQy8lMcIjFpt8i57tlFf1RtgMh6B/STQiDTLC8VX9I+6mItdgUCjmXCcKM0wKZlEKBeVsnDvImLhl19Dx1DANLimGR5d0wytd2kutfwbpUP2+UTDtXF9zP6kZ3rjfXiX+53Vy7O0lhTRZjmDEV1AvVxRTWjVAu9KCQNX3hAkr/a1U3DDLBPqefqRw7f9g4EGkWjPTLeLTsoirQM6L07LqK/rdzl9yvt2MWs2dk1Zj/2DUXI2skXWySSKyS/bJETkmbSLIHXkiz2QQDIKX4DV4+xodC0Y7q+QHgvdPwk6j+A==</latexit>\n(a) Switch Transformer\n<latexit sha1_base64=\"coici4VCDuPUzL9aaJHeradIHR4=\">AAACMXicbVDLSiNBFK32PT6jLgehMAi6Cd2SYVyKblz6igrpEG5XbkxhVXVTdVsNTa/8mtmOX+NO3PoNA1Mds/B1oOBwzr2cuifJlHQUhk/BxOTU9Mzs3I/5hcWl5ZXa6tqFS3MrsCVSldqrBBwqabBFkhReZRZBJwovk5vDyr+8Retkas5pmGFHw7WRfSmAvNStbcSE91Rsww4/u5MkBvzcgnH91Gq0ZbdWDxvhCPwricakzsY47tb+xb1U5BoNCQXOtaMwo04BlqRQWM7HucMMxA1cY9tTAxpdpxidUfItr/S4j/bPEB+p7zcK0M4NdeInNdDAffYq8TuvnVN/r1NIk+WERrwF9XPFKeVVJ7wnLQpSQ09AWOn/ysUALAjyzX1ISbS/weCdSLUG0yvi07KIq8AkKU7Lqq/ocztfycVuI2o2fp006/sH4+bm2E+2ybZZxH6zfXbEjlmLCfbA/rC/7DF4DJ6C5+DlbXQiGO+ssw8IXv8DLFGrZA==</latexit>\n(b) T5\n<latexit sha1_base64=\"5UBweJgqeV2wPvP0iLOauF3We0w=\">AAACH3icbVDLSgMxFM34rO+qSzfBIuimzEhFl6IblypWhc4gSXqrwSQzJnfUMsx3uLVf407c+jGCmdqFrwOBwzn3cm4Oz5R0GIbvwdj4xOTUdG1mdm5+YXGpvrxy7tLcCmiLVKX2kjMHShpoo0QFl5kFprmCC357WPkX92CdTM0Z9jNINLs2sicFQy8lMcIjFpt8i57tlFf1RtgMh6B/STQiDTLC8VX9I+6mItdgUCjmXCcKM0wKZlEKBeVsnDvImLhl19Dx1DANLimGR5d0wytd2kutfwbpUP2+UTDtXF9zP6kZ3rjfXiX+53Vy7O0lhTRZjmDEV1AvVxRTWjVAu9KCQNX3hAkr/a1U3DDLBPqefqRw7f9g4EGkWjPTLeLTsoirQM6L07LqK/rdzl9yvt2MWs2dk1Zj/2DUXI2skXWySSKyS/bJETkmbSLIHXkiz2QQDIKX4DV4+xodC0Y7q+QHgvdPwk6j+A==</latexit>\nFigure 6: Changing curves of the proportion of sub-functional experts and their modularization degree. The\nhorizontal line is the value for random partitioning.\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013\n/uni00000036/uni00000057/uni00000048/uni00000053/uni00000003/uni0000000b×103/uni0000000c\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019/uni00000036/uni00000053/uni00000048/uni00000044/uni00000055/uni00000050/uni00000044/uni00000051/uni0000000a/uni00000056/uni00000003\n/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055\n/uni00000014\n/uni00000016\n/uni00000018\n/uni0000001a\n/uni0000001c\n/uni00000014/uni00000014\nFigure 7: Changing curve of the average clustering\nscores. We plot the curve for each MoE layer in Switch\nTransformer.\nFor a given k, we denote O(k)\ni,j as the top k ex-\npert overlap for wi and wj. When Spearman’s\nrank correlation between Si,: and O(k)\ni,: (denoted\nas V(k)\ni ) is high, it indicates that the sub-functions\nsimilar to wi share more functional experts than\nsub-functions dissimilar to wi do, and vice versa.\nAccording to the meaning, we call V(k)\ni clustering\nscore.\nWe do a case study on the semantic function of\nthe Switch Transformer, which focuses on under-\nstanding word meanings. Since relatively mature\nis the method of quantifying word similarity, we\ntake S as the word similarity matrix calculated by\nspaCy (Honnibal and Montani, 2017). Note that\nthe features at the word level are the lowest level\nof semantic information, so the word similarity re-\nflects the lowest level of similarity between two\nsemantic sub-functions.\nWe report the curve of\n∑K\nk=1\n∑M\ni=1 V (k)\ni\nKM for each\nlayer in Figure 7. We also report the result for ran-\ndom partitioning in Figure 8. From this figure, we\nhave the following observations. (1) During pre-\ntraining, the clustering score of the lowest MoE\nlayer (Layer 1) quickly achieves 0.6 and then keeps\nstable till the end. It proves that the pre-training\ntends to organize sub-functions sharing similar low-\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013\n/uni00000036/uni00000057/uni00000048/uni00000053/uni00000003/uni0000000b×103/uni0000000c\n/uni00000013/uni00000011/uni00000013/uni00000016/uni00000018\n/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000036/uni00000053/uni00000048/uni00000044/uni00000055/uni00000050/uni00000044/uni00000051/uni0000000a/uni00000056/uni00000003\n/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055\n/uni00000014\n/uni00000016\n/uni00000018\n/uni0000001a\n/uni0000001c\n/uni00000014/uni00000014\nFigure 8: Changing curves of the average clustering\nscores on random partitioning. We plot the curve for\neach MoE layer in Switch Transformer.\nlevel information into the same functional experts\nin the low layer. However, the final clustering score\nof higher MoE layers is close to 0, indicating that\nhigh layers do not organize sub-functions based on\nword similarity. We guess that the reason is that the\nhigh layers may process high-level semantic infor-\nmation, which is not related to the word similarity.\n(3) We see three interesting curves of layers 3, 5,\nand 11. Their clustering scores achieve a high point\nwhen the clustering score of layer 1 first achieves its\nhighest point, and then they continuously decrease\nto 0. The trend that high layers become increas-\ningly responsible for high-level features may grow\nfaster when the low-layer organization has been\nestablished than when the organization is forming.\n4081\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nAfter the conclusion.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n5,A.2, A.3\n□\u0013 B1. Did you cite the creators of artifacts you used?\n5,A.2\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThese datasets are publicly available and free of use for research purposes.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nThe use is consistent with their intended use.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe use established public datasets, which should not cause privacy issues.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nA.2\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nA.2,A.3\nC □\u0013 Did you run computational experiments?\n4,5,6\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nA.3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4082\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nA.3\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nA.3\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nA.7\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4083",
  "topic": "Modularity (biology)",
  "concepts": [
    {
      "name": "Modularity (biology)",
      "score": 0.7744476795196533
    },
    {
      "name": "Zhàng",
      "score": 0.7288214564323425
    },
    {
      "name": "Computer science",
      "score": 0.4960957467556
    },
    {
      "name": "Transformer",
      "score": 0.42655253410339355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3924805521965027
    },
    {
      "name": "Natural language processing",
      "score": 0.3639093339443207
    },
    {
      "name": "Engineering",
      "score": 0.2542523145675659
    },
    {
      "name": "China",
      "score": 0.1282726228237152
    },
    {
      "name": "History",
      "score": 0.1134820282459259
    },
    {
      "name": "Electrical engineering",
      "score": 0.08314931392669678
    },
    {
      "name": "Biology",
      "score": 0.05800795555114746
    },
    {
      "name": "Archaeology",
      "score": 0.05453881621360779
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}