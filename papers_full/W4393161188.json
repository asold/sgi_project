{
  "title": "Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models",
  "url": "https://openalex.org/W4393161188",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5052188338",
      "name": "Antoine Louis",
      "affiliations": [
        "Maastricht University"
      ]
    },
    {
      "id": "https://openalex.org/A5054598855",
      "name": "Gijs van Dijck",
      "affiliations": [
        "Maastricht University"
      ]
    },
    {
      "id": "https://openalex.org/A5010354377",
      "name": "Gerasimos Spanakis",
      "affiliations": [
        "Maastricht University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2946263948",
    "https://openalex.org/W2785076661",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2932110532",
    "https://openalex.org/W2127589108",
    "https://openalex.org/W2979532866",
    "https://openalex.org/W6763079019",
    "https://openalex.org/W3136888420",
    "https://openalex.org/W6736172710",
    "https://openalex.org/W4382603041",
    "https://openalex.org/W2640408555",
    "https://openalex.org/W6839982393",
    "https://openalex.org/W2196893547",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3105134823",
    "https://openalex.org/W2460795814",
    "https://openalex.org/W2611029872",
    "https://openalex.org/W6761551260",
    "https://openalex.org/W3025458843",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3015587287",
    "https://openalex.org/W2809090039",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W3015883388",
    "https://openalex.org/W6791164735",
    "https://openalex.org/W3027825046",
    "https://openalex.org/W6664011276",
    "https://openalex.org/W3015770160",
    "https://openalex.org/W6718322557",
    "https://openalex.org/W6793601707",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W6742639074",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6700445400",
    "https://openalex.org/W6852077520",
    "https://openalex.org/W6849760658",
    "https://openalex.org/W3023690688",
    "https://openalex.org/W2611254175",
    "https://openalex.org/W2144245630",
    "https://openalex.org/W4213191780",
    "https://openalex.org/W2948036864",
    "https://openalex.org/W2970728484",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W6695661434",
    "https://openalex.org/W6628905179",
    "https://openalex.org/W2729046720",
    "https://openalex.org/W4306704940",
    "https://openalex.org/W3155806510",
    "https://openalex.org/W2028009320",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4310829037",
    "https://openalex.org/W6672077861",
    "https://openalex.org/W4310923309",
    "https://openalex.org/W4281257301",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W3093530468",
    "https://openalex.org/W3038572442",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4378771494",
    "https://openalex.org/W2961915345",
    "https://openalex.org/W2609569121",
    "https://openalex.org/W6852874933",
    "https://openalex.org/W6770242157",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4287779512",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4385571412",
    "https://openalex.org/W4287025505",
    "https://openalex.org/W2086511124",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3195296860",
    "https://openalex.org/W2949227999",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4390822940",
    "https://openalex.org/W4300756893",
    "https://openalex.org/W3212191244",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W3034649382",
    "https://openalex.org/W4386576685",
    "https://openalex.org/W2998733856",
    "https://openalex.org/W2962854673",
    "https://openalex.org/W2963443217",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2461148453",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W2741995392",
    "https://openalex.org/W2963655104",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W4293583643",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2104009457",
    "https://openalex.org/W3121076170",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W3035628711",
    "https://openalex.org/W4365460714"
  ],
  "abstract": "Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a \"retrieve-then-read\" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising performance on automatic evaluation metrics, but a qualitative analysis uncovers areas for refinement. As one of the only comprehensive, expert-annotated long-form LQA dataset, LLeQA has the potential to not only accelerate research towards resolving a significant real-world issue, but also act as a rigorous benchmark for evaluating NLP models in specialized domains. We publicly release our code, data, and models.",
  "full_text": "Interpretable Long-Form Legal Question Answering with\nRetrieval-Augmented Large Language Models\nAntoine Louis, Gijs van Dijck, Gerasimos Spanakis\nLaw & Tech Lab, Maastricht University\n{a.louis, gijs.vandijck, jerry.spanakis}@maastrichtuniversity.nl\nAbstract\nMany individuals are likely to face a legal dispute at some\npoint in their lives, but their lack of understanding of how to\nnavigate these complex issues often renders them vulnerable.\nThe advancement of natural language processing opens new\navenues for bridging this legal literacy gap through the devel-\nopment of automated legal aid systems. However, existing le-\ngal question answering (LQA) approaches often suffer from a\nnarrow scope, being either confined to specific legal domains\nor limited to brief, uninformative responses. In this work,\nwe propose an end-to-end methodology designed to generate\nlong-form answers to any statutory law questions, utilizing\na “retrieve-then-read” pipeline. To support this approach, we\nintroduce and release the Long-form Legal Question Answer-\ning (LLeQA) dataset, comprising 1,868 expert-annotated le-\ngal questions in the French language, complete with detailed\nanswers rooted in pertinent legal provisions. Our experimen-\ntal results demonstrate promising performance on automatic\nevaluation metrics, but a qualitative analysis uncovers areas\nfor refinement. As one of the only comprehensive, expert-\nannotated long-form LQA dataset, LLeQA has the potential\nto not only accelerate research towards resolving a signifi-\ncant real-world issue, but also act as a rigorous benchmark\nfor evaluating NLP models in specialized domains. We pub-\nlicly release our code, data, and models.\nIntroduction\nLegal disputes are an inevitable part of everyday life, with\nmany individuals finding themselves entangled in issues re-\nlated to marriage, debts, or employment (Farrow et al. 2016;\nPonce et al. 2019). However, most people have little to no\nknowledge about their rights and fundamental legal pro-\ncesses (Balmer et al. 2010). As a result, they either take no\naction or turn to the internet for advice (Denvir 2016). Un-\nfortunately, the latter often directs users towards commer-\ncial websites that prioritize their own marketing efforts over\nproviding thorough, useful legal guidance (Hagan and Li\n2020). While invaluable, expert legal assistance is often pro-\nhibitively expensive, which results in a considerable number\nof vulnerable individuals being left unprotected or exploited\ndue to their inability to afford it. This barrier to accessing\nlegal information fosters a significant imbalance within the\nlegal system, impeding the universal right to equal access\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nto justice for all. The global implications of this issue are\nsignificant: an estimated 1.5 billion individuals wrestle with\nunresolved legal challenges, and 4.5 billion may be excluded\nfrom the protective measures that the law provides (Gara-\nvano et al. 2019). In light of these circumstances, there is\ngrowing consensus that improved access to legal informa-\ntion could dramatically enhance the outcomes of legal dis-\nputes for many people (Currie 2009).\nThe rapid progress in natural language processing and\nthe growing availability of digitized legal data present un-\nprecedented opportunities to bridge the gap between people\nand the law. For instance, legal text summarization (Bhat-\ntacharya et al. 2019; Shukla et al. 2022) holds the potential to\nsimplify complex legal documents for the layperson, while\nlegal judgment prediction (Chalkidis et al. 2019; Trautmann\net al. 2022) could unveil insightful correlations between an\nindividual’s situation and the probable legal outcome. Sim-\nilarly, legal question answering (LQA) could offer afford-\nable, expert-like assistance to the masses, thereby empow-\nering marginalized parties when utilized for public welfare.\nHowever, existing research on LQA tends to exhibit a con-\nstrained scope, often concentrating on specialized legal do-\nmains, such as tax law (Holzenberger et al. 2020) or privacy\npolicies (Ravichander et al. 2019), or limiting the responses\nto uninformative brief answers like yes/no replies (Rabelo\net al. 2022) or few-word spans (Duan et al. 2019).\nIn this paper, we present an end-to-end approach aimed at\ngenerating long-form responses to any statutory law ques-\ntions. Our methodology harnesses the popular “retrieve-\nthen-read” pipeline, which first leverages a retriever over\na large evidence corpus to fetch a set of relevant legisla-\ntive articles, and then employs a reader to peruse these ar-\nticles and formulate a comprehensive, interpretable answer.\nOur retriever relies on a lightweight bi-encoder model, wich\nenables fast and effective retrieval. For our reader, we use\nan instruction-tuned large language model (LLM) that we\nadapt to our task via two distinct learning strategies: in-\ncontext learning, wherein the model learns from instructions\nand a set of contextually provided examples; and parameter-\nefficient finetuning, where a small number of extra param-\neters are optimized on a downstream dataset while the base\nmodel’s weights are quantized and remain unchanged.\nTo support training and evaluating such systems, we col-\nlect and release the Long-form Legal Question Answering\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22266\n(LLeQA) dataset. LLeQA builds upon BSARD (Louis and\nSpanakis 2022), an information retrieval dataset in French\ncomprising 1,108 legal questions labeled with relevant pro-\nvisions from a corpus of 22,633 Belgian law articles, and\nenhance it in two ways. First, we introduce 760 new le-\ngal questions (+69%) and 5,308 additional statutory articles\n(+23%). Second, we supplement the data with new types of\nannotations, including an exhaustive taxonomy for the ques-\ntion, the jurisdictions concerned, the exact paragraph-level\nreferences within the relevant articles, and a comprehensive\nanswer written by seasoned legal professionals. Owing to\nthe rich variety of its annotations, LLeQA serves as a mul-\ntifaceted resource that extends its utility beyond legal ques-\ntion answering and has the potential to catalyze significant\nprogress in various legal tasks, such as legal inquiry classifi-\ncation, legal topic modeling, and legal information retrieval.\nOur experimental results show that retrieval-augmented\nLLMs exhibit commendable performance on automatic\nevaluation metrics, measuring alignment with target an-\nswers. Yet, a deeper qualitative analysis reveals that these\nsyntactically correct responses, despite seemingly covering\nthe intended topics, frequently harbor inaccuracies and er-\nroneous information. This discrepancy underscores the lim-\nitations inherent in relying solely on automatic metrics for\nassessing such systems, and indicates substantial room for\nimprovement both in terms of modeling and evaluation.\nIn summary, our main contributions are:\n1. A novel dataset for long-form question answering\n(LFQA) in the legal domain and French language, com-\nprising 1,868 legal questions, meticulously annotated by\nlegal professionals, with detailed answers and references\nto relevant legal provisions, drawn from a substantial\nknowledge corpus containing 27,942 statutory articles.\n2. A comprehensive evaluation of the retrieve-then-read\nframework in the context of legal LFQA, while em-\nphasizing interpretability and exploring various learning\nstrategies for the reader.\n3. A public release of our code, dataset, and checkpoints to\nfacilitate future research on interpretable legal LFQA.1\nRelated Work\nLegal question answering. Addressing legal questions\nhas long posed intricate challenges within the legal NLP\ncommunity, stemming from the inherent complexities of le-\ngal texts, including specialized terminology, complex struc-\nture, and nuanced temporal and logical connections. To stim-\nulate advancement in this field, an array of datasets and\nbenchmarks has emerged. Duan et al. (2019) craft a judi-\ncial reading comprehension dataset in the Chinese language,\naimed at fostering the development of systems capable of\nmining fine-grained elements from judgment documents.\nRavichander et al. (2019) present a corpus of questions about\nprivacy policies of mobile applications with the objective\nof empowering users to comprehend and selectively inves-\ntigate privacy matters. Holzenberger et al. (2020) introduce\na dataset for statutory reasoning in tax law. Zhong et al.\n1https://github.com/maastrichtlawtech/lleqa\n(2020) present a multi-choice question answering dataset de-\nsigned to asses professional legal expertise. Rabelo et al.\n(2022) hold a competition wherein a task consists in an-\nswering “yes” or “no” given a legal bar exam problem re-\nlated to Japanese civil law. Lastly, both Mansouri and Cam-\npos (2023) and Chen et al. (2023a) offer a corpus featuring\nquestion-answer pairs in English and Chinese, respectively,\nsourced from online law-oriented forums.\nKnowledge-grounded question answering. Mainstream\napproaches to tackling knowledge-intensive QA tasks com-\nmonly rely on external knowledge sources to enhance the\nanswer prediction, such as collected documents (V oorhees\n1999), web-pages (Kwok et al. 2001), or structured knowl-\nedge bases (Berant et al. 2013; Yu et al. 2017). These open-\nbook models (Roberts et al. 2020) typically index the knowl-\nedge corpus before employing a retrieve-then-read pipeline\nto predict a response based on multiple supporting docu-\nments (Chen et al. 2017). To an extent, this paradigm can\nbe likened to query-based multi-document summarization\n(Tombros and Sanderson 1998), where the objective lies in\nproviding users with a succinct and precise overview of the\ntop-ranked documents related to their queries. Query-driven\nsummarization may adopt different methodologies, mani-\nfesting either in an extractive form, where specific portions\nof evidence text are selected (Otterbacher et al. 2009; Litvak\nand Vanetik 2017), or in an abstractive form, where the in-\nformation is synthesized into new expressions (Nema et al.\n2017; Baumel et al. 2018; Ishigaki et al. 2020).\nRationale generation. To gain insights into model predic-\ntions (Lei et al. 2016), recent advancements have explored\nthe generation of abstractive textual explanations in areas\nsuch as commonsense reasoning (Rajani et al. 2019) and nat-\nural language inference (Kumar and Talukdar 2020). Alter-\nnatively, Lakhotia et al. (2021) proposed the extractive gen-\neration of predefined evidence markers instead of decoding\nraw explanations. Complementing generation, studies have\nconcentrated on extracting rationales from evidence input\nsegments (Bastings et al. 2019; Chalkidis et al. 2021), as\nwell as analyzing saliency maps to underscore key input to-\nkens instrumental to each prediction (Ribeiro et al. 2016).\nThe LLeQA Dataset\nDataset Construction\nIn this section, we describe our process to create LLeQA,\nwhich involves three main stages. First, we gather and re-\nfine annotated legal questions. Then, we build an expansive\ncorpus of supportive statutory articles drawn from Belgian\nlegislation. Finally, we enrich the question annotations by\ngenerating paragraph-level references within relevant arti-\ncles. We elaborate upon each of these steps below.\nCollecting question-answer pairs. The data construction\nprocess starts with collecting high-quality question-answer\npairs on a legal matter. Echoing Louis and Spanakis (2022),\nwe partner with Droits Quotidiens, a Belgian non-profit or-\nganization that endeavors to make the law comprehensible\nand accessible to the most vulnerable. To this end, the or-\nganization maintains a rich website featuring thousands of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22267\nDataset Average # of words # Pairs Answer type Domain Source Lang.Ques. Evid. Ans.\nJEC-QA (Zhong et al. 2020) 47 58 15 26,365 Multi-choice Statutory law Law exam zh\nSARA (Holzenberger et al. 2020) 46 489 1 376 Binary, numeric Tax law Jurists en\nPrivacyQA (Ravichander et al. 2019) 8 3,237 140 1,750 Multi-span Privacy policy Jurists en\nCRJC (Duan et al. 2019) unk. unk. unk. 51,333 Binary, span Case law Jurists zh\nFALQU (Mansouri et al. 2023) 144 - 244 9,880 Long-form Statutory law Web forum en\nCOLIEE-21 (Rabelo et al. 2022) 41 94 1 887 Binary Civil law Law exam ja, en\nEQUALS (Chen et al. 2023a) 32 252 69 6,914 Long-form Statutory law Web forum zh\nLLeQA (ours) 15 1,857 264 1,868 Long-form Statutory law Jurists fr\nTable 1: Comparison of public legal question answering (LQA) datasets. LLeQA has answers an order of magnitude longer and\nis the only expert-annotated long-form LQA dataset to cover any statutory law subjects.\nlegal questions commonly posed by Belgian citizens. Each\nquestion comes with its own individual page, encompass-\ning one or more categorizations, references to relevant leg-\nislative statutes, and a detailed answer written in layman’s\nterms by experienced jurists. With their help, we collect ap-\nproximately 2,550 legal questions. We then filter out ques-\ntions that are unsuitable for retrieval-based question answer-\ning. Specifically, we discard questions whose references are\neither missing, too vague (e.g., an entire law or book), or\nfrom a statute not collected in our knowledge corpus. Addi-\ntionally, we group duplicate questions found across different\nsubcategories on the website. This yields a final number of\n1,868 question-answer pairs, each with legal references.\nMining supporting information. Next, we build the\nknowledge corpus of statutory articles used to provide ev-\nidence that a system can draw upon when generating an\nanswer. We start by extracting provisions from all publicly\navailable Belgian codes of law via the official government\nwebsite, forming an exhaustive foundation of 23,759 articles\nacross 35 legal codes, thereby encapsulating a wide range of\nlegal subjects. To enhance the corpus, we then incorporate\nadditional laws, decrees, and ordinances that are frequently\ncited as supportive references but absent from the initial col-\nlection, adding 4,183 articles from 34 legal acts. This brings\nthe final evidence corpus to 27,942 articles. We proceed by\nassigning a unique identifier to each article and employ regu-\nlar expressions to match the plain text legal references linked\nto the questions with their corresponding article in our cor-\npus. Next, we cleanse the articles of recurrent noisy textual\nelements, such as nested brackets, superscripts, or footnotes\nthat may be present due to revisions or repeals by new legis-\nlation. Besides the article’s content, we also collate the com-\nplete legislative path leading up to that article, which starts\nfrom the statute’s name and progresses through the name\nof the book, act, chapter, section, and subsection where the\narticle resides. This auxiliary information provides valuable\ncontextual insight on the article’s subject matter. Lastly, we\npartition the articles into their constituent paragraphs, which\nserve as the basic units for rationale extraction.\nGenerating paragraph-level rationales. Only 10.4% of\nthe collected questions come with paragraph-level refer-\nences, i.e., with mentions to specific article paragraphs as\nthe relevant information to the question. To extend this level\nof interpretability across all questions in LLeQA, we lever-\nage a large-scale language model to synthetically generate\nthese paragraph-level relevance signals for the remaining\nsamples. This approach serves as an affordable and effi-\ncient alternative to hiring costly legal experts for annota-\ntion. Utilizing the closed-source gpt-3.5-turbo-0613 model\nvia the OpenAI API, we present the model with all para-\ngraphs from relevant legal articles for each question-answer\npair, and instruct it to identify those contributing to the an-\nswer. The model responds with a comma-separated list of\nidentifiers corresponding to the deemed relevant paragraphs,\nor “None” if it discerns no contribution. After parsing the\nmodel’s responses, we incorporate these synthetically gen-\nerated paragraph-level rationales into the dataset.\nFinalizing the dataset. We assign the questions with\nexpert-annotated paragraph-level references to the test set\nand randomly partition the remaining samples into training\nand development sets, yielding a 10/90/10 split, respectively.\nDataset Analysis\nComparative overview. Table 1 presents a comparative\nreview of existing LQA datasets, including ours, across sev-\neral key factors, such as the length of textual elements, num-\nber of samples, type of answer, legal domain covered, source\nof annotations, and language used. LLeQA distinguishes it-\nself by being the only dataset targeting any statutory law\nsubjects, with long-form answers derived from legal experts.\nIts questions are succinct, intending to mirror a real-world\nscenario where laypeople may struggle to elaborate on their\nlegal concerns. In contrast, the answers are more detailed\nthan in other datasets, as they often compensate for the lack\nof provided information by exploring all potential scenar-\nios contingent on an individual’s circumstances, such as age,\nemployment situation, or marital status.\nQuestion diversity. In Table 3, we provide a breakdown of\nthe major question subjects in LLeQA. Housing and health-\ncare represent the two largest topics, accounting for almost\nhalf of all questions together. Family, work, and immigration\nfollow, collectively constituting over a third of the dataset,\nwhile money, privacy, and justice questions are less preva-\nlent. We then examine the type of information requested in\nthe questions based on their interrogative words, as shown\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22268\nWord (%) Example Question\nCan 33.9 Can I continue to work if I am retired?\nHow 21.2 How can I end my student lease ?\nWhat 14.8 What is the role of the guardian of a minor?\nMust 8.6 Must I say I am pregnant in an interview?\nWho 3.9 Who has to pay the funeral expenses?\nWhich 3.8 Which costs are covered for work accidents?\nWhen 1.6 When do I have to hand in my resignation?\nWhere 0.5 Where can I get my criminal record extract?\nWhy 0.1 Why shall I declare the birth of my child?\nMisc. 11.6 Do my assets become joint after marriage?\nTable 2: LLeQA questions by interrogative word. All exam-\nples in the paper are translated from French for illustration.\nin Table 2. “Can”, “how”, “what”, and “must” are the most\nfrequently used question words, indicating that people pri-\nmarily seek information on legal permissions, procedures,\ndefinitions, and obligations. These distributions reflect the\nvariety of legal concerns that citizens face, thereby provid-\ning valuable guidance for user-centric LQA systems.\nQuestion evidence. In LLeQA, approximately 80% of the\nquestions associate with fewer than five articles from the\nknowledge corpus, with the median number of relevant arti-\ncles per question being two. These articles have a median\nlength of 84 words, yet 1,515 articles exceed 500 words,\nwith the longest reaching several thousand words. When\ncombining all relevant articles for each question, we find an\naverage evidence length of 1,857 words per question, posi-\ntioning LLeQA at the upper range among published datasets\nregarding evidence length. Interestingly, a mere 8% of the\narticles within our corpus are referenced as relevant to at\nleast one question within the dataset. Moreover, nearly half\nof these referenced articles originate from five statutes only,\nimplying the critical role a select few laws play in answering\nthe most frequently posed legal inquiries.\nAssessment of annotation quality. To assess the quality\nof the synthetically generated paragraph-level rationales, we\nevaluate the performance of gpt-3.5-turbo-0613 against the\nground truth annotations from the test set. Although far from\nexpert quality, we find that the model demonstrates decent\nannotation performance, achieving a F1 score of 47.5%. By\ncomparison, a naive baseline that randomly selects the rele-\nvant paragraphs achieves 15.3% F1, whereas one that always\nmarks the first paragraph as the relevant one scores 27.2%\nF1. After experimenting with alternative LLMs, we observe\nthat, as of the time of writing, gpt-3.5-turbo-0613 achieves\nthe best overall performance within a limited cost budget.\nDespite the apparent margin of error, we believe that these\nimperfect synthetic annotations may still be beneficial for\nin-context learning purposes as ground truth labels bear less\nsignificance in such settings (Min et al. 2022).\nMethod\nIn this section, we detail the “retrieve-then-read” framework\nwe use for interpretable long-form legal question answer-\ning, illustrated in Figure 1. First, a retriever selects a small\nTopic Train Dev Test (%)\nHousing 382 54 83 27.8\nHealthcare 286 40 67 21.0\nFamily 217 22 16 13.7\nWork 167 26 9 10.8\nImmigration 156 22 3 9.7\nMoney 120 14 7 7.5\nPrivacy 80 14 10 5.6\nJustice 64 9 0 3.9\nTotal 1472 201 195 -\nTable 3: Topic distribution of questions in LLeQA.\nsubset of statutory articles, some of which being relevant to\nthe question. Then, a generator conditions its answer on the\nsubset of articles returned by the retriever. We describe these\ntwo components in more detail below.\nRetriever\nThe role of our retrieval component is to pinpoint all statu-\ntory articles relevant to a question and present them at the\nforefront of the returned results. More formally, the retriever\ncan be expressed as a function R : (q, C) 7→ Fthat takes\nas input a question q along with a knowledge corpus of le-\ngal provisions C = {p1, p2, ··· , pN }, and returns a notably\nsmaller filtered set F ⊂ Cof the supposedly relevant provi-\nsions, ordered by decreasing relevance.\nWe employ the widely adopted bi-encoder architecture\n(Bromley et al. 1993) as the foundation of our retriever,\nwhich maps a question q and a legal provision p into dense\nvector representations and computes a relevance score s :\n(q, p) 7→ R+ between the two by the similarity of their em-\nbeddings hq, hp ∈ Rd, i.e.,\ns(q, p) = sim (hq, hp) , (1)\nwhere sim : Rd × Rd → R is a similarity function, such\nas the dot product or cosine similarity. These embeddings\nare typically derived from a pooling operation on the out-\nput representations of a pretrained autoencoding language\nmodel, such as BERT (Devlin et al. 2019), so that\nhq = pool (E(q; θ1)) , and\nhp = pool (E(p; θ2)) , (2)\nwhere the model E(·; θi) : Wn → Rn×d with weights\nθi transforms an input text sequence of n terms from vo-\ncabulary W to d-dimensional real-valued word vectors. The\npooling function pool : Rn×d → Rd implements a mean\nor max operation on the output word embeddings to dis-\ntill a global representation for the text passage. Beyond\nthe conventional two-tower bi-encoder (Karpukhin et al.\n2020; Yang et al. 2020), which employs two independent\nencoder models to map queries and articles separately into\ndistinct embedding spaces, thesiamese variant (Reimers and\nGurevych 2019; Xiong et al. 2021) uses a unique encoder,\ni.e., θ1 = θ2, to encode the question and article in a shared\ndense vector space. We use the latter variant in this work.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22269\nRetriever\nAm I still entitled to child benefit after my studies?\n...1\n2 ...\n3\n...4\n5 ...\n[§1] Child benefits shall be granted on behalf of the child until August\n31 of the calendar year in which the child reaches the age of 18.\n[§2] Beyond the date set by the first paragraph, child benefits shall be\ngranted until the age of 25 in favor of (a) the apprentice;(b) the child\nwho is taking classes, undertaking an internship to be appointed to a\npost, or is enrolled in training for which credits are granted in the \nsystem;(c) the child who is preparing a dissertation for higher education;\n(d) the child who is no longer subject to compulsory schooling, registered\nas a job seeker, and has completed their studies or apprenticeship;(e)...\n[§3] The right to child benefits shall be extended on behalf of the\nmissing child, in accordance with the following provisions:...\nArticle Collection\nQuestion Top-k Results\nLLM\nPrompt\n#4722\n#128\n#1183\n#27556\n#8347\nPlease respond to the \"QUESTION\" based on the provided \"CONTEXT\". \nYour response should be written in French.\nYour response should start with an \"ANSWER\" that gives a comprehensive \nanswer to the \"QUESTION\" in simple terms.\nYour response should end with \"RATIONALES\" as a list of comma-separated \nIDs of paragraphs from the \"CONTEXT\" that supports your \"ANSWER\".\nCONTEXT:\n[4722§1]  ...\n[4722§2]  ...\n[128§1]   ...\n[1183§1]  Child benefits shall be granted on behalf of the child until...\n[1183§2]  Beyond the date set by the first paragraph, child benefits...\n[1183§3]  The right to child benefits shall be extended on behalf of...\n[27556§1] ...\n[8347§1]  ...\nQUESTION: Am I still entitled to child benefit after my studies?\nANSWER:\n#1\nAnswer\nANSWER: Yes, under certain conditions: (a) you must be under 25; \n(b) you must be registered as a job seeker; (c) you are on a work\nintegration program (one year from the date of your registration as\na job seeker); (d) you do not work more than 240 hours a quarter;\n(e) you are no longer subject to compulsory education; (f) you must\nbe involuntarily unemployed (you cannot refuse a suitable job and\nmust actively seek employment). Your keep your child benefit for\none year after your studies, unless your register as a job seeker\nlate. This 1-year period starts from August 1 following your final\nacademic year (if you completed your studies) or the day after you\nstopped (if you did not). Note that there are special conditions if\nyou stopped your studies before turning 18.\nRATIONALES: [1183§2]\n[§1] Within this Code, the procurement of nationality is referred to as\nacquisition or attribution, depending on whether it is or is not\ncontingent upon the individual's voluntary action directed towards this\nprocurement.\n[§2] For the application of this Act, the following terms are defined as: \n(1) Principal residence: the location of registration in the population\nregister, the foreigners register, or the waiting register; (2) Foreigners\nlaw: the law of December 15, 1980, pertaining to access to the territory,\nresidency, and settlement of foreigners; (3) Regularization law:...\nFigure 1: An illustration of the “retrieve-then-read” pipeline for interpretable long-form legal question answering.\nThe training objective for our dense retriever is to learn\na high-quality low-dimensional embedding space for ques-\ntions and legal provisions such that relevant question-\nprovision pairs appear closer to each other than irrelevant\nones. Assume our training data D = {⟨qi, p+\ni ⟩}N\ni=1 contains\nN instances, each comprising a question qi linked to a rele-\nvant provision p+\ni . For each question qi, we sample a set of\nirrelevant provisions P−\ni , thereby constituting a training set\nT = {⟨qi, p+\ni , P−\ni ⟩}N\ni=1. Subsequently, we use the instances\nin T to contrastively optimize the negative log-likelihood of\nthe relevant provision against the non-relevant ones, i.e.,\nLθ\n\u0000\nqi, p+\ni , P−\ni\n\u0001\n= −log es(qi,p+\ni )/τ\nP\np∈P−\ni ∪{p+\ni } es(qi,p)/τ , (3)\nwhere τ >0 is a temperature parameter that we set to 0.01.\nTo select irrelevant provisions, we employ two distinct sam-\npling strategies: random sampling using in-batch negatives\n(Chen et al. 2017b; Henderson et al. 2017), which consid-\ners provisions paired with the other questions within the\nsame mini-batch; and hard negative samplingusing BM25\n(Robertson et al. 1994), which includes the top provisions\nreturned by BM25 that bear no relevance to the question.\nGenerator\nOur generator aims at formulating a comprehensive answer\nto a short legal question, leaning on corroborative data. For-\nmally, the task can be cast as a conditional text generation\nproblem, where the model requires conditioning its response\non a context string that incorporates the question and sev-\neral supporting statutory articles. We turn to autoregressive\nlarge language models (LLMs) based on the Transformer’s\ndecoder block as the backbone architecture for our genera-\ntor. We then delve into two learning scenarios, specifically\nin-context learning (Radford et al. 2019) and parameter-\nefficient finetuning (Lester et al. 2021; Liu et al. 2022).\nIn-context learning. To assess the innate performance of\nour generator without additional training, we start by exam-\nining three prevalent in-context learning strategies, namely\nzero-shot, one-shot, and few-shot learning. Under the zero-\nshot learning paradigm, the generator receives a natural lan-\nguage instruction and seeks to answer the question directly.\nThe context c provided to the model is formulated as\nc =\n\u0002\nd, P+\nt , qt\n\u0003\n, (4)\nwhere d stands for the task description, qt is the test ques-\ntion, and P+\nt represents the top-k most pertinent legal pro-\nvisions to qt as identified by the retriever (k is set to 5).\nIn one-shot learning, the generator benefits from an addi-\ntional demonstration to guide its understanding of the task,\nwhereas in a few-shot setting, the model accommodates as\nmany demonstrations as can be included within its context\nwindow. Under these scenarios, the context string becomes\nc =\nh\nd,\n\u0002\nP+\nj , qj, aj\n\u0003n\nj=1 , P+\nt , qt\ni\n, (5)\nwhere aj is the gold long-form answer to the training ques-\ntion qj, and n denotes the number of demonstrations. We dy-\nnamically select demonstrations in the training pool through\na similarity-based retrieval method based on the question.\nParameter-efficient finetuning. Due to the gigantic size\nof contemporary LLMs, performing full finetuning of all\nmodel parameters would be prohibitively expensive. Instead,\nwe employ parameter-efficient finetuning to train our gen-\nerator, which significantly trims both the training duration\nand computational cost. Specifically, we apply the QLoRA\ntechnique (Dettmers et al. 2023), which undertakes a pre-\nliminary quantization of the pretrained model to 4-bit before\nfreezing all its parameters for training. A small set of learn-\nable low-rank adapter weights (Hu et al. 2022) are then in-\njected into each linear layer of the Transformer and tuned by\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22270\nbackpropagating gradients through the quantized weights.\nFormally, given a training sample (qi, P+\ni , ai) where ai =\n(y1, ··· , yT ) represents the target output sequence, we op-\ntimize the adapter’s parameters ϕ using a standard language\nmodeling objective that maximizes the negative log likeli-\nhood of generating the target answer ai conditioned on the\ninput context, encompassing the source question qi and the\nset of relevant legal provisions P+\ni , i.e.,\nLϕ\n\u0000\nqi, P+\ni , ai\n\u0001\n= −log pϕ\n\u0000\nai | P+\ni , qi\n\u0001\n= −log\nTY\nt=1\npϕ\n\u0000\nyt | P+\ni , qi, y<t\n\u0001\n.\n(6)\nContext window extension. LLMs typically come with\na predefined context window limit, beyond which perplex-\nity steeply rises due to the weak extrapolation properties of\npositional encoding. This limitation poses significant chal-\nlenges for applications requiring the processing of extensive\ninputs, like ours. Recent efforts have aimed to extend the\ncontext window sizes of pretrained LLMs employing rotary\nposition embedding (Su et al. 2021, RoPE), such as LLaMA\n(Touvron et al. 2023), by interpolating positional encoding\n(Chen et al. 2023b). Guided by promising findings from the\nopen-source community, we perform dynamic NTK-aware\nscaling,2 which retains the exact position values within the\noriginal context window and progressively down-scales the\ninput position indices using a nonlinear interpolation from\nneural tangent kernel theory (Jacot et al. 2018). Preliminary\nresults suggest this approach substantially mitigates perplex-\nity degradation for sequences exceeding the maximum win-\ndow size, without necessitating additional finetuning.\nRationales extraction. Given the serious implications of\nflawed legal guidance, ensuring interpretability in gener-\nated answers is crucial. This enables users to cross-verify\nresponses through reliable sources while understanding the\nunderlying reasoning, thereby enhancing the trustworthiness\nof LQA systems. To this end, we impose additional con-\nstraints on the model to furnish proper justification for its an-\nswers. While prior work on rationale generation has predom-\ninantly focused on creating free-form natural language ex-\nplanations (Latcinnik and Berant 2020; Narang et al. 2020),\nabstractive models have shown a propensity for fabricating\nconvincing yet misleading justifications, inadvertently sup-\nporting inaccurate predictions (Camburu et al. 2020; Wiegr-\neffe et al. 2021). Besides, adapting this strategy for appli-\ncations involving multiple extensive evidence documents\nproves challenging. Consequently, we adopt an extractive\nrationale generation strategy (Lakhotia et al. 2021), prompt-\ning the model to generate evidence paragraph markers rather\nthan raw explanations. This technique ensures the produc-\ntion of unaltered rationales that are easily interpretable.\nExperiments\nExperimental Setup\nModels. Our dense retrieval model leverages Camem-\nBERT (Martin et al. 2020), a leading autoencoding model\n2https://reddit.com/r/LocalLLaMA/comments/14mrgpr/\nfor the French language. To furnish a well-rounded compar-\native analysis, we incorporate two robust retrieval baselines:\nBM25 (Robertson et al. 1994), a widely utilized bag-of-\nwords retrieval function; and mE5 (Wang et al. 2022a), cur-\nrently the top-performing multilingual dense model on the\nMTEB benchmark (Muennighoff et al. 2023). With regard\nto our generator, we experiment with several instruction-\ntuned open-source models derived from LLaMA (Touvron\net al. 2023) to harness the benefits of dynamic NTK-aware\nscaled RoPE. Specifically, we consider four models that are\nnotably high-ranking on the MT-Bench leaderboard (Zheng\net al. 2023): Vicuna-1.3 (Chiang et al. 2023), WizardLM-\n1.0 (Xu et al. 2023a), T ¨ULU (Wang et al. 2023), and Gua-\nnaco (Dettmers et al. 2023). Due to limited computational\nresources, we restrict our study to their 7B variant and cur-\ntail the extended context window size to 8192 tokens for in-\ncontext learning and 4096 tokens for finetuning.\nImplementation. We start by fully finetuning Camem-\nBERT on LLeQA with a batch size of 32 and a maximum se-\nquence length of 384 tokens for 20 epochs (i.e., 5.8k steps),\nusing AdamW (Loshchilov and Hutter 2017) withβ1 = 0.9,\nweight decay of 0.01, and learning rate warm up along the\nfirst 60 steps to a maximum value of 2e-5, after which linear\ndecay is applied. We use 16-bit automatic mixed precision\nto accelerate training, which takes about 1.7 hours. We then\noptimize our baseline LLMs through 4-bit QLoRA finetun-\ning with an effective batch size of 8 for 10 epochs (i.e., 1.1K\nsteps) using paged AdamW optimizer with default momen-\ntum parameters and constant learning rate schedule of 2e-4.\nWe employ NormalFloat4 with double quantization for the\nbase models and add LoRA adapters on all linear layers by\nsetting r = 16, α = 32while utilizing float16 as computa-\ntion datatype. Training takes around 7.5 hours to complete.\nWe generate from the LLMs using nucleus sampling (Holtz-\nman et al. 2020) with p = 0.95 and a temperature of 0.1.\nHardware & Libraries. Computations are performed on\na single 32GB NVIDIA V100 GPU hosted on a server with\na dual 20-core Intel Xeon E5-2698 v4 CPU and 512GB of\nRAM, operating under Ubuntu 16.04.\nAutomatic Evaluation\nEvaluation metrics. To evaluate our framework’s effec-\ntiveness, we assess three core aspects:retrieval performance,\ngeneration quality, andrationales accuracy. Firstly, it is es-\nsential that the retriever returns as many pertinent provisions\nas possible within the first top-k results, given the genera-\ntor’s limited context window size. This requirement implies\na primary interest in recall at small cutoffs. Additionally, we\nreport the mean reciprocal rank, as it offers valuable insights\ninto the position of the first relevant result. Gauging the qual-\nity of long-form answers presents more intricate challenges.\nAutomatic metrics, such as ROUGE (Lin 2004), proved to\nbe inadequate due to the intrinsically open-ended nature of\nlong-form responses, which allows for an array of possi-\nble formulations that retain semantic similarity (Wang et al.\n2022b; Xu et al. 2023b). Besides, these lexical overlap met-\nrics fail to assess essential aspects such as factual correctness\nand query relevance. Ultimately, a thorough assessment of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22271\nModel Size R@5 R@10 MRR@10\nBaselines\n1 BM25 - 17.4 22.8 22.0\n2 mE5BASE 278M 15.4 21.7 25.8\n3 mE5LARGE 560M 16.5 26.7 28.3\n4 CamemBERT 111M 48.6 60.6 60.0\nTable 4: Retrieval scores of our dense retriever benchmarked\nagainst other strong retrieval baselines on LLeQA dev set.\nsuch systems requires human evaluation, although the latter\nintroduces its own set of challenges (Krishna et al. 2021),\nin addition to being expensive and noisy. Owing to resource\nconstraints in our study, we opt for an automated evalua-\ntion metrics, fully cognizant of its limitations, and earmark\nhuman evaluation for future work. In particular, we report\nMETEOR (Banerjee and Lavie 2005), which demonstrated\nsuperior correlation with human judgment in the context of\nlong text generation (Sharma et al. 2017; Chen et al. 2022).\nLastly, we evaluate the model’s capacity for rationale ex-\ntraction by measuring the F1 score over the set of predicted\nparagraph markers as compared to the ground truth markers.\nResults. As depicted in Table 4, our dense retriever, fine-\ntuned on a mere 1.5k in-domain examples, significantly out-\nperforms robust retrieval baselines, underlining the essential\nrole of domain adaptation in enhancing performance. How-\never, the results leave substantial room for improvement; on\naverage, less than half of the relevant articles appear within\nthe top five returned results. This shortfall represents a ma-\njor bottleneck for our generator, which faces the challenge of\nanswering questions based on partially irrelevant provisions.\nThe impact of this limitation is palpable in the poor perfor-\nmance of rationale extraction, which approaches near-zero\neffectiveness for most LLMs, though part of this deficiency\ncan be attributed to the models’ tendency to hallucinate. In\nterms of answer quality, WizardLM and Vicuna display a\nhigh degree of overlap with the ground truth responses, indi-\ncating accurate engagement with the subject matter. Provid-\ning a demonstration appears to improve generation quality\nas compared to a zero-shot setup, except for Guanaco which\nshows strong zero-shot results. However, performance does\nnot seem to vary significantly when more demonstrations are\nprovided. Finally, the results suggest that finetuning on our\ntask-specific dataset consistently enhances performance.\nQualitative Analysis\nTo discern the strengths and shortcomings of our generators,\nwe conduct a detailed manual analysis of 10 randomly se-\nlected samples from the test set. We find that T¨ULU exhibits\na propensity for producing concise answers, a tendency\nlikely due to its extensive finetuning on instruction datasets\naveraging a relatively short completion length of 98.7 words,\nwhich also explains its low METEOR score as LLeQA an-\nswers are markedly longer. We further observe that Guanaco\nand WizardLM are prone to repetitiveness in their responses,\noccasionally echoing identical phrases. While presence and\nfrequency penalties could mitigate this issue, they may prove\nModel 0-shot 1-shot 2-shot Fine-tuned\nQuestion Answering (METEOR)\n1 Vicuna 11.6 16.2 15.3 19.7\n2 WizardLM 12.3 15.5 16.6 20.4\n3 T ¨ULU 2.9 4.6 8.5 12.7\n4 Guanaco 11.2 11.2 11.3 20.1\nRationales Extraction (F1)\n1 Vicuna 0.4 0.6 0.2 0.0\n2 WizardLM 0.0 0.0 0.0 2.0\n3 T ¨ULU 0.1 0.0 0.0 3.5\n4 Guanaco 1.3 0.4 0.0 0.0\nTable 5: Scores of our baseline LLMs on LLeQA test set.\nineffective in instances addressing specialized topics where\nthe same terminology is intrinsically repeated. Regarding re-\nsponse quality, WizardLM and Vicuna stand out, far exceed-\ning Guanaco, which tends to produce nonsensical or linguis-\ntically convoluted sentences. In contrast, WizardLM and Vi-\ncuna’s outputs are articulated in impeccable French, display-\ning a persuasive flair that could potentially mislead an unsus-\npecting reader. Nevertheless, a deeper probe unveils striking\nhallucinations (Ji et al. 2023). Despite seemingly addressing\nthe question, many facts, dates, sources, and conditions ap-\npear to be fabricated, as if the models leveraged the provided\ncontext less for factual accuracy and more as a foundation\nupon which to weave a convincing fictitious answer.\nConclusion\nIn this work, we introduce LLeQA, an expert-annotated\ndataset tailored to facilitate the development of models\naimed at generating comprehensive answers to legal ques-\ntions, while supplying interpretable justifications. We ex-\nperiment with the “retrieve-then-read” pipeline on LLeQA\nand explore various state-of-the-art large language models\nas readers, that we adapt to the task using several learn-\ning strategies. We find that this framework tends to produce\nsyntactically correct answers pertinent to the question’s sub-\nject matter but occasionally fabricate facts. Overall, we be-\nlieve LLeQA can serve as a robust foundation for advance-\nments in interpretable, long-form legal question answering,\nthereby contributing to the democratization of legal access.\nLimitations and future work. Despite our efforts to make\nLQA systems more factually grounded in supporting legal\nprovisions, the framework we propose remains vulnerable\nto hallucinations in both the constructed answers and asso-\nciated rationales. Additionally, consistent with prior studies\n(Krishna, Roy, and Iyyer 2021; Xu et al. 2023b), we observe\nthat conventional automatic metrics may not accurately mir-\nror answer quality, leading to potential misinterpretations.\nThese challenges present great avenues for future work.\nEthical considerations. The premature deployment of\nLQA systems poses a tangible risk to laypersons, who may\nuncritically rely on the furnished guidance and inadvertently\nexacerbate their circumstances. To ensure the responsible\ndevelopment of legal aid technologies, we are committed to\nlimiting the use of our dataset strictly to research purposes.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22272\nAcknowledgments\nThis research is partially supported by the Sector Plan Dig-\nital Legal Studies of the Dutch Ministry of Education, Cul-\nture, and Science. In addition, this research was made possi-\nble, in part, using the Data Science Research Infrastructure\n(DSRI) hosted at Maastricht University.\nReferences\nBalmer, N. J.; Buck, A.; Patel, A.; Denvir, C.; and Pleasence,\nP. 2010. Knowledge, capability and the experience of rights\nproblems. London: PLEnet.\nBanerjee, S.; and Lavie, A. 2005. METEOR: An Automatic\nMetric for MT Evaluation with Improved Correlation with\nHuman Judgments. In Proceedings of the ACL Workshop\non Intrinsic and Extrinsic Evaluation Measures for Machine\nTranslation and Summarization, 65–72.\nBastings, J.; Aziz, W.; and Titov, I. 2019. Interpretable Neu-\nral Predictions with Differentiable Binary Variables. InPro-\nceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2963–2977.\nBaumel, T.; Eyal, M.; and Elhadad, M. 2018. Query Fo-\ncused Abstractive Summarization: Incorporating Query Rel-\nevance, Multi-Document Coverage, and Summary Length\nConstraints into seq2seq Models. CoRR, abs/1801.07704.\nBerant, J.; Chou, A.; Frostig, R.; and Liang, P. 2013. Se-\nmantic Parsing on Freebase from Question-Answer Pairs. In\nProceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, 1533–1544.\nBhattacharya, P.; Hiware, K.; Rajgaria, S.; Pochhi, N.;\nGhosh, K.; and Ghosh, S. 2019. A Comparative Study\nof Summarization Algorithms Applied to Legal Case Judg-\nments. Advances in Information Retrieval, 11437: 413–428.\nBromley, J.; Guyon, I.; LeCun, Y .; S¨ackinger, E.; and Shah,\nR. 1993. Signature Verification Using a Siamese Time Delay\nNeural Network. Advances in Neural Information Process-\ning Systems, 6: 737–744.\nCamburu, O.; Shillingford, B.; Minervini, P.; Lukasiewicz,\nT.; and Blunsom, P. 2020. Make Up Your Mind! Adversarial\nGeneration of Inconsistent Natural Language Explanations.\nIn Proceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, 4157–4165.\nChalkidis, I.; Androutsopoulos, I.; and Aletras, N. 2019.\nNeural Legal Judgment Prediction in English. In Proceed-\nings of the 57th Conference of the Association for Computa-\ntional Linguistics, 4317–4323.\nChalkidis, I.; Fergadiotis, M.; Tsarapatsanis, D.; Aletras, N.;\nAndroutsopoulos, I.; and Malakasiotis, P. 2021. Paragraph-\nlevel Rationale Extraction through Regularization: A case\nstudy on European Court of Human Rights Cases. In Pro-\nceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics ,\n226–241.\nChen, A.; Yao, F.; Zhao, X.; Zhang, Y .; Sun, C.; Liu, Y .; and\nShen, W. 2023a. EQUALS: A Real-world Dataset for Legal\nQuestion Answering via Reading Chinese Laws. In Pro-\nceedings of the 19th International Conference on Artificial\nIntelligence and Law, 71–80.\nChen, D.; Fisch, A.; Weston, J.; and Bordes, A. 2017a.\nReading Wikipedia to Answer Open-Domain Questions. In\nProceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics, 1870–1879.\nChen, S.; Wong, S.; Chen, L.; and Tian, Y . 2023b. Extending\nContext Window of Large Language Models via Positional\nInterpolation. CoRR, abs/2306.15595.\nChen, T.; Sun, Y .; Shi, Y .; and Hong, L. 2017b. On Sampling\nStrategies for Neural Network-based Collaborative Filter-\ning. In Proceedings of the 23rd ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data Min-\ning, 767–776.\nChen, Y .; Song, Z.; Wu, X.; Wang, D.; Xu, J.; Chen, J.; Zhou,\nH.; and Li, L. 2022. MTG: A Benchmark Suite for Multi-\nlingual Text Generation. In Findings of the Association for\nComputational Linguistics: NAACL 2022, 2508–2527.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chat-\nbot Impressing GPT-4 with 90%* ChatGPT Quality. https:\n//lmsys.org/blog/2023-03-30-vicuna/. Accessed: 2023-12.\nCurrie, A. 2009. The legal problems of everyday life. In\nAccess to justice, 1–41. Emerald Group Publishing Limited.\nDenvir, C. 2016. Online and in the know? Public legal edu-\ncation, young people and the Internet. Computers & Educa-\ntion, 92-93: 204–220.\nDettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,\nL. 2023. QLoRA: Efficient Finetuning of Quantized LLMs.\nCoRR, abs/2305.14314.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics, 4171–4186.\nDuan, X.; Wang, B.; Wang, Z.; Ma, W.; Cui, Y .; Wu, D.;\nWang, S.; Liu, T.; Huo, T.; Hu, Z.; Wang, H.; and Liu,\nZ. 2019. CJRC: A Reliable Human-Annotated Bench-\nmark Dataset for Chinese Judicial Reading Comprehension.\nIn Proceedings of the 18th China National Conference on\nComputational Linguistics, 439–451.\nFarrow, T. C.; Currie, A.; Aylwin, N.; Jacobs, L.; Northrup,\nD.; and Moore, L. 2016. Everyday legal problems and the\ncost of justice in Canada: Overview report. Osgoode Legal\nStudies Research Paper, 12(57).\nGaravano, G. C.; Kaag, S.; Schwartz, P.; Jilani, H.; Alvarez,\nA.; Ardyanto, D.; Goldston, J.; de Greiff, P.; Hossain, S.;\nKennou, K.; Maru, V .; Maynard-Gibson, A.; Molokomme,\nA.; Pell, O.; Pais, M. S.; Rodriguez, M. F.; van Wieren, J.;\nand Osho, B. 2019. Justice for All: The Report of the Task\nForce on Justice. Center on International Cooperation.\nHagan, M.; and Li, Y . 2020. Legal Help Search Audit:\nAre Search Engines Effective Brokers of Legal Information?\nAvailable at SSRN 3623333.\nHenderson, M. L.; Al-Rfou, R.; Strope, B.; Sung, Y .;\nLuk´acs, L.; Guo, R.; Kumar, S.; Miklos, B.; and Kurzweil,\nR. 2017. Efficient Natural Language Response Suggestion\nfor Smart Reply. CoRR, abs/1705.00652.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22273\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2020. The Curious Case of Neural Text Degeneration. In\nProceedings of the 8th International Conference on Learn-\ning Representations.\nHolzenberger, N.; Blair-Stanek, A.; and Durme, B. V . 2020.\nA Dataset for Statutory Reasoning in Tax Law Entailment\nand Question Answering. In Proceedings of the Natural Le-\ngal Language Processing Workshop 2020, 31–38.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adap-\ntation of Large Language Models. InProceedings of the 10th\nInternational Conference on Learning Representations.\nIshigaki, T.; Huang, H.; Takamura, H.; Chen, H.; and Oku-\nmura, M. 2020. Neural Query-Biased Abstractive Summa-\nrization Using Copying Mechanism. Advances in Informa-\ntion Retrieval, 12036: 174–181.\nJacot, A.; Hongler, C.; and Gabriel, F. 2018. Neural Tan-\ngent Kernel: Convergence and Generalization in Neural Net-\nworks. Advances in Neural Information Processing Systems,\n31: 8580–8589.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\nBang, Y .; Madotto, A.; and Fung, P. 2023. Survey of Hallu-\ncination in Natural Language Generation. ACM Computing\nSurveys, 55(12): 248:1–248:38.\nKarpukhin, V .; Oguz, B.; Min, S.; Lewis, P. S. H.; Wu, L.;\nEdunov, S.; Chen, D.; and Yih, W. 2020. Dense Passage\nRetrieval for Open-Domain Question Answering. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, 6769–6781.\nKrishna, K.; Roy, A.; and Iyyer, M. 2021. Hurdles to\nProgress in Long-form Question Answering. InProceedings\nof the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics, 4940–4957.\nKumar, S.; and Talukdar, P. P. 2020. NILE : Natural Lan-\nguage Inference with Faithful Natural Language Explana-\ntions. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 8730–8742.\nKwok, C. C. T.; Etzioni, O.; and Weld, D. S. 2001. Scal-\ning question answering to the web. ACM Transactions on\nInformation Systems, 19(3): 242–262.\nLakhotia, K.; Paranjape, B.; Ghoshal, A.; Yih, S.; Mehdad,\nY .; and Iyer, S. 2021. FiD-Ex: Improving Sequence-to-\nSequence Models for Extractive Rationale Generation. In\nProceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, 3712–3727.\nLatcinnik, V .; and Berant, J. 2020. Explaining Ques-\ntion Answering Models through Text Generation. CoRR,\nabs/2004.05569.\nLei, T.; Barzilay, R.; and Jaakkola, T. S. 2016. Rationaliz-\ning Neural Predictions. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 107–117.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power\nof Scale for Parameter-Efficient Prompt Tuning. InProceed-\nings of the 2021 Conference on Empirical Methods in Natu-\nral Language Processing, 3045–3059.\nLin, C. 2004. ROUGE: A Package for Automatic Evaluation\nof Summaries. In Text Summarization Branches Out, 74–81.\nLitvak, M.; and Vanetik, N. 2017. Query-based summariza-\ntion using MDL principle. In Proceedings of the Workshop\non Summarization and Summary Evaluation Across Source\nTypes and Genres, 22–31.\nLiu, X.; Ji, K.; Fu, Y .; Tam, W.; Du, Z.; Yang, Z.; and Tang,\nJ. 2022. P-Tuning: Prompt Tuning Can Be Comparable to\nFine-tuning Across Scales and Tasks. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics, 61–68.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled Weight De-\ncay Regularization. In Proceedings of the 7th International\nConference on Learning Representations.\nLouis, A.; and Spanakis, G. 2022. A Statutory Article Re-\ntrieval Dataset in French. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics ,\n6789–6803.\nMansouri, B.; and Campos, R. 2023. FALQU: Finding An-\nswers to Legal Questions. In Proceedings of the 1st Interna-\ntional Workshop on Legal Information Retrieval, 22–24.\nMartin, L.; M ¨uller, B.; Su ´arez, P. J. O.; Dupont, Y .; Ro-\nmary, L.; de la Clergerie,´E.; Seddah, D.; and Sagot, B. 2020.\nCamemBERT: a Tasty French Language Model. InProceed-\nings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics, 7203–7219.\nMin, S.; Lyu, X.; Holtzman, A.; Artetxe, M.; Lewis, M.;\nHajishirzi, H.; and Zettlemoyer, L. 2022. Rethinking the\nRole of Demonstrations: What Makes In-Context Learning\nWork? In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, 11048–11064.\nMuennighoff, N.; Tazi, N.; Magne, L.; and Reimers, N.\n2023. MTEB: Massive Text Embedding Benchmark. InPro-\nceedings of the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, 2006–2029.\nNarang, S.; Raffel, C.; Lee, K.; Roberts, A.; Fiedel, N.; and\nMalkan, K. 2020. WT5?! Training Text-to-Text Models to\nExplain their Predictions. CoRR, abs/2004.14546.\nNema, P.; Khapra, M. M.; Laha, A.; and Ravindran, B. 2017.\nDiversity driven attention model for query-based abstractive\nsummarization. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics, 1063–\n1072.\nOtterbacher, J.; Erkan, G.; and Radev, D. R. 2009. Bi-\nased LexRank: Passage retrieval using random walks with\nquestion-based priors. Information Processing and Manage-\nment, 45(1): 42–54.\nPonce, A.; Chamness Long, S.; Andersen, E.; Gutier-\nrez Patino, C.; Harman, M.; A Morales, J.; Piccone, T.;\nRodriguez Cajamarca, N.; Stephan, A.; Gonzalez, K.; Van-\nRiper, J.; Evangelides, A.; Martin, R.; Khosla, P.; Bock, L.;\nCampbell, E.; Gray, E.; Gryskiewicz, A.; Ibrahim, A.; Solis,\nL.; Hearn-Desautels, G.; and Tinucci, F. 2019. Global In-\nsights on Access to Justice 2019: Findings from the World\nJustice Project General Population Poll in 101 Countries .\nWorld Justice Project.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22274\nRabelo, J.; Goebel, R.; Kim, M.; Kano, Y .; Yoshioka, M.;\nand Satoh, K. 2022. Overview and Discussion of the Com-\npetition on Legal Information Extraction/Entailment (COL-\nIEE) 2021. Review of Socionetwork Strategies, 16: 111–133.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRajani, N. F.; McCann, B.; Xiong, C.; and Socher, R. 2019.\nExplain Yourself! Leveraging Language Models for Com-\nmonsense Reasoning. In Proceedings of the 57th Con-\nference of the Association for Computational Linguistics ,\n4932–4942.\nRavichander, A.; Black, A. W.; Wilson, S.; Norton, T. B.;\nand Sadeh, N. M. 2019. Question Answering for Privacy\nPolicies: Combining Computational and Legal Perspectives.\nIn Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing, 4946–4957.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-\ntence Embeddings using Siamese BERT-Networks. In Pro-\nceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing, 3980–3990.\nRibeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ”Why\nShould I Trust You?”: Explaining the Predictions of Any\nClassifier. In Proceedings of the 22nd ACM SIGKDD In-\nternational Conference on Knowledge Discovery and Data\nMining, 1135–1144.\nRoberts, A.; Raffel, C.; and Shazeer, N. 2020. How Much\nKnowledge Can You Pack Into the Parameters of a Language\nModel? In Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing, 5418–5426.\nRobertson, S. E.; Walker, S.; Jones, S.; Hancock-Beaulieu,\nM.; and Gatford, M. 1994. Okapi at TREC-3. In Proceed-\nings of the 3rd Text REtrieval Conference, 109–126.\nSharma, S.; Asri, L. E.; Schulz, H.; and Zumer, J. 2017.\nRelevance of Unsupervised Metrics in Task-Oriented Dia-\nlogue for Evaluating Natural Language Generation. CoRR,\nabs/1706.09799.\nShukla, A.; Bhattacharya, P.; Poddar, S.; Mukherjee, R.;\nGhosh, K.; Goyal, P.; and Ghosh, S. 2022. Legal Case Doc-\nument Summarization: Extractive and Abstractive Methods\nand their Evaluation. In Proceedings of the 2nd Conference\nof the Asia-Pacific Chapter of the Association for Computa-\ntional Linguistics, 1048–1064.\nSu, J.; Lu, Y .; Pan, S.; Wen, B.; and Liu, Y . 2021. Ro-\nFormer: Enhanced Transformer with Rotary Position Em-\nbedding. CoRR, abs/2104.09864.\nTombros, A.; and Sanderson, M. 1998. Advantages of Query\nBiased Summaries in Information Retrieval. In Proceed-\nings of the 21st International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, 2–10.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar,\nF.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.\n2023. LLaMA: Open and Efficient Foundation Language\nModels. CoRR, abs/2302.13971.\nTrautmann, D.; Petrova, A.; and Schilder, F. 2022. Legal\nPrompt Engineering for Multilingual Legal Judgement Pre-\ndiction. CoRR, abs/2212.02199.\nV oorhees, E. M. 1999. The TREC-8 Question Answering\nTrack Report. In Proceedings of The 8th Text REtrieval Con-\nference, volume 500–246.\nWang, L.; Yang, N.; Huang, X.; Jiao, B.; Yang, L.; Jiang,\nD.; Majumder, R.; and Wei, F. 2022a. Text Embeddings\nby Weakly-Supervised Contrastive Pre-training. CoRR,\nabs/2212.03533.\nWang, S.; Xu, F.; Thompson, L.; Choi, E.; and Iyyer, M.\n2022b. Modeling Exemplification in Long-form Question\nAnswering via Retrieval. In Proceedings of the 2022 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics, 2079–2092.\nWang, Y .; Ivison, H.; Dasigi, P.; Hessel, J.; Khot, T.; Chandu,\nK. R.; Wadden, D.; MacMillan, K.; Smith, N. A.; Beltagy,\nI.; and Hajishirzi, H. 2023. How Far Can Camels Go? Ex-\nploring the State of Instruction Tuning on Open Resources.\nCoRR, abs/2306.04751.\nWiegreffe, S.; Marasovic, A.; and Smith, N. A. 2021. Mea-\nsuring Association Between Labels and Free-Text Ratio-\nnales. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, 10266–10284.\nXiong, L.; Xiong, C.; Li, Y .; Tang, K.; Liu, J.; Bennett, P. N.;\nAhmed, J.; and Overwijk, A. 2021. Approximate Nearest\nNeighbor Negative Contrastive Learning for Dense Text Re-\ntrieval. In Proceedings of the 9th International Conference\non Learning Representations.\nXu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.; Tao,\nC.; and Jiang, D. 2023a. WizardLM: Empowering Large\nLanguage Models to Follow Complex Instructions. CoRR,\nabs/2304.12244.\nXu, F.; Song, Y .; Iyyer, M.; and Choi, E. 2023b. A Criti-\ncal Evaluation of Evaluations for Long-form Question An-\nswering. In Proceedings of the 61th Annual Meeting of the\nAssociation for Computational Linguistics, 3225–3245.\nYang, Y .; Cer, D.; Ahmad, A.; Guo, M.; Law, J.; Constant,\nN.; ´Abrego, G. H.; Yuan, S.; Tar, C.; Sung, Y .; Strope, B.;\nand Kurzweil, R. 2020. Multilingual Universal Sentence\nEncoder for Semantic Retrieval. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lin-\nguistics: System Demonstrations, 87–94.\nYu, M.; Yin, W.; Hasan, K. S.; dos Santos, C. N.; Xiang, B.;\nand Zhou, B. 2017. Improved Neural Relation Detection for\nKnowledge Base Question Answering. InProceedings of the\n55th Annual Meeting of the Association for Computational\nLinguistics, 571–581.\nZheng, L.; Chiang, W.; Sheng, Y .; Zhuang, S.; Wu, Z.;\nZhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang,\nH.; Gonzalez, J. E.; and Stoica, I. 2023. Judging LLM-\nas-a-judge with MT-Bench and Chatbot Arena. CoRR,\nabs/2306.05685.\nZhong, H.; Xiao, C.; Tu, C.; Zhang, T.; Liu, Z.; and Sun,\nM. 2020. JEC-QA: A Legal-Domain Question Answering\nDataset. In Proceedings of the 34th AAAI Conference on\nArtificial Intelligence, 9701–9708.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22275",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8401103019714355
    },
    {
      "name": "Natural language processing",
      "score": 0.5132174491882324
    },
    {
      "name": "Computer science",
      "score": 0.5000662803649902
    },
    {
      "name": "Information retrieval",
      "score": 0.47851431369781494
    },
    {
      "name": "Linguistics",
      "score": 0.456339031457901
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39851275086402893
    },
    {
      "name": "Philosophy",
      "score": 0.1662309467792511
    }
  ]
}