{
  "title": "Byte Pair Encoding is Suboptimal for Language Model Pretraining",
  "url": "https://openalex.org/W3015650676",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3015503702",
      "name": "Kaj Bostrom",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A1978278429",
      "name": "Greg Durrett",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3183153947",
    "https://openalex.org/W2971081948",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W75508151",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2905510703",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2226734577",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE's greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4617–4624\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n4617\nByte Pair Encoding is Suboptimal for Language Model Pretraining\nKaj Bostrom and Greg Durrett\nDepartment of Computer Science\nThe University of Texas at Austin\n{kaj,gdurrett}@cs.utexas.edu\nAbstract\nThe success of pretrained transformer lan-\nguage models (LMs) in natural language\nprocessing has led to a wide range of\npretraining setups. In particular, these models\nemploy a variety of subword tokenization\nmethods, most notably byte-pair encoding\n(BPE) (Sennrich et al., 2016; Gage, 1994), the\nWordPiece method (Schuster and Nakajima,\n2012), and unigram language modeling (Kudo,\n2018), to segment text. However, to the\nbest of our knowledge, the literature does not\ncontain a direct evaluation of the impact of\ntokenization on language model pretraining.\nWe analyze differences between BPE and un-\nigram LM tokenization, ﬁnding that the latter\nmethod recovers subword units that align more\nclosely with morphology and avoids problems\nstemming from BPE’s greedy construction\nprocedure. We then compare the ﬁne-tuned\ntask performance of identical transformer\nmasked language models pretrained with these\ntokenizations. Across downstream tasks and\ntwo languages (English and Japanese), we\nﬁnd that the unigram LM tokenization method\nmatches or outperforms BPE. We hope that\ndevelopers of future pretrained LMs will\nconsider adopting the unigram LM method\nover the more prevalent BPE.\n1 Introduction\nLarge transformers (Vaswani et al., 2017) pre-\ntrained with variants of a language modeling ob-\njective, such as BERT (Devlin et al., 2019), have\nproven their effectiveness at ﬂexibly transferring to\na variety of domains and tasks. One design deci-\nsion that makes them particularly adaptable is their\ngraceful handling of the open vocabulary problem\nthrough subword tokenization. Subword tokeniza-\ntion, popularized in the neural machine translation\nliterature (Sennrich et al., 2016; Vaswani et al.,\n2017; Wu et al., 2016), produces tokens at multiple\nlevels of granularity, from individual characters to\nfull words. As a result, rare words are broken down\ninto a collection of subword units, bottoming out\nin characters in the worst case.\nCritically, a pretrained language model’s sub-\nword vocabulary cannot be altered: any down-\nstream application of these models must tokenize\ninput or generate output using the original subword\nvocabulary, making the choice of tokenization a\nparticularly signiﬁcant decision.\nA variety of subword tokenization methods have\nseen use in pretrained language models. BERT\nuses the WordPiece method (Schuster and Naka-\njima, 2012), a language-modeling based variant of\nBPE; T5 (Raffel et al., 2019) uses character-level\nBPE; GPT2 (Radford et al., 2019) and ROBERTA\n(Liu et al., 2019) use BPE over raw bytes instead\nof unicode characters; XLN ET (Yang et al., 2019)\nand ALBERT (Lan et al., 2019) use the Sentence-\nPiece library (Kudo and Richardson, 2018) which\nimplements both BPE and unigram language model\ntokenization, but in both cases fail to clarify which\nof these methods they chose. The effects of tok-\nenization are not examined in a reported experi-\nment in any of the above works except Liu et al.\n(2019), who note that WordPiece gave a small ad-\nvantage over BPE in their preliminary investigation.\nIn the machine translation literature, Kudo (2018)\nintroduced the unigram language model tokeniza-\ntion method in the context of machine translation\nand found it comparable in performance to BPE.\nDomingo et al. (2018) performed further experi-\nments to investigate the effects of tokenization on\nneural machine translation, but used a shared BPE\nvocabulary across all experiments. Gall ´e (2019)\nexamined algorithms in the BPE family, but did not\ncompare to unigram language modeling.\nIn this work, we characterize the space of pro-\nposed subword tokenization algorithms and ana-\nlyze the differences between the two methods with\n4618\npublicly available implementations: BPE (merg-\ning tokens based on bigram frequency) and uni-\ngram language modeling (pruning tokens based on\nunigram LM perplexity). While the vocabularies\nresulting from these schemes are heavily overlap-\nping, we compare each method to reference mor-\nphological segmentations and ﬁnd that the unigram\nLM method produces tokens better aligned with\nmorphology. To understand whether this more nat-\nural tokenization leads to improved performance,\nwe pretrain separate language models using the\nROBERTA objective (Liu et al., 2019) with each\ntokenization for both English and Japanese, two\ntypologically distant languages. On downstream\ntasks, we ﬁnd a performance gap across tasks and\nlanguages, with the unigram LM method provid-\ning an improvement over BPE of up to 10% in\nour Japanese QA experiments, indicating the ben-\neﬁts of adopting this technique in the context of\nlanguage model pretraining.\n2 Algorithms\nSubword tokenization algorithms consist of two\ncomponents: a vocabulary construction procedure,\nwhich takes a corpus of text and returns a vocabu-\nlary with the desired size, and a tokenization proce-\ndure, which takes the built vocabulary and applies it\nto new text, returning a sequence of tokens. In the-\nory, these two steps can be independent, although\nfor the algorithms we examine the tokenization\nprocedure is tightly coupled to the vocabulary con-\nstruction procedure.\nA BPE vocabulary is constructed as follows:\nAlgorithm 1Byte-pair encoding (Sennrich et al.,\n2016; Gage, 1994)\n1: Input: set of strings D, target vocab size k\n2: procedure BPE( D,k)\n3: V ←all unique characters in D\n4: (about 4,000 in English Wikipedia)\n5: while |V|<k do ⊿Merge tokens\n6: tL,tR ←Most frequent bigram in D\n7: tNEW ←tL + tR ⊿Make new token\n8: V ←V + [tNEW ]\n9: Replace each occurrence of tL,tR in\n10: Dwith tNEW\n11: end while\n12: return V\n13: end procedure\nBPE tokenization takes the vocabulary V con-\ntaining ordered merges and applies them to new\ntext in the same order as they occurred during vo-\ncabulary construction.\nThe WordPiece algorithm (Schuster and Naka-\njima, 2012), used to construct BERT’s vocabulary,\nclosely resembles BPE. However, instead of merg-\ning the most frequent token bigram, each poten-\ntial merge is scored based on the likelihood of an\nn-gram language model trained on a version of\nthe corpus incorporating that merge. Schuster and\nNakajima (2012) note that the process of estimat-\ning language model parameters for every potential\nmerge is prohibitive, so they employ aggressive\nheuristics to reduce the number of potential merges\nconsidered. As their implementation is not public,1\nwe are unable to make a comparison to this method.\nThe unigram LM method (Kudo, 2018), in con-\ntrast to the bottom-up construction process of BPE\nand WordPiece, begins with a superset of the ﬁnal\nvocabulary, pruning it to the desired size:\nAlgorithm 2Unigram LM (Kudo, 2018)\n1: Input: set of strings D, target vocab size k\n2: procedure UNIGRAM LM(D,k)\n3: V ←all substrings occurring more than\n4: once in D(not crossing words)\n5: while |V|>k do ⊿Prune tokens\n6: Fit unigram LM θto D\n7: for t∈V do ⊿Estimate token ‘loss’\n8: Lt ←pθ(D) −pθ′ (D)\n9: where θ′ is the LM without token t\n10: end for\n11: Remove min(|V|−k,⌊α|V|⌋) of the\n12: tokens twith highest Lt from V,\n13: where α∈[0,1] is a hyperparameter\n14: end while\n15: Fit ﬁnal unigram LM θto D\n16: return V,θ\n17: end procedure\nUnigram LM tokenization takes the vocabulary\nV and unigram LM parameters θ and performs\nViterbi inference to decode the segmentation with\nmaximum likelihood under θ. This method is\nsimilar to Morfessor’s unsupervised segmentation\n(Creutz and Lagus, 2005) without its informed prior\nover token length.\n1Although its name and association with Google might sug-\ngest otherwise, the SentencePiece library (Kudo and Richard-\nson, 2018) does not, in fact, implement the WordPiece algo-\nrithm; it provides implementations of BPE and unigram LM\nbased tokenization.\n4619\nOriginal: furiously\nBPE: fur iously\nUni. LM: fur ious ly\nOriginal: tricycles\nBPE: t ric y cles\nUni. LM: tri cycle s\nOriginal: nanotechnology\nBPE: n an ote chn ology\nUni. LM: nano technology\nOriginal: Completely preposterous suggestions\nBPE: Comple t ely prep ost erous suggest ions\nUnigram LM: Complete ly pre post er ous suggestion s\nOriginal: corrupted\nBPE: cor rupted\nUnigram LM: corrupt ed\nOriginal: 1848 and 1852,\nBPE: 184 8 and 185 2,\nUnigram LM: 1848 and 1852 ,\nOriginal 磁性は様々に分類がなされている。\nBPE 磁　性は 様々 に分類 がなされている 。\nUnigram LM 磁　性 は 様々 に 分類 がなされている 。\nGloss magnetism (top.) various ways in classiﬁcation is done .\nTranslation Magnetism is classiﬁed in various ways.\nFigure 1: Example tokenizations. The character ‘ ’ is a word boundary marker. BPE merges common tokens,\nsuch as English inﬂectional sufﬁxes and Japanese particles, into their neighbors even when the resulting unit is not\nsemantically meaningful.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nToken length\n0\n1000\n2000\n3000\n4000Number of tokens\nBPE\nUnigram LM\n(a) Token length distributions within each vocabulary\n1 20000\nToken frequency rank\n102\n103\n104\n105\n106\n107\nToken frequency\nBPE\nUnigram LM (b) Token frequency proﬁles over the corpus\nFigure 2: English subword vocabulary and corpus proﬁles. The unigram LM method produces longer tokens on\naverage (a) and uses its vocabulary space more effectively (b), with more tokens of moderate frequency.\nIn the course of our experiments we did not ob-\nserve a major difference in speed between the two\nalgorithms. Both require similar amounts of time to\nconstruct a vocabulary, and both have a negligible\nimpact on overall model inference latency.\n3 Comparison of Segmentations\n3.1 Morphology\nIn Figure 1 we illustrate the differences in tok-\nenization output between BPE and the unigram\nLM method. We observe that the unigram LM\nmethod produces subword units that qualitatively\nalign with morphology much better than those pro-\nduced by BPE. In particular, we note that the un-\nigram LM method recovers common afﬁxes such\nas -ly, -s, pre-, and tri- while BPE does not, instead\nabsorbing them into adjacent units ( -cles) while\nalso producing meaningless single-character units.\nThis trend is supported by Table 1, in which\nMore frequent in\nBPE Unigram LM\nH L M T B\nP C K D R\ns . , ed d\ning e ly t a\nTable 1: Tokens with the highest difference in fre-\nquency between tokenizations. The unigram LM\nmethod tends to produce more parsimonious preﬁxes\nand sufﬁxes.\nTokenization\nBPE Unigram LM\nTokens per word type 4.721 4.633\nTokens per word 1.343 1.318\nTable 2: Mean subword units per word for each method\nacross all of English Wikipedia.\nwe observe that recognizable afﬁxes appear much\nmore frequently in the unigram LM tokenization of\nour pretraining corpus than in the BPE tokenization.\n4620\nMethod English (w.r.t. CELEX2) Japanese (w.r.t. MeCab)\nPrecision Recall F1 Precision Recall F1\nBPE 38.6% 12.9% 19.3% 78.6% 69.5% 73.8%\nUni. LM 62.2% 20.1% 30.3% 82.2% 72.8% 77.2%\nTable 3: Correspondence of subword boundaries between unsupervised tokenization methods and morphological\nreference segmentations.\nAs the BPE tokenization is constructed greedily\naccording to frequency, common afﬁxes (and punc-\ntuation) are frequently absorbed into other tokens.2\nWe see in Figure 2a that the unigram LM tok-\nenization tends to have longer subword units than\nBPE. This is closer to the length distribution of\ngold-standard English morphs, which have a mean\nlength of approximately 6 characters (Creutz and\nLinden, 2004).\nComparison with morphological segmenters\nIn Table 3, we further corroborate these observa-\ntions by performing a quantitative evaluation of the\ndegree to which each unsupervised segmentation\nalgorithm aligns with morphological baselines for\neach language. For English, we produce gold sur-\nface allomorph boundaries from the CELEX2 lexi-\ncal database (Baayen et al., 1995) in the manner of\nCreutz and Lind´en (2004). We then compare each\nalgorithm’s subword unit boundaries with gold mor-\npheme boundaries for words with 2 or more mor-\nphemes, weighted by their frequency in English\nWikipedia. For Japanese, we compare subword\ntokenizations of Japanese Wikipedia sentences to\nmorphological reference tokenizations produced\nusing the MeCab morphological analysis and tok-\nenization tool (Kudo, 2006) using version 2.3.0 of\nthe UniDic dictionary (Den et al., 2007).\nWe ﬁnd that for both languages, the segmenta-\ntions produced by the unigram LM method cor-\nrespond more closely to the morphological refer-\nences, conﬁrming our qualitative analysis. On En-\nglish data, both unsupervised methods exhibit low\nboundary recall; we attribute this to the fact that\nthey represent many common words with underly-\ning derivational morphology as single tokens, al-\nthough for BPE this is compounded by effects we\ndiscuss in Section 3.2.\nThe ability of the unigram LM method to recover\nthe morphological structure of the text without ex-\nplicit supervision aligns with the main ﬁndings of\n2Note that the BPE vocabulary still includes these afﬁxes,\nbut when they are encountered during tokenization, they are\nalmost always merged into larger units as in Figure 1.\nCreutz and Lagus (2005), who successfully use\nmaximum-a-posteriori unigram language models\nto perform unsupervised morphological segmenta-\ntion of English and Finnish.\n3.2 Vocabulary Allocation\nBy surfacing subword units that align with mor-\nphology, the unigram LM tokenization provides\nthe opportunity for the model to learn composable\nsubword embeddings. If an afﬁx reliably signals a\nlinguistic feature, rather than needing to store that\ninformation redundantly across the embeddings of\nmany tokens containing the afﬁx, the model can\nstore it in just the embedding of the afﬁx.\nThese results suggest that the unigram LM\nmethod may allocate its vocabulary more economi-\ncally. We note in Figure 2b that both vocabularies\ncontain a “dead zone” of tokens whose frequency\nis much lower than the rest of the vocabulary. This\nis largely the result of the presence of a number of\nvery uncommon characters, including Chinese and\nJapanese kanji, in the training corpus. In the BPE\ntokenization, however, this effect is exacerbated,\nwith the dead zone containing about 1500 more\nentries as a result of the tendency of its vocabulary\nconstruction process to produce intermediate “junk”\ntokens. For example, in the case where three tokens\nalmost always occur as a group, in order to merge\nthem into a single token, BPE must ﬁrst merge one\npair before incorporating the third token; this leaves\nan intermediate token in the vocabulary that will\nonly occur rarely on its own. Additionally, tokens\nthat appear in many contexts, such as inﬂectional\nafﬁxes (-s, -ed), will tend to merge with many adja-\ncent units due to their frequency. However, these\nmerges lead to embedding redundancy, as these\nafﬁxes usually have the same linguistic function in\nevery context. Since the unigram LM method se-\nlects tokens during vocabulary construction using a\nglobal optimization procedure, it does not produce\njunk tokens; this property also allows it to avoid\nmerging frequent tokens with their neighbors too\naggressively.\nJapanese vocabulary comparisons are included\n4621\nEnglish Japanese\nModel SQuAD 1.1(dev.) MNLI (dev.) CoNLL NER TyDi QA(dev.)\nEM F1 Acc. (m) Acc. (mm) Dev. F1 Test F1 EM F1\nOurs, BPE 80.6 ± .2 88 .2 ± .1 81 .4 ± .3 82 .4 ± .3 94 .0 ± .1 90 .2 ± .0 41.4 ± 0.6 42 .1 ± 0.6\nOurs, Uni. LM 81.8 ± .2 89 .3 ± .1 82 .8 ± .2 82 .9 ± .2 94 .3 ± .1 90 .4 ± .1 53.7 ± 1.3 54 .4 ± 1.2\nBERTBASE 80.5 88.5 84.6 83.4 96.4 92.4 – –\nTable 4: Fine-tuning results. Metrics are averaged across 5 ﬁne-tuning seeds with standard deviations indicated\nby ±; due to computational constraints we did not pretrain more than once per tokenization. We include ﬁne-\ntuning results for a transformer with a comparable architecture, BERT BASE , for reference, although we note that a\ndirect comparison cannot be made due to BERT BASE using both a larger pretraining corpus and a larger subword\nvocabulary.\nin Appendix B.\n4 Downstream Task Experiments\nIn order to make a fair experimental comparison be-\ntween these two methods on downstream tasks, we\ndo not use an existing pretrained language model\nlike BERT, but instead train our own language mod-\nels from scratch, controlling for the data, training\nobjective, and optimization procedure. We pre-\ntrain four transformer masked language models\nusing the architecture and training objective of\nROBERTA-BASE (Liu et al., 2019) using the refer-\nence fairseq implementation (Ott et al., 2019).\nTwo are pretrained on the text of English Wikipedia,\ncomprising ∼3B tokens under either tokenization.\nThe other two are pretrained on the text of Japanese\nWikipedia, comprising ∼0.6B tokens. In each pair,\none model is pretrained on the BPE tokenization of\nthe corpus, and the other on the unigram LM tok-\nenization, each with a vocabulary of 20,000 tokens.\nHyperparameters are listed in Appendix A.\nWe subsequently ﬁne-tune each of the pretrained\nEnglish models on the SQuAD question-answering\ntask (Rajpurkar et al., 2016), the MNLI textual\nentailment task (Williams et al., 2018), and the\nEnglish portion of the CoNLL 2003 named-entity\nrecognition shared task (Tjong Kim Sang and\nDe Meulder, 2003). We ﬁne-tune the Japanese\nmodels on the Japanese minimal-answer subset\nof the TyDi question-answering task (Clark et al.,\n2020). We base our ﬁne-tuning implementations on\nthose of the transformers toolkit (Wolf et al.,\n2019).\nThe results of our ﬁne-tuning experiments are\npresented in Table 4. We show that ﬁne-tuning\nmodels pretrained with unigram LM tokenization\nproduces better performance than ﬁne-tuning mod-\nels pretrained with BPE tokenization for all tasks.\nThese results suggest that the higher morpholog-\nical plausibility of the unigram LM tokenization\nmay translate into better downstream task perfor-\nmance as well. Larger performance gaps are ev-\nident on SQuAD and MNLI, but the largest gap\nappears on Japanese TyDi. Differences in pretrain-\ning may be more evident in this setting due to the\nfact that the Japanese portion of the TyDi train-\ning split only contains ∼5k examples, compared\nto the ∼88k examples available for ﬁne-tuning on\nSQuAD. Additionally, written Japanese does not\nfeature whitespace between words, so it is possi-\nble for tokenizations to differ in word boundary\nplacement as well as subword segmentation.\n5 Conclusion\nIn this work we show that the choice of input en-\ncoding makes a difference in how well pretrained\nlanguage models are able to perform end tasks.\nThis indicates that tokenization encodes a surpris-\ning amount of inductive bias, and we suggest that\nunigram LM tokenization may be the better choice\nfor development of future pretrained models.\nAcknowledgments\nThis work was partially supported by NSF Grant\nIIS-1814522 and a gift from Arm. This material\nis also based on research that is supported by the\nAir Force Research Laboratory (AFRL), DARPA,\nfor the KAIROS program under agreement num-\nber FA8750-19-2-1003. The U.S. Government is\nauthorized to reproduce and distribute reprints for\nGovernmental purposes notwithstanding any copy-\nright notation thereon. The views and conclusions\ncontained herein are those of the authors and should\nnot be interpreted as necessarily representing the\nofﬁcial policies or endorsements, either expressed\nor implied, of the Air Force Research Laboratory\n(AFRL), DARPA, or the U.S. Government.\n4622\nReferences\nR. Harald Baayen, Richard Piepenbrock, and Leon Gu-\nlikers. 1995. The CELEX lexical database (release\n2).\nJonathan Clark, Jennimaria Palomaki, Vitaly Nikolaev,\nEunsol Choi, Dan Garrette, Michael Collins, and\nTom Kwiatkowski. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8(0):454–\n470.\nMathias Creutz and Krista Lagus. 2005. Unsupervised\nmorpheme segmentation and morphology induction\nfrom text corpora using Morfessor 1.0. Helsinki\nUniversity of Technology Helsinki.\nMathias Creutz and Krister Lind ´en. 2004. Morpheme\nsegmentation gold standards for ﬁnnish and english.\nReport, Helsinki University of Technology.\nMathias Johan Philip Creutz and Bo Krister Johan Lin-\nden. 2004. Morpheme segmentation gold standards\nfor Finnish and English. Publications in Computer\nand Information Science Report A77.\nYasuharu Den, Toshinobu Ogiso, Hideki Ogura, At-\nsushi Yamada, Nobuaki Minematsu, Kiyotaka Uchi-\nmoto, and Hanae Koiso. 2007. The development\nof an electronic dictionary for morphological analy-\nsis and its application to japanese corpus linguistics.\nJapanese Linguistics, 22:101–123.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMiguel Domingo, Mercedes Garcıa-Martınez, Alexan-\ndre Helle, and Francisco Casacuberta. 2018. How\nmuch does tokenization affect in neural machine\ntranslation? arXiv preprint arXiv:1812.08621.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. C Users Journal, 12(2):23–38.\nMatthias Gall´e. 2019. Investigating the effectiveness of\nBPE: The power of shorter sequences. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 1375–1381, Hong\nKong, China. Association for Computational Lin-\nguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nTaku Kudo. 2006. MeCab: Yet another part-of-speech\nand morphological analyzer.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A lite BERT for self-supervised\nlearning of language representations. arXiv preprint\narXiv:1909.11942.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand Korean voice search. 2012 IEEE International\nConference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 5149–5152.\n4623\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, et al. 2019. Transformers: State-of-the-\nart natural language processing. arXiv preprint\narXiv:1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. XLNet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nA Hyperparameters\nPretraining\nModel architecture\nROBERTA-BASE\n(Liu et al., 2019)\nImplementation\nfairseq\n(Ott et al., 2019)\nOptimizer\nADAM, ϵ= 1e-6\nβ = (0.9,0.98)\n(Kingma and Ba, 2015)\nLearning rate decay Polynomial\nPeak learning rate 0.0005\nWarmup steps 10000\nWeight decay 0.01\nBatch size 2048\nSequence length 512\nTotal updates 125000\nMLP dropout 0.1\nAttention dropout 0.1\nPrecision 16-bit\nFine-tuning\nImplementations\ntransformers\n(Wolf et al., 2019)\nOptimizer\nADAM, ϵ= 1e-8\nβ = (0.9,0.999)\nLearning rate decay Linear\nPeak learning rate 5e-5\nWarmup steps 0\nWeight decay 0\nBatch size 32\nSequence length\n(SQuAD, TyDi QA) 512\nPassage stride\n(SQuAD, TyDi QA) 192\nSequence length\n(MNLI, NER) 128\nEpochs 3\nPrecision 16-bit\nTokenization\nImplementations SentencePiece\n(Kudo and Richardson, 2018)\nV ocabulary size 20000\nUnigram LM α 0.25\n4624\nB Japanese vocabulary comparison\nMore frequent in\nBPE Unigram LM\n)、 )。 ) ンの スの\nは のは の 、2 ンは\nli lo ていく vi てしまう\nhi 0% to no ta\nTable 5: Tokens with the highest difference in frequency between tokenizations. The BPE method merges common\ntokens, such as particles and punctuation, even when they do not form meaningful units. The unigram LM method\nrecovers the units ていくand てしまう, which are productive components of the Japanese verb conjugation\nsystem.\n1 2 3 4 5 6 7 8 9\nToken length\n0\n2000\n4000\n6000\n8000Number of tokens\nBPE\nUnigram LM\n(a) Token length distributions within each vocabulary\n1 20000\nToken frequency rank\n102\n104\n106\nToken frequency\nBPE\nUnigram LM (b) Token frequency proﬁles over the corpus\nFigure 3: Japanese subword vocabulary and corpus proﬁles. (a) The unigram LM method produces longer tokens,\nas it does in English. (b) Token frequency proﬁles resemble those of English, though the effect of the “dead zone”\nis less pronounced.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8324757814407349
    },
    {
      "name": "Lexical analysis",
      "score": 0.7905160784721375
    },
    {
      "name": "Language model",
      "score": 0.774237334728241
    },
    {
      "name": "Transformer",
      "score": 0.7127121686935425
    },
    {
      "name": "Natural language processing",
      "score": 0.6348382830619812
    },
    {
      "name": "Artificial intelligence",
      "score": 0.616924524307251
    },
    {
      "name": "Byte",
      "score": 0.6010062098503113
    },
    {
      "name": "Encoding (memory)",
      "score": 0.48689571022987366
    },
    {
      "name": "Question answering",
      "score": 0.47642597556114197
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4260578155517578
    },
    {
      "name": "Natural language",
      "score": 0.4252592921257019
    },
    {
      "name": "Speech recognition",
      "score": 0.3320491313934326
    },
    {
      "name": "Programming language",
      "score": 0.19343259930610657
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}