{
  "title": "Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation",
  "url": "https://openalex.org/W4382457852",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097517997",
      "name": "Han Li",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2138483075",
      "name": "Bowen Shi",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2110344549",
      "name": "Wenrui Dai",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2059154194",
      "name": "Hongwei Zheng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2096757069",
      "name": "Botao Wang",
      "affiliations": [
        "Qualcomm (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2097179453",
      "name": "Yu Sun",
      "affiliations": [
        "Qualcomm (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2135665427",
      "name": "Min Guo",
      "affiliations": [
        "Qualcomm (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2108813301",
      "name": "Chenglin Li",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2137565673",
      "name": "Junni Zou",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2134657262",
      "name": "Hongkai Xiong",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2097517997",
      "name": "Han Li",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2138483075",
      "name": "Bowen Shi",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2110344549",
      "name": "Wenrui Dai",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2059154194",
      "name": "Hongwei Zheng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2096757069",
      "name": "Botao Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097179453",
      "name": "Yu Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135665427",
      "name": "Min Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108813301",
      "name": "Chenglin Li",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2137565673",
      "name": "Junni Zou",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2134657262",
      "name": "Hongkai Xiong",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2972662547",
    "https://openalex.org/W2769331938",
    "https://openalex.org/W2966735886",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6803556686",
    "https://openalex.org/W3043405370",
    "https://openalex.org/W6792014136",
    "https://openalex.org/W3097623574",
    "https://openalex.org/W2893627667",
    "https://openalex.org/W2612706635",
    "https://openalex.org/W2797184202",
    "https://openalex.org/W2964784655",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2798646183",
    "https://openalex.org/W2903549000",
    "https://openalex.org/W3089969279",
    "https://openalex.org/W3173225380",
    "https://openalex.org/W6736066200",
    "https://openalex.org/W3146860202",
    "https://openalex.org/W3184641178",
    "https://openalex.org/W3151072205",
    "https://openalex.org/W3203700770",
    "https://openalex.org/W6749791135",
    "https://openalex.org/W4221154956",
    "https://openalex.org/W3098612954",
    "https://openalex.org/W4313068951",
    "https://openalex.org/W3136525061",
    "https://openalex.org/W2981661132",
    "https://openalex.org/W2756050327",
    "https://openalex.org/W3188906027",
    "https://openalex.org/W3127588730",
    "https://openalex.org/W3202716970",
    "https://openalex.org/W2962896489",
    "https://openalex.org/W582134693",
    "https://openalex.org/W4312417903",
    "https://openalex.org/W2998273692",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2795089319",
    "https://openalex.org/W4287265181",
    "https://openalex.org/W2989465897",
    "https://openalex.org/W3203617912",
    "https://openalex.org/W2964221239",
    "https://openalex.org/W3117675859",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2899818084",
    "https://openalex.org/W3173811519",
    "https://openalex.org/W3176376875",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W3216189954"
  ],
  "abstract": "There has been a recent surge of interest in introducing transformers to 3D human pose estimation (HPE) due to their powerful capabilities in modeling long-term dependencies. However, existing transformer-based methods treat body joints as equally important inputs and ignore the prior knowledge of human skeleton topology in the self-attention mechanism. To tackle this issue, in this paper, we propose a Pose-Oriented Transformer (POT) with uncertainty guided refinement for 3D HPE. Specifically, we first develop novel pose-oriented self-attention mechanism and distance-related position embedding for POT to explicitly exploit the human skeleton topology. The pose-oriented self-attention mechanism explicitly models the topological interactions between body joints, whereas the distance-related position embedding encodes the distance of joints to the root joint to distinguish groups of joints with different difficulties in regression. Furthermore, we present an Uncertainty-Guided Refinement Network (UGRN) to refine pose predictions from POT, especially for the difficult joints, by considering the estimated uncertainty of each joint with uncertainty-guided sampling strategy and self-attention mechanism. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art methods with reduced model parameters on 3D HPE benchmarks such as Human3.6M and MPI-INF-3DHP.",
  "full_text": "Pose-Oriented Transformer with Uncertainty-Guided Refinement\nfor 2D-to-3D Human Pose Estimation\nHan Li1, Bowen Shi1, Wenrui Dai1*, Hongwei Zheng1, Botao Wang2,\nYu Sun2, Min Guo2, Chenglin Li1, Junni Zou1, Hongkai Xiong1\n1Shanghai Jiao Tong University, Shanghai, China\n2Qualcomm AI Research‚Ä†, Shanghai, China\n{qingshi9974, sjtu shibowen, daiwenrui, 1424977324}@sjtu.edu.cn, {botaow, sunyu, mguo}@qti.qualcomm.com,\n{lcl1985, zoujunni, xionghongkai}@sjtu.edu.cn\nAbstract\nThere has been a recent surge of interest in introducing trans-\nformers to 3D human pose estimation (HPE) due to their pow-\nerful capabilities in modeling long-term dependencies. How-\never, existing transformer-based methods treat body joints as\nequally important inputs and ignore the prior knowledge of\nhuman skeleton topology in the self-attention mechanism. To\ntackle this issue, in this paper, we propose a Pose-Oriented\nTransformer (POT) with uncertainty guided refinement for\n3D HPE. Specifically, we first develop novel pose-oriented\nself-attention mechanism and distance-related position em-\nbedding for POT to explicitly exploit the human skeleton\ntopology. The pose-oriented self-attention mechanism explic-\nitly models the topological interactions between body joints,\nwhereas the distance-related position embedding encodes the\ndistance of joints to the root joint to distinguish groups\nof joints with different difficulties in regression. Further-\nmore, we present an Uncertainty-Guided Refinement Net-\nwork (UGRN) to refine pose predictions from POT, especially\nfor the difficult joints, by considering the estimated uncer-\ntainty of each joint with uncertainty-guided sampling strategy\nand self-attention mechanism. Extensive experiments demon-\nstrate that our method significantly outperforms the state-of-\nthe-art methods with reduced model parameters on 3D HPE\nbenchmarks such as Human3.6M and MPI-INF-3DHP.\nIntroduction\n3D human pose estimation (HPE) aims to obtain the 3D spa-\ntial coordinates of body joints from monocular images or\nvideos. It has attracted extensive attention in a wide range of\napplications such as autonomous driving, augmented/virtual\nreality (AR/VR) and virtual avatar. The 2D-to-3D pipeline is\nprevailing in recent works (Martinez et al. 2017; Zhao et al.\n2019; Cai et al. 2019; Li et al. 2021), where 2D joint co-\nordinates are taken as the inputs to directly regress the 3D\npose target. Despite its promising performance, the 2D-to-\n3D pipeline is restricted by depth ambiguity caused by the\nmany-to-one mapping from multiple 3D poses to one same\n2D projection.\n*Corresponding author\n‚Ä†Qualcomm AI Research is an initiative of Qualcomm Tech-\nnologies, Inc. Datasets were downloaded and evaluated by Shang-\nhai Jiao Tong University researchers.\nCopyright ¬© 2023, Association for the Advancement of Artificial\nroot joint\n(pelvis)\nleft wrist\n0 1 2 3 4\ndistance towards the root joint\n0\n10\n20\n30\n40\n50\n60joint-wise mean estimation error (mm)\nFigure 1: Left: Human skeleton topology. We consider the\ndistance for each joint towards the root joint (pelvis) based\non the human skeleton topology.Right: Impact of distance\ntowards the root joint on the joint-wise estimation error.\nBased on a baseline model, we empirically find that joints far\nfrom the root joint tend to have large prediction errors. This\ninspires us to introduce targeted designs for these joints.\nConsidering that the human body can be modeled as a\nhighly structured graph, the problem of depth ambiguity can\nbe alleviated by exploiting the interactions between body\njoints. Graph convolution networks (GCNs) have been natu-\nrally adopted to exploit these interactions (Zhao et al. 2019;\nCai et al. 2019; Li et al. 2021). However, GCNs are usu-\nally limited in receptive fields and impede the relationship\nmodeling. Inspired by the success of Transformer (Vaswani\net al. 2017), the self-attention mechanism is leveraged in re-\ncent works (Zheng et al. 2021; Zhu et al. 2021; Zhao, Wang,\nand Tian 2022; Zhang et al. 2022) to facilitate global inter-\nactions for 3D HPE and yield state-of-the-art performance.\nHowever, these methods treat body joints as input to-\nkens of equal importance but ignore the human body\npriors (e.g., human skeleton topology) in designing the\nself-attention mechanism.\nIn this paper, we argue that introducing pose-oriented\ndesigns to the transformer is important for 3D HPE and\nthereby propose a Pose-Oriented Transformer (POT) for re-\nliable pose prediction. We design a novel pose-oriented self-\nattention (PO-SA) mechanism for POT that is the first to ex-\nIntelligence (www.aaai.org). All rights reserved.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1296\nplicitly exploit human skeleton topology without implicitly\ninjecting graph convolutions. The relative distance is com-\nputed for each joints pair and is encoded as attention bias\ninto the self-attention mechanism to enhance the ability of\nmodeling the human skeleton dependence. Furthermore, as\nshown in Figure 1, we empirically find that joints far from\nthe root joint (pelvis) tend to have large prediction errors. To\nbetter model these difficult joints, we split body joints into\nseveral groups according to their distance toward the root\njoint and assign additional distance-related position embed-\ndings to different groups.\nIn addition to POT, a second stage of pose refine-\nment is developed to further improve the prediction of\ndifficult joints. Specifically, we propose a transformer-\nbased Uncertainty-Guided Refinement Network (UGRN)\nfor pose refinement by explicitly considering the pre-\ndiction uncertainty. The proposed UGRN comprises an\nuncertainty-guided sampling strategy and an uncertainty-\nguided self-attention (UG-SA) mechanism. The uncertainty-\nguided sampling strategy incorporates the estimated uncer-\ntainty for each joint (that implies the difficulty of predic-\ntion) into the learning procedure. The joint coordinates are\nsampled around the prediction from POT following a Gaus-\nsian distribution with the estimated uncertainty as variance.\nThen, we use the sampled coordinates as the input of UGRN\nto make the model more robust to errors. Subsequently, the\nUG-SA is developed in UGRN to reduce the contribution of\nthe joints with high uncertainty during learning.\nThis paper makes the following contributions:\n‚Ä¢ We propose a novel pose-oriented transformer for 3D\nHPE with the self-attention and position embedding\nmechanisms explicitly designed to exploit human skele-\nton topology.\n‚Ä¢ We present an uncertainty-guided refinement network to\nfurther improve pose predictions for difficult joints with\nuncertainty-guided sampling strategy and self-attention\nmechanism.\n‚Ä¢ We demonstrate our method achieves SOTA performance\non the Human3.6M and MPI-INF-3DHP benchmarks\nand shed light on the task-oriented transformer design for\nsingle-frame input human pose estimation.\nRelated Work\n3D Human Pose Estimation\nThe methods of 3D human pose estimation can be di-\nvided into two categorizes: one-stage methods and two-stage\nmethods. The one-stage methods take RGB image as input\nand directly predict the 3D pose. Thanks to the development\nof deep learning, recent works (Zhou et al. 2017; Shi et al.\n2020; Pavlakos, Zhou, and Daniilidis 2018; Moon, Chang,\nand Lee 2019; Lin and Lee 2020; Sun et al. 2017) can\nleverage the advantages of Convolutional Neural Networks\n(CNNs) to obtain promising results for image-to-3D human\npose estimation. In which (Zhou et al. 2017) built a weakly-\nsupervised transfer learning framework to make full use of\nmixed 2D and 3D labels, and augmented the 2D pose esti-\nmation sub-network with a 3D depth regression sub-network\nto estimate the depth. (Pavlakos, Zhou, and Daniilidis 2018)\nrepresented the space around the human body discretely as\nvoxel and used 3D heatmaps to regress 3D human pose. Tak-\ning the feature extracted by CNNs as input, (Lin, Wang, and\nLiu 2021) further proposed a graph-convolution-reinforced\ntransformer to predict 3D pose. (Wehrbein et al. 2021) pro-\nposed a normalizing flow method that can generate a diverse\nset of feasible 3D poses.\nThe second category of methods first estimate the 2D po-\nsition of human joints from the input image, and then regress\nthe 3D pose in the camera coordinate system. Pioneering\nwork (Martinez et al. 2017) revealed that only using 2D\njoints as input can also gets highly accurate results, and pro-\nposed a simple yet effective baseline for 3D HPE. Since the\nhuman body can be regarded as a highly structured graph,\n(Zhao et al. 2019) proposed Semantic Graph Convolution\n(SemGConv) for 3D HPE, it added a parameter matrix to\nlearn the semantic relations among body joints. (Zou et al.\n2020) further extended SemGConv to a high-order GCN To\nlearn long-range dependencies among body joints . Never-\ntheless, GCN-based methods still suffer from limited recep-\ntive field. In this work, we leverage the powerful long-term\nmodeling capability of transformer to construct our model.\nTransformer and Self-Attention Mechanism\nTransformer was firstly introduced in (Vaswani et al. 2017)\nfor the natural language processing (NLP) tasks such as\nmachine translation, whose core component is the self-\nattention mechanism that can model the long-term depen-\ndence of the input sequential data. Recently, with the ap-\npearance of VIT (Dosovitskiy et al. 2020), transformer also\nattracted much attention in various visual tasks. In addition,\n(Ying et al. 2021) also generalized transformer to graph-\nstructured data for graph-level predictions tasks including\nlink prediction and knowledge graphs. For the 3D HPE,\nPoseFormer (Zheng et al. 2021) first built a transformer-\nbased model to sequentially capture the temporal and spatial\ndependency of the input 2D pose sequence. PoseGTAC (Zhu\net al. 2021) and Graformer (Zhao, Wang, and Tian 2022)\nboth injected graph convolution into transformer in different\nways to exploit the structure information of human skele-\nton topology. However, we argue that simply stacking self-\nattention and graph convolution can not fully utilize the hu-\nman skeleton topology and propose our pose-oriented trans-\nformer to take the topology information into account in the\nself-attention mechanism.\nUncertainty Estimation\nUncertainty in the deep learning models can be categorized\ninto two types: aleatoric uncertainty and epistemic uncer-\ntainty. It can be estimated by sampling-based method (Glo-\nrot and Bengio 2010) and dropout method (Gal and Ghahra-\nmani 2016). (Kendall and Gal 2017) further revealed that\nthe heteroscedastic uncertainty dependent on the input data\nis vitally important for computer vision application. For\nexample, (Song et al. 2021) considered the uncertainty of\nthe noisy input data and proposed the uncertain graph neu-\nral networks for facial action unit detection. (Wang et al.\n1297\nùê∫\n!\n(\n#\n)\nlinear\tProjection\nùêæ# ùêæ%\n ùê∫\n!\n(\n&\n)\nùê∫\n!\n(\n#\n)\n ùê∫\n!\n(\n%\n)ùêæ&\nfeature\tembeddings\t(ùíÅ) group\tposition\tembeddings\t(ùëÆ)ùêæ#\nùê∫\n!\n(\n#\n)ùêæ#\nùê∫\n!\n(\n#\n)ùêæ# ùëç#$!\nùëç%$!\nùëç&$!\nPose-OrientedTransformerEncoder\nùëç&'\n Regression\tHead\nUncertainty\tEstimationHead\nduringtraining\nlinear\tProjectionUncertainty-GuidedRefinementNetwork\nLN\nUG\n-\nSA\n LN\nMLP!ùëç!œÉ !ùëç!\"#\nLN\nPO\n-\nSA\n LN\nMLPùëç! ùëç!\"#xùêø#Pose-Oriented\tTransformer\tEncoderUncertainty-Guided\tRefinement\tNetworkxùêø$\nsampling\nkeypointposition\tembeddings\t(ùë≤)\nùëç#'\nùëç%'\nùê∫\n!\n(\n#\n)ùêæ#\nùê∫\n!\n(\n#\n)ùêæ#\nùê∫\n!\n(\n#\n)ùêæ#\n'ùëç&'\n'ùëç#'\n'ùëç%'\nFigure 2: The overview of proposed method, which contains two major module: pose-oriented transformer (POT) and\nuncertainty-guided refinement network (UGRN). Given the 2D pose X ‚àà RJ√ó2 estimated by an off-the-shelf 2D pose de-\ntector, POT with pose-oriented attention and position embedding designs are first used for pose-related feature extracting and\nfirst-stage 3D pose predicting. Then, UGRN leverage uncertainty information œÉ ‚àà RJ√ó3 to generate refined pose ÀÜY ‚àà RJ√ó3.\n2021) utilized the data-uncertainty as guidance to pro-\npose a multi-phase learning method for semi-supervised ob-\nject detection. (Yang et al. 2021) combined the benefits\nof Bayesian learning and transformer-based reasoning, and\nbuilt an uncertainty-guided transformer for camouflaged ob-\nject detection. However, previous 2D-to-3D HPE methods\ndid not take uncertainty information of human pose into ac-\ncount in the training and inference procedure. For our work,\nwe estimate the uncertainty for each joint of first-stage 3D\npose and propose our UG-sampling and UG-SA to obtain\nthe refined 3D pose.\nMethod\nThe overview of the proposed method is depicted in Fig-\nure 2. Our method is a two-stage framework which consists\nof two major module: pose-oriented transformer (POT) and\nuncertainty-guided refinement network (UGRN). Given the\n2D pose X ‚àà RJ√ó2 estimated by an off-the-shelf 2D pose\ndetector from an image, POT is designed by utilizing human\nskeleton topology for better pose-related feature extracting\nand first-stage 3D pose predicting, while UGRN leverages\nuncertainty information œÉ ‚àà RJ√ó3 to further refine the pre-\ndicting pose. Details are included in the following.\nPreliminaries\nIn this work, we leverage transformer to model the long-\ndistance relationship between body joints. We first briefly\nintroduce the basic components in the transformer, includ-\ning multi-head self-attention (MH-SA), position-wise feed-\nforward network (FFN) and position embeddings.\nMH-SA The basic self-attention mechanism transfers the\ninputs Z ‚àà RN√óC into corresponding query Q, key Kand\nvalue V with the same dimensions N √ó C by projection\nmatrices PQ, PK, PV ‚àà RC√óC respectively, where N de-\nnotes the sequence length, and C is the number of hidden\ndimension.\nQ = ZP Q, K = ZP K, V = ZP V , (1)\nThen we can calculate self-attention by:\nA = QKT /\n‚àö\nd, MH-SA(X) = softmax(A)V, (2)\nwhere A ‚àà RN√óN denotes the attention weight matrix.\nBased on the basic self-attention, MH-SA further splits the\nQ, K, Vfor h times to perform attention in parallel and then\nthe outputs of all the heads are concatenated.\nFFN position-wise FFN is used for non-linear feature\ntransformation and it contains two Multilayer Perceptron\n(MLP) and an GELU activation layer. This procedure can\nbe formulated as follows:\nF F N(X) = MLP (GELU (MLP (X))) + X. (3)\nPosition Embeddings As MH-SA and FFN in trans-\nformer are permutation equivariant operation, additional\nmechanisms are required to encode the structure of input\ndata into model. In particular, we can utilize sine and cosine\nfunctions or learnable vectors as the position embeddings,\nwhich can be formulated as\nPt = P E(t) ‚àà RC, (4)\nwhere t denotes the position index.\nPose-oriented Transformer\nPOT aims at better utilizing the human skeleton information\nfor feature extracting. It includes target position embedding\nand self-attention design for 3D HPE. Specifically, given the\ninput 2D joints X ‚àà RJ√ó2, we first project it into high-\ndimensional feature embeddings Z ‚àà RJ√óC, where J de-\nnotes the number of human body joints and C denotes the\nembedding dimension. Then we add keypoint position em-\nbeddings K and our proposed group position embeddingsG\nto Z as the input of POT encoder. In POT encoder, we also\ndesign pose-oriented self-attention (PO-SA) which takes the\ntopological connections of body joints into consideration.\n1298\nKeypoint and Group Position Embeddings Following\nprevious design (Zheng et al. 2021; Zhang et al. 2022),\nwe first introduce a learnable keypoint position embeddings\nK ‚àà RJ√óC to represent the absolute position of each body\njoint. In addition, as shown in Figure 3, according to the\ndistance between each joint and the root joint (Pelvis), we\nsplit body joints into five groups and design another learn-\nable embeddings called group position embeddings, i.e. ,\nG ‚àà R5√óC. Therefore, additional distance-related knowl-\neage can be encoded into model, helping transformer bet-\nter model the difficult body joints that are far from the root.\nIn this way, the input of pose-oriented transformer encoder,\nZ(0), can be obtained by:\nZ(0)\ni = Zi + Ki + GœÜ(i), for i ‚àà [1, ¬∑¬∑¬∑ , J], (5)\nwhere i is the joint index and œÜ(i) = D(i, 1) represents the\nshortest path distance between i-th joint and the root joint.\nPose-Oriented Self-Attention (PO-SA) We also propose\nour pose-oriented self-attention (PO-SA) that explicitly\nmodeling the topological connections of body joints. Specif-\nically, we compute the relative distance for each joints pair\n(i, j), and encode it as the attention bias for the self-attention\nmechanism. In this way, we rewrite the self-attention in\nEq (2), in which the (i, j)-th element of attention matrix A\ncan be computed by:\nAi,j = (ZiPQ)(ZjPK)T /\n‚àö\nd + Œ¶(D(i, j)), (6)\nwhere Œ¶ is a MLP network which projects the relative dis-\ntance (1-dimension) to an H-dimension vector where H is\nthe number of heads in the SA mechanism, it makes each\nPO-SA have the ability to adjust the desired distance-related\nreceptive field and the additional parameters can be ignored.\nPOT Encoder Based on the PO-SA, we can obtain output\nfeatures by sending Z(0) to a cascaded transformer with L1\nlayers. These procedure can be formulated as :\nZ‚Ä≤l = PO-SA(LN(Zl‚àí1)) + Zl‚àí1, (7)\nZl = FFN(LN(Z‚Ä≤l)) + Z‚Ä≤l, (8)\nwhere LN(¬∑) represents the layer normalization and l ‚àà\n[1, 2, ¬∑¬∑¬∑ , L1] is the index of POT encoder layers.\nRegression Head In the regression head, we apply a MLP\non the output feature ZL1 to perform pose regression, gen-\nerating the first-stage 3D pose eY ‚àà RJ√ó3.\nUncertainty-guided Refinement\nTaking the first-stage 3D pose eY from POT, we further send\nit together with the input 2D poseX to another Uncertainty-\nguided Refinement Network (UGRN) for pose refinement.\nThe proposed UGRN contains the following components.\nUncertainty Estimation We first model the uncertainty\nfor each joint. Specifically, features of POT encoderZL1 are\nsent to another uncertainty estimation head, producing the\nuncertainty œÉ ‚àà RJ√ó3 of the first-stage 3D poses by using\nan uncertainty estimation loss LœÉ (Kendall and Gal 2017).\nFigure 3: The depiction of distance-related group for human\nbody joints.\nUncertainty-Guided Sampling Instead of directly utiliz-\ning the first-stage 3D predictionseY , we randomly sample 3D\ncoordinates ¬ØY around eY according to a Gaussian distribution\nN(eY , œÉ) with the predicted uncertainty œÉ as variance, and\nsend the sampled coordinates to UGRN. This uncertainty-\nguided sampling strategy ensures that the sampled coordi-\nnates have large variance on difficult joints, which requires\nthe model to focus more on making use of context from other\njoints to compensate for the difficult joint predictions, thus\nfurther enhancing the model robustness.\nTo enable correct back-propagation, we employ a re-\nparameterization trick to draw a sample œµ from the standard\nGaussian distribution N(0, 1) randomly, i.e., œµ ‚àº N(0, 1).\nIn this way, we can obtain the sampled 3D coordinates by:\n¬ØY = eY + œÉ ¬∑ œµ. (9)\nNote that this sample strategy is only implemented in the\ntraining stage. In the inference stage, we set ¬ØY = eY directly.\nUncertainty-guided Refinement Network After obtain-\ning the sampled 3D pose ¬ØY , we first concatenate it with the\ninput 2D pose X and obtain eX, i.e., eX = Concat( ¬ØY , X).\nThen we project eX to feature embeddings eZ and equip them\nwith keypoint position embeddings K and group position\nembedding G:\neZ(0)\ni = eZi + Ki + GœÜ(i), for i ‚àà [1, J]. (10)\nNext, ÀúZ(0)\ni is sent to the following L2 transformer lay-\ners of UGRN to perform uncertainty-guided refinement. The\ntransform layers of UGRN is similar to those of POT, but we\nreplace the distance-related term of Eq. 6 with uncertainty\nguildance to dynamically adjust the attention weights:\nAi,j = (ZiPQ)(ZjPK)T /\n\u0010‚àö\nd ¬∑ Sum(œÉj)\n\u0011\n, (11)\nwhere œÉj ‚àà R3 is the predicted uncertainty of j-th joint.\nThe above uncertainty-guided self-attention (UG-SA) en-\nsures that the body joints with high uncertainty will con-\ntribute less in the self-attention mechanism, which can not\nonly alleviate the error propagation, but also enhance the\ncontext understanding ability of the model.\nFinally, we apply another regression head toÀúZL2 and gen-\nerate our second-stage refined 3D pose ÀÜY ‚àà RJ√ó3.\n1299\nMethods Dire.Disc. Eat GreetPhonePhotoPosePuch. Sit SitD.SmokeWaitWalkDWalkWalkT Avg.\n(Martinez et al. 2017) (SH) 51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9\n(Zhao et al. 2019) (SH) 48.2 60.8 51.8 64.0 64.6 53.6 51.1 67.4 88.7 57.7 73.2 65.6 48.9 64.8 51.9 60.8\n(Liu et al. 2020) (CPN) 46.3 52.2 47.3 50.7 55.5 67.1 49.2 46.0 60.4 71.1 51.5 50.1 54.5 40.3 43.7 52.4\n(Zou et al. 2020)(CPN) 49.0 54.5 52.3 53.6 59.2 71.6 49.6 49.8 66.0 75.5 55.1 53.8 58.5 40.9 45.4 55.6\n(Xu and Takano 2021)(CPN) 45.2 49.9 47.5 50.9 54.9 66.1 48.5 46.3 59.7 71.5 51.4 48.6 53.9 39.9 44.1 51.9\nOurs (CPN) 47.9 50.0 47.1 51.3 51.2 59.5 48.7 46.9 56.0 61.9 51.1 48.9 54.3 40.0 42.9 50.5\n(Martinez et al. 2017) (GT) 37.7 44.4 40.3 42.1 48.2 54.9 44.4 42.1 54.6 58.0 45.1 46.4 47.6 36.4 40.4 45.5\n(Zhao et al. 2019) (GT) 37.8 49.4 37.6 40.9 45.1 41.4 40.1 48.3 50.1 42.2 53.5 44.3 40.5 47.3 39.0 43.8\n(Liu et al. 2020) (GT) 36.8 40.3 33.0 36.3 37.5 45.0 39.7 34.9 40.3 47.7 37.4 38.5 38.6 29.6 32.0 37.8\n(Xu and Takano 2021) (GT) 35.8 38.1 31.0 35.3 35.8 43.2 37.3 31.7 38.4 45.5 35.4 36.7 36.8 27.9 30.7 35.8\n(Zhao, Wang, and Tian 2022) (GT)32.0 38.0 30.4 34.4 34.7 43.3 35.2 31.4 38.0 46.2 34.2 35.7 36.1 27.4 30.6 35.2\nOurs (GT) 32.9 38.3 28.3 33.8 34.9 38.7 37.2 30.7 34.5 39.7 33.9 34.7 34.3 26.1 28.9 33.8\nTable 1: Quantitative evaluation results using MPJPE in millimeter on Human3.6M . No rigid alignment or transform is applied\nin post-processing. We split this table into 2 groups. The inputs for the top group methods are the detection 2D pose, SH denotes\nthe 2D pose detected by Stacked Hourglass network (Newell, Yang, and Deng 2016), and CPN denotes the cascaded pyramid\nnetwork (Chen et al. 2018). The inputs for the bottom group are ground truth (GT) of 2D pose. Best results are showed in bold.\nMethods Trainning data GS noGS Outdoor ALL (PCK ‚Üë) ALL (AUC ‚Üë)\n(Martinez et al. 2017) H36M 49.8 42.5 31.2 42.5 17.0\n(Mehta et al. 2017) H36M 70.8 62.3 58.8 64.7 31.7\n(Yang et al. 2018) H36M+MPII - - - 69.0 32.0\n(Zhou et al. 2017) H36M+MPII 71.1 64.7 72.7 69.2 32.5\n(Luo, Chu, and Yuille 2020) H36M 71.3 59.4 65.7 65.6 33.2\n(Ci et al. 2019) H36M 74.8 70.8 77.3 74.0 36.7\n(Zhou et al. 2019) H36M+MPII 75.6 71.3 80.3 75.3 38.0\n(Xu and Takano 2021) H36M 81.5 81.7 75.2 80.1 45.8\n(Zhao, Wang, and Tian 2022) H36M 80.1 77.9 74.1 79.0 43.8\nOurs H36M 86.2 84.7 81.9 84.1 53.7\nTable 2: Results on the test set of MPI-INF-3DHP (Mehta et al. 2017) by scene. The results are shown in PCK and AUC.\nLoss Function\nStage I We first train our POT for the first-stage 3D pose\nregressing. The objective function can be formulated as :\nLstageI = 1\nJ\nJX\ni=1\n\u0012\r\r\n\reYi ‚àí Yi\n\r\n\r\n\r\n2\u0013\n, (12)\nwhere eYi and Yi are the estimated first-stage 3D positions\nand the ground truth of i-th joint respectively.\nStage II We aim to predict the uncertainty correctly as\nwell as estimate an accurate refined 3D pose in Stage II. Dur-\ning this stage, we freeze the model parameters of POT and\nonly train the UGRN for stable results. Following (Kendall\nand Gal 2017), we set our uncertainty estimation loss as:\nLœÉ = 1\nJ\nJX\ni=1\nÔ£´\nÔ£≠\n\r\r\r\n\r\r\neYi ‚àí Yi\nœÉi\n\r\n\r\r\n\r\r\n2\n+ log(‚à•œÉi‚à•2)\nÔ£∂\nÔ£∏. (13)\nIn addition, we also apply L2 loss to minimize the errors\nbetween the refined 3D poses and ground truths:\nLrefine = 1\nJ\nJX\ni=1\n\u0012\r\r\n\rÀÜYi ‚àí Yi\n\r\n\r\n\r\n2\u0013\n, (14)\nThe final loss function of Stage II is computed byLstageII =\nLrefine + ŒªLœÉ, where Œª is the trade-off factor. We set Œª to\n0.001 such that the two loss terms are of the same order of\nmagnitude.\nExperiments\nExperimental Setups\nDataset Human3.6M dataset (Ionescu et al. 2013) is\nwidely used in the 3D HPE task which provides 3.6 mil-\nlion indoor RGB images, including 11 subjects actors per-\nforming 15 different actions. For fairness, we follow previ-\nous works (Martinez et al. 2017; Zhao et al. 2019; Xu and\nTakano 2021) and take 5 subjects (S1, S5, S6, S7, S8) for\ntraining and the other 2 subjects (S9, S11) for testing. In our\nwork, We evaluate our proposed method and conduct ab-\nlation study on the Human3.6M dataset. Besides, the MPI-\nINF-3DHP (Mehta et al. 2017) test set provides images in\nthree different scenarios: studio with a green screen (GS),\nstudio without green screen (noGS) and outdoor scene (Out-\ndoor). We also apply our method to it to demonstrate the\ngeneralization capabilities of our proposed method.\nEvaluation metrics For Human3.6M, we follow previous\nworks (Martinez et al. 2017; Zhao et al. 2019) to use the\n1300\nposition embeddings PO-SA MPJPE(mm) #Paramkeypoint group\n! 37.57 0.97M\n! ! 36.69 0.97M\n! ! 36.43 0.98M\n! ! ! 35.59 0.98M\nTable 3: Ablation Study on different pose-oriented design in\nthe pose-oriented transformer.\nMethod MPJPE(mm) #Param\nPOT 35.59 0.79M\nPOT+UGRN 34.72 0.98M\nPOT+UGRN+UG-Sampling 33.82 0.98M\nTable 4: Ablation Study on Uncertainty-Guided Refinement.\nmean per-joint position error (MPJPE) as evaluation metric.\nMPJPE computes the per-joints mean Euclidean distance be-\ntween the predicted 3D joints and the ground truth after the\norigin (pelvis) alignment. For MPI-INF-3DHP, we employ\n3D-PCK and AUC as evaluation metrics.\nImplement details In our experiment, we set the dimen-\nsion of embeddings to 96 and adopts 6 heads for self-\nattention with a dropout rate of 0.25. The MLP ratio of FFN\nis set to 1.5 to reduce the model parameters. We implement\nour method within the PyTorch framework. During the train-\ning stage, we adopt the Adam (Kingma and Ba 2014) op-\ntimizer. For both Stage I and Stage II, the learning rate is\ninitialized to 0.001 and decayed by 0.96 per 4 epochs, and\nwe train each stage for 25 epochs using a mini-batch size of\n256. We initialize weights of the our model using the initial-\nization method described in (Glorot and Bengio 2010). We\nalso adopt Max-norm regularization to avoid overfitting.\nComparison with the State-of-the-Art\nThe performance compared with the state-of-the-art are\nshown in Table 1. In the top group, following the setting\nof previous works (Pavllo et al. 2019; Zhou et al. 2017;\nCai et al. 2019), We use the cascaded pyramid network\n(CPN) (Chen et al. 2018) as 2D pose detector to obtain 2D\njoints for benchmark evaluation. In the bottom group, we\ntake the ground truth (GT) 2D pose as input to predict the\n3D human pose. It can be seen that, our method outperforms\nall other methods with both GT and detected 2D pose as in-\nput, demonstrating the effectiveness of our method.\nGeneralization Ability\nWe further apply our model to MPI-INF-3DHP to test the\ngeneralization abilities. As shown in Table 2, our model\nachieves 84.1 in PCK and 53.7 in AUC while only using\nHuman3.6M dataset for training, which outperforms all the\nprevious SOTA by a large margin. These results verify the\nstrong generalization capability of our method.\nMethod MPJPE(mm) #Param\nPOT+UGRN (MH-SA) 35.22 0.98M\nPOT+UGRN (PO-SA) 35.07 0.98M\nPOT+UGRN (UG-SA) 34.72 0.98M\nTable 5: Ablation study on UG-SA\nL1 L2 C MPJPE(mm) #Param\n4 1 96 37.08 0.33M\n8 2 96 35.20 0.66M\n12 3 96 33.82 0.98M\n16 4 96 34.47 1.31M\n12 3 48 34.20 0.25M\n12 3 96 33.82 0.98M\n12 3 144 34.68 2.20M\nTable 6: Ablations on different parameters of POT and\nUGRN. L1 and L2 are the number of layers of POT encoder\nand UGRN, respectively. C is the embedding dimension.\nMethod MPJPE(mm) #Param\n(Liu et al. 2020) 37.80 4.22M\n(Xu and Takano\n2021) 35.80 3.70M\n(Zou and Tang\n2021) 37.43 1.10M\n(Zhao, Wang, and\nTian 2022) 35.20 0.62M\nOur-S 34.20 0.25M\nOur-L 33.82 0.98M\nTable 7: Comparison on model complexity.\n1 2 3 4 5\nGroup Index of body joints\n0\n10\n20\n30\n40\n50MPJPE(mm)\nPOT w/o Group Position Emebedings \nPOT\nPOT+UGRN+UG-sampling\nFigure 4: Analysis on difficult joints. Our proposed group\nposition embeddings and uncertainty-guided refinement\nmainly benefit the difficult joints in group 4 and 5.\nAblation Study and Discussion\nWe conduct a series of ablation studies to better under-\nstand how each component affects the performance. The 2D\nground truth (GT) is taken as input in the ablation.\nEffect on different pose-oriented design We first diag-\nnose how each pose-oriented design in the POT affects\n1301\nInput\t\n Input\tGraformer GraformerOurs OursGT GT\nFigure 5: Qualitative results on Human3.6M.\nthe performance. In this section, the UGRN is excluded\nand the first stage 3D pose eY is used for evaluated. As\nshown in Table 3, our method achieves the best performance\nwhen all the pose-oriented designs are included. Compared\nwith only using keypoint position embeddings , we achieve\n0.88mm (37.57mm to 36.69mm) improvement by adding the\ndistance-related group position embeddings, proving that the\nrepresentation of difficult joints is effectively facilitated. In\naddition, by replacing the standard self-attention with our\nPO-SA, we also achieve 1.10mm (36.69mm to 35.59mm)\nimprovement with only 0.01M model parameters increase,\nwhich reflects the benefits of enhancing the ability of mod-\neling the topological interactions.\nEffect on uncertainty-guided refinement We then in-\nspect how uncertainty-guided refinement benefits perfor-\nmance. It can be seen from Table 4 that our first-stage\nprediction obtained directly by POT can achieve 35.59\nmm in MPJPE, while adding UGRN for refinement can\nbring 0.83mm (35.59mm to 34.72mm) performance im-\nprovement, and UG-sampling can facilitate the learning pro-\ncedure and further bring 0.9 mm (34.72mm to 33.82mm)\ngains. To demonstrate that the performance improvement\nis not brought by the increased model parameters, we also\ntest other refinement model design using other kinds of self-\nattention, and the results are shown in Table 5. When we\nreplacing UG-RA with standard MH-SA, the performance\ndegrades from 34.72mm to 35.22mm. In addition, when us-\ning the proposed PO-SA in the UGRN, the performance\nalso degrades (34.72mm to 35.07mm), which reflects that\nthe uncertainty-related information is more important than\ndistance-related information in the second refinement stage.\nComparison on different parameters in POT and UGRN\nTable 6 reports how different parameters impact the perfor-\nmance and the complexity of our model. The results show\nthat, enlarging the embedding dimension from 48 to 96 can\nboost the performance, but using dimensions larger than 96\ncannot bring further benefits. In addition, we observe the\nbest performance when using 12 and 3 transformer layers\nin POT encoder and UGRN, respectively, and no more gains\ncan be obtained by stacking more layers. Therefore, we set\nthe basic setting to L1 = 12, L2 = 3, and C = 96.\nComparison on model complexity In Table 7, We com-\npare both the accuracy and the model complexity with other\nbenchmarks on the Human3.6M dataset. We provide two\nconfigurations of our method, in which the embedding di-\nmension of Our-S is 48 while that of Our-L is set to 96.\nResults show that our method can achieve better results with\neven much fewer parameters.\nUnderstanding the performance improvement In Fig-\nure 4, we present the average estimation errors of different\nbody joints according to its group index. It can be seen that,\nboth our group position embedding and UGRN bring more\nperformance improvement for group 4 and 5, in which joints\nare far from the root joint. The results confirm that our ben-\nefit mainly comes form the difficult joints.\nQualitative results Figure 5 demonstrates some quali-\ntative results on the Human3.6M dataset compared with\nGraformer (Zhao, Wang, and Tian 2022). It can be seen that\nour method can make accurate pose prediction, especially\nfor the difficult joints that are far from the root.\nConclusion\nIn this paper, we proposed a two-stage transformer-based\nframework for 3D HPE. First, we introduce targeted im-\nprovements for the basic components of transformers and\nfabricate Pose-Oriented Transformer (POT). Specifically,\nwe design a novel self-attention mechanism in which the\ntopological connections of body joints can be well consid-\nered. We also split body joints into several groups accord-\ning to their distance toward the root joint and provide ad-\nditional learnable distance-related position embedding for\neach group. Then, the second stage Uncertainty-Guided Re-\nfinement Network (UGRN) is introduced to further refine\npose predictions, by considering the estimated uncertainty\nof each joint with uncertainty-guided sampling strategy and\nself-attention mechanism. Extensive results on Human3.6M\nand MPI-INF-3DHP reveal the benefits of our design.\n1302\nAcknowledgments\nThis work was supported in part by the National Natu-\nral Science Foundation of China under Grant 61932022,\nGrant 61931023, Grant 61971285, Grant 61831018, Grant\n61871267, Grant 61720106001, Grant 62120106007, Grant\n61972256, Grant T2122024, Grant 62125109, and in part\nby the Program of Shanghai Science and Technology Inno-\nvation Project under Grant 20511100100.\nReferences\nCai, Y .; Ge, L.; Liu, J.; Cai, J.; Cham, T.-J.; Yuan, J.; and\nThalmann, N. M. 2019. Exploiting Spatial-Temporal Rela-\ntionships for 3D Pose Estimation via Graph Convolutional\nNetworks. In 2019 IEEE/CVF International Conference on\nComputer Vision (ICCV), 2272‚Äì2281.\nChen, Y .; Wang, Z.; Peng, Y .; Zhang, Z.; Yu, G.; and Sun,\nJ. 2018. Cascaded pyramid network for multi-person pose\nestimation. In 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 7103‚Äì7112.\nCi, H.; Wang, C.; Ma, X.; and Wang, Y . 2019. Optimiz-\ning network structure for 3D human pose estimation. In\n2019 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 2262‚Äì2271.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nGal, Y .; and Ghahramani, Z. 2016. Dropout as a bayesian ap-\nproximation: Representing model uncertainty in deep learn-\ning. In international conference on machine learning, 1050‚Äì\n1059. PMLR.\nGlorot, X.; and Bengio, Y . 2010. Understanding the diffi-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the 13th International Conference on Artificial\nIntelligence and Statistics (AISTATS) 2010, 249‚Äì256.\nIonescu, C.; Papava, D.; Olaru, V .; and Sminchisescu, C.\n2013. Human3.6M: Large scale datasets and predictive\nmethods for 3D human sensing in natural environments.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 36(7): 1325‚Äì1339.\nKendall, A.; and Gal, Y . 2017. What uncertainties do we\nneed in bayesian deep learning for computer vision? Ad-\nvances in neural information processing systems, 30.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. In 2nd International Conference on\nLearning Representations.\nLi, H.; Shi, B.; Dai, W.; Chen, Y .; Wang, B.; Sun, Y .; Guo,\nM.; Li, C.; Zou, J.; and Xiong, H. 2021. Hierarchical Graph\nNetworks for 3D Human Pose Estimation. The British Ma-\nchine Vision Conference.\nLin, J.; and Lee, G. H. 2020. HDNet: Human Depth Estima-\ntion for Multi-Person Camera-Space Localization. In Pro-\nceedings of the European Conference on Computer Vision ,\n633‚Äì648.\nLin, K.; Wang, L.; and Liu, Z. 2021. Mesh graphormer. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 12939‚Äì12948.\nLiu, K.; Ding, R.; Zou, Z.; Wang, L.; and Tang, W. 2020.\nA comprehensive study of weight sharing in graph networks\nfor 3D human pose estimation. In Proceedings of the Euro-\npean Conference on Computer Vision, 318‚Äì334.\nLuo, C.; Chu, X.; and Yuille, A. 2020. Orinet: A fully con-\nvolutional network for 3d human pose estimation. In British\nMachine Vision Conference.\nMartinez, J.; Hossain, R.; Romero, J.; and Little, J. J. 2017.\nA simple yet effective baseline for 3D human pose estima-\ntion. In 2017 IEEE International Conference on Computer\nVision (ICCV), 2640‚Äì2649.\nMehta, D.; Rhodin, H.; Casas, D.; Fua, P.; Sotnychenko, O.;\nXu, W.; and Theobalt, C. 2017. Monocular 3d human pose\nestimation in the wild using improved cnn supervision. In\ninternational conference on 3D vision, 506‚Äì516. IEEE.\nMoon, G.; Chang, J. Y .; and Lee, K. M. 2019. Camera\ndistance-aware top-down approach for 3D multi-person pose\nestimation from a single RGB image. In2019 IEEE/CVF In-\nternational Conference on Computer Vision, 10133‚Äì10142.\nNewell, A.; Yang, K.; and Deng, J. 2016. Stacked hourglass\nnetworks for human pose estimation. In Proceedings of the\nEuropean Conference on Computer Vision, 483‚Äì499.\nPavlakos, G.; Zhou, X.; and Daniilidis, K. 2018. Ordi-\nnal depth supervision for 3D human pose estimation. In\n2018 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 7307‚Äì7316.\nPavllo, D.; Feichtenhofer, C.; Grangier, D.; and Auli, M.\n2019. 3D human pose estimation in video with tempo-\nral convolutions and semi-supervised training. In 2019\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 7753‚Äì7762.\nShi, B.; Xu, Y .; Dai, W.; Wang, B.; Zhang, S.; Li, C.; Zou,\nJ.; and Xiong, H. 2020. Tiny-Hourglassnet: An Efficient De-\nsign For 3D Human Pose Estimation. In2020 IEEE Interna-\ntional Conference on Image Processing (ICIP), 1491‚Äì1495.\nSong, T.; Chen, L.; Zheng, W.; and Ji, Q. 2021. Uncer-\ntain graph neural networks for facial action unit detection.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 35, 5993‚Äì6001.\nSun, X.; Shang, J.; Liang, S.; and Wei, Y . 2017. Composi-\ntional human pose regression. In 2017 IEEE International\nConference on Computer Vision (ICCV), 2602‚Äì2611.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, Z.; Li, Y .; Guo, Y .; Fang, L.; and Wang, S.\n2021. Data-uncertainty guided multi-phase learning for\nsemi-supervised object detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 4568‚Äì4577.\nWehrbein, T.; Rudolph, M.; Rosenhahn, B.; and Wandt, B.\n2021. Probabilistic monocular 3d human pose estimation\n1303\nwith normalizing flows. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, 11199‚Äì11208.\nXu, T.; and Takano, W. 2021. Graph Stacked Hourglass Net-\nworks for 3D Human Pose Estimation. In 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 16105‚Äì16114.\nYang, F.; Zhai, Q.; Li, X.; Huang, R.; Luo, A.; Cheng, H.;\nand Fan, D.-P. 2021. Uncertainty-guided transformer rea-\nsoning for camouflaged object detection. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 4146‚Äì4155.\nYang, W.; Ouyang, W.; Wang, X.; Ren, J.; Li, H.; and Wang,\nX. 2018. 3d human pose estimation in the wild by adversar-\nial learning. In 2018 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 5255‚Äì5264.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.; Shen,\nY .; and Liu, T.-Y . 2021. Do transformers really perform\nbadly for graph representation? Advances in Neural Infor-\nmation Processing Systems, 34: 28877‚Äì28888.\nZhang, J.; Tu, Z.; Yang, J.; Chen, Y .; and Yuan, J. 2022.\nMixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D\nHuman Pose Estimation in Video. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 13232‚Äì13242.\nZhao, L.; Peng, X.; Tian, Y .; Kapadia, M.; and Metaxas,\nD. N. 2019. Semantic graph convolutional networks for 3D\nhuman pose regression. In 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 3425‚Äì\n3435.\nZhao, W.; Wang, W.; and Tian, Y . 2022. GraFormer: Graph-\nOriented Transformer for 3D Pose Estimation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 20438‚Äì20447.\nZheng, C.; Zhu, S.; Mendieta, M.; Yang, T.; Chen, C.; and\nDing, Z. 2021. 3d human pose estimation with spatial and\ntemporal transformers. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 11656‚Äì11665.\nZhou, K.; Han, X.; Jiang, N.; Jia, K.; and Lu, J. 2019. Hem-\nlets pose: Learning part-centric heatmap triplets for accurate\n3d human pose estimation. In2017 IEEE International Con-\nference on Computer Vision (ICCV), 2344‚Äì2353.\nZhou, X.; Huang, Q.; Sun, X.; Xue, X.; and Wei, Y . 2017.\nTowards 3D human pose estimation in the wild: A weakly-\nsupervised approach. In 2017 IEEE International Confer-\nence on Computer Vision (ICCV), 398‚Äì407.\nZhu, Y .; Xu, X.; Shen, F.; Ji, Y .; Gao, L.; and Shen, H. T.\n2021. PoseGTAC: Graph Transformer Encoder-Decoder\nwith Atrous Convolution for 3D Human Pose Estimation.\nIn IJCAI, 1359‚Äì1365.\nZou, Z.; Liu, K.; Wang, L.; and Tang, W. 2020. High-order\nGraph Convolutional Networks for 3D Human Pose Estima-\ntion. In British Machine Vision Conference.\nZou, Z.; and Tang, W. 2021. Modulated graph convolutional\nnetwork for 3d human pose estimation. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 11477‚Äì11487.\n1304",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.8033962249755859
    },
    {
      "name": "Pose",
      "score": 0.6974350214004517
    },
    {
      "name": "Computer science",
      "score": 0.6930458545684814
    },
    {
      "name": "Transformer",
      "score": 0.681976854801178
    },
    {
      "name": "Exploit",
      "score": 0.5785830616950989
    },
    {
      "name": "Artificial intelligence",
      "score": 0.496223509311676
    },
    {
      "name": "Topology (electrical circuits)",
      "score": 0.46119025349617004
    },
    {
      "name": "Machine learning",
      "score": 0.36711007356643677
    },
    {
      "name": "Algorithm",
      "score": 0.36598166823387146
    },
    {
      "name": "Mathematics",
      "score": 0.1738671362400055
    },
    {
      "name": "Engineering",
      "score": 0.10599294304847717
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19268510",
      "name": "Qualcomm (United Kingdom)",
      "country": "GB"
    }
  ]
}