{
  "title": "Offline dominance and zeugmatic similarity normings of variably ambiguous words assessed against a neural language model (BERT)",
  "url": "https://openalex.org/W4281616419",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5042166991",
      "name": "Katherine A. DeLong",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A5054191699",
      "name": "Sean Trott",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A5036480250",
      "name": "Marta Kutas",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2036931463",
    "https://openalex.org/W4250570372",
    "https://openalex.org/W6831833583",
    "https://openalex.org/W2572211363",
    "https://openalex.org/W2056763217",
    "https://openalex.org/W6629577645",
    "https://openalex.org/W1951724000",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2081125288",
    "https://openalex.org/W2334841817",
    "https://openalex.org/W2757187782",
    "https://openalex.org/W2112184938",
    "https://openalex.org/W6676970723",
    "https://openalex.org/W1974991592",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W6764724796",
    "https://openalex.org/W6827807658",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2084182646",
    "https://openalex.org/W6671708514",
    "https://openalex.org/W1998652374",
    "https://openalex.org/W1989705633",
    "https://openalex.org/W2070586582",
    "https://openalex.org/W6668008104",
    "https://openalex.org/W6769430610",
    "https://openalex.org/W2167187566",
    "https://openalex.org/W2119728020",
    "https://openalex.org/W1984866353",
    "https://openalex.org/W2019535988",
    "https://openalex.org/W6650698052",
    "https://openalex.org/W2510413766",
    "https://openalex.org/W2963366649",
    "https://openalex.org/W3212817950",
    "https://openalex.org/W4226029669",
    "https://openalex.org/W2064600544",
    "https://openalex.org/W2053753663",
    "https://openalex.org/W2795342569",
    "https://openalex.org/W2154858926",
    "https://openalex.org/W6682899146",
    "https://openalex.org/W3118471049",
    "https://openalex.org/W2142625445",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2071574108",
    "https://openalex.org/W1989804431",
    "https://openalex.org/W2099584159",
    "https://openalex.org/W6674872719",
    "https://openalex.org/W2021286869",
    "https://openalex.org/W2042689703",
    "https://openalex.org/W2028647943",
    "https://openalex.org/W1975593115",
    "https://openalex.org/W2155499572",
    "https://openalex.org/W2100313834",
    "https://openalex.org/W266716723",
    "https://openalex.org/W6997569583",
    "https://openalex.org/W1983578042",
    "https://openalex.org/W2622006541",
    "https://openalex.org/W3156782505",
    "https://openalex.org/W3081304984",
    "https://openalex.org/W2808073060",
    "https://openalex.org/W2772696116",
    "https://openalex.org/W2073257493",
    "https://openalex.org/W2078894097",
    "https://openalex.org/W3102485638",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2086863345",
    "https://openalex.org/W2950226244",
    "https://openalex.org/W6763716646",
    "https://openalex.org/W2040187703",
    "https://openalex.org/W6660559553",
    "https://openalex.org/W3100700939",
    "https://openalex.org/W2012156767",
    "https://openalex.org/W6653614733",
    "https://openalex.org/W2040101315",
    "https://openalex.org/W6660496441",
    "https://openalex.org/W2296757461",
    "https://openalex.org/W85089493",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W4255203228",
    "https://openalex.org/W6634489030",
    "https://openalex.org/W2582743722",
    "https://openalex.org/W2153501447",
    "https://openalex.org/W1964424952",
    "https://openalex.org/W2035069072",
    "https://openalex.org/W2465093504",
    "https://openalex.org/W6828807935",
    "https://openalex.org/W2046121397",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W6604617241",
    "https://openalex.org/W1979296634",
    "https://openalex.org/W1997381876",
    "https://openalex.org/W2064730881",
    "https://openalex.org/W2130414943",
    "https://openalex.org/W2096672526",
    "https://openalex.org/W2046330369",
    "https://openalex.org/W2072235409",
    "https://openalex.org/W2998443519",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W6762537594",
    "https://openalex.org/W3165845326",
    "https://openalex.org/W3177005832",
    "https://openalex.org/W2080149055",
    "https://openalex.org/W6670618154",
    "https://openalex.org/W2072405529",
    "https://openalex.org/W2806716363",
    "https://openalex.org/W6752424950",
    "https://openalex.org/W2015982820",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3081649546",
    "https://openalex.org/W2604272474",
    "https://openalex.org/W2001949459",
    "https://openalex.org/W392761142"
  ],
  "abstract": "Abstract For any research program examining how ambiguous words are processed in broader linguistic contexts, a first step is to establish factors relating to the frequency balance or dominance of those words’ multiple meanings, as well as the similarity of those meanings to one other. Homonyms—words with divergent meanings—are one ambiguous word type commonly utilized in psycholinguistic research. In contrast, although polysemes—words with multiple related senses—are far more common in English, they have been less frequently used as tools for understanding one-to-many word-to-meaning mappings. The current paper details two norming studies of a relatively large number of ambiguous English words. In the first, offline dominance norming is detailed for 547 homonyms and polysemes via a free association task suitable for words across the ambiguity continuum, with a goal of identifying words with more equibiased meanings. The second norming assesses offline meaning similarity for a partial subset of 318 ambiguous words (including homonyms, unambiguous words, and polysemes divided into regular and irregular types) using a novel, continuous rating method reliant on the linguistic phenomenon of zeugma. In addition, we conduct computational analyses on the human similarity norming data using the BERT pretrained neural language model (Devlin et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. ArXiv Preprint. arXiv:1810.04805) to evaluate factors that may explain variance beyond that accounted for by dictionary-criteria ambiguity categories. Finally, we make available the summarized item dominance values and similarity ratings in resultant appendices (see supplementary material), as well as individual item and participant norming data, which can be accessed online ( https://osf.io/g7fmv/ ).",
  "full_text": "Vol.:(0123456789)1 3\nBehavior Research Methods (2023) 55:1537–1557 \nhttps://doi.org/10.3758/s13428-022-01869-6\nOffline dominance and zeugmatic similarity normings of variably \nambiguous words assessed against a neural language model (BERT)\nKatherine A. DeLong1  · Sean Trott1 · Marta Kutas1,2,3,4\nAccepted: 29 April 2022 / Published online: 10 June 2022 \n© The Author(s) 2022\nAbstract\nFor any research program examining how ambiguous words are processed in broader linguistic contexts, a first step is to establish factors \nrelating to the frequency balance or dominance of those words’ multiple meanings, as well as the similarity of those meanings to one other. \nHomonyms—words with divergent meanings—are one ambiguous word type commonly utilized in psycholinguistic research. In contrast, \nalthough polysemes—words with multiple related senses—are far more common in English, they have been less frequently used as tools \nfor understanding one-to-many word-to-meaning mappings. The current paper details two norming studies of a relatively large number \nof ambiguous English words. In the first, offline dominance norming is detailed for 547 homonyms and polysemes via a free association \ntask suitable for words across the ambiguity continuum, with a goal of identifying words with more equibiased meanings. The second \nnorming assesses offline meaning similarity for a partial subset of 318 ambiguous words (including homonyms, unambiguous words, \nand polysemes divided into regular and irregular types) using a novel, continuous rating method reliant on the linguistic phenomenon \nof zeugma. In addition, we conduct computational analyses on the human similarity norming data using the BERT pretrained neural \nlanguage model (Devlin et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. ArXiv Preprint. \narXiv:1810.04805) to evaluate factors that may explain variance beyond that accounted for by dictionary-criteria ambiguity categories. \nFinally, we make available the summarized item dominance values and similarity ratings in resultant appendices (see supplementary \nmaterial), as well as individual item and participant norming data, which can be accessed online (https:// osf. io/ g7fmv/).\nKeywords Semantic ambiguity · Homonyms · Polysemes · Dominance norming · Similarity rating · Zeugma\nOne-to-many word-to-meaning mappings are ubiquitous, with \nthe majority of words in English having multiple meanings or \nsenses (Eddington & Tokowicz, 2015; Rodd, 2018). In the-\nory, subsequent exponential ambiguity of phrases, sentences, \ndiscourses, or any larger chunk of language would seem to \nmake comprehension challenging if not impossible; in reality, \nhumans negotiate such ambiguities with relative ease. This \nis due to the roles of intra- and extralinguistic contexts in the \nactivation of different word meanings or senses. However, the \nnature of on-the-fly interactions of context with word repre-\nsentations in semantic memory, the kinds of and how many \nambiguous word meanings or senses are activated in the brain, \nwith what specificity, to what degree, and when or for how \nlong, are questions with complex answers, some yet unsolved.\nDecades of behavioral and eye tracking studies investi-\ngating word recognition and meaning access have employed \nambiguous word paradigms, which have identified the pri-\nmary influences on isolated ambiguous word interpreta-\ntion as (1) the relative frequency or dominance1 of a word’s \nmeanings or senses, and (2) the similarity of a word’s mul-\ntiple meanings or senses to each other. Much of this work is \ngrounded in a “mental lexicon” framework—the idea that the \n * Katherine A. DeLong \n kadelong@ucsd.edu\n Sean Trott \n sttrott@ucsd.edu\n Marta Kutas \n mkutas@ucsd.edu\n1 Department of Cognitive Science, University of California, \nSan Diego (UCSD), 9500 Gilman Drive, La Jolla, \nCA 92093-0515, USA\n2 UCSD Center for Research in Language, La Jolla, CA, USA\n3 UCSD Department of Neurosciences, La Jolla, CA, USA\n4 UCSD Kavli Institute for Brain and Mind, La Jolla, CA, USA\n1 The literature uses various terms, including balanced/unbalanced, \nbiased/unbiased, meaning frequency, polarization, dominance, and \nrelative frequency to refer to roughly similar ideas: the relative usage \nproportion of a word’s multiple meanings or senses to each other.\n1538 Behavior Research Methods (2023) 55:1537–1557\n1 3\nbrain stores word information in neural dictionary-like entries \nfor which meanings, pronunciations, and syntactic properties \n(among other things) can be looked up. Early results from \nambiguous word studies were used to support theories of \nmodularity (e.g., Swinney, 1979) but also interactivity (Kel-\nlas & Vu, 1999; McClelland & Rumelhart, 1981; Morton, \n1969; Vu & Kellas, 1999). However, early eye tracking data \n(e.g., subordinate bias effects; Duffy et al., 1988; Pacht & \nRayner, 1993; Rayner & Duffy, 1986) did not accord with a \nstrong version of either view and led to hybrid models like \nthe popular reordered access account. This model posits that \nexcept in highly constraining contexts, word meanings are \ninitially exhaustively—not selectively—accessed, but with \nfrequent and/or contextually congruent meanings selected \nvery quickly postlexically (Binder & Rayner, 1998; Rayner \net al., 1994; Swinney, 1981). This model had considerable \nsupport across a variety of paradigms (e.g., Kambe et al., \n2001; Reichle et al., 2007; Sereno, 1995; Sereno et al., 2003; \nSheridan et al., 2009; Swaab et al., 2003). Outside of larger \nlinguistic contexts, however, dominant meanings were found \nto prevail (e.g., Duffy et al., 1988).\nUntil more recently, homonyms (e.g., batter, mint, port, \ncolon) were the primary type of ambiguous word employed \nin psycholinguistic studies of word representations, online \nmeaning activation, and language processing. Homonyms \nare typically operationalized by their multiple semantically \ndistinctive meanings of different origins, listed under sepa-\nrate dictionary entries. It is only by chance that homonyms’ \ndifferent meanings share the same orthographic and pho-\nnological forms. Along the dimension of meaning domi-\nnance, homonyms can have relatively equibiased meaning \nfrequencies (e.g., calf [cow/leg], colon [punctuation/organ], \npitcher [ container/baseball]), or they can be biased, with \none obviously dominant meaning (e.g., ball [throw/dance], \nnag [pester/horse], or pen [writing instrument/enclosure]).\nHomonyms, however, represent just one type of ambig-\nuous word, and the aforementioned work only rarely \nemployed another type—polysemes—even though they out-\nnumber homonyms in English by far (84% to 7%, according \nto Rodd et al., 2002). Polysemes (e.g., cotton, church, slate, \nspeaker) are word forms with multiple related senses deriv-\ning from a common origin that are often operationalized as \nhaving senses listed under a single dictionary entry. Poly -\nsemous senses vary in the nature of their semantic relations \n(e.g., bottle [container/contents], chicken [animal/meat], for-\ntune [money/luck], or straw [drinking/plant fiber]). Despite \nbeing related, they may have semantic features that are \nquite different (Klein & Murphy, 2001); for instance, when \ncomparing freshly planted cotton and blue striped cotton, \nalthough it is clear that both senses of cotton have something \nin common, they vary along dimensions of living/non-living, \nnatural/manmade, color, texture, function. These character-\nistics make polysemes particularly interesting test cases for \nlanguage processing/ambiguity experiments, although they \nhave traditionally been less utilized experimentally than \nhomonyms—possibly due to known difficulties in distin -\nguishing polysemy from related phenomena like vagueness \n(Geeraerts, 1993; Tuggy, 1993).\nExperimental tests of polysemes have begun to attract \nmore attention in recent years, and with their greater seman-\ntic similarity and feature overlap, it is, for instance, unclear \nhow well models like reordered access apply to polyseme \nprocessing. In fact, several studies have failed to observe \ndominance/frequency effects for polysemes (Frazier & \nRayner, 1990; Frisson & Pickering, 1999). Current theo-\nretical debates on polyseme meaning retrieval center on \nwhether polysemes are stored and retrieved with separate \nrepresentations for each of their multiple (related) senses, \nsimilar to how homonyms have traditionally been proposed \nto be represented; or, whether they are stored and processed \nqualitatively differently than homonyms, with overlapping \nmultiple sense representations (for overviews, see Brocher \net al., 2018; Eddington & Tokowicz, 2015).\nIn order to study how ambiguous words—both homonyms \nand polysemes—are processed in wider contexts, whether \nonline or offline, it is essential to establish the nature of \ntheir multiple meanings and the meanings’ relations to each \nother. Identifying these factors also serves a second-order \ngoal: a better understanding of how human comprehend-\ners represent and process the meanings of ambiguous words \nwould aid natural language processing (NLP) practitioners \nin developing more humanlike models of word meaning, \nparticularly for tasks that explicitly involve adjudicating \nbetween distinct meanings of an ambiguous word, such as \nword sense disambiguation (Haber & Poesio, 2020; Loureiro \net al., 2020; Trott & Bergen, 2021).\nWith the bulk of work on ambiguous word comprehension \nhaving centered on homonyms, there are far fewer published \nresources for assessing polyseme sense frequencies and/or \nsimilarities (but see Foraker & Murphy, 2012; Klepousniotou \n& Baum, 2007). In contrast to homonymous words that have \nmultiple unrelated meanings, discerning different but related \nsenses of polysemes (and in turn their individual frequen-\ncies) is less straightforward (Tuggy, 1993). This is perhaps \ncomplicated further by the possibility that there are differ -\nent types of polysemy, as described in theoretical linguistics \nand experimental work (e.g., Apresjan, 1974; Brocher et al., \n2016; Moldovan, 2021; Rabagliati & Snedeker, 2013). For \ninstance, Apresjan (1974) classifies polysemous words as \nmetonymic (regular) or metaphoric (irregular). Frisson and \nPickering (1999) describe metonymic constructions as ones \nin which “one salient aspect of an entity is used to refer to \nthe entity as a whole or to some other part of the entity.” The \nrelationships between the meanings of metonymic polyse-\nmes are often highly systematic across the lexicon and even \nacross languages (Apresjan, 1974; Lakoff, 1987; Srinivasan \n1539Behavior Research Methods (2023) 55:1537–1557 \n1 3\n& Rabagliati, 2015), and typically reflect regular functional \npatterns; for instance, container versus its contents (bottle, \nbowl); animal versus its meat (chicken, fish); organization \nversus publication-produced-by-organization (newspaper, \nmagazine); metal/gem versus color-typical-of-a-metal/gem \n(gold, copper, topaz, emerald). Metaphorical polysemes, on \nthe other hand, are ones with analogical relations, where one \nsense is literal and another figurative, and the relations are \nunpredictable and not necessarily obvious or easy to identify; \nfor instance, leg (belonging to an animal or table), pig (an \nanimal or sloppy person), or star (a celestial body or famed \nindividual). However, as Klepousniotou and Baum (2007) \ndescribe, there is neither a sharp line distinguishing homo -\nnyms from polysemes, nor is there one that clearly delineates \nmetaphoric from metonymic polysemes.\nTake, for example, a word like panel—a polyseme, accord-\ning to the single dictionary entry criterion (Wordsmyth online \ndictionary; https:// www. words myth. net). Multiple senses of \nthis word include: (1) a section, as of a door, wall, or the like, \nusually flat and often set apart from the surrounding area \nby being raised, recessed, or decorated, and (2) a group of \npersons assembled for a particular purpose, as to investi-\ngate or evaluate something. These senses seem distinct but \nhave a mutual historical derivation; in other words, it is not \ncoincidental that they share a single phonological word form \n(which would lead to their being categorized as homonyms). \nIn addition, both senses seem literal, at least in current, eve-\nryday usage, which might endorse the classification as a more \nmetonymic relation; however, there is also no regular, produc-\ntive relation here, which undermines their classification as \nmetonymic. Without access to panel’s etymology, it is not \nclear that contemporary English language users would intuit \nwhich sense was derived from which. In fact, metaphorical \nshifts are not always apparent to language users (Apresjan, \n1974; Yurchenko et al., 2020). Although these two senses of \npanel index very different sets of information, the dictionary \nspecifies—and perhaps the average language user consciously \nor unconsciously grasps—that these senses are not completely \ndistinctive (i.e., are not homonyms), but rather have some \nsemantic overlap or shared origin. What is unknown, and the \noutstanding question that lies at the heart of our larger experi-\nmental research program, is how the brain stores, organizes, \nand accesses information about such ambiguous words during \nreal time comprehension. Because dictionaries do not reflect \nthe psychological realities of language users in natural envi-\nronments, and neural organization is unlikely to be character-\nized by dictionary-like entries cataloged along etymological \nlines, alternative methods are required to assess the range of \nmeaning frequencies and similarities.\nIf we consider that these different ambiguous word types \nmay lie along a continuum of meaning or sense similar -\nity (e.g., Eddington & Tokowicz, 2015 ; Klepousniotou, \n2002; Tuggy, 1993), homonyms would represent the most \nambiguous end with their multiple unrelated meanings, and \nunambiguous words would anchor the opposite end, with \nwords operationalized by their single-meaning dictionary \nentries (e.g., monk, midnight, icicle, easel, wig). In terms of \ntheir more related senses, the different classes of polysemes \nwould lie somewhere between these two extremes. Impor -\ntantly, differences in the type or degree of word ambiguity \nmight correspond to differences in how words are processed \nonline and stored/organized in the brain (e.g., Klepousniotou \net al., 2008; Klepousniotou et al., 2012). Thus, accounting \nfor these offline lexical factors becomes a crucial first step.\nWith these considerations in mind, we set out to conduct \ntwo norming studies to generate, norm, and select a set of \nwords with variable degrees of ambiguity that could eventu-\nally be used to test the proposal that words function as ‘cues \nto meaning’ (Elman, 2004). Instead of static entries in a \ndictionary that must be looked up, Elman’s framework con-\nceptualizes words as inputs to a dynamic, context-sensitive \nnetwork, such that meaning is equivalent to the trajectory \nthrough this network’s state-space elicited by a particular \nwordform, in a particular context. We sought words that \nspanned the ambiguity continuum but that also had rela-\ntively balanced meaning/sense frequencies. Although the \nprominent role of meaning dominance and its effects on \nambiguous word interpretation (particularly for homonyms) \nhave been well documented over the years, less is understood \nabout the nature of dominance for polysemous senses of \nwords. Given our interest in exploring questions about words \nthat span a continuum of ambiguity, combined with our lab-\noratory’s research focus on the role of (sentence) context in \nstimulus interpretation and meaning construction, we aimed \nto generate a list of ambiguous words for which differences \nin dominance values—as a potential source of variability—\nwere minimized, while the meanings or senses of individual \nwords spanned a wide range semantic similarity.\nStudy 1 is a meaning/sense dominance norming study \ndesigned to assess isolated (i.e., outside explicitly disambig-\nuating or informative sentential context) ambiguous words’ \nmeaning/sense dominance using a method tried and proven \nwith homonyms. This involved classifying associates provided \nby participants for individual ambiguous words based on the \nword meanings or senses to which they are related. Study 2 is \na similarity norming designed to quantify the relatedness of \nwords’ meanings/senses using a novel rating paradigm that \nrelies on the linguistic phenomenon of zeugma. These methods \nallow flexibility for assessing meaning distance/similarity over \na range of ambiguous word types, but in particular they offer \nsensitivity to the nuanced sense distinctions of polysemes.\nFinally, as noted earlier, these similarity ratings serve a useful \nsecond-order function in helping to evaluate computational mod-\nels of word meaning. Although there are many lexical resources \ncontaining similarity judgments of different words in isolation \n(e.g., dolphin vs. shark; Bruni et al., 2014; Gerz et al., 2016; \n1540 Behavior Research Methods (2023) 55:1537–1557\n1 3\nHalawi et al., 2012; Hill et al., 2015; Taieb et al., 2020), there are \ncomparatively fewer that contain graded relatedness judgments \nfor different meanings of the same ambiguous word, with inflec-\ntion and part-of-speech controlled (Haber & Poesio, 2020; Trott \n& Bergen, 2021). Thus, in Experiment 2, we compare the human \nsimilarity ratings to two measures obtained from BERT (Devlin \net al., 2018), a large pretrained neural language model.\nStudy 1: Ambiguous word meaning/sense \ndominance norming\nOur goal in this first of two norming studies was to quantify \nmeaning or sense frequency for a large number of ambigu-\nous words spanning homonyms and polysemes, from which a \nsubset of meaning/sense-balanced items could be selected for \nsimilarity norming and, for our purposes, eventual sentence \nembedding in word-by-word reading experiments. One method \nthat has been used to establish meaning dominance of ambigu-\nous words is to ask participants to rate the relative familiarity \nor frequency of the meanings or senses of ambiguous words \non a continuous scale (e.g., Klepousniotou et al., 2012) or by \nestimating relative percentages of homonym meanings (e.g., \nArmstrong et al., 2012). However, we thought that estimating \nfrequency of meaning/sense usage might be disproportionately \ndifficult for the polysemes, with their more subtle semantic \nsense distinctions. Instead, we considered another commonly \nused method, which begins by collecting and analyzing the \nfree associates generated to ambiguous words from new or \nexisting norms (e.g., Mirman et al., 2010; Nelson et al., 1980; \nTwilley et al., 1994). A separate group of raters then classifies \nthe responses based on their associated word meanings. While \nthis method has been used frequently to assess homonym dom-\ninance, we determined that the same general approach would \nbe well suited for a broad range of ambiguous word types, \nincluding polysemes. This method, moreover, has been shown \nto generate fairly consistent measures of dominance across a \nvariety of studies (see Twilley et al., 1994).\nMethod\nParticipants\nNinety-seven native English-speaking UCSD undergradu-\nates (80 female, 17 male) 2 between the ages of 18 and 24 \nyears took part in an online dominance norming study for \ncourse credit. Participants were recruited from the UCSD \nPsychology Department subject pool’s online scheduling \nsystem. Informed consent was obtained from all participants \nafter the norming procedure had been outlined for them.\nMaterials\nFor our initial selection of ambiguous words to norm, we \nrelied on published resources (in particular for homonyms) \nto seed our list (e.g., Armstrong et al., 2012; Gorfein et al., \n1982; Nelson et al., 1980; Twilley et al., 1994). Since acquir-\ning relatively meaning/sense-balanced words was a goal, \nwe focused on studies that had either published dominance \nmeasures or had included words labeled as having equibi-\nased or balanced meanings. Armstrong et al. (2012) report \nthat most homonyms have biased meaning frequencies, with \nthe relative dominance of the most frequent meaning often \nexceeding 75% (a common cut-off value for classifying rela-\ntively balanced versus unbalanced homonyms). Accordingly, \nwe used the 75% criterion to guide our selection of more \nbalanced ambiguous words, in particular for homonyms, for \nwhich more resources were readily available.\nFor polysemes, we identified candidate words by con-\nsulting the more limited experimental literature in which \npolysemous words were specifically examined or discussed \n(e.g., Durkin & Manning, 1989; Foraker & Murphy, 2012; \nKlein & Murphy, 2001; Klepousniotou & Baum, 2005, \n2007). Given the scarcity of reported polyseme dominance \nratings, we supplemented our candidate word list with items \ntaken from theoretical linguistics work on polysemes (e.g., \nApresjan, 1974; Nunberg, 1979 ), as well as by expanding \nfrom known metonymic (regular) polysemic relations (e.g., \nsubstance/color, container/contents, building/organization). \nWe also mined resources describing or testing “ambiguous \nwords”, which did not specifically differentiate between \nhomonyms and polysemes (e.g., Duffy et al., 1988; Gilhooly \n& Logie, 1980b).\nA total of 558 ambiguous words (Appendix 1, Column \nA in supplementary material) were selected for dominance \nnorming. Utilizing the online Wordsmyth dictionary (https:// \nwww. words myth. net), a resource often employed for semantic \nambiguity research (e.g., Armstrong & Plaut, 2011; Armstrong \net al., 2012; Rodd et al., 2002), we began by labeling these \nambiguous words as either homonyms or polysemes based \non whether there were separate dictionary entries for multiple \nmeanings (homonyms) or if all senses were listed under a \nsingle entry (see Appendix 1, Column B in supplementary \nmaterial). At this point in the experiment, these category \nlabels merely served as reference points, as one of the \ngoals for the current study was to characterize ambiguous \nwords’ most frequent meanings based on human data rather \nthan dictionary criteria that do not necessarily reflect the \nfamiliarities of words’ multiple meanings/senses. In addition, \n2 Because of the gender imbalance of participants in both norming \nstudies, we conducted an analysis outlined in Supplementary Analy -\nsis 1 that demonstrates negligible differences between the responses \nof female and male participants.\n1541Behavior Research Methods (2023) 55:1537–1557 \n1 3\nbecause homonyms can and often do have multiple senses \nlisted under at least one of their distinct meanings, (e.g., bridge \ncan have homonymous meanings of ‘a structure that extends’ \nand ‘a card game’, but under the first meaning can also have \npolysemous senses relating to, for instance, ‘artificial teeth’ or \n‘part of a human nose’), we also indicated this information in \nColumn B of Appendix 1 in supplementary material. In fact, \nof our dominance-normed items, all but 8 (BOWLER, CAPE, \nFORD, MUSTANG, NOODLE, NOVEL, POKER, PUPIL) \nof the dictionary-criteria homonyms also had multiple senses \nlisted under at least one of their Wordsmyth-listed meanings. It \nis interesting to note that to our knowledge there has been little \ndiscussion in the experimental literature about this “polysemy \nof homonyms”. Typically, when homonyms and polysemes \nhave been contrasted experimentally, the homonymous \nmeanings are taken to be the superseding ones. As we describe \nin the following section, our procedure for determining \nambiguous words’ most frequent meanings was dictated by \nparticipants’ responses, and when relevant, polysemous senses \nof dictionary-criteria homonyms were considered.\nNorming procedure\nWe conducted a web-based dominance norming to assess \nambiguous words’ meaning frequencies by presenting puta-\ntive homonyms and polysemes without any surrounding \n(preceding or following) context. Individuals were instructed \nto “type the first word/phrase that comes to mind, and then \ntype a second word/phrase that comes to mind.” Words \nwere presented in uppercase font to avoid biasing against \nmeanings or senses that when capitalized could constitute \na proper noun or acronym. Participants were informed to \ngo with their initial instincts and to answer as quickly as \npossible, without repeating words in their first and second \nresponses.\nFor example, an ambiguous word stimulus with possible \nparticipant-generated word associates might be:\nBEAR: ___grizzly___\nBEAR: ___teddy bear___\nOther potential responses might be: polar, stuffed, endure, \nput up with, brunt, burden, witness, weight, brown, furry, \nfierce, Pooh, honey, tree, claw, etc. Participants were not \ninformed that the words they would be encountering were \nambiguous, nor were they instructed to provide norming \nresponses that did or did not draw from separate meanings \nor senses of the words. Thus, an individual might provide \ntwo responses that fell under a single meaning or word sense \n(e.g., grizzly or polar) or multiple meanings (e.g., furry, wit-\nness). Participants were instructed to type the word “blank” \nif they were unable to provide an answer.\nEach norming participant was presented with one of two \nlists that included 319 ambiguous words comprised of a mix \nof homonyms and polysemes, with a subset of items included \nin both stimulus lists. Stimuli were presented in unique, ran-\ndomized orderings for each participant. Participants gener -\nally complied in providing two norming responses for the \nmajority of words presented to them. Aiming for a sample \nsize roughly consistent with or greater than those from pre-\nvious dominance normings (e.g., Gilhooly & Logie, 1980a; \nGorfein et al., 1982; Nelson et al., 1980), each ambiguous \nword was normed by between 47 to 95 participants, with an \naverage of 106 responses collected per word (range: 93–189, \nSD = 33.4).3 The individual participant and item dominance \nnorming responses from Study 1 may be accessed at https:// \nosf. io/ g7fmv/.\nCalculating meaning/sense dominance\nDetermining the most frequent norming responses Fol -\nlowing data collection, obvious spelling errors and typos \nin the participant responses were corrected. As a first step \ntoward determining the relative frequencies of the ambigu-\nous words’ different meanings/senses, we began by generat-\ning a list of candidate definitions based on the most frequent \nnorming associates provided. Since each normed word had \na large number of open-ended, highly variable, sometimes \nmultiword responses, a simple alphabetical sorting would \n(a) not have grouped responses like game and card game  \ntogether, and (b) would have required time consuming man-\nual review to determine the most common responses. To \nfacilitate sorting, we adopted the novel (to our knowledge) \napproach of utilizing an online word cloud visualization tool. \nGenerally, word clouds are used to visualize input from text \ndocuments, with their outputs displaying the most frequent \nof those words, with font sizes proportional to the individual \nwords’ relevance or frequency, sometimes clustered semanti-\ncally. The free online semantics-preserving word cloud visu-\nalization application we used was “Semantic Cloud” (http:// \nwordc loud. cs. arizo na. edu/). Using this tool entailed pasting \nthe complete sample of norming responses for each word \ninto the application to generate the visual output. From the \noutput, we were able to rapidly identify the most frequent \nword associates provided as norming responses for each \nambiguous word based on font size. The word cloud tool \n3 During the time that our paper was submitted to the current jour -\nnal and under review, Gilbert and Rodd (2022) published a discus-\nsion of how the number of observations needed to estimate the “true \ndominance” for an ambiguous word meaning is greater for words \nwith more equibiased meanings (like the current materials) compared \nto words with more polarized (dominant and subordinate) meanings. \nOur sample size is in the recommended range. When N = 100 (a \nnumber approximating the average number of responses per item in \nour Study 1), true dominance of 0.5 is associated with a margin of \nerror of ~10%.\n1542 Behavior Research Methods (2023) 55:1537–1557\n1 3\nalso offers an option for visually sorting the input words \nbased on their frequency rank, which expedites the process \nof tallying and determining the most frequent responses. For \ninstance, for the 94 norming responses provided by partici-\npants for the ambiguous word APPEAL, which were then \npasted into the word cloud tool, we were quickly able to \nsee that court, interest, attractive, nice, and overturn were \nthe most commonly provided responses. We note that in the \nmapping of norming responses to word cloud output, multi-\nword entries are not preserved. For our purposes—using the \ntool to easily identify the most frequent associates as a step \ntoward determining a set of candidate meanings—this level \nof imprecision was unproblematic. For instance, if several \nnorming respondents provided Golden Gate as a response \nto the prompt BRIDGE, the first author was able to ascer -\ntain from the identical frequencies of golden and gate  in \nthe word cloud output that these words were likely part of \na compound phrase, at which point the norming responses \ncould be referred to for hand checking.\nDetermining the most frequent meanings or senses Once \nthe most frequent norming responses for each ambiguous \nword were determined (the number ranged from 2 to 8, \nmean = 5.5, SD = 0.9), the next step was to use them to \nascertain the most frequent associated meanings or senses. \nUtilizing multiple resources, including the online Collins \nCOBUILD dictionary (https:// www. colli nsdic tiona ry. com/ \ndicti onary/ engli sh), Google’s English dictionary provided \nby Oxford Languages, and the Wordsmyth online diction-\nary, we selected 2–5 easy-to-understand, succinctly worded \ndefinitions or senses under which the most frequent nor -\nming associates fell (mean number of definitions = 2.30, \nSD = 0.55).4 Typically, the definitions chosen were the top \ndictionary entries for those words, over multiple entries for \nhomonyms or within a single entry for polysemes. Although \nthis process was sufficient for identifying the candidate defi-\nnitions or senses for most of the norming items, sometimes \nthe frequent norming responses alluded to alternative mean-\nings not listed in the dictionary (e.g., AMAZON  as in the \ncompany, BOLT as in the name of a dog from the Disney \nmovie of the same name, or HOOD as in a shortened, slang \nversion of the word neighborhood.)\nDefinition/sense rating and dominance score calcula‑\ntion The next step was to categorize every participant nor -\nming response for each word by its meaning or sense. Two \nraters (the first author, Rater 1, and a fellow lab researcher, \nRater 2) worked individually, deciding whether each norming \nresponse corresponded either with (a) one of the 2–5 selected \ndefinitions, or (b) with multiple, undifferentiable, or none of \nthe 2–5 definitions. After the rating process was completed, \nthe total number of ratings per item aligning with any single \ndefinition were tallied, along with the total number of rated \nresponses corresponding with the 2–5 selected senses/defini-\ntions, and the percentages were calculated. If the two raters \nassigned different meanings to a norming response, each rat-\ning was factored into the count for the respective meanings. \nThe two definitions/senses with the highest percentages were \nidentified and a dominance score (per Armstrong et al., 2012) \nwas calculated by dividing the percentage difference by the \nhighest definition/sense percentage.\nThe resultant dominance scores for individual items could \nrange from 0.00 to 1.00, with a score of 0.00 representing \nan ambiguous word with its most frequent senses perfectly \nbalanced, and a score of 1.00 indicating that one sense was \ncompletely dominant over the other. See Appendix 1, Column I \nin supplementary material.\nInterrater reliability\nInterrater reliability was calculated for the assignment of \nthe norming responses to different meanings or senses \nof ambiguous words. Of the total number of norming \nresponses, 75.7% were assigned meanings by both raters, \nwith the remaining 24.3% assigned meanings by a single \nrater. Of the items assigned to meanings by both raters, there \nwas 94.9% interrater consistency. These results indicate that \nthe meaning/sense classification was overall highly reliable.\nSample Item\nTo demonstrate the entire process for arriving at an ambigu-\nous word’s dominance score, we here outline the steps taken \nfor an individual item: Example: APPEAL.\n1. APPEAL was initially classified as a polyseme accord-\ning to the criteria of there being multiple senses that fell \nunder a single dictionary entry.\n2. Word associates were obtained through norming from \n47 participants, who provided two responses each.\n3. Ninety-four norming responses were pasted into the \nword cloud software and court, interest, attractive, and \nnice were revealed to be the most frequently provided \nassociates.\n4. Based on the top associates provided in the norming, two \ndictionary senses were identified as encompassing the \nmost frequent responses: APPEAL SENSE (1) the power \nto attract, please, stimulate, or interest, and APPEAL  \nSENSE (2) judicial review by a superior court of the \ndecision of a lower tribunal.\n4 Of the 558 dominance normed words, 410 items had responses that \naligned with only two definitions/senses, 129 items aligned with three \nmeanings, 16 items with four meanings, and three items with five \nmeanings.\n1543Behavior Research Methods (2023) 55:1537–1557 \n1 3\n5. The two raters then determined under which diction-\nary sense each norming response fell, with 63% of the \nresponses aligning with SENSE 1 and 37% with SENSE \n2. The two raters’ categorizations were highly consistent.\n6. A dominance score was then calculated by dividing the \npercentage difference of the two most frequent senses \nby the highest sense percentage ([0.631 − 0.369]/0.631 \n= 0.415).\nAnalysis and results\nData rejections\nOf the 558 dominance-normed ambiguous words, there were \n11 items that we did not rate and for which dominance scores \nwere not calculated (see asterisked items in Column A of \nAppendix 1 in supplementary material), leaving 547 remain-\ning ambiguous words. The reasons for rejecting these 11 items \nvaried. For some words, a majority of the norming responses \ncould be associated with multiple meanings or senses, and \nthus it was impossible to assign them to a single definition. \nIn other cases, all associates aligned with a single definition. \nIn two cases there were problems with the words supplied \nin the norming: CHANEL was read as CHANNEL by some \nparticipants, who therefore provided associates for the wrong \nword; ROSES (not an ambiguous word) was inadvertently \nincluded by experimenters instead of ROSE (a homonym).\nDominance values\nAlthough there were numerous items for which the range of \nnorming responses aligned with more than two meanings or \nsenses (our initial experimenter tallies allowed for up to 5), \nover the set of 547 ambiguous words, the mean percentage \nof responses aligning with the two most frequently related \nmeanings/senses for each item was 97% (range: 0.56–1.00, \nSD = .07). Thus, in most cases, the top two meanings/senses \naccounted for nearly all of the norming associates provided.5\nThe range of dominance values for the 547 ambiguous \nwords was 0.00 to 1.00 (mean = 0.61, SD = 0.27). Of these, \n349 words had dominance values ≤0.75, a typical cutoff for \nclassifying ambiguous words as being more or less meaning/\nsense-balanced. See Fig.  1 for a frequency distribution \nof item mean dominance values. There were a number of \nitems for which the dominance norming responses did not \nalign with the homonym/polyseme classification based on \ndictionary criteria (indicated in Column J of Appendix 1 in \nsupplementary material). In many instances, this was because \none of the dictionary homonym meanings was obscure or \ninfrequent, with norming respondents not supplying any \nassociates related to that meaning (e.g., BOXER, CARD, \nCHIP, DEAL). In other cases, word meanings or senses \nwere interpreted by some as proper names or acronyms (e.g., \nBOND, MUSTANG, SUBWAY , NIRVANA, ACT ), which were \nnot listed in the dictionary. For our purposes, we identified \nand labeled these ambiguous word types based on the top two \nmeanings/senses associated with the norming responses; we \nprovide our reasons for these shifts in Column K of Appendix 1 \nin supplementary material.\nOne consideration in soliciting two responses per item \nin the current study is that it diverged from more traditional \nFig. 1  Frequency distribution of dominance values for 547 ambiguous words\n5 Of the 547 ambiguous words, 476 items had 90–100% of norm-\ning responses accounted for by the two most frequent meanings/\nsenses; 41 items had 80–90%; 20 items had 70–80%; and 10 items \nhad 56–70%.\n1544 Behavior Research Methods (2023) 55:1537–1557\n1 3\nsingle response paradigms (e.g., Rodd et al., 2002; Twil-\nley et al., 1994). We note, however, that others have solic-\nited multiple associates in dominance norming tasks; for \ninstance, Brocher et al. (2016) and Brocher et al. (2018) \nsupplied blanks to solicit 5 responses per participant. Our \napproach of soliciting two associates, then, was a com-\npromise between the single and many-response methods. \nIn addition to providing an increased amount of data, we \nthought that the multiple-blank approach was appropriate \ngiven our interest in studying relatively meaning/sense-\nbalanced words. However, a potential concern is this may \nhave led participants to adopt certain response strategies; \nfor instance, the two blanks could have acted to cue words’ \nambiguity, leading participants to assume that semantically \ndifferent responses were required or could have led respond-\nents to “double down” particularly on dominant meanings. \nSome support for this point comes from Twilley et al. (1994), \nwho suggest that requiring multiple associations may make \nresponses especially prone to strategic factors. In particular, \nthe authors cite a norming study by Gorfein et al. (1982) in \nwhich participants were asked to produce four associations \nto homographs. In that study, 82% of participants produced \nassociations to the same meaning on their first and second \nresponses. To examine the degree of “doubling down” in \nour data, we conducted a post hoc examination to ascertain, \nif—like Gorfein et al. (1982)—there was a tendency for indi-\nvidual participants to produce associates linked to the same \nmeanings on their first and second responses. Although we \ncannot know whether conscious or unconscious strategies \nwere employed by our experimental participants, a tally of \nthe dominance norming data revealed that 46% of individu-\nals’ first and second responses to items were associated with \nthe same meanings while 54% were associated with differ -\nent meanings. These relatively balanced percentages suggest \nthat there was not a consistent strategy employed by partici-\npants in providing free responses in our study.\nConclusions\nIn sum, the present norming study applied a tested method \nof assessing meaning dominance for homonymous words to \na wider range (than is typically described in the literature) of \nambiguous words that included polysemes. The results yielded \nlists of associative responses that were categorized by raters \naccording to their meanings or senses. The relative frequen-\ncies of the most common senses or meanings for each word \nwere then calculated, yielding a resultant list of dominance \nvalues for 547 ambiguous words. Although our main purpose \nfor assessing dominance values was for use in planned written \nsentence comprehension studies, the data collected via this \nnorming are likely to be useful to other researchers. Through \nthis report, our results will be available to anyone interested in \nevaluating meaning/sense frequencies not only of ambiguous \nwords with highly divergent multiple meanings but also of \nthose whose senses are relationally or historically linked. This \ncould be especially useful because (a) there are very few exist-\ning polyseme dominance norms, and (b) as our norming find-\nings show, traditional dictionary entry criteria for assessing \nmultiple meanings or senses do not always accurately reflect \ncontemporary language users’ knowledge of or levels of famili-\narity with different semantic senses.\nStudy 2: Ambiguous word meaning/sense \nsimilarity norming\nIn addition to determining the most frequent meanings/\nsenses of isolated ambiguous words through dominance nor-\nming, we also set out to assess meaning and sense similarity. \nIt was clear from the dominance norming that dictionary \ncriteria for categorizing ambiguous words as either homony-\nmous or polysemous did not always correspond with the \nmeanings most familiar to our norming respondents (refer \nto Column J of Appendix 1 in supplementary material). For \ninstance, sometimes words listed in the dictionary as homo-\nnyms elicited norming responses that aligned with only one \nword meaning or multiple senses of that meaning, with other \nlisted meanings being arcane, obscure, or infrequent (e.g., \nCARD, as in wool; CHIP, as in chirp; DEAL, as in planks). \nYet other words turned out to have norming responses asso-\nciated with a sense or meaning not listed in the dictionary, \nsometimes because that sense represented the name of a con-\ntemporary product or brand (e.g., MUSTANG, NIRVANA, \nSUBWAY ). There were still other dictionary-criteria polyse-\nmes whose shared origins may be unknown or opaque to the \naverage undergraduate, with contemporary semantic senses \nthat seem quite distinct (e.g., PANEL, as in section or com-\nmittee; CHARM as in charisma or ornament; STOCK, as in \ninvestment or inventory). These polysemes tended to be ones \nconsidered irregular or having senses that represented more \nmetaphorical relations (Apresjan, 1974). Apresjan (1974) \nand Klepousniotou and Baum (2007) suggest that irregular/\nmetaphorical polysemy may be closer to homonymy than \nmetonymic/regular polysemy, with Klepousniotou et al. \n(2012) describing metaphorical polysemes as perhaps lack-\ning a fixed status in a continuum of lexical ambiguity; rather, \nthese are words that may be in transition from generated \nsenses to separate meaning representations. Metonymic/\nregular polysemes tend to exhibit greater sense contiguity \nand relatedness, with a variety of relation types that extend \nproductively across broad categories of lexical items (e.g., \ncontainer/contents [VASE, BAG]; animal/food [CHICKEN, \nLOBSTER]; producers/products [MOZART , NEWSPAPER]).\nThese different examples highlighted by the dominance \nnorming led us to consider a further division of the ambigu-\nous words for our similarity norming. Instead of sorting the \n1545Behavior Research Methods (2023) 55:1537–1557 \n1 3\nambiguous words into only two categories (homonyms and \npolysemes) as we did in the dominance norming, we fur -\nther subdivided the polysemes into regular (metonymic) and \nirregular (metaphorical) polysemes, following the condition \nbreakdowns in the experimental work of Klepousniotou and \ncolleagues (e.g., Klepousniotou, 2002; Klepousniotou & \nBaum, 2007; Klepousniotou et al., 2012) among others. In \naddition, we decided to anchor our ambiguous word list with \nitems arguably considered unambiguous (with single diction-\nary meanings/senses). With four levels of lexical ambiguity \nthen (homonyms, irregular polysemes, regular polysemes, and \nunambiguous), the current norming offers the opportunity to \nobserve whether the ratings for such linguistic categories \nalign with patterns of increasing similarity ratings similar to \nthose demonstrated in relatively limited experimental work \n(e.g., of Klepousniotou and colleagues), as well as the kind \nof ambiguity continuum proposed by theoretical linguists \n(e.g., Apresjan, 1974). In both cases, the order from least to \nmost meaning similarity is homonyms, irregular (metaphoric) \npolysemes, regular (metonymic) polysemes, and unambiguous \nwords. Ultimately, this second norming would also allow us \nto go beyond dictionary criteria for assessing meaning/sense \nsimilarity, providing contemporary U.S. English user-gener-\nated data about the relatedness of words’ multiple meanings/\nsenses, across a range of ambiguous word types.\nRating the similarity of ambiguous word meanings/\nsenses\nThe goal was to assess the meaning similarity of the two \nmost dominant meanings/senses of each ambiguous word \nas determined through the dominance norming. Some tra-\nditional approaches to evaluating the semantic similarity of \nambiguous word meanings are for norming participants to \njudge the relatedness of two tokens of the same word used in \ndifferent sentence or word pair contexts or through compari-\nson and relatedness ratings of the multiple definitions (e.g., \nBrocher et al., 2018; Klein & Murphy, 2002; Rodd et al., \n2002). For instance, individuals might be asked to compare \nand rate the similarity of the meanings of cotton in phrases \nsuch as ‘the farm owners discussed the cotton . . .’ versus \n‘the fashion designers discussed the cotton . . .’ (Foraker & \nMurphy, 2012), or in word pairs such as ‘marinated lamb’ \nversus ‘friendly lamb’ (Klepousniotou et al., 2008).\nFor the current similarity norming, we considered modifying \nthese more traditional approaches slightly, by still testing dif-\nferent word meanings/senses in strongly biasing contexts, but \nin a way that could maximize sensitivity across the range of \nhomonyms and both classes of polysemes, and test in a man-\nner by which explicit decision making is supported by implicit \nparticipant reactions to meaning similarity or differences. As a \nmethod for assessing a continuum of ambiguity, we thus consid-\nered the linguistic phenomenon of zeugma. Zeugma is defined \nas ‘the use of a word to modify or govern two or more words \nusually in such a manner that it applies to each in a different \nsense or makes sense with only one’ (Merriam-Webster Online \nDictionary; https:// www. merri am- webst er. com/ dicti onary). \nFor instance, for the musical lyric phrase ‘held your breath and \nthe door’ (Morissette & Ballard, 1995), the word door is sur-\nprising because when it is encountered it necessitates a differ-\nent interpretation of held than the one activated when the first \ndirect object ‘your breath’ is encountered. Zeugma refers to this \nkind of infelicity, humor, or strangeness that results from such \nconflicts. So-called zeugma tests, in which putative ambiguous \nwords’ multiple potential meanings are referred to within the \nsame sentence, have been argued to serve as diagnostics for \ndetecting lexical ambiguity (see Cruse, 1986; Lewandowska-\nTomaszczyk, 2007; Sennet, 2021; Tuggy, 1993). They are one \namong several alternative methods (e.g., contradiction, cross-\nlinguistic and definitional tests) that have been proposed for this \npurpose, traditionally by philosophers, logicians, and theoretical \nlinguists (see Viebahn, 2018), each with certain strengths and \nweaknesses. Some drawbacks to zeugmatic tests, for instance, \nare that they have been suggested to be better for detecting \nhomonymy than for some kinds of polysemy (for discussion, \nsee Moldovan, 2021; Viebahn, 2018) and have been argued to \nbe context-inconsistent. For example, Sennet (2021) points out \nthat while the sentence ‘Judy’s dissertation is thought provok-\ning and yellowed with age’ is zeugmatic, the alternation ‘Judy’s \ndissertation is still thought provoking although yellowed with \nage’ is not. Although these judgments might be debated, it is \nimportant to point out that such proposed weaknesses of zeugma \ntests primarily come into play when categorical judgments are \nsolicited. Under our proposed use of zeugma for similarity test-\ning, we would not ask participants to make binary acceptability \njudgments, but rather provide ratings on a continuous scale of \ncontext-biased tokens’ meanings/similarities, which is more in \nline with the idea of a continuum of ambiguity. Indeed, Lewan-\ndowska-Tomaszczyk (2007) describes how examples like the \ndissertation sentences highlight the “fuzzy boundaries” between \ndifferent types of lexical ambiguity and argue against rigid cat-\negorical lexical ambiguity distinctions.\nAs an alternative to more traditional methods of similarity \nnorming, we also suggest that an added benefit to employing \nthe zeugmatic manipulation for similarity norming is that the \nsense or degree of meaning mismatch may offer readers a more \nimplicit cue about the degree to which the two meanings of the \nword are similar or different. In contrast to between-sentence \nor word pair designs, within-sentence infelicity arises in zeug-\nmatic constructions because a second meaning of a word is \nobligatorily and unconsciously mapped to an alternative and \nalready activated meaning of the same word. Participants still \nhave to reach a judgment about meaning similarity, but their \nreaction to the zeugma may support this decision and possibly \ntap into different stages of word processing—an idea that will \nultimately need to be put to the test experimentally.\n1546 Behavior Research Methods (2023) 55:1537–1557\n1 3\nMethod\nMaterials\nWe set out to assess meaning/sense similarity for 320 items, \nwith equal numbers of items from each of the following four \ncategories: homonyms, irregular polysemes, regular polyse-\nmes, and unambiguous words. See Appendix 2, Column A \nin supplementary material for the list of words. The ambigu-\nous words, when possible, were drawn from the dominance-\nnormed items in Study 1. While our pre-dominance norming \ncategorization relied on dictionary criteria, our revised post-\ndominance norming sorting considered the top two mean-\nings/senses associated with the responses provided in the \ndominance norming. For example, although the word BAR  \nwas originally categorized as a homonym based on separate \ndictionary entries (rigid solid shape vs. a unit of pressure), \nthe dominance norming indicated that the two most frequent \nresponses were related to polysemous senses of the first defi-\nnition only (where drinks are served vs. an amount of food \nserved in a rectangular shape, e.g., a chocolate bar). For \nthis reason, BAR was considered as a polyseme for the next \nstage of norming. See Column D of Appendix 2 in supple-\nmentary material for ambiguous word categories.\nIn choosing ambiguous words from our dominance nor -\nming for the similarity norming, we prioritized selection of \nmore meaning/sense-balanced items. As an initial criterion, \nwe aimed to select items with dominance scores of ≤0.75, \na cited value in the literature for differentiating ambiguous \nwords with biased versus unbiased meanings (Armstrong \net al., 2012). Again, under Armstrong’s dominance calcula-\ntion, a dominance score of 0.0 represents a perfectly bal-\nanced item and a score of 1.0 represents a completely biased \nitem. In addition, a second preference for inclusion in the \nsimilarity norming was for the ambiguous words’ most com-\nmonly provided meanings or senses to share the same part \nof speech. This was in part due to our experimental goals \nfor subsequent studies, which would require construction of \nsentence stimuli around multiple meanings/senses of these \nwords. This would be difficult if a word functioned as, for \ninstance, a verb in one case but as a noun in another (e.g., \nCLASH, DESIGN, FILM, LIMP). We detail word selection \nfor the individual ambiguity categories in the following \nparagraphs.\nUnambiguous words Including an unambiguous word cat-\negory in our similarity norming was intended to provide a \nbaseline for assessing meaning/sense similarity for the more \nambiguous items, and to encourage participants to use the \nfull range of the similarity rating scale. The criterion used \nfor unambiguous word selection was a single dictionary \nentry with a single meaning listed (e.g., COFFIN, EASEL, \nHEXAGON, NOON).\nPolysemes From the list of words that were dominance normed, \nwe categorized the polysemes (Column D of Appendix 2 \nin supplementary) material as either regular (aligning with \nthe metonymic categorization) or irregular (aligning with \nmetaphoric polysemes), according to whether or not they \nfollowed one of many generative polysemous relationships \noutlined in the literature (e.g., Apresjan, 1974; Barque & \nChaumartin, 2009; Pustejovsky, 1998; see Table 1 for some \nexamples of these relations). From the dominance-normed \nitems, we were able to select 80 irregular polysemes that \nmet our criteria (e.g., STRAW, SPEAKER). For the regular \npolysemes (e.g., LIBRARY, ORANGE), 70 items met our cri-\nteria with dominance scores ≤0.75, and an additional two \nhad dominance scores of 0.76. Since our planned eventual \nexperiments required 80 regular polysemes, we supplemented \nthis list with eight additional items not included in our domi-\nnance norming (CREAM, FLASK, LILAC, RECIPE, SILVER, \nTAN, TUNNEL, VASE), but whose senses shared the same \npolysemic relationships (substance/color; container/contents; \nfigure/ground; producer/product) as the items selected from \nthe dominance norming. We presumed that their dominance \nvalues would lie within a similar range to those normed. \nSince there are few existing dominance norms for polysemes, \nwe were unable to rely on norming values from other studies.\nHomonyms We aimed to select 80 homonyms from the list \nof dominance-normed items whose top meanings were both \nrelatively balanced and the same part of speech. Sixty-five \nhomonyms met our part of speech criterion and had domi-\nnance ratings ranging from 0.0 to 0.79 (extending our cutoff  \nslightly). To supplement this list, we included nine homo-\nnyms from Armstrong et al. (2012) whose dominance values \nwere <0.75; to round out this list, we chose six additional \nhomonyms from our dominance norming whose dominance \nvalues exceeded the 0.75 cutoff, but for which the dominance \nscores from Armstrong et al.’s (2012) study were ≤0.75. \nRefer to Appendix 2, Columns B and C in supplementary \nmaterial, where these exceptions are noted.\nZeugma sentence construction To test meaning/sense \nsimilarity for ambiguous words ranging from homonyms to \nirregular and regular polysemes, as well as unambiguous \nwords, we began by writing two sentence contexts for each \ncritical word. For the homonyms and polysemes, each of \nthose two contexts constrained for one of the top two mean-\nings or senses as determined through dominance norming. \nFor example, for a homonym like BARK, the first context \nwas ‘The trees had a rough bark.’, and the second was ‘The \npoodle had a loud bark.’, For unambiguous words, both con-\ntexts constrained for the same (the only) meaning; for exam-\nple, for pigeons, the first context was ‘Messages were carried \nby the pigeons.’, and the second was ‘A homeless man fed \nthe pigeons.’ Note that the two contexts were not intended \n1547Behavior Research Methods (2023) 55:1537–1557 \n1 3\nto differ systematically in the degree to which they disam-\nbiguated the target word; in other words, each disambiguat-\ning clause was intended to be equally informative about the \nmeaning it selected.6 The two contexts were then combined \ninto a single sentence by use of a conjunction (and for 315 \nitems, and but or or for the remaining five). For the major -\nity of resultant sentences, the second instance of the critical \nword was replaced by an anaphoric referring expression: for \n8 items, the critical word was repeated because an anaphoric \nreferring expression was not appropriate due to issues relat-\ning to animacy or part of speech (e.g., JAPANESE food vs. \npeople). Although commas would have been grammatically \ncorrect, they were omitted at the conjunction to enhance \npotential zeugmatic effects. See Table  2 and Column E of \nAppendix 2 in supplementary material for zeugmatic stimu-\nlus sentence examples from the similarity norming.\nParticipants\nGiven our novel approach to similarity norming, we elected \nfor a larger sample size than previous ambiguous word \nsimilarity/relatedness normings described in the literature, \nwhere the number of similarity ratings collected per item \nhas often not exceeded N  = 30 (e.g., Brocher et al., 2018; \nDurkin & Manning, 1989; Klepousniotou et al., 2008; \nRodd et al., 2002); thus, 98 native English-speaking UCSD \nundergraduates (74 females, 22 males, two nonbinary) \nbetween the ages of 18 and 25 years took part in the online \nsimilarity norming study for course credit. Participants were \nrecruited from the UCSD Psychology Department subject \npool’s online scheduling system. Informed consent was \nobtained from all participants after the norming procedure \nhad been outlined for them.\nNorming procedure\nSimilarity norming was administered through an online sur-\nvey, designed using Qualtrix software, with each participant \nnorming the same 320 sentences. Stimuli were presented \nwith unique, randomized orderings for each participant. The \nambiguous (or unambiguous) critical word of each sentence \nwas presented in an uppercase black font and the anaphoric \nreferring expression in a red font color. In terms of assess-\ning meaning similarity, criteria to guide decision-making \nwere taken from McRae et al. (2005), including whether the \ntokens share features, have similar properties, or whether \nthey occur in similar contexts. Participants were instructed \nas follows:\nYou will read a number of sentences. Each has a word \nhighlighted in black and another in red. The black words \nare mostly nouns and the red words (mostly pronouns) refer \nto the black word. For each sentence, please judge the simi-\nlarity in meaning between the black word and what’s being \nreferred to in red. Meaning similarity should be ranked on \na scale ranging from 1 (“meanings not similar at all”) to 7 \n(“the very same meaning”).\nTable 1  Representative Regular Polysemic Sense Relations\nRelation Example\nX represents Y playing card represents person or entity (QUEEN, KING)\ncontainer represents contents (BOWL, GLASS, TEASPOON)\nartifact represents activity (SHOWER, BATH, BASEBALL)\nbuilding represents institution (CHURCH, COURT, COLLEGE)\ncompany represents product (STARBUCKS, KLEENEX)\nX is caused by Y fee is caused by action (ADMISSION, ANCHORAGE)\nX is produced by Y art from artist (MOZART, PICASSO, HEMINGWAY )\nX produces Y business firm produces publication (NEWSPAPER, MAGAZINE)\ndevice/broadcast (TELEVISION, RADIO)\nX is from Y flesh/meat/food from animal (CHICKEN, EGG, LOBSTER)\nproduct from plant (OAK, COTTON)\nX is about Y division responsible for work type (EDUCATION, ENERGY, TRANSPORT)\nX accompanies Y music that accompanies dance (HIP HOP, POLKA, WALTZ)\nX covers Y cloth covering that covers body part (ELBOW, KNEE)\nX is included in Y river passes by region (MISSISSIPPI, AMAZON, MISSOURI)\nX is typical of Y color typical of a substance (EMERALD, LILAC, IVORY)\nlanguage spoken/food eaten by a person (KOREAN, ITALIAN, FRENCH)\nX is part of Y Object figure/ground or enclosure/opening (CHIMNEY, WINDOW, GATE)\n6 See Supplementary Analysis 2 for empirical evidence that the \ninformativity of the disambiguating clause did not systematically dif-\nfer across the first or second context, or across conditions.\n1548 Behavior Research Methods (2023) 55:1537–1557\n1 3\nAs a guide to judging similarity of meaning, consider the \nfollowing questions:\n(a) Can the two meanings appear in similar contexts?\n(b) Do the two meanings share physical or functional properties?\n(c) Do the two meanings taste, smell, sound or feel similarly?\n(d) Do the two meanings behave similarly?\nThese instructions were followed by several example \nsentences with critical words that spanned the ambiguity \ncontinuum. Possible explanations for why the critical words \nand their anaphoric referring expressions might be rated as \nmore or less similar were also provided to participants.\nUsing the zeugma test for similarity norming offered a way \nof biasing words’ different meanings or senses within a single \nsentence so that individuals could respond fairly intuitively to \nthe mismatch or correspondence of the different meanings or \nsenses, without having to draw comparisons across sentences. \nThis design was also appropriate across the different types of \nambiguous as well as unambiguous words, allowing for graded \nresponses to meaning similarity that went beyond categori-\ncal classifications that stemmed from historical associations, \nwhich may or may not have been obvious to comprehenders.\nData preprocessing/rejections\nData from 10 participants were dropped (leaving 88 remaining \nparticipants) based on one or more of the following reasons: (1) \nincomplete or uncooperative responses, (2) variance of ≤0.5 \nbetween condition means for the Homonym and Unambiguous \nconditions (i.e., those with the anticipated most and least similar \nmeanings, therefore intended to identify participants who gave \nsimilar ratings regardless of item type), or (3) apparent lack of \nparticipant understanding of instructions. In addition, two of \nthe total 320 sentences were excluded from analysis: (1) one \nitem (CORN), although reclassified by the experimenters as \na polyseme following the dominance norming, was inadvert-\nently tested in a sentence based on the homonym meanings, and \n(2) there was an experimenter error in designing the zeugma \nstimulus sentence for another homonym (POACHED). This left \n78 items in the homonym condition and 80 items in each of the \nother three conditions. For each of these 318 items, there were \n87–88 responses. The individual participant and item similarity \nratings from Study 2 may be accessed at https:// osf. io/ g7fmv/.\nAnalysis and results\nThe similarity rating means, standard deviations, and stand-\nard errors for the individual items are shown in Appendix 2, \nColumns F, G, and H in supplementary material.\nInter‑annotator agreement\nTo assess the reliability of the ratings from our similarity \nnorming, we calculated inter-annotator agreement using a \nleave-one-out method. For each participant, we calculated \nthe Spearman’s rank correlation between that participant’s \nset of similarity ratings (i.e., for all items that a given par -\nticipant observed), and the mean rating assigned to those \nitems by the 87 remaining participants. This yields a score \nindicating how much each participant’s judgments were \naligned with the judgments made by other participants. \nSimilar approaches have been used in past work (e.g., Trott \n& Bergen, 2021); crucially, this score can be compared to \nthe average correlation obtained using language models (see \ndescription of computational analysis of human similarity \nnorms below), providing an upper bound of how much \nagreement can be expected.\nAcross the 88 participants, the mean correlation was \n0.83 (SD = 0.1, median = 0.86). This is comparable to—if \nslightly higher than––other work investigating inter-annota-\ntor agreement for similarity norms (Hill et al., 2015; Trott & \nBergen, 2021). The scores ranged from a minimum of 0.39 \nto a maximum of 0.93. However, as depicted in Figure  2, \nonly two scores fell below 0.5, and the majority (75%) of \nscores were above 0.8.\nDescriptive statistics of similarity ratings\nWe begin with some descriptive statistics for the similarity nor-\nming data set. Similarity rating condition means with standard \ndeviations are as follows: Homonyms (M = 1.63, SD = 0.33), \nTable 2  Similarity Norming Examples of Zeugmatic Sentence Stimuli\nSimilarity norming sentence Ambiguity category Item mean similarity rating\nMildew spray removed the MOLD and concrete was poured into one. Homonym 1.69\n(SD = 1.28, SEM = 0.14)\nThe stereo system was missing a SPEAKER and the conference organizers sched-\nuled one.\nIrregular polyseme 2.38\n(SD = 1.35, SEM = 0.14)\nThe paint she picked was TAN and in the summer she always gets one. Regular polyseme 3.63\n(SD = 1.56, SEM = 0.17)\nThe story was about a MERMAID and the girl swam like one. Unambiguous 6.41\n(SD = 0.92, SEM = 0.10)\n1549Behavior Research Methods (2023) 55:1537–1557 \n1 3\nIrregular Polysemes (M = 2.57, SD = 1.14), Regular Polyse-\nmes (M = 4.96, SD = 1.27), and Unambiguous (M  = 6.28, \nSD = 0.44). Mean similarity ratings of items within the four \nambiguity conditions are also displayed in a box-and-whisker \nplot in Fig. 3. These plots suggest that, on average, there was \nan expected range of similarity values between the ambiguity \nextremes, with homonyms exhibiting the most dissimilar mean-\nings and unambiguous words the most similar. Values for the \npolysemes fell in between, with the irregular polysemes nota-\nbly showing, on average, more distinct meanings (more closely \nresembling homonyms) and the regular polysemes showing \nmore similar meanings (closer to unambiguous words).\nPost hoc pairwise statistical comparisons (t  tests with \nBonferroni correction for multiple comparisons) indicated \nthat the similarity ratings for each condition differed signifi-\ncantly (p < .01) from those of each of the other conditions. \nThese data are consistent with proposals of a continuum \nof ambiguity across the four conditions, within both theo -\nretical linguistics (e.g., Apresjan, 1974) and experimental \nwork (e.g., Klepousniotou et al., 2012). Figure  3 and the \nstandard deviations of the conditions illustrate that both \npolyseme categories exhibited within-category variability \ngreater than either the homonyms or unambiguous words. \nHowever, across all four categories, there also appeared to \nbe a good deal of overlap in similarity values; for example, \nsome words classified as regular polysemes were rated as \nhaving meanings equally similar to those of many unam-\nbiguous words. Both patterns suggest potential other factors \n(i.e., beyond dictionary-based ambiguous word categories) \nthat may be contributing to the variance in similarity ratings, \nboth within and across conditions. One intriguing candidate, \nfor instance, is the rated concreteness of ambiguous words, \nwith Gilhooly and Logie (1980a) noting that abstract words \ntend to be more ambiguous than concrete ones. Aligning \nwith this idea, a lookup of concreteness ratings (consult-\ning norms from Brysbaert et al., 2014, using a scale of 1 to \n5, with 5 being most concrete and 1 most abstract) for our \ntested regular and irregular polysemes indicates categorical \nmean concreteness differences, with regular polysemes (hav-\ning more similar meanings) being on average more concrete \nthan irregular polysemes (mean concreteness ratings being \n4.7 and 3.7, respectively). This raises interesting questions \nrelating to the perhaps more inherent difficulty of rating the \nsimilarity of more abstract versus more concrete word mean-\nings, but also about what it means to rate the concreteness of \nFig. 2  Inter-annotator agreement for similarity norming, plotting the \ndistribution of Spearman’s rank correlation scores, calculated using a \nleave-one-annotator-out scheme. The dotted line represents the mean \ninter-annotator agreement (0.83)\nFig. 3  Item mean similarity ratings (318 items, 88 participants), box-and-whisker plots within ambiguity conditions, with X indicating mean \ncondition values\n1550 Behavior Research Methods (2023) 55:1537–1557\n1 3\na word with multiple meanings. Although intriguing, further \nexploration is a matter for future research and is beyond the \nscope of the current paper.\nQualitatively, these descriptive findings are broadly \nconsistent with the predictions of the “cues to meaning” \nframework (mentioned in the Introduction), in which word \nmeanings are characterized as occupying a continuous, con-\ntext-sensitive landscape (Elman, 2004; Li & Joanisse, 2021). \nIt should be noted, however, that this study was not con-\nducted with the explicit goal of confirming or disconfirming \nthe cues to meaning framework (nor was a statistical test run \nto do so); rather, the norms collected here could serve as a \ncrucial first step to testing that framework in future work.\nComputational analysis of human similarity norms\nGiven the results of the Study 2 similarity norming, we deter-\nmined that conducting an additional computational analysis \nwould allow for the evaluation of factors that may help explain \nvariance beyond that accounted for by our dictionary-crite-\nria ambiguity categories (Homonyms, Irregular Polysemes, \nRegular Polysemes and Unambiguous). Doing so involved \ncomparing the human meaning similarity judgments to BERT \n(Devlin et al., 2018), a neural language model (NLM) trained \non a large corpus of (mostly written) English text.\nNLMs such as BERT are well suited to operationalizing \nthe cues to meaning framework described earlier. Indeed, \na recurrent neural network––a precursor to models like \nBERT––was an inspiration for this framework (Elman, \n2004), and several recent studies have used NLMs as compu-\ntational models of human sense similarity (Haber & Poesio, \n2020; Li & Joanisse, 2021; Nair et al., 2020; Trott & Bergen, \n2021). Rather than representing word meanings as discrete \nentries in a mental dictionary, BERT’s “representation” of \na word corresponds to the distribution of hidden unit acti -\nvations in each layer of the network. These representations \nreflect information about a word’s distributional profile and \nits relationship to the immediate sentential context––neces-\nsarily, then, they encode both semantic and syntactic infor -\nmation7, though there is some evidence that earlier layers \nof BERT encode syntactic information, while later layers \nencode semantic information (Tenney et al., 2019). Impor -\ntantly, this activation profile is both continuous (i.e., it situ-\nates a word at some location in vector-space), and context-\nsensitive (i.e., the exact state elicited by a given wordform is \ndependent on the immediate context). This allows for extrac-\ntion of continuous measures of meaning similarity (as meas-\nured by BERT), which can be compared to human ratings.\nOf course, BERT represents a particular operationali-\nzation of the cues to meaning framework: It is trained on \nlinguistic input alone, and it has a particular network archi-\ntecture. This means that BERT has no access to grounded \ninformation about word meaning (Bender & Koller, 2020). \nThis potential limitation, however, makes BERT suitable \nfor asking how much information about a word’s meaning \ncan be gleaned solely from statistical regularities in that \nword’s pattern of use. The distributional hypothesis states \nthat words with more similar meanings should appear in \nmore similar contexts (Firth, 1957; Harris, 1954; Sahlgren, \n2008). If so, then word meaning should be derivable, at least \nin part, from the contexts in which the word occurs. On some \naccounts, distributional statistics play a significant role in \ninforming human lexical knowledge (Andrews et al., 2009; \nLupyan & Lewis, 2019; Lupyan & Winter, 2018), and there \nis psycholinguistic evidence that humans can infer a novel \nword’s meaning from its distributional similarity to other \nwords (Ouyang et al., 2017). Further, computational models \nthat exploit distributional statistics––sometimes called dis-\ntributional semantic models––such as Latent Semantic Anal-\nysis (Landauer & Dumais, 1997) and word2vec (Mikolov \net al., 2013), have proven relatively successful in predicting \nhuman judgments of similarity and relatedness (McDonald \n& Ramscar, 2001; Mikolov et al., 2013), particularly for \ndecontextualized (which we have referred to in this paper as \n‘isolated’) words (e.g., dolphin vs. shark). More recent mod-\nels like BERT (Devlin et al., 2018) produce “contextualized \nembeddings”, which reflect not only a word’s overall pattern \nof use but also its immediate sentential context; in princi-\nple, this allows differentiation between distinct senses of an \nambiguous word. The extent to which these models can pre-\ndict human similarity judgments can be seen as a proxy for \nhow much human semantic knowledge can be derived from \ndistributional statistics alone, and how much might require \nother, perhaps extra-linguistic information (Andrews et al., \n2009). This also serves an applied goal: if NLMs like BERT \nfail to capture crucial semantic distinctions, then there may \nbe room for improvement, such as augmenting the sources \nof information they are exposed to during training (Bender \n& Koller, 2020).\nThere are a number of distinct pretrained NLMs to choose \nfrom, all of which are trained on linguistic input alone and \nprovide continuous meaning representations. We selected \nBERT because of its state-of-the-art performance on word \nsense disambiguation tasks (Loureiro et al., 2020), as well \nas the extensive body of literature probing where and what \nkind of information is encoded in BERT (Rogers et al., 2020; \nTenney et al., 2019). Future work could extend these analy-\nses to other models, such as ELMo (Peters et al., 2018) or \nXLNet (Yang et al., 2019).\nLike other statistical language models, BERT is a neu-\nral network trained on a large corpus (>4B word tokens) \n7 It is unlikely that the representations encode phonological informa-\ntion, except to the extent that a word’s orthographic or phonological \nprofile is correlated with its textual distribution.\n1551Behavior Research Methods (2023) 55:1537–1557 \n1 3\nof mostly written text, using either masked language mod-\neling (MLM) or next sentence prediction. In MLM, a given \nword token is “masked” from a sentence (e.g., “the cup \n[MASK]”), and BERT must predict the identity of that \nmasked word; over many iterations, BERT tunes its repre-\nsentations of each word and surrounding tokens to improve \nits predictions, eventually allowing it to be used to catego-\nrize part-of-speech, semantic roles, word senses, and more \n(Tenney et al., 2019). In addition to being beneficial for \ntraining, MLM can be used “online” to obtain the probabil-\nity BERT assigns to a given word token that it would appear \nin a given context.\nWe obtained two different (but related) measurements \nfrom BERT. First, for each zeugmatic sentence (e.g., The \nplayer swung a bat and the vampire was bit by one.), we cal-\nculated the cosine distance between BERT’s contextualized \nembeddings for the target word (e.g., bat) and the anaphoric \nreferring expression (e.g., one). Cosine distance is often used \nto model human similarity or relatedness judgments, both \nfor words appearing in isolation (Hill et al., 2015) and in \ncontext (Haber & Poesio, 2020; Trott & Bergen, 2021). Each \ncontextualized embedding corresponds to BERT’s represen-\ntation of a given word token and is thought to reflect syntac-\ntic and semantic factors (Tenney et al., 2019). Larger cosine \ndistances correspond to more dissimilar vectors, and thus \nshould correlate with lower similarity ratings (i.e., less simi-\nlar meanings); conversely, smaller cosine distances should \ncorrespond with higher similarity ratings––with a cosine \ndistance of zero reflecting that a vector is being compared \nwith itself. Because different layers of BERT are thought \nto encode different information (Tenney et al., 2019 ), we \ncalculated cosine distance between the contextualized rep-\nresentations obtained for all 12 layers of BERT (base).\nSecond, using masked language modeling, we calcu-\nlated the surprisal (i.e., the negative log probability), for \neach anaphoric referring expression (e.g., “one”). Past work \nhas used surprisal to predict a number of “online” measures \nof language processing, such as reading time (Goodkind \n& Bicknell, 2018) and the N400 effect (Frank et al., 2015; \nMichaelov & Bergen, 2020). Like cosine distance, surprisal \nlikely reflects syntactic as well as semantic features of a \nword. Unlike cosine distance, surprisal captures the unex-\npectedness of a particular word token appearing in a particu-\nlar context. For zeugmatic sentences, this can be viewed as a \nproxy for how “surprised” BERT would be that two mean-\nings of an ambiguous word are coreferenced (and copredi -\ncated). Thus, we predicted that surprisal should correlate \nboth with condition (e.g., Homonymous vs. Unambiguous) \nand human similarity ratings; that is, more similar mean-\nings should be more likely to be coreferenced, while less \nsimilar meanings should be less likely to be coreferenced. \nIn our use, both cosine distance and surprisal were intended \nto measure how similarly BERT views two meanings of an \nambiguous word––where “similar” likely incorporates both \nsemantic and syntactic features of a word’s distributional \nprofile and immediate sentential context.\nMaterials There were 318 sentences for which we obtained \nhuman similarity ratings. Of these, we excluded four sen-\ntences in which the anaphoric expression contained multiple \nwords (e.g., “a lot”). Thus, we considered 314 sentences \n(and their similarity ratings) in the final analysis.\nProcedure We calculated two measures: (1) the cosine dis-\ntance between the target word and the anaphoric expression, \nand (2) the surprisal of the anaphoric expression.\nTo calculate cosine distance, we ran each sentence \nthrough the pretrained BERT-base (uncased) model using \nthe HuggingFace transformers library 8 in Python (Wolf \net al., 2020). BERT-base has 12 layers, each with 768 hidden \nunits, and 12 attention heads. We obtained the vector repre-\nsentations for the target word (e.g., “bat”) and the anaphor \n(e.g., “one”) in each layer, then calculated the Cosine Dis-\ntance between those vectors. Note that BERT uses a Word-\nPiece tokenizer (Wu et al., 2016), which splits words into \neither the full form (i.e., one word corresponds to one token) \nor into multiple components (e.g., the word “surfing” might \nbe decomposed into “surf” and “##ing”). Each individual \ntoken receives its own embedding. In cases where a given \nword was decomposed into multiple WordPiece tokens, \nwe computed the average of the corresponding WordPiece \nembeddings (Wu et al., 2016). There were 38 sentences for \nwhich this was necessary (representing about 12% of the 314 \nsentences in the final dataset).\nTo calculate Surprisal, we used the pretrained BertFor -\nMaskedLM model object (BERT-base, uncased), from the \nHuggingFace library. For each sentence, we masked the ana-\nphoric expression (e.g., “one”) with the “[MASK]” token, \nand then used BERT to calculate the probability of the given \nanaphor occurring in that slot. We calculated surprisal by \ntaking the negative logarithm of this probability.\nAnalysis and results We had two primary questions. First, do \nthe measures obtained (cosine distance and surprisal) cor -\nrespond to the categorical condition variable (e.g., Homo-\nnym vs. Unambiguous)? Second, do the measures obtained \npredict human judgments of similarity––above and beyond \ncondition? We approached both questions using nested \nmodel comparisons in R (R Core Team, 2020). Specifically, \nin each case, we compared a full model including the pre-\ndictor of interest (e.g., surprisal) to a model omitting only \nthat predictor, and conducted a log-likelihood ratio test to \ndetermine whether the full model explained significantly \nmore variance. Models were built using the lme4 package \n8 https:// huggi ngface. co/ trans forme rs/ index. html\n1552 Behavior Research Methods (2023) 55:1537–1557\n1 3\n(Bates et al., 2015). For each question, the reported p values \nwere adjusted for multiple comparisons using Bonferroni \ncorrections to account for the fact that two separate BERT-\nderived metrics (cosine distance and surprisal) were used to \ntest equivalent hypotheses.\nFirst, we built a full model with Surprisal as the depend-\nent variable, a fixed effect of condition, and random inter -\ncepts for Anaphor (i.e., the specific anaphoric expression \nthat was used for that sentence) and compared this model \nto a model omitting only condition. We also performed an \nanalogous analysis using cosine distance as the dependent \nvariable. In both cases, the condition variable contained four \nlevels: Homonymy, Irregular Polysemy, Regular Polysemy, \nand Unambiguous. Thus, these analyses asked about whether \nthere was variance overall in the dependent variable (cosine \ndistance or surprisal) that was related to the different levels \nof the condition variable. We relied on R’s default coding \nscheme, which automatically selected the mean of the alpha-\nbetically-first level as the Intercept, which in this case was \nHomonymy. After correcting for multiple comparisons, the \nimprovement in model fit when predicting Surprisal was no \nlonger statistically significant, χ2(3) = 8.94, p = .06, nor was \nthe improvement in model fit when predicting cosine distance, \nχ2(3) = 7.47, p = .11. The distribution of surprisal values by \ncondition is displayed in Fig. 4. The results of both statistical \nanalyses suggest that any potential relationship between these \nBERT-derived metrics and ambiguity type was weak at best.\nSecond, we built a full model with mean similarity as \nthe dependent variable, fixed effects of both condition and \nsurprisal, and random intercepts for anaphor. This full model \nexplained significantly more variance than a model omit-\nting only the effect of condition, χ2(1) = 12.82, p = .0006, \nindicating that variation in the Surprisal of the anaphoric \nexpression––as measured by BERT––captured variance \nin participants’ similarity judgments, above and beyond \nthe categorical variable of condition. One explanation \nfor this result is that both polysemy conditions spanned a \nlarge spectrum of similarity; for example, similarity ratings \nfor items in the Regular Polyseme condition ranged from \n1.57 to 6.69 (see Fig.  3). Thus, although similarity ratings \ntended to covary with condition, there remained a substantial \namount of variance to be explained in participants’ simi -\nlarity judgments for polysemous items––and crucially, sur -\nprisal explained some of this variance. We also conducted \nan identical analysis with Cosine Distance (from the final \nlayer) instead of Surprisal. In this case, the improvement \nin model fit was no longer significant after correcting for \nmultiple comparisons, χ 2(1) = 2.56, p  = .21. Finally, we \nasked explicitly whether Surprisal explained variance in \nSimilarity above and beyond cosine distance. A model with \nfixed effects of both surprisal and cosine distance exhibited \nbetter fit than a model containing only cosine distance, χ2(1) \n= 13.74, p  = .0004; in contrast, the improvement of the \nfull model compared to a model containing only Surprisal \nwas statistically non-significant after correcting for multiple \ncomparisons (p = .19).\nBecause earlier work (Tenney et al., 2019) suggests that \ndifferent layers of BERT encode different kinds of infor -\nmation, we also conducted an exploratory analysis asking \nwhich layers of BERT exhibited the strongest correlation \nbetween cosine distance and similarity. The strongest (most \nnegative) correlation was obtained in the final layer (layer \n12) of BERT-base (r  = −.23). As depicted in Fig.  5, how-\never, the correlation did not change considerably beyond \nthe fourth layer. Importantly, even the strongest correlation \nwas much lower (by a factor of 4×) than the average inter-\nannotator agreement (r = .83). Additionally, the correlation \nbetween mean similarity and surprisal (r  = −.35) was less \nthan half the average inter-annotator agreement. Both find-\nings indicate that there is considerable room for improve-\nment to match the human benchmark.\nConclusions\nWith decades of psycholinguistic ambiguity research having \nrelied heavily upon experimental manipulations involving \nhomonyms, another class of ambiguous words—polysemes, \nwhich are more frequent in natural language but much less \ntested—also offers a window into how lexical meanings are \nstored and processed. In experimental comparisons to date, \nthe majority of studies have concluded that polysemes (with \ntheir significant sense overlap) exhibit underspecified mean-\ning interpretation, and unlike homonyms, their interpretation \nhas rarely been shown to be influenced by sense frequency or \ncontext (e.g., Frazier & Rayner, 1990; Klepousniotou, 2002; \nfor reviews, see Eddington & Tokowicz, 2015; Frisson, \n2009). Such findings have implied that the two ambiguous \nFig. 4  Distribution of surprisal values by condition. The surprisal of \nthe anaphoric expression tended to be higher for homonymous and \nirregular polysemy items than unambiguous or regular polysemy \nitems\n1553Behavior Research Methods (2023) 55:1537–1557 \n1 3\nword types are differentially processed, with only a small \nnumber of studies arguing for rapid polyseme sense selection \nand meaning representations more akin to those proposed for \nhomonyms (e.g., Foraker & Murphy, 2012; Klein & Mur -\nphy, 2001). With some of this debate hinging on the very \nnature of the specific experimental materials utilized, and \nin particular on the composition of the polysemic stimuli, it \nis crucial to establish the sense/meaning relations of a wide \nrange of ambiguous word types.\nA clear finding from the current similarity norming is \nthat polysemes do not constitute a unitary category—not \nonly in linguistic theory but in how contemporary language \nusers interpret their multiple semantic senses. Recalling \nthe definition of polysemes outlined earlier in this paper, \na defining characteristic is that the different senses have a \nshared semantic origin. The current results indicate that—on \naverage—even with a common etymology, contemporary \nlanguage users recognize varying degrees of sense similarity \nand semantic overlap across regular and irregular polysemes. \nThis difference between polyseme types was on par with \nthe categorical differences observed between the meanings \nof homonyms and irregular polysemes, as well as between \nregular polysemes and unambiguous words. The polyseme \ndifference may be particularly relevant because relatively \nfew published experimental reports examining online ambi-\nguity processing have distinguished between the two types. \nAn advisable approach for future studies would be, at a \nminimum, to consider the different types of polysemes, the \nnature of the polysemic relations, and the degree of semantic \nsense similarity. Moreover, although our findings indicated \nthat on average there were significant differences in similar-\nity ratings between the different ambiguous word types, the \nfuzzy borders and overlap between all of the categories we \nhave examined suggest that an even better approach might \nbe to consider that words lie along a continuum of ambiguity \n(meaning/sense similarity). This working assumption may \nultimately prove more useful for guiding research about how \nwords’ multiple meanings or senses are neurally activated \nand functionally organized in the brain. In turn, future work \ncould use the norms reported here to investigate the cues to \nmeaning framework (Elman, 2004).\nIn addition, our computational analyses of the similar -\nity ratings assessed using zeugmatic sentences indicate that \nalthough dictionary-based ambiguous word categories tend \nto be predictive of human meaning similarity judgments, \nthere is substantial variance unaccounted for by these clas-\nsifications—some of which seems to be explicable by the \ndegree of Surprisal of the anaphoric expressions. This \nfinding, along with several other recent studies (Haber & \nPoesio, 2020; Li & Joanisse, 2021; Nair et al., 2020; Trott \n& Bergen, 2021), suggests that neural language models \nlike BERT might be useful tools for modeling human sense \nFig. 5  Correlation between cosine distance (i.e., between the vectors \nfor the target word and the anaphoric expression) and human simi-\nlarity ratings for each successive layer of BERT-base. Vertical bars \nrepresent the 95% confidence interval for values of r (calculated using \nthe cor.test function in R)\n1554 Behavior Research Methods (2023) 55:1537–1557\n1 3\nknowledge––particularly for theoretical frameworks that \nview meaning as occupying a continuous, context-sensitive \nlandscape (Elman, 2004). Second, because BERT is trained \non linguistic input alone, its ability to explain variance in \nhuman similarity ratings indicates that some degree of lexi-\ncal semantic knowledge can in principle be derived from \ndistributional regularities. This serves as partial validation \nof the distributional hypothesis (Firth, 1957; Harris, 1954).\nOn the other hand, the correlation between Surprisal and \nhuman similarity ratings fell far short of human inter-annota-\ntor agreement (by a factor of over 2x) and was also lower than \nthe correlation reported in similar studies (Nair et al., 2020; \nTrott & Bergen, 2021). The contrast with previous studies \ncould be explained by the fact that the linguistic stimuli in \nthis task involved anaphora (e.g., “one”), thereby requiring a \nform of coreference resolution; this may be more difficult for \nBERT than tasks that involved a repetition of the same word \nform in different sentential contexts (Nair et al., 2020; Trott \n& Bergen, 2021). Importantly, the gap between human and \nmodel performance reveals room for improvement: clearly, \nhuman participants were drawing on semantic knowledge \nthat was either unavailable to or unrepresented by BERT. It \nalso demonstrates the utility of this lexical resource: a gap in \nperformance offers space for theoretical explanations as well \nas engineering improvements. Future psycholinguistic work \ncould seek to disentangle the factors responsible for explain-\ning human similarity judgments––beyond dictionary-based \ncategories of ambiguity, and beyond the “distributional base-\nline” provided by BERT. In turn, NLP practitioners could \nask about the extent to which introducing different/various \nsources of information (e.g., multimodal grounding) into \nBERT’s training regime would improve its ability to approxi-\nmate human judgments, and if so, figure out how to attain and \nintegrate this information. Additionally, practitioners could \ninvestigate how well other BERT-derived metrics predict \nhuman similarity ratings. For example, previous work (Clark \net al., 2019) found that the behavior of BERT’s attention heads \nencoded information about syntactic relations. In the current \ncase, syntactic information from the attention heads could \nbe used to assist in coreference resolution (e.g., identifying \nmultiple mentions of the same entity).\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 3758/ s13428- 022- 01869-6.\nAcknowledgments The authors gratefully acknowledge the contributions \nof Anna Stoermann to this work, in particular her efforts in coordinating \nand collecting data for the norming studies. This work was supported \nby the Eunice Kennedy Shriver National Institute of Child Health and \nHuman Development under Grant R01HD22614 to Marta Kutas.\nAuthors’ contributions K.A.D. and M.K. conceived of and designed \nthe study. Material preparation and supervision of data collection were \nperformed by K.A.D. Analyses were performed by K.A.D. and S.T. \nThe first draft of the manuscript was written by K.A.D., with all authors \ncommenting on and contributing to subsequent revisions. All authors \nread and approved the final manuscript.\nFunding This research was supported by NICHD Grant R01HD22614 \nto M.K.\nData availability Materials and summarized information for individ-\nual items from these dominance and similarity norming studies are \nincluded as Appendices in the Supplementary Information. The indi-\nvidual subject/item norming data collected and analyzed for the cur -\nrent studies are available in the OSF repository (https:// osf. io/ g7fmv/). \nThese studies were not preregistered.\nCode availability Code to reproduce the primary analyses is available \non GitHub: https:// github. com/ seant rott/ zeugma_ norms.\nDeclarations \nConflict of interest The authors confirm that there are no known con-\nflicts of interest associated with this publication, and there has been no \nsignificant financial support for this work that could have influenced \nits outcome.\nEthics approval This research was approved by the Institutional Review \nBoard (IRB) of the University of California, San Diego.\nConsent to participate Informed consent to participate in these norm-\ning studies was obtained from participants. A statement to this effect \nappears in the manuscript.\nConsent for publication Not applicable.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAndrews, M., Vigliocco, G., & Vinson, D. (2009). Integrating expe -\nriential and distributional data to learn semantic representations. \nPsychological Review, 116(3), 463.\nApresjan, J. D. (1974). Regular polysemy. Linguistics, 12(142), 5–32.\nArmstrong, B., & Plaut, D. (2011). Inducing homonymy effects via \nstimulus quality and (not) nonword difficulty: Implications for \nmodels of semantic ambiguity and word recognition. Proceedings \nof the Annual Meeting of the Cognitive Science Society, 33(33).\nArmstrong, B. C., Tokowicz, N., & Plaut, D. C. (2012). eDom: Norm-\ning software and relative meaning frequencies for 544 English \nhomonyms. Behavior Research Methods, 44(4), 1015–1027.\nBarque, L., & Chaumartin, F. R. (2009). Regular polysemy in WordNet. Journal \nfor Language Technology and Computational Linguistics, 24(2), 5–18.\n1555Behavior Research Methods (2023) 55:1537–1557 \n1 3\nBates, D., Maechler, M., Bolker, B., & Walker, S. (2015). Fitting linear \nmixed-effects models using lme4. Journal of Statistical Software, \n67(1), 1–48. https:// doi. org/ 10. 18637/ jss. v067. i01\nBender, E. M., & Koller, A. (2020). Climbing towards NLU: On mean-\ning, form, and understanding in the age of data. Proceedings of \nthe 58th Annual Meeting of the Association for Computational \nLinguistics, 5185–5198.\nBinder, K. S., & Rayner, K. (1998). Contextual strength does not \nmodulate the subordinate bias effect: Evidence from eye fixa-\ntions and self-paced reading. Psychonomic Bulletin & Review, \n5(2), 271–276.\nBrocher, A., Foraker, S., & Koenig, J.-P. (2016). Processing of irregu-\nlar polysemes in sentence reading. Journal of Experimental Psy -\nchology: Learning, Memory, and Cognition, 42(11), 1798–1813. \nhttps:// doi. org/ 10. 1037/ xlm00 00271\nBrocher, A., Koenig, J. P., Mauner, G., & Foraker, S. (2018). About \nsharing and commitment: The retrieval of biased and balanced \nirregular polysemes. Language, Cognition and Neuroscience, \n33(4), 443–466.\nBruni, E., Tran, N. K., & Baroni, M. (2014). Multimodal distribu-\ntional semantics. Journal of Artificial Intelligence Research, \n49, 1–47.\nBrysbaert, M., Warriner, A. B., & Kuperman, V. (2014). Concreteness \nratings for 40 thousand generally known English word lemmas. \nBehavior Research Methods, 46(3), 904–911.\nClark, K., Khandelwal, U., Levy, O., & Manning, C. D. (2019). What \ndoes BERT look at? An Analysis of BERT’s Attention. Proceed-\nings of the Second BlackboxNLP Workshop on Analyzing and \nInterpreting Neural Networks for NLP, Florence, Italy.\nCruse, D. A. (1986). Lexical semantics. Cambridge University Press.\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-\ntraining of deep bidirectional transformers for language under -\nstanding. ArXiv Preprint. arXiv:1810.04805.\nDuffy, S. A., Morris, R. K., & Rayner, K. (1988). Lexical ambiguity \nand fixation times in reading. Journal of Memory and Language, \n27(4), 429–446.\nDurkin, K., & Manning, J. (1989). Polysemy and the subjective lexicon: \nSemantic relatedness and the salience of intraword senses. Journal \nof Psycholinguistic Research, 18(6), 577–612.\nEddington, C. M., & Tokowicz, N. (2015). How meaning similarity \ninfluences ambiguous word processing: The current state of the \nliterature. Psychonomic Bulletin & Review, 22(1), 13–37.\nElman, J. L. (2004). An alternative view of the mental lexicon. Trends \nin Cognitive Sciences, 8(7), 301–306.\nFirth, J. R. (1957). A synopsis of linguistic theory, 1930–1955. In J. R. \nFirth (Ed.), Studies in linguistic analysis (pp. 1–32). Blackwell.\nForaker, S., & Murphy, G. L. (2012). Polysemy in sentence compre-\nhension: Effects of meaning dominance. Journal of Memory and \nLanguage, 67(4), 407–425.\nFrank, S. L., Otten, L. J., Galli, G., & Vigliocco, G. (2015). The ERP \nresponse to the amount of information conveyed by words in sen-\ntences. Brain and Language, 140, 1–11.\nFrazier, L., & Rayner, K. (1990). Taking on semantic commitments: \nProcessing multiple meanings vs. multiple senses. Journal of \nMemory and Language, 29(2), 181–200.\nFrisson, S. (2009). Semantic underspecification in language processing. \nLanguage and Linguistics Compass, 3(1), 111–127.\nFrisson, S., & Pickering, M. J. (1999). The processing of metonymy: \nevidence from eye movements. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition, 25(6), 1366.\nGeeraerts, D. (1993). Vagueness's puzzles, polysemy's vagaries. Cog-\nnitive Linguistics (includes Cognitive Linguistic Bibliography), \n4(3), 223–272.\nGerz, D., Vulić, I., Hill, F., Reichart, R., & Korhonen, A. (2016). Sim-\nverb-3500: A large-scale evaluation set of verb similarity. ArXiv \nPreprint. arXiv:1608.00869.\nGilbert, R. A., & Rodd, J. M. (2022). Dominance norms and data for \nspoken ambiguous words in British English. Journal of Cogni-\ntion, 5(1).\nGilhooly, K. J., & Logie, R. H. (1980a). Age-of-acquisition, imagery, \nconcreteness, familiarity, and ambiguity measures for 1,944 \nwords. Behavior Research Methods & Instrumentation, 12(4), \n395–427.\nGilhooly, K. J., & Logie, R. H. (1980b). Meaning-dependent ratings of \nimagery, age of acquisition, familiarity, and concreteness for 387 \nambiguous words. Behavior Research Methods & Instrumenta-\ntion, 12(4), 428–450.\nGoodkind, A., & Bicknell, K. (2018). Predictive power of word sur -\nprisal for reading times is a linear function of language model \nquality. Proceedings of the 8th workshop on cognitive modeling \nand computational linguistics (CMCL 2018), 10–18.\nGorfein, D. S., Viviani, J. M., & Leddo, J. (1982). Norms as a tool for \nthe study of homography. Memory & Cognition, 10(5), 503–509.\nHaber, J., & Poesio, M. (2020). Assessing polyseme sense similarity \nthrough co-predication acceptability and contextualised embed-\nding distance. Proceedings of the Ninth Joint Conference on Lexi-\ncal and Computational Semantics, 114–124.\nHalawi, G., Dror, G., Gabrilovich, E., & Koren, Y. (2012). Large-scale \nlearning of word relatedness with constraints. Proceedings of the \n18th ACM SIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining, 1406–1414.\nHarris, Z. (1954). Distributional hypothesis. Word . World, 10(23), \n146–162.\nHill, F., Reichart, R., & Korhonen, A. (2015). Simlex-999: Evaluating \nsemantic models with (genuine) similarity estimation. Computa-\ntional Linguistics, 41(4), 665–695.\nKambe, G., Rayner, K., & Duffy, S. A. (2001). Global context effects \non processing lexically ambiguous words: Evidence from eye fixa-\ntions. Memory & Cognition, 29(2), 363–372.\nKellas, G., & Vu, H. (1999). Strength of context does modulate the sub-\nordinate bias effect: A reply to Binder and Rayner. Psychonomic \nBulletin & Review, 6(3), 511–517.\nKlein, D. E., & Murphy, G. L. (2001). The representation of polyse-\nmous words. Journal of Memory and Language, 45(2), 259–282.\nKlein, D. E., & Murphy, G. L. (2002). Paper has been my ruin: Con-\nceptual relations of polysemous senses. Journal of Memory and \nLanguage, 47(4), 548–570.\nKlepousniotou, E. (2002). The processing of lexical ambiguity: \nHomonymy and polysemy in the mental lexicon. Brain and Lan-\nguage, 81(1/3), 205–223.\nKlepousniotou, E., & Baum, S. R. (2005). Processing homonymy and \npolysemy: Effects of sentential context and time-course following \nunilateral brain damage. Brain and Language, 95(3), 365–382.\nKlepousniotou, E., & Baum, S. R. (2007). Disambiguating the ambigu-\nity advantage effect in word recognition: An advantage for poly -\nsemous but not homonymous words. Journal of Neurolinguistics, \n20(1), 1–24.\nKlepousniotou, E., Pike, G. B., Steinhauer, K., & Gracco, V. (2012). \nNot all ambiguous words are created equal: An EEG investigation \nof homonymy and polysemy. Brain and Language, 123(1), 11–21.\nKlepousniotou, E., Titone, D., & Romero, C. (2008). Making sense of \nword senses: the comprehension of polysemy depends on sense \noverlap. Journal of Experimental Psychology: Learning, Memory, \nand Cognition, 34(6), 1534.\nLakoff, G. (1987). Women, fire, and dangerous things: What categories \nreveal about the mind. University of Chicago Press.\nLandauer, T. K., & Dumais, S. T. (1997). A solution to Plato's prob-\nlem: The latent semantic analysis theory of acquisition, induc-\ntion, and representation of knowledge. Psychological Review, \n104(2), 211.\n1556 Behavior Research Methods (2023) 55:1537–1557\n1 3\nLewandowska-Tomaszczyk, B. (2007). Polysemy, prototypes, and \nradial categories. The Oxford handbook of cognitive linguistics  \n(pp. 139–169). Oxford University Press.\nLi, J., & Joanisse, M. F. (2021). Word senses as clusters of meaning \nmodulations: A computational model of polysemy. Cognitive Sci-\nence, 45(4), Article e12955.\nLoureiro, D., Rezaee, K., Pilehvar, M. T., & Camacho-Collados, J. \n(2020). Language models and word sense disambiguation: An \noverview and analysis. ArXiv Preprint. arXiv:2008.11608\nLupyan, G., & Winter, B. (2018). Language is more abstract than you \nthink, or, why aren't languages more iconic? Philosophical Trans-\nactions of the Royal Society B: Biological Sciences, 373(1752), \nArticle 20170137.\nLupyan, G., & Lewis, M. (2019). From words-as-mappings to words-\nas-cues: The role of language in semantic knowledge. Language, \nCognition and Neuroscience, 34(10), 1319–1337.\nMcDonald, S., & Ramscar, M. (2001). Testing the distributional \nhypothesis: The influence of context on judgements of semantic \nsimilarity. Proceedings of the Annual Meeting of the Cognitive \nScience Society, 23(23).\nMcClelland, J. L., & Rumelhart, D. E. (1981). An interactive activation \nmodel of context effects in letter perception: I. An account of basic \nfindings. Psychological Review, 88(5), 375.\nMcRae, K., Cree, G. S., Seidenberg, M. S., & McNorgan, C. (2005). \nSemantic feature production norms for a large set of living and \nnonliving things. Behavior Research Methods, 37(4), 547–559.\nMichaelov, J. A., & Bergen, B. K. (2020). How well does surprisal \nexplain N400 amplitude under different experimental conditions? \nArXiv Preprint. arXiv:2010.04844\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). \nDistributed representations of words and phrases and their com-\npositionality. In M. I. Jordan, Y. LeCun, & S. A. Solla (Eds.), \nAdvances in neural information processing systems (pp. 3111–\n3119). MIT Press.\nMirman, D., Strauss, T. J., Dixon, J. A., & Magnuson, J. S. (2010). \nEffect of representational distance between meanings on recog -\nnition of ambiguous spoken words. Cognitive Science, 34(1), \n161–173.\nMoldovan, A. (2021). Descriptions and tests for polysemy. Axiomathes, \n31(3), 229–249.\nMorissette, A., & Ballard, G. (1995). Head over feet [Song]. On Jagged \nLittle Pill, Maverick.\nMorton, J. (1969). Interaction of information in word recognition. Psy-\nchological Review, 76(2), 165.\nNair, S., Srinivasan, M., & Meylan, S. (2020). Contextualized word \nembeddings encode aspects of human-like word sense knowledge. \nArXiv Preprint. arXiv:2010.13057.\nNelson, D. L., McEvoy, C. L., Walling, J. R., & Wheeler, J. W. (1980). \nThe University of South Florida homograph norms. Behavior \nResearch Methods & Instrumentation, 12(1), 16–37.\nNunberg, G. (1979). The non-uniqueness of semantic solutions: Poly-\nsemy. Linguistics and Philosophy, 3(2), 143–184.\nOuyang, L., Boroditsky, L., & Frank, M. C. (2017). Semantic coher -\nence facilitates distributional learning. Cognitive Science, 41 , \n855–884.\nPacht, J. M., & Rayner, K. (1993). The processing of homophonic \nhomographs during reading: Evidence from eye movement stud-\nies. Journal of Psycholinguistic Research, 22(2), 251–271.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., \n& Zettlemoyer, L. (2018). Deep contextualized word representa-\ntions. ArXiv Preprint. arXiv:1802.05365.\nPustejovsky, J. (1998). The generative lexicon. MIT Press.\nR Core Team. (2020). R: A language and environment for statisti-\ncal computing. R Foundation for Statistical Computing. https://  \nwww.R- proje ct. org/\nRabagliati, H., & Snedeker, J. (2013). The truth about chickens and \nbats: Ambiguity avoidance distinguishes types of polysemy. Psy-\nchological Science, 24(7), 1354–1360.\nRayner, K., & Duffy, S. A. (1986). Lexical complexity and fixation \ntimes in reading: Effects of word frequency, verb complexity, and \nlexical ambiguity. Memory & Cognition, 14(3), 191–201.\nRayner, K., Pacht, J. M., & Duffy, S. A. (1994). Effects of prior encoun-\nter and global discourse bias on the processing of lexically ambig-\nuous words—Evidence from eye fixations. Journal of Memory \nand Language, 33(4), 527–544.\nReichle, E. D., Pollatsek, A., & Rayner, K. (2007). Modeling the effects \nof lexical ambiguity on eye movements during reading. In R. P. \nG. van Gompel, M. H. Fischer, W. S. Murray, & R. L. Hill (Eds.), \nEye movements (pp. 271–292). Elsevier.\nRodd, J. (2018). Lexical ambiguity. Oxford handbook of psycholinguis-\ntics (pp. 120–144). Oxford University Press.\nRodd, J., Gaskell, G., & Marslen-Wilson, W. (2002). Making sense \nof semantic ambiguity: Semantic competition in lexical access. \nJournal of Memory and Language, 46(2), 245–266.\nRogers, A., Kovaleva, O., & Rumshisky, A. (2020). A primer in BER-\nTology: What we know about how BERT works. Transactions of \nthe Association for Computational Linguistics, 8, 842–866.\nSahlgren, M. (2008). The distributional hypothesis. Italian Journal of \nDisability Studies, 20, 33–53.\nSennet, A. (2021). Ambiguity. In E. N. Zalta (Ed.), The Stanford ency-\nclopedia of philosophy (Fall 2021 ed.). https:// plato. stanf ord. edu/ \narchi ves/ fall2 021/ entri es/ ambig uity/\nSereno, S. C. (1995). Resolution of lexical ambiguity: evidence from an \neye movement priming paradigm. Journal of Experimental Psy -\nchology: Learning, Memory, and Cognition, 21(3), 582.\nSereno, S. C., Brewer, C. C., & O’Donnell, P. J. (2003). Context effects \nin word recognition: evidence for early interactive processing. \nPsychological Science, 14(4), 328–333.\nSheridan, H., Reingold, E. M., & Daneman, M. (2009). Using puns \nto study contextual influences on lexical ambiguity resolution: \nEvidence from eye movements. Psychonomic Bulletin & Review, \n16(5), 875–881.\nSrinivasan, M., & Rabagliati, H. (2015). How concepts and conven-\ntions structure the lexicon: Cross-linguistic evidence from poly -\nsemy. Lingua, 157, 124–152.\nSwaab, T., Brown, C., & Hagoort, P. (2003). Understanding words in \nsentence contexts: The time course of ambiguity resolution. Brain \nand Language, 86(2), 326–343.\nSwinney, D.A. (1979). Lexical access during sentence comprehension: \n(Re)consideration of context effects. Journal of Verbal Learning \nand Verbal Behavior, 18(6), 645–659.\nSwinney, D. (1981). The process of language comprehension: An \napproach to examining issues in cognition and language. Cogni-\ntion, 10(1/3), 307–312.\nTaieb, M. A. H., Zesch, T., & Aouicha, M. B. (2020). A survey of \nsemantic relatedness evaluation datasets and procedures. Artificial \nIntelligence Review, 53(6), 4407–4448.\nTenney, I., Das, D., & Pavlick, E. (2019). BERT rediscovers the classi-\ncal NLP pipeline. ArXiv Preprint. arXiv:1905.05950.\nTrott, S., & Bergen, B. (2021). RAW-C: Relatedness of ambiguous \nwords—In context (A new lexical resource for English). ArXiv \nPreprint. arXiv:2105.13266.\nTuggy, D. (1993). Ambiguity, polysemy, and vagueness. Cognitive \nLinguistics, 4(3), 273–290.\nTwilley, L. C., Dixon, P., Taylor, D., & Clark, K. (1994). University of \nAlberta norms of relative meaning frequency for 566 homographs. \nMemory & Cognition, 22(1), 111–126.\nViebahn, E. (2018). Ambiguity and zeugma. Pacific Philosophical \nQuarterly, 99(4), 749–762.\nVu, H., & Kellas, G. (1999). Contextual strength modulates the sub-\nordinate bias effect: Reply to Rayner, Binder, and Duffy. The \n1557Behavior Research Methods (2023) 55:1537–1557 \n1 3\nQuarterly Journal of Experimental Psychology: Section A, 52(4), \n853–855.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., \nCistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Schleifer, \nS., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., \nGugger, S., Drame, M., Lhoest, Q., Rush, A. (2020). Transform-\ners: State-of-the-art natural language processing. Proceedings of \nthe 2020 Conference on Empirical Methods in Natural Language \nProcessing: System Demonstrations (pp. 38–45). Association for \nComputational Linguistics.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., \nKlingner, J., Shah, AP., Johnson, M., Liu, X., Kaiser, L., Gouws, \nS., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, \nN., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vin-\nyals, O., Corrado, G., Hughes, M., Dean, J. (2016). Google’s \nneural machine translation system: Bridging the gap between \nhuman and machine translation (pp. 1–23). http:// arxiv. org/ abs/ \n1609. 08144\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, \nQ. V. (2019). Xlnet: Generalized autoregressive pretraining for \nlanguage understanding. Advances in Neural Information Pro-\ncessing Systems, 32.\nYurchenko, A., Lopukhina, A., & Dragoy, O. (2020). Metaphor is \nbetween metonymy and homonymy: Evidence from event-related \npotentials. Frontiers in Psychology, 11, 2113.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Ambiguity",
  "concepts": [
    {
      "name": "Ambiguity",
      "score": 0.6759755611419678
    },
    {
      "name": "Psychology",
      "score": 0.5425698161125183
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5417492985725403
    },
    {
      "name": "Linguistics",
      "score": 0.5362702012062073
    },
    {
      "name": "Natural language processing",
      "score": 0.5280773639678955
    },
    {
      "name": "Dominance (genetics)",
      "score": 0.5077701210975647
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43783241510391235
    },
    {
      "name": "Computer science",
      "score": 0.437358558177948
    },
    {
      "name": "Word (group theory)",
      "score": 0.4354037642478943
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3934014141559601
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}