{
  "title": "GENERATIVE PRE-TRAINED TRANSFORMER 3",
  "url": "https://openalex.org/W4318591505",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4318592549",
      "name": "Олександр Іванович ГОЛУБЕНКО",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4318592550",
      "name": "Олександр Олександрович ПІДМОГИЛЬНИЙ",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4309394263",
    "https://openalex.org/W4309137725",
    "https://openalex.org/W4307936861",
    "https://openalex.org/W4385574031",
    "https://openalex.org/W4310997991",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W4312091845",
    "https://openalex.org/W4311991135",
    "https://openalex.org/W4308193385"
  ],
  "abstract": "GPT (Generative Pre-training Transformer) — це тип штучного інтелекту (AI), який використовує алгоритми машинного навчання для створення тексту природною мовою. Перша версія GPT, випущена в 2018 році, стала революційним досягненням у сфері ШІ та обробки природної мови (NLP). Однак він також мав деякі обмеження та проблеми, які були розглянуті в наступних версіях моделі.&#x0D; Однією з головних проблем першої версії GPT була відсутність контролю над контентом, який вона генерувала. Модель було навчено на великому наборі даних тексту, створеного людиною, і вона змогла створити зв’язний і, здавалося б, людиноподібний текст на широкий спектр тем. Однак він часто створював текст, який був упередженим, образливим або іншим чином недоречним, оскільки він не міг повністю зрозуміти контекст або значення використаних слів.&#x0D; Іншою проблемою першої версії GPT була її нездатність виконувати складніші завдання NLP, такі як переклад або конспектування. Хоча він міг створити зв’язний текст, він не міг зрозуміти значення чи структуру тексту так, як це може зробити людина.&#x0D; Подальші версії GPT, такі як GPT-2 і GPT-3, вирішували ці проблеми та додавали нові можливості, такі як здатність виконувати складніші завдання NLP і генерувати більш зв’язний і відповідний контексту текст. Однак вони все ще мають обмеження і можуть давати необ’єктивні або невідповідні результати, якщо не використовувати їх відповідально.",
  "full_text": "Науковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n19\nУДК 004.942\nDOI: https://doi.org/10.53920/ITS-2022-2-2\nОлександр Іванович ГОЛУБЕНКО,\nканд. техн. наук, доцент\nЗВО «Міжнародний науково-технічний університет\nімені академіка Юрія Бугая»\nORCID ID: 0000-0002-1776-5160\nОлександр Олександрович ПІДМОГИЛЬНИЙ,\nаспірант\nДержавний університет телекомунікацій\nORCID ID: 0000-0001-8689-2086\nGENERATIVE PRE-TRAINED TRANSFORMER 3\nGPT (Generative Pre-training Transformer) — це тип штучного ін -\nтелекту (AI), який використовує алгоритми машинного навчання для \nстворення тексту природною мовою. Перша версія GPT, випущена в \n2018 році, стала революційним досягненням у сфері ШІ та обробки \nприродної мови (NLP). Однак він також мав деякі обмеження та про-\nблеми, які були розглянуті в наступних версіях моделі.\nОднією з головних проблем першої версії GPT була відсутність \nконтролю над контентом, який вона генерувала. Модель було нав -\nчено на великому наборі даних тексту, створеного людиною, і вона \nзмогла створити зв’язний і, здавалося б, людиноподібний текст на \nширокий спектр тем. Однак він часто створював текст, який був упе-\nредженим, образливим або іншим чином недоречним, оскільки він \nне міг повністю зрозуміти контекст або значення використаних слів.\nІншою проблемою першої версії GPT була її нездатність викону-\nвати складніші завдання NLP, такі як переклад або конспектування. \nХоча він міг створити зв’язний текст, він не міг зрозуміти значення \nчи структуру тексту так, як це може зробити людина.\nПодальші версії GPT, такі як GPT-2 і GPT-3, вирішували ці проблеми \nта додавали нові можливості, такі як здатність виконувати складніші \nзавдання NLP і генерувати більш зв’язний і відповідний контексту текст. \nОднак вони все ще мають обмеження і можуть давати необ’єктивні або \nневідповідні результати, якщо не використовувати їх відповідально.\nКлючові слова: штучний інтелект (AI), машинне навчання, обробка \nприродної мови (NLP), генеративний передтренувальний трансформа -\nтор (GPT), генерація тексту, глибоке навчання, нейронна мережа.\nНауковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n20\nOleksandr GOLUBENKO\nCandidate of technical sciences, associate professor\nIHE «Academician Yuri Bugay\ninternational science and technical university»\nORCID ID: 0000-0002-1776-5160\nOleksandr PIDMOGYLNYI\nPostgraduate\nState University of Telecommunications\nORCID ID: 0000-0001-8689-2086\nGENERATIVE PRE-TRAINED TRANSFORMER 3\nGPT (Generative Pre-training Transformer) is a type of artificial in -\ntelligence (AI) that uses machine learning algorithms to generate text in \nnatural language. The first version of GPT, released in 2018, was a rev -\nolutionary breakthrough in AI and natural language processing (NLP). \nHowever, it also had some limitations and issues that were addressed in \nsubsequent versions of the model.\nOne of the main problems with the first version of GPT was the lack \nof control over the content it generated. The model was trained on a large \ndataset of human-generated text and was able to generate coherent and \nseemingly human-like text on a wide range of topics. However, he often \nproduced text that was biased, offensive, or otherwise inappropriate be-\ncause he could not fully understand the context or meaning of the words \nused.\nAnother problem with the first version of GPT was its inability to \nhandle more complex NLP tasks such as translation or annotation. Al -\nthough he could produce coherent text, he could not understand the \nmeaning or structure of the text as a human could.\nLater versions of GPT, such as GPT-2 and GPT-3, addressed these \nissues and added new capabilities, such as the ability to perform more \ncomplex NLP tasks and generate more coherent and context-appropri -\nate text. However, they still have limitations and can produce biased or \ninconsistent results if not used responsibly.\nKeywords: artificial intelligence (AI), machine learning, natural \nlanguage processing (NLP), generative pretraining transformer (GPT), text \ngeneration, deep learning, neural network.\nПостановка проблеми. Generative Pre-trained Transformer 3 \n(GPT-3) — це модель штучного інтелекту (AI) для генерації мови, \nНауковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n21\nрозроблена OpenAI, яка привернула значну увагу як ЗМІ, так і тех-\nнічної спільноти. Маючи 175 мільярдів параметрів, GPT-3 наразі є \nнайбільшою та найпотужнішою мовною моделлю з існуючих, а її \nможливості виходять далеко за рамки простого генерування тек-\nсту, що потребує детального аналізу та досліджень.\nАналіз останніх досліджень і публікацій. GPT-3 — це тип \nмовної моделі на основі Transformer це означає, що він використо-\nвує архітектуру transformer для обробки та генерації тексту. Ар -\nхітектура трансформатора базується на ідеї самоуважності, що \nдозволяє моделі обробляти вхідні послідовності паралельно, а не \nпослідовно. Це дає змогу моделі фіксувати довгострокові залеж -\nності та зв’язки між словами в реченні, що важливо для створення \nзв’язного тексту.\nНа додаток до самоуважності, мовні моделі на основі тран -\nсформаторів також використовують інші методи, такі як моделю -\nвання замаскованої мови та динамічне керування. Моделювання \nзамаскованої мови передбачає маскування частини вхідного тек-\nсту та передбачення відсутніх слів на основі контексту, наданого \nнезамаскованими словами. Це допомагає моделі навчитися ро -\nзуміти зв’язки між словами та створювати текст, який є зв’язним і \nмає сенс у контексті.\nДинамічний контроль, з іншого боку, дозволяє моделі регулю-\nвати рівень деталізації та конкретності вихідних даних на основі \nвхідних даних. Це дозволяє моделі створювати текст, який підхо-\nдить для даного контексту та завдання.\nМета статті  – дослідження технічних особливостей GPT-3, \nаналіз потенційного застосування та впливу, а також розгляд дея-\nких етичних проблем, пов’язаних з його використанням.\nВиклад основного матеріалу. Архітектура трансформа -\nтора була представлена в оригінальній моделі GPT, але GPT-3 \nробить крок далі, використовуючи попередньо навчену версію \nмоделі. Попереднє навчання передбачає навчання моделі на ве -\nликому наборі даних і подальше її тонке налаштування для кон -\nкретного завдання, наприклад перекладу мови або відповідей \nна запитання.\nОднією з ключових особливостей GPT-3 є його здатність ге -\nнерувати зв’язний і схожий на людину текст. Це досягається за \nрахунок використання механізмів уваги, які дозволяють моделі \nвраховувати контекст і зв’язки між словами в реченні. GPT-3 та -\nкож має можливість виконувати широкий спектр мовних завдань, \nНауковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n22\nвключаючи переклад, узагальнення та відповіді на запитання, \nі навіть може виконувати завдання програмування [1].\nАрхітектура трансформатора, що використовується в GPT-3, \nбазується на ідеї самоуважності, яка дозволяє моделі обробляти \nвхідні послідовності паралельно, а не послідовно. Це дозволяє \nмоделі фіксувати довгострокові залежності та зв’язки між слова -\nми в реченні, що важливо для створення зв’язного тексту.\nGPT-3 також використовує техніку під назвою масковане \nмоделювання мови, яка передбачає маскування частини вхід -\nного тексту та передбачення відсутніх слів на основі контексту, \nнаданого незамаскованими словами. Щоб реалізувати маско -\nване моделювання мови, частина вхідного тексту вибирається \nвипадковим чином і замінюється спеціальним маркером, таким \nяк «[MASK]». Потім модель навчається передбачати пропущене \nслово або слова на основі контексту, наданого незамасковани -\nми словами. Наприклад, враховуючи вхідні дані «[МАСКА] си -\nділа на килимку», модель буде навчена передбачати пропуще -\nне слово «кіт» на основі контексту, наданого іншими словами \nв реченні. Це допомагає моделі навчитися розуміти зв’язки між \nсловами та створювати текст, який є зв’язним і має сенс у кон -\nтексті [2].\nНа додаток до цих методів GPT-3 також використовує техні -\nку, яка називається динамічним керуванням, яка дозволяє моде -\nлі регулювати рівень деталізації та конкретності вихідних даних \nна основі вхідних даних. Модуль динамічного керування може \nбути реалізований за допомогою різноманітних методів, таких як \nTransformer encoder або окрема нейронна мережа прямого зв’яз-\nку [5]. Transformer encoder в модулі керування обробляє вхідні \nдані та генерує керуючий сигнал, який використовується для на -\nлаштування рівня деталізації та конкретності на виході.\nДинамічний контроль дозволяє GPT-3 генерувати текст, який \nвідповідає заданому контексту та завданню. Наприклад, якщо \nвведенням є запит на детальний опис особи, модуль керування \nгенеруватиме керуючий сигнал, який повідомляє моделі створити \nбільш детальний і конкретний опис. Якщо введенням є підказка \nіз запитом про загальний огляд теми, модуль керування згенерує \nкеруючий сигнал, який скаже моделі створити більш загальний \nі високорівневий опис. Це дозволяє GPT-3 генерувати текст, який \nпідходить для заданого контексту та завдання.\nНауковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n23\nGPT-3 також може включати зовнішні знання у свій вихід, що \nдозволяє генерувати текст, який є фактично точним та інформа -\nтивним. Це досягається завдяки використанню трансформатора \nзнань, який є окремим компонентом моделі, яка навчається на ве-\nликому наборі даних із багатим на знання текстом.\nGPT-3 був навчений на великому наборі текстових даних \nпід назвою Internet Archive Books dataset. Цей набір даних склада-\nється з понад 8 мільйонів книг та інших текстів, які були оцифро -\nвані Інтернет-архівом, некомерційною організацією, яка працює \nнад збереженням і наданням доступу до творів культури. Набір \nданих включає широкий спектр текстів, включаючи книги, статті \nта веб-сайти, і охоплює широкий діапазон мов і тем.\nОкрім набору даних Internet Archive Books, GPT-3 також на -\nвчався на інших наборах даних, включаючи англійську Вікіпедію \nта набір даних WebText, який складається з тексту, взятого з Інтер-\nнету [4]. Комбінація цих наборів даних дозволяє GPT-3 навчатися \nна різноманітних і репрезентативних зразках тексту, що допома -\nгає генерувати високоякісний і зв’язний результат.\nПотенційне застосування та вплив\nGPT-3 має потенціал для революції в обробці природної мови \nта штучному інтелекті, і він уже використовується для широкого \nспектру програм. Деякі з найбільш перспективних потенційних \nзастосувань GPT-3 включають:\n• Чат-боти: GPT-3 можна використовувати для створення \nчат-ботів, які можуть вести природні розмови з користу -\nвачами. Це може мати широкий спектр застосувань, на -\nприклад, обслуговування клієнтів, освіта та розваги.\n• Мовний переклад: GPT-3 може виконувати мовний пере -\nклад з високою точністю, що може мати значні наслідки \nдля глобального спілкування та співпраці.\n• Створення вмісту: GPT-3 має можливість генерувати текст, \nсхожий на людину, який можна використовувати для ав -\nтоматизації створення вмісту для веб-сайтів, соціальних \nмереж та інших платформ.\n• Композиція музики: GPT-3 навіть використовувався для \nстворення музики, демонструючи його здатність розумі -\nти та генерувати складні моделі.\nОдним із ключових застосувань GPT-3 є створення контенту [3]. \nМодель може генерувати високоякісні статті, публікації в блогах і со-\nНауковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n24\nціальних мережах, які можуть бути корисними для підприємств і ор-\nганізацій, які прагнуть швидко й ефективно створювати вміст. GPT-3 \nтакож можна використовувати для більш творчих додатків, таких як \nстворення музики та написання історій [6].\nОкрім можливостей створення мови, GPT-3 також має здат -\nність виконувати такі завдання, як очищення та форматування \nданих, що може бути корисним для підприємств і організацій, які \nобробляють великі набори даних.\nНезважаючи на свої вражаючі можливості, GPT-3 не позбав -\nлений обмежень. Одним із потенційних обмежень GPT-3 є його \nзалежність від даних, на яких він навчався [7]. Якщо навчаль -\nні дані містять упереджений або невідповідний вміст, модель \nможе створити упереджений або невідповідний вихід. Важли -\nво ретельно розглянути різноманітність і якість даних, які ви -\nкористовуються для навчання GPT-3, і переконатися, що вони є \nрепрезентативними для населення або завдання, для якого вони \nвикористовуються.\nІншим потенційним обмеженням GPT-3 є його вартість і вимо-\nги до ресурсів. Модель вимагає великої кількості обчислювальних \nресурсів і даних для ефективного навчання, що може зробити її \nдорогим і ресурсомістким у використанні [8]. Це може обмежити \nйого доступність для деяких організацій.\nВисновки та пропозиції. Загалом GPT-3 — це потужна та уні-\nверсальна мовна модель, яка здатна виконувати широкий спектр \nмовних завдань і генерувати високоякісний текст. Однак важливо \nретельно розглянути потенційні обмеження та етичні міркування \nмоделі, використовуючи її для конкретних цілей.\nЄ кілька ключових особливостей і характеристик GPT-3, які \nроблять його унікальним і потужним інструментом для дослі -\nджень НЛП і ШІ. Ось деякі з основних ключів до розуміння GPT-3:\n• Великий масштаб: GPT-3 є однією з найбільших мовних \nмоделей, коли-небудь розроблених, із 175 мільярдами па-\nраметрів. Це дозволяє генерувати високоякісний текст і \nвиконувати складні завдання NLP з рівнем продуктивнос-\nті, якого важко досягти з меншими моделями.\n• Трансформаторна архітектура: GPT-3 використовує тран-\nсформаторну архітектуру, яка є типом нейронної мережі, \nяка широко використовується в завданнях НЛП. Архітек -\nтура трансформатора дозволяє GPT-3 обробляти введе -\nНауковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n25\nний текст більш ефективним і результативним способом, \nдозволяючи генерувати текст, який є більш зв’язним і від-\nповідним контексту.\n• Моделювання замаскованої мови (MLM): GPT-3 вико -\nристовує техніку під назвою моделювання замаскованої \nмови (MLM), яка передбачає маскування частини вхідно -\nго тексту та прогнозування відсутніх слів на основі навко-\nлишнього контексту. Це допомагає GPT-3 дізнатися про \nзв’язки між словами та структурою мови.\n• Динамічне керування: GPT-3 представляє нову техніку, \nяка називається динамічним керуванням, яка дозволяє \nмоделі адаптувати свої результати на основі контексту чи \nпоточного завдання. Це дозволяє GPT-3 генерувати текст, \nякий є більш актуальним і підходить для різних ситуацій.\nGPT-3 є потужним і універсальним інструментом, який має по-\nтенціал зробити революцію в дослідженнях НЛП та ШІ. Його ве -\nликий масштаб і розширені можливості роблять його цінним ре -\nсурсом для дослідників і розробників, які працюють над широким \nспектром програм і проектів.\n© Голубенко О.І., Підмогильний О.О., 2022\nЛІТЕРАТУРА\n1. Language models are better than humans at next-token \nprediction Authors: Buck Shlegeris, Fabien Roger, Lawrence Chan, \nEuan McLean. arXiv:2212.10560.\n2. Self-Instruct: Aligning Language Model with Self Generated \nInstructions. Authors: Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, \nAlisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. \narXiv:2212.10509.\n3. Interleaving Retrieval with Chain-of-Thought Reasoning \nfor Knowledge-Intensive Multi-Step Questions Authors: Harsh \nTrivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal. \narXiv:2212.08072.\n4. Foresight -- Deep Generative Modelling of Patient Timelines \nusing Electronic Health Records Authors: Zeljko Kraljevic, Dan Bean, \nAnthony Shek, Rebecca Bendayan, Joshua Au Yeung, Alexander Deng, \nAlfie Baston, Jack Ross, Esther Idowu, James T Teo, Richard J Dobson. \narXiv:2212.04037.\nНауковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n26\n5. Demystifying Prompts in Language Models via Perplexity \nEstimation Authors: Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, \nLuke Zettlemoyer. arXiv:2211.09267.  \n6. Reflect, Not Reflex: Inference-Based Common Ground \nImproves Dialogue Response Quality.\nAuthors: Pei Zhou, Hyundong Cho, Pegah Jandaghi, Dong-Ho Lee, \nBill Yuchen Lin, Jay Pujara, Xiang Ren. arXiv:2211.07615.\n7. UGIF: UI Grounded Instruction Following. Authors: Sagar \nGubbi Venkatesh, Partha Talukdar, Srini Narayanan. arXiv:2210.17497.  \n8. Leveraging Pre-trained Models for Failure Analysis Triplets \nGeneration. Authors: Kenneth Ezukwoke, Anis Hoayek, Mireille \nBatton-Hubert, Xavier Boucher, Pascal Gounet, Jerome Adrian. \narXiv:2210.17238.\nREFERENCES\n1. Language models are better than humans at next-token \nprediction Authors: Buck Shlegeris, Fabien Roger, Lawrence Chan, \nEuan McLean. arXiv:2212.10560\n2. Self-Instruct: Aligning Language Model with Self Generated \nInstructions. Authors: Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, \nAlisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. \narXiv:2212.10509\n3. Interleaving Retrieval with Chain-of-Thought Reasoning \nfor Knowledge-Intensive Multi-Step Questions Authors: Harsh \nTrivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal. \narXiv:2212.08072\n4. Foresight -- Deep Generative Modelling of Patient Timelines \nusing Electronic Health Records Authors: Zeljko Kraljevic, Dan Bean, \nAnthony Shek, Rebecca Bendayan, Joshua Au Yeung, Alexander Deng, \nAlfie Baston, Jack Ross, Esther Idowu, James T Teo, Richard J Dobson. \narXiv:2212.04037\n5. Demystifying Prompts in Language Models via Perplexity \nEstimation Authors: Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, \nLuke Zettlemoyer. arXiv:2211.09267  \n6. Reflect, Not Reflex: Inference-Based Common Ground \nImproves Dialogue Response Quality Authors: Pei Zhou, Hyundong \nCho, Pegah Jandaghi, Dong-Ho Lee, Bill Yuchen Lin, Jay Pujara, Xiang \nRen. arXiv:2211.07615\nНауковий журнал «IT SYNERGY», 2022, випуск 2 (3)\n27\n7. UGIF: UI Grounded Instruction Following. Authors: Sagar \nGubbi Venkatesh, Partha Talukdar, Srini Narayanan. arXiv:2210.17497  \n8. Leveraging Pre-trained Models for Failure Analysis Triplets \nGeneration. Authors: Kenneth Ezukwoke, Anis Hoayek, Mireille \nBatton-Hubert, Xavier Boucher, Pascal Gounet, Jerome Adrian. \narXiv:2210.17238\nСТАТТЯ НАДІЙШЛА ДО РЕДАКЦІЇ 23.11.2022",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8073292374610901
    },
    {
      "name": "Generative grammar",
      "score": 0.6847354173660278
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5782716274261475
    },
    {
      "name": "Computer science",
      "score": 0.5151852369308472
    },
    {
      "name": "Natural language processing",
      "score": 0.4896243214607239
    },
    {
      "name": "Engineering",
      "score": 0.17004773020744324
    },
    {
      "name": "Electrical engineering",
      "score": 0.06947115063667297
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}