{
  "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models",
  "url": "https://openalex.org/W4385570473",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2901480933",
      "name": "Julia Mendelsohn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967926312",
      "name": "Ronan Le Bras",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133417374",
      "name": "Yejin Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2170935826",
      "name": "Maarten Sap",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2397944029",
    "https://openalex.org/W3123652978",
    "https://openalex.org/W3116423999",
    "https://openalex.org/W3022516804",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288700724",
    "https://openalex.org/W2763679290",
    "https://openalex.org/W2965794172",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W4214848916",
    "https://openalex.org/W3216230448",
    "https://openalex.org/W3166642164",
    "https://openalex.org/W2135017311",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3167384116",
    "https://openalex.org/W3176015097",
    "https://openalex.org/W3129830916",
    "https://openalex.org/W3117768806",
    "https://openalex.org/W3155472365",
    "https://openalex.org/W2963992732",
    "https://openalex.org/W1985949226",
    "https://openalex.org/W2954462910",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3100718630",
    "https://openalex.org/W3175487198",
    "https://openalex.org/W4287252377",
    "https://openalex.org/W3045967779",
    "https://openalex.org/W4385571749",
    "https://openalex.org/W3102825016",
    "https://openalex.org/W3195432409",
    "https://openalex.org/W3113763975",
    "https://openalex.org/W2016387120",
    "https://openalex.org/W3201622928",
    "https://openalex.org/W3174661351",
    "https://openalex.org/W4235286696",
    "https://openalex.org/W1973704580",
    "https://openalex.org/W2007190116",
    "https://openalex.org/W4287888454",
    "https://openalex.org/W2468889841",
    "https://openalex.org/W3026526919",
    "https://openalex.org/W2888448279",
    "https://openalex.org/W2963061684",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W2797630160",
    "https://openalex.org/W2611235126",
    "https://openalex.org/W2970395295",
    "https://openalex.org/W2143678067"
  ],
  "abstract": "Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word \"cosmopolitan\" in a sentence such as \"we need to end the cosmopolitan experiment\" can mean \"worldly\" to many but also secretly mean \"Jewish\" to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians' speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3's performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15162‚Äì15180\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nFrom Dogwhistles to Bullhorns:\nUnveiling Coded Rhetoric with Language Models\nJulia Mendelsohn‚ô¢‚àó Ronan Le Bras‚ô£ Yejin Choi‚ô†‚ô£ Maarten Sap‚ô°‚ô£\n‚ô¢University of Michigan School of Information ‚ô£Allen Institute for AI\n‚ô†Paul G. Allen School of Computer Science & Engineering, University of Washington\n‚ô°Language Technologies Institute, Carnegie Mellon University\n/envel‚å¢pejuliame@umich.edu /gl‚å¢bedogwhistles.allen.ai\nAbstract\nWarning: content in this paper may be upset-\nting or offensive to some readers.\nDogwhistles are coded expressions that simul-\ntaneously convey one meaning to a broad audi-\nence and a second one, often hateful or provoca-\ntive, to a narrow in-group; they are deployed\nto evade both political repercussions and algo-\nrithmic content moderation. For example, in\nthe sentence ‚Äúwe need to end the cosmopolitan\nexperiment,‚Äù the word ‚Äú cosmopolitan‚Äù likely\nmeans ‚Äúworldly‚Äù to many, but secretly means\n‚ÄúJewish‚Äù to a select few. We present the first\nlarge-scale computational investigation of dog-\nwhistles. We develop a typology of dogwhis-\ntles, curate the largest-to-date glossary of over\n300 dogwhistles with rich contextual informa-\ntion and examples, and analyze their usage in\nhistorical U.S. politicians‚Äô speeches. We then\nassess whether a large language model (GPT-\n3) can identify dogwhistles and their mean-\nings, and find that GPT-3‚Äôs performance varies\nwidely across types of dogwhistles and targeted\ngroups. Finally, we show that harmful content\ncontaining dogwhistles avoids toxicity detec-\ntion, highlighting online risks of such coded\nlanguage. This work sheds light on the theo-\nretical and applied importance of dogwhistles\nin both NLP and computational social science,\nand provides resources for future research in\nmodeling dogwhistles and mitigating their on-\nline harms.\n1 Introduction\nThe cosmopolitan elite look down on the com-\nmon affections that once bound this nation to-\ngether: things like place and national feeling and\nreligious faith. . . The cosmopolitan agenda has\ndriven both Left and Right. . . It‚Äôs time we ended\nthe cosmopolitan experiment and recovered the\npromise of the republic.\n‚ÄìJosh Hawley (R-MO), 2019\n‚àóWork done while interning at the Allen Institute for AI.\nFigure 1: Schematic of how dogwhistles work, based\non Henderson and McCready (2018) with the exam-\nple of cosmopolitan. First, a speaker simultaneously\ncommunicates the dogwhistle message and their per-\nsona (identity). The in-group recovers both the message\ncontent and speaker persona, enabling them to arrive at\nthe coded meaning (e.g. Jewish). The out-group only\nrecognizes the message‚Äôs content and thus interprets it\nliterally. This literal meaning also provides the speaker\nwith plausible deniability; if confronted, the speaker can\nclaim that they solely intended the literal meaning.\nWe have got this tailspin of culture, in our\ninner cities in particular, of men not working and\njust generations of men not even thinking about\nworking or learning to value the culture of work.\n‚ÄìPaul Ryan (R-WI), 2014\nCosmopolitan and inner city are examples of\ndogwhistles, expressions that ‚Äúsend one message\nto an out-group and a second (often taboo, contro-\nversial, or inflammatory) message to an in-group‚Äù\n(Henderson and McCready, 2018). Many listen-\ners would believe that Hawley is simply criticizing\nwell-traveled or worldly people, but others recog-\nnize it as an attack on the Jewish people. Similarly,\nmany assume that Ryan is discussing issues within\na geographic location, but others hear a pernicious\nstereotype of Black men as lazy. Crucially, Hawley\nand Ryan can avoid alienating the out-group by\nmaintaining plausible deniability: they never ex-\nplicitly say ‚ÄúJewish‚Äù or ‚ÄúBlack‚Äù, so they can reject\naccusations of racism (Haney-L√≥pez, 2014).\nBecause dogwhistles can bolster support for par-\n15162\nticular policies or politicians among the in-group\nwhile avoiding social or political backlash from\nthe out-group, they are a powerful mechanism of\npolitical influence (Mendelberg, 2001; Goodin and\nSaward, 2005). For example, racist dogwhistles\nsuch as states‚Äô rights and law and order were part\nof the post-Civil Rights Republican Southern Strat-\negy to appeal to white Southerners, a historically\nDemocratic bloc (Haney-L√≥pez, 2014). Despite po-\nlarization and technology that enables message tar-\ngeting to different audiences, dogwhistles are still\nwidely used by politicians (Haney-L√≥pez, 2014;\nTilley et al., 2020) and civilians in online conversa-\ntions (Bhat and Klein, 2020; √Ökerlund, 2021).\nBeyond political science, research on dogwhis-\ntles is urgent and essential for NLP, but they remain\na challenge to study. Dogwhistles are actively and\nintentionally deployed to evade automated content\nmoderation, especially hate speech detection sys-\ntems (Magu et al., 2017). They may also have harm-\nful unseen impacts in other NLP systems by infil-\ntrating data used for pretraining language models.\nHowever, researchers face many difficulties. First,\nunless they are a part of the in-group, researchers\nmay be completely unaware of a dogwhistle‚Äôs ex-\nistence. Second, dogwhistles‚Äô meanings cannot\nbe determined by form alone, unlike most overt\nhateful or toxic language. Rather, their interpre-\ntation relies on complex interplay of different fac-\ntors (context, personae, content, audience identities,\netc.; Khoo, 2017; Henderson and McCready, 2018,\n2019; Lee and Kosse, 2020), as illustrated in Fig-\nure 1. Third, since their power is derived from\nthe differences between in-group and out-group in-\nterpretations, dogwhistles continuously evolve in\norder to avoid being noticed by the out-group.\nWe establish foundations for large-scale compu-\ntational study of dogwhistles by developing theory,\nproviding resources, and empirically analyzing\ndogwhistles in several NLP systems. Prior work\nlargely focuses on underlying mechanisms or polit-\nical effects of dogwhistle communication (Albert-\nson, 2015; Henderson and McCready, 2018) and\ntypically considers a very small number of dog-\nwhistles (often just one). To aid larger-scale efforts,\nwe first create a new taxonomy that highlights both\nthe systematicity and wide variation in kinds of\ndogwhistles (¬ß2.1). This taxonomy characterizes\ndogwhistles based on their covert meanings, style\nand register, and the personae signaled by their\nusers. We then compile a glossary of 340 dogwhis-\ntles, each of which is labeled with our taxonomy,\nrich contextual information, explanations, and real-\nworld examples with source links (¬ß2.2-2.3). As\nthis glossary is the first of its kind, we highlight\nits value with a case study of racial dogwhistles in\nhistorical U.S. Congressional Speeches (¬ß3).\nWe then apply our taxonomy and glossary to\ninvestigate how dogwhistles interact with existing\nNLP systems (¬ß4). Specifically, we evaluate the\nability of large language models (i.e. GPT-3) to\nretrieve potential dogwhistles and identify their\ncovert meanings. We find that GPT-3 has a lim-\nited capacity to recognize dogwhistles, and perfor-\nmance varies widely based on taxonomic features\nand prompt constructions; for example, GPT-3 is\nmuch worse at recognizing transphobic dogwhis-\ntles than racist ones. Finally, we show that hateful\nmessages with standard group labels (e.g. Jewish)\nreplaced with dogwhistles (e.g. cosmopolitan) are\nconsistently rated as far less toxic by a commer-\ncially deployed toxicity detection system (Perspec-\ntive API), and such vulnerabilities can exacerbate\nonline harms against marginalized groups (¬ß5).\nThis work highlights the significance of dog-\nwhistles for NLP and computational social sci-\nence, and offers resources for further research in\nrecognizing dogwhistles and reducing their harm-\nful impacts. Our glossary, code, results, GPT-\n3 outputs, and a form for adding new dogwhis-\ntles to our glossary are all available at: https:\n//dogwhistles.allen.ai.\n2 Curating a dogwhistle glossary\n2.1 Taxonomy\nBased on prior work and our own investigations,\nwe craft a new taxonomy (Figure 2). We categorize\ndogwhistles by register, type, and persona.\nRegister We label all dogwhistles as either part\nof a formal/offline or informal/online register.\nFormal/offline dogwhistles originated in offline\ncontexts or are likely to appear in statements by\nmainstream political elites (e.g. family values). The\ninformal/online register includes dogwhistles that\noriginated on the internet and are unlikely to be\nused in political speech (e.g. cuckservative).\nType I Henderson and McCready (2018) distin-\nguish dogwhistles into two types: Type I dogwhis-\ntles covertly signal the speaker‚Äôs persona but do\nnot alter the implicatures of the message itself,\nwhile Type II dogwhistles additionally alter the\n15163\nanti-Asian antisemitic climate change denier\nanti-GMO liberal racist (anti-Black)\nanti-Latinoconservative religious\nanti-liberalhomophobic transphobic\nanti-vax Islamophobic white supremacist\nDogwhistle\nRegister\nPersona\nType\nPersona signal + \nadded meaning \n(Type II)\nPersona signal \n(Type I)\nShared culture\nSymbol\nSelf-referential\nConcept\nT arget \nGroup \nLabel\nPolicy\nValues\nOther\nStereotype \ndescriptor\nStereotype \ngroup label\nPhonetic or \narbitrary label\nHumor\nBogeyman\nWar on crime\nFamily values\nNew World Order\nBix nood\nCosmopolitan\nGeorge Soros\nGoogle\nInner-city\nWonder-working power\nüíúü§çüíö\nAdult human female\nInformal (online)\nFormal (ofÔ¨Çine)\nFigure 2: Visual hierarchical representation of our dogwhistle taxonomy along with examples of each type.\nmessage‚Äôs implied meaning. We extend this typol-\nogy to highlight the wide variety of dogwhistles,\nwhich has important consequences for building a\ntheory of dogwhistles as well as future computa-\ntional modeling. We identify three subcategories\nof ‚Äúonly persona-signaling‚Äù (Type I) dogwhistles:\nsymbols (including emojis, abbreviations, and im-\nagery), self-referential terms for members of the\nin-group, and dogwhistles that require specialized\nknowledge from a shared in-group culture.\nType II Dogwhistles with an ‚Äúadded message\nmeaning‚Äù (Type II) tend to fall into two subcate-\ngories: they name a concept or serve as a substitute\nfor a target group label. We further divide con-\ncepts into policies (titles for initiatives with covert\nimplications, such as law and order), values that\nthe in-group purports to uphold, expressions whose\ncovert meanings are grounded in in-group humor,\nand other concepts, which are often coded names\nfor entities that are not group labels (e.g. the New\nWorld Order conspiracy theory is antisemitic but\ndoes not name or describe Jewish people).\nDogwhistles serve as target group labels in three\nways. Many are stereotype-based, whose interpre-\ntations rely on pre-existing associations between\nthe dogwhistle and target group; we separate these\ninto stereotype-based target group labels, which\ndirectly name the target group (e.g. cosmopoli-\ntan), while stereotype-based descriptors are less\ndirect but still refer to the target group (e.g. inner-\ncity). Others have an arbitrary or phonetic rela-\ntionship to the group label; these are commonly\nused to evade content moderation, such as ‚ÄúOpera-\ntion Google‚Äù terms invented by white supremacists\non 4chan to replace various slurs (Magu et al.,\n2017; Bhat and Klein, 2020). The final subcat-\negory, Bogeyman, includes names of people or\ninstitutions taken to represent the target group (e.g.\nGeorge Soros‚ÜîJewish, or Willie Horton‚ÜîBlack).\nPersona Persona refers to the in-group identity\nsignalled by the dogwhistle. Figure 2 lists some per-\nsonae, but this is an open class with many potential\nin-groups. There is considerable overlap in mem-\nbership of listed in-groups (e.g. white supremacists\nare often antisemitic), so we label persona based\ndirectly on explanations from sources referenced in\nour glossary (as described in 2.2). Drawing upon\nthird-wave sociolinguistics, personae are not static\nlabels or stereotypes; rather, people actively con-\nstruct and communicate personae through linguistic\nresources, such as dogwhistles (Eckert, 2008).\n2.2 Gathering dogwhistles\nWe draw from academic literature, media cover-\nage, blogs, and community-sourced wikis about\ndogwhistles, implicit appeals, and coded language.\nSince academic literature tends to focus on a small\nset of examples, we expanded our search to media\ncoverage that identifies dogwhistles in recent polit-\nical campaigns and speeches (e.g. Burack, 2020)\nor attempts to expose code words in hateful on-\nline communities (e.g. Caffier, 2017). During our\nsearch, we found several community-sourced wikis\nthat provided numerous examples of dogwhistles,\nparticularly the RationalWiki ‚ÄúAlt-right glossary‚Äù,\n‚ÄúTERF glossary‚Äù, and ‚ÄúCode word‚Äù pages.1\n2.3 Glossary contents\nOur glossary contains 340 English-language dog-\nwhistles and over 1,000 surface forms (morpho-\nlogical variants and closely-related terms), mostly\nfrom the U.S. context. Each dogwhistle is labeled\n1rationalwiki.org/wiki/{Alt-right_\nglossary,TERF_glossary,Code_word}\n15164\nDogwhistle Sex-based rights\nIn-groupmeaning Trans people threaten cis women‚Äôs rights\nPersona Transphobic\nType Concept: Value\nRegister Formal\nExplanation\nMany anti-transgender people [claim that] women‚Äôs ‚Äúsex-basedrights‚Äù are somehow being threatened, removed, weakened,eroded, or erased by transgender rights. . . ‚ÄúSex-based rights‚Äù,by the plain English meaning of those words, cannot exist in acountry that has equality law. . . it‚Äôs mostly a dog-whistle: arallying slogan much like ‚Äúfamily values‚Äù for religiousconservatives, which sounds wholesome but is a deniable andslippery code-word for a whole raft of unpleasant bigotry.\nSource Medium post by David Allsopp\nExample\nWhen so-called leftists like @lloyd_rm demand that we give upour hard-won sex-based rights, they align themselves squarelywith men‚Äôs rights activists. To both groups, female trauma iswhite noise, an irrelevance, or else exaggerated or invented.\nContext Tweet by J.K. Rowling on June 28, 2020\nTable 1: Example glossary entry for the transphobic\ndogwhistle sex-based rights\nwith its register, type, and signaled persona, an ex-\nplanation from a linked source, and at least one\nexample with linguistic, speaker, situational, and\ntemporal context included, as well as a link to the\nexample text. Table 1 shows one glossary entry for\nthe transphobic dogwhistle sex-based rights.\nAntisemitic, transphobic, and racist (mostly anti-\nBlack but sometimes generally against people of\ncolor) dogwhistles are the most common, with over\n70 entries for each persona. The glossary includes\ndogwhistles with other personae, such as homo-\nphobic, anti-Latinx, Islamophobic, anti-vax, and\nreligious. See Table A.1 in the Appendix for glos-\nsary statistics across register, type, and persona.\nBecause dogwhistles continuously evolve, we in-\ntend for this resource to be a living glossary and\ninvite the public to submit new entries or examples.\n3 Case study: racial dogwhistles in\nhistorical U.S. Congressional speeches\nWe showcase the usefulness of our glossary, with\na diachronic case study of racial dogwhistles in\npoliticians‚Äô speeches from the U.S. Congressional\nRecord (Gentzkow et al., 2019; Card et al., 2022) to\nanalyze the frequency of speeches containing racist\ndogwhistles from 1920-2020. For this case study,\nwe simply identify glossary terms based on regular\nexpressions and do not distinguish between covert\nand literal meanings of the same expressions. We\nalso measure how ideologies of speakers using dog-\nwhistles changed over time using DW-NOMINATE\n(Poole and Rosenthal, 1985), a scaling procedure\nthat places politicians on a two dimensional map\nbased on roll call voting records, such that ideo-\nlogically similar politicians are located near each\nother (Carroll et al., 2009; Lewis et al., 2023). We\nconsider the first dimension of DW-NOMINATE,\nwhich corresponds to a liberal-conservative axis.2\nAs shown in Figure 3, dogwhistle use began\nto increase during the Civil Rights Era, following\nthe 1954 Brown vs. Board of Education Supreme\nCourt decision mandating racial integration of pub-\nlic schools. This aligns with qualitative accounts of\nthe Republican Southern Strategy: because explicit\nracism was no longer acceptable, politicians turned\nto dogwhistles to make the same appeals implicitly,\nparticularly aiming to gain the support of white\nvoters in the Southern United States (Mendelberg,\n2001). Their frequency continued to increase from\nthe 1970s through the 1990s, paralleling Haney-\nL√≥pez (2014)‚Äôs account of dogwhistles during the\nNixon, Reagan, Bush Sr., and Clinton presiden-\ncies. Since the 1990s, the frequency of racial dog-\nwhistles has fluctuated but remained high. Like\nHaney-L√≥pez (2014), we qualitatively observe that\nthe dogwhistles invoked post-9/11 have shifted to-\nwards being more Islamophobic and anti-Latinx\nrather than exclusively anti-Black. We caution that\nthis case study and Figure 3 do not make novel\nclaims; rather, our goal is to show that even a naive\napplication of our glossary illustrates qualitatively\nwell-established historical patterns in U.S. politics.\nFigure 4 shows how the average ideologies of\nspeakers who use particular dogwhistles (property\nrights, thug, welfare reform, hardworking Amer-\nicans, and Willie Horton) have shifted over time,\nand reveals interesting insights into the evolution\nand lifecycle of dogwhistles. Most racial dogwhis-\ntles in the U.S. Congressional Speeches have be-\ncome increasingly associated with more conserva-\ntive speakers over time. However, the inflection\npoint when speaker ideologies shift varies across\ndogwhistles, suggesting that they emerged as dog-\nwhistles at different points. For example, property\nrights became increasingly associated with more\nconservative speakers since the 1960s, while the\naverage ideology of speakers using welfare reform\n2The second dimension captures salient cross-cutting is-\nsues, and some argue that this dimension primarily captures\nrace relations (Poole and Rosenthal, 1985). However, the sec-\nond dimension‚Äôs interpretation is less clear as the vast majority\nof voting variation is along the first dimension, and is often\nignored by political scientists (Bateman and Lapinski, 2016).\nWe thus restrict this case study to the first dimension though\nfuture work may opt to consider the second dimension as well.\n15165\n1920 1940 1960 1980 2000 2020\n0.00\n0.01\n0.02\n0.03\n0.04\nProportion of speeches\ncontaining dogwhistles\nBrown vs BoE\nCivil Rights Act\nReagan\nCrime Bill\n9 / 11\nObama\nTrump\nFigure 3: Frequency of speeches containing racial dog-\nwhistles in the U.S. Congressional Record (as a fraction\nof total speeches) over time. The dotted red vertical\nlines represent noteworthy years. Use of racial dogwhis-\ntles began to increase during the Civil Rights Movement\nand their frequency continued to rise until the 1990s.\nSince the 1990s, the frequency of speeches containing\ndogwhistles has fluctuated but remained at overall high\nlevels compared to earlier years.\ndid not change until the 1990s.\nWillie Hortonpresents an interesting example. In\nhis 1988 presidential campaign, George Bush ran a\ntelevision advertisement featuring Willie Horton, a\nBlack man convicted of rape and murder while on\nprison furlough (Mendelberg, 2001). The ad was so\npowerful among white voters that it propelled Bush\nto victory, but shortly afterwards was explicitly\ncalled out as racist (Haney-L√≥pez, 2014). We see\nthis pattern in Figure 4: in 1988, Willie Horton was\nassociated with extremely conservative speakers,\nbut quickly became more liberal, andWillie Horton\nno longer functioned as a successful dogwhistle.\n4 Recognition of dogwhistles in GPT-3\nWe conduct two experiments to assess if a large lan-\nguage model, GPT-3 (Brown et al., 2020), can rec-\nognize dogwhistles. First, we interrogate whether\nGPT-3 is able to identify covert meanings of dog-\nwhistles from our glossary, an ability that would\nbe instrumental in understanding the breadth of on-\nline bigotry. Second, we measure GPT-3‚Äôs ability\nto surface dogwhistles, motivated by the fact that\ndogwhistles are often intentionally obscured from\nresearchers which makes it impossible to ensure\nthat a manual search is complete or comprehensive.\nSince GPT-3 is trained on large portions of internet\ndata, it may be able to reveal a more comprehen-\nsive and diverse set of dogwhistles than manual\nefforts. Finally, we present a small manual analysis\nof in-context dogwhistle recognition using our\nglossary examples and GPT-4 as a proof of concept\nFigure 4: Average ideology score (DW-NOMINATE\nfirst dimension) for speakers who used selected dog-\nwhistles over time: welfare reform (top left), thug (top\nright), property rights (bottom left), and Willie Horton\n(bottom right). Higher values indicate that the dog-\nwhistle‚Äôs speakers were more conservative, while lower\nvalues indicate that the dogwhistle‚Äôs speakers were more\nliberal. For visualization, trends are Lowess-smoothed.\nfor future work.\n4.1 Identifying covert meanings\nTo gauge which types of in-group language GPT-3\nmight have seen during training, we examine if it\nis able to identify dogwhistles‚Äô covert meanings.\nExperimental setup We create prompts that vary\nin (1) which of the five definitions of dogwhistles\nis provided, if any, and (2) if the prompt includes\na secret cue. For each input term, we construct\n12 prompts (6 definitions including none, and each\nwith or without a secret cue). Consider the example\nbelow, with a definition, secret cue, dogwhistle and\nGPT-3‚Äôs response . Prompts with no secret clue\nsimply exclude the word secretly. We generate\nresponses for each dogwhistle in the glossary and\nconsider multiple forms for some, such as actual\nemojis as well as descriptions. We thus test 480\nvariants with 12 prompts for each, leading to 5,760\ngenerations.3\nA dogwhistle is the use of coded or suggestive\nlanguage in political messaging to garner support\nfrom a particular group without provoking opposi-\ntion. For example, ‚Äúcosmopolitan‚Äù secretly means\n‚ÄúJewish to many anti-Semitic people .\n3We use the text-davinci-002 GPT-3 model with\nmax_tokens=256, temperature=0.0 (more deterministic out-\nputs) and num_outputs=1.\n15166\nNo Secret Cue With Secret Cue\nNo Definition .085 [¬±.025] .296 [¬±.021]\nWith Definition .437 [¬±.010] .543 [¬±.010]\nTable 2: Proportion of dogwhistles with covert mean-\nings identified by GPT-3 when prompted with a dog-\nwhistle definition, a secret cue, both, or neither. Confi-\ndence intervals are shown in brackets.\nResults The first author manually evaluated\nwhether or not each of the 5,760 GPT-3 generations\ncontains the covert meaning for each dogwhistle.\n80.3% of dogwhistles had their covert meanings\nidentified in at least one generation. Overall, 56.0%\ngenerations contained the correct covert meaning\nfor dogwhistles that are part of the formal/offline\nregister, but just 29.4% for dogwhistles in the infor-\nmal/online register. We refer readers to Appendix\nA.2 (Figure A.2) for more details about register-\nbased variation and examples of dogwhistles for\nwhich GPT-3 performed particularly well or poorly.\nThe specific prompt form strongly impacts GPT-\n3‚Äôs ability to generate covert meanings (Table 2).\nWithout a definition or secret cue, covert meanings\nare identified in just 8.5% of generations. Including\nboth a definition and secret cue improves GPT-3‚Äôs\nperformance over 5-fold, with dogwhistles‚Äô covert\nmeanings identified in 54.3% of generations.\nWe observe wide variation in GPT-3‚Äôs ability to\nidentify covert meanings across personae. Among\nthe most represented personae in our glossary\n(at least 100 generations for each), GPT-3 has\nthe lowest recognition of transphobic dogwhis-\ntles, the highest recognition of homophobic and\nIslamophobic dogwhistles, with antisemitic, white\nsupremacist, and racist dogwhistles in the mid-\ndle (Appendix Table A.3). There is also variation\nin performance by dogwhistle type and the spe-\ncific definition provided; we refer the reader to\nAppendix A.2 and Figure A.3 for more details.\n4.2 Surfacing dogwhistles\nIn addition to evaluating if GPT-3 can identify dog-\nwhistles‚Äô covert meanings, we assess GPT-3‚Äôs abil-\nity to surface dogwhistles in text generation.\nExperimental setup We construct a series of\nprompts that begin with one of five definitions of\ndogwhistles from prior work (Table A.2). The def-\ninition is followed by a question or request for\nexamples (see Appendix A.1 for more prompting\ndetails). In the following example, the definition is\nmarked in blue, the request in purple, and GPT-3‚Äôs\nresponse is highlighted in yellow .\nA dogwhistle is the use of coded or suggestive\nlanguage in political messaging to garner support\nfrom a particular group without provoking oppo-\nsition. What are examples of dogwhistles?\n1. ‚ÄúLaw and order‚Äù\n2. ‚ÄúThe silent majority‚Äù\n3. ‚ÄúFamily values‚Äù\n4. ‚ÄúWelfare queens‚Äù\n5. ‚ÄúIllegal aliens‚Äù\nEvaluation We use our glossary as a proxy to\nmeasure precision and recall of GPT-3‚Äôs ability to\nsurface dogwhistles because an exhaustive ground-\ntruth set of dogwhistles does not exist. We cal-\nculate recall as the proportion of dogwhistles in\nour glossary that were also surfaced at least once\nby GPT-3. For precision, the authors manually\ninspect candidates appearing in at least 4% of GPT-\n3 text generations for generic, white supremacist,\nracist, antisemitic, Islamophobic, and transphobic\nprompt types. Because our glossary is not exhaus-\ntive, this method yields conservative estimates (see\nAppendix A.1 for more evaluation details).\nPrecision Results We find that GPT-3 does have\nthe ability to surface dogwhistles when prompted\nto do so, but caution that such results are imperfect\nand require manual verification. The most common\nerrors involve explicit mentions of groups in stereo-\ntypes or conspiracy theories (Jews are behind the\n9/11 attacks) or phrases that may accompany dog-\nwhistles but are not dogwhistles themselves ( I‚Äôm\nnot racist but...). Precision in dogwhistle surfacing\nvaries across prompt types; while the average pre-\ncision over all six prompt types is 66.8%, scores\nrange from just 50% for transphobic dogwhistle\nprompts to 91.3% for generic prompts (Figure A.1).\nRecall Results GPT-3 surfaced 153 of 340 dog-\nwhistles in our glossary (45%). We observe sig-\nnificant differences by register: GPT-3 surfaced\n69.4% of formal/offline dogwhistles but just 12.9%\nof informal/online dogwhistles. Despite its ability\nto generate emojis and other symbols, GPT-3 did\nnot surface any symbols or emojis from our glos-\nsary except for the antisemitic triple parentheses\n‚Äú((()))‚Äù.\nFigure 5 shows GPT-3 surfacing recall results\nby both register and in-group personae. We show\nresults for the five most-frequent personae repre-\nsented in our glossary. Recall of dogwhistles in the\n15167\nInformal/Online Formal/Offline\nRegister\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\"Recall\" of GPT-3 dogwhistle surfacing\ntransphobic\nantisemitic\nracist\nwhite supremacist\nIslamophobic\nFigure 5: Recall of GPT-3 dogwhistle surfacing sep-\narated by persona and register. Across all personae,\nGPT-3 surfaces under 20% of dogwhistles in the infor-\nmal/online register. Performance is much higher for the\nformal/offline register but varies across personae, rang-\ning from 44.8% (transphobic) to 100% (Islamophobic).\ninformal/online register is low across the board. For\nthe formal/offline register, recall is considerably\nhigher although it varies widely across personae.\nAs with precision, GPT-3 has the lowest perfor-\nmance for transphobic dogwhistles, surfacing just\n44.8% of formal/offline transphobic dogwhistles.\nFor formal/offline antisemitic dogwhistles, recall is\nconsiderably higher but far from perfect at 71.7%.\nGPT-3 has 80.3% and 83.3% recall of racist and\nwhite supremacist dogwhistles, respectively, and\nfull 100% recall of Islamophobic dogwhistles.\n4.3 Identifying dogwhistles in context\nOur experiments show that LLMs have some ability\nto identify covert meanings of a given dogwhistle\nspecified in the prompt (Section 4.1) and gener-\nate dogwhistles (Section 4.2). We now consider\na natural follow-up question: can LLMs correctly\nidentify dogwhistles in real-world texts?\nFully addressing this question would require a\nlarger dataset including a variety of linguistic con-\ntexts per dogwhistle, documents containing no dog-\nwhistles, and documents with dogwhistle expres-\nsions that do not carry the dogwhistle meaning.\nWe leave such a rigorous analysis for future work.\nFor now, we present a small manual analysis with\nChatGPT/GPT-4 (OpenAI, 2023), using several\nreal-world examples from our glossary as a proof\nof concept for in-context dogwhistle recognition.\nWe prompt GPT-4 with the instructions below.\nAppendix Table A.5 shows the exact glossary exam-\nple and GPT-4 output text for the dogwhistles cos-\nmopolitan (elite) (antisemitic), inner-city (racist),\n#IStandWithJKRowling,\n (transphobic), and\ndid you see Kyle? (white supremacist).\nA dogwhistle is the use of coded or suggestive\nlanguage in political messaging to garner support\nfrom a particular group without provoking oppo-\nsition. Identify the dogwhistle in the following\nmessage: [GLOSSARY EXAMPLE].\nUsing the examples presented in Section 1, GPT-\n4 correctly identifies the dogwhistles cosmopolitan\n(elite) and inner city, and generates a correct ex-\nplanation for their covert meanings. However, the\nmodel does not work as well for the other examples.\nFor the example containing #IStandWithJKRowl-\ning, GPT-4 correctly identifies that text covertly\nsignals transphobia through support of JK Rowling,\nbut does not select this hashtag as the dogwhis-\ntle. On the other hand, GPT-4 correctly identifies\nthe dogwhistle\n in a tweet from JK Rowling,\nand correctly relates this symbol to the women‚Äôs\nsuffrage movement, but does not capture the ap-\npropriation of this symbol to covertly communi-\ncate transphobia. Finally, GPT-4 misses both the\ndogwhistle and the precise covert meaning for did\nyou see Kyle? (‚Äúsee Kyle‚Äù sounds similar to the\nNazi slogan ‚ÄúSieg Heil‚Äù); while the model still ul-\ntimately identifies covert white supremacy, it gen-\nerates a false explanation connecting the glossary\nexample to this persona.\n5 Dogwhistles and toxicity detection\nBeyond evaluating language models‚Äô ability to rec-\nognize dogwhistles, we seek to understand how\ndogwhistles affect the decisions that NLP systems\nmake, and how this has downstream implications\nfor content moderation and online safety. We begin\nto address this with a study of how dogwhistles are\nhandled by a widely-deployed toxic language de-\ntection system, Google/Jigsaw‚Äôs Perspective API.4\nPerspective API scores a text between 0 and 1 for\na range of attributes (e.g. toxicity, identity attack,\nprofanity), representing the estimated probability\nthat a reader would perceive the text to contain\nthat attribute. Perspective API‚Äôs models are mul-\ntilingual BERT-based models distilled into single-\nlanguage convolutional neural networks for faster\ninference, and are trained on annotated data from\nonline forums. We refer readers to the Perspective\nAPI Model Cards for more details.5\n4https://perspectiveapi.com/\n5https://developers.perspectiveapi.\ncom/s/about-the-api-model-cards\n15168\nToxicity Severe Toxicity Identity Attack\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Perspective API Score\n0.54\n0.11\n0.24\n0.71\n0.28\n0.56\n0.76\n0.33\n0.73Term Category\ndogwhistle\nslur\nstandard\nFigure 6: Average Perspective API toxicity, severe toxi-\ncity, and identity attack scores for HateCheck template\nsentences filled in with dogwhistles, slurs, or standard\ngroup labels.\nExperimental setup We consider 237 hateful\nsentence templates from HateCheck (R√∂ttger et al.,\n2021), a test suite for bias in hate speech detec-\ntion, that contain placeholders for identity terms\n(group referents) in either adjectival, singular nom-\ninal, or plural nominal forms. We fill filled with a\nstandard group label, a slur, or a dogwhistle in the\ncorresponding grammatical form requested by the\ntemplate. For this experiment, we consider racist\n(mostly anti-Black), antisemitic, and transphobic\nterms, as these personae are the most common in\nour glossary (see Tables A.7 and A.8 for a sample\nof sentence templates and group label terms, re-\nspectively). We feed our resulting 7,665 sentences\nto Perspective API to get scores for toxicity, severe\ntoxicity, and identity attack.\nResults Hateful sentences are rated as less toxic,\nless severely toxic, and less identity-attacking when\ndogwhistles are used instead of standard group la-\nbels or slurs (Figure 6). This pattern holds for all\nthree personae (Appendix Figure A.4).\nInterestingly, mean toxicity scores for slurs are\nlower than for standard group labels, especially for\nantisemitic slurs. We observe relatively wide varia-\ntion in Perspective API‚Äôs ratings depending on the\nspecific choice of slur. For example, sentences con-\ntaining the N-word are almost always rated as more\ntoxic than the same sentences containing Black or\nBlack people. Lower toxicity ratings for other slurs,\nsuch as the highly derogatory antisemitic K-word6\nmay be because, similar to dogwhistles, Perspec-\ntive API does not recognize that these terms refer\n6https://ajc.org/translatehate/kike\nto identity groups. However, deeper analysis of\nslurs is outside the scope of the current work.\n6 Discussion & Conclusion\nWe lay the groundwork for NLP and computational\nsocial science research on dogwhistles by develop-\ning a new taxonomy and glossary with rich contex-\ntual information and examples. We demonstrate\nour glossary‚Äôs utility in a case study of historical\nU.S. Congressional speeches, where our quantita-\ntive analysis aligns closely with historical accounts.\nWe further use our glossary to show that GPT-3\nhas some, but limited, ability to retrieve dogwhis-\ntles and recognize their covert meanings. Finally,\nwe verify that dogwhistles readily evade Perspec-\ntiveAPI‚Äôs toxicity detection. We now turn to several\nimplications of this work, highlighting potential fu-\nture directions across disciplines.\nDogwhistles and toxic language Dogwhistles\nare closely related to other forms of subtle biases\nstudied in NLP, such as implicit hate speech and\nsymbols (Magu et al., 2017; Magu and Luo, 2018;\nElSherief et al., 2018, 2021; Qian et al., 2019;\nCaselli et al., 2020; Menini et al., 2021; Arviv\net al., 2021; Botelho et al., 2021; Wiegand et al.,\n2021a,b; Hartvigsen et al., 2022), microaggressions\n(Breitfeller et al., 2019), dehumanization (Mendel-\nsohn et al., 2020), propaganda (Da San Martino\net al., 2020), condescension (P√©rez-Almendros\net al., 2020), and stereotypes (Nangia et al., 2020;\nSap et al., 2020; Nadeem et al., 2021).\nHowever, dogwhistles are distinct from toxic lan-\nguage in several important ways. First, although of-\nten implicitly abusive, they are not exclusively hate-\nful; for example, wonder-working power covertly\nsignals the speaker‚Äôs Evangelical Christian iden-\ntity (Albertson, 2015). Second, dogwhistles are\ncharacterized by dual meanings, wherein different\nsub-audiences interpret the exact same message dif-\nferently (Henderson and McCready, 2018). Third,\ndogwhistles‚Äô true meanings are intentionally hid-\nden from the out-group (Saul, 2018). Nevertheless,\nbecause dogwhistles are often deployed specifically\nto avoid hate speech detection and other content\nmoderation tools, NLP researchers should consider\nhow dogwhistles highlight a vulnerability in ex-\ntant language technologies, which ultimately puts\npeople‚Äôs safety and well-being at risk.\nWe show that hateful speech using dogwhistles\nevades toxicity detection, and is one way that NLP\nsystems (unintentionally) perpetuate harms against\n15169\nmarginalized groups. This finding is not surprising,\nas prior work shows that toxicity detection often\nfails on subtle language (Han and Tsvetkov, 2020;\nHartvigsen et al., 2022), but underscores the need\nfor toxicity and hate speech detection models to\nbe able to flag hateful dogwhistles. One poten-\ntial approach to improve such models could be to\ntrain them to recognize dogwhistles in naturally-\noccurring in-group contexts (starting with model-\ning contextual factors; Zhou et al., 2023). More\nbroadly, content moderation pipelines should take\ncontext into account and consider mechanisms to\nidentify when a dogwhistle has potentially negative\nconsequences. Beyond toxicity detection, future\nwork ought to consider the impact of dogwhistles\nin a broader range of NLP tasks, such as bias miti-\ngation or story generation.\nHow do LLMs know about dogwhistles? Our\nfindings regarding GPT-3‚Äôs ability to surface and\nidentify dogwhistles‚Äô covert meanings are probably\ndriven by the contents of the training data. GPT-3‚Äôs\ntraining data likely includes right-wing extremist\ncontent, as has been shown with its predecessor\nGPT-2 (Gehman et al., 2020), which may result\nin high performance for dogwhistles from these\nin-groups. Or perhaps the model is simply memo-\nrizing articles or social media posts that explicitly\ncall out certain expressions as dogwhistles. Fu-\nture work could evaluate if large language models\ncan learn dogwhistles‚Äô covert meanings from in-\ncontext usage alone by experimentally controlling\nfor whether or not these terms are explicitly ex-\nposed as dogwhistles in the training data.\nMoreover, we find that GPT-3‚Äôs performance\nvaries widely across target groups. Transphobic\ndogwhistles are notably difficult for GPT-3 to sur-\nface and identify. Perhaps this is because the model\nis trained on fewer data from transphobic communi-\nties compared to other in-groups considered in this\nwork. Furthermore, transphobic dogwhistles may\nbe less frequent in the training data because many\nhave emerged relatively recently. Another reason\nmay be formatting: transphobic dogwhistles are of-\nten emoji-based and appear in social media screen\nnames and profile bios rather than in posts them-\nselves. We hope that future work will investigate\nthe links between language models‚Äô knowledge of\ndogwhistles and training data.\nPotential of LLMs for dogwhistle research Be-\nyond the risks presented by current NLP technolo-\ngies, we wish to highlight the potential benefits of\nusing NLP to advance dogwhistle research. Even\nthough LLMs‚Äô performance is likely due to vast\ntraining data, and even then, their outputs require\nmanual verification, our experiments with GPT-3\ndemonstrate that LLMs have some ability to sur-\nface dogwhistles and explain their covert meanings.\nThis is particularly valuable as dogwhistles are in-\ntentionally hidden from out-group members, and\nout-group researchers may have no other way to\naccess this information. There is thus a unique op-\nportunity for LLMs to assist dogwhistle research,\nand political content analysis more broadly.\nBridging large-scale analysis and mathematical\nmodels Our work builds foundations for large-\nscale computational analysis of dogwhistles in real-\nworld political discourse. We diverge from prior\nquantitative dogwhistle research, which focuses\non mathematically modeling the process underly-\ning dogwhistle communication using probabilis-\ntic, game-theoretic, deep learning, and network-\nbased approaches on simulation data (Smaldino\net al., 2018; D√©nigot and Burnett, 2020; Hender-\nson and McCready, 2020; Breitholtz and Cooper,\n2021; Smaldino and Turner, 2021; Xu et al., 2021;\nHertzberg et al., 2022; van der Does et al., 2022).\nWe are optimistic about future research synthesiz-\ning these two strands of work to address many of\nthe challenges presented by dogwhistles. For exam-\nple, future work could use our resources along with\nthese mathematical models to develop systems that\ncan automatically detect dogwhistle usages, emer-\ngence of new dogwhistles, or decline of older terms\nas dogwhistles due to out-group awareness.\nImplications for social science research Under-\nstanding dogwhistles at scale has vast implications\nacross disciplines, so we develop resources useful\nfor both NLP and social science researchers. We\nprovide the most comprehensive-to-date glossary\nof dogwhistles and demonstrate through our case\nstudy how this resource can be used to analyze po-\nlitical speeches and other corpora, such as social\nmedia posts and newspaper articles. Dogwhistles\nhave mostly been studied using primarily qualita-\ntive methods (Moshin, 2018; √Ökerlund, 2021) and\nexperiments (Albertson, 2015; Wetts and Willer,\n2019; Thompson and Busby, 2021), and we hope\nthat by facilitating quantitative content analysis,\nour resources can add to dogwhistle researchers‚Äô\nmethodological repertoires.\n15170\n7 Limitations\nThis work represents an initial push to bring dog-\nwhistles to the forefront of NLP and computational\nsocial science research, and as such, has many lim-\nitations. Our glossary is the most comprehensive\nresource to date (to the best of our knowledge) but\naims to document a moving target, as dogwhistles\ncontinuously emerge or fall out of use due to out-\ngroup awareness. We aim to make this resource a\n‚Äúliving glossary‚Äù and encourage others to submit\nnew entries or examples. We further encourage\nfuture research to develop models to automatically\ndetect the emergence of new dogwhistles.\nAnother major limitation in this work is that we\nidentify as out-group members for nearly all dog-\nwhistles in the glossary and have an adversarial\nrelationship with many of the communities stud-\nied (e.g. white supremacists). Although our work\nwould ideally be validated by members of the in-\ngroups, they have very little incentive to share this\ninformation, as that would damage the dogwhistle‚Äôs\nutility as a tool for covert in-group communication.\nThis work, like most prior work, is limited in that\nwe operationalize dogwhistles as a static binary;\nwe assume each term either does or does not have\na dogwhistle interpretation and is categorically in-\ncluded or excluded from our glossary and analyses.\nIn reality, dogwhistles are far more complicated\nconstructs. For example, Lee and Kosse (2020)\ncharacterize dogwhistles along two dimensions:\nthe size of their in-group and the degree to which\ntheir usage is conventionalized. Other axes of vari-\nation may include the level of out-group awareness,\nand the social and political risks of backlash to the\ncommunicator if the dogwhistle interpretation is\nexposed. It is even possible that audience mem-\nbers who hear a dogwhistle further recirculate it\neven if they themselves do not recognize the covert\nmeaning (Saul, 2018). We hope future work will\nconsider multifaceted and continuous measures of\n‚Äúdogwhistleness\" that account for such nuances.\nFinally, the current work is limited in the scope\nof dogwhistles considered: they are all in English\nwith the vast majority coming from the U.S. polit-\nical and cultural contexts. However, dogwhistles\nare prominent across cultures (Pal et al., 2018; √Ök-\nerlund, 2021) and we hope that future work will\nconsider other languages and cultures, especially\ninvolving researchers who have high awareness of\nor expertise in non-U.S political environments.\n8 Ethical Implications\nWe caution readers about several potential ethi-\ncal risks of this work. First is the risk of readers\nmisusing or misunderstanding our glossary. We\nemphasize that dogwhistles are extremely context-\ndependent, and most terms in the glossary have\nbenign literal meanings that may be more common\nthan the covert dogwhistle meanings. For example,\nmany entities from the financial sector have been\nused as antisemitic dogwhistles (e.g. the Federal\nReserve, bankers) but their primary usage has no\nantisemitic connotations.\nRelatedly, some glossary entries include terms\nthat originate from the target group but were ap-\npropriated by the dogwhistles‚Äô in-group. Examples\ninclude the appropriation of goy (a Yiddish word\nfor non-Jewish people) as an antisemitic in-group\nsignal, and baby mama (originally from African\nAmerican English) as a racist dogwhistle. As with\nhate speech detection (Sap et al., 2019), there is a\nrisk of social bias in dogwhistle detection.\nAs we have discussed throughout this work, dog-\nwhistle researchers face a challenge with no exhaus-\ntive ground truth and an unknown search space. We\nanticipate our glossary being a helpful resource for\nthis reason, but because we also lack such exhaus-\ntive ground truth, there are bound to be biases in the\nrepresentation of dogwhistles in our glossary. The\ncurrent version of the glossary may exclude groups\nand thus lead to worse performance in dogwhis-\ntle detection, toxic language detection, and other\ndownstream NLP tasks.\nOur glossary also includes real-world examples\nof how each dogwhistle is used. This presents\na privacy risk, which we mitigate by prioritizing\nexamples from public figures or examples from\nanonymous social media accounts whenever possi-\nble. We do not release personal information of any\nspeaker who is not a well-known public figure.\nFinally, we do not pursue any computational\nmodeling or prediction of dogwhistle usages in this\nwork, but see it as a natural direction for future\nwork. However, we caution researchers to con-\nsider dual-use issues in doing so. Many people use\ncoded language in order to avoid censorship from\nauthoritarian regimes (Yang, 2016) and marginal-\nized groups may also use coded language for their\nown safety (Queen, 2007). When building com-\nputational models, we urge researchers to mitigate\nthis dual-use risk as much as possible.\n15171\nAcknowledgements\nWe thank Ceren Budak, Yulia Tsvetkov, and audi-\nences at Text as Data 2022 (TADA) and New Ways\nof Analyzing Variation 50 (NW A V) for their helpful\nfeedback on an earlier version of this work. We also\nthank the anonymous reviewers for their comments\nand suggestions. J.M. gratefully acknowledges sup-\nport from the Google PhD Fellowship.\nReferences\nMathilda √Ökerlund. 2021. Dog whistling far-right code\nwords: the case of ‚Äòculture enricher‚Äôon the swedish\nweb. Information, Communication & Society, pages\n1‚Äì18.\nBethany L Albertson. 2015. Dog-whistle politics: Mul-\ntivocal communication and religious appeals. Politi-\ncal Behavior, 37(1):3‚Äì26.\nEyal Arviv, Simo Hanouna, and Oren Tsur. 2021. It‚Äôs\na thin line between love and hate: Using the echo\nin modeling dynamics of racist online communities.\nProceedings of the International AAAI Conference\non Web and Social Media, 15(1):61‚Äì70.\nDavid A Bateman and John Lapinski. 2016. Ideal points\nand american political development: Beyond dw-\nnominate. Studies in American Political Develop-\nment, 30(2):147‚Äì171.\nPrashanth Bhat and Ofra Klein. 2020. Covert hate\nspeech: White nationalists and dog whistle commu-\nnication on twitter. In Twitter, the public sphere,\nand the chaos of online deliberation, pages 151‚Äì172.\nSpringer.\nAustin Botelho, Scott Hale, and Bertie Vidgen. 2021.\nDeciphering implicit hate: Evaluating automated de-\ntection algorithms for multimodal hate. In Findings\nof the Association for Computational Linguistics:\nACL-IJCNLP 2021, pages 1896‚Äì1907.\nLuke Breitfeller, Emily Ahn, David Jurgens, and Yulia\nTsvetkov. 2019. Finding microaggressions in the\nwild: A case for locating elusive phenomena in social\nmedia posts. In Proceedings of the 2019 conference\non empirical methods in natural language processing\nand the 9th international joint conference on natural\nlanguage processing (EMNLP-IJCNLP), pages 1664‚Äì\n1674.\nEllen Breitholtz and Robin Cooper. 2021. Dogwhis-\ntles as inferences in interaction. In Proceedings of\nthe Reasoning and Interaction Conference (ReInAct\n2021), pages 40‚Äì46.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nEmily Burack. 2020. A list of antisemitic dogwhistles\nused by donald trump. Hey Alma.\nJustin Caffier. 2017. Get to know the memes of the\nalt-right and never miss a dog-whistle again. Vice.\nDallas Card, Serina Chang, Chris Becker, Julia Mendel-\nsohn, Rob V oigt, Leah Boustan, Ran Abramitzky, and\nDan Jurafsky. 2022. Computational analysis of 140\nyears of us political speeches reveals more positive\nbut increasingly polarized framing of immigration.\nProceedings of the National Academy of Sciences ,\n119(31):e2120510119.\nRoyce Carroll, Jeffrey B Lewis, James Lo, Keith T\nPoole, and Howard Rosenthal. 2009. Measuring\nbias and uncertainty in dw-nominate ideal point esti-\nmates via the parametric bootstrap. Political analysis,\n17(3):261‚Äì275.\nTommaso Caselli, Valerio Basile, Jelena Mitrovi¬¥c, Inga\nKartoziya, and Michael Granitzer. 2020. I feel of-\nfended, don‚Äôt be abusive! implicit/explicit messages\nin offensive and abusive language. In Proceedings of\nthe 12th language resources and evaluation confer-\nence, pages 6193‚Äì6202.\nGiovanni Da San Martino, Alberto Barr√≥n-Cede√±o,\nHenning Wachsmuth, Rostislav Petrov, and Preslav\nNakov. 2020. SemEval-2020 task 11: Detection of\npropaganda techniques in news articles. In Proceed-\nings of the Fourteenth Workshop on Semantic Evalu-\nation, pages 1377‚Äì1414, Barcelona (online). Interna-\ntional Committee for Computational Linguistics.\nQuentin D√©nigot and Heather Burnett. 2020. Dogwhis-\ntles as identity-based interpretative variation. In Pro-\nceedings of the Probability and Meaning Conference\n(PaM 2020).\nPenelope Eckert. 2008. Variation and the indexical field\n1. Journal of sociolinguistics, 12(4):453‚Äì476.\nMai ElSherief, Vivek Kulkarni, Dana Nguyen,\nWilliam Yang Wang, and Elizabeth Belding. 2018.\nHate lingo: A target-based linguistic analysis of hate\nspeech in social media. In Proceedings of the Inter-\nnational AAAI Conference on Web and Social Media,\nvolume 12.\nMai ElSherief, Caleb Ziems, David Muchlinski, Vaish-\nnavi Anupindi, Jordyn Seybolt, Munmun De Choud-\nhury, and Diyi Yang. 2021. Latent hatred: A bench-\nmark for understanding implicit hate speech. arXiv\npreprint arXiv:2109.05322.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356‚Äì3369.\nMatthew Gentzkow, Jesse M Shapiro, and Matt\nTaddy. 2019. Measuring group differences in high-\ndimensional choices: method and application to con-\ngressional speech. Econometrica, 87(4):1307‚Äì1340.\n15172\nRobert E Goodin and Michael Saward. 2005. Dog whis-\ntles and democratic mandates. The Political Quar-\nterly, 76(4):471‚Äì476.\nXiaochuang Han and Yulia Tsvetkov. 2020. Fortify-\ning toxic speech detectors against veiled toxicity. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7732‚Äì7739, Online. Association for Computa-\ntional Linguistics.\nIan Haney-L√≥pez. 2014. Dog whistle politics: How\ncoded racial appeals have reinvented racism and\nwrecked the middle class. Oxford University Press.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxigen: A large-scale machine-generated dataset\nfor adversarial and implicit hate speech detection.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3309‚Äì3326.\nRobert Henderson and Elin McCready. 2018. How\ndogwhistles work. New Frontiers in Artificial Intelli-\ngence, pages 231‚Äì240.\nRobert Henderson and Elin McCready. 2019. Dogwhis-\ntles, trust and ideology. In Proceedings of the 22nd\nAmsterdam Colloquium, pages 152‚Äì160.\nRobert Henderson and Elin McCready. 2020. Towards\nfunctional, agent-based models of dogwhistle com-\nmunication. In Proceedings of the Probability and\nMeaning Conference (PaM 2020), pages 73‚Äì77.\nNiclas Hertzberg, Robin Cooper, Elina Lindgren, Bj√∂rn\nR√∂nnerstrand, Gregor Rettenegger, Ellen Breitholtz,\nand Asad Sayeed. 2022. Distributional properties\nof political dogwhistle representations in swedish\nbert. In Proceedings of the Sixth Workshop on Online\nAbuse and Harms (WOAH), pages 170‚Äì175.\nJustin Khoo. 2017. Code words in political discourse.\nPhilosophical Topics, 45(2):33‚Äì64.\nRebecca Lee and Maureen Kosse. 2020. The social do-\nmain of understanding: Ethnographically-informed\nframe semantics of dog whistles. High Desert Lin-\nguistics Society 14.\nJeffrey B Lewis, Keith Poole, Howard Rosenthal, Adam\nBoche, Aaron Rudkin, and Luke Sonnet. 2023.\nV oteview: Congressional roll-call votes database.\nhttps://voteview. com/.\nRijul Magu, Kshitij Joshi, and Jiebo Luo. 2017. Detect-\ning the hate code on social media. In Proceedings\nof the International AAAI Conference on Web and\nSocial Media, volume 11, pages 608‚Äì611.\nRijul Magu and Jiebo Luo. 2018. Determining code\nwords in euphemistic hate speech using word embed-\nding networks. In Proceedings of the 2nd workshop\non abusive language online (ALW2), pages 93‚Äì100.\nTali Mendelberg. 2001. The Race Card: Campaign\nStrategy, Implicit Messages, and the Norm of Equal-\nity. Princeton University Press.\nJulia Mendelsohn, Yulia Tsvetkov, and Dan Jurafsky.\n2020. A framework for the computational linguistic\nanalysis of dehumanization. Frontiers in artificial\nintelligence, 3:55.\nStefano Menini, Alessio Palmero Aprosio, and Sara\nTonelli. 2021. Abuse is contextual, what about nlp?\nthe role of context in abusive language annotation\nand detection. arXiv preprint arXiv:2103.14916.\nJamie Moshin. 2018. Hello darkness: Antisemitism\nand rhetorical silence in the\" trump era\". Journal of\nContemporary Rhetoric, 8.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoset: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356‚Äì5371.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel Bowman. 2020. Crows-pairs: A challenge\ndataset for measuring social biases in masked lan-\nguage models. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953‚Äì1967.\nOpenAI. 2023. Gpt-4 technical report. arXiv.\nJoyojeet Pal, Dinsha Mistree, and Tanya Madhani.\n2018. A friendly neighborhood hindu. In Ce-\nDEM Asia 2018: Proceedings of the International\nConference for E-Democracy and Open Govern-\nment; Japan 2018 , pages 97‚Äì121. Edition Donau-\nUniversit√§t Krems.\nCarla P√©rez-Almendros, Luis Espinosa Anke, and\nSteven Schockaert. 2020. Don‚Äôt patronize me! an\nannotated dataset with patronizing and condescend-\ning language towards vulnerable communities. In\nProceedings of the 28th International Conference on\nComputational Linguistics, pages 5891‚Äì5902.\nKeith T Poole and Howard Rosenthal. 1985. A spatial\nmodel for legislative roll call analysis. American\njournal of political science, pages 357‚Äì384.\nJing Qian, Mai ElSherief, Elizabeth Belding, and\nWilliam Yang Wang. 2019. Learning to decipher\nhate symbols. arXiv preprint arXiv:1904.02418.\nRobin Queen. 2007. Sociolinguistic horizons: Lan-\nguage and sexuality. Language and Linguistics Com-\npass, 1(4):314‚Äì330.\nPaul R√∂ttger, Bertie Vidgen, Dong Nguyen, Zeerak\nWaseem, Helen Margetts, and Janet Pierrehumbert.\n2021. Hatecheck: Functional tests for hate speech\ndetection models. In Proceedings of the 59th Annual\n15173\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 41‚Äì58.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1668‚Äì1678.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A Smith, and Yejin Choi. 2020. Social\nbias frames: Reasoning about social and power im-\nplications of language. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5477‚Äì5490.\nJennifer Saul. 2018. Dogwhistles, political manipula-\ntion, and philosophy of language. In Daniel Fogal,\nDaniel W. Harris, and Matt Moss, editors, New work\non speech acts, volume 360, page 84. Oxford Univer-\nsity Press Oxford.\nPaul E Smaldino, Thomas J Flamson, and Richard McEl-\nreath. 2018. The evolution of covert signaling. Sci-\nentific reports, 8(1):1‚Äì10.\nPaul E Smaldino and Matthew A Turner. 2021. Covert\nsignaling is an adaptive communication strategy in\ndiverse populations. Psychological review.\nAndrew Ifedapo Thompson and Ethan C Busby. 2021.\nDefending the dog whistle: The role of justifications\nin racial messaging. Political Behavior, pages 1‚Äì22.\nBrian P Tilley et al. 2020. ‚Äúi am the law and order\ncandidate‚Äù: A content analysis of donald trump‚Äôs\nrace-baiting dog whistles in the 2016 presidential\ncampaign. Psychology, 11(12):1941.\nTamara van der Does, Mirta Galesic, Zackary Okun\nDunivin, and Paul E Smaldino. 2022. Strate-\ngic identity signaling in heterogeneous networks.\nProceedings of the National Academy of Sciences ,\n119(10):e2117898119.\nRachel Wetts and Robb Willer. 2019. Who is called\nby the dog whistle? experimental evidence that\nracial resentment and political ideology condition\nresponses to racially encoded messages. Socius,\n5:2378023119866268.\nMichael Wiegand, Maja Geulig, and Josef Ruppenhofer.\n2021a. Implicitly abusive comparisons‚Äìa new dataset\nand linguistic analysis. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 358‚Äì368.\nMichael Wiegand, Josef Ruppenhofer, and Elisabeth\nEder. 2021b. Implicitly abusive language‚Äìwhat does\nit actually look like and why are we not getting there?\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 576‚Äì587. Association for Computational Lin-\nguistics.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian\nMcAuley, and Furu Wei. 2021. Blow the dog whistle:\nA chinese dataset for cant understanding with com-\nmon sense and world knowledge. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2139‚Äì2145.\nFan Yang. 2016. Rethinking china‚Äôs internet censorship:\nThe practice of recoding and the politics of visibility.\nNew Media & Society, 18(7):1364‚Äì1381.\nXuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas David-\nson, Jena D. Hwang, Swabha Swayamdipta, and\nMaarten Sap. 2023. Cobra frames: Contextual rea-\nsoning about effects and harms of offensive state-\nments. In Findings of ACL.\n15174\nA Appendix\nCategory Count\nformal/offline 193Register informal/online 147\nstereotype-based target group label 64\nconcept (policy) 41\nconcept (values) 37\npersona signal (symbol) 35\nstereotype-based descriptor 34\npersona signal (self-referential) 32\nconcept (other) 29\narbitrary target group label 23\npersona signal (shared culture) 18\nhumor/mockery/sarcasm 11\nrepresentative (Bogeyman) 10\nphonetic-based target group label 4\nType\npersona signal (in-group label) 2\nracist 76\ntransphobic 73\nantisemitic 73\nwhite supremacist 48\nIslamophobic 16\nconservative 8\nanti-liberal 7\nanti-Latino 6\nhomophobic 6\nanti-vax 5\nreligious 4\nclimate change denier 4\nanti-Asian 3\nanti-LGBTQ 3\nliberal 3\nxenophobic 2\nanti-GMO 2\nPersona\nmisogynistic 1\nTable A.1: Distribution of glossary entries across all\nregisters, types, and personae.\nA.1 Details for dogwhistle surfacing\nWe create 51 total request formulations that ask for\ngeneric examples of dogwhistles (n=17), dogwhis-\ntles that target specific social groups (n=25), and\ndogwhistles that are used by certain personae/in-\ngroups (n=9). For each prompt, we also consider\nthree spelling variations of ‚Äúdogwhistle‚Äù: dogwhis-\ntle, dog-whistle, and dog whistle. Exact prompt\ntext can be found in our project repository.\nTo encourage GPT-3 to generate a list, we con-\nclude all prompts with a newline token followed\nby ‚Äú1.‚Äù. All prompts were provided to a GPT-3\nInstruct model (text-davinci-002) with de-\ngeneric white supremacist racist antisemitic Islamophobic transphobic\nPersona cued in prompt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\"Precision\" of GPT-3 dogwhistle surfacing\nFigure A.1: Precision of GPT-3 dogwhistle surfacing by\nprompt type. Precision was highest for dogwhistles that\nwere commonly surfaced in response to generic prompts,\nand lowest for dogwhistles that were commonly sur-\nfaced in response to prompts requesting examples of\nIslamophobic or transphobic dogwhistles.\nfault hyperparameters except for max_tokens=256,\ntemperature=0.7, and num_outputs= 5 (5 genera-\ntions per prompt). The resulting texts are strings\nthat take the form of an enumerated list. To ag-\ngregate and compare surfaced dogwhistles across\neach text completion, we post-process by: split-\nting by newline characters, removing enumeration\nand other punctuation, converting all outputs to\nlowercase, lemmatizing each surfaced term with\nSpaCy, and removing definite articles that precede\ngenerated dogwhistles. We then aggregate over all\ngenerations to determine how often each dogwhis-\ntle is surfaced for each in-group.\nIn calculating precision of dogwhistle surfacing,\nwe mark each of the 154 candidate terms as true\npositives if they appear in the glossary. Some sur-\nfaced dogwhistles were marked as ‚Äúcorrect‚Äù if they\nwere closely related to a dogwhistle entry in our\nglossary, even if the exact term did not appear. Ex-\namples include national security, identity politics,\nthe swamp, tax relief, and patriot. However, this is\nstill a conservative estimate because our glossary is\nnot exhaustive. GPT-3 surfaces a number of terms\nthat potentially have dogwhistle usages but were\nnot covered by our glossary, and thus not included\nin our precision estimates. Examples of these terms\ninclude names of Muslim political organizations\n(Hezbollah, Hamas, Muslim Brotherhood) and Sec-\nond Amendment rights. Figure A.1 shows variation\nin precision of dogwhistle surfacing across prompt\ntypes (in-groups and generic prompting).\nA.2 Details for identifying covert meaning\nVariation across registers We identify varia-\ntion in GPT-3‚Äôs ability to identify dogwhistles‚Äô\ncovert meanings based on prompt features, dog-\n15175\nSource Definition\nAlbertson (2015) A dogwhistle is an expression that has different meanings to different audiences.\nHenderson and McCready (2018)\nA dogwhistle is a term that sends one message to an outgroup while\nat the same time sending a second (often taboo, controversial, or\ninflammatory) message to an ingroup.\nBhat and Klein (2020)\nA dogwhistle is a word or phrase that means one thing to the public\nat large, but that carry an additional, implicit meaning only recognized\nby a specific subset of the audience.\nMerriam-Webster A dogwhistle is a coded message communicated through words or phrases\ncommonly understood by a particular group of people, but not by others.\nWikipedia A dogwhistle is the use of coded or suggestive language in political messaging\nto garner support from a particular group without provoking opposition.\nTable A.2: Definitions of dogwhistles and their sources used for prompting GPT-3.\nBelow are links for the Merriam-Webster and Wikipedia sources:\nhttps://www.merriam-webster.com/words-at-play/dog-whistle-political-meaning\nhttps://en.wikipedia.org/wiki/Dog_whistle_(politics)\nNo Definition\nNor Secret Cue\nOnly\nSecret Cue\nOnly Dogwhistle\nDefinition\nDefinition\n& Secret Cue\nPrompt T ype\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nGPT-3 Identified Covert Meaning \n(frac. of generations)\nInformal Register\nFormal Register\nFigure A.2: Percent of GPT-3 generations that capture\ndogwhistles‚Äô covert meanings, separated by register and\nif the prompt includes a definition or secret cue.\nwhistle register, and the interaction between the\ntwo. Figure A.2 shows that including the definition\nin prompts consistently improves GPT-3‚Äôs covert\nmeaning identification for both formal and informal\ndogwhistles. However, including the secret cue has\nminimal effect for informal dogwhistles, and only\nleads to substantial improvement for identifying\nformal dogwhistles‚Äô covert meanings.\nVariation across personae There is significant\nvariation in GPT-3‚Äôs performance across personae,\nas can be seen in Table A.3.\nVariation across dogwhistle types GPT-3‚Äôs per-\nformance varies widely across dogwhistle types in\nour taxonomy (¬ß2.1; Fig. 2). GPT-3 has the lowest\nperformance for humor-based and arbitrary target\ngroup label dogwhistles, and the highest perfor-\nmance for representatives (Bogeymen), phonetic-\nbased target group labels, and policies (Table A.4).\nPersona Proportion 95% CI\nhomophobic 0.737 0.069\nIslamophobic 0.654 0.060\nclimate change denier 0.595 0.106\nanti-Asian 0.583 0.126\nconservative 0.563 0.100\nanti-Latino 0.560 0.107\nracist 0.532 0.029\nanti-vax 0.490 0.101\nantisemitic 0.462 0.029\nwhite supremacist 0.448 0.029\nanti-liberal 0.292 0.106\ntransphobic 0.229 0.024\nTable A.3: Proportion of dogwhistles with covert mean-\nings identified by GPT-3 across personae with at least 50\nGPT-3 generations. GPT-3 has the lowest recognition\nof transphobic dogwhistles and the highest recognition\nof homophobic and Islamophobic dogwhistles.\nDogwhistle Type Proportion 95% CI\nconcept (humor) 0.244 0.063\narbitrary group label 0.261 0.046\nstereotype-based descriptor 0.311 0.060\npersona signal (symbol) 0.331 0.032\npersona signal (self-referential) 0.444 0.046\npersona signal (shared culture) 0.448 0.054\nconcept (values) 0.475 0.026\nstereotype-based group label 0.497 0.031\nconcept (policy) 0.519 0.036\nphonetic-based group label 0.533 0.127\nrepresentative (Bogeyman) 0.618 0.063\nTable A.4: Proportion of dogwhistles with covert mean-\nings identified by GPT-3 by the dogwhistle type.\n15176\nGlossary Example GPT-4 Response Term Meaning\nThecosmopolitan elitelook down on the common\naffections that once bound this nation together: things\nlike place and national feeling and religious faith. The\ncosmopolitan agendahas driven both Left and Right.\nIt‚Äôs time we ended thecosmopolitanexperiment and\nrecovered the promise of the republic.\nThe dogwhistle in this message is the term \"cosmopolitan elite,\"\nwhich is often used to target a specific group of people, such as\nurban, educated, and often liberal individuals, without explicitly\nnaming them. It can also be interpreted as an anti-Semitic or\nanti-immigrant message, as \"cosmopolitan\" has historically been\nused to criticize Jewish people and immigrants for their perceived\nlack of loyalty to their country.\n‚úî ‚úî\nWe have got this tailspin of culture, in ourinner citiesin\nparticular, of men not working and just generations of men\nnot even thinking about working or learning the value and\nthe culture or work.\nThe dogwhistle in this message is the reference to \"inner cities\"\nand \"generations of men not working,\" which can be interpreted\nas coded language targeting African Americans or other minority\ngroups living in urban areas, perpetuating stereotypes about\nlaziness and unemployment.\n‚úî ‚úî\n#IStandWithJKRowlingbecause she wrote a series of\nbooks that made readers out of millions of children and\nadolescents. And she has every right to express her opinion\non cultural matters. And because she‚Äôs right about them.\nThe dogwhistle in this message is \"she‚Äôs right about them,\" which\nindirectly supports J.K. Rowling‚Äôs controversial opinions on gender\nand transgender issues without explicitly mentioning them.\n‚úò ‚úî\nAnd together, we will win\nThe dogwhistle in this message is the use of the colors\n,\nwhich are associated with the women‚Äôs suffrage movement. This\nsuggests that the message is aimed at garnering support from\nwomen or feminists without explicitly stating it.\n‚úî ‚úò\nDid you see Kyle?He‚Äôs about this tall..\nThe dogwhistle in this message is \"Did you see Kyle? He‚Äôs about\nthis tall..\" which could be a reference to the white supremacist\nsymbol of the \"K\" hand sign, where the fingers form the letters\n\"W\" and \"P\" for \"white power.\" The name \"Kyle\" is sometimes\nassociated with white supremacist groups.\n‚úò ‚úò\nTable A.5: Manual proof-of-concept analysis for using GPT-4 to identify dogwhistles in-context. The columns on\nthe right indicate whether GPT-4 correctly identifies the dogwhistle term and its covert meaning, respectively.\nNone Merriam-\nWebster\nAlbertson\n(2015)\nBhat&\nKlein\n(2020)\nHenderson&\nMcCready\n(2018)\nWikipedia\nDefinition Source\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nGPT-3 Identified Covert Meaning\n(frac. of generations)\nNo secret cue\nSecret cue\nFigure A.3: Proportion of GPT-3 generations that cor-\nrectly identify dogwhistles‚Äô covert meanings across\nprompted dogwhistle definitions and secret cues.\nDefinition Source Mean 95% CI\nNone Provided 0.191 0.025\nMerriam-Webster 0.438 0.031\nAlbertson (2015) 0.449 0.031\nBhat and Klein (2020) 0.513 0.032\nHenderson and McCready (2018) 0.515 0.032\nWikipedia 0.534 0.032\nTable A.6: Proportion of GPT-3 generations that cor-\nrectly identify dogwhistles‚Äô covert meanings for each\ndogwhistle definition provided in prompting.\nVariation across dogwhistle definitions Only\n19.1% of GPT-3 generations include the correct\ncovert meaning when prompted with no dogwhis-\ntle definition. Prompting GPT-3 with any of the\nfive dogwhistle definitions greatly improved perfor-\nmance over no definition provided, but the extent\nvaried, with the Merriam-Webster definition yield-\ning the lowest improvement (43.8%) and Wikipedia\nyielding the highest (54.3%) (Table A.6). The boost\nin performance by adding the secret cue depends\non the specific definition used; the secret cue has a\nbigger effect when using the Merriam-Webster and\nAlbertson (2015) definitions (Figure A.3).\nWhere does GPT-3 perform poorly? Most un-\nrecognized dogwhistles are part of the informal\nregister, especially symbols (e.g. the transphobic\nspiderweb or cherry emojis). Other unrecognized\ndogwhistles include ‚ÄúOperation Google‚Äù terms (e.g.\nSkype, Yahoo), more recent terms (e.g. Let‚Äôs Go\nBrandon), and several antisemitic and transphobic\ndogwhistles whose covert meanings are especially\ncontext-dependent (e.g. adult human female, XX,\n(Wikipedia) early life, fellow white people). Unrec-\nognized formal dogwhistles tend to be extremely\nsubtle and nuanced (e.g. Dred Scott as a con-\nservative anti-abortion dogwhistle) or are highly-\nconventionalized phrases that may be far more com-\nmonly used without the covert implicature (e.g. the\nantisemitic dogwhistle poisoning the well).\n15177\nantisemitic racist transphobic\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Perspective API Score\nT oxicity\nantisemitic racist transphobic\nSevere T oxicity\nantisemitic racist transphobic\nIdentity Attack\nT erm Category\ndogwhistle\nslur\nstandard\nFigure A.4: Toxicity, severe toxicity, and identity-attacking scores from Perspective API preliminary experiment.\nWhen slurs or standard group labels are substituted with dogwhistles, sentences are rated as significantly less toxic.\nWhere does GPT-3 perform well? GPT-3 read-\nily identifies Islamophobic dogwhistles (e.g. rad-\nical Islam, Barack Hussein Obama ), many anti-\nsemitic conspiracy theories (e.g. Khazars, Kalergi\nPlan, Israel Lobby), some racist dogwhistles whose\ncovert meanings are more widely discussed among\nthe out-group (e.g. inner-city, ghetto, thug, war\non drugs, black-on-black crime), some alt-right\nmemes (e.g. Pepe the Frog), and conservative po-\nlitical rhetoric (e.g. balance the budget, judicial\nactivism, job creators).\nA.3 Details for toxicity detection\nTable A.7 shows a sample of the 237 hateful sen-\ntence templates used from HateCheck (R√∂ttger\net al., 2021). We used the subset of HateCheck\ntemplates labeled as hateful and containing a place-\nholder for an identity term in either adjectival, sin-\ngular nominal, or plural nominal form. We filled\nin these placeholders with either a standard group\nlabel, dogwhistle, or slur for three personae: an-\ntisemitic, racist, and transphobic. Specific terms\nused can be found in Table A.8.\nFor all personae and toxicity measures provided\nby Perspective API, dogwhistles are rated as less\ntoxic than slurs and standard group labels (Figure\nA.4. Interestingly, except for the N-word, Perspec-\ntive seems to not recognize the extreme offensive-\nness of most slurs, and thus toxicity scores for\nhateful sentences containing slurs are similar to or\nlower than scores for the same hateful sentences\ncontaining standard group labels.\nTemplate\nSo sick of being around [IDENTITY_P] like you.\n[IDENTITY_P] are disgusting.\nYou [IDENTITY_P] are all dishonest.\n[IDENTITY_P] are human garbage.\nYour [IDENTITY_A] lives are worthless.\nNever trust a single word said by a [IDENTITY_S].\nI will cut their [IDENTITY_A] throats.\nI would love to see those [IDENTITY_P] suffer.\nAll [IDENTITY_P] should fuck off forever.\nOf course that dumbass was a [IDENTITY_S].\n[IDENTITY_P] think they‚Äôre special. I hate them.\nIsn‚Äôt it typical of a [IDENTITY_S] to be that stupid?\nTable A.7: Sample of template hateful sentences from\nHateCheck (n=237). We create inputs for Perspective\nAPI by replacing identity term placeholders with a stan-\ndard group label, dogwhistle, or slur in either adjectival\n(IDENTITY_A), singular nominal (IDENTITY_S), or\nplural nominal form (IDENTITY_P).\nRacist Antisemitic Transphobic\nAfrican-American Jewish transgender\nBlack Jew trans\ninner-city cosmopolitan AGP\nwelfare queen cosmopolitan elite autogynephile\nthug coastal elite AAP\ndindu globalist autoandrophile\ngangbanger international banker clownfish\nghetto cultural Marxist troon\nurban Hollywood elite TIF\nsuperpredator Khazar TIM\nn****r k**e t****y\nc**n h**b s*****e\nTable A.8: Racist, antisemitic, and transphobic terms\nused for toxicity analysis. We substitute identity place-\nholders in HateCheck templates (R√∂ttger et al., 2021)\nwith these terms to create inputs to Perspective API.\n15178\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nSection 7\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nSection 8\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nSections 0 and 1\n‚ñ°\u0013 A4. Have you used AI writing assistants when working on this paper?\nYes, we used ChatGPT to rephrase a single sentence from the abstract to repeat the same point at the\nend of the introduction.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nSection 2\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nCongressional records data (Section 3), Glossary sources (Section 2), GPT-3 (Section 4), and Hate\nspeech templates + models (Section 5)\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nOur glossary and taxonomy will be open and available to the public (Section 2)\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nYes, in Sections 2, 5, and 8\n‚ñ°\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nData released is anonymized with no identifying information\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 2 and Section 8\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nSection 2\nC ‚ñ°\u0013 Did you run computational experiments?\nSection 4\n‚ñ° C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. We did not train our own model, and just used GPT-3 with the OpenAI API\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15179\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nYes, section 4 and Appendix A.1 and A.2\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nYes, Sections 3-5, with additional statistics in A.1-A.3 in the appendix\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nYes, Section 4 and Appendix A.1-A.2\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n15180",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6986702680587769
    },
    {
      "name": "Meaning (existential)",
      "score": 0.6960132122039795
    },
    {
      "name": "Glossary",
      "score": 0.6859327554702759
    },
    {
      "name": "Typology",
      "score": 0.6587508916854858
    },
    {
      "name": "Sentence",
      "score": 0.5720579624176025
    },
    {
      "name": "Rhetoric",
      "score": 0.5543251037597656
    },
    {
      "name": "Language model",
      "score": 0.5375622510910034
    },
    {
      "name": "Moderation",
      "score": 0.4819076359272003
    },
    {
      "name": "Natural language processing",
      "score": 0.48087337613105774
    },
    {
      "name": "Word (group theory)",
      "score": 0.4544539451599121
    },
    {
      "name": "Social media",
      "score": 0.4455469846725464
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43468931317329407
    },
    {
      "name": "Topic model",
      "score": 0.4110730290412903
    },
    {
      "name": "Linguistics",
      "score": 0.399001806974411
    },
    {
      "name": "Sociology",
      "score": 0.2325657308101654
    },
    {
      "name": "Psychology",
      "score": 0.2256908416748047
    },
    {
      "name": "World Wide Web",
      "score": 0.22061339020729065
    },
    {
      "name": "Machine learning",
      "score": 0.12754952907562256
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ],
  "cited_by": 11
}