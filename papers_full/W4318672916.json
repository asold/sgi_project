{
    "title": "Data-Efficient French Language Modeling with CamemBERTa",
    "url": "https://openalex.org/W4318672916",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2971064649",
            "name": "Wissam Antoun",
            "affiliations": [
                "Institut national de recherche en informatique et en automatique"
            ]
        },
        {
            "id": "https://openalex.org/A1632339985",
            "name": "Benoît Sagot",
            "affiliations": [
                "Institut national de recherche en informatique et en automatique"
            ]
        },
        {
            "id": "https://openalex.org/A249347002",
            "name": "Djamé Seddah",
            "affiliations": [
                "Institut national de recherche en informatique et en automatique"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2552110825",
        "https://openalex.org/W3213674000",
        "https://openalex.org/W4308245305",
        "https://openalex.org/W1547514161",
        "https://openalex.org/W331019419",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W3157788795",
        "https://openalex.org/W2948902769",
        "https://openalex.org/W3214298066",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3099756172",
        "https://openalex.org/W3207731338",
        "https://openalex.org/W3103187652",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W1973346430",
        "https://openalex.org/W4221149474",
        "https://openalex.org/W1967086305",
        "https://openalex.org/W4287760320",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4310561961",
        "https://openalex.org/W3198722157",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W1014376541",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W4221142967",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3035207248",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W2995435108",
        "https://openalex.org/W3105069964",
        "https://openalex.org/W2970752815",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3211686893",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W4205410664",
        "https://openalex.org/W1865928303",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4291431937",
        "https://openalex.org/W4313304472",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W4206136559",
        "https://openalex.org/W4301239768",
        "https://openalex.org/W2996428491"
    ],
    "abstract": "Recent advances in NLP have significantly improved the performance of\\nlanguage models on a variety of tasks. While these advances are largely driven\\nby the availability of large amounts of data and computational power, they also\\nbenefit from the development of better training methods and architectures. In\\nthis paper, we introduce CamemBERTa, a French DeBERTa model that builds upon\\nthe DeBERTaV3 architecture and training objective. We evaluate our model's\\nperformance on a variety of French downstream tasks and datasets, including\\nquestion answering, part-of-speech tagging, dependency parsing, named entity\\nrecognition, and the FLUE benchmark, and compare against CamemBERT, the\\nstate-of-the-art monolingual model for French. Our results show that, given the\\nsame amount of training tokens, our model outperforms BERT-based models trained\\nwith MLM on most tasks. Furthermore, our new model reaches similar or superior\\nperformance on downstream tasks compared to CamemBERT, despite being trained on\\nonly 30% of its total number of input tokens. In addition to our experimental\\nresults, we also publicly release the weights and code implementation of\\nCamemBERTa, making it the first publicly available DeBERTaV3 model outside of\\nthe original paper and the first openly available implementation of a DeBERTaV3\\ntraining objective. https://gitlab.inria.fr/almanach/CamemBERTa\\n",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5174–5185\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nData-Efficient French Language Modeling with CAMEM BERTA\nWissam Antoun Benoît Sagot Djamé Seddah\nInria, Paris\n{firstname,lastname}@inria.fr\nAbstract\nRecent advances in NLP have significantly im-\nproved the performance of language models\non a variety of tasks. While these advances\nare largely driven by the availability of large\namounts of data and computational power, they\nalso benefit from the development of better\ntraining methods and architectures. In this pa-\nper, we introduce CAMEM BERTA, a French\nDeBERTa model that builds upon the DeBER-\nTaV3 architecture and training objective. We\nevaluate our model’s performance on a variety\nof French downstream tasks and datasets, in-\ncluding question answering, part-of-speech tag-\nging, dependency parsing, named entity recog-\nnition, and the FLUE benchmark, and com-\npare against CamemBERT, the state-of-the-art\nmonolingual model for French. Our results\nshow that, given the same amount of training\ntokens, our model outperforms BERT-based\nmodels trained with MLM on most tasks. Fur-\nthermore, our new model reaches similar or su-\nperior performance on downstream tasks com-\npared to CamemBERT, despite being trained\non only 30% of its total number of input to-\nkens. In addition to our experimental results,\nwe also publicly release the weights and code\nimplementation of CAMEM BERTA, making it\nthe first publicly available DeBERTaV3 model\noutside of the original paper and the first openly\navailable implementation of a DeBERTaV3\ntraining objective.1\n1 Introduction\nAdvances in natural language processing (NLP)\nhave been driven mainly by scaling up the size\nof pre-trained language models, along with the\namount of data and compute required for train-\ning (Raffel et al., 2020; Radford et al., 2019; Rae\net al., 2021; Fedus et al., 2021; Hoffmann et al.,\n2022). However, these are not the only factors to de-\ntermine a model’s downstream performance, as the\nmodel’s architecture and training objective are also\n1https://gitlab.inria.fr/almanach/CamemBERTa\nimportant. He et al. (2021b) showed that we can\nimprove a model’s performance by using disentan-\ngled attention, which uses two vectors to represent\na token, one for position and one for content. He\net al. (2021a) later showed that performance could\nbe further improved by using ELECTRA’s (Clark\net al., 2020) self-supervised and sample-efficient\nreplaced token detection objective. Another cru-\ncial aspect lies in the ability to train models faster,\nwhich allows for quick iteration and thus acceler-\nates the research process and allows for more effi-\ncient exploration of new ideas (Izsak et al., 2021;\nPan et al., 2022; Geiping and Goldstein, 2022).\nThis research aims to develop data-efficient and\noptimized training techniques that can improve per-\nformance in downstream tasks, while reducing the\nrequired training corpus size and compute. To\nachieve this goal, we propose a new data-efficient\nFrench language model based on DeBERTaV3 (He\net al., 2021a). Our proposed model aims to op-\ntimize the training process by using a sample-\nefficient training objective, a state-of-the-art model\narchitecture, and an efficient implementation. We\nevaluate downstream performance with a variety\nof NLP tasks, including dependency parsing, part-\nof-speech tagging, named entity recognition, text\nclassification, and question answering. We com-\npare our model to a BERT model trained with the\nmasked language modeling (MLM) objective using\nthe same tokenizer and training corpus, and to the\nstate-of-the-art French language model, Camem-\nBERT (Martin et al., 2020), which required three\ntimes as many training iterations. Our results show\nthat our proposed model reaches or establishes a\nnew state-of-the-art using one third of the compu-\ntational budget of its main predecessors.\nOur contributions can be summarized as follows:\n• We propose a new data-efficient French lan-\nguage model, which we train based on our\nDeBERTaV3 re-implementation with our op-\ntimized training recipe.\n5174\n• We empirically show that under the same con-\nditions, our model outperforms Transformer\nmodels trained with MLM on most tasks,\nand that it reaches or establishes a new state-\nof-the-art even when compared with models\ntrained for three times as long.\n• Our release is the only publicly available\nimplementation of DeBERTaV3’s training\nobjective, and the first for a monolingual\nDeBERTaV3 model other than the original\npaper.\nOur code and models are available under an\nopen-source license 2, making it easy for re-\nsearchers to reproduce our results and build upon\nour work.\n2 Related Works\nTransformers. This architecture has been widely\nadopted in NLP tasks such as language modeling,\nmainly due to the use of the self-attention mech-\nanisms (Vaswani et al., 2017), which allow the\nmodel to weigh the importance of different parts\nof the input when making predictions. A downside\nof the Transformer block is that it is permutation-\ninvariant, which inhibits the model from encoding\nword order information. Originally, the authors pro-\nposed to add either a fixed sinusoidal pattern or a\nlearned positional embedding as positional bias the\ninput token embedding. Later studies have shown\nthat using relative positional embeddings is more\neffective (Shaw et al., 2018; Dai et al., 2019; Qu\net al., 2021). Recently, He et al. (2021b) proposed\na new disentangled attention mechanism, which\nconsiders both the relative position and the content\nof the input tokens as separate vectors.\nPre-trained French Language Models. Cur-\nrent language models available for French are ei-\nther trained using Masked Language Modeling\n(MLM) or Causal Language Modeling (CLM).\nCamemBERT (Martin et al., 2020) and FlauBERT\n(Le et al., 2020) are two of the most popular\ncontemporary French models, both trained with\nmasked language modeling. Other models in-\nclude FrALBERT (Cattan et al., 2021), a French\nversion of ALBERT (Lan et al., 2020), LeP-\netit (Micheli et al., 2020) which is a small ver-\nsion of CamemBERT, and D’AlemBERT (Gabay\net al., 2022), a RoBERTa (Liu et al., 2020) based\nlanguage model targeted towards Early Modern\n2https://gitlab.inria.fr/almanach/CamemBERTa\nFrench. BARThez (Kamal Eddine et al., 2021) is a\nsequence-to-sequence model trained with BART’s\nobjective (Lewis et al., 2020), and PAGnol (Lau-\nnay et al., 2022) and Cedille (Müller and Laurent,\n2022) are models trained with the CLM objective.\nTo the best of our knowledge, there is no prior\neffort in developing language models with this im-\nproved disentangled attention mechanism and ob-\njectives other than MLM/CLM beyond English.\n3 C AMEM BERTA: Methodology\nThe following section details our proposed archi-\ntecture and pre-training objective, along with de-\nscriptions for the downstream tasks.\nArchitecture CAMEM BERTA is based on the\nDeBERTaV3 (He et al., 2021b) architecture which\nuses two vectors to encode the word and its posi-\ntion, with the premise being that the relative posi-\ntion of a word pair should also directly affect the\ncomputed attention weights. The V3 version opti-\nmizes the initial DeBERTa architecture by sharing\nthe relative position embedding projection layers\nacross all the encoder layers, and by adding a con-\nvolution layer aside the first encoder layer.3 We use\na base model configuration with 12 layers and 12\nattention heads, 768 hidden dimensions with 32k\nfor vocabulary size.\nTraining Objective We follow the DeBER-\nTaV3 (He et al., 2021a) pretraining strategy by us-\ning the replaced token detection (RTD) pre-training\nloss first introduced in ELECTRA (Clark et al.,\n2020), with a generator and discriminator based\non the DeBERTa architecture. During pre-training\nwe project the generator embeddings to 256 dimen-\nsions and keep the generator model at 12 layers.\nDuring pre-training the generator model is\ntrained using the MLM objective where we dy-\nnamically mask 15% of the input tokens. We then\nsample from the generator the masked tokens, and\nfeed the output along with the unmasked tokens to\nthe discriminator which is tasked to identify tokens\nthat were replaced by the generator. The RTD ob-\njective increases sample efficiency since the model\nis predicting over all input tokens instead of the\n15% masked tokens.\nIn DeBERTaV3, the authors hypothesized and\nshowed that sharing token embeddings between\nthe generator and the discriminator results in a tug-\nof-war situation, where the MLM and RTD tasks\n3See Section 5.3 of the DeBERTa paper (He et al., 2021b)\n5175\npull the embedding vectors into opposing direc-\ntions. To alleviate this problem, the authors imple-\nmented Gradient-Disentangled Embedding Sharing\n(GDES), a method that re-parameterize the discrim-\ninator’s token embeddings asED = sg(EG) +E∆,\nwhere sgstops the gradient flow from the RTD loss\nto the generator token embeddings EG, and hence\nthe loss gradient only updates a Difference Embed-\nding matrix E∆ that is added to EG to form the\ndiscriminator token embeddings ED. After pre-\ntraining, E∆ and EG are summed to get the final\nED and E∆ is then discarded.\nPre-Training We pre-train on the French subset\nof CCNet4 (Wenzek et al., 2020), the same corpus\nused to pre-train CamemBERTCCNet (Martin et al.,\n2020).5 Moreover we reuse CamemBERTCCNet ’s\ntokenizer (Kudo and Richardson, 2018). By\nreusing the pre-training corpus and tokenizer, we\nisolate the performance differences to the model\narchitecture and training objective variables.\nOptimization To speed up the pre-training exper-\niments, we split the pre-training into two phases; in\nphase 1, the model is trained with a maximum se-\nquence length of 128 tokens for 10,000 steps with\n2,000 warm-up steps and a very large batch size of\n67,584. In phase 2, maximum sequence length is\nincreased to the full model capacity of 512 tokens\nfor 3,300 steps with 200 warm-up steps and a batch\nsize of 27,648. Because we use very large batch\nsizes, we optimize the model using the LAMB op-\ntimizer (You et al., 2020) with a learning rate of\n6e−3, β1 = 0.878, and β2 = 0.974.\n4 Experiments and Results\nPre-Training Setup We re-implement the\nDeBERTaV3 RTD pre-training objective with\nGDES, since no public implementation was\navailable at the time of writing. Our training\nimplementation is based on Nvidia’s ELECTRA\nand BERT TensorFlow2 implementations. 6 We\ntrain our models for 8 days on 6 Nvidia A40 with\nHorovod (Sergeev and Balso, 2018), and make use\nof XLA compilation, mixed-precision and gradient\naccumulation to speed-up training and to fit large\nbatch sizes with our limited compute.\nDuring pre-training, our model would have\nseen 133B tokens compared to 419B tokens for\n4See Appendix 4 for more information on dataset choice.\n5We go over the pertaining dataset choice in the experi-\nments section.\n6https://github.com/NVIDIA/DeepLearningExamples/\nCamemBERTCCNet which was trained for 100K\nsteps. This represents roughly 30% of Camem-\nBERT’s full training. Hence for a fair com-\nparison, we train a RoBERTa model, which we\ndub CamemBERT30%, using our same exact pre-\ntraining setup but with the MLM objective.\nDownstream Evaluation We compare our mod-\nels, CamemBERTCCNet , and CamemBERT 30%,\non a diverse set of French downstream tasks and\ndatasets, namely: Question Answering (QA) on\nFQuAD 1.0 (d’Hoffschmidt et al., 2020), Part-Of-\nSpeech (POS) tagging and Dependency Parsing on\nGSD (McDonald et al., 2013), Rhapsodie (Lacheret\net al., 2014), Sequoia (Candito and Seddah, 2012;\nCandito et al., 2014) in their UD v2.2 versions\nand the French Social Media Bank7 (Seddah et al.,\n2012), Named Entity Recognition (NER) on the\n2008 version of FTB (Abeillé et al., 2000; Candito\nand Crabbé, 2009) with NER annotation by Sagot\net al. (2012), and the FLUE benchmark (Le et al.,\n2020).\nWe use the dataset splits as provided by their\nrespective authors, and we finetune using well-\ntested scripts from the Hugging Face Transformers\nlibrary and the HOPS parser (Grobol and Crabbé,\n2021). We only perform hyper-parameter tuning\nfor the NER and QA tasks. See Appendix C for\ntask-specific details. Bold text shows the best sta-\ntistically significant score over 5 seeds.\nQuestion Answering. We evaluate our model\non the FQuAD 1.0 dataset (d’Hoffschmidt et al.,\n2020), which is a SQuAD (Rajpurkar et al.,\n2016) style French question-answering dataset with\n20731 examples for training, and 3188 for evalua-\ntion.\nThe results shown in Table 2 show that our\nmodel outperforms CamemBERT30% by 6.01 F1\npoints, but shows no statistically significant im-\nprovement over CamemBERTCCNet F1 score, and\nexact match (EM) score.\nPart-of-Speech and Dependency Parsing. We\nreport our results on 4 diverse French treebanks.\nFor the parser training, we make use of the HOPS\nparser (Grobol and Crabbé, 2021) implementation,\nwhich is a graph-based dependency parser inspired\nby Dozat and Manning (2017). Our configuration\nuses the Transformer model’s last layer in addi-\n7We follow Riabi et al. (2021) and use their shuffled ver-\nsion of the treebank, which they split into around 2000 sen-\ntences for training, and 1000 for each the dev and test sets\n5176\nGSD R HAPSODIE SEQUOIA FSMB NER\nMODEL UPOS LAS UPOS LAS UPOS LAS UPOS LAS F1\nCamemBERT30% 98.55±0.05 94.26±0.03 97.61±0.12 83.19±0.62 99.32±0.08 94.09±0.06 94.63±0.11 80.13±0.41 91.04±0.76\nCamemBERTCCNet 98.57±0.07 94.35±0.15 97.62±0.08 84.29±0.56 99.35±0.09 94.78±0.12 94.80±0.16 81.34±0.63 89.97±0.50\nCAMEMBERTA 98.55±0.05 94.38±0.15 97.52±0.14 84.23±0.08 99.44±0.07 94.85±0.14 94.80±0.09 80.74±0.25 90.33±0.54\nTable 1: POS tagging, dependency parsing and NER results on the test sets of our French datasets. UPOS\n(Universal Part-of-Speech) refers here to POS tagging accuracy, and LAS measures the overall accuracy of labeled\ndependencies in a parsed sentence.\nModel F1 EM\nFrALBERT 72.6 ∗XX0 55.1 ∗XXX\nCamemBERT30% 75.14±0.17 56.19±0.27\nCamemBERTCCNet 80.98±0.48 62.51±0.54\nCAMEM BERTA 81.15±0.38 62.01±0.45\nTable 2: Question Answering results on FQuAD 1.0.\ntion to FastText embeddings (Bojanowski et al.,\n2017), character-level bi-directional RNN embed-\ndings, and word embeddings trained during the\nfine-tuning phase.\nTable 1 shows that our proposed model con-\nsistently outperforms CamemBERT30%, and com-\npetes with CamemBERTCCNet on all 4 treebanks.\nNamed Entity Recognition is performed on\nthe French Treebank (FTB) which contains\n350k tokens in 27k sentences extracted from\nnews articles. Our results in Table 1, surpris-\ningly show that CamemBERT 30% outperforms\nCamemBERTCCNet , while not being statistically\nbetter than our model.\nFLUE Benchmark We use datasets from\nthe French Language Understanding Evaluation\n(FLUE) benchmark (Le et al., 2020), namely the\nFrench part of the paraphrase identification dataset\nPAWS-X (Yang et al., 2019), and of XNLI (Con-\nneau et al., 2018), in addition to CLS, a binary\nclassification dataset with Amazon reviews taken\nfrom Amazon.\nOur results (Table 3) show that our model out-\nperforms all models on the CLS movie classi-\nfication task, and matches the performance of\nCamemBERTCCNet on the other FLUE tasks.\nPre-training Dataset Choice We choose CCNet\nas our pre-training dataset instead of the more com-\nmon OSCAR dataset (Ortiz Suárez et al., 2019),\nas (i) it was shown to produce less offensive out-\nput (Launay et al., 2022) and (ii) it allowed us\nto be fully comparable with many of the Camem-\nModel CLS PAWS-X XNLI\nFrALBERT 72.17 ±3.32 76.29±1.28 66.87±0.42\nFlauBERT 93.22 ∗000 89.49 ∗000 80.6 ∗0000\nCamemBERT30% 93.28±0.19 88.94±0.14 79.89±0.64\nCamemBERTCCNet 94.62±0.04 91.36±0.38 81.95±0.51\nCAMEMBERTA 94.92±0.13 91.67±0.17 82.00±0.17\nTable 3: Text classification results (Accuracy) on the\nFLUE benchmark. ∗Results taken from Le et al. (2020).\nBERT models (Martin et al., 2020), enabling\nthus meaningful comparisons. Nevertheless, we\nalso ran experiments with CamemBERT OSCAR ,\nand found that it performed slightly worse than\nCamemBERTCCNet , as shown in Table 5 Ap-\npendix A.\nPre-training Compute and CO2 Impact Our\nmodel was trained for 8 days on 6 A40 GPUs, com-\npared to CamemBERT which was trained on 256\nV100 GPUs for one day, which is roughly equiva-\nlent to 28 days of training on 6 A40 GPUs, since\nan NVIDIA A40 GPU is about 1.5x faster than a\nV100 GPU on language modeling tasks according\nto recent benchmarks.8\nFollowing the reports by Luccioni et al. (2022)\nand Cattan et al. (2022) on the environmental im-\npact of language model training, we use Lanne-\nlongue et al.’s (2021) online carbon footprint calcu-\nlator to provide the following estimates: CAMEM -\nBERTA’s pre-training used 700kWh and emitted\n36kg CO2 compared to 3.32MWh and 170kg for\nCamemBERT.9\n5 Discussion\nOur experiments clearly show that given the same\ntraining corpus, tokenizer, and total number of ex-\namples seen during training, CAMEM BERTA out-\nperforms the MLM trained CamemBERT model\n8See https://lambdalabs.com/blog/nvidia-rtx-a40-\nbenchmarks.\n9These estimates are specific to our training infrastructure\nsituated in France. These estimates highlight the remarkable\nefficiency achieved by CamemBERTa’s pretraining process.\n5177\non all tasks except NER on FTB and POS tagging\non Rhapsodie. Moreover, our model implementa-\ntion is able to match or outperform a fully trained\nCamemBERT model, trained on around 3 times\nmore samples and more compute. The strong per-\nformance of our model on higher level FLUE tasks\nsuggest that lower level tasks such as POS tagging\nand dependency parsing are less challenging for\ncurrent generation models, since they mostly re-\nquire surface level information which the model\ncan capture early in the training process, as sug-\ngested by Martin et al. (2020), compared to tasks\nsuch as question answering and text classification\nwhich require more complex processing.\nTaking a step back and looking at the only De-\nBERTa model that includes French, mDeBERTa\n(He et al., 2021a) we can see (cf. Table 4) that\nour model only requires 6.6% of its multilingual\ncounterpart training samples to achieve competitive\nperformance while additionally also outperforming\nthe XLM-R model (Conneau et al., 2020) trained\non a much larger training sample size.\nXNLI Steps # tokens † Size‡\nmDeBERTa∗ 84.4 500k 2T 0.295T\nCAMEMBERTA 82.0 33k †† 0.139T 0.032T\nXLM-R∗∗ 81.4 1.5M 6T 0.295T\nC.BERTCCNet 81.95 100k 0.419T 0.032T\nTable 4: Comparison of XNLI results for different pre-\ntraining settings. ††step count was converted assuming\n8k batch size. †the total number of tokens seen during\ntraining. ‡Total dataset size in tokens.∗He et al. (2021a),\n∗∗Conneau et al. (2020).\nThis confirms the interest in using such training\nparadigms in compute limited scenarios for seman-\ntically demanding tasks such as question-answering\nor natural-language inference.\nLast but not least, other competitive language\nmodels for French are available and although not\nthe primary focus of this paper, we conducted\na comparative analysis involving FlauBERT (Le\net al., 2020) and FrALBERT (Cattan et al., 2021).\nThe results, presented in Table 5 in Appendix A,\ndemonstrate the better performance of our model\nacross all evaluated tasks in comparison to these\nFrench models. Additionally, it is worth noting that\nFlauBERT was trained for 17 days with 32 V100\nGPUs, which is equivalent to 60 days of training on\n6 A40 GPUs. This represents a 7.5-fold increase\nin computational resources employed compared to\nCAMEM BERTA.\n6 Conclusion\nWe presented CAMEM BERTA, a data-efficient\nFrench language model trained on a large cor-\npus of French text and the first publicly available\nDeBERTaV3-style pretrained model and imple-\nmentation. For a fair evaluation we reused the\nsame corpus and tokenizer as CamemBERTCCNet ,\nbut using only 30% of the total number of input\ntraining tokens. We compared the performance of\nboth models in addition to an MLM model trained\nfrom scratch under the same setup as CAMEM -\nBERTA, CamemBERT30%, on a variety of down-\nstream tasks. Our experiments showed that our\nmodel outperforms CamemBERT30% on all tasks\nexcept NER on FTB, and that it is able to match and\neven surpass CamemBERTCCNet . Furthermore,\nwe have also made our optimized code implemen-\ntation and pretrained model weights publicly avail-\nable for others to use.\nLimitations\nAlthough our model is more efficient than previous\nmodels trained using the MLM objective and the\nstandard transformer architecture, we notice that\nthe models runs around 30% slower. This is due\nto the disentangled attention mechanism, which is\nmore computationally expensive than the standard\nattention mechanism. We also note that at the time\nof writing, the DeBERTaV3 TensorFLow 2 imple-\nmentation available on HuggingFace’s Transform-\ners library (Wolf et al., 2020) experiences heavy\nslowdowns with TPU backends. Our attempts to\nsolve this issue were unsuccessful, and we were\nunable to train our model on TPUs.\nEthics Statement\nWe propose a model trained using DeBERTaV3\nstyle pre-training along with an optimized training\nimplementation, which reduces training computa-\ntion cost when compared to previous models, and\nhence greatly reduces the energy cost and environ-\nmental impact of language model training. We\ntrained our model using the CCNet dataset, for\nwhich we direct the reader to for further discussion\non bias and ethical considerations. Our experi-\nments do not include any additional data collection\nor human annotators. Like other language models\ntrained on massive corpora, there may be potential\nbiases present in the training data, which could af-\nfect the output of our models. Therefore, we advise\n5178\nagainst using these models in production without\nthorough testing. All our experiments were carried\nout on clusters with energy sources consisting of\nnuclear (65–75%), 20% renewable, and the remain-\ning from gas.\nAcknowledgements\nThis work was partly funded by Benoît Sagot’s\nchair in the PRAIRIE institute funded by the\nFrench national reseach agency (ANR as part of\nthe “Investissements d’avenir” programme under\nthe reference ANR-19-P3IA-0001. This work also\nreceived funding from the European Union’s Hori-\nzon 2020 research and innovation programme un-\nder grant agreement No. 101021607. The authors\nare grateful to the OPAL infrastructure from Uni-\nversité Côte d’Azur for providing resources and\nsupport.\nReferences\nAnne Abeillé, Lionel Clément, and Alexandra Kinyon.\n2000. Building a treebank for French. In Proceed-\nings of the Second International Conference on Lan-\nguage Resources and Evaluation (LREC’00), Athens,\nGreece. European Language Resources Association\n(ELRA).\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nMarie Candito and Benoît Crabbé. 2009. Improving\ngenerative statistical parsing with semi-supervised\nword clustering. In Proceedings of the 11th In-\nternational Conference on Parsing Technologies\n(IWPT’09), pages 138–141, Paris, France. Associ-\nation for Computational Linguistics.\nMarie Candito, Guy Perrier, Bruno Guillaume, Corentin\nRibeyre, Karën Fort, Djamé Seddah, and Eric De La\nClergerie. 2014. Deep syntax annotation of the se-\nquoia french treebank. In Proceedings of the Ninth\nInternational Conference on Language Resources\nand Evaluation (LREC’14), Reykjavik, Iceland. Eu-\nropean Language Resources Association (ELRA).\nMarie Candito and Djamé Seddah. 2012. Le corpus se-\nquoia : annotation syntaxique et exploitation pour\nl’adaptation d’analyseur par pont lexical (the se-\nquoia corpus : Syntactic annotation and use for a\nparser lexical domain adaptation method) [in French].\nIn Proceedings of the Joint Conference JEP-TALN-\nRECITAL 2012, volume 2: TALN, pages 321–334,\nGrenoble, France. ATALA/AFCP.\nOralie Cattan, Sahar Ghannay, Christophe Servan, and\nSophie Rosset. 2022. Benchmarking transformers-\nbased models on french spoken language understand-\ning tasks. arXiv preprint arXiv:2207.09152.\nOralie Cattan, Christophe Servan, and Sophie Rosset.\n2021. On the Usability of Transformers-based mod-\nels for a French Question-Answering task. In Recent\nAdvances in Natural Language Processing (RANLP),\nVarna, Bulgaria.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMartin d’Hoffschmidt, Maxime Vidal, Wacim Belb-\nlidia, and Tom Brendlé. 2020. FQuAD: French\nQuestion Answering Dataset. arXiv e-prints, page\narXiv:2002.06071.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biaffine attention for neural dependency pars-\ning. In International Conference on Learning Repre-\nsentations.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. arXiv\npreprint arXiv:2101.03961.\nSimon Gabay, Pedro Ortiz Suarez, Alexandre Bartz,\nAlix Chagué, Rachel Bawden, Philippe Gam-\nbette, and Benoît Sagot. 2022. From FreEM to\nd’AlemBERT: a large corpus and a language model\nfor early Modern French. In Proceedings of the Thir-\nteenth Language Resources and Evaluation Confer-\nence, pages 3367–3374, Marseille, France. European\nLanguage Resources Association.\n5179\nJonas Geiping and Tom Goldstein. 2022. Cramming:\nTraining a language model on a single gpu in one\nday.\nLoïc Grobol and Benoît Crabbé. 2021. Analyse en\ndépendances du français avec des plongements con-\ntextualisés. In Actes de la 28ème Conférence sur le\nTraitement Automatique des Langues Naturelles.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021a.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021b. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nPeter Izsak, Moshe Berchansky, and Omer Levy. 2021.\nHow to train BERT with an academic budget. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 10644–\n10652, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nMoussa Kamal Eddine, Antoine Tixier, and Michalis\nVazirgiannis. 2021. BARThez: a skilled pretrained\nFrench sequence-to-sequence model. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 9369–9390, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 66–71.\nAnne Lacheret, Sylvain Kahane, Julie Beliao, Anne\nDister, Kim Gerdes, Jean-Philippe Goldman, Nico-\nlas Obin, Paola Pietrandrea, and Atanas Tchobanov.\n2014. Rhapsodie: un Treebank annoté pour l’étude\nde l’interface syntaxe-prosodie en français parlé. In\n4e Congrès Mondial de Linguistique Française, vol-\nume 8, pages 2675–2689, Berlin, Germany.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nLoïc Lannelongue, Jason Grealey, and Michael In-\nouye. 2021. Green algorithms: quantifying the car-\nbon footprint of computation. Advanced science,\n8(12):2100707.\nJulien Launay, E.l. Tommasone, Baptiste Pannier,\nFrançois Boniface, Amélie Chatelain, Alessandro\nCappelli, Iacopo Poli, and Djamé Seddah. 2022.\nPAGnol: An extra-large French generative model. In\nProceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 4275–4284, Mar-\nseille, France. European Language Resources Asso-\nciation.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. FlauBERT: Unsupervised language\nmodel pre-training for French. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 2479–2490, Marseille, France. European\nLanguage Resources Association.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert} pretraining\napproach.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-\nLaure Ligozat. 2022. Estimating the carbon footprint\nof bloom, a 176b parameter language model. arXiv\npreprint arXiv:2211.02001.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7203–\n7219, Online. Association for Computational Lin-\nguistics.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-\nBrundage, Yoav Goldberg, Dipanjan Das, Kuzman\nGanchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar\nTäckström, Claudia Bedini, Núria Bertomeu Castelló,\nand Jungmee Lee. 2013. Universal Dependency an-\nnotation for multilingual parsing. In Proceedings\nof the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 92–97, Sofia, Bulgaria. Association for Com-\nputational Linguistics.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training data\nvolume for compact language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n7853–7858, Online. Association for Computational\nLinguistics.\n5180\nMartin Müller and Florian Laurent. 2022. Cedille: A\nlarge autoregressive french language model.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for process-\ning huge corpora on medium to low resource infras-\ntructures. Proceedings of the Workshop on Chal-\nlenges in the Management of Large Corpora (CMLC-\n7) 2019. Cardiff, 22nd July 2019, pages 9 – 16,\nMannheim. Leibniz-Institut für Deutsche Sprache.\nRui Pan, Shizhe Diao, Jianlin Chen, and Tong Zhang.\n2022. Extremebert: A toolkit for accelerating pre-\ntraining of customized bert.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. BPE-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1882–1892, Online. Association for\nComputational Linguistics.\nAnlin Qu, Jianwei Niu, and Shasha Mo. 2021. Explore\nbetter relative position embeddings from encoding\nperspective for transformer models. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2989–2997, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nArij Riabi, Benoît Sagot, and Djamé Seddah. 2021.\nCan character-based language models improve down-\nstream task performances in low-resource and noisy\nlanguage scenarios? In Proceedings of the Seventh\nWorkshop on Noisy User-generated Text (W-NUT\n2021), pages 423–436, Online. Association for Com-\nputational Linguistics.\nBenoît Sagot, Marion Richard, and Rosa Stern. 2012.\nAnnotation référentielle du corpus arboré de Paris 7\nen entités nommées (referential named entity anno-\ntation of the Paris 7 French TreeBank) [in French].\nIn Proceedings of the Joint Conference JEP-TALN-\nRECITAL 2012, volume 2: TALN, pages 535–542,\nGrenoble, France. ATALA/AFCP.\nDjamé Seddah, Benoit Sagot, Marie Candito, Virginie\nMouilleron, and Vanessa Combet. 2012. The French\nSocial Media Bank: a treebank of noisy user gener-\nated content. In Proceedings of COLING 2012, pages\n2441–2458, Mumbai, India. The COLING 2012 Or-\nganizing Committee.\nAlexander Sergeev and Mike Del Balso. 2018. Horovod:\nfast and easy distributed deep learning in TensorFlow.\narXiv preprint arXiv:1802.05799.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 464–468, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual adversar-\nial dataset for paraphrase identification. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3687–3692, Hong\nKong, China. Association for Computational Linguis-\ntics.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\n5181\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In International Confer-\nence on Learning Representations.\n5182\nAppendix\nA Experiments Results on OSCAR and Dropout\nModel UPOS LAS NER CLS PAWS-X XNLI F1 FQuAD EMFQuAD\nFrALBERT 93.53 78.89 69.83 72.17 76.29 66.87 72.6 ∗ 55.1∗\nFlauBERT 97.51 87.92 - 93.22 ∗ 89.49∗ 80.6∗ - -\nCamemBERTOSCAR 97.50 88.24 88.19 94.61 90.87 81.38 79.92 61.15\nCamemBERTCCNet 97.59 88.69 89.97 94.62 91.36 81.95 80.98 62.51\nCAMEM BERTA 97.57 88.55 90.33 94.92 91.67 82.00 81.15 62.01\nCAMEM BERTAdropout 97.56 88.57 90.03 94.46 91.42 81.91 79.37 60.29\nTable 5: Comparison results of CamemBERTOSCAR and CamemBERTCCNet , and our model CAMEM BERTA,\nwith and without dropout. Due to compatibility issues with FlauBERT’s tokenizer, we were unable to conduct\nFlauBERT testing on FQuAD and NER using standard finetuning scripts. ∗Results from the models’ respective\npapers Cattan et al. (2021) and (Le et al., 2020).\nB Negative Results\nIn addition to our main results, we attempted to improve the performance of our model by adding BPE-\nDropout (Provilkov et al., 2020) to the tokenization process, as it was shown that this method of subword\nregularization improves performance on translation tasks. We retrain our model with BPE-Dropout,\ndubbed CamemBERTadropout, and compare the results to our original model in Table 5. We observe that\nby adding BPE-Dropout, we obtain a decrease in performance on most tasks, except for POS tagging and\ndependency parsing, where the performance does not change.\nC Hyper-parameters\nHyper-parameter Value\nMax sequence length 512\nBatch size 16\nFP16 Enabled\nLearning rate {1.5e-5,2e-5,3e-5}\nEpochs 8\nScheduler linear\nWarmup steps {0,0.1%}\nSeed {1,25,42,666,1337}\nTable 6: Hyper-parameters used for the Question Answering and Named Entity Recognition experiments.\nFor experiments on the FLUE benchmark we use the same hyper-parameters as the authors of Camem-\nBERT on the NLI task. As for POS tagging and dependency parsing, we use the same configurations as\nthe one used in Riabi et al. (2021).\n5183\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitiations section\n□\u0013 A2. Did you discuss any potential risks of your work?\nethics section\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n3, 4, and Appendix C\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5184\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4 and Appendix D\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n5185"
}