{
    "title": "A Large Language Model Approach to Extracting Causal Evidence across Study Designs for Evidence Triangulation",
    "url": "https://openalex.org/W4392939106",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5083789253",
            "name": "Xuanyu Shi",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A5100622939",
            "name": "Wenjing Zhao",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A5101966734",
            "name": "Ting Chen",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5119011515",
            "name": "Chao Yang",
            "affiliations": [
                "Peking University",
                "Peking University First Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5100412664",
            "name": "Jian Du",
            "affiliations": [
                "Peking University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4384338100",
        "https://openalex.org/W3113178943",
        "https://openalex.org/W2798958557",
        "https://openalex.org/W2786371198",
        "https://openalex.org/W4304084205",
        "https://openalex.org/W2791681873",
        "https://openalex.org/W2579779633",
        "https://openalex.org/W3113742933",
        "https://openalex.org/W4401798123",
        "https://openalex.org/W4386424352",
        "https://openalex.org/W4392011999",
        "https://openalex.org/W4394984866",
        "https://openalex.org/W3087185831",
        "https://openalex.org/W4324321244",
        "https://openalex.org/W3157905252",
        "https://openalex.org/W4386508492",
        "https://openalex.org/W4401793751",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2098201295",
        "https://openalex.org/W3026538346",
        "https://openalex.org/W4398174032",
        "https://openalex.org/W2277886283",
        "https://openalex.org/W3094181226",
        "https://openalex.org/W4392343921",
        "https://openalex.org/W4399812563",
        "https://openalex.org/W4389508866",
        "https://openalex.org/W4398144060",
        "https://openalex.org/W4400414272",
        "https://openalex.org/W4399653983",
        "https://openalex.org/W4322757808",
        "https://openalex.org/W2802057120",
        "https://openalex.org/W3185326394",
        "https://openalex.org/W2404369708",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W3109097500",
        "https://openalex.org/W4399836663",
        "https://openalex.org/W4399837985",
        "https://openalex.org/W3106503709"
    ],
    "abstract": "Abstract Health strategies increasingly emphasize both behavioral and biomedical interventions, yet the complex and often contradictory guidance on diet, behavior, and health outcomes complicates evidence-based decision-making. Evidence triangulation across diverse study designs is essential for establishing causality, but scalable, automated methods for achieving this are lacking. In this study, we assess the performance of large language models (LLMs) in extracting both ontological and methodological information from scientific literature to automate evidence triangulation. A two-step extraction approach—focusing on cause-effect concepts first, followed by relation extraction—outperformed a one-step method, particularly in identifying effect direction and statistical significance. Using salt intake and blood pressure as a case study, we calculated the Convergeny of Evidence (CoE) and Level of Evidence (LoE), finding a trending excitatory effect of salt on hypertension risk, with a moderate LoE. This approach complements traditional meta-analyses by integrating evidence across study designs, thereby facilitating more comprehensive assessments of public health recommendations.",
    "full_text": " 1 \nEvidenceTriangulator: A Large Language Model Approach to Synthesizing \nCausal Evidence across Study Designs \n \nXuanyu Shi, MS1,2, Wenjing Zhao, PhD1,2, Chao Yang, MS 3, Jian Du*, PhD1,2 \n \n1Institute of Medical Technology, Peking University, Beijing, China;  \n2National Institute of Health Data Science, Peking University, Beijing, China;  \n3Renal Division, Department of Medicine, Peking University First Hospital, Peking \nUniversity Institute of Nephrology, Beijing, China \n \nAbstract \nIn managing chronic diseases, the role of social determinants like lifestyle and diet is crucial. A comprehensive \nstrategy combining biomedical and lifestyle changes is necessary for optimal health. However, the complexity of \nevidence from varied study des igns on lifestyle interventions poses a challenge  to decision-making. To tackle this \nchallenge, our work focused on leveraging large language model to construct a dataset primed for  evidence \ntriangulation. This approach automates the process of gathering a nd preparing evidence for analysis, thereby \nsimplifying the integration of reliable insights and reducing the dependency on labor-intensive manual curation. Our \napproach, validated by expert evaluations, demonstrates significant utility, especially illustrated through a case study \non reduced salt intake and its effect on blood pressure. This highlights the potential of leveraging large language \nmodels to enhance evidence-based decision-making in health care. \n \nIntroduction \nSocial determinants of health (SDoH), especially lifestyle factors such as diet and exercise, are pivotal in managing \nmajor chronic diseases such as cardiovascular diseases, cancer, chronic respiratory diseases, and diabetes.  For \ninstances, according to data from the Institute for Health Metrics and Evaluation (IHME), behavioral factors contribute \nsignificantly to ischemic heart disease and stroke, accounting for 69.2% and 47.4% of Disability-Adjusted Life Years \n(DALYs), respectively—the highest among all diseases. In particular, dietary factors contributed 57.1% and 30.6% \nof DALYs, respectively (1). It is increasingly recognized that health strategies should prioritize both SDoH-based \nbehavioral interventions and biomedical interventions (e.g., medications) (2). Developing evidence-based intervention \nstrategies encounters significant challenges due to the rapidly growing and piecemeal evidence, along with complex \ncausal relationships from various study designs, including confounding and reverse causation.  Evaluating the \nreliability of causal relationships within a body of scientific evidence is essential for evidence-based decision-making, \nespecially when research findings are inconsistent (3, 4). \n \nMeta-analysis (META) is an effective scientific method for quantitatively synthesizing research conclusions. Utilizing \nstatistical techniques, it combines the results of different studies to obtain an overall quantitative estimate of the impact \nof specific interventions (e.g. , salt restriction) on particular outcomes (e.g., blood pressure). It balances conflicting \nevidence quantitatively to achieve evidence-based decision-making based on synthesized scientific evidence. Since \nits introduction in the 1970s, meta-analysis has had a significant impact on various fields such as medicine, economics, \nsociology, and environmental science  (5). Over the past four decades, meta -analysis has evolved to include \nincreasingly complex methods for quantifying evidence, particularly concerning the consistency of results from the \nsame study design or the replicability of studies. In contrast, convergenc e, reflecting the extent to which a given \nhypothesis is supported by different study designs, has not received the same attention  (4). Currently, considering \nconsistency and convergence is recognized as an important strategy for addressing the reproducibility crisis for the \nscientific community (4). \n \nIn recent years, the idea of “triangulation” has been introduced into the scientific community to measure the \nconvergence of scientific conclusions derived from different study designs (4, 6, 7). These study designs have different \nand independent potential sources of bias  (6). Triangulation is a research strategy involving the use of at least two \nresearch methods to investigate and analyze the same problem, mutually validating each other to enhance the \nrobustness and reproducibility of conclusions. If conclusions derived from di fferent research designs  (such as \nobservational studies (OS), mendelian randomization studies  (MR), and randomized controlled trials  (RCT), etc.) \n \n* Correspondence to: dujian@bjmu.edu.cn \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304457doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n 2 \nregarding the same scientific question are consistent, the reliability of causality is stronger. When the results point to \ndifferent answers, understanding the major source of bias instruct researchers future study designs  (6). However, \ncurrently, scholars primarily employ qualitative methods to explain the reliability of causal relationships through \ntriangulation, lacking quantitative approaches. Qualitative methods mainly involve retrospective analysis of relevant \nliterature in the \"Discussion\" section of papers, simply summarizing and discussing how many studies support the \nconclusions of the current study, how many do not, and reasons for lack of support, such as different experimental \nconditions (7). Current evidence triangulation in medicine involves a very high proportion of manual work (8). \nHowever, such retrospective, qualitative triangulation methods are susceptible to issues such as subjective selectivity \nof evidence and biases in understanding among different researchers. \n \nImplementing a fully quantitative method for evidence triangulation requires a computable representation of research \nresults and relevant metadata obtained from different study designs. Apart from determining the presence and direction \nof the effects (i.e., significant increase, significant decrease, and null) given an intervention and outcome, finer-grained \ninformation of research design among many lines of studies need to be extracted. For evidence triangulation task, it is \nimportant to extract information such as the duration and intensity of intervention, measured outcomes, intervention \ntargets (prevention vs. treatment), characteristics of study populations (e.g., demographics), and other relevant \ncontextual information.  \n \nCurrently, there are natural language processing methods available for extracting conclusions from clinical research \nreports. This includes the utilization of Large Language Models (LLMs) to extract entities and relationships from RCT \nreports (9, 10). However, these methods are predominantly based on the broad framework of evidence-based medicine, \nwhich emphasizes Population-Intervention-Comparation-Outcome (PICO) related concepts, such as Trialstreamer and \nthe EvidenceMap (11, 12). While some of these methods involve effect size and direction  (13, 14), extracting and \nrepresenting research design information from various sources of evidence , which is essential for triangulation,  \nremains a subject for ongoing research . Most recently, t here are attempts trying to accelerate the process by taking \nadvantage of computable knowledgebase, such as SemMedDB and natural language processing tools (15). However, \nthe accuracy and recall rates of medical concepts and their relationships extracted in SemMedDB are relatively low, \nposing significant challenges to triangulation based on a complete medical evidence system. \n \nIn this study, we try to examine the capabilities of LLMs in extracting intervention-outcome concepts, determining \ncausal directions, as well as identifying study design information. Our objective is to develop an automatic approach \nto aggregate various lines of SDoH-related evidence across different study designs into a computable and comparable \nformat that is ready for quantitative evidence triangulation. \n \nMethods \nOur procedure begins by collecting titles and abstracts from relevant literature. We then apply a LLM to systematically \nprocess these texts across various study designs, extracting key outcomes and methodological details. This leads to \nthe aggregation of data into a coherent, transparent dataset that is ready for triangulation analysis (Fig. 1.). \n \n \nFigure 1. Overall workflow of automatic evidence triangulation using LLM \n \n(1) Data source \nRegarding data sources, the study utilizes literature categorized under publication types marked as meta -analysis, \nsystematic reviews, observational studies, randomized controlled trials, clinical trials and related types available on \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304457doi: medRxiv preprint \n 3 \nPubMed. The MeSH terms \"cardiovascular diseases\" and \"Diet, Food, and Nutrition\" are utilized as search terms, with \nMeSH major topic as the search field. The resulting search query is outlined below: “(cardiovascular diseases[MeSH \nMajor Topic]) AND (Diet Food,and Nutrition[MeSH Major Topic])”.For studies employing mendelian randomization \n(not a conventional publication type in PubMed), we additionally narrow ed down the search to include only \npublication titles and abstracts containing the phrase \"Mendelian randomization\". In total, 4,268 articles were retrieved. \nThis first dataset will be the corpus for validations for extracted results by LLM. \n \nTo provide a specific example of the relationship between a particular disease and dietary factors, we further selected \nsalt and hypertension as the intervention -outcome pair and retrieved relevant publications. Consistent with the \naforementioned limitations on publication types, we refined the search terms to include MeSH terms related to salt \nand hypertension. The constructed search query is as follows: (\"sodium chloride\"[MesH Major Topic] OR \n\"salts\"[MeSHMajor Topic]) AND (\"hypertension\"[MesH Major Topic]  OR (\"bloodpressure\"[MeSH Major Topic] \nOR \"blood pressure determination[MesH Major Topic] OR \"arterial pressure\"[MesH Major Topic])). Ultimately, we \nretrieved a total of 289 articles. This second dataset will be used to exhibit the formation of automatic-extracted ready-\nfor-triangulation evidence dataset in the results section. \n \n(2) LLM-based study results extraction \nFor the task of extracting precise and insightful results from health -related documents, we employed GPT-4 (model \nversion: gpt-4-0125-preview), a GPT-based (Generative Pre-trained Transformer) LLM model introduced by OpenAI \n(16, 17), renowned for its high performance in information extraction within health domain(18, 19).  \n \nThe specific extraction tasks for the model are designed as following: \n• Identification of study design \nThe initial step involves using GPT-4 to categorize the study design present in medical abstracts. The designs \nconsidered include RCT, MR, OS, and META. \n• Extraction for meta-analyses and systematic reviews \nFor abstracts identified as META , we ask GPT -4 to extract the number of included studies and their \nrespective study designs. This step is crucial for understanding the strength and diversity of evidence in these \ncomprehensive analyses. \n• Primary result identification \nNext, we ask GPT -4 to identify the primary result from each abstract. This involves recognizing the main \nfindings that the study reports, which is essential for summarizing the study’s major contribution to the field. \n• Intervention/Exposure and outcome extraction \nFollowing the identification of the primary results, the model extracts key entities including intervention or \nexposure and the corresponding primary outcome. \n• Relationship and statistical significance \nFirst the model extracts the direction of the relationship from the intervention/exposure to the outcome. The \nmodel assesses whether the intervention/exposure increases, decreases or an effect was not found. Then we \nask GPT -4 to extract statistical significance of the identified relationship, ensuring the ability to \ndistinguishing positive results from negative results.  \n• Population, Participant Number and Comparator Group information \nAdhering to the standard representation medical evidence, we ask the model to extract information on the \npopulation condition under study, the number of participants, and details of the comparator group if \napplicable.  \n \nThis prompt is to follow a logical progression from study-level information (study design), to more specific study \nresult extraction  (intervention/exposure, primary outcome, relationship direction, statistical significance) , then \ncontextual details (population, participant number, comparator). Fig.2. shows a graphical illustration of the overflow \nand logics of the designed prompt. For each abstract, GPT-4 first determines the study design. If the abstract pertains \nto a meta-analysis, the model then identifies the number and types of included studies. Subsequently, it locates the \nprimary result, extracts relevant details about the intervention/exposure and outcome, and assesses the direction and \nsignificance of the relationship. Information about the study population, the number of participants, and comparator \ngroup details are also extracted, providing a comprehensive overview of each study's evidence. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304457doi: medRxiv preprint \n 4 \n \nFigure 2. A flowchart describing the overall logic of using LLM to extract medical evidence in structured format. Gray \nboxes represent each part of the prompt in each step. Circled numbers (1-9) represent the extracted information by the model. \nResults \n(1) Human evaluation of model extraction \nTo assess the efficacy and precision of our model in organizing medical evidence into a structured format, we engaged \nfive experts from relevant domains to review the extraction outcomes.  Expert background s include oncology, \ncardiovascular disease, clinical pharmacy, public health, and pediatric clinical and big data.  A hundred publications \nwere randomly selected for this evaluation process. The experts utilized a Likert Scale (a scale that measures opinions \nfrom \"strongly agree\" to \"strongly disagree.\" ) to rate the LLM's performance on the extraction tasks. Before the \nevaluation, we provided the experts with an overview of the study's goals and rationale to ensure a comprehensive \nunderstanding. \n \nEach of the five experts was tasked with evaluating the performance across five criteria:  \n• Intervention/exposure, comparator and primary outcome accuracy \n• Study design accuracy \n• Population accuracy \n• Relationship reasoning \n• Statistical significance reasoning \n \nTo guarantee the reliability of our evaluation, we had two different experts review ing each extraction result. We \ncalculated the average scores from both reviewers for each publication and then determined the overall mean scores \nfor entity accuracy. For relationship evaluation, we analyzed the distribution of correct, incorrect, or missed extractions. \nExperts also provided written feedback memos on any issues they identified in the extracted results. These feedback \nmemos were summarized and analyzed. \n \nIn the evaluation feedback  (Table 1) (30 missing ratings) , the model performed notably well in extracting study \ninformation, with high accuracy in identifying exposure, comparator, and outcomes (mean score 4.69), and study \ndesign (mean score 4.84). The model frequently achieved perfect scores for accurate information extraction, especially \nin METAs, with minimal instances of inaccuracies. In terms of identifying study populations, the model achieved a \nhigh mean score of 4.68, effectively pinpointing nearly all relevant entities. For reasoning about relationships and \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304457doi: medRxiv preprint \n 5 \nstatistical significance, the model demonstrated satisfactory accurate reasoning, with a substantial proportion of \ncorrect instances compared to incorrect or absent reasoning.  \n \nTable 1. Expert human evaluation results \nSCORE INDICATOR DESCRIPTION COUNT \n  Exposure, Comparator, and Outcomes Accuracy  mean=4.69 \n5 Completely accurate Accurately extracted all relevant information  153/200 (76.5%) \n4 Mostly accurate Extracted most of the relevant information with minor omissions 31/200 (15.5%) \n3 Partially accurate Extracted significant relevant information with notable omissions  13/200 (6.5%) \n2 Poorly accurate Extracted key aspects but missed a substantial amount of detail  0/200 (0.0%) \n1 Inaccurate Failed to extract key information  1/200 (0.5%) \n  Study Design Accuracy mean=4.84 \n5 Completely accurate Correctly identified design and extracted study counts for META  181/200 (90.5%) \n4 Mostly accurate Correctly identified design but errors in study counts or designs for META 0/200 (0.0%) \n3 Partially accurate Correctly identified design but not classified into desired categories  4/200 (2.0%) \n2 Poorly accurate Failed to identify valid design and included studies for META  2/200 (1.0%) \n1 Inaccurate Incorrectly identified design and study counts for META  5/200 (2.5%) \n  Study Population Accuracy mean=4.68 \n5 Almost all identified Almost all relevant entities correctly identified  153/200 (76.5%) \n4 Mostly identified Most relevant entities correctly identified 7/200 (3.5%) \n3 Partially identified A significant number of entities correctly identified  13/200 (6.5%) \n2 Few identified Few entities correctly identified  4/200 (2.0%) \n1 Very few identified Very few entities correctly identified 3/200 (1.5%) \nCategorical indicators \nRelationship Direction Reasoning   \nCorrect 145/200 (72.5%) \nIncorrect 34/200 (17.0%) \nNot reasoned 21/200 (10.5%) \nStatistical Significance Reasoning   \nCorrect 157/200 (78.5%) \nIncorrect 22/200 (11.0%) \nNot reasoned 21/200 (10.5%) \n \nThe expert evaluation feedback memo highlights several key points: 1) Full texts of the articles are needed to evaluate \nif some of the extracted results are accurate . 2) It notes discrepancies in outcome measures, with most indicators \nshowing decline and variations in significance and direction across different outcomes. 3) Some studies were identified \nas potentially out of scope, such as animal experiments and descriptive articles. 4) Additionally, there was confusion \nabout the direction of health actions and outcomes.  \n \nRegarding the mentioned feedbacks, we look forward to improving our future model by : 1) Acquire full texts using \nPubMed Central to provide a more comprehensive corpus. 2) Using a finer -grained representation of study results, \ni.e., one-to-one relationships among multiple health actions and outcomes. 3) Adding a forehanded task to identify if \nthe study matches the scope of design. 4)  Extracting the directions of health actions, e.g.,  lower or higher; and the \ndirections of outcome measure, e.g., improvement or worsen. \n \n(2) A case of evidence triangulation of salt on blood pressure \nIn our analysis, we explored the effect of salt intake on blood pressure using a dataset derived from 289 studies. This \ndataset included 36 METAs, 124 RCTs, 117 OSs, and 12 studies where the design was not identified. We employed \nthe proposed LLM pipeline to extract structured information based on the PICO framework along with primary \nefficacy results and other relevant metadata from these publications. A sample of the extracted evidence is presented \nin Table 2. It is important to note that no MR  studies were found to support any direction of the relationship in this \ndataset.  \n \nThe evidence from META s and RCTs mainly supports the intervention of reducing salt intake. In contrast, OSs \ntypically involved higher salt intakes as exposures. Findings from META s consistently indicate that lowering salt \nintake is linked with reduced blood pressure levels. Similarly, OSs suggest that higher salt intake is associated with \nan increased risk of hypertension-related conditions. In summary, the evidence across different study designs points \ntowards a beneficial effect of lower salt consumption on blood pressure, providing a clear narrative on the impact of \ndietary salt on cardiovascular health. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304457doi: medRxiv preprint \n 6 \nTable 2. Example of automatic-extracted ready-for-triangulation evidence dataset of salt-on-hypertension \n \n \nConclusion and discussion \nTo conclude, this study demonstrates the potential using LLM to automate the extraction and triangulation of SDoH-\nrelated evidence from diverse study designs. Through expert evaluations and a focused case study on the impact of \nsalt intake on blood pressure, we illustrated that LLM can significantly simplify the synthesis of medical evidence, \nenhancing the efficiency of evidence -based decision -making. Despite its promise, the study faced challenges in \naccurately classifying study designs and interpreting outcomes due to inconsistencies in the data. Additionally, the \nreliance on expert evaluations introduces subjective bias. Moving forward, addressing th ese limita tions through \ncontinuous model training and exploring more objective evaluation methods is crucial for maximizing the utility of \nLLMs in evidence triangulation. \n \nDecision-making should be grounded in causal relationships between interventions and outcomes, wh ile prediction \ncan rely solely on correlative relationships. In this study, our objective is not for an automatic meta-analysis focusing \nsolely on evidence derived from the same study design (such as RCTs). Instead, we aim to utilize LLMs to enable a \nconvergence analysis between results obtained from different types of study designs, known as triangulation analysis. \nWe argue that the primary distinction between meta-analysis and triangulation analysis lies in the consideration of \nevidence from various study designs. The former assesses the consistency of evidence within a single study design, \nwhile the latter examines the convergence of conclusions across diverse study designs. Although there are currently \nno widely accepted quantitative methods for assessing convergence, insights may be drawn from Cumulative Evidence \nIndex (CEI) of convergence in neurobiological experimental results, primarily utilizing a vote -counting approach \nacross different study designs (20, 21).  \n \nAcknowledgement \nThis study was funded by the National Key R&D Program for Young Scientists (Project number 2022YFF0712000 \nto JD) and the National Natural Science Foundation of China (Project number 72074006 to JD).  We declare no \nconflicts of interest. \n \nWe are grateful to the following experts for their invaluable contributions and insightful feedback on this study: Dr. \nGuohua He from Sun Yat-sen University First Affiliated Hospital, Dr. Na He from Peking University Third Hospital, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304457doi: medRxiv preprint \n 7 \nDr. Zhenhua Lu from Peking University Cancer Hospital, Dr. Weihua Hu from Peking University, and Dr. Mingming \nZhao from Peking University Third Hospital.  \n \nAuthors contribution \nXS was responsible for manuscript drafting, data collection, data analysis, literature review, results evaluation, and \nthe formation of the conclusion. WZ was involved in data collection. CY was involved in method design. JD was \ninvolved in research conceptualization, and responsible for primary supervision. \n \n \nReferences \n1. Roth GA, Mensah GA, Johnson CO, Addolorato G, Ammirati E, Baddour LM, et al. Global burden of \ncardiovascular diseases and risk factors, 1990 –2019: update from the GBD 2019 study. Journal of the American \ncollege of cardiology. 2020;76(25):2982-3021. \n2. Ali MK, Sudharsanan N, Thirumurthy H. Behaviour change in the era of biomedical advances. Nature human \nbehaviour. 2023;7(9):1417-9. \n3. Hernán MA, Hsu J, Healy B. A Second Chance to Get Causal Inference Right: A Classification of Data \nScience Tasks. CHANCE. 2019;32(1):42-9. \n4. Munafò MR, Davey Smith G. Robust research needs many lines of evidence. Nature. 2018;553(7689):399-\n401. \n5. Gurevitch J, Koricheva J, Nakagawa S, Stewart G. Meta -analysis and the science of research synthesis. \nNature. 2018;555(7695):175-82. \n6. Lawlor DA, Tilling K, Davey Smith G. Triangulation in aetiological epidemiology. International journal of \nepidemiology. 2016;45(6):1866-86. \n7. Munafò MR, Higgins JPT, Smith GD. Triangulating Evidence through the Inclusion of Genetically Informed \nDesigns. Cold Spring Harbor perspectives in medicine. 2021;11(8). \n8. Sae-Jie W, Supasai S, Kivimaki M, Price JF, Wong A, Kumari M, et al. Triangulating evidence from \nobservational and Mendelian randomization studies of ketone bodies for cognitive performance. BMC medicine. \n2023;21(1):340. \n9. Joseph SA, Chen L, Trienes J, Göke HL, Coers M, Xu W, et al. FactPICO: Factuality Evaluation for Plain \nLanguage Summarization of Medical Evidence. arXiv preprint arXiv:240211456. 2024. \n10. Wadhwa S, DeYoung J, Nye B, Amir S, Wallace BC, editors. Jointly extracting interventions, outcomes, and \nfindings from RCT reports with LLMs. Machine Learning for Healthcare Conference; 2023: PMLR. \n11. Marshall IJ, Nye B, Kuiper J, Noel -Storr A, Marshall R, Maclean R, et al. Trialstreamer: A living, \nautomatically updated database of clinical trial reports. Journal of the American Medical Informatics Association : \nJAMIA. 2020;27(12):1903-12. \n12. Kang T, Sun Y, Kim JH, Ta C, Perotte A, Schiffer K, et al. EvidenceMap: a three -level knowledge \nrepresentation for medical evidence computation and comprehension. Journal of the American Medical Informatics \nAssociation : JAMIA. 2023;30(6):1022-31. \n13. Mayer T, Marro S, Cabrio E, Villata S. Enhancing evidence -based medicine with natural language \nargumentative analysis of clinical trials. Artificial intelligence in medicine. 2021;118:102098. \n14. Whitton J, Hunter A. Automated tabulation of clinical trial results: A joint entity and relation extraction \napproach with transformer-based language representations. Artificial intelligence in medicine. 2023;144:102661. \n15. Liu Y, Gaunt TR. Triangulating evidence in health sciences with Annotated Semantic Queries. medRxiv. \n2022:2022.04. 12.22273803. \n16. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al. Language models are few -shot \nlearners. Advances in neural information processing systems. 2020;33:1877-901. \n17. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al. Gpt-4 technical report. arXiv preprint \narXiv:230308774. 2023. \n18. Lee P, Goldberg C, Kohane I. The AI revolution in medicine: GPT-4 and beyond: Pearson; 2023. \n19. Korngiebel DM, Mooney SD. Considering the possibilities and pitfalls of Generative Pre -trained \nTransformer 3 (GPT-3) in healthcare delivery. NPJ Digital Medicine. 2021;4(1):93. \n20. Matiasz N, Wood J, Silva A. Quantifying convergence and consistency. Authorea Preprints. 2023. \n21. Matiasz NJ, Wood J, Doshi P, Speier W, Beckemeyer B, Wang W, et al. ResearchMaps. org for integrating \nand planning research. PloS one. 2018;13(5):e0195271. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304457doi: medRxiv preprint "
}