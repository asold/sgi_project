{
  "title": "Automated Educational Question Generation at Different Bloom’s Skill Levels Using Large Language Models: Strategies and Evaluation",
  "url": "https://openalex.org/W4400210662",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Scaria, Nicy",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Chenna, Suma Dharani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288798953",
      "name": "Subramani, Deepak",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4381956373",
    "https://openalex.org/W2037789405",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W4294768175",
    "https://openalex.org/W4294768397",
    "https://openalex.org/W3197661515",
    "https://openalex.org/W4385573917",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4288059420",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3048263575",
    "https://openalex.org/W2116817443",
    "https://openalex.org/W3121857126"
  ],
  "abstract": null,
  "full_text": "Automated Educational Question Generation at\nDifferent Bloom’s Skill Levels using Large\nLanguage Models: Strategies and Evaluation\nNicy Scaria1 , Suma Dharani Chenna1,2 , Deepak Subramani1\n1 Computational and Data Sciences, Indian Institute of Science, India\n2 School of Computer Science and Engineering, VIT-AP University, India\n{nicyscaria,deepakns}@iisc.ac.in, sumadharanichenna@gmail.com\nAbstract. Developing questions that are pedagogically sound, relevant,\nand promote learning is a challenging and time-consuming task for edu-\ncators. Modern-day large language models (LLMs) generate high-quality\ncontent across multiple domains, potentially helping educators to de-\nvelophigh-qualityquestions.Automatededucationalquestiongeneration\n(AEQG) is important in scaling online education catering to a diverse\nstudent population. Past attempts at AEQG have shown limited abil-\nities to generate questions at higher cognitive levels. In this study, we\nexamine the ability of five state-of-the-art LLMs of different sizes to gen-\nerate diverse and high-quality questions of different cognitive levels, as\ndefined by Bloom’s taxonomy. We use advanced prompting techniques\nwith varying complexity for AEQG. We conducted expert and LLM-\nbased evaluations to assess the linguistic and pedagogical relevance and\nqualityofthequestions.OurfindingssuggestthatLLMscangeneraterel-\nevant and high-quality educational questions of different cognitive levels\nwhen prompted with adequate information, although there is a signifi-\ncant variance in the performance of the five LLMs considered. We also\nshow that automated evaluation is not on par with human evaluation.\nKeywords: Large Language Models· Automated Educational Question\nGeneration · Bloom’s Taxonomy.\n1 Introduction\nTransformer-based pre-trained large language models developed in recent years\nhavedrasticallyimprovedthequalityofnaturallanguagegeneration(NLG)tasks\n[24]. With an exponential increase in training data and model size, these models\ncan generate complex text with human expert-level quality. The release of Ope-\nnAI’s ChatGPT made LLMs accessible to a wider audience who are not experts\nin natural language processing (NLP), allowing them to use them for their daily\ntasks. The language models are tuned to follow the user instructions through\ninstruction-tuning [24]. They have zero-shot capabilities [10], which means that\nThis is a preprint. The Version of Record of this contribution is published in In-\nternational Conference on Artificial Intelligence in Education (LNAI,volume 14830),\nand is available online at https://doi.org/10.1007/978-3-031-64299-9_12\narXiv:2408.04394v1  [cs.CL]  8 Aug 2024\n2 N. Scaria et al.\nif you prompt the LLM with detailed task descriptions, the model will create\nmeaningful outputs. These LLMs have the potential to be used in different ways\nin education [9], including the creation of personalized content, assessments, and\nfeedback.\nHigh-quality assessments enable learners to deeply engage with the subject\nand relate their learning to the real world. Assessments that focus on different\ncognitive skills as defined in Bloom’s taxonomy levels [2] (described in Table 1)\nhelp educators identify the gaps in student learning. This information allows\nthem to adapt their teaching to better support students and also helps students\nunderstand their strengths and weaknesses. However, creating such assessments\nrequires significant time and effort from educators [11]. Automated Educational\nQuestion Generation (AEQG) systems reduce the effort and cognitive load on\nteachers. Past research on AEQG methods required context information for the\nmodels to generate high-quality questions. Educational information is available\nfrom multiple sources today and choosing the right resource is challenging.\nRelated Work: In the pre-LLM era, AQG research focused mainly on gen-\nerating questions using question-answer datasets such as SQuAD 2.0, and NQ.\nThese data sets contained a context and an answer for which the question had\nto be created [25]. However, the limited availability of public datasets impeded\nthe progress of AQG systems capable of producing good quality questions. Re-\ncent research in question generation is focused on using pretrained or fine-tuned\nLLMs for the process. Encoder decoder models, such as the Text-To-Text Trans-\nfer Transformer (T5) and decoder-only models such as GPT3, along with context\ninformation, were used to generate questions [17]. Pre-training these models with\neducational text also improved the quality of the questions generated [3].\nRecent research has shown promising results in evaluating the quality of\nmachine-generated content using LLMs using Chain-of-Thought (CoT) prompt-\ning on different evaluation criteria. G-EVAL [13], an evaluator model based\non GPT4, significantly outperformed previous models and aligned with human\njudgments on summarization tasks. However, the results of some studies that\nused fine-tuned GPT3 models to evaluate the pedagogical quality of machine-\ngenerated questions were unsatisfactory [16,3]. Human expert or crowd evalua-\ntions have been extensively used to analyze the pedagogical quality of machine-\ngenerated questions [19,6].\nThe questions generated by most AQG models generally test lower-order\nskills [20] or create questions that have answers directly mentioned in the text\n[25]. These questions are not enough to test the higher-order cognitive skills\nof students. Bloom’s taxonomy [2] serves as a guide for educators to generate\nquestions to test different cognitive skills. Recent work [18] has used GPT4 to\ndevelop course material based on Bloom’s taxonomy.\n1.1 Objective and Research Questions\nOur approach utilizes the knowledge of the content inherently present in LLMs\nalong with the addition of technical information on the question generation pro-\ncess in the prompt to generate educational questions. Although LLMs excel in\nAEQG at Different Bloom’s Skill Levels using LLMs 3\nTable 1.Revised Bloom’s taxonomy [2] in ascending order in the cognitive dimension\nBloom’s level Description\nRemember Retrieve relevant knowledge from long-term memory.\nUnderstand Construct meaning from instructional messages, including oral, writ-\nten, and graphic communication.\nApply Carry out or use a procedure in a given situation.\nAnalyze Break material into foundational parts and determine how parts relate\nto one another and the overall structure or purpose\nEvaluate Make judgments based on criteria and standards.\nCreate Put elements together to form a coherent whole; reorganize into a new\npattern or structure.\nvarious downstream tasks, they produce errors and inconsistencies [8], which can\ncompromise the quality of the questions generated. This also varies significantly\nbetween different LLMs. Therefore, evaluating the quality of the questions gen-\nerated by LLMs is essential. While metrics such as the BLEU score or perplexity\ncan assess machine-generated questions, they typically only examine linguistic\ncharacteristics [22]. In the present work, we perform a manual expert evalua-\ntion using the services of two educators in the AEQG topic’s domain and an\nautomated LLM evaluation using an LLM that is not employed for AEQG.\nWe used zero-shot and few-shot techniques and CoT prompting to generate\nquestions for a graduate-level data science course using LLMs of different sizes.\nFive different prompt strategies of varying complexity were used to create these\nquestions. Then, we performed a manual expert evaluation using the services of\ntwo educators in the AEQG topic’s domain and an automated LLM evaluation\nusing an LLM that is not employed for AEQG. The evaluation was performed on\na nine-item rubric to assess their linguistic and pedagogical quality [7] by experts\nand the LLM. The LLM evaluation is performed as a zero-shot classification task\nthrough a specially designed prompt.\nSpecifically, we investigated answers to the following research questions.\nRQ1: Can instruction fine-tuned modern LLMs create high-quality and diverse\neducational questions at different cognitive levels based on Bloom’s taxonomy?\nRQ2: Does the size of the LLM significantly impact the model’s performance in\neducational question generation?\nRQ3: How does the amount of information provided in the prompt affect the\nquality of the questions generated?\nRQ4: Can LLMs create questions that are relatable to a specific population or\ncontext?\nRQ5:Caninstructionfine-tunedLLMsevaluategeneratededucationalquestions\neffectively, similar to human evaluators, when given the same instructions?\nIn what follows, we first discuss the methodology. Then, we present and\nanalyze the results before concluding with directions for future research.\n4 N. Scaria et al.\n2 Methodology\nOur study consists of two parts. We use modern LLMs for AEQG in the first\npartthroughvariouspromptingstrategies.Inthesecondpart,weperformhuman\nevaluation and LLM evaluation.\n2.1 Language Models\nTraining a language model from scratch or fine-tuning the available models is\nexpensive due to constraints in the availability of educational data and the\ncost associated with training. Therefore, we used a mix of open-source and\nproprietary state-of-the-art LLMs for the study. The models used for question\ngeneration are Mistral (Mistral-7B-Instruct-v0.1), Llama2 (Llama-2-70b-chat-\nhf), Palm 2 (chat-bison-001), GPT-3.5 (gpt-3.5-turbo-0613), and GPT-4 (gpt-\n4-0613). Among these, Mistral has 7 billion parameters, and GPT models are\nrumored to have trillions of parameters1. For LLM-based evaluation of the ques-\ntions, we used Gemini Pro (gemini-pro).\n2.2 Question Generation\nWe used the five LLMs mentioned in Sect. 2.1 to generate questions of different\ncognitive levels. A higher temperature setting in LLMs results in a varied and\nunpredictable text, while a lower temperature setting makes the model output\nmore deterministic and repetitive. Thus, we set the temperature of the LLMs at\n0.9 to promote variety and diversity in the generated questions.\nContent : The educational questions were generated for a graduate-level data\nscience course comprising topics ranging from traditional machine learning algo-\nrithms, such as linear regression, to advanced topics in natural language process-\ning, such as prompt engineering. We did not provide domain-specific information\nor context on these topics to the models for question generation. This approach\nwas guided by the hypothesis that these models, trained using large amounts of\nrecent Internet data, would possess inherent knowledge related to these contem-\nporary course topics.\nPrompt Design : In the present study, we generated questions by instructing\nthe models with five prompt styles/strategies (PS1 to PS5), each differing in\ncomplexity. These prompts followed specific techniques of pattern reframing,\nitemizing reframing, and assertions to make it easier for the instruction fine-\ntuned LLMs to follow the specific instructions [15]. Furthermore, the prompts\nencouraged the model to incorporate Indian-specific examples or context within\nthe question to ensure relevance for Indian students. The first set of prompts\n1 The information about the number of parameters of the model is not publicly avail-\nable\nAEQG at Different Bloom’s Skill Levels using LLMs 5\n(PS1) consisted solely of these core instructions. In contrast, subsequent sets\nprogressively added more specific information and instructions in the prompt to\nfurther refine the quality and relevance of the generated questions.\nThere were mainly three significant additions to the prompts. First, the\nprompts were augmented with CoT instructions to make the LLM think se-\nquentially about how to proceed with the task. CoT prompting has been shown\nto improve the quality of LLM-generated content [23]. In the prompt, the LLM\nwas also given the persona of a graduate-level university course instructor creat-\ning questions for their students [15]. Second, the questions included definitions\nof the six cognitive levels of the revised Bloom’s taxonomy. This approach was\ntaken under the hypothesis that augmenting the LLM with explicit knowledge\nof the revised Bloom’s taxonomy levels would enhance the quality of the gener-\nated questions. Third, an expert-crafted example was provided for each Bloom’s\ntaxonomy level. This few-shot approach, proven effective in different generative\nmodels [12], leverages human-crafted questions to guide the LLM’s understand-\ning of how the questions need to be framed and, in turn, enhance its question\ngeneration capabilities. Using these three, we created four different prompts:\n(PS2) CoT prompt with skill explanation, (PS3) CoT prompt with example\nquestions, (PS4) CoT prompt with skill and example questions, and (PS5) CoT\nprompt with skill, skill explanation, and example questions.\nWe gave the same prompt to all LLMs in a specific combination, except for\nthe topic-specific variables corresponding to each course topic, as given in the\nrepository2. Each LLM generated six questions, one for each level of Bloom’s\ntaxonomy corresponding to the 17 course topics. Each model generated 102\nquestions, resulting in 510 questions for one combination, making it 2550 for the\nfive prompt combinations.\n2.3 Human Evaluation\nTwo experts evaluated the AEQG questions. Both experts deeply understand\ndata science concepts and have experience teaching the subject to large graduate\nclasses. The experts assessed the relevance and quality of the questions based on\na nine-item rubric (Table 2; a modified version of [7]).\nThe experts were presented with LLM-generated questions in random order\nwithout information other than the course topic on the prompt corresponding to\nthe question. The experts hierarchically assessed each question, starting from the\ntop of the rubric to the bottom. In the evaluation, each group of the evaluation\ncriteria, as indicated in Table 2, has a stopping point. This structured approach\nstreamlines the evaluation process, minimizing the overall time and effort re-\nquired for expert annotation. IfUnderstandable is marked ‘no’, then none of the\nsubsequent items are evaluated for that question and are automatically marked\nas ‘NA’, indicating not applicable. This design choice reflects the underlying\nprinciple that further evaluation of a question that is not understandable does\nnot make sense. Similarly, within group 2, if the answer toclear is ‘no’, then the\n2 https://github.com/nicyscaria/AEQG_Blooms_Evaluation_LLMs\n6 N. Scaria et al.\nevaluation stops, and the remaining items are automatically marked as ‘NA’. Ad-\nditionally, if the response toClear is ‘more_or_less’, then theRephrase criteria\nof group 3 should be marked, and the evaluator should rephrase the question for\nclarity. The rephrased version of the question will be used for further evaluation.\nIf the answer toAnswerable in group 3 is ‘no’, the evaluation is stopped, and\nthe remaining items of the rubric are marked ‘NA’. In group 4, if eitherCen-\ntral or WouldYouUseIt is marked ‘no’, then the evaluation is stopped, and the\nBloom’sLevel criteria is labeled ‘NA’; otherwise, the experts select the Bloom’s\nlevel for the question and concludes the evaluation of one question. The process\nis repeated for all the questions.\nTable 2.Hierarchical nine-item rubric used to evaluate questions generated by LLMs\nRubric item Definition and response option\nUnderstandable Could you understand what the question is asking?(yes/no)\nTopicRelated Is the question related to the topic given in the prompt? (yes/no)\nGrammatical Is the question grammatically well-formed? (yes/no)\nClear Is it clear what the question asks for? (yes/more_or_less/no)\nRephrase Could you rephrase the question to make it clearer and error-free?\n(yes/no)\nAnswerable Can students answer the question with the information or context\nprovided within?(yes/no)\nCentral Do you think being able to answer the question is important to work\non the topic given in the prompt?(yes/no)\nWouldYouUseIt If you were a teacher teaching the course topic would you use this\nquestion or the rephrased version in the course?(yes/maybe/no)\nBloom’sLevel What is the Bloom’s skill associated with the question? (remember,\nunderstand, apply, analyze, evaluate, and create)\nThe AEQG questions were considered high quality if they met the following\ncriteria: (1) experts marked ‘yes’ forUnderstandable, Grammatical, Clear, and\nAnswerable; (2) received a ‘yes’ or ‘maybe’ forWouldYouUseIt; or (3) being\nmarked ‘yes’ for ‘more_or_less’ in theClear criteria and subsequently marked\n‘yes’ forRephrase. Furthermore, we utilizedBloom’sSkill to understand whether\nthe LLM adheres to the instructions provided in the prompt. The LLM adhered\nto the instructions provided if theBloom’sSkill labels of the experts match the\nBloom’s skill level on the prompt.\nExperts perceive the questions generated by LLMs in different ways. This\nperception is influenced by various factors, such as their preference for writing,\npersonal assumptions, prior knowledge, and attention to detail [1]. Therefore, it\nis essential to have a measure to ensure the consistency of the expert evalua-\ntion. We measure inter-rater reliability using percentage agreement and Cohen’s\nKappa κ [14]. For the ordinal metrics,Clear, WillYouUseIt and Bloom’sLevel,\nAEQG at Different Bloom’s Skill Levels using LLMs 7\nwe use quadratic weighted Cohen’sκ [5] instead of simple Cohen’sκ to penalize\nsituations with a significant rating difference.\nAlong with the metrics discussed above, we explored the ability of LLMs to\ncreate questions relatable to a specific population or context, which, in this case,\nis India. To understand this, we curated and analyzed the contexts and themes\nspecific to India that came up in the questions.\n2.4 Automated Evaluation\nClearly, the above evaluation by a human expert is a laborious process. Auto-\nmated evaluation offers a scalable and efficient alternative for assessing large-\nscale educational content. We use Gemini Pro for the LLM-based evaluation of\nthe generated questions. To ensure deterministic behavior, we set the decoding\ntemperature of the model to 0. In automated evaluation, we assess the quality\nof LLM-generated questions without reference questions using the same crite-\nria outlined in Sect.2.3. Recent studies have demonstrated the ability of LLMs\nto perform a reference-free evaluation for a variety of NLG tasks [13,21]. The\nprompt used in the prompt-based evaluator consisted of two components: (1) a\ndetailed description of the evaluation criteria, evaluation instructions (instruc-\ntions to evaluate the questions in a hierarchical manner as discussed in Sect.2.3)\nalong with the question and the course topic for which the question was gen-\nerated, and (2) CoT instructions describing the evaluation steps, along with\nproviding the evaluator LLM the persona of a graduate-level data science course\ninstructor. The detailed prompt template can be found on github.\nIn addition to automated evaluation, we assessed the linguistic quality of the\nAEQG questions using a diversity measure based on the Paraphrase In N-gram\nChanges (PINC) score [4].\nPINC (s, c) = 1\nN\nNX\nn=1\n\u0012\n1 − |n-grams ∩ n-gramc|\n|n-gramc|\n\u0013\n(1)\nPINC score is generally used to measure the novelty of n-grams in the auto-\nmatically generated paraphrase of a sentence. In our case, we wanted to check\nif the questions generated by an LLM for a specific Bloom’s skill use the same\nstructure or words on different topics. For that, we considered every question\ngenerated for a specific skill by the model as the source and calculated the PINC\nscore considering every other question of the same skill and by the same model\nas the candidate question. Finally, the average of these scores was calculated\nfor the AEQG questions generated by each LLM. A higher average PINC score\nindicates considerable diversity in the AEQG task.\n3 Results and Analysis\nWe will be releasing the dataset, ‘DataScienceQ’4 containing 2550 questions\ngenerated for the present study. First, we present and analyze the results of the\n8 N. Scaria et al.\nexpert evaluation of these 2550 AEQG questions. We start by examining the\nagreement between experts on their evaluation using the percentage agreement\nand modified Cohen’sκ values (Table 3). The percentage agreements and Co-\nhen’s κ values are calculated only for questions not labeled ‘NA’ as discussed\nin Sect.2.3. The values in the table indicate that there is substantial agreement\nbetween experts on different evaluation criteria. The agreement is highest for the\ncriteria TopicRelated, Grammatical, andCentral. As expected, subjective criteria\nlikeRephrase and WouldYouUseIt have the lowest agreement. In our analysis, an\nAEQG question is considered as “High Quality” only when both evaluators rate\nit as “High Quality”. To assess alignment with Bloom’s taxonomy, the subsequent\nanalysis focused only on these “High Quality” questions. In what follows, we dis-\ncuss the results for each research question stated in Sect. 1.1. Table 4 presents\nthe key evaluation metrics (Sect. 2.3) for the analysis. Quality is measured as\nthe percentage of AEQG questions that were selected as “High Quality” and skill\nis measured as the percentage of “High Quality” questions judged by experts to\nbe at the same Bloom’s taxonomy levels as the one given in the prompt to the\nLLM for the AEQG task.\nTable 3. Expert inter-annotator agreement on the nine-item hierarchical rubric for\nAEQG questions.\nSimple prompt CoT & skill explanation CoT & example\nRubric item % agree κ % agree κ % agree κ\nUnderstandable 99.60% 0.67 99.61% 0.80 100.00% 1.00\nTopicRelated 99.80% 0.95 99.80% 0.80 98.80% 0.93\nGrammatical 100.00% 1.00 100.00% 1.00 100.00% 1.00\nClear 91.75% 0.67 97.42% 0.62 94.30% 0.61\nRephrase 90.38% 0.76 90.91% 0.62 80.77% 0.59\nAnswerable 93.41% 0.65 96.47% 0.69 94.75% 0.61\nCentral 97.42% 0.78 99.77% 0.98 99.77% 0.94\nWouldYouUseIt 89.92% 0.58 94.82% 0.66 96.81% 0.69\nBloom’sLevel 82.07% 0.83 88.14% 0.90 90.00% 0.89\nCoT, skill, and example CoT, skill, skill explanation and example\nRubric item % agree κ % agree κ\nUnderstandable 100.00% 1.00 100.00% 1.00\nTopicRelated 99.80% 0.99 99.20% 0.96\nGrammatical 100.00% 1.00 100.00% 1.00\nClear 93.30% 0.53 92.74% 0.66\nRephrase 92.31% 0.73 75.00% 0.53\nAnswerable 96.54% 0.81 93.89% 0.68\nCentral 100.00% 1.00 100.00% 1.00\nWouldYouUseIt 95.64% 0.85 95.29% 0.82\nBloom’s Level 90.36% 0.93 92.65% 0.93\nRQ1: Can instruction fine-tuned modern LLMs create high-quality\nand diverse educational questions at different cognitive levels based\non Bloom’s taxonomy?Among all AEQG questions from the different LLMs\n4 https://github.com/nicyscaria/AEQG_Blooms_Evaluation_LLMs\nAEQG at Different Bloom’s Skill Levels using LLMs 9\nand prompting strategies, 78% were rated as “High Quality” and among these\n65.56% were rated to match the intended skill level by both human raters (Ta-\nble 4 ‘Overall’ Columns and ‘Overall’ row). The temperature of all LLMs was\nset at 0.9 to promote textual diversity, resulting in a PINC score average of\n0.92. These findings suggest that instruction fine-tuned LLMs demonstrate con-\nsiderable potential to generate diverse and high-quality educational questions\nat different cognitive levels based on Bloom’s taxonomy. For PS1-PS5, 72.55%,\n70.58%, 71.56%, 86.27%, and 89.02% of the questions were identified as “High\nQuality\" for Mistral 7B, Llama2 70B, Palm 2, GPT 3.5 and GPT 4 respectively.\nSimilarly, 60%, 61.67%, 60%, 74.41%, and 70.04% of the questions followed ad-\nhered to Bloom’s taxonomy level given by experts respectively.\nRQ2: Does the size of the LLM significantly impact the model’s per-\nformance in educational question generation?Table 4 presents the per-\nformance metrics of the five LLMs for the five sets of prompts. For quality and\nadherence to Bloom’s taxonomy levels, GPT 4 and GPT 3.5 emerged as the top\nperformers. Palm 2, despite its larger size compared to Mistral 7B and LLama2\n70B, demonstrated wide variance in the quality of the AEQG task for different\nprompt strategies. Palm 2 has only 36.99% Skill matching in a detailed and com-\nplex prompt (PS5), but it scores 70.51% in the PS3 prompt strategy. For PS5\nprompts, the Mistral 7B model performs better than the Llama2 70B model,\nwhich is counterintuitive. The reason for this performance difference could be\ndue to the way these models process a long prompt. Thus, no clear pattern exists\nbetween the model size and AEQG performance.\nTable 4. Performance of the LLMs in the AEQG task. For each model and set of\nprompts PS1-PS5, the percentage of questions that are of high quality (Quality), ad-\nherence to Bloom’s taxonomy level (Skill), and the PINC score are presented.\nPS1: Simple prompt PS2: CoT & skill explanation PS3: CoT & example\nLLM Quality Skill PINC Quality Skill PINC Quality Skill PINC\nMistral 7B 70.59% 56.94% 0.94 75.49% 68.83% 0.95 78.43% 45.00% 0.94\nLlama 2 70B 73.53% 57.33% 0.94 75.49% 71.43% 0.93 77.45% 58.23% 0.93\nPalm 2 61.76% 55.56% 0.93 73.53% 65.33% 0.94 76.47% 70.51% 0.93\nGPT 3.5 69.61% 81.69% 0.94 94.12% 89.58% 0.89 89.22% 64.84% 0.92\nGPT 4 75.49% 74.03% 0.93 87.25% 82.02% 0.92 91.18% 65.59% 0.92\nOverall 70.20% 66.36% 0.93 81.18% 76.32% 0.93 82.55% 61.04% 0.93\nPS4: CoT, skill, & PS5: CoT, skill, skill Overall\nexample explanation & example\nLLM Quality Skill PINC Quality Skill PINC Quality Skill PINC\nMistral 7B 71.57% 71.23% 0.93 66.67% 58.82% 0.94 72.55% 60.00% 0.94\nLlama 2 70B 77.45% 73.42% 0.91 49.02% 40.00% 0.93 70.58% 61.67% 0.93\nPalm 2 74.51% 69.74% 0.93 71.57% 36.99% 0.93 71.56% 60.00% 0.93\nGPT 3.5 87.25% 73.03% 0.90 91.18% 59.14% 0.91 86.27% 73.41% 0.91\nGPT 4 96.08% 75.51% 0.92 95.10% 56.64% 0.90 89.02% 70.04% 0.92\nOverall 81.37% 72.77% 0.92 74.71% 51.18% 0.92 78.00% 65.56% 0.93\n10 N. Scaria et al.\nRQ3: How does the amount of information provided in the prompt\naffect the quality of the questions generated?It is observed that the sim-\nple prompt (PS1) performed poorly in the quality of the questions generated\n(Table 4). Overall, the performance improved with the addition of more infor-\nmation to the prompt. However, the amount of improvement varied between the\nfive LLMs. Figure 1 shows that for Mistral, Llama 2 and Palm 2, PS3 gave the\nhighest quality questions, with PS4 being close behind. PS4 gave the highest\nskill match for these three models, while the skill match for PS3 was low. Inter-\nestingly, PS5, which is the most complicated prompt used, reduced both quality\nand skill for these three models, indicating that while information enrichment\nimproves AEQG, too much information in the prompt can be counterproductive.\nThe GPT models also gave good performance for PS4, but their performance for\nPS2 to PS5 are more or less the same with respect to quality of questions. These\ntwo models did well in terms of skill match for PS2-PS4, and similar to the other\nthree LLMs, the PS5 skill scores drop significantly. Our results indicate that a\nCoT prompt with a description of the skill and an example question performs\nbest for AEQG.\nFig. 1.Quality and Skill of prompting strategies for AEQG by different LLMs.\nRQ4: Can LLMs create questions that are relatable to a specific popu-\nlation or context?In our study, LLMs were asked to create questions that are\nrelatable to students in India. Nine recurring themes that are specific to India\nemerged (Fig. 2). These included questions on Bollywood movies, traffic chal-\nlenges in Indian cities and their mitigation using computer vision techniques, and\nthe Indian educational system. Given India’s significant dependence on agricul-\nture, many questions focused on climate, crop yield, crop diseases, and cropping\npatterns.Inaddition,numerousquestions,specificallyonthetopicofnaturallan-\nguage processing, used Indian languages as examples. Interestingly, the Indian\nlanguage text generated by open-source models, Mistral 7B and Llama2 70B, was\noften inaccurate and poor quality. From Fig. 2, it is also clear that compared to\nAEQG at Different Bloom’s Skill Levels using LLMs 11\nGPT 4 and GPT 3.5, the recurring themes occur less in the questions generated\nby other models. Some questions exhibited a tendency to unnecessarily specify\nIndia when the context was general. The expert evaluators rephrased these ques-\ntions. Furthermore, some questions reflect certain cultural generalizations about\nIndia that are not necessarily true, as evaluators have indicated.\nFig. 2.Frequently repeated Indian contexts in the AEQG questions.\nRQ5:Caninstructionfine-tunedLLMsevaluategeneratededucational\nquestions effectively, similar to human evaluators, when given the\nsame instructions? We conducted an LLM-based evaluation to analyze the\nquality and adherence of machine-generated questions to different cognitive lev-\nels on the nine-item rubric in addition to the expert evaluation. We used Gemini\nPro (gemini-pro), an LLM that is different from the five used for the AEQG task,\nfor the evaluation (detailed methodology in Sect. 2.4). The results of the evalua-\ntion are given in Table 5. There is a significant discrepancy between LLM-based\nand expert evaluations. Interesting discrepancies emerged between Gemini Pro\nand expert evaluations, with Palm 2 excelling in automated evaluation, but un-\nderperforming in expert evaluation. In the Gemini Pro evaluation, even Llama\n2 70B and Mistral 7B also performed better in some cases. Our automated\nevaluation using Gemini Pro revealed a tendency of the model to classify most\nmachine-generated questions as belonging to the ‘Apply’ or ‘Analyze’ levels on\nBloom’sLevel. The observed performance dip of the LLMs in adhering to the\n12 N. Scaria et al.\nevaluator-LLM’s Bloom’s level can be attributed to this fact. This result indi-\ncates that extreme caution must be exercised when using LLMs for automated\nevaluation of generative tasks. In the future, our data set can be used to improve\nthe automated evaluation of the questions as well.\nTable 5.Automated evaluation of AEQG questions: percentage of high-quality ques-\ntions and adherence to Bloom’s taxonomy level given by Gemini Pro.\nSimple prompt CoT & skill explanation CoT & example\nLLM Quality Skill Quality Skill Quality Skill\nMistral 7B 82.35% 53.98% 61.76% 44.44% 70.59% 19.44%\nLlama 2 70B 79.41% 40.02% 65.69% 43.28% 80.39% 40.24%\nPalm 2 69.61% 39.43% 65.69% 49.25% 78.43% 40.00%\nGPT 3.5 82.35% 48.15% 67.65% 34.78% 75.49% 28.24%\nGPT 4 80.39% 38.09% 75.49% 35.06% 77.45% 37.97%\nCoT, skill, and example CoT, skill, skill explanation and example.\nLLM Quality Skill Quality Skill\nMistral 7B 58.82% 40.00% 55.88% 31.57%\nLlama 2 70B 62.75% 39.06% 53.92% 34.54%\nPalm 2 77.45% 44.30% 69.61% 36.62%\nGPT 3.5 66.67% 33.82% 51.96% 30.18%\nGPT 4 73.53% 40.00% 66.67% 38.24%\n4 Discussion and Conclusion\nOur study demonstrates that LLMs can produce high-quality and diverse educa-\ntional questions aligned with Bloom’s taxonomy, requiring minimal input from\neducators, but the performance varies based on the size of the model and the\nprompt used to generate these questions. Larger proprietary models like GPT\n4 and GPT 3.5 outperform smaller open-source models across all the metrics\nin expert evaluation, but the same does not hold for the Palm 2 model. While\nadding a lot of information (skill explanation, example questions, and CoT in-\nstructions) significantly reduced the performance of the LLMs, particularly for\nopen-source models, optimal results were achieved with prompts including CoT\ninstructions paired with either skill, skill explanation, or example questions. CoT\ninstructions, with examples, resulted in more high-quality questions while com-\npromising on the adherence to Bloom’s skill. On the other hand, Bloom’s skill\nexplanations with CoT instructions slightly reduced the number of high-quality\nquestions but significantly boosted the performance of adherence to Bloom’s\nskill. The questions generated often incorporated contextually relevant Indian\ncontexts, although some instances exhibited generalizations about India.\nIn our research, the evaluation of 2550 questions took a considerable amount\nof time and effort from expert evaluators. Although attention was paid to making\ntheevaluationprocessobjective,experts’decisionscanstillbesubjectivedepend-\ning on who is evaluating them. However, the LLM-based evaluation proved to be\nless effective in our case. There was a considerable difference across all metrics\nAEQG at Different Bloom’s Skill Levels using LLMs 13\nin the case of the Gemini Pro evaluation compared to the expert evaluation. In-\nterestingly, in the Gemini Pro evaluation, Palm 2 outperformed other models on\ndifferent evaluation metrics, even though expert evaluation suggested otherwise.\nThis discrepancy might stem from the fact that Palm 2 and Gemini Pro are both\nGoogle’s models, potentially sharing similar training data or methodologies. We\nfound that our evaluator model is suboptimal and does not align with the expert\nevaluation. This could be due to the lack of such examples that these models\nwould have seen during their training. This requires the training of the LLM on\nevaluation datasets on specific subjects and evaluation metrics to make it robust\nfor evaluation. This is a potential future direction of research.\nOur approach used the inherent knowledge of the content possessed by the\nLLMs on the topic for AEQG. This revealed limitations in their understanding of\nspecific domains. For example, Mistral 7B and Llama2 70B struggled on the top-\nics of \"prompt engineering\", producing questions related to general engineering.\nAnother interesting observation in the study was the inability of most models to\ngenerate high-quality questions at the ‘Create’ level of Bloom’s taxonomy. Thus,\nthe present paper can be extended to use existing resources from the Internet\nor course material through databases to improve the performance of the model\nin question generation. We did not study the generation of questions for topics\nother than data science content, and such an exploration using the methodology\nshowcased here could be a potential future direction for research.\nReferences\n1. Amidei, J., Piwek, P., Willis, A.: Rethinking the agreement in human evaluation\ntasks. In: Proceedings of the 27th International COLING. pp. 3318–3329 (2018)\n2. Anderson, L.W., Krathwohl, D.R.: A taxonomy for learning, teaching, and assess-\ning: A revision of Bloom’s taxonomy of educational objectives: complete edition.\nAddison Wesley Longman, Inc. (2001)\n3. Bulathwela, S., Muse, H., Yilmaz, E.: Scalable educational question generation\nwith pre-trained language models. In: AIED ’23. pp. 327–339. Springer (2023)\n4. Chen, D., Dolan, W.B.: Collecting highly parallel data for paraphrase evaluation.\nIn: Proc of ACL’21. pp. 190–200 (2011)\n5. Cohen, J.: Weighted kappa: nominal scale agreement provision for scaled disagree-\nment or partial credit. Psychological bulletin70(4), 213 (1968)\n6. Horbach, A., Aldabe, I., Bexte, M., de Lacalle, O.L., Maritxalar, M.: Linguistic\nappropriateness and pedagogic usefulness of reading comprehension questions. In:\nProc of LREC 2020. pp. 1753–1762 (2020)\n7. Horbach, A., Aldabe, I., Bexte, M., de Lacalle, O.L., Maritxalar, M.: Linguistic\nappropriateness and pedagogic usefulness of reading comprehension questions. In:\nProceedings of the Twelfth Language Resources and Evaluation Conference. pp.\n1753–1762 (2020)\n8. Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A.,\nFung, P.: Survey of hallucination in natural language generation. ACM Computing\nSurveys 55(12), 1–38 (2023)\n9. Kasneci, E., Seßler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F.,\nGasser, U., Groh, G., Günnemann, S., Hüllermeier, E., et al.: ChatGPT for good?\n14 N. Scaria et al.\nOn opportunities and challenges of large language models for education. Learning\nand individual differences103, 102274 (2023)\n10. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models\nare zero-shot reasoners. Advances in NeurIPS35, 22199–22213 (2022)\n11. Kurdi, G., Leo, J., Parsia, B., Sattler, U., Al-Emari, S.: A systematic review of au-\ntomatic question generation for educational purposes. IJAIED30, 121–204 (2020)\n12. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and\npredict: A systematic survey of prompting methods in natural language processing.\nACM Computing Surveys55(9), 1–35 (2023)\n13. Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., Zhu, C.: G-eval: NLG evaluation using\ngpt-4 with better human alignment. In: Bouamor, H., Pino, J., Bali, K. (eds.)\nProceedings of the 2023 Conference on EMNLP. pp. 2511–2522 (Dec 2023)\n14. McHugh, M.L.: Interrater reliability: the kappa statistic. Biochemia medica22(3),\n276–282 (2012)\n15. Mishra, S., Khashabi, D., Baral, C., Choi, Y., Hajishirzi, H.: Reframing instruc-\ntional prompts to gptk’s language. In: Findings of ACL 2022. pp. 589–612 (2022)\n16. Moore, S., Nguyen, H.A., Bier, N., Domadia, T., Stamper, J.: Assessing the quality\nof student-generated short answer questions using gpt-3. In: EC-TEL. pp. 243–257.\nSpringer (2022)\n17. Nguyen, H.A., Bhat, S., Moore, S., Bier, N., Stamper, J.: Towards generalized\nmethods for automatic question generation in educational domains. In: EC-TEL.\npp. 272–284. Springer (2022)\n18. Sridhar, P., Doyle, A., Agarwal, A., Bogart, C., Savelka, J., Sakr, M.: Harnessing\nllms in curricular design: Using gpt-4 to support authoring of learning objectives.\narXiv preprint arXiv:2306.17459 (2023)\n19. Steuer, T., Bongard, L., Uhlig, J., Zimmer, G.: On the linguistic and pedagogical\nquality of automatic question generation via neural machine translation. In: EC-\nTEL 2021, Proceedings. pp. 289–294. Springer (2021)\n20. Ushio, A., Alva-Manchego, F., Camacho-Collados, J.: Generative Language Models\nfor Paragraph-Level Question Generation. In: Proc of EMNLP ’22’ (Dec 2022)\n21. Wang, J., Liang, Y., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., Zhou, J.:\nIs ChatGPT a good NLG evaluator? a preliminary study. In: Dong, Y., Xiao, W.,\nWang, L., Liu, F., Carenini, G. (eds.) Proceedings of the 4th New Frontiers in\nSummarization Workshop. ACL (Dec 2023)\n22. Wang, Z., Valdez, J., Basu Mallick, D., Baraniuk, R.G.: Towards human-like educa-\ntional question generation with large language models. In: AIED ’22. pp. 153–166.\nSpringer (2022)\n23. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in NeurIPS35, 24824–24837 (2022)\n24. Zhang, H., Song, H., Li, S., Zhou, M., Song, D.: A survey of controllable text\ngeneration using transformer-based pre-trained language models. ACM Computing\nSurveys (2022)\n25. Zhang, R., Guo, J., Chen, L., Fan, Y., Cheng, X.: A review on question generation\nfrom natural language text. ACM TOIS40(1), 1–43 (2021)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8606373071670532
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4813328683376312
    },
    {
      "name": "Bloom",
      "score": 0.4786473512649536
    },
    {
      "name": "Natural language processing",
      "score": 0.46834418177604675
    },
    {
      "name": "Language model",
      "score": 0.4406633973121643
    },
    {
      "name": "Machine learning",
      "score": 0.3787016272544861
    },
    {
      "name": "Oceanography",
      "score": 0.06608936190605164
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": []
}