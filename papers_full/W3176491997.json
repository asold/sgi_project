{
  "title": "Transforming Term Extraction: Transformer-Based Approaches to Multilingual Term Extraction Across Domains",
  "url": "https://openalex.org/W3176491997",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5110235377",
      "name": "Christian A. Lang",
      "affiliations": [
        "University of Vienna",
        "Center for Applied Linguistics"
      ]
    },
    {
      "id": "https://openalex.org/A5015902125",
      "name": "Lennart Wachowiak",
      "affiliations": [
        "University of Vienna",
        "Center for Applied Linguistics"
      ]
    },
    {
      "id": "https://openalex.org/A5044282179",
      "name": "Barbara Heinisch",
      "affiliations": [
        "University of Vienna",
        "Center for Applied Linguistics"
      ]
    },
    {
      "id": "https://openalex.org/A5011442841",
      "name": "Dagmar Gromann",
      "affiliations": [
        "University of Vienna",
        "Center for Applied Linguistics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3088869463",
    "https://openalex.org/W941635066",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W179719743",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3088593447",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2979736636",
    "https://openalex.org/W2579538068",
    "https://openalex.org/W2924225635",
    "https://openalex.org/W3144414978",
    "https://openalex.org/W2049819023",
    "https://openalex.org/W2107434887",
    "https://openalex.org/W2888790259",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W1546842539",
    "https://openalex.org/W2796219422",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2165997480",
    "https://openalex.org/W3024396974",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2766214391",
    "https://openalex.org/W2049107599",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2215459068",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2553301550",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035223328",
    "https://openalex.org/W2898786064",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W230988620",
    "https://openalex.org/W2896328475",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W2846961231",
    "https://openalex.org/W2970393213"
  ],
  "abstract": "Automated Term Extraction (ATE), even though well-investigated, continues to be a challenging task.Approaches conventionally extract terms on corpus or document level and the benefits of neural models still remain underexplored with very few exceptions.We introduce three transformer-based term extraction models operating on sentence level: a language model for token classification, one for sequence classification, and an innovative use of Neural Machine Translation (NMT), which learns to reduce sentences to terms.All three models are trained and tested on the dataset of the ATE challenge TermEval 2020 in English, French, and Dutch across four specialized domains.The two best performing approaches are also evaluated on the ACL RD-TEC 2.0 dataset.Our models outperform previous baselines, one of which is BERT-based, by a substantial margin, with the token-classifier language model performing best.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3607–3620\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3607\nTransforming Term Extraction: Transformer-Based Approaches to\nMultilingual Term Extraction Across Domains\nChristian Lang∗, Lennart Wachowiak*, Barbara Heinisch, Dagmar Gromann\nUniversity of Vienna, Center for Translation Studies\n[christian.lang,lennart.wachowiak]@univie.ac.at\nAbstract\nAutomated Term Extraction (ATE), even\nthough well-investigated, continues to be a\nchallenging task. Approaches conventionally\nextract terms on corpus or document level and\nthe beneﬁts of neural models still remain un-\nderexplored with very few exceptions. We in-\ntroduce three transformer-based term extrac-\ntion models operating on sentence level: a lan-\nguage model for token classiﬁcation, one for\nsequence classiﬁcation, and an innovative use\nof Neural Machine Translation (NMT), which\nlearns to reduce sentences to terms. All three\nmodels are trained and tested on the dataset of\nthe ATE challenge TermEval 2020 in English,\nFrench, and Dutch across four specialized do-\nmains. The two best performing approaches\nare also evaluated on the ACL RD-TEC 2.0\ndataset. Our models outperform previous base-\nlines, one of which is BERT-based, by a sub-\nstantial margin, with the token-classiﬁer lan-\nguage model performing best.\n1 Introduction\nAutomated Term Extraction (ATE) aims at extract-\ning terms, i.e., single- or multi-word sequences,\nfrom domain-speciﬁc text. ATE plays a role in\nmany NLP tasks, such as information extraction,\nknowledge graph learning, and text summariza-\ntion. In a corpus-level setting, methods range\nfrom frequency-based to utilizing Wikipedia links,\nwhere no single method has been found to perform\nconsistently best across domains in English (As-\ntrakhantsev, 2018). In document-level ATE, Key-\nConceptRelatedness (Astrakhantsev, 2014), which\nrelies on keyphrase extraction and semantic related-\nness, outperforms other methods (ˇSajatovi´c et al.,\n2019). The use of neural networks in these methods\nis mostly limited to generating embeddings.\nA ﬁrst use of BERT-based language models is\ndocumented by Hazem et al. (2020), the winning\n∗* Equal contributions\nsystem of the recent ATE challenge TermEval 2020\n(Rigouts Terryn et al., 2020) and the baseline for\nthe proposed approaches. Inspired by this ﬁrst\nsuccess of transformer-based models, we compare\ntwo variations of the multilingual pretrained lan-\nguage model XLM-RoBERTa (XLM-R) (Conneau\net al., 2020) with an innovative use of the multi-\nlingual pretrained NMT model mBART (Liu et al.,\n2020) on the Annotated Corpora for Term Extrac-\ntion Research (ACTER) dataset (Rigouts Terryn\net al., 2019) utilized in TermEval 2020 as well as\non the ACL RD-TEC 2.0 dataset (QasemiZadeh\nand Schumann, 2016). Since masked language and\nNMT models take sentences as input, the proposed\nATE methods operate on sentence level. In spite\nof this reduced context of sentence input rather\nthan documents or corpora, the models achieve F1\nscores of up to 69.8% on ACTER, strongly outper-\nforming the previous baseline of 48.1% .\nAn XLM-R-based sequence classiﬁer relies on\npositive (term) and negative (non-term) samples,\nwhich are generated based on all n-grams up to a\nlength of six of a given sentence. A second XLM-\nR-based token classiﬁer decides for each word in\na sequence whether it can be considered (part of)\na term. Since the second model operates with-\nout upfront n-gram generation and only processes\neach sentence once, it is considerably more time-\nefﬁcient than the ﬁrst. Finally, the pretrained NMT\nmodel mBART is adapted to transform input sen-\ntences to sequences of comma-separated terms, an\napproach inspired by NMT-based ontology learn-\ning (Petrucci et al., 2018).\nAnalyses of results reveal interesting insights\ninto the performance of the different input process-\ning strategies and transformer-based models, in-\ncluding their ability to handle multi-word terms,\ntraining time required, and a comparison between\nbaseline monolingual and multilingual language\nmodels in ATE. To achieve sentence-level ATE\n3608\nthe ACTER dataset had to be preprocessed align-\ning terms with their occurrences in sentences,\nwhich we made publicly available together with\nour source code.1\nIn summary, our main contributions are: (i) We\nshow that transformer-based models can be suc-\ncessfully applied to ATE across three languages\nand ﬁve domains, without the need for text pre-\nprocessing or feature extraction; (ii) We show that\nATE can be performed successfully on sentence\nlevel; (iii) We conduct robust experiments to show\nthat our models outperform competitive baselines;\n(iv) We investigate the models’ abilities to handle\nsingle- and multi-word terms, distinct term types,\nand differences in performance depending on train\nand test language combinations.\n2 Related Work\nAn initial classiﬁcation of ATE methods into sta-\ntistical, linguistic or hybrid (e.g. by Kageura and\nUmino (1996)) has recently been reﬁned by As-\ntrakhantsev (2018) to methods based on term oc-\ncurrence frequencies (e.g. C/NC-value Frantzi\net al., 2000), occurrence contexts (e.g. Bordea et al.,\n2013), domain-speciﬁc corpora combined with gen-\neral language corpora (e.g. Weirdness (Ahmad\net al., 1999)), topic modeling (e.g. Li et al., 2013),\nand those utilizing Wikipedia. Methods are ad-\nditionally categorized by the type of context, i.e.,\ncorpus-level (e.g. Zhang et al., 2008; Astrakhant-\nsev, 2018) and document-level (e.g. ˇSajatovi´c et al.,\n2019) settings.\nThese classiﬁcations cannot easily accommodate\nrecent neural ATE methods that generally oper-\nate on sentence level. An approach most closely\nrelated and our baseline by Hazem et al. (2020)\nutilized RoBERTa (Liu et al., 2019) for English\nand CamemBERT (Martin et al., 2020) for French\nand won the TermEval 2020 challenge. In their\nwork, pretrained language models clearly outper-\nformed a classiﬁcation method based on a vari-\nety of features, such as statistical descriptors and\nthe domain-speciﬁcity measure termhood (Kageura\nand Umino, 1996). A recently published approach\n(Rokas et al., 2020) relies on LSTM, GRU and\nBERT embeddings and achieves high F1 scores\nfor ATE of Lithuanian terms in the cybersecurity\ndomain. Several approaches build on word embed-\n1https://github.com/Text2TCS/\nTerm-Extraction-With-Language-Models\nand https://github.com/Text2TCS/\nmBART-termextraction\ndings to perform ATE on speciﬁc domains, such\nas medicine (e.g. Bay et al., 2020), or to sepa-\nrate general-language from domain-speciﬁc embed-\ndings (H¨atty et al., 2020). In contrast, our models\nperform ATE on four domains and in three lan-\nguages utilizing a pretrained language and a pre-\ntrained NMT model. Extracting terms is also vi-\ntal to learning expressive ontologies from text, for\nwhich Petrucci et al. (2018) train an NMT model to\ntransform sentences to Description Logic formulas,\nan idea that inspired our NMT-based ATE model.\n3 Language Models and NMT\nNeural Language Models, which create contextu-\nalized language representations, were responsible\nfor many of the recent improvements in NLP. Such\nmodels acquire rich contextualized language rep-\nresentations in a pretraining stage in which they\nlearn to predict a masked word in a sentence, a\ntask for which large amounts of training data are\nreadily available. The thereby learned representa-\ntions can be reused for various downstream tasks in\nthe so-called ﬁne-tuning stage, where task-speciﬁc\nlayers are added on top of the pretrained language\nmodel. One of the most popular language models\nis BERT (Devlin et al., 2019), utilizing the trans-\nformer architecture (Vaswani et al., 2017). XLM-R\n(Conneau et al., 2020) is a multilingual variant\nof BERT, which was pretrained in 100 languages\nusing 2.5 terabytes of Common Crawl data. More-\nover, it makes use of the improved training routine\nintroduced by RoBERTa (Liu et al., 2019).\nDespite the widespread use of neural language\nmodels for NLP, adoption of such self-supervised\npretraining approaches in NMT has only recently\nstarted to gain traction. NMT is traditionally\nperformed with sequence-to-sequence encoder-\ndecoder models that generate a target language\noutput sequence based on a source language input\nsequence. Conventional language models trained\non predicting masked words from a sequence, such\nas BERT, have only recently been incorporated\ninto NMT (Zhu et al., 2020). A very interesting\nalternative is to pretrain an NMT transformer ar-\nchitecture, as done by Lewis et al. (2020) in form\nof a Bidirectional and Autoregressive Transformer\n(BART) (Lewis et al., 2020). This is achieved by\ncombining a bidirectional encoder similar to that\nof BERT with an autoregressive decoder, as seen\nin GPT (Radford et al., 2018). Thereby, contex-\ntualized language representations are trained and\n3609\na model that is proﬁcient in text generation and\ntranslation is created. Liu et al. (2020) applied\nthe BART architecture to large-scale monolingual\ncorpora across 25 languages, creating multilingual\nBART (mBART) that can be directly ﬁne-tuned for\nmachine translation (MT).\n4 Dataset\nIn order to compare to a strong baseline, we train\nand test on the ACTER dataset (Rigouts Terryn\net al., 2019) utilized in the recent TermEval 2020\nchallenge. The domains wind energy and corrup-\ntion represent the training set, dressage (equitation)\nthe validation set, and heart failure the hold-out\ntest set, for which the count of words and unique\ngold standard terms including named entities for\nEnglish, French and Dutch are presented in Table 1.\nIn the ACTER dataset, words were labeled\nas speciﬁc, common, and out-of-domain (OOD)\nterms, and named entities (NE). Speciﬁc terms\nare understood by domain experts, while common\nterms might also be additionally understood by\nlaypersons. OOD terms might be speciﬁc to a dif-\nferent domain, but used in the domain at hand, e.g.\nstatistical terms in the medical domain.\nSince the time of the challenge the dataset has\nundergone some minor updates, that is, unicode\nencoding, dash and quote normalization.2 We be-\nlieve that these minor normalization changes do\nnot signiﬁcantly impact comparability to TermEval\nresults, which is conﬁrmed by the fact that our most\nsimilar model to the baseline, the sequence classi-\nﬁer, achieves comparable results. Furthermore, the\nACTER dataset provides terms as a single list for\nall documents in a domain. However, we required\ninline sentence-level term annotation, which we\ngenerated. In rare cases, this generation of inline\nannotations might have lead to erroneous results\nfor single-word terms. For instance, the term “gain”\nas in “private gain” lead to the verb “gain” as in\n“gain acceptance” to be erroneously annotated in\nthe corruption domain. We manually analyzed 300\ninline annotated sentences and since the above ex-\nample was the only error found, we consider this a\nnegligible issue.\nThe fully inline annotated dataset ACL RD-TEC\n2.0 (henceforth ACLR2) dataset provides cleaner\ntraining and test data and could therefore poten-\ntially further boost model performance as we show\n2This normalized version 1.4 is available at https://\ngithub.com/AylaRT/ACTER\nACTER Train Val Test\nWordsen 97,145 51,470 45,788\nTermsen 2,708 1,575 2,585\nWordsfr 106,792 53,316 46,751\nTermsfr 2,185 1,183 2,423\nWordsnl 96,887 50,882 47,888\nTermsnl 2,540 1,546 2,257\nACLR2 Train Val Test\nWordsan.1 11,473 3,846 4,032\nTermsan.1 1,306 420 477\nWordsan.2 16,939 5,757 5,441\nTermsan.2 1,743 583 673\nTable 1: Train, validation, and test split by word count\nand term count per language/annotator\nin Section 7.2. The ACLR2 dataset provides a to-\ntal of 471 inline human annotated abstract texts\nfrom articles in the ACL Anthology Reference Cor-\npus. As shown in the split of numbers in Table 1,\ntwo separate annotations by two human experts are\nprovided. Since no ofﬁcial train/val/test split is pro-\nvided, we chose to split the ACLR2 dataset with a\n60/20/20 split per annotator. In contrast to the AC-\nTER dataset, ACLR2 is only available in English\nand exclusively covers scientiﬁc abstracts in the do-\nmain of computational linguistics. In terms of base-\nline, previous work generally reported precision at\nk top terms extracted (P@k) (Zhang et al., 2018b)\nor F1 on Recoverable True Positives (F1@RTP)\n(Zhang et al., 2018a), due to the necessity to de-\nﬁne an arbitrary cut-off point with traditional ATE\nmethods. In another work attempting ATE with\nneural networks, due to the lack of an ofﬁcial data\nsplit and a restriction to domain speciﬁc terms, F1\nscores are reported on arbitrary parts of the dataset\n(Kucza et al., 2018).\n5 Neural Language Model-based ATE\nWe introduce two possible architectures for ATE\nbased on the multilingual language model XLM-R.\nFor the experiments we use the base-size model ver-\nsion in form of the implementation made available\nby the transformers library (Wolf et al., 2019).\n5.1 Sequence Classiﬁer\nAs with the winning approach of TermEval 2020\n(Hazem et al., 2020), our ﬁrst architecture utilizes\nlanguage models for binary sequence classiﬁcation\nby using a fully connected layer to classify the rep-\nresentation of the special classiﬁcation token <s>,\n3610\nwhich encoded by XLM-R carries information re-\ngarding the whole input sequence. Instead of using\nlanguage speciﬁc models, however, we make use\nof the multilingual model XLM-R, which enables\nthe use of a single model for all languages and has\nthe ability to generalize to unseen languages.\nThe model receives pairs consisting of a term\ncandidate and a context sentence in which the can-\ndidate appears as input as exempliﬁed in Table 2.\nTerm candidates are created by producing all pos-\nsible n-grams of a given sentence. Due to perfor-\nmance reasons and the term length distribution in\nthe dataset (mostly <5 words), n-grams were only\ncreated up to a length of 6 words. For instance,\ngiven the input sentence “We meta-analyzed mor-\ntality using random-effect models” a positive sam-\nple, i.e., one labeled as term, is “random-effect\nmodels. We meta-analyzed mortality using random-\neffect models”, while a negative sample is “mor-\ntality using. We meta-analyzed mortality using\nrandom-effect models”. For training the model,\nwe undersample the negative samples so that their\namount matches the amount of positive samples to\ncompare to Hazem et al. (2020). For the evaluation\non the validation and test set we use all possible\nn-grams for each input sentence, thus, creating a set\nof extracted terms which we can evaluate against\nthe gold standard. The model was trained for 4\nepochs with a batch size of 32 using the Adam\noptimizer with a learning rate of 2e-5.\n5.2 Token Classiﬁer\nThe second architecture we use for experimenta-\ntion classiﬁes each token of an input sentence sepa-\nrately, utilizing the same fully connected layer for\nall tokens after they have been processed by XLM-\nR. This leads to a signiﬁcant reduction in training\nand inference time as each sentence has to be only\nprocessed once by XLM-R. This type of architec-\nture is usually utilized in tasks like Named Entity\nRecognition (NER) (Devlin et al., 2019), where\neach word of a sequence needs to be classiﬁed.\nThe input provided to the model now simply con-\nsists of the sentences of the document which we\nwant to process. The model then assigns each input\ntoken one of three possible output labels: “B-T” for\nthe beginning of a term, “T” for the continuation of\na term, and “n” in the case the token is not part of\na term. For instance, the input sentence “We meta-\nanalyzed mortality using random-effect models.”\nwould be labeled as ‘n’, ‘B-T’, ‘B-T’, ‘n’, ‘B-T’,\n‘T’, ‘n’, with the last label annotating the punctua-\ntion at the end of the sentence.3 Table 2 compares\nthis input and output pattern with the other two\nmethods. Since XLM-R’s tokenizer is a Sentence-\nPiece tokenizer that splits the input into tokens on a\nsubword level, the output labels obtained from the\nmodel are also subwords and have to be matched\nto the original words of the sentence afterwards.\nFor training we used the Adam optimizer with a\nlearning rate of 2e-5. Moreover, we used a batch\nsize of 8 evaluating the model every 100 steps to\nbe able to load the best model at the end.\n6 NMT-based ATE\nAs a third experiment, we present a novel approach\nto ATE building on a recent sequence-to-sequence\ndenoising auto-encoder model trained for NMT.\nWe chose the recent and robust mBART model\ntrained on the Common Crawl corpus in 25 lan-\nguages (mBART25) (Liu et al., 2020) available in\nthe Fairseq library (Ott et al., 2019).\n6.1 Data Preprocessing for NMT-based ATE\nSince we construct the downstream task of ATE\nas an MT task, we required parallel text data for\nsupervised ﬁne-tuning of mBART. We opted for\na sentence-level approach, which speciﬁcally re-\nquires sentence-aligned parallel data. Sentence\ntokenization was performed with the Punkt tok-\nenizer of NLTK and terms were inline annotated\nwith the ﬂashtext algorithm (Singh, 2017). For the\nACLR2 dataset, individual sentences and the terms\nwithin were extracted with an XML parser. In or-\nder to distinguish single- and multi-word terms in\nthe model’s output sequence, a separator between\nterms or a unifying character between components\nof multi-word terms was required. Preliminary test-\ning showed that using a semicolon surrounded by\nwhite-spaces ( ; ) as separator would achieve the\nsame ﬁnal F1 score as using more complex sepa-\nrators like a tag (for example <term>). Notably\nusing an underscore (w w) to connect the individ-\nual constituents of a term (w) lowered the score of\nthe output signiﬁcantly, that is, F1 performance of\nthe best model was 5.3% lower on average across\nall test languages when compared to utilizing semi-\ncolons. Irrespective of the separator, the model\nwould at times add or omit a white-space between\nseparator and term, which had the effect that the\n3The separation of “meta-analyzed” and “mortality” as\ndistinct terms corresponds to the gold standard.\n3611\nModel Input Example Output Example\nSequence\nClassiﬁer\nrandom-effect models. We meta-\nanalyzed mortality using random-effect\nmodels\nTerm\nToken\nClassiﬁer\nWe meta-analyzed mortality using\nrandom-effect models\n[’n’, ’B-T’, ’B-T’, ’n’, ’B-T’, ’T’, ’n’]\nNMT We meta-analyzed mortality using\nrandom-effect models\nmeta-analyzed ; mortality ; random-\neffects models\nTable 2: Input and output examples for all three transformer-based models\nterm would not be considered in the evaluation.\nThis was remedied in the process of extracting in-\ndividual terms from the output sequence and the\nresults reported in Section 7 are with unwanted\nwhite-spaces removed. Tokenization during train-\ning was performed with SentencePiece (Kudo and\nRichardson, 2018) and data was binarized with the\nfairseq-preprocess CLI tool.\n6.2 NMT Model Fine-Tuning\nThe pretrained mBART model was ﬁne-tuned with\nthe preprocessed data described in Section 6.1. In-\nput to the encoder model was a given sentence, such\nas “Codes of conduct forbid corruption, irrespec-\ntive of its intended purpose.”, while the decoder\nwould be shown the expected term labels, such\nas “codes of conduct ; corruption”. No language-\nspeciﬁc tags were added to input or output, which\nis compared to the other methods in Table 2. For\nfaster and more memory-efﬁcient training we used\nautomated mixed precision training of Fairseq with\nthe Fused Adam Optimizer of the NVIDIA Apex\nPyTorch extensions.4 We ﬁne-tuned a separate\nmodel for each language of the dataset and a sin-\ngle model with all languages combined. Following\nthe original publication of the pretrained model,\neach model was ﬁne-tuned with 0.3 dropout, 0.2 la-\nbel smoothing, 2500 warm-up steps and a learning\nrate of 3e-5. Furthermore, we opted for a dynamic\nbatch size by limiting the maximum tokens per\nbatch to 768, while updating the gradients every 4\nsteps (more details in Section 7.4).\nWhile preliminary testing showed faster conver-\ngence and slightly higher ﬁnal scores with higher\ntokens per batch, availability of the V100 GPU was\nnot guaranteed and therefore training hyperparame-\nters had to be adjusted to also run on an RTX2080Ti\nGPU, which limited the maximum tokens per GPU\nto 768. Model performance was evaluated every\n4https://github.com/NVIDIA/apex\nfull epoch. Results were generated using the stan-\ndard generation parameters of Fairseq.\n7 Results\nThis section ﬁrst presents the results on ACTER\nincluding an analysis per language (combination)\nand the results on ACLR2, then details the term\nlength and type behavior of the models, and ﬁ-\nnally compares their training time efﬁciency. We\nadditionally report on the validation performance\nof the best performing token classiﬁer in Table 4,\nwhich shows some performance differences to the\ntest domain, especially with French as training and\nvalidation language. For further comparability we\nalso provide precision, recall and F1 scores at k top\nterms of 15 methods offered by the term extraction\ntoolkit ATR4S, which implements a large range of\nexisting ATE methods, in Appendix A.\n7.1 Results on ACTER\nTo compare our results to the strongest participant\nof TermEval 2020, we report precision, recall and\nF1 scores in Table 3. These metrics are calculated\non the basis of the available annotation in the origi-\nnal ACTER dataset, where we opted for the more\ncomprehensive list of terms including named en-\ntities. All three models are evaluated on differ-\nent combinations of training and test languages as\nshown in Table 3, where the heart failure domain is\nthe hold-out test set as done for the SOTA baseline.\nThe overall best results are marked in bold for each\ntest language, while the best results of each model\n(if not bold) are highlighted in italics.\nThe overall best result for our approaches was\nan F1 score of 69.8%, which could be achieved\nby training the token classiﬁer model on English\nand testing it on Dutch. With the exact same set-\ntings as the baseline (Hazem et al., 2020) that is\nbased on RoBERTa (Liu et al., 2019) for English\nas training and test language, the token classiﬁer\n3612\nTraining Test Sequence Classiﬁer Token Classiﬁer NMT Previous SOTA\nPrec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1\nEN EN 30.9 84.0 45.2 54.9 62.2 58.3 45.7 63.5 53.2 34.8 70.9 46.7\nFR EN 31.1 79.5 44.7 56.7 36.2 44.2 50.0 59.3 54.2\nNL EN 22.3 91.1 35.9 55.3 61.8 58.3 48.3 64.3 55.2\nALL EN 31.4 85.8 46.0 54.4 58.2 56.2 50.2 61.6 55.3\nEN FR 34.6 79.0 48.1 65.4 51.4 57.6 48.8 61.3 54.4\nFR FR 32.2 80.2 46.0 68.7 43.0 52.9 52.7 59.6 55.9 44.2 51.5 48.1\nNL FR 26.1 84.7 40.0 62.3 48.5 54.5 54.3 60.9 57.4\nALL FR 33.2 78.9 46.7 62.7 49.4 55.3 55.0 60.4 57.6\nEN NL 42.8 89.8 58.0 67.9 71.7 69.8 48.8 63.9 55.4\nFR NL 41.3 87.6 56.1 69.2 55.2 61.4 56.2 63.4 59.6\nNL NL 32.7 94.1 48.5 71.4 67.8 69.6 60.6 70.7 65.2 18.9 18.6 18.7\nALL NL 40.4 91.5 56.0 70.0 65.8 67.8 60.6 70.0 64.9\nTable 3: Test set results represented by training and test languages of the ACTER heart failure domain and in\ncomparison to the state-of-the-art (SOTA) results from TermEval 2020.\nTraining EN Val FR Val NL Val\nEN (ACTER) 55.6 45.3 60.5\nFR (ACTER) 41.9 33.6 49.6\nNL (ACTER) 54.6 47.7 57.8\nALL (ACTER) 50.0 40.4 51.5\nACLR2 An.1 75.5 / /\nACLR2 An.2 79.3 / /\nTable 4: Validation performance of token classiﬁer on\ndressage domain of the ACTER dataset and 20% vali-\ndation data of the ACLR2 dataset.\nachieves an 11.6% higher F1 score and the NMT\nmodel an improvement of 6.5% on the F1 score.\nThe sequence classiﬁer struggles with precision\nand cannot outperform the baseline in this setting.\nBest performance for English as test language can\nbe achieved by the token classiﬁer trained on Dutch\nand by the NMT model trained on all languages.\nWhen testing on French, the sequence classi-\nﬁer is on par with the F1 baseline (Hazem et al.,\n2020) building on CamemBERT (Martin et al.,\n2020), while the token classiﬁer outperforms it by\n9.5% and the NMT model obtains an additional\n7.8%. Best performance on French as a test lan-\nguage is achieved by the token classiﬁer when\ntrained on English and by the NMT model when\ntrained on all languages again. The baseline for\nDutch is provided by a bidirectional LSTM with\nGLOVE.5 With Dutch as a test language, the se-\nquence and token classiﬁer achieve their best result\n5No system description paper was submitted for this ap-\nproach after participation in the challenge.\nwhen trained on English, the NMT model when\ntrained on Dutch.\nA signiﬁcant result is the substantial improve-\nment of precision of the token classiﬁer and NMT\nmodel over the baseline, even though the recall for\nEnglish as test language lags behind. For French,\nthe recall could be improved with the NMT model\nand matched by the token classiﬁer when trained\non English. Interestingly, the sequence classiﬁer\nachieves a remarkable improvement on recall, how-\never, lags behind on precision for all settings.\nThis can be explained by the fact that we perform\nundersampling of the negative samples to match\nthe number of positive samples, a strategy adopted\nfrom Hazem et al. (2020) to obtain comparable\nresults. If undersampling is reduced, the precision\nand recall scores are more balanced and closer to\nthe performance of the token classiﬁer, however,\ntraining time is considerably increased. Another\nreason for the higher number of extracted phrases\nby the sequence classiﬁer compared to the other\nmodels is that it can extract multi-word terms as\nwell as words which are part of these multi-word\nterms separately, since both are used as input in the\nform of potential term candidate n-grams.\nAll three models show remarkable zero-shot\ntransfer learning capabilities, i.e., they are trained\non one language and show strong test scores on an-\nother. This is especially true for the token classiﬁer,\nwhere models trained on a single language often\noutperform those trained on all three languages.\nThis transfer learning ability across languages can\nalso be observed in the overall highest F1 scores\n3613\nfor the English test set, which was achieved by a\nmodel trained on Dutch, and for the French test set,\nwhich was achieved by a model trained on English.\n7.2 Results on ACLR2\nIn addition to evaluating our models on the AC-\nTER dataset, we compared the two best perform-\ning architectures, i.e., the token classiﬁer and the\nNMT model, on the ACLR2 dataset. Both models\nachieve similar test scores as reported in Table 5\nand higher than the scores achieved on the ACTER\ndataset. As with the ACTER dataset, we addition-\nally report validation performance of the best per-\nforming token classiﬁer model in Table 4, which is\nin line with the test performance.\nData Token Classiﬁer NMT\nPrec Rec F1 Prec Rec F1\nAn.1 74.4 77.2 75.8 73.2 77.2 75.2\nAn.2 80.1 79.3. 80.0 79.4 80.7 80.0\nTable 5: Test set results of token classiﬁer on data from\nAnnotator 1 and 2 of the ACLR2 dataset.\n7.3 Term-based Analysis\nA qualitative analysis of the lists of false positives\nand false negatives based on the ACTER dataset\ndemonstrated that all models handle acronyms\nwell. This may be due to the text type in AC-\nTER, which is partially based on scientiﬁc abstracts\nthat frequently introduce acronyms in brackets. If\nacronyms are part of the term, e.g. “LV strain\nrate”, there was a high number of false negatives in\nboth models. Moreover, false negatives occurred\nin all models if a term included a proper name and\nan apostrophe, e.g. “Chaga’s disease” or “Cron-\nbach’s α”, or frequently if it included a ﬁgure,\ne.g. “p38alpha” or “6-min walk test”. In addition,\nnamed entities that included version numbers or\nconsisted of multiple words often resulted in false\nnegatives, e.g. “Self-Care of Heart Failure Index\nVersion 6.2”, “Multicenter Automatic Deﬁbrilla-\ntor Implantation Trial-Cardiac Resynchronization\nTherapy”. In the token classiﬁer and NMT model,\nthe class of named entities of cities, e.g. “New\nYork” and “Seattle”, were frequently not identiﬁed\nas terms. False negatives also occurred in all mod-\nels if it was a particularly long multi-word term,\ne.g. “resynchronization reverses remodeling in sys-\ntolic left ventricular dysfunction”. A tendency by\nthe token classiﬁer to split longer terms could be\nobserved, e.g. splitting adjectives and nouns.\nTo quantitatively evaluate how well the different\nmodel types handled terms of different lengths, we\ncomputed the F1 scores individually for terms of\na speciﬁc length, based on the terms in the AC-\nTER test set. The results in Table 6 were com-\nputed using the best model of each method, i.e.,\nthe model trained in English for the token and the\nsequence classiﬁer and the model trained on all\nlanguage for the NMT model. We can see that\nthe scores of all models decrease with term length.\nSecondly, we observer that for English and Dutch\nthe token classiﬁer has the strongest results for\nall term lengths. However, for French the token\nclassiﬁer scores strongly decrease for multi-word\nterms, even though it is still the best model for\nunigrams. This is due to a very low recall, e.g.\nfor 4-grams and higher the token classiﬁer recalls\nonly 7% of all French terms. The NMT model\nshows more consistency between languages, thus,\nperforming strongest for French multi-word terms.\nAs already the case with the overall scores the se-\nquence classiﬁer shows the highest recall values\nfor both single-word and multi-word terms, how-\never, lagging behind in precision, which leads to\nan overall lower F1 score.\nFurthermore, based on the ACTER term type\nannotation (see Section 4), we could compare the\ntypes of terms extracted by the individual models.\nAs can be seen in Fig. 1, the models all achieve\na very similar distribution of extracted term types\nwhen compared to the gold test set distribution. We\ncan observe, however, that the sequence classiﬁer\nshowed a slight tendency to extract more common\nand OOD terms and noticeably less NEs than the\nother models. All models tended to extract more\nspeciﬁc terms, with the token classiﬁer and the\nNMT model interestingly extracting comparatively\nfew OOD terms.\nFigure 1: Distribution of term types across languages\nin the models’ true positives and the ACTER gold test\ndataset.\n3614\nTerm Length Sequence Classiﬁer (F1) Token Classiﬁer (F1) NMT (F1)\nEN FR NL EN FR NL EN FR NL\nUnigram 61.6 61.2 68.0 63.3 69.1 73.8 61.7 61.8 70.4\nBigram 38.0 41.0 39.8 55.9 43.0 58.4 52.2 57.6 56.3\nTrigram 37.0 35.4 40.4 55.4 31.3 49.1 51.6 52.4 47.7\n≥ 4-gram 32.2 22.6 30.3 44.3 12.2 42.9 43.4 33.4 38.8\nTable 6: F1 Scores based on different term lengths using the overall best model for each method on the ACTER\ndataset. In bold the best scores per row for each language.\n7.4 Training Time Efﬁciency\nLooking at the epochs required to reach the best\nscore on the ACTER validation set, we can ob-\nserve that in most cases the token classiﬁer model\nrequires not even a single training epoch. Train-\ning with the English dataset required 300 steps\nwith a full epoch consisting of 432 steps. The\nmodel trained on French was the only model with\nits best performance being reached during the sec-\nond epoch after 700 steps while a full epoch con-\nsists of 437 steps. The model trained on Dutch\nperformed best after 400 steps while one epoch\ntakes 553 steps. The multilingual model converged\nthe quickest needing only 200 steps whereas a full\nepoch consists of 1,421 steps. The token classiﬁer\nmodels trained on the ACLR2 dataset need more\nepochs and achieve their highest scores after 3 and\n5 epochs respectively. However, due to the lower\ntraining set size of the ACLR2 corpus, this also\ncorresponds to less than 500 steps, thus, being sim-\nilar with the training times reported for the models\ntrained on the ACTER data. In comparison, the\nsequence classiﬁer achieved its best performances\non the ACTER validation set after 4 epochs of train-\ning.\nThe NMT model also required several epochs\nto reach the best performance. Initially, all models\nwere trained for 80 epochs, with the model hav-\ning the lowest validation loss being loaded at the\nend. The models trained on monolingual data ben-\neﬁted from longer training compared to the models\ntrained on the combined multilingual data. For\ncompleteness, we report the training epochs, label\nsmoothed cross entropy loss, and log perplexity\non the validation set for the best models. For the\nEnglish dataset the reported score was achieved\nat epoch 49 with a loss of 5.82 and perplexity of\n3.94. For the French dataset peak performance was\nreached at epoch 40, with a loss of 5.82 and per-\nplexity of 3.78. Like the French model, the Dutch\nmodel achieved its best performance at epoch 40\nModel Train Time Val Time GPU\nSeq. 19 44 P100\nTok. 9 1 P100\nNMT 49 2 V100\nTable 7: Training/validation times in minutes on the\nEnglish ACTER data and GPUs used.\nhaving a loss of 5.69 and a perplexity of 3.37.\nWhen trained on one language, model performance\nwas observed to drop for unseen languages when\ntraining beyond the best validation score. For in-\nstance, while the English model at epoch 49 ob-\ntained F1 scores of 53.2%, 54.4%, 55.4% for the\nEnglish, French, and Dutch test data respectively,\nat epoch 80 these scores were at 53.6%, 50.6% and\n52.1% respectively, gaining little for English and\nlosing for unseen languages. Finally, for the multi-\nlingual dataset the model reached the reported peak\nperformance already at epoch 22 as it trains on a\nlot more data per single epoch. Loss and perplexity\nwere at 5.50 and 2.89 respectively. The training\nand validation times as well as the used GPUs are\nreported in Table 7. Training times denotes the full\ntraining time over all epochs without any valida-\ntions. Validation time denotes the time for a single\nvalidation. The token classiﬁer is the most efﬁcient.\n8 Discussion\nAlthough the ACLR2 dataset is smaller in size than\nthe ACTER dataset, the resulting F1 scores are\nconsiderably higher. Apart from the fact that it\nonly covers a single domain, ACLR2 already pro-\nvides inline annotations and more consistent term\nannotations, which seems to facilitate learning the\ntask. Inconsistencies in the ACTER annotations\nwere mainly noted when analyzing false positives\nof the models. For instance, “patient” is considered\na common term in the heart-failure domain, but\n“serum” is not annotated at all, although in our view\nit would also qualify as common term.\n3615\nWe also noted that more training data does not\nnecessarily increase model performance. As indi-\ncated by the training times on the ACTER dataset,\nthe token classiﬁer achieved its best evaluation\nscores long before training for a whole epoch, i.e.,\nhaving seen only a small fraction of the available\ndata before reaching its strongest performance.\nIn this paper we compare the performance of a\npretrained monolingual language model baseline\nwith pretrained multilingual language models. Pre-\nvious work indicates that monolingual language\nmodels like RoBERTa or CamemBERT outperform\nmultilingual language models on tasks posed in a\nsingle language (R¨onnqvist et al., 2019). The dif-\nference increases the higher the complexity of the\ngiven task but is negligible on simple tasks that\nmostly rely on syntactic features. Since in our\ncase the multilingual model XLM-R in form of a\nsequence classiﬁer performs very similar to the se-\nquence classiﬁer-based RoBERTa model winning\nTermEval 2020, it indicates that successful ATE\ndoes not require very strong language understand-\ning but corresponds more to simpler tasks relying\nmostly on syntactic features. Nevertheless, the\nremarkable zero-shot transfer learning of the mul-\ntilingual models ﬁne-tuned on a single language\nwould also suggest that the multilingual pretraining\nmight aid the model in deﬁning what a term is, as\nhighly domain-speciﬁc terms might be similar be-\ntween languages tested, e.g. rooted in Latin. In the\nNMT output analysis, we found that the knowledge\ntransfer between languages could cause curious\nside-effects, where at times terms are predicted by\nthe model in a semi-translated way. For instance,\nwhen training on English the model would at times\ninvent “toxicity cardiaque ” for the French test set\ninstead of extracting “toxicit´e cardiaque”.\nBesides stronger performance, the NMT model\nas well as the token classiﬁer have a higher poten-\ntial to better handle the possible extension of the\nterm extraction task to include discontinuous en-\ntities, which, however, are so far not annotated in\nthe datasets we used. An example of a discontinu-\nous entity can be found in the expression “left and\nright ventricular failure”, where “right ventricular\nfailure” but also “left ventricular failure” are terms,\nthe latter not being continuous in the original ex-\npression. While the NMT model does not require\nany special adaptations to deal with such an addi-\ntion, the sequence classiﬁer would have to consider\nmany more n-gram combinations leading again to\neven higher training and inference times per sen-\ntence. To consider discontinuous entities with the\ntoken classiﬁers labels, the annotation and training\nprocess would have to be adapted to a multi-label\ntoken classiﬁcation, e.g. the above phrase would\nbe labeled as [B-T, n, n, T, T] and [n, n, B-T, T,\nT]. Since in the ﬁrst label “ventricular” and “fail-\nure” are labeled as “T” they still clearly belong to\nthe word “left” labeled as “B-T”, which could be\nconsidered in a post processing step.\n9 Conclusion\nIn this paper, we adapt and evaluate three\ntransformer-based models on the task of ATE,\nbuilding on pretrained multilingual language and\nNMT models. In this evaluation, these multilin-\ngual models outperform a baseline of monolingual\nlanguage models and show remarkable zero-shot\nabilities. A token classiﬁcation strategy building on\na language model achieved the best performance,\nhowever, the NMT-based model seemed to be able\nto handle multi-word expressions more consistently\nacross languages and not lag far behind in per-\nformance. One aspect that became very clear is\na prevalence for quality over quantity when ﬁne-\ntuning pretrained models to the task of ATE.\nRecently, both NMT and masked language mod-\nels show a trend towards increased input sequence\ncapacity. Thus, it would be interesting to evalu-\nate the impact of context length on the proposed\nmodels by testing with more domain context than\nonly single sentences. Furthermore, to test the abil-\nity of the token classiﬁer and the NMT model to\nhandle discontinuous terms, such as elliptical ex-\npressions, a dataset containing and annotating such\nterms would be interesting.\nAcknowledgments\nThe Text2TCS project6 was supported by the Euro-\npean Language Grid project through its open call\nfor pilot projects. The European Language Grid\nproject has received funding from the European\nUnion’s Horizon 2020 Research and Innovation\nprogramme under Grant Agreement no. 825627\n(ELG). The computational results presented have\nbeen achieved [in part] using the Vienna Scientiﬁc\nCluster (VSC).\n6https://text2tcs.univie.ac.at/\n3616\nImpact Statement\nAutomatically extracting domain-speciﬁc terms\nacross domains and languages with high accuracy\nprovides a valuable means to reduce time and re-\nsource effort in creating terminological resources.\nSuch resources are important to ensure termino-\nlogical consistency in specialized communication,\nsuch as communication between different groups\nin times of crisis, and to avoid misunderstandings.\nFrom a technological perspective, we introduce\nmultilingual pretrained language models to the ﬁeld\nof Automated Term Extraction (ATE) with detailed\ntests on three different transformer-based models\nacross four domains and three languages. Since\nthese models support considerably more languages\nthan tested, the approach can be transferred to other\nlanguages. This transfer capability has been tested\nby training in a speciﬁc language and then testing\nmodels in another language. Transfer capabilities\nextend to domains, since we trained and validated\non three domains and achieved results strongly out-\nperforming previous approaches on a previously un-\nseen test domain. Up to this point, such ﬂexibility\nhas been achieved by statistical approaches, how-\never, with considerably lower results in precision\nand recall. In contrast to previous ATE methods\nperforming on corpora, our models extract terms\non sentence level. This makes ATE more ﬂexible\nsince neither large domain-speciﬁc nor reference\ncorpora are required.\nFrom a societal perspective, terminological in-\nconsistencies are a major source of misunderstand-\ning in the communication among experts, between\nexperts and laypersons, and between laypersons\nin reference to a specialized domain. This issue\ncan be mitigated by publishing agreed upon desig-\nnations for real-world phenomena in a specialized\ndomain that can be consulted for domain-speciﬁc\ncommunication. However, manually preparing a\ncollection of natural language terms is extremely\nhuman resource- and time-intensive. We reduce\nthis workload for governmental institutions, pri-\nvate and public organizations, and private persons\nby providing a method to automate the detection\nof such domain-speciﬁc terms in natural language\ntexts across languages and domains.\nIn terms of risk, such a highly ﬂexible solution\nto automated term extraction fully depends on the\nquality of the input text. Misleading, erroneous, or\nbiased contents will inevitably be propagated to the\nresulting terminologies. Relying on terminologies\nextracted from such problematic contents can nega-\ntively impact specialized communication or conclu-\nsions drawn from it. Thus, it is of vital importance\nfor any user of this approach to mitigate the uncer-\ntainty of the reliability of extracted terms by only\nconsidering high-quality and reliable sources in the\nterm extraction process and have domain experts\ncarefully review the outcome prior to utilizing it\nin communication. We cannot guarantee that in\na real-life setting all important terms have been\nextracted and all extracted terms are indeed cen-\ntral to the domain at hand. Furthermore, training\nneural network models is a process known to leave\nan environmental footprint, which we try to miti-\ngate by ﬁne-tuning pretrained models. Fine-tuning\nis less resource- and time-intensive than training\nfrom scratch, but still requires high-performance\ncomputing clusters.\n3617\nReferences\nKhurshid Ahmad, Lee Gillam, Lena Tostevin, et al.\n1999. University of surrey participation in trec8:\nWeirdness indexing for logical document extrapola-\ntion and retrieval (wilder). In TREC, pages 1–8.\nNikita Astrakhantsev. 2014. Automatic term acquisi-\ntion from domain-speciﬁc text collection by using\nwikipedia. Proceedings of the Institute for System\nProgramming, 26(4):7–20.\nNikita Astrakhantsev. 2018. ATR4S: toolkit with\nstate-of-the-art automatic terms recognition meth-\nods in scala. Language Resources and Evaluation ,\n52(3):853–872.\nMatthias Bay, Daniel Bruneß, Miriam Herold, Chris-\ntian Schulze, Michael Guckert, and Mirjam Minor.\n2020. Term extraction from medical documents us-\ning word embeddings. In 4th IEEE Conference on\nMachine Learning and Natural Language Process-\ning (MNLP 2020). IEEE Computer Society.\nGeorgeta Bordea, Paul Buitelaar, and Tamara Polajnar.\n2013. Domain-independent term extraction through\ndomain modelling. In The 10th international confer-\nence on terminology and artiﬁcial intelligence (TIA\n2013), Paris, France. 10th International Conference\non Terminology and Artiﬁcial Intelligence.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKaterina Frantzi, Sophia Ananiadou, and Hideki Mima.\n2000. Automatic recognition of multi-word terms:.\nthe c-value/nc-value method. International journal\non digital libraries, 3(2):115–130.\nAnna H ¨atty, Dominik Schlechtweg, Michael Dorna,\nand Sabine Schulte im Walde. 2020. Predicting de-\ngrees of technicality in automatic terminology ex-\ntraction. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2883–2889, Online. Association for Computa-\ntional Linguistics.\nAmir Hazem, M´erieme Bouhandi, Florian Boudin, and\nBeatrice Daille. 2020. TermEval 2020: TALN-\nLS2N system for automatic term extraction. In Pro-\nceedings of the 6th International Workshop on Com-\nputational Terminology, pages 95–100, Marseille,\nFrance. European Language Resources Association.\nKyo Kageura and Bin Umino. 1996. Methods of au-\ntomatic term recognition: A review. Terminology.\nInternational Journal of Theoretical and Applied Is-\nsues in Specialized Communication, 3(2):259–289.\nMaren Kucza, Jan Niehues, Thomas Zenkel, Alex\nWaibel, and Sebastian St¨uker. 2018. Term extraction\nvia neural sequence labeling a comparative evalua-\ntion of strategies using recurrent neural networks. In\nProceedings of INTERSPEECH 2018 , pages 2072–\n2076.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nSujian Li, Jiwei Li, Tao Song, Wenjie Li, and Baobao\nChang. 2013. A novel topic model for automatic\nterm extraction. In Proceedings of the 36th interna-\ntional ACM SIGIR conference on Research and de-\nvelopment in information retrieval, pages 885–888.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\n3618\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nGiulio Petrucci, Marco Rospocher, and Chiara Ghidini.\n2018. Expressive ontology learning as neural ma-\nchine translation. Journal of Web Semantics, 52:66–\n82.\nBehrang QasemiZadeh and Anne-Kathrin Schumann.\n2016. The ACL RD-TEC 2.0: A language resource\nfor evaluating term extraction and entity recogni-\ntion methods. In Proceedings of the Tenth Inter-\nnational Conference on Language Resources and\nEvaluation (LREC’16), pages 1862–1868, Portoro ˇz,\nSlovenia. European Language Resources Associa-\ntion (ELRA).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAyla Rigouts Terryn, Veronique Hoste, Patrick Drouin,\nand Els Lefever. 2020. TermEval 2020: Shared task\non automatic term extraction using the annotated cor-\npora for term extraction research (ACTER) dataset.\nIn Proceedings of the 6th International Workshop\non Computational Terminology, pages 85–94, Mar-\nseille, France. European Language Resources Asso-\nciation.\nAyla Rigouts Terryn, V´eronique Hoste, and Els Lefever.\n2019. In no uncertain terms: a dataset for mono-\nlingual and multilingual automatic term extraction\nfrom comparable corpora. Language Resources and\nEvaluation, 54:385–418.\nAivaras Rokas, Sigita Rackeviˇcien˙e, and Andrius Utka.\n2020. Automatic extraction of lithuanian cyberse-\ncurity terms using deep learning approaches. In Hu-\nman Language Technologies–The Baltic Perspective,\nvolume 328, pages 39–46. IOS Press.\nSamuel R ¨onnqvist, Jenna Kanerva, Tapio Salakoski,\nand Filip Ginter. 2019. Is multilingual BERT ﬂu-\nent in language generation? In Proceedings of the\nFirst NLPL Workshop on Deep Learning for Natural\nLanguage Processing, pages 29–36, Turku, Finland.\nLink¨oping University Electronic Press.\nAntonio ˇSajatovi´c, Maja Buljan, Jan ˇSnajder, and Bo-\njana Dalbelo Ba ˇsi´c. 2019. Evaluating automatic\nterm extraction methods on individual documents.\nIn Proceedings of the Joint Workshop on Multiword\nExpressions and WordNet (MWE-WN 2019) , pages\n149–154, Florence, Italy. Association for Computa-\ntional Linguistics.\nVikash Singh. 2017. Replace or retrieve keywords in\ndocuments at scale. CoRR, abs/1711.00046.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nZiqi Zhang, Jie Gao, and Fabio Ciravegna. 2018a.\nSemre-rank: Improving automatic term extraction\nby incorporating semantic relatedness with person-\nalised pagerank. ACM Trans. Knowl. Discov. Data,\n12(5).\nZiqi Zhang, Jose Iria, Christopher Brewster, and Fabio\nCiravegna. 2008. A comparative evaluation of\nterm recognition algorithms. In Proceedings of\nthe Sixth International Conference on Language Re-\nsources and Evaluation (LREC’08), Marrakech, Mo-\nrocco. European Language Resources Association\n(ELRA).\nZiqi Zhang, Johann Petrak, and Diana Maynard. 2018b.\nAdapted textrank for term extraction: A generic\nmethod of improving automatic term extraction algo-\nrithms. Procedia Computer Science, 137:102–108.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tieyan Liu. 2020.\nIncorporating BERT into neural machine translation.\nIn International Conference on Learning Represen-\ntations.\n3619\nA Appendix\nFor further comparability, we provide results of\n15 prior term extraction methods provided by the\nATR4S toolkit (Astrakhantsev, 2018). All methods\nprovided by ATR4S are re-ranking methods based\non a previous term candidate extraction process.\nTable A1 shows the results of ATR4S on the\nACTER heart-failure domain in English. While\nsome methods achieve good precision, most meth-\nods show precision scores below our best models,\neven at only 100 terms extracted. Increasing the\nmanually speciﬁed amount of k terms to extract\nresults in a decrease of precision in favor of recall.\nThe scores of the different methods level out to-\nwards the maximum of 2,000 terms extracted. The\nbest F1 score is achieved by the DomainPertinence\nmethod at 2,000 terms extracted with an F1 score\nof 30.32%.\nTable A2 shows the results of ATR4S on our\nACLR2 test splits. One major drawback of prior\nmethods is the required corpus size. The small test\nset in ACLR2 does not provide enough data for\nmany of the statistical approaches or in fact the re-\nranking to be effective at all after a certain amount\nof terms extracted. For the smaller Annotator 1\ntest set, we can observe virtually identical scores\nbetween all methods from 300 extracted terms on-\nwards. For Annotator 2, this phenomena can be\nobserved at 400 extracted terms. Best overall re-\nsults are an F1 score of 21.83% for Weirdness at\n200 terms extracted on the Annotator 1 test set\nand and F1 Score of 18.28% for Weirdness, PU\nand DomainPertinence at 300 terms extracted on\nthe Annotator 2 test set. In comparison, our best\nmodels achieve an F1 score of over 75% for both\nAnnotators.\n3620\nACTER: heart-failure EN (ATR4S)\nMethod Top 100 Top 500 Top 1000 Top 2000\nPrec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1\nAvgTermFrequency 55.0 2.13 4.1 41.4 8.01 13.42 35.2 13.62 19.64 30.8 23.83 26.87\nBasic 48.0 1.86 3.58 36.0 6.96 11.67 32.9 12.73 18.35 29.7 22.98 25.91\nComboBasic 48.0 1.86 3.58 36.0 6.96 11.67 31.9 12.34 17.8 29.7 22.98 25.91\nCValue 54.0 2.09 4.02 39.6 7.66 12.84 33.7 13.04 18.8 32.1 24.84 28.0\nDomainPertinence 58.0 2.24 4.32 46.8 9.05 15.17 35.8 13.85 19.97 34.75 26.89 30.32\nKeyConceptRelatedness 81.0 3.13 6.03 59.4 11.49 19.25 42.5 16.44 23.71 31.1 24.06 27.13\nLinkProbability 83.0 3.21 6.18 73.0 14.12 23.66 52.8 20.43 29.46 31.85 24.64 27.79\nNovelTopicModel 49.0 1.9 3.65 39.8 7.7 12.9 34.9 13.5 19.47 29.95 23.17 26.13\nPostRankDC 31.0 1.2 2.31 37.6 7.27 12.19 35.0 13.54 19.53 30.3 23.44 26.43\nPU 61.0 2.36 4.54 46.2 8.94 14.98 38.3 14.82 21.37 34.65 26.81 30.23\nRelevance 52.0 2.01 3.87 47.0 9.09 15.24 37.6 14.55 20.98 34.45 26.65 30.05\nResidualIDF 54.0 2.09 4.02 41.4 8.01 13.42 32.6 12.61 18.19 31.0 23.98 27.04\nTotalTFIDF 31.0 1.2 2.31 39.8 7.7 12.9 36.7 14.2 20.47 31.05 24.02 27.09\nV oting 62.0 2.4 4.62 57.0 11.03 18.48 48.7 18.84 27.17 34.5 26.69 30.1\nWeirdness 31.0 1.2 2.31 38.0 7.35 12.32 37.4 14.47 20.86 31.65 24.49 27.61\nTable A1: Precision, recall and F1 @ top k terms extracted of prior methods (ATR4S) on the English heart-failure\ndomain texts of ACTER. Best three results per top k marked in bold.\nACL RD-TEC 2.0: Annotator 1 (ATR4S)\nMethod Top 100 Top 200 Top 300 Top 400\nPrec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1\nAvgTermFrequency 36.0 7.53 12.46 29.0 12.13 17.11 28.73 16.53 20.98 28.73 16.53 20.98\nBasic 40.0 8.37 13.84 36.0 15.06 21.24 28.73 16.53 20.98 28.73 16.53 20.98\nComboBasic 40.0 8.37 13.84 36.0 15.06 21.24 28.73 16.53 20.98 28.73 16.53 20.98\nCValue 47.0 9.83 16.26 34.5 14.44 20.35 28.73 16.53 20.98 28.73 16.53 20.98\nDomainPertinence 46.0 9.62 15.92 35.5 14.85 20.94 28.73 16.53 20.98 28.73 16.53 20.98\nKeyConceptRelatedness 38.0 7.95 13.15 28.0 11.72 16.52 28.73 16.53 20.98 28.73 16.53 20.98\nLinkProbability 40.0 8.37 13.84 29.0 12.13 17.11 28.73 16.53 20.98 28.73 16.53 20.98\nNovelTopicModel 42.0 8.79 14.53 34.0 14.23 20.06 28.73 16.53 20.98 28.73 16.53 20.98\nPostRankDC 43.0 9.0 14.88 36.0 15.06 21.24 28.73 16.53 20.98 28.73 16.53 20.98\nPU 43.0 9.0 14.88 33.5 14.02 19.76 28.73 16.53 20.98 28.73 16.53 20.98\nRelevance 46.0 9.62 15.92 35.5 14.85 20.94 28.73 16.53 20.98 28.73 16.53 20.98\nResidualIDF 36.0 7.53 12.46 29.0 12.13 17.11 28.73 16.53 20.98 28.73 16.53 20.98\nTotalTFIDF 23.0 4.81 7.96 23.5 9.83 13.86 28.73 16.53 20.98 28.73 16.53 20.98\nV oting 47.0 9.83 16.26 32.0 13.39 18.88 28.73 16.53 20.98 28.73 16.53 20.98\nWeirdness 38.0 7.95 13.15 37.0 15.48 21.83 28.73 16.53 20.98 28.73 16.53 20.98\nACL RD-TEC 2.0: Annotator 2 (ATR4S)\nMethod Top 100 Top 200 Top 300 Top 400\nPrec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1\nAvgTermFrequency 36.0 5.34 9.3 28.0 8.31 12.81 26.33 11.72 16.22 27.44 13.35 17.96\nBasic 40.0 5.93 10.34 34.5 10.24 15.79 27.67 12.31 17.04 27.44 13.35 17.96\nComboBasic 40.0 5.93 10.34 34.5 10.24 15.79 27.67 12.31 17.04 27.44 13.35 17.96\nCValue 44.0 6.53 11.37 34.0 10.09 15.56 29.0 12.91 17.86 27.44 13.35 17.96\nDomainPertinence 38.0 5.64 9.82 37.0 10.98 16.93 29.67 13.2 18.28 27.44 13.35 17.96\nKeyConceptRelatedness 35.0 5.19 9.04 31.0 9.2 14.19 27.33 12.17 16.84 27.44 13.35 17.96\nLinkProbability 39.0 5.79 10.08 30.0 8.9 13.73 27.67 12.31 17.04 27.44 13.35 17.96\nNovelTopicModel 39.0 5.79 10.08 34.5 10.24 15.79 27.67 12.31 17.04 27.44 13.35 17.96\nPostRankDC 42.0 6.23 10.85 34.5 10.24 15.79 27.67 12.31 17.04 27.44 13.35 17.96\nPU 42.0 6.23 10.85 34.0 10.09 15.56 28.0 12.46 17.25 27.44 13.35 17.96\nRelevance 38.0 5.64 9.82 37.0 10.98 16.93 29.67 13.2 18.28 27.44 13.35 17.96\nResidualIDF 36.0 5.34 9.3 28.0 8.31 12.81 26.67 11.87 16.43 27.44 13.35 17.96\nTotalTFIDF 19.0 2.82 4.91 24.0 7.12 10.98 26.33 11.72 16.22 27.44 13.35 17.96\nV oting 46.0 6.82 11.89 36.0 10.68 16.48 29.0 12.91 17.86 27.44 13.35 17.96\nWeirdness 38.0 5.64 9.82 29.5 8.75 13.5 29.67 13.2 18.28 27.44 13.35 17.96\nTable A2: Precision, recall and F1 @ top k terms extracted of prior methods (ATR4S) on test split of ACLR2. Best\nresult per top k marked in bold.",
  "topic": "Term (time)",
  "concepts": [
    {
      "name": "Term (time)",
      "score": 0.7666529417037964
    },
    {
      "name": "Computer science",
      "score": 0.5780718326568604
    },
    {
      "name": "Transformer",
      "score": 0.5240205526351929
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.5212191343307495
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3538820743560791
    },
    {
      "name": "Engineering",
      "score": 0.10869181156158447
    },
    {
      "name": "Voltage",
      "score": 0.0883307158946991
    },
    {
      "name": "Electrical engineering",
      "score": 0.08529505133628845
    },
    {
      "name": "Chromatography",
      "score": 0.08229467272758484
    },
    {
      "name": "Chemistry",
      "score": 0.06890672445297241
    },
    {
      "name": "Physics",
      "score": 0.05952462553977966
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}