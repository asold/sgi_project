{
  "title": "Leveraging Similar Users for Personalized Language Modeling with Limited Data",
  "url": "https://openalex.org/W4285290591",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2106004680",
      "name": "Charles Welch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2479369952",
      "name": "Chenxi Gu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287857838",
      "name": "Jonathan Kummerfeld",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4295531668",
      "name": "Veronica Perez-Rosas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2068190112",
      "name": "Rada Mihalcea",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3037817142",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2759869292",
    "https://openalex.org/W3175236579",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2991324852",
    "https://openalex.org/W4211134195",
    "https://openalex.org/W4250089123",
    "https://openalex.org/W2141277304",
    "https://openalex.org/W3200437427",
    "https://openalex.org/W3030151190",
    "https://openalex.org/W2018571751",
    "https://openalex.org/W3099689069",
    "https://openalex.org/W3177201695",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2758729290",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2890560993",
    "https://openalex.org/W2252235528",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035509916",
    "https://openalex.org/W3035291402",
    "https://openalex.org/W2281063384",
    "https://openalex.org/W3014084332"
  ],
  "abstract": "Charles Welch, Chenxi Gu, Jonathan Kummerfeld, Veronica Perez-Rosas, Rada Mihalcea. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1742 - 1752\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nLeveraging Similar Users for Personalized\nLanguage Modeling with Limited Data\nCharles Welch\n ∗ and Chenxi Gu\n ∗ and Jonathan K. Kummerfeld\n and\nVerónica Pérez-Rosas\n and Rada Mihalcea\nConversational AI and Social Analytics (CAISA) Lab\nDepartment of Mathematics and Computer Science, University of Marburg\nLanguage and Information Technologies Lab (LIT)\nDepartment of Computer Science and Engineering, University of Michigan\nAbstract\nPersonalized language models are designed and\ntrained to capture language patterns specific\nto individual users. This makes them more\naccurate at predicting what a user will write.\nHowever, when a new user joins a platform and\nnot enough text is available, it is harder to build\neffective personalized language models. We\npropose a solution for this problem, using a\nmodel trained on users that are similar to a new\nuser. In this paper, we explore strategies for\nfinding the similarity between new users and\nexisting ones and methods for using the data\nfrom existing users who are a good match. We\nfurther explore the trade-off between available\ndata for new users and how well their language\ncan be modeled.\n1 Introduction\nRecent work has suggested that there are several\nbenefits to personalized models in natural language\nprocessing (NLP) over one-size-fits-all solutions:\nthey are more accurate for individual users; they\nhelp us understand communities better; and they\nfocus the attention of our evaluations on the end-\nuser (Flek, 2020). Generation tasks in particular\nbenefit from a personalized approach, for example,\nDudy et al. (2021) argue that user intention is more\noften difficult to recover from the context alone.\nWe study personalization in language modeling,\na core task in NLP. Direct applications of language\nmodels (LM) include predictive text, authorship\nattribution, and dialog systems used to model the\nstyle of an individual or profession (e.g., thera-\npist, counselor). LMs are increasingly used as the\nbackbone of models for a range of tasks in NLP,\nincreasing the potential impact of personalization\neven further (Brown et al., 2020).\nThe standard non-personalized approach is to\nuse pretrained models trained on a large volume\n*Authors contributed equally and work was performed\nwhile at the University of Michigan.\nof data written by many people. This approach\ndoes not take into account the differences between\nindividuals and their language patterns. Given the\nsame context, different people may act or write dif-\nferently, but these general models cannot produce\nthat type of variation. Approaches like fine-tuning\ncan be used to tailor a pretrained model to an indi-\nvidual, but perform well only when enough data is\navailable, which is often not the case.\nPrevious work on personalized and demographic\nword embeddings has seen successful application\nin downstream tasks. Garimella et al. (2017) look\nat location and gender and how they affect asso-\nciations with words like “health” and many other\nstimulus words like “stack”– does it make you think\nof books or pancakes? Welch et al. (2020) discuss\nother associations, for instance, “embodying” an\nidea may more often refer to a religious or eco-\nnomic concept depending on your beliefs. Simi-\nlarly, “wicked” may mean “evil” or may function\nas an intensifier depending on where you live (Bam-\nman et al., 2014). These exemplify how person-\nalized representations can help make distinctions\nin meaning, however, static representations have\nlimitations. For example, Hofmann et al. (2021)\nfind that in some contexts “testing” refers to seeing\nif a device works and “sanitation” refers to a pest\ncontrol issue, while in another context both refer to\nconditions of the COVID-19 pandemic. Personal-\nized LMs, or language models built to better predict\nwhat an individual will say, could better address\nthese cases, as LMs learn dynamic encodings of\nwords.\nIn this paper, we consider approaches to fine-\ntuning and interpolation that are novel in that they\nleverage data from similar users to boost person-\nalized LM performance. We consider the case of\nusers with a small number of available tokens and\npropose ways to (1) find similar users in our corpus\nand (2) leverage data from similar users to build a\npersonalized LM for a new user. We explore the\n1742\ntrade-offs between the amount of available data\nfrom existing users, the number of existing users\nand new users, and how our similarity metrics and\nmethods scale. We then show an analysis to explore\nwhat types of words our method predicts more ac-\ncurately and are thus more important to consider in\npersonalization methods.\n2 Related Work\nPersonalized Language Modeling. King and\nCook (2020) examined methods for creating per-\nsonalized LMs and their work is most similar to\nours. They consider interpolating, fine-tuning,\nand priming LMs as methods of personalization,\nthough they use these methods with a large generic\nmodel. In contrast, our work shows that perfor-\nmance can be improved by leveraging data from\nsimilar users. They also analyzed model adapta-\ntion for models trained on users with similar de-\nmographics, inspired by Lynn et al. (2017), who\nshowed that these demographic factors could help\nmodel a variety of tasks, and found that personal-\nized models perform better than those adapted from\nsimilar demographics. Shao et al. (2020) have also\nexplored models for personalization but focused on\nhandling OOV tokens.\nWu et al. (2020) proposed a framework to learn\nuser embeddings from Reddit posts. Their user\nembeddings were built on the sentence embed-\ndings generated by a BERT model. By using the\nlearned user embeddings to predict gender, detect\ndepression and classify MBTI personality, they con-\ncluded that their embeddings incorporate intrinsic\nattributes of users. In our work, user embeddings\nare learned in a different approach, and we focus\non how to use similarity calculated from user em-\nbeddings to build better LMs.\nAuthorship Attribution.One of the tasks we\nconsider as a means of computing similarity is au-\nthorship attribution, i.e., identifying the author of a\ndocument. Early work on this task used lexical fea-\ntures like word frequencies and word n-grams (Kop-\npel et al., 2009; Stamatatos, 2009). As in Ge et al.\n(2016), we employ neural networks to model simi-\nlarity between users and predict authorship.\nLearning from Limited Data.Antonello et al.\n(2021) explored training a model to predict what\ndata will be most informative for fine-tuning and\nselect individual data points to improve language\nmodeling. The similarity metrics that we derive\nare used to select data for fine-tuning in one of our\nmethods of leveraging similar user data, however\nwe consider indivisible sets of data grouped by\nauthor.\nThe cold start problem is a well-known problem\nin recommendation systems. A great amount of\nprevious work addressed how to recommend items\nto new users, about whom the system has little or no\nhistory, often with a focus on matrix factorization\nmethods (Zhou et al., 2011). Work from Huang\net al. (2016) approached language modeling as a\ncold-start problem, in that they had no writing from\na user, though they had a social network, from\nwhich they interpolated LMs from users linked in\ntheir social graph.\nLanguage Models.We use a recently developed\nLM that has received widespread attention (Mer-\nity et al., 2018b). The LSTM-based model com-\nbines a number of regularization and optimization\ntechniques explored in recent literature, including\naveraged SGD, embedding dropout, and recurrent\ndropout. Subsequent work has developed variations\nof the model with improved perplexity, but these\ntake at least twice as much time to train (Gong\net al., 2018), making them less practical for the\nuser-specific experiments we consider.\nAnother direction of research has shown impres-\nsive results using extremely large models (Radford\net al., 2019; Devlin et al., 2019). Using these as a\nbasis for experiments could be an interesting direc-\ntion, but fine-tuning models in low data settings is\nknown to be difficult and highly variable (Dodge\net al., 2020). Similar transformer models have been\nused for controlled generation. Zellers et al. (2019)\ndeveloped a model for news generation that con-\nditioned on meta-data including domain, date, au-\nthors, and headline. No ablation is performed, and\nthough it would be interesting to compare to a trans-\nformer method that conditions on authors alone,\nwe opted for a model that is faster and cheaper\nto train (Grover-Mega from Zellers et al. (2019)\nwas trained for two weeks and cost around 25k\nUSD). Additionally, when fine-tuning models for\nnew users, little data is available. Contextualized\nembedding models often require a large amount of\ndata to train effectively, though this type of com-\nparison would be an interesting future direction to\nexplore. Variations of the LSTM have consistently\nachieved state-of-the-art performance without mas-\nsive compute resources and thus we chose this ar-\nchitecture for our experiments (Merity et al., 2018a;\nMelis et al., 2019; Merity, 2019; Li et al., 2020).\n1743\nRule Example\n(1) it contains more than 20 tokens but the average token length\nis less than 3\n\" i \" \" \" \" w \" \" i \" \" l \" \" l \" \" \" \" n \" \" e \" \" v \" \" e \" \" r \" \"\n\" \" g \" \" i \" \" v \" \" e \"\n(2) it contains a long token whose length is greater than 30 COOLCOOLCOOLCOOLCOOLCOOLCOOLCOOL...\n(There is usually duplication inside this kind of post)\n(3) it contains less than 8 tokens among which more than 3 are\nURLs\nURL URL URL URL\n(4) it contains more than 3 math related symbols, such as “ |\",\n“+\" and “=\"\nbefore humanity , maybe 2 +2 = 5 . no , before humanity\n2 +2 = 4 did not exist .\n(5) it contains symbols like “{\", “}\" and “( )\" with only white\nspaces in the parentheses\nwe specialize in ( ) ( ) ( ) ( ) ( ) ( ) ( )\n(6) it contains less than 5 tokens and the last token is \"*\" (This kind of post is usually a spelling correction to a\nprevious post)\n(7) there are less than 4 unique tokens in every sequence of 8\nadjacent tokens\nw , w , w , w , would n’t it be better if we just bend over\nand follow their rules ?\n(8) it contains hashtags, indicated by: [ ] ( / / # [ ** if i were a rich man ... ** ] ( / / #ggj )\n(9) it is a duplicate of another post in the user’s data\n(10) more than 60% of the characters are non-alphabetical. =+=+= 1st =+=+= 2nd =+=+= End\nTable 1: Examples of rules for filtering posts as described in Section 3.1.\n3 Dataset\nWe examine a corpus of publicly available Red-\ndit comments and select users active on Reddit be-\ntween the years of 2007-2015 who have at least 60k\ntokens of text.1 We refer to the existing users with\nat least 250k tokens of text as anchor users. These\nare users that are leveraged through interpolation\nor fine-tuning in order to improve performance on\nnew users. Reddit posts are mostly in English.\nWe experiment with two settings: In the small\nanchor setting, there are 100 anchor users, with\na 200k, 25k, 25k split for training, validation, and\ntest, and 50 new users, with 2k tokens for training,\nand 25k for each of validation and test. In thelarge\nanchor setting, there are 10k anchor users and 100\nnew users, each having 2k tokens for training and\nvalidation and 20k for test.\nPreprocessing Reddit data can be noisy, contain-\ning URLs, structured content (e.g., tables, lists),\nSubreddit-specific emoticons, generated, or deleted\ncontent. We first extract all posts for each user\nin our dataset. During this process we remove\nnoisy posts, where a post is considered “noisy” if\nit matches one of ten rules. These rules and ex-\namples of each are shown in Table 1. After this\nfiltering step, we remove markup for emojis and\nhyperlinks from the remaining posts (keeping the\nposts themselves). We take these steps to ensure\nthat we capture language used by the authors, rather\n1Posts are retrieved from https://www.reddit.\ncom/r/datasets/comments/3bxlg7/i_have_\nevery_publicly_available_reddit_comment/\nand we exclude known bots and do not include posts in the\n/r/counting subreddit in our dataset.\nthan reposts, collections of links, ASCII tables and\nart, equations, or code. Tokens that occur fewer\nthan 5 times are replaced with ⟨UNK⟩, which re-\nsults in a vocab size of 55k for the small anchor set\nand 167k for the larger one.\n4 Experiments\nOur method for constructing personalized LMs\nconsists of a similarity metric and a method for\nleveraging similar user data to train a personal-\nized LM. The similarity metric measures which\nanchor users are most similar to a new user. That\nis, given a set of users ( anchors), a new user\n(n), and a similarity function (sim), we compute\nz = sim(n, anchors); z ⊂ anchors to get a set\nof similar users z. We explore three similarity met-\nrics and two methods of applying them to the con-\nstruction of personalized models. Figure 1 shows\nhow user data is used for each step.\n4.1 Calculating User Similarity\nWe explore three methods for measuring the sim-\nilarity between users. Two of them, authorship\nconfusion and user embeddings, are derived from\nclassifiers trained for other tasks, while the third,\nperplexity-based similarity, is obtained from the\nperformance of LMs on the new user. The user\nembedding method results in a vector space where\nwe can use cosine similarity to measure the dis-\ntance between individuals. The perplexity directly\ngives a distance between each pair and the author-\nship confusion vectors can be treated as a vector of\ncontinuous values where each value represents the\nsimilarity to an anchor user.\n1744\nPersonalized ModelsLearning Similarity Metrics\nAA\nA1\nLM \n+UEUE\nAnchor \nUsers\nNew\nUser\nAnchor \nUsers A1LMA1\nConfusion Vector\nUser Embedding\nPerplexity-based\nSimilar \nUsersFilter\nWFT \nLM \nStep 1\nWFT LM \nStep 2\nFT LMLM\n-UE\nInterpolation\nReweightAnchor \nUsers\nFigure 1: This diagram shows how data, models, and metrics are used in this paper. There are two main sections, a\nrectangle on the left showing how the three similarity metrics are computed, and a rectangle on the right showing\nour two methods of leveraging similar user data to create personalized models. The solid lines indicate the flow\nof anchor user data, while a dashed line indicates data from a new user. Anchor user data is used to create the\nauthorship attribution model (AA), the individual user LMs for the perplexity-based metric (denoted as a set with\nthe first as LMA1), and the user embeddings (UE). The three metrics can be used to filter anchor user data to find\nsimilar users. With these users, we fine-tune a baseline LM (without UE, denoted LM-UE), which is then further\nfine-tuned with new user data for the weighted fine-tuning method (WFT LM). When interpolating, the individual\nanchor user LMs are reweighted, and combined with the predictions of an LM fine-tuned on new user data (FT LM).\nAuthorship Attribution Confusion (AA).Simi-\nlarity can be measured from the confusion matrix of\nan authorship attribution model. This model takes a\npost as input and encodes it with an LSTM (Hochre-\niter and Schmidhuber, 1997). The final state is\npassed to a feed-forward layer and then a softmax\nto get a distribution over authors. We denote this\nmodel A, and A(U) as the class distribution out-\nput by the model for a given utterance set. For a\nnew user, we take their set of utterances, Un and\npass them to our model A(Un) which will give us\na confusion vector of length K, one value for each\nauthor.\nWe train this model on the data from anchor\nusers.2 Embeddings are initialized with 200d\nGloVe vectors pretrained on 6 billion tokens from\nrandomly sampled Reddit posts (Pennington et al.,\n2014). For K = 100 anchors the test accuracy\nis 42.88% and K = 10, 000 the test accuracy is\n2.42%. These accuracies are reasonably high given\nthe difficulty of the task.3 The classifier does not\n2See Appendix A for hyperparameters\n3Note that when K = 10, 000 the majority class is 0.01%.\nhave to be high performing given our application\nto computing a user similarity metric.\nWe apply this model to each post in the training\ndata from new users. The scores produced by the\nmodel for each new post indicate which of the an-\nchor users has the most similar writing. The more\nfrequently posts from a new user are predicted as\ncoming from a specific anchor user, the more simi-\nlar this anchor user is to the new user.\nUser Embeddings (UE). We first train an LM\nwith a user embedding layer on the data from an-\nchor users. The model is adapted from Merity et al.\n(2018b) with an added user embedding layer. This\ntoken embedding layer is initialized with our pre-\ntrained GloVe vectors and frozen during training.\nThe output of the LSTM layer is concatenated to\nthe user embedding at each time step based on the\nauthor of the token at that time step.4 Note that this\nis then passed through another feed-forward layer\nbefore being used for prediction. Our optimizer\nstarts with SGD and will switch to ASGD if there\n4See Appendix B for hyperparameters\n1745\nis no improvement in validation loss in the past 5\nepochs (Polyak and Juditsky, 1992). We removed\ncontinuous cache pointers (Grave et al., 2016) to\nspeed up training. For K = 100, the validation\nperplexity converges to 59.06 and test perplexity\nis 58.86. When training with K = 10, 000 the\nvalidation perplexity converges to 88.71 with test\nperplexity 88.54.\nThe embeddings of anchor users can be obtained\nfrom the user embedding layer in the trained model.\nTo learn the embeddings of new users, we freeze\nall parameters of the trained model except the user\nembedding layer. We train the model on the data\nfrom each new user separately with the same train-\ning strategy. It takes 2 minutes to learn the em-\nbedding of each new user. The average test per-\nplexity is 66.67 when K = 100 and 90.48 when\nK = 10, 000. For each pair of new user and an-\nchor user, we use the cosine similarity between two\nembeddings as the similarity.\nPerplexity-Based (PPLB). Given N trained\nLMs, one for each user, we can then use the per-\nplexity of one LM on another user’s data as a mea-\nsure of distance. We could compare the word-level\ndistributions, though this would be very compu-\ntationally expensive. In our experiments, we use\nthe probability of the correct words only, or the\nperplexity of each model on each new user’s data.\nWe take the large LM trained on all anchor users,\nas described in the user embedding section and fine-\ntune it for each anchor user. We then measure the\nperplexity of each model on the data of each new\nuser. For this matrix of new ×anchor perplexities,\nwe turn each row, representing a new user, into a\nsimilarity vector by computing 1 − c−min(row)\nmax(row) for\neach cell, c. This step is expensive, taking close\nto 24 hours for K = 100and intractable given our\nhardware constraints in the K = 10, 000 setting.\n5 Leveraging Similar Users\nOur three similarity methods provide a way to iden-\ntify anchor users with the most relevant data for a\nnew user. In this section, we describe two methods\nto learn from that data to construct a personalized\nmodel.\n5.1 Weighted Sample Fine-tuning\nUsers who speak in a similar style or about similar\ncontent may be harder to distinguish from each\nother and should then be more similar. For a given\nsimilarity metric, we compute similar users and\nuse data from these users to fine-tune an LM before\nfine-tuning for the new user.\nWe compare to two baselines, (1) a model trained\non all anchor users with no fine-tuning and (2) a\nmodel trained on all anchor users that is fine-tuned\non the new user’s data, as is done in standard fine-\ntuning. Our method of weighted sample fine-tuning\nhas two steps. The first step is to fine-tune the\nmodel trained on all anchor users on a new set of\nsimilar users, as determined by our chosen simi-\nlarity metric. Then we fine-tune as in the standard\ncase, by tuning on the new user’s data.\n5.2 Interpolation Model\nOur interpolation model is built from individual\nLMs constructed for each anchor user. It takes the\npredictions of each anchor user model and weights\ntheir predictions by that anchor’s similarity to the\nnew user. No model updates are done in this step,\nwhich makes it immediately applicable, without\nrequiring further training, even if the aggregation\nof output from all anchor models is more resource\nintensive.\nWe also want to incorporate the predictions of\nthe model fine-tuned on the new user data with\nthe predictions of models trained on similar anchor\nusers. We define a set of similar anchor users, σ,\neach of which has a similarity to the new user,\nn. We vary s for each similarity function. The\nweight to give the new user fine-tuned model is η,\nand we interpolate as follows for a given resulting\nprobability pr, of a word, w:\npr(w|·) =ηpn(w|·)+(1 −η)\nX\ni∈σ\ns(σi, n)pσi (w|·)\nThe similarities are adjusted to the range (0, 1) and\nnormalized to sum to one.\n6 Results\nWe divide our results into separate subsections for\neach of the anchor sets. On the small anchor set\nwe were able to perform more exploration of the\nweighted fine-tuning method, as it does not scale\nas well to the large anchor set.\nWe present results using standard perplexity mea-\nsurements as a function of the probability of a cor-\nrect prediction of a token. We also present results\nwith accuracy at N, where a prediction is counted\nas correct if the correct token occurs within the top\nN most probable words given by the model.\n1746\n#Sim. ∆ Perplexity ∆ Accuracy@1\nMethod Users UE AA PPLB UE AA PPLB\nWeighted Fine-tuning 5 0.276 1.728 0.627 0.159 0.155 0.148\nInterpolation 100 -2.055 -2.415 -1.992 0.249 0.277 0.223\nInterpolation 50 -2.163 -2.415 -2.043 0.260 0.277 0.204\nInterpolation 25 -2.242 -2.415 -2.022 0.248 0.277 0.232\nInterpolation 10 -2.286 -2.435 -2.183 0.235 0.260 0.249\nTable 2: Difference in perplexity for our interpolated model and weighted fine-tuning results on the small anchor\nset. The baseline metrics are subtracted from our model, meaning that more negative perplexity and more positive\naccuracy are better. The baseline Merity et al. (2018a) perplexity average is 64.3 for a model that uses standard\nfine-tuning and 67.6 without fine-tuning. Bold indicates best performance.\n10 20 30 40 50\n−10\n0\n10\nNumber of Similar Users\n∆ Perplexity (Ours-Baseline)\nAuthorship Attribution\nUser Embedding\nPerplexity-Based\nFigure 2: Change in perplexity for varying number of\nsimilar users considered in weighted fine-tuning for the\nthree similarity metrics.\n6.1 Small Anchor Set\nIn this section, we compare our weighted sam-\nple fine-tuning and interpolation approaches to the\nmore standard fine-tuning, where a large pretrained\nmodel is fine-tuned only on the new user’s data.\nWith no fine-tuning our LM achieves a perplexity\nof 67.6 and when fine-tuning on the new user only,\nthis perplexity drops to 64.3. For weighted fine-\ntuning, we attempt to fine-tune the large pretrained\nmodel on 100 anchors using our two step method,\nfirst fine-tuning on a million tokens from most sim-\nilar users, and then fine-tuning on new user data.\nThrough tuning the number of similar users, we\nfound 5 worked best. For the interpolation model,\nwe found more similar users improved accuracy,\nthough perplexity was slightly higher for ten sim-\nilar users. Our interpolation model combines pre-\ndictions from similar anchor user LMs. We have an\nLM fine-tuned to each of our anchor users and for\na given new user we predict words by weighting\nthe predictions of the models representing the most\nsimilar users.\nResults in Table 2 show that our weighted sample\nfine-tuning is not able to outperform the baseline\nfor any of our three similarity metrics. Perplexity\nand accuracy results are reported averaged over\nthe test set users. We also tried fine-tuning with\nrandom user’s data and found that this performance\nwas better than no fine-tuning but worse than fine-\ntuning on new user data only, showing that there\nis no added benefit from simply continuing to fine-\ntune on all data.\nFor the interpolation model, we tune η (see Sec-\ntion 5.2) on a held-out set and use a value of 0.7.\nThe results show that the authorship attribution sim-\nilarity performs best on both metrics. We find that\nas the number of similar users increases it has little\neffect past around ten similar users, as the similarity\nweights decrease and have a smaller impact.\nRetraining with Similar User Data:It appears\nthat having similar user data does not help the\nweighted fine-tuning model. To further investi-\ngate this we looked at settings where the amount\nof training data is fixed, but the source is either\nrandom, or a sample of similar user’s data. For\neach new user, we build six datasets: a random\ndataset and five datasets consisting of data from\ntop-k similar anchor users for this new user where\nk is in {10, 20, 30, 40, 50}. Each of these datasets\nhas 2m tokens. The random dataset is comprised of\n20k tokens from each anchor user. For the dataset\nbuilt from the top-k similar users, we want the num-\nber of tokens selected from each anchor user to be\nproportional to the similarity between the new user\nand each anchor user. To do this, we normalize\nthe three similarities by subtracting the minimum\nand dividing by the maximum such that they are\nbetween zero and one.\nFor a given set of k users and similarity metric,\nwe sort all anchor users in descending order by\ntheir similarity to the new user and choose the top\nk anchor users. For the rank 1 anchor user a1, we\nchoose the following number of tokens from the\n1747\n#Sim. Users ∆ Perplexity Std.Dev.\nRandom 10 0.176 0.367\n10 -0.354 0.659\n20 -0.534 0.977\n30 -0.673 1.080\n40 -0.714 1.040\n50 -0.803 1.127\n100 -0.941 1.351\n150 -0.986 1.560\n200 -1.069 1.549\nTable 3: Difference in perplexity for fine-tuning varying\nnumber of similar users on the large anchorset, first\nfine-tuning on similar users, and second on the new\nuser’s data, as compared to Merity et al. (2018a) fine-\ntuned on new user data only with perplexity 89.7. Each\nsimilar user has 2k tokens and each new user has 2k.\ntraining data, where s(·, ·) is the similarity between\na pair of users:\nna1 = 2000k ∗ s(newuser, a1)Pk\ni=1 s(newuser, ai)\nIf na1 > 200k, we choose na1 = 200k. For the\nrank x anchor user ax, we choose\nnax = (2000k −\nx−1X\nj=1\nnaj ) ∗ s(newuser, ax)Pk\ni=x s(newuser, ai)\ntokens from their training data. If nax > 200k, we\nchoose nax = 200k. We repeat this procedure until\nthe rank k anchor user. The ratio of similarities\nin this equation enforces that the amount of data\nwe select from each of the top-k similar users is\nproportional to their similarity.\nWe then train a separate model on each dataset.\nThe architecture of the model is the same as what\nis described in Section 4.1 except that it does not\nhave a user embedding layer. We then fine-tune the\ntrained models on the training data of the new user.\nFor a chosen similarity metric and number k, we\naverage the test perplexity of the fine-tuned models\nfor all new users and subtract from it the average\ntest perplexity of the fine-tuned models trained on\nrandom datasets, whose average perplexity is 111.0.\nThe results are shown in Figure 2 with shaded areas\nindicating standard deviation. In the figure, the\nlower a point is, the better the datasets built using\nthe corresponding similarity metric and number k\nis for training an LM for new users, which we infer\nis because the weighted sample datasets are closer\nto the data from new users.\nWe see that in terms of similarity metrics, the\nuser embedding is the best while perplexity-based\nis the worst. As k increases, the performance first\nincreases then decreases. The best performance\nis achieved when using the similarities calculated\nwith user embeddings and using top 20 or 30 simi-\nlar anchor users. After that, including more users\nhas little effect, as their similarity weights continue\nto decrease. The main takeaway from this experi-\nment is that although similar user data helps more\nthan random data, the benefit does not transfer to\nthe larger fine-tuning scenario. This area may be\nworth further exploring for fine-tuning strategies\nor for training data selection in applications where\nnew models must be trained.\n6.2 Large Anchor Set\nIn a set of only one hundred anchor users, it may be\nthe case that existing users are not similar enough\nto the new user to benefit from our approach. To\ntest this idea we ran experiments using the larger\nset of 10k anchor users and 100 new users.\nTaking our most promising user embedding simi-\nlarity metric from the weighted sample fine-tuning,\nwe tested this method’s performance varying the\nnumber of similar users. Our results in Table 3\nshow a reduction in perplexity of 0.94 at 100 sim-\nilar users and over one point at 200 users. There\nis a logarithmic improvement with the number of\nsimilar users considered, as we would expect more\ndissimilar users to be less informative. The results\nin this table suggest that the anchor set must be di-\nverse enough to contain similar users to new users,\nin order to benefit from this method.\nWe also try the interpolation model with a larger\nset of anchor users. Our base model is trained on\n10k anchor users and 2k tokens from each anchor.\nNote that we are controlling for the total points\nfrom anchor users, using 100 times fewer points\nper user and 100 times more users. Scaling up\nthese experiments to more points and users is com-\nputationally expensive but may be worth exploring\nin future work. We fine-tune this model to each\nsimilar anchor user for weighting predictions. On\na held-out set we tune η and find that in this setting\nperformance starts to drop after around 10 similar\nusers. It is computationally expensive to run each\nof the 10k models on each new user. The perplex-\nity similarity metric requires that all of these are\nrun in order to determine similarity and thus is not\nscalable to the large anchor user setting. The user\nembedding metric scales better because similarity\ncan be determined by tuning an existing LM on\n1748\n#Sim. 2k per Anchor ∆ 6k per Anchor ∆\nUsers PPL Acc @1 Acc @3 Acc @5 Acc @10 PPL Acc @1 Acc @3 Acc @5 Acc @10\n10 -0.692 0.097 0.111 0.100 0.058 -11.726 0.497 0.697 0.723 0.718\n5 -0.615 0.091 0.103 0.090 0.049 -11.463 0.491 0.656 0.694 0.705\n4 -0.590 0.088 0.091 0.079 0.045 -11.287 0.486 0.650 0.677 0.684\n3 -0.553 0.084 0.087 0.072 0.039 -11.001 0.457 0.622 0.657 0.654\n2 -0.415 0.084 0.060 0.052 0.033 -10.604 0.439 0.588 0.602 0.617\n1 -0.006 0.047 0.016 0.002 -0.002 -8.866 0.282 0.423 0.485 0.516\nTable 4: Comparison of our interpolated user embedding similarity model on the large anchorset to a standard\nfine-tuned Merity et al. (2018a) baseline measured in perplexity and accuracy @N. We show results for 2k and 6k\ntokens per anchor user, showing improved performance when more data per anchor is available. Bold indicates best\nperformance.\nFigure 3: Heat maps showing normalized similarity for each metric on our 100 author anchor set.\nMetric 1 Metric 2 Pearson’s r Spearman’s ρ\nUE AA 0.360 0.362\nUE PPL 0.280 0.316\nPPL AA 0.073 0.025\nTable 5: Spearman and Pearson correlation coeffi-\ncients for each pair of similarity metrics (User Embed-\ndings (UE), Authorship Attribution (AA), and Perplex-\nity (PPL)) computed for each of our 100 anchor users\nsimilarity to each new user.\nnew user data. For ten similar users we require\n1,000 times fewer computations than we would to\nweight all 10k users. We found that authorship at-\ntribution performed much worse in this setting, as\nthe confusion matrix becomes very sparse.\nThe results for our best similarity metric, user\nembeddings, are shown in Table 4. On the left\nwe see performance for our model on the larger\nset containing 2k tokens per anchor user. For this\nanalysis of our best, scalable model, we include\naccuracy @N, a metric denoting the percentage of\ntimes the correct word was in the top-N most prob-\nable choices. This is comparable to Table 3, where\nwe used the same amount of data for the weighted\nsample fine-tuning approach. On the right we see\nperformance when the amount of data per anchor\nuser is tripled. The baseline and fine-tuned models\nall benefit from this additional data, however we\nfind that the difference in perplexity is much larger,\nas having additional data will allow the models to\nlearn more accurate similarity metrics. We also\nfind that when tuning η it tends toward 0.6 when\nthere are 2k tokens per anchor user but 0.3 when\nthere are 6k. As the amount of data from the anchor\nusers increases, the optimal interpolation weights\nshift to weight the anchor user models more heavily\nthan the model fine-tuned on the new user. How\nthe tuning of η could be done on a per-user basis,\nrather than globally, is an interesting open question.\n7 Analysis\n7.1 Differences in Similarity Functions\nWe looked at the differences between our three sim-\nilarity functions by computing the correlation coef-\nficients for Spearman’s ρ and Pearson’s r in Table\n5. Interestingly, the perplexity and authorship attri-\nbution metrics correlate much more strongly with\nthe user embedding metric than with each other. It\nis possible that the user embedding metric performs\nbest in our experiments because it contains more of\nthe useful information from both of the other met-\nrics. Additional heat maps for each metric are in\nFigure 3. In general, they show that the three met-\nrics seem to capture different information about the\nrelationships between users. The user embedding\nmetric leads to more evenly distributed similarities,\nwhile the other two metrics have outlier anchor\nusers that show stronger correlation with a subset\nof the new users.\n1749\nfuels, qaeda, zealand, inte, al., antonio, facto, neutrality, kong, differ, olds, custody, cruise, obliga-\ntion, arts, beck, guise, scrolls, vegas, mph, dame, conclusions, laden, pedestal, throne, ck, charm,\noccasions, disorders, correctness, disposal, capita, hominem, floyd, thrones, sarcastic, ghz, explorer,\ncomprehension, standpoint, ambulance, noting, diego, accusations, cares, forth, enforcement, amp,\nnukem, convicted\nTable 6: Top 50 words for which our best model outperforms the baseline based on the frequency of word correctly\npredicted normalized by the word’s total frequency.\n7.2 Personalized Words\nWe take the highest performing model using user\nembedding similarity trained on our large anchor\nuser set and compare it to our baseline model to\nlook at which words are more accurately predicted.\nBy taking the number of times each word is cor-\nrectly predicted by the best model when the base-\nline was wrong and dividing by the total number\nof occurrences of that word in our language model-\ning data, we can find words that have the highest\nnormalized frequency of being improved by our\nmodel.\nThe top 50 words for which we see improvement\nare shown in Table 6. We see the second word of\nmany two-word proper nouns in this set. Many\nnames can start with “San” or “Las” and so we see\n“vegas”, “diego”, and “antonio”, in this list. Simi-\nlarly, “new” precedes “zealand” and other location\nnames. The top word is “fuels”, which occurs of-\nten in the data in conversation about “fossil fuels”,\nthough there are also many others that mention\nother kinds of fuels, or use “fuels” as a verb, as\nin “it fuels outrage”. We also see that units such\nas “mph” or “ghz” are more accurately predicted.\nThe units that one chooses may be more common\ndepending on where one lives, or in the case of\n“ghz” it may depend more on the subject matter that\na user is familiar with or tends to talk about. Other\nproper nouns such as “game of thrones”, or “hong\nkong” vs. “donkey kong”, contain common words,\nwhich individually may be hard to predict, but with\nknowledge of an individual’s preferences could be\npredicted more accurately.\n8 Ethical Considerations\nWork on personalized LMs could be used for\nsurveillance by detecting language from individ-\nuals or groups (Stamatatos, 2009). We recommend\nagainst such applications, as they threaten intel-\nlectual freedom and risk discrimination (Richards,\n2013). There may be a risk in storing private data\nnecessary to construct these models, as data may\nnot be properly secured or used. Furthermore, a\npersonalized model could reinforce incorrect lan-\nguage usage, which may be an issue for individ-\nuals learning to speak a new language, making it\nmore difficult to learn. Learning personal language\npatterns in a given context and suggesting these\npatterns in other contexts may lead to potentially\nincorrect or offensive results and we recommend\nthat if this type of personalization is deemed appro-\npriate, users are made aware of how their data is\nbeing used and potential consequences.\n9 Conclusions\nIn this paper, we addressed the issue of language\nmodeling in a low data setting where a new user\nmay not have enough data to train a personalized\nLM and presented a novel approach that lever-\nages data from similar users. We considered three\nsimilarity metrics and two methods of leveraging\ndata from similar anchor users to improve the per-\nformance of language modeling over a standard\nfine-tuning baseline, and showed how our results\nvary with the amount of data available for anchor\nusers and the number of available anchor users.\nWe found that the most easily scalable and high-\nest performing method was to use user embedding\nsimilarity and to interpolate similar user fine-tuned\nmodels. Additionally, we provided an analysis of\nthe kind of words that our personalized models\nare able to more accurately predict and further dis-\ncussed limitations of our methods.\nAcknowledgments\nThis material is based in part on work supported by\nthe NSF (grant #1815291) and the John Templeton\nFoundation (grant #61156). Any opinions, findings,\nconclusions, or recommendations in this material\nare those of the authors and do not necessarily re-\nflect the views of the NSF or the John Templeton\nFoundation. Clover icons taken from Freepik at\nflaticon.com.\n1750\nReferences\nRichard Antonello, Nicole Beckage, Javier Turek, and\nAlexander Huth. 2021. Selecting informative con-\ntexts improves language model fine-tuning. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers).\nDavid Bamman, Chris Dyer, and Noah A. Smith. 2014.\nDistributed representations of geographically situated\nlanguage. In Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers).\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping. arXiv\npreprint arXiv:2002.06305.\nShiran Dudy, Steven Bedrick, and Bonnie Webber. 2021.\nRefocusing on relevance: Personalization in NLG.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing.\nLucie Flek. 2020. Returning the N to NLP: Towards\ncontextually personalized classification models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics.\nAparna Garimella, Carmen Banea, and Rada Mihalcea.\n2017. Demographic-aware word associations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing.\nZhenhao Ge, Yufang Sun, and Mark J. T. Smith. 2016.\nAuthorship Attribution Using a Neural Network Lan-\nguage Model. In Thirtieth AAAI Conference on Arti-\nficial Intelligence.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: frequency-agnostic\nword representation. In Advances in Neural Informa-\ntion Processing Systems.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a\ncontinuous cache. In International Conference on\nLearning Representations.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2021. Dynamic contextualized word em-\nbeddings. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers).\nYu-Yang Huang, Rui Yan, Tsung-Ting Kuo, and Shou-\nDe Lin. 2016. Enriching cold start personalized lan-\nguage model using social network information. In\nInternational Journal of Computational Linguistics\n& Chinese Language Processing, Volume 21, Number\n1, June 2016.\nMilton King and Paul Cook. 2020. Evaluating ap-\nproaches to personalizing language models. In Pro-\nceedings of the 12th Language Resources and Evalu-\nation Conference.\nMoshe Koppel, Jonathan Schler, and Shlomo Argamon.\n2009. Computational methods in authorship attribu-\ntion. Journal of the American Society for information\nScience and Technology, 60(1):9–26.\nYinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang,\nTong Xiao, Jingbo Zhu, Tongran Liu, and Changliang\nLi. 2020. Learning architectures from an extended\nsearch space for language modeling. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics.\nVeronica Lynn, Youngseo Son, Vivek Kulkarni, Niran-\njan Balasubramanian, and H. Andrew Schwartz. 2017.\nHuman centered NLP with user-factor adaptation. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing.\nGábor Melis, Tomáš Koˇcisk`y, and Phil Blunsom. 2019.\nMogrifier LSTM. In International Conference on\nLearning Representations.\nStephen Merity. 2019. Single headed attention RNN:\nStop thinking with your head. arXiv preprint\narXiv:1911.11423.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018a. An analysis of neural language\nmodeling at multiple scales. arXiv preprint\narXiv:1803.08240.\n1751\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018b. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP).\nBoris T Polyak and Anatoli B Juditsky. 1992. Accelera-\ntion of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization , 30(4):838–\n855.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nNeil M Richards. 2013. The dangers of surveillance.\nHarv. L. Rev., 126.\nLiqun Shao, Sahitya Mantravadi, Tom Manzini, Alejan-\ndro Buendia, Manon Knoertzer, Soundar Srinivasan,\nand Chris Quirk. 2020. Examination and extension\nof strategies for improving personalized language\nmodeling via interpolation. In Proceedings of the\nFirst Workshop on Natural Language Interfaces.\nEfstathios Stamatatos. 2009. A survey of modern au-\nthorship attribution methods. Journal of the Ameri-\ncan Society for information Science and Technology,\n60(3):538–556.\nCharles Welch, Jonathan K. Kummerfeld, Verónica\nPérez-Rosas, and Rada Mihalcea. 2020. Compo-\nsitional demographic word embeddings. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nXiaodong Wu, Weizhe Lin, Zhilin Wang, and Elena\nRastorgueva. 2020. Author2Vec: A Framework for\nGenerating User Embedding. arXiv e-prints, page\narXiv:2003.11627.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Processing\nSystems.\nKe Zhou, Shuang-Hong Yang, and Hongyuan Zha. 2011.\nFunctional matrix factorizations for cold-start recom-\nmendation. In Proceedings of the 34th international\nACM SIGIR conference on Research and develop-\nment in Information Retrieval.\nA Hyperparameters for Authorship\nAttribution Model\n• Bidirectional LSTM layers=3\n• LSTM hidden dim=400\n• output dropout=0.5\n• fully-connected layer dim= 800 × K\n• Adam optimizer\n• cross-entropy loss\n• learning rate=1e-3\n• batch size=64\n• early stopping if no improvement over 10\nepochs\nB Hyperparameters for User Embedding\nModel\n• scalar dropout=0.1\n• embedding dropout=0.2\n• LSTM layers=3\n• LSTM hidden dim=1,150\n• recurrent dropout=0.2\n• user embedding dim=50 (tried 20,50,100 but\n50 worked best)\n• cross-entropy loss\n• early stopping if no improvement over 20\nepochs\n• sequence length=70\n• batch size=20\n• learning rate=3\n• parameter clipping=0.25\nC Running Times\nAuthorship attribution models are trained on an\nNVIDIA GeForce RTX-2080Ti GPU and take 2.5\nhours for K = 100 anchors and 4 hours for K =\n10, 000 anchors.\nTraining a new language model for weighted\nfine-tuning as described in Section 6.1 takes about\n2.5 hours to train a model on a dataset on an\nNVIDIA Tesla V100 GPU. Fine-tuning the trained\nmodels on the training data of the new user takes\nabout one minute on average.\nThe user embedding models are trained on an\nNVIDIA GeForce RTX-2080Ti GPU. For K =\n100 anchors, it took 132 hours. When training with\nK = 10, 000, we reduced the hidden LSTM size\nto 500, which reduced training time to 112 hours.\n1752",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6684824228286743
    },
    {
      "name": "Computational linguistics",
      "score": 0.52348792552948
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.49184584617614746
    },
    {
      "name": "Association (psychology)",
      "score": 0.48612117767333984
    },
    {
      "name": "Natural language processing",
      "score": 0.43932798504829407
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3434561491012573
    },
    {
      "name": "Linguistics",
      "score": 0.3278290629386902
    },
    {
      "name": "World Wide Web",
      "score": 0.3259729743003845
    },
    {
      "name": "Psychology",
      "score": 0.10633707046508789
    },
    {
      "name": "Philosophy",
      "score": 0.07618683576583862
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ],
  "cited_by": 16
}