{
  "title": "pACP-HybDeep: predicting anticancer peptides using binary tree growth based transformer and structural feature encoding with deep-hybrid learning",
  "url": "https://openalex.org/W4406015449",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5101658420",
      "name": "Muhammad Khalil Shahid",
      "affiliations": [
        "Abdul Wali Khan University Mardan"
      ]
    },
    {
      "id": "https://openalex.org/A5037634953",
      "name": "Maqsood Hayat",
      "affiliations": [
        "Abdul Wali Khan University Mardan"
      ]
    },
    {
      "id": "https://openalex.org/A5059292065",
      "name": "Wajdi Alghamdi",
      "affiliations": [
        "King Abdulaziz University"
      ]
    },
    {
      "id": "https://openalex.org/A5003063566",
      "name": "Shahid Akbar",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5030196845",
      "name": "Ali Raza",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5029682293",
      "name": "Rabiah Abdul Kadir",
      "affiliations": [
        "National University of Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5038272012",
      "name": "Mahidur R. Sarker",
      "affiliations": [
        "National University of Malaysia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2907031619",
    "https://openalex.org/W4315754639",
    "https://openalex.org/W1520340830",
    "https://openalex.org/W4311676588",
    "https://openalex.org/W2118911320",
    "https://openalex.org/W2074196504",
    "https://openalex.org/W2340970647",
    "https://openalex.org/W2625609557",
    "https://openalex.org/W2747758005",
    "https://openalex.org/W2793278326",
    "https://openalex.org/W2891164245",
    "https://openalex.org/W2945375732",
    "https://openalex.org/W2133073150",
    "https://openalex.org/W2806146459",
    "https://openalex.org/W3127712659",
    "https://openalex.org/W3048086963",
    "https://openalex.org/W3043293280",
    "https://openalex.org/W4312113249",
    "https://openalex.org/W1999798000",
    "https://openalex.org/W2891465508",
    "https://openalex.org/W2945297971",
    "https://openalex.org/W3184125572",
    "https://openalex.org/W2943935116",
    "https://openalex.org/W2971874382",
    "https://openalex.org/W4284973298",
    "https://openalex.org/W4225983992",
    "https://openalex.org/W4387860362",
    "https://openalex.org/W4285098081",
    "https://openalex.org/W4285744167",
    "https://openalex.org/W4313278994",
    "https://openalex.org/W4386475504",
    "https://openalex.org/W4391288425",
    "https://openalex.org/W4390460649",
    "https://openalex.org/W4393234394",
    "https://openalex.org/W2961926658",
    "https://openalex.org/W2987011545",
    "https://openalex.org/W4404522132",
    "https://openalex.org/W3033211610",
    "https://openalex.org/W4402071909",
    "https://openalex.org/W4396707011",
    "https://openalex.org/W1995757481",
    "https://openalex.org/W2776648280",
    "https://openalex.org/W3194982939",
    "https://openalex.org/W3184871101",
    "https://openalex.org/W4392358980",
    "https://openalex.org/W4403420423",
    "https://openalex.org/W4308589354",
    "https://openalex.org/W4220831705",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W3037888463",
    "https://openalex.org/W4388033280",
    "https://openalex.org/W4393187479",
    "https://openalex.org/W4321597590",
    "https://openalex.org/W2047672715",
    "https://openalex.org/W2548536600",
    "https://openalex.org/W4392562486",
    "https://openalex.org/W4401315156",
    "https://openalex.org/W2946492269",
    "https://openalex.org/W4220900187",
    "https://openalex.org/W4366451146",
    "https://openalex.org/W2994622524",
    "https://openalex.org/W2588704895",
    "https://openalex.org/W2904857268",
    "https://openalex.org/W4389916307",
    "https://openalex.org/W2799991731",
    "https://openalex.org/W3109659125",
    "https://openalex.org/W4386913943",
    "https://openalex.org/W4404961170",
    "https://openalex.org/W3117695338",
    "https://openalex.org/W3107307063",
    "https://openalex.org/W4403001755",
    "https://openalex.org/W2995175313",
    "https://openalex.org/W2079334893",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2521029800",
    "https://openalex.org/W3208498407",
    "https://openalex.org/W4382195656",
    "https://openalex.org/W2970541594",
    "https://openalex.org/W4387272159",
    "https://openalex.org/W2884524796",
    "https://openalex.org/W4210522876",
    "https://openalex.org/W4367365713",
    "https://openalex.org/W4402314649",
    "https://openalex.org/W3102846393"
  ],
  "abstract": "Worldwide, Cancer remains a significant health concern due to its high mortality rates. Despite numerous traditional therapies and wet-laboratory methods for treating cancer-affected cells, these approaches often face limitations, including high costs and substantial side effects. Recently the high selectivity of peptides has garnered significant attention from scientists due to their reliable targeted actions and minimal adverse effects. Furthermore, keeping the significant outcomes of the existing computational models, we propose a highly reliable and effective model namely, pACP-HybDeep for the accurate prediction of anticancer peptides. In this model, training peptides are numerically encoded using an attention-based ProtBERT-BFD encoder to extract semantic features along with CTDT-based structural information. Furthermore, a k-nearest neighbor-based binary tree growth (BTG) algorithm is employed to select an optimal feature set from the multi-perspective vector. The selected feature vector is subsequently trained using a CNN + RNN-based deep learning model. Our proposed pACP-HybDeep model demonstrated a high training accuracy of 95.33%, and an AUC of 0.97. To validate the generalization capabilities of the model, our pACP-HybDeep model achieved accuracies of 94.92%, 92.26%, and 91.16% on independent datasets Ind-S1, Ind-S2, and Ind-S3, respectively. The demonstrated efficacy, and reliability of the pACP-HybDeep model using test datasets establish it as a valuable tool for researchers in academia and pharmaceutical drug design.",
  "full_text": "pACP-HybDeep: predicting \nanticancer peptides using binary \ntree growth based transformer and \nstructural feature encoding with \ndeep-hybrid learning\nShahid1, Maqsood Hayat1, Wajdi Alghamdi2, Shahid Akbar1,3, Ali Raza4,  \nRabiah Abdul Kadir5 & Mahidur R. Sarker5,6\nWorldwide, Cancer remains a significant health concern due to its high mortality rates. Despite \nnumerous traditional therapies and wet-laboratory methods for treating cancer-affected cells, these \napproaches often face limitations, including high costs and substantial side effects. Recently the high \nselectivity of peptides has garnered significant attention from scientists due to their reliable targeted \nactions and minimal adverse effects. Furthermore, keeping the significant outcomes of the existing \ncomputational models, we propose a highly reliable and effective model namely, pACP-HybDeep \nfor the accurate prediction of anticancer peptides. In this model, training peptides are numerically \nencoded using an attention-based ProtBERT-BFD encoder to extract semantic features along with \nCTDT-based structural information. Furthermore, a k-nearest neighbor-based binary tree growth (BTG) \nalgorithm is employed to select an optimal feature set from the multi-perspective vector. The selected \nfeature vector is subsequently trained using a CNN + RNN-based deep learning model. Our proposed \npACP-HybDeep model demonstrated a high training accuracy of 95.33%, and an AUC of 0.97. To \nvalidate the generalization capabilities of the model, our pACP-HybDeep model achieved accuracies \nof 94.92%, 92.26%, and 91.16% on independent datasets Ind-S1, Ind-S2, and Ind-S3, respectively. The \ndemonstrated efficacy, and reliability of the pACP-HybDeep model using test datasets establish it as a \nvaluable tool for researchers in academia and pharmaceutical drug design.\nKeywords Anticancer peptides, Transformer encoder, Physiochemical properties, Deep Hybrid neural \nnetwork, Binary tree growth feature selection\nCancer has emerged as a significant threat to human life and health1. According to the Global Cancer Statistics \n2022, ~ 19–20 million new cancer cases are diagnosed annually 2. Additionally, the National Center for Health \nStatistics reported 1,958,310 new cancer cases and 609,820 cancer-related deaths in the United States alone \nin 2023 3. Conventional treatments, such as radiation therapy, chemotherapy, immunotherapy, and surgery, \nremain the primary approaches for treating cancer diseases4. However, these methods often suffer from limited \neffectiveness due to risks of recurrence and severe side effects, including nausea, compromised immunity, and \nhair loss. As a result, the development of novel therapeutic strategies has become imperative. Over the past \ndecade, the discovery of anticancer peptides (ACPs) has offered promising alternatives for cancer treatment due \nto their minimal side effects and high selectivity toward targeted cancer cells5. However, experimental methods \nfor identifying ACPs are expensive, complex, labor-intensive, and time-consuming. Therefore, the reliable and \nrapid identification of ACPs is of critical importance for advancing cancer therapies.\n1Department of Computer Science, Abdul Wali Khan University Mardan, Mardan 23200, KP , Pakistan. 2Department \nof Information Technology, Faculty of Computing and Information Technology, King Abdulaziz University, \nJeddah, Saudi Arabia. 3Institute of Fundamental and Frontier Sciences, University of Electronic Science and \nTechnology of China, Chengdu 610054, China. 4Department of Computer Science, MY University, Islamabad \n45750, Pakistan. 5Institute of Visual Informatics, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor, Malaysia. \n6Universidad de Dise˜no, Innovaci´on y Tecnología, UDIT, Av. Alfonso XIII, 97, 28016 Madrid, Spain. email:  \nm.hayat@awkum.edu.pk; shahid.akbar@awkum.edu.pk; rabiahivi@ukm.edu.my\nOPEN\nScientific Reports |          (2025) 15:565 1| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports\n\nIn the past decade, several machine-learning models have been developed to predict anticancer peptides \n(ACPs). Tyagi et al. introduced AntiCP6, SVM-based model utilizing sequential and binary features. Subsequently, \nHajisharifi et al. represented ACP samples using local alignment and PseAAC-based schemes 7. Subsequently, \nChen et al. proposed the first sequential predictor, the iACP model, for ACP prediction 8. Later, Akbar et al. \npresented iACP-GAEnsC, a genetic algorithm-based ensemble classifier 9. Where hybrid features are derived \nby combining sequential encoding with physicochemical and structural properties. In the MLACP predictor, \nSVMACP and RFACP models were developed by leveraging sequential and structural properties to predict \nACP samples10. Similarly, Hu et al. introduced a g-gap dipeptides feature vector trained with RF and SVM \nmodels11. Kabir et al. in TargetACP , evaluated composite protein sequence representation and PSSM profile-\nbased features using KNN, RF , and SVM classifiers 12. Likewise, ACPred 13, and ACPred-FL 14 utilized various \nsequential, physicochemical, and structural features in conjunction with SVM-based models. Charoenkwan et \nal. developed the iACPFSCM model, focusing on interpretability through weighted-sum-based prediction 15. \nAdditionally, Agrawal et al. proposed AntiCP 2.0 , a sequence-based approach for ACP prediction 16. In the \ncACP-2LFS model, a two-level feature selection strategy was applied to sequential features for ACP prediction17. \nLiang et al. developed iACP-GE, a predictor that employed a multi-perspective feature vector with a gradient-\nboosting decision tree and an extra tree-based ensemble classifier for ACP identification18.\nTo enhance discriminative power, alternative techniques that preserve sequence order information and enable \nlocal intrinsic feature identification are essential. Over the past decade, deep learning models have gained prominence \ndue to their efficiency in handling large datasets and their ability to autonomously represent features19–22. Yi et al. \nintroduced ACP-DL, which integrated binary profile features and k-mer sparse matrix features into an LSTM training \nmodel23. Wu et al. employed a word2vec encoding-based deep training model24. While He et al. proposed ACPred-\nLAF , a multi-scaled embedding model that used a multi-sense-scaled embedding algorithm to map sequences22. \nThese embedding methods effectively capture peptide sequence information while preserving original data properties; \nhowever, they often overlook prior biological information about amino acids in peptide sequence representations. \nAkbar et al. presented cACP-DeepGram, a word embedding-based deep learning model for ACP prediction25. Ahmed \net al. introduced ACP-MHCNN, incorporating binary features, amino acid physicochemical characteristics, and \nsequential features26. Sun et al. developed ACP-BC, a model based on Bidirectional LSTM and BERT transformers27. \nIn the ACP-Check model, Zhu et al. employed a Bi-LSTM network integrated with five handcrafted features and a \nfully connected network to identify ACPs28. Similarly, models such as ACP-MCAM29 and ACPred-BMF30 have been \ndeveloped for ACP prediction. Azim et al. introduced iACP-RF, an ensemble random forest-based model utilizing \nbinary profile and amino acid frequency representations31. The ACP-ML model employed majority voting-based \nensemble learning32. It utilized five sequential formulation methods to extract peptide features, applied SMOTENN \nand SMOTETomek techniques for class balancing, and implemented a two-tier feature selection process to identify \noptimal features. Ten machine-learning models were trained on these features, culminating in a voting mechanism-\nbased ensemble learning model for ACP prediction. Karim et al. developed ANNprob-ACPs, a probability-based \nfeature integration model for ACPs33. Initially, nine sequential encoding methods were used to formulate peptide \nsamples, which were evaluated across six machine-learning models. Features from all encoding methods were then \nintegrated using a probabilistic approach, yielding improved predictive results. Similarly, the MA-PEP model employed \ntwo encoding modules—sequential-based and chemical information-based34. These features were integrated using \nmultiple attention mechanisms and trained with a multilayer perceptron for validation. After investigating the existing \nACP computational models, it is evident most approaches primarily rely on sequence-based residue encoding while \nneglecting the intrinsic structure of sequence order. Additionally, these models often fail to account for contextual \nrelationships, limiting their ability to capture hidden structural features in peptide samples. As a result, there is a need \nfor enhanced methods capable of effectively identifying internal motifs and structural variations in peptides. In terms \nof feature selection, current methods predominantly use traditional filter-based techniques, which are suboptimal \nin selecting highly discriminative and relevant features. This limitation adversely affects the performance of training \nmodels. During the model training phase, many existing approaches rely on conventional learning algorithms, which \nare associated with high computational costs and limited scalability. Hence, comprehensive improvements and \nalternative solutions in several aspects: encoding schemes, feature selection strategies, training methodologies, model \ninterpretability and generalization, computational efficiency, and predictive performance. Addressing these challenges \nwill enable more accurate discrimination between ACPs and non-ACPs.\nRecently, computational deep methods such as 2L-piRNADNN 35, iPredCNC 36, Deep-m5U 37, m6A-\nword2vec38, and PSSM-Sumo39 have demonstrated remarkable predictive performance using smaller datasets. \nThese models efficiently highlight the growing importance of deep learning in bioinformatics. Building on this \nsignificance, we aim to develop a hybrid computational predictor, pACP-HybDeep, for the accurate prediction \nof ACPs and non-ACPs. The step-by-step framework of our pACP-HybDeep model is illustrated in Fig. 1. The \nkey contributions of our proposed model are as follows:\n (i)  The input peptide samples are encoded using attention mechanism-based ProtBERT-BFD to gather rich \ncontextual information. Furthermore, to collect the local intrinsic and structural features, Composition, \nTransition, and Distribution (CTDT) are employed. Moreover, in generating a high discriminative vector, \nboth the ProtBERT-BFD and CTDT features are integrated, serially.\n (ii)  To reduce the computational cost of the proposed model by selecting an optimal feature set, we introduced \na novel k-nearest neighbor-based Binary Tree Growth (BTG) based feature selection algorithm.\n (iii)  Several deep learning and machine learning models are trained, among which our proposed hybrid deep \ntraining model (CNN + RNN) demonstrated superior performance.\n (iv)  The extracted features are visually interpreted via SHAP and t-SNE-based feature analysis.\n (v)  To validate the generalization of the model, three unseen independent datasets are used to prove the effec-\ntiveness of the training model.\nScientific Reports |          (2025) 15:565 2| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nMaterials and methods\nBenchmark dataset\nIn bioinformatics, selecting a benchmark dataset is a critical step in developing a deep learning-based \ncomputational training model40. In this study, we effectively evaluate the proposed approach using two training \ndatasets, namely Training-I and Training-II, balanced datasets developed by Agrawal et al. and sourced from \nthe AntiCP 2.0 webserver 16. The Training-I dataset contains experimentally validated ACPs, considered as the \npositive class, and AMPs as the negative class (non-ACPs). The AMPs were obtained from previous models such \nas ACPred-FL14, iACP8, ACPP41, and AntiCP6, which did not exhibit any anticancer activity. After removing all \nsamples representing both anticancer and antimicrobial activities, 861 samples were retained. Consequently, the \nTraining-I dataset comprised 861 experimentally validated ACPs and 861 non-ACPs.The Training-II dataset \ncontains ACPs and random samples, with the random samples considered as non-ACPs. The random peptide \nsamples were collected from SwissProt42. A balanced dataset was then created, consisting of 970 experimentally \nvalidated ACP samples and 970 non-ACP samples. To evaluate the reliability of the trained model, we employed \nthree independent datasets : Ind-S1, Ind-S2, and Ind-S3 . The Ind-S1 and Ind-S2 datasets were collected from \nYi et al23. The Ind-S1 dataset contains 740 samples, including 376 ACPs and 364 non-ACPs. The Ind-S2 dataset \ncomprises 240 samples, with 129 validated ACPs and 111 antimicrobial peptides that lack anticancer activity. \nLastly, the Ind-S3 dataset, sourced from Wei et al., includes 82 positive and 82 negative samples14.\nFeature extraction\nIn deep learning-based computational models, effectively representing peptide sequences as numerical values for \nmodel training remains a significant challenge43,44. Various encoding methods, including sequential, evolutionary, \nand word embedding techniques, have been employed in the literature39,45,46. In this study, we utilized ProtBERT-\nBFD, a transformer-based self-attention mechanism, alongside a CTDT-based sequential physicochemical structure \nencoder to extract valuable contextual and intrinsic features from peptide samples. These techniques enable the \ncapture of both sequence order information and structural properties. The following subsections provide a detailed \nexplanation of these techniques.\nProtein bidirectional encoder representations from transformers (ProtBERT-BFD)\nIn natural language processing, transformer-based encoding with deep computational models has recently achieved \nsignificant advancements in bioinformatics47. Bidirectional Encoder Representations from Transformers (BERT) was \ndeveloped to capture contextual information and semantic relationships among peptide “words”48. In this approach, \nindividual amino acids are treated as words, and entire peptide sequences are considered as sentences. Building on the \nsubstantial progress of BERT-based transformers in protein science49,50. we propose a self-attention-based ProtBERT-\nBFD model to effectively represent peptide samples, to extract the high-informative features, enabling more accurate \nand meaningful analysis of peptide sequences51.\nIn this computational model, we applied the pre-trained ProtBERT-BFD approach to convert the peptide samples into \nword-embedding vectors52. Each peptide sample is treated as a sentence and divided into 200 tokens, beginning with a \nspecial token, “CLS, ” which aggregates the embedding features of all words within the input sample53. To ensure a fixed \nFig. 1. Framework of the proposed pACP-HybDeep model.\n \nScientific Reports |          (2025) 15:565 3| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\n200-dimensional vector representation, the sequence is padded with a “PAD” token to handle variations in peptide lengths. \nAdditionally, a “SEP” token is employed to delineate the information of each peptide sample. After tokenizing the amino \nacid sequences, the input is processed by the transformer function, which generates hidden states at the self-attention layer as \noutputs50. Each peptide sentence, represented by 200 tokens, is then embedded into a 1024-dimensional feature space using \nglobal average pooling. These feature vectors are subsequently fed into the input layer of the deep learning model for training. \nFigure 2 illustrates the complete workflow of the proposed ProtBERT-BFD model.\nComposition/transition/distribution\nFixed-dimension vectors for all peptide sequences are essential as input in machine-learning approaches in \nclassification tasks. A form of physicochemical property for amino acid sequences is the Composition Transition and \nDistribution (CTD) feature, which captures the patterns of global distribution and physicochemical characteristics of \nprotein sequences. The goal of the CTD method is to transform variable-length protein sequences into fixed-length \nrepresentations that retain significant biological information54,55. Composition in CTD (CTDC), transition in CTD \n(CTDT), and distribution in CTD (CTDD) are all examples of CTD features. The CTD encoding technique has 13 \ndifferent categories of physicochemical characteristics. Which include FASG890101, ZIMJ680101, PONP930101, \nARGP820101, CASG920101, ENGD860101, and PRAM900101, represent hydrophobicity, along with normalized van \nder Waals volume, polarizability, polarity, secondary structure, charge, and solvent accessibility. For each characteristic, \nthe 20 natural amino acids are divided into three groups. In this work, we used only T descriptors, which calculate the \nfrequency of two adjacently distinct group types in the peptide. For example, consider the hydrophobicity attribute: \nin polypeptides of length L, the frequency of amino acid residue pairs formed by contiguous polar and neutral amino \nacids represents a transition value between the polar and neutral groups. The other transition values, which involve \nthe polar group and hydrophobic groups as well as the neutral group and hydrophobic group, are computed as follows:\n T(p,n) = N (p, n)+ N (n, p)\nL − 1  (1)\nwhere p, n∈{ (polar, hydrophobic), (neutral, hydrophobic), (neutral, polar)},N(p, n) and N(n, p) \ndenote the frequencies of amino acid pairs pn and np in the peptides, respectively. The length of each peptide is \ndenoted by L. The CTDT feature vector consists of 13 × 3 = 39 features.\nHybrid feature vector\nIn this study, we fused the extracted feature vectors of CTDT and ProtBERT-BFD to represent both the contextual semantic \nrelationships and physiochemical characteristics of the peptide samples. Compared to the individual encoding method, the \nhybrid vector provides a high multi-view representation of the samples, enhancing the discriminative power of the training \nfeatures for high predictive outcomes56,57. The hybrid feature vector can be mathematically formulated as follows:\n Hybrid_V ector= ProtBERT − BFD ∪ CTDT  (2)\nThus, a hybrid vector of 1063 dimensions is formed, where the ProtBERT-BFD vector consists of 1024 features and \nthe CTDT vector consists of 39 features.\nFig. 2. peptide embedding using ProtBERT-BFD.\n \nScientific Reports |          (2025) 15:565 4| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nFeature selection\nIn bioinformatics, developing a computationally efficient model with low training costs requires an effective feature \nselection method. Feature selection enhances the model throughput by reducing redundancy and dimensionality, \nwhich helps avoid overfitting58,59. Various feature selection approaches, including wrapper60, filters61, and Intrinsic62 \ntechniques, are available in the literature. In this study, we applied an optimized Binary tree Growth (BTG)-based \nfeature selection to address overfitting and reduce the computational cost of the training vector while enhancing \npredictive accuracy. The BTG algorithm was first introduced by Too et al.63, and its efficacy has been demonstrated \nin several studies64. It is a binary type of the Tree Growth algorithm65. The BTG approach can be discussed as follows:\nInitially, the population of trees is arbitrarily created, and the fitness value for each tree is computed as follows:\n \nF itness= β ∗ Ir + (1− β) ∗ |SV |\n|FV |  (3)\nwhere β  denotes the control of both feature reduction and the prediction error and its value ranges (0–1),Ir is the \nlearning error rate, FV  represents the number of training vector, and SV  denotes the selected vector.\nSubsequently, the fitness values are employed to sort out the population of trees in ascending order, the first tree group \nreceives the best T1 trees, and the mathematical formula used to produce the new tree in this group is as follows:\n Mt+1\ni = Mt\ni\nθ + rMt\ni  (4)\nwhere Mi at order i in the population represents the tree (solution), r is the randomly disturbed number ranges (0–1), \nθ signifies the tree diminution rate of power, and t represents the current number of iterations. The existing tree is \nexchanged if the newly built tree has an improved fitness score otherwise, it is kept for the next generation.\nIn the next step, T2 trees are assigned to the second group, and for each tree, the two closest trees from the first \nand second groups are identified via the Euclidian distance:\n \ndi =\n(T1+T2∑\ni=1\n(Mt\nT2 − Mt\ni )2\n)1/2\n (5)\nwhere MT2  denotes the present tree and Mi represents the tree at i - th position in the population. The distance \nbecomes infinite when MT2 = Mi , where T2 = i. Then, the two nearest trees y1,y2 with minimum di are \nselected, and the linear combination of the selected trees can be calculated as follows:\n Q = λy1 + (1− λ)y2 (6)\nwhere the parameter λ is employed to control the impact of the closest tree. The location of the tree in the second \ngroup is updated using:\n Mt+1\nT 2 = Mt\nT 2 + αQ (7)\nwhere α is the angle distribution ranges (0–1). The T3 worst trees in the third group are eliminated and substituted \nwith the new trees. The T3 can be computed as follows:\n T3 = T − T1 − T2 (8)\nwhere T is the population size.\nWithin the final group, new trees are built around the best trees using a masked operator. Following the addition \nof the newly built trees to the population, the merged population is sorted using fitness values in ascender order. In \nthe subsequent iteration, the best T trees are chosen to denote the new population. The process is repeated till the \ntermination criterion, and hence finally the optimal tree is selected as a feature set.\nTo select the optimal features, the BTG uses a transfer function to decode the location of the trees into probability \nvalues, ranges (0–1). A feature vector with a large probability number has a high possibility of being selected. In this \nmodel, we applied the sigmoid function as the transfer function which can be represented as:\n \nδ(qt\nid)= 1\n1+ e−qt\nid\n (9)\nwhere q  denotes the d - th dimensionality of the search space. the location of the tree is updated depending on \nthe value of the probability described below:\nScientific Reports |          (2025) 15:565 5| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\n \nqt+1\nid =\n{ 1, if ∂<δ t\nid\n0, otherwise  (10)\nwhere ∂ denotes a random number in ranges (0–1). The process of the mask operation in the BTG approach is \nprovided in Table 1.\nAlgorithm 1. Algorithm for BTG-based Feature Selection\nScientific Reports |          (2025) 15:565 6| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nClassification algorithms\nIn bioinformatics, consistent and dependent training models are essential for evaluating computational \nmodels66–68. To identify the most suitable hypothesis learner for our proposed model, we utilized the following \nclassifiers:\nConvolutional neural networks (CNN)\nConvolutional neural networks are widely employed for tasks such as object detection, image recognition, \nvideo analysis, and other computer vision applications 69. CNNs can automatically and efficiently determine \nthe positional rankings of features from the input data, Their architecture typically includes multiple layers, \nsuch as convolutional, pooling, and fully connected layers70. Each neuron in a CNN layer is connected to a set \nof neighboring nodes in the preceding layer. By leveraging local connections and weight-sharing mechanisms, \nCNNs exploit spatial locality patterns in the data. This approach significantly reduces computational overhead \nwhile extracting translation-invariant features 71. A convolutional layer typically consists of multiple kernels, \neach designed to detect distinct local patterns in different regions of the input72. Neurons in these layers process \nfeatures extracted by prior layers to derive higher-level abstractions. Inspired by biological visual systems, the \nconvolution operation can be mathematically expressed as follows:\n Convolution(x)ik = ReLU\n(∑M−1\nm=0\n∑N−1\nn=0\nWk\nmnxi + m, n\n)\n (11)\nwhere X indicates the input matrix, i denotes the index of the output position, and k is the index of the filter. \nWk =( Wmnk)(M×N) specifi es the weight matrix of the kth convolution kernel with size M × N, where \nM denotes the size of the window and N shows the number of input channels. ReLU, a nonlinear activation \nfunction, is applied to the convolution outputs to set negative values to zeros.\n ReLU (x) = max(0,m) (12)\nPooling layers reduce dimensionality, capturing higher-level features over broader input regions and ensuring \ntranslation invariance. Pooling is defined as:\n pooling(X)ik = max (xiM,k,x iM+1,k, ..., xiM+M−1,k) (13)\nwhere M is the pooling window size, i is the output position index, k is the filter index, and X represents the \noutput of the convolution layer. In the fully connected phase, CNNs transform pooled features into fixed-\ndimension vectors. The final output layer combines these features using a sigmoid activation function for binary \nclassification:\n sigmoid (x)= 1\n1+ e−x  (14)\nRecurrent neural network\nRecurrent neural networks are highly effective for sequence modeling in bioinformatics, particularly for \nhandling protein sequences of varying lengths. RNNs maintain internal state memory to capture information \nfrom previous time steps, allowing them to model temporal dependencies within data 73,74. This makes RNNs \nsuitable for tasks where sequence order and contextual information are critical. Unlike conventional forward \npropagation, RNNs employ backpropagation through time t, updating the hidden state at each time step as \nfollows:\n ht = ∂ (ht−1Whh + xtWhx + bsh) (15)\nwhere ht is the hidden state at time t, xt is the input at time t, Whx is the weight-matrix for input to hidden \nconnections, Ot = soft max(Wohht + bsO)  is a matrix from among hidden connections, bsh is the bias \nvector for the hidden layer, and ∂ denotes the activation functions.\nThe output layer can be computed as follows:\n Ot = soft max(Wohht + bsO) (16)\nNew_tree 1 0 1 0 1 0 1\nRandom finest_tree 0 1 1 1 0 1 0\nMask_Operator 1 0 1 1 0 0 0\nNew_Tree after masking 0 0 1 1 1 0 1\nTable 1. Mask operation method using an example.\n \nScientific Reports |          (2025) 15:565 7| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nOt denotes the output at time t, Woh is the weight matrix for hidden to-output connections, bsO is the bias \nvector at the output layer, and the softmax activation function is used to predict the output classes.\nThe loss function can computed as:\n \nLoss = − 1\nS\nS∑\ni=1\nT∑\nt=1\nlog(Oti\ni) (17)\nwhere D is the training samples, T denotes the length of the sequences, Oti\ni is the predicted probability \ndistribution for the ith sequence at a time ‘t’ .\nDetails of other classifiers, including Extremely Randomized Trees (ERT), Support Vector Machines (SVM), and \nRandom Forest (RF), are provided in the supplementary information.\nProposed model architecture\nThe architecture of our proposed model is shown in Figure 3. To address sequential dependencies within peptide \nsequences, we employed a Hybrid CNN +RNN model. This approach effectively leverages both structural \ncharacterization (via CNNs) and temporal information (via RNNs). This network type combines CNN layers for \nfeature extraction from input data with RNN layers for sequence prediction. The input layer feeds the extracted \nfeatures into the network. Initially, a 1D convolutional layer with 32 filters and a kernel size of 3, employing the \nReLU activation function, is applied. Subsequently, a max pooling layer with a pool size of 2 is added for feature \nconcatenation. A flattening layer is then used, followed by a dropout layer for regularization, before passing \nthe features to the RNN layer. Simultaneously, an RNN module captures sequential dependencies using two \nrecurrent layers with ReLU activations and dropout. Two RNN layers with 32 units each are designed, with a \ndropout layer (0.5) to prevent overfitting, and then a fully connected layer. Both the CNN and RNN models \nreceive the same input and produce separate outputs, which are concatenated using a concatenation layer. A \ndense layer with 1 unit and sigmoid activation combines the outputs of the 1DCNN and RNN models into \na single prediction. Finally, for binary classification, a standard sigmoid function is employed on the output \nlayer. Predictions below 0.5 indicate non-ACPs, while predictions above 0.5 indicate ACPs. The model was \ntrained for 40 epochs with a batch size of 32 and a validation split of 0.1. The validation dataset is used to \nmonitor convergence during training, allowing for early cancellation based on changes in convergence. During \noptimization, the Adam optimizer with a learning rate of 0.001 was used, along with categorical cross-entropy as \nthe loss function. Details of the hyperparameters used in our model are summarized in Table 2.\nEvaluation metrics\nIn deep learning, evaluating computational models’ efficacy requires appropriate performance metrics 75–77. \nFor binary classification models, a confusion matrix is typically employed, summarizing four key values: false \nnegatives (FN), true negatives (TN), false positives (FP), and true positives (TP) 78. While predictive accuracy \nis a commonly used performance metric, it is often insufficient for imbalanced datasets 9,58,78,79. To provide a \ncomprehensive evaluation of model performance, we utilized several additional metrics, including sensitivity , \nspecificity, Matthews Correlation Coefficient (MCC), and the Area Under the Curve (AUC)80. The formulas for \nthese metrics are as follows:\n Accuracy = TP + TN\nTP + TN + FP + FN  (18)\n Specificity = TN\nTN + FP  (19)\n Sensitivity = TP\nTP + FN  (20)\n \nMCC = (TP ∗ TN ) − (FP ∗ FN )√\n(TP + FP )( TP + FN )( TN + FP )( TN + FN ) (21)\nHere TP represents correctly identified positives, TN denotes correctly identified negatives, FP refers to false \npositives, and indicates false negatives. MCC values range from -1 to + 1.\nResults and discussion\nThis study evaluates the predictive performance of antimicrobial peptide (ACP) samples using tenfold cross-\nvalidation (CV) with individual classifiers and hybrid deep learning models. The mean CV results were calculated \nby repeating the stratified loop process 100 times 81,82, ensuring robust and reliable outcomes by randomly \ndistributing the training data across folds. Firstly, the peptide samples are formulated via ProtBert-BFD-\nbased transformer encoding and CTDT-based composite physiochemical properties. Furthermore, the multi-\nperspective vector is formed by fusing the extracted features of ProtBert-BFD, and CTDT. The computational \ncost of the multi-perspective feature vector is reduced by selecting the highly relevant feature set via BTG feature \nselection. All the extracted vectors (individual, hybrid, and selected) are evaluated via proposed training models. \nIn the below subsections, the detailed predictive results of the training sets and independent sets are discussed.\nScientific Reports |          (2025) 15:565 8| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nPredictive outcomes of the proposed classifiers using training datasets\nThe predictive outcomes of both training datasets (Training-I, Training-II), evaluated using various classification \nmodels, are presented in Table 3. In this study, the training samples from both datasets were encoded using the \nattention transformer-based ProtBERT-BFD. Additionally, to capture structure-based internal motif features \nof amino acids, the CTDT encoding was applied. Initially, the individual feature vectors (ProtBERT-BFD and \nCTDT) of the Training-I dataset were assessed using traditional machine learning models, such as Extremely \nRandomized Trees (ERT), Support Vector Machine (SVM), and Random Forest (RF). To further evaluate the \nextracted training features, different deep learning models, including RNN, CNN, and CNN + RNN, were \nemployed. For the CTDT feature set, the accuracies of traditional machine learning classifiers ranged from \n66.53% to 70.79%, with AUC values between 0.72 and 0.78. Deep learning models demonstrated improved \nperformance, with the CNN model achieving an accuracy of 81.83% and an AUC of 0.89, while the RNN model \nobtained an accuracy of 84.72% and an AUC of 0.92. To enhance predictive performance further, a CNN + RNN-\nbased hybrid training model was applied, achieving an accuracy of 88.69% and an AUC of 0.93 when using the \nCTDT feature vector. The ProtBERT-BFD features were similarly evaluated using the proposed classification \nmodels. Machine learning models achieved accuracies ranging from 71.59% to 75.87% and AUC values between \n0.77 and 0.83. The CNN + RNN model, utilizing the ProtBERT-BFD feature vector, reported an accuracy of \n90.37% and an AUC of 0.94. Furthermore, a hybrid feature vector combining attention-based ProtBERT-BFD \nfeatures and CTDT-based structural features was constructed. When evaluated using the CNN + RNN model, \nthe hybrid feature vector delivered significant results, reporting an accuracy of 93.61%, a specificity of 96.78%, \nand an AUC of 0.96. For the Training-II dataset, detailed predictive results for the encoding methods are also \nprovided in Table 3. Using the CNN + RNN model, the CTDT feature set achieved an accuracy of 91.78%, a \nsensitivity of 98.23%, and an AUC of 0.98. Similarly, the ProtBERT-BFD feature set attained a sensitivity of \nParameter Values\nNo; neurons in convolution layers 32,64,128\nNo; neurons in RNN layer 32,64,128\nActivation function (Convolution & RNN layer) ReLu\nDense layer Activation function sigmoid\nNumber of Epochs 40,60\nBatch size 32,64\nDropout rate 0.1, 0.2, 0.3, 0.4\nLearning rate 0.01,0.001\nRegularization 0.001\nOptimizer Adam\nTable 2. Optimal hyper-parameters of the proposed hybrid model.\n \nFig. 3. Architecture of pACP-HybDeep model.\n \nScientific Reports |          (2025) 15:565 9| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\n99.35%, an accuracy of 92.61%, and an AUC of 0.98. However, the hybrid vector, combining semantic and \nstructural features, did not exhibit significant improvement. It achieved a specificity of 96.77%, an accuracy of \n92.62%, and an AUC of 0.97 when evaluated using the CNN + RNN model.\nPredictive analysis of BTG feature selection using training datasets\nIn Table 4, the predictive outcomes of the hybrid encoding vectors after applying BTG-based feature selection \nfor the Training-I, and Training-II datasets are presented. The integration of ProtBERT-BFD and CTDT features \nenhances the predictive performance. The hybrid vector (ProtBERT-BFD + CTDT) comprises 1063 dimensions, \nwith CTDT contributing 39 features and ProtBERT-BFD contributing 1024 features. The BTG feature selection \nmethod was applied using the KNN training model to enable computationally efficient training, resulting in \nthe selection of 149 high-discriminative features from the 1063-dimensional hybrid vector. The optimal feature \nset for the Training-I dataset achieved accuracies ranging from 75.78% to 81.74% and AUC values between \n0.82 and 0.90. In comparison, the proposed CNN + RNN model attained an accuracy of 95.33%, a specificity \nof 97.08%, a sensitivity of 91.65%, and MCC and AUC values of 0.88 and 0.97, respectively. For the Training-II \ndataset, the CNN + RNN model achieved a sensitivity of 98.48%, an accuracy of 93.87%, a specificity of 92.21%, \nan AUC of 0.98, and an MCC of 0.92. The accuracy versus epoch analysis and loss versus epoch analysis of the \nTraining-I model are illustrated in Fig. 4(A) and 4(B), respectively. Figure 4(A) demonstrates consistently high \naccuracy with minimal fluctuation after several epochs. The similarity in accuracy during both the training \nand validation phases validates the stability and efficacy of the model for the Training-I dataset. This stability \nindicates the absence of overfitting and highlights the model’s generalization capability. Similarly, Fig. 4(B) shows \nthat the proposed algorithm optimizes both training and validation losses effectively, stabilizing at an optimal, \nlow loss within a few epochs. The absence of significant differences between the training and validation losses \ndemonstrates the model’s ability to learn from the training dataset and generalize effectively to the validation \ndataset, thereby ensuring robust predictive performance. The consistency of the proposed pACP-HybDeep \nmodel across both training datasets is demonstrated in Fig. 5. Figure 5(A) and 5(B) highlight that deep learning \nmodels significantly outperform classical machine learning models. The hybrid CNN + RNN model achieved \nthe highest AUC values of 0.97 and 0.98 for the Training-I, and Training-II datasets, respectively. Individually, \nthe RNN model achieved an AUC of 0.96, while the CNN model achieved AUC values of 0.95 and 0.96 for the \nTraining-I, and Training-II datasets, respectively. In contrast, classical machine learning models, including ET, \nSVM, and RF , demonstrated relatively lower accuracy and performance. These findings underscore that the \nproposed CNN + RNN hybrid model is the most effective approach for the classification task, outperforming \nother models across both datasets. \nVisualization of the extracted features\nTo visualize the effectiveness of the extracted features from the main training dataset in distinguishing the target \nclasses, we employed a t-SNE-based data visualization approach 83,84. The t-SNE algorithm transforms high-\ndimensional feature representations of the training vector into a two-dimensional space, as shown in Fig. 6. An \ninvestigation of the extracted features, including CTDT, ProtBERT-BFD, and the hybrid vector, reveals that the \ncluster shapes appear relatively constrained, offering limited discernibility of the sample distributions for ACP \nand non-ACP classes, as illustrated in Figs. 6(A-C). In the analysis of individual features, CTDT (Fig. 6(A)) and \nProtBERT-BFD (Fig. 6(B)), as well as the hybrid vector (Fig.  6(C)), exhibit overlapping distributions among \nsamples, which indicates limited potential for discriminating between the classes. In contrast, the incorporation \nof BTG-based discriminative features (Fig. 6(D)) not only reduces the computational complexity of the training \nmodel but also significantly improves class separation. This enhanced separation leads to higher predictive \naccuracy in distinguishing ACP from non-ACP samples.\nPredictive analysis of the independent datasets using proposed model\nThe predictive performance of the proposed model on independent datasets is detailed in Table 5. For the Ind-S1 \ndataset, the accuracies achieved by machine-learning models ranged from 79.12% to 85.92%, while individual \ndeep-learning models yielded accuracies between 89.36% and 90.20%. The CNN + RNN model outperformed \nthese approaches, achieving an accuracy of 94.92%. Using the Ind-S2 dataset, the accuracies of the machine \nlearning models ranged from 68.56% to 72.45%, and the deep learning models achieved accuracies between \n90.66% and 91.04%. Again, the CNN + RNN model demonstrated superior performance with an accuracy of \n92.26%. Similarly, for the Ind-S3 dataset, the proposed CNN + RNN model achieved an accuracy of 91.16%.\nTo further assess the stability of the proposed model, the area under the receiver operating characteristic curve \n(AUC) values were calculated. The CNN + RNN model reported AUC values of 0.96, 0.97, and 0.95 for the \nInd-S1, Ind-S2, and Ind-S3 datasets, respectively. These results indicate that the proposed CNN + RNN model \nconsistently outperformed other methods across all evaluation metrics, demonstrating its robustness and \neffectiveness when applied to independent datasets.\nPerformance analysis of pACP-HybDeep model with existing computational models\nTable 6 presents a comparative analysis of the proposed pACP-HybDeep training model against existing \ncomputational models, using both training and independent datasets. For the training-I dataset, the ACPred-\nBMF model, which leverages sequential features and an LSTM classifier, reported an accuracy of 80.81%, a \nsensitivity of 88.37%, a specificity of 73.26%, and an MCC of 0.62. Similarly, the ACP-MHCNN model, utilizing \nclassical sequential and evolutionary features with a multi-headed CNN mechanism, achieved an accuracy of \n73%, a sensitivity of 78.50%, a specificity of 67.40%, and an MCC of 0.46. In contrast, the pACP-HybDeep \nmodel outperformed these methods significantly, achieving an accuracy of 95.33%, a specificity of 97.08%, a \nsensitivity of 91.65%, and an MCC of 0.88. When evaluated on the training-II dataset, the pACP-HybDeep \nScientific Reports |          (2025) 15:565 10| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nDataset Encoding method Classifier Accuracy (%) Sensitivity (%) Specificity (%) MCC AUC\nTraining-I Hybrid Vector\nRNN 91.74 93.16 90.23 0.83 0.96\nCNN 91.20 92.16 89.97 0.82 0.95\nERT 78.45 78.55 78.35 0.56 0.88\nSVM 81.74 80.80 82.55 0.60 0.90\nRF 75.78 68.98 82.57 0.52 0.82\nCNN + RNN 95.33 91.65 97.08 0.88 0.97\nTraining-II Hybrid Vector\nRNN 92.62 91.65 96.65 0.89 0.96\nCNN 92.48 90.42 97.50 0.89 0.96\nERT 91.04 89.07 93.40 0.82 0.96\nSVM 87.61 85.62 93.60 0.81 0.90\nRF 90.20 89.17 91.23 0.80 0.95\nCNN + RNN 93.87 98.48 92.21 0.92 0.98\nTable 4. Predictive performance of the hybrid training vector using BTG feature selection.\n \nDataset Encoding method Classifier Accuracy (%) Sensitivity (%) Specificity (%) MCC AUC\nTraining-I\nProtBERT-BFD\nRNN 89.37 86.54 94.10 0.80 0.95\nCNN 87.69 86.13 89.06 0.75 0.94\nERT 71.87 63.48 80.25 0.44 0.79\nSVM 75.87 76.07 75.60 0.51 0.83\nRF 71.59 63.06 80.13 0.43 0.77\nCNN + RNN 90.37 92.26 88.31 0.80 0.94\nCTDT\nRNN 84.72 82.03 86.45 0.69 0.92\nCNN 81.83 83.90 79.57 0.62 0.89\nERT 70.79 70.96 70.61 0.41 0.78\nSVM 66.53 54.41 78.62 0.34 0.72\nRF 69.72 62.20 77.23 0.39 0.75\nCNN + RNN 88.69 90.07 87.64 0.77 0.93\nHybrid Vector\nRNN 90.63 89.94 90.58 0.81 0.95\nCNN 90.10 91.16 88.40 0.80 0.95\nERT 75.77 69.10 82.46 0.52 0.83\nSVM 77.46 75.72 79.21 0.54 0.85\nRF 74.97 67.59 82.34 0.50 0.82\nCNN + RNN 93.61 90.46 96.78 0.87 0.96\nTraining-II\nProtBERT-BFD\nRNN 90.37 92.26 88.31 0.80 0.94\nCNN 87.69 86.13 89.06 0.75 0.94\nERT 71.87 63.48 80.25 0.44 0.79\nSVM 75.87 76.07 75.60 0.51 0.83\nRF 71.59 63.06 80.13 0.43 0.77\nCNN + RNN 92.61 99.35 91.83 0.91 0.98\nCTDT\nRNN 90.65 99.22 89.97 0.89 0.97\nCNN 86.90 85.86 87.93 0.73 0.93\nERT 84.70 81.87 86.94 0.69 0.91\nSVM 83.70 83.38 84.02 0.67 0.90\nRF 84.37 81.15 87.73 0.69 0.91\nCNN + RNN 91.78 98.23 90.26 0.89 0.98\nHybrid Vector\nRNN 93.47 93.54 93.37 0.86 0.96\nCNN 92.60 88.58 96.60 0.85 0.95\nERT 86.95 82.57 91.34 0.74 0.93\nSVM 90.0 88.96 91.03 0.80 0.95\nRF 88.55 85.05 92.06 0.77 0.95\nCNN + RNN 92.62 92.46 96.77 0.89 0.97\nTable 3. Predictive performance of the classifiers using training datasets.\n \nScientific Reports |          (2025) 15:565 11| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nmodel demonstrated a ~ 3% improvement in accuracy and a 6% and 5% increase in sensitivity and MCC, \nrespectively, compared to the training-I dataset. To evaluate the generalization capability of the proposed model, \nits performance was tested on independent datasets. Using the Ind-S1 dataset, the pACP-HybDeep model \ndemonstrated superior predictive performance with improvements of 4% in accuracy, 7% in sensitivity, and 9% \nin MCC compared to existing models. Similarly, the pACP-HybDeep model also outperformed other methods \nwhen applied to the Ind-S2 and Ind-S3 independent datasets, as detailed in Table 6. These results underscore \nthe effectiveness and robustness of the pACP-HybDeep model in achieving higher predictive performance \ncompared to existing computational models across both training and independent datasets.\nDiscussion\nAnticancer peptides (ACPs) have emerged as promising therapeutic agents for cancer treatment due to their \nhigh selectivity for target cells and minimal side effects on healthy tissues. Numerous laboratory-based in vitro \nstudies and machine learning-based computational models have been proposed in the literature to facilitate ACP \nprediction. However, these existing models face several challenges, including inadequate encoding methods, \nlimited training efficacy, poor interpretability, high computational costs, and insufficient generalization power. \nTo address these limitations, this study introduces the pACP-HybDeep model, which incorporates an attention \nmechanism using the ProtBERT-BFD approach to extract contextual and semantic information from peptide \nsequences. Additionally, to retain structural information from the input sequences, CTDT-based cluster \nphysicochemical properties are employed. To overcome the limitations of individual feature representations, a \nFig. 5. ROC of pACP-HybDeep Model using (A) Training-I Dataset and (B) Training-II Dataset.\n \nFig. 4. ( A) Accuracy vs Epochs (B) Loss vs Epochs graphs on Training-I Dataset.\n \nScientific Reports |          (2025) 15:565 12| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nmulti-perspective feature vector is constructed by serially integrating the CTDT and ProtBERT-BFD features. \nTo reduce computational complexity, the KNN-based Binary Tree Growth (BTG) method is applied, selecting \n149 optimal features from the integrated hybrid vector of 1063 dimensions. Among the tested classifiers, the \nproposed CNN+RNN-based deep hybrid model achieved the highest predictive accuracies, with 95.33% and \n93.87% using the training-I, and training-II datasets, respectively. The superior performance of the pACP-\nHybDeep model can be attributed to its effective feature encoding strategies, innovative feature selection \nmethodology, and robust training capabilities of the deep hybrid model. By combining ProtBERT-BFD and \nCTDT methods, the model captures both the semantic information derived from the large BFD database and the \nstructural representation of peptide sequences, addressing the challenge of missing sequence order information. \nFinally, compared to traditional classifiers, the proposed deep hybrid model demonstrates significantly improved \npredictive performance and enhanced generalization capabilities, making it a robust tool for anticancer peptide \nprediction.\nConclusion\nIn this study, we introduced an efficient and reliable hybrid model, pACP-HybDeep, for predicting ACPs \nand non-ACPs. Existing feature encoding methods face several challenges, including the lengthy process of \ngenerating evolutionary features from large databases and the absence of sequence-ordering information in \nsequential features. To address these limitations, we employed an attention-based transformer, ProtBERT-\nBFD, to capture contextual and semantic information and CTDT-based structural physicochemical properties \nto encode essential structural features. To further optimize the model and reduce training costs, we applied \nBinary Tree Growth (BTG)-based feature selection to identify an informative subset of features. The predictive \nperformance of the optimal feature set was evaluated using various training models. The integration of a \ndeep hybrid CNN +RNN model significantly enhanced prediction accuracy with the selected feature set. Our \nproposed pACP-HybDeep model achieved outstanding accuracies of 95.33% and 93.87% using the training-I \nand training-II datasets, respectively. To ensure the stability and generalizability of the model, it was validated \non independent datasets, achieving accuracies of 94.92% (Ind-S1), 92.26% (Ind-S2), and 91.16% (Ind-S3). These \nFig. 6. Training Dataset t-SNE Visualization using (A) CTDT (B) ProtBERT-BFD (C) hybrid features (D) \nBTG-based selected Features\n \nScientific Reports |          (2025) 15:565 13| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nresults demonstrate the superior predictive performance of the pACP-HybDeep model compared to existing \ncomputational models for ACP prediction. Thus, we anticipate that the pACP-HybDeep model will become a \nwidely adopted tool in drug development and academic research, contributing to the advancement of peptide-\nbased therapeutics.\nData availability\nThe data used in this work are publically available at https://github.com/shahidawkum/pACP-HybDeep.\nReceived: 12 August 2024; Accepted: 20 December 2024\nDataset Classifier Accuracy (%) Sensitivity (%) Specificity (%) MCC\nTraining-I\nACPred-BMF30 80.81 88.37 73.26 0.62\nACP-MHCNN85 73.00 78.50 67.40 0.46\npACP-HybDeep 95.33 91.65 97.08 0.88\nTraining-II\nACPred-BMF30 93.56 92.27 94.85 0.87\nACP-MHCNN85 90.00 86.60 86.60 0.81\npACP-HybDeep 93.87 98.48 92.21 0.92\nInd-S1\nACP-MHCNN85 86.0 88.9 83.1 0.72\niACP-GE18 90.54 88.63 92.55 0.81\nACP-BC27 87 87 88 0.75\nACP-Check28 87 86 88 0.75\npACP-HybDeep 94.92 96.53 93.21 0.90\nInd-S2\nACP-MHCNN85 83.0 90.1 75.6 0.67\niACP-GE18 91.25 92.83 89.09 0.82\nACP-BC27 90 90 89 0.78\nACP-Check28 90 94 86 0.80\npACP-HybDeep 92.26 97.39 87.71 0.86\nInd-S3\nACPred-FL86 85.3 69.5 85.8 0.25\nACP-MCAM87 90.9 85.4 96.3 0.82\npACP-HybDeep 91.16 91.21 90.69 0.82\nTable 6. Comparison of the proposed model with available state-of-the-art methods.\n \nDataset Encoding method Classifier Accuracy (%) Sensitivity (%) Specificity (%) MCC AUC\nInd-S1 Hybrid Vector + \nBTG\nRNN 90.02 91.32 88.37 0.80 0.95\nCNN 89.36 88.72 89.77 0.78 0.95\nERT 79.12 76.86 81.38 0.58 0.87\nSVM 85.69 85.63 85.90 0.71 0.92\nRF 82.29 85.10 79.39 0.64 0.89\nCNN + RNN 94.92 96.53 93.21 0.90 0.96\nInd-S2\nHybrid Vector\n + \nBTG\nRNN 91.04 92.38 89.56 0.82 0.95\nCNN 90.66 92.00 88.87 0.81 0.95\nERT 68.56 61.24 75.96 0.37 0.79\nSVM 72.45 64.34 80.52 0.45 0.83\nRF 68.60 67.44 69.76 0.37 0.72\nCNN + RNN 92.26 97.39 87.71 0.86 0.97\nInd-S3 Hybrid Vector + \nBTG\nRNN 88.63 86.57 91.62 0.78 0.94\nCNN 87.89 84.75 91.33 0.76 0.91\nERT 71.73 85.36 58.02 0.45 0.79\nSVM 74.49 73.17 75.60 0.48 0.81\nRF 70.78 80.48 60.97 0.42 0.75\nCNN + RNN 91.16 91.21 90.69 0.82 0.95\nTable 5. Predictive performance of the proposed classifiers via independent sets.\n \nScientific Reports |          (2025) 15:565 14| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nReferences\n 1. Lee, P . Y ., Low, T. Y . & Jamal, R. in Advances in Clinical Chemistry Vol. 88 (ed Gregory S. Makowski) 67–89 (Elsevier, 2019).\n 2. Chhikara, B. S. & Parang, K. Global Cancer Statistics 2022: the trends projection analysis. Chem. Bio. Lett. 10, 451–451 (2023).\n 3. Siegel, R. L., Miller, K. D., Wagle, N. S. & Jemal, A. Cancer statistics, 2023. Ca Cancer J. Clin. 73, 17–48 (2023).\n 4. Kuroda, K., Okumura, K., Isogai, H. & Isogai, E. The human cathelicidin antimicrobial peptide LL-37 and mimics are potential \nanticancer drugs. Front. Oncol. 5, 144 (2015).\n 5. Al-Khdhairawi, A. et al. Machine learning and molecular simulation ascertain antimicrobial peptide against klebsiella pneumoniae \nfrom public database. Comput. Bio. Chem. 102, 107800. https://doi.org/10.1016/j.compbiolchem.2022.107800 (2023).\n 6. Tyagi, A. et al. In silico models for designing and discovering novel anticancer peptides. Sci. Rep. 3, 1–8 (2013).\n 7. Hajisharifi, Z., Piryaiee, M., Beigi, M. M., Behbahani, M. & Mohabatkar, H. Predicting anticancer peptides with Chou ′ s pseudo \namino acid composition and investigating their mutagenicity via Ames test. J. Theor. Bio. 341, 34–40 (2014).\n 8. Chen, W ., Ding, H., Feng, P ., Lin, H. & Chou, K.-C. iACP: a sequence-based tool for identifying anticancer peptides. Oncotarget 7, \n16895 (2016).\n 9. Akbar, S., Hayat, M., Iqbal, M. & Jan, M. A. iACP-GAEnsC: Evolutionary genetic algorithm based ensemble classification of \nanticancer peptides by utilizing hybrid feature space. Artif. Intell. Med. 79, 62–70 (2017).\n 10. Manavalan, B. et al. MLACP: machine-learning-based prediction of anticancer peptides. Oncotarget 8, 77121 (2017).\n 11. Xu, L., Liang, G., Wang, L. & Liao, C. A novel hybrid sequence-based model for identifying anticancer peptides. Genes 9, 158 \n(2018).\n 12. Kabir, M. et al. Intelligent computational method for discrimination of anticancer peptides by incorporating sequential and \nevolutionary profiles information. Chemom. Intell. Lab. Syst. 182, 158–165 (2018).\n 13. Schaduangrat, N., Nantasenamat, C., Prachayasittikul, V . & Shoombuatong, W . ACPred: a computational tool for the prediction \nand analysis of anticancer peptides. Molecules 24, 1973 (2019).\n 14. Wei, L., Zhou, C., Chen, H., Song, J. & Su, R. ACPred-FL: a sequence-based predictor using effective feature representation to \nimprove the prediction of anti-cancer peptides. Bioinformatics 34, 4007–4016 (2018).\n 15. Charoenkwan, P . et al. Improved prediction and characterization of anticancer activities of peptides using a novel flexible scoring \ncard method. Sci. Rep. 11, 3017 (2021).\n 16. Agrawal, P ., Bhagat, D., Mahalwal, M., Sharma, N. & Raghava, G. P . AntiCP 2.0: an updated model for predicting anticancer \npeptides. Brief. Bioinform. 22(3), 153 (2021).\n 17. Akbar, S., Hayat, M., Tahir, M. & Chong, K. T. cACP-2LFS: classification of anticancer peptides using sequential discriminative \nmodel of KSAAP and two-level feature selection approach. IEEE Access 8, 131939–131948 (2020).\n 18. Liang, Y . & Ma, X. iACP-GE: accurate identification of anticancer peptides by using gradient boosting decision tree and extra tree. \nSAR QSAR Environ. Res. 34, 1–19 (2023).\n 19. Ma, J., Sheridan, R. P ., Liaw, A., Dahl, G. E. & Svetnik, V . Deep neural nets as a method for quantitative structure–activity \nrelationships. J. Chem. Inf. Model. 55, 263–274 (2015).\n 20. Vazhayil, A. & KP , S. DeepProteomics: protein family classification using Shallow and Deep Networks. arXiv preprint  \narXiv:1809.04461 (2018).\n 21. Sureyya Rifaioglu, A., Doğan, T., Jesus Martin, M., Cetin-Atalay, R. & Atalay, V . DEEPred: automated protein function prediction \nwith multi-task feed-forward deep neural networks. Sci. Rep. 9, 7344 (2019).\n 22. He, W ., Wang, Y ., Cui, L., Su, R. & Wei, L. Learning embedding features based on multisense-scaled attention architecture to \nimprove the predictive performance of anticancer peptides. Bioinformatics 37, 4684–4693 (2021).\n 23. Yi, H.-C. et al. ACP-DL: a deep learning long short-term memory model to predict anticancer peptides using high-efficiency \nfeature representation. Mol. Therapy-Nucleic Acids 17, 1–9 (2019).\n 24. Wu, C., Gao, R., Zhang, Y . & De Marinis, Y . PTPD: predicting therapeutic peptides by deep learning and word2vec. BMC Bioinform. \n20, 1–8 (2019).\n 25. Akbar, S., Hayat, M., Tahir, M., Khan, S. & Alarfaj, F . K. cACP-DeepGram: classification of anticancer peptides via deep neural \nnetwork and skip-gram-based word embedding model. Artif. Intell. Med. 131, 102349 (2022).\n 26. Ahmed, S. et al. ACP-MHCNN: an accurate multi-headed deep-convolutional neural network to predict anticancer peptides. Sci. \nRep. 11, 1–15 (2021).\n 27. Sun, M., Hu, H., Pang, W . & Zhou, Y . ACP-BC: A model for accurate identification of anticancer peptides based on fusion features \nof bidirectional long Short-Term memory and chemically derived information. Int. J. Mol. Sci. 24, 15447 (2023).\n 28. Zhu, L., Y e, C., Hu, X., Y ang, S. & Zhu, C. ACP-check: An anticancer peptide prediction model based on bidirectional long short-\nterm memory and multi-features fusion strategy. Comput. Biol. Med. 148, 105868 (2022).\n 29. Wu, X., Zeng, W ., Lin, F ., Xu, P . & Li, X. Anticancer peptide prediction via multi-kernel CNN and attention model. Front. Genet. \n13, 887894 (2022).\n 30. Han, B., Zhao, N., Zeng, C., Mu, Z. & Gong, X. ACPred-BMF: bidirectional LSTM with multiple feature representations for \nexplainable anticancer peptide prediction. Sci. Rep. 12, 21915 (2022).\n 31. Azim, S. M. et al. Accurately predicting anticancer peptide using an ensemble of heterogeneously trained classifiers. Inf. Med. \nUnlocked 42, 101348. https://doi.org/10.1016/j.imu.2023.101348 (2023).\n 32. Bian, J. et al. ACP-ML: A sequence-based method for anticancer peptide prediction. Comput. Biol. Med. 170, 108063.  h t t p s : / / d o i . o \nr g / 1 0 . 1 0 1 6 / j . c o m p b i o m e d . 2 0 2 4 . 1 0 8 0 6 3     (2024).\n 33. Karim, T., Shaon, M. S. H., Sultan, M. F ., Hasan, M. Z. & Kafy, A. A. ANNprob-ACPs: A novel anticancer peptide identifier based \non probabilistic feature fusion approach. Comput. Biol. Med.  169, 107915. https://doi.org/10.1016/j.compbiomed.2023.107915 \n(2024).\n 34. Liang, X., Zhao, H. & Wang, J. MA-PEP: A novel anticancer peptide prediction framework with multimodal feature fusion based \non attention mechanism. Prot. Sci. 33, e4966 (2024).\n 35. Khan, S. et al. A Two-Level computation model based on deep learning algorithm for identification of piRNA and their functions \nvia chou’s 5-steps rule. Int. J. Pept. Res. Therapeutics 26, 795–809. https://doi.org/10.1007/s10989-019-09887-3 (2020).\n 36. Khan, Z. U., Ali, F ., Ahmad, I., Hayat, M. & Pi, D. iPredCNC: computational prediction model for cancerlectins and non-\ncancerlectins using novel cascade features subset selection. Chem. Int. Lab. Syst. 195, 103876 (2019).\n 37. Noor, S. et al. Deep-m5U: a deep learning-based approach for RNA 5-methyluridine modification prediction using optimized \nfeature integration. BMC Bioinform. 25, 360. https://doi.org/10.1186/s12859-024-05978-1 (2024).\n 38. Tahir, M., Hayat, M. & Chong, K. T. Prediction of N6-methyladenosine sites using convolution neural network model based on \ndistributed feature representations. Neural Netw. 129, 385–391 (2020).\n 39. Khan, S., AlQahtani, S. A., Noor, S. & Ahmad, N. PSSM-Sumo: deep learning based intelligent model for prediction of sumoylation \nsites using discriminative features. BMC Bioinform. 25, 284. https://doi.org/10.1186/s12859-024-05917-0 (2024).\n 40. Ullah, M., Akbar, S., Raza, A. & Zou, Q. DeepAVP-TPPred: identification of antiviral peptides using transformed image-based \nlocalized descriptors and binary tree growth algorithm. Bioinformatics https://doi.org/10.1093/bioinformatics/btae305 (2024).\n 41. Vijayakumar, S. & Ptv, L. ACPP: A web server for prediction and design of anti-cancer peptides. Int. J. Peptide Res. Ther. 21, 99–106. \nhttps://doi.org/10.1007/s10989-014-9435-7 (2015).\n 42. Nagpal, G., Chaudhary, K., Agrawal, P . & Raghava, G. P . S. Computer-aided prediction of antigen presenting cell modulators for \ndesigning peptide-based vaccine adjuvants. J. Transl. Med. 16, 181. https://doi.org/10.1186/s12967-018-1560-1 (2018).\nScientific Reports |          (2025) 15:565 15| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\n 43. Akbar, S. et al. iAtbP-Hyb-EnC: Prediction of antitubercular peptides via heterogeneous feature representation and genetic \nalgorithm based ensemble learning model. Comput. Biol. Med. 137, 104778. https://doi.org/10.1016/j.compbiomed.2021.104778 \n(2021).\n 44. Barukab, O., Ali, F . & Khan, S. A. DBP-GAPred: an intelligent method for prediction of DNA-binding proteins types by enhanced \nevolutionary profile features with ensemble learning. J. Bioinform. Comput. Biol. 19, 2150018 (2021).\n 45. Raza, A. et al. Comprehensive analysis of computational methods for predicting anti-inflammatory peptides. Arch. Comput. \nMethods Eng. 31, 3211–3229 (2024).\n 46. Bibi, N. et al. Sequence-Based intelligent model for identification of tumor t cell antigens using fusion features. IEEE Access 12, \n155040–155051. https://doi.org/10.1109/ACCESS.2024.3481244 (2024).\n 47. An, J. & Weng, X. Collectively encoding protein properties enriches protein language models. BMC Bioinform. 23, 1–18 (2022).\n 48. Lin, K., Quan, X., Jin, C., Shi, Z. & Y ang, J. An interpretable double-scale attention model for enzyme protein class prediction based \non transformer encoders and multi-scale convolutions. Front. Genet. 13, 885627 (2022).\n 49. Brandes, N., Ofer, D., Peleg, Y ., Rappoport, N. & Linial, M. ProteinBERT: a universal deep-learning model of protein sequence and \nfunction. Bioinformatics 38, 2102–2110 (2022).\n 50. Vig, J. et al. Bertology meets biology: Interpreting attention in protein language models. arXiv preprint  arXiv:2006.15222 (2020).\n 51. Raza, A. et al. AIPs-SnTCN: Predicting anti-inflammatory peptides using fastText and transformer encoder-based hybrid word \nembedding with self-normalized temporal convolutional networks. J. Chem. Inf. Model. 63, 6537–6554.  h t t p s : / / d o i . o r g / 1 0 . 1 0 2 1 / a c \ns . j c i m . 3 c 0 1 5 6 3     (2023).\n 52. Akbar, S., Zou, Q., Raza, A. & Alarfaj, F . K. iAFPs-Mv-BiTCN: Predicting antifungal peptides using self-attention transformer \nembedding and transform evolutionary based multi-view features with bidirectional temporal convolutional networks. J. Med. \nArtif. Intell. 151, 102860. https://doi.org/10.1016/j.artmed.2024.102860 (2024).\n 53. Pei, H. et al. Identification of thermophilic proteins based on sequence-based bidirectional representations from transformer-\nembedding features. Appl. Sci. 13, 2858 (2023).\n 54. Dubchak, I., Muchnik, I., Holbrook, S. R. & Kim, S.-H. Prediction of protein folding class using global description of amino acid \nsequence. Proc. Natl. Acad. Sci. 92, 8700–8704 (1995).\n 55. Govindan, G. & Nair, A. S. in 2011 Annual IEEE India Conference. 1–6 (Ieee).\n 56. Akbar, S., Raza, A. & Zou, Q. Deepstacked-AVPs: predicting antiviral peptides using tri-segment evolutionary profile and word \nembedding based multi-perspective features with deep stacking model. BMC Bioinform. 25, 102.  h t t p s : / / d o i . o r g / 1 0 . 1 1 8 6 / s 1 2 8 5 9 - 0 \n2 4 - 0 5 7 2 6 - 5     (2024).\n 57. Rukh, G., Akbar, S., Rehman, G., Alarfaj, F . K. & Zou, Q. StackedEnC-AOP: prediction of antioxidant proteins using transform \nevolutionary and sequential features based multi-scale vector with stacked ensemble learning. BMC Bioinform. 25, 256.  h t t p s : / / d o \ni . o r g / 1 0 . 1 1 8 6 / s 1 2 8 5 9 - 0 2 4 - 0 5 8 8 4 - 6     (2024).\n 58. Ali, F ., Ahmed, S., Swati, Z. N. K. & Akbar, S. DP-BINDER: machine learning model for prediction of DNA-binding proteins by \nfusing evolutionary and physicochemical information. J. Comput. Aided Mol. Des. 33, 645–658 (2019).\n 59. Sikander, R., Ghulam, A. & Ali, F . XGB-DrugPred: computational prediction of druggable proteins using eXtreme gradient \nboosting and optimized features set. Sci. Rep. 12, 5505. https://doi.org/10.1038/s41598-022-09484-3 (2022).\n 60. Akbar, S. et al. Prediction of amyloid proteins using embedded evolutionary & ensemble feature selection based descriptors with \nextreme gradient boosting model. IEEE Access https://doi.org/10.1109/ACCESS.2023.3268523 (2023).\n 61. Akbar, S., Rahman, A. U., Hayat, M. & Sohail, M. cACP: Classifying anticancer peptides using discriminative intelligent model via \nChou’s 5-step rules and general pseudo components. Chem. Intell. Lab. Syst. 196, 103912.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . c h e m o l a b . 2 0 1 9 \n. 1 0 3 9 1 2     (2020).\n 62. Sheikhpour, R., Sarram, M. A. & Gharaghani, S. Constraint score for semi-supervised feature selection in ligand-and receptor-\nbased QSAR on serine/threonine-protein kinase PLK3 inhibitors. Chemom. Intell. Lab. Syst. 163, 31–40.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . \nc h e m o l a b . 2 0 1 7 . 0 2 . 0 0 6     (2017).\n 63. Too, J., Abdullah, A. R., Mohd Saad, N. & Mohd Ali, N. Feature selection based on binary tree growth algorithm for the classification \nof myoelectric signals. Machines 6, 65 (2018).\n 64. Kumar, S., Nath, V . K. & Hazarika, D. Blend of deep features and binary tree growth algorithm for skin lesion classification. \nSymmetry 15, 2213 (2023).\n 65. Cheraghalipour, A., Hajiaghaei-Keshteli, M. & Paydar, M. M. Tree growth algorithm (TGA): A novel approach for solving \noptimization problems. Eng. Appl. Artif. Intell. 72, 393–414. https://doi.org/10.1016/j.engappai.2018.04.021 (2018).\n 66. Ahmad, A. et al. Deep-AntiFP: Prediction of antifungal peptides using distanct multi-informative features incorporating with deep \nneural networks. Chemom. Intell. Lab. Syst. 208, 104214 (2021).\n 67. Hamed, S. K., Ab Aziz, M. J. & Y aakub, M. R. A review of fake news detection approaches: A critical analysis of relevant studies and \nhighlighting key challenges associated with the dataset, feature representation, and data fusion. Heliyon 9, e20382.  h t t p s : / / d o i . o r g / \n1 0 . 1 0 1 6 / j . h e l i y o n . 2 0 2 3 . e 2 0 3 8 2     (2023).\n 68. Akbar, S., Ullah, M., Raza, A., Zou, Q. & Alghamdi, W . DeepAIPs-Pred: predicting anti-inflammatory peptides using local \nevolutionary transformation images and structural embedding-based optimal descriptors with Self-Normalized BiTCNs. J Chem. \nInf. Model. https://doi.org/10.1021/acs.jcim.4c01758 (2024).\n 69. Shujaat, M., Wahab, A., Tayara, H. & Chong, K. T. pcPromoter-CNN: a CNN-based prediction and classification of promoters. \nGenes 11, 1529 (2020).\n 70. Tahir, M., Tayara, H., Hayat, M. & Chong, K. T. kDeepBind: prediction of RNA-Proteins binding sites using convolution neural \nnetwork and k-gram features. Chemom. Intell. Lab. Syst. 208, 104217 (2021).\n 71. Raza, A. et al. AIPs-DeepEnC-GA: Predicting anti-inflammatory peptides using embedded evolutionary and sequential feature \nintegration with genetic algorithm based deep ensemble model. Chemom. Intell. Lab. Syst. 254, 105239.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . c \nh e m o l a b . 2 0 2 4 . 1 0 5 2 3 9     (2024).\n 72. Koo, P . K. & Eddy, S. R. Representation learning of genomic sequence motifs with convolutional neural networks. PLoS Comput. \nBiol. 15, e1007560 (2019).\n 73. Cohen, J. D., Servan-Schreiber, D. & McClelland, J. L. A parallel distributed processing approach to automaticity. Am. J. Psychol. \n105, 239–269 (1992).\n 74. Elman, J. Finding Structure in Time. Cognitive Science. 14, 179–211 (1990).\n 75. Dwivedi, A. K. Performance evaluation of different machine learning techniques for prediction of heart disease. Neural Comput. \nAppl. 29, 685–693 (2018).\n 76. Baratloo, A., Hosseini, M., Negida, A. & El Ashal, G. Part 1: simple definition and calculation of accuracy, sensitivity and specificity. \n(2015).\n 77. Ali, F . et al. AFP-CMBPred: Computational identification of antifreeze proteins by extending consensus sequences into multi-\nblocks evolutionary information. Comput. Biol. Med. 139, 105006 (2021).\n 78. Akbar, S. et al. Identifying Neuropeptides via Evolutionary and Sequential based Multi-perspective Descriptors by Incorporation \nwith Ensemble Classification Strategy. IEEE Access https://doi.org/10.1109/ACCESS.2023.3274601 (2023).\n 79. Akbar, S., Hayat, M., Iqbal, M. & Tahir, M. iRNA-PseTNC: identification of RNA 5-methylcytosine sites using hybrid vector space \nof pseudo nucleotide composition. Front. Comput. Sci. 14, 451–460 (2020).\n 80. Akbar, S. et al. pAtbP-EnC: Identifying anti-tubercular peptides using multi-feature representation and genetic algorithm-based \ndeep ensemble model. IEEE Access 11, 137099–137114. https://doi.org/10.1109/access.2023.3321100 (2023).\nScientific Reports |          (2025) 15:565 16| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\n 81. Akbar, S. & Hayat, M. iMethyl-STTNC: Identification of N6-methyladenosine sites by extending the idea of SAAC into Chou’s \nPseAAC to formulate RNA sequences. J. Theor. Biol. 455, 205–211. https://doi.org/10.1016/j.jtbi.2018.07.018 (2018).\n 82. Ahmad, A., Akbar, S., Tahir, M., Hayat, M. & Ali, F . iAFPs-EnC-GA: Identifying antifungal peptides using sequential and \nevolutionary descriptors based multi-information fusion and ensemble learning approach. Chemom. Intell. Lab. Syst. 222, 104516. \nhttps://doi.org/10.1016/j.chemolab.2022.104516 (2022).\n 83. Yu, S., Liao, B., Zhu, W ., Peng, D. & Wu, F . Accurate prediction and key protein sequence feature identification of cyclins. Brief. \nFunct. Genom. https://doi.org/10.1093/bfgp/elad014 (2023).\n 84. Uddin, I. et al. A hybrid residue based sequential encoding mechanism with XGBoost improved ensemble model for identifying \n5-hydroxymethylcytosine modifications. Sci. Rep. 14, 20819 (2024).\n 85. Ahmed, S. et al. ACP-MHCNN: an accurate multi-headed deep-convolutional neural network to predict anticancer peptides. Sci. \nRep. 11, 23676 (2021).\n 86. Leyi, W . C. Z., Huangrong, C., Jiangning, S. & Ran, S. ACPred-FL: a sequence-based predictor using effective feature representation \nto improve the prediction of anti-cancer peptides. Bioinformatics 34, 4007–4016. https://doi.org/10.1093/bioinformatics/bty451 \n(2018).\n 87. Wu, X., W . Z., Lin, F ., Xu, P ., Li, X. Anticancer Peptide Prediction via Multi-Kernel CNN and Attention Model. (2022).\nAcknowledgements\nThe work was supported by a university research grant PP-IVI-2024.\nAuthor contributions\nAll authors contributed equally.\nFunding\nThis research article was supported by Universiti Kebangsaan Malaysia with the Code ZG-2024-009.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 4 - 8 4 1 4 6 - 0     .  \nCorrespondence and requests for materials should be addressed to M.H., S.A. or R.A.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2024 \nScientific Reports |          (2025) 15:565 17| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/\nScientific Reports |          (2025) 15:565 18| https://doi.org/10.1038/s41598-024-84146-0\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6556706428527832
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6012915372848511
    },
    {
      "name": "Machine learning",
      "score": 0.5352815985679626
    },
    {
      "name": "Encoder",
      "score": 0.5019574165344238
    },
    {
      "name": "Deep learning",
      "score": 0.45745861530303955
    },
    {
      "name": "Support vector machine",
      "score": 0.45615166425704956
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4266752302646637
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}