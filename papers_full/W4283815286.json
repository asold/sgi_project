{
  "title": "RETRACTED: Pan-Sharpening with Customized Transformer and Invertible Neural Network",
  "url": "https://openalex.org/W4283815286",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097992355",
      "name": "Man Zhou",
      "affiliations": [
        "University of Science and Technology of China",
        "Hefei Institutes of Physical Science",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2104755790",
      "name": "Jie Huang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2688705721",
      "name": "Yanchi Fang",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2120091478",
      "name": "Xueyang Fu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2120139213",
      "name": "Aiping Liu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2097992355",
      "name": "Man Zhou",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Hefei Institutes of Physical Science",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2104755790",
      "name": "Jie Huang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2688705721",
      "name": "Yanchi Fang",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2120091478",
      "name": "Xueyang Fu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2120139213",
      "name": "Aiping Liu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1943127271",
    "https://openalex.org/W2171108951",
    "https://openalex.org/W2100329651",
    "https://openalex.org/W3165488702",
    "https://openalex.org/W3081397212",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2064366277",
    "https://openalex.org/W1583912456",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2172185514",
    "https://openalex.org/W3107036272",
    "https://openalex.org/W2772149471",
    "https://openalex.org/W3043042355",
    "https://openalex.org/W817971873",
    "https://openalex.org/W1985866156",
    "https://openalex.org/W6794906783",
    "https://openalex.org/W2296385512",
    "https://openalex.org/W3115223653",
    "https://openalex.org/W3153623182",
    "https://openalex.org/W3182074013",
    "https://openalex.org/W6718563727",
    "https://openalex.org/W3138005505",
    "https://openalex.org/W3165848914",
    "https://openalex.org/W2144436897",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3194157367",
    "https://openalex.org/W6798172501",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W3088339669",
    "https://openalex.org/W6791601067",
    "https://openalex.org/W3033492948",
    "https://openalex.org/W6746722832",
    "https://openalex.org/W1990231296",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W2777442120",
    "https://openalex.org/W3151922143",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3176970595",
    "https://openalex.org/W2017121865",
    "https://openalex.org/W3173217100",
    "https://openalex.org/W4287572671",
    "https://openalex.org/W4297798428",
    "https://openalex.org/W3180251767",
    "https://openalex.org/W3176096490",
    "https://openalex.org/W2001800591",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2462592242",
    "https://openalex.org/W3172472472",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W2963183385",
    "https://openalex.org/W2293167795",
    "https://openalex.org/W2777033955",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3173794271",
    "https://openalex.org/W3035022492"
  ],
  "abstract": "In remote sensing imaging systems, pan-sharpening is an important technique to obtain high-resolution multispectral images from a high-resolution panchromatic image and its corresponding low-resolution multispectral image. Owing to the powerful learning capability of convolution neural network (CNN), CNN-based methods have dominated this field. However, due to the limitation of the convolution operator, long-range spatial features are often not accurately obtained, thus limiting the overall performance. To this end, we propose a novel and effective method by exploiting a customized transformer architecture and information-lossless invertible neural module for long-range dependencies modeling and effective feature fusion in this paper. Specifically, the customized transformer formulates the PAN and MS features as queries and keys to encourage joint feature learning across two modalities while the designed invertible neural module enables effective feature fusion to generate the expected pan-sharpened results. To the best of our knowledge, this is the first attempt to introduce transformer and invertible neural network into pan-sharpening field. Extensive experiments over different kinds of satellite datasets demonstrate that our method outperforms state-of-the-art algorithms both visually and quantitatively with fewer parameters and flops. Further, the ablation experiments also prove the effectiveness of the proposed customized long-range transformer and effective invertible neural feature fusion module for pan-sharpening.Editorial NotesThis article, which was published in Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI 2022), has been retracted by agreement between the authors and the journal.",
  "full_text": "Retraction Note to:\nPan-Sharpening with Customized Transformer and Invert-\nible Neural Network\nThe authors of this paper have formally requested its retraction. This request has been re-\nviewed and approved by AAAI Press, along with the Program and General Chairs of AAAI\n2022.\nDuring recent verification tests (as of June 2025), the authors identified significant dis-\ncrepancies between their current experimental results and those originally reported in the\npaper. A subsequent investigation revealed that a subset of the test samples had been in-\nadvertently included in the training set. This data leakage led to artificially inflated perfor-\nmance metrics that cannot be reproduced when using correctly partitioned datasets.\nIn light of these findings, the authors have concluded that retracting the paper is nec-\nessary to prevent the continued dissemination of potentially misleading results.\nAAAI Press and the Program and General Chairs of AAAI 2022 fully support this de-\ncision. The authors sincerely apologize for any confusion or concern this publication may\nhave caused within the research community.\nJuly 30th, 2025\nRetraction Note to:\nPan-Sharpening with Customized Transformer and Invert-\nible Neural Network\nThe authors of this paper have formally requested its retraction. This request has been re-\nviewed and approved by AAAI Press, along with the Program and General Chairs of AAAI\n2022.\nDuring recent verification tests (as of June 2025), the authors identified significant dis-\ncrepancies between their current experimental results and those originally reported in the\npaper. A subsequent investigation revealed that a subset of the test samples had been in-\nadvertently included in the training set. This data leakage led to artificially inflated perfor-\nmance metrics that cannot be reproduced when using correctly partitioned datasets.\nIn light of these findings, the authors have concluded that retracting the paper is nec-\nessary to prevent the continued dissemination of potentially misleading results.\nAAAI Press and the Program and General Chairs of AAAI 2022 fully support this de-\ncision. The authors sincerely apologize for any confusion or concern this publication may\nhave caused within the research community.\nJuly 30th, 2025\nPan-Sharpening with Customized Transformer and Invertible Neural Network\nMan Zhou2, 1 *, Jie Huang1 *, Yanchi Fang3, Xueyang Fu1, Aiping Liu1†\n1University of Science and Technology of China, China\n2Hefei Institute of Physical Science, Chinese Academy of Sciences, China\n3University of Toronto, Canada\nAbstract\nIn remote sensing imaging systems, pan-sharpening is an\nimportant technique to obtain high-resolution multispectral\nimages from a high-resolution panchromatic image and its\ncorresponding low-resolution multispectral image. Owing to\nthe powerful learning capability of convolution neural net-\nwork (CNN), CNN-based methods have dominated this field.\nHowever, due to the limitation of the convolution operator,\nlong-range spatial features are often not accurately obtained,\nthus limiting the overall performance. To this end, we pro-\npose a novel and effective method by exploiting a customized\ntransformer architecture and information-lossless invertible\nneural module for long-range dependencies modeling and\neffective feature fusion in this paper. Specifically, the cus-\ntomized transformer formulates the PAN and MS features as\nqueries and keys to encourage joint feature learning across\ntwo modalities while the designed invertible neural mod-\nule enables effective feature fusion to generate the expected\npan-sharpened results. To the best of our knowledge, this is\nthe first attempt to introduce transformer and invertible neu-\nral network into pan-sharpening field. Extensive experiments\nover different kinds of satellite datasets demonstrate that our\nmethod outperforms state-of-the-art algorithms both visually\nand quantitatively with fewer parameters and flops. Further,\nthe ablation experiments also prove the effectiveness of the\nproposed customized long-range transformer and effective in-\nvertible neural feature fusion module for pan-sharpening.\nIntroduction\nWith the rapid development of satellite sensors, satellite im-\nages have been used in a wide range of applications like mil-\nitary systems, environmental monitoring, and mapping ser-\nvices. However, due to the technological and physical limita-\ntion of imaging devices, satellites are usually equipped with\nboth multispectral (MS) and panchromatic (PAN) sensors to\nsimultaneously measure the complementary images, MS im-\nages with low spatial resolution and high spectral resolution,\nand PAN images with low spectral resolution and high spa-\ntial resolution. To obtain the images with both high spectral\nand high spatial resolutions, the pan-sharpening technique\nthat fuses the low-resolution MS images and high spatial\n*These authors contributed equally.\n†corresponding author: Aiping Liu (aipingl@ustc.edu.cn).\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Trade-off between PSNR, number of parameters\nand FLOPs over WorldView-II dataset.\nPAN images to break the technological limits, has drawn\nmuch attention from either image processing and remote\nsensing communities.\nIn the past few decades, a deal of pan-sharpening al-\ngorithms has been proposed and obtained promising re-\nsults. The traditional algorithms include component substi-\ntutes (Aiazzi and Selva 2007; Choi and Kim 2011; Kang\nand Benediktsson 2014), multiresolution analysis (Aiazzi\net al. 2003; Kaplan and Erer 2012; Yokoya et al. 2012;\nShah, Younan, and King 2008) and model-based meth-\nods (Ghahremani et al. 2016; Garzelli, Nencini, and Capo-\nbianco 2008). However, all of them are generally based\non handcrafted features, with limited capacity to recon-\nstruct the missing information in the MS images. Very re-\ncently, to overcome the aforementioned shortcomings, re-\nsearchers focus on exploiting the powerful feature represen-\ntation capability of convolution neural networks (CNNs) to\nconstruct numerous CNNs-based pan-sharpening methods\n(Wang et al. 2021a,b; Xu et al. 2021a; Peng et al. 2021; Ben-\nzenati, Kallel, and Kessentini 2021; Hu et al. 2021; Liu et al.\n2020; Xu et al. 2021b; Cai and Huang 2021), which outper-\nforms previous state-of-the-art methods by a large margin.\nHowever, existing CNN-based methods remain some limi-\ntations: 1) lacking the modeling of long-range dependency\nowing to the local neighbor reception characterize of convo-\nlution operator, 2) ineffective feature extraction and fusion.\nBoth result in the loss of some essential feature that might\nbe useful for an exemplary pan-sharpened image.\nLong-Range Dependency Modeling.Transformer archi-\ntecture is firstly proposed and has achieved a remarkable per-\nformance in the natural language processing field (Vaswani\net al. 2017). Different from the local reception characterize\nof convolution operator, transformer architecture is naturally\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n3553\nRETRACTED\ngood at capturing the long-range dependencies by employ-\ning the multi-head global attention mechanism among dif-\nferent ordered input feature parts. Afterward, motivated by\ntheir success, many researchers have begun to introduce the\ntransformer structure into computer vision (Yuan et al. 2021;\nLi et al. 2021). The pioneering work is the visual trans-\nformer (ViT) (Vaswani et al. 2017) for the image recognition\ntask, which obtains excellent results compared with state-of-\nthe-art CNN-based methods. Since then, transformer-based\nmethods have emerged to successfully work in other com-\nputer vision problems like object detection (Dosovitskiy\net al. 2020; Carion et al. 2020; Zhu et al. 2020), image seg-\nmentation (H. Wang and Chen 2021) and image restoration\n(Wang et al. 2021c,d) as well. However, it has not been ex-\nplored in the pan-sharpening task. In addition, existing trans-\nformer architectures are designed to find the self-similarity\nin a single image. The goal of pan-sharpening is to seek\nthe interactive information between two kinds of modality\nimages, MS image and PAN image. To achieve this, in-\nspired by (Yang et al. 2020), we redevelop a customized pan-\nsharpening transformer architecture. Specifically, the pro-\nposed transformer formulates the PAN and MS features as\nqueries and keys to encourage joint feature learning across\ntwo modalities for searching the long-range features, shown\nin Figure 4.\nEffective Feature Extraction and Fusion.The goal of\nthe pan-sharpening task is to fuse the complementary infor-\nmation from MS image and PAN image to generate high spa-\ntial resolution MS image. As recognized, how to effectively\nextract and fuse complementary information is crucial for\npan-sharpening performance. Specifically, most of the ex-\nisting pan-sharpening methods directly concatenate the MS\nand PAN image in the image space and then feed them into a\nsingle-stream shared convolution encoder for feature extrac-\ntion and fusion. The remaining methods adapt two-stream\nindependent convolution encoders to provide the modality-\nspecific feature maps from MS and PAN images, and then\nconcatenate the obtained feature maps for fusion in the fea-\nture space. However, the above methods have not fully in-\nvestigated the feature extraction and fusion potentials. To\nthis end, we design two schemes: 1) local and long-range\nfeature extraction module; 2) densely-connected invertible\nneural network fusion module. Specifically, the former con-\nsists of two branches, local convolution branch and long-\nrange transformer branch. Both of them receive the MS im-\nage and PAN image as input for local and long-range feature\nextraction. Due to the natural information lossless capability\nof invertible neural architecture (Dinh, Krueger, and Bengio\n2015; Laurent Dinh and Bengio. 2017), different from ex-\nisting methods adapting pure convolution layers to achieve\nfusion, we design a new densely-connected invertible neu-\nral network for effective feature fusion. The implementation\ndetails can refer to Figure 3.\nIn a word, we propose a novel effective pan-sharpening\nmethod by combining the advantages of long-range de-\npendencies modeling of transformer architecture and\ninformation-lossless invertible neural network in this paper.\nTo the best of our knowledge, this is the first attempt to in-\ntroduce transformer and invertible neural network into pan-\nsharpening field. As shown in Figure 2, our method consists\nof three procedures: 1) local and long-range feature extrac-\ntion by convolution and transformer, 2) effective local and\nlong-range feature fusion by densely-connected invertible\nneural module, and 3) high-resolution MS image reconstruc-\ntion. Extensive experiments over different kinds of satellite\ndatasets demonstrate that our method outperforms state-of-\nthe-art algorithms both visually and quantitatively. Further,\nthe ablation experiments also prove the effectiveness of the\nproposed customized long-range modeling of transformer\nand effective invertible neural feature fusion module.\nOur contributions can be summarized as follows:\n• We propose a novel pan-sharpening method by combin-\ning advantages of long-range dependencies modeling of\ntransformer architecture and effective feature fusion ca-\npability of invertible neural network in this paper. To the\nbest of our knowledge, this is the first attempt to intro-\nduce transformer and invertible neural network into the\npan-sharpening field.\n• We design a customized Transformer architecture for\npan-sharpening and a new densely-connected invertible\nneural module. The ablation experiments also prove the\neffectiveness of the proposed transformer and invertible\nneural feature fusion module.\n• Extensive experiments over different kinds of satellite\ndatasets demonstrate that our method outperforms state-\nof-the-art algorithms both visually and quantitatively\nwith fewer parameters and running flops.\nMethodology\nIn this section, we first illustrate the overall architecture of\nour pan-sharpening network. It has two core designs to make\nit suitable for pan-sharpening, local and long-range feature\nextraction module, and effective densely-connected invert-\nible feature fusion neural module. The details will be illus-\ntrated below.\nOverall Network Architecture\nThe overall structure is shown in Figure 2. It takes the MS\nimage and PAN image as input and integrates the texture\ndetails of the high-resolution (HR) PAN images with the\nspectral information from low-resolution (LR) MS images\nto generate HR-MS images. To be specific, given the PAN\nimage P ∈R1×H×W and MS image M ∈RC×H\n4 ×W\n4 , our\nmethod firstly applies two independent 3 ×3 convolution\nlayers to project the up-sampling MS with four times and\nPAN image into feature space with modality-specific fea-\ntures, P0 and M0. Next, the feature maps P0, and M0 are\npassed through two-stream local and long-range feature ex-\ntraction module. The local branch consists of several con-\nvolution layers and provides the local-range feature maps,\nwhile the transformer branch takes advantage of multi-head\nattention to generates the long-range features between the\nflatten feature patches from P0 and M0. The obtained lo-\ncal and long-range features are remarked as L0 and G0.\nFollowed by, these two features are further propagated to\ndensely-connected invertible feature fusion neural module\n3554\nRETRACTED\nFigure 2: The overall structure of our proposed method. The MS and PAN image pair is firstly projected into modality-\naware features by pre-convolution layer. Then, above features are fed into a customized Transformer-based long-range feature\nmodeling and CNN-based local feature extraction module. Next, the obtained long and local-term features are passed through\na newly-designed INN-based module for effective feature fusion. Finally, the fused feature combined with skip-connection MS\nimage is exploited for reconstruction.\nFigure 3: Architecture of the proposed densely-connected invertible feature fusion module. The sub-figure (a) and (c) detail\nthe invertible unit and Post-conv of Figure 2 respectively, while the sub-figure (b) deepens into the HINM of sub-figure (a).\nto achieve effective fusion. Specifically, these two kinds\nof features interact with each other to enhance their repre-\nsentation. Then the enhanced representation is transformed\nto the same size and channel of the upsampling MS im-\nages. Finally, we construct the HR-MS images by adding\nthe upsampling MS images to the transformed representa-\ntion with skip-connection. The pan-sharpening process can\nbe described as:\nH = (M) ↑s +f([P, (M) ↑s]). (1)\n3555\nRETRACTED\nFigure 4: The structure of pan-sharpening transformer.\nNote that the direct output of our networkf(.) is the residual\nhigh-frequency details, which is a common technique used\nin existing methods to ease learning.\nLocal and Long-Range Feature Extraction\nAs shown in Figure 2, our designed feature extraction mod-\nule consists of two branches, the local-range feature branch\nby convolution layer and the long-range feature branch by\ntransformer architecture. To preserve the initial features of\nthe MS image and PAN image, the up-sampling MS images\nˆM ∈RC×H×W and PAN image P ∈R1×H×W are firstly\nfed into two independent 3 ×3 convolution layers to ob-\ntain shallow features, M0 ∈R8×H×W and P0 ∈R8×H×W\nrespectively. Then, concatenatingM0 and P0 by channel di-\nmension is passed into the above two branches.\nSpecifically, the local feature branch is implemented by a\n3 ×3 convolution layer, and receives the full-resolution fea-\nture maps M0 and P0 to extract the local-range feature L0.\nIn the long-range feature branch, a newly-designed trans-\nformer is used for generating the long-range dependency. As\nrecognized, the standard transformer is designed to capture\nthe long-range self-similarity dependency among all the to-\nkens of single image. Owing to the nature of pan-sharpening\nthat needs to integrate the complementary information be-\ntween two kinds of images, the MS image and PAN im-\nage, directly applying standard Transformer architecture for\npan-sharpening task is not suitable. The structure of our\ntransformer is shown in Figure 4. The transformer takes\nthe divided MS and PAN feature patches M1, . . . , Mn and\nP1, . . . , Pn with 16 ×16 pixel size as input from shallow\nfull-resolution features M0 and P0.\nFirstly, we use several convolution layers to project the\nMS and PAN feature patches M1, . . . , Mn and P1, . . . , Pn\nwith 16 ×16 pixel size to the texture features, Q (query), K\n(key), and V (value) of three basic elements inside a trans-\nformer. Different from standard transformer, we expand the\nV (value) with two component, V1 and V2 as\nQ = Conv([M1, . . . , Mn]), (2)\nK = Conv([P1, . . . , Pn]), (3)\nV 1 =Conv([M1, . . . , Mn]), (4)\nV 2 =Conv([P1, . . . , Pn]) (5)\nwhere Conv and [.] represent the convolution operation and\nconcatenation by channel dimension, respectively. Then, K\nand Q will be used in our relevance metric moduleto esti-\nmating the similarity. We unfold bothK and Q into patches,\ndenoted as qi (i ∈[1, H×W]) and kj (j ∈[1, H×W]).\nThen for each patchqi in Q and kj in K, we calculate the rel-\nevance ri,j between these two patches by normalized inner\nproduct as ri,j = ( qi\n||qi||, kj\n||kj||). The whole relevance matrix\nis remarked,\nR = QT K. (6)\nThen, we further use the relevance matrix R to generate\nthe hard-attention and soft-attention map. Different from\ntraditional attention mechanism takeing a weighted sum of\nV for each query qi, we propose hard-attention and soft-\nattention module to transfer the image texture features V\nto the HR-MS image. More specifically, we first calculate\na hard-attention map H in which the i −th element hi\n(i ∈ [1, H×W]) is calculated from the relevance ri,j:\nhj = argmax\nj\n(ri,j). Then we apply an index selection oper-\nation to the unfolded patches of V 1 and V 2 using the hard-\nattention map as the index:\nt1\ni = v1\nhi, (7)\nt2\ni = v2\nhi (8)\nwhere t1\ni and t2\ni denote the value selected from the hi-th po-\nsition of V 1 and V 2. As a result, we obtain a HR feature\nrepresentation T1 and T2 for the PAN feature and MS fea-\nture by position index attention. Furthermore, we calculate\nthe soft-attention map as,\nS = softmax(R), (9)\nwhere softmax is the softmax function in mathematical. Fi-\nnally, we obtain the enhanced long-range features G0 by in-\ntegrating the soft-attention and hard-attention map with the\nPAN features\nG1\n0 = P0 + Conv([P0, T1]) ⊙S, (10)\nG2\n0 = M0 + Conv([M0, T2]) ⊙S, (11)\nG0 = Conv([G1\n0, G2\n0]) (12)\nwhere P0, M0, [.] and Conv represent the PAN features and\nMS feature from pre-convolution, concatenation operation\nby channel dimension and convolution layer respectively.\n3556\nRETRACTED\nInvertible Neural Module for Feature Fusion\nDifferent from pure convolution layer, invertible networks\nare information-lossless during the transformation (Liu et al.\n2021; Zhang et al. 2021; Xing, Qian, and Chen 2021; Lu\net al. 2021; Paschalidou et al. 2021). For the invertible\nmodel, the input needs to be divided into two parts. In our\nwork, the input of our invertible module naturally consists of\ntwo parts, local and long-range features L0 and G0, which\nexactly match the splitting of input. To take advantage of in-\nvertible networks for preserving the extracted features, we\ndesign a densely-connected invertible feature fusion neural\nmodule with the composition of a stack of invertible basic\nunits. As shown in Figure 3 (a), each basic unit we follow in\nthis work is the affine coupling layer.\nTo increase the representational capacity of the network,\ntwo kinds of schemes are proposed, 1) immediate sequential\nfeatures of each invertible unit are propagated to the final\nunit by skip-connection and then concatenated to enhance\nits representation, 2) effective transformation operation be-\ntween two parts is designed. To be specific, we use an addi-\ntive transformation for the long-range branch, and employ an\nenhanced affine transformation for the local-range branch.\nTake the first affine coupling layer for example, given local\nand long-range features L0 and G0, the output will be cal-\nculated as\nL1 = L0 + ϕ(G0), (13)\nG1 = G0 ⊙exp(ρ(L1)) +η(L1) (14)\nwhere exp(.) is Exponential function in mathematical, and\nρ(.) and η(.) represent the scale and translation functions\nfrom the channels of local feature L0 to the channels of\nlong-range feature G0, respectively. ϕ(.) performs the in-\nverse function as ρ(.) and η(.). ⊙is the Hadamard product.\nNote that the scale and translation functions are not neces-\nsarily invertible, and thus we realize them by neural net-\nworks. By doing so, the other k −1 invertible blocks re-\nceive the output of the previous and generate the results. All\nthe outputs L0/G0, . . . , Lk/Gk of each invertible unit are\nconcatenated to generate the high-frequency details by us-\ning the residual channel attention block and then added with\nthe input low-spatial MS image to obtain HR-MS image by\nskip-connection\nH = (M) ↑s +RCAB([L0, G0, . . . , Lk, Gk]). (15)\nwhere RCAB (Zhang et al. 2018) and[.] represent the resid-\nual channel attention and concatenation by the channel di-\nmension. The k is the number of our stacked invertible neu-\nral units and set as 3 to reduce the computational cost.\nIn addition, to enhance the interaction with two-part fea-\ntures, we implement the transformation operation ρ(.), η(.)\nand ϕ(.) with two cascaded Half Instance Normalization\nblocks (HIN) (Chen et al. 2021). As shown in Figure 3 (b),\nHIN block firstly employs 3 ×3 convolution to project in-\nput features Fin ∈ RCin×H×W to intermediate features\nFmid ∈R16×H×W . Then, the featuresFmid are divided into\ntwo parts (Fmid1 /Fmid2 ∈R8×H×W ). The first part Fmid1\nis normalized by Instance Normalization (IN) and then con-\ncatenates with Fmid2 in channel dimension. HIN blocks use\nInstance Normalization (IN) on the half of the channels and\nkeep context information by the other half of the channels.\nAfter the concatenation operation, the obtained featuresFres\nare passed through one3×3 convolution layer and two leaky\nReLU layers. Finally, HIN blocks output the enhanced fea-\nture Fout by adding Fres with shortcut features (obtained\nafter 1 ×1 convolution) as\nFmid = Conv3∗3(Fin), (16)\nFmid1 , Fmid2 = split(Fmid), (17)\nFres = concat(IN (Fmid1 ), Fmid2 ), (18)\nFout = Fres + Fin. (19)\nwhere Conv3∗3 represents the 3 ×3-kernel convolution op-\nerator. The split(.) and concate is the splitting and concate-\nnation function in channel dimension. IN is the Instance\nNormalization.\nNetwork Loss Function\nWe adopt the the mean absolute error (L1 loss) to optimize\nour proposed method\nL=\nKX\ni=1\n∥Hi −Hgt,i∥1 , (20)\nwhere K is the number of training data, Hi and Hgt,i de-\nnote the output high-resolution MS image and ground truth,\nrespectively.\nExperiments\nBaseline Methods\nTo verify the effectiveness of the proposed method, a se-\nries of experiments are carried out between our proposed\nmethod and ten state-of-the-art pan-sharpening algorithms.\nTo be specific, five representative deep learning based meth-\nods are selected for comparison, namely, PNN (Masi et al.\n2016), PANNET (Yang et al. 2017), MSDCNN (Yuan et al.\n2018), SRPPNN (Cai and Huang 2021), and GPPNN (Xu\net al. 2021b). Our method is also compared with five classic\nmethods, including, SFIM (Liu. 2000), Brovey (Gillespie,\nKahle, and Walker 1987), GS (Laben and Brower 2000),\nIHS (Haydn et al. 1982) and GFPCA (Liao et al. 2017).\nImplementation Details\nWe implement all our networks in PyTorch framework on\nthe PC with a single NVIDIA GeForce GTX 2080Ti GPU.\nIn the training phase, they are optimized by Adam optimizer\nover 1000 epochs with a learning rate of8×10−4 and a batch\nsize of 4. When reaching 200 epochs, the learning rate is de-\ncayed by multiplying 0.5. The paired training samples are\nunavailable in practice. When we construct the training set,\nthe Wald protocol is employed to generate the paired sam-\nples. For example, given the MS image H ∈RM×N×C and\nthe PAN image P ∈RrM×rN×b, both of them are down-\nsampled with ratio r, and the downsampled versions are de-\nnoted by L ∈RM/r×N/r×C and p ∈RM×N×b. In the train-\ning set, L and p are regarded as the inputs, while H is the\nground truth.\n3557\nRETRACTED\nMethod Num of\nParams\nWorldV\niew II GaoFen2 WorldView III\nPSNR↑ SSIM↑ SAM↓ ERGAS↓ PSNR↑ SSIM↑ SAM↓ EGAS↓ PSNR↑ SSIM↑ SAM↓ EGAS↓\nSFIM - 34.1297 0.8975\n0.0439 2.3449 36.906 0.8882\n0.0318 1.7398 21.8212 0.5457\n0.1208 8.973\nBrov\ney - 35.8646 0.9216\n0.0403 1.8238 37.7974 0.9026\n0.0218 1.372 22.506 0.5466\n0.1159 8.2331\nGS - 35.6376 0.9176\n0.0423 1.8774 37.226 0.9034\n0.0309 1.6736 22.5608 0.547\n0.1217 8.2433\nIHS - 35.2962 0.9027\n0.04610 2.0278 38.1754 0.9100\n0.0243 1.5336 22.5579 0.5354\n0.1266 8.3616\nGFPCA - 34.5581 0.9038\n0.0488 2.1411 37.9443 0.9204\n0.0314 1.5604 22.3344 0.4826\n0.1294 8.3964\nPNN 0.689 40.7550 0.9624\n0.0259 1.0646 43.1208 0.9704\n0.0172 0.8528 29.9418 0.9121\n0.0824 3.3206\nPANNET 0.688 40.8176 0.9626\n0.0257 1.0557 43.0659 0.9685\n0.0178 0.8577 29.684 0.9072\n0.0851 3.4263\nMSDCNN 2.390 41.3355 0.9664\n0.0242 0.994 45.6874 0.9827\n0.0135 0.6389 30.3038 0.9184\n0.0782 3.1884\nSRPPNN 17.114 41.4538 0.9679\n0.0233 0.9899 47.1998 0.9877\n0.0106 0.5586 30.4346 0.9202\n0.077 3.1553\nGPPNN 1.198 41.1622 0.9684\n0.0244 1.0315 44.2145 0.9815\n0.0137 0.7361 30.1785 0.9175\n0.0776 3.2593\nOurs 0.706 41.6903 0.9704\n0.0227 0.9514 47.3528 0.9893\n0.0102 0.5479 30.5365 0.9225\n0.0747 3.0997\nTable 1: The four metrics on test datasets. The best and the second best values are highlighted by the red bold and underline,\nrespectively. The up or down arrow indicates higher or lower metric corresponds to better images.\nPNN P\nANNET MSDCNN SRPPNN GPPNN Ours\nparams 0.689\n0.688 2.390 17.114 1.198 0.706\nflops 1.1289 1.1275 3.9158 21.1059 1.3967 1.3907\nTable 2: Comparisons on flops and parameter numbers.\nDataset and Evaluation Metrics\nRemote sensing images acquired by three satellites are used\nin our experiments, including WorldView II, WorldView III\n, and GaoFen2, the basic information of which are listed in\nsupplementary materials. For each satellite, we have hun-\ndreds of image pairs, and they are divided into two parts\nfor training and test. In the training set, the MS images are\ncropped into patches with the size of 128 ×128 , and the\ncorresponding PAN patches are with the size of32×32. For\nnumerical stability, each patch is normalized by dividing the\nmaximum value to make the pixels range from 0 to 1.\nSeveral widely used image quality assessment (IQA) met-\nrics are employed to evaluate the performance, including\nthe relative dimensionless global error in synthesis (ER-\nGAS) (Alparone et al. 2007), the peak signal-to-noise ratio\n(PSNR), the spectral angle mapper (SAM) (J. R. H. Yuhas\nand Boardman 1992).\nComparison with SOTA Methods\nThe evaluation metrics on three datasets are reported in Ta-\nble 1, where the values highlighted by red color represent\nthe best results. It is clearly found that our method surpasses\nother comparative algorithms in all evaluation metrics on\nthree satellites. In addition, We also show the comparison of\nthe visual results to testify the effectiveness of our method\nin Figure 5. Images in the last row are the MSE residues\nbetween the pan-sharpened results and the ground truth. To\nbe specific, other comparison methods suffer from severe\nspatial and spectral distortion. However, our method has\nthe most minor spatial and spectral distortions. Specifically,\nfrom the amplified local regions, we observe that our pro-\nposed method has finer-grained textures and coarser-grained\nstructures compared with other methods. As for the MSE\nresidues, we can figure out that our proposed method is the\nclosest to the ground truth than other comparison methods.\nFlops and Parameter Numbers\nIn this section, we investigate the complexity of the proposed\nmethod, including the flops and the number of parameters\n(in 10 M). Comparisons on parameter numbers and model\nperformance (representation by PSNR) are shown in Table 2\nand in Figure 1. It can be seen that our network can achieve\na good trade-off between calculation and performance com-\npared to other deep learning-based methods. We use the ten-\nsor with 1 ×4 ×32 ×32 and 1 ×1 ×128 ×128 to represent\nthe MS and PAN roles for evaluation.\nAblation Experiments\nSince the transformer module and densely-connected in-\nvertible neural network fusion module are the core of our\nmethod, to investigate their necessity and effectiveness, a se-\nries of ablation experiments are carried out. There are 3 dif-\nferent configurations for the corresponding network variants\nof our proposed method and the results of ablation experi-\nments are shown in Table 3.\nTransformer Module.The Transformer module is respon-\nsible for capturing long-range dependency, which is critical\nfor pan-sharpening performance. In the first experiment, we\ndelete the Transformer module to verify its necessity while\nexpanding the feature channels of local-range branch for fair\ncomparison. Table 3 shows that deleting Transformer mod-\nule will degrade all metrics dramatically. Therefore, Trans-\nformer module plays a significant role in our network.\nInvertible Neural Network Fusion Module.In the second\nexperiment, to verify the effectiveness of densely-connected\ninvertible neural network fusion module, we replace it with\nits transformation units . In other words, the extracted long\n3558\nRETRACTED\nFigure 5: Qualitative comparison of our method with nine counterparts on a typical satellite image pair from the GaoFen-2\ndataset. Images in the last row visualize the MSE residues between the pan-sharpened results and the ground truth.\nConfigurations T\nransformer Invertible Fusion\nWorldV\niew II GaoFen2 WorldV\niew III\nPSNR↑ SSIM↑ SAM↓ ERGAS↓ PSNR↑ SSIM↑ SAM↓ EGAS↓ PSNR↑ SSIM↑ SAM↓ EGAS↓\n(I) # ! 41.1932 0.9684\n0.0238 1.0059 46.7804 0.9879\n0.011 0.5835 30.2943 0.9193\n0.0784 3.1882\n(II) ! # 41.2232 0.9683\n0.0238 1.0049 46.9368 0.988\n0.0106 0.5728 30.2588 0.9181\n0.0785 3.2053\nOurs ! ! 41.6903 0.9704\n0.0227 0.9514 47.3528 0.9893\n0.0102 0.5479 30.5365 0.9225\n0.0747 3.0997\nTable 3: The results of ablation experiments over three datasets. The best values are highlighted by the red bold. The up or\ndown arrow indicates higher or lower metric corresponds to better images.\nand local-range features are concatenated and then fed into\npure densely-connected architecture. For fair comparison,\nwe keep the above two comparisons with the same num-\nber of parameters. The results in Table 3 demonstrate that\nremoving the invertible fusion module will weaken our net-\nwork’s performance. Therefore, invertible neural network\nfusion module is critical in our method.\nOur Complete Network.In the last row of Table 3, we can\nclearly find that compared with above two variants, taking\nWorldView-II dataset for example, adding the Transformer\nmodule achieves an improvement of 0.5 dB and 0.01 on av-\nerage PSNR and SSIM, respectively. Similarly, the invertible\nfusion module improves the baseline by 0.47 dB and 0.01.\nOther datasets also keep consistent as above in model per-\nformance. This is because the two modules are beneficial to\ncapture the long-range dependency spatially and effectively\nfuse the features for the pan-sharpening task. The best re-\nsults can be obtained by combining the two modules.\nConclusion\nIn this paper, we propose a novel and effective pan-\nsharpening method by integrating long-range dependen-\ncies modeling of Transformer architecture and Information-\nlossless invertible neural network in this paper. To the best\nof our knowledge, this is the first attempt to introduce trans-\nformer and invertible neural network into pan-sharpening\nfield. Extensive experiments over different kinds of satellite\ndatasets demonstrate that our method outperforms state-of-\nthe-art algorithms both visually and quantitatively.\nIn the future, we will explore the potential of our proposed\ncustomized transformer and invertible module into existing\npan-sharpening methods.\nAcknowledgments\nThis work was supported by the National Natural Science\nFoundation of China (61701158) and the USTC Research\nFunds of the Double First-Class Initiative under Grants\nYD2100002004.\n3559\nRETRACTED\nReferences\nAiazzi, B.; Alparone, L.; Baronti, S.; Garzelli, A.; and Selva,\nM. 2003. An MTF-based spectral distortion minimizing\nmodel for pan-sharpening of very high resolution multispec-\ntral images of urban areas. In Joint Workshop on Remote\nSensing and Data Fusion over Urban Areas.\nAiazzi, B. S., B.; and Selva, M. 2007. Improving Compo-\nnent Substitution Pansharpening Through Multivariate Re-\ngression of MS +Pan Data. IEEE Transactions on Geo-\nscience and Remote Sensing, 45(10): 3230–3239.\nAlparone, L.; Wald, L.; Chanussot, J.; Thomas, C.; Gamba,\nP.; and Bruce, L. M. 2007. Comparison of Pansharpening\nAlgorithms: Outcome of the 2006 GRS-S Data Fusion Con-\ntest. IEEE Transactions on Geoscience and Remote Sensing,\n45(10): 3012–3021.\nBenzenati, T.; Kallel, A.; and Kessentini, Y . 2021. Two\nStages Pan-Sharpening Details Injection Approach Based on\nVery Deep Residual Networks. IEEE Transactions on Geo-\nscience and Remote Sensing, 59(6): 4984–4992.\nCai, J.; and Huang, B. 2021. Super-Resolution-Guided\nProgressive Pansharpening Based on a Deep Convolutional\nNeural Network. IEEE Transactions on Geoscience and Re-\nmote Sensing, 59(6): 5206–5220.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-End Object Detection\nwith Transformers. In European Conference on Computer\nVision.\nChen, L.; Lu, X.; Zhang, J.; Chu, X.; and Chen, C. 2021.\nHINet: Half Instance Normalization Network for Image\nRestoration. arXiv:2105.06086.\nChoi, Y . K., J.; and Kim, Y . 2011. A New Adaptive\nComponent-Substitution-Based Satellite Image Fusion by\nUsing Partial Replacement. IEEE Transactions on Geo-\nscience and Remote Sensing, 49(1): p.295–309.\nDinh, L.; Krueger, D.; and Bengio, Y . 2015. NICE: Non-\nlinear Independent Components Estimation. International\nConference on Learning Representations.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.;\nand Houlsby, N. 2020. An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale.\nGarzelli, A.; Nencini, F.; and Capobianco, L. 2008. Optimal\nMMSE Pan Sharpening of Very High Resolution Multispec-\ntral Images. IEEE Transactions on Geoscience and Remote\nSensing, 46(1): 228–236.\nGhahremani; Morteza; Ghassemian; and Hassan. 2016.\nA Compressed-Sensing-Based Pan-Sharpening Method for\nSpectral Distortion Reduction. IEEE Transactions on Geo-\nscience and Remote Sensing.\nGillespie, A. R.; Kahle, A. B.; and Walker, R. E. 1987. Color\nenhancement of highly correlated images. II. Channel ratio\nand ”chromaticity” transformation techniques - ScienceDi-\nrect. Remote Sensing of Environment, 22(3): 343–365.\nH. Wang, H. A. A. Y ., Y . Zhu; and Chen, L.-C. 2021.\nMax-deeplab: End-to-end panoptic segmentation with mask\ntransformers. In IEEE Conference on Computer Vision and\nPattern Recognition.\nHaydn, R.; Dalke, G. W.; Henkel, J.; and Bare, J. E.\n1982. Application of the IHS color transform to the pro-\ncessing of multisensor data and image enhancement. Na-\ntional Academy of Sciences of the United States of America,\n79(13): 571–577.\nHu, J.; Hu, P.; Kang, X.; Zhang, H.; and Fan, S. 2021. Pan-\nSharpening via Multiscale Dynamic Convolutional Neural\nNetwork. IEEE Transactions on Geoscience and Remote\nSensing, 59(3): 2231–2244.\nJ. R. H. Yuhas, A. F. G.; and Boardman, J. M. 1992. Discrim-\nination among semi-arid landscape endmembers using the\nspectral angle mapper (SAM) algorithm. Proc. Summaries\nAnnu. JPL Airborne Geosci. Workshop, 147–149.\nKang, L. S., X.; and Benediktsson, J. A. 2014. Pansharpen-\ning With Matting Model. IEEE Transactions on Geoscience\nand Remote Sensing, 52(8): 5088–5099.\nKaplan, N. H.; and Erer, I. 2012. Bilateral pyramid based\npansharpening of multispectral satellite images. In Geo-\nscience and Remote Sensing Symposium.\nLaben, C.; and Brower, B. 2000. Process for Enhancing\nthe Spatial Resolution of Multispectral Imagery Using Pan-\nSharpening. US Patent 6011875A.\nLaurent Dinh, J. S.-D.; and Bengio., S. 2017. Density esti-\nmation using real NVP. ICLR.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Gool, L. V . 2021.\nLocalViT: Bringing Locality to Vision Transformers.CoRR,\nabs/2104.05707.\nLiao, W.; Xin, H.; Coillie, F. V .; Thoonen, G.; and Philips,\nW. 2017. Two-stage fusion of thermal hyperspectral and\nvisible RGB image by PCA and guided filter. In Workshop\non Hyperspectral Image and Signal Processing: Evolution\nin Remote Sensing.\nLiu., J. G. 2000. Smoothing filter-based intensity modula-\ntion: A spectral preserve image fusion technique for improv-\ning spatial details. International Journal of Remote Sensing,\n21(18): 3461–3472.\nLiu, Q.; Zhou, H.; Xu, Q.; Liu, X.; and Wang, Y . 2020. PS-\nGAN: A Generative Adversarial Network for Remote Sens-\ning Image Pan-Sharpening. IEEE Transactions on Geo-\nscience and Remote Sensing, 1–16.\nLiu, Y .; Qin, Z.; Anwar, S.; Ji, P.; Kim, D.; Caldwell, S.; and\nGedeon, T. 2021. Invertible Denoising Network: A Light\nSolution for Real Noise Removal. In IEEE Conference on\nComputer Vision and Pattern Recognition, 13365–13374.\nLu, S.-P.; Wang, R.; Zhong, T.; and Rosin, P. L. 2021. Large-\nCapacity Image Steganography Based on Invertible Neural\nNetworks. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, 10816–10825.\nMasi, G.; Cozzolino, D.; Verdoliva, L.; and Scarpa, G. 2016.\nPansharpening by convolutional neural networks. Remote\nSensing, 8(7): 594.\nPaschalidou, D.; Katharopoulos, A.; Geiger, A.; and Fidler,\nS. 2021. Neural Parts: Learning Expressive 3D Shape Ab-\nstractions With Invertible Neural Networks. In IEEE Con-\nference on Computer Vision and Pattern Recognition, 3204–\n3215.\n3560\nRETRACTED\nPeng, J.; Liu, L.; Wang, J.; Zhang, E.; Zhu, X.; Zhang,\nY .; Feng, J.; and Jiao, L. 2021. PSMD-Net: A Novel\nPan-Sharpening Method Based on a Multiscale Dense Net-\nwork. IEEE Transactions on Geoscience and Remote Sens-\ning, 59(6): 4957–4971.\nShah, V . P.; Younan, N. H.; and King, R. L. 2008. An Ef-\nficient Pan-Sharpening Method via a Combined Adaptive\nPCA Approach and Contourlets.IEEE Transactions on Geo-\nscience and Remote Sensing, 46(5): 1323–1335.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Advances in Neural Information\nProcessing Systems, volume 30.\nWang, D.; Bai, Y .; Wu, C.; Li, Y .; Shang, C.; and Shen, Q.\n2021a. Convolutional LSTM-Based Hierarchical Feature\nFusion for Multispectral Pan-Sharpening. IEEE Transac-\ntions on Geoscience and Remote Sensing, 1–16.\nWang, J.; Shao, Z.; Huang, X.; Lu, T.; and Zhang, R. 2021b.\nA Dual-Path Fusion Network for Pan-Sharpening. IEEE\nTransactions on Geoscience and Remote Sensing, 1–14.\nWang, Z.; Cun, X.; Bao, J.; and Liu, J. 2021c. Uformer:\nA General U-Shaped Transformer for Image Restoration.\nCoRR, abs/2106.03106.\nWang, Z.; Cun, X.; Bao, J.; and Liu, J. 2021d. Uformer: A\nGeneral U-Shaped Transformer for Image Restoration.\nXing, Y .; Qian, Z.; and Chen, Q. 2021. Invertible Image\nSignal Processing. In IEEE Conference on Computer Vision\nand Pattern Recognition, 6287–6296.\nXu, H.; Ma, J.; Shao, Z.; Zhang, H.; Jiang, J.; and Guo, X.\n2021a. SDPNet: A Deep Network for Pan-Sharpening With\nEnhanced Information Representation. IEEE Transactions\non Geoscience and Remote Sensing, 59(5): 4120–4134.\nXu, S.; Zhang, J.; Zhao, Z.; Sun, K.; Liu, J.; and Zhang,\nC. 2021b. Deep Gradient Projection Networks for Pan-\nsharpening. In IEEE Conference on Computer Vision and\nPattern Recognition, 1366–1375.\nYang, F.; Yang, H.; Fu, J.; Lu, H.; and Guo, B. 2020.\nLearning Texture Transformer Network for Image Super-\nResolution. In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR).\nYang, J.; Fu, X.; Hu, Y .; Huang, Y .; Ding, X.; and Pais-\nley, J. 2017. PanNet: A deep network architecture for pan-\nsharpening. In IEEE International Conference on Computer\nVision, 5449–5457.\nYokoya, N.; Member, S.; IEEE; Yairi, T.; and Iwasaki, A.\n2012. Coupled Nonnegative Matrix Factorization Unmix-\ning for Hyperspectral and Multispectral Data Fusion. IEEE\nTransactions on Geoscience and Remote Sensing, 50(2):\n528–537.\nYuan, K.; Guo, S.; Liu, Z.; Zhou, A.; Yu, F.; and Wu, W.\n2021. Incorporating Convolution Designs Into Visual Trans-\nformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 579–588.\nYuan, Q.; Wei, Y .; Meng, X.; Shen, H.; and Zhang, L. 2018.\nA Multiscale and Multidepth Convolutional Neural Network\nfor Remote Sensing Imagery Pan-Sharpening. IEEE Jour-\nnal of Selected Topics in Applied Earth Observations and\nRemote Sensing, 11(3): 978–989.\nZhang, S.; Zhang, C.; Kang, N.; and Li, Z. 2021. iVPF:\nNumerical Invertible V olume Preserving Flow for Efficient\nLossless Compression. In IEEE Conference on Computer\nVision and Pattern Recognition, 620–629.\nZhang, Y .; Li, K.; Li, K.; Wang, L.; Zhong, B.; and Fu, Y .\n2018. Image super-resolution using very deep residual chan-\nnel attention networks. In European Conference on Com-\nputer Vision, 286–301.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J.\n2020. Deformable DETR: Deformable Transformers for\nEnd-to-End Object Detection. In International Conference\non Learning Representations.\n3561\nRETRACTED",
  "topic": "Sharpening",
  "concepts": [
    {
      "name": "Sharpening",
      "score": 0.8373938798904419
    },
    {
      "name": "Panchromatic film",
      "score": 0.7324109077453613
    },
    {
      "name": "Computer science",
      "score": 0.7136984467506409
    },
    {
      "name": "Artificial intelligence",
      "score": 0.580635130405426
    },
    {
      "name": "Multispectral image",
      "score": 0.5676267147064209
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5607770085334778
    },
    {
      "name": "Transformer",
      "score": 0.5445232391357422
    },
    {
      "name": "Artificial neural network",
      "score": 0.5300559401512146
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41877514123916626
    },
    {
      "name": "Computer vision",
      "score": 0.3316619396209717
    },
    {
      "name": "Engineering",
      "score": 0.111107736825943
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2802624667",
      "name": "Hefei Institutes of Physical Science",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    }
  ]
}