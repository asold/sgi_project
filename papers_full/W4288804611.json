{
  "title": "HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative",
  "url": "https://openalex.org/W4288804611",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5072385246",
      "name": "Xiaomin Fang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100380497",
      "name": "Fan Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5086931226",
      "name": "Lihang Liu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5032113405",
      "name": "Jingzhou He",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5108737864",
      "name": "Dayong Lin",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5044309879",
      "name": "Yingfei Xiang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100441235",
      "name": "Xiaonan Zhang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100677198",
      "name": "Hua Wu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100423946",
      "name": "Hui Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5030589527",
      "name": "Le Song",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2557595285",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2152811165",
    "https://openalex.org/W2161151688",
    "https://openalex.org/W4296060337",
    "https://openalex.org/W1987134040",
    "https://openalex.org/W3191761521",
    "https://openalex.org/W3212854871",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W3171848268",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W4287724045",
    "https://openalex.org/W2149741699",
    "https://openalex.org/W4281291878",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W2997234557",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3104537585",
    "https://openalex.org/W3193589100",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4313430582",
    "https://openalex.org/W3211795435",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4285483966",
    "https://openalex.org/W2073758233"
  ],
  "abstract": "AI-based protein structure prediction pipelines, such as AlphaFold2, have achieved near-experimental accuracy. These advanced pipelines mainly rely on Multiple Sequence Alignments (MSAs) as inputs to learn the co-evolution information from the homologous sequences. Nonetheless, searching MSAs from protein databases is time-consuming, usually taking dozens of minutes. Consequently, we attempt to explore the limits of fast protein structure prediction by using only primary sequences of proteins. HelixFold-Single is proposed to combine a large-scale protein language model with the superior geometric learning capability of AlphaFold2. Our proposed method, HelixFold-Single, first pre-trains a large-scale protein language model (PLM) with thousands of millions of primary sequences utilizing the self-supervised learning paradigm, which will be used as an alternative to MSAs for learning the co-evolution information. Then, by combining the pre-trained PLM and the essential components of AlphaFold2, we obtain an end-to-end differentiable model to predict the 3D coordinates of atoms from only the primary sequence. HelixFold-Single is validated in datasets CASP14 and CAMEO, achieving competitive accuracy with the MSA-based methods on the targets with large homologous families. Furthermore, HelixFold-Single consumes much less time than the mainstream pipelines for protein structure prediction, demonstrating its potential in tasks requiring many predictions. The code of HelixFold-Single is available at https://github.com/PaddlePaddle/PaddleHelix/tree/dev/apps/protein_folding/helixfold-single, and we also provide stable web services on https://paddlehelix.baidu.com/app/drug/protein-single/forecast.",
  "full_text": "HELIX FOLD -SINGLE :\nMSA- FREE PROTEIN STRUCTURE PREDICTION\nBY USING PROTEIN LANGUAGE MODEL AS AN ALTERNATIVE\nXiaomin Fang1‚àó, Fan Wang1‚àó‚Ä†, Lihang Liu1‚àó, Jingzhou He1,\nDayong Lin1, Yingfei Xiang1, Xiaonan Zhang1, Hua Wu1, Hui Li2, Le Song2‚Ä†,\n1Baidu Inc., 2BioMap,\nABSTRACT\nAI-based protein structure prediction pipelines, such as AlphaFold2, have achieved near-experimental\naccuracy. These advanced pipelines mainly rely on Multiple Sequence Alignments (MSAs) as inputs\nto learn the co-evolution information from the homologous sequences. Nonetheless, searching MSAs\nfrom protein databases is time-consuming, usually taking dozens of minutes. Consequently, we\nattempt to explore the limits of fast protein structure prediction by using only primary sequences of pro-\nteins. HelixFold-Single is proposed to combine a large-scale protein language model with the superior\ngeometric learning capability of AlphaFold2. Our proposed method, HelixFold-Single, Ô¨Årst pre-trains\na large-scale protein language model (PLM) with thousands of millions of primary sequences utilizing\nthe self-supervised learning paradigm, which will be used as an alternative to MSAs for learning the\nco-evolution information. Then, by combining the pre-trained PLM and the essential components of\nAlphaFold2, we obtain an end-to-end differentiable model to predict the 3D coordinates of atoms\nfrom only the primary sequence. HelixFold-Single is validated in datasets CASP14 and CAMEO,\nachieving competitive accuracy with the MSA-based methods on the targets with large homologous\nfamilies. Furthermore, HelixFold-Single consumes much less time than the mainstream pipelines for\nprotein structure prediction, demonstrating its potential in tasks requiring many predictions. The code\nof HelixFold-Single is available at https://github.com/PaddlePaddle/PaddleHelix/tree/\ndev/apps/protein_folding/helixfold-single, and we also provide stable web services on\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast.\nKeywords Protein structure prediction ¬∑Primary sequence ¬∑Protein language model ¬∑Large-scale\n1 Introduction\nProteins participate in essentially all biological processes and play critical roles for an organism. The structures of\nproteins are highly correlated to their functions in the biological processes. Determining the protein structures to\nunderstand their functions can bring considerable contributions to life science.\nIn recent years, AI-based protein structure prediction technologies have made signiÔ¨Åcant progress in prediction accuracy,\ndemonstrating great prospects for the drug and vaccine industry. Particularly, AlphaFold2 [1] pushes the performance\nto a new frontier in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14 [2]), approaching\nthe accuracy of experimental determination methods. Mainstream protein structure prediction pipelines heavily rely on\nco-evolution information extracted from Multiple Sequence Alignments (MSAs). MSAs can be simply regarded as\nprotein chains similar to the target protein chain in sequence. MSA is related to the co-evolution information of protein\nsequences, which is crucial to predicting its structure. However, over-reliance on MSAs becomes the bottleneck of\nvarious protein-related tasks.\n‚àóEqual contribution.\n‚Ä†Corresponding to wang.fan@baidu.com and songle@biomap.com.\narXiv:2207.13921v3  [q-bio.BM]  22 Feb 2023\nHelixFold-Single\nFirst, compared with the time (usually several seconds) required for model inference in the structure prediction pipeline,\nsearching MSAs is time-consuming, costing dozens of minutes for a protein. The time-consuming searching is\ndevastating in the tasks demanding high-throughput requests, such as protein design. Second, the primary structures\n(single sequence), rather than the MSAs, drive the folding of the proteins. The MSA extracting methods are also not\ndesigned speciÔ¨Åcally for protein folding. Thus, the MSA-based pipelines only memorize the determined structures of\nsimilar proteins for prediction but do not entirely understand the mechanism of protein folding.\nConsequently, designing an accurate MSA-free protein structure prediction method to address the mentioned issues\nis likely to beneÔ¨Åt and accelerate the development of protein studies. We argue that a large-scale protein language\nmodel (PLM) can be served as an alternative to the MSAs to learn the co-evolution knowledge for MSA-free prediction.\nWe speculate that a PLM with billions of parameters can effectively memorize the MSAs and infer the co-evolution\ninformation. The past few years have seen the tremendous success of large-scale language models [ 3, 4, 5] in\nNatural Language Processing, a Ô¨Åeld that shares a lot of characters with protein studying. With the increase of the\nmodel parameters, the capacity for learning language knowledge grows substantially. Using self-supervised learning\non large-scale unlabeled proteins, PLMs can reveal the long-range relation along protein sequences and improve\ndownstream protein-related tasks. Advanced works have attempted to adopt PLMs to enhance the performance of\nmultiple downstream tasks, such as estimating the secondary structures and the functions [ 6, 7, 8, 9]. Particularly,\nseveral studies [10, 11, 12] attempted to apply PLMs to protein structure prediction. Most works Ô¨Årst predict the\ninter-residue 2D geometry by neural networks and then reconstruct the 3D structure based on energy minimization,\nwhich can not provide end-to-end 3D structure prediction. Besides, compared with the geometric learning capability\nof EvoFormer and Structure Module proposed by AlphaFold, the capacities of the geometric models used by these\nmethods, such as recursive models and ResNets, are also unsatisfactory in understanding the co-evolution and spatial\nrelations between the residues in a single sequence.\nInspired by the progress of PLMs and AlphaFold2, we propose an end-to-end MSA-free protein structure prediction\npipeline, HelixFold-Single. The model used in HelixFold-Single consists of two major components: a large-scale PLM\nas the foundation and the essential components from AlphaFold2 for folding. The PLM can encode the primary structure\ninto single representation and pair representation to learn the domain knowledge. The EvoFormer and Structure Module\nfrom AlphaFold2 are then integrated to process the representation, learn the geometric knowledge, and then predict the\ncoordinates of atoms. The two components are wired up to give an end-to-end differentiable model. HelixFold-Single\ncontains two training stages. In the Ô¨Årst stage, the large-scale PLM is trained with thousands of millions of unlabeled\nsingle sequences by the task of masked language prediction. In the second stage, we train the whole model with the\nprotein structures composed of experimental ground-truth and augmentation structures generated by AlphaFold2.\nWe compare HelixFold-Single with AlphaFold2 and RoseTTAFold on datasets CASP14 and CAMEO. HelixFold-\nSingle achieves competitive accuracy with those methods on proteins with sufÔ¨Åcient homologous sequences. We also\nanalyze the performance of HelixFold-Single on targets with various homologous sequences, and HelixFold-Single\nis capable of providing accurate structure predictions on most targets, especially the targets with large homologous\nfamilies. The ablation study comparing the PLMs of different sizes demonstrates the importance of the size of PLM\nfor structure prediction. Furthermore, HelixFold-Single shows great superiority in prediction efÔ¨Åciency compared\nwith the MSA-based methods and could be applied to protein-related tasks demanding a great number of predictions.\nThe code of HelixFold-Single is publicly released at GitHub https://github.com/PaddlePaddle/PaddleHelix/\ntree/dev/apps/protein_folding/helixfold-single. Web service of HelixFold-Single is also available at\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast to provide efÔ¨Åcient protein structure\npredictions.\n2 HelixFold-Single\nHelixFold-Single aims to take advantage of both the protein language model (PLM) and the main modules used in\nAlphaFold2 for single sequence-based protein structure prediction. As exhibited in Figure 1, HelixFold-Single consists\nof three components: PLM Base, Adaptor, and Geometric Modeling. A large-scale PLM Base is employed to encode\nthe co-evolution information in the parameters, which is used as an alternative to MSAs. Then, in Geometric Modeling,\nfollowing AlphaFold2, we use modiÔ¨Åed EvoFormer and Structure Module to sufÔ¨Åciently exchange the information\nbetween the single representations and pair representations to capture the geometric information and recover the 3D\ncoordinates of the atoms. We adopt an Adaptor layer to extract the co-evolution information from PLM to effectively\ngenerate the sequence and pair representations required as inputs to the Geometric modeling. The whole differentiable\npipeline is trained by both self-supervised pre-training with bulks of unlabeled single sequences and supervised learning\nwith geometric labels.\n2\nHelixFold-Single\nEvoformer\n(nEvoFormer blocks)\nStructure \nModule\n(nStructure blocks)\npair repr.\nsingle repr.\nProtein \nLanguage \nModel\n(nPLM blocks)\nAdaptor\nsingle repr. single repr.\npair repr.\nattention\nweights\nrecycle\nGeometric Modeling\n~300M primary \nsequences\n~120K determined \nstructures\n~1M estimated \nstructures\nPre-train with\nself-supervised learning\nHelixFold-Single\nGWEIPEPYVWDESFR\nPLM Base\nInput primary sequence\nTrain with\nsupervised learning\nFigure 1: The framework of HelixFold-Single with a protein language model as PLM Base, the compose of EvoFormer\nand Structure Module of AlphaFold2 as Geometric Modeling, and Adaptor to connect PLM Base and Geometric\nModeling.\n2.1 Large-Scale PLM Base\nInspired by large-scale pre-trained language models, we follow previous works on pre-training a protein language\nmodel (PLM). The PLM processes the primary protein sequences (i.e., the amino acid sequences) and extracts the\nknowledge needed for further geometric modeling. A protein of length Lcan be uniquely represented by a sequence of\ntypes of amino acids denoted by x = (x1,x2,...,x L). An embedding layer E(xl) maps the type id to dPLM-dimension\nembedding vectors:\nx(0) = (E(x1),E(x2),...,E (xL)).\nNotice that x(k) ‚ààRL√ódPLM is the representation of the amino acid sequence.\nWe then apply the widely used Transformer-style blocks ([3] to process the embedding vectors, denoted by\nx(k+1) = DisentangledAttentionTransformer(x(k)). (1)\nAccurately predicting the contacts between the residues, especially the long-rage contacts, is critical for protein structure\nprediction. Considering the contact between the residues is more dependent on the relative positions rather than\nthe absolute positions (counted from the start of the sequence), we employ DisentangledAttentionTransformer from\nDeBerTa [13] to focus on the modeling of interactions between the residue representations and the relative positions.\nDisentangledAttentionTransformer adopts the attention mechanism to learn the interactions between the residues as\nwell as the interactions of the interaction-position pairs.\nBesides, we take advantage of multi-head self-attention weights in DisentangledAttentionTransformer to construct the\ninitial pair representation. The attention weights of the k-th block is denoted by z(k) ‚ààRL√óL√óhPLM , where hPLM is the\nnumber of heads of self-attention.\nWe add an additional Adaptor to map the output of PLM Base to the input of Geometric Modeling module.\nÀúx(0) = Linear(x(nPLM)),\nÀúz(0) = Linear([z(1),z(2),¬∑¬∑¬∑ ,z(nPLM)]), (2)\nwhere nPLM is the number of blocks in PLM Base, and operator [] refers to concatenation. Àúx(0) ‚ààRL√ódSingle and\nÀúz(0) ‚ààRL√óL√ódPair are the initial single representations and pair representations of the Geometric Modeling module,\nrespectively.\n2.2 Geometric Modeling\nWe employ the EvoFormer and Structure Moduleproposed by AlphaFold2 [ 1] to model the relations between the\nresidues and then estimate the 3D coordinates of the atoms in the proteins. We slightly modify the original EvoFormer\n3\nHelixFold-Single\nand Structure Module‚Äôs architecture to match our settings. First, the original EvoFormer takes the MSA representation\nand pair representation, encoded from the searched MSAs, as input. As an alternative, we take the output of the Adaptor\n(including the single representations (Àúx(0)) and pair representations (Àúz(0))). Second, Evoformer adopts various attention\nmechanisms to exchange the information within the single and pair representations to learn the spatial relationships.\nNote that, compared with the original version of Evoformer proposed by AlphaFold2, we remove the column-wise\ngated self-attention because HelixFold-Single focuses on MSA-free protein structure prediction and is no need to\nexchange the messages within the MSAs. We follow the other geometric components of AlphaFold2, including the\nStructure Module that takes the single representation and pair representation yielded by the EvoFormer, and exploits\nInvariant Point Attention and other geometric transformation operators to end-to-end predict the 3D coordinates of\nthe atoms. Also, following AlpahFold2, we recycle the whole Geometric Modeling module to reÔ¨Åne the predicted\nstructures iteratively.\n2.3 Model Optimization\nFor the sake of leveraging the domain knowledge from the protein database, we operate two-stage parameter optimization\non HelixFold-Single.\nIn the Ô¨Årst stage, the PLM is pre-trained to capture the co-evolution information. The PLM is trained with about 300\nmillion of single sequences recorded in a protein database. To encourage PLM to observe the diverse single sequences\nas soon as possible, we cluster the proteins by the similarity of single sequences and sample the proteins to balance the\ndistributions of different clusters in our training data. We apply the self-supervised technique masked language model\n(MLM) to optimize the parameters of the PLM, by randomly masking 15% of residues in the single sequences and then\nreconstructing those masked residues. More concretely, MLM attempts to predict p(xl|x1,...,x l‚àí1,xM,xl+1,...,x L)\ngiven the residue in the l-th position xl being masked by xM. A crucial proposal of this work is that PLM can learn\nthe dependency between the masked residue and the other residues, and thus represent the co-evolution information.\nPrevious works [6] have already veriÔ¨Åed that PLMs can reveal secondary structures of the proteins, but little has been\ndiscussed on the relation between PLM and co-evolution. Co-evolution is the phenomenon that two residues in contact\ntend to evolve at the same time to preserve the structure and thus the function of the protein. In PLM, if a residue at\nanother position shas a profound impact (the residue at position sis changed, the masked residue will also change) on\nthe masked residue, then those two residues are likely to evolve at the same time.\nIn the second stage, since merely relying on PLM to predict the structure is inadequate to capture the geometric\ninformation, PLM Base and Geometric Modeling modules in HelixFold-Single are jointly optimized. We utilize\n100 thousand experimentally determined protein structures. We also use additional one million estimated protein\nstructures for training in this stage (distilled from AlphaFold2). Following AlphaFold2, we end-to-end train the network\nwith the main losses, including Frame Aligned Point Error (FAPE) loss and other auxiliary losses. By combining\nthe computational efÔ¨Åcient PLM Base module (compared with MSA search) and the Geometric Modeling module,\nHelixFols-Single is capable of providing efÔ¨Åcient and precise protein structure prediction.\n3 Results\n3.1 Datasets\nWe used UniRef30 (2021-03) [ 14] to pre-train the PLM, which clusters UniProtKB [ 15] sequences at the level of\n30% pairwise sequence identity. Then, three datasets are used to train the whole network, including the proteins in\nRCSB PDB [16, 17] released before 2020-05-14 and two self-distillation datasets constructed from Uniclust30 (version\n2018-08) and AlphaFold Protein Structure Database [18].\n3.2 Overall Comparison\nCASP14 [1, 19, 20] with 87 domain targets and CAMEO [21] with 371 targets collected from 2021-09-04 to 2022-02-19\nare used to compare the overall accuracy of HelixFold-Single with the several baseline structure prediction pipelines,\nincluding the MSA-based and MSA-free methods. AlphaFold2 [ 1] and RoseTTAFold [22] are currently the most\nadvanced methods for protein structure prediction, relying on MSAs to provide predictions. We test the accuracy of\nAlphaFold2 and RossTTAFold with and without homologous sequences, respectively. A commonly used metric, i.e.,\nTM-score [23], is exploited to evaluate the accuracy of HelixFold-Single and other methods.\nFigure 2 exhibits the test results of our proposed HelixFold-Single and the compared methods on CASP14 and CAMEO.\nFrom the results, we have the following observations:\n4\nHelixFold-Single\nFM FM/TBM TBM-easy TBM-hard All\nTM-score\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAlphaFold2 (input:MSA) RoseTTAFold (input:MSA) AlphaFold2 (input:single)\nRoseTTAFold (input:single) HelixFold-Single\n(a) CASP14 (87 targets classiÔ¨Åed into FM and TBM based on\ntheir relatedness to existing structures.)\ndepth<=100 100<depth<=1000 1000<depth<=1000 0 depth>10000 All\nTM-score\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAlphaFold2 (input:MSA) RoseTTAFold (input:MSA) AlphaFold2 (input:single)\nRoseTTAFold (input:single) HelixFold-Single\n(b) CAMEO (371 targets classiÔ¨Åed into four categories de-\npending on different MSA depths.)\nFigure 2: Overall comparison of HelixFold-Single and other methods onCASP14 and CAMEO. AlphaFold2 (input:MSA)\nand RoseTTAFold (input:MSA)are MSA-based methods, while the remaining use the primary structures as input.\n(1) In general, HelixFold-Single signiÔ¨Åcantly surpasses all the MSA-free methods on CASP14 and CAMEO and is\ncompetitive with the MSA-based methods in some cases. Notably, the accuracy of HelixFold-Single on CAMEO\nis comparable to that of AlphaFold2 (input:MSA)and outshines another strong baseline, RoseTTAFold (input:MSA).\nHelixFold-Single demonstrates the great potential of incorporating PLM into geometric modeling for protein structure\nprediction.\n(2) HelixFold-Single can be par with the MSA-based methods on the targets with large homologous families, e.g., TBM-\neasy domain targets in CASP14 with a median of seven homologous sequences and targets with more than a thousand\nhomologous sequences (MSA depth > 1000) in CAMEO. These results indicate that the accuracy of HelixFold-Single is\ncorrelated to the richness of homologous sequences, revealing that the large-scale PLM adopted by HelixFold-Single is\ncapable of embedding the information, e.g., co-evolution knowledge, of MSAs used by the MSA-based methods.\n(3) Compared HelidFold-Single with other MSA-free methods, HelixFold-Single exhibits its great superiority on all the\ncategories of CASP14 and CAMEO. Since AlphaFold2 and RoseTTAFold rely on MSAs as input during the training\nprocess, it is challenging for those methods to provide accurate predicts when taking only the single sequences as input.\n3.3 Effect of Number of Homologous Sequences\nThe results on CASP14 and CAMEO indicate that the accuracy of HelixFold-Single is related to the number of\nhomologous sequences. We further compare the performance of HelixFold-Single and other methods on the targets\nwith variant MSA depths. We collected the targets released between 2020-05 and 2021-10 from PDB, from which we\npicked the targets with relatively sparse homologous sequences. We blended those targets with the data of CASP14 and\nCAMEO as a new evaluation set. Figure 3a compares the TM-scores of HelixFold-Single and the baseline methods on\nthe evaluation set, grouped by the number of homologous sequences (MSA depths). Figure 3b shows the distribution of\nthe proteins in different groups in this evaluation set. We can see that as the available homologous sequences grow,\nthe average TM-score of both HelixFold-Single and the MSA-based methods increases, while the scores of the other\nMSA-free methods decrease. For the proteins with sparse homologous sequences, the TM-scores of all the compared\nmethods are unsatisfactory. For the proteins with larger homologous families, especially those with more than thousands,\nHelixFold-Single can compete with the MSA-based methods. Given that 90% of the targets in PDB have more than\n1024 homologous sequences, we can reasonably extrapolate that HelixFold-Single can achieve satisfying accuracy on\nthe most frequently investigated proteins.\nIn order to further investigate the relationship between the capacity of the PLM, the accuracy of protein structure\nprediction, and the size of the homologous family, we utilized the targets inCASP14 and CAMEO datasets to exhibit\ntheir relations, as shown in Figure 3c, Figure 3d, and Figure 3e. As we expected, from Figure 3c, a protein‚Äôs structure\naccuracy (TM-score) is correlated to the size of its homologous family (MSA depth), and the results are consistent\nwith those in Figure 3b. Besides, we use a probability metric, Perplexity [24], to indicate the capacity of the protein\nlanguage model. If the PLM can predict or reconstruct a protein sequence well, the Perplexity is low in predicting that\ntarget. From Figure 3d and Figure 3e, we can observe that the Perplexity of the PLM and the MSA depths are negatively\ncorrelated. The Perplexity of the PLM and the TM-scores of HelixFold-Single are also negatively correlated. The\nresults indicate that if the PLM Base module can well predict (model) a protein sequence, there is a high probability that\nthe PLM module can learn the co-evolution information of this protein and serves as an alternative to MSAs. Thus, the\n5\nHelixFold-Single\nMSA depth\n[1,4] (4,16] (16,64] (64,256] (256,1024] (1024,4096] (4096,16384] (16384,+‚àû)\nTM-score\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAlphaFold2(input:MSA) RoseTTAFold (input:MSA) AlphaFold2 (input:single) RoseTTAFold (input:single) HelixFold-Single\n(a) Comparison between HelixFold-Single and the baseline methods on protein\ntargets with a various numbers of homologous sequences (MSA depths).\n0.54% 0.74% 1.66% 2.88% 4.76%\n21.66%\n66.84%\n0.92%\n0.00%\n10.00%\n20.00%\n30.00%\n40.00%\n50.00%\n60.00%\n70.00%\n80.00%\n90.00%\n100.00%\n[1,4] (4,16] (16,64](64,256](256,1024](1024,4096](4096,16384](16384,+‚àû)\nProportion\nMSA depth\n(b) Distribution of proteins with different\nhomologous sequences in PDB.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1 10 100 1000 10000 100000 1000000\nTM-score\nMSA depth\nCASP14 CAMEO\n(c) Relations between proteins‚Äô MSA\ndepths and TM-scores\n1\n10\n100\n1000\n10000\n100000\n1000000\n0 10 20 30 40\nMSA depth\nPerplexity\nCASP14 CAMEO\n(d) Relations between Perplexity of PLM\nand MSA depths\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 5 10 15 20 25 30 35 40\nTM-score\nPerplexity\nCASP14 CAMEO\n(e) Relation between Perplexity of PLM\nand TM-scores of HelixFold-Single\nFigure 3: Analysis of the impact of homologous sequences (MSA depths) and investigation of the relations between\nMSA depths, TM-scores, and perplexity of the PLM.\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0 5 10 15 20 25 30 35 40\nPLM-100M\nPLM-1B\nCASP14 CAMEO\n0\n5\n10\n15\n20\n25\n30\nPerplexity\n1B_ppl 100M_ppl Á≥ªÂàó3 Á≥ªÂàó4\nÁ≥ªÂàó5 1B_ppl 100M_ppl\nCASP14 CAMEO\n(a) Perplexity of PLM-1B and PLM-100M\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.2 0.4 0.6 0.8 1\nPLM-100M\nPLM-1B\nCASP14 CAMEO\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nP@L/5\n1 1 Á≥ªÂàó3 Á≥ªÂàó4\nÁ≥ªÂàó5 1B_p@L/5 1\nCASP14 CAMEO (b) Contact prediction of PLM-1B and PLM-100M\nFigure 4: Comparison of PLMs of different sizes.\nGeometric Modeling module can leverage the co-evolution embedded in the PLM to provide a more accurate structure\nfor that protein.\n3.4 Effect of the Sizes of the PLMs\nTo comprehensively study the ability of the PLMs of different sizes to learn the co-evolution information, we compare a\npre-trained PLM of 1B parameters (denoted by PLM-1B) and another pre-trained PLM of 100M (denoted by PLM-\n100M). Table 4a exhibits the Perplexity of PLM-1B and PLM-100M of the targets from datasets CASP14 and CAMEO.\nIn general, the smaller the perplexity is, the stronger the capacity of the PLM is. Thus, PLM-1B with more model\nparameters performs better than PLM-100M with fewer parameters on both datasets CASP14 and CAMEO. In addition,\nwe apply the PLM-1B and PLM-100M on the task of protein residue contact prediction to compare their performance on\n6\nHelixFold-Single\nthe downstream tasks. We simply Ô¨Åt a logistic regression that takes the attention weights, i.e., [z(1),z(2),¬∑¬∑¬∑ ,z(nPLM)],\nfrom the PLMs as input and predict the contact of residues on the targets in datasets CASP14 and CAMEO. Following\n[6, 25], we use top L/5 long-range contact precision, denoted by P@L/5, as the evaluation metric, and the results are\nshown in Figure 4b. As we can see, PLM-1B is signiÔ¨Åcantly superior to PLM-100M on the contact prediction task. The\nresults from Figure 4a and Figure 4b both support the hypothesis that the larger the size of the PLM is, the stronger its\ncapacity is. Therefore, it can be reasonably inferred that the performance of the PLM will continue to improve as the\nsize of the PLM increases to a larger size.\n3.5 Prediction Speed Comparison\n1.0\n10.0\n100.0\n1000.0\n[1,100] (100,200] (200,400] (400,800] (800,+‚àû)\nMedian time (sec)\nProtein length\nSea rch MSA AlphaFold2 HelixFold-Single\nFigure 5: Median times of MSA search, AlphaFold2, and HelixFold-Single on proteins with various lengths.\nMassive time consumption for searching MSAs is one of the bottlenecks of the MSA-based folding, and accelerating\nthe speed of protein structure prediction can considerably broader its applications. The MSA-free HelixFold-Single has\na tremendous advantage for inference efÔ¨Åciency for exempting MSA searching. Figure 5 exhibits the computation time\ncost of 1. MSA searching; 2. Whole inference pipeline of AlphaFold2; 3. Inference of HelixFold-Single. All the tests\nare executed in a single NVIDIA A100(40G) GPU. In general, Helixfold-Single consumes much less time than the\nAlphafold2, while AlphaFold2 pipeline spends most of its time in MSA searching. For proteins less than 100 in length,\nHelixFold-Single‚Äôs prediction time is only about one-thousandth of that of AlphaFold2. Even for the proteins with more\nthan 800 amino acids, HelixFold-Single still has great efÔ¨Åciency superiority. The high efÔ¨Åciency of HelixFold-Single\ndemonstrates the potential of its application in tasks with a great demand for structural prediction.\n3.6 Case Study\nR66 \nR29 \nE36 \nR66 \nE36 \nR29 \n(a) 7KWT:B, AlphaFold2,\nTM-score=0.2425\nR66 \nR29 \nE36 \nR66 \nR29 \nE36 \n(b) 7KWT:B, HelixFold-\nSingle, TM-score=0.8066\nR56 \nR106 \nR121 \nR123 R56 \nR106 \nR121 \nR123 (c) 7BCJ:A, AlphaFold2, TM-\nscore=0.2914\nR56 \nR106 \nR121 \nR123 \nR106 \nR56 \nR121 \nR123 \n(d) 7BCJ:A, HelixFold-Single,\nTM-score=0.6420\nFigure 6: HelixFold-Single predicts PlyC and RoxP structure more accurately than AlphaFold2. PlyC structures\npredicted by (a) AlphaFold2 and (b) HelixFold-Single is aligned with the reference structure (PDB ID: 7KWT, chain B);\nRoxP structure predicted by (c) AlphaFold2 and (d) HelixFold-Single is aligned with the reference structure (PDB ID:\n7BCJ, chain A). A-D) Green: structure predicted by AlphaFold2. Magentas: structure predicted by HelixFold-Single.\nCyan: reference crystal structure measured by X-RAY diffraction approach (resolution<1.8A). Key residues related to\nprotein function are shown as sticks.\nMost proteins exert their functions by interacting with other molecules. Changes in the structure of a protein, especially\nthose in the key interacting residues, can signiÔ¨Åcantly affect its biological function. As a result, a protein‚Äôs function is\n7\nHelixFold-Single\nclosely associated with its structure, and accurately predicting the structure would facilitate our understanding of its\nbiological role. While AlphaFold2 achieves outstanding accuracy in most of the protein structure prediction tasks, its\nperformance can still be poor in some situations. Here, we demonstrate that HelixFold-Single complements AlphaFold2\nin several of these cases. Endolysin enzymes from bacteriophages cause bacterial lysis by degrading the peptidoglycan\ncell wall. The streptococcal C1 phage endolysin PlyC is the most potent endolysin and can rapidly lyse group A, C, and\nE streptococci. Study on PlyC structure revealed that the key residues, including R66, E36, R29, etc, are important for\nthe binding of PlyC to its target and hence are critical to its function [26]. However, AlphaFold2 failed to produce the\nreliable structure of the protein (Figure 6(a)). This is probably due to insufÔ¨Åcient co-evolution information extracted\nfrom MSAs. In contrast, the structure predicted by HelixFold-Single (Figure 6(b)) more closely resembles the one\nmeasured by the experiment, likely attributed to its little dependence on the information from MSAs. A similar result\nis observed for another protein RoxP. This protein is produced by Cutibacterium acnes, a predominant bacterium on\nhuman skin, and was shown to alleviate radical-induced cell damage. The key residues R56, R106, R121, R123 on\nRoxP form a positively charged groove, which acts as the binding site for substrate and cofactors [27]. HelixFold-Single\naccurately predicts the formation of the positively charged groove(Figure 6(d)), which is not observed in the structure\npredicted by AlphaFold2 (Figure 6(c)). Furthermore, the TM-score of HelixFold-Single for RoxP is much higher\nthan that of AlphaFold2, suggesting an overall better performance of HelixFold-Single in predicting RoxP structure.\nAltogether, our case studies indicate that HelixFold-Single outperforms AlphaFold2 in some situations and can be used\nas a reliable tool to analyze the function of proteins without known X-RAY structures.\n4 Related Works\n4.1 Protein Language Models\nLarge-scale language models [3] with the self-supervised learning (SSL) paradigm, such as masked language model\n(MLM) [4] and auto-regression [26], have achieved extraordinary success in Natural Language Processing (NLP) tasks.\nRecent progress has revealed that their capabilities are deeply related to the scale of the model parameters: the larger\nthe scale of the parameters, the better the performance [5]. The community has not yet seen a sign of stopping growth\nby moving from billions to hundreds of billions of parameters. Those language models are capable of memorizing\nand generalizing massive common-sense knowledge and professional expertise implicitly included in the large-scale\nunlabeled data. Inspired by those achievements, Protein Language Models (PLMs) tried to transfer language models\nand SSL tasks to protein modeling. A protein can be represented by an amino acid sequence, similar to the sequences of\nwords or tokens in NLP. Previous works [6, 7, 8, 9] have shown that by pre-training with only single sequences without\nmuch supervision, protein language models can reveal the protein classiÔ¨Åcation, stability, and lower-level structure\ninformation (including secondary, tertiary structures and 2D contact maps). However, the accuracy of these models\nin structure prediction is still far from that of the mainstream folding models supervised by the ground-truth protein\nstructure.\n4.2 Protein Structure Prediction\nMainstream pipelines [27, 28, 29, 30] rely on extracting the co-evolution information from Multiple Sequence Align-\nments (MSAs) to predict the protein structures. Earlier works manually designed the features derived from MSAs, such\nas inverse covariance matrices of MSAs. Then, deep neural networks (DNNs), e.g., convolutional networks, are utilized\nto model the relations between the residues. Advanced studies [1, 29], directly take the MSAs as input and apply DNNs\nto predict the 3D coordinates of the proteins. Particularly, the appearance of AlphaFold2 [1] has dramatically narrowed\nthe accuracy gap between the experimentally determined structures and model estimated structures, employing the\nEvoFormer module to enhance the interaction between MSA sequences and pairwise geometric information and the\nStructure module to directly predict the atoms‚Äô coordinates. However, the reliance on MSA inevitably impedes the\ncomputation efÔ¨Åciency and accurate prediction of orphan proteins and designed proteins, as well as downstream tasks\nsuch as protein design.\nAlthough the structure of a protein is dependent on its primary structure, it is incredibly challenging to train an accurate\nmodel that can infer the protein structures with only the primary structures. Only a small number of samples, i.e.,\nexperimentally determined structures recorded in the PDB database, are available for model training. Several works\nattempt to incorporate the protein language models (PLMs) for MSA-free protein structure prediction. RGN2 [ 10]\nemploys a protein language model (AminoBERT) with a recurrent geometric network that utilizes Frenet-Serret frames\nto generate the backbone structure. Besides, advanced studies [11, 12] combine pre-trained PLMs, such as ProT5 [7]\nand ESM-1b [31], with ResNets to predict 2D structures, e.g., contact map of a protein, yielding superior performance\nin orphan proteins. Nonetheless, the overall accuracy of those works is still unsatisfactory due to the limited capacity of\nthe used model architectures.\n8\nHelixFold-Single\n5 Conclusion and Future Work\nOn the one hand, mainstream protein structure prediction methods, such as AlphaFold2 and RoseTTAFold, rely on the\nMSAs to extract the homologous information. However, searching MSAs is time-consuming, limiting the application\nof those methods to broader protein-related tasks. On the other hand, the large-scale protein language model learns\nthe protein correlations from a great number of unlabeled proteins through self-supervised learning tasks. By utilizing\nlarge-scale parameters to embed the homologous information, we prove it can be used as an alternative to MSAs to\nreduce the time consumption required by the protein structure prediction methods. HelixFold-Single attempts to take\nadvantage of both the protein language model and the geometric modeling, end-to-end predicting the protein structures\nwith only the primary structures. HelixFold-Single can be par with the MSA-based methods on targets with large\nhomologous families and is much more efÔ¨Åcient than the MSA-based methods, demonstrating its application prospect\nfor protein study.\nIn the future, as the experimental results indicate that the larger size of the PLM can achieve superior performance,\nwe will continue investigating the PLM with a larger size for protein structure prediction. In addition, the accuracy of\nthe targets with only a few homologous sequences is still unsatisfactory. Thus we will try to introduce more diverse\ntraining data to alleviate this problem.\n6 Data Availability\nTo pre-train the PLM, UniRef30 (2021-03) is publicly available in https://wwwuser.gwdg.de/~compbiol/\nuniclust/2021_03/. While to train the whole network, RCSB PDB can be downloaded in https://www.rcsb.\norg/docs/programmatic-access/file-download-services and AlphaFold Protein Structure Database as the\ndistillation dataset can be downloaded in https://ftp.ebi.ac.uk/pub/databases/alphafold/v2/.\n7 Code Availability\nThe source code, trained weights and inference code of HelixFold-Single are freely available at GitHub https://\ngithub.com/PaddlePaddle/PaddleHelix/tree/dev/apps/protein_folding/helixfold-single to ensure\nthe reproduction of our experimental results.\n9\nHelixFold-Single\nAppendix A: Training and Evaluation Data\nTraining Data\nUniRef30 (2021-03) [14], containing about 260 millions protein sequences is utilized to pre-train the PLM, clustering\nUniProtKB [15] sequences at the level of 30% pairwise sequence identity.\nThree datasets are utilized to train HelixFold-Single for MSA-free protein strcture prediction.\n‚Ä¢ RCSB PDB [16, 17]:The targets released before 2020-05-14 in PDB are used to train HelixFold-Single. We\nÔ¨Ålter out the targets with resolution larger than 3√Ö and whose number of amino acids less than 10. The targets\nare clustered at 40% sequence identity cutoff.\n‚Ä¢ Distillation-Uniclust30: We inference the structures of the targets in Uniclust30 (version 2018-08) by\nAlphaFold2 for self-distillation. We follow the data-prepossess procedure reported in AlphaFold2. Further,\nthe target structures with average pLDDT less than 0.5 are Ô¨Åltered out. Then, the targets are clustered at 30%\nsequence identity cutoff.\n‚Ä¢ Distillation-EBI: About one million protein structures are extracted from AlphaFold Protein Structure\nDatabase [ 18]. We removed the protein structures with average pLDDT less than 0.5. The remaining\ntargets are clustered at 50% sequence identity cutoff.\nEvaluation Data\nWe exploit three datasets to evaluate the accuracy and efÔ¨Åciency of HelixFold-Single and the baseline methods.\n‚Ä¢ CASP14: 61 targets are collected from CASP14 [19, 20] for overall evaluation, which includes 87 domains\nwith classiÔ¨Åcation of FM (free modeling), TBM-easy (easy template-based modeling), TBM-hard (hard\ntemplate-based modeling) and FM/TBM (modeling with only remote structural homologies).\n‚Ä¢ CAMEO: We collect 371 targets from CAMEO [21] between 2021-09-04 and 2022-02-19, which consists of\nvarious target difÔ¨Åculties including Easy, Medium, and Hard.\n‚Ä¢ MSA Depth Test:We create a test set obtained from RCSB PDB, including 793 targets with a wide range of\ndifferent MSA depths from 2020-05 to 2021-10, especially the targets with only a few homologous sequences.\nThis test set is combined with datasets CASP14 and CAMEO to investigate the effect of the number of\nhomologous sequences.\nAppendix B: Detailed Settings of HelixFold-Single\nTraining Settings\nThe implementation of HelixFold-Single is based on our previous work, HelixFold [32], and we use 128 NVIDIA A100\nGPUs to train HelixFold-Single. Table 1 exhibits the architecture setting of HelixFold-Single. We train two version\nof PLMs for ablation study. To balance the computation costs of multiple GPUs for pre-training, the batch size used\nin each GPU is dynamically adjusted according to the lengths of protein sequences. We use AdamW optimizer [33]\nwith learning rate of 5e-4, Œ≤1 = 0.9, Œ≤2 = 0.999, weight decay of 0.01, learning rate warm-up over the Ô¨Årst 30,000\nsteps. When training the whole network for protein structure estimation, we use Adam optimizer [34] to optimize the\nparameters. We apply two stages of training: initial training stage and Ô¨Åne-tuning stage. In the initial training stage, the\nlearning rate is set to be 1e-3 and the lengths of the input protein sequences are cropped to be 256. In the Ô¨Åne-tuning\nstage, we use learning rate of 2e-4 and the lengths of the input protein sequences are cropped to be 384. Gradient\nclipping by the global norm [35] is adopted on the parameters with a clipping value of 1.0.\nModel Architecture\nPLM Base\nAs shown in Figure 7, PLM Base is mainly based on DeBerTa [ 13]. We make two slight modiÔ¨Åcations: (1) To\nstabilize the pre-training of PLM, instead of using Post-Norm in DeBerTa, Pre-Norm [36] is applied in PLM Base of\nHelixFold-Single. (2) We Ô¨Ånd that using residue-to-position and residue-to-residue (Equation 3) is enough, while the\n10\nHelixFold-Single\nTable 1: Architecture setting of HelixFold-Single.\nComponents Model size Layer num Hidden size Intermediate size Head num\nPLM-1B 1.09B nPLM = 20 dPLM = 2048 8192 hPLM = 16\nPLM-100M 100M nPLM = 12 dPLM = 768 3072 hPLM = 12\nEvoFormer 87M nEvoFormer = 24 dSingle = 512\ndPair = 64\nStructure Module 1.7M nStructure = 8 dStructure = 384\nDisentangledAttention\nLayerNorm\nFeedForward\nLayerNorm\nùë•(\")\nùë•$%\nùë•&'(\nùë•(\")*)\nFigure 7: Architecture of DisentangledAttentionTransformer (superscript kdenotes the layer id).\nperformance gain by adding position-to-residue is trivial. Thus, we left out the position-to-residue term in DeBerTa. As\na result, we have the DisentangledAttention layer denoted by\nq = xinWq, k = xinWk, v = xinWv, p = epWp\nAi,j = qikT\njÓ¥ô Ó¥òÓ¥ó Ó¥ö\n(a) residue-to-residue\n+ qipT\nŒ¥(i,j)\nÓ¥ô Ó¥òÓ¥ó Ó¥ö\n(b) residue-to-position\nxout = softmax( A‚àö2dPLM\n)v\n(3)\nW‚àó are trainable parameters; ep is the trainable position embedding; Œ¥(i,j) denotes the relative distance between\nposition iand j.\nGeometric Modeling\nSince HelixFold-Single takes only the single sequence as input, we slightly modify the architecture of Evoformer,\nremoving the columnwise attention. The architecture of the revised Evoformer is shown in Figure 8.\n11\nHelixFold-Single\nRevised Evoformer block\nRow-wise gated \nattention with \npair bias\nTransition\nTriangle update \nusing ‚Äúoutgoing‚Äù \nedges\nTriangle self-\nattention around \nstarting node\nOuter\nproduct\nPair\nrepresentation\n(L, L, dPair)\n√ó\tnEvoformer\nTriangle update \nusing ‚Äúingoing‚Äù \nedges\nTriangle self-\nattention around \nending node\nPair\nrepresentation\n(L, L, dPair)\nSingle\nrepresentation\n(L, dSingle)\nSingle\nrepresentation\n(L, dSingle)\nFigure 8: Architecture of revised Evoformer.\nTable 2: Median times (seconds) of MSA search, AlphaFold2, and HelixFold-Single on proteins with various lengths,\ndetailed results corresponding to Figure 5.\n[1,100] (100, 200] (200, 400] (400, 800] (800, +‚àû)\nMSA search 737.5 755.4 853.7 977.0 1203.8\nAlphaFold2 766.1 795.8 908.3 1125.2 1611.2\nHelixFold-Single 1.5 1.5 2.1 6.2 37.5\n12\nHelixFold-Single\nReferences\n[1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn\nTunyasuvunakool, Russ Bates, Augustin ≈Ω√≠dek, Anna Potapenko, et al. Highly accurate protein structure\nprediction with alphafold. Nature, 596(7873):583‚Äì589, 2021.\n[2] John Moult. A decade of casp: progress, bottlenecks and prognosis in protein structure prediction. Current\nopinion in structural biology, 15(3):285‚Äì289, 2005.\n[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[4] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of NAACL-HLT, pages 4171‚Äì4186, 2019.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877‚Äì1901, 2020.\n[6] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun\nSong. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32,\n2019.\n[7] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs,\nTamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: towards cracking the language of life‚Äôs code\nthrough self-supervised deep learning and high performance computing. arXiv preprint arXiv:2007.06225, 2020.\n[8] Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer protein language\nmodels are unsupervised structure learners. Biorxiv, 2020.\n[9] Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, and Jie Tang. Modeling protein using large-scale pretrain\nlanguage model. arXiv preprint arXiv:2108.07435, 2021.\n[10] Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Charlotte Rochereau, George M Church, Peter Karl Sorger,\nand Mohammed N AlQuraishi. Single-sequence protein structure prediction using language models from deep\nlearning. bioRxiv, 2021.\n[11] Konstantin Wei√üenow, Michael Heinzinger, and Burkhard Rost. Protein language-model embeddings for fast,\naccurate, and alignment-free protein structure prediction. Structure, 2022.\n[12] Wenkai Wang, Zhenling Peng, and Jianyi Yang. Single-sequence protein structure prediction using supervised\ntransformer protein language models. bioRxiv, 2022.\n[13] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentan-\ngled attention. In International Conference on Learning Representations, 2020.\n[14] Milot Mirdita, Lars V on Den Driesch, Clovis Galiez, Maria J Martin, Johannes S√∂ding, and Martin Steinegger.\nUniclust databases of clustered and deeply annotated protein sequences and alignments. Nucleic acids research,\n45(D1):D170‚ÄìD176, 2017.\n[15] Baris E. Suzek, Yuqi Wang, Hongzhan Huang, Peter B. McGarvey, Cathy H. Wu, and the UniProt Consor-\ntium. UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches.\nBioinformatics, 31(6):926‚Äì932, 11 2014.\n[16] Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weissig, Ilya N. Shindyalov,\nand Philip E. Bourne. The Protein Data Bank. Nucleic Acids Research, 28(1):235‚Äì242, 01 2000.\n[17] Stephen K Burley, Charmi Bhikadiya, Chunxiao Bi, Sebastian Bittrich, Li Chen, Gregg V Crichlow, Cole H\nChristie, Kenneth Dalenberg, Luigi Di Costanzo, Jose M Duarte, Shuchismita Dutta, Zukang Feng, Sai Ganesan,\nDavid S Goodsell, Sutapa Ghosh, Rachel Kramer Green, Vladimir Guranovi¬¥c, Dmytro Guzenko, Brian P Hudson,\nCatherine L Lawson, Yuhe Liang, Robert Lowe, Harry Namkoong, Ezra Peisach, Irina Persikova, Chris Randle,\nAlexander Rose, Yana Rose, Andrej Sali, Joan Segura, Monica Sekharan, Chenghua Shao, Yi-Ping Tao, Maria\nV oigt, John D Westbrook, Jasmine Y Young, Christine Zardecki, and Marina Zhuravleva. RCSB Protein Data\nBank: powerful new tools for exploring 3D structures of biological macromolecules for basic and applied research\nand education in fundamental biology, biomedicine, biotechnology, bioengineering and energy sciences. Nucleic\nAcids Research, 49(D1):D437‚ÄìD451, 11 2020.\n[18] Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova,\nDavid Yuan, Oana Stroe, Gemma Wood, Agata Laydon, Augustin ≈Ω√≠dek, Tim Green, Kathryn Tunyasuvunakool,\nStig Petersen, John Jumper, Ellen Clancy, Richard Green, Ankur V ora, Mira LutÔ¨Å, Michael Figurnov, Andrew\n13\nHelixFold-Single\nCowie, Nicole Hobbs, Pushmeet Kohli, Gerard Kleywegt, Ewan Birney, Demis Hassabis, and Sameer Velankar.\nAlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space\nwith high-accuracy models. Nucleic Acids Research, 50(D1):D439‚ÄìD444, 11 2021.\n[19] Lisa N. Kinch, R. Dustin Schaeffer, Andriy Kryshtafovych, and Nick V . Grishin. Target classiÔ¨Åcation in the\n14th round of the critical assessment of protein structure prediction (casp14). Proteins: Structure, Function, and\nBioinformatics, 89(12):1618‚Äì1632, 2021.\n[20] Andriy Kryshtafovych, Torsten Schwede, Maya Topf, Krzysztof Fidelis, and John Moult. Critical assessment of\nmethods of protein structure prediction (casp)‚Äîround xiv. Proteins: Structure, Function, and Bioinformatics,\n89(12):1607‚Äì1617, 2021.\n[21] Xavier Robin, Juergen Haas, Rafal Gumienny, Anna Smolinski, Gerardo Tauriello, and Torsten Schwede. Contin-\nuous automated model evaluation (cameo)‚Äîperspectives on the future of fully automated evaluation of structure\nprediction methods. Proteins: Structure, Function, and Bioinformatics, 89(12):1977‚Äì1986, 2021.\n[22] Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue\nWang, Qian Cong, Lisa N. Kinch, R. Dustin Schaeffer, Claudia Mill√°n, Hahnbeom Park, Carson Adams, Caleb R.\nGlassman, Andy DeGiovanni, Jose H. Pereira, Andria V . Rodrigues, Alberdina A. van Dijk, Ana C. Ebrecht,\nDiederik J. Opperman, Theo Sagmeister, Christoph Buhlheller, Tea Pavkov-Keller, Manoj K. Rathinaswamy, Udit\nDalwadi, Calvin K. Yip, John E. Burke, K. Christopher Garcia, Nick V . Grishin, Paul D. Adams, Randy J. Read,\nand David Baker. Accurate prediction of protein structures and interactions using a three-track neural network.\nScience, 373(6557):871‚Äì876, 2021.\n[23] Yang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure template quality.\nProteins: Structure, Function, and Bioinformatics, 57(4):702‚Äì710, 2004.\n[24] Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, Jennifer C Lai, and Robert L Mercer. An estimate\nof an upper bound for the entropy of english. Computational Linguistics, 18(1):31‚Äì40, 1992.\n[25] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander\nRives. Msa transformer. In International Conference on Machine Learning, pages 8844‚Äì8856. PMLR, 2021.\n[26] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\ngenerative pre-training. 2018.\n[27] Jianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov, and David Baker. Improved\nprotein structure prediction using predicted interresidue orientations. Proceedings of the National Academy of\nSciences, 117(3):1496‚Äì1503, 2020.\n[28] Jianyi Yang, Renxiang Yan, Ambrish Roy, Dong Xu, Jonathan Poisson, and Yang Zhang. The i-tasser suite:\nprotein structure and function prediction. Nature methods, 12(1):7‚Äì8, 2015.\n[29] Zongyang Du, Hong Su, Wenkai Wang, Lisha Ye, Hong Wei, Zhenling Peng, Ivan Anishchenko, David Baker,\nand Jianyi Yang. The trrosetta server for fast and accurate protein structure prediction. Nature protocols,\n16(12):5634‚Äì5651, 2021.\n[30] Jian Peng and Jinbo Xu. Raptorx: exploiting structure information for protein alignment by statistical inference.\nProteins: Structure, Function, and Bioinformatics, 79(S10):161‚Äì171, 2011.\n[31] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott,\nC Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning\nto 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.\n[32] Guoxia Wang, Xiaomin Fang, Zhihua Wu, Yiqun Liu, Yang Xue, Yingfei Xiang, Dianhai Yu, Fan Wang,\nand Yanjun Ma. Helixfold: An efÔ¨Åcient implementation of alphafold2 using paddlepaddle. arXiv preprint\narXiv:2207.05477, 2022.\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations, 2018.\n[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n[35] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difÔ¨Åculty of training recurrent neural networks. In\nInternational conference on machine learning, pages 1310‚Äì1318. PMLR, 2013.\n[36] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,\nLiwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference\non Machine Learning, pages 10524‚Äì10533. PMLR, 2020.\n14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7300595045089722
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5111749768257141
    },
    {
      "name": "Machine learning",
      "score": 0.5056807994842529
    },
    {
      "name": "Protein structure prediction",
      "score": 0.49486202001571655
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4808974266052246
    },
    {
      "name": "Code (set theory)",
      "score": 0.4784441292285919
    },
    {
      "name": "Protein sequencing",
      "score": 0.43661078810691833
    },
    {
      "name": "CASP",
      "score": 0.4280036389827728
    },
    {
      "name": "Data mining",
      "score": 0.40100154280662537
    },
    {
      "name": "Protein structure",
      "score": 0.3635091781616211
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.27717310190200806
    },
    {
      "name": "Peptide sequence",
      "score": 0.14661851525306702
    },
    {
      "name": "Biology",
      "score": 0.08838769793510437
    },
    {
      "name": "Programming language",
      "score": 0.08714085817337036
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    }
  ]
}