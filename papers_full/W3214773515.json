{
  "title": "Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers",
  "url": "https://openalex.org/W3214773515",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226517832",
      "name": "Guibas, John",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226517833",
      "name": "Mardani, Morteza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2559426286",
      "name": "Li, Zongyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2983731418",
      "name": "Tao, Andrew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202058456",
      "name": "Anandkumar, Anima",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2988481184",
      "name": "Catanzaro, Bryan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W1622444467",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W3137474564",
    "https://openalex.org/W3202621745",
    "https://openalex.org/W3174980028",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3172643943",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3194436063",
    "https://openalex.org/W3128976935",
    "https://openalex.org/W3092923133",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3134226059",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3165088525",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W967544008",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2970100546",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W3162090017",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W3186979696",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W3010501116",
    "https://openalex.org/W3181262653",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2135046866",
    "https://openalex.org/W3167788848",
    "https://openalex.org/W3197308578",
    "https://openalex.org/W3034573343"
  ],
  "abstract": "Vision transformers have delivered tremendous success in representation learning. This is primarily due to effective token mixing through self attention. However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer that learns to mix in the Fourier domain. AFNO is based on a principled foundation of operator learning which allows us to frame token mixing as a continuous global convolution without any dependence on the input resolution. This principle was previously used to design FNO, which solves global convolution efficiently in the Fourier domain and has shown promise in learning challenging PDEs. To handle challenges in visual representation learning such as discontinuities in images and high resolution inputs, we propose principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. The resulting model is highly parallel with a quasi-linear complexity and has linear memory in the sequence size. AFNO outperforms self-attention mechanisms for few-shot segmentation in terms of both efficiency and accuracy. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of 65k and outperforms other efficient self-attention mechanisms.",
  "full_text": "Published as a conference paper at ICLR 2022\nADAPTIVE FOURIER NEURAL OPERATORS : E FFICIENT\nTOKEN MIXERS FOR TRANSFORMERS\nJohn Guibas3∗, Morteza Mardani1∗, Zongyi Li1,2, Andrew Tao1,\nAnima Aanandkumar1,2, Bryan Catanzaro1\nNVIDIA1, California Institute of Technology2, Stanford University3\njtguibas@stanford.edu,\n{mmardani,zongyil,atao,bcatanzaro,aanandkumar}@nvidia.com\nABSTRACT\nVision transformers have delivered tremendous success in representation learning.\nThis is primarily due to effective token mixing through self-attention. However,\nthis scales quadratically with the number of pixels, which becomes infeasible for\nhigh-resolution inputs. To cope with this challenge, we propose Adaptive Fourier\nNeural Operator (AFNO) as an efﬁcient token mixer that learns to mix in the\nFourier domain. AFNO is based on a principled foundation of operator learning\nwhich allows us to frame token mixing as a continuous global convolution with-\nout any dependence on the input resolution. This principle was previously used\nto design FNO, which solves global convolution efﬁciently in the Fourier domain\nand has shown promise in learning challenging PDEs. To handle challenges in vi-\nsual representation learning such as discontinuities in images and high resolution\ninputs, we propose principled architectural modiﬁcations to FNO which results in\nmemory and computational efﬁciency. This includes imposing a block-diagonal\nstructure on the channel mixing weights, adaptively sharing weights across to-\nkens, and sparsifying the frequency modes via soft-thresholding and shrinkage.\nThe resulting model is highly parallel with a quasi-linear complexity and has lin-\near memory in the sequence size. AFNO outperforms self-attention mechanisms\nfor few-shot segmentation in terms of both efﬁciency and accuracy. For Cityscapes\nsegmentation with the Segformer-B3 backbone, AFNO can handle a sequence size\nof 65k and outperforms other self-attention mechanisms. Code is available1.\n1 I NTRODUCTION\nFigure 1: Parameter count and mIoU for Segformer,\nSwin, and other models at different scales. AFNO con-\nsistently outperforms other mixers (see Section 5.7).\nVision transformers have recently shown\npromise in producing rich contextual represen-\ntations for recognition and generation tasks.\nHowever, a major challenge is posed by long\nsequences from high resolution images and\nvideos. Here, long-range and multiway depen-\ndencies are crucial to understand the composi-\ntionality and relationships among the objects in\na scene. A key component for the effective-\nness of transformers is attributed to proper mix-\ning of tokens. Finding a good mixer is how-\never challenging as it needs to scale with the\nsequence size, and systematically generalize to\ndownstream tasks.\nRecently, there has been extensive research to\nﬁnd good token mixers; see e.g., Tay et al.\n(2020b) and references therein. The origi-\nnal self-attention imposes graph structures, and\nuses the similarity among the tokens to capture\n∗Joint ﬁrst authors, contributed equally. The ﬁrst author has done this work during internship at NVIDIA,\nand the second author was leading the project. 1 Code: github.com/jtguibas/AdaptiveFourierNeuralOperator.\n1\narXiv:2111.13587v2  [cs.CV]  27 Mar 2022\nPublished as a conference paper at ICLR 2022\nFigure 2: The multi-layer transformer network with FNO, GFN, and AFNO mixers. GFNet performs element-\nwise matrix multiplication with separate weights across channels ( k). FNO performs full matrix multiplica-\ntion that mixes all the channels. AFNO performs block-wise channel mixing using MLP along with soft-\nthresholding. The symbols h, w, d, and krefer to the height, width, channel size, and block count, respectively.\nModels Complexity (FLOPs) Parameter Count Interpretation\nSelf-Attention N2d+ 3Nd2 3d2 Graph Global Conv.\nGFN Nd + Ndlog N Nd Depthwise Global Conv.\nFNO Nd2 + Ndlog N Nd 2 Global Conv.\nAFNO (ours) Nd2/k+ Ndlog N (1 + 4/k)d2 + 4d Adaptive Global Conv.\nTable 1: Complexity, parameter count, and interpretation for FNO, AFNO, GFN, and Self-Attention. N :=\nhw, d, and krefer to the sequence size, channel size, and block count, respectively.\nthe long-range dependencies Vaswani et al. (2017); Dosovitskiy et al. (2020) . It is parameter efﬁ-\ncient and adaptive, but suffers from a quadratic complexity in the sequence size. To achieve efﬁcient\nmixing with linear complexity, several approximations have been introduced for self-attention; see\nSection 2. These approximations typically compromise accuracy for the sake of efﬁciency. For\ninstance, long-short (LS) transformer aggregates a long-range attention with dynamic projection to\nmodel distant correlations and a short-term attention to capture local correlations Zhu et al. (2021).\nLong range dependencies are modeled in low dimensions, which can limit expressiveness.\nMore recently, alternatives have been introduced for self-attention that relax the graph assumption\nfor efﬁcient mixing. Instead, they leverage the geometric structuresusing Fourier transform Rao\net al. (2021); Lee-Thorp et al. (2021). For instance, the Global Filter Networks (GFN) proposes\ndepthwise global convolution for token mixing that enjoys an efﬁcient implementation in the Fourier\ndomain Rao et al. (2021). GFN mainly involves three steps:(i) spatial token mixing via fast Fourier\ntransform (FFT); (ii) frequency gating; and (iii) inverse FFT for token demixing. GFN however\nlacks adaptivity and expressiveness at high resolutions since the parameter count grows with the\nsequence size, and no channel mixing is involved in (ii).\nOur Approach. To address these shortcomings, we frame token mixing as operator learning that\nlearns mappings between continuous functions in inﬁnite dimensional spaces. We treat tokens as\ncontinuous elements in the function space, and model token mixing as continuous global convolu-\ntion, which captures global relationships in the geometric space. One way to solve global convolu-\ntion efﬁciently is through FFT. More generally, we compose such global convolution operations with\nnonlinearity such as ReLU to learn any general non-linear operator. This forms the basis for design-\ning Fourier Neural operators (FNOs) which has shown promise in solving PDEs Li et al. (2020a).\nWe thus adopt FNO as a starting point for designing efﬁcient token mixing.\nDesigning AFNO. Adapting FNO from PDEs to vision needs several design modiﬁcations. Images\nhave high-resolution content with discontinuities due to edges and other structures. The channel\nmixing in standard FNO incurs a quadratic complexity in the channel size. To control this com-\n2\nPublished as a conference paper at ICLR 2022\nplexity, we impose a block-diagonal structure on the channel mixing weights. Also, to enhance\ngeneralization, inspired by sparse regression, we sparsify the frequencies via soft-thresholding Tib-\nshirani (1996). Also, for parameter efﬁciency, our MLP layer shares weights across tokens (see\nTable 1). We term the resulting model as adaptive FNO (AFNO).\nWe perform extensive experiments with pretraining vision transformers for upstream classiﬁcation\nand inpainting that are then ﬁnetuned for downstream segmentation. Compared with the state-of-the-\nart, our AFNO using the ViT-B backbone outperforms existing GFN, LS, and self-attention for few-\nshot segmentation in terms of both efﬁciency and accuracy, e.g, compared with self-attention, AFNO\nachieves slightly better accuracy while being 30% more efﬁcient. For Cityscapes segmentation\nwith the Segformer-B3 backbone, AFNO achieves state-of-the-art and beats previous methods, e.g.\nAFNO achieves more than 2% better mIoU compared with efﬁcient self-attention Xie et al. (2021),\nand is also competitive with GFN and LS.\nKey Contributions. Our main contributions are summarized as follows:\n• We establish a link between operator learning and high-resolution token mixing and adapt FNO\nfrom PDEs as an efﬁcient mixer with a quasi-linear complexity in the sequence length.\n• We design AFNO in a principled way to improve its expressiveness and generalization by impos-\ning block-diagonal structure, adaptive weight-sharing, and sparsity.\n• We conduct experiments for pretraining and ﬁnetuning. AFNO outperforms existing mixers for\nfew-shot segmentation. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO\n(sequence: 65k) achieves state-of-the-art, e.g., with 2% gain over the efﬁcient self-attention.\n2 R ELATED WORKS\nOur work is at the intersection of operator learning and efﬁcient transformers. Since the inception\nof transformers, there have been several works to improve the efﬁciency of self-attention. We divide\nthem into three lines of work based on the structural constraints.\nGraph-Based Mixers primarily focus on ﬁnding efﬁcient surrogates to approximate self-attention.\nThose include: (i) sparse attentions that promote predeﬁned sparse patterns; see e.g., sparse trans-\nformer Child et al. (2019), image transformer Parmar et al. (2018), axial transformer Ho et al. (2019),\nand longformer Beltagy et al. (2020); (ii) low-rank attention that use linear sketching such as lin-\nformers Wang et al. (2020), long-short transformers Lian et al. (2021), Nystr ¨omformer Xiong et al.\n(2021); (iii) kernel methods that approximate attention with ensemble of kernels such as performer\nChoromanski et al. (2020), linear transformer Katharopoulos et al. (2020), and random feature at-\ntention Peng et al. (2021); and (iv) clustering-based methods such as reformer Kitaev et al. (2020),\nrouting transformer Roy et al. (2021), and Sinkhorn transformer Tay et al. (2020a). These surrogates\nhowever compromise accuracy for efﬁciency.\nMLP-Based Mixers relax the graph similarity constraints of the self-attention and spatially mix\ntokens using MLP projections. The original MLP-mixer Tolstikhin et al. (2021) achieves similar\naccuracy as self-attention. It is further accelerated by ResMLP Touvron et al. (2021) that replaces\nthe layer norm with the afﬁne transforms. gMLP Liu et al. (2021a) also uses an additional gating\nto weight tokens before mixing. This class of methods however lack scalability due to quadratic\ncomplexity of MLP projection, and their parameter inefﬁciency for high resolution images.\nFourier-Based Mixers apply the Fourier transform to spatially mix tokens. FNet Lee-Thorp et al.\n(2021) resembles the MLP-mixer with token mixer simply being pre-ﬁxed DFT. No ﬁltering is\ndone to adapt the data distribution. Global ﬁlter networks (GFNs) Rao et al. (2021) however learn\nFourier ﬁlters to perform depthwise global convolution, where no channel mixing is involved. Also,\nGFN ﬁlters lack adaptivity that could negatively impact generalization. In contrast, our proposed\nAFNO performs global convolution with dynamic ﬁltering and channel mixing that leads to better\nexpressivity and generalization.\nOperator Learning deals with mapping from functions to functions and commonly used for PDEs.\nOperator learning can be deployed in computer vision as images are RGB-valued functions on a\n2D plane. This continuous generalization allows us to permeate beneﬁts from operators. Recent\nadvances in operator learning include DeepONet Lu et al. (2019) that learns the coefﬁcients and\nbasis of the operators, and neural operators Kovachki et al. (2021) that are parameterized by integral\n3\nPublished as a conference paper at ICLR 2022\noperators. In this work, we adopt Fourier neural operators Li et al. (2020a) that implement global\nconvolution via FFT which has been very successful for solving nonlinear and chaotic PDEs.\n3 P RELIMINARIES AND PROBLEM STATEMENT\nConsider a 2D image that is divided into a h×wgrid of small and non-overlapping patches. Each\npatch is represented as a d-dimensional token, and the image can be represented as a token tensor\nX ∈Rh×w×d. Treating image as a token sequence, transformers then aim to learn a contextual\nembedding that transfers well to downstream tasks. To end up with a rich representation, the tokens\nneed to be effectively mixed over the layers.\nSelf-attention is an effective mixing that learns the graph similarity among tokens. It however scales\nquadratically with the sequence size, which impedes training high resolution images. Our goal is\nthen to ﬁnd an alternative mixing strategy that achieves favorable scaling trade-offs in terms of\ncomputational complexity, memory, and downstream transfer accuracy.\n3.1 K ERNEL INTEGRATION\nThe self-attention mechanism can be written as a kernel integration (Tsai et al., 2019; Cao, 2021;\nKovachki et al., 2021). For the input tensor X we denote the (n,m)-th token as xn,m ∈Rd. For\nnotation convenience, we index the token sequence as X[s] := X[ns,ms] for some s,t ∈[hw].\nDeﬁne also N := hwas the sequence length. The self-attention mixing is then deﬁned as follows:\nDeﬁnition 1 (Self Attention). Att :RN×d →RN×d\nAtt(X) := softmax\n(XWq(XWk)⊤\n√\nd\n)\nXWv (1)\nwhere Wq,Wk,Wv ∈Rd×d are the query, key, and value matrices, respectively. Deﬁne K :=\nsoftmax(⟨XWq,XWk⟩/\n√\nd) as the N ×N score array with ⟨·,·⟩being inner product in Rd. We\nthen treat self-attention as an asymmetric matrix-valued kernelκ: [N]×[N] →Rd×dparameterized\nas κ[s,t] =K[s,t]·Wv (where K[s,t] is scalar valued and “·” is scalar-matrix multiplication). Then\nthe self-attention can be viewed as a kernel summation.\nAtt(X)[s] :=\nN∑\nt=1\nX[t]κ[s,t] ∀s∈[N]. (2)\nWhere X[t]κ[s,t] = X[t]K[s,t]Wv = K[s,t]X[t]Wv. This kernel summation can be extended\nto continuous kernel integrals. The input tensor X is no longer a ﬁnite-dimensional vector in the\nEuclidean space X ∈RN×d, but rather a spatial function in the function spaceX ∈(D,Rd) deﬁned\non domain D ⊂R2 which is the physical space of the images. In this continuum formulation,\nthe neural network becomes an operator that acts on the input functions. This brings us efﬁcient\ncharacterization originating from operator learning.\nDeﬁnition 2 (Kernel Integral). We deﬁne the kernel integral operator K: (D,Rd) →(D,Rd) as\nK(X)(s) =\n∫\nD\nκ(s,t)X(t) dt ∀s∈D. (3)\nwith a continuous kernel function κ: D×D→Rd×d Li et al. (2020b). For the special case of the\nGreen’s kernelκ(s,t) =κ(s−t), the integral leads to global convolution deﬁned below.\nDeﬁnition 3 (Global Convolution). Assuming κ(s,t) =κ(s−t), the kernel operator admits\nK(X)(s) =\n∫\nD\nκ(s−t)X(t) dt ∀s∈D. (4)\nThe convolution is a smaller complexity class of operation compared to integration. The Green’s\nkernel has beneﬁcial regularization effect but it is also expressive enough to capture global interac-\ntions. Furthermore, the global convolution can be efﬁciently implemented by the FFT.\n4\nPublished as a conference paper at ICLR 2022\n3.2 F OURIER NEURAL OPERATOR AS TOKEN MIXER\nThe class of shift-equivariant kernels has a desirable property that they can be decomposed as a linear\ncombination of eigen functions. Eigen transforms have a magical property where according to the\nconvolution theorem Soliman & Srinath (1990), global convolution in the spatial domain amounts\nto multiplication in the eigen transform domain. A popular example of such eigen functions is the\nFourier transform. Accordingly, one can deﬁne the Fourier neural operator (FNO) Li et al. (2020a).\nDeﬁnition 4 (Fourier Neural Operator). For the continuous input X ∈Dand kernel κ, the kernel\nintegral at token sis found as\nK(X)(s) =F−1(\nF(κ) ·F(X)\n)\n(s) ∀s∈D,\nwhere ·denotes matrix multiplication, and F,F−1 denote the continous Fourier transform and its\ninverse, respectively.\nDiscrete FNO. Inspired by FNO, for images with ﬁnite dimension on a discrete grid, our idea is to\nmix tokens using the discrete Fourier transform (DFT). For the input token tensor X ∈R h×w×d,\ndeﬁne the complex-valued weight tensor W := DFT(κ) ∈C h×w×d×d to parameterize the kernel.\nFNO mixing then entails the following operations per token (m,n) ∈[h] ×[w]\nstep (1).token mixing zm,n = [DFT(X)]m,n\nstep (2).channel mixing ˜zm,n = Wm,nzm,n\nstep (3).token demixing ym,n = [IDFT(˜Z)]m,n\nLocal Features. DFT assumes a global convolution applied on periodic images, which is not typ-\nically true for real-world images. To compensate local features and non-periodic boundaries, we\ncan add a residual term xm,n (can also be parameterized as a simple local convolution) to the token\ndemixing step 3 in FNO; see also Wen et al. (2021).\nResolution Invariance. The FNO model is invariant to the discretization h,w. It parameterizes the\ntokens function via Fourier bases which are invariant to the underlying resolution. Thus, after train-\ning on one resolution it can be directly evaluated at another resolution (zero-shot super-resolution).\nFurther, the FNO model encodes the higher-frequency information in the channel dimension. Thus,\neven after truncating the higher frequency modes zm,n, FNO can still output the full spectrum.\nIt is also important to recognize step (2) of FNO, where d×d weight matrix Wm,n mixes the\nchannels. This implies mixed-channel global convolution. Note that the concurrent GFN work Rao\net al. (2021) is a special case of FNO, when Wm,n is diagonal and the channels are separable.\nThe FNO incurs O(Nlog(N)d2) complexity, and thus quasi-linear in the sequence size. The pa-\nrameter count is however O(Nd2) as each token has its own channel mixing weights, which poorly\nscales with the image resolution. In addition, the weights Wm,n are static, which can negatively\nimpact the generalization. The next section enhances FNO to cope with these shortcomings.x\n4 A DAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS\nThis section ﬁxes the shortcomings of FNO for images to improve scalability and robustness.\nBlock-Diagonal Structure on W. FNO involves d×dweight matrices for each token. That results\nin O(Nd2) parameter count that could be prohibitive. To reduce the paramater count we impose a\nblock diagonal structure on W, where it is divided into k weight blocks of size d/k×d/k. The\nkernel then operates independently on each block as follows\n˜z(ℓ)\nm,n = W(ℓ)\nm,nz(ℓ)\nm,n, ℓ = 1,...,k (5)\nThe block diagonal weights are both interpretable and computationally parallelizable. In essence,\neach block can be interpreted a head as in multi-head self-attention, which projects into a subspace\nof the data. The number of blocks should be chosen properly so each subspace has a sufﬁciently\nlarge dimension. In the special case, that the block size is one, FNO coincides with the GFN kernels.\nMoreover, the multiplications in (5) are performed independently, which is quite parallelizable.\n5\nPublished as a conference paper at ICLR 2022\ndef AFNO(x)\nbias = x\nx = RFFT2(x)\nx = x.reshape(b, h, w//2+1, k, d/k)\nx = BlockMLP(x)\nx = x.reshape(b, h, w//2+1, d)\nx = SoftShrink(x)\nx = IRFFT2(x)\nreturn x + bias\nx = Tensor[b, h, w, d]\nW_1, W_2 = ComplexTensor[k, d/k, d/k]\nb_1, b_2 = ComplexTensor[k, d/k]\ndef BlockMLP(x):\nx = MatMul(x, W_1) + b_1\nx = ReLU(x)\nreturn MatMul(x, W_2) + b_2\nFigure 3: Pseudocode for AFNO with adaptive weight sharing and adaptive masking.\nWeight Sharing. Another caveat with FNO is that the weights are static and once learned they will\nnot be adaptively changed for the new samples. Inspired by self-attention we want the tokens to be\nadaptive. In addition, static weights are independent across tokens, but we want the tokens interact\nand decide about passing certain low and high frequency modes. To this end, we adopt a two-layer\nperceptron that is supposed to approximate any function for a sufﬁciently large hidden layer. For\n(n,m)-th token, it admits\n˜zm,n = MLP(zm,n) =W2σ(W1zm,n) +b (6)\nNote, that the weights W1,W2,b are shared for all tokens, and thus the parameter count can be\nsigniﬁcantly reduced.\nSoft-Thresholding and Shrinkage. Images are inherently sparse in the Fourier domain, and most\nof the energy is concentrated around low frequency modes. Thus, one can adaptively mask the\ntokens according to their importance towards the end task. This can use the expressivity towards\nrepresenting the important tokens. To sparsify the tokens, instead of linear combination as in (5),\nwe use the nonlinear LASSO Tibshirani (1996) channel mixing as follow\nmin ∥˜zm,n −Wm,nzm,n∥2 + λ∥˜zm,n∥1 (7)\nThis can be solved via soft-thresholding and shrinkage operation\n˜zm,n = Sλ(Wm,nzm,n) (8)\nthat is deﬁned as Sλ(x) = sign(x) max{|x|− λ,0}, where λ is a tuning parameter that controls\nthe sparsity. It is also worth noting that the promoted sparsity can also regularize the network and\nimprove the robustness.\nWith the aforementioned modiﬁcations, the overall AFNO mixer module is shown in Fig 1 along\nwith the pseudo code in Fig 2. Also, for the sake of comparison, the AFNO is compared against\nFNO, GFN, and self-attention in Table 1 in terms of interpretation, memory, and complexity.\n5 E XPERIMENTS\nWe conduct extensive experiments to demonstrate the merits of our proposed AFNO transformer.\nNamely, 1) we evaluate the efﬁciencyy-accuracy trade-off between AFNO and alternative mixing\nmechanisms on inpainting and classiﬁcation pretraining tasks; and then 2) measure performance\non few-shot semantic segmentation with inpainting pretraining; and 3) evaluate the performance of\nAFNO in high resolution settings with semantic segmentation. Our experiments cover a wide-range\nof datasets, including ImageNet-1k, CelebA-Faces, LSUN-Cats, ADE-Cars, and Cityscapes as in\nDeng et al. (2009); Liu et al. (2015); Yu et al. (2015); Krause et al. (2013); Cordts et al. (2016).\n5.1 I MAGE NET-1K I NPAINTING\nWe conduct image inpainting experiments which compare AFNO to other competitive mixing mech-\nanisms. The image inpainting task is deﬁned as follows: given an input image X of size [h,w,d ],\nwhere h,w,d denote height, width, and channels respectively, we randomly mask pixel intensities to\nzero based on a uniformly random walk. The loss function used to train the model is mean squared\nerror between the original image and the reconstruction. We measure performance via the Peak\nSignal-to-Noise Ratio (PSNR) and structural similarity index measure (SSIM) between the ground\ntruth and the reconstruction. More details about the experiments are provided in the appendix.\n6\nPublished as a conference paper at ICLR 2022\nBackbone Mixer Params GFLOPs Latency(sec) SSIM PSNR(dB)\nViT-B/4 Self-Attention 87M 357.2 1.2 0.931 27.06\nViT-B/4 LS 87M 274.2 1.4 0.920 26.18\nViT-B/4 GFN 87M 177.8 0.7 0.928 26.76\nViT-B/4 AFNO (ours) 87M 257.2 0.8 0.931 27.05\nTable 2: Inpainting PSNR and SSIM for ImageNet-1k validation data. AFNO matches the performance of\nSelf-Attention despite using signiﬁcantly less FLOPs.\nInpainting Results. PSNR and SSIM are reported in Table 2 for AFNO versus alternative mix-\ners. It appears that AFNO is competitive with self-attention. However, AFNO uses signiﬁcantly\nless GFLOPs than Self-Attention. Compared to both LS and GFN, AFNO acheives signiﬁcantly\nbetter PSNR and SSIM. More importantly, AFNO achieves favorable downstream transfer, which is\nelaborated in the next section for few-shot segmentation.\n5.2 F EW SHOT SEGMENTATION\nAfter pretraining on image inpainting, we evaluate the few-shot sematic segmentation performance\nof the models. We construct three few-shot segmentation datasets by selecting training and valida-\ntion images from CelebA-Faces, ADE-Cars, and LSUN-Cats as in Zhang et al. (2021b). The model\nis trained using cross-entropy loss. We measure mIoU over the validation set. More details about\nthe experiments are deferred to the Appendix.\nBackbone Mixer Params GFLOPs LSUN-Cats ADE-Cars CelebA-Faces\nViT-B/4 Self-Attention 87M 357.2 35.57 49.26 56.91\nViT-B/4 LS 87M 274.2 20.29 29.66 41.36\nViT-B/4 GFN 87M 177.8 34.52 47.84 55.21\nViT-B/4 AFNO (ours) 87M 257.2 35.73 49.60 55.75\nTable 3: Few-shot segmentation mIoU for AFNO versus alternative mixers. AFNO surpasses Self-Attention\nfor 2/3 datasets while using less ﬂops.\nFew-Shot Segmentation Results. Results are reported in Table 3. It is evident that AFNO performs\non par with self-attention. Furthermore, for out-of-domain datasets such as ADE-Cars or LSUN-\nCats it slightly outperforms self-attention, which is partly attributed to the sparsity regularization\nendowed in AFNO.\n5.3 C ITYSCAPES SEGMENTATION\nTo test the scalability of AFNO for high resolution images with respect to alternative mixers, we\nevaluate high-resolution (1024 ×1024) semantic segmentation for the Cityscapes dataset. We use\nthe SegFormer-B3 backbone which is a hiearchical vision transformer Xie et al. (2021). We train the\nmodel using the cross-entropy loss and measure performance via reporting mIoU over the validation\nset. More details about the experiments and the model are available in the Appendix.\nBackbone Mixer Params Total GFLOPs Mixer GFLOPs mIoU\nSegformer-B3/4 SA 45M N/A 825.7 N/A\nSegformer-B3/4 Efﬁcient SA 45M 380.7 129.9 79.7\nSegformer-B3/4 LS 45M 409.1 85.0 80.5\nSegformer-B3/4 GFN 45M 363.4 2.6 80.4\nSegformer-B3/4 AFNO-100% (ours) 45M 440.0 23.7 80.9\nSegformer-B3/4 AFNO-25% (ours) 45M 429.0 12.4 80.4\nTable 4: mIoU and FLOPs for Cityscapes segmentation at 1024 × 1024 resolution. Note, both the mixer and\ntotal FLOPs are included. For GFN and AFNO, the MLP layers are the bottleneck for the complexity. Also,\nAFNO-25% only keeps 25% of the low frequency modes, while AFNO-100% keeps all the modes. Results for\nself-attention cannot be obtained due to the long sequence length in the ﬁrst few layers.\n7\nPublished as a conference paper at ICLR 2022\nCityscapes Segmentation Results. We report the ﬁnal numbers for Cityscapes semantic segmenta-\ntion in Table 4. AFNO-100% outperforms all other methods in terms of mIoU. Furthermore, we ﬁnd\nthat the AFNO-25% model which truncates 75% of high frequency modes during ﬁnetuning only\nloses 0.05 mIoU and is competitive with the other mixers. It is important to note that the majority\nof computations is spent for the MLP layers after the attention module.\n5.4 I MAGE NET-1K C LASSIFICATION\nWe run image classiﬁcation experiments with the AFNO mixer module using the ViT backbone\non ImageNet-1K dataset containing 1.28M training images and 50K validation images from 1,000\nclasses at 224 ×224 resolution. We measure performance via reporting top-1 and top-5 valida-\ntion accuracy along with theoretical FLOPs of the model. More details about the experiments are\nprovided in the appendix.\nBackbone Mixer Params GFLOPs Top-1 Accuracy Top-5 Accuracy\nViT-S/4 LS 16M 15.8 80.87 95.31\nViT-S/4 GFN 16M 6.1 78.77 94.4\nViT-S/4 AFNO (ours) 16M 15.3 80.89 95.39\nTable 5: ImageNet-1K classiﬁcation efﬁciencyy-accuracy trade-off when the input resolution is 224 × 224.\nClassiﬁcation Results. The classiﬁcation accuracy for different token mixers are listed in Table\n5. It can be observed that AFNO outperforms GFN by more than 2% top-1 accuracy thanks to\nthe adaptive weight sharing which allows for a larger channel size. Furthermore, our experiments\ndemonstrate that AFNO is competitive with LS for classiﬁcation.\n5.5 A BLATION STUDIES\nWe also conduct experiments to investigate how different components of AFNO contribute to per-\nformance.\nFigure 4: Ablations for the sparsity thresholds and block count measured by inpainting validation PSNR. The\nresults suggest that soft thresholding and blocks are effective\nSparsity Threshold. We vary the sparsity threshold λ from 0 to 10. For each λ we pretrain the\nnetwork ﬁrst, and then ﬁnetune for few-shot segmentation on the CelebA-Faces dataset. We report\nboth the inpainting PSNR from pretraining and the segmentation mIoU. The results are shown in\nFigure 3. λ = 0corresponds to no sparsity. It is evident that the PSNR/mIoU peaks at λ = 0.01,\nindicating that the sparsity is effective. We also compare to hard thresholding (always removing\nhigher frequencies as in FNO) in Table 6. We truncate 65% of the higher frequencies for both\ninpainting pretraining and few-shot segmentation ﬁnetuning.\nNumber of Blocks. We vary the number of blocks used when we impose structure on our weights\nW. To make the comparison fair, we simultaneously adjust the hidden size so the overall parameter\ncount of the model are equal. We vary the number of blocks from 1 to 64 and measure the resulting\ninpainting PSNR on ImageNet-1K. It is seen that 8 blocks achieves the best PSNR. This shows that\nblocking is effective.\nImpact of Adaptive Weights. We evaluate how removing adaptive weights and instead using static\nweights affects the performance of AFNO for ImageNet-1K inpainting and few-shot segmentation.\n8\nPublished as a conference paper at ICLR 2022\nThe results are presented in Table 6. The results suggest that adaptive weights are crucial to AFNO’s\nperformance.\nBackbone Mixer Parameter Count PSNR CelebA-Faces mIoU\nViT-XS/4 FNO 16M 24.8 39.27\nViT-XS/4 AFNO [Non-Adaptive Weights] 16M 25.1 44.04\nViT-XS/4 AFNO [Hard Thresholding 35%] 16M 23.58 34.17\nViT-XS/4 AFNO 16M 25.69 49.49\nTable 6: Ablations for AFNO versus FNO, AFNO without adaptive weights, and hard thresholding. Results\nare on inpainting pretraining with 10% of ImageNet along with few-show segmentation mIoU on CelebA-\nFaces. Hard thresholding only keeps 35% of low frequency modes. AFNO demonstrates superior performance\nfor the same parameter count in both tasks.\nComparison to FNO. To show that AFNO’s modiﬁcations ﬁx the shortcomings of FNO for images,\nwe directly compare AFNO and FNO on ImageNet-1K inpainting pretraining and few-shot segmen-\ntation on CelebA-Faces. The results are also presented in Table 6. The results suggest that AFNO’s\nmodiﬁcations are crucial to performance in both tasks.\n5.6 C OMPARISON WITH DIFFERENT TRUNKS AT DIFFERENT SCALES\nIn order to provide more extensive comparison with the state-of-the-art efﬁcient transformers we\nhave included experiments for different trunks at different scales. Since the primary motivation of\nthis work is to deal with high resolution vision, we focus on the task of Cityscapes semantic segmen-\ntation a the benchmark that is a challenging task due to the high 1024 ×2048 resolution of images.\nFor the trunks we adopt: i) the Segformer Xie et al. (2021) backbones B0, B1, B2, B3, under three\ndifferent mixers namely AFNO, GFN and efﬁcient self-attention (ESA); ii) Swin backbones Liu\net al. (2021b) (T, S, B), and iii) ResNet He et al. (2016) and MobileNetV2 Sandler et al. (2018).\nResults for LS and self-attention are not reported due to instability issues with half-precision train-\ning and quadratic memory usage with sequence length respectively. Note that efﬁcient self-attention\nis self-attention but with a sequence reduction technique introduced in Wang et al. (2021). It is\nmeant to be a cheap approximation to self-attention. Numbers for ResNet and MobileNetV2 are\ndirectly adopted from Xie et al. (2021). For training Segformer we use the same recipe as discussed\nin Section A.3 which consists of pretraining on ImageNet-1K classiﬁcation for 300 epochs. For\ntraining Swin, we use pretrained classiﬁcation checkpoints available from the original authors and\nthen combine Swin with the Segformer head Xie et al. (2021). We use the same training recipe as\nthe Segformer models for Swin.\nThe mIoU scores are listed in Fig. 1 versus the parameter size. It is ﬁrst observed that AFNO\noutperforms other mixers when using the same Segformer backbone under the same parameter size.\nAlso, when using AFNO with the hierarchical segformer backbone, it consistently outperforms the\nSwin backbone for semantic segmentation.\n6 C ONCLUSIONS\nWe leverage the geometric structure of images in order to build an efﬁcient token mixer in compar-\nison to self-attention. Inspired by global convolution, we borrow Fourier Neural Operators (FNO)\nfrom PDEs for mixing tokens and propose principled architectural modiﬁcations to adapt FNO for\nimages. Speciﬁcally, we impose a block diagonal structure on the weights, adaptive weight shar-\ning, and sparsify the frequency with soft-thresholding and shrinkage. We call the proposed mixer\nAdaptive Fourier Neural Operator (AFNO) and it incurs quasi-linear complexity in sequence length.\nOur experiments indicate favorable accuracy-efﬁciency trade-off for few-shot segmentation, and\ncompetitive high-resolution segmentation compared with state-of-the-art. There are still important\navenues to explore for the future work such as exploring alterantives for the DFT such as the Wavelet\ntransform to better capture locality as in Gupta et al. (2021).\n9\nPublished as a conference paper at ICLR 2022\nREFERENCES\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nShuhao Cao. Choose a transformer: Fourier or galerkin. arXiv preprint arXiv:2105.14995, 2021.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794, 2020.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban\nscene understanding. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 3213–3223, 2016.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\nGaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differen-\ntial equations. arXiv preprint arXiv:2109.13459, 2021.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv preprint arXiv:1912.12180, 2019.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret. Transformers are\nrnns: Fast autoregressive transformers with linear attention. In International Conference on Ma-\nchine Learning, pp. 5156–5165. PMLR, 2020.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451, 2020.\nNikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, An-\ndrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces.\narXiv preprint arXiv:2108.08481, 2021.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained\ncategorization. In 4th International IEEE Workshop on 3D Representation and Recognition\n(3dRR-13), Sydney, Australia, 2013.\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with\nfourier transforms. arXiv preprint arXiv:2105.03824, 2021.\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-\ndrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential\nequations. arXiv preprint arXiv:2010.08895, 2020a.\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-\ndrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differ-\nential equations. arXiv preprint arXiv:2003.03485, 2020b.\n10\nPublished as a conference paper at ICLR 2022\nDongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. As-mlp: An axial shifted mlp architecture\nfor vision. arXiv preprint arXiv:2107.08391, 2021.\nHanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint\narXiv:2105.08050, 2021a.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021b.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\nIn Proceedings of International Conference on Computer Vision (ICCV), December 2015.\nLu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for iden-\ntifying differential equations based on the universal approximation theorem of operators. arXiv\npreprint arXiv:1910.03193, 2019.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In International Conference on Machine Learning, pp. 4055–\n4064. PMLR, 2018.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.\nRandom feature attention. arXiv preprint arXiv:2103.02143, 2021.\nYongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global ﬁlter networks for\nimage classiﬁcation. arXiv preprint arXiv:2107.00645, 2021.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse\nattention with routing transformers. Transactions of the Association for Computational Linguis-\ntics, 9:53–68, 2021.\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-\nbilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 4510–4520, 2018.\nSamir S Soliman and Mandyam D Srinath. Continuous and discrete signals and systems.Englewood\nCliffs, 1990.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In\nInternational Conference on Machine Learning, pp. 9438–9447. PMLR, 2020a.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey.arXiv\npreprint arXiv:2009.06732, 2020b.\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\nSociety: Series B (Methodological), 58(1):267–288, 1996.\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An\nall-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.\nHugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard\nGrave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Herv´e J´egou. Resmlp: Feedforward\nnetworks for image classiﬁcation with data-efﬁcient training. arXiv preprint arXiv:2105.03404,\n2021.\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. Transformer dissection: A uniﬁed understanding of transformer’s attention via\nthe lens of kernel. arXiv preprint arXiv:1908.11775, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\n11\nPublished as a conference paper at ICLR 2022\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021.\nGege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-\nfno–an enhanced fourier neural operator based-deep learning model for multiphase ﬂow. arXiv\npreprint arXiv:2109.03697, 2021.\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Seg-\nformer: Simple and efﬁcient design for semantic segmentation with transformers. arXiv preprint\narXiv:2105.15203, 2021.\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and\nVikas Singh. Nystr\\” omformer: A nystr\\” om-based algorithm for approximating self-attention.\narXiv preprint arXiv:2102.03902, 2021.\nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of\na large-scale image dataset using deep learning with humans in the loop. arXiv preprint\narXiv:1506.03365, 2015.\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Laﬂeche, Adela Barriuso, Antonio\nTorralba, and Sanja Fidler. Datasetgan: Efﬁcient labeled data factory with minimal human effort.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n10145–10155, 2021a.\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Laﬂeche, Adela Barriuso, Antonio\nTorralba, and Sanja Fidler. Datasetgan: Efﬁcient labeled data factory with minimal human effort.\nIn CVPR, 2021b.\nChen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar,\nand Bryan Catanzaro. Long-short transformer: Efﬁcient transformers for language and vision.\narXiv preprint arXiv:2107.02192, 2021.\n12\nPublished as a conference paper at ICLR 2022\nA A PPENDIX\nThis section includes visualizations of AFNO as well as the details of the experiments.\nA.1 V ISUALIZATION OF AFNO\nTo gain insight into how AFO works, we produce visualizations of AFNO’s weights and represen-\ntations. In particular, we visualize the spectral clustering of the tokens and sparsity masks from soft\nthresholding.\nAFNO clustering versus other mixers . We show the clustering of tokens after each transformer\nlayer. For a 10-layer transformer pretrained with 10% of ImageNet-1k, we apply spectral clustering\non the intermediate features when we use k-NN kernel withk= 10and the number of clusters is also\nset to 4. It appears that AFNO clusters are as good as self-attention. AFNO clusters seem to be more\naligned with the image objects than GFN ones. Also, long-short transformer seems not to preserve\nthe objectness in the last layers. This observation is consistent with the the few-shot segmentation\nresults in section 5.2 that show the superior performance of AFNO over the alternatives.\nSA GFN LS AFNO\nFigure 5: Spectral clustering of tokens for different token mixers. From top to bottom, it shows the input and\nthe layers 2,4,6,8,10 for the inpainting pretrained model.\nSparsity Masks . We also explore how the sparsity mask affects the magnitude of the values in\ntokens. We calculate the fraction of values over the channel dimension and blocks that have been\nmasked to zero by the Softshrink function. As one can see in Figure 6, it is clear that the input images\nare very sparse in the Fourier domain. Furthermore, this sparsity suggests that we can aggressively\ntruncate higher frequencies and maintain performance.\nFigure 6: Log magnitude of tokens (56×56) after soft-thresholding and shrinkage (λ= 0.1) and the sparsity\nmask averaged over channels for inpainting pretrained network. Left to right shows layers 1 to 5, respectively.\nA.2 I NPAINTING\nWe use the ViT-B/4 backbone for the ImageNet-1K inpainting experiments. The ViT-B backbone has\n12 layers and is described in more detail in the original paper Dosovitskiy et al. (2020). Importantly,\nwe use a 4x4 patch size to model the long sequence size setting.\n• Self-Attention Vaswani et al. (2017) uses 16 attention heads and a hidden size of 768.\n13\nPublished as a conference paper at ICLR 2022\n• Long-Short transformer Zhu et al. (2021) (LS) uses a window-size of 4 and dynamic pro-\njection rank of 8 with a hidden size of 768.\n• Global Filter Network Rao et al. (2021) (GFN) uses a hidden size of 768.\n• Adaptive Fourier Neural Operator (AFNO) uses 1 block, a hidden dimension of 750, and a\nsparsity threshold of 0.1, and a 1D convolution layer as the bias.\nThe training procedure can be summarized as follows. Given an image x ∈ R3×224×224 from\nImageNet-1K, we randomly mask pixel intensities to zero by initially sampling uniformly from\n{(i,j)}h,w\ni,j=1 and then apply the transition function T((i,j)) = UniformSample({(i−1,j),(i+\n1,j),(i,j −1),(i,j + 1)}) for 3136 steps. We use a linear projection layer at the end to convert\ntokens into a reconstructed image. Our loss function computes the mean squared error between\nthe ground truth and reconstruction of the masked pixels. We measure performance via the Peak\nSignal-to-Noise Ratio (PSNR) and structural similarity index measure (SSIM) between the ground\ntruth and the reconstruction.\nWe train for 100 epochs using the Adam optimizer with a learning rate of 10−4 for self-attention\nand 10−3 for all the other mechanisms using the cosine-decay schedule to a minimum learning rate\nof 10−5. We use gradient clipping threshold of 1.0 and weight-decay of 0.01.\nA.3 C ITYSCAPES SEGMENTATION\nWe use the SegFormer-B3 backbone for the Cityscapes segmentation experiments. The SegFormer-\nB3 backbone is a four-stage architecture which reduces the sequence size and increase the hidden\nsize as you progress through the network. The model is described in more detail in Xie et al. (2021).\nMore details about how the mixing mechanisms are combined with SegFormer-B3 is described\nbelow. All models do not modify the number layers in each stage which is [3, 4, 18, 3].\n• Efﬁcient Self-Attention uses a hidden size of [64, 128, 320, 512] and [1, 2, 5, 8] for the\nfour stages\n• Global Filter Network uses a hidden size of [128, 256, 440, 512] in order to match the\nparameter count of the other networks. Because GFN is not resolution invaraint, we use\nbilinear interpolation in the forward pass to make the ﬁlters match the input resolution of\nthe tokens.\n• Long-Short uses a hidden size of [128, 256, 360, 512] to match the parameter count of the\nother networks and [1, 2, 5, 8] attention heads.\n• Adaptive Fourier Neural Operator uses [208, 288, 440, 512] to match the parameter count\nof the other networks. It uses [1, 2, 5, 8] blocks in the four stages.\nWe pretrain the SegFormer-B3 backbone on ImageNet-1K classiﬁcation for 300 epochs. Our setup\nconsists of using the Adam optimizer, a learning rate of 10−3 with cosine decay to 10−5, weight\nregularization of 0.05, a batch size of 1024, gradient clipping threshold of 1.0, and learning rate\nwarmup for 6250 iterations. We then ﬁnetune these models on Cityscapes for 450 epochs using a\nlearning rate of 1.2 ·10−4. We train on random 1024x1024 crops and also evaluate at 1024x1024.\nA.4 F EW-SHOT SEGMENTATION\nThe models used for few-shot segmentation are described in B.1. We use the inpainting pretrained\nmodels and ﬁnetune them on few-shot segmentation on CelebA-Faces, ADE-Cars, and LSUN-Cats\nat 224x224 resolution. The samples for the few-shot dataset are selected as done in DatasetGAN in\nZhang et al. (2021a). To train the network, we use the per-pixel cross entropy loss.\nWe ﬁnetune the models on the few-shot datasets for 2000 epochs with a learning rate of 10−4 for\nself-attention and 10−3 for other mixers. We use no gradient clipping or weight decay. We measure\nvalidation performance every 100 epochs and report the maximum across the entire training run.\n14\nPublished as a conference paper at ICLR 2022\nA.5 C LASSIFICATION\nThe models for classiﬁcation are based on the Global Filter Network GFN-XS models but with 4x4\npatch size. In particular, we utilize 12 transformer layers and adjust the hidden size and attention-\nspeciﬁc hyperparameters to reach a parameter count of 16M. Due to some attention mechanisms not\nbeing able to support class tokens, we use global average pooling at the last layer to produce output\nsoftmax probabilities for the 1,000 classes in ImageNet-1k. More details about each of the models\nis provided below.\n• Self-Attention Vaswani et al. (2017) uses 12 attention heads and a hidden size of 324.\n• Long-Short transformer Zhu et al. (2021) (LS) uses a window-size of 4 and dynamic pro-\njection rank of 8 with a hidden size of 312.\n• Global Filter Network Rao et al. (2021) (GFN) uses a hidden size of 245. The hidden size\nis smaller due to the need to make all the models have the same parameter count.\n• Adaptive Fourier Neural Operator (AFNO) uses 16 blocks, a hidden dimension of 384,\nsparsity threshold of 0.1, and a 1D convolution layer as the bias.\nWe trained for 300 epochs with Adam optimizer and cross-entropy loss using the learning rate of\n(BatchSize/512)×5×10−4 for the models. We also use ﬁve epochs of linear learning-rate warmup,\nand after a cosine-decay schedule to the minimum value 10−5. Along with this, the gradient norm\nis clipped not to exceed 1.0 and weight-decay regularization is set to 0.05.\nA.6 A BLATION\nFor the ablation studies, we use a backbone we denote ViT-XS which refers to models which only\nhave 5 layers and have attention-specifc hyperparameters adjusted to reach a parameter count of\n16M. Details of these models are described below.\n• For FNO, we use a hidden size of 64 and ﬁve layers to make the parameter count 16M.\n• For AFNO with Static Weights, we use a hidden size of 124, four blocks, and a sparsity\ntheshold of 0.01.\n• For AFNO-35%, we hard threshold and only keep the bottom 35% frequencies. In practice,\nthis means we keep 32/56 frequncies of the tokens along each spatial dimension. We use a\nhidden size of 124, four blocks and no sparsity theshold.\n• For AFNO, we use a hidden size of 584, four blocks, and a sparsity theshold of 0.01.\nThese models are inpaint pretrained on a randomly chosen subset of only 10% of ImageNet-1K\nand trained for 100 epochs. For ﬁnetuning, we use the same setup as the few-shot segmentation\nexperiments described in A.4. We only evaluate on the CelebA-Faces dataset.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6813297271728516
    },
    {
      "name": "Security token",
      "score": 0.4599418342113495
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45526695251464844
    },
    {
      "name": "Frequency domain",
      "score": 0.4408780336380005
    },
    {
      "name": "Algorithm",
      "score": 0.4359695315361023
    },
    {
      "name": "Theoretical computer science",
      "score": 0.32742127776145935
    },
    {
      "name": "Computer vision",
      "score": 0.26071006059646606
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}