{
  "title": "Baby’s CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models",
  "url": "https://openalex.org/W4389523732",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105988519",
      "name": "Zheyu Zhang",
      "affiliations": [
        "GESIS - Leibniz-Institute for the Social Sciences",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2095758230",
      "name": "Han, Yang",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "GESIS - Leibniz-Institute for the Social Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4384670069",
      "name": "Bolei Ma",
      "affiliations": [
        "Munich School of Philosophy",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2395702597",
      "name": "David Rugamer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5049198824",
      "name": "Ercong Nie",
      "affiliations": [
        "GESIS - Leibniz-Institute for the Social Sciences",
        "Ludwig-Maximilians-Universität München"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3109746563",
    "https://openalex.org/W4210489638",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4385569785",
    "https://openalex.org/W4385571329",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2168488947",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2091923611",
    "https://openalex.org/W4389519222",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2093254167",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287812655",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W4327810433",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3211593711",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W1997396881",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W1599016936"
  ],
  "abstract": "Large Language Models (LLMs) demonstrate remarkable performance on a variety of natural language understanding (NLU) tasks, primarily due to their in-context learning ability.This ability could be applied to building babylike models, i.e. models at small scales, improving training efficiency.In this paper, we propose a \"CoThought\" pipeline, which efficiently trains smaller \"baby\" language models (BabyLMs) by leveraging the Chain of Thought prompting of LLMs.Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo,transforming it into taskoriented, human-readable texts that are comparable to the school texts for language learners.The BabyLM is then pretrained on this restructured dataset in a RoBERTa fashion.In evaluations across 4 benchmarks, our BabyLM outperforms the vanilla RoBERTa in 10 linguistic, NLU, and question-answering tasks by more than 3 points, showing a superior ability to extract contextual information.These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve improved performance.",
  "full_text": "Proceedings of the 27th Conference on Computational Natural Language Learning:\nVolume 2: The BabyLM Challenge, pages 158–170\nDecember 6-7, 2023 ©2023 Association for Computational Linguistics\nBaby’s CoThought: Leveraging Large Language Models for Enhanced\nReasoning in Compact Models\nZheyu Zhang∗♣ Han Yang∗♣,♢ Bolei Ma∗♠ David Rügamer♠,♡ Ercong Nie†♣,♡\n♣Center for Information and Language Processing, LMU Munich\n♢GESIS - Leibniz Institute for the Social Sciences, Cologne\n♠Department of Statistics, LMU Munich ♡Munich Center for Machine Learning\nzheyu.zhang@campus.lmu.de han.yang@gesis.org\n{bolei.ma, david.ruegamer}@stat.uni-muenchen.de\nnie@cis.lmu.de\nAbstract\nLarge Language Models (LLMs) demonstrate\nremarkable performance on a variety of natu-\nral language understanding (NLU) tasks, pri-\nmarily due to their in-context learning ability.\nThis ability could be applied to building baby-\nlike models, i.e. models at small scales, im-\nproving training efficiency. In this paper, we\npropose a “CoThought” pipeline, which effi-\nciently trains smaller “baby” language mod-\nels (BabyLMs) by leveraging the Chain of\nThought prompting of LLMs. Our pipeline\nrestructures a dataset of less than 100M in size\nusing GPT-3.5-turbo, transforming it into task-\noriented, human-readable texts that are com-\nparable to the school texts for language learn-\ners. The BabyLM is then pretrained on this\nrestructured dataset in a RoBERTa fashion. In\nevaluations across 4 benchmarks, our BabyLM\noutperforms the vanilla RoBERTa in 10 lin-\nguistic, NLU, and question-answering tasks by\nmore than 3 points, showing a superior ability\nto extract contextual information. These results\nsuggest that compact LMs pretrained on small,\nLLM-restructured data can better understand\ntasks and achieve improved performance.1\n1 Introduction\nRecent advances in language modeling of Large\nLanguage Models (LLMs) have shown great per-\nformance potential on diverse NLP tasks. A large\nnumber of work has been proposed towards en-\nhancing LLMs pretraining at massive scales (De-\nvlin et al., 2019; Radford and Narasimhan, 2018;\nBrown et al., 2020). However, less attention has\nbeen paid to language model (LM) pretraining at\nsmaller human-like data scales, i.e. smaller data\n∗Equal contribution.\n†Corresponding author.\n1The code for data processing and model training is avail-\nable at: https://github.com/oooranz/Baby-CoThought.\nscales, which are similar to the amount of language\ndata for human language acquisition.\nStudies in language acquisition demonstrate that\nhumans predominantly acquire language in early\nlife stages by observing their environment. Sig-\nnificant progress in language communication and\nusage is typically achieved by early childhood\n(Tomasello, 2003; Saxton, 2010). Previous studies\nshow that language modeling is to some extent sim-\nilar to children’s language acquisition, as they both\nrequire input data from the outside world and learn\nthe data by updating knowledge about the outside\nworld repeatedly (Nikolaus and Fourtassi, 2021;\nChang and Bergen, 2022; Evanson et al., 2023). It\nis reasonable to apply this human cognitive process\nto LM pretraining by using relatively small sets of\npretraining data that are comparable to the text data\nfor human language acquisition.\nWhile a child learns a piece of knowledge by\ncontinuously obtaining relevant examples from the\noutside world and updating its knowledge base, pre-\ntrained LLMs have the capacity to learn and com-\nplete previously unknown tasks when given several\ntask samples or instructions already from the inside\nof their context, the process of which is known as\n“In-Context Learning” (ICL) (Brown et al., 2020).\nA more recent advance of ICL called “ Chain of\nThought” (CoT) (Wei et al., 2022) significantly\nenhances the reasoning abilities of LLMs. CoT\nenables LLMs to perform a series of intermediate\nreasoning steps by providing a few CoT demon-\nstrations as examples during the training process.\nThis method has been found to be very effective,\nespecially in complex reasoning tasks.\nThe LLM is like a teacher who is able to trans-\nfer knowledge by reformulating raw data from the\noutside world into a task-like text format by CoT\nprompting, making the data more suitable for teach-\ning. The BabyLM is like a student who is trained\n158\n�����\n�������������������������������������������������������������\n������������������������������������������������������������������\n���������������������������������������������������������������������\n������������������������������������������������������������������������\n�����������������������������������������������������������������\n����������������������������������������������������������������\n������������������������������ ���������������������������������\n��������������������������������������������\n������������������\n����������������������\n��������������������\n������������������\n����������������������\n�����������������\n�������\n����������������������������������\n���������������������������������\n��������������������������������\n������������������������������\n����������������������������������\n�����������������������������������\n�������������������������������\n�������������������\n�������\n����������\n����������\n�\n�\n����������\n����������������������������������������������������������\n��������������������������������������������������\n���������������������������������������������������������\n������������������������������������������������������������\n��������������������������������������������������������\n��������������������������������������������������������\n��������������������������������� ��������������������������\n��������������������������������������������������\n������������������������������������������������������\n��������������������������������������������������\n������\n���������������������\n�\n�������������������������\n�\n��������������������\n������������������������������������������\n��������������������\n���������������������������������������\n����������������������������������������������\n������������\nFigure 1: Overview of the “CoThought” pipeline. We propose to generate NLU examples from discrete short\nsentences using CoT prompting and an automatic scoring mechanism. This constructs a pretraining dataset in a\n[Reason] + [Example]format, which is then used to pretrain smaller models.\nbased on this generated text. In this work, we pro-\npose “CoThought” pipeline to pretrain a BabyLM\nwith human-like smaller corpus data, by leverag-\ning the LLM’s Chain of Thought feature and the\nchild’s cognitive learning ability. In this way, the\nLLM and the child are “co-thinking” during the\ntraining process. We use the “CoThought” ap-\nproach to train our BabyLM, combining the produc-\ntivity of the LLM with the effectiveness of human\nlanguage acquisition for LM pretraining.\nOur overall framework is illustrated in Fig-\nure 1. The raw pretraining data is provided by\nWarstadt et al. (2023) in the BabyLM Challenge,\nwhich has the goal of sample-efficient pretraining\non a developmentally plausible corpus at a small\nhuman-like data scale. We choose the loose track\nof the BabyLM Challenge, where we apply our\n“CoThought” pipeline and use the LLM GPT-3.5-\nturbo2 to preprocess the raw data. For every 5\nsentences of the raw data, the GPT-3.5-turbo uses\nCoT prompting to propose different NLU tasks and\nselects the best task. Then, it combines these 5\n2https://platform.openai.com/docs/models/\ngpt-3-5\nsentences into a task-like text based on the best\ntask for our BabyLM to learn. The BabyLM is\npretrained on the augmented data in a RoBERTa\n(Liu et al., 2019) fashion. Our BabyLM pretrained\nin the CoThought pipeline notably outperforms the\noriginal RoBERTa model on common benchmarks.\nOur work makes contributions in\n1) proposing the CoThought pretraining pipeline\nfitting the human-like data scenarios,\n2) pretraining a BabyLM model of the RoBERTa-\nbase architect in the CoThought pipeline sur-\npassing the original RoBERTa model on sev-\neral tasks, and\n3) providing insights of the CoThought pipeline\nby conducting linguistic case analysis on rep-\nresentative tasks.\n2 Related Work\nLanguage Acquisition and Modelling The lan-\nguage acquisition of children is a widely studied\ntopic in linguistics. The empiricism of language ac-\nquisition contends that language ability is a compo-\nnent of social cognitive ability and children acquire\n159\nlanguage through language communication and lan-\nguage use (Bybee, 2001; Pullum and Scholz, 2002;\nTomasello, 2003; Saxton, 2010). According to the\nUniversal Grammar (Chomsky, 1957), language\nnorms and parameters are hard-wired within ev-\nery single person, and learning a language is just a\nmatter of adjusting those parameters (Gegov et al.,\n2014). In this way, child language acquisition and\nlanguage modeling are similar, as the neural lan-\nguage models such as BERT (Devlin et al., 2019)\nand GPT (Radford and Narasimhan, 2018) are pre-\ntrained based on big corpora with their model pa-\nrameters tuned during pretaining. Recent studies\nshow the applicability of language models to child\nlanguage development tracking. Nikolaus and Four-\ntassi (2021) propose an integrated perception- and\nproduction-based learning and highlight that chil-\ndren are not only understood as passively absorbing\nthe input but also as actively participating in the\nconstruction of their linguistic knowledge in lan-\nguage learning. Chang and Bergen (2022) study\nthe factors that predict words’ ages of acquisition in\ncontemporary language models compared to word\nacquisition in children. Evanson et al. (2023) com-\npare the sequence of learning stages of language\nmodels with child language acquisition.\nIn-Context Learning (ICL) LLMs like GPT-3\n(Brown et al., 2020) make “In-Context Learning”\npossible, which means the model makes predic-\ntions by learning from a natural language prompt\ndescribing the language task or learning from (only\na few) examples. Based on the concept of ICL,\nrecent research has demonstrated that LLMs can be\nused to extract relevant knowledge from the content.\nLiu et al. (2022) propose to use GPT-3 to generate\npertinent contexts and then supply those contexts\nas extra input in order to answer a commonsense\nquestion. Yu et al. (2023) employ a generate-then-\nread pipeline which first prompts a large language\nmodel to generate contextual documents based on\na given question, and then reads the generated doc-\numents to produce the final answer.\nChain of Thought (CoT) Wei et al. (2022) in-\ntroduced “Chain of Thought”, which is a series\nof intermediate reasoning steps a few chain of\nthought demonstrations are provided as exemplars\nin prompting, in order to improve the ICL ability\nof LLMs to perform complex reasoning. Kojima\net al. (2023) demonstrate the zero-shot performance\nof CoT. Paranjape et al. (2023) introduces a frame-\nwork that uses frozen LLMs to automatically gener-\nate intermediate reasoning steps as a program. Yao\net al. (2023) put forward a “Tree of Thoughts” (ToT)\nframework, which generalizes over CoT to prompt-\ning language models and enables exploration over\ncoherent units of text (“thoughts”) that serve as in-\ntermediate steps toward problem solving. A more\nrecent study (Gu et al., 2023) proposes a pretrain-\ning for ICL framework which pretrains the model\non a set of “intrinsic tasks” in the general plain-text\ncorpus using the simple language modeling objec-\ntive to enhance the language models’ ICL ability.\n3 Method\nIn the realm of cognitive learning, the teacher’s\nthought process greatly influences the way instruc-\ntional content is delivered, which in turn impacts\nthe students’ understanding (Chew and Cerbin,\n2021). Our method attempts to mimic this pro-\ncess. The LLMs, in the role of the teacher, use CoT\nprompting to reinterpret the raw data, generating\ntask-like text that incorporates the context of the\nsentences and enriches the learning materials.\nWe first introduce an overview of our CoThought\npipeline (see Figure 1 for an illustration) and then\ndescribe the details in the following sections.\n3.1 Problem Statement\nThe genesis of our research lies in addressing a sig-\nnificant problem within the context of the BabyLM\nChallenge as proposed by Warstadt et al. (2023).\nThe goal of this challenge is to conduct sample-\nefficient pretraining on a developmentally plausible\ncorpus at a small human-like data scale, which we\npreviously introduced. Nevertheless, the majority\nof the training data provided consists of discrete\nshort sentences. As an illustration, below are some\nof the provided sentences:\n- You want your book back, don’t you?\n- Let’s see, do you want to see who this is?\n- This is Big Bird.\n- Enough with that.\n- Can you read your book again? You like the\nbook?\nThese sentences, albeit contextually rich, are\nsampled from a wide range of sources including\ndialogues, scripted content, fiction, nonfiction, and\nchild-directed materials. Due to the diverse and\nfragmented nature of this dataset, the sentences\n160\noften lack strong semantic ties with each other,\nmaking it difficult for models to learn contextual\nand coherent representations.\nIn response, we propose a method that trans-\nforms these fragmented sentences into cohesive\nunits using LLMs, subsequently enabling more ef-\nfective learning for the smaller models. The suc-\nceeding sections will provide a succinct outline of\nour pipeline and process.\n3.2 Creative NLU-Example Generation\nInspired by recent studies that demonstrate the ca-\npability of LLMs to generate rationales support-\ning their predictions, we invent a novel task called\nCreative NLU-Example Generation (CNLU-EG),\ninspired by the Creative Writing task proposed by\nthe “Tree of Thought” (Yao et al., 2023). Instead\nof creating coherent paragraphs from random sen-\ntences, CNLU-EG employs the provided sentences\nto generate coherent paragraphs, which define a\nplausible intrinsic NLU task and its corresponding\nlabels. In this task, we employ the reasoning capa-\nbility of LLMs to generate rationales for training\nsmaller baby models.\nWe first remove any duplicate sentences from\nthe BabyLM_100M (Warstadt et al., 2023) D. Af-\nter the cleaning process, we randomly sample five\nunique sentences {xi}i∈D from the cleaned dataset\nD. We initiate the task by providing a specific\nCoT prompt p to the LLM. This prompt instructs\nthe LLM to first create a plan, then use the pro-\nvided sentences to compose an example paragraph\nthat illustrates a possible intrinsic NLU task, and fi-\nnally generate the corresponding labels for this task.\nGiven the creative nature of the task, we use a zero-\nshot prompt here. The prompt is structured such\nthat it encourages the LLM to present the output in\nfour distinct sections: the plan, the paragraph, the\ntask, and the labels.\nOnce the LLM receives the prompt p, for each\nsentence xi, i∈D, the LLM generates an execu-\ntion plan ˆri, a paragraph ˆei embodying an example\nof a possible NLU task, the task name ˆti, and the\ncorresponding labels ˆyi.\nCNLU-EG essentially transforms the original,\ndiscrete sentences into a structured task, anchoring\nthe sentences to a common theme or question. This\n‘taskification’ process helps to create a more cohe-\nsive narrative, enabling the baby model to gain a\nmore contextual and comprehensive understanding\nof the sentences.\nWe also incorporate a scoring mechanism, to as-\nsess the coherence of the generated content. We\nuse a separate simple zero-shot prompt, ps, to in-\nstruct the LLM to analyze the composed paragraph\nand assign a coherence score ranging from 1 to 10.\nFor each task output, the LLM generates five such\ncoherence scores from the same scoring prompt ps,\nand these scores are then averaged to produce a fi-\nnal coherence score. According to our settings, we\nexplicitly direct the LLM to generate two distinct\nplans for each task. Each plan is independently\nscored, and the one that achieves a higher coher-\nence score is selected for subsequent steps.\nIn this way, the LLM functions as a teacher, gen-\nerating examples of possible NLU tasks, providing\ninsights into how these examples were created, and\nsupplying the corresponding labels. This collec-\ntion of generated plans and example paragraphs\nforms the training data for the smaller model to\nlearn from.\n3.3 Training Data Construction\nOur objective is to construct a high-quality dataset\nfor pretraining our small model, ensuring the in-\nstances included in the training set are coherent\nand task-relevant. As previously discussed, each in-\nstance in our data comprises a tuple: an example e\nand a corresponding plan r, denoted as [e, r]. How-\never, not all generated instances meet the quality\ncriteria necessary for effective learning.\nTo filter out lower-quality instances, we em-\nploy the coherency score obtained through the ps\nprompt. We set a threshold, stipulating that only\ninstances with a coherency score of s ≥7.0 are\nincluded in the training data. This threshold was\nempirically established based on extensive manual\nanalysis to ensure a satisfactory level of coherence\nand quality in the dataset. Mathematically, this can\nbe represented as:\nDselect = [ei, ri] :i ∈D, si ≥7.0 (1)\nHere, D denotes the initial set of generated in-\nstances and Dselect represents the selected high-\nquality instances that are used for training.\nAnother important aspect of our methodology is\nleveraging the correlation between segments with\nsimilar intrinsic tasks. Studies indicate that such\nsegments when grouped together, provide valuable\ninformation for ICL (Gu et al., 2023). Therefore,\nwe aim to collate instances with similar tasks, de-\nnoted as T, into grouped sets, which we denote as\n161\nGT .\nGT = [ei, ri] :i ∈Dselect, ti = T (2)\nIn the equation above, ti represents the task type\nof the i-th instance, and GT denotes the set of\ninstances from Dselect that are associated with task\ntype T.\nIn the end, we amalgamate these grouped sets to\ncreate a comprehensive pretraining dataset contain-\ning N instances.\nDpretrain =\n⋃\nT∈T\nGT (3)\nHere, T represents the set of all task types and GT\ndenotes the set of instances corresponding to each\ntask type T in Dselect.\nThrough these rigorous steps, we ensure that the\nfinal training data is both high-quality and task-\nrelevant, optimally structured to facilitate effective\nlearning in our small model.\n4 Experimental Setups\nWe conducted our experiments in three parts, the\ngeneration of the additional data used for training,\nthe pretraining of the language model, and the eval-\nuation.\n4.1 Data Generation via CoT Prompting\nWe generated first our extended data based on\nthe dataset babylm_100M (Warstadt et al., 2023),\nwhich contains subsets including AOCHILDES,\nBNC spoken, cbt, children stories, Gutenberg, pen\nsubtitles, qed, simple Wikipedia, switchboard, and\nWikipedia.3\nWe leveraged the API of GPT-3.5-turbo from\nOpenAI and provided CoT prompt with the format:\n- Use the given sentences to create an example\nparagraph of an NLU task and its corresponding\nlabels. The 5 sentences are: input.\n- Make a plan then write and determine. Your\noutput should be of the following format:\n- Plan:\n- Your plan here.\n- Paragraph:\n- Your paragraph here.\n- Task:\n3The full datasets could be downloaded here:\nhttps://github.com/babylm/babylm.github.io/raw/\nmain/babylm_data.zip\n- [Only the task name here, without additional\ninformation.]\n- Labels:\n- [Only the labels here, without additional\ninformation.]\nThe GPT will generate the corresponding an-\nswers in the defined format. To evaluate the gener-\nated task plans, we prompt the GPT again with the\nscore prompt in the format:\n- Analyze the following paragraph, then at the\nlast line conclude “Thus the coherency score\nis s”, where s is an integer from 1 to 10.\nWe filter out the generated texts with a score\nlower than 7. The additional data will be generated\nby the GPT with the selected proposals as prompts.\n4.2 Pretraining\nWe then trained a RoBERTa model with the ex-\ntended dataset using RobertaForMaskedLM pro-\nvided by the huggingface library 4, which uses\nthe default settings of RobertaConfiglibrary and\nis also the same settings as the hyperparameter of\nthe baseline provided by the organizers. In the train-\ning phase, we trained 5 epochs using the Trainer\nprovided by the huggingface. We refer §C for\ndetailed hyperparameters in Appendix.\n4.3 Benchmarks and Evaluation\nWe evaluated the model using the evaluation\npipeline tools 5 also provided by the organizer\n(Warstadt et al., 2023; Gao et al., 2021). This tool\nautomatically performs experiments on 4 bench-\nmarks:\n1) Benchmark of Linguistic Minimal Pairs\n(BLiMP) (Warstadt et al., 2020a);\n2) BLiMP Supplement6, including Hypernym,\nQA Congruence Easy, QA Congruence Tricky,\nSubject Aux Inversion, and Turn Taking\ndatasets;\n3) General Language Understanding Evaluation\n(GLUE) (Wang et al., 2019), and\n4https://huggingface.co/docs/transformers/\nmodel_doc/roberta\n5https://github.com/babylm/\nevaluation-pipeline\n6The relevant paper for this benchmark had not been\npublished at the time of this project, and the relevant\ndata can be found here https://github.com/babylm/\nevaluation-pipeline/blob/main/filter_data.zip\n162\n4) Mixed Signals Generalization Set (MSGS)\n(Warstadt et al., 2020b).\nThe detailed documentation of each benchmark\ncan be found in §D. The organizer (Warstadt et al.,\n2023) also provided 3 models as baselines, in-\ncluding OPT-125M, RoBERTa-base, and T5-base,\ntrained on the babylm_100Mdata.\n5 Results\nWe compare the performance of our BabyLM\n(trained in the RoBERTa way) to the original\nRoBERTa-base (baseline). Table 1 shows our se-\nlected experimental results with: i) performance\nimprovement by at least 3 points (+3), and ii) per-\nformance reduction over 3 points (-3). We report\nthe performance with absolute performance differ-\nence of our BabyLM over baseline on the selected\ntasks, as well as the overall performance of the\nwhole tasks. The full results are available in §D.\nTasks Models Diff.\nOurs Baseline\nBLiMP\nFiller Gap 78.52 68 10.52\nSub.-Verb Agr. 85.17 76.2 8.97\nArg. Structure 78.06 71.3 6.76\nDet.-Noun Agr. 97.75 93.1 4.65\nAnaphor Agr. 93.61 89.5 4.11\nEllipsis 77.02 83.8 -6.78\nIsland Effects 45.85 54.5 -8.65\nBLiMP Supplement\nSub. Aux Inversion 77.73 45.6 32.13\nQA Cong. Easy 62.5 34.4 28.1\nTurn Taking 62.5 46.8 15.7\nGLUE\nBoolQ 65.84 59.9 5.94\nMNLI 73.73 68.7 5.03\nMNLI-mm 74.76 78 -3.24\nQNLI 76.86 82.3 -5.44\nRTE 45.45 51.5 -6.05\nA VG. (overall) 73.95 71.75 2.2\nTable 1: Selected results of our BabyLM and\nthe RoBERTa (baseline), where the performance of\nBabyLM improved by at least 3 points (in bold), or\nreduced (-) over 3. The metric in this table is all accu-\nracy score.\nWe noticed that on the BLiMP benchmark,\n5 indicators increased by more than 3 points,\nnamely Filler Gap (+10.52), Subject-Verb Agree-\nment (+8.97), Argument Structure (+6.76), De-\nterminer Noun Agreement (+4.65) and Anaphor\nAgreement (+4.11), while two tasks dropped by\nmore than 3 points, namely Ellipsis (-6.78) and Is-\nland Effects (-8.65). The average performance on\nthis benchmark has also increased by 2.24.\nOn the GLUE benchmark, BoolQ and MNLI\nshowed an improvement of 5.94 and 5.03, respec-\ntively, while MNLI-mm, QNLI, and RTE showed\na decrease of -3.24, -5.44, and -6.05 respectively.\nOn top of that, we surprisingly find significant\nimprovements in 3 tasks of the BLiMP Supple-\nment benchmark: Subject Aux Inversion (+32.13),\nQA Congruence Easy (+28.10), and Turn Taking\n(+15.70). The average performance on this bench-\nmark improved by 14.85 points.\nThe overall average performance is increased by\n2.2, which shows that our model, pretrained with\nour reinterpreted small data, already demonstrates\na great improvement.\n6 Discussion\n6.1 Augmented Dataset via CoT Prompting\nWe generated our data via the above-mentioned\nCoT prompting and the GPT-3.5-turbo for nearly\n700, 000 lines, we show a case study of a part of\nthe generated data here.\n- Paragraph:\n- We have a few topics to cover in this\nparagraph. Firstly, a possible I.D. has\nbeen found in one of Gina’s snapshots.\nSecondly, there is a new technology\nin development called autostereoscopic\n3D that will allow people to watch\n3D movies without glasses. This is\ngreat news for those who find wearing\n3D glasses uncomfortable and causes eye\nstrain. Unfortunately, the narrator\nregrets not asking Jean for the details\nabout something. Lastly, the police\nare seen moving down the main street\nof Atenco, and we are tracking their\nmovements.\n- Plan:\n- 1. Introduce the topic of the paragraph\n2. Mention the possible I.D. from Gina’s\nsnapshots\n3. Talk about the new technology called\nautostereoscopic 3D\n4. Mention the difficulty of wearing 3D\nglasses\n5. Mention the regret of not asking Jean\nfor details\n6. Talk about the police and their movement\ndown the main street of Atenco\n163\n- Task:\n- Text Classification\n- Labels:\n- 1. I.D. Mentioned\n2. Technology Mentioned\n3. Regret Expressed\n4. Police Mentioned\nAs we can see from the script, the paragraph is\nan extension of the input sentences sampled from\nthe original dataset, while the plan and labels gener-\nated by the language model are the outlines, where\nthe scenes also are the critical information from the\ngenerated paragraph. It means that our approach\naugmented the original data with interpretation, em-\nphasis, and simplification, with which the model\nis possible to learn about a story with different\nversions and sizes and finally get a clearer under-\nstanding.\n6.2 Performance in QA Congruence Easy\nWe analyzed the most noticeable improvement of\nthe QA Congruence Easy dataset from the BLiMP\nSupplement benchmark, and dived deep into each\ncase. This dataset consists of 64 single-choice ques-\ntions with 20 what-questions, 25 who-questions,\nand 19 where-questions. Each question contains\na question mark, and each answer ends with a pe-\nriod. Each question corresponds to 2 candidate an-\nswers, and the boundary of the candidate answers\nis clear, i.e., for the what- and who-questions, the\nanswers contain an inanimate or an animate, and\nfor the where-questions the answer is a location\nor a noun phrase. Obviously, the answer to the\nwhat-questions should be inanimate, like a car, the\nanswer to the who-question should be animate, like\na doctoror person’s name Sarah, and the answer\nto the where-question should be location, like at\nhome. The model is expected to select the answer\nthat matches the question. For example, a question\nis “Who did you see?” and the candidate answers\nare 1. “ A doctor”, 2. “ A car”, and it is clear that\nthe answer should be “A doctor”. The final metric\nfor the evaluation is accuracy.\n6.2.1 Influence of the 3 Types of Questions\nIn these three kinds of questions, our model is bet-\nter at answering the what-questions, where the ac-\ncuracy is 75. Besides, it obtains an accuracy of\n64 for the who-questions, and 47 for the where-\nquestions.\n6.2.2 Influence of the 2 Types of Answers\nWe also note that there are two forms of the an-\nswers:\n1) sentence, where the answer is a complete sen-\ntence that includes at least the verb, e.g. “ I\nsent the package to europe”;\n2) fragment, where the answer is a single word\nor a simple phrase, and does not include the\nverb, e.g. “a car”.\nThe form of the two candidates’ answers to\neach question is consistent, i.e., both candidates’\nanswers are either sentences or fragments. The\ndataset contains 27 question-answer pairs in the\nform of sentences (42%) and 37 cases in fragments\n(57%). We also counted the accuracy on the above\ntwo forms, where the accuracy is 77.78 for sen-\ntences and 51.35 for fragments. Additionally, we\nalso counted the accuracy with the different forms\nof the three questions i.e. what-, who-, and where-\nquestions. The accuracy of the sentence labels on\nthe what-questions is 80, while the fragment is 70.\nThe accuracy on the who-question with sentence\nanswers was 71 and 61 with fragment answers. On\nwhere-questions, the tasks with sentence answers\nobtained an accuracy of 80, however, it was only 11\nwith the fragment answers. Thus we can observe\nthat our model is better at deciding with complete\nanswers rather than fragments.\n6.2.3 Influence of the 3 Types of Dialogues\nBesides, we also notice that there are three types of\ndialogues for each question,\n1) direct dialogues, where the question is started\nby a question word directly and the answer is\ndirect with the answer, e.g., question: “What\ndid you get?”, candidate answers: “ I got a\nchair”, “I got a doctor”;\n2) A-B dialogues, where the letters A and B are\nused as names for both sides of the conver-\nsation before proposing the question and the\ncandidate answers respectively, e.g. question\n“A: What did you sell?”, candidate answers:\n“B: A chair.”, “B: A doctor.”;\n3) David-Sarah dialogues, the person’s name\nDavid is used as the questioner’s name be-\nfore the question, and Sarah is used as the\nanswerer’s name before the answer.\n164\nThe dataset comprises 21 direct dialogues (32%),\n22 A-B dialogues (34%), and 21 David-Sarah di-\nalogues (32%), with the model’s accuracy consis-\ntently ranging between 61-63% across these types.\nWe then explored the proportionality between\nthese three forms of dialogue and the three kinds of\nquestions. Of the 20 what-questions, 7 are written\nin direct dialogues, 6 are in A-B dialogues, and 7\nare David-Sarah dialogues. we notice a difference\nin the accuracy, where the accuracy with direct dia-\nlogues is 100, the A-B dialogues have an accuracy\nof 83, and the David-Sarah dialogues reached only\n45.\nOf the 25 who-questions, 8 direct dialogues ob-\ntained an accuracy only of 25, while 7 A-B dia-\nlogues gained 85 accuracy and the accuracy of the\n10 David-Sarah dialogues is 80. Out of the 19\nwhere-questions, the accuracy of the 6 direct dia-\nlogues is 66%, 33% of A-B dialogues are correct,\nand the accuracy of the 4 David-Sarah dialogues is\n50%.\nFrom the above results, we can see that our\nmodel is good at selecting answers from direct\nand A-B dialogues on the what-questions. In con-\ntrast, for the who-questions, our model is good at\nselecting animates from the David-Sarah dialogues\nand the A-B dialogues, but not good at selecting the\nanimate from the direct dialogues. It might be posi-\ntively affected by the presence of the person’s name.\nIn the where-questions, the form of dialogues has a\nmore limited effect on the performance.\n6.3 Performance in QA Congruence Tricky\nWe compared the performance on the QA Congru-\nence Tricky dataset, on which we have a very simi-\nlar performance (35) to the baseline model. It con-\ntains 165 tricky questions including who-, where-,\nwhen-, why-, and how many-questions, where the\nproportions of the who- and the where-questions\nare 15% and 16% respectively. The accuracy of\nthe who- and where-questions are only 37 and 30\nrespectively, differ from the accuracies in the QA\nCongruence Easy dataset.\nWe also notice that, in this dataset, our model\nis better at selecting fragment answers rather than\nanswers in the form of sentences, where the ac-\ncuracy with fragments is 62, while the accuracy\nof the sentences is only 10. On both who- and\nwhere-questions, our model is better at finding the\nanswer in the David-Sarah dialogues (55 and 45\nrespectively in accuracy), and the accuracies of\nboth questions in the other two dialogue forms are\nunder 30. Similar to the fact shown in the easy\ndataset, the presence of people’s names probably\nprovides a sign to the animate and thus influences\nthe performance, especially on the who-questions.\nWe analyzed the questions-candidate answers\npairs from the tricky dataset, where both the ques-\ntions and the candidate answers are generally\nshorter, e.g., the question is “ Who ate?”, and the\ncandidate answers are “A teacher ate.”, and “Pasta\nate.”, where the question only contains the wh-\nword, a verb, and a question mark, and the candi-\ndate answers contain only a subjective and a verb.\nThe answers in the form of fragments are even\nshorter, e.g. to a question “Who cooked?”, the can-\ndidate answers are “Sarah”, and “A sandwich”.\nBesides the questions being more varied and\ncomplex, this dataset is more tricky, because the\ncontext is short. The candidate answers written\nin sentences are generally very similar to the frag-\nments with only an additional verb, where the verb\nhas been mentioned in the questions, which means\nthe form of sentence possibly doesn’t provide addi-\ntional information, but may confuse the model to\nunderstand the answers.\n7 Conclusion\nIn this work, we proposed the CoThought pipeline\nfor training a BabyLM at a small scale, combin-\ning the LLMs’ productivity with the concept of a\nchild’s cognitive learning ability. We let the raw\ntraining data for the BabyLM be reformulated by\nthe LLM’s CoT prompting (i.e. let the teacher\nthink) and then train a BabyLM in a pretraining\nfashion based on the newly structured data (i.e. let\nthe child co-think and learn). We compare the per-\nformance results of our BabyLM to another vanilla\npretrained LM RoBERTa and demonstrate that our\nmodel achieves higher performance in many tasks\nincluding linguistic, question and answer, espe-\ncially congruence tasks. This suggests that data\nprocessed by LLMs based on their contextual rea-\nsoning is more natural and efficient in the learning\nprocess, just as text revised by experienced teach-\ners in the school is more suitable for students to\nlearn and understand. And when we use data re-\nstructured by LLMs, even in the case of small data\nvolume, the model is able to achieve the effect of a\nmodel trained from a large amount of data, or to be\neven better.\n165\nLimitations\nOne limitation of our work is the exclusive use of\na specific LLM for data generation. It would be\ninsightful to explore how performance varies when\nusing different LLMs to generate the pre-training\ndata. Different LLMs may introduce variability\nand diversity in the generated data, which could in-\nfluence the effectiveness of the pre-training process.\nThis aspect, while not explored in our current work,\npresents a promising avenue for future research to\nunderstand the impact of various LLMs on data\ngeneration and subsequent model performance.\nAnother limitation of our work is that our pri-\nmary focus is on data generation, leaving potential\nimprovements or optimizations in this domain un-\nexplored.\nAdditionally, our model training exclusively uti-\nlized the RoBERTa architecture. Other architec-\ntures, including causal language models and var-\nious transformer variants, also showed potential\nresearch value. Therefore, exploring our approach\nacross a broader range of architectures and iden-\ntifying pretraining methods most compatible with\nour generated data remains an important area for\nfuture research.\nBy acknowledging these limitations, we hope to\nspur further research in this area, encouraging the\nexploration of data generation techniques, model\narchitectures, and extended data methods in the\ncontext of small-scale language modeling.\nEthics Statement\nThis research was conducted in accordance with\nthe ACM Code of Ethics. The datasets that we use\nare publicly available (Warstadt et al., 2023). We\nreport only aggregated results in the main paper.\nWe have not intended or do not intend to share any\nPersonally Identifiable Data with this paper.\nAcknowledgements\nWe thank the anonymous reviewers and the organiz-\ning committee for their efforts and helpful advice.\nE.N. was supported by MCML and CSC.\nReferences\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. TAC, 7:8.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nJoan Bybee. 2001. Phonology and Language Use. Cam-\nbridge Studies in Linguistics. Cambridge University\nPress.\nTyler A. Chang and Benjamin K. Bergen. 2022. Word\nacquisition in neural language models. Transactions\nof the Association for Computational Linguistics,\n10:1–16.\nStephen L Chew and William J Cerbin. 2021. The cog-\nnitive challenges of effective teaching. The Journal\nof Economic Education, 52(1):17–40.\nNoam Chomsky. 1957. Syntactic Structures. The\nHague: Mouton and Co.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine learning challenges workshop,\npages 177–190. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nLinnea Evanson, Yair Lakretz, and Jean Rémi King.\n2023. Language acquisition: do children and lan-\nguage models follow similar learning stages? In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, pages 12205–12218, Toronto,\nCanada. Association for Computational Linguistics.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\n166\nEmil Gegov, Fernand Gobet, Mark Atherton, Daniel\nFreudenthal, and Julian Pine. 2014. Modelling lan-\nguage acquisition in children using network theory.\nIn European Perspectives on Cognitive Science.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nWilliam B Dolan. 2007. The third pascal recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023.\nPre-training to learn in context. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n4849–4870, Toronto, Canada. Association for Com-\nputational Linguistics.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual entail-\nment challenge. In Proceedings of the Second PAS-\nCAL Challenges Workshop on Recognising Textual\nEntailment, volume 7, pages 785–794.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface:a challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof North American Chapter of the Association for\nComputational Linguistics (NAACL).\nVid Kocijan, Thomas Lukasiewicz, Ernest Davis, Gary\nMarcus, and Leora Morgenstern. 2020. A review of\nwinograd schema challenge datasets and approaches.\narXiv preprint arXiv:2004.13831.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nHJ Levesque. 2011. The winograd schema challenge.\naaai spring symposium: Logical formalizations of\ncommonsense reasoning. Palo Alto CA.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3154–3169, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nMitja Nikolaus and Abdellah Fourtassi. 2021. Mod-\neling the interaction between perception-based and\nproduction-based learning in children’s early acqui-\nsition of semantic knowledge. In Proceedings of\nthe 25th Conference on Computational Natural Lan-\nguage Learning, pages 391–407, Online. Association\nfor Computational Linguistics.\nBhargavi Paranjape, Scott Lundberg, Sameer Singh,\nHannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Tulio Ribeiro. 2023. Art: Automatic multi-\nstep reasoning and tool-use for large language mod-\nels.\nGeoffrey K Pullum and Barbara C Scholz. 2002. Empir-\nical assessment of stimulus poverty arguments. The\nLinguistic Review, 19(1-2):9–50.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining. OpenAI.\nMatthew Saxton. 2010. Child Language: Acquisition\nand Development. Sage Publications.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nMichael Tomasello. 2003. Constructing a Language: A\nUsage-Based Theory of Language Acquisition. Har-\nvard University Press.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nAlex Warstadt, Aaron Mueller, Leshem Choshen,\nEthan Gotlieb Wilcox, Chengxu Zhuang, Juan Ciro,\nRafael Mosquera, Adina Williams, Bhargavi Paran-\njabe, Tal Linzen, and Ryan Cotterell. 2023. Findings\nof the 2023 BabyLM Challenge: Sample-efficient\npretraining on developmentally plausible corpora. In\nProceedings of the 2023 BabyLM Challenge. Associ-\nation for Computational Linguistics (ACL).\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020a. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics, 8:377–\n392.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020b. Learning which\nfeatures matter: Roberta acquires a preference for\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 217–235. Association\nfor Computational Linguistics.\n167\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In International Confer-\nence for Learning Representation (ICLR).\nA Code and Model\nThe code for data processing and model training\nis available at: https://github.com/oooranz/\nBaby-CoThought.\nOur BabyLM is available at: https://\nhuggingface.co/yaanhaan/Baby-CoThought.\nB Pretraining Data Statistics\nThe generated dataset for LM pretraining is avail-\nable at: https://huggingface.co/datasets/\nyaanhaan/Baby-CoThought-Data.\nWe present a statistical analysis of the gener-\nated dataset. Given that our task revolves around\ncreative NLU example generation, the dataset in-\nherently encompasses a wide variety of tasks. This\ndiversity is reflective of the creative nature of the\ntask, allowing for a richer and more comprehensive\npretraining process. Each example in the dataset\nincludes an NLU example and its corresponding\nreason.\nWe plot the task distribution of the pretraining\ndataset in Figure 2. Tasks that appeared only once\nin the dataset are categorized as others.\nThe average number of words in the paragraphs\nacross all examples in the dataset is approximately\n115.25 words.\nFigure 2: The distribution of the different NLU task\nexamples in the pretraining dataset.\nC Hyperparameter\nWe followed the instruction 7 and trained\nthe tokenizers separately for the original\ndataset and our enhanced dataset via the\nByteLevelBPETokenizer library with the\nhyperparameters shown in Table 2. Other hyperpa-\nrameters were set to default and can be found in\nthe document 8.\nHyperparameter Value\nvocab_size 52000\nmin_frequency 2\nspecial_tokens <s>, <pad>, </s>, <unk>,<mask>\nTable 2: Hyperparameters used for tokenizers\nBesides, we report our hyperparameters dur-\ning the pretraining of our RoBERTa models in\nTable 3. We used the default settings from the\nRobertaConfig library. More default values and\ntechnical details can be found in the documents\n31119.\nAdditionally, the evaluation process was done\nautomatically via the evaluation tool provided by\nthe organizer, without changing the hyperparame-\nters, which can be found on the webpage 10.\n7https://huggingface.co/blog/how-to-train\n8https://github.com/huggingface/tokenizers/\nblob/main/bindings/python/py_src/tokenizers/\nimplementations/byte_level_bpe.py\n9https://huggingface.co/docs/transformers/\nmodel_doc/roberta#transformers.RobertaConfig\n10https://github.com/babylm/\n168\nHyperparameter Value\nattention_probs_dropout_prob 0.1\nbos_token_id 0\nclassifier_dropout null\neos_token_id 2\nhidden_act gelu\nhidden_dropout_prob 0.1\nhidden_size 768\ninitializer_range 0.02\nintermediate_size 3072\nlayer_norm_eps 1.00E-12\nmax_position_embeddings 512\nmodel_type roberta\nnum_attention_heads 12\nnum_hidden_layers 12\npad_token_id 1\nposition_embedding_type absolute\ntorch_dtype float32\ntransformers_version 4.17.0\ntype_vocab_size 1\nuse_cache TRUE\nvocab_size 52000\nTable 3: Hyperparameters used for pretraining\nD Full Results\nWe used 4 benchmarks:\n1) Benchmark of Linguistic Minimal Pairs\n(BLiMP) (Warstadt et al., 2020a), includ-\ning Anaphor Agreement, Argument Struc-\nture, Binding, Control Raising, Determiner\nNoun Agreement, Ellipsis, Filler Gap, Irreg-\nular Forms, Island Effects, NPI Licensing,\nQuantifiers, and Subject Verb Agreement;\n2) BLiMP Supplement11, including Hypernym,\nQA Congruence Easy, QA Congruence Tricky,\nSubject Aux Inversion, and Turn Taking;\n3) General Language Understanding Evaluation\n(GLUE) (Wang et al., 2019), including CoLA\n(Warstadt et al., 2018), SST-2 (Socher et al.,\n2013), MRPC (F1) (Dolan and Brockett,\n2005), QQP12 (F1), MNLI (Williams et al.,\n2018), MNLI-mm, QNLI (Levesque, 2011),\nRTE (Dagan et al., 2005; Haim et al., 2006;\nevaluation-pipeline#hyperparameters\n11https://github.com/babylm/\nevaluation-pipeline/blob/main/filter_data.zip\n12https://quoradata.quora.com/\nFirst-Quora-Dataset-Release-Question-Pairs\nGiampiccolo et al., 2007; Bentivogli et al.,\n2009), BoolQ (Clark et al., 2019), MultiRC\n(Khashabi et al., 2018) and WSC (Kocijan\net al., 2020);\n4) Mixed Signals Generalization Set (MSGS)\n(Warstadt et al., 2020b), including Control\nRaising Control (CR Control), Lexical Con-\ntent The Control (LC Control), Main Verb\nControl (MV Control), Relative Position Con-\ntrol (RP Control), Syntactic Category Control\n(SC Control), Control Raising Lexical Con-\ntent The (CR LC), Control Raising Relative\nToken Position (CR RTP), Main Verb Lexical\nContent The (MV LC), Main Verb Relative\nToken Position (MV RTP), Syntactic Cate-\ngory Lexical Content The (SC LC), Syntactic\nCategory Relative Position (SC RP).\nto process our evaluation.\nThe organizer provided three baseline models,\nincluding OPT-125M13 , RoBERTa-base14 , and\nT5-base15. We show our full results in Table 4.\n13https://huggingface.co/facebook/opt-125m\n14https://huggingface.co/roberta-base\n15https://huggingface.co/t5-base\n169\nTasks Models Difference\nOurs OPT-125m RoBERTa-base T5-base in abs in rel.\nBLiMP\nAnaphor Agreement 93.61 94.90 89.50 66.70 4.11 4.59%\nArgument Structure 78.06 73.80 71.30 61.20 6.76 9.48%\nBinding 72.84 73.80 71.00 59.40 1.84 2.59%\nControl Raising 69.55 72.20 67.10 59.80 2.45 3.65%\nDeterminer Noun Agreement 97.75 93.10 93.10 53.80 4.65 4.99%\nEllipsis 77.02 80.50 83.80 49.10 -6.78 -8.09%\nFiller Gap 78.52 73.60 68.00 70.00 10.52 15.47%\nIrregular Forms 91.25 80.80 89.60 75.50 1.65 1.84%\nIsland Effects 45.85 57.80 54.50 43.60 -8.65 -15.87%\nNPI Licensing 67.35 51.60 66.30 45.60 1.05 1.58%\nQuantifiers 70.58 74.50 70.30 34.20 0.28 0.40%\nSubject Verb Agreement 85.17 77.30 76.20 53.20 8.97 11.77%\nBLiMP Supplement\nHypernym 49.07 46.30 50.80 51.10 -1.73 -3.41%\nQA Congruence Easy 62.50 76.50 34.40 45.30 28.10 81.69%\nQA Congruence Tricky 34.55 47.90 34.50 25.50 0.05 0.14%\nSubject Aux Inversion 77.73 85.30 45.60 69.20 32.13 70.46%\nTurn Taking 62.50 82.90 46.80 48.90 15.70 33.55%\nGLUE\nCoLA 74.09 73.70 75.90 76.30 -1.81 -2.38%\nSST-2 88.78 86.60 88.60 88.00 0.18 0.20%\nMRPC (F1) 80.45 82.10 80.50 85.90 -0.05 -0.06%\nQQP (F1) 81.20 77.80 78.50 79.70 2.70 3.44%\nMNLI 73.73 70.10 68.70 71.50 5.03 7.32%\nMNLI-mm 74.76 71.90 78.00 74.00 -3.24 -4.15%\nQNLI 76.86 80.10 82.30 83.10 -5.44 -6.61%\nRTE 45.45 67.70 51.50 60.60 -6.05 -11.74%\nBoolQ 65.84 66.00 59.90 69.00 5.94 9.91%\nMultiRC 62.21 61.10 61.30 62.40 0.91 1.49%\nWSC 61.45 59.00 61.40 60.20 0.05 0.07%\nMSGS\nCR (Control) 83.96 97.20 93.00 95.10 -9.04 -9.72%\nLC (Control) 94.49 82.60 100.00 100.00 -5.51 -5.51%\nMV (Control) 99.98 100.00 100.00 100.00 -0.02 -0.02%\nRP (Control) 100.00 99.80 100.00 99.80 0.00 0.00%\nSC (Control) 88.44 88.10 89.00 88.70 -0.56 -0.62%\nCR LC 67.07 75.30 68.30 76.70 -1.23 -1.80%\nCR RTP 70.71 67.10 66.80 69.40 3.91 5.86%\nMV LC 66.61 66.30 66.60 67.00 0.01 0.01%\nMV RTP 67.59 66.80 80.20 67.70 -12.61 -15.72%\nSC LC 75.47 84.80 67.40 72.70 8.07 11.98%\nSC RP 70.90 62.00 67.40 68.00 3.50 5.19%\nTable 4: Full results, with difference of our BabyLM over RoBERTa-base (baseline). Metric of MRPC and QQP\nfrom GLUE is F1, in other tasks the metric is accuracy. The best results of the four models are marked in bold.\n170",
  "topic": "Pipeline (software)",
  "concepts": [
    {
      "name": "Pipeline (software)",
      "score": 0.790635347366333
    },
    {
      "name": "Computer science",
      "score": 0.7802467942237854
    },
    {
      "name": "Natural language understanding",
      "score": 0.6429324150085449
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6175281405448914
    },
    {
      "name": "Language model",
      "score": 0.556386411190033
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5260241627693176
    },
    {
      "name": "Question answering",
      "score": 0.4959965646266937
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48620671033859253
    },
    {
      "name": "Natural language processing",
      "score": 0.47326424717903137
    },
    {
      "name": "Natural language",
      "score": 0.4349285960197449
    },
    {
      "name": "Programming language",
      "score": 0.09086552262306213
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210101898",
      "name": "GESIS - Leibniz-Institute for the Social Sciences",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4403386549",
      "name": "Munich Center for Machine Learning",
      "country": null
    }
  ]
}