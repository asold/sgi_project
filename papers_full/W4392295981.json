{
    "title": "Learning stochastic dynamics and predicting emergent behavior using transformers",
    "url": "https://openalex.org/W4392295981",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2884103727",
            "name": "Corneel Casert",
            "affiliations": [
                "Lawrence Berkeley National Laboratory",
                "Ghent University"
            ]
        },
        {
            "id": "https://openalex.org/A1255532406",
            "name": "Isaac Tamblyn",
            "affiliations": [
                "University of Ottawa",
                "Vector Institute"
            ]
        },
        {
            "id": "https://openalex.org/A1750730",
            "name": "Stephen Whitelam",
            "affiliations": [
                "Lawrence Berkeley National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2884103727",
            "name": "Corneel Casert",
            "affiliations": [
                "Lawrence Berkeley National Laboratory",
                "Ghent University"
            ]
        },
        {
            "id": "https://openalex.org/A1255532406",
            "name": "Isaac Tamblyn",
            "affiliations": [
                "Canadian Transplant Association",
                "University of Ottawa",
                "Vector Institute"
            ]
        },
        {
            "id": "https://openalex.org/A1750730",
            "name": "Stephen Whitelam",
            "affiliations": [
                "Lawrence Berkeley National Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2085213650",
        "https://openalex.org/W585413800",
        "https://openalex.org/W3208532694",
        "https://openalex.org/W2964917500",
        "https://openalex.org/W2777639606",
        "https://openalex.org/W3123083763",
        "https://openalex.org/W3019533439",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W6603732165",
        "https://openalex.org/W2891545126",
        "https://openalex.org/W1746449354",
        "https://openalex.org/W2900864862",
        "https://openalex.org/W3180501569",
        "https://openalex.org/W3206227571",
        "https://openalex.org/W4283751222",
        "https://openalex.org/W3080930191",
        "https://openalex.org/W6601676141",
        "https://openalex.org/W4317754497",
        "https://openalex.org/W2755965960",
        "https://openalex.org/W638791184",
        "https://openalex.org/W2153920886",
        "https://openalex.org/W4360854476",
        "https://openalex.org/W2292328547",
        "https://openalex.org/W3158830114",
        "https://openalex.org/W6630827713",
        "https://openalex.org/W3092862347",
        "https://openalex.org/W3099423575",
        "https://openalex.org/W2886639453",
        "https://openalex.org/W3099820377",
        "https://openalex.org/W4392295981",
        "https://openalex.org/W4400040834",
        "https://openalex.org/W3092454864"
    ],
    "abstract": null,
    "full_text": "Article https://doi.org/10.1038/s41467-024-45629-w\nLearning stochastic dynamics and predicting\nemergent behavior using transformers\nCorneel Casert1,2 ,I s a a cT a m b l y n3,4,5 & Stephen Whitelam1\nWe show that a neural network originally designed for language processing\ncan learn the dynamical rules of a stochastic system by observation of a single\ndynamical trajectory of the system, and can accurately predict its emergent\nbehavior under conditions not observedduring training. We consider a lattice\nmodel of active matter undergoing continuous-time Monte Carlo dynamics,\ns i m u l a t e da tad e n s i t ya tw h i c hi t ss t eady state comprises small, dispersed\nclusters. We train a neural network called a transformer on a single trajectory\nof the model. The transformer, which we show has the capacity to represent\ndynamical rules that are numerous and nonlocal, learns that the dynamics of\nthis model consists of a small number of processes. Forward-propagated tra-\njectories of the trained transformer, at densities not encountered during\ntraining, exhibit motility-induced phase separation and so predict the exis-\ntence of a nonequilibrium phase transition. Transformers have theﬂexibility to\nlearn dynamical rules from observation without explicit enumeration of rates\nor coarse-graining of conﬁguration space, and so the procedure used here can\nbe applied to a wide range of physical systems, including those with large and\ncomplex dynamical generators.\nLearning the dynamics governing a simulation or experiment is a\ndifﬁcult task, because the number of possible dynamical transi-\ntions increases exponentially with the physical size of the system.\nFor large systems, these transitions are too numerous be enum-\nerated explicitly, and what is usually done is to coarse-grain or\nproject a system’s dynamical degrees of freedom into a subspace\nsmall enough to be learned explicitly\n1–8.H e r ew ep r e s e n ta\ndynamics-learning method that does not require projection or\ncoarse-graining, even for large systems. We show that a recently-\nintroduced neural network called a transformer\n9,p o p u l a ri nt h e\nﬁelds of natural-language processing and computer vision 10–14,\ncan express a general dynamics without the need for coarse-\ngraining over the model’s degrees of freedom or choosing a sub-\nspace of dynamical processes to learn. It can be trained ofﬂine,\ni.e., by observation only\n15, to learn the dynamical rules of a model,\neven when those rules are numerous and nonlocal. Forward-\npropagated trajectories of the trained transformer can then be\nused to reproduce the behavior of the observed model, and to\npredict its behavior when applied to conditions not seen during\ntraining.\nPrevious work has shown that it is possible to learn the rules of\ndeterministic dynamics, such as deterministic cellular automata\n16–18,o r\nof stochastic dynamics for small state spaces, using maximum-\nlikelihood estimation on the rates of the generator19,20. Similar meth-\nods have been used to learn the form of intermolecular potentials that\ninﬂuence the dynamical trajectories of particle systems\n21–25. Machine\nlearning and symbolic regression have been used to rediscover New-\nton’s formula for the gravitational force from trajectories of the solar\nsystem\n26. The accurate prediction ofﬂuid dynamics and turbulent\nﬂows has been achieved with physics-informed neural networks27–29.\nSeveral approaches exist in which high-dimensional dynamical sys-\ntems are approximated by lower-dimensional ones, such as Markov-\nstate models\n1–3. In some cases the coarse-graining procedures used to\nproduce such models involve variational methods 4 and neural\nReceived: 15 March 2022\nAccepted: 31 January 2024\nCheck for updates\n1Molecular Foundry, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720, USA.2Department of Physics and Astronomy, Ghent\nUniversity, 9000 Ghent, Belgium.3Cash App, Block, Toronto, ON M5A 1J7, Canada.4Vector Institute for Artiﬁcial Intelligence, Toronto, ON M5G 1M1, Canada.\n5Department of Physics, University of Ottawa, Ottawa, ON K1N 6N5, Canada.e-mail: ccasert@lbl.gov; isaac.tamblyn@uottawa.ca; swhitelam@lbl.gov\nNature Communications|         (2024) 15:1875 1\n1234567890():,;\n1234567890():,;\nnetworks5. Coarse-graining methods have also been used to learn\nmolecular dynamics8, and to obtain deterministic hydrodynamic\nequations from stochastic trajectories of active matter, allowing for\nthe extraction of hydrodynamic transport coefﬁcients\n6,7.O u rw o r k\ncomplements these approaches by showing that it is possible to learn\nthe dynamical rules of stochastic systems without explicit enumera-\ntion of rates or coarse-graining of conﬁguration space, thereby\nallowing treatment of large and complex systems. From the observa-\ntion of a single dynamical trajectory a transformer can identify how\nmany classes of move exist and what are their rates, providing physical\ninsight into the dynamics and allowing it to be simulated in new set-\ntings, where new phenomena can be discovered.\nWe focus on the case of a lattice model of active matter, simulated\nusing continuous-time Monte Carlo dynamics\n30 (in the Supplemental\nInformation (SI) we show that the transformer can be used to treat a\nsecond class of model, one realization of which has nonlocal dynamical\nrules.). We allow the transformer to know that the rates for this\ndynamics are independent of time, and that possible moves consist of\nsingle particles rotating in place or translating one lattice site at a time\n(both restrictions can be relaxed within our framework). However, we\ndo not allow the transformer to know the rates for each move, and,\nbecause each rate could in principle depend on the state of the entire\nsystem, explicit enumeration of rates would require a generator with\nmany more than 10\n100 entries for the system size considered. From\nobservation of a single trajectory of the model, carried out at a density\nat which its steady state comprises small, dispersed clusters, the\ntransformer learns that particle moves fall into a small number of\nclasses, and accurately determines the associated rates. Forward-\npropagated trajectoriesof the trained transformer at the training den-\nsity reproduce the model’s behavior. Moreover, forward-propagated\ntrajectories of the transformer carried out at densities higher than that\nused in training exhibit motility-induced phase separation (MIPS)\n31–35.\nT h ed e t a i l so ft h i sp h a s es e p a r a t i o nm a t c ht h o s es e e nu s i n gt h eo r i g i n a l\nmodel, although that information was not available to the transformer\nduring training. The trained transformer is therefore able to accurately\nextrapolate a learned dynamics to predict the existence and details of\nan emergent phenomenon that it had not previously observed. Given\nthat the transformer is expressive enough to represent a nonlocal\ndynamics, these results indicate the potential of such devices to learn\ndynamical rules and study emergentphenomena from observations of\ndynamical trajectories in a wide variety of settings.\nImagine that we are given a dynamical trajectoryω of total timeT.\nThe trajectory starts in conﬁguration (microstate)C\n0,a n dv i s i t sK\nadditional conﬁgurationsCk (Fig. 1a). In conﬁguration Ck it is resident\nFig. 1 | Schematic of our dynamics-learning procedure. aWe are provided with a\ntrajectoryω, a time series of conﬁgurations, and wish to learn the dynamics that\ncreated it. For the lattice-based active-matter model studied here, red or blue\nindicates a particles whose orientation vector points toward an occupied or empty\nsite, respectively.b We parameterize a general dynamics using a neural network\ncalled a transformer. Rates connecting conﬁgurations depend on the weights of\nthe transformer, which are adjusted during training in order to maximize the log-\nlikelihood with which it would have generatedω. c The transformer receives the\nposition and orientation of all particles, and must calculate the transition rates to\ntranslate or rotate each particle. To do so, it must learn which interactions affect\nthese rates (line thickness denotes attention given to each particle), and their\nnumerical values.d Once trained, the neural-network dynamics can be forward-\npropagated to generate new trajectories, even under conditions not observed inω.\nThe transformer calculates the rates for all possible transitionsC\nk !f C0\nk g, repre-\nsented by the blue blobs, at each step.\nArticle https://doi.org/10.1038/s41467-024-45629-w\nNature Communications|         (2024) 15:1875 2\nfor timeΔtCk\n. Schematically,\nω = C0 /C0!\nΔtC0\nC1 /C0!\nΔtC1\n/C1/C1/C1 CK/C0 1 /C0!\nΔtCK/C0 1\nCK /C0!\nΔtK\nCK ,\nwhere ΔtK /C17 T /C0 PK/C0 1\nk =0 ΔtCk\n. We are told thatω was generated by a\ndynamics whose ratesW?\nC!C0 for passing between conﬁgurationsC and\nC0 we do not know. We will call this unknown dynamics the original\ndynamics. Here we show it is possible to efﬁciently learn the original\ndynamics ofﬂine, i.e., solely by observation ofω.W es t a r tb yc o n -\nstructing a synthetic dynamics, which consists of a set of allowed\nconﬁguration changesfC ! C\n0g (which must include those observed in\nω) and associated ratesWðθÞ\nC!C0 . Without prior knowledge of the system\nwe should allow the rates for these moves to depend, in principle, on\nthe entire conﬁguration of the system. The number of possible rates\ngrows exponentially with system size, and so treating a system of\nappreciable size requires the use of an expressive parameterization of\nthe synthetic dynamics. Here we parameterize the ratesW\nðθÞ\nC!C0 of the\nsynthetic dynamics using the weightsθ of a neural network.\nOne way to learn the original dynamics is to propagate the syn-\nthetic dynamics and alter its parametersθ until the dynamical trajec-\ntories it generates resembleω. One drawback of this approach is that\noriginal and synthetic dynamics are stochastic, and so comparison of\ntrajectories can be made only in a statistical sense, potentially requir-\ning the generation of many synthetic trajectories at each stage of\ntraining. In addition, a comparison of this nature would require the\nintroduction of additional order parameters, different combinations\nof which may result in different outcomes of training. Instead, we train\nthe synthetic dynamics by maximizing the log-likelihoodU\nðθÞ\nω with\nwhich it would have generatedω19. We consider continuous-time\nMonte Carlo dynamics, in which case\nUðθÞ\nω =\nXK/C0 1\nk =0\nln WðθÞ\nCk !Ck +1\n/C0 ΔtCk\nRðθÞ\nCk\n/C16/C17\n/C0 ΔtK RðθÞ\nCK\n; ð1Þ\nsee Methods. Training proceeds by adjusting the parametersθ of the\nneural network untilUðθÞ\nω no longer increases; see Fig.1(b). For a suf-\nﬁciently long trajectoryω, the dynamics that maximizesUðθÞ\nω is the\noriginal dynamics,W?\nC!C0 . The synthetic dynamics obtained in this way\n— the learned dynamics— is then the best approximation to the original\ndynamics that our choice of allowed conﬁguration changes and\nmethod of training allows. We focus here on the case of continuous-\ntime Monte Carlo dynamics and lattice conﬁgurations, but the method\ncan be straightforwardly adapted to other scenarios. Working with\nanother class of dynamics (e.g., Langevin dynamics) requires deﬁning a\nreplacement for the trajectory log-likelihood Eq. (1). Working with off-\nlattice conﬁgurations requires an appropriate parameterization of the\npossible microscopic moves, but the transformer itself is not restricted\nto taking lattice-based conﬁgurations as inputs.\nResults and discussion\nThe original dynamics we consider is a lattice model of active matter\nsimulated using continuous-time Monte Carlo30 (we also consider a\nlattice model of a supercooled liquid in the SI, see Supplementary\nFigs. S1–S5). It consists of a two-dimensional periodic square lattice of\nsize L\n2, occupied by n volume-excluding particles. Each particle\nα ∈ {1, … , n} possesses a unit orientation vectoreα that points toward\none of the four neighboring sites. The orientation vector of each par-\nticle rotatesπ/2 clockwise or counter-clockwise with rateD.Ap a r t i c l e\nmoves to a vacant adjacent lattice site with ratev\n+ if it points toward\nthat lattice site, and with ratev0 otherwise. The steady state of this\nmodel depends on the particle densityϕ = n/L2.A ts m a l lv a l u e so fϕ,\ntypical conﬁgurations consist of small clusters of particles. Upon\nincreasing ϕ,f o rs u fﬁciently large v+, the system undergoes the\nnonequilibrium phase transition called MIPS. We shall show that the\nexistence of this phase transition can be deduced by observation of a\nsingle trajectory obtained at a value ofϕ at which MIPS is not present.\nWe introduce a general synthetic dynamics using a neural-\nnetwork architecture called a transformer9.W ea l l o wt h et r a n s f o r m e r\nto know only that the dynamics is time-independent and consists of\nsingle-particle translations and rotations, though these restrictions can\nbe lifted within this framework. In microstateC,t h et r a n s f o r m e r\nrepresents the transition ratesW\nðθÞ\nC!C0 to each of the microstatesC0\nconnected to C through translation or rotation of a single particle\n(Fig. 1c). The transformer learns which particle interactions are rele-\nvant to each of these moves, and what their rates are. To train the\ntransformer we perform gradient descent on its weights using back-\npropagation in order to maximize the log-likelihoodU\nðθÞ\nω ,E q .(1), with\nwhich it would have generated ω. This trajectory is of length\nT =5×1 03, using a 30 × 30 lattice, with parameters\nϕ =0 :124, v+ =1 0 ,v0 =1 , a n dD = 0.1. MIPS is not present at these\nparameter values; see Fig.3.\nDuring training we operate the transformer in one of two modes.\nIn Mode 1, the transformer freely predicts lnWðθÞ\nC!C0 for each possible\ntransition. In Mode 2, the transformer assigns each transition to one of\nan integer numberN\nðθÞ\nW of classes, and a second neural network assigns\nav a l u el nWðθÞ\nC!C0 to each class.NðθÞ\nW is a hyperparameter that constrains\nthe complexity of the learned dynamics, and provides a measure of the\nnumber of distinct classes of move (or processes) present in the ori-\nginal dynamics: the maximum value ofU\nðθÞ\nω obtained under training\nincreases withNðθÞ\nW up to a valueN?\nW .T h ev a l u eN?\nW provides insight\ninto the structure of the generator of the original dynamics, signaling,\nfor instance, the presence of translational invariance. In Methods,\nadditional details of the architectures of both types of neural-network\ndynamics and their optimization are provided. We have used lattice\nmodels in this paper, but the transformer architecture can be directly\napplied to off-lattice models in any dimension.\nIn Fig.2a we show the results of training in Mode 1. The trajectory\nlog-likelihoodU\nðθÞ\nω increases with the number of observations (epochs)\nof the trajectoryω, and converges to the valueU?\nω that is obtained\nusing the original dynamics. This value, not available to the transfor-\nmer during training, indicates that the learned transition ratesW\n(θ) are\nnumerically very close to those of the original dynamics,W ⋆. In Fig.2b\nwe show the results of training in Mode 2, for several values ofNðθÞ\nW .\nThese results show thatN?\nW = 4, indicating that the transformer has\ncorrectly learned the degree of complexity of the original model,\nwhose dynamical rules are translationally invariant and consist of 4\ndistinct rates. The inset to Fig.2b shows the evolution with training\ntime of the values of the 4 rates, compared with their values in the\noriginal model.\nDuring training we did not assume that the dynamical rules are\nlocal, nor that some processes (those that violate volume exclusion)\nare suppressed. The transformer was able to learn both things. If we\nknow that interactions are ofﬁn i t er a n g et h e ns u c hk n o w l e d g ec a nb e\nused to reduce the number of transformer parameters required to\nlearn dynamics (see the SI). Transformers can also learn long-ranged\ninteractions if they are present, which we illustrate in Supplementary\nFigs. S6 and S7 in the SI. We also note that learned rates for forbidden\nprocesses (inset Fig.2b) are small and decrease with training time, but\nare not exactly zero: the result is that in forward-propagated trajec-\ntories a small fraction of particles can experience overlaps. If volume\nexclusion is suspected then it can be imposed directly. In addition,\nwith Monte Carlo methods it is possible to determine that the rate of a\nforbidden process is exactly zero, even given aﬁnite-length training\ntrajectory; see Table S1 in the SI.\nIn Fig. 3 we show that trajectories generated by the trained\ntransformer can be used to determine the existence of a none-\nquilibrium phase transition not seen during training. We randomly\ninitialize a conﬁguration at a chosen densityϕ and propagate the\ntransformer dynamics forﬁxed timeT (see Fig.1d and Methods). At the\nArticle https://doi.org/10.1038/s41467-024-45629-w\nNature Communications|         (2024) 15:1875 3\ntraining densityϕ =0 :124, the model’s steady state consists of small\nclusters, but trajectories generated by the transformer at larger values\nof ϕ show MIPS: the transformer has therefore predicted this emergent\nphenomenon.\nIn Fig. 4 we quantify the details of this phase separation. We\nmeasure the fraction of particles with four neighboring occupied sites\nf\n4, and the variance of that quantity, as well as the number of clustersnc\nand the average cluster sizesc. The time averages of these observables\nare shown as a function ofϕ for trajectories obtained with the trans-\nformer, both in Mode 1 and Mode 2. For comparison, we show the\nsame quantities from trajectories generated using the original\ndynamics. The agreement between original and learned dynamics is\ngood, and slightly better using Mode 2, indicating that the transfor-\nmer, trained under conditions for which no phase separation is\nobserved (see the vertical line in theﬁgure), has predicted the exis-\ntence and details of a non-equilibrium phase transition (we have ver-\niﬁed that we can similarly learn the dynamics at high density and\naccurately predict the behavior at low density).\nWe have shown that the stochastic dynamics of a many-body\nsystem can be efﬁciently determined using machine-learning tools\ndeveloped for language processing. A neural network called a trans-\nformer can function as an expressive ansatz for the generator of a\nmany-body dynamics, for systems large enough that its possible rates\nare too numerous to represent explicitly. For instance, for the lattice\nmodel of active matter considered here, a 30 × 30 lattice at density\nϕ =0 . 1a d m i t s900\n90\n/C18/C19\n∼10\n125 arrangements of particles. Each particle\ntakes 1 of 4 rotational states, can move in 4 directions and undergo 2\ntypes of rotation, meaning that there are in principle∼10180 possible\nrates. Trained on this model, the transformer learns its dynamics,\ncorrectly identifying its local and translationally-invariant nature, and\nthe numerical values of the associated rates. Forward-propagated\ntrajectories of the transformer, carried out at higher densities than that\nobserved during training, show MIPS. The details of this none-\nquilibrium phase transition predicted by the transformer agree with\nthose of the original model. Our work shows that it is possible to learn\nthe dynamical rules of stochastic systems without explicit enumera-\ntion of rates or coarse-graining of conﬁguration space, complementing\nexisting papers on learning dynamics and pointing the way to the\ntreatment of large and complex systems.\nMethods\nDerivation of the path weight of a continuous-time Monte Carlo\ndynamics\nConsider a dynamical trajectoryω of total timeT, which starts in con-\nﬁgurationC0 and visitsK additional conﬁgurationsCk. Schematically,\nω = C0 /C0!\nΔtC0\nC1 /C0!\nΔtC1\n/C1/C1/C1 CK/C0 1 /C0!\nΔtCK/C0 1\nCK /C0!\nΔtK\nCK ,\nwhere ΔtCk\nis the time spent in con ﬁguration Ck and ΔtK /C17\nT/C0 PK/C0 1\nk =0 ΔtCk\n.\nThe trajectoryω was generated by a continuous-time Monte Carlo\ndynamics (the original dynamics), whose rates whose ratesW?\nC!C0 for\npassing between conﬁgurationsC and C0 are unknown. In order to learn\nthe original dynamics, we introduce a new continuous-time Monte\nCarlo model called the synthetic dynamics. The synthetic dynamics\nconsists of a set of allowed conﬁguration changesfC ! C\n0g (which\nmust include those observed inω) and associated ratesWðθÞ\nC!C0 .R a t e s\nare parameterized by a vectorθ ={ θ1, … , θN}o fN numbers (in the main\ntext these numbers corresponds to the weights of the transformer).\nWe train the synthetic dynamics by maximizing the log-likelihoodU\nðθÞ\nω\nwith which it would have generatedω. To calculateUðθÞ\nω we start by\nconsidering the portion\nCk /C0!\nΔtCk\nCk +1\nð2Þ\nFig. 3 | Trajectories of the lattice active-matter model generated using the\ndynamics learned by the transformer.The top row shows time-ordered snap-\nshots of a trajectory generated at densityϕ =0 :124, the value used during training.\nThe two middle rows use densitiesϕ =0 . 3a n dϕ = 0.5; here, motility-induced phase\nseparation can be seen. For comparison, the bottom row shows a trajectory gen-\nerated with the original dynamics atϕ =0 . 5 .\nFig. 2 | Learning the dynamics of the lattice active-matter model. aTraining of a\ntransformer in Mode 1 (unrestricted rates) to maximize the log-likelihoodUðθÞ\nω ,E q .\n(1), of the training trajectoryω. The horizontal black line denotes the value of the\npath weight associated with the original model.b Dependence ofUðθÞ\nω for a trans-\nformer trained in Mode 2, in which it is asked to identifyNðθÞ\nW distinct classes of\nmove. This procedure allows us to identify the existence ofN?\nW = 4 distinct rates.\nInset: Evolution of the rates during training in Mode 2, withNðθÞ\nW = 4. The horizontal\nblack lines denote the values of the rates in the original dynamics.\nArticle https://doi.org/10.1038/s41467-024-45629-w\nNature Communications|         (2024) 15:1875 4\nof ω, which involves a transitionCk ! Ck +1 and a residence timeΔtCk\n.\nThe probability with which the synthetic dynamics would have gen-\nerated the transitionC\nk ! Ck +1 is\nWðθÞ\nCk !Ck +1\n=RðθÞ\nCk\n, ð3Þ\nwhere RðθÞ\nCk\n/C17 P\nC0 WðθÞ\nCk !C0 , the sum running over all transitions allowed\nfrom Ck. The probability density with which the synthetic dynamics\nwould have chosen the associated residence timeΔtCk\nis\nRðθÞ\nCk\ne\n/C0 ΔtCk RðθÞ\nCk ð4Þ\nThe product of transition- and residence-time factors is\nWðθÞ\nCk !Ck +1\ne\n/C0 ΔtC RðθÞ\nCk /C17 pCk\n: ð5Þ\nNoting that the probability of theﬁnal portion of the trajectory,\nCK /C0!\nΔtK\nCK ,i s\n1 /C0\nZ ΔtK\n0\ndτ RðθÞ\nCk\ne\n/C0 RðθÞ\nCk\nτ\n=e\n/C0 ΔtK RðθÞ\nCK /C17 pK , ð6Þ\nthe log-likelihood with which thesynthetic dynamics would have\ngeneratedω is\nUðθÞ\nω =l n pK\nYK/C0 1\nk =0\npCk\n !\n=\nXK/C0 1\nk =0\nln WðθÞ\nCk !Ck +1\n/C0 ΔtCk\nRðθÞ\nCk\n/C16/C17\n/C0 ΔtK RðθÞ\nCK\n:\nð7Þ\nThe sum in (7) is taken over the trajectoryω, i.e., over all conﬁguration\nchanges and corresponding residence times (we note that working\nwith the probabilityRðθÞ\nCk\ne\n/C0 ΔtCk RðθÞ\nCk ΔtCk\nfor the residence time gives rise\nto an additional termPK/C0 1\nk =0 ΔtCk\nin (7) that does not depend on the\nchoice of synthetic dynamics and may be omitted without con-\nsequence). To train the synthetic dynamics we adjust its parametersθ\nuntil (7)n ol o n g e ri n c r e a s e s .\nNeural-network architecture and training\nThe neural network used to treat the active-matter model described in\nthe main text (and the models described in the SI) is a transformer9,\noriginally developed for language processing. We have opted for this\narchitecture for two main reasons: (1) a transformer does not intro-\nduce a bias toward interaction ranges when learning the dynamics\nmost likely to have generated the observed trajectory, and (2) a\ntransformer can efﬁciently learn symmetries and locality in the inter-\naction rules. This ability stands in contrast to other neural-network\narchitectures such as fully-connected neural networks or convolu-\ntional neural networks. A convolutional neural network, for instance, is\nparameterized using small kernels which slide along the input con-\nﬁguration. This means that in order to capture long-range interactions\nin the data, we need to apply many convolutional layers successively,\nand the choice of neural-network depth introduces a bias on the range\nof interactions we want to learn. Likewise, the weight sharing of the\nkernels in the convolutional layer introduces a bias toward transla-\ntional invariance of the interaction rules. A fully-connected neural\nnetwork does consider interactions between all elements of the sys-\ntem, but because it lacks meaningful positional information it can not\nefﬁciently learn whether interactions are local, or whether there are\nsymmetries present in the data.\nA transformer possesses an attention mechanism— explained\nbelow— that allows it to learn which parts of a conﬁguration are rele-\nvant for a particular process. This generality ensures that it is not\nbiased toward learning local interactions, as is the case for e.g., con-\nvolutional neural networks, but can efﬁciently learn locality if needed.\nThe ﬁr s ts t e pi nc a l c u l a t i n gt h et r a n s i t i o nr a t e si sal e a r n e d\nrepresentation of the current state of the system. Weﬁrst embed\nparticle positions and orientations asd\nh-dimensional vectors using\ntrainable weight matrices;dh is a hyperparameter controlling the\nexpressivity of our neural-network model. For the positional embed-\nding of the active matter model, we map thex-a n dy-coordinate of\neach particle to a vector of sized\nh/2 using a weight matrix, and then\nconcatenate these representations. For computational efﬁciency, we\ndo not use the empty sites. Instead, the transformer must learn which\nneighboring sites are occupied for each particle through the positional\nembedding. We do not impose the boundary conditions of our lattice\nmodels; the transformer has to learn these through its positional\nembedding.\nWe then sum the representations of the position and spin for each\nparticle, which serve as the input to theﬁrst layer of the transformer.\nNext, we calculate the attention matrix for the conﬁguration using\nscaled dot-product attention\n9. This means that we construct a query,\nkey, and value vector for each input particle through a linear trans-\nformation. We match the query vector of each particle against all the\nkeys through a dot product, resulting in an attention score for all\ncombinations of keys with the query. These scores are then normal-\nized, and the output of the attention layer is obtained through a sum of\nthe value vectors of every particle, each weighted by the attention\nscore. As a result, we obtain ad\nh-dimensional vector for each particle,\ncontaining a weighted sum of features of all other particles (the\nweighting being a measure of the attention paid to each particle). This\nFig. 4 | Quantitative comparison of the learned and original dynamics at dif-\nferent densities. aTime-averaged fraction of particles with four neighboring\noccupied sites,f4, as a function of densityϕ, averaged over 10 trajectories of length\n104, generated using a transformer trained in Mode 1 (crosses) and Mode 2\n(plusses). Training was done only atϕ =0 :124 (vertical dashed line). Squares\ndenote results obtained using the original dynamics. The remaining panels have\nthe same format and show (b) the variance off4,( c) the number of clustersnc,a n d\n(d) the averaged cluster sizesc. Angle brackets denote time averages.\nArticle https://doi.org/10.1038/s41467-024-45629-w\nNature Communications|         (2024) 15:1875 5\nmechanism can be applied in parallel, where multiple key, query, and\nvalue combinations (“heads”) are produced at each stage, allowing to\nattend to different properties of the input sentence simultaneously.\nThese vectors are then processed using fully-connected neural\nnetworks, where we apply the same fully-connected network to each\nvector. We apply this alternating process of attention and application\nof fully-connected neural networksn\nl times. Theﬁnal output of these\ntransformations is used to calculate the transition rate for each pos-\nsible particle update (a particle rotation or translation for the active-\nmatter model).\nTraining in Mode 1, the rates are obtained by applying a fully-\nconnected neural network to the output vectors of the transformer.\nWe apply the same network for each particle. This fully-connected\nneural network has one output node for each possible particle update,\nthe value of lnW assigned to the corresponding transition. Training in\nMode 2, weﬁrst classify the transformer’s output vectors using a fully-\nconnected neural network withN\nW output nodes and a softmax acti-\nvation function, again for each possible particle update. The class with\nthe highest probability is sent, as a one-hot vector, to another fully-\nconnected neural network with one output node, which calculates the\nvalue of lnW for each of theN\nW classes. Picking the highest-probability\nclass is not a differentiable operation, and so we use a straight-through\nestimator to obtain the gradients to optimize these neural networks\n36.\nThe results in this paper were obtained with the hyperparameters\ndh =6 4 a n dnl = 2. We used the AdaBelief optimizer37 with a learning\nrate of 10−4 to optimize the transformer’s weights. To obtain a baseline\nfor the trajectory log-likelihoodUðθÞ\nω ,w e ﬁrst train a Mode 1 neural-\nnetwork dynamics on the provided trajectory. For efﬁciency we train\nfor several epochs on smaller sections of the trajectory; during theﬁnal\nstages of training we use the entire trajectory to obtain more accurate\ngradients of the trajectory log-likelihood. Next, we train a Mode 2\nneural-network dynamics to gain insight into the model’s generator.\nWe initialize theﬁrst layers of the neural network (the embedding and\ntransformer layers) with the weights obtained with the Mode 1\ndynamics, which leads to much faster convergence.\nWe have here assumed that the dynamics are independent of\ntime, and the only possible moves are single-particle translations and\nrotations. We note that these assumptions may also be lifted: time\ncould be used as an additional input to the neural network, and col-\nlective updates could be achieved using an encoder-decoder archi-\ntecture as used in language translation\n9.\nThe transformer architecture can by construction be applied to\nconﬁgurations consisting of a different number of particles (much like\ntransformers used in natural language processing can be used to\nmodel sentences with a different number of words). The transformer\nreceives as input a sequence ofn particles (i.e., their position and their\nstate), and returns the transition rates for each particle in the input\nsequence. This means that we can naturally apply the trained trans-\nformer to lattice conﬁgurations of the active matter model at the same\nsystem size, but at a different particle density than seen during train-\ning. In order to provide accurate results at a different density, the\ntransformer has to have learned an accurate representation of how\nparticles interact with one another through its positional embedding.\nData availability\nTraining trajectories can be generated using the code in Ref.38.\nCode availability\nTraining code and a tutorial for learning dynamics can be found in\nref. 38.\nReferences\n1. Prinz, J.-H. et al. Markov models of molecular kinetics: Generation\nand validation.J. Chem. Phys.134, 174105 (2011).\n2 . B o w m a n ,G .R . ,P a n d e ,V .S .&N o é ,F .An introduction to Markov state\nmodels and their application to long timescale molecular simulation,\nvol. 797 (Springer Science & Business Media, 2013).\n3. Hoffmann, M. et al. Deeptime: a python library for machine learning\ndynamical models from time series data.Mach. Learn.: Sci. Technol.\n3, 015009 (2021).\n4. Wu, H. & Noé, F. Variational approach for learning markov pro-\ncesses from time series data.J. Nonlinear Sci.30,2 3– 66 (2020).\n5 . M a r d t ,A . ,P a s q u a l i ,L . ,W u ,H .&N o é ,F .V a m p n e t sf o rd e e pl e a r n i n g\nof molecular kinetics.Nat. Commun.9,1 – 11 (2018).\n6 . S u p e k a r ,R . ,S o n g ,B . ,H a s t e w e l l ,A . ,C h o i ,G . P . ,M i e t k e ,A .&D u n k e l ,\nJ. Learning hydrodynamic equationsf o ra c t i v em a t t e rf r o mp a r t i c l e\nsimulations and experiments.Proc. Natl Acad. Sci.120,\ne2206994120 (2023).\n7. Maddu, S., Vagne, Q. & Sbalzarini, I. F. Learning deterministic\nhydrodynamic equations from stocha s t i ca c t i v ep a r t i c l ed y n a m i c s .\narXiv preprint arXiv:2201.08623 (2022).\n8 . T s a i ,S . - T . ,K u o ,E . - J .&T i w a r y ,P .L e a r n i n gm o l e c u l a rd y n a m i c sw i t h\nsimple language model built upon long short-term memory neural\nnetwork.Nat. Commun.11, 5115 (2020).\n9. Vaswani, A. et al. Attention is all you need. InAdvances in neural\ninformation processing systems, 5998– 6008 (2017).\n10. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n11. Radford, A. et al. Language models are unsupervised multitask\nlearners (2019).\n12. Brown, T. B. et al. Language models are few-shot learners.Advances\nin neural information processing systems33,1 8 7 7– 1901 (2020).\n13. Parmar, N. et al. Image transformer. InInternational Conference on\nMachine Learning,4 0 5 5– 4064 (PMLR, 2018).\n14. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers\nfor image recognition at scale.ICLR (2021).\n1 5 . L e v i n e ,S . ,K u m a r ,A . ,T u c k e r ,G .&F u ,J .O fﬂine reinforcement\nlearning: tutorial, review, and perspectives on open problems. arXiv\npreprint arXiv:2005.01643 (2020).\n16. Wulff, N. & Hertz, J. A. Learningcellular automaton dynamics with\nneural networks.Adv. Neural Inf. Process. Syst.5,6 3 1– 638 (1992).\n17. Gilpin, W. Cellular automata as convolutional neural networks.\nP h y s .R e v .E100,0 3 2 4 0 2( 2 0 1 9 ) .\n18. Grattarola, D., Livi, L. & Alippi, C. Learning graph cellular automata.\nAdv. Neural Inf. Process. Syst.34,2 0 9 8 3– 20994 (2021).\n19. McGibbon, R. T. & Pande, V. S. Efﬁcient maximum likelihood para-\nmeterization of continuous-time markov processes.\nJ. Chem. Phys.\n143, 034109 (2015).\n2 0 . H a r u n a r i ,P .E . ,D u t t a ,A . ,P o l e t t i n i ,M .&R o l d á n ,É .W h a tt ol e a r nf r o m\na few visible transitions’ statistics?Phys. Rev. X12, 041026 (2022).\n21. Frishman, A. & Ronceray, P. Learning forceﬁelds from stochastic\ntrajectories.Phys. Rev. X10, 021009 (2020).\n2 2 . G a r c í a ,L .P . ,P é r e z ,J .D . ,V o l p e ,G . ,A r z o l a ,A .V .&V o l p e ,G .H i g h -\nperformance reconstruction of microscopic forceﬁelds from\nbrownian trajectories.Nat. Commun.9,1 – 9( 2 0 1 8 ) .\n23. Chen, X. Maximum likelihood estimation of potential energy in\ninteracting particle systems from single-trajectory data.Electron.\nCommun. Probab.26,1 – 13 (2021).\n2 4 . C a m p o s - V i l l a l o b o s ,G . ,B o a t t i n i ,E . ,F i l i o n ,L .&D i j k s t r a ,M .M a c h i n e\nlearning many-body potentials for colloidal systems.J. Chem. Phys.\n155,1 7 4 9 0 2( 2 0 2 1 ) .\n25. Ruiz-Garcia, M. et al. Discovering dynamic laws from observations:\nt h ec a s eo fs e l f - p r o p e l l e d ,i n t e r a c t i n gc o l l o i d s .a r X i vp r e p r i n t\narXiv:2203.14846 (2022).\n2 6 . L e m o s ,P . ,J e f f r e y ,N . ,C r a n m e r ,M . ,H o ,S .&B a t t a g l i a ,P .R e d i s -\ncovering orbital mechanics with machine learning.Mach. Learn.:\nSci. Technol.4,0 4 5 0 0 2( 2 0 2 3 ) .\nArticle https://doi.org/10.1038/s41467-024-45629-w\nNature Communications|         (2024) 15:1875 6\n27. Wang, R., Kashinath, K., Mustafa, M., Albert, A. & Yu, R. Towards\nphysics-informed deep learning for turbulentﬂow prediction. In\nProc. 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining,1 4 5 7– 1466 (2020).\n28. Wang, R. & Yu, R. Physics-guided deep learning for dynamical\nsystems: A survey. arXiv preprint arXiv:2107.01272 (2021).\n29. Pershin, A., Beaume, C., Li, K. & Tobias, S. M. Training a neural\nnetwork to predict dynamics it has never seen.P h y s .R e v .E107,\n014304 (2023).\n30. Whitelam, S., Klymko, K. & Mandal, D. Phase separation and large\ndeviations of lattice active matter.J. Chem. Phys.148,\n154902 (2018).\n31. Gonnella, G., Marenduzzo, D., Sum a ,A .&T i r i b o c c h i ,A .M o t i l i t y -\ninduced phase separation and coarsening in active matter.Comp-\nt e sR e n d u sP h y s i q u e16,3 1 6– 331 (2015).\n32. Cates, M. E. & Tailleur, J. Motility-induced phase separation.Annu.\nRev. Condens. Matter Phys.6, 219 (2015).\n33. O’Byrne, J., Solon, A., Tailleur, J. & Zhao, Y. An introduction to\nmotility-induced phase separation in Out-of-equilibrium Soft Mat-\nt e r ,( e d sK u r z t h a l e r ,C . ,G e n t i l e ,L .&S t o n e , H .A . )c h .4 ,p p .1 0 7– 150\n(The Royal Society of Chemistry, 2023).\n3 4 . R e d n e r ,G .S . ,W a g n e r ,C .G . ,B a s k a r a n ,A .&H a g a n ,M .F .C l a s s i c a l\nnucleation theory description of active colloid assembly.Phys. Rev.\nLett. 117,1 4 8 0 0 2( 2 0 1 6 ) .\n35. Omar, A. K., Klymko, K., GrandPre, T. & Geissler, P. L. Phase diagram\nof active brownian spheres: crystallization and the metastability of\nmotility-induced phase separation.P h y s .R e v .L e t t .126,\n188002 (2021).\n36. Jang, E., Gu, S. & Poole, B. Categorical reparameterization with\ngumbel-softmax. arXiv preprint arXiv:1611.01144 (2016).\n37. Zhuang, J. et al. Adabelief optimizer: Adapting stepsizes by the\nbelief in observed gradients.Conf. Neural Inf. Process. Syst.33,\n18795– 18806 (2020).\n3 8 . C a s e r t ,C . ,T a m b l y n ,I .&W h i t e l a m ,S .L e a r n i n gs t o c h a s t i cd y n a m i c s\nand predicting emergent behavior using transformers (2024).\nhttps://zenodo.org/doi/10.5281/zenodo.10521014.\nAcknowledgements\nThis work was performed as part of a user project at the Molecular\nFoundry, Lawrence Berkeley National Laboratory, supported by the\nOfﬁce of Science, Ofﬁce of Basic Energy Sciences, of the U.S. Depart-\nment of Energy under Contract No. DE-AC02– 0 5 C H 1 1 2 3 1 .C . C .w a s\nsupported through a Francqui Fellowship of the Belgian American\nEducational Foundation, and through FWO grant V426923N. I.T.\nacknowledges NSERC. The computational resources (Stevin Super-\ncomputer Infrastructure) and services used in this work were provided\nby the VSC (Flemish Supercomputer Center), funded by Ghent Uni-\nversity, FWO and the Flemish Government - department EWI, and the\nNational Energy Research Scientiﬁc Computing Center (NERSC), a U.S.\nDepartment of Energy Ofﬁce of Science User Facility operated under\nContract No. DE-AC02-05CH11231.\nAuthor contributions\nS.W. and I.T. initiated the study. C.C. designed the neural-network ansatz\nand performed the simulations discussed in the manuscript. S.W. did the\nanalytic work and the simulations of the FA model with a local ansatz\ndescribed in the SI. All authors discussed the results and contributed to\nwriting the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-45629-w.\nCorrespondenceand requests for materials should be addressed to\nCorneel Casert, Isaac Tamblyn or Stephen Whitelam.\nPeer review informationNature Communicationsthanks the anon-\nymous reviewers for their contribution to the peer review of this work.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-45629-w\nNature Communications|         (2024) 15:1875 7"
}