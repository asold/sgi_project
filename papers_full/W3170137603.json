{
    "title": "Template Filling with Generative Transformers",
    "url": "https://openalex.org/W3170137603",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2676585443",
            "name": "Xin-Ya Du",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2294834069",
            "name": "Alexander Rush",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A229573375",
            "name": "Claire Cardie",
            "affiliations": [
                "Cornell University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2078523032",
        "https://openalex.org/W2134486566",
        "https://openalex.org/W2984582583",
        "https://openalex.org/W2250836735",
        "https://openalex.org/W3024298906",
        "https://openalex.org/W2118161437",
        "https://openalex.org/W2093238926",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2169943035",
        "https://openalex.org/W2211728022",
        "https://openalex.org/W2407338347",
        "https://openalex.org/W2155524176",
        "https://openalex.org/W2974604908",
        "https://openalex.org/W2964206023",
        "https://openalex.org/W1579838312",
        "https://openalex.org/W2077929656",
        "https://openalex.org/W3035229828",
        "https://openalex.org/W3154063293"
    ],
    "abstract": "Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 909–914\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n909\nTemplate Filling with Generative Transformers\nXinya Du Alexander M. Rush Claire Cardie\nDepartment of Computer Science\nCornell University\n{xdu, cardie}@cs.cornell.edu\narush@cornell.edu\nAbstract\nTemplate ﬁlling is generally tackled by a\npipeline of two separate supervised systems –\none for role-ﬁller extraction and another for\ntemplate/event recognition. Since pipelines\nconsider events in isolation, they can suffer\nfrom error propagation. We introduce a frame-\nwork based on end-to-end generative trans-\nformers for this task (i.e., G TT). It naturally\nmodels the dependence between entities both\nwithin a single event and across the multiple\nevents described in a document. Experiments\ndemonstrate that this framework substantially\noutperforms pipeline-based approaches, and\nother neural end-to-end baselines that do not\nmodel between-event dependencies. We fur-\nther show that our framework speciﬁcally im-\nproves performance on documents containing\nmultiple events.\n1 Introduction\nThe classic template-ﬁlling task in information ex-\ntraction involves extracting event-based templates\nfrom documents (Grishman and Sundheim, 1996;\nJurafsky and Martin, 2009; Grishman, 2019). It is\nusually tackled by a pipeline of two separate sys-\ntems, one for role-ﬁller entity extraction – extract-\ning event-relevant entities (e.g., noun phrases) from\nthe document; another for template/event recogni-\ntion – assigning each of the candidate role-ﬁllers to\nthe event(s)/template(s) that it participates in and\nidentifying the type of each event/template.\nSimpliﬁcations of the task (Patwardhan and\nRiloff, 2009; Huang and Riloff, 2011, 2012; Du\net al., 2020) assume that there is one generic tem-\nplate and focus only on role-ﬁller entity extraction.\nHowever, real documents often describe multiple\nevents (Figure 1). From the example, we can ob-\nserve that between-event dependencies are impor-\ntant (e.g., a single organization can participate in\nmultiple events) and can span the entire document\n(e.g., event-speciﬁc targets can be distant from their\nSeveral attacks were carried out in La Paz last night, one in front of government house ...The self-styled \"Zarate armed forces\" sent simultaneous written messages to the media, calling on the people to oppose ...The first attack occurred at 22:30 in front of the economic ministry, just before President Paz Zamora concluded his message to ...Roberto Barbery, has reported that dynamite stickswere hurled from a car.The second attack occurred at 23:35, just after the cabinet members had left government housewhere they had listened to the presidential message.A bombwas placed outside government house in the parking lot that is used by cabinet ministers.  The police ...As of 5:00  today, people found that an old shack on the estate was set ablaze, \nEvent 2 TemplateBombingPerpetrator Indiv.-Perpetrator OrgZarate armed forcesPhysical Targetgovernment house WeaponbombVictim-\nEvent 1 TemplateAttackPerpetrator Indiv.-Perpetrator OrgZarate armed forcesPhysical Targeteconomic ministryWeapondynamite sticks Victim-\nEvent 3 TemplateArsonPerpetrator Indiv.-Perpetrator OrgZarate armed forcesPhysical Targetold shack Weapon-Victim-\nFigure 1: The template-ﬁlling task. Role-ﬁller entity\nextraction is shown on the left, and template recogni-\ntion is shown on the right. Our system performs both\nof these document-level tasks with a single end-to-end\nmodel.\nshared perpetrator organization). Alternative end-\nto-end event extraction models, even those incorpo-\nrating pretrained LM representations, only model\nevents in isolation (Wadden et al., 2019; Du and\nCardie, 2020), and are mainly evaluated on ACE-\nstyle (Doddington et al., 2004) event extraction\nfrom single sentences (Yang and Mitchell, 2016;\nLin et al., 2020).\nTo naturally model between-event dependencies\nacross a document for template ﬁlling, we pro-\npose a framework called “ GTT” based on gener-\native transformers (Figure 2). To our best knowl-\nedge, this is the ﬁrst attempt to build an end-to-end\nlearning framework for this task. We build our\nframework upon GRIT (Du et al., 2020), which\ntackles role-ﬁller entity extraction (REE), but not\ntemplate/event recognition. GRIT performs REE\nby “generating” a sequence of role-ﬁller entities,\none role at a time in a prescribed manner. For the\ntemplate-ﬁlling setting, we ﬁrst extend theGRIT ap-\nproach to include tokens representing event types\n910\nGenerative Transformers\nTarget tokens\nModel\n[CLS] Attack, Bombing, Arson, Kidnapping, ...[SEP_T](Document tokens): Several attacks were carried out in La Paz last night ...[SEP] Source tokens\n[CLS]Attack<T1 REEs>[SEP_T]Bombing<T2 REEs>[SEP_T]\nAttack<T1 REEs>[SEP_T]Bombing<T2 REEs>[SEP_T]...\nTemplate 1Template 2\nFigure 2: Our generative framework for end-to-end template ﬁlling.\n(e.g., “attack”, “bombing”) as part of the input se-\nquence. We further modify the decoder to attend\nto the event type tokens, allowing it to distinguish\namong events and associate event types to each\nrole-ﬁller entity that it generates.\nWe evaluate our model on the MUC-4 (1992)\ntemplate ﬁlling task. Empirically, our model sub-\nstantially outperforms both pipeline-based and end-\nto-end baseline models. In our analysis, we demon-\nstrate that our model is better at capturing between-\nevent dependencies, which are critical for docu-\nments that describe multiple events. Code and eval-\nuation scripts for the project is open-sourced at\nhttps://github.com/xinyadu/gtt.\n2 Task Deﬁnition: Template Filling\nAssume we are given a set of m event types (T1, ...,\nTm). Each event template contains a set of k roles\n(r1, ..., rk). For a document consisting n words\nx1, x2, ..., xn, the system is required to extract d\ntemplates, where d ≥ 0 (d is not given as input).\nEach template consists of k + 1 slots: the ﬁrst slot\nrepresents the event type (one of T1, ..., Tm). The\nrest of the k slots correspond to an event role (one\nof r1, ..., rk). The system is required to ﬁll in\nentities for the corresponding role, which may be\nﬁlled in as null.\n3 Methodology\nOur framework is illustrated in Figure 2. First\nwe transform the template ﬁlling task into a se-\nquence generation problem. Then, we train the\nbase model on the source-target sequence pairs,\nand apply the model to generate the sequence; ﬁ-\nnally the sequence is transformed back to structured\ntemplates.\n3.1 Template Filling as Sequence Generation\nWe ﬁrst transform the task’s input and output data\ninto specialized source and target sequence pair\nencodings. As shown in Figure 2 and below, the\nsource sequence consists of the words of the doc-\nument (x1, x2, ..., xn) prepended with the general\nset of tokens representing all event/template types\n(T1, ..., Tm); as well as a separator token denoting\nthe boundary between event templates ([SEP_T]).\nWe also add a classiﬁcation token ([CLS]) and an-\nother separator token ([SEP]) at the beginning and\nend of this source sequence. [CLS] works as the\nstart token, [SEP] denotes the boundary between\nREEs.\n[CLS] T1, ..., Tm [SEP_T]\nx1, x2, ..., xn [SEP]\nThe target sequence consists of the concatena-\ntion of template extractions, separated by the sep-\narator token ([SEP_T]). For template i, the sub-\nsequence consists of its event type T(i) and its role-\n911\nﬁller entity extractions < Role-ﬁller Entities >(i):\n[CLS] T(1), <Role-ﬁller Entities >(1)\n[SEP_T] T(2), <Role-ﬁller Entities >(2)\n...\n[SEP_T] T(i), <Role-ﬁller Entities >(i)\n...\nFor the < Role-ﬁller Entities > of template i,\nfollowing Du et al. (2020), we use the concatena-\ntion of target entity extractions for each role, sep-\narated by the separator token ([SEP]). Each entity\nis represented with its ﬁrst mention’s beginning (b)\nand end (e) tokens:\ne1\n1b , e1\n1e , ..[SEP] e2\n1b , e2\n1e , ..[SEP] e3\n1b , e3\n1e , ..\n3.2 Base Model and Decoding Constraints\nNext we describe the base model as well as special\ndecoding constraints for template ﬁlling.\nBERT as Encoder and Decoder Our model ex-\ntends upon the GRIT model for REE (Du et al.,\n2020). The base setup utilizes one BERT (Devlin\net al., 2019) model for processing both the source\nand target tokens embeddings. To distinguish the\nencoder / decoder representations, it uses partial\ncausal attention mask on the decoder side (Du et al.,\n2020). The joint sequence of source tokens’ em-\nbeddings (a0, a1, ...,am) and target tokens’ embed-\ndings (b0, b1, ...,bn) are passed through BERT to\nobtain their contextualized representations,\nˆ a0, ˆ a1, ...,ˆ alsrc , ˆb0..., ˆbltgt\n= BERT(a0, b1, ...,alsrc , b0, ...,bltgt )\nPointer Decoding For the ﬁnal decoder layer,\nwe replace word prediction with a simple pointer\nselection mechanism. For target time step t, we\nﬁrst calculate the dot-product between ˆbt and\nˆ a0, ˆ a1, ...,ˆ am,\nc0, c1, ..., clsrc = ˆbt · ˆ a0, ˆbt · ˆ a1, ...,ˆbt · ˆ alsrc\nThen we apply softmax to c0, c1, ..., clsrc to ob-\ntain the probabilities of pointing to each source\ntoken (which may be a word or an event type), test\nprediction is done with greedy decoding. At each\ntime step, argmax is applied to ﬁnd the source token\nwhich has the highest probability. The decoding\nstops when a stop token is predicted.\np0, p1, ..., plsrc = softmax(c0, c1, ..., clsrc )\nWe also add several special decoding constraints\nfor template ﬁlling: (1) downweighting factor\n(0.01) to the probability of generating [SEP] and\n[SEP_T], in order to calibrate recall; (2) decod-\ning cutoff stop when it ends the kth template\n(k =maximum number of events in one document);\n(3) a constraint to ensure that the pointers for the\nstart and end token for one entity are in order.\n4 Experiments\nWe conduct evaluations on the MUC-4\ndataset (1992). MUC-4 consists of 1,700\ndocuments with associated templates. We follow\nprior work in split: 1,300 documents for training,\n200 documents ( TST1+TST2) as the develop-\nment set and 200 documents ( TST3+TST4)\nas the test set. We use the metric for template\nﬁlling (Chinchor, 1992) and, as in previous work,\nmap predicted templates to gold templates during\nevaluation so as to optimize scores. We follow\ncontent-based mapping restrictions, i.e., the event\ntype of the template is considered essential for\nthe mapping to occur. 1 Missing template’s slots\nare scored as missing, spurious template’s slots\nare scored as spurious. Note that in our work,\nsince we do not extract the set ﬁllers other than\nthe event/template type, they do not affect the\nperformance.\nBaselines and Additional Related Work As an\nablation baseline, we employ a pipeline, GRIT-\nPIPELINE , that ﬁrst uses the GRIT model for role-\nﬁller entity extraction, and then assigns event types\nto each of the entities as a multi-label classiﬁcation\nproblem. We assign types by transforming the prob-\nlem to multi-class classiﬁcation (MCC) (Spolaor\net al., 2013). As there are 6 event types (i.e., kid-\nnapping, attack, bombing, robbery, arson, forced\nwork stoppage) in MUC-4, we use 26 labels for the\nMCC problem.\nWe also compare to end-to-end baselines\nwithout modeling between-event dependencies,\n1The content-based mapping restrictions were added to\nMUC-4 to prevent fortuitous mappings which occurred in\nMUC-3 (Chinchor, 1992).\n912\nModels Event Type P ERP IND PERP ORG TARGET VICTIM WEAPON\nGRIT-PIPELINE 62.28 38.40 35.36 36.30 54.97 53.45\nDYGIE++ (Wadden et al., 2019) 61.95 32.44 25.73 45.04 49.48 51.60\nSEQ TAGGING (Du and Cardie, 2020) 60.22 30.59 26.79 36.60 43.62 51.70\nGTT 67.44 44.04 41.79 32.39 54.12 59.71\nTable 1: Per-slot F1 score.\nDYGIE++ (Wadden et al., 2019) 2 is a span-\nenumeration based extractive model for informa-\ntion extraction. The model enumerates all the possi-\nble spans in the document and passes each represen-\ntation through a classiﬁer layer to predict whether\nthe span represents certain role-ﬁller entity and\nwhat the role is. SEQ TAGGING is a BERT-based se-\nquence tagging model for extracting the role-ﬁllers\nentities. A role-ﬁller entity can appear in templates\nof different event types (e.g., “Zarate armed force”\nappear in both attack and bombing event). For\nboth baselines, the prediction goal is multi-class\nclassiﬁcation. More specially, we adapt the DY-\nGIE++ output layer implementation to ﬁrst predict\nthe role-ﬁller entity’s role class, and then predicts\nits event classes conditioned on the entity’s role.\nNote that Chambers (2013) and Cheung et al.\n(2013) propose to do event schema induction with\nunsupervised learning. Given their unsupervised\nnature, empirically the performance is worse than\nsupervised models (Patwardhan and Riloff, 2009).\nThus we do not add these as comparisons.\nModels P R F1\nGRIT-PIPELINE 63.88 37.56 47.31\nDYGIE++\n(Wadden et al., 2019) 61.90 36.33 45.79\nSEQ TAGGING\n(Du and Cardie, 2020) 46.80 38.30 42.13\nGTT 61.69 42.36 50.23 ∗\nTable 2: Micro-average results on the full test set.\n5 Results and Analysis\nResults on the full test set are shown in Table 2. We\nreport the micro-average performance (precision,\nrecall and F1). We see that our framework substan-\ntially outperforms the baseline extraction models\nin precision, recall and F1, with approximately a\n4% F1 increase over the end-to-end baselines. It\noutperforms the GRIT-PIPELINE system by around\n3% F1 (∗ denotes p <0.05).\n2Our own re-implementation.\nModels P R F1 ∆\nGRIT-PIPELINE 65.17 26.05 37.22 -21.33%\nDYGIE++ 69.90 27.05 39.01 -14.81%\nSEQ TAGGING 51.00 29.06 37.02 -12.13%\nGTT 56.76 38.08 45.58 -9.26%\nTable 3: Performance on the subset of documents\nwhich contain more than one gold event. ∆: relative\nchange of F1, as compared to the Full Test setting.\nPer-slot F1 score is reported in Table 1. The\nresults demonstrate that our framework more of-\nten predicts the correct event type, performs better\non PERP IND and PERP ORG, and achieves slightly\nworse performance with GRIT-PIPELINE on roles\nthat appear later in the template (i.e., TAR-\nGET and VICTIM ). We also found that DY-\nGIE++ performs better on TARGET , mainly due to\nits high precision in role assignment for spans.\nBetween-Event Dependencies We also show re-\nsults (Table 3) on the subset of documents that\ncontains more than one gold event. We see the\nF1 score for all systems drops substantially, prov-\ning the difﬁculty of the task, as compared to the\nsingle/no event case. When compared to the Full\nTest setting in Table 2, the baselines all increase\nin precision and drop substantially in recall, while\nour approach’s precision and recall drop a little.\nThis change is understandable, as the baseline sys-\ntems are more conservative and tend to predict\nfewer templates. As the number of gold templates\nincreases, the fewer templates predictions have a\nbetter chance of getting matched, but their recall\ndrops as well.\nHow performance changes when E increases\nIn Figure 3, we see that when the number of gold\nevents in the document is smaller (E = 1, 2), our\napproach performs on par with the pipeline-based\nand DYGIE++ baselines. However, as E grows\nlarger, the baselines’ F1 drop signiﬁcantly (e.g.,\nover -10% as E grows from 2 to 3).\n913\nNumber of Events (E)\nF1\n30\n40\n50\n60\n70\nE=1 E=2 E=3 E=4\nGRIT-pipeline DyGIE++ Ours\nFigure 3: F1 on subset of documents with E events.\nQualitative Case Analysis Consider the input\ndocument (doc id TST3-MUC4-0080)3, which con-\ntains an attack and a bombing template. In the\ngold annotations, “Farabundo Marti National Lib-\neration Front” acts as PERP ORG in both events.\nOur model correctly extracts the two events and the\nPERP ORG in each while DYGIE++ only predicts\nthe attack event with its PERP ORG role entity cor-\nrectly. Although GRIT-PIPELINE gets both events\ncorrect, it failed to extract this PERP ORG entity for\nthe second event.\n6 Conclusion\nWe revisit the classic NLP problem of template\nﬁlling and propose an end-to-end learning frame-\nwork called GTT. Through modeling events rela-\ntion, our approach better captures dependencies\nacross the document and performs substantially\nbetter on multi-event documents.\nAcknowledgments\nWe thank the anonymous reviewers for helpful feed-\nback and suggestions. The work of XD and AMR\nwas supported by NSF CAREER 2037519; that\nof XD and CC was supported in part by DARPA\nLwLL Grant FA8750-19-2-0039.\nReferences\nNathanael Chambers. 2013. Event schema induction\nwith a probabilistic entity-driven model. In Proceed-\nings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, pages 1797–1807,\nSeattle, Washington, USA. Association for Compu-\ntational Linguistics.\nJackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-\nderwende. 2013. Probabilistic frame induction. In\nProceedings of the 2013 Conference of the North\n3For the full document, please refer to Appendix A.\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 837–846, Atlanta, Georgia. Association for\nComputational Linguistics.\nNancy Chinchor. 1992. Muc-4 evaluation metrics. In\nMUC.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nGeorge R Doddington, Alexis Mitchell, Mark A Przy-\nbocki, Lance A Ramshaw, Stephanie M Strassel, and\nRalph M Weischedel. 2004. The automatic content\nextraction (ace) program-tasks, data, and evaluation.\nIn Lrec, volume 2, page 1. Lisbon.\nXinya Du and Claire Cardie. 2020. Document-level\nevent role ﬁller extraction using multi-granularity\ncontextualized encoding. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 8010–8020, Online. Asso-\nciation for Computational Linguistics.\nXinya Du, Alexander M. Rush, and Claire Cardie.\n2020. Grit: Generative role-ﬁller transformers for\ndocument-level event entity extraction. In EACL.\nRalph Grishman. 2019. Twenty-ﬁve years of infor-\nmation extraction. Natural Language Engineering ,\n25(6):677–692.\nRalph Grishman and Beth Sundheim. 1996. Design\nof the MUC-6 evaluation. In TIPSTER TEXT PRO-\nGRAM PHASE II: Proceedings of a Workshop held\nat Vienna, Virginia, May 6-8, 1996, pages 413–422,\nVienna, Virginia, USA. Association for Computa-\ntional Linguistics.\nRuihong Huang and Ellen Riloff. 2011. Peeling back\nthe layers: Detecting event role ﬁllers in secondary\ncontexts. In Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguistics:\nHuman Language Technologies , pages 1137–1147,\nPortland, Oregon, USA. Association for Computa-\ntional Linguistics.\nRuihong Huang and Ellen Riloff. 2012. Modeling tex-\ntual cohesion for event extraction. In Twenty-Sixth\nAAAI Conference on Artiﬁcial Intelligence.\nDan Jurafsky and James H. Martin. 2009. Speech\nand language processing: an introduction to natural\nlanguage processing, computational linguistics, and\nspeech recognition, 2nd Edition . Prentice Hall se-\nries in artiﬁcial intelligence. Prentice Hall, Pearson\nEducation International.\n914\nYing Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020.\nA joint neural model for information extraction with\nglobal features. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7999–8009, Online. Association for\nComputational Linguistics.\nMUC-4. 1992. Fourth message understanding confer-\nence (MUC-4). In Proceedings of FOURTH MES-\nSAGE UNDERSTANDING CONFERENCE (MUC-\n4), McLean, Virginia.\nSiddharth Patwardhan and Ellen Riloff. 2009. A uni-\nﬁed model of phrasal and sentential evidence for in-\nformation extraction. In Proceedings of the 2009\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 151–160, Singapore. Asso-\nciation for Computational Linguistics.\nNewton Spolaor, Everton Alvares Cherman, Maria Car-\nolina Monard, and Huei Diana Lee. 2013. A compar-\nison of multi-label feature selection methods using\nthe problem transformation approach. Electronic\nNotes in Theoretical Computer Science , 292:135–\n151.\nDavid Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. 2019. Entity, relation, and event\nextraction with contextualized span representations.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing (EMNLP-\nIJCNLP), pages 5784–5789, Hong Kong, China. As-\nsociation for Computational Linguistics.\nBishan Yang and Tom M. Mitchell. 2016. Joint extrac-\ntion of events and entities within a document context.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 289–299, San Diego, California. Association\nfor Computational Linguistics.\nA Example document for qualitative analysis\nOfﬁcial sources today reported that at least eight people, including soldiers, rebels, and civilians, were\nkilled during clashes between the army and guerrillas over the past weekend in various points of the\ncountry.\nMilitary spokesmen for the 6th infantry brigade, headquartered in the eastern usulutan department, told\nacanefe that two rebels were killed and one wounded during a clash with government troops in San Agustin.\nMeanwhile, the armed forces press committee (Coprefa) reported that the bodies of two guerrillas, who\nwere presumably killed during clashes with the army, were found by soldiers in the outskirts of Santa\nTecla, in the central la libertad department.\nCoprefa reported that two soldiers were killed during a clash with members of the Farabundo Marti\nNational Liberation Front (FMLN) in Comasagua, about 28 km to the southwest of (San) Salvador,\nwhere a rebel attack on a coffee processing plant was successfully repelled.\nIt reported that a civilian was killed in the crossﬁre and that a soldier was also killed during clashes in\nZaragoza, south of San Salvador, where two guerrillas were wounded.\n...\nSalvadoran (red) cross sources today reported that a 48-year-old woman identiﬁed as Maria Luz Lopez\nwas wounded last night when a powerful bomb, which damaged several businesses in (San) Salvador,\nexploded.\nThe bomb was planted in a heavily commercial area of downtown (San) Salvador causing heavy\nproperty loses, according to the owners who provided no speciﬁc ﬁgures.\nThis is the fourth dynamite attack on businesses in (San) Salvador so far in 1990.\nB Hyper-Parameters\nhparam name value\nBERT model type bert-base-uncased\ntrain batch size 1\neval batch size 1\nnum train epochs 18\nseed 1\nnumber of GPU 1\nlearning rate 5e-5\nADAM epsilon 1e-8\nwarmup steps 0\ndownweigh factor 0.01\nC Implementations\nWe build our model upon the HuggingFace’s NER models’ implementation (rb.gy/nryu2q).\nDependencies\n• Python 3.6.10\n• Pytorch 1.4.0\n• Pytorch-Lightning 0.7.1\n• Transformers: transformers 2.4.1 installed from source.\nLink to Corpus We obtain the raw corpus from https://github.com/brendano/muc4_proc"
}