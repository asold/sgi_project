{
  "title": "3D-Transformer: Molecular Representation with Transformer in 3D Space",
  "url": "https://openalex.org/W3203710967",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2001693504",
      "name": "Fang Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2011352411",
      "name": "Qiang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2913816252",
      "name": "Dragomir Radev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2605948148",
      "name": "Jiyu Cui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098385419",
      "name": "Wen Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153525095",
      "name": "Huabin Xing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114954316",
      "name": "Huajun Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2986232138",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W1993046136",
    "https://openalex.org/W3010328141",
    "https://openalex.org/W2726184500",
    "https://openalex.org/W3036931110",
    "https://openalex.org/W2778051509",
    "https://openalex.org/W2788775653",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3041603413",
    "https://openalex.org/W2968734407",
    "https://openalex.org/W2950399211",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2999044305",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W2948196881",
    "https://openalex.org/W2983028326",
    "https://openalex.org/W2809216727",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W3168218067",
    "https://openalex.org/W3088999551",
    "https://openalex.org/W3035206215",
    "https://openalex.org/W3166597321",
    "https://openalex.org/W2153693853",
    "https://openalex.org/W3159861524",
    "https://openalex.org/W3134435812",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2996443485",
    "https://openalex.org/W1799366690",
    "https://openalex.org/W3128917876",
    "https://openalex.org/W2605291555",
    "https://openalex.org/W2970009611",
    "https://openalex.org/W3166470370",
    "https://openalex.org/W3021975806",
    "https://openalex.org/W3169622372",
    "https://openalex.org/W2962876364",
    "https://openalex.org/W3121309507",
    "https://openalex.org/W2983180560",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W3036737467",
    "https://openalex.org/W2173183968",
    "https://openalex.org/W2997945091",
    "https://openalex.org/W2901003004",
    "https://openalex.org/W2766856748",
    "https://openalex.org/W2998116736",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W2947722296",
    "https://openalex.org/W2107340950",
    "https://openalex.org/W2945811181",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W3185087522",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2949095042",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W3102795188",
    "https://openalex.org/W3102797483",
    "https://openalex.org/W2527189750",
    "https://openalex.org/W3169316583",
    "https://openalex.org/W1738019091",
    "https://openalex.org/W2971690404",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2784213390",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2948990653",
    "https://openalex.org/W3107511504",
    "https://openalex.org/W2951690294",
    "https://openalex.org/W3111835451",
    "https://openalex.org/W2784883284",
    "https://openalex.org/W3034806393",
    "https://openalex.org/W2319902168",
    "https://openalex.org/W3030508707",
    "https://openalex.org/W3129766184",
    "https://openalex.org/W3132607382",
    "https://openalex.org/W3102419180"
  ],
  "abstract": "Spatial structures in the 3D space are important to determine molecular properties. Recent papers use geometric deep learning to represent molecules and predict properties. These papers, however, are computationally expensive in capturing long-range dependencies of input atoms; and have not considered the non-uniformity of interatomic distances, thus failing to learn context-dependent representations at different scales. To deal with such issues, we introduce 3D-Transformer, a variant of the Transformer for molecular representations that incorporates 3D spatial information. 3D-Transformer operates on a fully-connected graph with direct connections between atoms. To cope with the non-uniformity of interatomic distances, we develop a multi-scale self-attention module that exploits local fine-grained patterns with increasing contextual scales. As molecules of different sizes rely on different kinds of spatial features, we design an adaptive position encoding module that adopts different position encoding methods for small and large molecules. Finally, to attain the molecular representation from atom embeddings, we propose an attentive farthest point sampling algorithm that selects a portion of atoms with the assistance of attention scores, overcoming handicaps of the virtual node and previous distance-dominant downsampling methods. We validate 3D-Transformer across three important scientific domains: quantum chemistry, material science, and proteomics. Our experiments show significant improvements over state-of-the-art models on the crystal property prediction task and the protein-ligand binding affinity prediction task, and show better or competitive performance in quantum chemistry molecular datasets. This work provides clear evidence that biochemical tasks can gain consistent benefits from 3D molecular representations and different tasks require different position encoding methods.",
  "full_text": "Molformer: Motif-based Transformer on 3D Heterogeneous Molecular Graphs\nFang Wu,13 Dragomir Radev, 2 Stan Z. Li 1*\n1 School of Engineering, Westlake University\n2 Department of Computer Science, Yale University\n3 Institute of AI Industry Research, Tsinghua University\nfw2359@columbia.edu, dragomir.radev@yale.edu, stan.zq.li@westlake.edu.cn\nAbstract\nProcuring expressive molecular representations underpins\nAI-driven molecule design and scientiÔ¨Åc discovery. The re-\nsearch mainly focuses on atom-level homogeneous molecu-\nlar graphs, ignoring the rich information in subgraphs or mo-\ntifs. However, it has been widely accepted that substructures\nplay a dominant role in identifying and determining molec-\nular properties. To address such issues, we formulate hetero-\ngeneous molecular graphs (HMGs), and introduce a novel ar-\nchitecture to exploit both molecular motifs and 3D geometry.\nPrecisely, we extract functional groups as motifs for small\nmolecules and employ reinforcement learning to adaptively\nselect quaternary amino acids as motif candidates for pro-\nteins. Then HMGs are constructed with both atom-level and\nmotif-level nodes. To better accommodate those HMGs, we\nintroduce a variant of the Transformer named Molformer,\nwhich adopts a heterogeneous self-attention layer to distin-\nguish the interactions between multi-level nodes. Besides,\nit is also coupled with a multi-scale mechanism to capture\nÔ¨Åne-grained local patterns with increasing contextual scales.\nAn attentive farthest point sampling algorithm is also pro-\nposed to obtain the molecular representations. We validate\nMolformer across a broad range of domains, including quan-\ntum chemistry, physiology, and biophysics. Extensive experi-\nments show that Molformer outperforms or achieves the com-\nparable performance of several state-of-the-art baselines. Our\nwork provides a promising way to utilize informative motifs\nfrom the perspective of multi-level graph construction. The\ncode is available at https://github.com/smiles724/Molformer.\nIntroduction\nThe past decade has witnessed the extraordinary success\nof deep learning (DL) in many scientiÔ¨Åc areas. Inspired\nby these achievements, researchers have shown increas-\ning interest in exploiting DL for drug discovery and ma-\nterial design with the hope of rapidly identifying desirable\nmolecules. A key aspect of fast screening is how to represent\nmolecules effectively, where graphs are a natural choice to\npreserve their internal structures. As a consequence, a num-\nber of graph neural networks (GNNs) (Gilmer et al. 2017;\nIshida et al. 2021) have been invented and applied to molec-\nular representation learning with noticeable performance.\n*The corresponding author.\nCopyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nHowever, most existing GNNs only take atom-level infor-\nmation in homogeneous molecular graphs as input, which\nfails to adequately exploit rich semantic information in\nmotifs. Remarkably, motifs are signiÔ¨Åcant subgraph pat-\nterns that frequently occur, and can be leveraged to un-\ncover molecular properties. For instance, a carboxyl group\n(COOH) acts as a hydrogen-bond acceptor, which con-\ntributes to better stability and higher boiling points. Besides,\nsimilar to the role of N-gram in natural language, molecu-\nlar motifs can promote the segmentation of atomic semantic\nmeanings. While some regard motifs as additional features\nof atoms (Maziarka et al. 2020, 2021), these methods in-\ncrease the difÔ¨Åculty to separate the semantic meanings of\nmotifs from atoms explicitly and hinder models from view-\ning motifs from an integral perspective. Others (Huang et al.\n2020) take motifs as the only input, but ignore the inÔ¨Çuence\nof single atoms and infrequent substructures.\nTo overcome these problems, we formulate a novel het-\nerogeneous molecular graph (HMG) that is comprised of\nboth atom-level and motif-level nodes as the model input.\nIt provides a clean interface to incorporate nodes of differ-\nent levels and prevents the error propagation caused by in-\ncorrect semantic segmentation of atoms. As for the determi-\nnation of motifs, we adopt different strategies for different\ntypes of molecules. On the one hand, for small molecules,\nthe motif lexicon is deÔ¨Åned by functional groups based on\nchemical domain knowledge. On the other hand, for proteins\nthat are constituted of sequential amino acids, a reinforce-\nment learning (RL) motif mining technique is introduced to\ndiscover the most meaningful amino acid subsequences for\ndownstream tasks.\nIn order to better align with HMGs, we present Mol-\nformer, an equivariant geometric model based on Trans-\nformer (Vaswani et al. 2017). Molformer differs from pre-\nceding Transformer-based models in two major aspects.\nFirst, it utilizes heterogeneous self-attention (HSA) to dis-\ntinguish the interactions between nodes of different levels\nand incorporates them into the self-attention computation.\nSecond, an Attentive Farthest Point Sampling (AFPS) algo-\nrithm is introduced to aggregate node features and obtain a\ncomprehensive representation of the entire molecule.\nTo summarize, our contributions are as follows:\n‚Ä¢ To the best of our knowledge, we are the foremost to in-\ncorporate motifs and construct 3D heterogeneous molec-\narXiv:2110.01191v7  [q-bio.QM]  7 Jan 2023\nular graphs for molecular representation learning.\n‚Ä¢ We propose a novel Transformer architecture to perform\non these heterogeneous molecular graphs. It has a mod-\niÔ¨Åed self-attention to take into account the interactions\nbetween multi-level nodes, and an AFPS algorithm to in-\ntegrate molecular representations.\n‚Ä¢ We empirically outperform or achieve competitive re-\nsults compared to state-of-the-art baselines on several\nbenchmarks of small molecules and proteins.\nPreliminaries\nProblem DeÔ¨Ånition. Suppose a molecule S = ( P,H)\nhas N atoms, where P = {pi}N\ni=1 ‚ààRN√ó3 describes 3D\ncoordinates associated to each atom and H = {hi}N\ni=1 ‚àà\nRN√óh contains a set of h-dimension roto-translationally in-\nvariant features (e.g. atom types and weights). hi can be\nconverted to a dense vector xi ‚ààRœàembed via a multi-layer\nperceptron (MLP). A representation learning model f acts\non S, and obtains its representation r = f(S). Then r is\nforwarded to a predictor g and attains the prediction of a\nbiochemical property ÀÜy= g(r).\nSelf-Attention Mechanism. Given input {xi}N\ni=1, the\nstandard dot-product self-attention layer is computed as fol-\nlows:\nqi = fQ(xi), ki = fK(xi), vi = fV(xi) (1)\naij = qikT\nj /\n‚àö\nœàmodel, zi =\nN‚àë\nj=1\nœÉ(aij)vj (2)\nwhere {fQ,fK,fV}are embedding transformations, and\n{qi,ki,vi}are respectively the query, key, and value vec-\ntors with the same dimension œàmodel. aij is the attention that\nthe token ipays to the token j. œÉdenotes the Softmax func-\ntion and zi is the output embedding of the token i. This for-\nmula conforms to a non-local network (Wang et al. 2018),\nwhich indicates its poor ability to capture Ô¨Åne-grained pat-\nterns in a local context.\nPosition Encoding. Self-attention is invariant to the per-\nmutation of the input, and position encoding (PE) is the only\ntechnique to reveal position information. PE can be based on\nabsolute positions or relative distances. The former uses raw\npositions and is not robust to spatial transformations. The\nlatter manipulates the attention score by incorporating rela-\ntive distances: aij = qikT\nj /‚àöœàmodel + fPE(pi‚àípj), where\nfPE is a translation-invariant PE function. The rotation in-\nvariance can be further accomplished by taking a L2-norm\ndij = ||pi ‚àípj||2 between the i-th and j-th atom.\nHeterogeneous Molecular Graphs\nMotifs are frequently-occurring substructure patterns and\nserve as the building blocks of complex molecular struc-\ntures. They have great expressiveness of the biochemical\ncharacteristics of the whole molecules. In this section, we\nÔ¨Årst describe how to extract motifs from small molecules\nand proteins respectively, and then present how to formulate\nHMGs.\nMotif   Vocabulary \nMotif   Extraction\nMotif-level Layers Motif   Embeddings\nToken\nEmbedding Layer \nMolformer\nC C H\n‚Ä¶\n‚Ä¶Atom-level \nLayer \nMotif-level\nLayer \nAtom-level \nLayer \nC ‚Ä¶ C\n F ‚Ä¶ N H H O H\nO\nF ‚Ä¶ O\nC ‚Ä¶ C F ‚Ä¶ N H H O HO\nReward\nPolicy Network\n(e.g., ProtBERT)\nMotif Lexicon Molformer\nDownstream Tasks\n(e.g., PDBbind)\nAction\nInput\nModel? ? ? ?\nùê∂160,000\nùêæ\nAction Space\nMotif   Vocabulary \nF\n O\nOH\nO\nN\nCl\n O\nO\nNN\nFigure 1: The workÔ¨Çow of RL motif mining method in\nproteins. In each iteration, the policy network is responsible\nfor producing a motif lexicon. Molformer‚Äôs performance on\ndownstream tasks and the diversity of the lexicon are con-\nsidered as the reward.\nMotifs in Small Molecules\nIn the chemical community, a set of standard criteria have\nbeen developed to recognize motifs with essential func-\ntionalities in small molecules. There, we build motif tem-\nplates of four categories of functional groups, containing\ngroups that contain only carbon and hydrogen (Hydrocar-\nbons), groups that contain halogen (Haloalkanes), groups\nthat contain oxygen, and groups that contain nitrogen (see\nAppendix for more discussion). Practically, we rely on RD-\nKit (Landrum 2013) to draw them from SMILES of small\nmolecules.\nMotifs in Proteins\nIn large protein molecules, motifs are local regions of 3D\nstructures or amino acid sequences shared among proteins\nthat inÔ¨Çuence their functions (Somnath, Bunne, and Krause\n2021). Each motif usually consists of only a few elements,\nsuch as the ‚Äôhelix-turn-helix‚Äô motif, and can describe the\nconnectivity between secondary structural elements. The de-\ntection of protein motifs has been long studied. Neverthe-\nless, existing tools are either from the context of a pro-\ntein surface (Somnath, Bunne, and Krause 2021) or are\ntask-independent and computationally expensive (Macken-\nzie, Zhou, and Grigoryan 2016). On the basis of this pecu-\nliarity, we design an RL mining method to discover task-\nspeciÔ¨Åc protein motifs heuristically.\nTherefore, we consider motifs with four amino acids be-\ncause they make up the smallest polypeptide and have spe-\ncial functionalities in proteins. For instance, Œ≤-turns are\ncomposed of four amino acids and are a non-regular sec-\nondary structure that causes a change in the direction of the\npolypeptide chain. Each amino acid can be of 20 different\npossibilities, such as Alanine, Isoleucine, and Methionine,\nso there are 1.6 √ó105 (= 204) potential quaternary motifs.\nOur goal is to Ô¨Ånd the most effective lexicon V‚àó ‚àà V\ncomposed of K quaternary amino acid templates, where V\ndenotes the space of all CK\n1.6√ó105 potential lexicons. Since\nwe aim to mine the optimal task-speciÔ¨Åc lexicon, it is prac-\ntically feasible to only consider the existing quaternions in\nthe downstream datasets instead of all 1.6 √ó105 possible\nquaternions.\nIn each iteration of the parameter update, we use a pre-\ntrained ProtBERT (Elnaggar et al. 2020) with an MLP as\nthe policy network œÄŒ∏. SpeciÔ¨Åcally, all possible quaternions\nare fed into ProtBert to obtain their corresponding repre-\nsentations {ei}1.6√ó105\ni=1 , which are subsequently sent to the\nMLP to acquire their scores {si}1.6√ó105\ni=1 . These scores il-\nlustrate each quaternion‚Äôs signiÔ¨Åcance to beneÔ¨Åt the down-\nstream tasks if they are chosen as a part of the vocabulary.\nThen top-K motifs with the highest scores are selected to\ncomprise V ‚ààV in accordance with {si}1.6√ó105\ni=1 , and V is\nused as templates to extract motifs and construct HMGs in\ndownstream tasks. After that, a Molformer is trained based\non these HMGs. Its validation performance is regarded as\nthe reward rto update parameters Œ∏by means of policy gra-\ndients. With adequate iterations, the agent can select the op-\ntimal task-speciÔ¨Åc quaternary motif lexicon V‚àó.\nRemarkably, our motif mining process is a one-step game,\nsince the policy networkœÄŒ∏ only generates the vocabularyV\nonce in each iteration. Thus, the trajectory consists of only\none action, and the performance of Molformer based on the\nchosen lexicon V composes a part of the total reward. More-\nover, we also consider the diversity of motif templates within\nthe lexicon and calculate it as:\nddiv(V) = 1\n|V|\n‚àë\nmi‚ààV\n‚àë\nmj‚ààV\ndlev(mi,mj) (3)\nwhere dlev is the Levenshtein distance of two quaternary se-\nquences mi and mj. The Ô¨Ånal reward, therefore, becomes\nR(V) = r+ Œ≥ddiv(V), where Œ≥ is a weight to balance two\nreward terms, and the policy gradient is computed as the fol-\nlowing objective:\n‚àáŒ∏J(Œ∏) = EV‚ààV[‚àáŒ∏log œÄŒ∏(V)R(V)] (4)\nFormulation of Heterogeneous Molecular Graphs\nMost prior studies (Maziarka et al. 2020; Rong et al. 2020;\nMaziarka et al. 2021) simply incorporate motifs into atom\nfeatures. For instance, they differentiate carbons into aro-\nmatic or non-aromatic and deem them as extra features. We\nargue its ineffectiveness for two reasons. First, the fusion\nof multi-level features increases the difÔ¨Åculty to summarize\nthe functionality of motifs. Second, it hinders models to see\nmotifs from a unitary perspective. To Ô¨Åll these gaps, we sep-\narate apart motifs and atoms, regarding motifs as new nodes\nto formulate an HMG. This way disentangles motif-level and\natom-level representations, thus alleviating the difÔ¨Åculty for\nmodels to properly mine the motif-level semantic meanings.\nSimilar to the relation between phrases and single words\nin natural language, motifs in molecules carry higher-level\nsemantic meanings than atoms. Therefore, they play an es-\nsential part in identifying the functionalities of their atomic\nconstituents. Inspired by the employment of dynamic lattice\nstructures in named entity recognition, we treat each cate-\ngory of motifs as a new type of node and build HMGs as\nthe input of our Molformer. To begin with, motifs are ex-\ntracted according to a motif vocabulary V. We assume M\nmotifs {mi}M\ni=1 are detected in the molecule S. Conse-\nquently, an HMG includes both the motif-level and atom-\nlevel nodes as {x1,..., xN,xm1 ,..., xmM }, where xmi ‚àà\nRœàembed is obtained through a learnable embedding matrix\nWM ‚àà RC‚Ä≤√óœàembed and C‚Ä≤ denotes the number of mo-\ntif categories. As for positions of each motif, we adopt\na weighted sum of 3D coordinates of its components as\npmi = ‚àë\nxi‚ààmi\n(\nwi‚àë\nxi‚ààmi wi\n)\n¬∑pi, where wi are the atomic\nweights. Analogous to word segmentation, our HMGs com-\nposed of multi-level nodes avoid error propagation due to\ninappropriate semantic segmentation while leveraging the\natom information for molecular representation learning.\nMolformer\nIn this section, we propose Molformer, which modiÔ¨Åes\nTransformer with several novel components speciÔ¨Åcally de-\nsigned for 3D HMGs. First, we build a motif vocabulary\nand match each molecule with this lexicon to obtain all its\ncontained motifs. Then both atoms and motifs acquire their\ncorresponding embeddings and are forwarded into L fea-\nture learning blocks. Each block consists of an HSA, a feed-\nforward network (FFN), and two-layer normalizations. After\nthat, an AFPS is followed to adaptively produce the molec-\nular representation, which is later fed into a dense predictor\nto forecast properties in a broad range of downstream tasks.\nHeterogeneous Self-attention\nAfter formulating an HMG with N atom-level nodes and M\nmotif-level nodes, it is important to endow the model with\nthe capacity of separating the interactions between multi-\norder nodes. To this end, we utilize a function œÜ(i,j) :\nR(N+M)√ó(N+M) ‚Üí Z, which identiÔ¨Åes the relationship\nbetween any two nodes into three sorts: atom-atom, atom-\nmotif, and motif-motif. Then a learnable scalarbœÜ(i,j) : Z ‚Üí\nR indexed by œÜ(i,j) is introduced so that each node can\nadaptively attend to all other nodes according to their hier-\narchical relationship inside our HMGs.\nIn addition, we consider exploiting 3D molecular geome-\ntry (see Figure 2). Since robustness to global changes such\nas 3D translations and rotations is an underlying principle\nfor molecular representation learning, we seek to satisfy\nroto-translation invariance. There, we borrow ideas from\nSE(3)-Transformer (Sch¬®utt, Unke, and Gastegger 2021) and\nAlphaFold2 (Jumper et al. 2021), and apply a convolu-\ntional operation to the pairwise distance matrix D =\n[di,j]i,j‚àà[N+M] ‚ààR(N+M)√ó(N+M) as ÀÜD = Conv2d(D),\nwhere Conv2d denotes a 2D shallow convolutional network\nwith a kernel size of1√ó1. Consequently, the attention score\nis computed as follows:\nÀÜaij =\n(\nqikT\nj /\n‚àö\nœàmodel\n)\n¬∑ÀÜdij + bœÜ(i,j), (5)\nwhere ÀÜdi,j ‚àà ÀÜD controls the impact of interatomic distance\nover the attention score, and bœÜ(i,j) is shared across all lay-\ners.\nMoreover, exploiting local context has proven to be im-\nportant in sparse 3D space. However, it has been pointed out\nHeterogeneous Self-attention \nLayer Norm\nFFN\nL √ó\nGlobal Pooling\nProperties\nLayer Norm\nVirtual Atom \nw\nw\nw\nww\nw\nw w\nH\nAttentive FPS\nMLP\nN\nO\nC\nMulti-scale Mechanism\nLocal Global\nUnselected Atom \nMotif   Extraction\nC C H‚Ä¶\nEmbedding Layer \nMotifsAtoms\nFigure 2: The architecture of Molformer.Given a heterogeneous molecular graph with both atom-level and motif-level nodes,\nstacked feature learning blocks composed of a heterogeneous self-attention module and an FFN compute their updated features.\nAfterward, an attentive subsampling module integrates the molecular representation for downstream predictions. Local features\nwith different scales are purple and orange; yellow corresponds to global features.\nthat self-attention is good at capturing global data patterns\nbut ignores local context (Wu et al. 2020). Based on this\nfact, we impose a distance-based constraint on self-attention\nin order to extract multi-scaled patterns from both local and\nglobal contexts. Guo et al. (2020) propose to use integer-\nbased distances to limit attention to local word neighbors,\nwhich cannot be used in molecules. This is because different\ntypes of molecules have different densities and molecules of\nthe same type have different spatial regularity, which results\nin the non-uniformity of interatomic distances. Normally,\nsmall molecules have a mean interatomic distance of 1-2 ÀöA\n(Angstrom, 10‚àí10m), which is denser than large molecules\nlike proteins with approximately 5 ÀöA on average. To tackle\nthat, we design a multi-scale methodology to robustly cap-\nture details. SpeciÔ¨Åcally, we mask nodes beyond a certain\ndistance œÑs at each scale s. The attention calculation is mod-\niÔ¨Åed as:\naœÑs\nij = ÀÜaij ¬∑1{dij<œÑs},zœÑs\ni =\nN‚àë\nj=1\nœÉ\n(\naœÑs\nij\n)\nvj, (6)\nwhere 1{dij<œÑs} is the indicator function. Notably, Equa-\ntion 6 can be complementarily combined with Equation 5.\nThen, features extracted from S different scales {œÑs}S\ns=1,\nas well as the informative global feature, are concatenated\ntogether to form a multi-scale representation, denoted by\nz‚Ä≤\ni = zœÑ1\ni ‚äï...‚äïzœÑS\ni ‚äïzglobal\ni ‚ààR(S+1)œàmodel . After that,\nz‚Ä≤\ni is forwarded into a FFN to obtain z‚Ä≤‚Ä≤\ni with the original\ndimension œàmodel.\nAttentive Farthest Point Sampling\nAfter having the node embeddings{z‚Ä≤‚Ä≤\ni}N+M\ni=1 , we study how\nto obtain the molecular representation r. For GNNs, several\nreadout functions such as set2set (Vinyals, Bengio, and Kud-\nlur 2015) and GG-NN (Gilmer et al. 2017) are invented. For\nTransformers, one way is via a virtual node. Though Ying\net al. (2021) state that it signiÔ¨Åcantly improves the perfor-\nmance of existing models in the leaderboard of Open Graph\nBenchmark (OGB), this way concentrates more on its close\nadjacent nodes and less on distant ones, and may lead to\ninadvertent over-smoothing of information propagation. Be-\nsides, it is difÔ¨Åcult to locate a virtual node in 3D space and\nbuild connections to existing vertices. The other way selects\na subset of nodes via a downsampling algorithm named Far-\nthest Point Search (FPS), but it ignores nodes‚Äô differences\nand has the sensitivity to outlier points as well as uncon-\ntrollable randomness. To address these issues, we propose a\nnew algorithm named AFPS. It aims to sample vertices by\nnot merely spatial distances, but also their signiÔ¨Åcance in\nterms of attention scores.\nSpeciÔ¨Åcally, we choose the virtual atom x# as the\nstarting point and initialize two lists P = {x#} and\nM= {xi}N+M\ni=1 to store remaining candidate points. Then\nthe process begins with the attention score matrix ÀÜA =\n[ÀÜai,j]i,j‚àà[N+M] ‚ààR(N+M)√ó(N+M) and the interatomic dis-\ntance matrix D ‚ààR(N+M)√ó(N+M). It can be easily proved\nthat each row of ÀÜA sums up to 1 after the Softmax op-\neration along columns, i.e. ‚àë\nj ÀÜaij = 1 ,‚àÄi ‚àà [N + M].\nAlgorithm 1: Attentive Farthest Point Sampling\nInput: The attention score matrix A ‚ààR(N+M)√ó(N+M),\na Euclidean distance matrix D ‚ààR(N+M)√ó(N+M).\nOutput: Ksampled points.\nÀúA ‚Üê‚àë\niÀÜaij ‚ààRN+M ‚äøsum up along rows\nÀúD ‚Üê D‚àímin D\nmax D‚àímin D ‚ààR(N+M)√ó(N+M) ‚äønormalize the\ndistance matrix\nP= {x#}\nM= {xi}N+M\ni=1\nwhile length(P) <k do\nxnew ‚Üêargmax\ni‚ààM\n(min\nj‚ààP\nÀúDij + ŒªÀúAi) ‚äøpick up the node\nthat maximize the objective\nP.append(xnew)\nM.remove(xnew)\nend whilereturn P\nIn order to obtain the importance of each atom in the self-\nattention computation, we accumulate ÀÜA along rows and get\nÀúA = ‚àë\niÀÜaij ‚àà RN+M. Besides, we adopt the min-max\nnormalization to rescale the distance matrix D into values\nbetween 0 and 1, and obtain ÀúD = D‚àímin D\nmax D‚àímin D .\nAfter the above preprocess, we repeatedly move a point\nxnew from Mto P, which ensures that xnew is as far from\nPas possible by maximizing ÀúDij and also plays a crucial\nrole in attention computation by maximizing ÀúAi. Mathemat-\nically, the AFPS aims to achieve the objective:\nmax\n‚àë\ni‚ààM\n( min\nj‚ààP\\{i}\nÀúDij + ŒªÀúAi), (7)\nwhere Œª is a hyperparameter to balance those two differ-\nent goals. This process is repeated until Phas reached K\npoints. Algorithm 1 provides a greedy approximation solu-\ntion to this AFPS optimization objective for sake of compu-\ntational efÔ¨Åciency.\nAfter that, sampled features {z‚Ä≤‚Ä≤\ni}i‚ààP are gathered by a\nGlobal Average Pooling layer to attain the molecular repre-\nsentation r ‚ààRœàmodel .\nRemarkably, our proposed AFPS has considerable differ-\nences and superiority over a body of previous hierarchical\napproaches (Eismann et al. 2021). Their subsampling oper-\nations are mainly designed for protein complexities, which\noften have uniform structures. To be speciÔ¨Åc, they hierarchi-\ncally use alpha carbons as the intermediate set of points and\naggregate information at the level of those carbons for the\nentire complex. However, the structures of small molecules\nhave no such stable paradigm, and we provide a universal\nmethod to adaptively subsample atoms without any prior as-\nsumptions on the atom arrangement.\nExperiments\nWe conduct extensive experiments on 7 datasets about both\nsmall molecules and large protein molecules from three dif-\nferent domains, including quantum chemistry, physiology,\nand biophysics. The appendix summarises statistical infor-\nmation of these 7 benchmark datasets, such as the num-\nber of tasks and task types, the number of molecules and\natom classes, the minimum and the maximum number of\natoms, and the density (mean interatomic distances) of all\nmolecules.\nDatasets. We test Molformer on a series of small molecule\ndatasets, containing QM7 (Blum and Reymond 2009),\nQM8 (Ramakrishnan et al. 2015), QM9 (Ramakrishnan et al.\n2014), BBBP (Martins et al. 2012), ClinTox (Gayvert, Mad-\nhukar, and Elemento 2016), and BACE (Subramanian et al.\n2016) 1. QM7 is a subset of GDB-13 and is composed of 7K\nmolecules. QM8 and QM9 are subsets of GDB-17 with 22K\nand 133K molecules respectively. BBBP involves records\nof whether a compound carries the permeability property of\npenetrating the blood-brain barrier. ClinTox compares drugs\napproved through FDA and drugs eliminated due to toxicity\nduring clinical trials. BACE is collected for recording com-\npounds that could act as the inhibitors of human Œ≤-secretase\n1 (BACE-1).\nWe also inspect Molformer‚Äôs ability to learn mutual re-\nlations between proteins and molecules on the PDBbind\ndataset (Wang et al. 2005). Following Townshend et al.\n(2020), we split protein-ligand complexes by protein se-\nquence identity at 30%. As for the target, we predict pS =\n‚àílog(S), where S is the binding afÔ¨Ånity in Molar unit. In\naddition, we only use the pocket of each protein and put\npocket-ligand pairs together as the input.\nFor QM9, we use the exact train/validation/test split\nas Townshend et al. (2020). For PDBbind, 90% of the data\nis used for training and the rest is divided equally between\nvalidation and test like Chen et al. (2019). For others, we\nadopt the scaffold splitting method with a ratio of 8:1:1 for\ntrain/validation/test as Rong et al. (2020). More implement-\ning details can be found in Appendix.\nBaselines For small molecules, we compare Molformer\nwith the following baselines. TF Robust (Ramsundar\net al. 2015) takes molecular Ô¨Ångerprints as the input.\nWeave (Kearnes et al. 2016), MPNN (Gilmer et al. 2017),\nSchnet (Sch ¬®utt et al. 2018), MEGNet (Chen et al. 2019),\nGROVER (Rong et al. 2020), DMPNN (Yang et al.\n2019), MGCN (Lu et al. 2019), AttentiveFP (Xiong et al.\n2019), DimeNet (Klicpera, Gro√ü, and G ¬®unnemann 2020),\nDimeNet++ (Klicpera et al. 2020), PaiNN (Sch ¬®utt, Unke,\nand Gastegger 2021), and SphereNet (Liu et al. 2021) are\nall graph convolutional models. Graph Transformer (Chen,\nBarzilay, and Jaakkola 2019), MAT (Maziarka et al. 2020),\nR-MAT (Maziarka et al. 2021), SE(3)-Transformer (Fuchs\net al. 2020), and LieTransformer (Hutchinson et al.\n2021) are Transformer-based Equivariant Neural Networks\n(ENNs) (Thomas et al. 2018).\nFor PDBbind, we choose seven baselines. Deep-\nDTA ( ¬®Ozt¬®urk, ¬®Ozg¬®ur, and Ozkirimli 2018) and DeepAfÔ¨Ån-\nity (Karimi et al. 2019) take in pairs of ligand and protein\nSMILES as input. Cormorant (Anderson, Hy, and Kondor\n2019) is an ENN that represents each atom by its abso-\nlute 3D coordinates. HoloProt (Somnath, Bunne, and Krause\n1For BBBP, ClinTox, and BACE, we use RDKit (Landrum\n2013) to procure 3D coordinates from SMILES.\nMethod QM7 QM8 BBBP ClinTox BACE\nTF-Robust (Ramsundar et al. 2015)120.6 0.024 0.860 0.765 0.824\nWeave (Kearnes et al. 2016) 94.7 0.022 0.837 0.823 0.791MPNN (Gilmer et al. 2017) 113.0 0.015 0.913 0.879 0.815Schnet (Sch¬®utt et al. 2018) 74.2 0.020 0.847 0.717 0.750DMPNN (Yang et al. 2019) 105.8 0.014 0.9190.897 0.852MGCN (Lu et al. 2019) 77.6 0.022 0.850 0.634 0.734Attentive FP (Xiong et al. 2019) 126.7 0.028 0.908 0.9330.863\nGraph Transformer (Chen, Barzilay, and Jaakkola 2019)47.80.0100.913 - 0.880MAT (Maziarka et al. 2020) 102.8 - 0.728 - 0.846R-MAT (Maziarka et al. 2021) 68.6 - 0.746 - 0.871GROVERlarge(Rong et al. 2020) 89.4 0.017 0.911 0.884 0.858\nMolformer 43.2 0.009 0.926 0.937 0.884\nTable 1: For regression tasks in QM7 and QM8, lower MAE\nis better. For classiÔ¨Åcation tasks in BBBP, ClinTox, and\nBace, higher values are better.\n2021) captures higher-level Ô¨Ångerprint motifs on the pro-\ntein surface. Schnet, 3DCNN and 3DGCN (Townshend et al.\n2020) are 3D methods.\nOverall Results on Benchmark Tasks\nMolecules. Table 1 and Table 2 document the overall re-\nsults on small molecules datasets, where the best perfor-\nmance is marked bold and the second best is underlined\nfor clear comparison. It can be discovered that Molformer\nachieves the lowest MAE of 43.2 on QM7 and 0.009 on\nQM8, beating several strong baselines including DMPNN\nand Graph Transformer. Besides, Molformer offers compet-\nitive performance in all property regression tasks on QM9.\nParticularly, we outperform all Transformer-based ENNs,\nincluding SE(3)-Transformer and LieTransformer. As for\nclassiÔ¨Åcation problems, we surpass all baselines mostly by a\nfairly large margin.\nProteins. Table 3 reports the Root-Mean-Squared Devia-\ntion (RMSD), the Pearson correlation ( Rp), and the Spear-\nman correlation (Rs) on PDBbind. Molformer achieves the\nlowest RMSD and the best Pearson and Spearman correla-\ntions. As Wu et al. (2018) claim, appropriate featurization\nwhich holds pertinent information is signiÔ¨Åcant for PDB-\nbind. However, an important observation in our work is that\ndeep learning approaches with the exploitation of 3D ge-\nometric information can perform better than conventional\nmethods like DeepDTA and DeepAfÔ¨Ånity which use a set\nof physicochemical descriptors but ignore 3D structures.\nAblation Study and Discussion\nWhat Is the Effect of Each Component? We investigate\nthe effectiveness of each component of our Molformer in\nTable 4. It can be observed that HSA along with HMGs\nsubstantially boosts the model‚Äôs performance compared with\nthe naive method that immediately adds 3D coordinates as\nthe atom input feature. MAE declines from 132.2 to 46.5\nin QM7 while decreasing from 0.0205 to 0.0097 in QM8. In\naddition, AFPS produces better predictions than the counter-\npart that utilizes the virtual node as the molecular representa-\ntion (a case study of AFPS is in the Appendix). We also dis-\ncover that the multi-scale mechanism signiÔ¨Åcantly reduces\nRMSD from 50.1 to 46.5 on QM7, but its improvements in\nQM8 are much smaller. This phenomenon indicates that the\nmulti-scale mechanism is an appropriate way to alleviate the\nNo Motif +Hydrocarbon +Haloalkane +Oxygen +Nitrogen\n40\n60\n80\n100\n120MAE\nQM7\nBBBP\n0.85\n0.86\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\n0.93\nROC\nFigure 3: The ablation study of consecutively adding four\nmotif groups (from left to right) in the QM7 and BBBP\ndatasets.\nproblem of inadequate training in small datasets. It endows\nMolformer with the capability to extract local features by\nregulating the scope of self-attention. However, as the data\nsize gets larger, Molformer does not require the assistance\nof the multi-scale mechanism to abstract local patterns, since\nthe parameters of convolution operators are properly trained.\nWhat Is the Contribution of HMGs? The ideology of\nconstructing heterogeneous graphs has already been proven\nsuccessful in not only chemical knowledge graphs, but\nnamed entity recognition (Li et al. 2020). The former views\nthe chemical characteristics obtained from domain knowl-\nedge of elements as shared nodes, while the latter converts\nthe lattice structure into a Ô¨Çat structure consisting of spans.\nTo further verify its efÔ¨Åcacy, we compare our motif-based\nHMGs with the naive fusion of multi-level features. Table 5\nshows a noticeable improvement in our HMGs over the other\ntwo variants.\nHave We Found Good Candidates of Motifs? How to\ndetermine motifs is critical to HMGs. Concerning small\nmolecules, we deÔ¨Åne motifs on the basis of functional\ngroups, which refers to a substituent or moiety that causes\nmolecules‚Äô characteristic chemical reactions. To further ex-\nplore their contributions, we divide functional groups into\nfour categories: Hydrocarbons, Haloalkanes, groups that\ncontain oxygen, and groups that contain nitrogen (see\nAppendix). The ablation studies (see Figure 3) demon-\nstrate that Molformer can gain improvements from all\nfour groups of motifs, where Hydrocarbons and Haloalka-\nnes are the most and the least effective types respec-\ntively. This is in line with the fact that Hydrocarbons\noccur most frequently in organic molecules. Moreover,\nthe best performance is achieved when all categories are\nconsidered, implying a promising direction to discover\nmore effective motifs. As for proteins, motifs discovered\nby our RL mining method share the same backbone as\nCC(C(NC(C)C(O)=O)=O)NC(CNC(CN)=O)=O (see Ap-\npendix), which is a hydrogen bond donor and implies a mark\nto distinguish potential binding site. Moreover, the portion\nof those motifs in the pocket (1.38%) is nearly twice that in\nother locations (0.73%), conforming to the fact that pockets\nare the most preferable part for ligands to bind with.\nTarget œµHOMO œµLUMO ‚àÜœµ ¬µ Œ± R 2 ZPVE U0 U H G c v\nUnit eV eV eV D bohr 3 a20 meV meV meV meV meV cal /mol K\nMPNN (Gilmer et al. 2017) .043 .037 .069 .030 .092 .150 1.27 45 45 39 44 .800\nSchnet (Sch¬®utt et al. 2018) .041 .034 .063 .033 .235 .073 1.7 14 19 14 14 .033\nMEGNetfull(Chen et al. 2019) .038 .031 .061 .040 .083 .265 1.4 9 10 10 10 .030\nMGCN (Lu et al. 2019) .042 .057 .064 .056 .030 .110 1.12 12.9 14.4 14.6 16.2 .038\nDimeNet (Klicpera, Gro√ü, and G¬®unnemann 2020).027 .019 .034 .028 .046 .331 1.29 8.02 7.89 8.11 8.98 .024\nDimeNet++ (Klicpera et al. 2020) .024 .019 .032 .029 .043 .331 1.21 6.32 6.28 6.53 7.56 .023\nSphereNet (Liu et al. 2021) .024 .019 .032 .026 .047 .292 1.12 6.26 7.33 6.40 8.0 .021\nPaiNN (Sch¬®utt, Unke, and Gastegger 2021) .028 .020 .046 .012 .045 .066 1.28 5.85 5.53 5.98 7.35 .024\nSE(3)-Transformer (Fuchs et al. 2020) .035 .033 .053 .051 .142 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nLieTransformer (Hutchinson et al. 2021) .033 .029 .052 .061 .104 2.29 3.55 17 16 27 23 .041\nMolformer .025 .026 .039 .028 .041 .350 2.05 7.52 7.46 7.38 8.11 .025\nTable 2: Comparison of MAE on QM9. The last three methods are Transformer-based methods.\nMethod GeometryRMSD Rp Rs\nDeepDTA (¬®Ozt¬®urk,¬®Ozg¬®ur, and Ozkirimli 2018)Non-3D 1.565 0.573 0.574DeepAfÔ¨Ånity (Karimi et al. 2019) Non-3D 1.893 0.415 0.426\nSchnet (Sch¬®utt et al. 2018) 3D 1.892 0.601 -Cormorant (Anderson, Hy, and Kondor 2019)3D 1.4290.541 0.5323DCNN (Townshend et al. 2020) 3D 1.520 0.558 0.5563DGCN (Townshend et al. 2020) 3D 1.963 0.581 0.647HoloProt (Somnath, Bunne, and Krause 2021)3D 1.464 0.509 0.500\nMolformer 3D 1.386 0.623 0.651\nTable 3: Comparison of RMSD, Rp, and Rs on PDBbind.\nHSA AFPS HMG QM7 QM8 PDBbind\n1 - - - 132.2 0.0205 1.925\n2 ‚úì - ‚úì 46.5 0.0097 1.441\n3 ‚úì ‚úì ‚úì 43.2 0.0095 1.386\nTable 4: Effects of each module on QM7, QM8 and PDB-\nbind (RMSD).\nRelated Works\nMotifs in Molecular Graphs. Motifs have been proven\nto beneÔ¨Åt many tasks from exploratory analysis to trans-\nfer learning. Various algorithms have been proposed to ex-\nploit them for contrastive learning (Zhang et al. 2020), self-\nsupervised pretraining (Zhang et al. 2021), generation (Jin,\nBarzilay, and Jaakkola 2020), protein design (Li et al.) and\ndrug-drug interaction prediction (Huang et al. 2020). To the\nbest of our knowledge, none of them take advantage of mo-\ntifs to build a heterogeneous graph for molecular property\nprediction.\nAs for motif extraction, previous motif mining methods\neither depend on exact counting (Cantoni, Gatti, and Lom-\nbardi 2011) or sampling and statistical estimation (Wernicke\n2006). No preceding studies extract task-speciÔ¨Åc motifs to\nenhance model performance.\nMolecular Representation Learning. DL has been\nwidely applied to predict molecular properties. Molecules\nare usually represented as 1D sequences, including amino\nacid sequences and SMILES (Xu et al. 2017), and 2D\ngraphs (Duvenaud et al. 2015). Despite that, more evidence\nindicates that 3D structures lead to better modeling and su-\nperior performance. 3D CNNs (Anand-Achim et al. 2021)\nand GNNs (Cho and Choi 2018) become popular to cap-\nQM7 QM8 PDBbind\nNo Motifs 132.2 0.0205 1.925\nMulti-Level Fusion 89.7 0.0154 1.427\nHeterogeneous Graphs43.2 0.0095 1.386\nTable 5: Comparison of HMGs with simple feature fusion\non QM7, QM8 and PDBbind (RMSD).\nture these complex geometries in various bio-molecular ap-\nplications. Nonetheless, the aforementioned methods is in-\nefÔ¨Åcient at grabbing local contextual feature and long-range\ndependencies.\nAttempts have been taken to address that issue based\non the Transformer. It assumes full connection and uses\nself-attention to capture long-term dependencies. Some re-\nsearchers feed SMILES in Transformer to obtain represen-\ntations (Honda, Shi, and Ueda 2019) and conduct pretrain-\ning (Chithrananda, Grand, and Ramsundar 2020). Others\nemploy Transformer to solve generative tasks (Ingraham\net al. 2019) or fulÔ¨Åll equivariance (Fuchs et al. 2020) via\nspherical harmonics. However, the foregoing methods are ei-\nther incapable to encode 3D geometry, non-sensitive to local\ncontextual patterns, or inefÔ¨Åcient to aggregate atom features.\nMore essentially, they are not specially designed to operate\non heterogeneous graphs of molecules.\nConclusion\nThis paper presents Molformer for 3D molecular representa-\ntion learning on heterogeneous molecular graphs. First, we\nextract informative motifs by means of functional groups\nand a reinforcement learning mining method to formu-\nlate heterogeneous molecular graphs. After that, Molformer\nadopts a heterogeneous self-attention to distinguish the in-\nteractions between multi-level nodes and exploit spatial in-\nformation with multiplicate scales for the sake of catching\nlocal features. Then a simple but efÔ¨Åcient downsampling al-\ngorithm is introduced to better accumulate molecular repre-\nsentations. Experiments demonstrate the superiority of Mol-\nformer in various domains, which beats a group of baseline\nalgorithms.\nReferences\nAnand-Achim, N.; Eguchi, R. R.; Mathews, I. I.; Perez,\nC. P.; Derry, A.; Altman, R. B.; and Huang, P. 2021. Protein\nsequence design with a learned potential. bioRxiv, 2020‚Äì01.\nAnderson, B.; Hy, T.-S.; and Kondor, R. 2019. Cormorant:\nCovariant molecular neural networks. arXiv preprint\narXiv:1906.04015.\nBlum, L. C.; and Reymond, J.-L. 2009. 970 million drug-\nlike small molecules for virtual screening in the chemical\nuniverse database GDB-13. Journal of the American Chem-\nical Society, 131(25): 8732‚Äì8733.\nCantoni, V .; Gatti, R.; and Lombardi, L. 2011. 3D Protein\nSurface Segmentation through Mathematical Morphology.\nIn International Joint Conference on Biomedical Engineer-\ning Systems and Technologies, 97‚Äì109. Springer.\nChen, B.; Barzilay, R.; and Jaakkola, T. 2019. Path-\naugmented graph transformer network. arXiv preprint\narXiv:1905.12712.\nChen, C.; Ye, W.; Zuo, Y .; Zheng, C.; and Ong, S. P. 2019.\nGraph networks as a universal machine learning framework\nfor molecules and crystals. Chemistry of Materials , 31(9):\n3564‚Äì3572.\nChithrananda, S.; Grand, G.; and Ramsundar, B. 2020.\nChemberta: Large-scale self-supervised pretraining\nfor molecular property prediction. arXiv preprint\narXiv:2010.09885.\nCho, H.; and Choi, I. S. 2018. Three-dimensionally em-\nbedded graph convolutional network (3dgcn) for molecule\ninterpretation. arXiv preprint arXiv:1811.09794.\nDuvenaud, D.; Maclaurin, D.; Aguilera-Iparraguirre, J.;\nG¬¥omez-Bombarelli, R.; Hirzel, T.; Aspuru-Guzik, A.;\nand Adams, R. P. 2015. Convolutional networks on\ngraphs for learning molecular Ô¨Ångerprints. arXiv preprint\narXiv:1509.09292.\nEismann, S.; Townshend, R. J.; Thomas, N.; Jagota, M.;\nJing, B.; and Dror, R. O. 2021. Hierarchical, rotation-\nequivariant neural networks to select structural models of\nprotein complexes. Proteins: Structure, Function, and\nBioinformatics, 89(5): 493‚Äì501.\nElnaggar, A.; Heinzinger, M.; Dallago, C.; Rihawi, G.;\nWang, Y .; Jones, L.; Gibbs, T.; Feher, T.; Angerer, C.;\nSteinegger, M.; et al. 2020. ProtTrans: towards cracking\nthe language of Life‚Äôs code through self-supervised deep\nlearning and high performance computing. arXiv preprint\narXiv:2007.06225.\nFuchs, F. B.; Worrall, D. E.; Fischer, V .; and Welling, M.\n2020. Se (3)-transformers: 3d roto-translation equivariant\nattention networks. arXiv preprint arXiv:2006.10503.\nGayvert, K. M.; Madhukar, N. S.; and Elemento, O. 2016. A\ndata-driven approach to predicting successes and failures of\nclinical trials. Cell chemical biology, 23(10): 1294‚Äì1301.\nGilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and\nDahl, G. E. 2017. Neural message passing for quantum\nchemistry. In International conference on machine learn-\ning, 1263‚Äì1272. PMLR.\nGuo, Q.; Qiu, X.; Liu, P.; Xue, X.; and Zhang, Z. 2020.\nMulti-scale self-attention for text classiÔ¨Åcation. In Proceed-\nings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , vol-\nume 34, 7847‚Äì7854.\nHonda, S.; Shi, S.; and Ueda, H. R. 2019. Smiles trans-\nformer: Pre-trained molecular Ô¨Ångerprint for low data drug\ndiscovery. arXiv preprint arXiv:1911.04738.\nHuang, K.; Xiao, C.; Hoang, T.; Glass, L.; and Sun, J. 2020.\nCaster: Predicting drug interactions with chemical substruc-\nture representation. In Proceedings of the AAAI Conference\non ArtiÔ¨Åcial Intelligence, volume 34, 702‚Äì709.\nHutchinson, M. J.; Le Lan, C.; Zaidi, S.; Dupont, E.; Teh,\nY . W.; and Kim, H. 2021. Lietransformer: Equivariant self-\nattention for lie groups. In International Conference on Ma-\nchine Learning, 4533‚Äì4543. PMLR.\nIngraham, J.; Garg, V . K.; Barzilay, R.; and Jaakkola, T.\n2019. Generative models for graph-based protein design.\nAdvances in neural information processing systems.\nIshida, S.; Miyazaki, T.; Sugaya, Y .; and Omachi, S. 2021.\nGraph Neural Networks with Multiple Feature Extraction\nPaths for Chemical Property Estimation. Molecules, 26(11):\n3125.\nJin, W.; Barzilay, R.; and Jaakkola, T. 2020. Hierarchi-\ncal generation of molecular graphs using structural motifs.\nIn International Conference on Machine Learning , 4839‚Äì\n4848. PMLR.\nJumper, J.; Evans, R.; Pritzel, A.; Green, T.; Figurnov, M.;\nRonneberger, O.; Tunyasuvunakool, K.; Bates, R.; ÀáZ¬¥ƒ±dek,\nA.; Potapenko, A.; et al. 2021. Highly accurate protein struc-\nture prediction with AlphaFold. Nature, 596(7873): 583‚Äì\n589.\nKarimi, M.; Wu, D.; Wang, Z.; and Shen, Y . 2019. Deep-\nAfÔ¨Ånity: interpretable deep learning of compound‚Äìprotein\nafÔ¨Ånity through uniÔ¨Åed recurrent and convolutional neural\nnetworks. Bioinformatics, 35(18): 3329‚Äì3338.\nKearnes, S.; McCloskey, K.; Berndl, M.; Pande, V .; and Ri-\nley, P. 2016. Molecular graph convolutions: moving beyond\nÔ¨Ångerprints. Journal of computer-aided molecular design ,\n30(8): 595‚Äì608.\nKlicpera, J.; Giri, S.; Margraf, J. T.; and G ¬®unnemann,\nS. 2020. Fast and uncertainty-aware directional message\npassing for non-equilibrium molecules. arXiv preprint\narXiv:2011.14115.\nKlicpera, J.; Gro√ü, J.; and G ¬®unnemann, S. 2020. Direc-\ntional message passing for molecular graphs. arXiv preprint\narXiv:2003.03123.\nLandrum, G. 2013. RDKit: A software suite for cheminfor-\nmatics, computational chemistry, and predictive modeling.\nLi, A. J.; Sundar, V .; Grigoryan, G.; and Keating, A. E. ????\nTERMinator: A Neural Framework for Structure-Based Pro-\ntein Design using Tertiary Repeating Motifs.\nLi, X.; Yan, H.; Qiu, X.; and Huang, X. 2020. FLAT:\nChinese NER using Ô¨Çat-lattice transformer. arXiv preprint\narXiv:2004.11795.\nLiu, Y .; Wang, L.; Liu, M.; Zhang, X.; Oztekin, B.; and Ji,\nS. 2021. Spherical message passing for 3d graph networks.\narXiv preprint arXiv:2102.05013.\nLu, C.; Liu, Q.; Wang, C.; Huang, Z.; Lin, P.; and He, L.\n2019. Molecular property prediction: A multilevel quan-\ntum interactions modeling perspective. In Proceedings of\nthe AAAI Conference on ArtiÔ¨Åcial Intelligence , volume 33,\n1052‚Äì1060.\nMackenzie, C. O.; Zhou, J.; and Grigoryan, G. 2016. Ter-\ntiary alphabet for the observable protein structural universe.\nProceedings of the National Academy of Sciences, 113(47):\nE7438‚ÄìE7447.\nMartins, I. F.; Teixeira, A. L.; Pinheiro, L.; and Falcao, A. O.\n2012. A Bayesian approach to in silico blood-brain barrier\npenetration modeling. Journal of chemical information and\nmodeling, 52(6): 1686‚Äì1697.\nMaziarka, ≈Å.; Danel, T.; Mucha, S.; Rataj, K.; Tabor, J.; and\nJastrzebski, S. 2020. Molecule attention transformer. arXiv\npreprint arXiv:2002.08264.\nMaziarka, ≈Å.; Majchrowski, D.; Danel, T.; Gai ¬¥nski, P.;\nTabor, J.; Podolak, I.; Morkisz, P.; and Jastrzebski, S.\n2021. Relative Molecule Self-Attention Transformer. arXiv\npreprint arXiv:2110.05841.\n¬®Ozt¬®urk, H.; ¬®Ozg¬®ur, A.; and Ozkirimli, E. 2018. DeepDTA:\ndeep drug‚Äìtarget binding afÔ¨Ånity prediction.Bioinformatics,\n34(17): i821‚Äìi829.\nRamakrishnan, R.; Dral, P. O.; Rupp, M.; and von Lilienfeld,\nO. A. 2014. Quantum chemistry structures and properties of\n134 kilo molecules. ScientiÔ¨Åc Data, 1.\nRamakrishnan, R.; Hartmann, M.; Tapavicza, E.; and\nV on Lilienfeld, O. A. 2015. Electronic spectra from TDDFT\nand machine learning in chemical space. The Journal of\nchemical physics, 143(8): 084111.\nRamsundar, B.; Kearnes, S.; Riley, P.; Webster, D.; Konerd-\ning, D.; and Pande, V . 2015. Massively multitask networks\nfor drug discovery. arXiv preprint arXiv:1502.02072.\nRong, Y .; Bian, Y .; Xu, T.; Xie, W.; Wei, Y .; Huang, W.; and\nHuang, J. 2020. Self-supervised graph transformer on large-\nscale molecular data. arXiv preprint arXiv:2007.02835.\nSch¬®utt, K. T.; Sauceda, H. E.; Kindermans, P.-J.;\nTkatchenko, A.; and M ¬®uller, K.-R. 2018. Schnet‚Äìa\ndeep learning architecture for molecules and materials. The\nJournal of Chemical Physics, 148(24): 241722.\nSch¬®utt, K. T.; Unke, O. T.; and Gastegger, M. 2021.\nEquivariant message passing for the prediction of ten-\nsorial properties and molecular spectra. arXiv preprint\narXiv:2102.03150.\nSomnath, V . R.; Bunne, C.; and Krause, A. 2021. Multi-\nScale Representation Learning on Proteins. In Thirty-Fifth\nConference on Neural Information Processing Systems.\nSubramanian, G.; Ramsundar, B.; Pande, V .; and Denny,\nR. A. 2016. Computational modeling of Œ≤-secretase 1\n(BACE-1) inhibitors using ligand based approaches.Journal\nof chemical information and modeling, 56(10): 1936‚Äì1949.\nThomas, N.; Smidt, T.; Kearnes, S.; Yang, L.; Li, L.;\nKohlhoff, K.; and Riley, P. 2018. Tensor Ô¨Åeld networks:\nRotation-and translation-equivariant neural networks for 3d\npoint clouds. arXiv preprint arXiv:1802.08219.\nTownshend, R. J.; V ¬®ogele, M.; Suriana, P.; Derry, A.;\nPowers, A.; Laloudakis, Y .; Balachandar, S.; Anderson,\nB.; Eismann, S.; Kondor, R.; et al. 2020. ATOM3D:\nTasks On Molecules in Three Dimensions. arXiv preprint\narXiv:2012.04035.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998‚Äì6008.\nVinyals, O.; Bengio, S.; and Kudlur, M. 2015. Order\nmatters: Sequence to sequence for sets. arXiv preprint\narXiv:1511.06391.\nWang, R.; Fang, X.; Lu, Y .; Yang, C.-Y .; and Wang, S. 2005.\nThe PDBbind database: methodologies and updates.Journal\nof medicinal chemistry, 48(12): 4111‚Äì4119.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nlocal neural networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , 7794‚Äì\n7803.\nWernicke, S. 2006. EfÔ¨Åcient detection of network mo-\ntifs. IEEE/ACM transactions on computational biology and\nbioinformatics, 3(4): 347‚Äì359.\nWu, Z.; Liu, Z.; Lin, J.; Lin, Y .; and Han, S. 2020. Lite\ntransformer with long-short range attention. arXiv preprint\narXiv:2004.11886.\nWu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Ge-\nniesse, C.; Pappu, A. S.; Leswing, K.; and Pande, V . 2018.\nMoleculeNet: a benchmark for molecular machine learning.\nChemical science, 9(2): 513‚Äì530.\nXiong, Z.; Wang, D.; Liu, X.; Zhong, F.; Wan, X.; Li, X.;\nLi, Z.; Luo, X.; Chen, K.; Jiang, H.; et al. 2019. Pushing the\nboundaries of molecular representation for drug discovery\nwith the graph attention mechanism. Journal of medicinal\nchemistry, 63(16): 8749‚Äì8760.\nXu, Z.; Wang, S.; Zhu, F.; and Huang, J. 2017. Seq2seq\nÔ¨Ångerprint: An unsupervised deep molecular embedding for\ndrug discovery. In Proceedings of the 8th ACM interna-\ntional conference on bioinformatics, computational biology,\nand health informatics, 285‚Äì294.\nYang, K.; Swanson, K.; Jin, W.; Coley, C.; Eiden, P.; Gao,\nH.; Guzman-Perez, A.; Hopper, T.; Kelley, B.; Mathea, M.;\net al. 2019. Analyzing learned molecular representations for\nproperty prediction. Journal of chemical information and\nmodeling, 59(8): 3370‚Äì3388.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.;\nShen, Y .; and Liu, T.-Y . 2021. Do Transformers Really\nPerform Bad for Graph Representation? arXiv preprint\narXiv:2106.05234.\nZhang, S.; Hu, Z.; Subramonian, A.; and Sun, Y . 2020.\nMotif-driven contrastive learning of graph representations.\narXiv preprint arXiv:2012.12533.\nZhang, Z.; Liu, Q.; Wang, H.; Lu, C.; and Lee, C.-K. 2021.\nMotif-based Graph Self-Supervised Learning for Molecular\nProperty Prediction. arXiv preprint arXiv:2110.00987.\nExperimental Setup\nMolformer Architecture\nA standard Molformer has 6 heterogeneous self-attention\nlayers, and each layer has 3 scales and 8 heads. Normally,\nscales are set by œÑ = [ œÅ\n2 ,œÅ, 2œÅ], where œÅ is the density of\neach corresponding dataset. The number of selected atoms\nK and the weight ratio Œª in AFPS is set as 4 and 0.1, re-\nspectively. We use ReLU as the activation function and a\ndropout rate of 0.1 for all layers if not speciÔ¨Åed. The input\nembedding size is 512 and the hidden size for FFN is 2048.\nFor BBBP and ClinTox, we use Molformer with 2 heteroge-\nneous self-attention layers with 4 heads. The scales are 0.8,\n1.6, and 3.0 ÀöA. The dropout rate is 0.2 and 0.6 for BBBP and\nClinTox, respectively. For BACE, we use a standard Mol-\nformer but with a dropout rate of 0.2.\nTraining Details\nWe use Pytorch to implement Molformer and data paral-\nlelism in two GeForce RTX 3090. An Adam optimizer is\nused and a ReduceLROnPlateau scheduler is enforced to ad-\njust it with a factor of 0.6 and a patience of 10. We apply no\nweight decay there. Each model is trained with 300 epochs,\nexcept for PDBbind where we solely train the model for 30\nepochs. The data summary is in Table 7.\nFor QM7 and QM8, we use a batch size of 128 and a\nlearning rate of 10‚àí4. For QM9, we use a batch size of\n256 and a learning rate of 10‚àí3. For PDBbind, we use a\nbatch size of 16 and a learning rate of 10‚àí4. All hyper-\nparameters are tuned based on validation sets. For all molec-\nular datasets, we impose no limitation on the input length\nand normalize the values of each regression task by mean\nand the standard deviation of the training set. We used grid\nsearch to tune the hyper-parameters of our model and base-\nlines based on the validation dataset (see Table 6).\nMotif Extraction\nWe adopt RDKit to search motifs from SMILES repre-\nsentations of small molecules. For QM9 and PDBbind,\nAtom3D provides both 3D coordinates and SMILES. For\nQM7, DeepChem offers SMILES. For QM8, we Ô¨Årst at-\ntain SMILES from their 3D representations using RDKit and\nthen extract motifs. For motifs in proteins, Besides, we uti-\nlize a Latin hyper-cube sampling to sample 1K quaternary\namino acids as the candidates in each iteration, and Œ≥ is set\nas 1e‚àí3. The motif lexicon explored by our RL method for\nthe PDBbind task is in Figure 5.\nFor motifs in proteins, since we aim to mine the optimal\ntask-speciÔ¨Åc lexicon, it is unnecessary to take into account\nall 160,000 possible quaternions. Instead, we only need to\nconsider the existing quaternions in the datasets. SpeciÔ¨Å-\ncally, there are only 29,871 kinds of quaternions in PDB-\nbind. Besides, we utilize a Latin hyper-cube sampling to\nsample 1K quaternary amino acids as the candidates in each\niteration, and Œ≥is set as 1e‚àí3. The motif lexicon explored by\nour RL method for the PDBbind task is in Figure 5. More-\nover, the portion of motifs in a protein (or some part such as\na pocket) is the number of motifs divided by the number of\nall amino acids in that protein.\nAdditional Experimental Results\nConformation ClassiÔ¨Åcation\nTask and Data. To explore the inÔ¨Çuence of multiple con-\nformations, we introduce a new task, conformation clas-\nsiÔ¨Åcation, to evaluate the model‚Äôs capacity to differenti-\nate molecules with various low-energy conformations. We\nuse the recent GEOM-QM9 which is an extension to the\nQM9 dataset. It contains multiple conformations for most\nmolecules, while the original QM9 only contains one.\nWe randomly draw 1000 different molecules from\nGEOM-QM9, each with 20 different conformations. Models\nare required to distinguish the molecular type given different\nconformations. We take half of each molecule‚Äôs conforma-\ntions as the training set and another half as the test split.\nSince it is a multi-class classiÔ¨Åcation problem with 1000\nclasses, we compute the micro-average and macro-average\nROC-AUC as well as the accuracy for evaluations.\nResults. Molformer achieves a perfect micro-average and\nmacro-average ROC-AUC as well as a high accuracy (see\nTable 8). This indicates the strong robustness of our model\nagainst different spatial conformations of molecules.\nAFPS vs. FPS.\nTo have a vivid understanding of the atom sampling algo-\nrithm, we conducted a case study on a random molecule (see\nFigure 6). Points selected by FPS are randomized and ex-\nclude vital atoms like the heavy metal Nickel (Ni). With the\nadoption of AFPS, sampled points include Ni, Nitrogen (N),\nand the benzene ring besides that they keep remote distances\nfrom each other. Moreover, FPS integrates too many fea-\ntures of trivial atoms like Hydrogen (H) while missing out\non key atoms and motifs, which will signiÔ¨Åcantly smooth the\nmolecular representations and lead to poor predictions. This\nillustrative example shows the effectiveness of our AFPS\nto offset the disadvantages of the conventional FPS in 3D\nmolecular representation.\nProtein\nWe envision a protein-ligand pair in PDBbind in Figure 7.\nIt can be observed that motifs occur much more frequently\nin the area of the protein pocket than in other places. To be\nspeciÔ¨Åc, the ligand is exactly surrounded by our discovered\nmotifs, which strongly demonstrates the effectiveness of our\nRL method to mine motifs with semantic meanings.\nHyper-parameter Description Range\nbs The input batch size. [16, 128, 256]\nlr The initial learning rate of ReduceLROnPlateau learning rate scheduler. [1e‚àí4, 1e‚àí5]\nminlr The minimum learning rate of ReduceLROnPlateau learning rate scheduler. 5e‚àí7\ndropout The dropout ratio. [0.1, 0.2, 0.6]\nn encoder The number of heterogeneous self-attention layers. [2, 6]\nhead The number of self-attention heads. [4, 8]\nembeddim The dimension of input embeddings. 512\nffndim The hidden size of MLP layers. 1024\nk The number of sampled points in AFPS. [4, 8, 10, 20]\nlambda The balance ratio in AFPS. [0.1, 0.5, 1.0, 2.0]\ndistbar The scales of the multi-scale mechanism (ÀöA). [[0.75, 1.55, 3.0], [0.8, 1.6, 3.0], [1.5, 3.0, 6.0]]\nTable 6: The training hyper-parameters.\nCategory Dataset Tasks Task Type Molecules Atom\nClass\nMin.\nAtoms\nMax.\nAtoms\nDensity\n( ÀöA) Metric\nQuantum\nChemistry\nQM7 1 regression 7,160 5 4 23 2.91 MAE\nQM8 12 regression 21,786 5 3 26 1.54 MAE\nQM9 12 regression 133,885 5 3 28 1.61 MAE\nPhysiology BBBP 1 classiÔ¨Åcation 2,039 13 2 132 2.64 ROC-AUC\nClinTox 2 classiÔ¨Åcation 1,478 27 1 136 2.83 ROC-AUC\nBiophysics PDBind2 1 regression 11,908 23 115 1,085 5.89 RMSE\nBACE 1 classiÔ¨Åcation 1,513 8 10 73 3.24 ROC-AUC\nTable 7: Key statistics of datasets from three different categories.\nHydrocarbons\nHaloalkanes\nF\n Cl\n I\nGroups containing Oxygen\nO\nOH\nO\nO\nO\nO\nGroups containing Nitrogen\nO\nN\n NN NH2\nFigure 4: Examples of the four different motif categories that we apply in Molformer based on functional groups.\nMetrics Acc. Micro. Macro.\nMolformer 0.999 1.000 1.000\nTable 8: Molformer performance on conformation classiÔ¨Åcation.\nGADD ADDT\nDDTV LDTG\nVIGG PVIG\nIGGI\nLVSL\nRLDT\nTGAD\nFigure 5: The motif lexicon found by our RL method, where each motif is composed of ten quaternary amino acids and the\nupper-case names correspond to their compositions.\nH\nF\nCl\nC\nNi\nO\nN\nFigure 6: Sampled points using FPS and AFPS. The red circle represents a benzene ring. We do not show dummy nodes there.\nFigure 7: The protein-ligand pair of 2aqu in PDBbind. The yellow dot-halos denotes the motifs found in this protein.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6628275513648987
    },
    {
      "name": "Transformer",
      "score": 0.5394732356071472
    },
    {
      "name": "Molecular graph",
      "score": 0.44529539346694946
    },
    {
      "name": "Embedding",
      "score": 0.42036232352256775
    },
    {
      "name": "Theoretical computer science",
      "score": 0.41455116868019104
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38959234952926636
    },
    {
      "name": "Graph",
      "score": 0.3827514052391052
    },
    {
      "name": "Topology (electrical circuits)",
      "score": 0.3619872033596039
    },
    {
      "name": "Algorithm",
      "score": 0.34773263335227966
    },
    {
      "name": "Physics",
      "score": 0.16314992308616638
    },
    {
      "name": "Mathematics",
      "score": 0.1400468945503235
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}