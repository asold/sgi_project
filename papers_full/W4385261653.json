{
  "title": "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation",
  "url": "https://openalex.org/W4385261653",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2351550429",
      "name": "Zhang, Cen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3047964412",
      "name": "Zheng Yaowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2202791573",
      "name": "Bai Ming-qiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750108534",
      "name": "Li, Yeting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118688212",
      "name": "Ma Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2364446491",
      "name": "Xie, Xiaofei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3083994039",
      "name": "Li Yuekang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2048077395",
      "name": "Sun Li-min",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102234800",
      "name": "Liu Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964241064",
    "https://openalex.org/W3162605691",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W4389364446",
    "https://openalex.org/W4254879869",
    "https://openalex.org/W4385750097",
    "https://openalex.org/W4378591002",
    "https://openalex.org/W4387947606",
    "https://openalex.org/W3136127207",
    "https://openalex.org/W4388857347",
    "https://openalex.org/W4308538981",
    "https://openalex.org/W3017944659",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2963298680",
    "https://openalex.org/W4324325724",
    "https://openalex.org/W3048197573",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W3195365517",
    "https://openalex.org/W4390527999",
    "https://openalex.org/W4320854935",
    "https://openalex.org/W4362656212",
    "https://openalex.org/W4384345745",
    "https://openalex.org/W4384345667",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W4367694420",
    "https://openalex.org/W4367860052",
    "https://openalex.org/W4362598291"
  ],
  "abstract": "LLM-based (Large Language Model) fuzz driver generation is a promising research area. Unlike traditional program analysis-based method, this text-based approach is more general and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges. To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that: - While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; - LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; - While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection. Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.",
  "full_text": "How Effective Are They? Exploring Large Language Model Based\nFuzz Driver Generation\nCen Zhang\nNanyang Technological University\nSingapore\nYaowen Zheng‚àó\nNanyang Technological University\nSingapore\nMingqiang Bai\nIIE, CAS; Sch of Cyber Security, UCAS\nBeijing, China\nYeting Li\nIIE, CAS; Sch of Cyber Security, UCAS\nBeijing, China\nWei Ma\nNanyang Technological University\nSingapore\nXiaofei Xie\nSingapore Management University\nSingapore\nYuekang Li\nThe University of New South Wales\nSydney, Australia\nLimin Sun\nIIE, CAS; Sch of Cyber Security, UCAS\nBeijing, China\nYang Liu\nNanyang Technological University\nSingapore\nAbstract\nFuzz drivers are essential for library API fuzzing. However, auto-\nmatically generating fuzz drivers is a complex task, as it demands\nthe creation of high-quality, correct, and robust API usage code. An\nLLM-based (Large Language Model) approach for generating fuzz\ndrivers is a promising area of research. Unlike traditional program\nanalysis-based generators, this text-based approach is more gener-\nalized and capable of harnessing a variety of API usage information,\nresulting in code that is friendly for human readers. However, there\nis still a lack of understanding regarding the fundamental issues on\nthis direction, such as its effectiveness and potential challenges.\nTo bridge this gap, we conducted the first in-depth study target-\ning the important issues of using LLMs to generate effective fuzz\ndrivers. Our study features a curated dataset with 86 fuzz driver\ngeneration questions from 30 widely-used C projects. Six prompting\nstrategies are designed and tested across five state-of-the-art LLMs\nwith five different temperature settings. In total, our study evaluated\n736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+\ncharged tokens). Additionally, we compared the LLM-generated\ndrivers against those utilized in industry, conducting extensive\nfuzzing experiments (3.75 CPU-year). Our study uncovered that: 1)\nWhile LLM-based fuzz driver generation is a promising direction, it\nstill encounters several obstacles towards practical applications; 2)\nLLMs face difficulties in generating effective fuzz drivers for APIs\nwith intricate specifics. Three featured design choices of prompt\nstrategies can be beneficial: issuing repeat queries, querying with\nexamples, and employing an iterative querying process; 3) While\nLLM-generated drivers can yield fuzzing outcomes that are on par\nwith those used in the industry, there are substantial opportuni-\nties for enhancement, such as extending contained API usage, or\nintegrating semantic oracles to facilitate logical bug detection. Our\ninsights have been implemented to improve the OSS-Fuzz-Gen\nproject, facilitating practical fuzz driver generation in industry.\n‚àóYaowen Zheng is the corresponding author.\nIIE ‚ÜíInstitute of Information Engineering, CAS ‚ÜíChinese Academy of Sciences.\nEmail To: cen001@e.ntu.edu.sg & yaowen.zheng@ntu.edu.sg.\nConference‚Äô17, July 2017, Washington, DC, USA\n2024. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nCCS Concepts\n‚Ä¢ Security and privacy ‚ÜíSoftware security engineering .\nKeywords\nFuzz Driver Generation, Fuzz Testing, Large Language Model\nACM Reference Format:\nCen Zhang, Yaowen Zheng, Mingqiang Bai, Yeting Li, Wei Ma, Xiaofei\nXie, Yuekang Li, Limin Sun, and Yang Liu. 2024. How Effective Are They?\nExploring Large Language Model Based Fuzz Driver Generation. In . ACM,\nNew York, NY, USA, 13 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 Introduction\nFuzz testing, aka fuzzing, has become the standard approach for\ndiscovering zero-day vulnerabilities. Fuzz drivers are necessary\ncomponents for fuzzing library APIs since fuzzing requires a di-\nrectly executable target program. Essentially, a fuzz driver is a piece\nof code responsible for accepting mutated input from fuzzers and\nexecuting the APIs accordingly. An effective driver must contain a\ncorrect and robust API usage since incorrect or unsound usage can\nresult in extensive false positive or negative fuzzing results, incur-\nring extra manual validation efforts or testing resources waste. Due\nto the high standard required, fuzz drivers are typically written by\nhuman experts, which is a labor-intensive and time-consuming pro-\ncess. For instance, OSS-Fuzz [19], the largest fuzzing framework for\nopen-source projects, maintains thousands of fuzz drivers written\nby hundreds of contributors over the past seven years.\nGenerative LLMs (Large Language Models) have gained signifi-\ncant attention for their ability in code generation tasks [31, 33, 35,\n40]. They are language models trained on vast quantities of text and\ncode, providing a conversational workflow where natural language\nbased queries are posed and answered. LLM-based fuzz driver gen-\neration is an attractive direction. On one hand, LLMs inherently\nsupport fuzz driver generation as API usage inference is a basic\nscenario in LLM-based code generation. On the other hand, LLMs\nare lightweight and general code generation platforms. Existing\nworks [2, 22, 24, 26, 62‚Äì64], which generate drivers by learning\nAPI usage from examples, requires program analysis on examples,\nwhile LLM-based generation can mostly work on texts. This offers\nenhanced generality which facilitates not only the application on\nmassive quantity of real-world projects but also the utilization of\narXiv:2307.12469v5  [cs.CR]  29 Jul 2024\nConference‚Äô17, July 2017, Washington, DC, USA Cen Zhang, Yaowen Zheng, Mingqiang Bai, Yeting Li, Wei Ma, Xiaofei Xie, Yuekang Li, Limin Sun, and Yang Liu\nlearning inputs in different forms. Various sources of API usage\nknowledge such as documentation, error information, and code\nsnippets can be seamlessly integrated in text form, benefiting the\ngeneration. Moreover, LLMs can generate human-friendly code.\nWhile some research efforts have been devoted to LLM-based code\ngeneration tasks [15, 23, 25, 29, 32, 38, 42, 49], none of them can\nprovide a fundamental understanding on this direction.\nTo address this gap, we conducted an empirical study for un-\nderstanding the effectiveness of zero-shot fuzz driver generation\nusing LLMs. Note that our primary goal is to understand the basics\ntowards generating \"more\" effective fuzz drivers, rather than gener-\nating \"more effective \" fuzz drivers. This is because creating effective\ndrivers for more targets is a more fundamental issue than improving\nexisting ones. Overall, four research questions are studied:\n‚Ä¢RQ1 To what extent can current LLMs generate effective\nfuzz drivers for software testing?\n‚Ä¢RQ2 What are the primary challenges associated with\ngenerating effective fuzz drivers using LLMs?\n‚Ä¢RQ3 What are the effectiveness and characteristics for\ndifferent prompting strategies?\n‚Ä¢RQ4 How do LLM-generated drivers perform comparing\nto those practically used in the industry?\nTo answer these RQs, we assembled a dataset of 86 fuzz driver\ngeneration questions collected from 30 widely-used C projects from\nOSS-Fuzz projects. Each question represents a library API for which\na corresponding fuzz driver is needed to conduct effective fuzz test-\ning. We devised six prompt strategies, taking into account three\nkey factors: the content of the prompts, the nature of interactions\nbetween the strategies and models, and the repetition of the entire\nquery process. Our evaluation encompassed five state-of-the-art\nLLMs with five different temperature settings. The assessed LLMs\nincluded closed-source LLMs such as gpt-4-0613 [37], gpt-3.5-turbo-\n0613 [36], and text-bison-001 [20], as well as open-source LLMs op-\ntimized for code generation, namely, codellama-34b-instruct [4] and\nwizardcoder-15b-v1.0 [58]. For a rigorous assessment, we developed\nan evaluation framework automatically validating the generated dri-\nvers based on the results of compilation and short-term fuzzing, and\nmanually crafted checkers on API usage semantic correctness. In\ntotal, 736,430 fuzz drivers, at the cost of 0.85 billion tokens ($8,000+\ncharged tokens, 0.17/0.21 billion for gpt-4-0613/gpt-3.5-turbo-0613),\nwere evaluated. Besides, comparison with manually written drivers\nin industry on code and fuzzing metrics,e.g., 24-hour fuzzing exper-\niments (3.75 CPU-year), are conducted to seek practical insights.\nThe overall implications for the effectiveness of using LLM to\ngenerate fuzz drivers are two-fold. On one hand, LLMs have demon-\nstrated outstanding performance in evaluated configurations1, sug-\ngesting a strong potential for this approach. For instance, the op-\ntimal configuration can address 91% questions (78/86) and all top\n20 configurations can address at least half of the questions. On the\nother hand, resolving a question means successful generation of at\nleast one effective fuzz driver for the assessed API, which does not\nnecessarily imply full practicality. For high automation and usabil-\nity, three challenges have been identified: ‚ù∂ Improving the success\nrate to reduce generation costs. Although most questions can be\nresolved by LLMs, the cost can be exceptionally high. Typically, 71%\n1A configuration stands for a combination of <Model, Prompt Strategy, Temperature>.\nof questions are resolved by repeating the entire query process at\nleast five times and 45% require repeating the process ten times. By\nenhancing their accuracy, significant financial costs for automation\ncan be saved. ‚ù∑ Ensuring semantic correctness in API usage. Occa-\nsionally, validating the effectiveness of a generated driver requires\nthe understanding of API usage semantics. Failed to do so can result\nin ineffective fuzzing with false positive or negative results. In our\nevaluation, this requirement was observed in 34% of the assessed\nAPIs (29/86), impeding practical application.‚ù∏ Addressing complex\nAPI dependencies. 6% of questions (5/86) cannot be resolved by any\nevaluated configurations since their drivers‚Äô generation requires\nnontrivial preparation of the API execution contexts, which cannot\nbe appropriately hinted by any collected usage information. For\nexample, some drivers require a standby network server or client\nto be created for interacting with the target API. These are typical\ncases representing complex real-world testing requirements which\ndeserves exploration of advanced solutions.\nPrompt strategy, temperature, and model are key factors consid-\nerably affect the overall performance. Our evaluation suggests that\nthe dominant strategy is the one incorporating three key designs:\nrepeatedly query, query with extended information, and iterative\nquery. Comparing with naive strategy, its question resolve rate\nsoars from 10% to 91%. Evaluation with lower temperature settings,\nespecially below the threshold of 1.0, have higher performance.\nThis is intuitive since lower temperatures lead to more consistent\nand predictable outputs, which fits the goal of generating an ef-\nfective fuzz driver. Besides, the optimal temperature setting in our\nevaluation is 0.5. As for models, gpt-4-0613, wizardcoder-15b-v1.0\nare the best closed-source, open-source models, respectively.\nFundamentally, LLMs struggle to generate fuzz drivers which\nrequire complex API usage specifics. We identified three benefi-\ncial designs that have distinct characteristics: ‚ù∂ repeatedly queries.\nWhen the configurations are stronger, the benefits of repetition\nbecome higher. Besides, the benefits significantly drop after the\nfirst few rounds. A suggested repetition value is 6. ‚ù∑ query with\nextended information. Adding API documentation is less helpful\nwhile adding example snippets can help significantly. Specifically,\ntest/example files of the target project or its variants are high-\nquality example sources; ‚ù∏ iterative queries. It adds a cyclic driver\nfix progress after the initial query, which improves LLM‚Äôs perfor-\nmance through its step-by-step problem-solving approach and a\nmore thorough utilization of existing usage. Besides, all the above\ndesigns will significantly increase the token cost. In comparison\nto OSS-Fuzz drivers, LLM-generated drivers demonstrated com-\nparable fuzzing outcomes. However, since LLMs tend to generate\nfuzz drivers with minimal API usages, significant room is still left\nfor improving generated drivers, such as expanding API usage and\nincorporating semantic oracles.\nTo further translate our research insights into practical values,\npart of our prompting strategies, including checking, categorizing,\nand fixing driver with runtime errors, have been implemented into\nOSS-Fuzz-Gen [49], the largest LLM-based fuzz driver generation\nframework operated by Google OSS-Fuzz team, facilitating the\ncontinuous fuzzing of real-world projects.\nIn summary, our contributions are:\nHow Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation Conference‚Äô17, July 2017, Washington, DC, USA\n1 int LLVMFuzzerTestOneInput(const uint8_t *Data, size_t Size) {\n2 // Input_Arrangement(Data, Size, ...);\n3 Init_Context_And_Arguments(Data, Size, ...);\n4 Call_Target_API(...);\n5 // Extended_API_Usage(...);\n6 // Semantic_Oracle_Checks(...);\n7 Resource_Cleaning(...);\n8 return 0;\n9 }\nFigure 1: Key Components of A Fuzz Driver.\n‚Ä¢we conducted the first in-depth study on the effectiveness\nof LLM-based fuzz driver generation, which showcases the\npotentials and challenges of this direction;\n‚Ä¢we designed and implemented six driver generation strate-\ngies. They are evaluated in large scale, with a systematic\nanalysis on the effectiveness, the pros and the cons;\n‚Ä¢we compared generated drivers with industrial used ones,\nand summarized the implications on future improvements.\n‚Ä¢we ported our strategies to improve the largest industrial\nfuzz driver generation framework, facilitating the continuous\nfuzzing of hundreds open-source projects.\n2 Preliminaries\nFuzz Driver Basics. The key components of a fuzz driver are\nillustrated in Figure 1. A typical fuzz driver has three necessary\nparts: prerequisites initialization (line 3), execution (line 4), and\npost-cleaning (line 7). Besides, there are three optional parts com-\nmetned in lines 2, 5, and 6 that can improve a driver‚Äôs effectiveness.\nLine 2 part improves a driver by proper input arrangement such\nas rejecting too short or too long inputs, interpreting input data as\nmultiple testing arguments, etc. Line 5 part enables a driver to call\nmore APIs which triggers more program behaviors during fuzzing.\nFinally, line 6 part adds semantic oracles for detecting logical bugs.\nThese oracles are similar to assert statements in unit tests, abort-\ning execution when certain program properties are unsatisfied.\nSince a driver will be repeatedly executed with randomly mutated\ninput, there is a high requirement on its correctness and robust-\nness. Incorrect or unrobust usage can lead to both false positives\nand negatives. For instance, if a driver failed to feed the mutated\ndata into the API, its fuzzing can never find any bug. Or if an API\nargument is incorrectly initialized, false crashes may be raised.\nMinimal Requirements of Effective Fuzz Drivers. The min-\nimal requirements covers the line 3,4, and 7 of Figure 1, which\nmainly include correctly initializing the arguments and satisfying\nnecessary control flow dependencies. Argument initialization can\nbe one of the following cases (in the order of simplicity): ‚ù∂ C1:\nIf the argument value can be any value or should be naive values\nlike 0 or NULL, a variable is declared or a literal constant is used\ndirectly; ‚ù∑C2: If the argument is supposed to be a macro or a global\nvariable that is already defined in common libraries or the target\nAPI‚Äôs project, it is located and used; ‚ù∏ C3: If creating the argument\nrequires the use of common library APIs, such as creating a file\nand writing specific content, common practices are followed; ‚ùπ\nC4: If initializing the argument requires the output of other APIs\nwithin the project, those APIs are initialized first following the\nabove initialization cases.\n3 Methodology\n3.1 Design of Prompt Strategies\nFigure 2 illustrates our designed prompt strategies. From left to\nright, the figure first provides a tabular overview for all proposed\nstrategies, then details two types of prompt templates involved, and\nlastly maps the templates to concrete query examples. Note that the\nlisted examples are simplified for demonstration purposes, while un-\nmodified real-world examples for each strategy can be found at [1].\nKey Designs. As shown in the top-left side of Figure 2, there\nare three key designs for prompt strategies, including query with\ndifferent types of API information, query repeatedly, and query\niteratively. Design I aims for understanding the generation effec-\ntiveness given different API information as query contexts. The\ninformation is divided as two types: the basic API information and\nthe extended. The former includes fundamental information such as\nheader file name and API declaration, which are precisely specified\nand generally accessible in library API fuzzing scenario, while the\nlatter requires additional resources like API-specific documentation\nor usage example code snippets, whose quality and availability vary\nfor different targets. To account for the inherent randomness in LLM\noutput generation, design II, repeatedly query, is introduced. Given\nrepetition time as value K, the entire query process of a strategy\nwill be repeated K times (ùêæ ‚â•1), generating K independent drivers.\nThe maximum value of K is set as 40 in our study. This is an em-\npirically value we believe is comprehensive enough to understand\nthe effectiveness of repetition. Design III is used to understand the\neffectiveness of different query workflows. The driver generation\nin non-iterative strategies follows a one-and-done manner where\nthe final driver is synthesized via a single query without further\nrefinement. Iterative strategies have a generate-and-fix workflow.\nIf the driver generated in first query fails to pass the automatic\nvalidation, subsequent fix prompts are composed based on the error\nfeedback and queried. The fix continues until the driver passes val-\nidation or a pre-defined maximum number of iterations is reached.\nThe iteration is limit as five in our evaluation.\nAcronym. Strategies are named by concatenating the abbrevi-\nations of the three key designs. For all strategies, there is a suffix\n\"K\" indicating that the repetition times of their query process. If a\nstrategy name contains \"ITER\", it is an iterative prompt strategy.\nOtherwise, it is non-iterative. Besides, different combinations of\nAPI information used in generation prompt have different abbre-\nviations. As shown in the bottom-left side of the Figure 2, there\nare four different combinations: the NAIVE query context (abbr\nas NAIVE, ‚ë†), the BAsic query ConTeXt (abbr as BACTX, ‚ë† + ‚ë°),\nextending API DOCumentation to basic ConTeXt (abbr as DOCTX,\n‚ë† + ‚ë° + ‚ë£), and extending example UsaGe code snippets to the\nbasic ConTeXt (abbr as UGCTX, ‚ë† + ‚ë° + ‚ë¢). Lastly, the prefix\nALL in ALL-ITER-K indicates that its prompts can be the prompt\ndesigned in any other strategies.\nNAIVE-K & BACTX-K. Both two strategies only use basic API\ninformation in query and non-iterative workflow. Their only differ-\nence is the richness of the prompt context information. Specifically,\nNAIVE-K directly asks LLMs to implement the fuzz driver solely\nbased on a specified function name, while BACTX-K provides a basic\ndescription of the API. In prompts of BACTX-K, it first indicates the\ntask scope using #include statement, then provides the function\nConference‚Äô17, July 2017, Washington, DC, USA Cen Zhang, Yaowen Zheng, Mingqiang Bai, Yeting Li, Wei Ma, Xiaofei Xie, Yuekang Li, Limin Sun, and Yang Liu\nFix Prompt Template\n[Content]Target API specific content in prompt template\nFix Prompt Example\nPrompt Strategy Design\n```\n... obj = bpf_open_mem(Data, Size, NULL); ...\n```\nThe above C code can be built successfully but will crash imme-\ndiately during execution(ASAN-assertion failure)\nError line: `obj = bpf_open_mem(Data, Size, NULL);`\nCrash stack and nearby code:\n#3 0x5f7744 in bpf_open_mem /src/bpf/libbpf.c:256:28\n \n 256 assert(buf != NULL && sz != 0);\nBased on the above information, fix the code.\nNAIVE-K ‚ë†\n‚ë† + ‚ë°\n‚ë†+‚ë°+‚ë£\n‚ë†+‚ë°+‚ë¢\nBACTX-K\nDOCTX-K\nUGCTX-K\n‚ë† + ‚ë° ‚ìê\n‚ë†+‚ë°+‚ë¢\n‚ìê  + ‚ìë\nBA-ITER-K\nALL-ITER-K\nLegend: Basic API info only Contain extended API info\n‚ë† + ‚ë°\n‚ë†+‚ë°+‚ë£\n‚ìê\nGeneration Prompt Template\n‚ìë\n‚ìê\n```\n[Code of error fuzz driver]\n```\n[One sentence error summary]\n[Error line code]\n[Error details]\n[Other supplemental info]\nTask description\n‚ìê\n‚ë†\n‚ë°\n‚ë†\n‚ë£\n‚ë¢\n‚ë°\nTask description\n[API documentation]\n[API declaration]\n[Example code snippets\nwhich shows API usage]\n[Header file inclusion]\nTask description\nAcronym Prompts (Generation + Fix)ITEX\nor\nor\nor\n‚úì \n‚úó \n‚úó \n‚úì \n‚úì ‚úì \n‚úì ‚úó \n‚úó \n‚úó \n‚úó \n‚úó \nEX‚Üíuse extended API info; IT‚Üíiterative query & fix\nGeneration Prompt Example\n// The following is a fuzz driver written in C language, complete\nthe implementation. Output the continued code in reply only.\n#include \"bpf/libbpf.h\"\n// @ examples of API usage from bpf-loader.c \n// void test_bpf(const char *bpf_file) {\n//   ... \n//   obj = bpf_open_mem(_buffer, _size, NULL);\n//   ... }\n/* @brief it creates a bpf_object by reading the BPF objects ...\n * @param buf pointer to the buffer containing BPF ... */\nextern bpf* bpf_open_mem(char *buf, int sz, struct opts *opts);\n// the following function fuzzes bpf_open_mem\nint LLVMFuzzerTestOneInput(const uint8_t *Data, size_t Size) {\nDesign II - „Äé Query Repeatedly „Äè\n- Repeat whole query process K\ntimes for K independent results\n- Suffix \"-K\" in name...\n1 K2\nDesign I - „Äé Query With Different Types of API Info „Äè\nDesign III - „Äé Query Iteratively „Äè\n- Basic Info: Precisely specified & generally accessible,\ne.g., header file and API declaration\n- Extended Info: Not guaranteed in quality and avail-\nability, e.g., API documentation and usage snippets\n- Non-Iterative: one generation\nquery\n- Iterative: one generation query\nwith up to X fix queries1 2 X+1...\n1\nFigure 2: Prompt Strategies Overview. K is 1 or 40 in our evaluation and X is 5. The examples are simplified for demonstration purpose. In the fix\nprompt example, the driver error is caused by missing check of Size > 0 before calling the API, and the nearby code of #3 stack frame hints the error.\ndeclaration, and finally requests implementation. The declaration\nis extracted from the Abstract Syntax Tree (AST) of the header file,\nincluding both the signature and argument variable names.\nDOCTX-K & UGCTX-K. These two strategies are extended\nfrom BACTX-K by adding extended usage information in query.\nTheir effectiveness represents the effects of two types of extended\ninformation: API documentation and example code snippets. Note\nthat, for DOCTX-K, not all APIs have associated documentation\n(49/86 questions in our study). The documentation of 20 questions\nwas automatically extracted from the header files, while the remain-\ning 29 were manually collected from sources like project websites,\nrepositories, and developer manuals. For UGCTX-K, example code\nsnippets of an API are collected as follows: ‚ù∂ retrieving the files\ncontaining usage code viaSourceGraphcli [44]. This is a keyword\nsearch among all public code repositories including Github, Git-\nlab, etc. The crawling command is src search -json \"file:.*Àôc\nlang:c count:all API\" where API should be replaced by the\ntarget API name. ‚ù∑ identifying and excluding fuzz drivers by re-\nmoving the files containing function LLVMFuzzerTestOneInput.\n‚ù∏ extracting all functions directly calling the target API as example\ncode snippets via ANTLRbased source code analysis. ‚ùπ deduplicat-\ning the snippets by if the Jaccard Similarity [21] of any two snippets\n‚â•95%. UGCTX-K will randomly use one snippet in the prompt. For\nsnippet that was too long to be included into prompt, it is truncated\nline by line until satisfying the token length limitation.\nBA-ITER-K & ALL-ITER-K. Iterative strategies have two types\nof prompt templates for initial generation query and subsequent fix\nquery. The initial generation prompt can be either of BACTX-K‚Äôs,\nDOCTX-K‚Äôs, and UGCTX-K‚Äôs. As for fix queries, we have designed\nseven fix templates to address seven prevalent types of errors in\nthe generated drivers. They follow one general fix template shown\nin Figure 2 but are filled with error type specific details. Due to\nthe page limit, we discuss the key concepts of these fix prompts,\nleaving the detailed designs and examples in [1]. These errors are\nof compilation errors (1/7), linkage errors (1/7), and fuzzing run-\ntime errors (5/7). The error information (abbr of [One sentence\nerror summary], [Error line code], and [Error details]) for\nthe first two error types can be programmatically retrieved from\nthe compiler. According to different abnormal behaviors observed\nin fuzzing, fuzzing runtime errors have five subtypes, including\nmemory leakage, out-of-memory, timeout, crash, and non-effective\nfuzzing (no coverage increase in one-minute short-term fuzzing).\nThe error information of runtime errors are retrieved by extract-\ning the crash stacks and sanitizer summary from libfuzzer logs.\nLastly, for the errors that can locate its error line, we further infer its\nroot cause API and fill API information like declaration, documenta-\ntion, or usage snippets into[Other supplemental information]\nin fix prompt. For simplicity, root cause API is identified by naively\nfinding the last executed API based on the error line located.\nNote that iterative strategies exclusively utilize automated check-\ners, ensuring that the manually crafted semantic checkers described\nin Section 3.2 are only used for thorough evaluation of the effec-\ntiveness of strategies. The principal distinction between the two\niterative strategies, BA-ITER-K and ALL-ITER-K, lies in the scope\nof information utilized within the queries. BA-ITER-K confines\nits use to only basic API details and error information, whereas\nALL-ITER-K encompasses all available information. As shown in\nHow Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation Conference‚Äô17, July 2017, Washington, DC, USA\nFuzz\nDriver\nCompile & Link Check\nShort-Term Fuzz CheckTarget Bugs Filtering\nSemantic Test\nEffective\nResultEffectiveness Validation\nFigure 3: Fuzz Driver Effectiveness Validation Process.\nFigure 2, this leads to multiple options for the included extended\ninformation. The ALL-ITER-K strategy selects options randomly.\n3.2 Evaluation Framework\nEvaluation Question Collection. One question used in evalua-\ntion is simply designed as generating fuzz drivers for one given API.\nHowever, not all APIs are suitable to be set as questions. Naively\ncollecting all APIs from projects will lead to the creation of mean-\ningless or confusing questions which influences the evaluation\nresult. Specifically, some APIs, such as void libxxx_init(void),\nare meaningless fuzz targets since the code executed behind the\nAPIs can not be affected by any input data. Some APIs can only be\nsupplemental APIs rather than the main fuzzing target due to the\nnature of their functionalities. For example, given two APIsobject\n*parse_from_str(char *input)and void free_object(object\n*obj), feeding the mutated data into input is apparently a better\nchoice than feeding a meaningless pointer to the obj argument.\nHowever, calling the latter API when fuzzing the former is mean-\ningful since ‚ù∂ it may uncover the hidden bug when the former\ndoes not correctly initialize the object *by freeing the members\nof object; ‚ù∑ it releases the resources allocated in this iteration of\nfuzzing, which prevents the falsely reported memory leak issue.\nTo guarantee the collected APIs are qualified and representative,\nwe collected the core APIs of existing fuzz drivers from OSS-Fuzz\nprojects. A driver‚Äôs core APIs are identified using the following crite-\nria: ‚ù∂they are the target APIs explicitly pointed out by the author in\nits driver file name or the code comments,e.g., dns_name_fromwire\nis the core API of driver dns_name_fromwire.c; ‚ù∑ otherwise, we\npick the basic APIs as the core rather than the supplemental ones.\nFor example, we picked the former between parse and use/free APIs.\nFor the fuzz drivers which are composite drivers fuzzing multiple\nAPIs simultaneously, we identified multiple core APIs from them.\nSpecifically, we randomly selected 30 projects from OSS-Fuzz (com-\nmit 135b000926) C projects, manually extracted 86 core APIs from\n51 fuzz drivers. Full list of questions are post at [1].\nEffectiveness Validation Criteria. Assessing the effectiveness\nof a generated fuzz driver is complex since identifying both false\npositives (bugs caused by the driver code) and negatives (can never\nfind bugs given its incorrect usage) rely on the understanding of API\nusage semantics. Figure 3 is a streamlined four-step semi-automatic\nvalidation process:‚ù∂Use a compiler to check for grammatical errors\nin the driver code. ‚ù∑ Observe the driver in a one-minute fuzzing\nsession starting with no initial seed. It is ineffective if it either fails\nto show coverage progress or reports any bugs. The assumption\nbehind is that, given a poor fuzzing setup, neither the zero coverage\nprogress nor the quick identification of bugs for a well-tested API\nare normal behaviours. Considering that this criteria can still lead to\nincorrect validation, two additional steps are introduced for result\nrefinements. ‚ù∏ If the driver reports bugs in ‚ù∑, we filter the true\nbugs contained inside. This is done by first collecting target true\nbugs via ten cpu-day fuzzing using existing OSS-Fuzz drivers, then\nmanually building filters based on the root causes of these bugs. ‚ùπ\nFor drivers reporting no bugs after ‚ù∏, we check whether they are\nsubstantially testing the target API or not. To this end, we write\nAPI-specific semantic tests to detect common ineffective patterns\nobserved in LLM-generated fuzz drivers. The tests include verifying\nthe target API is called for all 86 questions, checking the correct\nusage of fuzzing data to key arguments for 8 questions (such as\nfuzzing file contents instead of file names), ensuring critical depen-\ndent APIs are invoked in 16 questions, and confirming necessary\nexecution contexts are prepared for 5 questions (for instance, hav-\ning a standby server process available for testing client APIs). We\nimplement these tests by injecting hooking code into the driver.\nFor more details on these tests, please refer to our website [1].\nEvaluation Configuration. In this paper, a configuration rep-\nresents a specific combination of the three factors: the LLM used,\nthe prompt strategy employed, and the selected temperature set-\nting, abbr as <model, prompt strategy, temperature>. As shown in\nTable 1, we evaluated six prompt strategies on five LLMs with five\ndifferent temperatures. A configuration‚Äôs top_p is set as its model‚Äôs\ndefault value. And the system role [34] is set as \"Youareasecurity\nauditorwhowrites fuzzdriversforlibraryAPIs.\".\n4 Overall Effectiveness (RQ1)\nTable 1 presents the results of all evaluated configurations. The\nprincipal data displayed in the table are the question solve rates,\nformatted as X/Y, where X denotes the number of questions a lan-\nguage model successfully solves, and Y represents the total number\nof questions presented. A configuration is considered to have solved\na question if at least one effective fuzz driver has been generated.\nFor gpt-4-0613, temperature 2.0 results were incomplete due to the\nservices‚Äô slow response time in extreme temperature settings [1].\nFor the text-bison-001, Google‚Äôs query API limits the requests with a\ntemperature setting above 1.0. Nevertheless, given the poor or even\nzero performance of all other models with a temperatures setting 2.0,\nthe data absence does not substantially affect our evaluation. This\ntable only lists the number of solved questions while [1] posts full\nevaluation details of each model such as success rate per question.\nOverall, the results offer promising evidence of the practi-\ncality of utilizing language model-based fuzz driver genera-\ntion. The optimal configurations, namely <gpt-4-0613, ALL-ITER-K,\n0.5>, achieved impressive success rates, effectively generating fuzz\ndrivers that solved about 91% (78/86) questions. Moreover, three out\nof five LLMs assessed ‚Äì including an open-source option ‚Äì and half\nof the strategies explored can resolve over half of the questions.\nThe substantial variation in success rates across different\nconfigurations underscores the significant influence of the\nthree factors. By analyzing the data, we observe that results can\ngreatly fluctuate when varying a single factor ‚Äì such as changing\nthe temperature in a row, or switching models or prompting strate-\ngies in a column. For example, <gpt-3.5-turbo-0613, NAIVE-1, 0.0>\nfailed to solve any questions, whereas <gpt-3.5-turbo-0613, ALL-\nITER-K, 0.0> managed to correctly address 76% (65/86) of them.\nThis indicates that achieving a high solve rate relies heavily on\navoiding suboptimal combinations of factors. Given that the table\nis sorted to reflect performance trends, the better outcomes tend\nConference‚Äô17, July 2017, Washington, DC, USA Cen Zhang, Yaowen Zheng, Mingqiang Bai, Yeting Li, Wei Ma, Xiaofei Xie, Yuekang Li, Limin Sun, and Yang Liu\nTable 1: Overall Evaluation Result. K represents 40, \"-\" means failed\nto retrieve full query results or not applicable for the given model.\nTemperature\nStrategy, Model 0.0 0.5 1.0 1.5 2.0\ngpt-4-0613 9/86 9/86 9/86 0/86 --\ngpt-3.5-turbo-0613 0/86 1/86 0/86 0/86 0/86\nwizardcoder-15b-v1.0 3/86 1/86 1/86 0/86 0/86\ntext-bison-001 2/86 2/86 1/86 -- --\nNAIVE-1codellama-34b-instruct 0/86 0/86 0/86 0/86 0/86\ngpt-4-0613 12/86 30/86 30/86 5/86 --\ngpt-3.5-turbo-0613 0/86 6/86 8/86 8/86 0/86\nwizardcoder-15b-v1.0 3/86 8/86 11/86 1/86 0/86\ntext-bison-001 2/86 5/86 5/86 -- --\nNAIVE-Kcodellama-34b-instruct 0/86 1/86 3/86 0/86 0/86\ngpt-4-0613 29/86 41/86 41/86 21/86 --\ngpt-3.5-turbo-0613 12/86 29/86 30/86 24/86 1/86\nwizardcoder-15b-v1.0 7/86 23/86 25/86 17/86 0/86\ntext-bison-001 7/86 13/86 15/86 -- --\nBACTX-Kcodellama-34b-instruct 0/86 1/86 11/86 0/86 0/86\ngpt-4-0613 29/86 40/86 41/86 22/86 --\ngpt-3.5-turbo-0613 11/86 22/86 29/86 24/86 1/86\nwizardcoder-15b-v1.0 7/86 24/86 25/86 12/86 0/86\ntext-bison-001 9/86 14/86 14/86 -- --\nDOCTX-Kcodellama-34b-instruct 0/86 9/86 13/86 1/86 0/86\ngpt-4-0613 55/86 63/86 62/86 26/86 --\ngpt-3.5-turbo-0613 30/86 47/86 43/86 31/86 0/86\nwizardcoder-15b-v1.0 39/86 50/86 48/86 13/86 0/86\ntext-bison-001 21/86 27/86 38/86 -- --\nUGCTX-Kcodellama-34b-instruct 0/86 8/86 21/86 0/86 0/86\ngpt-4-0613 56/86 57/86 62/86 23/86 --\ngpt-3.5-turbo-0613 32/86 47/86 43/86 28/86 2/86\nwizardcoder-15b-v1.0 8/86 24/86 37/86 13/86 0/86\ntext-bison-001 9/86 15/86 20/86 -- --\nBA-ITER-Kcodellama-34b-instruct 6/86 28/86 22/86 0/86 0/86\ngpt-4-0613 77/86 78/86 76/86 25/86 --\ngpt-3.5-turbo-0613 65/86 68/86 65/86 37/86 0/86\nwizardcoder-15b-v1.0 41/86 48/86 53/86 11/86 0/86\ntext-bison-001 21/86 37/86 42/86 -- --\nALL-ITER-Kcodellama-34b-instruct 13/86 18/86 26/86 1/86 0/86\nto cluster in ‚Äôgreen areas‚Äô, highlighting configurations where all\ncontributing factors are well-adjusted.\n4.1 Analysis of Effectiveness Factors\nPrompt Strategies. The observed impacts of different prompt-\ning strategies exceeded our initial expectations during their design\nphase. A comparison between NAIVE-1 and ALL-ITER-K show-\ncases a dramatic improvement in optimal question solve rates, soar-\ning from 10% to 90%, emphasizing the critical role of prompt de-\nsign on tool effectiveness. To better understand the performance\ntrends, Table 1 presents the prompting strategies ranked by their\noverall effectiveness. The trends in the results are intuitive: in gen-\neral, strategies that more comprehensively leverage available\ninformation tend to yield superior results . For example, the\nstrategy UGCTX-K markedly outperforms BACTX-K. This can be\nattributed to UGCTX-K‚Äôs inclusion of example code snippets that\nillustrate certain usage of the target API. A notable performance\ndiscrepancy is also seen when comparing BA-ITER-K with BACTX-\nK. Despite starting with the same initial information, BA-ITER-K\nsignificantly surpasses BACTX-K. The reason for this performance\ndifference lies in BA-ITER-K‚Äôs iterative method ‚Äì collecting debug-\nging information to guide the model to fix the previous fuzz driver if\nit is ineffective. Among all the strategies, ALL-ITER-K stands out as\nthe most effective across different combinations of temperature set-\ntings and models. This makes sense considering that ALL-ITER-K\nnot only incorporates all extended API information but also adopts\na recursive problem-solving methodology. Conclusively, its design\nleads to the superior performance in our evaluation. The detailed\nanalysis of these strategies are discussed in Section 7.\nTemperatures. Table 1 clearly demonstrates that configura-\ntions with a temperature setting of 0.5 tend to achieve the\nhighest success rates . In contrast, models under a temperature\nsetting above 1.0 experience a noticeable drop in performance.\nInterestingly, it appears that in general, lower temperatures,\nespecially below the threshold of 1.0, show substantial per-\nformance advantage compared to models operating at higher\ntemperatures. A surprising outcome is that models with 0.0 tem-\nperature perform remarkably well. For instance, both <gpt-4-0613,\nALL-ITER-K, 0.0> and <gpt-3.5-turbo-0613, BA-ITER-K, 0.0> stand\nout as second-best configuration when compared across the various\ntemperature settings. These results are reasonable considering the\nnature of fuzz driver generation task. With a lower temperature\nsetting, models tend to generate more consistent and predictable\noutputs, which benefits the synthesis of high-quality code. High\ntemperatures, while fostering creativity and randomness, may not\nprovide any notable advantages in this context. Specifically, these\nfeatures are either substituted by the randomness contained in\nprompt strategies or deemed irrelevant by the assessment criteria.\nFor example, a prompting strategy like ALL-ITER-K inherently\ncontains a built-in search process that brings the randomness from\nmodel input. And the evaluation strictly assesses the quantity of\neffective drivers without considering the API usage diversity. This\ncriteria fits our evaluation goal, but discounts the creative diversity\nthat could be introduced by higher temperatures.\nOpen-Source LLMs vs Closed-Source LLMs. As commonly\nunderstood in the industry, closed-source LLMs tend to outperform\ntheir open-source counterparts. Among these, gpt-4-0613 is consid-\nered the front-runner in terms of generation capabilities. Following\nclosely behind is gpt-3.5-turbo-0613, which offers a cost-effective\nalternative due to its significantly lower token pricing. However,\nit‚Äôs worth noting that in the open-source domain, wizardcoder-\n15b-v1.0 has made remarkable strides, even surpassing Google‚Äôs\nclosed-source model, text-bison-001. While wizardcoder-15b-v1.0\nis nearly on par with gpt-3.5-turbo-0613, certain performance gaps\ncan still be observed, but it stands as a commendable achievement\nfor an open-source model.\n4.2 How Far Are We to Total Practicality?\nThe above evaluation indicates that with the optimal configuration\n<gpt-4-0613, ALL-ITER-K, 0.5>, the LLM can solve 91% predefined\nquestions. In other words, it can produce at least one effective\nfuzz driver for 78 out of 86 APIs examined. However, this does not\nnecessarily mean that LLMs are ready to be used in production.\nHow Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation Conference‚Äô17, July 2017, Washington, DC, USA\n[1, 10)\n55%\n[10, +‚àû)\n45%\n[1, 2)  10%\n[2, 5)  26% \n[5, 10) 19%\n[10, 20) 14%\n[20, 40) 13%\n[40,+‚àû) 18%\nFigure 4: Question Cost Distribution For All Configurations\non Resolved Questions. ùê∂ùëúùë†ùë°ùëúùëì ùê¥ùëÑùë¢ùëíùë†ùë°ùëñùëúùëõ = # ùëúùëì ùëÑùë¢ùëíùëüùëñùëíùë†\n# ùëúùëì ùëÜùëúùëôùë¢ùë°ùëñùëúùëõùë† .\nUpon further examination of our APIs, we identified three primary\nchallenges and detailed them as follows.\nC1: High Token Cost in Fuzz Driver Generation. Though\nmany configurations have shown a high rate of successful problem\nresolution, our analysis indicates the results come with substantial\ncosts. The data in Figure 4 details the percentage of high cost ques-\ntions for all evaluated configurations. Remarkably, it reveals that\non average, resolving 45% questions entail costs exceeding 10. This\nsuggests that for 50% of the resolved questions, a prompt-based\nstrategy may yield just one effective fuzz driver after repeating\nthe entire query process 10 times or more. When considering only\nthe questions with costs surpassing 20 or even 40, the percentages\nremain notable at 31% and 18%, respectively. These findings under-\nscore a strong incentive for further research into cost reduction\ntechniques. Reducing costs is not only a practical concern with\ndirect financial consequences but also essential for improving the\nefficiency of LLM-based fuzz driver generation.\nC2: Ensuring Semantic Correctness of API Usage. In our\nevaluation, we found that there is a discrepancy for approximately\n34% (29/86) of the APIs ‚Äì assuming LLMs can successfully create\nat least one effective fuzz driver for each in the evaluation setting,\nthis success cannot be translated into the full automation of fuzz\ndriver generation for them. The issue at hand lies in the potential\nmisuse of APIs within the generated drivers, which requires val-\nidation to ensure semantic correctness. For example, LLMs may\nincorrectly initializing the argument of an API, such as passing\na mutated filename to the API instead of passing a created file\nfirst and then mutating its content for fuzzing, or missing some\ncondition checks before calling API. In our evaluation process, we\nmanually implemented semantic checkers to identify such API mis-\nuses for accurate assessment (details on our semantic checkers are\nprovided in Section 3.2). However, fully automating the validation\nof semantic correctness remains a significant hurdle. Consequently,\neven though it is feasible to generate effective fuzz drivers with\nthe help of these LLMs, distinguishing them from the ineffective\nones can be problematic due to the absence of automated methods\nfor validating semantic correctness. This challenge underscores\nthe need for developing robust techniques to automatically ensure\nthe semantic accuracy of generated fuzz drivers before they can be\nreliably deployed in production.\nC3: Satisfying Complex API Usage Dependencies. Over-\nall, there are five questions cannot be resolved by any assessed\nconfigurations. These questions are challengeable since their driver\ngeneration requires the deep understanding of specific contexts. For\ninstance, generating the driver for tmux [11] requires the construc-\ntion of various concepts, such as session, window, pane, etc, and\ntheir relationships. Similarly, for network-related questions [5, 9],\n0204060\n[1, 3][4, 6][7, 9][10, 12]‚â• 13\ngpt-3.5-turbo-0613gpt-4-0613codellama-34b-instructwizardcoder-15b-v1.0text-bison-001\nFigure 5: Average Query Success Rate Per Question Complex-\nity Score Bucket. Data: All BACTX-K (K = 40) configurations.\na standby network server or client is required to be created before\ncalling the target API. The effective drivers can only be generated\nby respecting these specific contextual requirements.\nWhile LLM-based generation has shown promising potential,\nit still faces certain challenges towards high practicality.\n5 Fundamental Challenge (RQ2)\n5.1 Links Between Question and Performance\nTo investigate the core difficulties in generating fuzz drivers with\nLLMs, we scrutinized the outcomes of the BACTX-K strategy. This\nstrategy is a proper starting point for understanding the fundamen-\ntal challenges since it merely uses generally accessible information\nand has simple query workflow. In Figure 5, there is a clear in-\nverse proportion relationship between the query success rate\nand the complexity of a question, irrespective of the used\nmodels and temperatures . The complexity of a question is mea-\nsured by first constructing the minimal fuzz driver of each question\nand then quantifying the API specific usage contained in the min-\nimized code. A minimal effective driver for a question is created\nbased on the OSS-Fuzz driver by removing the unnecessary part\nof the code and replacing the argument initialization into a simpler\nsolution according to the cases enumerated in Section 2. Then the\ncomplexity is quantified as the sum of the count of the following\nelements inside code: ‚ù∂ unique project APIs; ‚ù∑ unique common\nAPI usage patterns; ‚ù∏ unique identifiers including non-zero literals\nand project global variables excluding the common API usage code;\n‚ùπ branches and loops excluding the common API usage code. Note\nthat all branches of one condition will be counted as one. Overall,‚ù∂,\n‚ù∏ measure API specific vocabularies while ‚ùπ for API specific con-\ntrol flow dependencies‚Äô. We put detailed calculation examples at [1].\nConsidering the generation process, it is intuitive that LLMs‚Äô\nperformance degrades when the complexity of target API spe-\ncific usage increases . To generate effective drivers, LLMs should\nat least generate code satisfying minimal requirements. In other\nwords, they must accurately predict the API argument usage and\ncontrol flow dependencies. However, this is challenging since LLMs\ncannot validate their predictions against documentation or imple-\nmentations as humans do. It is reasonable to assume that LLMs\nhave learned the language basics and common programming prac-\ntices due to their training on vast amounts of code. But the API\nspecific usage, such as the semantic constraints on the argument,\ncannot be assumed. On one hand, there may only have limited data\nabout this in training. On the other hand, details can be lost during\npreprocessing or the learning stage while the accurate generation is\nrequired. Therefore, the more API usage a LLM needs to predict, the\nConference‚Äô17, July 2017, Washington, DC, USA Cen Zhang, Yaowen Zheng, Mingqiang Bai, Yeting Li, Wei Ma, Xiaofei Xie, Yuekang Li, Limin Sun, and Yang Liu\nGrammatical Error Semantic Error\nG1 - Corrupted Code\nG2 - Language Basics\nViolation\nG3 - Non-Existing Identifier\nG4 - Type Error,\n19% (10,108)\nS1 - Incorrect Input\nArrangement, < 1% (291)\nS2 - Misinitialized Func Args\nS3 - Inexact Ctrl-Flow Deps\nS4 - Improper Resource Cleaning, 1% (464)S5 - Failure on Common Practices,\n<1% (93)\n79%\n(41,729)\n21%\n(11,095)\n30%\n(16,002)\n11%\n(5,582) 19%\n(10,037)\n19%\n8%(4,409)\n11%(5,838)\nFigure 6: Failure Taxonomy. Data: BACTX-K (K = 40) configurations.\ngreater the likelihood of errors, particularly for less common usages\nthat do not follow the mainstream design patterns or have special\nsemantic constraints. Such situations are common in C projects,\nwhose APIs often contain low-level project-specific details.\nThe performance of LLM-based generation declines signifi-\ncantly when the complexity of API specific usage increases.\n5.2 Failure Analysis\nTo understand how the generation fails on API specifics, we con-\nducted failure analysis on BACTX-K. The direct failure reason of the\ndriver is collected to reveal the generation blockers. In total, 52,824\nineffective drivers were analyzed. The runtime errors of 11,095\ndrivers are semi-automatically analyzed while the compilation and\nlink errors are categorized based on the compiler outputs.\nFailure Taxonomies. Figure 6 details the root cause taxonomy.\nThere are nine root causes fallen into two categories: the grammat-\nical errors reported by compilers in build stage, and the semantic\nerrors which are abnormal runtime behaviors identified from the\nshort-term fuzzing results. ‚ù∂ G1 - Corrupted Code , the drivers do\nnot contain a complete function of code due to either the token\nlimitation or mismatched brackets; ‚ù∑ G2 - Language Basics Viola-\ntion, the code violates the language basics like variable redefinition,\nparentheses mismatch, incomplete expressions, etc; ‚ù∏ G3 - Non-\nExisting Identifier , the code refers to non-existing things such as\nheader files, macros, global variables, members of a struct, etc;\n‚ùπ G4 - Type Error . One main subcategory here is the code passes\nmismatched number of arguments to a function. The rest are ei-\nther unsupported type conversions or operations such as calling\nnon-callable object, assigning voidto a variable, allocating an in-\ncomplete struct, etc; ‚ù∫ S1 - Incorrect Input Arrangement , the input\nsize check either is missed when required or contains an incorrect\ncondition; ‚ùª S2 - Misinitialized Function Args , the value or inner\nstatus of initialized argument does not fit the requirements of callee\nfunction. Typical cases are closing a file handle before passing it,\nusing wrong enumeration value as option parameter, missing re-\nquired APIs for proper initialization, etc; ‚ùº S3 - Inexact Ctrl-Flow\nDeps, the control-flow dependencies of a function does not properly\nimplemented. Typical cases are missing condition checks such as\nensuring a pointer is not NULL, missing APIs for setting up execu-\ntion context, missing APIs for ignoring project internal abort, using\nincorrect conditions, etc. ‚ùΩ S4 - Improper Resource Cleaning , the\n0204060\n121416181101\n(a) Number of Questions Solved by\nRepeat for All Configurations.\n(6, 0.2182)\n00.20.40.60.8\n2122232\n(b) Avg Pct of Solved Questions Per\nRepeat Round in Top-20 Configs.\nFigure 7: Statistics on the Effectiveness of Repeatedly Query.\ncleaning API such as xxxfreeis either missing when required or is\nused without proper condition checks; ‚ùæ S5 - Failure on Common\nPractices, the code failed on standard libraries function usage like\nmessing up memory boundary in memcpy, passing read-only buffer\nto mkstemp, etc. Examples of these categories are shown in [1].\nOverall, the failures cover API usages in various dimensions:\nfrom grammatical level detail to semantic level direction, and from\ntarget API control flow conditions to dependent APIs‚Äô declarations.\nImproving this is challengeable since:‚ù∂ the involved usage is too\nbroad to be fully put into one prompt , which may either exceed\nthe token limitation or distract the model; ‚ù∑ the useful usage for\ngenerating one driver cannot be fully predetermined . On one\nhand, models are inherently blackbox and probabilistics, whose\nmistakes cannot be fully predicted. On the other hand, there are\nusually multiple implementation choices for a given API.\nMost failures are of mistakes in API usage specifics. The broad-\nness of the involved usage is the major challenge.\n6 Characteristics of Key Design (RQ3)\n6.1 Repeatedly Query\nRepeated querying is a critical aspect of prompt strategies, greatly\nenhancing the success rate in generating fuzz drivers regardless of\nemployed models, temperatures, and prompt designs. Specifically,\nfor the optimal configuration <gpt-4-0613, 0.5, ALL-ITER-K>, ap-\nproximately 47.44% of the issues were resolved by reinitiating the\nquery process (37 out of 78 total resolved issues were solved upon\nrepetition). For the top-20 configurations, this contribution remains\nsignificantly high at an average of 67.50%.\nFigure 7a displays the count of questions resolved through re-\npeated querying across all evaluated configurations, ranked by their\noverall effectiveness as detailed in Table 1. This demonstrates a di-\nrect correlation between the benefit of repeated queries and the\nefficacy of the configuration‚Äîthe more effective a configuration,\nthe greater the gains from repeating the queries .\nAdditionally, Figure 7b presents the average percentage of ques-\ntions resolved in each subsequent round of querying for the top-20\nconfigurations. Here, the percentage for round X is determined\nby ùëÖùë†ùëôùë°(ùëã)‚àíùëÖùë†ùëôùë°(ùëã‚àí1)\nùëÖùë†ùëôùë°(1) , with ùëÖùë†ùëôùë°(ùëã)indicating the number of ques-\ntions resolved by round X. The X-axis starting from round two,\nhighlighting that the first round corresponds to the initial query.\nThis data shows that the gain of repeated queries drops signif-\nicantly after the initial few rounds . From our evaluation, we\nrecommend limiting repeated queries to no more than six, where the\nHow Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation Conference‚Äô17, July 2017, Washington, DC, USA\n0.9275\n0.1933\n0.1433\n0.0491\n0 0.51\nQstnSucc Rate\nQuerySucc Rate\nExternalInternal\n0.6733\n0.3008\n0.3591\n0.0875\n0 0.51\nQstnSucc Rate\nQuerySucc Rate\nOthersTest & Example\nFigure 8: Average Query/Question Success Rate of Different\nExample Sources for All Configurations.\nsixth round still manages to resolve an additional 20% of questions\ncompared to the results of the first round.\n6.2 Query With Extended Information\nQuerying With API Documentation. By comparing DOCTX-\nK and BACTX-K, we found that there is no significant changes\nbetween their results in the metrics of resolved questions .\nOn one hand, a significant percentage (43%) of APIs in the evalu-\nated questions do not have API documentation (49 out of 86 have).\nWhen there is no documentation for an API, the DOCTX-K queries\nare identical to BACTX-K‚Äôs. On the other hand, adding API docu-\nmentation in the queries may not provide enough details directly\nstating the API usage. This is because these API documentations\nusually contain a high-level description of the usage, typically a\nsummary of main functionality with one-sentence explanations\nfor arguments. However, the blocker-solving usage information\ndiscussed in Section 5.2, such as low level argument initialization\nspecifics, control flow dependencies, or the usages of its dependent\nAPIs, is usually not included.\nAPI documentation has minor performance benefits due to\nthe limited usage description it contained.\nQuerying With Example Code Snippets. When comparing\nthe results of BACTX-K and UGCTX-K presented in Table 1, we can\nclearly observe that incorporating example code snippets substan-\ntially enhances performance in most configurations. In particular,\nthe addition of example snippets results in an average resolution of\n104% more questions across the 22 evaluated configurations, which\nincludes five models and five different temperature settings.\nNonetheless, further analysis reveals that the inclusion of us-\nage examples incurs a much higher token cost, with an av-\nerage increase of tenfold . The ratio of token costs for these two\napproaches varies from 4.20 to 39.71 across all configurations, with\nan average ratio of 14.65. Notably, the UGCTX-K approach demands\nan average of 32,367 tokens to generate a single correct solution.\nFigure 8 depicts our investigation into the impact of different\nsources of example snippets on the quality of solutions. This figure\nassesses the success rates of queries/questions associated with vari-\nous example sources, which are categorized in two distinct manners\nbased on their file paths: first, as ‚ù∂ External vs. Internal, with Inter-\nnal comprising the target project and its variations, and External\nconsisting of all other sources; second, as ‚ù∑ Test & Example vs.\nOthers, where the first group includes files with paths that contain\n\"test\" or \"example\" in any capitalization. The underlying data for\nthese plots stems from questions that were solved by UGCTX-K\nbut not by BACTX-K across all tested configurations. According\nto this analysis, it is clear that both Internal and Test & Exam-\nple sources are associated with significantly higher quality\nexample snippets in comparison to their counterparts .\nCase Studies. # 9 wc_Str_conv_with_detect This case is\nchallenging due to the unintuitiveness of its API usage. The API\ndeclaration is \"Str wc_Str_conv_with_detect(Str is,wc_ces *\nf_ces,wc_ces hint,wc_ces t_ces)\". It is used for converting the\ninput stream is from one CES (character encoding scheme, f_ces)\nto another (t_ces). Most basic strategy drivers made mistakes on\nthe creations of either is(the confusing type Str) or CESs, where\nis has to be created using particular APIs like Strnew_charp_n\nand CESs should be specific macros or carefully initialized struct.\nExample helps here by directly providing the usage to models.\n# 37 igraph_read_graph_graphdb The hardest part in this\ncase is the implicit control flow dependency it required. Besides\ncorrectly initializing the arguments, it has to call an API to mute the\nbuiltin error handlers. By default, the API will abort immediately\nwhen any abnormal input is detected, which causes frequent false\ncrashes blocking the fuzzing progress. To mute it, the driver needs to\ncustmoize the error handler,e.g., call igraph_set_error_handler-\n(igraph_error_handler_ignore). This requirement is hard to be\ninferred beforehand due to its semantic nature and few inference\nclues. However, some unit tests in project such asforeign_empty.c\ncontain this usage, which directly instructs the generation.\nExample code snippets can greatly enhance model per-\nformance by providing direct insights on API usage.\n\"test/example files\", \"code files from the target/variant\nprojects\" are high quality sources.\n6.3 Iterative Query\nThe iterative query strategy is another key design that can lead to\nsignificant improvements in performance. Referring to Table 1, we\nfind that, on average, incorporating an iterative query strategy into\nBACTX-K ‚Äì that is, adopting the BA-ITER-K approach ‚Äì helps solve\n159% more questions. Similarly, ALL-ITER-K resolves 23% more\nquestions than UGCTX-K. However, this strategy does come at a\ncost. The inclusion of iterative design tends to lead to higher\ntoken usage when generating correct solutions . On average,\nthe iterative strategy increases token costs by 57% for BACTX-K\nper successful driver generation and by 17% for UGCTX-K.\nThe effectiveness of the iterative strategy can be attributed to\ntwo key factors. Firstly, it leverages a wider array of informa-\ntion, including error data generated from validating previously\ngenerated drivers. Secondly, it tackles the problem incremen-\ntally, employing a step-by-step, divide-and-conquer approach that\nsimplifies the complexity of the generation task. This methodology\nis exemplified in the case studies that follow, illustrating how the\niterative strategy typically operates through practical examples.\nCase Studies. #5 md_html This API requires the preparation\nof a customized callback function pointer as the argument, where\nall previous strategies failed to figure out. The callback function\nis used to handle the output data of API. All the drivers generated\nby UGCTX either pass a NULL pointer or a non-existing function\nConference‚Äô17, July 2017, Washington, DC, USA Cen Zhang, Yaowen Zheng, Mingqiang Bai, Yeting Li, Wei Ma, Xiaofei Xie, Yuekang Li, Limin Sun, and Yang Liu\n00.511.52\nAFL++ cov ratio, OSS-Fuzz/gpt-4AFL++ cov ratio, OSS-Fuzz/wizardcoder\n00.511.52\nlibfuzzer cov ratio, OSS-Fuzz/gpt-4libfuzzer cov ratio, OSS-Fuzz/wizardcoder\n(a) Comparison in Average Coverage Ratio. Ratio = OSS-Fuzz/LLM, ratio < 1 ‚ÜíOSS-Fuzz‚Äôs driver has lower coverage.\n-20-10010\nAFL++ crash diff, OSS-Fuzz - gpt-4AFL++ crash diff, OSS-Fuzz - wizardcoder\n-6-4-202\nlibfuzzer crash diff, OSS-Fuzz - gpt-4libfuzzer crash diff, OSS-Fuzz - wizardcoder\n(b) Comparison in the Difference of Average Unique Crashes. Diff = OSS-Fuzz - LLM, diff < 0 ‚ÜíOSS-Fuzz‚Äôs driver find less unique crashes.\nFigure 9: Metric Comparison of LLM-Generated and OSS-Fuzz Drivers. Y-axis is the metric. For clarity, question id (x-axis) is omitted.\nname. Iterative query guides the fix by providing the link error\nhighlighting that this referred function is undefined.\n#73 pj_stun_msg_decode This is another typical case why\niterative strategy works. The initialization of its first argument\nhas multi-level API dependencies. The dependency chain is: ‚ù∂ the\nAPI -> ‚ù∑pj_pool_create-> ‚ù∏pj_caching_pool_init, where ->\nmeans depends. All non-iterative strategies failed to prepare a driver\nwith all correct usage detail of these indirect dependencies while\niterative strategies solve this by providing error related feedback to\nLLMs and solving multiple errors one by one. In one of the solved\niterative query, it first corrects the incorrect used API of ‚ù∏, then\nfigures out the mismatched type error when calling ‚ù∑. Lastly, for\nthe driver‚Äôs runtime crash, LLMs use two rounds to fix according\nto the assertion code located from crash stacks.\nIterative query helps in utilizing more diverse information\nand solving the problem in a step-by-step manner. However,\nit has higher token cost and increased complexity.\n7 OSS-Fuzz Driver Comparison (RQ4)\nComparison Overview. We compared LLM-generated drivers\nwith OSS-Fuzz‚Äôs to obtain more practical insights. Note that OSS-\nFuzz drivers are practically used in industry for continuous fuzzing\nand most of them are manually written and improved for years.\nParticularly, LLM-generated drivers under comparison are from\ngpt-4-0613 and wizardcoder-15b-v1.0 using iterative strategies with\ntemperature 0.5. These two configurations are the best representa-\ntive for closed-source and open-source LLMs. In total, we evaluated\n53 questions which are both resolved by all configurations. Multiple\ndrivers of one question are merged as one to ease the comparison.\nThis is done by adding a wrapper snippet which links the seed\nscheduling with the selection of the executed logic from merged\ndrivers. Specifically, a switch structure is added to determine which\ndriver it will execute based on a part of the input data. During each\nfuzzing iteration, only the logic of one merged driver is executed.\nBesides, some compound OSS-Fuzz drivers are designed to fuzz\nmultiple APIs. For clear comparison, we merged all drivers of ques-\ntions involved in one compound driver as one. In total, we prepared\n38 drivers for each assessed LLM or OSS-Fuzz. The comparisons\ncover both code and fuzzing metrics such as the number of used\nAPIs, oracles, coverage, and crashes.\nFuzzing Setup. Considering the randomness of fuzzing, we\nfollowed the suggestions from [27]: the fuzzing experiments are\nconducted with five times of repeat for collecting average coverage\ninformation and the fuzzing of each driver lasts for 24 hours. We\nused libfuzzer[28] and AFL++[18] as fuzzers with empty initial\nseed and dictionary. \" -close_fd_mask=3 -rss_limit_mb=2048\n-timeout=30\" is used for libfuzzer while AFL++‚Äôs is the default\nsetup of aflpp_driver. For fair comparison, the coverage of fuzz\ndriver itself is excluded in post-fuzzing data collection stage (the\nmerged driver can have thousands of lines of code) but kept in\nfuzzing stage for obtaining coverage feedback. In total, the experi-\nments took 3.75 CPU year.\nCode Metric: API Usage. The API usage is measured by the\nnumber of unique project APIs used in the fuzz driver. Overall,\n14% (17/35) gpt-4-0613 drivers have used less project APIs than\nOSS-Fuzz‚Äôs while 39% forwizardcoder-15b-v1.0. By manually in-\nvestigating these drivers, we found that LLMs conservatively use\nAPIs in driver generation if no explicit guidance in prompts .\nFor instance, some drivers only contain necessary usages such as\nargument initialization. And the API usage is hardly extended such\nas adding APIs to use an object after parsing it. This is a reasonable\nstrategy since aggressively extending APIs increases the risk of gen-\nerating invalid drivers. Adding example snippets in the prompt can\nalleviate this situation. As for OSS-Fuzz drivers, the API usage diver-\nsity is case by case since they are from different contributors. Some\ndrivers, e.g., [6] are minimally composed and some are extensively\nexploring more features of the target,e.g., [10]. One interesting find-\ning is that some OSS-Fuzz drivers are modified from the test files\nrather than written from scratch, which is a quite similar process\nas querying LLM with examples. For example, kamailiodriver [7]\nis modified from test file [8]. Prompting with this example, LLM\ncan generate similar driver code.\nCode Metric: Oracle. We did statistics on the oracles of the dri-\nvers. The result is quite clear: in all 78 questions resolved by LLMs,\nOSS-Fuzz drivers of 15 questions contain at least one oracle which\ncan detect semantic bugs, while there are no LLM-generated dri-\nvers have oracles. The used semantic oracles can be categorized as\nfollowing: ‚ù∂ check whether the return value or output content of\nan API is expected, e.g., [12]; ‚ù∑ check whether the project internal\nstatus has expected value,e.g., [14]; ‚ù∏compare whether the outputs\nof multiple APIs conform to specific relationships, e.g., [13].\nLLMs tend to generate fuzz drivers with minimal API usages,\nsignificant space are left for further improvement such as\nextending the use of API outputs or adding semantic oracles.\nFuzzing Metric: Coverage and Crash. Figure 9a, 9b plot the\ncoverage and crash comparison results. Instead of presenting every\nHow Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation Conference‚Äô17, July 2017, Washington, DC, USA\ndetail of the experiments for hundreds of drivers, the plots lists the\ncomparison in certain metrics while the full experiment details can\nbe found at [1]. Overall, in most questions, the LLM-generated\ndrivers demonstrate similar or better performance in metrics\nof both coverage and the number of uniquely found crashes .\nNote that there are no false positive since the generated fuzz drivers\nare already filtered by the semantic checkers provided from our\nevaluation framework. If only the fully automatic validation process\nare adopted, i.e., removing the last two checkers in Figure 3, the\nfuzzing outcome will be messed with huge number of false positives,\nincurring significant manual analysis efforts.\nLLM-generated drivers can produce comparable fuzzing out-\ncomes as OSS-Fuzz drivers. In large scale application, how to\npractically pick effective fuzz drivers is the major challenge.\n8 Discussion\nRelationships With OSS-Fuzz-Gen. The Google OSS-Fuzz\nteam has undertaken a parallel work called OSS-Fuzz-Gen [49] for\nLLM-based fuzz driver generation. Their public information con-\ntains one security blog [56] and the source code repository [49]. Our\nwork is complementary to theirs. Overall, at the time of submission,\nthey put high efforts on filling the engineering gap between LLM\ninterfaces and OSS-Fuzz projects. Their experiments are conducted\non top commercial LLMs, aiming to showcase that LLM-generated\nfuzz drivers can help in finding zero-day vulnerabilities and reach-\ning new testing coverage. However, there few discussion on the\nfundamental questions such as the design choices behind their\nprompt strategy, the pros and cons for different strategies, how\nthe effectiveness varies for different models and parameters, and\nwhat are the inherent challenges and potential future directions.\nOur study, on the other hand, complements theirs by exploring\nthese fundamental issues. We carefully designed prompt strategies,\nevaluated them on various models (open and commerical LLMs)\nand temperatures, and distilled findings from the results.\nContributing to OSS-Fuzz-Gen. We carefully examined the\nprompt strategies of OSS-Fuzz-Gen from their implementation and\nvalidated where our insights can help. Interestingly, their current\nstrategy support part of our insights. For instance, they adopted\n10 time repeat results [47] and used a lower temperature (0.4) in\nexperiments [48]. Besides, we found that OSS-Fuzz-Gen only identi-\nfies and fixes build errors while ignoring the runtime errors caused\nby driver. Their generation ends when a compilable fuzz driver is\nsynthesized and then they manually checks the validity of these\ndrivers. To improve this, we implemented our strategies for drivers\nwith fuzzing runtime errors in their platform, including the identifi-\ncation (automatic part of validation process) [50‚Äì52, 54], categoriza-\ntion, and the corresponding iterative fix procedure [53, 55]. These\nenhancements added new functionalities refining the generation\nresults, where the cases showing its effectiveness are quickly iden-\ntified [45, 46] during their benchmark tests (29 APIs, 18 projects).\nCurrently, the improvement is merged into the main branch and is\nactively used to fuzz all 282 supported projects, marking a signifi-\ncant milestone to us. We are keeping refine our commitments, such\nas integrating more fine-grained error information during fix.\nPotential Improvements. From our perspective, to improve\nthe performance of LLM-based fuzz driver generation, efforts from\nthree dimensions can be further explored. First, the domain knowl-\nedge contained inside the target scope can be modeled and utilized\nfor better generation. For instance, to test network protocol APIs,\nthe communication state machine of that protocol can be learned\nfirst and then used to guide the driver generation. Besides, more so-\nphisticated prompt-based solutions can be explored, such as hybrid\napproaches combining traditional program analysis and prompt\nstrategies, or agent-based approaches. Lastly, fine-tuning based\nmethods is also a promising direction since this can enhance both\nthe generation‚Äôs effectiveness and efficiency from a model level.\nThreat to Validity. One internal threat comes from the effec-\ntiveness validation of the generated drivers. To address this, we\ncarefully examined the APIs and manually wrote tests for them\nto check whether the semantic constraints of a specific API have\nbeen satisfied or not. Another threat to validity comes from the\nfact that some OSS-Fuzz drivers, e.g., code written before Sep 2021,\nmay already be contained in the model training data, which raises a\nquestion that whether the driver is directly memorized by the model\nfrom the training data. Though it is infeasible to thoroughly prove\nits generation ability, which requires the retrain of LLMs, we found\nseveral evidences that supports the answers provided by these mod-\nels are not memorized: Many generated drivers contain APIs that\ndo not appear in the OSS-Fuzz drivers, especially for those drivers\nhinted by example usage snippets or iteratively fixed by usage and\nerror information. Besides, the generated drivers share a distinct\ncoding style as OSS-Fuzz drivers. For example, the generated code\nare commented with explanation on why the API is used and what\nit is used for, etc. The main external threat to validity comes from\nour evaluation datasets. Our study focused on C projects while the\ninsights may not be necessarily generalizable to other languages.\n9 Related Work\nFuzz Driver Generation. Several works [2, 3, 22, 24, 26, 62‚Äì64]\nhave focused on developing automatic approaches to generate fuzz\ndrivers. Most of these works follow a common methodology, which\ninvolves generating fuzz drivers based on the API usage existed\nin consumer programs, i.e., programs containing code that uses\nthese APIs. For instance, by abstracting the API usage as specific\nmodels such as trees [63], graphs [22], and automatons [62], several\nworks propose program analysis-based methods to learn the usage\nmodels from consumer programs and conduct model-based driver\nsynthesis. In addition, a recent work [24] emphasizes that unit tests\nare high quality consumer programs and proposes techniques to\nconvert existing unit tests to fuzz drivers. Though these approaches\ncan produce effective fuzz drivers, their heavy requirements on the\nquality of the consumer programs, i.e., the consumers must contain\ncomplete API usage and are statically/dynamically analyzable, limit\ntheir generality. Furthermore, synthesized code often lacks human\nreadability and maintainability, limiting their practical application.\nSome parallel works [ 32, 49] have also explored the LLM-based\nfuzz driver generation. However, their main goal is to build tools\ndemonstrating the potential of LLM-based generation. Our study\ncomplements them by focusing on delivering the first comprehen-\nsive understanding of the fundamental issues in this direction.\nLLM for Generative Tasks. Recent works have explored the\npotential of LLM models for various generative tasks, such as code\nConference‚Äô17, July 2017, Washington, DC, USA Cen Zhang, Yaowen Zheng, Mingqiang Bai, Yeting Li, Wei Ma, Xiaofei Xie, Yuekang Li, Limin Sun, and Yang Liu\ncompletion [57], test case generation [ 15, 16, 30, 41, 43, 59, 61]\nand code repairing [ 17, 39, 60]. These works utilize the natural\nlanguage processing capabilities of LLM models and employ spe-\ncific prompt designs to achieve their respective tasks. To further\nimprove the models‚Äô performance, some works incorporate itera-\ntive/conversational strategies or use fine-tuning/in-context learning\ntechniques. In test case generation, previous research works have\nprimarily targeted on testing deep learning libraries [15, 16] and\nunit test generation [41, 43]. Considering the intrinsic differences\nbetween fuzz drivers and other tests and the difference on studied\nprogramming languages, these works cannot answer the funda-\nmental effectiveness issues of LLM-based fuzz driver generation,\nindicating the unique values of our study.\n10 Conclusion\nOur study centers around answering fundamental issues of LLM-\nbased fuzz driver generation‚Äôs effectiveness. To do that, we designed\na dataset and six prompt strategies, and did extensive evaluation on\ndifferent models and temperatures. Our study not only established\nthe basic understanding on this direction but also indicates the po-\ntential future improvements. Furthermore, our insights have been\napplied into industrial practical fuzz driver generation platform.\n11 Data Availability\nThe source code and data involved in our study can be found at [1].\nReferences\n[1] Anonymous. 2023. Website for Our Study. https://sites.google.com/view/llm4fdg/\nhome.\n[2] Domagoj Babiƒá, Stefan Bucur, Yaohui Chen, Franjo Ivanƒçiƒá, Tim King, Markus\nKusano, Caroline Lemieux, L√°szl√≥ Szekeres, and Wei Wang. 2019. Fudge: fuzz\ndriver generation at scale. In Proceedings of the 2019 27th ACM Joint Meeting on\nEuropean Software Engineering Conference and Symposium on the Foundations of\nSoftware Engineering . 975‚Äì985.\n[3] Peng Chen, Yuxuan Xie, Yunlong Lyu, Yuxiao Wang, and Hao Chen. 2023. Hopper:\nInterpretative Fuzzing for Libraries. In Proceedings of the 2023 ACM SIGSAC\nConference on Computer and Communications Security . 1600‚Äì1614.\n[4] Codellama. 2023. codellama-34b-instruct Model. https://huggingface.co/\ncodellama/CodeLlama-34b-hf.\n[5] Open Source Contributors. 2023. Fuzz Driver of Civetweb Project. https://github.\ncom/civetweb/civetweb/blob/master/fuzztest/fuzzmain.c.\n[6] Open Source Contributors. 2023. Fuzz Driver of Coraring Project. https://github.\ncom/RoaringBitmap/CRoaring/blob/master/fuzz/croaring_fuzzer.c.\n[7] Open Source Contributors. 2023. Fuzz Driver of Kamailio Project. https://github.\ncom/kamailio/kamailio/blob/f9cbe7ad01331e97852872c29b612409bf571c8d/\nmisc/fuzz/fuzz_parse_msg.c#L22.\n[8] Open Source Contributors. 2023. Fuzz Driver of Kamailio Project. https://github.\ncom/kamailio/kamailio/blob/f9cbe7ad01331e97852872c29b612409bf571c8d/src/\nmodules/misctest/misctest_mod.c#L273.\n[9] Open Source Contributors. 2023. Fuzz Driver of Libmodbus Project. https://\ngithub.com/google/oss-fuzz/blob/master/projects/libmodbus/fuzz/FuzzClient.c.\n[10] Open Source Contributors. 2023. Fuzz Driver of Lua Project. https://github.com/\ngoogle/oss-fuzz/blob/master/projects/lua/fuzz_lua.c.\n[11] Open Source Contributors. 2023. Fuzz Driver of Tmux Project. https://github.\ncom/tmux/tmux/blob/master/fuzz/input-fuzzer.c.\n[12] Open Source Contributors. 2023. The PermaLink for Oracle Checking Expected\nOutput of One API Used in Fuzz Driver of Bind9 Project. https://gitlab.isc.org/isc-\nprojects/bind9/-/blob/af5d0a0afbbc136074133e80a63565d668b8a40d/fuzz/dns_\nrdata_fromwire_text.c#L176.\n[13] Open Source Contributors. 2023. The PermaLink for Oracle Check-\ning Expected Relationship of Two APIs‚Äô Outputs Used in Fuzz Dri-\nver of Bind9 Project. https://gitlab.isc.org/isc-projects/bind9/-/blob/\naf5d0a0afbbc136074133e80a63565d668b8a40d/fuzz/dns_name_fromwire.c#L82.\n[14] Open Source Contributors. 2023. The PermaLink for Oracle Used in\nFuzz Driver of iGraph Project. https://github.com/igraph/igraph/blob/\n1b25b1102916bb99274af4bc7c6322f2fe193204/fuzzing/read_dl_fuzzer.cpp#L62.\n[15] Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming\nZhang. 2023. Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-\nLearning Libraries via Large Language Models. arXiv:2212.14834 [cs.SE]\n[16] Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing\nYang, and Lingming Zhang. 2023. Large Language Models are Edge-Case Fuzzers:\nTesting Deep Learning Libraries via FuzzGPT. arXiv preprint arXiv:2304.02014\n(2023).\n[17] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, and Shin Hwei Tan. 2022. Au-\ntomated Repair of Programs from Large Language Models. arXiv preprint\narXiv:2205.10583 (2022).\n[18] Andrea Fioraldi, Dominik Maier, Heiko Ei√üfeldt, and Marc Heuse. 2020.{AFL++}:\nCombining incremental steps of fuzzing research. In 14th USENIX Workshop on\nOffensive Technologies (WOOT 20) .\n[19] Google. 2017. OSS-Fuzz Github Repository. https://github.com/google/oss-fuzz.\n[20] Google. 2023. Google PaLM2 Text Models. https://cloud.google.com/vertex-\nai/docs/generative-ai/model-reference/text.\n[21] John Hancock. 2004. Jaccard Distance (Jaccard Index, Jaccard Similarity Coeffi-\ncient). https://doi.org/10.1002/9780471650126.dob0956\n[22] Kyriakos Ispoglou, Daniel Austin, Vishwath Mohan, and Mathias Payer. 2020. Fuz-\nzgen: Automatic fuzzer generation. In 29th USENIX Security Symposium (USENIX\nSecurity 20) .\n[23] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh\nParthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large lan-\nguage models meet program synthesis. In Proceedings of the 44th International\nConference on Software Engineering . 1219‚Äì1231.\n[24] B. Jeong, J. Jang, H. Yi, J. Moon, J. Kim, I. Jeon, T. Kim, W. Shim, and Y. Hwang.\n2023. UTOPIA: Automatic Generation of Fuzz Driver using Unit Tests. In 2023\n2023 IEEE Symposium on Security and Privacy (SP) . IEEE Computer Society, Los\nAlamitos, CA, USA, 746‚Äì762. https://doi.org/10.1109/SP46215.2023.00043\n[25] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. 2023. Self-\nplanning Code Generation with Large Language Model. arXiv:2303.06689 [cs.SE]\n[26] Jinho Jung, Stephen Tong, Hong Hu, Jungwon Lim, Yonghwi Jin, and Taesoo\nKim. 2021. WINNIE: fuzzing Windows applications with harness synthesis and\nfast cloning. In Proceedings of the 2021 Network and Distributed System Security\nSymposium (NDSS 2021) .\n[27] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018.\nEvaluating Fuzz Testing. In Proceedings of the 2018 ACM SIGSAC Conference on\nComputer and Communications Security (Toronto, Canada) (CCS ‚Äô18). Association\nfor Computing Machinery, New York, NY, USA, 2123‚Äì2138. https://doi.org/10.\n1145/3243734.3243804\n[28] libfuzzer@googlegroups.com. 2019. LibFuzzer. https://llvm.org/docs/LibFuzzer.\nhtml.\n[29] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your\ncode generated by chatgpt really correct? rigorous evaluation of large language\nmodels for code generation. arXiv preprint arXiv:2305.01210 (2023).\n[30] Zhe Liu, Chunyang Chen, Junjie Wang, Xing Che, Yuekai Huang, Jun Hu, and\nQing Wang. 2022. Fill in the Blank: Context-aware Automated Text Input Gener-\nation for Mobile GUI Testing. arXiv preprint arXiv:2212.04732 (2022).\n[31] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu,\nChongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder:\nEmpowering Code Large Language Models with Evol-Instruct. arXiv preprint\narXiv:2306.08568 (2023).\n[32] Yunlong Lyu, Yuxuan Xie, Peng Chen, and Hao Chen. 2023. Prompt Fuzzing for\nFuzz Driver Generation. arXiv:2312.17677 [cs.CR]\n[33] OpenAI. 2022. ChatGpt Website. https://chat.openai.com.\n[34] OPENAI. 2023. Example Usage of System Role. https://platform.openai.com/\ndocs/guides/text-generation/chat-completions-api.\n[35] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[36] OpenAI. 2023. OpenAI GPT-3.5 Models List. https://platform.openai.com/docs/\nmodels/gpt-3-5.\n[37] OpenAI. 2023. OpenAI GPT-4 Models List. https://platform.openai.com/docs/\nmodels/gpt-4-and-gpt-4-turbo.\n[38] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and\nRamesh Karri. 2022. Asleep at the keyboard? assessing the security of github\ncopilot‚Äôs code contributions. In 2022 IEEE Symposium on Security and Privacy\n(SP). IEEE, 754‚Äì768.\n[39] H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-Gavitt. 2023. Examining\nZero-Shot Vulnerability Repair with Large Language Models. In 2023 2023 IEEE\nSymposium on Security and Privacy (SP) (SP) . IEEE Computer Society, Los Alami-\ntos, CA, USA, 1‚Äì18. https://doi.org/10.1109/SP46215.2023.00001\n[40] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao-\nqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al. 2023. Code\nllama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).\n[41] Max Sch√§fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. Adaptive test\ngeneration using a large language model. arXiv preprint arXiv:2302.06527 (2023).\n[42] Max Sch√§fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. Adaptive Test\nGeneration Using a Large Language Model. arXiv:2302.06527 [cs.SE]\nHow Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation Conference‚Äô17, July 2017, Washington, DC, USA\n[43] Mohammed Latif Siddiq, Joanna Santos, Ridwanul Hasan Tanvir, Noshin Ulfat,\nFahmid Al Rifat, and Vinicius Carvalho Lopes. 2023. Exploring the Effectiveness of\nLarge Language Models in Generating Unit Tests.arXiv preprint arXiv:2305.00418\n(2023).\n[44] Sourcegraph. 2024. Sourcegraph CLI tool. https://docs.sourcegraph.com/cli.\n[45] Google OSS-Fuzz Team. 2024. Feedback of adding runtime error filtering. https:\n//github.com/google/oss-fuzz-gen/pull/185#issuecomment-2024141597.\n[46] Google OSS-Fuzz Team. 2024. Observed case for successfully generating fuzz\ndrivers for new APIs by fixing the fuzzing runtime error. https://github.com/\ngoogle/oss-fuzz-gen/pull/198#issuecomment-2044547163.\n[47] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen Default Repeat\nTime in Experiments. https://github.com/google/oss-fuzz-gen/blob/\nd2b1bfac45efe217b28d5d882d3c3b942adaf9f7/report/docker_run.sh#L111.\n[48] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen Default Tempera-\nture in Experiment. https://github.com/google/oss-fuzz-gen/blob/\nd2b1bfac45efe217b28d5d882d3c3b942adaf9f7/run_one_experiment.py#L53.\n[49] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen: LLM powered fuzzing via OSS-Fuzz.\nhttps://github.com/google/oss-fuzz-gen.\n[50] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen PR #185. https://github.com/google/\noss-fuzz-gen/pull/185.\n[51] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen PR #187. https://github.com/google/\noss-fuzz-gen/pull/187.\n[52] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen PR #191. https://github.com/google/\noss-fuzz-gen/pull/191.\n[53] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen PR #198. https://github.com/google/\noss-fuzz-gen/pull/198.\n[54] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen PR #199. https://github.com/google/\noss-fuzz-gen/pull/199.\n[55] Google OSS-Fuzz Team. 2024. OSS-Fuzz-Gen PR #204. https://github.com/google/\noss-fuzz-gen/pull/204.\n[56] Google OSS-Fuzz Team. 2024. Scaling security with AI: from detection to so-\nlution. https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-\nbug-hunting.html.\n[57] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023.\nMagicoder: Source Code Is All You Need. arXiv preprint arXiv:2312.02120 (2023).\n[58] WizardLM. 2023. wizardcoder-15b-v1.0 Model. https://huggingface.co/\nWizardLM/WizardCoder-15B-V1.0.\n[59] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Ling-\nming Zhang. 2023. Universal fuzzing via large language models. arXiv preprint\narXiv:2308.04748 (2023).\n[60] Chunqiu Steven Xia and Lingming Zhang. 2023. Keep the Conversation Go-\ning: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. arXiv preprint\narXiv:2304.00385 (2023).\n[61] Chenyuan Yang, Yinlin Deng, Runyu Lu, Jiayi Yao, Jiawei Liu, Reyhaneh Jabbar-\nvand, and Lingming Zhang. 2023. White-box compiler fuzzing empowered by\nlarge language models. arXiv preprint arXiv:2310.15991 (2023).\n[62] Cen Zhang, Yuekang Li, Hao Zhou, Xiaohan Zhang, Yaowen Zheng, Xian Zhan,\nXiaofei Xie, Xiapu Luo, Xinghua Li, Yang Liu, et al . 2023. Automata-Guided\nControl-Flow-Sensitive Fuzz Driver Generation. (2023).\n[63] Cen Zhang, Xingwei Lin, Yuekang Li, Yinxing Xue, Jundong Xie, Hongxu Chen,\nXinlei Ying, Jiashui Wang, and Yang Liu. 2021. APICraft: Fuzz Driver Generation\nfor Closed-source SDK Libraries. In 30th USENIX Security Symposium (USENIX\nSecurity 21) . 2811‚Äì2828.\n[64] Mingrui Zhang, Jianzhong Liu, Fuchen Ma, Huafeng Zhang, and Yu Jiang. 2021.\nIntelliGen: automatic driver synthesis for fuzz testing. In 2021 IEEE/ACM 43rd\nInternational Conference on Software Engineering: Software Engineering in Practice\n(ICSE-SEIP). IEEE, 318‚Äì327.",
  "topic": "Fuzz testing",
  "concepts": [
    {
      "name": "Fuzz testing",
      "score": 0.9537733793258667
    },
    {
      "name": "Computer science",
      "score": 0.7783381938934326
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6745271682739258
    },
    {
      "name": "Key (lock)",
      "score": 0.6368338465690613
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5332557559013367
    },
    {
      "name": "Code generation",
      "score": 0.48638036847114563
    },
    {
      "name": "Code (set theory)",
      "score": 0.4304961562156677
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.4229682385921478
    },
    {
      "name": "Software engineering",
      "score": 0.3871181905269623
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3290191888809204
    },
    {
      "name": "Data mining",
      "score": 0.3253164291381836
    },
    {
      "name": "Programming language",
      "score": 0.22574415802955627
    },
    {
      "name": "Software",
      "score": 0.19660979509353638
    },
    {
      "name": "Computer security",
      "score": 0.17000171542167664
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    }
  ]
}