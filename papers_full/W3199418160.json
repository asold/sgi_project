{
  "title": "Sequence Length is a Domain: Length-based Overfitting in Transformer Models",
  "url": "https://openalex.org/W3199418160",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2512118479",
      "name": "Dusan Varis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003230525",
      "name": "Ondřej Bojar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3092318106",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2989156240",
    "https://openalex.org/W2943493972",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3082760180",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W4287727391",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2972451902",
    "https://openalex.org/W2144600658",
    "https://openalex.org/W3039986555",
    "https://openalex.org/W3197479040",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3155808134",
    "https://openalex.org/W3170740203",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2794365787",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2951243568",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2069143585"
  ],
  "abstract": "Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing task and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8246–8257\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n8246\nSequence Length is a Domain:\nLength-based Overﬁtting in Transformer Models\nDušan Variš and Ondˇrej Bojar\nFaculty of Mathematics and Physics, Charles University,\nMalostranské námˇestí 25,\n118 00 Prague, Czechia\n{varis,bojar}@ufal.mff.cuni.cz\nAbstract\nTransformer-based sequence-to-sequence ar-\nchitectures, while achieving state-of-the-art re-\nsults on a large number of NLP tasks, can still\nsuffer from overﬁtting during training. In prac-\ntice, this is usually countered either by apply-\ning regularization methods (e.g. dropout, L2-\nregularization) or by providing huge amounts\nof training data. Additionally, Transformer\nand other architectures are known to struggle\nwhen generating very long sequences. For\nexample, in machine translation, the neural-\nbased systems perform worse on very long\nsequences when compared to the preceding\nphrase-based translation approaches (Koehn\nand Knowles, 2017).\nWe present results which suggest that the is-\nsue might also be in the mismatch between the\nlength distributions of the training and valida-\ntion data combined with the aforementioned\ntendency of the neural networks to overﬁt to\nthe training data. We demonstrate on a simple\nstring editing task and a machine translation\ntask that the Transformer model performance\ndrops signiﬁcantly when facing sequences of\nlength diverging from the length distribution\nin the training data. Additionally, we show that\nthe observed drop in performance is due to the\nhypothesis length corresponding to the lengths\nseen by the model during training rather than\nthe length of the input sequence.\n1 Introduction\nCurrent state-of-the-art Transformer-based se-\nquence generation models, either ﬁne-tuned for\nchosen downstream tasks (Devlin et al., 2019), or\ntrained from scratch for speciﬁc tasks such as ma-\nchine translation (Vaswani et al., 2017) or speech\nrecognition (Pham et al., 2019), more and more\noften achieve performance comparable to that of\nhumans (Hassan et al., 2018; Popel et al., 2020;\nNguyen et al., 2020). However, such models fre-\nquently require billions of trainable parameters\ntogether with huge amounts of data (billions of\ntokens) to reach such performance (Brown et al.,\n2020).\nThe good performance on held-out test sets\nseems to conﬁrm the good generalization power of\nthese models, although the inherent strong biases,\nsometimes leading to the use of a foul and toxic\nlanguage, preserving stereotypes, etc., are well ac-\nknowledged (Gehman et al., 2020). Brown et al.\n(2020) claim that their Transformer model is also\ncapable of simple arithmetics, however, it is yet\nto be validated whether the model truly learns the\narithmetic algorithms or simply encodes a lookup\ntable for a subset of speciﬁc examples.\nIn this paper, we argue that the assumed gen-\neralization power of the current state-of-the-art\nTransformer-based language generators does not\ncome from the architecture itself but rather from\nthe sheer volume of training data and the model’s\nability to exploit the similarities between the train-\ning and validation data. We demonstrate how the\nTransformer-based sequence-to-sequence models\nfail when the target sequence lengths of the training\nand validation data do not match. We show that\nthis holds not only for very long test sequences but\ncan be observed even with short sequences if they\nare omitted from the training data. Furthermore,\nwe show that we can artiﬁcially improve the test\nperformance on longer sequences by only using\nshorter training sequences and concatenating them\ninto longer training examples.\nWe do not argue about Transformer’s (in)ability\nto handle long-distance dependencies, but our re-\nsults suggest that a considerably simpler reason of\nmismatching sequence length can also contribute\nto the performance drop. We think that our ﬁnd-\nings can lead to better understanding of the Trans-\nformer architecture and help to design better train-\ning schemes (e.g. curriculum learning).\n8247\n2 Related Work\nThe problem of modeling very long sequences has\nbeen studied mainly in the context of recurrent neu-\nral networks (RNNs). Early studies showed that\nusing LSTMs (Sutskever et al., 2014) and intro-\nducing attention (Bahdanau et al., 2014; Luong\net al., 2015) can improve the model performance\non long sequences. However, these models still\ngot outperformed on long sequences by phrase-\nbased models (Koehn and Knowles, 2017). This\nproblem was not resolved with the introduction of\nTransformers (Vaswani et al., 2017). Surprisingly,\neven though there were previous studies explain-\ning the weaknesses of RNNs with respect to long\nsequence modeling (Hochreiter and Schmidhuber,\n1997; Hochreiter, 1998), similar analyses are yet to\nbe done for Transformers which are fundamentally\ndifferent from RNNs.\nThere is an ongoing debate about the proper way\nof splitting the available data to training and eval-\nuation subsets. Gorman and Bedrick (2019) show\nthat using only standard dataset splits can lead to\na biased evaluation resulting in overestimating the\ngeneralization ability of the model. Furthermore,\nSøgaard et al. (2020) argue that even using ran-\ndomly sampled dataset splits does not solve the\noverestimation problem. They instead suggest us-\ning multiple test sets possibly of an adversarial na-\nture to properly evaluate the generalization ability\nof the model.\nIn the following experiments, we evaluate vanilla\nTransformer on such adversarial splits created with\nrespect to the lengths of the modeled sequences.\nAlthough similar analyses were performed in the\npast (Neishi and Yoshinaga, 2019; Kondo et al.,\n2021), it was at a smaller scale and mainly in the\ncontext source-side length bucketing.\n3 Experiments\nWe demonstrate the lack of ability to generalize\nto sequences of lengths not seen during training\non two separate tasks: string editing and machine\ntranslation (MT).\nWe use Fairseq framework for sequence-to-\nsequence learning (Ott et al., 2019) in our exper-\niments.1 Details about the model parameters and\ntraining are available in Appendix A.\nInput Output\npush 1 | 1 0 1 0 1 0 1 0 1\nreverse − | 1 0 0 1 1 1 1 0 0 1\nTable 1: Input and output example for push and\nreverse tasks. Hyphen ( −) indicates an empty ar-\ngument for the latter task.\n0-10 11-15 16-20\ncopy 62.6 100.0 0.0\npush 59.1 100.0 0.0\npop 0.1 100.0 0.0\nshift 52.5 100.0 0.0\nunshift 41.2 100.0 0.0\nreverse 0.0 84.4 0.0\nall 15.822 97.5 0.978\nTable 2: Accuracy (in %) of models trained on various\nstring editing tasks using only training data from the\n11-15 length bucket evaluated on datasets with differ-\nent sequence lengths. Each model was evaluated on its\nrespective task domain.\n3.1 String Editing Operations\nIn the ﬁrst set of experiments, we focus on learning\nsimple string editing algorithms. We chose this task\nbecause we think it is an interesting alternative to\nstandard NLP tasks that often struggle with evalua-\ntion ambiguity (multiple possible outputs in MT or\ntext generation with nuanced degree of quality) and\nproper training/validation separation (partial over-\nlap between train and test sentences leading to lack\nof clarity how much model actually generalizes to\nnew inputs).\nWe chose to study the following tasks:\n• copy: copy the input sequence to the output,\n• unshift X, push X: add a single character (X)\nto the beginning or the end of the sequence\nrespectively\n• shift, pop: remove a single character from the\nbeginning or the end respectively,\n• reverse: reverse the character order in the in-\nput sequence\nAs for the experiment setup, we generate a\ndataset of sequences consisting of two characters\n(e.g. 0 and 1), separated by whitespace, with no\nduplicate sequences. Then, we split the dataset\ninto three separate buckets according to sequence\n1https://github.com/pytorch/fairseq\n8248\nBucket 0-10 11-20 21-30 31-40 41-50 51-60 61-70 71-80\n# of sent. pairs (M) 30.9 18.0 7.5 3.9 2.3 1.2 0.7 0.4\n# of tokens (M) 375.3 502.6 361.6 268.9 198.9 132.6 87.3 59.5\nTable 3: Sizes of the respective training buckets (created based on the target sequence length) in millions of\nsentence pairs and millions of tokens (after tokenization and applying BPE, combined source and target size).\nlengths, 0 −10, 11 −15 and 16 −20 respectively.\nWe sample 1,000 sequences from the 0 −10 and\n16 −20 buckets for test-time evaluation. We split\nthe 11 −15 bucket into a validation (1,000 exam-\nples), test (1,000 examples) and training (28,000\nexamples) from a sample of 30k examples without\nrepetition.\nGiven these data splits, we create datasets for\neach task by adding the task label, character (0, 1\nfor unshift and push, −for others) and a separator\n(|) to the beginning of each sequence.2 We create\ntarget sequences for each task according to the re-\nspective task deﬁnition. Table 1 shows examples\nof the networks inputs.\nFor each task, we train a separate net-\nwork on the 11-15 training data. Model de-\ntails are available in Appendix A.1. We\nevaluate the models by measuring accuracy\nACC = num_correct/num_examples, where\nnum_correct is the number of exact matches be-\ntween the hypothesis and reference strings. Table 2\nshows the accuracy of the models trained on each\ntask and evaluated on the varying test set buckets.\nWe can see that the models generalize very well\non the unseen sequences with length similar to the\ntraining sequences, all reaching the perfect accu-\nracy except the reverse task. On the other hand,\nwhen facing shorter or longer sequences, the per-\nformance drops signiﬁcantly.\nTable 2 also shows results of the training a net-\nwork on all tasks simultaneously (all; by concate-\nnating and shufﬂing respective training data and\nperforming evaluation on the concatenation of the\nrespective testsets). The resulting performance is\nsimilar to that of a single-task model.\nThese results suggest that the length distribution\nsimilarity between the training and validation data\nis important and that the vanilla Transformer de-\ncoder is prone to overﬁtting to the sequence lengths\nseen during training.\n2The arguments for unshift and push are sampled from a\nBernoulli distribution with 0 character having p = 0.5.\n3.2 Machine Translation\nTo see whether our ﬁndings within the string edit-\ning tasks also hold for natural language which has\nmore complex structure, we perform similar exper-\niments on English-Czech translation.\nWe use CzEng 2.03 (Kocmi et al., 2020) as our\ntraining corpus, a concatenation of WMT2020 (Bar-\nrault et al., 2020) newstest13-16 as held-out\ntest set and a concatenation of newstest17-20\nfor ﬁnal evaluation.4 We tokenize our data using\nMoses tokenizer.5 We use byte-pair encoding (Sen-\nnrich et al., 2016) on our training data, to create\nsubword segmentation of size 30k. 6 We split all\ntokenized and BPE-segmented datasets into buck-\nets of sizes 1-10, 11-20, ..., 91-100 (labeled as 10,\n20, ..., 100 respectively) based on the number of\ntokens on the target side. Table 3 shows the sizes\nof the respective training corpora. We train a sep-\narate model for each training bucket. Details on\nthe model hyper-parameters are available in Ap-\npendix A.2.\nWe evaluate how the length of the training\ndata affects the performance with respect to the\nlength of the test data using BLEU (Papineni et al.,\n2002), namely the SacreBLEU implementation\n(Post, 2018).7 Figure 1 (Top) shows that regard-\nless of the training bucket, the model performs best\nwhen presented with data of target-side length sim-\nilar to the length of the training data. This conﬁrms\nthat the model overﬁts to the length of the train-\ning data, affecting its performance even on shorter\nsentences. The performance further decreases with\nincreasing train-test length difference, although it\nneeds to be noted that the BLEU scores between\ndifferent testset buckets are not directly compara-\nble due to the nature of the scoring metric and the\nfact that each testset bucket contains different test\n3https://ufal.mff.cuni.cz/czeng\n4We download the newstest corpora using SacreBLEU\n(Post, 2018).\n5https://github.com/moses-smt/\nmosesdecoder.git\n6https://github.com/rsennrich/\nsubword-nmt.git\n7https://github.com/mjpost/sacrebleu\n8249\n0 10 20 30 40 50 60 70 80 90 100 110 120\n0\n10\n20\nBLEU\nTrainBucket = 10TrainBucket = 20TrainBucket = 30TrainBucket = 40TrainBucket = 50TrainBucket = 60TrainBucket = 70TrainBucket = 80Full CzEng\n0 10 20 30 40 50 60 70 80 90 100 110 120\n0\n5\nTest Bucket\nHyp/Ref Ratio\nTrainBucket = 10TrainBucket = 20TrainBucket = 30TrainBucket = 40TrainBucket = 50TrainBucket = 60TrainBucket = 70TrainBucket = 80Full CzEng\nFigure 1: Top: Varying performance of Transformers on test data trained only on the data from a speciﬁc target-\nside length bucket (various lines) when evaluated on a speciﬁc test bucket (x-axis). When the train-test sentence\nlength difference increases, the performance drops. Note that BLEU scores are not directly comparable across\ndifferent test sets (i.e. horizontally). Within each test set, we see that the Full CzEng and the training bucket of\nthe matching length are the two best results. Bottom: Average ratio between a hypothesis and reference. Dashed\nline indicates a ratio of 1.0. Systems trained on short sentences produce short outputs, systems trained on long\nsentences produce up to 10x longer outputs (Train Bucket 80 evaluated on Test Bucket 10).\nexamples. Figure 1 (Bottom) explains the main\nreason behind the BLEU decrease: the increased\nhypothesis/reference length ratio, further support-\ning the length overﬁtting argument. Note that the\nlower performance of the models trained on the\n70 and 80 buckets migth be due to signiﬁcantly\nsmaller size of training data (< 1M sentence pairs).\nIn Appendix B, we also provide a case study of the\nmodels trained on various length buckets.\nThe length-controlled experiment results pre-\nsented by Neishi and Yoshinaga (2019), while not\ndirectly focused on exploring the target-side length\noverﬁtting phenomenon, point to a similar behavior\nof vanilla Transformers with regards to both longer\nand shorter test sentences. Based on their results,\nthe replacement of the absolute positional embed-\ndings with a variation of relative-position embed-\ndings (Shaw et al., 2018; Neishi and Yoshinaga,\n2019) seems like a promising approach towards\nalleviating the length overﬁtting problem.\nTo see whether we can exploit the target-side\nlength overﬁtting behaviour, we also set up a sep-\narate experiment, similar to Kondo et al. (2021).\nWe take the 10, 20 and 30 training buckets and\nconcatenate the sentences in each of them to create\nsynthetic datasets with target-side lengths 51-60\n(containing on average 6, 3 and 2 sentences per\ntraining example, respectively). We apply the same\ntraining strategy using the synthetic data to see how\nstrongly can the length of the training examples (al-\nthough artiﬁcial) affect the model performance on\nthe test examples of similar length.\nFigure 2 shows that the simple concatenation\nof shorter training sentence pairs can lead to a\nperformance similar to the model trained on the\ngenuinely longer sentences. Only the performance\nof the model trained on the concatenation of very\nshort sentences (the line “TrainBucket.Concat=10”\nin Figure 2) drops signiﬁcantly, possibly because\nthe model does not learn to handle any dependen-\ncies beyond the length of 10 and such dependencies\nseem to emerge in test sentences with length over\n40, where the model starts to underperform.\nKondo et al. (2021) show that augmenting the\nexisting training data with additional training exam-\nples that were created by concatenation of shorter\nsentences can help to improve model performance\non very long sentences. Our results show that the\nsynthetic concatenated data on their own can be\nsufﬁcient to train a model that is competitive when\napplied to sentences from the similar target-length\ndomain as the training examples. We also argue\nthat due to a different bucket preparation strategy\n(based on the source-length in the previous work),\nthe target-side length overﬁtting phenomenon is not\nas clear in Kondo et al. (2021) as in our work. In\n8250\n20 40 60 80 100\n0\n5\n10\n15\nTest Bucket\nBLEU TrainBucket.Concat = 10TrainBucket.Concat = 20TrainBucket.Concat = 30TrainBucket = 60\nFigure 2: Comparison of the performance of a model\ntrained on genuine data from the 60-length bucket with\nmodels trained on synthetic 60-length datasets created\nby concatenation of 10, 20 and 30-bucket sentences re-\nspectively.\nAppendix C, we provide additional results from the\nexperiments where the dataset bucketing is based\non the source-side length instead of the target-side\nlength for comparison.\n4 Conclusion\nWe showed in our targeted experiment that vanilla\nTransformer sequence-to-sequence models have a\nstrong tendency to overﬁt with regard to the target-\nside length of the training sequences. On a sim-\nple algorithmic task, we documented that Trans-\nformer can generalize very well to unseen exam-\nples within the same length bucket but falls short\nif the same task is required for input of a different\nlength, shorter or longer. The algorithm of the task,\neven if very simple, is not learned.\nWe further conﬁrmed the overﬁtting problem on\nthe machine translation task. This suggests that\nlong-distance dependencies are not the only reason\nbehind the decreased performance when translating\nvery long sentences. We think that our ﬁndings can\nshed a new light on speciﬁc areas of deep learning\nresearch, namely domain adaptation and curricu-\nlum learning.\nWe also showed that data augmentation can\ntackle the data sparsity in the domain of very long\nsentences.\nAcknowledgements\nThis work was supported by the GA ˇCR\nNEUREM3 grant (Neural Representations in Multi-\nmodal and Multi-lingual Modelling, 19-26934X\n(RIV: GX19-26934X)) and by SVV 260 453 grant.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nLoïc Barrault, Ond ˇrej Bojar, Fethi Bougares, Rajen\nChatterjee, Marta R. Costa-jussà, Christian Feder-\nmann, Mark Fishel, Alexander Fraser, Yvette Gra-\nham, Paco Guzman, Barry Haddow, Matthias Huck,\nAntonio Jimeno Yepes, Philipp Koehn, André Mar-\ntins, Makoto Morishita, Christof Monz, Masaaki Na-\ngata, Toshiaki Nakazawa, and Matteo Negri, edi-\ntors. 2020. Proceedings of the Fifth Conference\non Machine Translation. Association for Computa-\ntional Linguistics, Online.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJonathan H. Clark, Chris Dyer, Alon Lavie, and\nNoah A. Smith. 2011. Better hypothesis testing for\nstatistical machine translation: Controlling for op-\ntimizer instability. In Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies , pages\n176–181, Portland, Oregon, USA. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. Realtoxi-\ncityprompts: Evaluating neural toxic degeneration\nin language models. In EMNLP (Findings), pages\n3356–3369. Association for Computational Linguis-\ntics.\nKyle Gorman and Steven Bedrick. 2019. We need to\ntalk about standard splits. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2786–2791, Florence,\nItaly. Association for Computational Linguistics.\n8251\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary, Jonathan Clark, Christian Feder-\nmann, Xuedong Huang, Marcin Junczys-Dowmunt,\nWilliam Lewis, Mu Li, Shujie Liu, Tie-Yan Liu,\nRenqian Luo, Arul Menezes, Tao Qin, Frank Seide,\nXu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce\nXia, Dongdong Zhang, Zhirui Zhang, and Ming\nZhou. 2018. Achieving human parity on auto-\nmatic chinese to english news translation. CoRR,\nabs/1803.05567.\nSepp Hochreiter. 1998. The vanishing gradient prob-\nlem during learning recurrent neural nets and prob-\nlem solutions. Int. J. Uncertain. Fuzziness Knowl.-\nBased Syst., 6(2):107–116.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural Comput. ,\n9(8):1735–1780.\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabi-\nnowitz, Joel Veness, Guillaume Desjardins, An-\ndrei A. Rusu, Kieran Milan, John Quan, Tiago Ra-\nmalho, Agnieszka Grabska-Barwinska, Demis Hass-\nabis, Claudia Clopath, Dharshan Kumaran, and Raia\nHadsell. 2017. Overcoming catastrophic forgetting\nin neural networks. Proceedings of the National\nAcademy of Sciences of the United States of Amer-\nica, 114 13:3521–3526.\nTom Kocmi, Martin Popel, and Ondrej Bojar. 2020.\nAnnouncing czeng 2.0 parallel corpus with over 2\ngigawords. arXiv preprint arXiv:2007.03006.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39, Vancouver. Association for\nComputational Linguistics.\nSeiichiro Kondo, Kengo Hotate, Tosho Hirasawa,\nMasahiro Kaneko, and Mamoru Komachi. 2021.\nSentence concatenation approach to data augmenta-\ntion for neural machine translation. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nStudent Research Workshop, pages 143–149, Online.\nAssociation for Computational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1412–1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nMasato Neishi and Naoki Yoshinaga. 2019. On the\nrelation between position information and sentence\nlength in neural machine translation. In Proceed-\nings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL) , pages 328–338,\nHong Kong, China. Association for Computational\nLinguistics.\nThai-Son Nguyen, Sebastian Stüker, and Alex Waibel.\n2020. Super-human performance in online low-\nlatency recognition of conversational speech. CoRR,\nabs/2010.03449.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nNgoc-Quan Pham, Thai-Son Nguyen, Jan Niehues,\nMarkus Müller, and Alex Waibel. 2019. Very deep\nself-attention networks for end-to-end speech recog-\nnition. In INTERSPEECH, pages 66–70. ISCA.\nMartin Popel, Marketa Tomkova, Jakub Tomek,\nŁukasz Kaiser, Jakob Uszkoreit, Ond ˇrej Bojar, and\nZdenˇek Žabokrtský. 2020. Transforming machine\ntranslation: a deep learning system reaches news\ntranslation quality comparable to human profession-\nals. Nature Communications, 11(4381):1–15.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAnders Søgaard, Sebastian Ebert, J. Bastings, and\nKatja Filippova. 2020. We need to talk about ran-\ndom splits. CoRR, abs/2005.00636.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Sys-\ntems, volume 27. Curran Associates, Inc.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\n8252\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 6000–6010. Curran Asso-\nciates, Inc.\n8253\nA Model Details\nIn the following section, we provide the details\nof the used models and their training. All the de-\nscribed models are implemented in Fairseq (Ott\net al., 2019).8 During training, we use word-level\ncross-entropy loss with teacher forcing (Bahdanau\net al., 2014; Vaswani et al., 2017) which is a current,\nwidely used approach to the sequence-to-sequence\nTransformer training. During decoding, we use\nbeam search with beam size 4 and length penalty\n0.6.\nA.1 String Editing\nIn the experiments with string editing, we use the\ntransformer parameter setting with the follow-\ning modiﬁcations:\n• embeddings size: 128,\n• feedforward size: 512,\n• number of attention heads: 8,\n• encoder/decoder depth: 1,\n• batch size: 4,096 tokens,\n• learning rate 5e-4,\n• warmup steps: 4,000,\n• dropout: 0.3,\n• train epochs: 100\nA.2 Machine Translation\nIn the machine translation experiments, we use the\ntransformer parameter setting with the follow-\ning modiﬁcations:\n• embeddings size: 512,\n• feedforward size: 2048,\n• number of attention heads: 8,\n• encoder/decoder depth: 6,\n• batch size: 4,096,\n• learning rate: 5e-4,\n• warmup steps: 4,000,\n• dropout: 0.3\n8https://github.com/pytorch/fairseq\nDuring training, we apply early stopping: if the\nmodel performance in BLEU (Papineni et al., 2002)\ndoes not improve for 10 epochs (evaluated on the\ncomplete held-out test set without length splits),\nthe training is terminated.\nB Translation Output Examples\nFigure 3 shows example outputs from models\ntrained on various target-length training buckets\n(10-, 30- and 60-bucket) produced by translating\na chosen 30-bucket testset inputs. The examples\ndemonstrate that he models have tendencies to pro-\nduce outputs with length similar to the training data\nwhile trying to satisfy the translation of the source\nsentence resulting in the longer, 60-bucket model\nrepeating certain phrases or sentences while intro-\nducing grammatical errors (e.g. wrong agreement,\npreposition choice) or mistranslations. On the other\nhand, the shorter, 10-bucket model manages to drop\nparts of the input sentence while maintaining a rea-\nsonable ﬂuency and grammatical correctness of the\noutput.\nFigure 4 shows example of outputs from mod-\nels trained on the synthetic 60-bucket data cre-\nated by concatenation of the shorter training buck-\nets. At ﬁrst glance, all three hypotheses are very\nsimilar and are reasonably good translations of\nthe source sentence, however, all systems made\na wrong surface form and preposition choice for\n“na Vinohradech” (the same grammatical mistake\nas with “na Žižkov ˇe” in Figure 3), producing an\nincorrect but literal translation of the English “in\nVinohrady”. Additionally, all three systems chose\na literal translation of the word “approach”, which\nis incorrect in the given context. The incorrect\nsurface form of the translation “založeno na do-\nporuˇcení” in Hyp1 suggests that training a model\non a concatenation of very short sentences may\nlead to incorrect modeling of long-range dependen-\ncies. Surprisingly, the Hyp3 system mistranslated\nthe phrase “work on the reconstruction” (“k rekon-\nstrukci” in the output) whileHyp2 system produced\na correct translation, though this error is most likely\na result of different set of training sentences in the\nHyp2 and Hyp3 training data rather than the lenght\nof the training sentences (before concatenation).\nC Source-Side Bucketing Experiments\nFor comparison, we repeated the translation exper-\niments using source-side length-based bucketing\nof the training and validation data. Figure 6 shows\n8254\nSource (30-bucket) The company does not collect its mail and it has closed its ofﬁcial headquarters in Žižkov more\nthan six years ago.\nHyp1 (10-bucket) Spole ˇcnost nesbírá poštu a zavˇrel oﬁciální sídlo.\nHyp1 (gloss) The company does not gather mail and closed ofﬁcial headquarters.\nHyp2 (30-bucket) Spoleˇcnost neshromažd’uje poštu a již pˇred více než šesti lety zavˇrela své oﬁciální sídlo v Žižkovˇe.\nHyp2 (gloss) The company does not collect mail and more than six years ago closed its ofﬁcial headquarters\nin Žižkov.\nHyp3 (60-bucket) Spoleˇcnost nevybírá poštu a uzavˇrela své oﬁciální sídlo v Žižkovˇe více než šest letago. v Žižkov ˇe.\nSpoleˇcnost neshromažd’uje poštu a uzavírá oﬁciální ústˇredí v Žižkovˇe více než šest let agr. o.\nHyp3 (gloss) The company does not pick up mail and closed up its ofﬁcial its ofﬁcial headquarters in Žižkov\nmore than six years ago. in Žižkov. The company does not collect mail and closes up ofﬁcial\nheadquarters in Žižkov more than six years agr. o.\nReference (30-bucket) Nep ˇrebírá poštu a oﬁciální sídlo na Žižkovˇe zrušila pˇred více než šesti lety.\nRef (gloss) (The company) does not collect mail and ofﬁcial headquarters in Žižkov closed up more than six\nyears ago.\nSource (30-bucket) The perpetrators ended up in custody, said Marie Štrbáková, the spokeswoman of Olomouc police.\nHyp1 (10-bucket) Mluvila s ní Marie Štrkováková\nHyp1 (gloss) Talked to her, Marie Štrkováková\nHyp2 (30-bucket) Pachatelé skon ˇcili ve vazbˇe, ˇrekla Marie Štrbákováová, mluvˇcí Olomouckého policie.\nHyp2 (gloss) The perpetrators ended up in custody, said Marie Štrbákováová, the spokeswoman of Olomouc\npolice.\nHyp3 (60-bucket) Uchaze ˇci skoncovali v úschovˇe, \"uvedla Marie Štrbákováová, mluv ˇcí Olomoucké\npolicie, která se stala mluvˇcí Olomouckého vojska, a to v úschovˇe.\nHyp3 (gloss) The candidates ended up in storage, \"introduced Marie Štrbákováová, the spokeswoman of Olo-\nmouc police, which became the spokeswoman of Olomouc army, and in storage.\nRef (30-bucket) Pachatelé skon ˇcili ve vazbˇe, informovala olomoucká policejní mluvˇcí Marie Štrbáková.\nRef (gloss) The perpetrators ended up in custody, informed Olomouc police spokeswoman Marie Štrbáková.\nFigure 3: Example translations from systems trained on speciﬁc target-length-restricted datasets. Both examples\ndemonstrate over and under-generation of systems trained on datasets containing longer (60-bucket) and shorter\n(10-bucket) sentences when applied to inputs with length of reference translation different from the training data\n(30-bucket). We provide rough, “word-for-word” translations of the produced outputs (in italics) with color high-\nlighting of some of the phrases and their corresponding English translation for better comprehension. The underline\nhighlights grammatical errors or mistranslations in the output.\nthe performance of the bucketed models with re-\nspect to testset of various bucket sizes. While the\nresults are similar to the target-side bucketing ex-\nperiments, the overﬁtting phenomenon is less clear\nin several cases (e.g. 20-bucket system reaching\nhigher BLEU than 10-bucket system on 10-bucket\ntestset or the relative system ranking on the 60-\nbucket testset).\nWe think that the possible reason is the differ-\nence between the source-side length and the length\nof training/validation reference leading to possible\noverlap of target-side lengths between the differ-\nent train/validation buckets. Figure 5 shows the\nlength distributions of target-side lengths within\neach training and validation bucket. Although the\nlength-wise overlap between the target-side of the\ntraining/validation examples is manifested mostly\nin the 1st and 4th quartile, we think that it helps\nto support the argument that the length-based over-\nﬁtting should be studied with respect to the target-\nside length instead of the source-side. Furthermore,\nthe length of the target-side (Czech) in the test\ndataset is generally smaller than the source-side\n(English), resulting in additional domain mismatch\nbetween the training-test buckets. Note that very\nlong target-side outliers in the training data are\nmost likely a result of an imperfect sentence-pair ﬁl-\ntering after the inclusion of the additional synthetic\nparallel data (forward and backward translation) to\nthe CzEng 2.0 corpus.\nBased on the reviewer’s suggestion, we also mea-\nsured the effect of ﬁnetuning a system trained on\nthe whole training dataset using a source-side buck-\neted training data. Each system was ﬁne-tuned for\n8255\nSource (60-bucket) We have already worked with Lenka Langerová on our ﬂat in the mountains based on a recom-\nmendation from another client and because everything worked well we decided to approach her\nto work on the reconstruction of our new ﬂat in Vinohrady.\nHyp1 (10-bucket-concat) Už jsme pracovali s Lenkou Langerovou na našem bytˇe v horách založeno na doporuˇcení od\njiného klienta a protože všechno fungovalo dob ˇre, rozhodli jsme se k ní pˇriblížit k práci na\nrekonstrukci našeho nového bytu ve Vinohrady.\nHyp2 (30-bucket-concat) Již jsme spolupracovali s Lenkou Langerovou na našem bytˇe v horách na základˇe doporuˇcení\njiného klienta a protože vše fungovalo dob ˇre, rozhodli jsme se, že se k ní pˇriblížíme, aby\npracovala na rekonstrukci našeho nového bytu ve Vinohrady.\nHyp3 (60-bucket) Již jsme s Lenkou Langerovou spolupracovali na našem bytu v horách na základ ˇe do-\nporuˇcení jiného klienta a protože vše fungovalo dobˇre, rozhodli jsme se, že se k ní pˇriblížíme\nk rekonstrukci našeho nového bytu ve Vinohrady.\nRef (60-bucket) S architektkou Lenkou Langerovou jsme spolupracovali už na našem horském apartmánu, tehdy\nna bázi osobního doporuˇcení jiného klienta, a vzhledem k tomu, že vše dobˇre fungovalo, byla\npro nás jasná volba i pˇri rekonstrukci našeho nového bytu na Vinohradech.\nFigure 4: Example of translation hypotheses generated by a system trained on a genuine 60-bucket data and sys-\ntems trained only on a concatenation of shorter training examples (10-bucket-concat, 30-bucket-concat) for com-\nparison. Color highlighting indicates the correspondence of Czech and English phrases. The underline highlights\ngrammatical errors in the output.\n30 epochs, although, it is important to note that the\nvalidation BLEU of each ﬁne-tuned system was\ndropping during training (compared to the BLEU\nof the initial model) when evaluated against the\nwhole non-bucketed validation dataset. In Figure 7,\nwe can see a growing effect of catastrophic forget-\nting (Kirkpatrick et al., 2017): all models initially\nsaw all lengths during pretraining but specialized\nfor a speciﬁc length bucket during ﬁnetuning. In-\nterestingly, the forgetting effect is stronger for test\nbuckets that are longer than the ﬁnetuning lengths\nwhile the models show much better retention of the\nability to model shorter sentences.\nLastly, we also performed a comparison be-\ntween the baseline MT system and the combina-\ntion of systems trained on a speciﬁc source-side\nlength buckets training datasets. We extracted\nsentences from our test dataset with source-side\nlength 0–80, translated them with the respective\nsystems and computed the BLEU scores using\nMultEval (Clark et al., 2011). 9 We compared\na system combination trained using only a spe-\nciﬁc length-bucket dataset ( bucketed) applied\non the respective “in-domain” parts of the test\ndataset. We also provide comparison with the\nsystem combination initialized by the baseline\nmodel and then ﬁne-tuned on the respective length-\nbucket datasets (bucketed.tuning). Addition-\nally, we also trained a system using CzEng 2.0 with\nadditional source-side labels indicating a length-\n9https://github.com/jhclark/multeval\nBLEU\nbaseline 19.1 ± 0.2\nbucketed 18.9 ± 0.2\nbucketed.tuning 17.1 ± 0.2\nbucket.labels 16.7 ± 0.2\nTable 4: Comparison of the translation performance of\nthe baseline model trained on the whole CzEng 2.0,\nand source-length specialized models. bucketed is\na combination of systems trained on the source-length\nbucketed training data, bucketed.tuning is a sim-\nilar combination, where systems were ﬁrst initialized\nby the baseline model and then ﬁne-tuned for 30\nepochs on their respective buckets. bucket.labels\nis a system trained on the whole CzEng 2.0 with inclu-\nsion of the source-side bucket length labels on the in-\nput. The systems were evaluated using MultEval (Clark\net al., 2011) using a bootstrapping over a test dataset\ncontaining sentences of source-side lengths 0–80. Only\na single optimizer run was performed for each evalu-\nated system.\nbucket in which a given training example ended\nup after the source-side length-based dataset split-\nting (bucket.labels), e.g “<20> Example sen-\ntence...” for a sentence from a bucket 11–20. This\nmodel was evaluated on the same test dataset with\ninclusion of these source-side length-bucket labels.\nThe results in Table 4 suggest that the length-\nbased specialization of the models does not outper-\nform the baseline. One of the possible explanations\nis a fact that baseline system was trained on the\n8256\n0 50 100 150 200 250 300\n0-10\n11-20\n21-30\n31-40\n41-50\n51-60\n61-70\n71-80\nTarget Length\nSource Length\n0 50 100 150 200 250 300\n0-10\n11-20\n21-30\n31-40\n41-50\n51-60\n61-70\n71-80\nTarget Length\nSource Length\nFigure 5: Distribution of lengths of target-side refer-\nences within the training (top) and validation (bottom)\ndatasets after splitting them into source-side length\nbuckets. Both ﬁgures have identical x-axis scaling for\nbetter comparison. The long whiskers of the training\nbucket length distributions are a result of a noisy nature\nof CzEng 2.0 training corpus.\nwhole CzEng 2.0 containing even sentences longer\nthan 80. Although the bucket.labels was also\ntrained using the whole CzEng 2.0, the results sug-\ngest that a simple inclusion of the source-length\nbucket information does not contribute towards a\nbetter translation performance.\n8257\n0 10 20 30 40 50 60 70 80 90 100 110 120\n0\n10\n20\nBLEU\nTrainBucket = 10TrainBucket = 20TrainBucket = 30TrainBucket = 40TrainBucket = 50TrainBucket = 60TrainBucket = 70TrainBucket = 80Full CzEng\n0 10 20 30 40 50 60 70 80 90 100 110 120\n0\n2\n4\nTest Bucket\nHyp/Ref Ratio\nTrainBucket = 10TrainBucket = 20TrainBucket = 30TrainBucket = 40TrainBucket = 50TrainBucket = 60TrainBucket = 70TrainBucket = 80Full CzEng\nFigure 6: Top: Varying performance of Transformers on test data trained only on the data from a speciﬁc source-\nside length bucket (various lines) when evaluated on a speciﬁc test bucket (x-axis). BLEU scores are not directly\ncomparable across different test sets (i.e. horizontally). Bottom: Average ratio between a hypothesis and reference.\nDashed line indicates a ratio of 1.0.\n0 10 20 30 40 50 60 70 80 90 100 110 120\n5\n10\n15\n20\nBLEU\nTrainBucket = 10TrainBucket = 20TrainBucket = 30TrainBucket = 40TrainBucket = 50TrainBucket = 60TrainBucket = 70TrainBucket = 80Full CzEng\n0 10 20 30 40 50 60 70 80 90 100 110 1200\n2\n4\nTest Bucket\nHyp/Ref Ratio\nTrainBucket = 10TrainBucket = 20TrainBucket = 30TrainBucket = 40TrainBucket = 50TrainBucket = 60TrainBucket = 70TrainBucket = 80Full CzEng\nFigure 7: Top: Varying performance of Transformers on test data trained on all of CzEng and ﬁne-tuned only on\nthe data from a speciﬁc source-side length bucket (various lines) when evaluated on a speciﬁc test bucket (x-axis).\nBLEU scores are not directly comparable across different test sets (i.e. horizontally). Bottom: Average ratio\nbetween a hypothesis and reference. Dashed line indicates a ratio of 1.0. We preserve the scaling of all the plots\nfor better comparability across the ﬁgures.",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8926107287406921
    },
    {
      "name": "Transformer",
      "score": 0.7020774483680725
    },
    {
      "name": "Computer science",
      "score": 0.6992436051368713
    },
    {
      "name": "Machine translation",
      "score": 0.6308317184448242
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5440778136253357
    },
    {
      "name": "Phrase",
      "score": 0.5098711252212524
    },
    {
      "name": "Training set",
      "score": 0.45670247077941895
    },
    {
      "name": "Minimum description length",
      "score": 0.43587806820869446
    },
    {
      "name": "Sequence (biology)",
      "score": 0.43266141414642334
    },
    {
      "name": "Machine learning",
      "score": 0.41981497406959534
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.4116482734680176
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3593882918357849
    },
    {
      "name": "Algorithm",
      "score": 0.3589591681957245
    },
    {
      "name": "Speech recognition",
      "score": 0.3433539867401123
    },
    {
      "name": "Artificial neural network",
      "score": 0.3277113735675812
    },
    {
      "name": "Voltage",
      "score": 0.10802215337753296
    },
    {
      "name": "Engineering",
      "score": 0.10331851243972778
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}