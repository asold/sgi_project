{
  "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers",
  "url": "https://openalex.org/W3014611590",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1910746463",
      "name": "Huang Zhi-cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2197854703",
      "name": "Zeng, Zhaoyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120709103",
      "name": "Liu, Bei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1896617709",
      "name": "Fu Dongmei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748452209",
      "name": "Fu, Jianlong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964727037",
    "https://openalex.org/W2992505801",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2968880719",
    "https://openalex.org/W2970869018",
    "https://openalex.org/W3001555892",
    "https://openalex.org/W2963499204",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963717374",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2965848243",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963521239",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2970231061"
  ],
  "abstract": "We propose Pixel-BERT to align image pixels with text by deep multi-modal transformers that jointly learn visual and language embedding in a unified end-to-end framework. We aim to build a more accurate and thorough connection between image pixels and language semantics directly from image and sentence pairs instead of using region-based image features as the most recent vision and language tasks. Our Pixel-BERT which aligns semantic connection in pixel and text level solves the limitation of task-specific visual representation for vision and language tasks. It also relieves the cost of bounding box annotations and overcomes the unbalance between semantic labels in visual task and language semantic. To provide a better representation for down-stream tasks, we pre-train a universal end-to-end model with image and sentence pairs from Visual Genome dataset and MS-COCO dataset. We propose to use a random pixel sampling mechanism to enhance the robustness of visual representation and to apply the Masked Language Model and Image-Text Matching as pre-training tasks. Extensive experiments on downstream tasks with our pre-trained model show that our approach makes the most state-of-the-arts in downstream tasks, including Visual Question Answering (VQA), image-text retrieval, Natural Language for Visual Reasoning for Real (NLVR). Particularly, we boost the performance of a single model in VQA task by 2.17 points compared with SOTA under fair comparison.",
  "full_text": "Pixel-BERT: Aligning Image Pixels with Text by\nDeep Multi-Modal Transformers\nZhicheng Huang1⋆, Zhaoyang Zeng2⋆, Bei Liu3, Dongmei Fu1, and Jianlong Fu3\n1 University of Science and Technology Beijing\nzhicheng.huang@xs.ustb.edu.cn, fdm ustb@ustb.edu.cn\n2 Sun Yat-sen University\nzengzhy5@mail2.sysu.edu.cn\n3 Microsoft Research\n{bei.liu, jianf}@microsoft.com\nAbstract. We propose Pixel-BERT to align image pixels with text by\ndeep multi-modal transformers that jointly learn visual and language\nembedding in a uniﬁed end-to-end framework. We aim to build a more\naccurate and thorough connection between image pixels and language\nsemantics directly from image and sentence pairs instead of using region-\nbased image features as the most recent vision and language tasks. Our\nPixel-BERT which aligns semantic connection in pixel and text level\nsolves the limitation of task-speciﬁc visual representation for vision and\nlanguage tasks. It also relieves the cost of bounding box annotations\nand overcomes the imbalance between semantic labels in visual task and\nlanguage semantic. To provide a better representation for down-stream\ntasks, we pre-train a universal end-to-end model with image and sentence\npairs from Visual Genome dataset and MS-COCO dataset. We propose\nto use a random pixel sampling mechanism to enhance the robustness\nof visual representation and to apply the Masked Language Model and\nImage-Text Matching as pre-training tasks. Extensive experiments on\ndownstream tasks with our pre-trained model show that our approach\nachieves state-of-the-art results in downstream tasks, including Visual\nQuestion Answering (VQA), image-text retrieval, Natural Language for\nVisual Reasoning for Real (NLVR 2). Particularly, we boost the perfor-\nmance of a single model in VQA task by 2.17 points compared with\nSOTA under fair comparison.\nKeywords: Vision and Language, Representation Learning\n1 Introduction\nWith the success of self-supervised learning applied for representation learning\nin natural language process ﬁeld [34,9], recent research has addressed the task of\n⋆ Equal contribution. This work was conducted when Zhicheng Huang and Zhaoyang\nZeng were research interns at Microsoft Research.\narXiv:2004.00849v2  [cs.CV]  22 Jun 2020\n2 Z. Huang et al.\nQ: Is the girl touching the ground?\nA: No\nQ: Is the animal moving?\nA: Yes\nExample (A) Example (B) Example (C)\nQ: What is the plane doing?\nA: Taking off\nFig. 1.Examples of images, questions (Q) and answers (A) in VQA2.0 dataset. In\nthese cases, region-based visual features cannot well handle the questions.\ncross-modality learning, especially in vision and language [6,18,19,21,22,29,33,41],\nin a similar self-supervised learning way. Pre-training is widely used in those\nworks to provide a strong representation for both vision and language in cross-\nmodality domain. Most of them utilize BERT-based language feature and region-\nbased visual feature as the input for joint embedding learning in pre-trained\nmodels.\nThe semantic gap between diﬀerent modalities has always been treated as\none of the most important challenges in cross-modality research. Early works in\nvision and language, such as Visual Question Answering (VQA) [3] and image\ncaptioning [35], utilize CNN features directly extracted from a pre-trained model\non image classiﬁcation task. Later on, with the introduction of Visual Genome\nDataset [16] and the proposal of Bottom-Up and Top-Down Attention model\n[2], most recent vision and language methods, including the pre-trained mod-\nels mentioned above, utilize region-based visual features extracted from object\ndetection model (e.g., Faster R-CNN) for better performance. However, region-\nbased visual feature extractors are designed for speciﬁc visual tasks (e.g. object\ndetection), and this will cause an information gap with language understanding.\nSome important factors of visual information are lost, such as shapes of ob-\njects, spatial relations between objects with overlap, etc. Moreover, the feature\nrepresentation capability is limited to the given categories of such task-speciﬁc\nmodel while visual information of much broader semantics, such as scene and\nsentiment, are lost in the object detection model. We show some examples that\nregion-based visual features cannot well handle in Fig. 1. In Example (A), it is\ndiﬃcult for object detection models to obtain the status of the plane. For Ex-\nample (B), even though we can detect the “girl” and “ground”, since there is\noverlap between their regions, it will be even hard for further fusion embedding\nmodels to judge the actual spatial relation given their bounding boxes. Similarly\nin Example (C), with only visual features of “giraﬀe”, it is diﬃcult to infer the\nstatus of the animals.\nExisting methods that use region-based visual features and language embed-\nding as input of Transformer for cross-modality joint learning are limited to the\nPixel-BERT 3\nvisual semantics represented by the visual features. Thus, we step out of the\nbounding box to make the full power of visual information in images for vision\nand language learning. We propose Pixel-BERT that learns to align image pix-\nels with text to build a more thorough semantic embedding between visual and\ntextual information. Pixel-BERT consists of three parts: a fully convolutional\nneural network (CNN) that takes image pixels as input for visual embedding\nlearning, a word-level token embedding based on BERT, and multi-modal trans-\nformers for jointly learning of visual and language embedding.\nTo learn a universal representation for most vision and language tasks, we\nﬁrst pre-train our model with image-sentence pair dataset similar to other cross-\nmodality pre-training methods [6,18,19,21,22,29,33,41]. Two pre-training tasks\nand one pre-training mechanism are used in our pre-training procedure. For\nlanguage, we follow other pre-training works [6,29,33] to use Masked Language\nModeling (MLM) for the prediction of masked tokens with surrounding text and\nimage. For vision, we propose a random pixel sampling mechanism to make up\nfor the diﬃculty of predicting pixel-level features. The random pixel sampling\nmechanism improves the robustness of visual feature learning and overcomes the\nproblem of overﬁtting. For vision and language interaction, we follow [6] to apply\nImage-Text Matching (ITM) to classify whether an image and sentence pair is\nmatched or not.\nThe contributions of this paper are summarized as follows:\n– We propose Pixel-BERT that consists of a CNN-based visual encoder and\ndeep multi-modal transformers to jointly learn visual and language embed-\nding. We are the ﬁrst to consider to align vision and language semantics in\npixel and text level using self-supervised learning.\n– We use our model in pre-training manner and propose a random pixel sam-\npling mechanism to improve the robustness of visual representation learning.\n– Extensive experiments demonstrate the eﬀectiveness of our approach by\nachieving state-of-the-art performances in various tasks, including VQA,\nImage-Text Retrieval and NLVR2. In particular, our approach improves the\nsingle model’s performance of VQA by 2.17 points compared with previous\nSOTA [6] in fair comparison and even higher than its larger model.\n2 Related Works\n2.1 Pre-training Mechanism\nFor vision and language tasks, a better understanding of semantics is impor-\ntant to get better joint representation. For visual-content understanding, several\nbackbone models [13,32,38] have been proposed for pure visual understanding,\nwhich have shown their eﬀectiveness on large datasets [8]. Pioneering work [24]\nalso shows the generalizability of pre-trained backbone models by ﬁne-tuning\nthem on diﬀerent downstream tasks. In terms of language understanding, we\nhave witnessed rapid progress towards building a universal backbone model with\nlarge-scale contextualized pre-training [7,9,23] in recent years. They improved\n4 Z. Huang et al.\nthe performances on various tasks to signiﬁcant levels. For cross-modality re-\nsearch, many methods [6,18,19,21,22,29,31,33,41] have been proposed recently.\nThey focus on leaning the visual and sentence dense connection between diﬀer-\nent modalities. Existing methods can be clustered into two groups based on their\nnetwork structure. Some works [21,33] utilize two-stream neural networks based\non the Transformer [34]. The two-stream neural networks process visual and\nlanguage information respectively and fuse them afterward by another Trans-\nformer layer. On the other hand, there are some methods [1,6,18,22,29] apply\nsingle-stream neural network. They use BERT [9] to learn a bi-directional joint\ndistribution over the detection bounding box feature and sentence embedding\nfeature. The diﬀerences among them are the training method, loss function, and\ndatasets. Pixel-BERT is categorized into second group while our way of visual\nembedding is diﬀerent from all those methods.\n2.2 Visual Feature Embedding in Vision and Language Tasks\nCross-modality tasks, such as VQA [11], image captioning [39], require under-\nstanding of both sentence and visual semantics. Early method [5] directly adopts\nCNN features extracted from pre-trained classiﬁcation model as visual repre-\nsentation. Later on, with the introduction of Visual Genome Dataset [16] and\nthe proposal of Bottom-Up and Top-Down Attention model [2], most recent\nresearch related to vision and language utilize region-based visual features ex-\ntracted from object detection models (e.g., Faster R-CNN [24]) for better per-\nformance [6,15,22,33]. With those methods, the semantic of visual features is\nlimited by Visual Genome detection categories. While the language domain con-\ntains much more semantic information. One key diﬀerence between our model\nand the other methods is the visual semantic embedding methods. Instead of\nusing detection bounding box features as the visual semantic representation for\nvisual and language embedding learning, we combine the visual encoder repre-\nsentation learning network into one framework and input the source image as\nthe visual input. We use this model for cross-modality pre-training for learning\nricher visual semantics.\n3 Approach\nThe overall architecture of our proposed Pixel-BERT, an end-to-end framework\nwith CNN-based visual encoder and cross-modal Transformers for visual and\nlanguage embedding learning, is illustrated in Fig. 2. Image-sentence pairs are\ntaken as input to produce joint embedding features. The whole network can be\nend-to-end pre-trained by MLM and ITM tasks, and is suitable to be applied to\ndownstream tasks.\nIn this section, we will ﬁrst revisit the Transformer model in Sec.3.1, ex-\nplain our model architecture of Pixel-BERT in detail in Sec.3.2 and pre-training\nprocedure in Sec.3.3.\nPixel-BERT 5\nCNN\nBackbone\nCNN-based Visual Encoder\n[CLS] The … a [MASK] umbrell\na [SEP]\nSentence Encoder \nTransformers\nCross-Modality \nAlignment\n[V]\n[V]\n…\n[V]\n[V]\nConv\nPooling\nRandom \nSampling\nPixel Feature Embedding\nEmbedding\nToken\nPosition\n[CLS]\nThe\n…\na\n[MASK]\numbrella\n[SEP]\n[CLS]\nthe\n…\na\n[MASK]\numbrella\n[SEP]\n[V]\n[V]\n…\n[V]\n[V]\nImage-Text \nMatching (ITM)\nMasked Language \nModel (MLM)\nSemantic\nSemantic\nEmbedding\nElementwise Sum\n[ ]   Special Token\nThe woman held a black \numbrella\nPre-Training \nTasks\nPixel-BERT\n[V]  Visual Token\n[MA TCH]\nblack\nFig. 2.Pixel-BERT: The model contains a visual feature embedding module, a sentence\nfeature embedding module, and a cross-modality alignment module. Pixel-BERT takes\nimage-sentence pairs as input, and outputs the attention features of each input element.\nImages are passed into a pixel feature embedding module pixel by pixel and sentences\nare fed into a sentence feature embedding module token by token. The model can be\npre-trained by MLM and ITM tasks, and can be ﬂexibly applied to downstream tasks\n(e.g. VQA, retrieval, etc).\n3.1 Revisit Transformer\nPixel-BERT adopts the BERT [9] as cross-modality alignment module. BERT\nis a multi-layer bidirectional Transformer encoder, which is able to model the\ndependency of all input elements. Before introducing our Pixel-BERT, we ﬁrst\nrevisit the architecture of Transformer.\nThe two key operations in the basic Transformer module are self-attention\nand feed-forward. Given the input X ∈Rn×d, where n is the element number\nand dindicates the feature dimension, we ﬁrst get query Q, key K and value V\nfrom the input by\nQ= WqX,K = WkX,V = WvX, (1)\nwhere Wq, Wk and Wv are corresponding weight matrices. We compute the\nattention output Xatt by\nA= softmax(QKT\n√\nd\n),\nXatt = AV,\n(2)\nwhere Aindicates the self-attention weight of each input element. The output\nis calculated by a feed-forward network as follows:\nXout = FFN(Xatt), (3)\n6 Z. Huang et al.\nwhere FFN consists of a group of fully-connected layers with ReLU activation\nfunction as in [34]. Above operations build dense connections among all input\nelements, including each element with itself.\nIn cross-modality tasks, the input elements come from visual and language\ndomains. We propose to build dense connections among both intra-domain (i.e.\nimage-image, sentence-sentence) and inter-domain (i.e. image-sentence) by Trans-\nformer, which will be explained in detail in Sec. 3.2.\n3.2 Model Architecture\nFor vision and language tasks, we have two types of inputs from diﬀerent modal-\nities. Natural language is usually in the form of a sentence which can be split\ninto a sequence of words. We follow [6,18,21,22,29,33,41] to ﬁrst tokenize each\nword in the sentence, and embed each token into a vector. The input of the\nvisual domain is usually an image. Most recent methods represent the visual in-\nput by extracting region-based features with object detection model like Faster\nR-CNN[2]. However, such region-based visual feature extractor is designed for\nspeciﬁc visual tasks (i.e., object detection), which will lead to an information\ngap with language understanding. Speciﬁcally, a bounding box is in the shape of\na rectangle, which may include noisy background and miss the shape and spatial\nrelation information. Besides, the feature representation capability is limited by\nthe provided categories of such task-speciﬁc model. Moreover, visual information\nabout broader semantics such as scene and sentiment is also lost in the object\ndetection model. To fully utilize visual information of the original image, we pro-\npose an end-to-end framework for vision and language tasks by learning visual\nembedding from pixels, named Pixel-BERT.\nSentence Feature EmbeddingWe follow BERT [9] to encode the language\ninformation of a sentence. Given a sentence as input, we ﬁrst split it into a\nsequence of words, and use WordPiece to tokenize each word into token. We\nthen adopt an embedding matrix to embed each token into a vector. Here we\nuse w = {w1,w2,...,w n}∈ Rd to represent the embedded sequence, where n\nindicates the sequence length, anddis the embedding dimension. We follow other\nBERT-based language methods and add the positional embedding to encode\nthe position information. The ﬁnal language representation of the sentence is\n{ˆw1, ˆw2,··· , ˆwn}. For each of the representation at position i, it is calculated by\nˆwi = LayerNorm(wi + pi + sw), (4)\nwhere pi indicates the embedding vector at position i, sw is a semantic em-\nbedding vector and LayerNorm is a normalization function described in [4].\nSince the summation of position and semantic embedding is the mathematic\nequivalence to one embedding, we will omit the sw term in our implementation.\nImage Feature Embedding Most recent vision and language methods fol-\nlow Bottom-Up and Top-Down Attention[2] to extract visual features by Faster\nPixel-BERT 7\nR-CNN [24] trained on Visual Genome dataset. The detector extracts region\nfeatures by ﬁrst detecting regions under pre-deﬁned categories, and then uses\nthe features before the ﬁnal classiﬁer as the output. The representation ability\nof such extracted features will be limited to the detection categories.\nTo overcome the limitation of task-speciﬁc categories, shapes and borders,\nwe learn from pixels to represent an image instead of using bounding boxes.\nThe pixel features are learned by a CNN visual backbone such as ResNet[13].\nGiven an input image I, we ﬁrst use CNN backbone to extract its feature, then\nﬂat the feature along the spatial dimension. We denote the ﬂatten feature as\nv = {v1,v2,...,v k}∈ Rd, where k indicates the number of feature pixels. The\nvisual embedding feature {ˆv1,ˆv2,..., ˆvk}can be computed by\nˆvi = vi + sv, (5)\nwhere sv is a semantic embedding vector to distinguish the diﬀerence with lan-\nguage embedding. Since all pixels share the same sv, this embedding vector can\nbe considered as a bias term to be combined with the CNN backbone. In our\nimplementation, we adopt ResNet or ResNeXt as backbone, and add a 2 ×2\nmax pooling layer to reduce the spatial dimension of visual feature maps. The\nspatial size of input image I will be down-sampled by 64 times in total.\nCross-Modality Module We adopt Transformer to learn cross-modality at-\ntention between image pixels and language tokens. After obtaining sentence em-\nbedding vectors and pixel features, we combine all vectors to construct the input\nsequence. We also adding two special tokens [CLS] and [SEP] for learning joint\nclassiﬁcation feature and specifying token length, respectively. The ﬁnal input\nsequence to the joint-learning Transformer is formulated as\n{[CLS], ˆw1, ˆw2,··· , ˆwn,[SEP],ˆv1,ˆv2,··· ,ˆvk}. (6)\nThe CNN backbone for visual representation learning and the Transformer for\nlanguage representation learning is combined into a single model, which is end-to-\nend trainable. When we apply learning supervision on the output of Transformer,\nthe gradient can backward to the CNN backbone, and thus the learned visual\nfeatures will be more suitable to the target task learning by breaking the domain\ngap between visual and sentence domain.\n3.3 Pre-Training\nIn order to learn a universal visual and sentence representation for vision and\nlanguage related tasks, we apply the self-supervised method to pre-train a model\non a large aggregated dataset. We follow [6,18,21,22,29,33,41] to conduct two pre-\ntraining tasks, including Masked Language Modeling (MLM) and Image-Text\nMatching (ITM). Compared with existing methods that rely on the detection\nmodel to extract region-based visual features, our model uses the source image\nas input to conduct pre-training tasks.\n8 Z. Huang et al.\nMasked Language ModelingTo pre-train the model and build the mapping\nbetween language tokens and visual contents, we take the Masked Language\nModel (MLM) task in cross-modality domain. Speciﬁcally, we randomly mask\nlanguage tokens with a probability of 0 .15, and require the model to predict\nthe masked tokens based on other non-masked tokens and visual tokens. The\nlearning target LMLM can be formulated as\nLMLM(θ) = −E(w,I)∼Dlog Pθ(wm|w\\m,I), (7)\nwhere wm indicates the masked token,θis the model parameters, andP indicates\nthe likelihood generated function.\nDiﬀerent from single-modality tasks with BERT where the masked tokens are\nonly predicted from the surrounding non-masked tokens in language domain, our\nmodel can handle the cross-modality scenario where ambiguity may occur using\nonly language modality. MLM task can encourage the model to infer the masked\ntokens from both language and visual tokens, which can help build the mapping\nbetween language modality and visual modality.\nImage-Text MatchingSome downstream tasks, such as image-text retrieval,\nrequire the model to distinguish whether a sentence can well describe an image,\nor in other words, whether they are matched or not. To enhance the cross-\nmodalities matching, we adopt image-text matching (ITM) task for pre-training\nas previous work [6]. During training, we sample all image-sentence pairs pro-\nvided by datasets, and consider them as positive samples. We also randomly\nshuﬄe the datasets consider the unmatched image-sentence pairs as negative\nsamples. To prevent learning bias, we adopt the same number of positive sam-\nples and negative samples.\nWe apply a binary classiﬁer on the joint embedding feature of [CLS] token\nto classify whether the input image and sentence are matched or not. ITM task\nis driven by following loss function:\nLITM(θ) = −E(w,I)∼D[ylog Sθ(w,I) + (1−y) log(1−Sθ(w,I))], (8)\nwhere y ∈{0,1}indicates whether the image and sentence is matched, and S\nindicates the classiﬁcation score generated function.\nPixel Random SamplingTo improve the robustness of feature learning and\navoid overﬁtting, inspired by dropout[28], we propose to randomly sample feature\npixels during pre-training. At each iteration, after extracting pixel features, we\nwill randomly sample a part from them and feed it into Transformer. Such\npixel random sampling can beneﬁt the model training in two ways. First, it can\nencourage the model to learn semantic knowledge from incomplete visual input,\nand thus enhance the robustness. Second, it reduce the number of input elements,\nso that it can reduce the computation cost and accelerate the training progress.\nWe will randomly sample a ﬁxed number of 100 pixels from the feature map\nfor each input image in our experiments. Note that such pixel random sampling\nPixel-BERT 9\nTask Dataset #Imgs #Text Training Testing Metric\nPretrain VG 101K 5.06M train+val - -\nCOCO 106K 533K train+restval - -\nVQA VQA2.0 204K 1.1M train+val test-dev/test-std VQA-score\nNLVR2 NLVR2 214K 107K train dev/test Accuracy\nIR & TR COCO 92K 460K train+restval test Recall@1,5,10Flickr30K 32K 160K train+restval test\nTable 1.Statistics of diﬀerent datasets, data splits and evaluation metrics used for\npre-training and downstream tasks.\nstrategy is only applied in pre-training stage. The ﬁrst reason is that random\nsampling in downstream tasks may lead to information missing since the ﬁne-\ntuning stage only lasts for a few epochs, and another reason is that we need to\nmake sure that inputs of downstream tasks training and testing are consistent.\n4 Experiments\n4.1 Pre-training\nDatasets We pre-train our Pixel-BERT on two large-scale image-sentence datasets:\nMS-COCO [20], Visual Genome [16]. We utilize the image-level caption anno-\ntations in MS-COCO and region-level caption annotations in Visual Genome\nas training data for pre-training. For Visual Genome dataset, we adopt data in\nboth train and val for training. For MS-COCO, we follow [14] to split the whole\ndataset into train, restval, val and test. Since one of our downstream tasks,\nimage-text retrieval, is conducted on MS-COCO dataset, to avoid data leak, we\nuse train and restval splits for training. The statistic of training samples can be\nfound in the ﬁrst two rows of Table 1.\nImplementation DetailsDuring pre-training, our Pixel-BERT receives a batch\nof image-sentence pairs as input in each iteration. We ﬁrst use the WordPiece\ntokenizer [37] as used in BERT to split each sentence into language tokens. We\nuse ResNet-50 as visual backbone for ablation analysis, and follow [27,29] to\nadopt more powerful ResNeXt-152 to obtain better performance. We use pub-\nlic accessible pre-trained model on ImageNet [8] to initialize the parameters of\nthe visual backbone. We resize the shorter edge of input images to 800, and\nlimit the longer edge lower than 1333 when using ResNet-50 as visual backbone.\nWhen using ResNeXt-152, considering the GPU memory usage, we adjust the\nsize of the shorter edge and longer edge limits to 600 and 1000, respectively. As\nclaimed in [40], the CNN visual backbone and Transformer may favor diﬀerent\nkinds of the optimizer, we adopt diﬀerent optimizer settings for visual backbone\nand Transformer. Speciﬁcally, we use SGD with learning rate 1e −2 and weight\ndecay 5e−4 to optimize the CNN backbone, and adopt AdamW with learning\n10 Z. Huang et al.\nModel test-dev test-std\nMUTAN[5] 60.17 -\nBUTD[2] 65.32 65.67\nViLBERT[21] 70.55 70.92\nVisualBERT[19] 70.80 71.00\nVLBERT[29] 71.79 72.22\nLXMERT[33] 72.42 72.54\nUNITER[6] 72.27 72.46\nPixel-BERT (r50) 71.35 71.42\nPixel-BERT (x152) 74.45 74.55\nTable 2. Evaluation of Pixel-BERT\nwith other methods on VQA.\nModel dev test-P\nImage Only[30] 51.6 51.9\nCNN+RNN[30] 53.5 52.4\nMaxEnt[30] 54.1 54.8\nVisualBERT[19] 67.4 67.0\nLXMERT†[33] 74.9 74.5\nUNITER†[6] 75.4 76.0\nUNITER‡[6] 77.1 77.9\nPixel-BERT† (r50) 71.7 72.4\nPixel-BERT† (x152) 76.5 77.2\nTable 3. Evaluation on NLVR 2\ntask.† indicates use paired method.\n‡ indicates use pair-biatt method.\nrate 1e−4 and weight decay 1e−2 as Transformer optimizer. We pre-train Pixel-\nBERT on 64 NVIDIA Tesla V100 GPUs with the batch size 4096 samples for 40\nepochs. We decay the learning rate by 10 at 25 th and 35th epoch.\n4.2 Downstream Tasks\nWe evaluate our model on several downstream vision and language tasks, includ-\ning Visual Question Answering (VQA) and Natural Language for Visual Rea-\nsoning for Real (NLVR 2) on VQA 2.0 [12], NLVR 2 [30] datasets, respectively.\nWe also conduct experiments on image-to-text and text-to-image retrieval tasks\non Flickr30K [38] and MS-COCO [20] dataset. The detailed statistics including\ndataset splits, numbers of training/validation/testing data and evaluation met-\nrics in all used datasets can be found in Table 1. We report the performance of\nPixel-BERT under two diﬀerent visual backbone setting in later tables, where r50\nindicates ResNet-50 and x152 indicates ResNeXt-152. Since our model adopts\n12-Layer Transformer as a language module, we mainly compare our experiments\nwith other approaches under the same Transformer setting.\nVisual Question AnsweringIn Visual Question and Answering (VQA) task,\nPixel-BERT takes an image and a question as input and predicts an answer as\noutput. We model it as a classiﬁcation problem by learning multi-layer perception\nfrom the [CLS] token via binary cross-entropy loss. We follow the same optimizer\nsetting as pre-training. We ﬁne-tune the model for 18 epochs on 16 NVIDIA\nTesla V100 GPUs with batch size 256. The initial learning rates are the same as\npre-training, and we decay the learning rate by 10 at 12 th and 16th epoch.\nWe report our experiment results on VQA task in Table 2. We compare our\napproach with recent state-of-the-art approaches. From Table 2, we can ﬁnd\nthat our approach with ResNet-50 as visual backbone achieves 71.35 score on\ntest-dev split, which already outperforms ViLBERT [21] and VisualBERT [19]\nwhich using more powerful visual backbone like ResNet-101 or ResNeXt-152.\nWhen equipped with ResNeXt-152 backbone, our model Pixel-BERT achieves\nPixel-BERT 11\nModel TR IR\nR@1 R@5 R@10 R@1 R@5 R@10\nVSE++[10] 52.9 80.5 87.2 39.6 70.1 79.5\nSCAN[17] 67.4 90.3 95.8 48.6 77.7 85.2\nSCG[26] 71.8 90.8 94.8 49.3 76.4 85.6\nPFAN[36] 70.0 91.8 95.0 50.4 78.7 86.1\nViLBERT[21] - - - 58.2 84.9 91.5\nUnicoder-VL[18] 86.2 96.3 99.0 71.5 90.9 94.9\nUNITER[6] 84.7 97.1 99.0 71.5 91.2 95.2\nours (R50) 75.7 94.7 97.1 59.8 85.5 91.6\nours (X152) 87.0 98.9 99.571.5 92.1 95.8\nTable 4. Evaluation of Pixel-BERT with other methods for image-to-text retrieval\n(TR) and text-to-image retrieval (IR) on Flickr30K dataset.\n74.45 on test-dev split and 74.55 on test-std split, which signiﬁcantly outper-\nforms all existing works. It worth noting that this result is even higher than\nthe performance of UNITER (Large), which using 24-Layer Transformer as a\nlanguage module and get 73 .40 score on VQA test-std split. This obvious im-\nprovement demonstrates that learning visual and language attention in image’s\npixel-level can beneﬁt the visual encoder representation and enhance the visual\nand language embedding learning afterward.\nNatural Language for Visual Reasoning for RealNatural Language for\nVisual Reasoning for Real (NLVR2) task requires a model to predict whether a\nlanguage description is related to a given pair of images. In our model, we feed\ntwo image-language pairs into Pixel-BERT to get two embedding vectors from\nthe [CLS] tokens, and use their concatenation to learn a classiﬁer over “true”\nor “false” by cross-entropy loss. The optimizer, epoch number and learning rate\nsettings are all the same as VQA settings explained above. And the batch size\nis the half of the VQA.\nWe evaluate NLVR2 on both dev and test-P split. Diﬀerent from pre-training\ntasks and other downstream tasks, NLVR 2 receives a pair of images at once.\nFrom the results shown in Table 3, we can ﬁnd that Pixel-BERT obtains 76.5\naccuracy on dev split and 77.2 accuracy ontest-P split. Our setting of composing\ntwo image-language pairs is the same as LXMERT and the “Pair” setting in\nUNITER, and from the comparison in Table 3 we can ﬁnd that Pixel-BERT\noutperforms them. These results show that Pixel-BERT can be also adapted to\nother similar input formats.\nImage-Text Retrieval We consider the retrieval task as a ranking problem\nsimilar to other works [6,18,29]. During training, for each image in an image-\nsentence pair, we use the ground-truth caption in the pair as the positive sample\nand randomly sample 20 unrelated captions from other pairs to make negative\nsamples. We predict the score of whether a pair is related by a fully-connected\n12 Z. Huang et al.\nModel TR IR\nR@1 R@5 R@10 R@1 R@5 R@10\n1K Test set\nVSE++[10] 64.6 90.0 95.7 52.0 84.3 92.0\nSCAN[17] 72.7 94.8 98.4 58.8 88.4 94.8\nSCG[26] 76.6 96.3 99.2 61.4 88.9 95.1\nPFAN[36] 76.5 96.3 99.0 61.6 89.6 95.2\nUnicoder-VL[18] 84.3 97.3 99.3 69.7 93.5 97.2\nours (R50) 77.8 95.4 98.2 64.1 91.0 96.2\nours (X152) 84.9 97.7 99.371.6 93.7 97.4\n5K Test set\nVSE++[10] 41.3 71.1 81.2 30.3 59.4 72.4\nSCAN[17] 50.4 82.2 90.0 38.6 69.3 80.4\nSCG[26] 56.6 84.5 92.0 39.2 68.0 81.3\nUnicoder-VL[18] 62.3 87.1 92.8 46.7 76.0 85.3\nUNITER[6] 63.3 87.0 93.1 48.4 76.7 85.9\nPixel-BERT (r50) 53.4 80.4 88.5 41.1 69.7 80.5\nPixel-BERT (x152) 63.6 87.5 93.650.1 77.6 86.2\nTable 5. Evaluation of Pixel-BERT with other methods for image-to-text retrieval\n(TR) and text-to-image retrieval (IR) on MS-COCO Dataset.\nlayer on the representation of all [CLS] tokens, and apply softmax cross-entropy\nloss to encourage that the positive image-caption pair to obtain the highest score.\nWe only backward the gradient on 5 negative samples with the highest loss for\neach image sample. Since the retrieval task is closely related to ITM task in pre-\ntraining, we only ﬁne-tune the parameters in Transformer. We adopt AdamW\nwith 1e−4 learning rate and 1e −2 weight decay as optimizer. We ﬁne-tune the\nmodel on 8 NVIDIA Tesla GPUs with a batch size of 64 samples per GPU. For\nFlickr30K, we train 10 epochs and decay the learning rate at 6 th epoch. For\nMS-COCO we train 4 epochs and decay the learning rate at 2 nd epoch.\nWe report recall@1, 5, 10 on both text-to-image retrieval (IR) and image-\nto-text retrieval (TR) sub-tasks to evaluate our approach. Table 4 shows the\n1K testing results on Flickr30K, and Tabel 5 shows the 5-fold 1K testing re-\nsults and 5K testing results on MS-COCO. We mainly compare Pixel-BERT\nwith Unicoder-VL and UNITER which both adopt 12-layers Transformer as a\nlanguage module. For image-to-text retrieval subtask, we obtain at least 0.6 per-\nformance gain on MS-COCO 1K test set, and 0.3 on MS-COCO 5K test set for\nrecall@1. And for the text-to-image retrieval subtask, we achieve more signiﬁ-\ncant results with improvements of at lease 1.9 on MS-COCO 1K testing set and\n1.7 on MS-COCO 5K testing set compared with Unicoder-VL and UNITER.\nThis is due to that the text-to-image retrieval task focuses more on the global\ndescription of an image, and our architecture can encourage the model to learn\nattention across language and image pixels.\nPixel-BERT 13\n# Visual Pre-traning Sampling VQA TR IR NLVR2\nBackbone Tasks Method test-dev val val dev\n1 ResNet-50 - Random 63.50 52.4 39.6 52.0\n2 ResNet-50 ITM Random 65.24 69.0 55.5 51.9\n3 ResNet-50 MLM Random 71.13 67.1 52.7 70.9\n4 ResNet-50 MLM+ITM ALL 70.84 72.0 57.7 71.3\n5 ResNet-50 MLM+ITM Random 71.35 75.7 59.8 71.7\n6 ResNext-152 MLM+ITM Random 74.45 87.0 71.5 76.5\nTable 6.Ablation study results on VQA, Flickr30K retrieval and NLVR2 downstream\ntasks. We evaluate the eﬀectiveness of pre-training tasks and our proposed sampling\nmethod. The ﬁrst row indicates training downstream tasks without pre-training. We\nreport VQA score for VQA task, Recall@1 for TR and IR, and accuracy for NLVR 2.\n4.3 Ablation Study\nWe conduct ablation experiments to evaluate the eﬀectiveness of each component\nof our Pixel-BERT. Since the performance of the pre-training model can not be\nwell measured by a single metric, we use the evaluated results on downstream\ntasks for evaluation. The ablation study results can be found in Table 6. We\nﬁrst evaluate the eﬀectiveness of each pre-training tasks. From the comparison\nof model (1) and model (2) (3), we can ﬁnd that both MLM and ITM can sig-\nniﬁcantly improve the performance on almost all downstream tasks. Speciﬁcally,\nfor VQA, MLM and ITM can bring about 7 .6 and 1 .6 improvement. For the\nretrieval task, ITM contributes more and brings at least 13 .0 improvement on\nboth TR and IR sub-tasks. NLVR 2 relies heavily on MLM task, without which\nthe training even can not converge. The eﬀectiveness of pre-training is consis-\ntent with the conclusions drawn in other works [6,29]. And their combination\nin model (5) can further improve the performance of each task than single task.\nFrom the comparison of model (4) and model (5), we can ﬁnd that our proposed\nrandomly pixel sampling method can contribute 0 .5 score on VQA, about 2 .0\nscore on retrieval tasks and 0.4 score on NLVR2. This demonstrates the eﬀective-\nness of our pixel random sampling mechanism. In model (6), we follow [27,29]\nto replace the visual backbone with ResNext-152, and the results show that our\nmodel with a powerful visual backbone will further accelerate the performance\nby a large margin.\n4.4 Visualization\nTo further check whether our approach Pixel-BERT can well learn the visual\nrepresentation by cross-modality attention across language and pixels, we visu-\nalize some intermediate results of attention maps on examples from MS-COCO\nval set. The visualization results can be found in Fig. 3. From the result of Case\n(A), we can see that the response areas of token “dog”, “grass” and “frisbee”\nare actually distributed on the correct region. For Case (B), we can ﬁnd that\nalthough “cutting” is a verb, it can attend to the most related region in which\nthe action of “cutting” is performed with a knife. From Case (C), we ﬁnd that\n14 Z. Huang et al.\nCase (A): a dog sits on the grass with its frisbee\ndog grass frisbee\nCase (B): a man cutting up carrots in long strips\nman cutting carrots\nCase (C): a cat sitting inside a purse in a room\ncat purse room\nFig. 3.Visualization of attention regions extracted from the ﬁrst Transformer layer of\nPixel-BERT. The attention regions are extracted by using the speciﬁc token as query\nand pixel features as keys. Highlight areas indicate regions with high attention score.\nthe token “room” can attend to the correct region in the image, which is diﬃcult\nto be represented by a bounding box. Although we did not apply any spatial\nsupervision (e.g., bounding box annotations) to guide the attention learning,\nthe results from Fig. 3 show that with well-deﬁned tasks, our Pixel-BERT can\nwell learn the visual representation in region level with cross-modality learn-\ning. This result also brings a lot of possibilities for further research to study\nwhether cross-modality learning can help the semantic understanding of visual\ninformation conversely.\n5 Conclusion and Discussion\nPre-training mechanism has shown its eﬀectiveness in vision and language do-\nmain. In this paper, we discuss the visual embedding method that is commonly\nused in existing works and aim to solve the limitation of region-based visual\nrepresentation. We propose CNN-based Visual Encoder and combine it with\nmulti-modal Transformers to construct Pixel-BERT in an end-to-end manner\nand build more accurate and more thorough embedding between visual and lin-\nguistic contents in pixel and text level. We use pixels of an image as input and\nPixel-BERT 15\napply a random pixel sampling mechanism for the robustness of visual embed-\nding learning. We build a pre-training model based on Pixel-BERT to learn a\nuniversal visual and language embedding on Visual Genome dataset and MS-\nCOCO dataset. Masked language model and image-text matching are two tasks\ndesigned for pre-training. We conduct downstream vision and language tasks\nwith our pre-trained model and achieve the best performances in most tasks,\nincluding VQA, NLVR2, image-to-text retrieval and text-to-image retrieval.\nWithout the restriction of annotated bounding boxes, our pre-trained model\nwith Pixel-BERT can provide much stronger representation for both images and\nsentences with a larger image-sentence pair dataset. We will consider to pre-\ntrain our model on Conceptual Caption Dataset [25] to further optimize the\nvisual and language embedding. Masked visual prediction is proposed in some\nworks. For example, [6] proposed three types of masked region modeling for the\nprediction of the masked region. In this paper, we use a random pixel sampling\nmechanism to replace this part due to the diﬃculty of pixel reconstruction com-\npared with regions. In the future, we will research on how to design and combine\nself-supervised tasks for visual contents in our current approach.\nReferences\n1. Alberti, C., Ling, J., Collins, M., Reitter, D.: Fusion of detected objects in text for\nvisual question answering. In: EMNLP. pp. 2131–2140 (2019)\n2. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,\nL.: Bottom-up and top-down attention for image captioning and visual question\nanswering. In: CVPR (June 2018)\n3. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,\nD.: Vqa: Visual question answering. In: ICCV (December 2015)\n4. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv (2016)\n5. Ben-Younes, H., Cadene, R., Cord, M., Thome, N.: Mutan: Multimodal tucker\nfusion for visual question answering. In: ICCV. pp. 2612–2620 (2017)\n6. Chen, Y.C., Li, L., Yu, L., Kholy, A.E., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.:\nUniter: Learning universal image-text representations. arXiv0 (2019)\n7. Conneau, A., Lample, G.: Cross-lingual language model pretraining. In: NIPS. pp.\n7057–7067 (2019)\n8. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\nhierarchical image database. In: CVPR. pp. 248–255. Ieee (2009)\n9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. In: ACL. pp. 4171–4186 (2019)\n10. Faghri, F., Fleet, D.J., Kiros, J.R., Fidler, S.: Vse++: Improving visual-semantic\nembeddings with hard negatives. In: BMVC (2017)\n11. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in\nVQA matter: Elevating the role of image understanding in Visual Question An-\nswering. In: CVPR (2017)\n12. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In:\nCVPR. pp. 6904–6913 (2017)\n13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR. pp. 770–778 (2016)\n16 Z. Huang et al.\n14. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image\ndescriptions. In: CVPR. pp. 3128–3137 (2015)\n15. Kim, J.H., Jun, J., Zhang, B.T.: Bilinear attention networks. In: NIPS. pp. 1564–\n1574 (2018)\n16. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalan-\ntidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. IJCV 123(1), 32–73 (2017)\n17. Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked cross attention for image-text\nmatching. In: ECCV. pp. 201–216 (2018)\n18. Li, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.: Unicoder-vl: A universal encoder\nfor vision and language by cross-modal pre-training. In: AAAI (2019)\n19. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and\nperformant baseline for vision and language. arXiv (2019)\n20. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740–755.\nSpringer (2014)\n21. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks. In: NIPS. pp. 13–23 (2019)\n22. Qi, D., Su, L., Song, J., Cui, E., Bharti, T., Sacheti, A.: Imagebert: Cross-modal\npre-training with large-scale weak-supervised image-text data. arXiv (2020)\n23. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws.\ncom/openai-assets/researchcovers/languageunsupervised/language understanding\npaper. pdf (2018)\n24. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object de-\ntection with region proposal networks. In: NIPS. pp. 91–99 (2015)\n25. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In: ACL. pp.\n2556–2565 (2018)\n26. Shi, B., Ji, L., Lu, P., Niu, Z., Duan, N.: Knowledge aware semantic concept ex-\npansion for image-text matching. In: IJCAI. pp. 5182–5189 (2019)\n27. Singh, A., Goswami, V., Natarajan, V., Jiang, Y., Chen, X., Shah, M., Rohrbach,\nM., Batra, D., Parikh, D.: Pythia-a platform for vision & language research. In:\nSysML Workshop, NeurIPS. vol. 2018 (2018)\n28. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:\nDropout: a simple way to prevent neural networks from overﬁtting. JMLR 15(1),\n1929–1958 (2014)\n29. Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert: Pre-training of\ngeneric visual-linguistic representations (2019)\n30. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., Artzi, Y.: A corpus for reasoning\nabout natural language grounded in photographs. In: ACL. pp. 6418–6428 (2019)\n31. Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.: Videobert: A joint\nmodel for video and language representation learning. In: ICCV. pp. 7464–7473\n(2019)\n32. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\nVanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. pp. 1–\n9 (2015)\n33. Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations from\ntransformers. In: ACL (2019)\n34. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: NIPS. pp. 5998–6008 (2017)\nPixel-BERT 17\n35. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image\ncaption generator. In: CVPR (June 2015)\n36. Wang, Y., Yang, H., Qian, X., Ma, L., Lu, J., Li, B., Fan, X.: Position focused\nattention network for image-text matching. In: IJCAI (2019)\n37. Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun,\nM., Cao, Y., Gao, Q., Macherey, K., et al.: Google’s neural machine translation\nsystem: Bridging the gap between human and machine translation. arXiv (2016)\n38. Xie, S., Girshick, R., Doll´ ar, P., Tu, Z., He, K.: Aggregated residual transformations\nfor deep neural networks. In: CVPR. pp. 1492–1500 (2017)\n39. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,\nBengio, Y.: Show, attend and tell: Neural image caption generation with visual\nattention. In: ICML. pp. 2048–2057 (2015)\n40. Zhang, J., Karimireddy, S.P., Veit, A., Kim, S., Reddi, S.J., Kumar, S., Sra, S.:\nWhy adam beats sgd for attention models. arXiv (2019)\n41. Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J.J., Gao, J.: Uniﬁed vision-\nlanguage pre-training for image captioning and vqa. In: ICLR (2018)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8154115676879883
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6563100814819336
    },
    {
      "name": "Pixel",
      "score": 0.5810841917991638
    },
    {
      "name": "Natural language processing",
      "score": 0.47687870264053345
    },
    {
      "name": "Transformer",
      "score": 0.471770316362381
    },
    {
      "name": "Sentence",
      "score": 0.4580693244934082
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4170989692211151
    },
    {
      "name": "Computer vision",
      "score": 0.4024242162704468
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3232182264328003
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}