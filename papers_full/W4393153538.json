{
  "title": "Enhancing Job Recommendation through LLM-Based Generative Adversarial Networks",
  "url": "https://openalex.org/W4393153538",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2343752505",
      "name": "Yingpeng Du",
      "affiliations": [
        "Nanyang Technological University",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2097766642",
      "name": "Di Luo",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2109109241",
      "name": "Rui Yan",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2132718864",
      "name": "Wang Xiaopei",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2072390117",
      "name": "Hongzhi Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2698367452",
      "name": "Hengshu Zhu",
      "affiliations": [
        "Venus Medtech (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2021276105",
      "name": "Yang Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2082197670",
      "name": "Jie Zhang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2343752505",
      "name": "Yingpeng Du",
      "affiliations": [
        "Peking University",
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2097766642",
      "name": "Di Luo",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2109109241",
      "name": "Rui Yan",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2132718864",
      "name": "Wang Xiaopei",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2072390117",
      "name": "Hongzhi Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2698367452",
      "name": "Hengshu Zhu",
      "affiliations": [
        "Venus Medtech (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2021276105",
      "name": "Yang Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2082197670",
      "name": "Jie Zhang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285294723",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W2776933647",
    "https://openalex.org/W2740098507",
    "https://openalex.org/W3004578093",
    "https://openalex.org/W6735804486",
    "https://openalex.org/W6809658605",
    "https://openalex.org/W3034753566",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W2989031759",
    "https://openalex.org/W2798507773",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W6810554422",
    "https://openalex.org/W6794144907",
    "https://openalex.org/W4292420207",
    "https://openalex.org/W2952255427",
    "https://openalex.org/W4366733551",
    "https://openalex.org/W4225886166",
    "https://openalex.org/W3198359486",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2605350416",
    "https://openalex.org/W3094607200",
    "https://openalex.org/W4361193179",
    "https://openalex.org/W4379251438",
    "https://openalex.org/W4376632920",
    "https://openalex.org/W4376654514",
    "https://openalex.org/W4363672019",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4221153514",
    "https://openalex.org/W3153325943",
    "https://openalex.org/W2963323306",
    "https://openalex.org/W4225631575",
    "https://openalex.org/W4319792257",
    "https://openalex.org/W4386501849"
  ],
  "abstract": "Recommending suitable jobs to users is a critical task in online recruitment platforms. While existing job recommendation methods encounter challenges such as the low quality of users' resumes, which hampers their accuracy and practical effectiveness.With the rapid development of large language models (LLMs), utilizing the rich external knowledge encapsulated within them, as well as their powerful reasoning capabilities, is a promising way to complete users' resumes for more accurate recommendations. However, directly leveraging LLMs to enhance recommendation results is not a one-size-fits-all solution, as LLMs may suffer from fabricated generation and few-shot problems, which degrade the quality of resume completion. In this paper, we propose a novel LLM-based approach for job recommendation. To alleviate the limitation of fabricated generation for LLMs, we extract accurate and valuable information beyond users' self-description, which helps the LLMs better profile users for resume completion. Specifically, we not only extract users' explicit properties (e.g., skills, interests) from their self-description but also infer users' implicit characteristics from their behaviors for more accurate and meaningful resume completion. Nevertheless, some users still suffer from few-shot problems, which arise due to scarce interaction records, leading to limited guidance for high-quality resume generation. To address this issue, we propose aligning unpaired low-quality with high-quality generated resumes by Generative Adversarial Networks (GANs), which can refine the resume representations for better recommendation results. Extensive experiments on three large real-world recruitment datasets demonstrate the effectiveness of our proposed method.",
  "full_text": "Enhancing Job Recommendation through\nLLM-Based Generative Adversarial Networks\nYingpeng Du1,4 *‚Ä†, Di Luo2*, Rui Yan2, Xiaopei Wang3‚Ä†, Hongzhi Liu4‚Ä†,\nHengshu Zhu5, Yang Song6‚Ä† , Jie Zhang1\n1School of Computer Science and Engineering, Nanyang Technological University, Singapore\n2Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\n3School of Languages and Communication Studies, Beijing Jiaotong University, Beijing, China\n4School of Software and Microelectronics, Peking University, Beijing, China\n5Career Science Lab, BOSS Zhipin, Beijing, China\n6NLP Center, BOSS Zhipin, Beijing, China\ndyp1993@pku.edu.cn,di luo@ruc.edu.cn,ruiyan@ruc.edu.cn,wangxp@bjtu.edu.cn,liuhz@pku.edu.cn,\nzhuhengshu@kanzhun.com,songyang@kanzhun.com,zhangj@ntu.edu.sg\nAbstract\nRecommending suitable jobs to users is a critical task in\nonline recruitment platforms. Existing job recommendation\nmethods often encounter challenges such as the low quality\nof users‚Äô resumes, which hampers their accuracy and practical\neffectiveness. With the rapid development of large language\nmodels (LLMs), utilizing the rich knowledge encapsulated\nwithin them, as well as their powerful reasoning capabilities,\noffers a promising avenue for enhancing resume complete-\nness to achieve more accurate recommendations. However,\ndirectly leveraging LLMs is not a one-size-fits-all solution,\nas it may suffer from issues like fabricated generation and\nfew-shot problem, both of which can degrade the quality of\nresume completion. In this paper, we propose a novel LLM-\nbased GANs Interactive Recommendation (LGIR) approach\nfor job recommendation. To alleviate the limitation of fabri-\ncated generation, we not only extract users‚Äô explicit proper-\nties (e.g., skills, interests) from their self-description but also\ninfer users‚Äô implicit characteristics from their behaviors for\nmore accurate and meaningful resume completion. Neverthe-\nless, some users still suffer from the few-shot problem, which\narises due to scarce interaction records, leading to limited\nguidance for high-quality resume generation. To address this\nissue, we propose aligning unpaired low-quality resumes with\nhigh-quality generated counterparts using Generative Adver-\nsarial Networks (GANs), which can refine resume represen-\ntations for better recommendation results. Extensive experi-\nments on three large real-world recruitment datasets demon-\nstrate the effectiveness of our proposed method.\nIntroduction\nJob recommendation is an essential task in today‚Äôs online\nrecruitment platforms, significantly improving recruitment\nefficiency by accurately matching job seekers (aka users)\nwith suitable positions. Although existing job recommenda-\ntion methods (Le et al. 2019; Jiang et al. 2020; Hou et al.\n*These authors contributed equally.\n‚Ä†Corresponding authors.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n2022) have achieved considerable success in recent years,\nthey still face significant challenges, such as the low quality\nof user resumes and interference from the few-shot prob-\nlem (Gope and Jain 2017), hindering their practical accu-\nracy and efficiency. For example, some users may not invest\nsufficient effort in crafting their resumes or lack comprehen-\nsive self-awareness, resulting in incomplete and low-quality\ndescriptions of their skills and job preferences. Inspired by\nthe recent remarkable capabilities and rapid development of\nlarge language models (LLMs), it is intuitive to utilize their\nextensive knowledge, powerful text comprehension, and rea-\nsoning abilities to improve and rectify low-quality resumes.\nHowever, simply leveraging LLMs (Touvron et al. 2023;\nBrown et al. 2020) to enhance user resumes is not a one-\nsize-fits-all solution for job recommendation. Due to the\nwidespread fabrications and hallucinations within LLMs\n(Zhang et al. 2023), it is difficult to generate high-quality re-\nsumes without users‚Äô reliable interactive information. Fig.1\n(A) illustrates the resume generation process for a user us-\ning simple completion with a well-known LLM, ChatGPT.\nIt underscores that the generated results often contain exces-\nsive unrelated and fabricated information, rendering them\nunsuitable for recommendation. To alleviate this fabricated\ngeneration, we propose exploring users‚Äô interactive behav-\niors with recommender systems to mine their relevance to\nusers‚Äô abilities and preferences, thereby assisting the LLMs\nin better profiling users for resume completion. Specifically,\nusers generally possess particular job skills, residential ad-\ndresses, and educational backgrounds, which make them in-\nteract with jobs that contain corresponding responsibilities,\nlocations, and levels. As a result, we propose inferring users‚Äô\nimplicit characteristics (e.g., skills, preferences) from their\ninteraction behaviors to help LLMs profile users and gener-\nate high-quality resumes.\nAlthough exploring users‚Äô interactive behaviors can help\nLLMs better profile users, they may still suffer from the\nfew-shot problem, limiting the quality of resume comple-\ntion for certain users. Specifically, users with few interaction\nrecords (aka the long-tail effect) still face challenges with\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8363\n‚Ä¶\nSkills:\nUI Design, Interface Design, Product Interaction, Web Design\nProficient in Photoshop, Illustrator, Sketch\nProjects:\nRedesigned the user interface for a web application, resulting in a \n30% increase in user engagement and improved user satisfaction.\nCreated a mobile app interface that streamlined the user journey, \nleading to a 20% reduction in user drop -off rates.\nDeveloped a responsive website design that improved \naccessibility and usability across multiple devices.\n‚Ä¶\nGenerate a resume  based on the following information:\nTwo years of experience in UI work, interface design, product \ninteraction, and web design; Proficient in Photoshop, Illustrator, \nSketch.\nGPT 3.5\n(A) Extract  valuable information beyond users‚Äô\noriginal resumes to allevaite fabrication of LLMs.\nFabrication \ngeneration\n(B) Align the low-quality generation of few-\nshot users with high-quality generation.\nMany-shot \nusers\nFew-shot \nusers\nHigh-quality\ngeneration\nLow-quality\ngeneration\nUser 1\nUser 3 User 4\nUser 2\nunpair\nunpair\nJob 1\nJob 3\nJob 5\nJob 7\nJob 9\nJob 2\nJob 4\nJob 6\nJob 8\nJob 10\nAlign\nFigure 1: The difficulty and motivation behind leveraging LLMs for job recommendation.\nfabrications and hallucinations within LLMs, as they lack\nsufficient interactive guidance for high-quality resume com-\npletion. To alleviate this problem, we propose aligning the\ngenerated resumes of few-shot users with the high-quality\nresumes of users who have extensive interaction records as\nshown in Fig.1 (B). Due to the lack of paired high-quality\nand low-quality resumes for a specific user in real-world\nscenarios, we introduce a Generative Adversarial Networks\n(GANs) (Goodfellow et al. 2020) based method to align the\nunpaired resumes across different users, which can refine\nthe generated resumes of few-shot users. Specifically, the\ngenerator aims to improve the representations of low-quality\nresumes by fooling the discriminator, while the discrimina-\ntor strives to distinguish between the refined representations\nand the high-quality representations as effectively as possi-\nble. Through iterative training of GANs, the generator plays\na crucial role in refining the representations of low-quality\nresumes, which can bridge the gap between few-shot users\nand many-shot users to enhance the quality of resume com-\npletion for all users.\nTo sum up, we propose an LLM-based GANs Interac-\ntive Recommendation (LGIR) method for job recommen-\ndation in this paper, which aims to address the limitations\nof fabricated generation in LLMs and the few-shot problem\nthat degrades the quality of resume completion. To tackle\nthe fabricated generation limitation, we extract valuable in-\nformation beyond users‚Äô resumes. Specifically, we not only\nextract users‚Äô explicit properties from their self-descriptions\nbut also infer their implicit characteristics from their behav-\niors, leading to accurate and meaningful resume completion.\nTo mitigate the few-shot problem that restricts the quality\nof generated resumes, we propose a transfer representation\nlearning strategy using GANs, which align low-quality re-\nsumes with unpaired high-quality resumes, enhancing the\noverall quality. We evaluate our model on three real-world\ndatasets, demonstrating consistent superiority over state-of-\nthe-art methods for job recommendation. Ablation experi-\nments and a case study further substantiate the motivations\nand effectiveness behind our proposed method.\nRelated Work\nJob Recommendation. Job recommendation has gained\nsignificant popularity in online recruitment platforms and\ncan be primarily categorized into three groups: behavior-\nbased methods, content-based methods, and hybrid methods.\nBehavior-based methods have been developed to leverage\nuser-item interaction for job recommendation. Collaborative\nfiltering based methods (Koren, Bell, and V olinsky 2009)\nhave gained popularity among these approaches, which can\nbe modified with deep neural networks (He and Chua 2017)\nand graph models (He et al. 2020) for more accurate rec-\nommendation results. Content-based methods utilize the\nrich semantic information present in resumes and job re-\nquirements using text-matching strategies or text enhance-\nment techniques, such as CNN (Zhu et al. 2018), RNN (Qin\net al. 2018), and memory networks (Yan et al. 2019). Hy-\nbrid methods combine the strengths of both behavior-based\nand content-based approaches. Specifically, they construct\nthe embeddings of users and jobs based on their text content\nand leverage user-item interaction for job recommendation\n(Le et al. 2019; Jiang et al. 2020; Hou et al. 2022). However,\nthese methods often suffer from the low quality of users‚Äô re-\nsumes. To address this challenge, we propose utilizing the\nrich knowledge and reasoning abilities encapsulated within\nLLMs to improve the resume quality for recommendation.\nLarge Language Models for Recommendation. Large\nLanguage Models (LLMs) (Touvron et al. 2023; Brown\net al. 2020) are revolutionizing recommendation systems\n(Wu et al. 2023). Due to their extensive assimilation of\nknowledge (Liu, Zhang, and Gulla 2023), LLMs have the\ndistinct advantage of comprehending contextual information\n(Geng et al. 2022), leading to improved recommendation ac-\ncuracy and user satisfaction. They offer potential solutions\nto the cold-start problem with zero-shot recommendation\ncapabilities (Sileo, V ossen, and Raymaekers 2022). Their\ncapacity to generate language-based explanations also en-\nhances recommendation interpretability (Gao et al. 2023).\nHowever, challenges arise in their direct application, includ-\ning knowledge gaps and a tendency for unrealistic results\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8364\nJob Description 1\nJob Description 2\nJob Description N\nResume\n‚Ä¶‚Ä¶\nInterestDescriptions\nLarge Language Model\nCompleted Resume\nConcat\nTarget Job Description\nInteract\nSim-BERT‚Ä¶ ‚Ä¶\nTrainableFixedInteractive Resume Representation\nClassifier‚Ä¶\n‚Ä¶\nGenerator‚Ä¶\nHigh-quality Resume\nLow-quality Resume\nAligned Resume\nDiscriminatorHigh-qualityorLow-quality?\nGenerative Adversarial Network\nMatch Layer\nPromptTemplate\nInteractive Resume Completion with LLMsInteraction Numbers\nLow-qualityHigh-qualityUnknown0 ‚àû\n‚ÑíùíûClassifier\nTraining Process of ClassifierDatasets\nBCE Loss\n‚Ñíùíü‚Ñíùí¢\n‚Ñíùìá‚ÑØùí∏\n[  ,  ,   ]‚Ä¶ ‚Ä¶\nTrainableFixed\nJob Description Representation\nTrain Phase:‚Ñíùíû ‚Ñíùíü ‚Ñíùí¢ ‚Ñíùìá‚ÑØùí∏\nJob Recommendation\nFigure 2: The architecture of the LLM-based GANs Interactive Recommendation (LGIR), mainly contains the interactive\nresume completion method for resume generation by LLMs and the GANs-based method for resume quality alignment.\n(Liu et al. 2023). Recent studies utilize constructive prompts\nand in-context learning to control and direct LLM outputs,\nwith methods such as (Hou et al. 2023)‚Äôs sequential recom-\nmendation prompts, (Gao et al. 2023)‚Äôs interactive recom-\nmendation framework, and (Wang et al. 2023)‚Äôs generative\nrecommendation framework. Some also harness user behav-\nior history for guidance (Chen 2023). Nonetheless, perva-\nsive long-tail issues remain challenges, which can further\nexacerbate the hallucination problem of LLMs. To address\nthese, our work uniquely employs Generative Adversarial\nNetworks (GANs) to enhance representations of few-shot\nusers, aiming to improve recommendation quality.\nProblem Definition\nLet C = {c1, ¬∑¬∑¬∑ , cN } and J = {j1, ¬∑¬∑¬∑ , jM } represent the\nsets of N users and M jobs, respectively. Each user or job is\nassociated with a text document describing the resume or job\nrequirement. Specifically, we denote the resume of user c as\nTc = [w1, ¬∑¬∑¬∑ , wlc ], where wi is the i-th word in the resume\nand lc denotes the the length of resume Tc. Similarly, the\nrequirement description of job j with length lj is denoted\nas Tj = [w1, ¬∑¬∑¬∑ , wlj ]. We suppose to know the interaction\nrecords between users and jobs, which can be represented as\nan interaction matrix R ‚ààRN√óM , where Rik = 1 if user ci\nhas interacted with the job jk, and Rik = 0 otherwise.\nIn this paper, our goal is to recommend appropriate jobs\nto users. Formally, we propose learning a matching function\ng(ci, jk) based on the interaction records R and the docu-\nments T. We then make the top- K recommendation based\non this matching function.\nThe Proposed Method\nThe overall architecture of the proposed method is shown in\nFig.2. Firstly, we propose an interactive resume completion\nmethod to alleviate the limitation of the fabricated genera-\ntion in LLMs. Secondly, we propose a GANs-based aligning\nmethod to refine LLMs‚Äô representations of low-quality re-\nsumes. Finally, we propose a multi-objective learning frame-\nwork for job recommendation.\nA LLM-based Method for Resume Completion\nTo enhance the quality of users‚Äô resumes and thereby im-\nprove job recommendations, we propose leveraging the ex-\ntensive knowledge and superior reasoning abilities of Large\nLanguage Models (LLMs). Specifically, we introduce two\nmethods, named Simple Resume Completion (SRC) and In-\nteractive Resume Completion (IRC), aimed at improving the\nquality of users‚Äô resumes for more accurate recommenda-\ntions.\nSimple Resume Completion with LLMs To improve the\nquality of users‚Äô resumes, we propose completing users‚Äô re-\nsumes using a prompting approach that directly leverages\nLLMs‚Äô knowledge and generation abilities. Specifically, we\nconstruct the prompt for LLMs based on the user‚Äôs self-\ndescription as follows:\nGc = LLMs(promptSRC, Tc) (1)\nwhere promptSRC denotes the command that triggers the\nLLMs to complete the useru‚Äôs resume based on his/her self-\ndescription Tc, the details of which are shown in the upper\npart of Fig.3. However, the SRC strategy may suffer from\nthe fabricated and hallucinated generation of LLMs.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8365\nInteractive Resume Completion with LLMs To miti-\ngate the limitation of fabricated generation in LLMs, we\npropose exploring users‚Äô interactive behaviors with recom-\nmender systems, thus assisting LLMs to better profile users\nfor resume completion. For instance, users typically have\nspecific job skills, residential addresses, and educational\nbackgrounds, which influence their interactions with job\npositions containing corresponding responsibilities. Conse-\nquently, users‚Äô implicit characteristics (e.g., skills, prefer-\nences) can be inferred from their interaction behaviors for\nmore accurate and meaningful resume completion. Specifi-\ncally, we adopt a particular prompting approach for resume\ncompletion by LLMs, with consideration of both user‚Äôs self-\ndescription and his/her interactive behaviors:\nGc = LLMs(promptIRC, Tc, Rc) (2)\nwhere Rc = {Tjk |Rc,jk = 1} denotes the requirements of\njobs that the user c has interacted with. The details of the\npromptIRC is shown in the lower part of Fig.3.\nTo utilize the user resumes and job requirements, we\nadopt the BERT to encode them into constant text embed-\ndings Wt ‚àà Rd (Yang et al. 2022). Specifically, we first\nmaintain the text order and place a unique token [CLS] be-\nfore it, then we feed the combined sequence into the SIM-\nBERT model and use the output of the token [CLS] as the\nsemantic embeddings of the descriptive text (e.g., WGci\n=\nSIM-BERT(Gci )). Finally, we employ a multi-layer percep-\ntron to encode these semantic embeddings:\nxci = MLPuser([Pi; WGci\n]), (3)\nxjk = MLPjob([Qk; WTjk\n]), (4)\nwhere Gci and Tjk denote the user ci‚Äôs LLMs-generated re-\nsume and the job jk‚Äôs requirement description. Pi, Qk ‚àà Rd\nrepresent the ID embeddings for user ci and job jk, respec-\ntively. MLPuser and MLPjob denote the multi-layer percep-\ntron with hidden layers [2 ¬∑ d ‚Üí de‚Ä≤ ‚Üí de] and the activa-\ntion function Relu(¬∑) = max(¬∑, 0). d, de and de‚Ä≤ indicate the\ndimensions of hidden layers in the multi-layer perceptron.\nA GAN-based Aligning Method for Resume Refine\nWhile the exploration of users‚Äô interactive behaviors does\nenable LLMs to more effectively profile users, it may still\nencounter the few-shot problem. Specifically, users with\nlimited interaction records might lead to difficulties in gen-\nerating high-quality resumes. To address this challenge, we\npropose refining the low-quality resumes of few-shot users.\nThe approach comprises two main components: a classifier\ndesigned to detect low-quality resumes, and Generative Ad-\nversarial Networks (GANs) employed for aligning resumes.\nClassifier To detect the low-quality resumes for align-\nment, we propose a classifier C to distinguish between high-\nquality resumes and low-quality resumes, i.e.,\nC(x) = œÉ(Wc\n2 ¬∑ Relu(Wc\n1 ¬∑ x)) (5)\nwhere Wc\n1 ‚àà Rdc√óde and Wc\n2 ‚àà R1√ódc represent the\nparameters within the classifier C and we define them as\nŒòC = {Wc\n1 , Wc\n2 }. We posit that users with either extremely\nJob \nDescription 1\nPrompt Template: Please make appropriate revisions \nand improvements based on the user‚Äôs original resume \nto generate a concise and clear new resume, and \nhighlight more skills and experience information. The \nuser‚Äôs resume is: [Resume content]\n.\nLLM\nResume Simple Resume \nCompletion\nJob \nDescription 2\nJob \nDescription N\nResume\n‚Ä¶ Interest\nDescriptions\nLLM\nInteractive \nResume\nCompletion\nPrompt Template: Please make\nappropriate revisionsand improvements\nbased on theuser‚Äôs original resume and\njob description ofinterest to generatea\nconcise and clear new resume,\nhighlighting more skills and experience\ninformation. The user‚Äôs resume is:\n[Resume content]. Job descriptions that\nthe user is interested in are: [Interest\nDescription1, ‚Ä¶,Interest Description K].\nFigure 3: The difference between Simple Resume Comple-\ntion and Interactive Resume Completion.\nfew or rich interaction records may respectively result in\nlow-quality and high-quality resume generation by LLMs.\nTo this end, we introduce the cross-entropy loss to train the\nclassifier C on these partial users, i.e.,\nLC = E(ci,yci )‚àºTC[yci ¬∑log(ÀÜyci )+(1‚àíyci )¬∑log(1‚àíÀÜyci )] (6)\nwhere ÀÜyci = C(xci ) denotes the quality prediction for user\nci‚Äôs generated resume, and TC = T‚Üë\nC\nST‚Üì\nC assembles the\nusers for classifier learning (T ‚Üë\nC = {(ci, 1)|P\nk Rik ‚â• Œ∫1}\nand T‚Üì\nC = {(ci, 0)|P\nk Rik ‚â§ Œ∫2} represent the many-shot\nand few-shot users). yci serves as the ground truth, where\nyci = 1 if ci ‚àà T‚Üë\nC and yci = 0 if ci ‚àà T‚Üì\nC . The thresholds Œ∫1\nand Œ∫2 are used to select the many-shot and few-shot users.\nGenerator To improve the resume quality, we introduce\na generator G to refine the representations of low-quality\nresumes as identified by the aforementioned classifier C.\nSpecifically, the generator G aims to map the low-quality\nresume representations to their high-quality counterparts:\nG(x) = Wg\n2 ¬∑ Relu(Wg\n1 ¬∑ x) (7)\nwhere Wg\n1 ‚àà Rdg√óde , Wg\n2 ‚àà Rde√ódg represent the parame-\nters in the generator G and are defined as ŒòG = {Wg\n1 , Wg\n2 }.\nDiscriminator The principal function of the discrimina-\ntor is to differentiate between samples originating from two\ndistinct distributions. Specifically, we introduce a discrimi-\nnator D to discern whether a given resume representation is\na product of the generator‚Äôs refinement process or a direct\nencoding of a high-quality resume:\nD(x) = œÉ(Wd\n2 ¬∑ Relu(Wd\n1 ¬∑ x)) (8)\nwhere Wd\n1 ‚àà Rds√óde , Wd\n2 ‚àà R1√óds represent the parame-\nters of D, and are defined as ŒòD = {Wd\n1 , Wd\n2 }.\nAdversarial Learning To align the representations of the\nlow-quality and high-quality resumes, we propose engaging\nin a mini-max game between a generator and a discriminator\n(Goodfellow et al. 2020).\nThe discriminator D is responsible for distinguishing\nsamples from distinct distributions. For the training of D,\nwe aim to maximize the following probability, which deter-\nmines whether a representation stems from the generator‚Äôs\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8366\nrefinement or a high-quality generated resume:\nmax\nŒòD\nLD = Eci1 ‚àºÀÜT‚Üë\nC\n[log D(xci1 )]+Eci2 ‚àºÀÜT‚Üì\nC\n[1‚àílog D(G(xci2 ))]\n(9)\nwhere ÀÜT‚Üë\nC and ÀÜT‚Üì\nC denote the high-quality and low-quality\ngenerated resumes detected by the classifier C, respectively.\nThe generator G focuses on refining low-quality generated\nresume representations to resemble high-quality resume rep-\nresentations. For the training of G, we minimize the genera-\ntor loss by deceiving the discriminator D:\nmin\nŒòG\nLG = Eci‚àº ÀÜT‚Üì\nC\n[1 ‚àí log D(G(xci ))] (10)\nThrough iterative training of the generator and discrimi-\nnator in a competitive manner, this adversarial training pro-\ncess drives both components to improve, ultimately leading\nto the creation of low-quality samples that increasingly re-\nsemble high-quality ones.\nMulti-objective Learning for Recommendation\nTo explore the high-quality resume representations for im-\nproved recommendation, we utilize the ClassifierC and Gen-\nerator G to obtain aligned resume representations, denoted\nas zci , for all users, regardless of whether they are few-shot\nusers or many-shot users, i.e.,\nzci =\n\u001axci , if C(xci ) ‚â• 0.5;\nG(xci ), if C(xci ) < 0.5 (11)\nTo predict users‚Äô behaviors on jobs, we propose a deep\nmodel to capture the non-linear and complex relationship\nbetween the user ci and the job jk, i.e.,\nÀÜRi,k = g(ci, jk) = Wp¬∑[zci +xjk ; zci ‚àíxjk ; zci ‚äôxjk ] (12)\nwhere ‚äô denotes the element-wise product, Wp ‚àà R1√ó3¬∑de\nmaps to a score or probability of jk that user ci will engage.\nFor the recommendation target, we adopt the pairwise loss\nto define the recommendation objective function as follows,\nLrec = max\nŒò\nX\n(i,j1,j2)‚ààD\nlog œÉ( ÀÜRi,j1 ‚àí ÀÜRi,j2 )‚àíŒª||Œò||2 (13)\nwhere the train set D = {(ci, j1, j2)} means that user ck\ngave positive feedback to job j1 (i.e., Ri,j1 = 1) instead\nof job j2 (i.e., Ri,j2 = 0). The Œò denotes all parameters\nthat need to be learned in the proposed model and Œª is the\nregularization coefficient of L2 norm || ¬∑ ||2.\nExperiment\nIn this section, we aim to evaluate the performance and ef-\nfectiveness of LGIR. Specifically, we conduct several exper-\niments to study the following research questions:\n‚Ä¢ RQ1: Whether the proposed method LGIR outperforms\nstate-of-the-art methods for job recommendation?\n‚Ä¢ RQ2: Whether LGIR benefits from inferring users‚Äô im-\nplicit characteristics from their behaviors for more accu-\nrate and meaningful resume generation?\n‚Ä¢ RQ3: Whether LGIR benefits from aligning the few-shot\nresumes with high-quality representations?\n‚Ä¢ RQ4: How LGIR achieves SOTA results in case level?\nDataset # Users # Items # Interaction\nDesigns 12,290 9,143 166,270\nSales 15,854 12,772 145,066\nTech 56,634 48,090 925,193\nTable 1: Statistics of the experimental datasets.\nExperimental Setup\nDatasets We evaluated the proposed method on three real-\nworld data sets, which were provided by a popular online\nrecruiting platform. These data sets were collected from 106\ndays of real online logs for job recommendation in the de-\nsigner, sales, and technology industries, respectively. These\ndata sets contained the rich interaction between users and\nemployers. In addition, these data sets also contained text\ndocument information, which were the resumes of the users\nand the descriptions of job positions. The characteristics of\nthese data sets are summarized in Table 1.\nEvaluation Methodology and Metrics We spitted the\ninteraction records into training, validation, and test sets\nequally. To evaluate the performance, we adopted three\nwidely used evaluation metrics for top-n recommendation\n(Zhao et al. 2022): mean average precision (MAP@n), nor-\nmalized discounted cumulative gain (NDCG@n) and mean\nreciprocal rank (MRR ), where n was set as 5 empirically.\nWe sampled 20 negative instances for each positive instance\nfrom users‚Äô interacted and non-interacted records. Experi-\nmental results were recorded as the average of five runs with\ndifferent random initialization of model parameters.\nBaselines We took the following state-of-the-art meth-\nods as the baselines, including content-based methods (i.e.,\nBPJFNN (Qin et al. 2018)), collaborative filtering based\nmethods (i.e., MF (Koren, Bell, and V olinsky 2009) and\nNCF (He et al. 2017)), hybrid methods (i.e., PJFFF (Jiang\net al. 2020), SHPJF (Hou et al. 2022), SGL-text(Wu\net al. 2021) , LightGCN-text(He et al. 2020), and Light-\nGCN+SRC), and LLMs based method (i.e., SGPT-BE\n(Muennighoff 2022), SGPT-ST (Reimers and Gurevych\n2019), SGPT-ST+SRC).\nImplementation Details We adopted the ChatGLM-6B\n(Du et al. 2022) as the LLM model in this paper. For a fair\ncomparison, all methods were optimized by the AdamW op-\ntimizer with the same latent space dimension (i.e.,64), batch\nsize (i.e., 1024), learning rate (i.e.,5√ó10‚àí5), and regulariza-\ntion coefficient (i.e., 1 √ó 10‚àí4). We set d = 768, de‚Ä≤ = 128,\nde = 64, and dc = ds = dg = 256 for the proposed method.\nWe carefully searched other special hyper-parameters for\nbest performance, and early stopping was used with the pa-\ntience of 50 epochs.\nModel Comparison (RQ1)\nTable 2 outlines the performance of various job recom-\nmendation methods, highlighting the top-2 results for each\ndataset. The conclusions drawn are as follows:\n1. Effectiveness of LGIR: The proposed method LGIR\nconsistently surpasses all baseline methods, improving\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8367\nModels\nDesigns Sales Tech\nMAP@5 NDCG@5 MRR MAP@5 NDCG@5 MRR MAP@5 NDCG@5 MRR\nSGPT-BE 0.0712 0.1140 0.2128 0.0526 0.0932 0.1726 0.1464 0.2092 0.3344\nSGPT-ST 0.0694 0.1107 0.2077 0.0519 0.0926 0.1714 0.1422 0.2025 0.3289\nSGPT-ST + SRC 0.0727 0.1177 0.2185 0.0511 0.0925 0.1719 0.1541 0.2194 0.3442\nBPJFNN 0.1415 0.2156 0.3436 0.1138 0.2038 0.3030 0.2018 0.2948 0.4704\nMF 0.1914 0.2913 0.4557 0.0887 0.1628 0.2789 0.4359 0.6054 0.7555\nNCF 0.2071 0.3230 0.4944 0.1463 0.2670 0.3941 0.4105 0.5706 0.7414\nPJFFF 0.1182 0.1855 0.3299 0.0690 0.1255 0.2199 0.2802 0.4040 0.6127\nSHPJF 0.1862 0.2875 0.4531 0.1334 0.2436 0.3705 0.3710 0.5189 0.7016\nSGL-text 0.2716 0.4309 0.5941 0.1508 0.2712 0.3945 0.4416 0.6230 0.7836\nLightGCN-text 0.2664 0.4218 0.5955 0.1629 0.2980 0.4271 0.4676 0.6591 0.8093\nLightGCN+SRC 0.2649 0.4189 0.5926 0.1611 0.2939 0.4204 0.4719 0.6661 0.8146\nLGIR(ours) 0.2887* 0.4622* 0.6319* 0.1751* 0.3225* 0.4548* 0.5086* 0.7191* 0.8434*\nImprvement 6.28% 7.26% 6.11% 7.50% 8.22% 6.49% 7.78% 7.96% 3.54%\nTable 2: Performance of the proposed and baseline methods for job recommendation. ‚àó indicates that the improvements are\nsignificant at the level of 0.01 with paired t-test.\nDataset Method MAP@5 NDCG@5 MRR\nDesigns\nBASE 0.2627 0.4128 0.5829\nSRC 0.2601 0.4076 0.5781\nIRC 0.2859 0.4560 0.6220\nLGIR 0.2887 0.4622 0.6319\nSales\nBASE 0.1617 0.2945 0.4250\nSRC 0.1652 0.3031 0.4331\nIRC 0.1671 0.3065 0.4359\nLGIR 0.1751 0.3225 0.4548\nTech\nBASE 0.4994 0.7088 0.8374\nSRC 0.5048 0.7148 0.8435\nIRC 0.5056 0.7153 0.8400\nLGIR 0.5086 0.7191 0.8434\nTable 3: Performance of the variants for ablation studies.\nthe best baseline by 6.65%, 7.40%, and 6.42% on de-\nsigns, sales, and tech datasets, respectively.\n2. Limitations of LLM-only Methods: LLMs methods\n(SGPT) perform poorly, indicating that relying solely on\ntextual descriptions is ineffective due to inherent limita-\ntions such as meaningless information.\n3. Challenges with Hybrid Methods:Hybrid methods like\nPJFFF and SHPJF, perform inadequately, likely due to\nthe unstructured and varying organization habits of users.\n4. Success of GCN-based Methods: GCN-based meth-\nods like LightGCN, which utilize preference encoding,\nachieve the best performance among baselines, signify-\ning the importance of combining interactions and text.\n5. Simple Resume Completion‚Äôs Limitations: The strat-\negy of simple resume completion (SRC) shows minimal\nimprovement (e.g., LGCN vs. LGCN + SRC), revealing\nthat merely leveraging LLMs isn‚Äôt universally effective\ndue to their tendency to generate fabricated content.\n20% 40% 60% 80% 100%\n0.55\n0.60\n0.65\n0.70\n0.75mrr\ndesigns\nLGIR\nIRC\n20% 40% 60% 80% 100%\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52mrr\nsales\nLGIR\nIRC\nFigure 4: Performance comparison of LGIR and the variant\nIRC for few-shot analysis.\nAblation Study (RQ2&3)\nTo assess the effectiveness of the LGIR‚Äôs module design, it‚Äôs\ncompared to several special cases:\n- BASE: A two-tower text matching model that uses the\noriginal self-description from users for recommendation.\n- SRC: Utilizes the generated resumes of users with a sim-\nple resume completion (SRC) strategy without GANs-\nbased learning for job recommendation.\n- IRC: Leverages the generated resumes with the inter-\nactive resume completion (IRC) strategy, but without\nGANs-based learning for aligning unpaired resumes.\n- LGIR: The proposed method, including both the IRC\nstrategy and GANs-based learning for recommendation.\nTable 3 shows the performance of these methods, i.e.\nLGIR, BASE, SRC, and IRC. From the experimental results,\nwe can get the following conclusions:\n‚Ä¢ RQ2: The SRC variant shows limited improvement over\nBASE, demonstrating that simply leveraging LLMs for\njob recommendation is not a one-size-fits-all solution.\nIssues with fabricated and hallucinated generation are\naddressed through the Interactive Resume Completion\n(IRC) strategy, which shows substantial improvement\nover both BASE and SRC. This highlights the necessity\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8368\nI am outgoing and have strong communication \nskills; ‚Ä¶.. Two years of experience in UI work, \ninterface design, product interaction, and web \ndesign; Proficient in Photoshop, Illustrator, Sketch;\nResume\n1. Having a college degree‚Ä¶ ,working\nin the interface ‚Ä¶, and having\nexperience in mobile user interface\ndesign; 2. Those with relevant\nexperience in hand drawn animation ...\nand commonly used prototype design\ntools such asAxure and OmniGraffe;\nJob\nDescription 1\n1. College \ndegree or \nabove, with \nat least one \nyear of ‚Ä¶.; \n2. ‚Ä¶\nJob\nDescription 2\nInteraction\nHistory\nSimple\nResume\nCompletion\nCustomer communication: ‚Ä¶; \nTeamwork:‚Ä¶; Sense of responsi-\nbility:‚Ä¶; ‚Ä¶ become proficient in \nvarious design tools, such as Photos-\nhop, Illustrator, Sketch, and Axure.\nInteractive\nResume\nCompletion\n1. As a UI designer, with a college degree in the field of design \nart, at least two years of experience in interface design, familiar \nwith mobile user interface design, and possessing high artistic \nliteracy and strong visual expression; 2. I also possess rich \nimagination and logical thinking abilities, and can proficiently \nuse various design software, such as Photoshop, Illustrator, \nSketch, and commonly used prototype design tools such as \nAxure and OmniGraffe; 3. Experience in interaction and \nprototype designs ..., design low fidelity prototype images and \nachieving interactive effects.4. Experience in hand drawn \nanimation and other related fields, and ...\n1. Be responsible for user interface design of mobile \nphone client and website products, design of the \ninterface, ‚Ä¶ and archive all resources of the project, \npromote and improve the design ability of the team, and \nassist GUI designers to complete Interaction design; 2. A \ncollege degree or above in design, art, or related fields ‚Ä¶; \n3. Having high artistic literacy and strong visual \nexpression ‚Ä¶; 4. Proficient in using various design \nsoftware such as Photoshop, Illustrator, Sketch, and \ncommonly used prototype design tools such as Axureand \nOmniGraffe.\nTarget Job\nDescription\nSimilarity Calculation: for text content ùë†ùë†1 and ùë†ùë†2, calculate the sum of the lengths of all \nmatching fragments ùëáùëá. Then the similarity score is: 2 √óùëáùëá/[ùëôùëôùëôùëôùëôùëô ùë†ùë†1 + ùëôùëôùëôùëôùëôùëô ùë†ùë†2 ].\nResume Target Job\nDescription\n0.452026\nSimple\nResume\nCompletion\nTarget Job\nDescription\n0.395652\nInteractive\nResume\nCompletion\nTarget Job\nDescription\n0.610526\nInteraction\nAI generated\nRelation History\nText Similarity\nText\nText Content in Resume\nContent in Interaction\nFigure 5: A real recruitment scenario where users have two historical interactions. The process explains how the model success-\nfully integrates pertinent information from user resumes and interactive job descriptions that better reflect the user‚Äôs abilities.\nof inferring users‚Äô implicit characteristics based on their\nbehaviors for more accurate resume generation.\n‚Ä¢ RQ3: The proposed method LGIR significantly outper-\nforms the variants across all data sets, which benefits\nfrom the GANs-based learning to align the generated\nresumes of few-shot users with high-quality representa-\ntions. Further in-depth analysis of the role of GANs is\nexplored in the subsequent few-shot analysis.\nFew-shot Analysis (RQ3)\nThe ablation study reveals the strengths of LGIR in aligning\nthe generated resumes of few-shot users with high-quality\nrepresentations. It is interesting to investigate how LGIR\nhandles the challenges associated with few-shot scenarios,\nso a few-shot analysis was conducted, comparing LGIR with\nthe IRC variant across different shot levels. Users were\nequally divided into five groups based on their interaction\nnumbers (for example, the group 40% denotes the user set\nthat falls within the 20% ‚àí 40% ranking range based on\nthe number of interactions), and the recommendation perfor-\nmance of LGIR and IRC was compared across these groups.\nThe results in Fig.4 show LGIR consistently outper-\nformed IRC in most cases, validating the effectiveness of\nthe GANs-based learning scheme. Especially, LGIR showed\na more pronounced improvement in groups with fewer in-\nteractions, confirming that GANs-based learning can align\nthe resumes of few-shot users with those of users who have\nrich interaction records. This indicates that LGIR can effec-\ntively mitigate the problems associated with few-shot sce-\nnarios that often limit the quality of resume generation.\nCase Study (RQ4)\nIn a real recruitment scenario depicted in Fig.5, we delve\ndeeper into the outputs of LLMs and explore how they as-\nsist LGIR in achieving state-of-the-art results. The figure\npresents the user‚Äôs resume, previous job interactions, tar-\nget job description and two resume completion approaches:\nSimple Resume Completion (LLMs alone) and Interactive\nResume Completion (LLMs guided by interactive history).\nWe also highlight content relevant to a target job in the user‚Äôs\nresume (in yellow) and interaction history (in blue).\nThe illustration reveals that the user‚Äôs interaction history\ncontains clues relevant to the target job, absent in the user‚Äôs\nown resume. Using only the user‚Äôs resume with LLMs re-\nsults in nonsensical content, reducing the proportion of valu-\nable information in the resume. Conversely, the interactive\napproach successfully integrates pertinent information and\ngenerates resumes that better express the user‚Äôs abilities,\neven those they may not have articulated or recognized. Fur-\nthermore, we quantify this by calculating the pairwise simi-\nlarity between texts, showing that interactive completion im-\nproved similarity from 0.45 to 0.61, a remarkable 35% en-\nhancement. Therefore, exploiting the interactive behaviors\nof users helps LLMs accurately capture skills and prefer-\nences, contributing to better job recommendation results.\nConclusion\nIn this paper, we propose an LLM-based GANs Interac-\ntive Recommendation (LGIR) method for job recommen-\ndation. To alleviate the fabricated generation of LLMs, we\ninfer users‚Äô implicit characteristics from their behaviors for\nmore accurate and meaningful resume completion. To ad-\ndress the few-shot problem encountered during resume gen-\neration, we propose the GANs-based method to refine the\nlow-quality resumes of users. The proposed method outper-\nforms state-of-the-art baselines, which demonstrates the su-\nperiority of utilizing LLMs with interactive resume comple-\ntion and alignment for job recommendation. The ablation\nstudy highlights the significance of each component within\nthe LGIR framework, and the case study further illustrates\nits superiority in capturing users‚Äô skills and preferences.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8369\nAcknowledgements\nThis research is partially supported by the Agency for Sci-\nence, Technology and Research (A*STAR) under its RIE\n2025 ‚Äì Industry Alignment Fund ‚Äì Pre Positioning (IAF-\nPP) funding scheme (Project No: M23L4a0001). This work\nis also partially supported by the MOE AcRF Tier 1 funding\n(RG90/20) awarded to Dr. Zhang Jie. This research is par-\ntially supported by National Natural Science Foundation of\nChina (NSFC Grant No. 62122089).\nReferences\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877‚Äì\n1901.\nChen, Z. 2023. PALR: Personalization Aware LLMs for\nRecommendation. arXiv preprint arXiv:2305.07622.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 320‚Äì335.\nGao, Y .; Sheng, T.; Xiang, Y .; Xiong, Y .; Wang, H.; and\nZhang, J. 2023. Chat-rec: Towards interactive and explain-\nable llms-augmented recommender system. arXiv preprint\narXiv:2303.14524.\nGeng, S.; Liu, S.; Fu, Z.; Ge, Y .; and Zhang, Y . 2022. Rec-\nommendation as language processing (rlp): A unified pre-\ntrain, personalized prompt & predict paradigm (p5). In Pro-\nceedings of the 16th ACM Conference on Recommender Sys-\ntems, 299‚Äì315.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2020. Generative adversarial networks. Communications of\nthe ACM, 63(11): 139‚Äì144.\nGope, J.; and Jain, S. K. 2017. A survey on solving cold start\nproblem in recommender systems. In 2017 International\nConference on Computing, Communication and Automation\n(ICCCA), 133‚Äì138. IEEE.\nHe, X.; and Chua, T.-S. 2017. Neural factorization machines\nfor sparse predictive analytics. In Proceedings of the 40th\nInternational ACM SIGIR conference on Research and De-\nvelopment in Information Retrieval, 355‚Äì364.\nHe, X.; Deng, K.; Wang, X.; Li, Y .; Zhang, Y .; and Wang,\nM. 2020. Lightgcn: Simplifying and powering graph convo-\nlution network for recommendation. In Proceedings of the\n43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval, 639‚Äì648.\nHe, X.; Liao, L.; Zhang, H.; Nie, L.; Hu, X.; and Chua, T.-S.\n2017. Neural collaborative filtering. In Proceedings of the\n26th international conference on world wide web, 173‚Äì182.\nHou, Y .; Pan, X.; Zhao, W. X.; Bian, S.; Song, Y .; Zhang, T.;\nand Wen, J.-R. 2022. Leveraging Search History for Improv-\ning Person-Job Fit. In Database Systems for Advanced Ap-\nplications: 27th International Conference, DASFAA 2022,\nVirtual Event, April 11‚Äì14, 2022, Proceedings, Part I, 38‚Äì\n54. Springer.\nHou, Y .; Zhang, J.; Lin, Z.; Lu, H.; Xie, R.; McAuley,\nJ.; and Zhao, W. X. 2023. Large language models are\nzero-shot rankers for recommender systems. arXiv preprint\narXiv:2305.08845.\nJiang, J.; Ye, S.; Wang, W.; Xu, J.; and Luo, X. 2020. Learn-\ning effective representations for person-job fit by feature fu-\nsion. In Proceedings of the 29th ACM International Con-\nference on Information & Knowledge Management, 2549‚Äì\n2556.\nKoren, Y .; Bell, R.; and V olinsky, C. 2009. Matrix factoriza-\ntion techniques for recommender systems.Computer, 42(8):\n30‚Äì37.\nLe, R.; Hu, W.; Song, Y .; Zhang, T.; Zhao, D.; and Yan, R.\n2019. Towards effective and interpretable person-job fitting.\nIn Proceedings of the 28th ACM International Conference\non Information and Knowledge Management, 1883‚Äì1892.\nLiu, J.; Liu, C.; Lv, R.; Zhou, K.; and Zhang, Y . 2023. Is\nchatgpt a good recommender? a preliminary study. arXiv\npreprint arXiv:2304.10149.\nLiu, P.; Zhang, L.; and Gulla, J. A. 2023. Pre-train, prompt\nand recommendation: A comprehensive survey of language\nmodelling paradigm adaptations in recommender systems.\narXiv preprint arXiv:2302.03735.\nMuennighoff, N. 2022. SGPT: GPT Sentence Embeddings\nfor Semantic Search. arXiv preprint arXiv:2202.08904.\nQin, C.; Zhu, H.; Xu, T.; Zhu, C.; Jiang, L.; Chen, E.; and\nXiong, H. 2018. Enhancing person-job fit for talent recruit-\nment: An ability-aware neural network approach. In The\n41st international ACM SIGIR conference on research & de-\nvelopment in information retrieval, 25‚Äì34.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-\ntence Embeddings using Siamese BERT-Networks. In Pro-\nceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing. Association for Computa-\ntional Linguistics.\nSileo, D.; V ossen, W.; and Raymaekers, R. 2022. Zero-shot\nrecommendation as language modeling. In European Con-\nference on Information Retrieval, 223‚Äì230. Springer.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nWang, W.; Lin, X.; Feng, F.; He, X.; and Chua, T.-S. 2023.\nGenerative recommendation: Towards next-generation rec-\nommender paradigm. arXiv preprint arXiv:2304.03516.\nWu, J.; Wang, X.; Feng, F.; He, X.; Chen, L.; Lian, J.; and\nXie, X. 2021. Self-supervised graph learning for recommen-\ndation. In Proceedings of the 44th international ACM SIGIR\nconference on research and development in information re-\ntrieval, 726‚Äì735.\nWu, L.; Zheng, Z.; Qiu, Z.; Wang, H.; Gu, H.; Shen, T.;\nQin, C.; Zhu, C.; Zhu, H.; Liu, Q.; et al. 2023. A Survey\non Large Language Models for Recommendation. arXiv\npreprint arXiv:2305.19860.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8370\nYan, R.; Le, R.; Song, Y .; Zhang, T.; Zhang, X.; and Zhao,\nD. 2019. Interview choice reveals your preference on the\nmarket: To improve job-resume matching through profiling\nmemories. In Proceedings of the 25th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery & Data Min-\ning, 914‚Äì922.\nYang, C.; Hou, Y .; Song, Y .; Zhang, T.; Wen, J.-R.; and\nZhao, W. X. 2022. Modeling Two-Way Selection Prefer-\nence for Person-Job Fit. In Proceedings of the 16th ACM\nConference on Recommender Systems, 102‚Äì112.\nZhang, Y .; Li, Y .; Cui, L.; Cai, D.; Liu, L.; Fu, T.; Huang, X.;\nZhao, E.; Zhang, Y .; Chen, Y .; et al. 2023. Siren‚Äôs Song in\nthe AI Ocean: A Survey on Hallucination in Large Language\nModels. arXiv preprint arXiv:2309.01219.\nZhao, W. X.; Lin, Z.; Feng, Z.; Wang, P.; and Wen, J.-R.\n2022. A revisiting study of appropriate offline evaluation\nfor top-N recommendation algorithms. ACM Transactions\non Information Systems, 41(2): 1‚Äì41.\nZhu, C.; Zhu, H.; Xiong, H.; Ma, C.; Xie, F.; Ding, P.; and Li,\nP. 2018. Person-job fit: Adapting the right talent for the right\njob with joint representation learning. ACM Transactions on\nManagement Information Systems (TMIS), 9(3): 1‚Äì17.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n8371",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.8785380125045776
    },
    {
      "name": "Generative grammar",
      "score": 0.7807216048240662
    },
    {
      "name": "Generative adversarial network",
      "score": 0.5716190338134766
    },
    {
      "name": "Computer science",
      "score": 0.5286325216293335
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40836939215660095
    },
    {
      "name": "Deep learning",
      "score": 0.15848761796951294
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I21193070",
      "name": "Beijing Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210157345",
      "name": "Venus Medtech (China)",
      "country": "CN"
    }
  ],
  "cited_by": 59
}