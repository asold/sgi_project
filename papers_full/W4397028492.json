{
  "title": "Distilling implicit multimodal knowledge into large language models for zero-resource dialogue generation",
  "url": "https://openalex.org/W4397028492",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1910347209",
      "name": "Zhang Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1997697239",
      "name": "Ma Hui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1959255347",
      "name": "Ding Jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A598734319",
      "name": "Wang Jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1937328933",
      "name": "Xu, Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2366246759",
      "name": "Lin Hongfei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3015063797",
    "https://openalex.org/W3173864402",
    "https://openalex.org/W3206527789",
    "https://openalex.org/W4393147165",
    "https://openalex.org/W6736392004",
    "https://openalex.org/W4387969125",
    "https://openalex.org/W3035448310",
    "https://openalex.org/W6600292188",
    "https://openalex.org/W4385959364",
    "https://openalex.org/W2099442061",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4249013746",
    "https://openalex.org/W2964213933",
    "https://openalex.org/W6601273488",
    "https://openalex.org/W3205638321",
    "https://openalex.org/W4386071687",
    "https://openalex.org/W6809415734",
    "https://openalex.org/W4226452284",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W6753938374",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2133012565"
  ],
  "abstract": null,
  "full_text": "Distilling Implicit Multimodal Knowledge into Large Language Models for\nZero-Resource Dialogue Generation\nBo Zhanga, Hui Mab, Jian Dinga, Jian Wanga,∗, Bo Xua, Hongfei Lina\naSchool of Computer Science and Technology, Dalian University of Technology, Dalian, 116024, Liaoning, China\nbSchool of Computer Science and Information Engineering, Hefei University of Technology, Hefei, 230601, Anhui, China\nAbstract\nIntegrating multimodal knowledge into large language models (LLMs) represents a significant advancement in dialogue\ngeneration capabilities. However, the effective incorporation of such knowledge in zero-resource scenarios remains a\nsubstantial challenge due to the scarcity of diverse, high-quality dialogue datasets. To address this, we propose the Visual\nImplicit Knowledge Distillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs for enriched\ndialogue generation in zero-resource contexts by leveraging implicit multimodal knowledge. VIKDF comprises two main\nstages: knowledge distillation, using an Implicit Query Transformer to extract and encode visual implicit knowledge\nfrom image-text pairs into knowledge vectors; and knowledge integration, employing a novel Bidirectional Variational\nInformation Fusion technique to seamlessly integrate these distilled vectors into LLMs. This enables the LLMs to\ngenerate dialogues that are not only coherent and engaging but also exhibit a deep understanding of the context through\nimplicit multimodal cues, effectively overcoming the limitations of zero-resource scenarios. Our extensive experimentation\nacross two dialogue datasets shows that VIKDF outperforms existing state-of-the-art models in generating high-quality\ndialogues. The code is available at https://github.com/zhangbo-nlp/VIKDF.\nKeywords: Large language models, Multimodal fusion, Zero resource, Dialogue generation\n1. Introduction\nDialogue generation, a pivotal component of natural lan-\nguage processing, aims to create responses that are both\nnatural and engaging within specific dialogue contexts.\nThe emergence of large language models (LLMs), such\nas the Generative Pre-trained Transformer (GPT) series\n[1, 2, 3], has marked significant advancements in this do-\nmain. These models excel in identifying complex linguistic\npatterns and semantic details due to their training on ex-\ntensive textual datasets. However, their effectiveness is\nlimited to text-based contexts, overlooking the rich, mul-\ntimodal aspects of human dialogue that incorporate visual,\nauditory, and other sensory inputs. This limitation high-\nlights a crucial challenge: enabling LLMs to navigate the\nmultimodal nature of human interactions, a capability that\nhumans possess inherently.\nThe integration of multimodal knowledge into dialogue\nsystems signifies a major progression towards more nu-\nanced and human-like communication capabilities. It en-\nables these systems to understand and interpret the nu-\nances of human communication that transcend beyond\n∗Corresponding author\nEmail addresses: zhangbo1998@mail.dlut.edu.cn (Bo Zhang),\nhuima@hfut.edu.cn (Hui Ma), 91mr_ding@mail.dlut.edu.cn (Jian\nDing), wangjian@dlut.edu.cn (Jian Wang ), xubo@dlut.edu.cn\n(Bo Xu), hflin@dlut.edu.cn (Hongfei Lin)\nmere text, capturing the essence of multimodal interac-\ntions [4, 5, 6, 7]. Building upon this concept, there has\nbeen an increase in research focused on augmenting LLMs\nwith multimodal knowledge [8, 9, 10, 11]. This involves\nprocessing and understanding information across differ-\nent modalities, such as images, videos, and audio, thereby\nequipping LLMs to perform tasks that necessitate cross-\nmodal comprehension. While these developments signifi-\ncantly expand the capabilities of LLMs in engaging with\nmultimodal content, challenges persist in effectively apply-\ning multimodal knowledge in dialogue generation, necessi-\ntating further exploration and innovation in this area.\nOne pivotal challenge in augmenting LLMs with mul-\ntimodal capabilities, crucial for advancing human-like di-\nalogue generation, is the scarcity of high-quality, diverse\nmultimodal dialogue datasets. This is particularly notable\nin domains that demand intricate interactions, such as\nimage-grounded dialogues [12]. Image-grounded dialogues\ninvolve conversations anchored on a shared image, neces-\nsitating visual reasoning and a wealth of common sense to\nelicit coherent and engaging responses. Current datasets\n[12, 13, 14], while foundational, often fall short in cap-\nturing the breadth and depth of human multimodal com-\nmunication, resulting in models that may not effectively\ngeneralize to varied real-world interactions. Moreover, ex-\nisting frameworks like ZRIGF [15], which represent pio-\nneering efforts in zero-resource image-grounded dialogue\ngeneration, do not seamlessly integrate into LLM archi-\nPreprint submitted to Information Fusion February 6, 2025\narXiv:2405.10121v2  [cs.CL]  5 Feb 2025\nFigure 1: Intuition of our proposed approach.\ntectures and depend on retrieving relevant images during\ninference to formulate responses. Thus, the challenge of\nenabling LLMs to generate multimodal dialogues in zero-\nresource scenarios, devoid of annotated multimodal dia-\nlogue datasets, remains unresolved.\nTo address the primary challenge of enabling LLMs to\ngenerate dialogues in zero-resource scenarios, we introduce\nthe concept of implicit multimodal knowledge. Unlike\nexisting approaches, which predominantly utilize explicit\nmultimodal inputs such as images or sounds presented dur-\ning interactions, we center on implicit multimodal knowl-\nedge. This form of knowledge, significantly distinct from\nthe explicit forms commonly integrated in current mod-\nels, refers to a mental imagery or conceptual understand-\ning individuals have developed through their experiences\n[16, 17]. Implicit multimodal knowledge encompasses a\nbroad spectrum of sensory, emotional, and contextual un-\nderstandings that, although not directly observable, pro-\nfoundly influence the nature of dialogues [18, 19]. By lever-\naging readily available large-scale image-text pair corpora\nto learn how to utilize implicit multimodal knowledge, it\nis possible to circumvent the issue of scarcity in high-\nquality, diverse multimodal dialogue datasets. However,\ncurrent multimodal LLMs primarily engage with explicit\nmultimodal inputs, focusing on direct responses to visual\nor auditory stimuli within dialogues [20], resulting in a\nlack of capability to incorporate such implicit knowledge.\nTherefore, the challenge becomes how to effectively distill\nand integrate implicit multimodal knowledge into LLMs,\nthereby significantly enhancing their ability to generate\nnuanced dialogues in zero-resource scenarios.\nTo navigate this refined challenge, we propose the Vi-\nsual Implicit Knowledge Distillation Framework (VIKDF),\na novel framework that reimagines the integration of mul-\ntimodal knowledge into LLMs, focusing on the distilla-\ntion and incorporation of visual implicit knowledge in a\nzero-resource scenario. VIKDF operates in two synergistic\nstages: knowledge distillation and knowledge integration,\nas illustrated in Figure 1. In the knowledge distillation\nstage, VIKDF utilizes a multimodal model that incorpo-\nrates a text encoder, an image encoder, and a novel query-\ning transformer termed the Implicit Query Transformer\n(IQ-Former). This transformer, an advancement of the\nQ-Former [8], is specially tailored for extracting implicit\nknowledge. It employs a set of learnable query vectors to\ndistill visual implicit knowledge from extensive image-text\npair corpora. These vectors serve as the representation\nof the visual implicit knowledge, which can be then ef-\nfectively integrated into LLMs. During the knowledge in-\ntegration stage, we introduce a pioneering technique to\nseamlessly integrate the distilled visual implicit knowl-\nedge into LLMs, namedBidirectional Variational Informa-\ntion Fusion (BVIF). BVIF leverages an instruction-aware\ndual-pathway approach to maximize the mutual informa-\ntion between textual context and distilled visual implicit\nknowledge, thereby capturing the essence of the visual\nimplicit knowledge. This simultaneous optimization en-\nsures coherent, context-rich dialogues and bridges the gap\nbetween explicit and implicit multimodal knowledge pro-\ncessing. Consequently, VIKDF enables LLMs to engage in\ncomplex dialogues without depending on annotated mul-\ntimodal datasets, marking a significant step forward in\nzero-resource dialogue generation.\nTo validate our framework’s efficacy, we conducted com-\nprehensive experiments on the Image-Chat [21] and Red-\ndit Conversation datasets [4], benchmarking our method\nagainst several state-of-the-art baselines such as ChatGPT\nand ZRIGF. Through both automatic and human evalu-\nations, VIKDF showcased its exceptional ability to flu-\nently incorporate visual implicit knowledge into dialogues,\nthereby generating contextually rich, engaging, and coher-\nent conversations, and outperforming existing models in\nzero-resource scenarios.\nOur main contributions are highlighted as follows:\n• We propose a novel framework that distills and inte-\ngrates visual implicit knowledge into LLMs, enabling\nthem to generate more engaging dialogues without re-\nlying on any explicit images in zero-resource scenarios.\n• We develop the Implicit Query Transformer and Bidi-\nrectional Variational Information Fusion techniques,\neffectively distilling and integrating visual implicit\nknowledge into LLMs and enhancing their dialogue\ngeneration capabilities.\n• We conduct extensive evaluations across two datasets\nin diverse scenarios, demonstrating the superior per-\nformance and robust generalization capabilities of\nVIKDF.\n2. Related Work\n2.1. Multimodal Dialogue Generation\nMultimodal dialogue generation aims to produce re-\nsponses that are natural and engaging, considering inputs\nfrom multiple modalities such as images, videos, or audio.\nThis field requires models that have the ability to under-\nstand or generate content across these different modalities,\n2\nleveraging this multimodal knowledge to enrich dialogues.\nEarly research in this area [22, 23] was primarily focused\non multimodal question-answering, where the goal was to\nrespond to queries with inputs from various modalities.\nHowever, there has been a noticeable shift towards gener-\nating open-domain dialogues. In these cases, multimodal\ninputs serve to enrich conversations rather than strictly\nguide them, leading to two main streams of research.\nThe first stream focuses on dialogue generation based\non multimodal information. In this approach, models uti-\nlize inputs such as images or videos to influence the dia-\nlogue generation process. These models typically employ\nmultimodal encoders or attention mechanisms to integrate\nmultimodal features with textual features, thus enhanc-\ning the relevance and diversity of the generated responses\n[4, 6, 24]. This approach mimics face-to-face interactions\nwhere non-verbal cues influence but do not solely dictate\nthe conversation. The second stream involves models that\nnot only interpret multimodal inputs but also generate\noutputs across multiple modalities. This approach is akin\nto network-based dialogues, where communication often\nincludes and sometimes relies on multimodal elements such\nas emojis, images, or videos. These models require more\nadvanced capabilities for handling cross-modal generation\nand alignment, as well as ensuring the coherence and con-\nsistency of multimodal outputs [25, 26, 27].\nOur research aligns with the first stream, aiming to en-\nhance dialogue using multimodal inputs without generat-\ning multimodal outputs. However, most existing method-\nologies rely on annotated multimodal dialogue data, which\nis both scarce and expensive to obtain. In an effort to\nbridge this gap, Zhang et al. [15] introduced a zero-\nresource image-grounded framework that leverages images\nto enrich dialogues through a two-stage learning strat-\negy. While innovative, this method requires access to\nrelevant images during inference, which may not always\nbe feasible. Our proposed approach differs by utilizing\nimplicit multimodal knowledge, distilled from extensive\ncollections of image-text pairs, to enhance dialogue gen-\neration. This strategy addresses the challenges of data\nscarcity and modality mismatch, enabling the generation\nof dialogues that are more natural, contextually rich, and\nauthentically human-like.\n2.2. Multimodal Knowledge Distillation and Integration\nMultimodal knowledge distillation and integration are\ncrucial for enabling LLMs to utilize multimodal informa-\ntion in dialogue generation. Distillation involves extract-\ning and compressing information from a broad spectrum\nof multimodal data, such as image-text pairs. Integration,\non the other hand, focuses on incorporating this distilled\ninformation into LLMs, thereby augmenting their capacity\nfor understanding and generating multimodal dialogues.\nPrior research on this topic has predominantly concen-\ntrated on explicit multimodal information, such as the di-\nrect fusion of image and textual features [5, 27, 28, 29]\nor employing pre-trained multimodal models [30] to ex-\ntract multimodal representations [20, 31]. However, these\nmethods face limitations when applied to LLMs. Vision-\nLanguage Models (VLMs), like LLaVA [32] and Mini-\nGemini [33], are designed to process explicit multimodal\ninputs by interpreting both visual and textual modali-\nties together. They typically train on large multimodal\ndatasets to maximize alignment and shared understand-\ning between vision and language components, enabling\ncomprehensive tasks like visual question answering, image\ncaptioning, and scene understanding. Moreover, Li et al.\n[8] proposed an innovative solution, the Q-Former, which\nuses learnable query vectors to distill multimodal knowl-\nedge and integrate it into LLMs. Although this method\nhas shown promising results in improving the multimodal\ncapabilities of LLMs, it still encounters challenges in han-\ndling implicit multimodal knowledge, especially for dia-\nlogue generation.\nOur work seeks to address a critical gap in multi-\nmodal dialogue generation by leveraging implicit multi-\nmodal knowledge to enhance LLMs’ dialogue generation\ncapabilities in zero-resource scenarios. This not only ad-\nvances the technology of multimodal dialogue generation\nbut also provides new insights into the complexities of hu-\nman conversational interactions.\n3. Methodology\n3.1. Task Formalization and Model Architecture\nThe task of dialogue generation based on multimodal\ninformation is defined as generating a response R based on\na given dialogue context C and corresponding multimodal\ninformation such as an imageI. Our methodology diverges\nfrom conventional practices that leverage multimodal data\ndirectly, focusing instead on scenarios where such data is\nabsent during both training and inference phases, known\nas zero-resource scenarios. Our strategy utilizes distilled\nvisual implicit knowledge K, derived from C via P(K|C),\nto augment the quality of dialogue generation. Therefore,\nour objective is to generate R by conditioning on both C\nand K, formalized as P(R|C, K). This leads to the model\nformulation:\nP(R|C) = P(R, K|C) = P(R|C, K)P(K|C) (1)\nThis formulation is justified as K is inherently determined\nby C, explaining the rationale behind the first part of the\nequation.\nTo achieve this, we introduce a two-stage framework\ncomprising knowledge distillation and integration, as\nshown in Figure 1. The framework is anchored by three\nprincipal components:\n3.1.1. Text Encoder and Image Encoder\nThe text encoder and image encoder are responsible for\nencoding the textual and visual information from a large\ncorpus of image-text pairs. We adopt the CLIP model [30]\n3\nFigure 2: The architecture of IQ-Former.\nas our text and image encoder, which is a pre-trained mul-\ntimodal model that learns to align image and text features\nin a unified latent space. The text encoder transforms text\ninput T into a hidden representation HT , utilizing a trans-\nformer encoder architecture [34]. Similarly, the image en-\ncoder, based on a vision transformer model [35], converts\nimage input I into a feature vector HI. The parameters\nof both encoders are set to remain unchanged.\n3.1.2. Implicit Query Transformer\nThe IQ-Former is a specially designed querying trans-\nformer that distills visual implicit knowledge from encoded\nimage-text pairs. As illustrated in Figure 2, the IQ-Former\nis structured as a transformer encoder equipped with a\ncross-attention mechanism. Uniquely, its inputs are a set\nof learnable query vectors Q = q1, q2, ...,qn, where n is\nthe number of queries. The IQ-Former uses the cross-\nattention mechanism to interact between the query vectors\nand both HT and HI, and generates two sets of knowledge\nvectors KT = kT1 , kT2 , ...,kTn and KI = kI1 , kI2 , ...,kIn ,\nrespectively. The vectors in KT contain enriched textual\ninformation, whereas those in KI are infused with explicit\nvisual information. By optimizing the IQ-Former with\nspecifically designed objectives, as described in Section 3.2,\nit can be efficiently distill visual implicit knowledge into\nKT .\n3.1.3. Large Language Model\nThe LLM is tasked with generating dialogue responses\nbased on the dialogue context and the distilled visual\nimplicit knowledge. We employ the Llama-2 model [36]\nas our LLM, a pre-trained autoregressive language model\nrenowned for its capability to produce natural and varied\ntext. The LLM takes the dialogue context C and distilled\nvisual implicit knowledge KT as inputs, and generates the\nresponse R as output. To facilitate the integration of KT\ninto the LLM, we introduce a novel technique termed Bidi-\nrectional Variational Information Fusion, detailed further\nin Section 3.3. The LLM’s parameters are also frozen dur-\ning training.\n3.2. Knowledge Distillation Stage\nThe goal of the knowledge distillation stage is to develop\na model, denoted as P(K|C), capable of deducing visual\nimplicit knowledge K from a given dialogue context C.\nTo achieve this, we use a large corpus of image-text pairs\nto optimize IQ-Former so that the knowledge vectors KT\ncan encapsulate visual implicit knowledge pertinent to the\ninput context. As depicted in Figure 3, the IQ-Former dis-\ntills the visual implicit knowledge into KT by using three\nobjectives.\n3.2.1. Text-Image Matching\nThe text-image matching objective ensures the align-\nment of knowledge vectors KT and KI within a shared la-\ntent space, promoting consistency and coherence between\nthe knowledge extracted from both text and image. We\nemploy a contrastive learning strategy [30] to achieve this,\nby maximizing the cosine similarity between KT and KI\nfor matching pairs and minimizing the cosine similarity for\nnon-matching pairs. The loss for text-image matching is\nformulated as follows:\nLtim = −\nNX\ni=1\n( log exp(cos(KT i, KIi)/τ)PN\nj=1 exp(cos(KT i, KIj)/τ)\n+ log exp(cos(KT i, KIi)/τ)PN\nj=1 exp(cos(KT j, KIi)/τ)\n)\n(2)\nwhere τ is a learnable temperature parameter that con-\ntrols the distribution’s concentration. By minimizing this\nloss, the IQ-Former learns to generate knowledge vectors\nthat are semantically similar to the corresponding image\nfeatures, thereby encapsulating visual implicit knowledge\nfrom the text input.\n3.2.2. Text-Assisted Masked Image Modeling\nThe objective of text-assisted masked image modeling\nis to reconstruct the masked portions of an image input,\ndenoted as I, utilizing text knowledge vectors KT . This\nprocess enables the IQ-Former to apply textual informa-\ntion to deduce missing visual data, thereby improving the\nextraction of visual implicit knowledge. Initially, a certain\npercentage of pixels in I is randomly masked, producing\na masked image I′. This image is then fed into the image\nencoder to obtain the masked image feature vectors HI′ .\nSubsequently, HI′ is combined with KT using a trans-\nformer block that includes multi-head attention and a feed-\nforward network. The resulting output vectors OI′ , now\nenriched with textual information, are input into a convo-\nlutional decoder consisting of a convolution layer followed\nby a pixel-shuffle operation to generate the reconstructed\nimage ˆI. The loss for text-assisted masked image model-\ning, expressed as the mean absolute error between I and ˆI\nin the masked regions, is defined as follows:\nLtamim = 1\nNi\nNiX\ni=1\n|Ii − ˆIi| (3)\n4\nFigure 3: Overview of the knowledge distillation stage. The central part illustrates Text-Image Matching. Below the dashed line lies Text-\nAssisted Masked Image Modeling, and above, Image-Assisted Masked Text Modeling.\nwhere Ni represents the count of masked pixels, with Ii\nand ˆIi being the pixel values of the original and recon-\nstructed images, respectively. Minimizing this loss enables\nthe IQ-Former to create knowledge vectors that contain\nsufficient visual implicit knowledge to assist the image re-\nconstruction.\n3.2.3. Image-Assisted Masked Text Modeling\nThe goal of image-assisted masked text modeling is to\nrecover the masked tokens in a text input T using im-\nage knowledge vectors KI. This objective encourages the\nIQ-Former to use visual information to deduce missing tex-\ntual content, enhancing cross-modal knowledge alignment.\nThis objective is similar to the masked language modeling\ntask in BERT [37], but with the addition of visual informa-\ntion. Similar to the text-assisted masked image modeling,\nwe can obtain the output vectors OT′ that incorporates\nvisual information. To reconstruct masked tokens, OT′ is\nfed into a linear layer followed by a softmax function to\npredict the vocabulary’s probability distribution for each\nmasked token. The loss for image-assisted masked text\nmodeling, calculated as the cross-entropy loss between the\npredicted distribution and the true masked tokens, is:\nLiamtm = − 1\nNt\nNtX\ni=1\nlog P(Ti|O′\nT ) (4)\nwhere Nt is the total number of masked tokens, and\nP(Ti|O′\nT ) is the predicted probability of the original token\nTi based on the output vectors O′\nT . By minimizing this\nloss, the IQ-Former is trained to produce output vectors\nthat contain sufficient cross-modal knowledge to assist the\ntext reconstruction, thus capturing cross-modal knowledge\nalignment.\nThe comprehensive loss function for the knowledge dis-\ntillation stage is the sum of the losses from the three ob-\njectives, weighted by their respective importance:\nLkd = λ1Ltim + λ2Ltamim + λ3Liamtm (5)\nwhere λ1, λ2, and λ3 are hyperparameters that determine\nthe significance of each objective. By minimizing this loss,\nthe IQ-Former is trained to effectively distill and encap-\nsulate visual implicit knowledge derived from image-text\npairings into the knowledge vectors KT . These vectors\ncan then be used as the input K for the LLM during the\nknowledge integration stage.\n3.3. Knowledge Integration Stage\nThe objective of the knowledge integration stage is to\ntrain a model, denoted as P(R|C, K), that generates a di-\nalogue response R informed by the dialogue context C and\ndistilled visual implicit knowledge K. This process lever-\nages image-text pair data to optimize the learnable knowl-\nedge vectors K, referred to earlier as KT , thus equipping\nthe LLM with the ability to comprehend and incorporate\nthe knowledge encapsulated in K. As depicted in Figure 4,\nthe LLM integrates visual implicit knowledge through a pi-\noneering technique named Bidirectional Variational Infor-\nmation Fusion. BVIF utilizes an instruction-aware dual-\npathway approach, with each path providing a distinct yet\ncomplementary mechanism for knowledge fusion.\n3.3.1. Instruction-aware Contextual Inference\nThe instruction-aware contextual inference pathway\naims to enable the LLM to decode and integrate the tex-\ntual intricacies contained within the distilled visual im-\nplicit knowledge. This pathway introduces distilled visual\nimplicit knowledge K as soft visual prompts, directing the\nLLM towards generating text T that aligns with the vi-\nsual context. Initially, we attach the text encoder to the\nIQ-Former and freeze its parameters. Subsequently, the\n5\nFigure 4: Overview of the knowledge integration stage. Instruction-aware Contextual Inference on the left and Instruction-aware Knowledge\nReconstruction on the right\nIQ-Former is connected to the LLM, using a linear layer\nto project the knowledge vectors K into the same dimen-\nsion as the LLM’s text embedding. These projected query\nembeddings are then positioned at the beginning of the\ninput text embeddings sequence for the LLM. Addition-\nally, we use a set of text-based prompts, such as “ Write a\nshort description for the image. ”, to fine-tune the LLM’s\ngeneration according to the specific task. The LLM then\ngenerates text T by optimizing the likelihood of predict-\ning each subsequent token, based on preceding tokens, the\nquery embeddings, and the text prompts. The instruction-\naware contextual inference loss is formally defined as:\nLiaci = − 1\nNt\nNtX\ni=1\nlog P(Ti|K, P, T<i) (6)\nwhere Nt represents the text’s token count, P the text\nprompt, and P(Ti|K, P, T<i) the probability of token Ti\nconditioned on the knowledge K, prompt P, and preced-\ning tokens T<i. Minimizing this loss instructs the LLM\nto produce text reflective of the visual implicit knowledge\ndistilled by the IQ-Former from the image-text pairs.\n3.3.2. Instruction-aware Knowledge Reconstruction\nThe instruction-aware knowledge reconstruction path-\nway is a crucial component of the BVIF technique, aimed\nat augmenting the LLM’s proficiency in interpreting and\nutilizing distilled visual implicit knowledge K for dia-\nlogue generation. This pathway focuses on reconstructing\nknowledge K from the generated text T, thereby estab-\nlishing a bidirectional information flow between the text\nand the visual implicit knowledge.\nA primary challenge in this task is to ensure that the\nknowledge K is deeply embedded and reflected in the gen-\nerated text T. While the instruction-aware contextual\ninference pathway can mitigate this challenge to an ex-\ntent, fully embedding visual implicit knowledge into the\nLLM’s learning process remains challenging. To address\nthis, we implement a mutual information maximization\nmechanism to quantify the dependency between K and T.\nThe mutual information, denoted as I(K, T), is defined as\nthe expected value of the logarithmic ratio between the\njoint probability distribution of K and T and the prod-\nuct of their marginal probability distributions. However,\ndirect optimization of I(K, T) is intractable due to compu-\ntational complexity. Consequently, we maximize a lower\nbound of I(K, T) through a variational information max-\nimization approach, as detailed in [38]. This approach is\nmathematically represented as:\nI(K, T) ≥ Ep(K)Ep(T|K) log qϕ(K|T) (7)\nwhere qϕ(K|T) is a variational approximation of the pos-\nterior probability of K given T, parameterized by ϕ. This\nformulation allows for the approximation of mutual infor-\nmation by learning a function qϕ that predicts K from T.\nIn practice, we use the LLM as the function qϕ that\ninfers K based on T. Specifically, we feed the generated\ntext T and a text prompt, such as “ Create an image that\nreflects the following description: ”, into the LLM to yield\noutput vectors OK. A linear layer then projects OK to\nmatch the dimensionality of the original knowledge vec-\ntors K. The instruction-aware knowledge reconstruction\nloss is calculated as the mean squared error between the\noriginal knowledge vectorsK and the reconstructed knowl-\nedge vectors ˆK across all queries:\nLiakr = 1\nNq\nNqX\ni=1\n(Ki − ˆKi)2 (8)\nwhere Nq is the total number of queries, with Ki and ˆKi\nrepresenting the original and reconstructed knowledge vec-\ntors, respectively. Minimizing this loss enables the LLM to\nproduce knowledge vectors consistent with the generated\ntext, thus reinforcing a bidirectional flow of information\nbetween the text and the visual implicit knowledge.\n6\nThe overall loss function for the knowledge integration\nstage is the weighted sum of the two objectives:\nLki = λ4Liaci + λ5Liakr (9)\nwhere λ4 and λ5 are hyperparameters that control the rela-\ntive importance of each objective. By minimizing this loss,\nthe LLM is trained to generate dialogue responses that are\ncoherent and engaging based on both the dialogue context\nand the distilled visual implicit knowledge.\n3.4. Zero-Resource Learning Detail\n3.4.1. Training\nTo train our framework, we initially use four large-\nscale image-text datasets, including COCO Captions [39],\nCC3M [40], CC12M [41], and SBU [42], for pre-training\nthe IQ-Former and aligning it with the LLM. We follow\nthe same data processing and augmentation methods as\nBLIP-2 [8], which generates synthetic captions for web\nimages using a pre-trained captioning model and a CLIP\nmodel. These datasets contain tens of millions of image-\ntext pairs that cover a wide range of topics and scenarios,\nproviding a rich source of visual implicit knowledge. We\nconcurrently train both stages of our framework, employ-\ning image-text pair data in accordance with Equations (5)\nand (9).\nFor fine-tuning on specific dialogue tasks, we utilize\nonly textual data without any image input, aligning with\nour goal of zero-resource scenarios where multimodal data\nis unavailable during fine-tuning and inference. During\nthis phase, the IQ-Former distills visual implicit knowl-\nedge K from the dialogue context C alone by modeling\nP(K|C). The fine-tuning process optimizes the negative\nlog-likelihood loss to enhance the probability P(R|C, K),\nas detailed in Section 3.4.2.\n3.4.2. Inference\nTo perform zero-resource inference, we leverage the\ntrained IQ-Former and the LLM to produce dialogue re-\nsponses enriched with the visual implicit knowledge dis-\ntilled from the dialogue context. Given a dialogue context\nC, we first pass it through the IQ-Former with the text\nencoder to model P(K|C), thereby acquiring knowledge\nvectors K that encapsulate visual implicit knowledge de-\nrived from C. Notably, no image input is provided at this\nstage; the IQ-Former operates solely on the textual in-\nput C. Next, we input K and C into the LLM, which\nmodels P(R|C, K), and then generate the response R by\nsampling from the probability distribution over the vocab-\nulary. Since the LLM has learned to integrate the visual\nimplicit knowledge into the dialogue generation during\ntraining, it can produce natural and engaging responses\nthat are consistent with the visual context implied by C,\neven in the absence of any explicit multimodal inputs.\n4. Experiments\nIn this section, we evaluate the performance of our pro-\nposed framework, VIKDF, on the task of zero-resource dia-\nlogue generation. We benchmark VIKDF against a range\nof baselines and ablation models, conducting both auto-\nmatic and human evaluations. Additionally, we present\nqualitative examples to demonstrate the effectiveness of\nour approach.\n4.1. Datasets\nWe employ two datasets to evaluate our framework:\nImage-Chat [21] and Reddit Conversation [4]. The Image-\nChat dataset, crucial for validating VIKDF’s efficacy in\nzero-resource scenarios, is a large-scale image-grounded di-\nalogue dataset featuring 202,000 dialogues across 202,000\nimages. Each dialogue comprises a single turn of con-\ntext and response, with the latter influenced by the cor-\nresponding image. This dataset is divided into 186,782\ntraining dialogues, 5,000 validation dialogues, and 9,997\ntesting dialogues. The Reddit Conversation dataset is\nserved to assess the performance of visual implicit knowl-\nedge. This dataset, sourced from the widely-used online\nforum, encompasses an extensive variety of dialogue topics\nand styles. It has been preprocessed to include 1,000,000\ntraining dialogues, with an additional 20,000 for validation\nand 20,000 for testing.\nSimilar to [15], we fine-tune the model using the Red-\ndit Conversation dataset to enhance the foundational dia-\nlogue generation capabilities of the LLM. To evaluate the\nmodel’s zero-resource dialogue generation capabilities, we\ndirectly perform inference on the Image-Chat dataset after\nfine-tuning on Reddit Conversation. Additionally, we con-\nduct experiments by fine-tuning the model on Image-Chat\nwithout providing any images.\n4.2. Implementation Details\nThe VIKDF implementation utilizes the Hugging Face\nTransformers library [21]. For the text and image en-\ncoders, we initialize them with the pre-trained CLIP model\nthat employs a ViT-L/14 Transformer architecture. The\nIQ-Former is initialized with the pre-trained BERT-base\nmodel, while the chat version of Llama-2 with 7B parame-\nters is used as the large language model. To ensure seam-\nless integration and information flow across models of vary-\ning dimensionalities, the necessary linear transformations\nare applied, which are not mentioned in the methodology.\nVIKDF adopts a simultaneous training regimen for both\nknowledge distillation and integration stages. This strat-\negy is executed on four NVIDIA RTX 4090 GPUs, uti-\nlizing a batch size of 128 across 100,000 training steps.\nFor optimization, we employ mixed-precision training with\nbfloat16 and utilize the AdamW optimizer [43] with a\nlearning rate of 1e−4. The learning rate is accompanied by\na weight decay of 0.05 and a linear learning rate warmup\nover the first 10% of the total steps. The hyperparameters\n7\nλ1 through λ5 are set as 0.5, 0.2, 0.2, 1, and 0.5, respec-\ntively. For the set of query vectors within the IQ-Former,\nwe set n to 32, with each vector having a dimensionality\nof 768. In the text-assisted masked image modeling, we\napply a random masking strategy with 28 × 28 patches at\na 0.6 ratio. For the image-assisted masked text modeling,\nthe strategy involves a 0.15 mask ratio.\n4.3. Baseline Models\nWe compare VIKDF with the following baseline models:\n• Seq2Seq [44], a foundational architecture for\nsequence-to-sequence processing that includes an en-\ncoder and a decoder equipped with Long Short-Term\nMemory (LSTM) units.\n• BART [45], a pre-eminent sequence-to-sequence pre-\ntraining model that utilizes Transformer architecture.\n• ImgVAE [4], which employs variational autoencoder\ntechnology to integrate visual information into dia-\nlogue generation.\n• Maria [5], a visual experience-powered conversational\nagent that enriches dialogues with experiences from\nthe visual world through a large-scale image index.\n• Llama-2 [36], the LLM serving as our framework’s\nbackbone. It is a pre-trained autoregressive model\ncapable of generating natural and diverse text.\n• ChatGPT [2] by OpenAI, which leverages the GPT\narchitecture to generate human-like text responses,\nincorporating a mix of supervised and reinforcement\nlearning techniques for dialogue systems. We use the\ngpt-3.5-turbo version in this paper.\n• ZRIGF [15], a state-of-the-art model for zero-resource\nimage-grounded dialogue generation that combines\nmultimodal learning with a two-stage strategy.\nAmong them, ImgVAE, Maria, and ZRIGF are multi-\nmodal models, whereas the rest are unimodal text-based\nmodels. We have implemented all methods except for Img-\nVAE, where we directly used the outcomes from [4]. We\nprompt Llama-2 following the approach outlined in the\nHugging Face blog1, and employ ChatGPT in accordance\nwith the methodology described in [15].\n4.4. Evaluation Metrics\nIn accordance with [5] and [15], we use both automatic\nand human evaluation metrics to assess the performance\nof VIKDF and the baseline models.\nFor automatic evaluation, we employ the following met-\nrics: (1) Perplexity (PPL) measures the model’s flu-\nency, with lower values indicating better performance. (2)\nBLEU-1 [46] and ROUGE-L [47] evaluate the alignment\nof generated responses with human references, focusing on\nword-level accuracy and sequence similarity, respectively.\n(3) For semantic analysis, we employAverage, Extrema\n1https://huggingface.co/blog/llama2\nand Greedy metrics [48] to measure the cosine similar-\nity between word embeddings of generated and reference\ntexts, capturing semantic coherence. (4) Dis-1 and Dis-2\nmetrics [49] quantify the diversity of the model’s output\nby calculating the uniqueness of unigrams and bigrams,\nrespectively, ensuring the model’s capability to produce\nvaried and engaging responses.\nFor human evaluation, we engage three evaluators to col-\nlect ratings from human annotators. We randomly sample\n100 dialogues from the test set, and ask three evaluators\nto rate each dialogue on a scale of 1 to 5, based on the fol-\nlowing criteria: (1) Relevance: How relevant and related\nis the generated response to the given context and image?\n(2) Informativeness: How much new and useful infor-\nmation does the generated response provide in the context\nof the dialogue? (3) Fluency: How natural, readable,\nand grammatically correct is the generated response? The\nfinal score for each criterion is computed as the average\nrating across all evaluators. To ensure the reliability of\nthe evaluation process and measure the agreement among\nevaluators, Fleiss’ Kappa [50] statistic is applied to eval-\nuate the concordance among evaluators.\n5. Results and Discussion\nOur evaluations differentiate between models operating\nin distinct zero-resource scenarios, as denoted by † and\n‡. The † symbol signifies scenarios where models operate\nwithout using annotated images during training and in-\nference, while ‡ denotes a fully zero-resource condition, in\nwhich models generate dialogues without any prior train-\ning on task-specific datasets.\n5.1. Automatic Evaluation Results\nOur proposed VIKDF demonstrates outstanding perfor-\nmance in zero-resource dialogue generation, outperforming\nboth traditional and multimodal baseline models across\nvarious metrics on the Reddit Conversation and Image-\nChat datasets. To ensure the robustness and reliability\nof our findings, we conducted additional experiments and\nstatistical significance tests. Specifically, we ran VIKDF\nfive times with different random seeds and performed one-\nsample t-tests comparing VIKDF to the best-performing\nbaseline for each metric. Table 1 presents a comprehensive\ncomparison of the automated evaluation metrics.\nIn the Reddit Conversation task, VIKDF achieves a sig-\nnificantly lower perplexity of 15.01, indicating superior flu-\nency in generated dialogues compared to the nearest com-\npetitor, ZRIGF, which records a PPL of 36.21 in scenar-\nios with explicit image guidance. Moreover, VIKDF out-\nperforms all baselines in BLEU-1 and ROUGE-L scores,\nachieving 16.41 and 14.82, respectively, which highlights\nits ability to generate responses that are closely aligned\nwith human references. Additionally, VIKDF surpasses\nother models in both Average and Greedy semantic simi-\nlarity metrics, underscoring its enhanced ability to main-\ntain semantic coherence in dialogues. Despite scoring\n8\nTable 1: Assessment of automated metrics: † denotes a zero-resource scenario with no annotated images, while ‡ indicates a fully zero-resource\nscenario without any prior training on task-specific datasets. Bold font highlights the best performance in each column, and underlines signify\nthe second-best performance. Significant improvements are marked by ⋆ (p <0.05).\nTask Methods #Tuned\nParams PPL BLEU-1 ROUGE-L Average Extrema Greedy Dis-1 Dis-2\nReddit\nConversation\nSeq2Seq 24M 77.27 12.21 10.81 78.38 40.06 62.64 0.53 1.96\nBART 406M 44.73 13.51 12.50 80.21 41.63 63.72 4.17 16.98\nLlama-2‡ 0M 155.69 10.27 10.52 81.72 35.95 60.70 4.94 31.19\nChatGPT‡ 0M - 11.62 11.29 82.39 37.48 62.05 5.28 38.63⋆\nImgVAE - 72.06 12.58 12.05 79.95 42.38 63.55 1.52 6.34\nMaria 135M 56.23 14.10 12.66 81.76 43.04 63.98 4.83 22.87\nZRIGF 658M 36.21 16.06 14.51 82.27 43.79 64.53 5.79 26.57\nVIKDF 159M 14.98⋆ 16.47⋆ 14.84⋆ 82.54⋆ 43.84 64.88⋆ 6.53⋆ 35.12\nImage-Chat\nSeq2Seq 24M 50.82 11.34 13.65 82.95 47.45 65.67 1.28 7.80\nBART 406M 37.26 13.41 14.24 84.48 48.57 66.49 2.44 15.79\nLlama-2‡ 0M 193.20 9.93 11.56 85.44 40.18 63.05 4.69 30.81\nChatGPT‡ 0M - 10.77 11.62 86.17 43.02 64.66 5.32 37.77⋆\nImgVAE - 41.94 16.07 15.98 85.81 49.59 67.44 1.68 7.22\nMaria† 135M 37.49 14.74 14.59 85.72 50.58 66.89 2.57 11.99\nMaria‡zero 0M 135.49 11.75 12.13 83.51 45.57 64.48 1.89 7.32\nZRIGF† 658M 29.82 16.86 17.21 86.30 51.41⋆ 68.56 2.59 10.62\nZRIGF†\n1/4 658M 35.41 16.35 16.59 85.75 49.95 67.20 4.61 22.66\nZRIGF‡zero 0M 105.12 15.17 15.13 84.52 45.95 65.70 5.25 29.38\nVIKDF† 159M 12.54⋆ 17.93⋆ 17.45⋆ 86.67⋆ 50.88 68.53 4.41 23.05\n1/4 Data† 159M 12.84 17.81 16.91 86.07 50.01 67.55 4.40 21.53\n1/8 Data† 159M 13.12 17.70 16.84 85.98 49.76 67.35 4.28 20.54\nZero Data‡ 0M 27.32 15.35 15.30 85.04 45.86 66.02 6.17⋆ 32.93\nslightly lower on the Extrema metric compared to ZRIGF,\nVIKDF remains competitive. Although it has a marginally\nlower Dis-2 score than ChatGPT, VIKDF’s strong per-\nformance in generating diverse dialogues is evident. This\nsuggests that while maintaining high relevance and accu-\nracy, VIKDF also generates a wide range of responses,\ncontributing to more dynamic and engaging dialogues. No-\ntably, VIKDF, with only 159M tuned parameters, signif-\nicantly outperforms ZRIGF, which has 658M tuned pa-\nrameters, in 7 out of 8 evaluation metrics. This indicates\nthat VIKDF utilizes its parameters more efficiently, re-\nsulting in more coherent and contextually appropriate re-\nsponses. Furthermore, VIKDF achieves superior perfor-\nmance compared to models that depend on explicit im-\nages, highlighting its effectiveness in enhancing dialogue\ncapabilities without direct visual inputs. The outstanding\nperformance of VIKDF in the Reddit Conversation task\nhighlights the substantial benefits of incorporating implicit\nknowledge into purely text-based dialogue systems, prov-\ning that a deep fusion of visual and contextual understand-\ning significantly enhances the quality and engagement of\nthe dialogues generated.\nIn the Image-Chat task, VIKDF’s capabilities are ex-\namined under various conditions, including limited data\navailability scenarios (1/4 and 1/8 of the full training set)\nand fully zero-resource scenarios. We can see that VIKDF\nshowcases superior performance in various zero-resource\nscenarios, especially notable in the absence of annotated\nimages (†). Even without explicit images, VIKDF’s perfor-\nmance notably surpasses that of the state-of-the-art model\nTable 2: Human evaluation outcomes for the Image-Chat dataset in\na fully zero-resource scenario.\nMethods Relevance Informativeness Fluency Kappa\nLlama-2 2.94 2.75 4.27 0.55\nChatGPT 3.22 3.28 4.50 0.60\nZRIGF 3.55 3.15 4.20 0.57\nVIKDF 3.84 3.30 4.34 0.59\nZRIGF in almost all metrics, except for Extrema and\nGreedy, where it still achieves second-best performance.\nThe t-test results confirm that these improvements are\nnot due to random chance. Additionally, VIKDF demon-\nstrates resilience by maintaining stable performance de-\nspite substantial reductions in training data, highlighting\nits robustness and generalization ability. This is in stark\ncontrast to other models, which exhibit significant perfor-\nmance drops when transitioning from full data to zero data\nscenarios. Remarkably, VIKDF maintains its competitive\nedge even in fully zero-resource conditions ( ‡), showcas-\ning its exceptional capability to generate diverse, relevant,\nand engaging dialogues without task-specific training data.\nThe achievements of VIKDF on the Image-Chat dataset\naffirm its leading position in zero-resource dialogue gener-\nation, illustrating unmatched adaptability and advanced\nintegration of visual implicit knowledge.\n5.2. Human Evaluation Results\nTable 2 presents the human evaluation outcomes for the\nImage-Chat dataset within a fully zero-resource scenario\n9\nTable 3: Ablation study. Zero Data means in a fully zero-resource\nscenario.\nMethods Reddit Conversation Image-Chat (Zero Data)\nBLEU-1 ROUGE-L BLEU-1 ROUGE-L\nVIKDF 16.41 14.82 15.35 15.30\n-TIM 14.75 13.51 13.80 13.96\n-TAMIM 16.03 14.18 14.81 15.01\n-IAMTM 16.18 14.18 15.11 15.11\n-BVIF 15.65 13.79 14.60 14.63\n(‡), comparing the performance of VIKDF against Llama-\n2, ChatGPT, and ZRIGF. Given that the human evalu-\nation was conducted using a 5-point Likert scale, even a\nsmall absolute improvement in scores should be considered\nsignificant. Notably, VIKDF achieves the highest scores\nin both Relevance and Informativeness, indicating its ex-\nceptional ability to generate dialogues that are not only\nclosely related to the given context and images but also\nrich in informative content. Specifically, VIKDF achieves\nan 8.17% improvement in Relevance, a 4.76% improvement\nin Informativeness, and a 3.33% improvement in Fluency\nover ZRIGF. Although ChatGPT receives the highest rat-\ning in Fluency, VIKDF remains competitive, with a score\nof 4.34, illustrating its capacity to produce responses that\nare natural, coherent, and grammatically correct. This\ndemonstrates that VIKDF can generate dialogues which\nare both contextually relevant and engaging, with a high\ndegree of linguistic quality. The Fleiss’ Kappa score, in-\ndicative of inter-rater agreement, falls within a moderate\nrange across all models, ensuring that the evaluation pro-\ncess maintains a degree of reliability despite its inherently\nsubjective nature.\nIn summary, these results affirm VIKDF’s exemplary\nperformance in zero-resource dialogue generation, partic-\nularly its adeptness at leveraging visual implicit knowledge\nto augment the relevance and informativeness of dialogues,\nwhile maintaining a high standard of fluency.\n5.3. Ablation Study\nTo evaluate the individual contributions of the com-\nponents within VIKDF, we conduct an ablation study\nby sequentially removing each component and assess-\ning the impact on performance metrics (BLEU-1 and\nROUGE-L) across both Reddit Conversation and Image-\nChat datasets. In the absence of BVIF, we adopt the\nvision-to-language generative learning as outlined in [8].\nThe results, detailed in Table 3, demonstrate a consistent\ndecline in performance upon the exclusion of any com-\nponent, underscoring their collective importance to the\nframework’s efficacy. Notably, the exclusion of TIM yields\nthe most pronounced decline in performance metrics. This\nindicates TIM’s critical function in maintaining alignment\nbetween textual and visual modalities, a foundational as-\npect of generating coherent and contextually relevant dia-\nlogues. Additionally, omitting BVIF leads to a noticeable\nTable 4: Performance comparison of VIKDF integrated with different\nLLMs.\nMethods Reddit Conversation Image-Chat (Zero Data)\nBLEU-1 ROUGE-L BLEU-1 ROUGE-L\nLlamaLoRA 14.81 13.13 13.34 12.04\nQwenLoRA 15.04 13.26 13.44 12.01\nMistralLoRA 15.15 13.54 13.48 12.10\nVIKDFLlama 16.47 14.84 15.35 15.30\nVIKDFQwen 16.27 14.65 15.22 15.29\nVIKDFMistral 16.61 14.94 15.53 15.47\ndip in performance metrics. This underscores BVIF’s im-\nportance in seamlessly integrating distilled visual knowl-\nedge into the dialogue generation process, further enhanc-\ning the model’s ability to produce contextually rich and\nengaging dialogues. The removal of TAMIM and IAMTM\nalso leads to decreased performance. This result highlights\ntheir significance in enriching the model’s capability to in-\nfer and align multimodal knowledge, thereby facilitating a\nmore nuanced dialogue generation process.\nFurthermore, to assess the generalizability and robust-\nness of VIKDF across different large language models, we\nintegrated our framework with three state-of-the-art open-\nsource LLMs: Llama-2, Qwen2.5 [51, 52], and Mistral-v0.3\n[53], each with a parameter size of 7B. We first fine-tuned\neach LLM using LoRA [54] to establish baseline perfor-\nmances. Subsequently, we applied VIKDF to each LLM,\nfollowing the same knowledge distillation and integration\nprocesses described earlier. The performance metrics, pre-\nsented in Table 4, reveal that integrating VIKDF consis-\ntently enhances the BLEU-1 and ROUGE-L scores across\nall models and datasets. Notably, the improvements are\nmore pronounced in the zero-resource conditions on the\nImage-Chat dataset. This significant enhancement under-\nlines VIKDF’s effectiveness in leveraging implicit multi-\nmodal information to boost dialogue generation, especially\nwhen multimodal data is scarce. The consistent perfor-\nmance gains across different LLM architectures demon-\nstrate the adaptability of our framework, showcasing its\nability to seamlessly integrate with various models and\nimprove their capabilities in challenging zero-resource sce-\nnarios. This underscores the robustness of VIKDF and\nits potential for broad applicability in advancing dialogue\nsystems.\n5.4. Case study\nTo further illustrate the superior capabilities of VIKDF,\nwe conduct a case study contrasting VIKDF against key\nbaseline models by examining a specific example from the\nImage-Chat test set in a fully zero-resource scenario, as\nshown in Figure 5. In a dialogue context that evokes cu-\nriosity and apprehension towards the sky, VIKDF lever-\nages distilled visual implicit knowledge to generate a re-\nsponse that captures both the awe of the sky’s vast beauty\nand the curiosity towards its unknown mysteries. In com-\nparison, Llama-2 produces a relevant but less informative\n10\nFigure 5: Case study on Image-Chat test set in a fully zero-resource scenario.\nFigure 6: Example of visual implicit knowledge textualized from\ndialogue context by VIKDF.\nresponse. ChatGPT, while producing a context-aware re-\nsponse, misses the mark on integrating the emotive com-\nplexity of the conversation, focusing instead on extracting\nadditional context from the user. This demonstrates the\nlimitations of LLMs in multimodal dialog generation sce-\nnarios. In contrast, ZRIGF’s response, though creative,\ndiverges from the contextual context due to its reliance on\nretrieved explicit images, whereas VIKDF overcomes this\nby distilling visual implicit knowledge from text.\nTo provide a more detailed analysis of visual implicit\nknowledge, we attempt to textualize the visual implicit\nknowledge through the LLM, following the process illus-\ntrated in Figure 6. The process involves transforming the\ndialogue context C through the IQ-Former, which distills\nvisual implicit knowledge K solely from C, and subse-\nquently textualizing K via the LLM using a text prompt.\nImportantly, no image input is provided at this stage; the\nmodel relies entirely on the textual dialogue context. We\ncan see that the textual output illustrates the model’s ca-\npacity to encapsulate complex human emotions and cu-\nriosities about the sky, derived from the dialogue context.\nThis demonstrates the model’s adeptness at integrating\nand expressing visual implicit knowledge. While the LLM\npossesses substantial world knowledge due to pre-training,\nthe integration of the distilled knowledge K allows it to\ncapture visual implicit knowledge that is contextually rel-\nevant and may not be explicitly encoded in the LLM’s pre-\ntrained weights. Thus, the LLM produces responses that\nreflect both its pre-existing knowledge and the newly dis-\ntilled visual implicit knowledge pertinent to the dialogue\ncontext.\nThis analysis underscores VIKDF’s superior ability to\nsynthesize and leverage visual implicit knowledge, enabling\nit to generate dialogues that are more engaging, visually\ngrounded, and contextually appropriate.\n11\n(a) Reddit Conversation\n (b) Image-Chat (Zero Data)\nFigure 7: Performance comparison with various numbers of queries.\n5.5. Impact of Query Vector Quantity\nExploring the influence of the hyper-parameter n, which\ndictates the number of query vectors deployed in the IQ-\nFormer, is crucial for understanding the dynamics of the\nVIKDF in dialogue generation tasks. To this end, we ad-\njust n across 4, 8, 16, 32, and 64 to examine its impact\non model performance, focusing on BLEU-1 and ROUGE-\nL scores within the Reddit Conversation and Image-Chat\ndatasets. Figure 7 presents the results, which reveal a nu-\nanced relationship between the quantity of query vectors\nand model performance. Remarkably, a configuration of\nn = 32 is identified as optimal, yielding the highest BLEU\nand ROUGE scores across both datasets. This suggests an\nideal balance in leveraging visual implicit knowledge: too\nfew query vectors (n <32) may not capture the breadth of\nimplicit knowledge available, whereas too many ( n >32)\ncould introduce unnecessary noise or dilute the relevance\nof the distilled knowledge.\nThe experiment highlights the importance of a balanced\nquery vector count in achieving effective dialogue genera-\ntion. An optimal n allows the IQ-Former to distill rele-\nvant visual implicit knowledge without overcomplicating\nthe model, demonstrating the delicate balance between\nquantity and quality of distilled knowledge for enhancing\ndialogue generation.\n6. Conclusion and Future Work\nIn this paper, we present VIKDF, an innovative method-\nology aimed at enhancing LLMs for dialogue generation in\nzero-resource scenarios through the distillation and inte-\ngration of implicit multimodal knowledge. VIKDF utilizes\nan IQ-Former to extract visual implicit knowledge and a\nBVIF technique to incorporate this knowledge into LLMs,\nenabling the generation of dialogues that are coherent, en-\ngaging, and rich in contextual understanding. Our com-\nprehensive experiments across diverse dialogue datasets\nhave shown that VIKDF outperforms existing state-of-the-\nart models in zero-resource scenarios, illustrating its effec-\ntiveness in leveraging implicit multimodal knowledge even\nwithout explicit multimodal inputs or annotated datasets.\nThe ablation study underscores the indispensable role of\neach component within VIKDF, and human evaluations\nhave confirmed its success in generating dialogues that are\nrelevant, informative, and naturally fluent, closely aligning\nhuman conversational standards. Consequently, VIKDF\nrepresents a significant advancement in the field of multi-\nmodal dialogue generation, highlighting the importance of\nimplicit multimodal knowledge in enhancing LLMs capa-\nbilities in zero-resource scenarios.\nThe proposed model utilizes only implicit multimodal\ninformation, which limits its applicability in tasks requir-\ning explicit multimodal inputs, such as visual question an-\nswering, and multimodal outputs, such as text-to-image\ngeneration. In future work, we plan to integrate both\nexplicit and implicit multimodal information to develop\na dialogue generation system capable of supporting both\nmultimodal inputs and outputs. This advancement will\nenable our model to engage more comprehensively with\nvarious types of content, potentially enhancing its perfor-\nmance and applicability in multimodal interaction scenar-\nios.\nAcknowledgements\nThis research is supported by the National Natural Sci-\nence Foundation of China (No. 62006034) and the Anhui\nProvincial Natural Science Foundation (2408085QF188).\nReferences\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\nS. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse,\n12\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei,\nLanguage models are few-shot learners, in: Advances in Neural\nInformation Processing Systems, Vol. 33, 2020, pp. 1877–1901.\n[2] OpenAI, Introducing ChatGPT (nov 2022).\nURL https://openai.com/blog/chatgpt/\n[3] OpenAI, GPT-4 technical report, arXiv:2303.08774 (2023).\n[4] Z. Yang, W. Wu, H. Hu, C. Xu, W. Wang, Z. Li, Open domain\ndialogue generation with latent images, in: Proceedings of the\nAAAI Conference on Artificial Intelligence, Vol. 35, 2021, pp.\n14239–14247.\n[5] Z. Liang, H. Hu, C. Xu, C. Tao, X. Geng, Y. Chen, F. Liang,\nD. Jiang, Maria: A visual experience powered conversational\nagent, in: Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), 2021, pp. 5596–5611.\n[6] L. Shen, H. Zhan, X. Shen, Y. Song, X. Zhao, Text is not\nenough: Integrating visual impressions into open-domain dia-\nlogue generation, in: Proceedings of the 29th ACM Interna-\ntional Conference on Multimedia, 2021, pp. 4287–4296.\n[7] Z. Ding, Z. Yang, L. Luo, Y. Sun, H. Lin, From retrieval to gen-\neration: A simple and unified generative model for end-to-end\ntask-oriented dialogue, in: Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 38, 2024, pp. 17907–17914.\n[8] J. Li, D. Li, S. Savarese, S. Hoi, BLIP-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and\nlarge language models, in: Proceedings of the 40th International\nConference on Machine Learning, Vol. 202, 2023, pp. 19730–\n19742.\n[9] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,\nK. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Sal-\nimans, J. Ho, D. J. Fleet, M. Norouzi, Photorealistic text-to-\nimage diffusion models with deep language understanding, in:\nAdvances in Neural Information Processing Systems, Vol. 35,\n2022, pp. 36479–36494.\n[10] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt:\nTowards detailed video understanding via large vision and lan-\nguage models, arXiv:2306.05424 (2023).\n[11] D. Ghosal, N. Majumder, A. Mehrish, S. Poria, Text-to-audio\ngeneration using instruction guided latent diffusion model, in:\nProceedings of the 31st ACM International Conference on Mul-\ntimedia, 2023, pp. 3590–3598.\n[12] N. Mostafazadeh, C. Brockett, B. Dolan, M. Galley, J. Gao,\nG. Spithourakis, L. Vanderwende, Image-grounded conversa-\ntions: Multimodal context for natural question and response\ngeneration, in: Proceedings of the Eighth International Joint\nConference on Natural Language Processing (Volume 1: Long\nPapers), 2017, pp. 462–472.\n[13] K. Shuster, S. Humeau, A. Bordes, J. Weston, Image-chat: En-\ngaging grounded conversations, in: Proceedings of the 58th An-\nnual Meeting of the Association for Computational Linguistics,\n2020, pp. 2414–2429.\n[14] Y. Zheng, G. Chen, X. Liu, J. Sun, MMChat: Multi-modal chat\ndataset on social media, in: Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, 2022, pp. 5778–\n5786.\n[15] B. Zhang, J. Wang, H. Ma, B. Xu, H. Lin, ZRIGF: An inno-\nvative multimodal framework for zero-resource image-grounded\ndialogue generation, in: Proceedings of the 31st ACM Interna-\ntional Conference on Multimedia, 2023, pp. 5464–5473.\n[16] A. Richardson, Mental Imagery, Springer, Berlin, Heidelberg,\n1969.\n[17] R. Jackendoff, Foundations of Language: Brain, Meaning,\nGrammar, Evolution, Oxford University Press, 2002.\n[18] S. M. Kosslyn, W. L. Thompson, G. Ganis, The Case for Mental\nImagery, Oxford University Press, 2006.\n[19] D. Roy, K.-Y. Hsiao, N. Mavridis, Mental imagery for a con-\nversational robot, IEEE Transactions on Systems, Man, and\nCybernetics, Part B (Cybernetics) 34 (3) (2004) 1374–1383.\n[20] J. Y. Koh, R. Salakhutdinov, D. Fried, Grounding language\nmodels to images for multimodal inputs and outputs, in: Pro-\nceedings of the 40th International Conference on Machine\nLearning, 2023.\n[21] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison,\nS. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu,\nT. Le Scao, S. Gugger, M. Drame, Q. Lhoest, A. Rush, Trans-\nformers: State-of-the-art natural language processing, in: Pro-\nceedings of the 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations, 2020, pp.\n38–45.\n[22] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura,\nD. Parikh, D. Batra, Visual dialog, in: Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017,\npp. 326–335.\n[23] H. Alamri, V. Cartillier, A. Das, J. Wang, A. Cherian, I. Essa,\nD. Batra, T. K. Marks, C. Hori, P. Anderson, et al., Audio\nvisual scene-aware dialog, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019,\npp. 7558–7567.\n[24] Z. Liang, H. Hu, C. Xu, C. Tao, X. Geng, Y. Chen, F. Liang,\nD. Jiang, Maria: A visual experience powered conversational\nagent, in: Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), 2021, pp. 5596–5611.\n[25] J. Feng, Q. Sun, C. Xu, P. Zhao, Y. Yang, C. Tao,\nD. Zhao, Q. Lin, Mmdialog: A large-scale multi-turn dia-\nlogue dataset towards multi-modal open-domain conversation,\narXiv:2211.05719 (2022).\n[26] J. Y. Koh, R. Salakhutdinov, D. Fried, Grounding language\nmodels to images for multimodal inputs and outputs, in: In-\nternational Conference on Machine Learning, 2023, pp. 17283–\n17300.\n[27] Q. Sun, Y. Wang, C. Xu, K. Zheng, Y. Yang, H. Hu, F. Xu,\nJ. Zhang, X. Geng, D. Jiang, Multimodal dialogue response\ngeneration, in: Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Pa-\npers), 2022, pp. 2854–2866.\n[28] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Ag-\ngarwal, O. K. Mohammed, S. Singhal, S. Som, F. Wei, Image\nas a foreign language: Beit pretraining for vision and vision-\nlanguage tasks, in: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2023,\npp. 19175–19186.\n[29] X. Xue, C. Zhang, Z. Niu, X. Wu, Multi-level attention map\nnetwork for multimodal sentiment analysis, IEEE Transactions\non Knowledge and Data Engineering 35 (5) (2023) 5105–5118.\n[30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\nG. Krueger, I. Sutskever, Learning transferable visual models\nfrom natural language supervision, in: Proceedings of the 38th\nInternational Conference on Machine Learning, Vol. 139, 2021,\npp. 8748–8763.\n[31] W. Dai, L. Hou, L. Shang, X. Jiang, Q. Liu, P. Fung, Enabling\nmultimodal generation on CLIP via vision-language knowledge\ndistillation, in: Findings of the Association for Computational\nLinguistics: ACL 2022, 2022, pp. 2383–2395.\n[32] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, in:\nAdvances in Neural Information Processing Systems, Vol. 36,\n2023, pp. 34892–34916.\n[33] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu,\nJ. Jia, Mini-gemini: Mining the potential of multi-modality vi-\nsion language models, arXiv:2403.18814 (2024).\n[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez,  L. Kaiser, I. Polosukhin, Attention is all you need, in:\nAdvances in neural information processing systems, 2017, pp.\n5998–6008.\n[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, et al., An image is worth 16x16 words: Transformers\n13\nfor image recognition at scale, in: International Conference on\nLearning Representations, 2020.\n[36] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,\nY. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,\net al., Llama 2: Open foundation and fine-tuned chat models,\narXiv:2307.09288 (2023).\n[37] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding, in: Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), 2019, pp. 4171–4186.\n[38] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,\nP. Abbeel, Infogan: Interpretable representation learning by in-\nformation maximizing generative adversarial nets, in: D. Lee,\nM. Sugiyama, U. Luxburg, I. Guyon, R. Garnett (Eds.), Ad-\nvances in Neural Information Processing Systems, Vol. 29, 2016.\n[39] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll´ ar, C. L. Zitnick, Microsoft coco: Common ob-\njects in context, in: Computer Vision – ECCV 2014, 2014, pp.\n740–755.\n[40] P. Sharma, N. Ding, S. Goodman, R. Soricut, Conceptual cap-\ntions: A cleaned, hypernymed, image alt-text dataset for au-\ntomatic image captioning, in: Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), Association for Computational Linguis-\ntics, Melbourne, Australia, 2018, pp. 2556–2565.\n[41] S. Changpinyo, P. Sharma, N. Ding, R. Soricut, Conceptual\n12m: Pushing web-scale image-text pre-training to recognize\nlong-tail visual concepts, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2021, pp. 3558–3568.\n[42] V. Ordonez, G. Kulkarni, T. Berg, Im2text: Describing images\nusing 1 million captioned photographs, in: Advances in Neural\nInformation Processing Systems, Vol. 24, 2011.\n[43] I. Loshchilov, F. Hutter, Decoupled weight decay regulariza-\ntion, in: International Conference on Learning Representations,\n2019.\n[44] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learn-\ning with neural networks, in: Z. Ghahramani, M. Welling,\nC. Cortes, N. Lawrence, K. Weinberger (Eds.), Advances in\nNeural Information Processing Systems, Vol. 27, 2014.\n[45] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V. Stoyanov, L. Zettlemoyer, BART: Denoising\nsequence-to-sequence pre-training for natural language gener-\nation, translation, and comprehension, in: Proceedings of the\n58th Annual Meeting of the Association for Computational Lin-\nguistics, 2020, pp. 7871–7880.\n[46] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, Bleu: a method for\nautomatic evaluation of machine translation, in: Proceedings of\nthe 40th Annual Meeting of the Association for Computational\nLinguistics, 2002, pp. 311–318.\n[47] C.-Y. Lin, ROUGE: A package for automatic evaluation of sum-\nmaries, in: Text Summarization Branches Out, 2004, pp. 74–81.\n[48] C.-W. Liu, R. Lowe, I. Serban, M. Noseworthy, L. Charlin,\nJ. Pineau, How NOT to evaluate your dialogue system: An\nempirical study of unsupervised evaluation metrics for dialogue\nresponse generation, in: Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing, 2016, pp.\n2122–2132.\n[49] J. Li, M. Galley, C. Brockett, J. Gao, B. Dolan, A diversity-\npromoting objective function for neural conversation models,\nin: Proceedings of the 2016 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2016, pp. 110–119.\n[50] J. L. Fleiss, J. Cohen, The equivalence of weighted kappa and\nthe intraclass correlation coefficient as measures of reliability,\nEducational and psychological measurement 33 (3) (1973) 613–\n619.\n[51] Q. Team, Qwen2.5: A party of foundation models (September\n2024).\nURL https://qwenlm.github.io/blog/qwen2.5/\n[52] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li,\nC. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang,\nJ. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Xu, J. Zhou,\nJ. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen, K. Yang, M. Li,\nM. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao,\nR. Lin, S. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge,\nX. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren, Y. Fan,\nY. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang,\nZ. Fan, Qwen2 technical report, arXiv:2407.10671 (2024).\n[53] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S.\nChaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample,\nL. Saulnier, et al., Mistral 7b, arXiv:2310.06825 (2023).\n[54] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,\nL. Wang, W. Chen, LoRA: Low-rank adaptation of large lan-\nguage models, in: International Conference on Learning Repre-\nsentations, 2022.\n14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6777538061141968
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.6776471138000488
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5122934579849243
    },
    {
      "name": "Natural language processing",
      "score": 0.3974666893482208
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3483387231826782
    },
    {
      "name": "Knowledge management",
      "score": 0.3396438658237457
    },
    {
      "name": "Linguistics",
      "score": 0.2844569683074951
    },
    {
      "name": "Philosophy",
      "score": 0.07291141152381897
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}