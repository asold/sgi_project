{
  "title": "Hybrid CNN-Transformer Feature Fusion for Single Image Deraining",
  "url": "https://openalex.org/W4382449856",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105016708",
      "name": "Xiang Chen",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100349836",
      "name": "Jinshan Pan",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4224881034",
      "name": "Jiyang Lu",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2409951955",
      "name": "Zhentao Fan",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2097763185",
      "name": "Hao Li",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105016708",
      "name": "Xiang Chen",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100349836",
      "name": "Jinshan Pan",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4224881034",
      "name": "Jiyang Lu",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2409951955",
      "name": "Zhentao Fan",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2097763185",
      "name": "Hao Li",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6682198495",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W6647720530",
    "https://openalex.org/W2121396509",
    "https://openalex.org/W1982471090",
    "https://openalex.org/W2209874411",
    "https://openalex.org/W6719777491",
    "https://openalex.org/W6691861242",
    "https://openalex.org/W2613034492",
    "https://openalex.org/W6730307226",
    "https://openalex.org/W2509784253",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W2788708632",
    "https://openalex.org/W2884068670",
    "https://openalex.org/W6755349620",
    "https://openalex.org/W3090791806",
    "https://openalex.org/W2902763051",
    "https://openalex.org/W2912435603",
    "https://openalex.org/W6750018563",
    "https://openalex.org/W2930755307",
    "https://openalex.org/W2995668749",
    "https://openalex.org/W3013510249",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3012965216",
    "https://openalex.org/W3105842518",
    "https://openalex.org/W3193890688",
    "https://openalex.org/W6799086289",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3202915404",
    "https://openalex.org/W6797232612",
    "https://openalex.org/W6790137093",
    "https://openalex.org/W6800689796",
    "https://openalex.org/W6786585107",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W4226113773",
    "https://openalex.org/W4286984630",
    "https://openalex.org/W6838994554",
    "https://openalex.org/W4312617404",
    "https://openalex.org/W4286750439",
    "https://openalex.org/W3212228063",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W6840201148",
    "https://openalex.org/W3041172212",
    "https://openalex.org/W2901339722",
    "https://openalex.org/W2780930362",
    "https://openalex.org/W6742120935",
    "https://openalex.org/W2913360047",
    "https://openalex.org/W3023003829",
    "https://openalex.org/W2114770744",
    "https://openalex.org/W3108452013",
    "https://openalex.org/W1990592195",
    "https://openalex.org/W4225871896",
    "https://openalex.org/W3035326127",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W2256362396",
    "https://openalex.org/W4312399981",
    "https://openalex.org/W2964297221",
    "https://openalex.org/W2559264300",
    "https://openalex.org/W2466666260",
    "https://openalex.org/W4313022653",
    "https://openalex.org/W4230654051",
    "https://openalex.org/W3028045870",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W3186182384",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3035250394",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4304099129",
    "https://openalex.org/W2964212750",
    "https://openalex.org/W2740982616",
    "https://openalex.org/W4304208733",
    "https://openalex.org/W3170697543",
    "https://openalex.org/W4283023197",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2894683476",
    "https://openalex.org/W3119109130",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W4287020683",
    "https://openalex.org/W3175186117",
    "https://openalex.org/W2962754725",
    "https://openalex.org/W2102166818"
  ],
  "abstract": "Since rain streaks exhibit diverse geometric appearances and irregular overlapped phenomena, these complex characteristics challenge the design of an effective single image deraining model. To this end, rich local-global information representations are increasingly indispensable for better satisfying rain removal. In this paper, we propose a lightweight Hybrid CNN-Transformer Feature Fusion Network (dubbed as HCT-FFN) in a stage-by-stage progressive manner, which can harmonize these two architectures to help image restoration by leveraging their individual learning strengths. Specifically, we stack a sequence of the degradation-aware mixture of experts (DaMoE) modules in the CNN-based stage, where appropriate local experts adaptively enable the model to emphasize spatially-varying rain distribution features. As for the Transformer-based stage, a background-aware vision Transformer (BaViT) module is employed to complement spatially-long feature dependencies of images, so as to achieve global texture recovery while preserving the required structure. Considering the indeterminate knowledge discrepancy among CNN features and Transformer features, we introduce an interactive fusion branch at adjacent stages to further facilitate the reconstruction of high-quality deraining results. Extensive evaluations show the effectiveness and extensibility of our developed HCT-FFN. The source code is available at https://github.com/cschenxiang/HCT-FFN.",
  "full_text": "Hybrid CNN-Transformer Feature Fusion for Single Image Deraining\nXiang Chen1, Jinshan Pan1*, Jiyang Lu2, Zhentao Fan2, Hao Li1\n1 School of Computer Science and Engineering, Nanjing University of Science and Technology\n2 College of Electronic Information Engineering, Shenyang Aerospace University\n{chenxiang, haoli}@njust.edu.cn, sdluran@gmail.com, {lujiyang1, fanzhentao}@stu.sau.edu.cn\nAbstract\nSince rain streaks exhibit diverse geometric appearances and\nirregular overlapped phenomena, these complex characteris-\ntics challenge the design of an effective single image derain-\ning model. To this end, rich local-global information repre-\nsentations are increasingly indispensable for better satisfy-\ning rain removal. In this paper, we propose a lightweight\nHybrid CNN-Transformer Feature Fusion Network (dubbed\nas HCT-FFN) in a stage-by-stage progressive manner, which\ncan harmonize these two architectures to help image restora-\ntion by leveraging their individual learning strengths. Specif-\nically, we stack a sequence of the degradation-aware mix-\nture of experts (DaMoE) modules in the CNN-based stage,\nwhere appropriate local experts adaptively enable the model\nto emphasize spatially-varying rain distribution features. As\nfor the Transformer-based stage, a background-aware vi-\nsion Transformer (BaViT) module is employed to comple-\nment spatially-long feature dependencies of images, so as to\nachieve global texture recovery while preserving the required\nstructure. Considering the indeterminate knowledge discrep-\nancy among CNN features and Transformer features, we in-\ntroduce an interactive fusion branch at adjacent stages to fur-\nther facilitate the reconstruction of high-quality deraining re-\nsults. Extensive evaluations show the effectiveness and ex-\ntensibility of our developed HCT-FFN. The source code is\navailable at https://github.com/cschenxiang/HCT-FFN.\nIntroduction\nSingle image deraining (SID) is the task of recovering clear\nand rain-free background from the given rainy images, since\nthe images captured under rainy conditions significantly de-\ngrade the performance of downstream computer vision sys-\ntems (including autonomous driving and video surveillance,\netc.), which has drawn widespread attention in recent years.\nEarly prior-based methods (Kang, Lin, and Fu 2011; Luo,\nXu, and Ji 2015; Li et al. 2016; Zhang and Patel 2017) at-\ntempt to remove the rain by relying on statistical properties\nof rain components and clear backgrounds. However, these\nhand-crated priors from human observation may not always\nhold in case of the complex and varying rainy scenarios.\nTo circumvent hypothetical priors dependency, numerous\nCNN-based networks (Yang et al. 2020; Yu et al. 2022) have\n*Corresponding author\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n0 5 10 15 20 25\nNumber…of…Parameters…(Millions)\n35\n36\n37\n38\n39\n40PSNR…(dB)\nDCSFNRESCAN\nPReNet\nSPANet\nNLEDN\nRCDNet Uformer-S\nMPRNet\nMSPFN\nOurs\nPSNR…vs.…Params\nFigure 1: Comparison results on the Rain100L dataset. Our\nmethod not only reconstructs a high-quality output but also\nachieves the best performance-parameter trade-off.\nbeen proposed for SID, which achieves remarkable progress\nthanks to the rapid growing complicated architectures (Jiang\net al. 2020; Zamir et al. 2021; Mou, Wang, and Zhang 2022)\nand learning strategies (Zhou et al. 2021; Xiao et al. 2021;\nChen et al. 2022). However, these approaches still encounter\nperformance bottlenecks due to the local receptive fields of\nthe CNN-based operations, which limits the ability to cope\nwith long-range dependency information. To this end, recent\nTransformers have emerged in computer vision field (Doso-\nvitskiy et al. 2020; Chen et al. 2021), which is attributed\nto the unique advantage of self-attention with global feature\ninteraction. Since then, several modified Transformer-based\narchitectures (Xiao et al. 2022; Wang et al. 2022; Zamir et al.\n2022) have also been developed for SID task achieving su-\nperior performance over previous CNN-based models.\nSince the rain streak layer and rain-free background layer\nare highly interlaced, global and local representation learn-\ning are equally important for the challenging SID task, while\nthe self-attention in Transformer does not manipulate the lo-\ncal invariance that CNNs do well. Afterwards, some studies\n(Yuan et al. 2021; Wu et al. 2021) attempt to introduce con-\nvolutional operations into vision Transformers, but they do\nnot play a full role for low-level image restoration. To miti-\ngate this problem, a few recent works (Guo et al. 2022; Jiang\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n378\net al. 2022) try to combine these two structures to construct\na hybrid model aiming to inherit advantages of CNN and\nTransformer. This naturally raises a crucial question: how\nto effectively integrate both CNN features and Transformer\nfeatures? In fact, an intuitive observation is that there are\nindeterminate knowledge discrepancies among convolution-\nbased CNN features and self-attention-based Transformer\nfeatures (Park and Kim 2022), thus simply concatenating\nor adding these features is inefficient for significant perfor-\nmance gain. Therefore, it is of great interest to tailor design\nfusion models so that they can better facilitate rain removal.\nIn this work, we present a new hybrid network that com-\nbines the features by CNN and Transformer for compre-\nhensive rain distribution prediction, which is expected to\nproduce better deraining results than any individual model.\nFollowing the past successful inspired designs (Ren et al.\n2019; Jiang et al. 2020), we specifically formulate our fu-\nsion framework in a stage-by-stage progressive fashion due\nto the complexity of SID. To alleviate the learning difficulty,\nwe propose to separately extract the intra-stage hierarchical\nfeature via a backbone branch and adaptively aggregate the\ninter-stage complementary feature via an auxiliary branch.\nAs such, all the stage representations are richer.\nSpecifically, the body of Hybrid CNN-Transformer Fea-\nture Fusion Network (HCT-FFN) consists of three sequential\nstages, which can excavate the useful information from pre-\nvious stage to guide the later stage. In the backbone branch,\nCNN-based backbone is applied in the first and last stages,\nwhile Transformer-based backbone is used in the intermedi-\nate stage. In terms of the CNN-based stage, we stack a series\nof degradation-aware mixture of experts (DaMoE) modules\nthat adaptively restore an image degraded by the spatially-\nvarying rain distribution. By doing so, experts (local CNN\noperations in parallel) are able to focus on assigning cor-\nresponding intensity weights for different degradation fac-\ntors depending on the inputs, so that we can facilitate the\nmodel to adaptively remove rainy effects of different appear-\nances. As for the Transformer-based stage, a background-\naware vision Transformer (BaViT) module is employed to\neliminate the spatially-long rain degradation by modeling\nlong-range dependencies, since the multi-head self-attention\nfacilitates global texture and structure recovery. Instead of\nsimply concatenating the features of two adjacent stages, we\nalso introduce an interactive fusion branch (IFB) to encode\nthe inter-stage correlation among backbone features and re-\nconstruction features. In this way, IFB can allow to explore\ncomplementary components of hybrid features by CNN and\nTransformer from each other through stage-wise reconstruc-\ntion for further refinement. Finally, comprehensive experi-\nments show that our hybrid fusion model achieves the best\nperformance-parameter trade-off, as shown in Fig. 1.\nOur main contributions are summarized as follows:\n• We propose an end-to-end hybrid model for SID, HCT-\nFFN, integrating the intra-stage advantages of CNN and\nTransformer paradigms to achieve a strong deraining\nbaseline in a stage-by-stage unified architecture.\n• We show that the inter-stage interactive fusion can alle-\nviate the knowledge discrepancy among the features by\nCNN and Transformer, in order to better facilitate rain\nremoval.\n• We perform extensive experiments to demonstrate the ef-\nfectiveness and extensibility of the proposed HCT-FFN.\nRelated Work\nSingle Image Deraining\nEarly deep CNN-based networks (Yang et al. 2017; Fu et al.\n2017a; Zhang and Patel 2018; Li et al. 2018) have emerged\nfor SID as a better option compared to hand-crafted pri-\nors. By further optimizing the network structure, researchers\nemploy the recursive computation (Ren et al. 2019; Zamir\net al. 2021) or the multi-scale representation (Yasarla and\nPatel 2019; Jiang et al. 2020) to effectively produce rain-free\nresults. Instead of prevailing CNN-based pipeline, Trans-\nformer is recently introduced as a new network backbone to\naccount for performance gain. For low-level image restora-\ntion, typical architectures include IPT (Chen et al. 2021),\nRestormer (Zamir et al. 2022), and Uformer (Wang et al.\n2022). However, most of these methods blindly stack pure\nTransformer-based components to replace original CNNs,\nwhich inevitably generates high computational cost lead-\ning to a bloated model with an excessive amount of pa-\nrameters. For instance, concurrent Restormer (Zamir et al.\n2022) requires 26.10 Million parameters to obtain competi-\ntive results. Few attempts have been made to fully consider\ncomplementary merits between CNN and Transformer, thus\ndifficult to enable the model to offer the optimal balance\nbetween size and performance. More recently, ELF (Jiang\net al. 2022) is first presented to unify these two architec-\ntures into an association learning-based lightweight hybrid\nderaining model. Different from it, inspired by the progres-\nsive learning-based formulation, we are committed to design\na new hybrid deraining network by gradually removing rain\nstreaks in a stage-by-stage manner.\nVision Transformer\nTransformer-based models (Vaswani et al. 2017) originally\nbring significant breakthroughs to the natural language pro-\ncessing (NLP) field. Benefiting from the powerful capability\nin modeling long-range information with the help of the self-\nattention mechanism, the birth of Vision Transformer (ViT)\n(Dosovitskiy et al. 2020) makes computer vision community\nshine again, which has witnessed prominent improvements\namong high-level vision tasks (Carion et al. 2020; Liu et al.\n2021; Zheng et al. 2021). Likewise, recent studies have ap-\nplied variants of ViT in a host of low-level vision problems\nand opened up a new perspective, such as image dehazing\n(Guo et al. 2022), and image super-resolution (Gao et al.\n2022). Furthermore, in terms of recent hybrid models, infor-\nmation fusion between Transformer features and CNN fea-\ntures has become a key step. In (Guo et al. 2022), these fea-\ntures are aggregated by learning the modulation matrices to\nsolve the feature inconsistency issue. However, these fusions\nlack interactivity because only limited intra-stage connec-\ntions are considered. In this paper, we propose to explore the\ninter-stage interactive fusion to guide image reconstruction\nby encoding the correlation among these two joint features.\n379\nFigure 2: The overall framework of the proposed Hybrid CNN-Transformer Feature Fusion Network (HCT-FFN), which mainly\ncontains (1) degradation-aware mixture of experts (DaMoE) module, (2) background-aware vision Transformer (BaViT) mod-\nule, and (3) interactive fusion branch (IFB) with prior guidance block (PGB) and coupled representation block (CRB).\nProposed Method\nThis section mainly introduces the proposed end-to-end Hy-\nbrid CNN-Transformer Feature Fusion Network (HCT-FFN)\nto remove undesirable rain streaks in a stage-by-stage man-\nner. The whole framework is illustrated in Fig. 2, which con-\ntains three recursive stages. In the first and last CNN-based\nstages, we stack the degradation-aware mixture of experts\n(DaMoE) modules as the network backbone to extract local\nfeatures for spatially-varying rain degradation. Meanwhile,\na background-aware vision Transformer (BaViT) module is\nemployed as the backbone of the intermediate Transformer-\nbased stage to capture global dependencies for spatially-long\nrain appearance. Furthermore, prior guidance block (PGB)\nand coupled representation block (CRB) are incorporated in\ninteractive fusion branch (IFB) to further provide comple-\nmentary information for the model, so that high-quality clear\noutputs can be gradually reconstructed. To enable the model\nto learn richer features during image restoration process, we\nfuse the output features of the previous stage with the out-\nput features of the current stage using the skip-connection\nand stage-level concatenation. With this design, useful in-\nformation from the previous stage can be fully excavated to\nguide the later stage, allowing the redundant feature to deep\nlayers without too much processing, thus selectively focus-\ning on more important information. In what follows, we will\ndescribe the details about the above-mentioned components.\nDegradation-aware Mixture of Experts\nIn the CNN-based stages, DaMoE module is the key part to\nsuccessfully restore complicated rain distribution. Consider-\ning the design of recent effective CNN models (Suganuma,\nLiu, and Okatani 2019), we elaborately select multiple local\nCNN operations to form parallel layers, dubbed as experts,\nwhich involve a average pooling with receptive field of3×3,\nseparable convolution layers with kernel sizes of1×1, 3×3,\n5 ×5, 7 ×7, and dilated convolution layers with kernel sizes\nof 3×3, 5×5, 7×7. Different from the conventional mixture\nof experts (Jacobs et al. 1991; Ren et al. 2018), our DaMoE\nmodule does not attach an external gating network. Instead,\nwe make the self-attention scheme (Hu, Shen, and Sun 2018;\nKim, Ahn, and Sohn 2020) become a switcher of different\nexperts to adaptively select the importance of diverse rep-\nresentations depending on the inputs, which will collabora-\ntively help context aggregation. Given an input feature map\nxc ∈ RC×H×W , we first apply the channel-wise average to\ngenerate C-dimensional channel descriptor zc ∈ RC:\nzc = 1\nH × W\nHX\ni=1\nWX\nj=1\nxc(i, j), (1)\nwhere xc(i, j) is the (y, x) position of the feature xc. Then,\nthe attention weight vector of each expert is allocated corre-\nsponding to the learnable weight matrices W1 ∈ RT×C and\nW2 ∈ RO×T . Here, T is the dimension of the weight matri-\nces. To avoid altering the sizes of its inputs and outputs, we\nzero pad the input feature maps computed by each expert.\nWith this formulation, the extracted degradation features by\nfeeding a DaMoE module denotes Fdeg, then we have\nFdeg = [fO\nexp(W2σ (W1z))], for O = 1,2, . . . , k(2)\nwhere fexp and O represent the expert layer and the number\nof experts respectively. σ(·) is a ReLU function, and [·] de-\nnotes the channel-wise concatenation. Note that we employ\na skip connection between each DaMoE module to bridge\nacross continuous intermediate features for stable training.\nFinally, the output of the N-th DaMoE module is calcu-\nlated by\nFN\nDaMoE = f1×1(Fdeg) +FN−1\nDaMoE , (3)\nwhere f1×1(·) denotes a convolutional layer with C filters.\n380\nBackground-aware Vision Transformer\nIn the Transformer-based stage, we introduce a BaViT mod-\nule to help accurate background recovery, thanks to the ad-\nvantage of Transformer in learning long-range dependencies\nwithin the global information. Unlike ViT (Dosovitskiy et al.\n2020), we first employ the unfold operation to split the input\nfeature maps Fin into H × W patches F∗\nin ∈ Rk×k×C by a\nk×k kernel. Intuitively, this pre-processing step naturally re-\nflects the position information of each patch. Following that,\nthese patches are sent directly to the body of BaViT module.\nMathematically, the encoding procedures are expressed as\nFmid = F∗\nin + fMHSA (fNorm (F∗\nin)) , (4)\nFBaV iT = Fmid + fFFN (fNorm (Fmid)) , (5)\nwhere fMHSA (·) and fFFN (·) denote the multi-head self-\nattention (MHSA) and feed-forward network (FFN), respec-\ntively. fNorm (·) refers to the layer normalization operation.\nFinally, we use thefold operation to reconstruct feature maps\nFBaV iT of the BaViT module.\nAs shown in Fig. 2, MHSA and FFN are key ingredients\nfor Transformer, which aim to perform interaction and trans-\nformation between tokens. Specifically, in the MHSA part,\nwe first halve the number of channels using a reduction layer\nand then project the input embedding to the Q (query), K\n(keys), and V (values) elements through a linear layer. To\nenrich the background representation, multi-head attention\n(Vaswani et al. 2017) is performed onQ, K and V . Inspired\nby (Lu et al. 2022), we adoptfeature splitoperation to divide\nQ, K, V into s equal segments along the channel dimen-\nsion to obtain {Q1, Q2, . . . , Qs}, {K1, K2, . . . , Ks}, and\n{V1, V2, . . . , Vs}. For each segment, it has Ck = C\nS chan-\nnels. Each triplet of these segments is usually calculated by\nscaled dot-product attention function:\nfsdpa = softmax\n\u0012QiK⊤\ni\n√Ck\n\u0013\nVi, for i = 1,2, . . . , s.(6)\nLastly, we concatenate all the output of multi-head attention,\nand then utilize an extension layer to recover the number of\nchannels. To keep the block simple, the FFN part is com-\nposed of two Multi-Layer Perceptions (MLPs) layers.\nInteractive Fusion Branch\nTo refine the inter-stage representation among CNN features\nand Transformer features, we formulate an IFB to provide\nadditional complementary information to the backbone net-\nwork. Compared to direct concatenating the features of two\nadjacent stages, our IFB tends to be more flexible and effec-\ntive. Specifically, we first make full use of image priors to\nembed into IFBs as feature guidances, thus facilitating the\nstage-wise reconstruction of high-quality results. Similar to\n(Li, Tan, and Cheong 2018; Yi et al. 2021), residual channel\nprior (RCP) is applied due its advantage in extracting clear\nobject structure. It is regarded as the residual result of the\nmaximum and minimum channel values of the rainy image,\nwhich is calculated without any additional parameters:\nFrcp(x) = max\nc∈{R,G,B}\nIc(x) − min\nd∈{R,G,B}\nId(x). (7)\nFollowing it, we adopt SE-Resblocks (Hu, Shen, and Sun\n2018) to further enhance channel-wise feature propagation.\nFormally, the final output Fpgb of PGB is defined by\nFpgb = f3×3\n\u0000\nf3\nSE (f3×3 (Frcp))\n\u0001\n+ f3×3 (Frcp) , (8)\nwhere f3\nSE (·) is the cascade feature of three SE-Resblocks.\nThen, PGB is fed into CRB to encode the mixture rela-\ntions among backbone features and reconstruction features,\nwhich can learn redundant components adaptively from each\nother for further refinement. Here, two convolution layers\nwith a kernel size of3×3 are used to map the backbone fea-\nture Fbac from the output of previous stage (i.e.,FN\nDaMoE or\nFBaV iT) and reconstructed image feature Fpgb from PGB,\nrespectively. Next, we use element multiplication to calcu-\nlate the similarity map S between two branch features:\nS = sigmoid (f3×3 (Fbac) ⊗ f3×3 (Fpgb)) , (9)\nwhere ⊗ represents pixel-wise product.\nLastly, the original features are further added to the acti-\nvated features, and the summed features are concatenated to\nreturn a refined joint representation Fcrb of CRB:\nFcrb = concat (S ⊗ Fbac + Fbac, S⊗ Fpgb + Fpgb) . (10)\nWith the help of this interactive fusion pattern architec-\nture, we can not only fully utilize the dependencies of deep\nfeatures across stages, but also boost the collaborative repre-\nsentation from CNN and Transformer to help image restora-\ntion.\nLoss Function\nTo supervise the learning process of the network, we choose\ntwo proper loss functions as training objectives to drive the\nmodel optimization. Mean squared error (MSE) loss (Zhang\nand Patel 2018) is widely adopted to compute the pixel-level\ndifference between the recovered image Bi and correspond-\ning ground truth B, expressed as follows:\nLmse = 1\nHWC\nHX\nx=1\nWX\ny=1\nCX\nz=1\n∥Bi − B∥2 , i= 1,2, 3 (11)\nwhere i denotes different stages, andH, W and C are height,\nwidth and number of channels, respectively.\nTo further improve the deraining results with high fidelity,\nwe consider the structural similarity (SSIM) to compare the\nstructural differences, which is calculated as follows:\nSSIM (Bi, B) = 2µBi µB + C1\nµ2\nBi + µ2\nB + C1\n· 2σBiB + C2\nσ2\nBi + σ2\nB + C2\n,\n(12)\nwhere µBi and µB are the average of Bi and B over pix-\nels, σBi and σB are the variances of Bi and B, σBiB is the\ncovariance between Bi and B. C1 and C2 are two fixed con-\nstants. Then, the negative SSIM loss (Ren et al. 2019) for\nrecovered image is given by:\nLssim = 1− SSIM (Bi, B). (13)\nFinally, the overall loss Ltotal for training our network is\nthe combination of the above two losses as:\nLtotal = Lmse + λLssim, (14)\nwhere the coefficientλ is empirically set to 0.2 for balancing\neach loss term.\n381\nDatasets Synthetic Real-world\nRain100L Rain100H Rain12 RainDS-RS100\nMethods Param PSNR SSIM PSNR SSIM\nPSNR SSIM PSNR SSIM\nRainy Input - 26.90 0.838 13.56 0.370\n30.14 0.855 23.58 0.651\nPrior-based methods GMM (CVPR’16) - 29.05 0.871 15.23 0.449\n32.14 0.914 23.73 0.559\nJCAS (CVPR’17) - 28.54 0.852 14.62 0.451\n33.10 0.930 24.04 0.556\nCNN-based methods\nDNN (CVPR’17) 0.06 32.38 0.925 22.85 0.725\n34.04 0.933 24.61 0.681\nRESCAN (ECCV’18) 0.15 38.52 0.981 29.62 0.872\n36.43 0.951 25.84 0.686\nPReNet (CVPR’19) 0.17 37.45 0.979 30.11 0.905\n36.66 0.961 26.29 0.718\nJORDER E (TPAMI’19) - 38.59 0.983 30.50 0.896\n36.69 0.962 26.48 0.715\nMSPFN (CVPR’20) 13.35 38.73 0.978 30.63 0.898\n36.85 0.957 26.55 0.721\nMPRNet (CVPR’21) 3.63 39.45 0.982 30.92 0.904\n37.26 0.960 26.86 0.725\nTransformer-based methods IPT (CVPR’21) 115.5 41.62 0.988 - - -\n- - -\nUformer-B (CVPR’22) 50.8 39.76 0.983 31.06 0.908 37.10 0.958 26.83 0.728\nHybrid-based methods ELF (MM’22) 1.53 36.67 0.968 30.48 0.896\n- - - -\nOurs 0.87 39.70 0.985 31.51 0.910 37.54 0.963 27.02 0.734\nTable 1: Comparison of quantitative results on four datasets. Bold and underline indicate the best and second-best results.\nExperiments\nExperimental Settings\nDatasets. We conduct deraining experiments on four public\nrain streak datasets, including Rain100L (Yang et al. 2017),\nRain100H (Yang et al. 2017), Rain12 (Li et al. 2016), and\nRainDS-Real (Quan et al. 2021). With light and heavy types\nof synthetic rain streaks, Rain100L and Rain100H contain\n1,800 image pairs for training and 100 image pairs for test-\ning. Rain12 contains 12 light rainy images. Based on au-\ntonomous driving scenario, (Quan et al. 2021) release var-\nious image pairs corrupted by raindrops and rain streaks,\nwhich consists of two subsets, RainDS-Syn and RainDS-\nReal. As we mainly focus on removing rain streaks, we\nonly adopt a subset of RainDS-Real, named RainDS-RS100,\nwhere 150 real-world rainy images are chosen as training\ndata and the other 100 pairs are selected for testing. In addi-\ntion, we also randomly choose 20 real rainy images without\nground truths from Internet-Data (Wang et al. 2019; Yang\net al. 2020) as the evaluation of generalization performance.\nComparison methods. We compare our method with two\nprior-based algorithms (i.e., GMM (Li et al. 2016) and JCAS\n(Gu et al. 2017)), six CNN-based approahces (i.e., DDN (Fu\net al. 2017b), RESCAN (Li et al. 2018), PReNet (Ren et al.\n2019), JORDER\nE (Yang et al. 2019), MSPFN (Jiang et al.\n2020), and MPRNet (Zamir et al. 2021)), two Transformer-\nbased networks (i.e., IPT (Chen et al. 2021) and Uformer-B\n(Wang et al. 2022)), and one hybrid-based model (i.e., ELF\n(Jiang et al. 2022)). Due to hard-ware constraints, IPT is\nonly evaluated on Rain100L, and the corresponding results\nrefer to their original paper. As the code of ELF is not avail-\nable, we refer to some results presented in their paper. For\nother approaches, we retrain the models using the default\nsettings provided by the authors if there are no pretrained\nmodels, otherwise we evaluate them with their online codes.\nEvaluation metrics. Since the ground truths available, we\nadopt two commonly-used metrics for quantitative com-\nparison, and they are Peak Signal to Noise Ratio (PSNR)\n(Huynh-Thu and Ghanbari 2008) and Structural Similarity\n(SSIM) (Wang et al. 2004). Following (Wang et al. 2020;\nXiao et al. 2022), we calculate PSNR/SSIM metrics in Y\nchannel of YCbCr space. For the rainy images without their\nclean labels, two popular non-reference indicators, Natural-\nness Image Quality Evaluator (NIQE) (Mittal, Soundarara-\njan, and Bovik 2012) and Blind/Referenceless Image Spatial\nQUality Evaluator (BRISQUE) (Mittal, Moorthy, and Bovik\n2012), are employed for evaluating deraining performance.\nImplementation details. The proposed network is imple-\nmented in PyTorch framework using Adam optimizer with a\nlearning rate of 0.0001 to minimizeLtotal by 400 epochs. In\nour model, {N1, N2, N3} are set to {4, 3, 4}. During train-\ning, we run all of our experiments with batch size of 4 and\npatch size of 128 on one NVIDIA Tesla V100 GPU (32G).\nIn the DaMoE module, we set k = 8for the number of ex-\nperts and T = 32for the weight matrixs. Each convolutional\nlayer has a C = 16filter with stride of 1. In the BaViT mod-\nule, we set k = 3for the kernel size and s = 4for splitting\nsegment. The number of heads in MHSA is set to 8. For data\naugmentation, vertical and horizontal flips are randomly ap-\nplied. The loss trade-off parameter is defined via cross vali-\ndation using the validation set, and the whole pipeline is per-\nformed in an end-to-end fashion without costly large-scale\npretraining (Chen et al. 2021).\nExperimental Results\nSynthetic datasets. Tab. 1 presents the quantitative results\non different deraining benchmarks. Apparently, the pro-\nposed method significantly competes previous popular de-\nrainers on the Rain100H and Rain12 datasets, which re-\nveals that our method can properly handle diverse types of\nspatially-varying rain streaks. And most remarkably, our de-\nsigned HCT-FFN achieves prominent improvement in term\nof PSNR on the Rain100L and Rain100H benchmarks. Fig.\n3 further shows visual comparison between samples gener-\nated by different baselines. It can be seen that Uformer-B is\nsensitive to local slender rain streaks. Besides, the results of\npure CNN-based models are flawed in terms of global tex-\nture recovery. By contrast, our results are more consistent\nwith that of the ground truths.\nReal-world datasets. In order to further practical evalution\n382\nFigure 3: Visual comparison on the Rain100H dataset. Best viewed by zooming in the figures on high-resolution displays.\nFigure 4: Visual comparison on the RainDS-RS100 dataset. Best viewed by zooming in the figures on high-resolution displays.\nMethod PReNet MSPFN MPRNet Uformer\nOurs\nNIQE 5.489 5.626 5.658 5.034 4.743\nBRISQ\nUE 33.576 42.159 37.195 33.293 28.709\nTable 2: Comparison of quantitative results on Internet-Data\ndataset. Note that lower scores indicate better image quality.\nStage Number Stage = 1 Stage\n= 2 Stage = 3\nPSNR / SSIM 36.31 / 0.952 38.87\n/ 0.974 39.70 / 0.985\nTable 3: Ablation analysis for different number of recursive\nstages. Stage = 1 also indicates that BaViT module is none.\nin real-world rainy scenes, Tab. 2 and the last column of\nTab. 1 compare the deraining results on the RainDS-RS100\ndataset quantitatively. As expected, our developed method\ncontinues to achieve the highest PSNR/SSIM values and the\nlowest NIQE/BRISQUE scores, demonstrating the effective-\nness and superiority of HCT-FFN, especially in the real rain\nwith complicated rainy conditions. The reason behind is that\nour model enjoys the powerful abilities from the hybrid fea-\nture fusion of CNN and Transformer. Through the compar-\nison in Fig. 4 and Fig. 6, our method successfully removes\nmost rain perturbation and owns visual pleasant recovery re-\nsults on several challenging exemples, which implies that it\ncan well generalize to unseen real-world data types.\nAblation Studies\nWe study the individual components and parameter choices\non the final deraining performance. Here, all ablation studies\nare conducted within the same training settings and environ-\nment using Rain100L dataset to ensure a fair comparison.\nThe Number of experts. To analyze the impact of differ-\nent number of experts in each DaMoE module, we perform\nan experiment based on the parallel layer configuration in\nFig. 5. When using single expert models, performance is\ndramatically degraded compared with multi-expert models.\nUnlike setting all experts to the same structure (Kim, Ahn,\nand Sohn 2020), our multi-expert structure is more diverse,\nFigure 5: Ablation analysis for different number of experts\nin each DaMoE module.\nwhich brings their own gains to the performance due to dif-\nferent receptive fields and disparate local CNN operations.\nThe Number of recursive stages. To analyze the effect\nabout different number of recursive stage, Tab. 3 records the\nPSNR/SSIM of corresponding models. We can observe that:\n(1) BaViT module brings a great contribution to the baseline\nmodel, thanks to its advantage of modeling global interac-\ntions from non-local regions. (2) Stage-by-stage progressive\nlearning can gradually eliminate the remaining rain streaks,\nthus achieving excellent deraining quality in the final stage.\nSequence of different stages. To analyze the influence of\nthe sequence of different stages on the deraining perfor-\nmance, we perform experiments based on different model\nvariants in Tab. 4. Compared to the baseline model (C-C-C),\nTransformer-based stage provides additional performance\nbenefits. In addition, we also note that Transformer lacks\nthe ability to encode local feature in the early stage, lead-\ning to suboptimal results. To ensure that CNN features and\nTransformer features alternate with each other in IFB, we set\nBaViT at the intermediate stage.\n383\nFigure 6: Visual comparison on the Internet-Data dataset. Best viewed by zooming in the figures on high-resolution displays.\nFigure 7: Visual comparison on the SateHaze1k dataset. Best viewed by zooming in the figures on high-resolution displays.\nC-C-C T-C-C C-C-T\nC-T-C\n39.48 / 0.974 39.62\n/ 0.979 39.65 / 0.981 39.70 / 0.985\nTable 4: Ablation analysis for sequence of different stages.\n“C” and “T” represent CNN/Transformer-based stage.\nPGB × ✓ ✓ ✓ ✓\nCRB × × ✓ ✓ ✓\nskip-connect × × × ✓ ✓\nstage-concate × × × × ✓\nPSNR (dB) 39.36 39.52 39.59 39.63 39.70\nTable 5: Ablation analysis for different fusion components.\nEffectiveness of feature fusion. To evaluate the effective-\nness of our fusion strategies, we implement alternative so-\nlutions on different variants of HCT-FFN. As evident from\nTab. 5, we consider the following components: (1) PGB, (2)\nCRB, (3) skip-connection, and (4) stage-level concatenation.\nCompared to direct feature concatenation (i.e., w/o all these\ncomponents above), our IFB (i.e., PGB and CRB ) tends to\nbe more suitable for combining CNN features and Trans-\nformer features. Meanwhile, we notice that skip-connection\nand stage-level feature concatenation also bring out perfor-\nmance improvement, which shows that these operations can\nreduce the noise during feature propagation so that the net-\nwork can adaptively learn more useful representations.\nExtension to Image Dehazing\nWe are curious about whether our method can be extended to\nthe image dehazing task. Here, we make comparison against\ndifferent dehazing methods on the SateHaze1k (Huang et al.\n2020) dataset, including DCP (He, Sun, and Tang 2010),\nDehazeNet (Cai et al. 2016), SAR-Opt-cGAN (Huang et al.\n2020), FCTF (Li and Chen 2020), CGAN-SAR (Grohnfeldt,\nSchmitt, and Zhu 2018), and SkyGAN (Mehta et al. 2021).\nThrough Tab. 6, our method produces the highest values. As\nDatasets Thin Haze Moderate Haze\nThick Haze\nDCP 13.15 / 0.724 9.78\n/ 0.573 10.25 / 0.585\nDehazeNet 19.75 / 0.895 18.13\n/ 0.855 14.33 / 0.706\nHuang et al. 20.20 / 0.841 21.66\n/ 0.794 19.66 / 0.757\nFCTF 22.77 / 0.891 24.96\n/ 0.932 24.14 / 0.821\nCGAN-SAR 24.16 / 0.906 25.31\n/ 0.926 25.07 / 0.864\nSkyGAN 25.38 / 0.924 25.58\n/ 0.903 23.43 / 0.892\nOurs 27.99 / 0.925 27.98 / 0.939 25.31 / 0.904\nTable 6: Quantitative results on SateHaze1k dataset, which\ncontains three hazy levels, as thin, moderate, and thick haze.\nshown in Fig. 7, our method can generate a clearer image.\nConclusion\nWe have presented an effective end-to-end HCT-FFN for im-\nage deraining. We introduce DaMoE modules into the CNN-\nbased stage to emphasize the spatially-varying rain distribu-\ntion, and also leverage BaViT modules into the Transformer-\nbased stage to eliminate the spatially-long rain degradation.\nImportantly, a progressive IFB is involved between adjacent\nstages to aggregate information from CNN and Transformer.\nExtensive experiments demonstrate the superiority and ex-\ntensibility of our method over the state-of-the-arts.\nLimitation. One limitation is that the interaction between\nthe global and local representation is explored since the hy-\nbrid fusion network only provides the final feature fusion\nbetween DaMoE and BaViT for each stage. However, the\nintermediate features are not considered, which are also cru-\ncial for accurate rain distribution.\nAcknowledgements\nThis work has been supported in part by the National Key\nR&D Program of China (No. 2018AAA0102001), the Na-\ntional Natural Science Foundation of China (Nos. 61922043,\n61872421), and the Fundamental Research Funds for the\nCentral Universities (No. 30920041109).\n384\nReferences\nJacobs, R. A.; Jordan, M. I.; Nowlan, S. J.; and Hinton,\nG. E. 1991. Adaptive mixtures of local experts. Neural\ncomputation, 3(1): 79–87.\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004. Image quality assessment: from error visibility to\nstructural similarity. IEEE TIP, 13(4): 600–612.\nHe, K.; Sun, J.; and Tang, X. 2010. Single image haze\nremoval using dark channel prior. IEEE TPAMI, 33(12):\n2341–2353.\nKang, L.-W.; Lin, C.-W.; and Fu, Y .-H. 2011. Automatic\nsingle-image-based rain streaks removal via image decom-\nposition. IEEE TIP, 21(4): 1742–1755.\nMittal, A.; Soundararajan, R.; and Bovik, A. C. 2012.\nMaking a “completely blind” image quality analyzer. IEEE\nSPL, 20(3): 209–212.\nMittal, A.; Moorthy, A. K.; and Bovik, A. C. 2012. No-\nreference image quality assessment in the spatial domain.\nIEEE TIP, 21(12): 4695–4708.\nLuo, Y .; Xu, Y .; and Ji, H. 2015. Removing rain from a\nsingle image via discriminative sparse coding. In ICCV,\n3397–3405.\nLi, Y .; Tan, R. T.; Guo, X.; Lu, J.; and Brown, M. S.\n2016. Rain streak removal using layer priors. In CVPR,\n2736–2744.\nCai, B.; Xu, X.; Jia, K.; Qing, C.; and Tao, D. 2016.\nDehazenet: An end-to-end system for single image haze\nremoval. IEEE TIP, 25(11): 5187–5198.\nZhang, H.; and Patel, V . M. 2017. Convolutional sparse\nand low-rank coding-based rain streak removal. In WACV,\n1259–1267.\nYang, W.; Tan, R. T.; Feng, J.; Liu, J.; Guo, Z.; and Yan, S.\n2017. Deep joint rain detection and removal from a single\nimage. In CVPR, 1357–1366.\nFu, X.; Huang, J.; Ding, X.; Liao, Y .; and Paisley, J.\n2017a. Clearing the skies: A deep network architecture for\nsingle-image rain removal. IEEE TIP, 26(6): 2944–2956.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017.\nAttention is all you need. Advances in neural information\nprocessing systems, 30.\nHu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation\nnetworks. In CVPR, 7132–7141.\nZhang, H.; and Patel, V . M. 2018. Density-aware single\nimage de-raining using a multi-stream dense network. In\nCVPR, 695–704.\nLi, X.; Wu, J.; Lin, Z.; Liu, H.; and Zha, H. 2018. Recurrent\nsqueeze-and-excitation context aggregation net for single\nimage deraining. In ECCV, 254–269.\nLi, R.; Tan, R. T.; and Cheong, L.-F. 2018. Robust optical\nflow in rainy scenes. In ECCV, 288–304.\nKim, S.; Ahn, N.; and Sohn, K.-A. 2020. Restoring\nSpatially-Heterogeneous Distortions Using Mixture of\nExperts Network. In ACCV.\nSuganuma, M.; Liu, X.; and Okatani, T. 2019. Attention-\nbased adaptive selection of operations for image restoration\nin the presence of unknown combined distortions. In CVPR,\n9039–9048.\nRen, D.; Zuo, W.; Hu, Q.; Zhu, P.; and Meng, D. 2019.\nProgressive image deraining networks: A better and simpler\nbaseline. In CVPR, 3937–3946.\nRen, W.; Ma, L.; Zhang, J.; Pan, J.; Cao, X.; Liu, W.; and\nYang, M.-H. 2018. Gated fusion network for single image\ndehazing. In CVPR, 3253–3261.\nHu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation\nnetworks. In CVPR, 7132–7141.\nWang, T.; Yang, X.; Xu, K.; Chen, S.; Zhang, Q.; and Lau,\nR. W. 2019. Spatial attentive single-image deraining with a\nhigh quality real rain dataset. In CVPR, 12270–12279.\nYang, W.; Tan, R. T.; Wang, S.; Fang, Y .; and Liu, J. 2020.\nSingle image deraining: From model-based to data-driven\nand beyond. IEEE TPAMI, 43(11): 4059–4077.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nJiang, K.; Wang, Z.; Yi, P.; Chen, C.; Huang, B.; Luo, Y .;\nMa, J.; and Jiang, J. 2020. Multi-scale progressive fusion\nnetwork for single image deraining. In CVPR, 8346–8355.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213–229.\nHuang, B.; Zhi, L.; Yang, C.; Sun, F.; and Song, Y . 2020.\nSingle satellite optical imagery dehazing using SAR image\nprior based on conditional generative adversarial networks.\nIn WACV, 1806–1813.\nMehta, A.; Sinha, H.; Mandal, M.; and Narang, P. 2021.\nDomain-aware unsupervised hyperspectral reconstruction\nfor aerial image dehazing. In WACV, 413–422.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking\nsemantic segmentation from a sequence-to-sequence per-\nspective with transformers. In CVPR, 6881–6890.\nYi, Q.; Li, J.; Dai, Q.; Fang, F.; Zhang, G.; and Zeng, T.\n2021. Structure-preserving deraining with residue channel\nprior guidance. In ICCV, 4238–4247.\nQuan, R.; Yu, X.; Liang, Y .; and Yang, Y . 2021. Removing\nraindrops and rain streaks in one go. In CVPR, 9147–9156.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV, 10012–10022.\nXiao, J.; Zhou, M.; Fu, X.; Liu, A.; and Zha, Z.-J. 2021.\nImproving de-raining generalization via neural reorganiza-\ntion. In ICCV, 4987–4996.\nZhou, M.; Xiao, J.; Chang, Y .; Fu, X.; Liu, A.; Pan, J.; and\nZha, Z.-J. 2021. Image de-raining via continual learning. In\nCVPR, 4907–4916.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nYang, M.-H.; and Shao, L. 2021. Multi-stage progressive\nimage restoration. In CVPR, 14821–14831.\n385\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and\nTimofte, R. 2021. SwinIR: Image Restoration Using Swin\nTransformer. In CVPRW, 1833–1844.\nChen, H.; Wang, Y .; Guo, T.; Xu, C.; Deng, Y .; Liu, Z.; Ma,\nS.; Xu, C.; Xu, C.; and Gao, W. 2021. Pre-trained image\nprocessing transformer. In CVPR, 12299–12310.\nYuan, K.; Guo, S.; Liu, Z.; Zhou, A.; Yu, F.; and Wu,\nW. 2021. Incorporating convolution designs into visual\ntransformers. In ICCV, 579–588.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. In ICCV, 22–31.\nYu, Y .; Yang, W.; Tan, Y .-P.; and Kot, A. C. 2022. Towards\nRobust Rain Removal Against Adversarial Attacks: A\nComprehensive Benchmark Analysis and Beyond. In\nCVPR, 6013–6022.\nChen, X.; Pan, J.; Jiang, K.; Li, Y .; Huang, Y .; Kong, C.;\nDai, L.; and Fan, Z. 2022. Unpaired Deep Image Deraining\nUsing Dual Contrastive Learning. In CVPR, 2017–2026.\nXiao, J.; Fu, X.; Liu, A.; Wu, F.; and Zha, Z.-J. 2022. Image\nDe-raining Transformer. IEEE TPAMI.\nGuo, C.-L.; Yan, Q.; Anwar, S.; Cong, R.; Ren, W.; and Li,\nC. 2022. Image Dehazing Transformer with Transmission-\nAware 3D Position Embedding. In CVPR, 5812–5820.\nJiang, K.; Wang, Z.; Chen, C.; Wang, Z.; Cui, L.; and\nLin, C.-W. 2022. Magic ELF: Image Deraining Meets\nAssociation Learning and Transformer. In ACM MM.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nand Yang, M.-H. 2022. Restormer: Efficient transformer for\nhigh-resolution image restoration. In CVPR, 5728–5739.\nWang, Z.; Cun, X.; Bao, J.; Zhou, W.; Liu, J.; and Li, H.\n2022. Uformer: A general u-shaped transformer for image\nrestoration. In CVPR, 17683–17693.\nGao, G.; Wang, Z.; Li, J.; Li, W.; Yu, Y .; and Zeng, T.\n2022. Lightweight Bimodal Network for Single-Image\nSuper-Resolution via Symmetric CNN and Recursive\nTransformer. arXiv preprint arXiv:2204.13286.\nLu, Z.; Li, J.; Liu, H.; Huang, C.; Zhang, L.; and Zeng, T.\n2022. Transformer for single image super-resolution. In\nCVPRW, 457–466.\nPark, N.; and Kim, S. 2022. How Do Vision Transformers\nWork? arXiv preprint arXiv:2202.06709.\nHuang, B.; Zhi, L.; Yang, C.; Sun, F.; and Song, Y . 2020.\nSingle satellite optical imagery dehazing using SAR image\nprior based on conditional generative adversarial networks.\nIn WACV, 1806–1813.\nLi, Y .; and Chen, X. 2020. A coarse-to-fine two-stage\nattentive network for haze removal of remote sensing\nimages. IEEE GRSL, 18(10): 1751–1755.\nGrohnfeldt, C.; Schmitt, M.; and Zhu, X. 2018. A con-\nditional generative adversarial network to fuse SAR and\nmultispectral optical data for cloud removal from Sentinel-2\nimages. In IGARSS, 1726–1729.\nGu, S.; Meng, D.; Zuo, W.; and Zhang, L. 2017. Joint\nconvolutional analysis and synthesis sparse representation\nfor single image layer separation. In ICCV, 1708–1716.\nFu, X.; Huang, J.; Zeng, D.; Huang, Y .; Ding, X.; and\nPaisley, J. 2017b. Removing rain from single images via a\ndeep detail network. In CVPR, 3855–3863.\nYang, W.; Tan, R. T.; Feng, J.; Guo, Z.; Yan, S.; and Liu, J.\n2019. Joint rain detection and removal from a single image\nwith contextualized deep networks. IEEE TPAMI, 42(6):\n1377–1393.\nWang, H.; Xie, Q.; Zhao, Q.; and Meng, D. 2020. A\nmodel-driven deep neural network for single image rain\nremoval. In CVPR, 3103–3112.\nHuynh-Thu, Q.; and Ghanbari, M. 2008. Scope of validity\nof PSNR in image/video quality assessment. Electronics\nletters, 44(13): 800–801.\n386",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.763329029083252
    },
    {
      "name": "Transformer",
      "score": 0.6540359258651733
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5593363642692566
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4857926368713379
    },
    {
      "name": "Source code",
      "score": 0.45674577355384827
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4431934952735901
    },
    {
      "name": "Engineering",
      "score": 0.08751067519187927
    },
    {
      "name": "Voltage",
      "score": 0.08610165119171143
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}