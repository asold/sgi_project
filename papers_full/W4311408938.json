{
  "title": "Thinking Fast and Slow in Large Language Models",
  "url": "https://openalex.org/W4311408938",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221416435",
      "name": "Hagendorff, Thilo",
      "affiliations": [
        "University of Stuttgart"
      ]
    },
    {
      "id": "https://openalex.org/A4221416434",
      "name": "Fabi, Sarah",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2219418931",
      "name": "Kosinski, Michal",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4221416435",
      "name": "Hagendorff, Thilo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221416434",
      "name": "Fabi, Sarah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2219418931",
      "name": "Kosinski, Michal",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4311408938",
    "https://openalex.org/W2512223493",
    "https://openalex.org/W2747940070",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W158727920",
    "https://openalex.org/W2081056419"
  ],
  "abstract": "Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.",
  "full_text": " \n \n1 \n \n \n \nThinking Fast and Slow in Large Language Models \n \n \nThilo Hagendorff \nthilo.hagendorff@iris.uni-stuttgart.de \nUniversity of Stuttgart \n \nSarah Fabi \nsfabi@ucsd.edu \nUniversity of California San Diego \n \nMichal Kosinski \nmichalk@stanford.edu \nStanford University \n \n \n \nAbstract ‚Äì Large language models (LLMs) are currently at the forefront of intertwining AI systems \nwith human communication and everyday life. Therefore, it is of great importance to evaluate their \nemerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly \nresembles human-like intuition ‚Äì and the cognitive errors that come with it. However, LLMs with higher \ncognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors \nand perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive \nReflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive \ndecision-making in humans. Our study demonstrates that investigating LLMs with methods from \npsychology has the potential to reveal otherwise unknown emergent traits. \n \nKeywords: large language models, machine behavior, intuition, cognitive reflection test, semantic \nillusions, computational social science \n \n  \n \n \n2 \n \n \nIntroduction \nAs the range of applications for large language models (LLMs) rapidly expands, it is of paramount \nimportance to understand the mechanisms through which LLMs reason and make decisions. Recent \nresearch has revealed that with the increasing complexity of LLMs, they exhibit a multitude of emergent \nproperties and abilities that were not explicitly anticipated or intended by their creators.1 Among these \nnewfound capabilities are the capacity to generate computer code, tackle mathematical problems, learn \nfrom examples, engage in introspection, solve theory of mind tasks, and a plethora of other skills.2‚Äì6 In \nthis paper, we aim to explore yet another emergent phenomenon, namely LLMs‚Äô reasoning capabilities, \nwhile shedding light on the intricacies of their cognitive processes. \nResearch on humans often distinguishes between two broad categories of reasoning or‚Äîmore broadly‚Äî\ncognitive processes: Systems 1 and 27‚Äì14. System 1 processes are fast, automatic, and instinctual. They \noften involve heuristics, or mental shortcuts, which enable quick judgments and decisions without \nconscious effort. System 1 is essential for everyday functioning, as it allows humans to navigate their \nenvironments and make rapid decisions with minimal effort. System 2 processes, on the other hand, are \ndeliberate and require conscious effort. This system is employed in logical reasoning, critical thinking, \nand problem-solving. System 2 processes are slower and more resource-intensive, but they are also more \naccurate and less susceptible to bias.  \nOn the surface, current-day LLMs are System 1 thinkers: The input text is processed by consecutive \nlayers of neurons to produce a distribution of probabilities of all possible single-token (word) \ncompletions. This process is automatic, unidirectional, and involves a single wave of propagation \nthrough the neural network for each consecutive predicted word.15 Current-day LLMs lack cognitive \ninfrastructure (such as short-term memory), which is needed for re-examining the starting assumptions, \ntesting alternative solutions, feedback loops, and other cognitive strategies typically associated with \ndeliberate System 2 processes.16 Yet, past research and the results presented in this paper suggest that, \nlike humans, LLMs can engage in both System 1 and System 2 cognitive processes. While generating \neach consecutive word, LLMs re-read the context, including the original input (e.g., a task provided by a \nuser) as well as the words they have thus far generated. As a result, the context may serve as an external \nshort-term memory, which LLMs can use to engage in chain-of-thought reasoning, re-examine the \nstarting assumptions, estimate partial solutions, or test alternative approaches. This is akin to how people \nuse notepads to solve mathematical problems or write essays to sharpen and develop their arguments.  \nIn this work, we build on psychological research on human reasoning and decision-making to explore \nSystem 1 and 2 processes in LLMs. We develop bespoke versions of two tests widely used in this field: \nthe cognitive reflection test (CRT)17 and semantic illusions18 (see Table 1 and Table S1 in \nSupplementary Materials). The CRT comprises three types of mathematical tasks that appear to be \nsimpler than they really are, thus triggering an intuitive but incorrect System 1 response. CRT Type 1 \ntasks, such as the widely known ‚ÄúA bat and a ball‚Äù task, use a ‚Äúmore than‚Äù phrase to trick participants \ninto subtracting two of the values rather than solving a somewhat more complex equation. Type 2 tasks \nexploit people‚Äôs tendency to complete a numerical triplet series, such as five machines making five \nwidgets in five minutes because two machines make two widgets in two minutes. Type 3 tasks describe \nan exponential process but trick the participants into treating it as linear. Solving CRT tasks correctly \nrequires engaging in deliberate System 2 reasoning or possessing well-developed System 1 intuitions. \nSemantic illusions are questions containing a disguised error aimed at triggering an intuitive but \n \n \n3 \n \n \nincorrect System 1 response. In the well-known Moses Illusion,18 for example, participants tend to be \ntricked into claiming that Moses took two animals of each kind on the Ark (it was Noah). \nPast research emphasized that LLMs tend to fall for the same traps as human participants in CRT \ntasks.19 Here we show that LLMs can also engage in processes resembling deliberate System 2 \nreasoning. Moreover, we show that as LLMs‚Äô performance increases, their System-1-like next-word \ngeneration process becomes sufficiently acute to avoid falling for the traps embedded in the tasks. We \nalso address some of the limitations of past studies. First, while past research focused on a single model \n(GPT-3), we study reasoning capabilities across a range of models of different sizes and complexities. \nSecond, as solving CRT tasks requires mathematical abilities,20 LLMs‚Äô performance could be limited by \ntheir mathematical skills. To address this issue, we complement the CRT tasks with semantic illusions \nthat do not rely on mathematical skills. Third, past research relied on three CRT tasks copied verbatim \nfrom human studies19. This is problematic, as observing LLMs‚Äô performance on three CRT tasks does \nnot allow for meaningful statistical comparisons. Moreover, these tasks (as well as their solutions) were \nlikely present in LLMs‚Äô training data.21 To circumvent these issues, we designed 50 bespoke versions of \neach type of task (200 in total; see Table S1).  \nFor brevity and convenience, we use words such as ‚Äúbehavior,‚Äù ‚Äúintuition,‚Äù ‚Äúdeliberation,‚Äù or ‚Äúability‚Äù \nwhen referring to LLMs. Yet, we do not mean to equate artificial intelligence (AI) and human cognitive \nprocesses. While AI‚Äôs outputs are often similar to the ones produced by humans, it typically operates in \nfundamentally different ways.  \n \nTable 1. The original tasks and examples of bespoke tasks written for our study.  \nOriginal tasks  Our version (examples) \nPrompt \nResponse  \nPrompt \nResponse \nCorrect Intuitive  Correct Intuitive \nCRT Type 1 \nA bat and a ball cost \n$1.10 in total. The bat \ncosts $1.00 more than the \nball. How much does the \nball cost? \n$0.05 $0.10 \n A potato and a camera \ntogether cost $1.40. The \npotato costs $1 more than \nthe camera. How much \ndoes the camera cost? \n$0.20 $0.40 \nCRT Type 2 \nIf it takes 5 machines 5 \nminutes to make 5 \nwidgets, how long would \nit take 100 machines to \nmake 100 widgets? \n5 \nminutes \n100 \nminutes \n How long does it take 4 \npeople to tailor 4 jackets, \nif it takes 7 people 7 hours \nto tailor 7 jackets? \n7 hours 4 hours \n \n \n4 \n \n \nCRT Type 3 \nIn a lake, there is a patch \nof lily pads. Every day, \nthe patch doubles in size. \nIf it takes 48 days for the \npatch to cover the entire \nlake, how long would it \ntake for the patch to \ncover half of the lake? \n47 days 24 days \n People are escaping from \nwar. Each day, the total \ncount of refugees doubles. \nIf it takes 22 days for the \nentire population to \nevacuate, how long would \nit take for half of the \npopulation to do so? \n21 days 11 days \nSemantic Illusions \nHow many animals of \neach kind did Moses take \non the Ark? \nIt was \nNoah Two \n \nWhere on their bodies do \nwhales have their gills? \nWhales \ndo not \nhave \ngills \nOn the \nsides of \ntheir \nheads \nMethods \nHypothesis-blind research assistants recruited on Upwork, a freelancing platform, prepared 50 semantic \nillusions and 50 CRT Type 3 tasks. CRT Type 1 and 2 tasks were generated automatically. All tasks can \nbe found in Table S1. The code and data used here are available at https://osf.io/w5vhp/. \nThe tasks were administered to the family of OpenAI‚Äôs Generative Pre-trained Transformer (GPT) \nmodels ranging from GPT-1 to ChatGPT-4.6,22‚Äì24 To minimize the variance in the models‚Äô responses and \nthus increase the replicability of our results, the ‚Äútemperature‚Äù parameter was set to 0. The response \nlength was set to 100 tokens but was extended as needed. The models‚Äô responses were trimmed once \nthey started repeating themselves or stopped responding to the task. The LLMs‚Äô responses were \nreviewed and scored manually. For clarity, the scoring is discussed in detail in the results section.  \nThe same tasks were also administered to 500 human participants recruited on Prolific.io. Each \nparticipant was presented with a random set of four tasks (one of each kind) followed by a control \nquestion inquiring whether they used a language model or another external resource. (45 participants \nresponded positively and were excluded from the analysis.) Human respondents‚Äô performance suggests \nthat our tasks were of similar difficulty to those used in past human studies. In CRT, 38% of responses \nwere correct, compared with 41% in the original study (n=3,428)9 (difference (ùõø) = 3%; ùúí2 =\n3.60; ùëù = .06). In semantic illusions, 64% of participants responded intuitively, compared with 52% in \nthe original study (n=61; they did not report the fraction of correct responses; (ùõø = 12%; ùúí2 =\n2.41; ùëù = .12).29  \nStudy 1: Cognitive Reflection Test \nWe first test the LLMs‚Äô performance on CRT tasks. To help the reader interpret the results, we discuss \nthem in the context of LLMs‚Äô exemplary responses to CRT Type 3 task #14: \n \n \n5 \n \n \n‚ÄúIn a cave, there is a colony of bats with a daily population doubling. Given that it takes 60 days \nfor the entire cave to be filled with bats, how many days would it take for the cave to be half-\nfilled with bats?‚Äù \nThe correct response to this task is ‚Äú59 days,‚Äù but it was designed to appear easier than it really is, \ntempting participants to simply divide the total time by two, triggering an intuitive (but incorrect) \nresponse of ‚Äú30 days.‚Äù LLMs and humans‚Äô responses to this and other CRT tasks were divided into three \ncategories: correct, intuitive (but incorrect), and atypical, which includes all other incorrect responses. \nMoreover, in each category of responses, the fraction of LLMs‚Äô responses preceded by a written chain-\nof-thought reasoning are marked by black dots (only one human described their reasoning process).  \nThe overall performance of humans and LLMs across 150 CRT tasks is presented in Figure 1 (the Study \n1 section). The results, split by CRT task types, are presented in Table S2. Four trends are apparent in \nFigure 1. First, most of the responses of early and smaller LLMs (up until GPT-3-curie) were classified \nas atypical (gray bars). This category includes responses that were evasive (e.g., GPT-1‚Äôs response ‚Äúa \nlot‚Äù); indicated failure to comprehend the task (e.g., GPT-2XL‚Äôs response ‚ÄúThe colony would take 60 \ndays to double in size‚Äù); or were incorrect in ways different from one that the task was designed to \ntrigger (e.g., GPT-3-babbage‚Äôs response: ‚ÄúIt would take about 10 days for the cave to be half-filled with \nbats‚Äù). Moreover, while 15% of responses of both GPT-3-babbage and GPT-3-curie were categorized as \ncorrect (green bars), they seemed accidental. All but one were given to CRT Type 2 tasks (see Table \nS2), which can be solved by simply repeating the number mentioned most frequently in the prompt‚Äî\nwhich these models tended to naively do in this and other tasks.  \nSecond, as the models grew larger and their ability to comprehend the task increased, atypical responses \nwere replaced by intuitive (but incorrect) responses (blue bars), which the tasks were designed to trigger \n(e.g., GPT-3-davinci-003: ‚Äú30 days‚Äù). They constituted below 5% of responses of early models (up to \nGPT-3-babbage); increased to 21% for GPT-3-curie (difference (ùõø) = 16%; ùúí2 = 16.98; ùëù < .001); \nand further increased to 70%‚Äì90% for the GPT-3-davinci family (ùõø ‚â• 49%; ùúí2 ‚â• 69.64; ùëù < .001), a \nfraction much higher than one observed in humans (55%; ùõø ‚â• 15%; ùúí2 ‚â• 11.79; ùëù < .001). \nIn humans, such responses are interpreted as evidence of System 1 reasoning and failure to engage \nSystem 2, but they could also stem from deliberate‚Äîbut erroneous‚ÄîSystem 2 reasoning. The \ngenerative process behind the LLMs‚Äô responses is less ambiguous. As we discuss in the introduction, \ncurrent-day LLMs lack the built-in cognitive infrastructure necessary to internally engage in System 2 \nprocesses. Consequently, their intuitive responses can only stem from a System-1-like process.  \nImportantly, LLMs‚Äô tendency to respond intuitively is unlikely to be driven by their insufficient \nmathematical skills. First, prior research showed that LLMs can solve basic mathematical problems.2,5 \nSecond, responding intuitively to CRT Type 1 and 3 tasks also requires solving a simple equation (e.g., \nhow much is ‚Äúhalf of 60‚Äù; responding intuitively to CRT Type 2 tasks requires no computation). \nMoreover, as we show in Study 3, GPT-3-davinci-003‚Äôs performance can be significantly improved by \npresenting it with training examples. \nThird, LLMs‚Äô strong tendency to respond intuitively stops abruptly with the arrival of ChatGPT. The \nfraction of correct responses equaled 59% for ChatGPT-3.5 and 96% for ChatGPT-4. This is much \nhigher than 5% of tasks solved correctly by GPT-3-davinci-003, an otherwise very apt model (ùõø ‚â•\n54%; ùúí2 ‚â• 102.44; ùëù < .001) or 38% achieved by humans (ùõø ‚â• 21%; ùúí2 ‚â• 25.60; ùëù < .001). \nChatGPT‚Äôs tendency to respond correctly was accompanied by a significant drop in their tendency to \n \n \n6 \n \n \nrespond intuitively: 15% for ChatGPT-3.5 and 0% for ChatGPT-4 versus 80% for GPT-3-davinci-003 \n(ùõø ‚â• 65%; ùúí2 ‚â• 125.81; ùëù < .001) and 55% for humans (ùõø ‚â• 40%; ùúí2 ‚â• 86.30; ùëù < .001). \n \nFigure 1. Overall human and LLM performance on CRT tasks.  \n \n\n \n \n7 \n \n \nA closer look at ChatGPT models‚Äô responses reveals that the dramatic increase in their performance was \naccompanied by a novel response style. While pre-ChatGPT LLMs responded with brief phrases or \nsingle sentences (e.g., GPT-3-davinci-003: ‚Äú30 days‚Äù), 97% of ChatGPT-3.5‚Äôs responses and 85% of \nChatGPT-4.5‚Äôs responses included some form of chain-of-thought reasoning (such responses are marked \nwith black dots on bars on Figure 1). See, for example, ChatGPT-3.5‚Äôs response to CRT Type 1 task \n#14:  \nChatGPT-3.5: ‚ÄúIf the population doubles every day, then the number of bats in the cave after n \ndays is 2^n times the original population. Let P be the original population of the colony. \nAfter 60 days, the population is 2^60 * P, which is the capacity of the cave. \nTo find the number of days it takes for the cave to be half-filled with bats, we need to find the \nvalue of n such that the population is half of the capacity of the cave. \n0.5 * 2^60 * P = 2^n * P \nDividing both sides by P, we get: \n0.5 * 2^60 = 2^n \nTaking the logarithm base 2 of both sides, we get: \nn = 60 + log2(0.5) = 60 - 1 = 59 \nTherefore, it would take 59 days for the cave to be half-filled with bats.‚Äù \n \nAs we discussed before, there is nothing deliberate in how ChatGPT-3.5 and other current-day LLMs \ngenerate the next word. Yet, each time the word is generated, LLM re-reads the task and the response it \ngenerated so far, combining System-1-like next-word generation into something resembling a System 2 \nprocess: generating a strategy needed to solve the task, dividing the task into more tractable subtasks, \nand solving them one by one. This is akin to how humans use a notebook to solve mathematical tasks \nwithout the need to process them in their short-term memory. In Study 2, we will show evidence \nsuggesting that chain-of-thought responses not only resemble but also serve as a System 2 process. \nStudy 2: Chain-of-Thought Reasoning in CRT Tasks \nThe results of Study 1 show that the abrupt increase in LLMs‚Äô performance on CRT tasks was \naccompanied by their employment of the chain-of-thought reasoning. Past research suggests that \ninstructing LLMs to engage in chain-of-thought reasoning improves their ability to solve various tasks.3 \nCould it be that we observed a similar phenomenon here?  \nTo answer this question, we first instruct GPT-3-davinci-003 to engage in the chain-of-thought \nreasoning by adding the following suffix after the prompt: ‚ÄúLet‚Äôs use algebra to solve this problem.‚Äù The \nresults presented in Figure 1 (in the Study 2 section) show that our manipulation was successful: The \nfraction of chain-of-thought responses increased from 0% in Study 1 to 100% (ùõø =  100%; ùúí2 =\n147.01; ùëù < .001). The model seemed to design and execute a task-solving strategy. In some cases, it \nwas sound, boosting the fraction of correct responses from 5% to 28% (ùõø = 23%; ùúí2 = 28.20; ùëù <\n.001). Most of the time, it was poorly conceived or executed, leading to the increase of atypical \nresponses from 15% to 43% (ùõø = 28%; ùúí2 = 14.72; ùëù < .001). Most importantly, it significantly \nreduced a model‚Äôs tendency to fall for the trap embedded in the task, as illustrated by the drop in the \nfraction of intuitive responses from 80% to 29% (ùõø = 51%; ùúí2 = 75.66; ùëù < .001).  \nNext, we prevent ChatGPT models from engaging in chain-of-thought reasoning by adding the \nfollowing suffix: ‚ÄúProvide the shortest possible answer (e.g., ‚Äò$2‚Äô or ‚Äò1 week‚Äô), do not explain your \n \n \n8 \n \n \nreasoning.‚Äù The results presented in Figure 1 (the Study 2 section) show that our manipulation was \nsuccessful: The fraction of chain-of-thought responses fell from 97% to 0% for ChatGPT-3.5 (ùõø =\n97%; ùúí2 = 276.79; ùëù < .001) and from 84% to 0% for ChatGPT-4 (ùõø = 84%; ùúí2 = 213.81; ùëù <\n.001). The fraction of correct responses for ChatGPT-3.5 did not significantly change (ùõø = 4%; ùúí2 =\n0.47; ùëù = .49). For ChatGPT-4, the fraction of intuitive responses increased from 0% to 10% (ùõø =\n10%; ùúí2 = 13.75; ùëù < .001), and correct responses fell from 95% to 88% (ùõø = 7%; ùúí2 = 4.36; ùëù <\n.05). \nThe results of Study 2 suggest that chain-of-thought reasoning helps LLMs to avoid falling for the traps \nembedded in the CRT tasks and improves their ability to solve them correctly. Yet, they also reveal that \nChatGPT models can solve a great majority of CRT tasks even when forced to provide a System-1-like \nresponse. This suggests that ChatGPT models have well-developed intuition, enabling them to solve \nCRT tasks without engaging in chain-of-thought reasoning. This is consistent with ChatGPT-4‚Äôs \nperformance in Study 1, where it solved 24% of the CRT task without using chain-of-thought reasoning. \nStudy 3 explores this phenomenon in greater detail. \nStudy 3: Improving LLMs‚Äô Intuition in CRT Tasks \nThe results of Study 2 suggest that the most recent of the LLMs examined here‚Äîthe ChatGPT family‚Äî\ncan avoid falling for the trap embedded in CRT tasks and solve them correctly, even when prevented \nfrom engaging in the chain-of-thought System-2-like reasoning. In humans, this would be taken as \nevidence of a well-developed intuition stemming from previous exposure to CRT or similar tasks25,26 \n(although the persistence and size of this effect is disputed27,28). Here we show results suggesting that \nthe same applies to LLMs. This is in line with past results showing that LLMs can learn, even from a \nsingle example. \n \n \nFigure 2. Change in the fraction of correct responses of GPT-3-davinci-003 against the number of \ntraining examples that the task was prefixed with.  \n \n\n \n \n9 \n \n \nAs ChatGPT models already seem to possess well-developed intuition, we attempt to improve the \nSystem-1-like responses of GPT-3-davinci-003. We present it with CRT tasks, each time preceding it \nwith 0 to 49 remaining tasks of the same type, accompanied by the correct solution. CRT tasks of the \nsame type are semantically very similar, enabling the model to develop System-1 intuitions akin to one \nexpressed by the ChatGPT model family.  \nThe results presented in Figure 2 show that GPT-3-davinci-003‚Äôs ability to answer correctly (rather than \nintuitively) increased with each additional example. The fastest gains were observed for CRT Type 2 \ntasks, where the accuracy increases from 2% to 92% after two examples (ùõø = 90%; ùúí2 = 77.72; ùëù <\n.001). This is to be expected, as they can be solved correctly by simply repeating the duration listed in \nthe task. To solve CRT Type 3, one needs to respond with the total time minus one unit. They proved to \nbe somewhat more complex: The accuracy increased from 12% to 92% after seven training examples \n(ùõø = 80%; ùúí2 = 60.94; ùëù < .001). It took most examples to develop the model‚Äôs intuition to solve CRT \nType 1 tasks, where the correct answer equals \nùë°ùëúùë°ùëéùëô ùëùùëüùëñùëêùëí‚àíùëòùëõùëúùë§ùëõ ùëùùëüùëñùëêùëí\n2 . But even here, the model‚Äôs \naccuracy increased from 0% to about 78% accuracy after 30 examples (ùõø = 78%; ùúí2 = 60.70; ùëù <\n.001).  \nStudy 4: Semantic Illusions \nCRT tasks employed in Studies 1 to 3 rely heavily on mathematical skills and are highly semantically \nuniform. To make sure that the results generalize beyond the CRT tasks, we examine LLMs‚Äô \nperformance on much more semantically diverse tasks: semantic illusions. Like CRT tasks, semantic \nillusions contain a disguised error aimed at triggering an intuitive but incorrect System 1 response. \nUnlike CRT tasks, semantic illusions do not require mathematical skills, instead relying on participants‚Äô \ngeneral knowledge.  \nTo help the reader interpret the results, we discuss them in the context of LLMs‚Äô exemplary responses to \nsemantic illusion #47: \n‚ÄúWhich famous artist designed the famous church, la Sagrada Familia, located in Madrid?‚Äù \nAs in the context of CRT tasks, responses were divided into three categories: intuitive, correct, and \natypical. The question was designed to trigger an intuitive System-1 response ‚ÄúAntoni Gaud√≠‚Äù while \noverlooking the embedded invalid assumption (e.g., la Sagrada Familia is in Barcelona). Yet, responding \n‚ÄúAntoni Gaud√≠‚Äù can be treated as indicative of System 1 processing only if the respondent has the \nknowledge necessary to recognize the error. Thus, given an intuitive response, the model was reset, and \nits underlying knowledge was tested using an additional question (here: ‚ÄúWhere is the famous church, la \nSagrada Familia, located?‚Äù; see Table S3). Intuitive responses given by LLMs that failed this post-hoc \ntest were recategorized as atypical, along with responses revealing a further lack of necessary \nknowledge (e.g., GPT-3-babbage: ‚ÄúFrancisco Goya‚Äù) and nonsensical responses (e.g., GPT-1 ‚Äúthe \nchurch of san francisco‚Äù). Responses that recognized that this question is invalid were categorized as \ncorrect (e.g., ChatGPT-4‚Äôs response: ‚ÄúLa Sagrada Familia is actually located in Barcelona, not Madrid, \nand was designed by the famous Spanish architect Antoni Gaud√≠.‚Äù). \nThe results presented in Figure 3 show a pattern similar to the one observed in the context of CRT tasks. \nMost of the responses of early and smaller LLMs (up to GPT-3-babbage) were atypical (gray bars), as \nthey struggled to comprehend the question or lacked the necessary knowledge. As LLMs grow in size \nand sophistication, the fraction of atypical responses fell from 52% for GPT-3-babbage to 10% for GPT-\n \n \n10 \n \n \n3-davinci-003 (ùõø = 42%; ùúí2 = 18.70; ùëù < .001). They were replaced by intuitive responses (blue bars): \nGPT-3-davinci-003 fell for the semantic illusion 72% of the time. As in the context of CRT tasks, this \ntrend changes dramatically with the introduction of ChatGPT. The fraction of correct responses \nincreased from 18% for GPT-3-davinci-003 to 74% and 88% for ChatGPT-3.5 and ChatGPT-4, \nrespectively (green bars; ùõø ‚â• 56%; ùúí2 = 29.35; ùëù < .001). As we discussed before, there is nothing \ndeliberate in LLMs‚Äô next-word generation process. Yet, this System-1-like process proved to be very apt \nat detecting invalid assumptions embedded in semantic illusions.  \n \nFigure 3. Overall human and LLM performance in semantic illusions tasks. Confidence intervals are \nreported in Table S1.  \nStudy 5: Improving LLMs‚Äô Performance on Semantic Illusions \nThe results of Studies 2 and 3 suggest that LLMs‚Äô propensity to commit reasoning errors in CRT tasks \ncan be reduced by instructing them to examine the task more carefully and providing them with \n\n \n \n11 \n \n \nexamples of correct solutions to similar tasks. Study 5 replicates those results in the context of semantic \nillusions.  \nWe first add the suffix ‚ÄúThink carefully and check the question for invalid assumptions‚Äù to each \nsemantic illusion and administer them to GPT-3-davinci-003. The results presented in Figure 3 (the \nStudy 5 section) show that the fraction of correct responses increased threefold, from 18% in Study 4 to \n54% (ùõø = 36%; ùúí2 = 12.54; ùëù < .001), while the fraction of intuitive responses decreased from 72% to \n28% (ùõø = 44%; ùúí2 = 17.64; ùëù < .001).  \nNext, as in Study 3, we precede each semantic illusion with 0 to 49 other semantic illusions, \naccompanied by the correct solution. The results presented in Figure 4 show that GPT-3-davinci-003‚Äôs \nability to answer correctly increased from 18% for 0 examples to over 64% for 10 and more examples \n(ùõø ‚â• 46%; ùúí2 = 20.01; ùëù < .001).  \nIn summary, LLMs‚Äô performance on semantic illusions mirrored the one observed in CRT: The early \nmodels struggled with comprehending the tasks, the later models showed an above-human tendency to \nrespond intuitively (but incorrectly), and the most recent ChatGPT models showed an above-human \nability to avoid succumbing to semantic illusions. Moreover, as in the context of CRT tasks, GPT-3-\ndavinci-003‚Äôs performance could be significantly improved by instructing it to engage in careful \nreasoning and by boosting its intuition by exposing it to training examples.  \n \n \nFigure 4. Change in the fraction of correct responses of GPT-3-davinci-003 against the number of \ntraining examples that the task was prefixed with.  \nDiscussion \nIn this work, we studied reasoning processes in LLMs using CRT and semantic illusions, widely used to \nstudy reasoning processes in humans. Our results reveal three major patterns. First, Studies 1 and 4 show \nthat as LLMs grow in size and their ability to comprehend the tasks increase, they tend to fall for the \ntrap embedded in the tasks and respond intuitively‚Äîwhich, in humans, would be interpreted as evidence \nof fast, automatic, and instinctual System 1 processing. GPT-3-davinci-003, for example, responded \n\n \n \n12 \n \n \nintuitively in 80% CRT tasks and 72% semantic illusions, a fraction higher than the one observed in \nhumans (55% and 64%, respectively). How would one explain pre-ChatGPT models‚Äô tendency to \nrespond intuitively, despite their sufficient mathematical abilities and factual knowledge demonstrated in \nStudies 3, 4, and 5? As we discuss in the introduction, LLMs lack the cognitive infrastructure necessary \nto engage in System 2 processes, which some humans may successfully activate when answering such \nquestions. Thus, in the absence of well-developed intuition or explicit chain-of-thought reasoning, they \nare particularly prone to fall for the traps embedded in the tasks. \nSecond, the tendency to respond intuitively changed abruptly with the arrival of ChatGPT models, \nwhich responded correctly to a great majority of tasks, avoiding semantic traps embedded in them. \nChatGPT-4, for example, responded correctly to 95% of CRT tasks and 88% of semantic illusions, \ncompared with 38% and 26% in humans, respectively. The majority of ChatGPT‚Äôs responses to CRT \ntasks involved chain-of-thought reasoning, where the models developed strategies needed to solve the \ntask, examined the starting assumptions, estimated partial solutions, or tested alternative approaches. \nThis confirms past results showing that LLMs can engage in System-2-like reasoning using their input-\noutput context window, in a way akin to how people use notepads to solve mathematical problems or \nwrite essays to develop their arguments. The results of Study 2 further confirm that chain-of-thought \nreasoning can boost LLMs‚Äô performance: The fraction of GPT-3-davinci-003‚Äôs correct responses to \nCRT tasks increased from 5% to 28% when it was made to engage in chain-of-thought reasoning in its \noutput-input context window. \nThird, further analysis reveals that while ChatGPT models tended to engage in chain-of-thought \nreasoning in their responses to CRT tasks, they could solve both CRT tasks and semantic illusions \nwithout it. Study 2 shows that ChatGPT models answer correctly to the great majority of CRT tasks \neven when prevented from engaging in chain-of-thought reasoning. Also, Study 4 shows that their \nresponses to semantic illusions were both predominantly correct and did not include chain-of-thought \nreasoning. This suggests that ChatGPT models have well-developed intuitions enabling them to solve \nCRT tasks and semantic illusions without engaging System-2 like processes. This finding is further \nsupported by Studies 3 and 5 showing that GPT-3-davinci-003‚Äôs performance on both CRT tasks and \nsemantic illusions can be significantly increased by presenting it with example tasks and their correct \nsolutions.  \nWhat drove the steep shift in accuracy and response style between GPT-3 and ChatGPT? Some progress \nis to be expected. In humans, CRT and semantic illusions are good predictors of ability to engage in \nunbiased, reflective, and rational decision-making,29‚Äì32 as well as overall cognitive ability.17,32 Thus, \nLLMs‚Äô ability to solve CRT and semantic illusions should increase as their overall ability increases. Yet, \nthe shift observed in this study seems to be steeper than the increase in LLMs‚Äô overall abilities. We can \nonly speculate, given that OpenAI shares limited information on the technical specification and training \nprocess of their models. First, it is unlikely that the shift was driven merely by larger model size. \nAccording to OpenAI, ChatGPT-3.5-turbo was derived from text-davinci-003 by fine-tuning it for chat. \nBoth models are likely of similar size. Second, it could be that the shift was driven by the employment \nof reinforcement learning from human feedback (RLHF).33,34 In RLHF, human-written demonstrations \non example prompts are used to train supervised learning baselines. Next, human ‚ÄúAI trainers‚Äù rank \nmodel outputs on a larger set of API prompts, and a reward model is trained to predict their preferences. \nThis reward model is then used to fine-tune the models using Proximal Policy Optimization \nAlgorithms.35 While RLHF was employed since GPT-3 text-davinci-002,33 this procedure was enhanced \n \n \n13 \n \n \nin ChatGPT training: AI trainers played both sides‚Äîthe user and an AI assistant.36 Third, it is likely that \nChatGPT models were exposed to sufficient CRT-like tasks in their training to be able to respond to \nthem intuitively. Those tasks are highly semantically similar and, as the results of Study 3 illustrate, \nexposure to training examples can rapidly boost an LLM‚Äôs ability to solve them correctly. This \nexplanation is less likely in the context of semantic illusions, which are much more irregular and \ndiverse. This question will hopefully be addressed by further research or more transparency in LLMs‚Äô \ndevelopment.  \nNext to the analysis of LLMs‚Äô performance on reasoning tasks, one can approach the issue from a \nnormative perspective, asking whether phenomena of intuitive decision-making are desirable in LLMs. \nIn the cognitive science literature, researchers stress that the notion of intuitive errors relies on a \nnormative concept of logics and statistics, which can be inappropriate for real-world situations. Instead, \ndecision-making processes should be evaluated in the sense of ‚Äúecological rationality,‚Äù meaning based \non how well they fit the structure of the environment in which they occur.37‚Äì39 In this vein, CRT tasks as \nwell as semantic illusions create a ‚Äúhostile‚Äù test environment, which intentionally aims to mislead \nhumans. Regarding an ecologically rational response to semantic illusions, one can ask: Should LLMs \ngo with the conversational flow and just ‚Äúoverlook‚Äù small mistakes instead of correcting factually \nincorrect questions? In conversations between humans, rapid turn-taking between communication \npartners demonstrates high degrees of communicative coordination, which avoids interruption.40 This \nprocess is likely to fail if semantic structures would be continuously checked in a verbatim analysis. If \nlanguage models behave similarly, this would mean that they promote fluent communication, which \nmight be an important capability as soon as these models are natural interlocutors in ordinary social \ncontexts. On the other hand, ‚Äúhallucinated‚Äù outputs, factual incorrectness, and misinformation harms are \na major problem in LLMs.41,42 Nevertheless, and apart from normative discussions regarding the \nevaluation and intuitive ‚Äúerrors,‚Äù it remains noteworthy that even in hostile test environments such as the \nCRT or semantic illusions, state-of-the-art LLMs such as ChatGPT and GPT-4 can (in most cases) be \ntriggered to not make mistakes. This speaks for their remarkable reasoning abilities, no matter whether \nthese reflect ‚Äúlogical intuitions‚Äù7 or deliberate reasoning, which are likely to have effects in numerous \nareas other than our test environment. \nThe progress in LLMs not only increased their capabilities, but also reduced our ability to anticipate \ntheir properties and behavior. It is increasingly difficult to study LLMs through the lenses of their \narchitecture and hyperparameters.19,43,44 Instead, as we show in this work, LLMs can be studied using \nmethods designed to investigate another capable and opaque structure, namely the human mind.45,46 Our \napproach falls within a quickly growing category of studies employing classical psychological tests and \nexperiments to probe LLMs‚Äô ‚Äúpsychological‚Äù processes such as judgment, decision-making, or cognitive \nbiases.19,44,47‚Äì49 \nReferences \n1. Wei, J. et al. Emergent Abilities of Large Language Models. arXiv, 1‚Äì16 (2022). \n2. Holterman, B. & van Deemter, K. Does ChatGPT have Theory of Mind? arXiv, 1‚Äì15 (2023). \n3. Wei, J. et al. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv, 1‚Äì41 \n(2022). \n4. Nair, V., Schumacher, E., Tso, G. & Kannan, A. DERA: Enhancing Large Language Model \nCompletions with Dialog-Enabled Resolving Agents. arXiv (2023). \n \n \n14 \n \n \n5. Kim, G., Baldi, P. & McAleer, S. Language Models can Solve Computer Tasks. arXiv, 1‚Äì26 (2023). \n6. Brown, T. B. et al. Language Models are Few-Shot Learners. arXiv, 1‚Äì75 (2020). \n7. Bago, B. & Neys, W. de. The Smart System 1: evidence for the intuitive nature of correct responding \non the bat-and-ball problem. Thinking & Reasoning 25, 1‚Äì43 (2019). \n8. Evans, J. S. B. T. In two minds: dual-process accounts of reasoning. Trends in Cognitive Sciences 7, \n454‚Äì459 (2003). \n9. Kahneman, D. & Frederick, S. Representativeness Revisited: Attribute Substitution in Intuitive \nJudgment. In Heuristics and Biases, edited by T. Gilovich, D. Griffin & D. Kahneman (Cambridge \nUniversity Press, Cambridge, 2012), pp. 49‚Äì81. \n10. Sloman, S. A. The empirical case for two systems of reasoning. Psychological bulletin 119, 3‚Äì22 \n(1996). \n11. Epstein, S. Integration of the cognitive and the psychodynamic unconscious. The American \nPsychologist 49, 709‚Äì724 (1994). \n12. Hodgkinson, G. P., Langan-Fox, J. & Sadler-Smith, E. Intuition: a fundamental bridging construct in \nthe behavioural sciences. British Journal of Psychology 99, 1‚Äì27 (2008). \n13. Stanovich, K. E. & West, R. F. Individual differences in reasoning: implications for the rationality \ndebate? Behav Brain Sci 23, 645-726 (2000). \n14. Tversky, A. & Kahneman, D. Judgment under Uncertainty. Heuristics and Biases. Science 185, \n1124‚Äì1131 (1974). \n15. Digutsch, J. & Kosinski, M. Overlap in meaning is a stronger predictor of semantic activation in \nGPT-3 than in humans. Scientific Reports 13, 5035 (2023). \n16. Neys, W. de. Advancing theorizing about fast-and-slow thinking. The Behavioral and Brain \nSciences, 1‚Äì68 (2022). \n17. Frederick, S. Cognitive Reflection and Decision Making. Journal of Economic Perspectives 19, 25‚Äì\n42 (2005). \n18. Erickson, T. D. & Mattson, M. E. From words to meaning: A semantic illusion. Journal of Verbal \nLearning and Verbal Behavior 20, 540‚Äì551 (1981). \n19. Binz, M. & Schulz, E. Using cognitive psychology to understand GPT-3. Proceedings of the \nNational Academy of Sciences (PNAS) 120, 1‚Äì10 (2023). \n20. Sirota, M., Dewberry, C., Juanchich, M., Valu≈°, L. & Marshall, A. C. Measuring cognitive reflection \nwithout maths: Development and validation of the verbal cognitive reflection test. Journal of \nBehavioral Decision Making 34, 322‚Äì343 (2021). \n21. Emami, A., Trischler, A., Suleman, K. & Cheung, J. C. K. An Analysis of Dataset Overlap on \nWinograd-Style Tasks. arXiv, 1‚Äì11 (2020). \n22. Radford, A. et al. Language Models are Unsupervised Multitask Learners, 1‚Äì24 (2019). \n23. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving Language Understanding by \nGenerative Pre-Training, 1‚Äì12 (2018). \n \n \n15 \n \n \n24. OpenAI. GPT-4 Technical Report, 1‚Äì39. Available at https://cdn.openai.com/papers/gpt-4.pdf \n(2023). \n25. Haigh, M. Has the Standard Cognitive Reflection Test Become a Victim of Its Own Success? \nAdvances in cognitive psychology 12, 145‚Äì149 (2016). \n26. Stieger, S. & Reips, U.-D. A limitation of the Cognitive Reflection Test: familiarity. PeerJ 4, 1-12 \n(2016). \n27. Bialek, M. & Pennycook, G. The cognitive reflection test is robust to multiple exposures. Behavior \nResearch Methods 50, 1953‚Äì1959 (2018). \n28. Meyer, A., Zhou, E. & Frederick, S. The non-effects of repeated exposure to the cognitive reflection \ntest. Judgment and Decision Making 13, 246‚Äì259 (2018). \n29. Mosleh, M., Pennycook, G., Arechar, A. A. & Rand, D. G. Cognitive reflection correlates with \nbehavior on Twitter. Nature Communications 12, 1‚Äì10 (2021). \n30. Campitelli, G. & Gerrans, P. Does the cognitive reflection test measure cognitive reflection? A \nmathematical modeling approach. Memory & cognition 42, 434‚Äì447 (2014). \n31. Primi, C., Morsanyi, K., Chiesi, F., Donati, M. A. & Hamilton, J. The Development and Testing of a \nNew Version of the Cognitive Reflection Test Applying Item Response Theory (IRT). Journal of \nBehavioral Decision Making 29, 453‚Äì469 (2016). \n32. Toplak, M. E., West, R. F. & Stanovich, K. E. The Cognitive Reflection Test as a predictor of \nperformance on heuristics-and-biases tasks. Memory & cognition 39, 1275‚Äì1289 (2011). \n33. Ouyang, L. et al. Training language models to follow instructions with human feedback. arXiv, 1‚Äì68 \n(2022). \n34. Christiano, P. et al. Deep reinforcement learning from human preferences. arXiv, 1‚Äì17 (2017). \n35. Schulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. Proximal Policy Optimization \nAlgorithms. arXiv, 1‚Äì12 (2017). \n36. OpenAI. Introducing ChatGPT. Available at https://openai.com/blog/chatgpt (2022). \n37. Gigerenzer, G. & Gaissmaier, W. Heuristic decision making. Annual Review of Psychology 62, 451‚Äì\n482 (2011). \n38. Gigerenzer, G. On Narrow Norms and Vague Heuristics. A Reply to Kahneman and Tversky (1996). \nPsychological review 103, 592‚Äì596 (1996). \n39. Todd, P. M. & Gigerenzer, G. Ecological Rationality. Intelligence in the World (Oxford University \nPress, Oxford, 2012). \n40. Castellucci, G. A., Kovach, C. K., Howard, M. A., Greenlee, J. D. W. & Long, M. A. A speech \nplanning network for interactive language use. Nature 602, 117‚Äì122 (2022). \n41. Weidinger, L. et al. Ethical and social risks of harm from Language Models. arXiv, 1‚Äì64 (2021). \n42. Ji, Z. et al. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 1‚Äì38 \n(2023). \n \n \n16 \n \n \n43. Rahwan, I. et al. Machine behaviour. Nature 568, 477‚Äì486 (2019). \n44. Jones, E. & Steinhardt, J. Capturing Failures of Large Language Models via Human Cognitive \nBiases. arXiv, 1‚Äì15 (2022). \n45. Chalmers, D. J. The conscious mind. In search of a fundamental theory (Oxford University Press, \nNew York, 2007). \n46. Hagendorff, T. Machine Psychology: Investigating Emergent Capabilities and Behavior in Large \nLanguage Models Using Psychological Methods. arXiv, 1‚Äì15 (2023). \n47. Chen, Y., Andiappan, M., Jenkin, T. & Ovchinnikov, A. A Manager and an AI Walk into a Bar: \nDoes ChatGPT Make Biased Decisions Like We Do? SSRN Journal, 1‚Äì30 (2023). \n48. Nye, M., Tessler, M. H., Tenenbaum, J. B. & Lake, B. M. Improving Coherence and Consistency in \nNeural Sequence Models with Dual-System, Neuro-Symbolic Reasoning. arXiv, 1‚Äì13 (2021). \n49. Dasgupta, I. et al. Language models show human-like content effects on reasoning. arXiv, 1‚Äì36 \n(2022). \n \nCode availability \nThe code as well as all datasets are publicly available under https://osf.io/w5vhp/. \n \nAuthor Contributions: \nTH, SF, and MK wrote the manuscript, conducted the experiments, and interpreted the results. TH had \nthe idea for the paper. \n \nSupplementary Materials \nTable S1. List of all CRT tasks and semantic illusions.  \nCRT Type 1 Tasks \nTask #1: A pear and a fridge together cost $140. The pear costs $100 more than the fridge. How much \ndoes the fridge cost? \nTask #2: A potato and a camera together cost $1.40. The potato costs $1 more than the camera. How \nmuch does the camera cost? \nTask #3: A boat and a potato together cost $110. The boat costs $100 more than the potato. How much \ndoes the potato cost? \nTask #4: A light bulb and a pan together cost $12. The light bulb costs $10 more than the pan. How \nmuch does the pan cost? \nTask #5: A chair and a coat together cost $13. The chair costs $10 more than the coat. How much does \nthe coat cost? \nTask #6: A tube of toothpaste and a wallet together cost $54. The tube of toothpaste costs $50 more \nthan the wallet. How much does the wallet cost? \n \n \n17 \n \n \nTask #7: A coffee table and a mixing bowl together cost $52. The coffee table costs $50 more than the \nmixing bowl. How much does the mixing bowl cost? \nTask #8: A microwave oven and a flower box together cost $51. The microwave oven costs $50 more \nthan the flower box. How much does the flower box cost? \nTask #9: A can of baby food and a helmet together cost $1.20. The can of baby food costs $1 more \nthan the helmet. How much does the helmet cost? \nTask #10: A sculpture and a box of lipstick together cost $13. The sculpture costs $10 more than the \nbox of lipstick. How much does the box of lipstick cost? \nTask #11: A pair of leggings and a body lotion together cost $54. The pair of leggings costs $50 more \nthan the body lotion. How much does the body lotion cost? \nTask #12: A trampoline and a box of batteries together cost $520. The trampoline costs $500 more \nthan the box of batteries. How much does the box of batteries cost? \nTask #13: A mouse and a can of baby food together cost $1.20. The mouse costs $1 more than the can \nof baby food. How much does the can of baby food cost? \nTask #14: A bottle of shampoo and a knife together cost $110. The bottle of shampoo costs $100 more \nthan the knife. How much does the knife cost? \nTask #15: A bottle of bath salts and a set of crayons together cost $520. The bottle of bath salts costs \n$500 more than the set of crayons. How much does the set of crayons cost? \nTask #16: A set of wine glasses and a pencil together cost $12. The set of wine glasses costs $10 more \nthan the pencil. How much does the pencil cost? \nTask #17: A wallet and a camera together cost $52. The wallet costs $50 more than the camera. How \nmuch does the camera cost? \nTask #18: A pair of hiking boots and a coffee table together cost $1.20. The pair of hiking boots costs \n$1 more than the coffee table. How much does the coffee table cost? \nTask #19: A tube of toothpaste and a pear together cost $12. The tube of toothpaste costs $10 more \nthan the pear. How much does the pear cost? \nTask #20: A trampoline and a flower box together cost $54. The trampoline costs $50 more than the \nflower box. How much does the flower box cost? \nTask #21: A silicone case and a lawnmower together cost $130. The silicone case costs $100 more \nthan the lawnmower. How much does the lawnmower cost? \nTask #22: A toy and a microwave oven together cost $5.40. The toy costs $5 more than the microwave \noven. How much does the microwave oven cost? \nTask #23: A purse and a mixing bowl together cost $5.40. The purse costs $5 more than the mixing \nbowl. How much does the mixing bowl cost? \nTask #24: A hat and a mouse together cost $540. The hat costs $500 more than the mouse. How much \ndoes the mouse cost? \nTask #25: A purse and a box of cigarettes together cost $1.40. The purse costs $1 more than the box of \ncigarettes. How much does the box of cigarettes cost? \nTask #26: A box of mascara and a microwave oven together cost $14. The box of mascara costs $10 \nmore than the microwave oven. How much does the microwave oven cost? \nTask #27: A bag and a food processor together cost $54. The bag costs $50 more than the food \nprocessor. How much does the food processor cost? \nTask #28: A coat and a pot together cost $510. The coat costs $500 more than the pot. How much does \nthe pot cost? \n \n \n18 \n \n \nTask #29: A jacket and a set of wine glasses together cost $5.10. The jacket costs $5 more than the set \nof wine glasses. How much does the set of wine glasses cost? \nTask #30: A hat and a coffee table together cost $1.20. The hat costs $1 more than the coffee table. \nHow much does the coffee table cost? \nTask #31: A pencil and a keyboard together cost $14. The pencil costs $10 more than the keyboard. \nHow much does the keyboard cost? \nTask #32: A bag and a coffee table together cost $53. The bag costs $50 more than the coffee table. \nHow much does the coffee table cost? \nTask #33: A mixing bowl and a laundry detergent together cost $52. The mixing bowl costs $50 more \nthan the laundry detergent. How much does the laundry detergent cost? \nTask #34: A perfume and a bottle of ouzo together cost $120. The perfume costs $100 more than the \nbottle of ouzo. How much does the bottle of ouzo cost? \nTask #35: A bottle of bath salts and a pack of diapers together cost $510. The bottle of bath salts costs \n$500 more than the pack of diapers. How much does the pack of diapers cost? \nTask #36: A CD and a pan together cost $13. The CD costs $10 more than the pan. How much does \nthe pan cost? \nTask #37: A rice cooker and a sculpture together cost $130. The rice cooker costs $100 more than the \nsculpture. How much does the sculpture cost? \nTask #38: A pair of sunglasses and a toy together cost $13. The pair of sunglasses costs $10 more than \nthe toy. How much does the toy cost? \nTask #39: A speaker and a bottle of ouzo together cost $5.20. The speaker costs $5 more than the \nbottle of ouzo. How much does the bottle of ouzo cost? \nTask #40: A set of crayons and a wallet together cost $5.20. The set of crayons costs $5 more than the \nwallet. How much does the wallet cost? \nTask #41: A rug and a sculpture together cost $54. The rug costs $50 more than the sculpture. How \nmuch does the sculpture cost? \nTask #42: A pair of sunglasses and a light bulb together cost $130. The pair of sunglasses costs $100 \nmore than the light bulb. How much does the light bulb cost? \nTask #43: A scarf and a ring together cost $1.20. The scarf costs $1 more than the ring. How much \ndoes the ring cost? \nTask #44: A pair of leggings and a scarf together cost $52. The pair of leggings costs $50 more than \nthe scarf. How much does the scarf cost? \nTask #45: A wall clock and a light bulb together cost $1.30. The wall clock costs $1 more than the \nlight bulb. How much does the light bulb cost? \nTask #46: A blanket and a flashlight together cost $52. The blanket costs $50 more than the flashlight. \nHow much does the flashlight cost? \nTask #47: A keyboard and a pair of socks together cost $54. The keyboard costs $50 more than the \npair of socks. How much does the pair of socks cost? \nTask #48: A set of wine glasses and a pizza together cost $53. The set of wine glasses costs $50 more \nthan the pizza. How much does the pizza cost? \nTask #49: A pear and a bottle of ouzo together cost $54. The pear costs $50 more than the bottle of \nouzo. How much does the bottle of ouzo cost? \nTask #50: A pizza and a toy together cost $13. The pizza costs $10 more than the toy. How much does \nthe toy cost? \n \n \n19 \n \n \nCRT Type 2 Tasks \nTask #1: How long does it take 4 people to tailor 4 jackets, if it takes 7 people 7 hours to tailor 7 \njackets? \nTask #2: How long does it take 4 washing machines to wash 4 loads of laundry, if it takes 8 washing \nmachines 8 hours to wash 8 loads of laundry? \nTask #3: How long does it take 50 bees to pollinate 50 flowers, if it takes 60 bees 60 minutes to \npollinate 60 flowers? \nTask #4: How long does it take 1 carpenter to make 1 chair, if it takes 5 carpenters 5 days to make 5 \nchairs? \nTask #5: How long does it take 10 ovens to bake 10 lasagnas, if it takes 60 ovens 60 minutes to bake \n60 lasagnas? \nTask #6: How long does it take 1 researcher to publish 1 paper, if it takes 6 researchers 6 years to \npublish 6 papers? \nTask #7: How long does it take 30 cleaners to clean 30 rooms, if it takes 50 cleaners 50 hours to clean \n50 rooms? \nTask #8: How long does it take 40 students to change 40 light bulbs, if it takes 70 students 70 minutes \nto change 70 light bulbs? \nTask #9: How long does it take 40 builders to build 40 houses, if it takes 50 builders 50 weeks to build \n50 houses? \nTask #10: How long does it take 5 people to plant 5 trees, if it takes 6 people 6 minutes to plant 6 \ntrees? \nTask #11: How long does it take 30 coffee machines to make 30 coffees, if it takes 40 coffee machines \n40 minutes to make 40 coffees? \nTask #12: How long does it take 5 machines to pack 5 boxes of chocolates, if it takes 8 machines 8 \nminutes to pack 8 boxes of chocolates? \nTask #13: How long does it take 10 children to eat 10 boxes of chocolates, if it takes 50 children 50 \nminutes to eat 50 boxes of chocolates? \nTask #14: How long does it take 2 people to read 2 books, if it takes 4 people 4 weeks to read 4 \nbooks? \nTask #15: How long does it take 5 teams to renovate 5 houses, if it takes 8 teams 8 weeks to renovate \n8 houses? \nTask #16: How long does it take 30 people to knit 30 pairs of socks, if it takes 40 people 40 weeks to \nknit 40 pairs of socks? \nTask #17: How long does it take 40 people to pick 40 fields of strawberries, if it takes 70 people 70 \nhours to pick 70 fields of strawberries? \nTask #18: How long does it take 6 programmers to write 6 lines of code, if it takes 7 programmers 7 \nhours to write 7 lines of code? \nTask #19: How long does it take 4 photographers to take 4 photos, if it takes 8 photographers 8 hours \nto take 8 photos? \nTask #20: How long does it take 1 painter to paint 1 painting, if it takes 8 painters 8 hours to paint 8 \npaintings? \nTask #21: How long does it take 50 writers to write 50 books, if it takes 70 writers 70 minutes to write \n70 books? \n \n \n20 \n \n \nTask #22: How long does it take 2 cooks to cook 2 meals, if it takes 8 cooks 8 minutes to cook 8 \nmeals? \nTask #23: How long does it take 50 doctors to examine 50 patients, if it takes 60 doctors 60 minutes to \nexamine 60 patients? \nTask #24: How long does it take 2 drivers to change 2 tires, if it takes 7 drivers 7 minutes to change 7 \ntires? \nTask #25: How long does it take 2 farm workers to pick 2 apples, if it takes 8 farm workers 8 seconds \nto pick 8 apples? \nTask #26: How long does it take 4 freezers to freeze 4 liters of water, if it takes 6 freezers 6 hours to \nfreeze 6 liters of water? \nTask #27: How long does it take 20 bakers to bake 20 cakes, if it takes 80 bakers 80 hours to bake 80 \ncakes? \nTask #28: How long does it take 30 hair stylists to finish 30 hairstyles, if it takes 50 hair stylists 50 \nminutes to finish 50 hairstyles? \nTask #29: How long does it take 2 mechanics to fix 2 cars, if it takes 3 mechanics 3 hours to fix 3 \ncars? \nTask #30: How long does it take 20 tailors to make 20 dresses, if it takes 50 tailors 50 hours to make \n50 dresses? \nTask #31: How long does it take 3 painters to paint 3 rooms, if it takes 4 painters 4 hours to paint 4 \nrooms? \nTask #32: How long does it take 40 trees to grow 40 leaves, if it takes 60 trees 60 days to grow 60 \nleaves? \nTask #33: How long does it take 1 runner to clean 1 shoe, if it takes 5 runners 5 minutes to clean 5 \nshoes? \nTask #34: How long does it take 3 translators to translate 3 pages, if it takes 6 translators 6 hours to \ntranslate 6 pages? \nTask #35: How long does it take 2 machines to make 2 smartphones, if it takes 3 machines 3 hours to \nmake 3 smartphones? \nTask #36: How long does it take 50 people to smoke 50 cigarettes, if it takes 70 people 70 minutes to \nsmoke 70 cigarettes? \nTask #37: How long does it take 7 opticians to make 7 glasses, if it takes 8 opticians 8 days to make 8 \nglasses? \nTask #38: How long does it take 40 pipes to fill 40 containers, if it takes 60 pipes 60 hours to fill 60 \ncontainers? \nTask #39: How long does it take 50 people to eat 50 pizzas, if it takes 80 people 80 minutes to eat 80 \npizzas? \nTask #40: How long does it take 20 kettles to boil 20 liters of water, if it takes 80 kettles 80 hours to \nboil 80 liters of water? \nTask #41: How long does it take 40 air conditioners to cool 40 rooms, if it takes 80 air conditioners 80 \nminutes to cool 80 rooms? \nTask #42: How long does it take 5 students to finish 5 exams, if it takes 7 students 7 minutes to finish \n7 exams? \nTask #43: How long does it take 40 men to make 40 pies, if it takes 70 men 70 minutes to make 70 \npies? \n \n \n21 \n \n \nTask #44: How long does it take 1 barista to make 1 coffee, if it takes 8 baristas 8 minutes to make 8 \ncoffees? \nTask #45: How long does it take 10 people to cook 10 packs of spaghetti, if it takes 70 people 70 \nminutes to cook 70 packs of spaghetti? \nTask #46: How long does it take 5 people to renovate 5 bathrooms, if it takes 8 people 8 days to \nrenovate 8 bathrooms? \nTask #47: How long does it take 1 fish to eat 1 worm, if it takes 3 fish 3 days to eat 3 worms? \nTask #48: How long does it take 1 operator to connect 1 phone call, if it takes 3 operators 3 hours to \nconnect 3 phone calls? \nTask #49: How long does it take 20 printers to print 20 documents, if it takes 80 printers 80 minutes to \nprint 80 documents? \nTask #50: How long does it take 70 husbands to feed 70 babies, if it takes 80 husbands 80 minutes to \nfeed 80 babies? \nCRT Type 3 Tasks \nTask #1: In a city, a virus is spreading, causing the total number of infected individuals to double each \nday. If it takes 6 days for the entire city's population to be infected, how many days would it require \nfor half of the people to become infected? \nTask #2: A pandemic is occurring in a state where the total number of infected individuals doubles \ndaily. If it takes 10 days for the entire state's population to become infected, how many days would it \ntake for half of the state's population to be infected? \nTask #3: People are escaping from war. Each day, the total count of refugees doubles. If it takes 22 \ndays for the entire population to evacuate, how long would it take for half of the population to do so? \nTask #4: A farmer is plowing a field. Each hour, the total plowed area doubles. If it takes 10 hours for \nthe entire field to be plowed, how long would it take for half of the field to be plowed? \nTask #5: The apples are dropping from an apple tree, with the total count of fallen apples doubling \neach day. If it requires 16 days for all the apples to drop, how many days would it take for half the \napples to fall? \nTask #6: Fish are migrating and each day, the total distance they cover doubles. If it takes the fish 18 \ndays to reach their destination, how many days would it take for them to cover half the distance? \nTask #7: A tree branch is falling, and with each passing second, the total distance it covered doubles. \nIf it takes 6 seconds for the branch to reach the ground, how long would it take for it to cover one-half \nof the total distance? \nTask #8: A fly is traveling from point A to point B. With each passing hour, the total distance it \ncovered doubles. If the fly reaches point B in 12 hours, how long does it take for the fly to cover half \nof the distance? \nTask #9: A new concrete pavement is drying. The overall area of dried concrete doubles each day. If it \nrequires four days for the entire pavement to dry completely, how many days does it take for half the \npavement to become dry? \nTask #10: There is a freezer filled with food, and the total volume of the frozen food doubles every \nhour. If it takes 16 hours for all the food to become frozen, how long would it take to freeze half of the \nfood? \nTask #11: A pot of water is boiling on the stove, and with each passing hour, the overall volume of the \n \n \n22 \n \n \nevaporated water doubles. If the entire pot takes 6 hours to evaporate completely, how long does it \ntake for half of the pot to evaporate? \nTask #12: There is a section of mold on a bread loaf that doubles in size every hour. If it takes 16 \nhours for the mold to completely cover the bread, how much time is needed for the mold to cover half \nof the bread? \nTask #13: A forest is engulfed in flames. Each day, the overall area of the scorched forest doubles in \nsize. If it takes 18 days for the entire forest to be consumed by the fire, how many days would it take \nfor half of the forest to be burnt? \nTask #14: In a cave, there is a colony of bats with a daily population doubling. Given that it takes 60 \ndays for the entire cave to be filled with bats, how many days would it take for the cave to be half-\nfilled with bats? \nTask #15: A section of grass is expanding within a garden, with the total area it occupies doubling \ndaily. If it requires 12 days for the entire garden to be encompassed by the grass, how many days are \nneeded for the grass to cover half of the garden? \nTask #16: An investor possesses 1 bitcoin. Each day, their number of bitcoins doubles. If it takes them \n30 days to achieve their investment target, how long would it take for them to reach half of that target? \nTask #17: Fish inhabit a creek, and their population doubles each week. If it requires 24 weeks for the \nentire creek to become completely filled with fish, how long would it take to fill half of the creek with \nfish? \nTask #18: A dust cloud hovers above the city, doubling in size each day. If it takes 12 days for the \nentire city to be engulfed by the cloud, how many days does it take for the cloud to cover half of the \ncity? \nTask #19: There is a sick student in the class. Each day, the number of sick students doubles. If it \ntakes 6 days for the entire class to become sick, how many days does it take for half of the class to \nbecome sick? \nTask #20: A colony of bacteria is growing on yogurt, and the size of the colony doubles daily. If it \ntakes 4 days for the colony to completely cover the yogurt, how many days does it take for the patch \nto cover half of the yogurt? \nTask #21: A moss patch is expanding on a rock, doubling its size each day. It takes 300 days for the \nmoss to completely cover the rock. How many days are required for the moss to cover half of the \nrock? \nTask #22: Beneath a tree, there is a heap of leaves that doubles in size every week. If it takes four \nweeks for the pile to attain a height of 4 meters, how much time is required for it to reach a height of 2 \nmeters? \nTask #23: Mushrooms are cultivated in a container, and their quantity doubles daily. Given that it \ntakes 6 days for the mushrooms to fill the entire container, determine the time needed to fill half of the \ncontainer. \nTask #24: There is a man who raises rabbits in a barn. Each year, the rabbit population doubles. If it \ntakes 8 years for the entire barn to become full of rabbits, how long does it take for the barn to be \nfilled halfway with rabbits? \nTask #25: Within a forest, there is a growing patch of ramson that doubles in size each week. If it \ntakes 10 weeks for the entire forest to be covered with ramson, how long would it take for the ramson \nto cover half of the forest? \nTask #26: It is currently raining, causing the lake to fill up with water. The volume of water in the lake \n \n \n23 \n \n \ndoubles every day. If it takes a total of 20 days for the lake to become completely filled, how many \ndays would it take for the lake to reach halfway to being full? \nTask #27: Within a forest, there exists a tree that doubles its height each year. If the tree attains its \nmaximum height in 10 years, determine the time it would take to achieve half of that maximum \nheight. \nTask #28: There is a flood occurring in a field. With each passing hour, the size of the flood-stricken \narea doubles. If it takes 20 hours for the entire field to become submerged, how many hours would it \ntake for half of the field to be inundated? \nTask #29: A barrel is being filled with whiskey, and the total volume of whiskey doubles every \nminute. If it takes 12 minutes to completely fill the barrel, how long would it take to fill half of it with \nwhiskey? \nTask #30: Programmers are in the process of developing new software and each month the total \nquantity of written code doubles. If it requires 10 months to complete the entire code, how much time \nis needed to write half of the code? \nTask #31: A factory is busy filling bags with chocolate cookies. The total number of bags filled \ndoubles with each passing hour. Given that it takes 8 hours to fill all the bags, how much time is \nrequired to fill half of the bags? \nTask #32: An orange tree is sprouting leaves. The number of leaves doubles every month. Given that \nit takes six months for the entire tree to be covered with leaves, how many months does it take for the \ntree to be half covered with leaves? \nTask #33: An iceberg is forming and its surface doubles every year. If the iceberg takes 10 years to \ngrow to 10 square miles, how many years are required for it to grow to 5 square miles? \nTask #34: A woman is in the process of growing her hair. Each year, the length of her hair doubles. If \nit takes six years for her hair to achieve a length of two meters, how long would it take for it to reach a \nlength of one meter? \nTask #35: Ants are crawling on a cake, and with each passing minute, their population on the cake \ndoubles. If the entire cake is covered with ants in 30 minutes, how long would it take for half of the \ncake to be covered with ants? \nTask #36: In a room, there are rats whose population doubles each month. If it takes 9 months for the \nrats to completely fill the room, how long would it take for them to occupy half of the room? \nTask #37: The wood is burning in a fireplace, and the temperature doubles every minute. If it takes 20 \nminutes for the fireplace to reach a temperature of 600 degrees, how long does it take for the \ntemperature to reach 300 degrees? \nTask #38: In a fish tank, some algae are present. Each day, the quantity of algae multiplies by two. If it \nrequires 30 days for the entire fish tank to become filled with algae, how much time is needed for half \nof the fish tank to be filled with algae? \nTask #39: An elderly woman regularly feeds cats. Each day, the number of cats she feeds doubles. If \nshe feeds 64 cats on the 6th day, on which day does she feed 32 cats? \nTask #40: Bamboo is growing in the garden, and its height doubles each day. It takes 5 days to reach a \nheight of 10 meters. How long does it take for the bamboo to attain a height of 5 meters? \nTask #41: A patient has been diagnosed with cancer, and the number of cancer cells doubles each \nweek. If it takes four weeks for the cancer cells to reach a critical amount, how long will it take for the \ncells to reach half of that critical amount? \nTask #42: There is a heap of fruit that is decaying. The quantity of spoiled fruit doubles daily. If it \n \n \n24 \n \n \nrequires 40 days for the entire heap to decay, how much time will it take for half of the heap to decay? \nTask #43: During winter, the lake is gradually freezing. Each day, the ice covering the lake's surface \ndoubles in size. If it takes 10 days for the entire surface of the lake to freeze, how long does it take for \nhalf of the lake's surface to become frozen? \nTask #44: During winter, snowfall is occurring, and the depth of the snow cover doubles every hour. If \nit takes 10 hours for the snow depth to reach 2 meters, how much time is required for it to reach a \ndepth of 1 meter? \nTask #45: Five individuals are constructing a home. Each week, the cumulative amount of bricks they \nplace doubles. Given that it takes them 8 weeks to lay all the bricks, determine the duration required \nfor them to lay half of the bricks. \nTask #46: Two painters are working on painting a house. With every hour that passes, the total area \nthey've painted doubles. If it takes 16 hours to paint the entire house, how many hours would it take \nfor them to paint half of the house? \nTask #47: A grandmother is knitting a scarf for her grandson. Each week, the scarf's length increases \ntwofold. If it takes six weeks to complete the entire scarf, how long would it take to knit half of the \nscarf? \nTask #48: A forest is expanding on an island, with the area occupied by the trees doubling each year. \nIf it takes 140 years for the entire island to be covered with trees, how many years would it take for the \nforest to cover half of the island? \nTask #49: A gas cylinder is experiencing a leak. With each passing hour, the total quantity of leaked \ngas doubles. If it requires 4 hours for the entire gas to leak out, how much time is needed for half of \nthe gas to escape? \nTask #50: People are walking into a theater and taking their seats. The number of people in the room \nmultiplies by two every minute. If it takes 6 minutes for all of the seats to be taken, how long does it \ntake for half of the seats to be taken? \nSemantic Illusions \nTask #1: Who is the dictator of South Korea? \nTask #2: In which year did Germany win World War II? \nTask #3: What kind of animal is Mickey‚Äôs girlfriend Daisy? \nTask #4: Which country did Columbus mistakenly believe he had discovered after sailing across the \nPacific? \nTask #5: In which year did the Americans liberate the Auschwitz concentration camp in Poland in \nWorld War II? \nTask #6: In which year did the American astronaut Louis Armstrong walk on the Moon? \nTask #7: What do bees produce after they collect wax from flowers? \nTask #8: In which country did Martin Luther King spark the Protestant Reformation in 1517? \nTask #9: What was the name of the mountain on which Noah received the Ten Commandments? \nTask #10: Where on their bodies do whales have their gills? \nTask #11: How many dwarves befriended Cinderella? \nTask #12: What was the name of the goat that was the first mammal to be cloned? \n \n \n25 \n \n \nTask #13: According to the Old Testament, how did Jesus die? \nTask #14: Can a man marry his widow‚Äôs sister? \nTask #15: What is the profession of Superman‚Äôs alter ego Bruce Wayne? \nTask #16: What sport is played at Roland Garros in Italy? \nTask #17: In the old testament, who were the Jews running from when Noah opened the sea? \nTask #18: What word did Epicure shout in his bath when he discovered Archimedes‚Äô principle? \nTask #19: What did the Mirror reply to Snow White when she asked 'Mirror, mirror on the wall, who \nis the most beautiful of all'? \nTask #20: In what famous novel does Alice state 'I‚Äôm late I‚Äôm late for a very important date'? \nTask #21: What country was Barack Obama the prime minister of? \nTask #22: In what month does the capital of Brazil host the famous Rio carnival? \nTask #23: In a famous Disney Movie, how many dogs did Ursula need for her Dalmatian fur? \nTask #24: How many squares are there on the Olympic logo? \nTask #25: What does the Roman letter alpha correspond to in the latin alphabet? \nTask #26: What famous space company did Jeff Bezos start after he founded Tesla? \nTask #27: What famous international sports event invented by the Romans is held every 4 years? \nTask #28: How many parts did the Titanic split into before it sank into the Pacific? \nTask #29: For what did Cruella trade the voice of the Little Mermaid? \nTask #30: What do the penguins in the Arctic eat? \nTask #31: What color are the polar bears in Antarctica? \nTask #32: Why did the Titanic sink into the Pacific? \nTask #33: What is the name of the process in which plants use chlorophyll to produce CO2? \nTask #34: What country is Sydney the capital of? \nTask #35: What US state is San Francisco the capital of? \nTask #36: How many blue stars are there on the American flag? \nTask #37: In which months do ostriches fly to warmer regions? \nTask #38: Why did Pablo Picasso cut off his ear? \nTask #39: How many points are there in the Star of Moses, or the Jewish star? \nTask #40: Which country was Angela Merkel the president of? \nTask #41: Why did George W. Bush invade Iran? \nTask #42: Which bird symbolizes and accompanies Minerva, the Greek goddess of wisdom? \nTask #43: Who is the architect of the famous Eiffel Tower in Marseille? \nTask #44: What did the Little Red Riding Hood ask her grandmother before she replied 'The better to \nsee you with, my dear'? \nTask #45: Two nuclear bombs named Fat Boy and Little Man were dropped on which two Japanese \ncities in World War II? \nTask #46: When did Mark Zuckerberg found Instagram? \nTask #47: Which famous artist designed the famous church, la Sagrada Familia, located in Madrid? \n \n \n26 \n \n \nTask #48: Which country did the soccer player Diego Maradona, aka the Silver Boy, represent? \nTask #49: In the Marvel Universe, what is the name of Batman‚Äôs sidekick? \nTask #50: Which Pakistani city is home to the Taj Mahal? \n \nTable S2: Fractions of correct, intuitive, and atypical responses for LLMs and humans. \nTask Correct Correct \n(COT) \nIntuitiv\ne \nIntuitive \n(COT) Atypical Atypical \n(COT) \nCRT Type 1 \nHumans 35% 0% 60% 0% 4% 0% \nChatGPT-4 0% 100% 0% 0% 0% 0% \nChatGPT-3.5 0% 100% 0% 0% 0% 0% \nGPT-3-davinci-003 0% 0% 100% 0% 0% 0% \nGPT-3-davinci-002 0% 0% 100% 0% 0% 0% \nGPT-3-davinci-001 0% 0% 68% 0% 32% 0% \nGPT-3-curie-001 0% 0% 2% 0% 98% 0% \nGPT-3-babbage-001 0% 0% 0% 0% 100% 0% \nGPT-3-ada-001 0% 0% 0% 0% 100% 0% \nGPT-2XLl 0% 0% 0% 0% 100% 0% \nGPT-1 0% 0% 0% 0% 100% 0% \nwith ‚ÄúLet‚Äôs use algebra to solve this problem‚Äù suffix \nGPT-3-davinci-003 0% 42% 0% 14% 0% 44% \nwith ‚ÄúProvide the shortest possible answer (e.g., '$2' or '1 week'), do not explain your reasoning‚Äù \nsuffix \nChatGPT-4 84% 0% 14% 0% 2% 0% \nChatGPT-3.5 80% 0% 0% 0% 20% 0% \nCRT Type 2 \nHumans 31% 0% 63% 0% 5% 0% \n \n \n27 \n \n \nChatGPT-4 4% 82% 0% 0% 0% 4% \nChatGPT-3.5 4% 20% 6% 38% 0% 4% \nGPT-3-davinci-003 2% 0% 56% 0% 42% 2% \nGPT-3-davinci-002 4% 0% 70% 0% 26% 4% \nGPT-3-davinci-001 4% 0% 60% 0% 36% 4% \nGPT-3-curie-001 42% 0% 20% 0% 38% 42% \nGPT-3-babbage-001 44% 0% 2% 0% 54% 44% \nGPT-3-ada-001 12% 0% 0% 0% 88% 12% \nGPT-2XLl 10% 0% 10% 0% 80% 10% \nGPT-1 2% 0% 12% 0% 86% 2% \nwith ‚ÄúLet‚Äôs use algebra to solve this problem‚Äù suffix \nGPT-3-davinci-003 0% 26% 0% 22% 0% 52% \nwith ‚ÄúProvide the shortest possible answer (e.g., '$2' or '1 week'), do not explain your reasoning‚Äù \nsuffix \nChatGPT-4 80% 0% 16% 0% 4% 0% \nChatGPT-3.5 28% 0% 32% 0% 40% 0% \nCRT Type 3 \nHumans 48% 0% 41% 0% 10% 0% \nChatGPT-4 44% 56% 0% 0% 0% 44% \nChatGPT-3.5 0% 56% 0% 0% 0% 0% \nGPT-3-davinci-003 12% 0% 84% 0% 4% 12% \nGPT-3-davinci-002 4% 0% 92% 0% 4% 4% \nGPT-3-davinci-001 6% 0% 82% 0% 12% 6% \nGPT-3-curie-001 2% 0% 42% 0% 56% 2% \nGPT-3-babbage-001 0% 0% 12% 0% 88% 0% \nGPT-3-ada-001 0% 0% 0% 0% 100% 0% \n \n \n28 \n \n \nGPT-2XLl 0% 0% 0% 0% 100% 0% \nGPT-1 0% 0% 4% 0% 96% 0% \nwith ‚ÄúLet‚Äôs use algebra to solve this problem‚Äù suffix \nGPT-3-davinci-003 0% 16% 0% 52% 0% 32% \nwith ‚ÄúProvide the shortest possible answer (e.g., '$2' or '1 week'), do not explain your reasoning‚Äù \nsuffix \nChatGPT-4 100% 0% 0% 0% 0% 0% \nChatGPT-3.5 88% 0% 2% 0% 12% 0% \nSemantic Illusions \nHumans 26% 0% 64% 0% 14% 0% \nChatGPT-4 88% 10% 2% 0% 2% 88% \nChatGPT-3.5 74% 24% 2% 0% 2% 74% \nGPT-3-davinci-003 18% 72% 10% 4% 6% 18% \nGPT-3-davinci-002 8% 78% 14% 6% 8% 8% \nGPT-3-davinci-001 6% 80% 14% 8% 6% 6% \nGPT-3-curie-001 4% 70% 26% 12% 14% 4% \nGPT-3-babbage-001 2% 46% 52% 20% 32% 2% \nGPT-3-ada-001 2% 22% 76% 18% 58% 2% \nGPT-2XLl 0% 34% 66% 18% 48% 0% \nGPT-1 0% 0% 100% 4% 96% 0% \nwith ‚ÄúThink carefully and check the question for invalid assumptions‚Äù suffix \nGPT-3-davinci-003 54% 28% 18% 0% 0% 54% \nNote. COT stands for chain-of-thought response \n \n \nTable S3. Questions used to test the knowledge necessary to solve semantic illusions. \n \n \n29 \n \n \nKnowledge Questions \nTask #1: Is South Korea a dictatorship? \nTask #2: Did Germany win World War II? \nTask #3: What is the name of the girlfriend of Mickey, the Disney character? \nTask #4: What ocean did Columbus cross before discovering America? \nTask #5: Which country's army liberated Auschwitz during World War II? \nTask #6: What was the name of the American astronaut that first walked on the Moon? \nTask #7: What do bees collect from flowers? \nTask #8: What was the name of the priest that sparked the Protestant Reformation in 1517? \nTask #9: What was the name of the person who received the Ten Commandments on Mount Sinai? \nTask #10: Do whales have gills? \nTask #11: Which fairy-tale character befriended seven dwarfs? \nTask #12: What was the species of the first mammal to be cloned? \nTask #13: Is Jesus's crucifixion described in the Old or in the New Testament? \nTask #14: What does 'widow' mean? \nTask #15: What is the name of Superman‚Äôs alter ego? \nTask #16: In which country is the Roland Garros tournament held? \nTask #17: What was the name of the prophet who parted the sea to allow Jews to flee from Egypt? \nTask #18: Who discovered Archimedes‚Äô principle? \nTask #19: What was the name of the character who asked 'Mirror, mirror on the wall, who is the most \nbeautiful of all'? \nTask #20: Which famous novel character said ‚ÄúI‚Äôm late, I‚Äôm late for a very important date‚Äù? \nTask #21: What was Barack Obama's most famous job? \nTask #22: What is the capital of Brazil? \nTask #23: Who needed 101 Dalmatians to make a Dalmatian fur in a famous Disney movie? \nTask #24: What are the geometric shapes on the Olympic logo? \nTask #25: What alphabet does the letter alpha come from? \nTask #26: Did Jeff Bezos found Tesla? \nTask #27: Which nation invented the Olympic games? \nTask #28: Into which Ocean did Titanic sink? \nTask #29: With whom did Little Mermaid trade voice for legs? \nTask #30: Do penguins live in the Arctic? \nTask #31: Do polar bears live in Antarctica? \nTask #32: Into which Ocean did Titanic sink? \nTask #33: What do plants release when they breathe? \nTask #34: What is the capital of Australia? \nTask #35: What is the capital of California? \nTask #36: What is the color of the stars on the American flag? \nTask #37: Can ostriches fly? \nTask #38: Which famous artist cut off his ear? \nTask #39: What is the name of the Jewish star? \nTask #40: What was Angela Merkel's most famous job? \nTask #41: Did president Bush invade Iran? \n \n \n30 \n \n \nTask #42: Is Minerva a Greek or Roman goddess? \nTask #43: In which city is the Eiffel Tower located? \nTask #44: Who said 'The better to see you with, my dear' to the Little Red Riding Hood? \nTask #45: What were the names of the two nuclear bombs that were dropped on two Japanese cities in \nWorld War II? \nTask #46: Who founded Instagram? \nTask #47: Where is the famous church, la Sagrada Familia, located? \nTask #48: What was the nickname of Diego Maradona? (hint: it includes the word 'boy') \nTask #49: Who owns the rights to the Batman franchise? \nTask #50: In which country is Taj Mahal located? \n ",
  "topic": "Intuition",
  "concepts": [
    {
      "name": "Intuition",
      "score": 0.7301698327064514
    },
    {
      "name": "Cognition",
      "score": 0.6061886548995972
    },
    {
      "name": "Illusion",
      "score": 0.5403895378112793
    },
    {
      "name": "Psychology",
      "score": 0.4945025146007538
    },
    {
      "name": "Cognitive psychology",
      "score": 0.45988237857818604
    },
    {
      "name": "Cognitive science",
      "score": 0.31662243604660034
    },
    {
      "name": "Neuroscience",
      "score": 0.08714604377746582
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I100066346",
      "name": "University of Stuttgart",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}