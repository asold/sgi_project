{
  "title": "Automated Theorem Provers Help Improve Large Language Model Reasoning",
  "url": "https://openalex.org/W4399037068",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2480088270",
      "name": "Lachlan McGinness",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A502358098",
      "name": "Peter Baumgartner",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4302305823",
    "https://openalex.org/W2963368301",
    "https://openalex.org/W4385570291",
    "https://openalex.org/W4385569771",
    "https://openalex.org/W995310127",
    "https://openalex.org/W4298084898",
    "https://openalex.org/W3154009503",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4389519519",
    "https://openalex.org/W4386211255",
    "https://openalex.org/W4287671158",
    "https://openalex.org/W4295529475",
    "https://openalex.org/W3176736844",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4386211517",
    "https://openalex.org/W4400047099",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4390940921",
    "https://openalex.org/W4381252438",
    "https://openalex.org/W4323927642",
    "https://openalex.org/W4379933260",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2970476646"
  ],
  "abstract": "In this paper we demonstrate how logic programming systems and Automated first- order logic Theorem Provers (ATPs) can improve the accuracy of Large Language Models (LLMs) for logical reasoning tasks where the baseline performance is given by direct LLM solutions. We first evaluate LLM reasoning on steamroller problems using the PRON- TOQA benchmark. We show how accuracy can be improved with a neuro-symbolic ar- chitecture where the LLM acts solely as a front-end for translating a given problem into a formal logic language and an automated reasoning engine is called for solving it. How- ever, this approach critically hinges on the correctness of the LLM translation. To assess this translation correctness, we secondly define a framework of syntactic and semantic er- ror categories. We implemented the framework and used it to identify errors that LLMs make in the benchmark domain. Based on these findings, we thirdly extended our method with capabilities for automatically correcting syntactic and semantic errors. For semantic error correction we integrate first-order logic ATPs, which is our main and novel contribu- tion. We demonstrate that this approach reduces semantic errors significantly and further increases the accurracy of LLM logical reasoning.",
  "full_text": "Automated Theorem Provers Help Improve Large\nLanguage Model Reasoning∗\nLachlan McGinness1 and Peter Baumgartner2\n1 School of Computer Science, Australian National University and Data61, CSIRO\nlachlan.mcginness@anu.edu.au\n2 Data61, CSIRO and School of Computer Science, Australian National University\npeter.baumgartner@data61.csiro.au\nAbstract\nIn this paper we demonstrate how logic programming systems and Automated first-\norder logic Theorem Provers (ATPs) can improve the accuracy of Large Language Models\n(LLMs) for logical reasoning tasks where the baseline performance is given by direct LLM\nsolutions. We first evaluate LLM reasoning on steamroller problems using the PRON-\nTOQA benchmark. We show how accuracy can be improved with a neuro-symbolic ar-\nchitecture where the LLM acts solely as a front-end for translating a given problem into\na formal logic language and an automated reasoning engine is called for solving it. How-\never, this approach critically hinges on the correctness of the LLM translation. To assess\nthis translation correctness, we secondly define a framework of syntactic and semantic er-\nror categories. We implemented the framework and used it to identify errors that LLMs\nmake in the benchmark domain. Based on these findings, we thirdly extended our method\nwith capabilities for automatically correcting syntactic and semantic errors. For semantic\nerror correction we integrate first-order logic ATPs, which is our main and novel contribu-\ntion. We demonstrate that this approach reduces semantic errors significantly and further\nincreases the accurracy of LLM logical reasoning.\n1 Introduction, Background and Related Work\nThe release of models like GPT [3] and Gemini [28] through platforms like ChatGPT and Bard\nhave transformed Large Language Models (LLMs) into general-purpose tools that can be used\nby everyone. Although designed for next token prediction, LLMs have been shown to have\nemergent abilities and are able to perform a wide variety of tasks without task-specific training\ndata [3, 20, 25, 30, 31].\nUnfortunately, LLMs also frequently return wrong results, such as fictitious claims (“hallucina-\ntions”) or conclusions that defy common sense or (naive qualitative) physics [13, 16, 27]. Such\nshortcoming may or may not be obvious but in any case impact trustworthiness. A recent fa-\nmous example was a lawyer who submitted a legal brief generated by ChatGPT which contained\nmany errors and false references [5, 6]. Asking the LLM for an explanation might help, but\nthe explanation might contain errors again and does not necessarily reflect the process used to\nobtain its answer. Equipping and checking LLMs with trustworthy (logical) reasoning remains\nto be a current major problem [21, 22].\nA general approach to address this problem equips LLMs with external functionality [8, 10, 13,\n19, 21]. These equipped models are referred to as Augmented Language Models (ALMs). The\ngeneral problem of combining neural networks with symbolic reasoners has attracted a lot of\n∗This paper has also been published in: N. Bjørner, M. Heule and A. Voronkov (eds.), LPAR 2024 (EPiC\nSeries in Computing, vol. 100), pp. 51–69\narXiv:2408.03492v1  [cs.AI]  7 Aug 2024\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nattention recently (a popular umbrella term is “neuro-symbolic computation”). An impressive\nexample is the work by Trinh et al. [29] which demonstrates that a neuro-symbolic architecture\nthat can solve International Mathematics Olympiad geometry questions at an expert level.\nOther proposed combination schemes range from end-to-end differentiable architectures with\ntightly integrated training regimes [7, 14, 15, 32] to more loosely coupled systems where pre-\ntrained models are linked with a reasoner through a formal language interface [23, 27]. In\nthis paper we consider combinations of the latter kind. Pre-trained LLMs are used as black\nboxes tasked with translating problems that require logical reasoning into a formal logic, so\nthat an Automated Reasoning (AR) system can be applied. We first show how accuracy can\nbe improved with such a neuro-symbolic architecture.\nThis approach naturally provides excellent explainability and trustworthiness on the AR side.\nTherefore the correctness of the overall system critically hinges on the correctness of the LLM\ntranslation. However, engineering prompts with high correctness requires many test cases\nand iterations. As a result, manual inspection of test case results quickly becomes unfeasible.\nKnowing the types of errors that the LLM makes has the capacity to inform prompt engineers\nallowing optimal performance to be reached more quickly.\nIn order to assess the correctness of the LLM translation from natural language into logic\nprograms, weneedareliableground-truthlogicrepresentationforthenaturallanguageproblem.\nTo make this possible, we follow current approaches and work in a controlled setting. We chose\npopular “steamroller” problems, which are readily available in useful variants and can be auto-\ngenerated in any number [24]. We wrote a standard Definite Clause Grammar (DCG) parser for\nthe required subset of English and that outputs First Order Logic (FOL) formulas, our ground\ntruth formulas. This puts us in a position to compare the two formal logic representations; the\nfirst from the LLM and the second from the DCG. We do that in a purely semantic way using\nSEDAC (Semantic Error Detection And Correction); an algorithm that calls an Automated\nTheorem Prover (ATP) that is capable of deciding entailments in the considered fragment for\ntwo given formalizations.\nWe are interested in analyzing the correctness beyond a binary true/false status. In case\nof incorrectness, we make certain modifications to the given formulas and check again for\nentailment. Depending on the result, this allows us to conclude certain error classes and carry\nout automatic corrections.\nWe can illustrate our approach with a metaphor from text processing. Virtually every natural\nlanguage text editor includes a spell-checker for (a) fixing spelling mistakes and (b) grammatical\nerrors. More recently, (c) semantic analysis for, e.g., finding the right words for a given writing\nstyle have been added. Roughly speaking, our error categories correspond to these three levels.\nWe have syntax errors (a), shallow semantic errors (b), and deep semantic errors (c). Like in\ntext processing, they come at different levels of automatic detectability, fixability and the need\nto validate the proposed fix with the user or environment.\nAs far as we know, ours is the first approach of its kind. We describe it and report on practical\nexperiments. The approach and the result statistics are valuable for at least three reasons:\n(1) they provide insights into expected problem areas of ALMs that are generalizable, (2) they\ncan give a human in the loop insights to create targeted improvements to LLM prompts, and\n(3) they offer the ability to ‘auto-correct’ some types of semantic errors made by LLMs when\ncalling tools leading to improved performance.\n2\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nRelated work. The reasoning capability of LLMs is an active area of research, we refer to\nHuang et al. (2023) for a general overview of this field [9]. The majority of this work focuses\non enhancing LLM reasoning capabilities with fine-tuning and prompt engineering but without\nthe use of external reasoning tools. Key methods include Chain of Thought (CoT) reasoning\n[31], zero-shot CoT reasoning [12], Selection-Inference [4] and backward chain reasoning [11].\nAlthough there are many works which measure the performance of LLMs on logical reasoning\nbenchmarks [12, 16, 18], very little work has been done to classify the types of errors they make.\nXu et al. [33] focuses on the emergent reasoning capabilities of LLMs (a fully sub-symbolic\napproach) and proposes two classes; evidence selection errors and reasoning process errors.\nThese categories are not appropriate for neuro-symbolic approaches such as ALMs which allow\nmodels to make use of external tools for logical reasoning [16]. In these approaches, reasoning\nprocess errors are not relevant, instead the LLM is required to select the correct evidence and\nsuccessfully translate it into instructions to be parsed by an external tool. Therefore for this\ndomain we propose different error classes: syntactic errors and semantic errors, see Section 2.1.\n2 Our Method\nNatural Language Processing is a fast moving area with multiple new LLMs being released\neach year. This work focuses on only three of the best performing models at the time of the\nexperiment; GPT3.5 [3], GPT4 [17] and Gemini-Pro [28]. This study investigates the logical\nreasoning skills of these models and how they could be augmented through the use of automated\nreasoning systems. Figure 1 provides an overview of the general architecture that we explore\nin our experiments.\nTo test these models we chose PRONTOQA [24], a logical reasoning dataset, because it has\ndifferent settings (‘ontologies’, ‘hops’ and ‘distractors’) which can be changed to adjust the\ndifficulty of the problems. PRONTOQA provides the Natural Language Problem Script for our\nspecific experiments. The code for PRONTOQA questions is published but not the questions\nthemselves, which helps prevent contamination of LLMs (reduces the likelihood that they will\nhave seen the exact questions and answers in their training data). We generated one hundred\nexamples of the most difficult problems (‘false ontology’ with ‘relevant distractors’) for one hop,\ntwo hops and three hops as our evaluation benchmark.\nWe implemented several experimental conditions for each LLM. In the baseline condition the\nmodel was given a question from the benchmark and needed to produce a ‘True’ or ‘False’\nanswer based on the text provided. This corresponds to the arrow pointing from the Large\nLanguage Model to the Model Answer in Figure 1. For the zero-shot condition, we provide the\nLLM with instructions explaining how to write a Logic Program (LP) in Prolog syntax and ask\nit to convert a natural language problem into such a logic program. The LPis the instructions\nshown in Figure 1.\nWe chose logic programs as the interchange language because their syntax is already known by\nthe LLMs, they are easy to “teach” to a LLM in a prompt and their simple if-then structure is\nsufficient for our purpose. For computing a ‘True’ or ‘False’ result we used our Fusemate LP\nsystem [1].1\nFor our specific case, Fusemate is the Tool illustrated in Figure 1 that produces the Model\n1The PRONTOQA problems are designed in such a way that both an open world or closed world semantics\nbased reasoner can be used with the same result.\n3\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nFigure 1: A diagram of the general structure of Large Language Model tool use. In order to\nsuccessfully use a tool an LLM must successfully generate instructions for that tool that are\nfree of both syntax errors and semantic errors. Our contributions to improving this process\nincluding auto-correcting and error type classification are shown blue.\nNatural Language\nProblem Script\nLarge Language Model Instructions\nToolModel AnswerCorrect Answer\nLLM reads and\ninterprets the\nproblem script\nInstructions are\nparsed to the tool\nTool carries out task,\nthe output is answer.\nLLM translates\nproblem into\ninstructions\nfor the tool\nFull SEDAC uses DCG to\nautocorrect se-\nmantic errors\nATP determines error types.\nScript cleans syntactic errors.\nPartial SEDAC auto-corrects\nsemantic errors.\nAccuracy is\ndetermined.\nAnswer. The arrow pointing from Large Language Model to the Instructions is the pipeline\nthat we are evaluating.\nFor the one-shot condition, we provide the LLM with instructions for how to write a logic\nprogram, an example natural language problem, the corresponding logic program and a new\nnatural language problem. Once again the resulting logic program is sent to Fusemate to\ncompute a ‘True’ or ‘False’ answer. We repeated this process for each problem in the benchmark\nand for each of the three LLMs.\nWe generated error logs from each trial which contain each problem, the corresponding model\nanswer, the correct answer and the logic programs generated by the models for the zero-shot and\none-shot conditions. These experiments and their result statistics revealed several weaknesses\nwith these approaches in terms of the error framework introduced above.\n2.1 Error Categorisation\nFew systems for error categorisation currently exist in the literature [33] and these are not\nappropriate for categorising errors when Augmented Language Models (ALMs) call upon tools.\nTherefore we propose a new error categorisation which has two broad classes; syntactic errors\nand semantic errors.\nA Syntactic Error is defined as an error in the LLM’s instructions which prevents the tool from\nparsing. There are a number of different sub-categories of syntactic error which can contribute\nto this including:\n• Symbol Errors - The LLM instructions contain incorrect symbols. As an example\nconsider a logic program which contained “-?” instead of “?-” for a query. This would\n4\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nprevent the script from running and so the instructions can no longer be parsed.\n• Natural Language Errors - The model includes natural language in addition to or\ninstead of machine interpretable instructions.\n• Communication Errors- A specific form of Natural Language Error where the model\nincorporates markers like “‘”’ or «» to separate natural language from tool instructions. A\nhuman can very easily interpret which parts are meant to be included in the instructions.\nThis type of error can be cleaned very easily.\n• Knowledge Errors- This is a form of evidence selection error [33]. Rather than trans-\nlating the problem directly, the LLM tries to incorporate some of its own pre-existing\nknowledge into the instructions. An example is when the model replaces ‘even number’\nwith ‘integer divisible by 2’.\n• Other Syntax Errors- Any other syntactic error that prevents the model from parsing\nwhich does not fall into the categories above. This is depended on the tool being used.\nA Semantic Error is an error in which the instructions are able to be parsed by the tool but\ngive an incorrect output. The exact types of semantic errors are tool dependent. However we\nrecommend breaking these into two sub-categories that will likely be helpful to developers:\n• Shallow Semantic Errors- Errors where the semantic meaning can confidently be re-\ncovered (auto-corrected) without viewing the original natural language script. We suggest\nthat these could be referred to as auto-correctable errors.\n• Deep Semantic Errors- Errors where the semantic meaning cannot be recovered with-\nout viewing the original natural language script. We suggest that these could be referred\nto as non-auto-correctable errors.\nEstablishing a system of well defined error categories provides a common language and allows\nfocus on specific common errors for the NLP community to address. This error classification\nis also important for developers to identify the best technique to improve the performance of\nLLMs. Forexample, ifadeveloperdiscoversalargenumberofsyntacticerrorsthentheyknowto\nfocus on techniques that can reduce these: one or few-shot prompts, fine-tuning the model with\na focus on the tool’s grammar or writing a script that will correct syntax on LLM instructions.\nWhen there are many semantic errors then the developer may focus on fine-tuning the model\nwith a focus on the meaning of the natural language or choose to flag common semantic errors\nin the prompt.\n2.2 Semantic Error Detection and Correction\nIn the following we describe our method for analyzing and auto-correcting errors according to\nour error framework. We start with a brief overview of the main ideas and its core algorithm,\nSEDAC (Semantic Error Detection And Correction) shown by the blue box in Figure 1.\nSEDAC takes as input a natural language scriptnl and the string representation of a logic\nprogram lp. The nl is the original problem statement and, in this sense, holds the “ground\ntruth”. The lp is meant as a faithful representation ofnl as obtained by a given LLM. The\npurpose ofSEDACis to assess the correctness of thelp wrt. thenl in terms of the error categories\ndefined above. It also carries out fixes for problems spotted along the way.\nSEDACfirst tries to automatically fix syntactic errors. Correct or fixed statements then proceed\n5\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nto the semantic error detection phase; statements with un-fixable syntax errors are ignored. We\ndistinguish betweenpartial and full error detection (and correction). These are (potentially in-\ncomplete) operational realizations of the shallow and deep error categories introduced above,\nrespectively. Partial error detection is concerned with unsuitable formal representation of ad-\njectives or nouns that can be discovered confidently on linguistic grounds. Sophisticated tools\nlike spaCy2 can help with this process. Full error detection is concerned with discovering\nmore speculative logical errors such as wrong introduction or removal of negation, and reversed\nimplication. Correspondingly, discovered shallow errors are always corrected without further\nvalidation, discovered deep errors require correctness validation wrt. the givennl possibly in\nconjunction with an external trusted source for domain knowledge.\nTechnically,SEDAC takes the facts and rulesp of lp and checks them one by one with a logic\nrepresentation of nl and computes a statusOK, NonFixableError or FixableError. The status\nof p is obtained by a soundness check: if nl entails p in first-order logic then p’s status is\nOK, otherwise apropose function is called that returns candidate fixes forp which are again\nchecked for soundness. Among all proposed sound fixes, if any, some “best” fix is noted withp\nas aFixableError. A best fix is one that maximizes the number ofnl statements entailed by a\ntentatively fixedlp.3 If no sound fix is produced thenp’s status isNonFixableError. Figure 2\nshows the “full” version ofSEDAC. There is also a “partial” version described below.\nFigure 2: TheSEDAC algorithm in pseudocode.\nAlgorithm 1 Semantic Error Detection and Correction,Full-SEDAC(nl, lp)\nInput: A PRONTOQA problemnl and its translation into a logic programlp by an LLM.\nOutput: A status report for every fact and rule oflp.\n1 nl_ax = {nl_to_fof(s) | s ∈ nl and s is not a query} Natural language as FOL\n2 lp_ax = {lp_to_fof(r) | r ∈ lp and r is not a query} Logic program as FOL\n3 lp_ax_status = {} Result status maps forlp\n4 for f ∈ lp_ax Soundness: check if LP entailed by NL\n5 if nl_ax |= f Check next fact or rulef\n6 lp_ax_status[f] =OK Record OK status off\n7 else Find best modification off, if any\n8 cand_fixes = {f′ ∈ propose(f) | nl_ax |= f′} Get modifications and keep sound ones\n9 if cand_fixes ==∅ No such modifications exist\n10 lp_ax_status[f] =NonFixableSemanticError\n11 else\n12 f_best = arg maxf′∈cand_fixes score(f′) f_best maximizes entailment ofnl_ax\n13 where score(f′) =|{g ∈ nl_ax | (lp_ax \\ {f}) ∪ {f′} |= g}|\n14 lp_ax_status[f] =FixableSemanticError(f_best)\n15 return lp_ax_status\n2https://spacy.io\n3It is tempting to instead require “completeness”, i.e., the converse of the sondness entailment. This criterion\nwould be too strong in practice in many cases, as thelp might lack some formulas but still entail the query.\n6\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nThe nl_to_fof function. SEDAC calls nl_to_fof(s) for translating a natural language sen-\ntence s into first-order logic. We implementednl_to_fof as a definite clause grammar (DCG)\nin (SWI-)Prolog. The grammar was reverse-engineered from PRONTOQA examples4; the syn-\ntactic elements nouns, verbs and adjectives we retrieved from the PRONTOQA source code.\nThe grammar recognizes quantifiers (determiners) like \"each\", \"any\", \"every\", \"a\" and tolerates\nsingular/plural formulations. As a side effect, the natural language parser emits first-order logic\nformulas. The parser adjusts nouns in plural form to singular form, as unary predicates. For\nexample, either sentence “Cats swim.” and “Every cat swims” become∀x cat(x) → swim(x).\nAdjectives are normalized into nouns, e.g.,even_number(x) instead of even(x). This way the\ngrammar defines a canonical logical form for PRONTOQA sentences. This form is taught to\nand expected from the LLM translation as well.\nThe lp_to_fof function. SEDAC calls lp_to_fof(r) for translating a string representation\nlp of a logic program suggested by the LLM into FOL. If the program contains symbol errors,\nnatural language errors or communication errors, an automated fix is attempted by a python\nscript. The resulting statements are parsed and translated into FOL one by one. Parsing may\nfail as not all syntax errors will always be caught. If it succeeds, translation into FOL is merely\nsyntax rewriting; if it fails then statement is ignored (taken as ‘true’).\nThe propose function. The propose function takes a FOL formulaf and returns a possibly\nempty set ofproposal formulas. The algorithm is presented as a set of rewrite rules “⇔” and\nderivation rules “⇒” in Figure 3.\nStarting from a singleton set comprised of a given formula, the rules are applied exhaustively, in\nany order, preferring rewrite rules over derivation rules. Rewrite rules replace the premise taken\nfrom the current set with its conclusion; derivation rules add to the current set. The result is\nthe saturated set withoutf. It is not difficult to see that this procedure always terminates.\nRewrite rules are meant for shallow error correction. They revolve around normalization of plu-\nral into singular forms, adjectives into nouns, and proper nouns from type positions (predicates)\nto individuum positions (terms). The derivation rules for deep error correction are more of a\nspeculative kind. We use them for replacing the direction of an implication and complementing\nliterals. These are general rules, not specific to PRONTOQA, but informed by the kinds of\nerrors we observed LLMs make.\nReasoning Complexity and PartialSEDAC. In our highly controlled and closed PRON-\nTOQA environment with its simple formula structure, full error detection poses no problem.\nThe FOL fragment is Bernays-Schönfinkel logic which is decided by our ATP Beagle [2]. Each\nentailment proof obligation was decided in very short time(< 1sec). The setsnl_ax and lp_ax\nhave at most 20 formulas each for a given problem. In the worst case, four candidate fixes are\nproposed per rule or fact, yielding a maximum of 20 + 4*20 = 100 ATP calls. We investigated\n440 problems with Full-SDEDAC which took 12h. This time could be shortened considerably\nby avoiding file-based ATP interface and with a faster ATP.\nMore realistic settings have open-world character where the problem statement does not con-\ntain full domain information and “ground truth oracles” may not be available. This let us chose\n4We found this easier than trying to modify the PRONTOQA code for emitting first-order logic formulas.\nIt also more useful in view of re-usability to domains that are not synthetically made.\n7\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nFigure 3: The rule system ofpropose for fixing shallow semantic errors (above the double lines)\nand deep semantic errors (below the double lines).\nPremise Kind Conclusion Condition Example\n∀x p(x) → f ⇔ ∀x (x = p) → f p is proper noun ∀x tom(x) → swims(x) ⇔\n∀x (x = tom) → swims(x)\n[¬]p(ns) ⇔ ∀x n(x) → [¬]p(x) ns is the plural\nform of a nounn\n¬swims(cats) ⇔\n∀x cat(x) → ¬swims(x)\n[¬]p(n) ⇔ ∀x n(x) → [¬]p(x) n is a singular\nnoun\n¬swims(cat) ⇔\n∀x cat(x) → ¬swims(x)\n[¬]p(a) ⇔ ∀x n(x) → [¬]p(x) a is an adjective\nform of a nounn\nfloral(even) ⇔\n∀x even_number(x) → floral(x)\n∀x ns(x) → f ⇔ ∀x n(x) → f ns is the plural of\nnoun n ̸= ns\n∀x cats(x) → swims(x) ⇔\n∀x cat(x) → swims(x)\n∀x f→ [¬]ns(x) ⇔ ∀x f→ [¬]n(x) same ∀x cat(x) → swims(x) ⇔\n∀x cat(x) → swim(x)\n[¬]a(t) ⇔ [¬]n(t) a is an adjective\nform of a nounn\neven(tom) ⇔\neven_number(tom)\n∀x a(x) → f ⇔ ∀x n(x) → f same ∀x even(x) → swim(x) ⇔\n∀x even_number(x) → swim(x)\n∀x f→ [¬]a(x) ⇔ ∀x f→ [¬]n(x) same ∀x floral(x) → even(x) ⇔\n∀x floral(x) → even_number(x)\n∀x f→ p(t) ⇒ ∀x f→ ¬p(t) none ∀x cat(x) → swim(x) ⇒\n∀x cat(x) → ¬swim(x)\n∀x f→ ¬p(t) ⇒ ∀x f→ p(t) none ∀x cat(x) → ¬swim(x) ⇒\n∀x cat(x) → swim(x)\n∀x f→ g ⇒ ∀x g→ f none ∀x cat(x) → swim(x) ⇒\n∀x swim(x) → cat(x)\nfirst-order logic semantics for the soundness tests; a closed world semantics seems too credu-\nlous for entailments (let alone having a highly undecidable entailment problem). As a trivial\nexample, a formula with a syntactic error is always dropped and, this way, could support an\nunintended entailment with a default negation inference. While the “tool” could, say, employ\nlogic programming for query answering, deep error fixes should be proposed cautiously and only\nif deductively valid.\nThese considerations motivated us to evaluate two versions ofSEDAC: the full version defined\nabove, and apartial version for shallow error correction. More precisely, partial-SEDAC differs\nfrom Full-SEDAC in that it receives thelp only (no nl) and then immediately callspropose\nrestricted to rewriting-rule error correction only. The result of the partial-SEDAC call is the\nresult of thepropose call if not empty (i.e., propose was effective), otherwise it is the givenlp.\n(We do not provide pseudo-code here.) These two version allowed us to assess the tradeoffs in\neffectiveness and expressivity. We report on the results in Section 3 below.\nExample. We demonstrate Partial-SEDAC and Full-SEDAC with a small example that we\ncompiled from actual PRONTOQA problems and LLM translations. The example consists of\nthe setsnl and lp shown on the left of the following table, which are converted tonl_ax and\nlp_ax shown on the right, respectively, in the first steps of (Full-)SEDAC. Here and below, FOL\n8\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nformulas are written in TPTP FOF syntax [26].\n_ax\nnl 1 Each integer is not fruity.\n2 Negative numbers are brown.\n3 Wren is an integer.\n4 True or false: Wren is not fruity.\n1 ! [A] : (integer(A) => ~ fruity(A))\n2 ! [A] : (negative_number(A) => brown(A))\n3 integer(wren)\n4 % Query ~ fruity(wren) ignored\nlp 1 even(X) :- integer(X), 0 is X mod 2.\n2 integer(X) :- fruity(X).\n3 integer(wren).\n4 integer(X).\n5 brown(negative).\n6 ?- \\+ fruity(wren).\n1 % Syntax error line ignored\n2 ! [X] : (fruity(X) => integer(X)))\n3 integer(wren)\n4 ! [X] : integer(X)\n5 brown(negative)\n6 % Query ~ fruity(wren) ignored\nOur parser for the FOL versions ofnl connects adjectives/noun pairs into single-name predi-\ncates, e.g., asinnegative_number(X). Shallowerrorcorrectionisdesignedtoalignlogicprograms\nwith this convention. Notice the attempt to bring in “background knowledge”0 is X mod 2 by\ntheLLMonline1of lp_ax withoutinstructingtodoingso; weclassifythisintothesub-category\nof Knowledge Error.\nThe FOL resulting from theSEDAC runs are as follows:\nPartial-SEDAC(lp) Full-SEDAC(nl, lp)\n1 % Syntax error line ignored\n2 ! [X] : (fruity(X) => integer(X))\n3 integer(wren)\n4 ! [X] : integer(X))\n5 ! [I] : (negative_number(I) => brown(I))\n6 % Query ~ fruity(wren) ignored\n1 % Syntax error line ignored\n2 ! [X] : (fruity(X) => ~ integer(X))\n3 integer(wren)\n4 % ! [X] : integer(X) is NonFixableError\n5 ! [I] : (negative_number(I) => brown(I))\n6 % Query ~ fruity(wren) ignored\nIt is instructive to compare the results of partial and fullSEDAC. Partial-SEDAC(lp) differs\nfrom lp_ax only on line 5 by noun and adjective corrections.Full-SEDAC(nl, lp) includes this\nfix as well. In addition, it fixes the formula f = ! [X] : fruity(X) => integer(X) on line\n2 of pl_ax by negating its conclusion. This happens in three steps. First, the entailment\ncheck on line 5 inFull-SEDAC finds nl_ax ̸|= f. Then, propose(f) returns four variants off\nbut only f′ = ! [X] : fruity(X) => ~ integer(X) satisfies nl_ax |= f′. Scoring is irrelevant\nin this case. The status for f, hence, is FixableError. As a further difference, the formula\nf = ! [X] : integer(X) on line 4 oflp_ax has statusNonFixableError as nl_ax ̸|= f and no fix\nis proposed.\nNow consider the query True or false: Wren is not fruity. The correct answer is True as\nnl_ax |= q where q = ~ fruity(wren). The LLM translation cannot show that (lp_ax ̸|= q),\nneither can the partial fix (Partial-SEDAC(lp) ̸|= q) but the full fix can (Full-SEDAC(nl, lp) |= q).\n3 Results\nTable 1 shows the overall accuracy of all three models with each experimental condition de-\nscribed in Section 2. The results show that the use of the LP system, Fusemate, increased the\naccuracy of each LLM by between 10% and 25% of the possible total.\nThe SEDAC auto-correction successfully reduced errors in all cases. The syntactic fix alone\nreduced the number of errors of each model by15 − 30%. The partial and full semantic fixes\n9\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nTable 1: Accuracy for each technique for each model type. Random guessing would be expected\nto achieve an accuracy of0.5 ± 0.05. The error values are half the range across three trials.\nPrompt Strategy GPT3 GPT4 Gemini-Pro\nNormal 0.48 ± 0.06 0.83 ± 0.12 0.47 ± 0.04\nChain of Thought + one-shot 0.65 ± 0.15 0.94 ± 0.04 0.74 ± 0.12\nFusemate 0.66 ± 0.05 0.94 ± 0.015 0.57 ± 0.03\nFusemate + one-Shot 0.76 ± 0.06 0.94 ± 0.015 0.67 ± 0.03\nFusemate + one-shot + syntax\nfix 0.83 ± 0.06 0.95 ± 0.02 0.74 ± 0.02\nFusemate + one-shot + partial\nfix 0.87 ± 0.05 0.983 ± 0.005 0.77 ± 0.04\nFusemate + one-shot + full fix 0.98 ± 0.01 0.995 ± 0.005 0.96 ± 0.04\nreduced the number of model errors by45 − 72% and 88 − 92% respectively. In addition to\nerror correction, theSEDAC algorithm also classifies the types of errors.\nTable 2: Error breakdown for each model type. For each entry, the left value is the average\nnumber of each type of error from 100 problems. The values given on the right are half of the\nrange across the three trials for each experimental condition.\nTechniques Commun-\nication\nErrors\nSymbol\nErrors\nKnowl-\nedge\nErrors\nNatural\nLan-\nguage\nErrors\nOther\nSyntax\nErrors\nShallow\nSemantic\nErrors\nDeep\nSemantic\nErrors\nTotal\nIn-\nstances\nwith\nErrors\nGPT3 0.0 ±0.0 1.3 ±0.5 2.0 ±1.5 3.7 ±1.5 0.3 ±0.5 12.0 ±1.5 32.3 ±3.0 34.3 ±5.5\n1ShotGPT3 0.0±0.0 3.3 ±1.5 0.3 ±0.5 1.7 ±0.5 0.0 ±0.0 10.0 ±2.5 19.3 ±4.5 24.3 ±4.0\nGPT4 0.7 ±0.5 1.0 ±1.0 0.0 ±0.0 0.0 ±0.0 0.0 ±0.0 4.7 ±1 1 ±1 6.0 ±1.5\n1ShotGPT4 0.0±0.0 0.0 ±0.0 1.0 ±1.0 0.0 ±0.0 0.0 ±0.0 3.7 ±1.0 1.67 ±0.5 6.0 ±1.5\nGemini 5.7 ±1.5 4.0 ±1.5 4.3 ±0.5 3.7 ±2.0 1.0 ±1.0 18.7 ±0.5 36.7 ±2.5 43.3 ±3.0\n1ShotGemini 0.7±1.0 2.7 ±3.0 5.3 ±2.5 1.0 ±1.0 0.0 ±0.0 16.7 ±2 27.7 ±0.5 32.7 ±3.0\nFor each of the Fusemate methods, the types of errors were determined as described in the\nSections 2.1 and 2.2. Table 2 shows the average frequency of each error type acrossn = 100test\nexamples. For each of the three models the most common error type is the Shallow Semantic\nErrors. Communication Errors, Symbol Errors, Natural Language Errors and Other Syntax\nErrors were decreased by introducing the example prompt. The one-shot case did not reduce\nthe number of semantic errors for the GPT4 model, however it did reduce semantic errors by\napproximately 30% for GPT3 and Gemini.\nFigure 4 shows the percentage of error cases which contained each error type for each model\nand technique. This graph shows that the most common errors for GPT3 and Gemini were\nDeep Semantic Errors, which occur in75% to 100% of cases. For GPT4 the most common error\nwas Shallow Semantic errors which occurred in approximately60% − 80% of cases. Note that\nas the graphed results are normalised, they do not allow for direct comparison of the models’\nability to translate the semantic meaning from natural language to logic programs.\nAppendix C contains a correlation matrix for each of the different error types. The matrix\n10\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nFigure 4: A graph of the percentage of error cases that contained each error type for each\nmodel. Note that this is an indication of the relative frequency of each error type for a given\nmodel and experimental condition. Error bars show the minimum and maximum values across\nthe three trials.\nGPT3 GPT3 1-Shot GPT4 GPT4 1-Shot Gemini Gemini 1-Shot\n0\n20\n40\n60\n80\n100\nPercentage of Error Cases Containing each Type of Error for each Model and Technique\nComm. Symbol Knowledge Natural Language Shallow Semantic Deep Semantic\nshows that most correlations are very weak (magnitude< 0.11) with only three exceptions.\nKnowledge Errors show a correlation of0.23 with Shallow Semantic Errors, Symbol errors have\na 0.31 correlation with Natural Language Errors and Shallow Semantic Errors anti-correlate\n(−0.37) with Deep Semantic Errors.\nFinally, we investigated the effectiveness of our error correction mechanisms. These are ‘syn-\ntactic fixes only’,Partial-SEDAC and Full-SEDAC. As said earlier, the PRONTOQA problems\nare agnostic of the reasoning type; with an error-free translation the LP (lp in Figure 2) is\nalways sufficiently complete in the sense that default reasoning (specifically, default negation)\ndoes not enable more conclusions than after reformulation wrt. classical first-order logic. This\nis no longer true if the transformation is not correct and the error correction is imperfect. In\nparticular, the corrected lp may miss relevant rules, which not only removes positive literal\nconclusions but also adds negative literal conclusions.\nFor this reason we re-evaluated question answering for different correction scenarios and both\nopen-world and closed-world reasoning. For that, we considered the problems with wrong\nanswers (n = 440). The results are summarized in Figure 5 which expands on the summarised\nresults in Table 1. This table shows that the precision values are systematically higher for the\nopen world semantics compared to the closed world semantics.\n4 Discussion\nThe results clearly show that during the time period of the experiments (December 2023),\nthe accuracy of GPT4 on all experimental conditions was significantly higher than GPT3 and\nGemini-Pro which were comparable in their performance. Using an AR tool improved the\nperformance comparable with Chain of Thought techniques and our method has the added\nbonus of trustworthy explainability; AR tools can produce a proof for any answer they produce.\n11\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nFigure 5: Re-running divergent problems after syntax only, Partial-SEDAC and Full-SEDAC\ncorrections wrt. open-world (classical first-order logic) and closed-world (LP) semantics.\nOpen-world (FOL) Closed-world (LP)\nRecall Precision Accuracy Recall Precision Accuracy\nSyntax errors fixed 0.22 0.57 0.53 0.18 0.16 0.17\nPartial-SEDAC 0.38 0.72 0.63 0.35 0.32 0.34\nFull-SEDAC 0.80 0.98 0.89 0.85 0.81 0.83\nFor all models, semantic errors were more common than syntax errors. Semantic errors occurred\nin more than80% of error cases. Therefore theSEDACalgorithm showed greater error reduction\nfor semantic errors than syntactic errors. Note that although theFull-SEDACreduced the total\nnumber of errors by90%, most real world scenarios would not have a full semantic fix available.\nEven in these cases theSEDAC algorithm is useful as it allows for classification of errors to\nrapidly improve prompting.\nAs expected, one-shot examples reduced the number of communication errors. Intuitively,\nproviding an example allows the model to better know the required output format. We also\nexpected one-shot to reduce the number of Symbol Errors, this was the case for GPT4, however\nit made little difference to Gemini and including examples unexpectedly increased the number\nof Symbol Errors for GPT3.\nSyntax errors occur in a relatively small number of cases compared to semantic errors. This\nindicates that the capability of state of the art LLMs to produce correct syntax exceeds their\nability to express the correct semantics to a tool. This demonstrates the importance of AR\ntools to enhance the models’ reasoning capabilities. We speculate that the ‘reasoning capacity’\nof an LLM may be effectively measured by the Chain of Thought accuracy as the corresponding\nerror rate is similar to the total semantic error rate.\nThere is currently no prevalent system of classifying types errors in LLM use of tools. There is\nhowever one more general error structure which exists in the literature which has some relevance\n[33]. See Appendix A for a comparison between this and our error classification.\nThe results in Figure 5 confirm our expectations that error correction increases recall consis-\ntently for open-world and closed-world semantics. Roughly speaking, recall depends mostly on\ndeductive reasoning, which is not as affected by the change of semantics as precision. A high\nprecision value requires a low false positive rate. In our scenario, false positive are often conclu-\nsions in the form of negative literals (“True or false: Tom is a not cat”) that become provable by\ndefault reasoning when relevant rules are removed by errors. This leads to significantly lower\nprecision than with the open-world semantics. Note that in practice the choice of semantics is\nmostly likely to be determined by the application domain.\nThe well defined structure of the natural language in PRONTOQA allows a DCG to achieve\n100% performance. However DCGs are not robust even to small deviations from the assumed\nstructure. Testing LLMs on the PRONTOQA dataset allowed for automated measurement of\nthe frequency and type of LLM errors. We hypothesise that LLMs will be significantly more\nrobust to small changes in wording than DCGs and one area for future work is to test LLM\nreasoning on unstructured natural language.\nLocal LLMs were not used in this study, instead we utilised APIs for pre-trained remote mod-\nels. Measuring the computational cost is therefore challenging as the structure and number\n12\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nof parameters in each model is not known. Run-time does not provide a reliable measure of\ncomputational cost as data transfer and network latency make a varying and significant contri-\nbutions. A typical response time was 0.5-5 seconds and most responses contained on the order\nof 100 tokens. One area for future work is to perform similar experiments using local models\nto accurately determine the computational cost.\n5 Conclusions\nIn this study we have investigated the intersection of Automated Reasoning and Large Language\nModels in three different ways. Firstly we have explored the capability of LLMs as stand\nalone reasoning engines. Secondly we have tried coupling LLMs with external Automated\nReasoning systems. Thirdly we have implemented automated reasoning technology to debug\nLLM reasoning.\nWe have demonstrated that augmenting an LLM with an AR system improves its reasoning by a\nsimilar level to Chain of Thought prompting but with the added bonus of reliable explainability.\nFurthermore we have introduced the SEDAC algorithm which can act as an auto-correct to\nreduce LLM errors by at least15% and up to90% for problems where a DCG is able to parse\nthe ground truth.\nAn error classification system was introduced for evaluating interactions between ALMs and\ntheir tools. It provides a systematic way to determine the types of errors that LLMs make\nwhen interacting with tools. Diagnosing error types provides insight and guidance into which\nstrategies should be implemented to improve model performance. This classification is broad\nenough that it can be generalised for any external tool while still providing specific information\nto improve ALM prompts. As the popularity of ALMs rises focus on types of errors gives\ndevelopers of LLMs a clear direction for improvement.\nOne key finding from the paper is that semantic errors are far more common than syntactic\nerrors when LLMs call external tools. This is significant for developers who are interested in\ndeploying LLMs for real-world applications. When prompting their models to use external\ntools, focus should be placed on enhancing model reasoning and semantics not just syntax.\nThis study considers only a restricted domain of steamroller problems which have highly pre-\ndictable structures. An area for future research is to apply and evaluate these techniques to a\nbroader class of problems or real-world application and to determine their computational cost.\nReferences\n[1] Peter Baumgartner and Elena Tartaglia. Bottom-Up Stratified Probabilistic Logic Pro-\ngramming with Fusemate. Electronic Proceedings in Theoretical Computer Science, 385:\n87–100, September 2023. ISSN 2075-2180. doi: 10.4204/EPTCS.385.11. URL http:\n//arxiv.org/abs/2308.15862v1.\n[2] Peter Baumgartner, Joshua Bax, and Uwe Waldmann. Beagle – A Hierarchic Superposition\nTheorem Prover. In Amy P. Felty and Aart Middeldorp, editors,Automated Deduction\n- CADE-25, Lecture Notes in Computer Science, pages 367–377, Cham, 2015. Springer\nInternational Publishing. ISBN 978-3-319-21401-6. doi: 10.1007/978-3-319-21401-6_25.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\n13\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-\nShot Learners. InAdvances in Neural Information Processing Systems, volume 33, pages\n1877–1901. Curran Associates, Inc., 2020. URLhttps://papers.nips.cc/paper_files/\npaper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n[4] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-Inference: Exploiting\nLarge Language Models for Interpretable Logical Reasoning. InConference on Learning\nRepresentations. ICLR2023, September 2022. URLhttps://openreview.net/forum?id=\n3Pf3Wg6o-A4.\n[5] Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel E. Ho. Hal-\nlucinating Law: Legal Mistakes with Large Language Models are\nPervasive, January 2024. URL https://hai.stanford.edu/news/\nhallucinating-law-legal-mistakes-large-language-models-are-pervasive .\n[6] Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel E. Ho. Large Legal Fictions:\nProfiling Legal Hallucinations in Large Language Models, January 2024. URL http:\n//arxiv.org/abs/2401.01301. arXiv:2401.01301 [cs].\n[7] Lennert De Smet, Pedro Zuidberg Dos Martires, Robin Manhaeve, Giuseppe Marra, An-\ngelika Kimmig, and Luc De Raedt. Neural Probabilistic Logic Programming in Discrete-\nContinuous Domains, March 2023. URLhttp://arxiv.org/abs/2303.04660.\n[8] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie\nCallan, and Graham Neubig. PAL: Program-aided Language Models, 2022.\n[9] Jie Huang and Kevin Chen-Chuan Chang. Towards Reasoning in Large Language Models:\nA Survey. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,Findings\nof the Association for Computational Linguistics: ACL 2023, pages 1049–1065, Toronto,\nCanada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nfindings-acl.67. URL https://aclanthology.org/2023.findings-acl.67.\n[10] Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schuetze, and\nPeter Clark. Language Models with Rationality, October 2023. URLhttp://arxiv.org/\nabs/2305.14250. arXiv:2305.14250 [cs].\n[11] Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. LAM-\nBADA: Backward Chaining for Automated Reasoning in Natural Language. In Anna\nRogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n6547–6568, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.acl-long.361. URL https://aclanthology.org/2023.acl-long.361.\n[12] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge Language Models are Zero-Shot Reasoners. In36th Conference on Neural Informa-\ntion Processing Systems. NeurIPS, 2022.\n[13] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui,\nDenny Zhou, and Andrew M Dai. MIND’S EYE: GROUNDED LANGUAGE MODEL\n14\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nREA- SONING THROUGH SIMULATION. InThe Eleventh International Conference on\nLearning Representations. ICLR, 2023.\n[14] Robin Manhaeve, Sebastijan Dumančić, Angelika Kimmig, Thomas Demeester, and Luc\nDe Raedt. DeepProbLog: Neural Probabilistic Logic Programming, December 2018. URL\nhttp://arxiv.org/abs/1805.10872.\n[15] Robin Manhaeve, Sebastijan Dumančić, Angelika Kimmig, Thomas Demeester, and Luc\nDe Raedt. Neural probabilistic logic programming in DeepProbLog.Artificial Intelligence,\n298:103504, September 2021. ISSN 00043702. doi: 10.1016/j.artint.2021.103504. URL\nhttps://linkinghub.elsevier.com/retrieve/pii/S0004370221000552.\n[16] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru,\nRoberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz,\nEdouard Grave, Yann LeCun, and Thomas Scialom. Augmented Language Models: a\nSurvey. Transactions on Machine Learning Research, 2023.\n[17] OpenAI. GPT-4 Technical Report. Technical report, 2023. URL https://api.\nsemanticscholar.org/CorpusID:257532815.\n[18] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander Miller. Language Models as Knowledge Bases? In Kentaro Inui,\nJing Jiang, Vincent Ng, and Xiaojun Wan, editors,Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/\nv1/D19-1250. URL https://aclanthology.org/D19-1250.\n[19] Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, and Noah D. Goodman. Certified De-\nductive Reasoning with Language Models, November 2023. URLhttp://arxiv.org/abs/\n2306.04031. arXiv:2306.04031 [cs].\n[20] Stanislas Polu and Ilya Sutskever. Generative Language Modeling for Automated Theorem\nProving, September 2020. URL http://arxiv.org/abs/2009.03393. arXiv:2009.03393\n[cs, stat].\n[21] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis.\nMeasuring and Narrowing the Compositionality Gap in Language Models. In Houda\nBouamor, Juan Pino, and Kalika Bali, editors,Findings of the Association for Compu-\ntational Linguistics: EMNLP 2023, pages 5687–5711, Singapore, December 2023. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.378. URL\nhttps://aclanthology.org/2023.findings-emnlp.378.\n[22] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi\nTan, FeiHuang, andHuajunChen. ReasoningwithLanguageModelPrompting: ASurvey.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 5368–5393, Toronto, Canada, July 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.acl-long.294. URLhttps://aclanthology.org/2023.\nacl-long.294.\n[23] Abhiramon Rajasekharan, Yankai Zeng, Parth Padalkar, and Gopal Gupta. Reliable Natu-\nral Language Understanding with Large Language Models and Answer Set Programming.\n15\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nElectronic Proceedings in Theoretical Computer Science, 385:274–287, September 2023.\nISSN 2075-2180. doi: 10.4204/EPTCS.385.27. URLhttp://arxiv.org/abs/2302.03780.\narXiv:2302.03780 [cs].\n[24] Abulhair Saparov and He He. Language Models Are Greedy Reasoners: A Systematic For-\nmal Analysis of Chain-of-Thought. InThe Eleventh International Conference on Learning\nRepresentations, March 2023. URLhttps://openreview.net/forum?id=qFVVBzXxR2V.\n[25] Aarohi Srivastava and et al. Beyond the Imitation Game: Quantifying and extrapolating\nthe capabilities of language models. Transactions on Machine Learning Research, 2023.\nISSN 2835-8856. URLhttps://openreview.net/forum?id=uyTL5Bvosj.\n[26] Geoff Sutcliffe. The logic languages of the TPTP world. Logic Journal of the IGPL,\n31(6):1153–1169, November 2023. ISSN 1367-0751. doi: 10.1093/jigpal/jzac068. URL\nhttps://doi.org/10.1093/jigpal/jzac068.\n[27] Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal.\nQUAREL: A Dataset and Models for Answering Questions about Qualitative Rela-\ntionships. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):7063–\n7071, July 2019. ISSN 2374-3468, 2159-5399. doi: 10.1609/aaai.v33i01.33017063. URL\nhttps://ojs.aaai.org/index.php/AAAI/article/view/4687.\n[28] Gemini Team. Gemini: A Family of Highly Capable Multimodal Models, 2023. URL\nhttp://arxiv.org/abs/2312.11805. arXiv:2312.11805 [cs].\n[29] Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad\ngeometry without human demonstrations.Nature, 625(7995):476–482, January 2024. ISSN\n0028-0836, 1476-4687. doi: 10.1038/s41586-023-06747-5. URLhttps://www.nature.com/\narticles/s41586-023-06747-5 .\n[30] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori\nHashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities\nof large language models.Transactions on Machine Learning Research, 2022. ISSN 2835-\n8856. URL https://openreview.net/forum?id=yzkSU5zdwD.\n[31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H\nChi, Quoc V Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in\nLarge Language Models. In36th Conference on Neural Information Processing Systems.\nNeurIPS, 2022.\n[32] Thomas Winters, Giuseppe Marra, Robin Manhaeve, and Luc De Raedt. DeepStochLog:\nNeural Stochastic Logic Programming.Proceedings of the AAAI Conference on Artificial\nIntelligence, 36(9):10090–10100, June 2022. ISSN 2374-3468, 2159-5399. doi: 10.1609/\naaai.v36i9.21248. URL https://ojs.aaai.org/index.php/AAAI/article/view/21248.\n[33] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. Are Large\nLanguage Models Really Good Logical Reasoners? A Comprehensive Evaluation and Be-\nyond, August 2023. URLhttp://arxiv.org/abs/2306.09841. arXiv:2306.09841 [cs].\n16\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nA Comparison with Existing Error Classification Systems\nXu et al. [33] have two major error categories for determining LLM reasoning capability; evi-\ndence selection errors and reasoning process errors. The evidence selection process category is\ndivided into two sub categories which are defined as [33]:\n• Wrong Selection- ‘LLMs select the wrong facts or ignore the necessary facts from the\nbeginning of the reasoning.’\n• Hallucination - ‘LLMs select the evidence which contradicts the given context or cannot\nbe verified from the context.’\nNote that these categories combined roughly correspond to Knowledge Errors and Deep Se-\nmantic Errors.\nFurthermore the reasoning process errors are divided into three sub-categories; no reasoning,\nperspective mistake and process mistake. In our context the model is not required to reason\nper se, instead it is required to translate natural language to a logic program. This best\napproximates the Shallow Semantic Errors as these clearly indicate a failure in logical reasoning.\nThe communication, symbol and natural language errors have no equivalent error in the system\nproposed by Xu et al. As the two systems of errors only have rough corresponding categories,\nany comparison of the frequency error categories should only be a rough approximation. This\nbreakdown would give the results displayed in Table 3.\nTable 3: This table compares the relative frequency of error categories found by this experiment\nand those reported by Xu et al. [33]. Note no uncertainty values were reported for the relative\nfrequency of the corresponding error categories. Note that only the GPT3 results were included\nin this comparison as they most accurate reflection of the models in the review.\nLiterature Error Relative Frequency Corresponding Average Relative\nCategories Error Types Frequency for GPT-3\nHallucination and 60.7% Knowledge Errors and 73 ± 15%Wong Selection Deep Semantic Errors\nPerspective Mistake 44.5% Shallow Semantic Errors 52 ± 6%\nNote that the results reported by Xu et al. would not consider syntactic errors types (except\nfor knowledge and other syntactic errors) as they do not indicate any error in reasoning, only\ninterfacing with an external tool [33]. Their study found that the total number of types of\nerrors per failure was1.61; our result for this value is comparable at1.55 ± 0.06.\nB Example LLM Prompt\nOne of the PRONTOQA steamroller problems reads as follows:5\nEach composite number is not liquid. Every composite number is a fraction. Every com-\nposite number is a number. Negative numbers are not large. Every fraction is large. Each\nfraction is a real number. Fractions are integers. Integers are temperate. Each number\nis slow. Each even number is loud. Even numbers are natural numbers. Alex is an even\nnumber. Alex is a composite number.\n5GPT3Fusemate_hops_2.txt, Iteration: 50\n17\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nTrue or false: Alex is large.\nThe correct answer is “True”. This can be seen from two modus ponens inferences. Several\nirrelevant utterances can be ignored.\nThe following instructions are given to the LLMs:\nConvert the given English statements into a Prolog program.\nUse the format and rules below, including an example for guidance.\n**Format and Rules:**\n- Specific statements become facts: 'Whiskers is a cat' -> 'cat(whiskers).'\n- General 'is a' statements become rules: 'All cats are birds' -> 'bird(X) :- cat(X).'\n- Use '\\+' for negations: 'No bird swims' -> '\\+swims(X) :- bird(X).'\n- Frame queries with '?-': 'Does Whiskers swim?' -> '?- swims(whiskers).'\n**Example:**\nEnglish: 'All cats are birds. No bird swims. Whiskers is a cat. Does Whiskers swim?'\nProblog: cat(whiskers).\nbird(X) :- cat(X).\n\\+swims(X) :- bird(X).\n?- swims(whiskers).\nNow, convert the following statements into a Prolog program:\nQuestion: 'Each composite number is not liquid. ...'\nProblog Program:\n(The concrete problem is given in full and only listed in abbreviated form above.)\nFor example, GPT3 answers with the lines:\ncomposite(X) :- number(X), \\+prime(X).\nliquid(X) :- \\+composite(X).\nfraction(X) :- composite(X).\nnumber(X).\nlarge(X) :- \\+negative(X), fraction(X).\nreal_number(X) :- fraction(X).\ninteger(X) :- fraction(X).\ntemperate(X) :- integer(X).\nslow(X) :- number(X).\nloud(X) :- even(X).\neven(X) :- number(X), X mod 2 =:= 0.\nnatural_number(X) :- even(X).\nalex is large.\nNotice the question is translated incorrectly in the last line, it should be a prolog query\n?- large(alex). Also the rule foreven is unexpected and considered a syntax error.\nC Correlation Matrix\nFigure 6 shows the correlation between error types for our experiments. Note that most of the\nexamples that contained errors came from experiments using GPT3 and Gemini, so GPT4 is\nunderrepresented. The correlation between Natural Language Errors and Symbol Errors can\nbe explained by the experimental conditions. In zero-shot examples the model is more likely to\nmake both natural language errors and symbol errors as shown in Figure 4, while the models\n18\nAutomated Theorem Provers Help Improve LLM Reasoning McGinness and Baumgartner\nmake less of these errors in one-shot exmaples. Therefore we would naturally expect to see a\ncorrelation between these error types when considering all examples.\nFigure 6: Error Type Correlation Matrix. This shows that there only two significant correla-\ntions and one anti-correlation between the types of errors. There is a strong anti-correlation\nbetween Shallow Semantic Errors and Deep Semantic Errors, indicating that there are many\nexamples where only one of these two types occurred. There is a correlation between Natural\nLanguage Errors and Symbol Errors and also a correlation between Shallow Semantic Errors\nand Knowledge Errors. All other correlations between errors types are close to 0.\nThe correlation between Knowledge Errors and Shallow Semantic has an interesting explana-\ntion; it is a feature of the dataset, not the error classification system. Knowledge Errors are\nsyntactic errors that cannot be corrected. Therefore when SEDAC investigates semantic errors,\nthese lines will always be disregarded. The results show that for the remaining lines, higher\nlikelihood that there will be Shallow Semantic Errors. This can be explained by looking at\nthe most common cause of knowledge errors: inclusion of mathematical expressions such as\neven(X) :- mod2(x)=0. These problems are also the most likely problems to mistake adjec-\ntives as nouns; for example prime(X) instead of prime_number(X) which can also fixed by\npartial SEDAC.\n19",
  "topic": "Correctness",
  "concepts": [
    {
      "name": "Correctness",
      "score": 0.7894932627677917
    },
    {
      "name": "Computer science",
      "score": 0.7706421613693237
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5859121084213257
    },
    {
      "name": "Programming language",
      "score": 0.5665844082832336
    },
    {
      "name": "Automated reasoning",
      "score": 0.5658226609230042
    },
    {
      "name": "Natural language processing",
      "score": 0.5472679734230042
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5003995895385742
    },
    {
      "name": "Description logic",
      "score": 0.4776362478733063
    },
    {
      "name": "Automated theorem proving",
      "score": 0.4710264801979065
    },
    {
      "name": "Logic programming",
      "score": 0.4674433469772339
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3332866132259369
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118347636",
      "name": "Australian National University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I42894916",
      "name": "Data61",
      "country": "AU"
    }
  ]
}