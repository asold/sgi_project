{
    "title": "Investigation of Different Language Models for Turkish Speech Recognition",
    "url": "https://openalex.org/W1531783358",
    "year": 2006,
    "authors": [
        {
            "id": null,
            "name": "A.O. Bayer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2974181668",
            "name": "T. Ciloglu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2673094857",
            "name": "M.T. Yondem",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W139293362",
        "https://openalex.org/W6629716778",
        "https://openalex.org/W6679565117",
        "https://openalex.org/W6600383931",
        "https://openalex.org/W1516736046",
        "https://openalex.org/W9388558",
        "https://openalex.org/W2134501463",
        "https://openalex.org/W1495314497",
        "https://openalex.org/W1508165687",
        "https://openalex.org/W1579758088"
    ],
    "abstract": "Large vocabulary continuous speech recognition can be performed with high accuracy for languages like English that do not have a rich morphological structure. However, the performance of these systems for agglutinative languages is very low. The major reason for that is, the language models that are built on the words do not perform well for agglutinative languages. In this study, three different language models that consider the structure of the agglutinative languages are investigated. Two of the models consider the subword units as the units of language modeling. The first one uses only the stem of the words as units, and the other one uses stems and endings of the words separately as the units. The third model, firstly, places the words into certain classes by using the co-occurrences of the words, and then uses these classes as the units of the language model. The performance of the models are tested by using two stage decoding; in the first stage, lattices are formed by using bi-gram models and then tri-gram models are used for recognition over these lattices. In this study, it is shown that the vocabulary coverage of the system seriously affects the recognition performance. For this reason, models that use stems and endings as the modeling unit perform better since their coverage of the vocabulary is higher. In addition to that, a single-pass decoder that can perform single pass decoding over these models is believed to increase the recognition performance.",
    "full_text": null
}