{
  "title": "Faster than Real-Time Facial Alignment: A 3D Spatial Transformer Network Approach in Unconstrained Poses",
  "url": "https://openalex.org/W2737357585",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2122924710",
      "name": "Chandrasekhar Bhagavatula",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2023713338",
      "name": "Chenchen Zhu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2005902965",
      "name": "Khoa Luu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A1959934690",
      "name": "Marios Savvides",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2122924710",
      "name": "Chandrasekhar Bhagavatula",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2023713338",
      "name": "Chenchen Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005902965",
      "name": "Khoa Luu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1959934690",
      "name": "Marios Savvides",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2058961190",
    "https://openalex.org/W1803059841",
    "https://openalex.org/W2125416623",
    "https://openalex.org/W6678569853",
    "https://openalex.org/W6638488279",
    "https://openalex.org/W2436453945",
    "https://openalex.org/W1998294030",
    "https://openalex.org/W2127182692",
    "https://openalex.org/W2441198140",
    "https://openalex.org/W2012885984",
    "https://openalex.org/W1682276745",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2962809185",
    "https://openalex.org/W2042535240",
    "https://openalex.org/W6683693282",
    "https://openalex.org/W2048005662",
    "https://openalex.org/W1916406603",
    "https://openalex.org/W1927238661",
    "https://openalex.org/W2155893237",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W6717992675",
    "https://openalex.org/W2964014798",
    "https://openalex.org/W6662335928",
    "https://openalex.org/W6640455264",
    "https://openalex.org/W1990937109",
    "https://openalex.org/W2171490473",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W6657940589",
    "https://openalex.org/W2152826865",
    "https://openalex.org/W2038952578",
    "https://openalex.org/W2163998463",
    "https://openalex.org/W2135132101",
    "https://openalex.org/W6640274710",
    "https://openalex.org/W2296659146",
    "https://openalex.org/W2912990735",
    "https://openalex.org/W2120420721",
    "https://openalex.org/W2032558548",
    "https://openalex.org/W2431101926",
    "https://openalex.org/W2128409098",
    "https://openalex.org/W2038891881",
    "https://openalex.org/W2111372597",
    "https://openalex.org/W2564603115",
    "https://openalex.org/W2124750300",
    "https://openalex.org/W1946919140",
    "https://openalex.org/W6764072534",
    "https://openalex.org/W1915668717",
    "https://openalex.org/W2157285372",
    "https://openalex.org/W1935685005",
    "https://openalex.org/W2432917172",
    "https://openalex.org/W2029835507",
    "https://openalex.org/W2952074561",
    "https://openalex.org/W1796263212",
    "https://openalex.org/W2949351356",
    "https://openalex.org/W2047508432",
    "https://openalex.org/W1598335904",
    "https://openalex.org/W2107037917",
    "https://openalex.org/W2950094539",
    "https://openalex.org/W1919814523",
    "https://openalex.org/W2950428624",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2160096928",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2124907686"
  ],
  "abstract": "Facial alignment involves finding a set of landmark points on an image with a known semantic meaning. However, this semantic meaning of landmark points is often lost in 2D approaches where landmarks are either moved to visible boundaries or ignored as the pose of the face changes. In order to extract consistent alignment points across large poses, the 3D structure of the face must be considered in the alignment step. However, extracting a 3D structure from a single 2D image usually requires alignment in the first place. We present our novel approach to simultaneously extract the 3D shape of the face and the semantically consistent 2D alignment through a 3D Spatial Transformer Network (3DSTN) to model both the camera projection matrix and the warping parameters of a 3D model. By utilizing a generic 3D model and a Thin Plate Spline (TPS) warping function, we are able to generate subject specific 3D shapes without the need for a large 3D shape basis. In addition, our proposed network can be trained in an end-to-end framework on entirely synthetic data from the 300W-LP dataset. Unlike other 3D methods, our approach only requires one pass through the network resulting in a faster than real-time alignment. Evaluations of our model on the Annotated Facial Landmarks in the Wild (AFLW) and AFLW2000-3D datasets show our method achieves state-of-the-art performance over other 3D approaches to alignment.",
  "full_text": "Faster Than Real-time Facial Alignment: A 3D Spatial Transformer Network\nApproach in Unconstrained Poses\nChandrasekhar Bhagavatula, Chenchen Zhu, Khoa Luu, and Marios Savvides\nCarnegie Mellon University\nPittsburgh, PA\ncbhagava@andrew.cmu.edu, zcckernel@cmu.edu, kluu@andrew.cmu.edu, msavvid@ri.cmu.edu\nAbstract\nFacial alignment involves ﬁnding a set of landmark\npoints on an image with a known semantic meaning. How-\never, this semantic meaning of landmark points is often lost\nin 2D approaches where landmarks are either moved to vis-\nible boundaries or ignored as the pose of the face changes.\nIn order to extract consistent alignment points across large\nposes, the 3D structure of the face must be considered in the\nalignment step. However, extracting a 3D structure from\na single 2D image usually requires alignment in the ﬁrst\nplace. We present our novel approach to simultaneously ex-\ntract the 3D shape of the face and the semantically consis-\ntent 2D alignment through a 3D Spatial Transformer Net-\nwork (3DSTN) to model both the camera projection matrix\nand the warping parameters of a 3D model. By utilizing a\ngeneric 3D model and a Thin Plate Spline (TPS) warping\nfunction, we are able to generate subject speciﬁc 3D shapes\nwithout the need for a large 3D shape basis. In addition, our\nproposed network can be trained in an end-to-end frame-\nwork on entirely synthetic data from the 300W-LP dataset.\nUnlike other 3D methods, our approach only requires one\npass through the network resulting in a faster than real-\ntime alignment. Evaluations of our model on the Annotated\nFacial Landmarks in the Wild (AFLW) and AFLW2000-3D\ndatasets show our method achieves state-of-the-art perfor-\nmance over other 3D approaches to alignment.\n1. Introduction\nRobust face recognition and analysis are contingent upon\naccurate localization of facial features. When modeling\nfaces, the landmark points of interest consist of points that\nlie along the shape boundaries of facial features, e.g. eyes,\nlips, mouth, etc. When dealing with face images collected\nin the wild conditions, facial occlusion of landmarks be-\ncomes a common problem for off-angle faces. Predicting\nthe occlusion state of each landmarking points is one of the\nFigure 1. A subject from the CMU Multi-PIE dataset [18, 19] land-\nmarked and frontalized by our method at various poses. Land-\nmarks found by our model are overlaid in green if they are de-\ntermined to be a visible landmark and blue if self-occluded. The\nnon-visible regions of the face are determined by the estimated\ncamera center and the estimated 3D shape. Best viewed in color.\nchallenges due to variations of objects in faces, e.g. beards\nand mustaches, sunglasses and other noisy objects. Addi-\ntionally, face images of interest nowadays usually contain\noff-angle poses, illumination variations, low resolutions,\nand partial occlusions.\nMany complex factors could affect the appearance of\na face image in real-world scenarios and providing toler-\nance to these factors is the main challenge for researchers.\nAmong these factors, pose is often the most important factor\nto be dealt with. It is known that as facial pose deviates from\na frontal view, most face recognition systems have difﬁculty\nin performing robustly. In order to handle a wide range of\npose changes, it becomes necessary to utilize 3D structural\ninformation of faces. However, many of the existing 3D\nface modeling schemes [1, 4, 42] have many drawbacks,\nsuch as computation time and complexity. Though these\ncan be mitigated by using depth sensors [23] or by track-\ning results from frame to frame in video [39], this can cause\ndifﬁculty when they have to be applied in real-world large\n1\narXiv:1707.05653v2  [cs.CV]  8 Sep 2017\nFigure 2. (a & d): Images in the wild from the AFLW dataset. (b &\nd): 3D landmarks (green: visible, blue: occluded) estimated from\ninput image. (e & f): 3D model generated from input image. Best\nviewed in color.\nscale unconstrained face recognition scenarios where video\nand depth information is not available. The 3D generic elas-\ntic model (3D-GEM) approach was proposed as an efﬁcient\nand reliable 3D modeling method from a single 2D image.\nHeo et al. [22, 35] claim that the depth information of a\nface is not extremely discriminative when factoring out the\n2D spatial location of facial features. In our method, we\nfollow this idea and observe that fairly accurate 3D models\ncan be generated by using a simple mean shape deformed to\nthe input image at a relatively low computational cost com-\npared to other approaches.\n1.1. Our Contributions in this Work\n(1) We take the approach of using a simple mean shape\nand using a parametric, non-linear warping of that shape\nthrough alignment on the image to be able to model any\nunseen example. A key ﬂaw in many approaches that rely\non a 3D Morphable Model (3DMM) is that it needs enough\nexamples of the data to be able to model unseen samples.\nHowever, in the case of 3D faces, most datasets are very\nsmall.\n(2) Our approach is efﬁciently implemented in an end-\nto-end deep learning framework allowing for the alignment\nand 3D modeling tasks to be codependent. This ensures that\nalignment points are semantically consistent across chang-\ning poses of the object which also allows for more consis-\ntent 3D model generation and frontalization on images in\nthe wild as shown in Figs. 1 and 2.\n(3) Our method only requires a single pass through the\nnetwork allowing us to achieve faster than real-time pro-\ncessing of images with state-of-the-art performance over\nother 2D and 3D approaches to alignment.\n2. Related Work\nThere have been numerous studies related to face align-\nment since the ﬁrst work of Active Shape Models (ASM)\n[14] in 1995. A comprehensive literature review in face\nalignment is beyond the scope of this work. In this paper,\nwe mainly focus on recent Convolutional Neural Network\n(CNN) approaches to solve the face alignment problem. Es-\npecially those methods aimed at using 3D approaches to\nachieve robust alignment results.\n2.1. Face Alignment Methods\nWhile Principal Component Analysis (PCA) and its vari-\nants [14, 13, 15] were successfully used to model the fa-\ncial shapes and appearances, there have since been many\nadvances in facial alignment. Landmark locations can be\ndirectly predicted by a regression from a learned feature\nspace [9, 16, 44]. Xiong et al. [45] presented the Global\nSupervised Descent Method (GSDM) method to solve the\nproblem of 2D face alignment. The objective function in\nGSDM is divided into multiple regions of similar gradient\ndirections. It then constructs a separate cascaded shape re-\ngressor for each region. Yu et al. [46] incorporated 3D\npose landmarking models with group sparsity to indicate\nthe best landmarks. These kind of methods shows an in-\ncrease of performance on landmark localization. However,\nthese methods all rely on hand-crafted features. Recently,\nCNN-based methods have achieved good results in facial\nalignment [51, 48]. 3DDFA [51] ﬁts a dense 3D face model\nto the image via CNN and DDN [48] proposes a novel cas-\ncaded framework incorporating geometric constraints for\nlocalizing landmarks in faces and other non-rigid objects.\nRecently, shape regression has been used in numerous fa-\ncial landmarking methods [41, 36, 43].\nThere are several recent works studying the human head\nrotations [12, 53], nonlinear statistical models ([17]) and\n3D shape models [8, 20]. Nonlinear statistical model ap-\nproaches are impractical in real-time applications. View-\nbased methods employ a separate model for each viewpoint\nmode. Traditionally, the modes are speciﬁed as part of the\nalgorithm design, and problems can arise at midpoints be-\ntween models.\n2.2. CNNs for 3D Object Modeling\nWhile estimating a 3D model from images is not a new\nproblem, the challenging task of modeling objects from a\nsingle image has always posed a challenge. This is, of\ncourse, due to the ambiguous nature of images where depth\ninformation is removed. With the recent success of deep\nlearning and especially CNNs in extracting salient informa-\ntion from images, there have been many explorations into\nhow to best use CNNs for modeling objects in 3 dimen-\nsions. Many of these approaches are aimed creating a depth\nestimation for natural images [33, 2, 37, 32, 31]. While\nthe results on uncontrolled images are impressive, the fact\nthat these models are very general means they tend to suf-\nfer when applied to speciﬁc objects, such as faces. In fact,\nmany times, the depth estimate for faces in the scene tend\n2\nto be fairly ﬂat. By limiting the scope of the method, the\nresulting estimated 3D model can be made much more ac-\ncurate. Hassner et al. [21] use a 3D model of the face to be\nable to frontalize faces in unseen images with the end goal\nof improving face recognition by limiting the variations the\nmatcher has to learn. However, this approach requires land-\nmarks on the input face in the same fashion as other meth-\nods [22, 35, 21, 34, 52].\nA 2D approach to landmarking inevitably suffers from\nthe problem of visibility and self-occlusion. As Zhu et\nal. [52] show, the problem of landmark marching, where\nlandmarks tend to move to the visible boundary, can cause\nissues when estimating 3D models from purely 2D align-\nment. However, this problem can be alleviated by using a\n3D model of the face in the alignment step itself as done in\n[27, 51]. Both of these methods make use of an underly-\ning 3D Morphable Model (3DMM) and try to ﬁt the model\nto the input image in order to ﬁnd the 2D landmarks. This\nof course requires a basis to use and the Basel Face Model\n(BFM) [24] is a very popular model to use. However, the\nBFM is only created from a set of 100 male and 100 fe-\nmale scans. As any basis can only recreate combinations\nof the underlying samples, this can severely limit the capa-\nbility of these models to ﬁt outlier faces or expressions not\nseen before. Although there has been recent efforts to gen-\nerate more accurate 3DMMs [6], neither the data nor the\nmodel is available to researchers in the ﬁeld of biometrics.\nTherefore, we propose to use a smooth warping function,\nThin Plate Splines (TPS) [5], to warp mean shapes to ﬁt the\ninput image and generate new 3D shapes. In this fashion,\nany new face can be modeled, even if its shape cannot be\nreconstructed by the BFM.\n3. 3D Spatial Transformer Networks\nIn order to model how a face truly changes from view-\npoint to viewpoint, it is necessary to have both the true 3D\nmodel of the subject in the image and the properties of the\ncamera used to capture the image, usually in the form of the\ncamera projection matrix. However, knowledge of the true\n3D model and the camera projection matrix are almost al-\nways not available. Jaderberg et al. [25], in their work on\nSpatial Transformer Networks, use a deep network to esti-\nmate the parameters of either an afﬁne transformation or a\n2D Thin Plate Spline (TPS) transformation. These param-\neters are then used to generate a new sampling grid which\ncan then be used to generate the transformed image.\nWe approach ﬁnding the unknown camera projection\nmatrix parameters and the parameters needed to generate\nthe 3D model of the head in a similar fashion. Both the\ncamera projection parameters and the warping parameters,\na TPS warp in this case, can be estimated from deep fea-\ntures generated from the image using any architecture. The\nTPS parameters can be used to warp a model of the face\nto match what the network estimates the true 3D shape is\nand the camera projection parameters can be used to texture\nthe 3D coordinates from the 2D image. Additionally, the\npose of the face can be determined from the camera param-\neters allowing for a visibility map to be generated for the\n3D model. This allows us to only texture vertexes that are\nvisible in the image as opposed to vertexes that are occluded\nby the face itself. The architecture of our model is shown\nin Figure 3. Sections 3.1,3.2, and 3.3 detail how to create\ndifferentiable modules to utilize the camera projection and\nTPS parameters that are estimated by the deep network to\nwarp and project a 3D model to a 2D image plane for texture\nsampling.\n3.1. Camera Projection Transformers\nIn order to be able to perform end-to-end training of a\nnetwork designed to model 3D transformations of the face,\na differentiable module that performs a camera projection\nmust be created. This will be part of the grid generator por-\ntion of the Spatial Transformer. Modeling how a 3D point\nwill map to the camera coordinates is expressed by the well\nknown camera projection equation\npc˜=Mpw (1)\nwhere pc is the homogeneous 2D point in the camera co-\nordinate system, pw is the homogeneous 3D point in the\nworld coordinate system, and M is the 3x4 camera projec-\ntion matrix. This relationship is only deﬁned up to scale\ndue to the ambiguity of scale present in projective geom-\netry, hence the ˜= instead of a hard equality. The camera\nprojection matrix has only 11 degrees of freedom since it\nis only deﬁned up to scale as well. Therefore, this mod-\nule takes in the 11 parameters estimated by a previous layer\nas the input in the form of a length 11 vector, a. In order\nto perform backpropogation on the new grid generator, the\nderivative of the generated grid with respect to a must be\ncomputed.\nSince Eqn. 1 is only deﬁned up to scale, the ﬁnal output\nof this module will have to divide out the scale factor. By\nﬁrst rewriting the camera projection matrix as\nM =\n\n\na1 a2 a3 a4\na5 a6 a7 a8\na9 a10 a11 1\n\n=\n\n\nmT\n1\nmT\n2\nmT\n3\n\n (2)\nwhere ai is the ith element of a, the ﬁnal output of the cam-\nera projection module can be written as\nO =\n[xc\nyc\n]\n=\n\n\nmT\n1 pw\nmT\n3 pw\nmT\n2 pw\nmT\n3 pw\n\n (3)\nThe gradient with respect to each of the rows of M can be\n3\nFigure 3. Network design of the 3D TPS Spatial Transformer for facial alignment. Because a 3D model and an estimate of the camera\nposition are found in the output of the network, visibility of landmarks can also be determined. Visible landmarks are shown in green while\nnon-visible landmarks are shown in blue.\nshown to be\nδO\nδmT\n1\n=\n[\npT\nw\nmT\n3 pw\n0\n]\nδO\nδmT\n2\n=\n[\n0\npT\nw\nmT\n3 pw\n]\nδO\nδmT\n3\n=\n\n\n−pT\nw(mT\n1 pw)\n(mT\n3 pw)\n2\n−pT\nw(mT\n2 pw)\n(mT\n3 pw)\n2\n\n (4)\nUsing the chain rule, the gradient of the loss of the network\nwith respect to the input can be found as\nδL\nδa =\n\n\n(\nδL\nδO\nδO\nδmT\n1\n)T\n(\nδL\nδO\nδO\nδmT\n2\n)T\n(\nδL\nδO\nδO\nδmT\n3\n)T\n\n\n(5)\nSince M is only deﬁned up to scale, the last element of M\ncan be deﬁned to be a constant which means that only the\nﬁrst 11 elements of this gradient are used to actually per-\nform the backpropogation ona. Since M relates many pairs\nof 2D and 3D points, the gradient is computed for every pair\nand added together to give the ﬁnal gradient that is used for\nupdating a.\n3.2. 3D Thin Plate Spline Transformers\nWhen modeling the 3D structure of a face, a generic\nmodel cannot represent the variety of shapes that might be\nseen in an image. Therefore, some method of warping a\nmodel must be used to allow the method to handle unseen\nshapes. Thin Plate Spline (TPS) warping has been used by\nmany applications to great effect [5, 11]. TPS warps have\nthe very dersirable features of providing a closed form of a\nsmooth, parameterized warping given a set of control points\nand desired destination points. Jaderberg et al. [25] showed\nhow 2D TPS Spatial Transformers could lead to good nor-\nmalization of nonlinearly transformed input images. Ap-\nplying a TPS to a 3D set of points follows a very similar\nprocess. As in [25], the TPS parameters would be estimated\nfrom a deep network of some sort and passed as input to a\n3D grid generator module.\nA 3D TPS function is of the form\nf∆x (x,y,z ) =\n\n\nb1x\nb2x\nb3x\nb4x\n\n\nT \n\n1\nx\ny\nz\n\n+\nn∑\nj=1\nwjxU(|cj −(x,y) |)\n(6)\nwhere b1x, b2x, b3x, b4x, and wjx are the parameters of the\nfunction, cj is the jth control point used in determining\nthe function parameters, and U(r) = r2 log r. This func-\ntion is normally learned by setting up a system of linear\nequations using the known control points, cj and the cor-\nresponding points in the warped 3D object. The function\nﬁnds the change in a single coordinate, the change in the\nx-coordinate in the case of Eqn. 6. Similarly, one such\n4\nfunction is created for each dimension, i.e. f∆x (x,y,z ),\nf∆y (x,y,z ), and f∆z (x,y,z ). The 3D TPS module would\nthen take in the parameters for all three of these functions\nas input and output the newly transformed points on a 3D\nstructure as\nO =\n\n\nx′\ny′\nz′\n\n=\n\n\nf∆x (x,y,z )\nf∆y (x,y,z )\nf∆z (x,y,z )\n\n+\n\n\nx\ny\nz\n\n (7)\nThis means that the 3D TPS module must have all of the 3D\nvertices of the generic model and the control points on the\ngeneric model as ﬁxed parameters speciﬁed from the start.\nThis will allow the module to warp the speciﬁed model by\nthe warps speciﬁed by the TPS parameters.\nAs in 3.1, the gradient of the loss with respect to the input\nparameters must be computed in order to perform backpro-\npogation on this module. As usual, the chain rule can be\nused to ﬁnd this by computing the gradient of the output\nwith respect to the input parameters. Since each 3D vertex\nin the generic model will give one 3D vertex as an output,\nit is easier to compute the gradient on one of these points,\npi = (xi,yi,zi), ﬁrst. This can be shown to be\nδO\nδθ∆x\n=\n\n\n1 0 0\nxi 0 0\nyi 0 0\nzi 0 0\nU(|c1 −(xi,yi,zi) |) 0 0\n... ... ...\nU(|cn −(xi,yi,zi) |) 0 0\n\n\nT\n(8)\nwhere θ∆x are the parameters of f∆x . Similarly, the gradi-\nents for θ∆y and θ∆z are the same with only the non-zeros\nvalues in either the second or third row respectively. The\nﬁnal gradient of the loss with respect to the parameters can\nbe computed as\nδL\nδθ∆x\n= δL\nδO\nδO\nδθ∆x\n(9)\nSince this is only for a single point, once again the gradi-\nent can be computed for every point and added for each set\nof parameters to get the ﬁnal gradient for each set of pa-\nrameters that can be used to update previous layers of the\nnetwork.\n3.3. Warped Camera Projection Transformers\nIn order to make use of the TPS warped 3D points in\nthe camera projection module of the transformer network,\nthe module must take in as input the warped coordinates.\nThis means that such a module would also have to do back-\npropogation on the 3D coordinates as well as the camera\nprojection parameters. Since 3.1 already speciﬁed how to\ncompute the gradient of the loss with respect to the camera\nprojection parameters, all that is left to do is compute the\ngradient of the loss with respect to the 3D coordinates in\nthis module. Taking the derivative of the output in Eqn. 3\nwith respect to the 3D point, pw results in\nδO\nδpw\n=\n\n\nmT\n1\nmT\n3 pw\n− mT\n1 pw\n(mT\n3 pw)\n2 mT\n3\nmT\n2\nmT\n3 pw\n− mT\n2 pw\n(mT\n3 pw)\n2 mT\n3\n\n (10)\nHowever, sincepw is in homogeneous coordinates and only\nthe gradient with respect to the x, y, and z coordinates are\nneeded, the actual gradient becomes\nδO\nδp′w\n=\n\n\nm′T\n1\nmT\n3 pw\n− mT\n1 pw\n(mT\n3 pw)\n2 m′T\n3\nm′T\n2\nmT\n3 pw\n− mT\n2 pw\n(mT\n3 pw)\n2 m′T\n3\n\n (11)\nwhere\np′\nw =\n\n\nxw\nyw\nzw\n\n m′\ni =\n\n\nmi1\nmi3\nmi3\n\n (12)\nand mij is the jthelement of mi. This gradient is computed\nfor every 3D point independently and used in the chain rule\nto compute\nδL\nδpw\n= δL\nδO\nδO\nδpw\n(13)\nwhich can then be used to perform backpropogation on each\npw.\n3.4. 2D Landmark Regression\nIn order to further improve the landmark accuracy, we\nextend our network with a landmark reﬁnement stage. This\nstage treats the projected 2D coordinates from the previ-\nous stage as initial points and estimates the offsets for each\npoint. To extract the feature vector for each point, a 3 ×3\nconvolution layer is attached on top of the last convolution\nlayer in the base model, followed by a 1 ×1 convolution\nlayer for more nonlinearity, resulting in a feature map with\nD channels. Then each initial point is projected onto this\nfeature map and its D-dimensional feature vector is ex-\ntracted along the channel direction. Notice that the initial\npoints are often not aligned with the grids on the feature\nmap. Therefore, their feature vectors are sampled with bi-\nlinear interpolation.\nGiven the feature vector for each landmark, it goes\nthrough a fully-connected (FC) layer to output the offsets,\ni.e. δx and δy. Then the offsets are added to the coordi-\nnates of the initial location. For each landmark we use an\nindependent FC layer. We don’t share the FC layer for all\nlandmarks because each landmark should have a unique be-\nhavior of offsets. For example, the center of the eye may\nmove left after regression whereas the corner of the eye may\n5\nmove right. Also, sometimes two initial landmarks may be\nprojected to the same location due to a certain pose. We\nwant them to move to different locations even when they\nhave the same feature vector.\n3.5. 3D Model Regression From 2D Landmarks\nOnce the 2D regression is performed, the mapping be-\ntween the 3D model and the 2D landmarks is broken. While\nthis is not necessarily a problem in the case of sparse facial\nalignment, if a denser scheme is needed, the entire model\nwould have to be retrained. In order to avoid this, we cre-\nate a new 3D model that does map to these 2D landmarks\nby ﬁnding a new set of 3D coordinates that project to the\nnew 2D landmarks and warping the 3D model to ﬁt these\nnew points. To ﬁnd the new 3D coordinates, we need to\nbackproject rays through each of the 2D landmarks through\n3D space using the camera projection matrix we have esti-\nmated. The equation for the ray of points associated with a\ngiven homogeneous 2D point, pi\n2D, is deﬁned as\npi′\n3D =\n[A−1b\n1\n]\n+ λ\n[A−1pi\n2D\n0\n]\n(14)\nwhere A and b are the ﬁrst three and the last column of the\nestimated camera projection matrix respectively.\nThese rays represent all possible points in 3D that could\nproject to the determined locations in the image. We then\nﬁnd the closest point, pi′\n3D, on the ray to the original 3D co-\nordinate, pi\n3D, to use as the new 3D point as shown in Fig.\n4. These new correspondences are used to perform a TPS\nwarping of the model. After this warping, the landmark\npoints on the model will project to exactly the regressed 2D\nlandmarks, recovering the mapping between the 3D model\nand the 2D image. This new model can then be projected\nonto the image to generate a much more accurate texturing\nof the 3D model. This same style of warping can be used to\nmove the 3D coordinates anywhere we choose. This means\nneutralizing out expressions, especially smiles, is very easy\nto do by using the texture from the regressed 3D shape.\nWhile the non-smiling shape will not be as accurate due to\nthe fact that a non-smiling image was not seen, it still gives\nconvincing qualitative results, as seen in Fig. 5, which indi-\ncate it may be a worthwhile avenue of exploration for future\nwork, especially in face recognition.\n4. Experiments\n4.1. Datasets\n300W-LP: The 300W-LP [51] dataset contains 122,450\nsynthetically generated views of faces from the AFW [54],\nLFPW [3], HELEN [30], and IBUG [38] datasets. These\nimages not only contain rotated faces but also attempt to\nmove the background in a convincing fashion, making it a\nFigure 4. Backprojection or rays through image landmarks. The\nclosest points are found for each ray-landmark pair to use as new\n3D coordinates for the face model. The original model (green) is\nwarped to ﬁt the new landmarks with a 3D TPS warp resulting in\na new face model (red).\nFigure 5. 3D renderings of input face with a smiling expression.\nThe resulting regressed 3D model (green box) maintains the smile\nand is very similar to the input image while the same texture ap-\nplied to the original shape (red box) suffers a small degradation in\nshape but allows for a non-smiling rendering of the input image.\nvery useful dataset for training 3D approaches to work on\nreal world images.\nAFLW: The Annotated Facial Landmarks in the Wild\n(AFLW) dataset [28] is a relatively large dataset for evalu-\nating facial alignment on wild images. It contains approxi-\nmately 25,000 faces annotated with 21 landmarks with vis-\nibility labels. The dataset provides pose estimates so re-\nsults are grouped into three different pose ranges, [0◦,30◦],\n(30◦,60◦], and (60◦,90◦]. Due to the inconsistency in the\nbounding boxes in the AFLW dataset, we adopt the use of\na face detector ﬁrst to normalize the scale of the faces. The\nMultiple Scale Faster Region-based CNN approach [49]\nhas shown good results and at a fast speed. We use the\nrecent extension to this work, the Contextual Multi-Scale\nRegion-based CNN (CMS-RCNN) approach [50] to per-\nform the face detection in any experiment where face detec-\ntion is needed. The CMS-RCNN approach detects 98.8%\n(13,865), 95.9% (5,710), and 86.5% (3,830) of the faces in\nthe [0◦,30◦], (30◦,60◦], and (60◦,90◦] pose ranges respec-\ntively.\nAFLW2000-3D: Zhu et al. [51] accurately pointed out\nhow merely evaluating an alignment scheme on the visible\nlandmarks in a dataset can result in artiﬁcially low errors.\nTherefore, a true evaluation of any 3D alignment method\n6\nmust also evaluate alignment on the non-visible landmarks\nas well. The AFLW2000-3D dataset contains the ﬁrst 2000\nimages of the AFLW dataset but with all 68 points deﬁned\nby the scheme in the CMU MPIE dataset [18, 19]. These\npoints were found by aligning the Basel Face Model to the\nimages. While this is a synthetic dataset, meaning the true\nlocation of the non-visible landmarks is not known, it is the\nbest one can do when dealing with real images. As these\nimages are from the AFLW dataset, they are also grouped\ninto the same pose ranges.\n4.2. Implementation Details\nOur network is implemented in the Caffe [26] frame-\nwork. A new layer is created consisting of the 3D TPS\ntransformation module, the camera projection module and\nthe bilinear sampler module. All modules are differentiable\nso that the whole network can be trained end-to-end.\nWe adopt two architectures, AlexNet [29] and VGG-16\n[40], as the pre-trained models for our shared feature extrac-\ntion networks in Fig. 3, i.e. we use the convolution layers\nfrom the pre-trained models to initialize ours. Since these\nnetworks already extract informative low-level features and\nwe do not want to lose this information, we freeze some of\nthe earlier convolution layers and ﬁnetune the rest. For the\nAlexNet architecture, we freeze the ﬁrst layer while for the\nVGG-16 architecture, the ﬁrst 4 layers are frozen.\nThe 2D landmark regression is implemented by attach-\ning additional layers on top of the last convolution layer.\nWith N landmarks to regress, we needN FC layers to com-\npute the offsets for each individual landmark. While it’s\npossible to setup N individual FC layers, here we imple-\nment this by adding one Scaling layer followed by a Re-\nduction layer and Bias layer. During training only the new\nlayers are updated and all previous layers are frozen.\n4.3. Training on 300W-LP\nWhen training our model, we train on the AFW, HE-\nLEN, and LFPW subsets of the 300W-LP dataset and use\nthe IBUG portion as a validation set. All sets are normalized\nusing the bounding boxes from the CMS-RCNN detector\nby reshaping the detected faces to 250 x 250 pixels. For the\nAlexNet architecture, we train for 100,000 iterations with a\nbatch size of 50. The initial learning rate is set to 0.001 and\ndrops by a factor of 2 after 50,000 iterations. When train-\ning the landmark regression, the initial learning rate is 0.01\nand drops by a factor of 10 every 40,000 iterations. For the\nVGG-16 architecture, we train for 200,000 iterations with a\nbatch size of 25. The initial learning rate is set to 0.001 and\ndrops by a factor of 2 after 100,000 iterations. When train-\ning the landmark regression, the initial learning rate is 0.01\nand drops by a factor of 10 every 70,000 iterations. The\nmomentum for all experiments is set to 0.9. Euclidean loss\nis applied to 3D vertexes, 2D projected landmarks and 2D\nTable 1. Alignment accuracy for both the AlexNet (AN) and VGG-\n16 (VGG) models. (LR: landmark regression)\nAFLW Dataset (21 pts)\n[0,30] (30,60] (60,90] mean std\nAN 4.88 5.55 7.10 5.84 1.14\nAN+LR 4.00 4.48 5.89 4.79 0.98\nVGG 4.15 4.64 5.96 4.92 0.94\nVGG+LR 3.46 3.78 4.77 4.00 0.69\nregressed landmarks.\n4.4. Ablation Experiments\nTo investigate the effect of each component in our net-\nwork, we conduct two ablation studies. All the models in\nthese experiments are trained on the same 300W LP dataset\nand tested on the detected images in AFLW. We ﬁrst test\nthe effect of the different pre-trained models. We ﬁne-tune\nour network from the AlexNet and VGG-16 models pre-\ntrained on the ImageNet dataset and evaluate the landmark\naccuracy before the regression step. The VGG-16 model\noutperforms the AlexNet model in all three pose ranges on\nthe AFLW detected set as shown in Table 1. This seems to\nindicate that a good base model is important for the param-\neter estimation portion of the network. Second, we evaluate\nthe effect of landmark regression stage. We compare the er-\nrors between the regressed and projected landmarks. Table\n1 shows that the landmark regression step greatly helps to\nimprove the accuracy.\n4.5. Comparison Experiments\nAFLW: Since the CMS-RCNN approach may only de-\ntect the easier to landmark faces, we use the provided\nbounding box anytime the face is not detected by the detec-\ntor. Due to the inconsistency between the two bounding box\nschemes, faces are not always normalized properly. How-\never, we feel this is the only way to get a fair comparison to\nother methods without artiﬁcially making the dataset easier\nby only evaluating on detected faces. We compare against\nbaseline methods used by [51] on the same dataset, namely\nCascaded Deformable Shape Models (CDM) [47], Robust\nCascaded Pose Regression (RCPR) [7], Explicit Shape Re-\ngression (ESR) [10], SDM [44] and 3DDFA [51]. All\nmethods except for CDM were retrained on the 300W-LP\ndataset. The Normalized Mean Error (NME) is computed\nby averaging the error of the visible landmarks and normal-\nizing it by the square root of the bounding box size ( h x\nw) provided in the dataset. Table 2 clearly shows that our\nmodel using the VGG-16 architecture has achieved better\naccuracy in all pose ranges, especially the (60◦,90◦] cate-\ngory, and has achieved a smaller standard deviation in the\nerror. This means that not only are the landmarks more ac-\ncurate, they are more consistent than the other methods.\n7\nFigure 6. CED curves for both the AlexNet (red) and VGG-16 (green) architectures on both the AFLW (left) and AFLW2000-3D (right)\ndataset. To balance the distributions, we randomly sample 13,209 faces from AFLW and 915 faces from AFLW2000-3D, split evenly\namong the 3 categories, and compute the CED curve. This is done 10 times and the average of the resulting CED curves are reported. The\nmean NME% for each architecture from Table 2 is also reported in the legend.\nTable 2. The NME(%) of face alignment results on AFLW and AFLW2000-3D. The best two numbers in each category are shown in bold.\nAFLW Dataset (21 pts) AFLW 2000-3D Dataset (68 pts)\nMethod [0,30] (30,60] (60,90] mean std [0,30] (30,60] (60,90] mean std\nCDM 8.15 13.02 16.17 12.44 4.04 - - - - -\nRCPR 5.43 6.58 11.53 7.85 3.24 4.26 5.96 13.18 7.80 4.74\nESR 5.66 7.12 11.94 8.24 3.29 4.60 6.70 12.67 7.99 4.19\nSDM 4.75 5.55 9.34 6.55 2.45 3.67 4.94 9.76 6.12 3.21\n3DDFA 5.00 5.06 6.74 5.60 0.99 3.78 4.54 7.93 5.42 2.21\n3DDFA+SDM 4.75 4.83 6.38 5.32 0.92 3.43 4.24 7.17 4.94 1.97\nOurs (AlexNet) 4.11 4.69 6.61 5.14 1.31 3.71 5.33 7.19 5.41 1.74\nOurs (VGG-16) 3.55 3.92 5.21 4.23 0.87 3.15 4.33 5.98 4.49 1.42\nAFLW2000-3D: The baseline methods were evaluated\nusing the bounding box of the 68 landmarks so we retrained\nour models using the same bounding box on the training\ndata. Generating these is trivial due to the 3D models. The\nNME is computed using the bounding box size. Here we\nsee that though 3DDFA+SDM performs well, the VGG-\n16 architecture of our model still performs best in both the\n[0◦,30◦] and (60◦,90◦] ranges. While the VGG-16 model is\nonly second best in the (30◦,60◦] range by a small amount,\nthe improvement in (60◦,90◦] means that, once again, our\nmethod generates more accurate and more consistent land-\nmarks, even in a 3D sense. Cumulative Error Distribution\n(CED) curves are reported for both architectures on both\ndatasets in Fig. 6.\n4.6. Running Speed\nIn order to evaluate the speed of our method, we evalu-\nate the models on a random subset of 1200 faces from the\nAFLW subset split evenly into the[0◦,30◦], (30◦,60◦], and\n(60◦,90◦] pose ranges. The images are processed one at a\ntime to avoid any beneﬁt from batch processing. The mod-\nels are evaluated on a 3.40 GHz Intel Core i7-6700 CPU and\nan NVIDIA GeForce GTX TITAN X GPU. Our AlexNet\ntrained model takes a total of 7.064 seconds to landmark\nthe 1200 faces for an average of 0.0059 seconds per im-\nage or approximately 170 faces per second. The deeper and\nmore accurate VGG-16 model landmarks the 1200 faces in\n22.765 seconds for an average of 0.0190 seconds or approx-\nimately 52 faces per second. In comparison, the 3DDFA\napproach [51] takes 75.72 ms (3 iterations at 25.24 ms per\niteration as speciﬁed in [51]) with 2/3 of the time being\nused to process data on the CPU.\n5. Conclusions\nIn this paper we propose a method using 3D Spatial\nTransformer Networks with TPS warping to generate both\na 3D model of the face and accurate 2D landmarks across\nlarge pose variation. The limited data used in the genera-\ntion of a 3DMM can mean that unseen face shapes cannot\nbe modeled. By using a TPS warp, any potential face can\nbe modeled through a regression of 2D landmarks, of which\nthere is much more data available. We have shown how this\napproach leads to more accurate and consistent landmarks\nover other 2D and 3D methods.\n8\nReferences\n[1] J. J. Atick, P. A. Grifﬁn, and A. N. Redlich. Statistical\napproach to shape from shading: Reconstruction of three-\ndimensional face surfaces from single two-dimensional im-\nages. Neural Comput., 8(6):1321–1340, Aug. 1996.\n[2] A. Bansal, B. Russell, and A. Gupta. Marr revisited: 2d-\n3d alignment via surface normal prediction. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 5965–5974, June 2016.\n[3] P. N. Belhumeur, D. W. Jacobs, D. J. Kriegman, and N. Ku-\nmar. Localizing parts of faces using a consensus of exem-\nplars. In CVPR 2011, pages 545–552, June 2011.\n[4] V . Blanz, S. Romdhani, and T. Vetter. Face identiﬁcation\nacross different poses and illuminations with a 3d morphable\nmodel. In Proceedings of Fifth IEEE International Confer-\nence on Automatic Face Gesture Recognition , pages 192–\n197, May 2002.\n[5] F. L. Bookstein. Principal warps: Thin-plate splines and the\ndecomposition of deformations. IEEE Trans. Pattern Anal.\nMach. Intell., 11(6):567–585, June 1989.\n[6] J. Booth, A. Roussos, S. Zafeiriou, A. Ponniahy, and D. Dun-\naway. A 3d morphable model learnt from 10,000 faces.\nIn 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5543–5552, June 2016.\n[7] X. P. Burgos-Artizzu, P. Perona, and P. Doll ´ar. Robust face\nlandmark estimation under occlusion. In Proceedings of the\n2013 IEEE International Conference on Computer Vision ,\nICCV ’13, pages 1513–1520, Washington, DC, USA, 2013.\nIEEE Computer Society.\n[8] C. Cao, Y . Weng, S. Lin, and K. Zhou. 3d shape regres-\nsion for real-time facial animation. In ACM Transactions on\nGraphics, 2013.\n[9] X. Cao, Y . Wei, F. Wen, and J. Sun. Face alignment by ex-\nplicit shape regression. International Journal of Computer\nVision, 107(2):177–190, 2014.\n[10] X. Cao, Y . Wei, F. Wen, and J. Sun. Face alignment by ex-\nplicit shape regression. Int. J. Comput. Vision, 107(2):177–\n190, Apr. 2014.\n[11] H. Chui and A. Rangarajan. A new point matching algo-\nrithm for non-rigid registration. Comput. Vis. Image Un-\nderst., 89(2-3):114–141, Feb. 2003.\n[12] T. Cootes, K. Walker, , and C. Taylor. View-based active\nappearance models. In IEEE Intl Conf. and Workshops on\nAutomatic Face and Gesture Recognition, 2015.\n[13] T. F. Cootes, G. J. Edwards, C. J. Taylor, et al. Active ap-\npearance models. IEEE Transactions on pattern analysis and\nmachine intelligence, 23(6):681–685, 2001.\n[14] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham. Ac-\ntive shape models-their training and application. Computer\nvision and image understanding, 61(1):38–59, 1995.\n[15] D. Cristinacce and T. Cootes. Automatic feature localisa-\ntion with constrained local models. Pattern Recognition,\n41(10):3054–3067, 2008.\n[16] M. Dantone, J. Gall, G. Fanelli, and L. Van Gool. Real-time\nfacial feature detection using conditional regression forests.\nIn Computer Vision and Pattern Recognition (CVPR), 2012\nIEEE Conference on, pages 2578–2585. IEEE, 2012.\n[17] C. N. Duong, K. Luu, K. G. Quach, and T. D. Bui. Be-\nyond principal components: Deep boltzmann machines for\nface modeling. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2015.\n[18] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.\nMulti-pie. In Proceedings of the IEEE International Con-\nference on Automatic Face and Gesture Recognition . IEEE\nComputer Society, September 2008.\n[19] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.\nMulti-pie. Image Vision Comput. , 28(5):807–813, May\n2010.\n[20] L. Gu and T. Kanade. 3d alignment of face in a single im-\nage. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2006.\n[21] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective\nface frontalization in unconstrained images. In 2015 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 4295–4304, June 2015.\n[22] J. Heo and M. Savvides. Gender and ethnicity speciﬁc\ngeneric elastic models from a single 2d image for novel 2d\npose face synthesis and recognition. IEEE Trans. Pattern\nAnal. Mach. Intell., 34(12):2341–2350, Dec. 2012.\n[23] P. L. Hsieh, C. Ma, J. Yu, and H. Li. Unconstrained real-\ntime facial performance capture. In 2015 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n1675–1683, June 2015.\n[24] IEEE. A 3D Face Model for Pose and Illumination Invariant\nFace Recognition, Genova, Italy, 2009.\n[25] M. Jaderberg, K. Simonyan, A. Zisserman, and\nK. Kavukcuoglu. Spatial transformer networks. CoRR,\nabs/1506.02025, 2015.\n[26] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. In Proceedings of\nthe ACM International Conference on Multimedia , pages\n675–678. ACM, 2014.\n[27] A. Jourabloo and X. Liu. Pose-invariant 3d face alignment.\nIn 2015 IEEE International Conference on Computer Vision\n(ICCV), pages 3694–3702, Dec 2015.\n[28] M. Koestinger, P. Wohlhart, P. M. Roth, and H. Bischof. An-\nnotated facial landmarks in the wild: A large-scale, real-\nworld database for facial landmark localization. In First\nIEEE International Workshop on Benchmarking Facial Im-\nage Analysis Technologies, 2011.\n[29] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems , pages\n1097–1105, 2012.\n[30] V . Le, J. Brandt, Z. Lin, L. Bourdev, and T. S. Huang. Inter-\nactive facial feature localization. In Proceedings of the 12th\nEuropean Conference on Computer Vision - Volume Part\nIII, ECCV’12, pages 679–692, Berlin, Heidelberg, 2012.\nSpringer-Verlag.\n[31] B. Li, C. Shen, Y . Dai, A. van den Hengel, and M. He. Depth\nand surface normal estimation from monocular images using\nregression on deep features and hierarchical crfs. In 2015\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 1119–1127, June 2015.\n9\n[32] F. Liu, C. Shen, and G. Lin. Deep convolutional neural\nﬁelds for depth estimation from a single image. In 2015\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 5162–5170, June 2015.\n[33] F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from sin-\ngle monocular images using deep convolutional neural ﬁelds.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 38(10):2024–2039, Oct 2016.\n[34] I. Masi, S. Rawls, G. Medioni, and P. Natarajan. Pose-\naware face recognition in the wild. In2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n4838–4846, June 2016.\n[35] U. Prabhu, J. Heo, and M. Savvides. Unconstrained pose-\ninvariant face recognition using 3d generic elastic models.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 33(10):1952–1961, Oct 2011.\n[36] S. Ren, X. Cao, Y . Wei, and J. Sun. Face alignment at\n3000 fps via regressing local binary features. In IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR) , June\n2014.\n[37] A. Roy and S. Todorovic. Monocular depth estimation using\nneural regression forest. In 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2016.\n[38] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic.\n300 faces in-the-wild challenge: The ﬁrst facial landmark lo-\ncalization challenge. In 2013 IEEE International Conference\non Computer Vision Workshops, pages 397–403, Dec 2013.\n[39] S. Saito, T. Li, and H. Li. Real-Time Facial Segmentation\nand Performance Capture from RGB Input, pages 244–261.\nSpringer International Publishing, Cham, 2016.\n[40] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. ICLR, 2015.\n[41] G. Tzimiropoulos. Project-out cascaded regression with an\napplication to face alignment. In IEEE Conf. on Computer\nVision and Pattern Recognition (CVPR), June 2015.\n[42] S.-F. Wang and S.-H. Lai. Efﬁcient 3D Face Reconstruction\nfrom a Single 2D Image by Combining Statistical and Geo-\nmetrical Information, pages 427–436. 2006.\n[43] P. P. X. P. Burgos-Artizzu and P. Dollr. Robust face landmark\nestimation under occlusion. In IEEE International Confer-\nence on Computer Vision, June 2013.\n[44] X. Xiong and F. De la Torre. Supervised descent method\nand its applications to face alignment. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 532–539, 2013.\n[45] X. Xiong and F. De la Torre. Global supervised descent\nmethod. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 2664–2673,\n2015.\n[46] X. Yu, J. Huang, S. Zhang, W. Yan, and D. Metaxas. Pose-\nfree facial landmark ﬁtting via optimized part mixtures and\ncascaded deformable shape model. In Proceedings of the\nIEEE International Conference on Computer Vision , pages\n1944–1951, 2013.\n[47] X. Yu, J. Huang, S. Zhang, W. Yan, and D. N. Metaxas. Pose-\nfree facial landmark ﬁtting via optimized part mixtures and\ncascaded deformable shape model. In 2013 IEEE Interna-\ntional Conference on Computer Vision , pages 1944–1951,\nDec 2013.\n[48] X. Yu, F. Zhou, and M. Chandraker. Deep deformation\nnetwork for object landmark localization. arXiv preprint\narXiv:1605.01014, 2016.\n[49] Y . Zheng, C. Zhu, K. Luu, C. Bhagavatula, T. H. N. Le, and\nM. Savvides. Towards a deep learning framework for un-\nconstrained face detection. In 2016 IEEE 8th International\nConference on Biometrics Theory, Applications and Systems\n(BTAS), pages 1–8, Sept 2016.\n[50] C. Zhu, Y . Zheng, K. Luu, and M. Savvides. CMS-RCNN:\ncontextual multi-scale region-based CNN for unconstrained\nface detection. CoRR, abs/1606.05413, 2016.\n[51] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Z. Li. Face alignment\nacross large poses: A 3d solution. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n146–155, June 2016.\n[52] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-ﬁdelity\npose and expression normalization for face recognition in the\nwild. In 2015 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 787–796, June 2015.\n[53] X. Zhu and D. Ramanan. Face detection, pose estima-\ntion, and landmark localization in the wild. In IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR) , June\n2012.\n[54] X. Zhu and D. Ramanan. Face detection, pose estimation,\nand landmark localization in the wild. In 2012 IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n2879–2886, June 2012.\n10",
  "topic": "Image warping",
  "concepts": [
    {
      "name": "Image warping",
      "score": 0.848733127117157
    },
    {
      "name": "Landmark",
      "score": 0.7674472332000732
    },
    {
      "name": "Computer science",
      "score": 0.7559657692909241
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7049391269683838
    },
    {
      "name": "Computer vision",
      "score": 0.6160886883735657
    },
    {
      "name": "Thin plate spline",
      "score": 0.5075763463973999
    },
    {
      "name": "Transformer",
      "score": 0.4356597661972046
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.4285106062889099
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4003838896751404
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Bilinear interpolation",
      "score": 0.0
    },
    {
      "name": "Spline interpolation",
      "score": 0.0
    }
  ]
}