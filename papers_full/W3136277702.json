{
  "title": "Bilingual Dictionary-based Language Model Pretraining for Neural Machine Translation",
  "url": "https://openalex.org/W3136277702",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5102945056",
      "name": "Yusen Lin",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A5106940836",
      "name": "Jiayong Lin",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A5080867831",
      "name": "Shuaicheng Zhang",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A5062495714",
      "name": "Haoying Dai",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2251033195",
    "https://openalex.org/W2971031524",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2963993537",
    "https://openalex.org/W2762484717",
    "https://openalex.org/W2741986357",
    "https://openalex.org/W2765961751",
    "https://openalex.org/W2116261113",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2542860122",
    "https://openalex.org/W2183341477"
  ],
  "abstract": "Recent studies have demonstrated a perceivable improvement on the performance of neural machine translation by applying cross-lingual language model pretraining (Lample and Conneau, 2019), especially the Translation Language Modeling (TLM). To alleviate the need for expensive parallel corpora by TLM, in this work, we incorporate the translation information from dictionaries into the pretraining process and propose a novel Bilingual Dictionary-based Language Model (BDLM). We evaluate our BDLM in Chinese, English, and Romanian. For Chinese-English, we obtained a 55.0 BLEU on WMT-News19 (Tiedemann, 2012) and a 24.3 BLEU on WMT20 news-commentary, outperforming the Vanilla Transformer (Vaswani et al., 2017) by more than 8.4 BLEU and 2.3 BLEU, respectively. According to our results, the BDLM also has advantages on convergence speed and predicting rare words. The increase in BLEU for WMT16 Romanian-English also shows its effectiveness in low-resources language translation.",
  "full_text": "Bilingual Dictionary-based Language Model Pretraining for Neural\nMachine Translation\nYusen Lin*\nUniv. of Maryland\nyusenlin@umd.edu\nJiayong Lin\nUniv. of Maryland\njiayong@umd.edu\nShuaicheng Zhang\nUniv. of Maryland\nzshuai8@umd.edu\nHaoying Dai\nUniv. of Maryland\ndhy@terpmail.umd.edu\nAbstract\nRecent studies have demonstrated a perceiv-\nable improvement on the performance of neu-\nral machine translation by applying cross-\nlingual language model pretraining (Lample\nand Conneau, 2019), especially the Transla-\ntion Language Modeling (TLM). To allevi-\nate the need for expensive parallel corpora by\nTLM, in this work, we incorporate the transla-\ntion information from dictionaries into the pre-\ntraining process and propose a novel Bilingual\nDictionary-based Language Model (BDLM).\nWe evaluate our BDLM in Chinese, English,\nand Romanian. For Chinese-English, we ob-\ntained a 55.0 BLEU on WMT-News’19 (Tiede-\nmann, 2012) and a 24.3 BLEU on WMT’20\nnews-commentary, outperforming the Vanilla\nTransformer (Vaswani et al., 2017) by more\nthan 8.4 BLEU and 2.3 BLEU, respectively.\nAccording to our results, the BDLM also has\nadvantages on convergence speed and predict-\ning rare words. The increase in BLEU for\nWMT’16 Romanian-English also shows its ef-\nfectiveness in low-resources language transla-\ntion.\n1 Introduction\nMany previous works in language representation\nmodels like BERT (Devlin et al., 2018) have shown\nthat prior knowledge is of great importance to\nmany downstream tasks such as natural language\nunderstanding (NLU) and neural machine transla-\ntion (NMT). As one of the most successful Lan-\nguage Models (LMs), cross-lingual language mod-\nels (XLMs) (Lample and Conneau, 2019), which\nconstruct LM upon corpora from multiple lan-\nguages, have demonstrated their ability to reach\nstate-of-the-art performance in NLU as well as\nNMT. However, XLMs like Translation Language\nModel (TLM) require expensive corpus with paral-\nlel sentences for training and such a dataset might\nnot be available in certain languages.\nMeanwhile, a dictionary contains information\nlike Part-of-Speech (POS) and synonyms that hu-\nman frequently refers to. Inspired by this, there\nhave been a few previous works, such as Zhang\nand Zong (2016) and Adams et al. (2017), that\nincorporate bilingual dictionaries into NMT to im-\nprove translation quality and to obtain translations\nof rare words. Zhang and Zong (2016) integrates\nthe dictionary information into the training process\nby mixing words or characters to the correspond-\ning tokens or synthesizing parallel sentences. This\nmethod inevitably introduces ambiguity into the\nmodel due to the ignorance of context and still has\nroom for improvement. As for Adams et al. (2017),\nit incorporates the dictionary information into the\npretraining process and then ﬁne-tunes the model\nwith the pretrained embeddings. Although it also\nevaluates ﬁne-tuning with the entire Long Short-\nTerm Memory Model (Gers et al., 1999), recent\nstudies show Vanilla Transformer (Vaswani et al.,\n2017) performs better on many NMT tasks.\nIn this article, we propose a novel LM named\nBilingual Dictionary-based Language Model\n(BDLM) that learns dictionary information dur-\ning pretraining. By replacing words in a sentence\nwith information from the dictionary, BDLM could\nbe trained on monolingual corpora. Besides, the\nambiguity problem is greatly alleviated because\nBDLM is only pretrained for initialization, which\nis ﬁned-tuned later to better ﬁt the NMT for word\nchoice. As we shall see later, the BDLM beneﬁts\nthe translation tasks in many aspects.\nOur main contributions are as follows:\n• BDLM incorporates rich information from the\ndictionary into the pretraining process. When\nbeing applied to NMT task, a steady increase\nof the BLEU score is observed.\n• BDLM could be trained only by monolingual\ncorpus with dictionary information, which is\narXiv:2103.07040v1  [cs.CL]  12 Mar 2021\nespecially helpful for improving translation\nquality for low-resource languages.\n• BDLM signiﬁcantly improves the translation\nprecision of rare words.\n• BDLM speeds up the convergence of Vanilla\nTransformer for the NMT tasks.\n2 Related Work\nLanguage Model Pretraining and BERT De-\nvlin et al. (2018)’s work has shown great success\nin pretraining a language representation model\nfor many natural language downstream tasks. In\nthis paper, two unsupervised pretraining tasks, the\nMasked LM (MLM) and Next Sentence Prediction\n(NSP), are ﬁrst proposed to train the BERT. The\nMLM is for getting better token representations\nbidirectionally while the NSP is for capturing the\nsentence-level relationships. The MLM randomly\nmasks a small ratio of tokens and set those masked\ntokens to be the ground truth of the corresponding\nsentences; the NSP classiﬁes the actual next sen-\ntence from a randomly selected sentence. However,\nthose LMs do not consider multi-lingual informa-\ntion for NMT.\nCross-lingual Language Model (XLM) Pre-\ntraining Taking advantage of BERT’s success\nby LM pretraining, Lample and Conneau (2019)\nproposes the idea of XLM pretraining. They pro-\npose a Translation Language Model (TLM) to take\naccount of multi-lingual information by simply\nconcatenating two parallel sentences processed by\nMLM. By masking words randomly in sentences of\nboth the source side and target side, the model can\nattend the surrounding source context as well as the\ntarget context. The TLM is typically used in com-\nbination with MLM. Although it achieves several\nstate-of-the-art results on NLU, unsupervised NMT\ntasks, supervised NMT tasks, and etc, it requires a\nlarge number of expensive parallel corpora, which\nare not available for low-resource languages.\nCompared to TLM, our model is able to train\non considerable monolingual corpora by introduc-\ning the dictionary information to the pretraining\nprocess. Opposed to Ren et al. (2019), we do\nnot explicitly build up a look-up table, which is\nlikely to bring additional bias by the errors of the\nlook-up table. Ren et al. (2019) proposes another\nXLM named cross-lingual masked language model\n(CMLM), which introduces a complicated N-gram\nembedding procedure and an N-gram translation\ntable. Another similar task of ours is known as\ncross-lingual word embedding (Klementiev et al.,\n2012), which aims to learn cross-lingual word em-\nbeddings by pretraining with a monolingual corpus\nwith the translation information from the dictio-\nnary.\n3 Bilingual Dictionary-based Language\nModel\nIn this section, we will give a detailed introduc-\ntion to the proposed BDLM. BDLM is a multi-LM\nmodel that three LM objectives are pretrained on\none uniﬁed architecture. After pretraining, it is ﬁne-\ntuned for NMT tasks with initialization with all\nthe pretrained parameters including the embedding\nlayers, encoder, and decoder. No dependency on\nparallel corpora and representation with rich prior\nknowledge are its advantages. Although BDLM\ncan be applied to any auto-regressive encoder-\ndecoder architecture models, we opt for implemen-\ntation with Vanilla Transformer.\n3.1 Input Representations\nFollowing Lample et al. (2017), we share the same\nvocabulary for all languages and leverage part of\ntheir input settings. Figure 1 demonstrates an ex-\nample of the input representations during training.\nInput of Encoder There are three types of em-\nbeddings at the input of the encoder: token embed-\ndings, type embeddings, and position embeddings.\nFor position embeddings, we use the sinusoidal po-\nsition encoding in Vaswani et al. (2017) while the\nother two embeddings are learned. The type em-\nbeddings are used to denote the information type\nof the corresponding token. It is extended from\nlanguage embeddings so that BDLM can adapt to\nmore information besides translation.\nInput of Decoder Except for token embeddings\nand type embeddings, there are two types of posi-\ntion embeddings at the input of the decoder: hard\nposition embeddings and soft position embeddings.\nThe hard position embeddings denote the abso-\nlute position of each token at the decoder input\nsequence and are used to record the order infor-\nmation. Meanwhile, the soft position embeddings\ndenote the ﬁrst position of the masked or replaced\nwords/phrases at the input of the encoder and are\nused to record their corresponding positions in the\noriginal sentence. For example, in the IPLM inside\n2\nFigure 1: Three LM objectives of BDLM with translation information.\nﬁgure 1 (discussed in Section 3.2), the ”活泼开朗”\ncan be translated to ”lively and cheerful”. The soft\npositions for ”lively and cheerful” are the ﬁrst posi-\ntion of the ”活泼开朗” in the encoder input, which\nhelp the model attend the ”活泼开朗”; while the\nhard positions help specify the relative positions in\nthe phrase ”lively and cheerful”. It’s worth noting\nthat the soft position embeddings are only exploited\nduring pre-training and will be dropped when being\nﬁne-tuned for NMT tasks.\n3.2 Multi-language-model Objectives\nBDLM contains three various LM objectives:\nMasked Language Model(MLM), Replaced Lan-\nguage Model (RLM), and Information Prediction\nLanguage Model (IPLM). Figure 1 shows these ob-\njectives for incorporating translation information\ninto the pretraining process.\nMLM: As proved by Lample and Conneau\n(2019), combining MLM with other LM objectives\ncould achieve a better representation than only us-\ning one LM objective. We leverage the strength of\nmulti-LM but only apply the MLM for the words\nthat can be mapped by the dictionary. A portion of\nthose words is randomly masked out with special\ntokens [mask] and then serve as the ground truth of\nthe corresponding input sequence. Then, the neural\nmodel is trained to predict the masked words from\ntheir contexts.\nRLM: The difference between the MLM and the\nRLM is that the replaced tokens are no longer the\nspecial mask tokens. Instead, tokens representing\nthe information from the dictionary are placed into\nthe masked positions. For the case of incorporating\ntranslation information into the BDLM, we replace\nthe masked tokens of the input sentence with the\ncorresponding translation tokens. In this case, the\nLM is forced to recover the original words from\nthe replaced translations and the context so that the\ntranslation knowledge could be integrated into the\nBDLM to some extent.\n3\nIPLM: Compared to the MLM, One distinct fea-\nture of the IPLM is that the LM learns to predict\nthe linguist information of the original tokens in-\nstead of merely the original masked tokens from\nthe context. In case the masked token has multi-\nple information, which is likely to happen in many\nscenarios like a word has multiple translations or\nPOS. we introduce a special separation token [sep],\nwhich aims to concatenate various information. For\nthe translation case of BDLM in Figure 1, the ”活\n泼开朗” in the sentence of the encoder input is\nmasked and the ground truth becomes a sequence\nof ”lively and cheerful” and ”outgoing”, divided\nby [sep] tokens. IPLM is trained to build up the\nconnections between the contextual information of\none word to its corresponding translation.\nHowever, IPLM brings up ambiguity because\ndictionary translation does not take account of con-\ntextual information. The ambiguity can be allevi-\nated because IPLM is only pretrained for initializa-\ntion. Besides, getting the knowledge of a similar\nrepresentation is beneﬁcial. Intuitively, IPLM is\na simulation of human learning a new language.\nWhen a human learns a new language by starting\nwith the vocabulary, he/she is probably learning\nnon-native and imprecise knowledge. It is easier to\nacquire the correct knowledge when a human gets a\nnew language environment. This process maps the\nﬁne-tuning stage. he/she can realize the vocabulary\nlearned before is helpful in the new environment.\n3.3 Training schedule\nThe multiple LM objectives are trained on one\nuniﬁed architecture with the same cross-entropy\nloss function with label smoothing (Szegedy et al.,\n2016). Before training, samples for each LM are\nrandomly drawn with a ﬁxed ratio to form a com-\nbined training set. For the model to distinguish the\nvarious LMs, distinct special start tokens and end\ntokens are added to the start and the end position\nof the input sentences for each LM. The dataset is\nthen shufﬂed and fed into the model. Finally, the\nmodel is able to automatically switch the pretrain-\ning process among multiple LMs according to the\ndata being fed.\n3.4 Integrating Dictionary Information\nFigure 1 illustrates an example of integrating trans-\nlation information into BDLM. Besides translation,\nBDLM can be extended to various information\nsuch as POS, synonyms, deﬁnition, and named\nentity (NE). To incorporate the POS or NE infor-\nmation, for the RLM, we could replace the masked\ntokens of a word or phrase with the corresponding\nPOS tags or NE tags. B, M, E, O are the preﬁx\nof tags for the beginning, middle, ending, and no-\nchunk tokens respectively. As for the IPLM, the\nground truth becomes the sequence of those POS\nor NE tags. The same methods also apply to syn-\nonyms and deﬁnitions.\n4 Experiment Settings\n4.1 Datasets\nDataset Train Val Test\nWMT-News’19 Zh-En 16k 2k 2k\nWMT’20 Zh-En(NC ) 300k 6k 6k\nWMT’16 Ro-En(6k) 5.4k 0.6k 0.6k\nWMT’16 Ro-En(60k) 54k 6k 6k\nTable 1: Statistics of the division of the dataset. The\nWMT’20 Zh-En(NC ) represents the WMT’20 Zh-En\nnews-commentary dataset. Follow Gu et al. (2018), the\nWMT’16 Ro-En(6k) and WMT’16 Ro-En(60k) are the\nsubsets of the WMT’16 Ro-En, containing 6k and 60k\nsentence pairs, respectively. The ”Train”, ”Val”, and\n”Test” columns indicate the count of sentence pairs of\nthe training set, validation set, and test set respectively.\nThe experiments were performed on WMT-\nNews’19 Zh-En (Tiedemann, 2012), WMT’20 Zh-\nEn news-commentary, and WMT’16 Ro-En. The\ndatasets contain 20K, 310K, and 610K bilingual\npairs and there are 510K, 6.9M, and 14M English\ntokens, respectively. Following Gu et al. (2018),\nwe only used 6k and 60k WMT’16 Ro-En sentence\npairs for the training set for the purpose of low-\nresource testing. Table 1 shows the ratio of the\ntraining, validation, and test sets of each dataset.\nFor the pretraining, only the training sets are used,\nand both the source and target sides of them are\nused as monolingual data. Besides, for each set\nof experiments, we built a joint vocabulary with\nvocabulary size 50k, 80k, 90k, and 90k, respec-\ntively. For the Chinese side, Jieba toolkit1 is used\nfor tokenization. After tokenization, we apply the\nSubwordTextEncoder toolkit in Tensorﬂow 2 for\nbyte-pair encoding (Sennrich et al., 2015). Sen-\ntences longer than 60 subwords would be removed\nfrom the dataset.\n1https://github.com/fxsjy/jieba\n2https://www.tensorﬂow.org/datasets/api docs/python/\ntfds/features/text/SubwordTextEncoder\n4\n4.2 Dictionaries\nWe integrated data from multiple sources, including\nFacebook research (Conneau et al., 2017), Face-\nbook Muse3, Wiki-titles4, NLTK wordnet 5, and\nECDICT6. The integrated dictionary consists of\ntranslation, Part-of-Speech (POS), synonyms, def-\ninitions, and named entity (NE) information. We\ncleaned the dictionary by only preserving tokens\nwhose translations occur in the combined corpus.\nThe dictionary remains 760K Chinese tokens, 1.4M\nEnglish tokens, and 120K Romanian tokens after\nthe cleaning process. The dictionary coverage of\nChinese, English and Romanian tokens are 70%,\n59% and 27% respectively, with approximately the\nsame coverage for training, validation and test sets.\n4.3 Sample Rate\nThe sample rate represents the number of repeated\nsampling for the same sentence during pretrain-\ning. For each sentence in pretraining, we perform\nword masking randomly. A higher sample rate\ncan increase the probability that the words in each\nsentence are learned.\n4.4 Evaluations\nPrecrare/dict = 1\nN\nN∑\ni=1\n∑count(yi)\nj=1 [yi,j = ˆyi,j]\ncount(yi)\n(1)\nFor the evaluation of supervised machine trans-\nlation performance, we use case-insensitive BLEU\nscore, precision for rare words (Prec (rare)), and\nprecision for words that appear in the dictionary\n(Prec(dict)), while for the pretraining tasks, we use\nperplexity and accuracy. We used case insensitive\nBLEU from nltk’s corpus bleu toolkit 7. Koehn\nand Knowles (2017) serves as a good foundation\nfor our research for metrics on rare words. Equa-\ntion 1 is used for Prec(rare) and Prec(dict). The N\nstands for the count of words that occurs less than\n10 times when calculating Prec (rare) and stands\nfor the count of words appear in dictionary when\ncalculating Prec(dict). yi and ˆyi,j respectively rep-\nresent the ground truth and corresponding predicted\nwords. The bracket returns 1 if yi equals to ˆyi,j,\nand returns 0 otherwise.\n3https://ai.facebook.com/tools/muse/\n4http://data.statmt.org/wikititles/\n5https://www.nltk.org/howto/wordnet.html\n6https://github.com/skywind3000/ECDICT\n7https://www.nltk.org/ modules/nltk/translate/bleu score.html\n4.5 Training Details\nIn all experiments, following Vaswani et al. (2017);\nLample and Conneau (2019) we use a vanilla Trans-\nformer architecture with 8 heads, 6 layers for the\nencoder, 6 layers for the decoder, ReLU activations,\na dropout rate of 0.1, and sinusoidal positional em-\nbeddings. For the reasons of training time and\nthe NMT performance of Vanilla Transformer on\nWMT-News’19 Zh-En, we choose 128 of hidden\nunits and of embedding size. The training set for\nNMT is used for pretraining. We train our models\nwith the Adam optimizer (Kingma and Ba, 2014),\nset the initial learning rate to 1e-4 and the mini-\nbatch size is 16. To build a competitive baseline\nmodel, we use a shared token embedding layer and\ntype embedding layer described in section 3 for\nboth encoder and decoder. Besides, Label smooth-\ning with ϵ= 0.1 is added for the cross-entropy loss.\nGreedy decoding is used and the average accuracy\nof the translated tokens in the validation set is used\nas a stopping criterion for training.\n5 Results\nIn this section, we empirically present BDLM’s\nﬁne-tuning results and demonstrate the effective-\nness of NMT, which shows our approach signiﬁ-\ncantly outperforms Vanilla Transformer and TLM.\n5.1 Golden information in Dictionary\nmodels BLEU Prec (rare) Prec(dict)\nVanilla 46.6 46.5 54.2\nMLM 46.7 46.8 54.3\nTLM 48.8 47.5 55.8\ntranslate 55.0 53.7 61.4\nPOS 44.5 45.5 52.3\nsynonyms 47.7 48.0 56.0\ndeﬁnitions 37.4 34.4 42.2\nNE 51.0 52.2 59.2\nTable 2: Supervised NMT Results on WMT-News’19\nZh-En. BLEU, precision of rare words, and preci-\nsion of words in the dictionary are used for evalua-\ntion. Models pretrained with different schemes are eval-\nuated. Vanilla Transformers without pretraining, with\nMLM pretraining, and with TLM pretraining are used\nas baselines. For BDLM, we investigate translation,\nPOS, synonym, deﬁnition, and NE information to ex-\namine which information is beneﬁcial to NMT.\nA typical bilingual dictionary contains rich in-\nformation includes but not limited to: translation,\nPOS, synonyms, deﬁnition, and named entity (NE).\n5\ntasks size epochs acc ppl\nMLM 80k 5 5.2 3.08\nTLM 91k 11 14.9 5.14\ntranslate 132k 80 13.3 2.06\nPOS 71k 8 12.1 2.28\nsynonyms 12k 66 7.4 2.15\ndeﬁnitions 23k 165 48.8 2.25\nNE 142k 48 17.2 2.03\nTable 3: Statistics and Performance of Different pre-\ntrain Tasks.For each model, we included the training\ndata size, number of epochs the pretraining runs, pre-\ntraining’s token-level accuracy, and perplexity.\nThe translation information naturally ﬁts the NMT\ntask because the pretraining process could build a\nconnection between the translation representation\nand the corresponding context; synonyms could\nalso lead to a better-generalized model with diver-\nsiﬁed translations; NE information helps the model\nto locate and recognize the entity.\nIn Table 2, we evaluate ﬁve BDLMs, each one\nleverages one distinct information from the dictio-\nnary, which is the translation, POS, synonym, deﬁ-\nnition, and NE information, respectively. To prove\nthe beneﬁt of incorporating information from dic-\ntionary into pretraining, we include three baselines:\nVanilla Transformers without pretrain, with MLM\npretrained, and with TLM pretrained. We evaluate\non WMT-News’19 Zh-En and use BLEU, preci-\nsion of rare words, and precision of words in the\ndictionary as metrics. Our results show that BDLM\nwith translation information (BDLMtranslate) gets\na superior performance of 55.0 BLEU over other\nBDLMs, which proves that the translation infor-\nmation is the most valuable for NMT. Besides\nBDLMtranslate, BDLMNE also signiﬁcantly out-\nperforms the three baselines. To better interpret\nthe results, Table 3 shows the statistics and perfor-\nmance of the models. The data sizes vary among\ndifferent tasks due to the various size of informa-\ntion; the epochs vary because the models early stop\nat different epochs automatically. One of the fac-\ntors that contribute to the better performance of\nBDLMtranslate and BDLMNE is their large data\nsize. However, although BDLM synonym has the\nsmallest data size, it still improves the Vanilla\nand MLM by 1.0+ BLEU; BDLMtranslate has the\nsecond-largest data size but performs the best on\nNMT. It proves that the data size is related to the\neffect of pretraining, but the relevance to the transla-\ntion task itself is more important. The BDLMPOS\nperforms poorly because of the under-ﬁtting prob-\nlem. Due to the noisiness and small size of data,\nonly 8 epochs were trained for BDLMPOS . Mean-\nwhile, the BDLMdefinition suffers from the over-\nﬁtting problem. BDLMdefinition trained too many\nepochs and the discrepancy between the deﬁni-\ntion and translation information is huge, leading to\nworse performance than Vanilla. Additionally, our\nempirical experience shows the token-level accu-\nracy and perplexity of pretraining have no direct\nrelationship with the NMT performance but only\ndemonstrate how well the task has been trained.\n5.2 Effect of Sample Rate\nsamplerate\nmodels 3.0 10.0\nBDLMtranslation 38.4 55.0\nBDLMNE 41.2 51.0\nTable 4: BLEU for BDLMs with Different Sample\nRate. We tested BDLMtranslate and BDLMNE with a\nsample rate of 3.0 and 10.0.\nIn Table 4, we highlight experiments on trans-\nlation and NE models with a sample rate of 3.0\nand 10.0. Based on experimental results, a higher\nsample rate yields better performance.\n5.3 Evaluation for Convergence Speed\nTo measure the effectiveness of pretraining,\nwe compared the Convergence Speed of\nBDLMtranslate with Vanilla. The train and\ntest data are from WMT’20 Zh-En news-\ncommentary. Figure 4 shows the obvious\nadvantages of BDLM translate in convergence\nspeed. BDLM translate−10.0 achieves 23.0\nBLEU at the 20th epoch, which is higher than\nBDLMtranslate−0.5 and Vanilla at the 100th epoch.\nIt implies that the model has learned abundant\neffective representation during pretraining.\n5.4 Effect on Translating Rare words\nWe further evaluate the precision of translated\nwords with different occurrence frequencies with\nthe BDLMtranslate in section 5.1. As shown in Fig-\nure 3, it compares the precision performance of the\nVanilla Transformer with the BDLMtranslate. The\nx-axis of the ﬁgure represents the training dataset\noccurrence groups and is set at a logarithmic scale.\nThe left portion of the graph corresponds to rarer\nwords. The y-axis corresponds to the ratio of words\nfound in the predicted sentences compared to the\n6\nFigure 2: Examples of Translation Results. We extracted 3 examples from test data to show how the\nBDLMtranslate−10.0 helps the translation quality.\nFigure 3: Precision of the translated words.The left\nportion of the graph corresponds to rarer words. The\nblue indicates Vanilla Transformer model performed\nbetter, the red indicates the BDLMtranslate performed\nbetter, and the magenta is their overlapping regions.\nFigure 4: The Convergence Speed Comparison on\nWMT’20 Zh-En news-commentary. We Calculate\nthe BLEU at multiple epochs during training.\nground truth. The red region is always on the top of\nthe magenta region and there is a bigger red area on\nthe left portion of the ﬁgure, which conﬁrms that\nBDLMtranslate is effective for improving the trans-\nlation of rare words. In addition, the precision of\nrare words and precision of words in the dictionary\nshown in Table 2 also proves that BDLMtranslate\nand BDLMNE have large gains on the translation\nof rare words.\nThrough the model trained in section 5.3, we use\nsome test cases to observe the advantages of BDLM\nin Rare words translation. Figure 2 shows transla-\ntion sentence samples from BDLMtranslation−10.0\nand Vanilla.\nIn the third example, the Chinese phrase ”实施\n了量化宽松方案” can be successfully translated\nto ”implemented a quantitative easing program”\nin BDLMtranslate−10.0 while Vanilla cannot. Four\nwords in the phrase ”实施”, ”量化”, ”宽松” and\n”方案” all appear in the dictionary used in pretrain-\ning. This indicates that pretraining improves the\naccuracy of rare words translation.\nFigures 5 show the attention maps for a Zh-\nEn translation example. The lighter the region\nis, the larger the attention value is. Figure 5.a\nuses Vanilla for translation while Figure 5.b uses\nBDLMtranslate−10.0 in section 5.3. When com-\nparing the two ﬁgures, we can ﬁnd that the di-\nagonal brightness of Figure 5.b is higher than\nthat of Figure 5.a. Especially the upper left cor-\nner area, where the ” 量化宽松方案” locates,\nBDLMtranslate−10.0’s attention is more observ-\nable.\n5.5 Effect for Low-resource Language\nIn previous experiments, we only veriﬁed the ef-\nfectiveness of BDLM on zh and en. Both Chinese\nand English are high-resource languages, mean-\ning that we can obtain a large number of parallel\ncorpus and dictionary resources. Gu et al. (2018)\nuses Multi-lingual NMT with a transfer-learning\nmethod to share lexical and sentence level repre-\nsentations across multiple source languages into\none target language. The main goal is to help low-\nresource languages by sharing the learned model.\nWe adapted their experimental settings to measure\nthe performance of BDLM in a low-resource lan-\nguage. We use WMT’16 Ro-En data as a monolin-\n7\nFigure 5: Attention map for the 3rd example in Figure 2\nmodels ro-en 6k ro-en60k en-ro6k en-ro60k\nVanilla 3.6 33.3 3.5 30.7\nBDLM 5.4 35.6 5.2 32.9\nVanilla(Gu et al., 2018) 1.2 12.1 – –\nMulti-NMT+UnivTok(Gu et al., 2018) 20.1 24.3 – –\nTable 5: BLEU for NMT on WMT’16 Ro-En\ngual resource to pretrain the model, and train NMT\nmodels on 6k and 60k sentence pairs respectively.\nIn addition, we tested our Vanilla and BDLM on\nthe En-Ro task to verify the generalization of the\nmodel. As shown in Table 5, due to our baseline\nboost settings described in Section 4.4, our Vanilla\nexceeds Gu et al. (2018)’s Vanilla signiﬁcantly. The\nBDLM outperforms Vanilla in every experiment. In\nthe 6k training setting, Multi-NMT+UnivTok’s per-\nformance is phenomenal because they used a large\nnumber of parallel corpora of other high-resource\nlanguages. However, the BDLM’s advantages are\nshown in the 60k training setting. It outperforms\nMulti-NMT+UnivTok by 11.3 BLEU. BDLM and\nMulti-NMT+UnivTok both signiﬁcantly improve\nthe performance of low-resource language.\n6 Conclusion\nWe proposed a novel BDLM that incorporates dic-\ntionary information into the pretraining process.\nWe investigated multiple information, including\ntranslation, POS, synonyms, deﬁnitions, and NE\ninformation, and prove that the translation informa-\ntion is the most valuable for NMT tasks. Compared\nwith TLM, our proposed BDLM does not need any\nparallel corpus. Our experiments show that BDLM\npretraining is effective for improving the BLEU\nscores for NMT tasks. Due to random masking dur-\ning pretraining, a higher sample rate can increase\nthe probability that each word of the sentence is\nlearned, thereby improving the BLEU value of the\nmodel. In addition, BDLM’s convergence speed is\nsigniﬁcant faster than Vanilla Transformer. When\npredicting rare words, BDLM also has obvious im-\nprovement for words that appear in the dictionary.\nThrough experiments of WMT’16 Ro-En, we also\nveriﬁed the considerable improvement of BDLM\nin a low-resource language NMT task.\n7 Future Work\nWe are interested in conducting ablation experi-\nments for the three pretrain LMs of BDLM to un-\nderstand how these LMs beneﬁt NMT indepen-\ndently. Besides, we plan to use a large amount of\ndata for pretraining with the support of more GPUs.\nWe also plan to apply a multi-task for incorporating\nthe translation, NE, and synonym information into\nthe pretraining process. Finally, extending the bilin-\ngual to multi-lingual model, especially for those\nlow-resource languages, is in the schedule.\n8\nReferences\nOliver Adams, Adam Makarucha, Graham Neubig,\nSteven Bird, and Trevor Cohn. 2017. Cross-lingual\nword embeddings for low-resource language model-\ning. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers, pages\n937–947.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv ´e J´egou. 2017.\nWord translation without parallel data.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nFelix A Gers, J¨urgen Schmidhuber, and Fred Cummins.\n1999. Learning to forget: Continual prediction with\nlstm.\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K.\nLi. 2018. Universal neural machine translation for\nextremely low resource languages. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 344–354, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nAlexandre Klementiev, Ivan Titov, and Binod Bhattarai.\n2012. Inducing crosslingual distributed representa-\ntions of words. In Proceedings of COLING 2012,\npages 1459–1474.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39, Vancouver. Association for\nComputational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In NeurIPS.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2017. Unsupervised ma-\nchine translation using monolingual corpora only.\narXiv preprint arXiv:1711.00043.\nShuo Ren, Yu Wu, Shujie Liu, Ming Zhou, and\nShuai Ma. 2019. Explicit cross-lingual pre-\ntraining for unsupervised machine translation. In\nEMNLP/IJCNLP.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 2818–2826.\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In Lrec, volume 2012, pages 2214–\n2218.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJiajun Zhang and Chengqing Zong. 2016. Bridging\nneural machine translation and bilingual dictionaries.\nArXiv, abs/1610.07272.\n9",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.7560235261917114
    },
    {
      "name": "Bilingual dictionary",
      "score": 0.6321049928665161
    },
    {
      "name": "Natural language processing",
      "score": 0.582427978515625
    },
    {
      "name": "Computer science",
      "score": 0.5789617896080017
    },
    {
      "name": "Translation (biology)",
      "score": 0.5759444236755371
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49736979603767395
    },
    {
      "name": "Linguistics",
      "score": 0.464255154132843
    },
    {
      "name": "Biology",
      "score": 0.08053797483444214
    },
    {
      "name": "Philosophy",
      "score": 0.06613785028457642
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "topic": "Machine translation",
  "institutions": [
    {
      "id": "https://openalex.org/I66946132",
      "name": "University of Maryland, College Park",
      "country": "US"
    }
  ],
  "cited_by": 1
}