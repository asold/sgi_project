{
    "title": "Ethical framework for AI education based on large language models",
    "url": "https://openalex.org/W4405696644",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2783886869",
            "name": "Yuyang Yan",
            "affiliations": [
                "Guangzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2073720603",
            "name": "Hui Liu",
            "affiliations": [
                "Guangzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2783886869",
            "name": "Yuyang Yan",
            "affiliations": [
                "Guangzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2073720603",
            "name": "Hui Liu",
            "affiliations": [
                "Guangzhou University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4297734170",
        "https://openalex.org/W4389437528",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W6621199667",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2150145438",
        "https://openalex.org/W2942523020",
        "https://openalex.org/W4379256134",
        "https://openalex.org/W1586532344",
        "https://openalex.org/W4402358740",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3143598665",
        "https://openalex.org/W6853517571",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W6812197722",
        "https://openalex.org/W2995481985",
        "https://openalex.org/W3035725276",
        "https://openalex.org/W4362472309",
        "https://openalex.org/W6806256995",
        "https://openalex.org/W4389897623",
        "https://openalex.org/W2884943453",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W4365512576",
        "https://openalex.org/W6758788198",
        "https://openalex.org/W2757528734",
        "https://openalex.org/W4288108708",
        "https://openalex.org/W6607974698",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2788997749",
        "https://openalex.org/W4365601405",
        "https://openalex.org/W2893767808",
        "https://openalex.org/W4321524280",
        "https://openalex.org/W3088409176",
        "https://openalex.org/W4230810468",
        "https://openalex.org/W4298859129",
        "https://openalex.org/W648152870"
    ],
    "abstract": null,
    "full_text": "Vol.:(0123456789)\nEducation and Information Technologies (2025) 30:10891–10909\nhttps://doi.org/10.1007/s10639-024-13241-6\nEthical framework for AI education based on large \nlanguage models\nYuyang Yan1 · Hui Liu1\nReceived: 7 August 2024 / Accepted: 5 December 2024 / Published online: 23 December 2024 \n© The Author(s) 2024\nAbstract\nWith the rapid development of Artificial Intelligence in Education (AIED), ensuring \nthat ethical principles are fully respected and implemented as AI technology drives \neducational innovation has become a pressing issue. Educators are actively explor -\ning and establishing ethical guidelines for AIED, but the fragmented nature of exist-\ning research makes it challenging to develop a comprehensive ethical framework. \nThrough a technical analysis of the AI development process, we found significant \nsimilarities between AI development and human education. Therefore, this study \nuses this analogy to help educators better understand AI principles and identify key \nareas where AIED may lack ethical standards. Consequently, we propose a five-step, \nmulti-layered ethical framework for AIED to guide the specific deployment and \nimplementation of ethical guidelines, and provide a set of ethical review processes \nfor educational policymakers to regulate and review AIED applications.\nKeywords ICT in education · AI in education · Large language models · Ethical \nframework · Ethical review processes\n1 Introduction\nArtificial Intelligence in Education (AIED) is an emerging technology and a rapidly \nexpanding project in recent years, involving numerous ethical issues, including data \nprivacy breaches, intellectual property rights, algorithmic bias, and the generation of \nfalse content. These issues have profound impacts on the fairness of education, stu-\ndents’ mental health, and social inclusion. As a result, education policymakers view \nthe ethical regulation of AIED as a major challenge in today’s educational sector \n(Bommasani et al., 2022; Cardona et al., 2023).\nSince 2024, several Chinese universities have issued notifications requiring grad-\nuation theses to undergo detection for AI-Generated Content (AIGC) to prevent \n * Hui Liu \n hankhuiliu@hotmail.com\n1 School of Education, Guangzhou University, Guangzhou 510006, China\n10892 Education and Information Technologies (2025) 30:10891–10909\nstudents from overly relying on AI-generated content. U.S. higher education insti-\ntutions have also begun checking students’ assignments and theses for AIGC since \n2020. However, the use of AIGC detection tools has raised concerns about data pri-\nvacy and security, bringing widespread attention to the ethical issues in AIED (Chen \net  al., 2023; Wu et  al., 2023). This includes questions about who should oversee \nthe ethical standards for the development of AIED applications and whether these \napplications can be released into the market without inspection and approval from \nrelevant authorities.\nRegarding the ethical issues raised by artificial intelligence, Buitem (2019) sug-\ngests enhancing the transparency of input data, algorithm testing, and decision-\nmaking models to make machine learning algorithms more interpretable (Buitem, \n2019). Gordon (2021) calls for the establishment of regulations to address issues \nsuch as machine bias, legal decisions, and legal responsibilities (Gordon, 2021). In \nMay 2023, the U.S. Department of Education issued four urgent recommendations \non AIED ethical standards, including: (1) using automation technology to advance \nlearning outcomes while protecting human decision-making and judgment; (2) \nreviewing the quality of foundational data in AI models to ensure accurate, contex-\ntually appropriate information is used in educational applications for fair and unbi-\nased pattern recognition and decision-making; (3) examining specific AI technolo-\ngies, such as those used in large educational technologies or systems, to determine \nif they enhance or undermine student fairness; (4) implementing measures to safe-\nguard and promote fairness, including providing human checks and balances and \nrestricting any AI systems and tools that diminish fairness (Cardona et al., 2023). \nIn August 2024, UNESCO (the United Nations Educational, Scientific and Cul-\ntural Organization) released guidelines on AI competence in education, highlight \nthe importance on the ethics of AI. It declared that educators must understand and \napply the essential ethical standards, guidelines, regulations, institutional structures, \nand practical moral principles outlined by AI. These principles originate from the \nswiftly growing domain of AI ethics and its implications for the education industry \n(Cukurova & Miao, 2024).\n2  Methodology\nSince current research on the ethics of AI is still insufficient, we can draw upon and \nreference technology ethics and utilitarian ethics as our methodological framework. \nThe four principles of technology ethics are proper use of technology, responsibil-\nity, fairness, and cost. Among them, “proper use of technology” is the cornerstone \nof technology ethics; “responsibility” is key to ensuring proper use of technology, \nfocusing on the accountability of participants; “fairness” refers to the responsibility \nand requirement for the reasonable distribution of technology; and “cost” encom-\npasses the technological risks and negative effects that must be borne when using \ntechnology (Milano et al., 2023; Xiao, 2023).\nAI is fundamentally a technology, and the principles of technology eth-\nics apply to it as they do to any other technological advancement. “Proper use \n10893\nEducation and Information Technologies (2025) 30:10891–10909 \nof technology” in the context of AI means that its development needs to align \nwith the goal of promoting good, including adherence to ethical standards and \ntransparency principles. This requires the joint efforts of the government, tech \ncompanies, educators, ethicists, and other stakeholders to ensure that AI devel-\nopment meets the standards. “Responsibility” pertains to the obligations of AI \ndevelopers to society and users, including the impact on their rights during \ndevelopment, application, and promotion, and the implementation of measures \nto protect their rights and safety. “Fairness” ensures that AI applications do not \ncontradict the current societal advocacy for educational fairness, such as embed-\nding gender, racial, ethnic, or economic discrimination into AI algorithms. \n“Cost” includes the technological risks and negative effects that AI developers \nshould bear, as well as the consequences of technical errors. Developers should \nnot transfer these costs to users or try to evade responsibility through legal loop-\nholes in user agreements. Users also bear corresponding risks and costs when \nusing AI, which necessitates ethical standards and regulations to reduce these \nrisks (Nasr et al., 2018; Song et al., 2017).\nUtilitarianism is a theory widely applied in the field of ethics, whose core \ntenet is that the correctness of actions depends on whether they produce the \ngreatest happiness or benefit (Mill, 1863). It emphasizes results, advocating for \nthe moral value of actions based on their overall happiness. The development \nand use of AI systems should serve humans, aiming to enhance the maximum \nhappiness or benefit of human groups, rather than prioritizing the interests of \nmachines. Utilizing utilitarian ethics can act as a constraint on machines in the \nface of human-machine conflicts. Mill also pointed out a potential risk of utili-\ntarianism in his book, namely the tyranny of the majority. If the principles of \nutilitarianism are abused, it may lead to the majority using their dominant posi-\ntion to oppress the minority, thereby undermining social fairness and justice. \nTherefore, citation should be used judiciously to ensure that the principles of \nutilitarianism are applied in a manner that respects the rights and interests of all \nindividuals (Mill, 1863).\nIn AIED, evaluating whether an AIED application improves educational \nquality, increases learning efficiency, and expands educational opportunities, \nall while aiming for the greatest benefit for the primary users (students, teach-\ners, education administrators, and parents), rather than prioritizing the interests \nof machines or the companies or individuals controlling them. While there are \ninternal conflicts among students, teachers, education administrators, and par -\nents, their educational goals align—focusing on the comprehensive development \nand success of students. Therefore, AIED ethics can combine the basic prin-\nciples of technology ethics—proper use of technology, responsibility, fairness, \nand cost—with utilitarian ethics to ensure that technological applications not \nonly meet educational goals but also adhere to ethical standards that prioritize \nthe maximum benefit for the primary users of AIED.\n10894 Education and Information Technologies (2025) 30:10891–10909\n3  Current ethical issues in AI in education\nAt present, artificial intelligence companies typically use vast amounts of data \nto train large models. Most of this data comes from publicly available internet \nsources, with a smaller portion consisting of preprocessed datasets and private \ndata. While internet data contains positive and useful information, it also includes \na significant amount of harmful and biased content. The expectation of tech com-\npanies and stakeholders is for AI models to be launched and monetized quickly. \nThis is why the ChatGPT-3 model sometimes provides biased, inaccurate, or \ninappropriate responses. These issues arise because ChatGPT-3 was trained on \nvast amounts of text that may contain biased and inaccurate information. The \ncapabilities of ChatGPT-3 have raised concerns about the ethical implications \nand potential misuse of such powerful language models, including the generation \nof false data and fake news (Kietzmann et al., 2020; Mubarak et al., 2023; Wu \net al., 2023).\nFor example, current AI technology can simulate historical narratives, but \nits primary goal is to generate content that feels real, not necessarily factual. If \nAIED applications are trained on large amounts of inaccurate or fictional data, \nsuch as a history teaching AIED application using historical novels instead of \naccurate historical records, the result could be that students believe the novel’s \ndescriptions are true facts, leading to misinformation and hindering proper his-\ntorical knowledge construction. Once AI models develop “biased values,” tech \ncompanies implement a series of remedial measures, commonly including setting \nfilters and prohibition commands within the system to prevent the model from \noutputting biased or harmful information.\nThis raises another ethical issue: who is responsible for filtering and reviewing \nthe AI’s output, and whether the ethical standards and values of AI tech com-\npanies are fair and unbiased. When AIED determines that certain information \nconflicts with its “filter” mechanism, such as topics related to “war” in history \nlessons, it may lead to the output of misleading or even falsified historical infor -\nmation to students.\nSecondly, there is the question of whether AI companies can use user input to \ncontinue training models, which raises concerns about privacy breaches or the \nmisuse of confidential user information. Currently, technology companies in the \nmarket are divided into two camps. Some companies use user input to further \ntrain their models, a method that significantly enhances the learning efficiency \nand output quality of AI systems. However, this approach can involve sensitive \ninformation, and if mishandled, it could lead to privacy breaches or misuse. To \nreassure users, some tech companies choose not to use user input data for training \nAI models, thereby avoiding potential privacy risks. This method typically relies \non pre-prepared, depersonalized datasets or simulated data. The downside is that \nit may limit the learning effectiveness and application scope of the AI model, as \nthe model cannot learn and iterate from real-time user feedback.\nIn the U.S., some tech companies prohibit employees from uploading com-\npany-related information to large models like ChatGPT. For example, Microsoft \n10895\nEducation and Information Technologies (2025) 30:10891–10909 \nimplemented a new policy banning its employees from using ChatGPT or any \nother third-party chatbots for work-related purposes. This move is driven by con-\ncerns about the potential leakage of internal confidential information. Micro-\nsoft worries that employees might inadvertently input queries containing sensi-\ntive company information, leading to these data being recorded or misused by \nexternal systems (Novet, 2023). Similarly, schools could face data leakage risks \nif they upload internal documents or sensitive information into unvetted AIED \napplications.\nWith Meta (formerly Facebook) first open-sourcing the code for its large AI \nmodel Llama on the internet in 2023, AI industry has also entered an era of rapid \ndevelopment, with various new AIED applications emerging. On January 19, 2024, \nthe European Commission, European Parliament, and Council of the European \nUnion finalized the Artificial Intelligence Act, which states that “AI systems used in \neducation or vocational training, particularly those determining admissions or place-\nments, assigning individuals to various levels of education and training institutions \nor programs, assessing individuals’ learning outcomes, determining the appropriate \nlevel of education for individuals, and impacting the education and training levels \nindividuals will receive or can receive, as well as those monitoring and detecting \nstudent misconduct in exams, should be classified as high-risk AI systems. This is \ndue to their potential to decide on a person’s educational and professional career, \nthereby affecting their livelihood” (European Commission, 2024).\nTherefore, to safeguard the interests and information security of students, parents, \nteachers, and schools, it is crucial to pay special attention to high-risk AIED systems \nthat may pose privacy risks. Establishing ethical norms and regulatory measures in \neducation is both urgent and necessary.\n4  Examples of AI education applications\nWith the widespread use of AI tools like ChatGPT in student assignments and \nexams, American universities have started employing AI plagiarism detection tools \nsuch as Turnitin and ZeroGPT to ensure academic integrity and uphold academic \nstandards. This study compares major AIED applications on the market, including \nChatGPT, Turnitin, and ZeroGPT, and delves into the underlying AI and founda-\ntional models behind them for further analysis and comparison.\n4.1  ChatGPT\nChatGPT is an AI-based conversational generation model designed to simulate nat-\nural language communication and generate contextually relevant dialogue (Brown \net al., 2020). Unlike other language models, ChatGPT focuses on generating con-\nversational text, offering unique advantages and challenges in the educational field. \nChatGPT operates using a transformer architecture similar to other language models, \nlearning from vast amounts of conversational data to understand language context \nand dialogue logic, thereby generating coherent conversation content. Its training \n10896 Education and Information Technologies (2025) 30:10891–10909\nprocess typically involves extensive dialogue data, such as social media conversa-\ntions or online chat logs. In education, ChatGPT is mainly used for simulating dia-\nlogue scenarios and providing personalized learning experiences. Educators can use \nChatGPT to create virtual teaching assistants or intelligent tutors to help students \nsolve problems, answer questions, or offer personalized learning advice. ChatGPT \ncan also be used to simulate conversational learning environments, helping students \nimprove their language communication skills (Baidoo-Anu & Ansah, 2023; Kasneci \net al., 2023).\nHowever, ChatGPT’s use also poses ethical and educational challenges. Since its \ngenerated content might be automated, there are concerns about the accuracy and \nreliability of the information. In educational settings, ChatGPT-generated dialogues \nmight mislead students with inaccurate information, affecting their learning out-\ncomes. Additionally, ChatGPT’s generated content might be influenced by biases, \nleading to unfair or discriminatory dialogues. While ChatGPT can offer personalized \nlearning experiences and real-time teaching support, it might also reduce students’ \ninteraction with real teachers or peers, impacting their social skills and emotional \ndevelopment, and increasing their dependency on technology. Furthermore, Chat-\nGPT’s potential use in completing assignments and exams has raised widespread \nethical concerns. Students might use ChatGPT to complete their work, which, \nalthough improving short-term grades, undermines the authenticity and value of the \nlearning process, as students fail to genuinely grasp knowledge and skills. Teachers \nmight find it difficult to assess whether submitted assignments truly reflect students’ \nabilities, posing a challenge to the fairness and accuracy of educational assessments. \nRelying on AI-generated tools could limit students’ development of independent \nthinking and problem-solving skills, which are core educational goals. Therefore, \ndespite ChatGPT’s potential in simulating dialogue scenarios and providing person-\nalized learning, educators and policymakers need to carefully consider its use and \ndevelop appropriate guidelines and educational strategies. Strengthening real dia-\nlogue and interaction between students, teachers, and peers is essential to promote \ncomprehensive development and effective learning (Ray, 2023).\n4.2  Turnitin\nTurnitin is a cloud-based tool used to detect plagiarism in academic papers and \nother written works. Its core function is to detect plagiarism through its extensive \ndatabase, which includes academic articles, books, student-submitted papers, and \nweb content. When students submit assignments, Turnitin’s algorithm compares the \nsubmitted text with the database content to find similarities. This comparison is typ-\nically based on aspects such as text grammar, vocabulary, and structure. If similari-\nties are found, Turnitin generates a similarity report indicating potential plagiarism.\nTurnitin employs pattern-matching technology to identify plagiarism in texts. \nThis technology does not require traditional “training datasets” but continuously \nupdates its database to include new source materials. Each time a new document \nis added to the database, the system indexes it for future comparisons with sub-\nmitted assignments (Buckley & Cowap, 2013). However, Turnitin is not without \n10897\nEducation and Information Technologies (2025) 30:10891–10909 \nflaws. Its algorithm may produce false positives, mistaking legitimate citations or \nshared texts for plagiarism. Additionally, there are privacy and copyright concerns \nregarding Turnitin’s database. Student submissions might be permanently stored in \nthe database, and unauthorized use could infringe on their privacy and intellectual \nproperty rights (Zaza & McKenzie, 2018). Despite Turnitin’s role in maintaining \nacademic integrity, it faces ethical and educational issues. The opacity of Turnitin’s \nalgorithm and database content prevents users from knowing how their work is pro-\ncessed and compared. Over-reliance on such tools by schools might stifle students’ \ncreative thinking, just as excessive supervision might limit their ability to explore \nand learn independently. Educational institutions should use Turnitin while also pro-\nviding educational guidance, emphasizing not only the consequences of plagiarism \nbut also teaching proper citation practices and independent thinking to ensure stu-\ndents understand the importance of academic integrity. Turnitin should be part of a \nbroader academic integrity education plan rather than the sole regulatory tool.\n4.3  ZeroGPT\nZeroGPT is a relatively new AI detection tool designed to identify whether texts \nare generated by AI, such as ChatGPT. It works by analyzing the linguistic features \nand patterns of texts to determine if they deviate from typical human writing (Liu et \nal., 2023). ZeroGPT is trained by comparing human-written texts with AI-generated \ntexts, focusing on metrics like “perplexity” and “burstiness.” Low perplexity indi-\ncates common text within language models, while high burstiness indicates vocabu-\nlary variations not typical in human writing.\nZeroGPT’s architecture typically involves several key steps. First, it analyzes the \ntext to detect linguistic features such as vocabulary choice, grammatical structure, \nand logical coherence. ZeroGPT then compares these features with pre-trained AI \nmodels to determine similarity with AI-generated texts. Finally, ZeroGPT generates \na confidence score indicating the likelihood that the text was generated by AI. In \neducation, ZeroGPT can be used to detect whether student assignments or papers \ncontain plagiarized or AI-generated content. Educators can use ZeroGPT to bet-\nter identify and prevent academic misconduct, maintaining academic integrity and \nethics. However, ZeroGPT faces challenges and ethical issues. Its accuracy can be \naffected by the model’s training data and algorithm design, leading to potential mis-\njudgments or omissions. Additionally, the use of ZeroGPT might raise privacy and \ndata security concerns, especially when analyzing personal or sensitive information. \nMoreover, ethical dilemmas might arise, such as balancing academic integrity with \nprotecting student privacy rights.\n5  Analysis of foundational models in AI education applications\nIn the field of education, the use of AI in Education (AIED) is becoming increas-\ningly prevalent. BERT and GPT, as two fundamental models of AIED, play a crucial \nrole in the development of machine learning and artificial intelligence by simulating \n10898 Education and Information Technologies (2025) 30:10891–10909\nthe way humans learn language and process information (Bommasani et al., 2022; \nBrown et al., 2020; Devlin et al., 2018). Despite the emergence of more advanced \nand modern models, BERT and GPT remain significant. Analyzing these two “foun-\ndational” models is essential due to their high recognition and widespread use in \nthe industry. By comparing and analyzing the architectures of BERT and GPT, we \ncan better understand their applications in education. This section systematically \ndissects the principles and structures of artificial intelligence by drawing parallels \nbetween AIED and human development and education, helping educators quickly \ngrasp the principles behind AIED and laying the foundation for constructing an ethi-\ncal framework for AIED.\n5.1  The transformer: The fundamental model of AI\nMost foundational AI models are based on the Transformer architecture. The Trans-\nformer is a deep learning model that uses an attention mechanism to weigh the \nimportance of different parts of the input data. This process is akin to how teachers \nidentify which students need extra attention and support and which teaching content \nis most crucial for the students. The attention mechanism enables the model to focus \non the most informative parts of the input data, similar to how teachers highlight key \npoints or difficult concepts during complex explanations to ensure students focus on \nthe most critical information. This model is widely used in natural language process-\ning and computer vision. Since its introduction in 2017 through the paper “Atten-\ntion is All You Need,” Transformers have been extensively studied. They are seen as \nan upgrade to recurrent neural networks and long short-term memory architectures, \noffering advantages like parallel processing (enhancing performance and scalabil-\nity) and bidirectionality (aiding in understanding ambiguous words and references) \n(Hochreiter & Schmidhuber, 1997; Staudemeyer & Morris, 2019; Sutskever et al., \n2011; Vaswani et al., 2017).\nWhy are Transformers so important in AI models? One can describe the ethical \nissues involved in training, processing, and filtering large language models using an \neducational analogy. Training a large language model is like raising a baby, an anal-\nogy that helps understand the complexities involved in the architecture of artificial \nintelligence. A baby’s genetic code and the foundational Transformer architecture in \nan LLM share a profound similarity: both determine future potential and direction.\nA baby’s genetic code carries the characteristics of the parents, determining not \nonly physical traits like eye color and hair type but also potentially influencing future \nhealth, intellectual development, and personality tendencies. Genetic code can be \nseen as a blueprint for the baby’s growth and development, setting certain biologi-\ncal parameters and potentials before birth. Similarly, the Transformer architecture \nused in the foundational model of an LLM plays a decisive role in the model’s func-\ntionality and efficiency. How the Transformer architecture is designed determines \nthe model’s ability to process language, learning patterns, problem-solving capabili-\nties, and efficiency in handling long-range dependencies (a method of considering \nrelationships between distant elements). This architecture includes multiple layers \nof attention mechanisms and feed-forward networks. Its depth (number of layers), \n10899\nEducation and Information Technologies (2025) 30:10891–10909 \nwidth (number of hidden units), and number of attention heads are akin to the \n“genetic code” of the large language model, presetting the model’s processing capa-\nbilities and learning potential. Just as a baby’s genetic code predefines their physi-\ncal and psychological potential, the Transformer architecture determines the model’s \ninformation processing ability and learning depth. The design of the Transformer \nimpacts the model’s performance, applicability, and scalability.\n5.2  Common features of BERT and GPT: The attention mechanism\nBERT and GPT are two widely applied foundational Transformer models, shar -\ning several common features: (1) Both are pre-trained on large unlabelled datasets, \nwhich is similar to how children naturally learn language in a family environment \nbefore entering the school system by listening to parents’ conversations, watching \nTV shows, and daily interactions. This stage lacks a formal curriculum structure, \nand children accumulate knowledge through vast auditory and visual information \n(Brown et  al., 2020; Devlin et  al., 2018; Qiu et  al., 2020). (2) Both adopt a self-\nsupervised learning mechanism, which in the field of artificial intelligence means \nthat the model can guide its own learning without the need for externally provided \ncorrect answers (Liu et  al., 2023). This is similar to how children learn through \ntrial and error during play and daily activities, understanding which behaviors are \nencouraged and which are not by observing the actions, reactions, and results of \nadults and peers. (3) Both can apply the knowledge learned to various downstream \ntasks, such as text classification, sentiment analysis, and content generation, simi-\nlar to how children apply language and social skills in different life scenarios, such \nas solving problems at school, communicating with friends, or helping parents at \nhome. (4) The transformer architecture allows them to handle complex data inputs \nand maintain the contextual relationship of information, which is crucial for under -\nstanding and generating language. This is similar to how children build cognitive \nmodels through continuous interaction, learning to understand complex instructions \nor abstract concepts based on context and applying them to real situations.\nThe original Transformer architecture includes both an encoder and a decoder. \nBoth rely on the attention mechanism. Simply put, the attention mechanism allows \nthe model to focus on important parts of the information while ignoring less relevant \ncontent (Bahdanau et al., 2015; Chorowski et al., 2014; Wang et al., 2018). This is \nsimilar to how teachers guide students to focus on the most critical information in a \nlesson. For example, when explaining complex scientific concepts, teachers might \nuse experiments or specific examples to highlight key points, helping students con-\ncentrate and avoid being distracted by irrelevant details.\nIn Transformer models, the mechanism of “queries,” “keys,” and “values” is \nsimilar to how students search for and connect information during reading compre-\nhension. When asked specific questions about reading material (queries), students \nsearch the text for relevant information (keys) and grasp the meaning of this infor -\nmation (values). For instance, if the question is “Why is the protagonist sad?” stu-\ndents scan the story for emotional descriptions, akin to how the model evaluates the \nrelevance of information and focuses on the most crucial parts.\n10900 Education and Information Technologies (2025) 30:10891–10909\nMore specifically, the attention mechanism allows the model to consider the \ninfluence of other words in a sentence when processing tasks like language transla-\ntion or content generation. This is similar to how students need to focus on key num-\nbers and operators in a math problem while considering how these elements interact \nto find the correct answer. Additionally, the model optimizes its ability to predict \nand generate text by learning the relationships between words. This is comparable \nto how students gradually understand the meaning and usage of words by seeing \nthem used in different contexts multiple times. For example, students might notice \nthat “restaurant” often appears with “eat,” helping them associate “restaurant” with \nrelevant contexts in future encounters.\n5.3  Differences between BERT and GPT: Encoder and decoder applications\nNot all foundational models use the complete encoder and decoder architecture. In \nthe architectures of BERT and GPT, BERT uses only the encoder part, while GPT \nuses only the decoder part (Brown et  al., 2020; Devlin et  al., 2018; Zhong et  al., \n2023). The encoder is responsible for understanding and processing input informa-\ntion, similar to how students absorb knowledge through listening to lessons, reading, \nor observing. BERT uses the encoder to deeply analyze and understand the semantic \nand grammatical structure of input text, similar to how teachers help students under-\nstand the main ideas and details of an article during reading comprehension. The \ndecoder’s role is more like the output stage in a student’s learning process, where \nthey need to express the acquired knowledge through assignments, tests, or other \nforms. GPT focuses on using the decoder to generate coherent text, just as students \ncreate stories or essays in writing class based on what they have learned.\nBERT’s Transformer architecture’s encoder part allows it to understand the \nsemantic and grammatical information of the text. BERT’s output is an embedded \nrepresentation, not a direct prediction result. To utilize these embedded representa-\ntions, additional layers, such as those for text classification or question-answering \ntasks, need to be added on top of BERT. This process is similar to how teachers \nguide students to understand sentences and paragraphs during reading comprehen-\nsion by identifying key words, phrases, and grammatical structures to help them \ngrasp the deep meaning and contextual connections of the text. This is akin to how \nBERT processes and parses text through the encoder. During this process, the model \nis pre-trained by randomly masking certain words in the input sentences (using \n[MASK] tokens) and then predicting these masked words. This is similar to cloze \ntests, where teachers remove certain words from a text and ask students to fill in \nthe blanks based on context clues, practicing and deepening their understanding and \napplication of the language. This method enables BERT to be effectively trained on \nlarge amounts of unlabelled data.\nOn the other hand, GPT, with its billions of parameters, performs exception-\nally well in handling language tasks. BERT and GPT have different application \nscenarios. BERT is more suitable for tasks requiring deep content understand-\ning, such as sentiment analysis, question answering, summarization, and named \nentity recognition, similar to students’ analytical or comprehension learning in \n10901\nEducation and Information Technologies (2025) 30:10891–10909 \nthe classroom. GPT excels at tasks like translation, text generation, and story \ncreation, akin to students exercising creativity in art and writing classes (Zhong \net al., 2023).\nThe outputs of these two models also have distinct characteristics. BERT’s \noutput includes attention information embedded representations that can be fur -\nther used for other tasks, while GPT directly outputs the probability distribution \nof the next word. Since these models are pre-trained, they can be easily reused \nand extended in different fields and tasks. The focus of pre-trained models is cru-\ncial because training these models requires extensive computational resources \nand time, which only a few companies can afford. Pre-trained models can be cus-\ntomized and extended for specific fields and tasks. Sometimes, adding a few extra \n“layers” on top of the model suffices without modifying its internal structure. \nHowever, in some cases, internal layer modifications might be necessary, requir -\ning more training and usually increasing costs. This customization technique, \nknown as transfer learning, allows a general model to be easily applied to other \nfields.\nThe term “large” in large language models refers to the significant expansion in \nparameter scale compared to early small models. Early small models were trained \nwith a limited number of parameters and smaller datasets, making supervision and \nreview relatively easy. Although large language models use network architectures \nand training methods similar to those of smaller pre-trained language models, they \nachieve impressive performance improvements by expanding the scale of parame-\nters, datasets, and computational resources. These models have demonstrated for the \nfirst time the ability of a single model to effectively solve many complex tasks.\nCurrently, there is no clear standard for the minimum parameter scale, but it is \ngenerally believed that models with hundreds of billions, tens of billions, or even \ntrillions of parameters are considered large language models. Some research sug-\ngests that models with billions of parameters after pre-training can also be classified \nas large language models. This expansion raises regulatory issues, as models with \nhundreds of billions or trillions of parameters make supervision and review almost \nimpossible, which is one of the reasons why AI generates fear among people.\nTo address this issue, we have outlined three major directions for AIED (AI in \nEducation) regulation and developed a multi-level AIED ethical framework based \non the creation steps of AI.\nThe three major directions for AIED regulation are: (1) National-level edu-\ncation administrators should have a comprehensive understanding of the model \nframeworks of large AIED applications on the market, ensuring that the model \nframeworks meet the development needs of our country’s AIED and understand-\ning their potential risks. (2) National-level education administrators should control \nthe data sources, quality, and preprocessing methods of AIED models, especially \nthose AIED applications widely used in schools. Educators (teachers and schools) \nshould also fully understand what data was used to train the AIED they are using. \n(3) National-level education administrators should control the self-supervised learn-\ning mechanisms (deep training) of AIED models, ensuring that AIED applications \ncan exhibit reasonable behavior and decision-making abilities when dealing with \ncomplex situations.\n10902 Education and Information Technologies (2025) 30:10891–10909\n6  Steps for constructing an ethical framework for AI in education\nBased on the ethical concern for AIED development, we have designed a multi-\nlevel ethical framework to guide the application of artificial intelligence in edu-\ncation. This framework includes protecting student privacy and data security, \nensuring the transparency and interpretability of AI tools, and respecting human \nvalues and moral principles throughout the educational process. Ensuring that \nAIED applications meet ethical and moral standards helps to enhance the quality \nand fairness of education, rather than causing negative impacts. Starting from the \nselection of the AIED Transformer, the following are the five main steps and an \noverview of each step:\nStep 1: Supervising the “Baby Genes” of AI Models - Oversight of Large Lan-\nguage Model (LLM) Framework Selection\nThis step emphasizes the rigorous evaluation required when selecting founda-\ntional models for AI, whether it be BERT, GPT, or any other base model, ensur -\ning that the chosen framework meets the needs for future AIED development. \nChoosing the appropriate foundational model is crucial for determining AI’s \nfuture performance and potential risks. This process is akin to selecting a spouse \nto shape ideal offspring. Just as a spouse provides the genetic material that affects \nthe future child’s health, intelligence, behavior, and personality, the large lan-\nguage model framework acts as the “genes” of artificial intelligence, determining \nits potential capabilities, performance, and risks.\nWhen humans choose a spouse, they consider genetic traits with the hope of \npassing on desirable genes to their children. They also consider the spouse’s char -\nacter, morality, sense of responsibility, and life values—ethical concerns that are \nequally important. Similarly, when selecting a large language model framework, \ndevelopers are essentially choosing the focus and risk profile for AIED develop-\nment. Therefore, in the field of AIED, it is crucial to supervise the “baby genes” \nof AI from the outset to ensure that early-stage technical development aligns with \nhuman ethical principles and does not introduce potential risks. Additionally, \nincorporating logging technology in AIED development is essential. Logging \ncan provide detailed records of AIED development processes and upload data to \ncloud servers for subsequent auditing and accountability. This ensures transpar -\nency and responsibility throughout the development lifecycle.\nStep 2: Supervising the “Kindergarten” Phase of AI Learning Basic Rules - \nMonitoring AI Model Data Sources and Training\nThis phase emphasizes strict supervision of the data sources, quality, and pro-\ncessing methods used for training AIED models. Ensuring the quality and suit-\nability of training data is crucial, as the data selected directly impacts the behav -\nior and performance of AIED. Just as careful selection and oversight of the “baby \ngenes” of AI are critical, so is the data used during training. The data influences \n10903\nEducation and Information Technologies (2025) 30:10891–10909 \nnot only the AI model’s behavior and performance but also its ethical conduct \nand real-world application effectiveness. Therefore, rigorous supervision of the \nsources and quality of AI training data is indispensable.\nMicrosoft’s study, “Textbook is All You Need,” highlighted the importance of \ndata quality over quantity. Microsoft trained a model named “phi-1” with only \n1.3 billion parameters using “textbook-quality” datasets and generated textbook-like \nexercises using GPT-3.5. They then assessed “phi-1’s” coding ability through the \nHumanEval test, which showed that “phi-1” outperformed other open-source mod-\nels that were ten times larger and used a hundred times more data. This demonstrates \nthat data quality can be more important than data quantity or model parameters. \nMicrosoft suggested that just as a comprehensive, well-crafted textbook provides \nstudents with the necessary knowledge to master a new subject, high-quality data \nsignificantly enhances the proficiency of language models in code generation tasks \n(Gunasekar et al., 2023).\nDatasets, much like student textbooks, provide examples and experiences that \nteach AI models how to understand and handle various tasks. A prompt extracted \nfrom a source dataset serves as a question or command for the AI, guiding its \nresponses or actions. Annotators play a role similar to teachers in the AI’s creation \nprocess by providing manual annotations or using semi-automated tools to offer \nguidance, clearly indicating the desired responses or behaviors. These annotated \ndata points are then used to train the model through supervised learning, helping it \nlearn the correct answers provided by the annotators and fine-tuning its parameters \nto better adapt to specific tasks or data.\nSelecting a high-quality kindergarten teacher for children mirrors the need for \npatience, wisdom, and strategy in educating AIED models during their formative \ntraining phase to help them grow and reach their full potential. During AI training, \ndemonstrating the correct behavior (data) and teaching the model to respond appro-\npriately in various situations through supervised learning (annotators’ expected out-\nputs) is similar to kindergarten children learning basic skills through imitation. It \nis essential to ensure that demonstrated behaviors are positive and ethically sound \nand that annotators (“kindergarten teachers”) are certified. Additionally, incorporat-\ning logging technology is crucial to prevent AI models from learning inappropriate \nbehaviors.\nStep 3: Supervising the “Elementary School” Phase of AI - Oversight of AI \nTraining and Reward Mechanisms\nAfter ensuring that the training data for AI models meets ethical and quality \nstandards, the next step involves gradually guiding the models from simple to com-\nplex tasks to optimize their responses and behaviors. This phase focuses on provid-\ning more in-depth education and training to AIED models to ensure they exhibit \nappropriate behavior and decision-making skills in complex situations. The opti-\nmization training of AIED models involves step-by-step guidance from annotators \n(“elementary school teachers”), progressing from simple to complex tasks. Annota-\ntors evaluate the output of AIED models, ranking the quality of responses from best \nto worst (similar to grading student assignments on a scale from 0 to 100).\n10904 Education and Information Technologies (2025) 30:10891–10909\nThese ranked data are then used to train a reward model, a tool for assessing \nAI performance. The reward model scores each response or behavior of the AI, \nwith the scores directly reflecting the quality of its performance. Through this \ngradual training process, AI models can learn to handle complex tasks. Just as \nelementary school students improve through teacher feedback on assignments, \nAI uses the reward model to score its performance, identifying which behaviors \nare good and which need improvement to drive self-optimization. This process \nclosely mirrors how students enhance their learning through assignment evalua-\ntion and reflection.\nThis phase is crucial for AIED models to learn and adapt to social environ-\nments, ensuring that the guidance they receive is beneficial and ethically sound. \nIt is also essential to ensure that evaluation standards are fair and do not reinforce \nany potential biases. Annotators (“elementary school teachers”) must be certified, \nand all operations should be logged and uploaded to the cloud for transparency and \naccountability.\nStep 4: Supervising the “High School” Phase of AI - Guidance and Monitoring of \nAI Self-Adjustment and Optimization\nAs AIED models mature, they will face more complex challenges. This step \ninvolves regulating the behavior of mature AIED models to ensure they make rea-\nsonable decisions in various situations. After successfully training the reward model, \nannotators can distinguish the quality of AI outputs and effectively reward behaviors \nthat meet expectations. However, training the reward model alone is insufficient for \ncontinuous optimization of AI behavior. The next step is to apply these rewards in a \ndynamic learning process to further adjust and optimize the AI’s behavior strategy.\nThis brings us to Step 4: using Proximal Policy Optimization (PPO) reinforce-\nment learning algorithms to optimize AI behavior based on feedback from the \nreward model. The PPO algorithm helps adjust and refine AI behavior strategies \nso that future actions achieve higher rewards (Schulman et al., 2017). This process \nis akin to teachers adjusting their teaching methods and content based on students’ \nlearning performance to ensure that teaching strategies effectively promote student \nprogress. The optimization process is continuous and iterative, with each strategy \nupdate building on previous learning experiences, gradually approaching an optimal \nstrategy.\nIn education, this mirrors the ongoing process of teaching improvement, where \nteachers continuously optimize their lesson plans based on student feedback and \nlearning outcomes. The PPO algorithm enables AI models to optimize their behavior \nthrough trial and error and continuous learning in complex environments, ultimately \nachieving more efficient decision-making. Reinforcement learning algorithms allow \nAI to adjust strategies based on reward model feedback, optimizing behavior similar \nto how high school students tackle advanced problems in mathematics, learning to \nuse different methods and techniques through practice and feedback.\nWhen using reinforcement learning, it is crucial to ensure that the reward sys-\ntem does not reinforce unethical behavior. Therefore, AI decisions must align with \nsocial ethical standards. Licensed annotators (“high school teachers”) should guide, \n10905\nEducation and Information Technologies (2025) 30:10891–10909 \nsupervise, and log all operations, uploading the information to the cloud for trans-\nparency and accountability.\nStep 5: Supervising adult AI Self-Adjustment and Final Outcome Review-ensur -\ning AI serves Humanity’s Best Interests\nUpon reaching full maturity, AIED models should possess the ability for self-\nadjustment and self-supervision to adapt to ever-changing external environments \nand new challenges. At this stage, AIED models should independently handle com-\nplex problems and self-correct when necessary. When reviewing AIED models, it is \ncrucial to ensure that the development and application of these AI products adhere \nto technological ethics and the core principle of utilitarian ethics—promoting the \ngreatest happiness and benefit for humanity, rather than prioritizing the interests of \nthe machines.\nAIED tools should be designed to enhance educational quality, ensure educa-\ntional equity, and provide personalized learning opportunities for all students. For \nexample, an AI-assisted personalized learning system should adapt to most students’ \nlearning paces and styles, helping them effectively grasp knowledge while also pay -\ning attention to disadvantaged groups, ensuring that technology does not exacerbate \nexisting educational inequalities. Additionally, these systems must strictly protect \nstudent data privacy and prevent data misuse during their design.\nComprehensive reviews ensure that AI educational tools genuinely serve to \nimprove the overall educational level and quality of society. When AIED models are \ndeployed, they should record feedback data from humans and society, including user \nfeedback and societal feedback. User feedback includes data such as code, score val-\nues, input-output pairs, and document contents. Societal feedback comprises code, \nannotated data, documents, media comments, and public opinion analysis. If an \nAIED application continuously updates and learns from user and societal feedback, \nit becomes extremely powerful and potentially unpredictable.\nReturning to a previously mentioned point, using user input for continued model \ntraining can improve AIED learning efficiency and output quality but may involve \nrisks such as privacy breaches, biased information, and the generation of false infor-\nmation. On the other hand, not using user input data for AI model training can avoid \npotential risks but may limit the learning effectiveness and application scope, as the \nmodel cannot learn and iterate from real-time user feedback. Therefore, educators \nmust consider their needs and actual circumstances when choosing an appropriate \nAIED application. Figure  1 illustrates the construction steps of an AIED ethical \nmodel based on GPT LLM.\n7  Discussion and conclusion\nLarge Language Models (LLMs) and annotators (humans) play crucial roles in the \ncreation of AI in Education (AIED). The construction of LLMs determines the \npotential capabilities and risks of AIED at its inception, while the ethics and morals \nof subsequent annotators determine whether the AIED model will benefit or harm \n10906 Education and Information Technologies (2025) 30:10891–10909\nhuman society. If annotators display cultural, gender, or racial biases when organ-\nizing and labeling educational content, these biases will directly affect the content \nof the AIED teaching platform. For example, if an AI for history education is based \non biased annotation data, it may present skewed descriptions of certain historical \nevents or figures, directly impacting students’ learning and worldview.\nIn automatic grading systems, inconsistent standards or personal preferences of \nannotators can affect the consistency and fairness of the model’s grading, thereby \ninfluencing students’ motivation and educational opportunities. When annotators \nhandle data involving students with different learning abilities and needs, they must \npossess a high degree of sensitivity and expertise to ensure that AI education tools \ncan fairly serve all students, including those with special needs. Additionally, pro-\ntecting student privacy is paramount in processing educational data. Annotators \nmust strictly adhere to privacy protection laws and ethical standards to prevent sen-\nsitive information from being disclosed or misused.\nDue to the complexity of AI technology, principals, teachers, students, and par -\nents find it challenging to assess whether an educational AI application meets ethical \nstandards. This necessitates joint regulation of AIED ethics by national and industry \nlevels. First, an AI Education Ethics Committee should be established at the indus-\ntry level, consisting of industry practitioners and educators, to develop a set of AIED \nethical standards and annually nominate companies that meet these standards. Sec-\nond, similar to AI accountants, third-party companies should conduct ethical audits \nof nominated AIED companies, issuing certifications to those that pass. Third, the \nstate should regulate third-party AI audit companies and hold them accountable for \nproblematic audits. By coordinating the “invisible hand” and the “visible hand” of \neconomics (Smith, 2008; Keynes, 2017), a healthy development environment for the \nAIED industry can be created. Furthermore, it is crucial to train auditors and anno-\ntators who possess knowledge of “artificial intelligence moral and ethical norms” \nand meet “artificial intelligence moral and ethical certification” standards, to maxi-\nmize the avoidance of ethical risks in AIED.\nToday, artificial intelligence is categorized in various ways: AlphaGo is con-\nsidered weak AI, ChatGPT is seen as strong AI, and the sentient machines in \nscience fiction movies are regarded as superintelligent AI or Artificial General \nFig. 1  AIED model construction steps and ethical norms detailed flowchart\n10907\nEducation and Information Technologies (2025) 30:10891–10909 \nIntelligence (AGI). AI thinker Nick Bostrom defines AGI as an intellect that is \nmuch smarter than the best human brains in practically every field, including sci-\nentific creativity, general wisdom, and social skills (Bostrom, 2014).\nAGI is predicted to possess intelligence surpassing that of humans. Such AGI \nwould not only perform specific tasks but also have the capability for self-learn-\ning and innovation, understanding and generating natural language, and solving \ncomplex problems. Furthermore, it might have consciousness and emotions. AGI \ncould excel in various fields, including scientific research, artistic creation, and \ndecision-making, profoundly impacting society, the economy, and ethics, poten-\ntially leading to numerous ethical issues. These include (1) Autonomy and Con-\ntrol: Once AGI surpasses human intelligence, will it still be under human con-\ntrol? How can we ensure AGI always prioritizes human interests? (2) Values and \nMoral Standards: AGI might develop its own values and moral systems, which \ncould conflict with human values. How can we reconcile these differences? (3) \nEmployment and Social Impact: AGI could replace many human jobs, exacerbat-\ning employment issues and social inequality. How do we address the employment \ndisruptions and social changes brought about by AI? (4) Privacy and Security: \nAGI’s ability to handle vast amounts of data and its powerful capabilities could \nbe misused for privacy invasions and opinion manipulation. How can we prevent \nthe misuse and abuse of AI?\nIn the future, there may be a game between AI (machines) and humans. This \nbrings us back to the ethical discussion on AIED. Using utilitarian ethics on top \nof technological ethics emphasizes outcomes centered on human well-being. Dur -\ning AI system development, the focus should be on maximizing human happiness \nand benefit, integrating ethical considerations into every aspect of AI systems, \nrather than prioritizing the interests of machines or those who control them. The \ndevelopment of AI technology is not science fiction. This double-edged sword of \nAI presents both opportunities and challenges for humanity’s future. By under -\nstanding and managing AI’s “black box” and annotations, we can trace AI’s \nactions, ensuring system transparency and accountability. Mastering the funda-\nmental principles and logic of AI technology is crucial for guiding the future \ndevelopment of AIED, which is the core intent of this research and our sincere \nhope for human prosperity.\nData availability No data is associated with this study.\nDeclarations \nEthical approval Not applicable.\nConsent to participate Not applicable.\nConsent for publication Not applicable.\nConflict of interest The authors declare that they have no competing interests.\n10908 Education and Information Technologies (2025) 30:10891–10909\nOpen Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDeriv -\natives 4.0 International License, which permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate if you modified the licensed mate-\nrial. You do not have permission under this licence to share adapted material derived from this article or \nparts of it. The images or other third party material in this article are included in the article’s Creative \nCommons licence, unless indicated otherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view \na copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\nReferences\nBahdanau, D., Cho, K., & Bengio, Y. (2015). Neural machine translation by jointly learning to align and \ntranslate. arXiv preprint: https:// doi. org/ 10. 48550/ arXiv. 1409. 0473 \nBaidoo-Anu, D., & Ansah, L. O. (2023). Education in the era of generative artificial intelligence (AI): \nUnderstanding the potential benefits of ChatGPT in promoting teaching and learning. Journal of AI, \n7(1), 52–62. https:// doi. org/ 10. 61969/ jai. 13375 00\nBommasani, R. (2022). On the opportunities and risks of foundation model. arXiv preprint:  https:// doi. \norg/ 10. 48550/ arXiv. 2108. 07258 \nBostrom, N. (2014). Superintelligence: Paths, dangers, strategies. Oxford University Press.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., \nSastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, \nA., Ziegler, D. M., Wu, J., Winter, C., . . . Amodei, D. (2020, May 28). Language Models are Few-\nShot Learners. arXiv.org. https:// doi. org/ 10. 48550/ arXiv. 2005. 14165\nBuckley, E., & Cowap, L. (2013). An evaluation of the use of turnitin for electronic submission and mark-\ning and as a formative feedback tool from an educator’s perspective. British Journal of Educational \nTechnology, 44(4), 562–570. https:// doi. org/ 10. 1111/ bjet. 12054\nBuitem, M. C. (2019). Towards intelligent regulation of artificial intelligence. European Journal of Risk \nRegulation, 10(1), 41–59.\nCardona, M. A., Rodríguez, R. J., & Ishmael, K. (2023). Artificial intelligence and the future of teaching \nand learning insights and recommendations. Office of Educational Technology. https:// tech. ed. gov/ \nfiles/ 2023/ 05/ ai- future- of- teach ing- and- learn ing- report. pdf\nChen, C., Wu, Z., Lai, Y., Ou, W., Liao, T., & Zheng, Z. (2023). Challenges and remedies to privacy and \nsecurity in aigc: Exploring the potential of privacy computing, blockchain, and beyond. arXiv pre-\nprint: https:// doi. org/ 10. 48550/ arXiv. 2306. 00419 \nChorowski, J., Bahdanau, D., Cho, K., & Bengio, Y. (2014). End-to-end continuous speech recognition \nusing attention-based recurrent. arXiv preprint: https:// doi. org/ 10. 48550/ arXiv. 1412. 1602 \nCukurova, M., & Miao, F. (2024). AI competency framework for teachers. UNESCO. https:// doi. org/ 10. \n54675/ ZJTE2 084\nDevlin, J., Ming-Wei, C., Kenton, L., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntansormers for language understanding. arXiv preprint: https:// doi. org/ 10. 48550/ arXiv. 1810. 04805 \nEuropean Commission (2024, August 6). COM/2021/206 final. https:// eur- lex. europa. eu/ legal- conte nt/ \nEN/ TXT/? uri= CELEX: 52021 PC0206\nGordon, J. S. (2021). AI and law: Ethical, legal, and socio-political implications. AI & Society, 36(2), \n403–404.\nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C.C., Giorno, A.D., Gopi, S., Javaheripi, M., Kauffmann, P., \nde Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl, H.S., Wang, X., Bubeck, S., Eldan, R., Kalai, \nA.T., Lee, Y.T., & Li, Y. (2023). Textbooks are all you need. ArXiv, abs/2306.11644.\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), \n1735–1780.\nKasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, \nG., Günnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, \nJ., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., . . . Kasneci, G. (2023). ChatGPT for good? On \n10909\nEducation and Information Technologies (2025) 30:10891–10909 \nopportunities and challenges of large language models for education. Learning and Individual Dif-\nferences, 103, 102274. https:// doi. org/ 10. 1016/j. lindif. 2023. 102274\nKeynes, J. M. (2017). The general theory of employment, interest and money. Wordsworth Editions.\nKietzmann, J., Linda, W. L., Ian, P. M., & Tim, C. K. (2020). Deepfakes: Trick or treat? Business Hori-\nzons, 63(2), 135–146. https:// doi. org/ 10. 1016/j. bushor. 2019. 11. 006\nLiu, X., Zhang, F., Hou, Z., Mian, L., Wang, Z., & Zhang, J. (2023). Self-supervised learning: Generative \nor contrastive. IEEE Transactions on Knowledge and Data Engineering, 35(1), 857–876. https:// doi. \norg/ 10. 1109/ TKDE. 2021. 30908 66\nMilano, S., McGrane, J. A., & Leonelli, S. (2023). Large language models challenge the future of higher \neducation. Nature Machine Intelligence, 5, 333–334. https:// doi. org/ 10. 1038/ s42256- 023- 00644-2\nMill, J. S. (1863). Utilitarianism. London, Parker, son, and Bourn. https:// www. loc. gov/ item/ 11015 966/\nMubarak, R., Alsboui, T., Alshaikh, O., Isa, I. D., Khan, S., & Parkinson, S. (2023). A Survey on the \ndetection and impacts of deepfakes in Visual, Audio, and Textual formats. Ieee Access : Practical \nInnovations, Open Solutions, 11, 144497–144529. https:// doi. org/ 10. 1109/ ACCESS. 2023. 33446 53\nNasr, M., Shokri, R., & Houmansadr, A. (2018). Machine Learning with Membership Privacy Using \nAdversarial Regularization. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and \nCommunications Security (Toronto, Canada) (CCS’18), 634–646. https:// doi. org/ 10. 1145/ 32437 34. \n32438 55\nNovet, J. (2023, November 9). Microsoft briefly restricted employee access to OpenAI’s ChatGPT, citing \nsecurity concerns. CNBC. https:// www. cnbc. com/ 2023/ 11/ 09/ micro soft- restr icts- emplo yee- access- \nto- opena is- chatg pt. html\nQiu, X., Sun, T., Xu, T., Shao, Y., Dai, N., & Huang, X. (2020). Pre-trained models for natural language \nprocessing: A survey. Science China Technological Sciences, 63(10), 1872–1897. https:// doi. org/ 10. \n1007/ s11431- 020- 1647-3\nRay, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key challenges, bias, \nethics, limitations and future scope. Internet of Things and Cyber-Physical Systems, 3, 121–154. \nhttps:// doi. org/ 10. 1016/j. iotcps. 2023. 04. 003\nSmith, A. (2008). An inquiry into the nature and causes of the wealth of nations. Oxford University Press.\nSong, C. Z., Ristenpart, T., & Shmatikov, V. (2017). Machine Learning Models That Remember Too \nMuch. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications \nSecurity (Dallas, Texas, USA) (CCS ’17), 587–601. https:// doi. org/ 10. 1145/ 31339 56. 31340 77\nStaudemeyer, R. C., & Morris, E. R. (2019). Understanding LSTM—a tutorial into long short-term mem-\nory. Recurrent Neural Networks. https:// doi. org/ 10. 48550/ arXiv. 1909. 09586\nSutskever, I., Martens, J., & Hinton, G. E. (2011). Generating text with recurrent neural networks. Pro -\nceedings of the 28th international conference on machine learning (ICML-11).\nVaswani, A., Shazeer, N. M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polo-\nsukhin, I. (2017). Attention is all you need. https:// doi. org/ 10. 48550/ arXiv. 1706. 03762\nWang, S., Hu, L., Cao, L., Huang, X., Lian, D., & Liu, W. (2018). Attention-based transactional context \nembedding for next-item recommendation. Proceedings of the AAAI Conference on Artificial Intel-\nligence, 32(1), 2532–2539. https:// doi. org/ 10. 1609/ aaai. v32i1. 11851\nWu, J., Gan, W., Chen, Z., Wan, S., & Lin, H. (2023). Ai-generated content (AIGC): A survey. arXiv pre-\nprint arXiv:https:// doi. org/ 10. 48550/ arXiv. 2304. 06632\nXiao, F. (2023). From the ethics of technology to the ethics of brain-computer interface(in Chinese). \nStudies in Dialectics of Nature, 08, 63–68. https:// doi. org/ 10. 19484/j. cnki. 1000- 8934. 2023. 08. 009\nZaza, C., & McKenzie, A. (2018).  Turnitin® Use at a Canadian University. The Canadian Journal for the \nScholarship of Teaching and Learning, 9(2).\nZhong, Q., Ding, L., Liu, J., Du, B., & Tao, D. (2023). Can ChatGPT understand too? A comparative \nstudy on ChatGPT and fine-tune BERT. . https:// doi. org/ 10. 48550/ arXiv. 2302. 10198\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations."
}