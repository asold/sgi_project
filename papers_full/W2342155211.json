{
    "title": "Using Sentence-Level LSTM Language Models for Script Inference",
    "url": "https://openalex.org/W2342155211",
    "year": 2016,
    "authors": [
        {
            "id": "https://openalex.org/A1850367186",
            "name": "Karl Pichotta",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2167433806",
            "name": "Raymond J. Mooney",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2185726469",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2578412240",
        "https://openalex.org/W2950340514",
        "https://openalex.org/W2257051837",
        "https://openalex.org/W2121773050",
        "https://openalex.org/W2962883855",
        "https://openalex.org/W2963069010",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2099136972",
        "https://openalex.org/W2000900121",
        "https://openalex.org/W2252215150",
        "https://openalex.org/W2158794898",
        "https://openalex.org/W2250506749",
        "https://openalex.org/W2151295812",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W1855867616",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2963842982",
        "https://openalex.org/W2252139350",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2133280805",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2250767751",
        "https://openalex.org/W2145374219",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2250836735",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W2186953502",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W1508977358",
        "https://openalex.org/W2950094539"
    ],
    "abstract": "There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.",
    "full_text": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 279–289,\nBerlin, Germany, August 7-12, 2016.c⃝2016 Association for Computational Linguistics\nUsing Sentence-Level LSTM Language Models for Script Inference\nKarl Pichotta\nDepartment of Computer Science\nThe University of Texas at Austin\npichotta@cs.utexas.edu\nRaymond J. Mooney\nDepartment of Computer Science\nThe University of Texas at Austin\nmooney@cs.utexas.edu\nAbstract\nThere is a small but growing body of\nresearch on statistical scripts, models of\nevent sequences that allow probabilistic\ninference of implicit events from docu-\nments. These systems operate on struc-\ntured verb-argument events produced by\nan NLP pipeline. We compare these sys-\ntems with recent Recurrent Neural Net\nmodels that directly operate on raw tokens\nto predict sentences, ﬁnding the latter to be\nroughly comparable to the former in terms\nof predicting missing events in documents.\n1 Introduction\nStatistical scripts are probabilistic models of event\nsequences (Chambers and Jurafsky, 2008). A\nlearned script model is capable of processing a\ndocument and inferring events that are probable\nbut not explicitly stated. These models operate on\nautomatically extracted structured events (for ex-\nample, verbs with entity arguments), which are de-\nrived from standard NLP tools such as dependency\nparsers and coreference resolution engines.\nRecent work has demonstrated that standard se-\nquence models applied to such extracted event\nsequences, e.g. discriminative language models\n(Rudinger et al., 2015) and Long Short Term\nMemory (LSTM) recurrent neural nets (Pichotta\nand Mooney, 2016), are able to infer held-out\nevents more accurately than previous approaches.\nThese results call into question the extent to which\nstatistical event inference systems require linguis-\ntic preprocessing and syntactic structure. In an at-\ntempt to shed light on this issue, we compare ex-\nisting script models to LSTMs trained assentence-\nlevel language models which try to predict the\nsequence of words in the next sentence from a\nlearned representation of the previous sentences\nusing no linguistic preprocessing.\nSome prior statistical script learning systems\nare focused on knowledge induction. These sys-\ntems are primarily designed to induce collections\nof co-occurring event types involving the same en-\ntities, and their ability to infer held-out events is\nnot their primary intended purpose (Chambers and\nJurafsky, 2008; Ferraro and Van Durme, 2016, in-\nter alia). In the present work, we instead investi-\ngate the behavior of systems trained to directly op-\ntimize performance on the task of predicting sub-\nsequent events; in other words, we are investigat-\ning statistical models of events in discourse.\nMuch prior research on statistical script learn-\ning has also evaluated on inferring missing events\nfrom documents. However, the exact form that\nthis task takes depends on the adopted deﬁnition\nof what constitutes an event: in previous work,\nevents are deﬁned in different ways, with differ-\ning degrees of structure. We consider simply us-\ning raw text, which requires no explicit syntactic\nannotation, as our mediating representation, and\nevaluate how raw text models compare to models\nof more structured events.\nKiros et al. (2015) introduced skip-thought vec-\ntor models, in which an RNN is trained to en-\ncode a sentence within a document into a low-\ndimensional vector that supports predicting the\nneighboring sentences in the document. Though\nthe objective function used to train networks max-\nimizes performance on the task of predicting sen-\ntences from their neighbors, Kiros et al. (2015)\ndo not evaluate directly on the ability of networks\nto predict text; they instead demonstrate that the\nintermediate low-dimensional vector embeddings\nare useful for other tasks. We directly evaluate the\ntext predictions produced by such sentence-level\nRNN encoder-decoder models, and measure their\nutility for the task of predicting subsequent events.\n279\nWe ﬁnd that, on the task of predicting the text of\nheld-out sentences, the systems we train to operate\non the level of raw text generally outperform the\nsystems we train to predict text mediated by auto-\nmatically extracted event structures. On the other\nhand, if we run an NLP pipeline on the automat-\nically generated text and extract structured events\nfrom these predictions, we achieve prediction per-\nformance roughly comparable to that of systems\ntrained to predict events directly. The difference\nbetween word-level and event-level models on the\ntask of event prediction is marginal, indicating that\nthe task of predicting the next event, particularly\nin an encoder-decoder setup, may not necessarily\nneed to be mediated by explicit event structures.\nTo our knowledge, this is the ﬁrst effort to evalu-\nate sentence-level RNN language models directly\non the task of predicting document text. Our re-\nsults show that such models are useful for pre-\ndicting missing information in text; and the fact\nthat they require no linguistic preprocessing makes\nthem more applicable to languages where quality\nparsing and co-reference tools are not available.\n2 Background\n2.1 Statistical Script Learning\nScripts, structured models of stereotypical se-\nquences of events, date back to AI research from\nthe 1970s, in particular the seminal work of\nSchank and Abelson (1977). In this concep-\ntion, scripts are modeled as temporally ordered\nsequences of symbolic structured events. These\nmodels are nonprobabilistic and brittle, and pose\nserious problems for automated learning.\nIn recent years, there has been a growing body\nof research into statistical script learning sys-\ntems, which enable statistical inference of im-\nplicit events from text. Chambers and Jurafsky\n(2008; 2009) describe a number of simple event\nco-occurrence based systems which infer (verb,\ndependency) pairs related to a particular discourse\nentity. For example, given the text:\nAndrew Wiles won the 2016 Abel prize\nfor proving Fermat’s last theorem,\nsuch a system will ideally be able to infer novel\nfacts like (accept, subject) or (publish, subject) for\nthe entity Andrew Wiles, and facts like (accept, ob-\nject) for the entity Abel prize. A number of other\nsystems inferring the same types of pair events\nhave been shown to provide superior performance\nin modeling events in documents (Jans et al., 2012;\nRudinger et al., 2015).\nPichotta and Mooney (2014) give a co-\noccurrence based script system that models and\ninfers more complex multi-argument events from\ntext. For example, in the above example, their\nmodel would ideally be able to infer a single event\nlike accept(Wiles, prize), as opposed to the two\nsimpler pairs from which it is composed. They\nprovide evidence that modeling and inferring more\ncomplex multi-argument events also yields supe-\nrior performance on the task of inferring simpler\n(verb, dependency) pair events. These events are\nconstructed using only coreference information;\nthat is, the learned event co-occurrence models do\nnot directly incorporate noun information.\nMore recently, Pichotta and Mooney (2016)\npresented an LSTM-based script inference model\nwhich models and infers multi-argument events,\nimproving on previous systems on the task of in-\nferring verbs with arguments. This system can in-\ncorporate both noun and coreference information\nabout event arguments. We will use this multi-\nargument event formulation (formalized below)\nand compare LSTM models using this event for-\nmulation to LSTM models using raw text.\n2.2 Recurrent Neural Networks\nRecurrent Neural Networks (RNNs) are neu-\nral nets whose computation graphs have cycles.\nIn particular, RNN sequence models are RNNs\nwhich map a sequence of inputs x1,...,x T to a\nsequence of outputs y1,...,y T via a learned la-\ntent vector whose value at timestep tis a function\nof its value at the previous timestep t−1.\nThe most basic RNN sequence models, so-\ncalled “vanilla RNNs” (Elman, 1990), are de-\nscribed by the following equations:\nzt = f(Wi,zxt + Wz,zzt−1)\not = g(Wz,ozt)\nwhere xt is the vector describing the input at time\nt; zt is the vector giving the hidden state at time\nt; ot is the vector giving the predicted output at\ntime t; f and g are element-wise nonlinear func-\ntions (typically sigmoids, hyperbolic tangent, or\nrectiﬁed linear units); and Wi,z, Wz,z, and Wz,o\nare learned matrices describing linear transforma-\ntions. The recurrency in the computation graph\narises from the fact that zt is a function of zt−1.\nThe more complex Long Short-Term Mem-\nory (LSTM) RNNs (Hochreiter and Schmidhuber,\n280\nzt\not\nft\nit gt\nzt-1xt\nmt\nFigure 1: Long Short-Term Memory unit at\ntimestep t. The four nonlinearity nodes ( it, gt, ft,\nand ot) all have, as inputs, xt and zt−1. Small cir-\ncles with dots are elementwise vector multiplica-\ntions.\n1997) have been shown to perform well on a wide\nvariety of NLP tasks (Sutskever et al., 2014; Her-\nmann et al., 2015; Vinyals et al., 2015, inter alia).\nThe LSTM we use is described by:\nit = σ(Wx,ixt + Wz,izt−1 + bi)\nft = σ(Wx,f xt + Wz,f zt−1 + bf )\not = σ(Wx,oxt + Wh,izt−1 + bo)\ngt = tanh (Wx,mxt + Wz,mzt−1 + bg)\nmt = ft ◦mt−1 + it ◦gt\nzt = ot ◦tanh mt.\nThe model is depicted graphically in Figure 1. The\nmemory vector mt is a function of both its previ-\nous value mt−1 and the input xt; the vector zt is\noutput both to any layers above the unit (which are\ntrained to predict the output valuesyt), and is addi-\ntionally given as input to the LSTM unit at the next\ntimestep t+ 1. The W∗,∗ matrices and b∗ vectors\nare learned model parameters, and u◦v signiﬁes\nelement-wise multiplication.\n2.3 Sentence-Level RNN Language Models\nRNN sequence models have recently been shown\nto be extremely effective for word-level and\ncharacter-level language models (Mikolov et al.,\n2011; Jozefowicz et al., 2016). At each timestep,\nthese models take a word or character as input,\nupdate a hidden state vector, and predict the next\ntimestep’s word or character. There is also a\ngrowing body of work on training RNN encoder-\ndecoder models for NLP problems. These systems\nﬁrst encode the entire input into the network’s hid-\nden state vector and then, in a second step, decode\nthe entire output from this vector (Sutskever et al.,\n2014; Vinyals et al., 2015; Serban et al., 2016).\nSentence-level RNN language models, for ex-\nample the skip-thought vector system of Kiros\net al. (2015), conceptually bridge these two ap-\nproaches. Whereas standard language models are\ntrained to predict the next token in the sequence of\ntokens, these systems are explicitly trained to pre-\ndict the nextsentence in the sequence of sentences.\nKiros et al. (2015) train an encoder-decoder model\nto encode a sentence into a ﬁxed-length vector\nand subsequently decode both the following and\npreceding sentence, using Gated Recurrent Units\n(Chung et al., 2014). In the present work, we train\nan LSTM model to predict a sentence’s succes-\nsor, which is essentially the forward component\nof the skip-thought system. Kiros et al. (2015)\nuse the skip-thought system as a means of project-\ning sentences into low-dimensional vector embed-\ndings, demonstrating the utility of these embed-\ndings on a number of other tasks; in contrast, we\nwill use our trained sentence-level RNN language\nmodel directly on the task its objective function\noptimizes: predicting a sentence’s successor.\n3 Methodology\n3.1 Narrative Cloze Evaluation\nThe evaluation of inference-focused statistical\nscript systems is not straightforward. Cham-\nbers and Jurafsky (2008) introduced the Narrative\nCloze evaluation, in which a single event is held\nout from a document and systems are judged by\nthe ability to infer this held-out event given the\nremaining events. This evaluation has been used\nby a number of published script systems (Cham-\nbers and Jurafsky, 2009; Jans et al., 2012; Pichotta\nand Mooney, 2014; Rudinger et al., 2015). This\nautomated evaluation measures systems’ ability to\nmodel and predict events as they co-occur in text.\nThe exact deﬁnition of the Narrative Cloze\nevaluation depends on the formulation of events\nused in a script system. For example, Cham-\nbers and Jurafsky (2008), Jans et al. (2012), and\nRudinger et al. (2015) evaluate inference of held-\nout (verb, dependency) pairs from documents; Pi-\nchotta and Mooney (2014) evaluate inference of\n281\nverbs with coreference information about multi-\nple arguments; and Pichotta and Mooney (2016)\nevaluate inference of verbs with noun informa-\ntion about multiple arguments. In order to gather\nhuman judgments of inference quality, the latter\nalso learn an encoder-decoder LSTM network for\ntransforming verbs and noun arguments into En-\nglish text to present to annotators for evaluation.\nWe evaluate instead on the task of directly in-\nferring sequences of words. That is, instead of\ndeﬁning the Narrative Cloze to be the evaluation\nof predictions of held-out events, we deﬁne the\ntask to be the evaluation of predictions of held-out\ntext; in this setup, predictions need not be medi-\nated by noisy, automatically-extracted events. To\nevaluate inferred text against gold standard text,\nwe argue that the BLEU metric (Papineni et al.,\n2002), commonly used to evaluate Statistical Ma-\nchine Translation systems, is a natural evaluation\nmetric. It is an n-gram-level analog to the event-\nlevel Narrative Cloze evaluation: whereas the Nar-\nrative Cloze evaluates a system on its ability to re-\nconstruct events as they occur in documents,BLEU\nevaluates a system on how well it reconstructs the\nn-grams.\nThis evaluation takes some inspiration from the\nevaluation of neural encoder-decoder translation\nmodels (Sutskever et al., 2014; Bahdanau et al.,\n2015), which use similar architectures for the task\nof Machine Translation. That is, the task we\npresent can be thought of as “translating” a sen-\ntence into its successor. While we do not claim\nthat BLEU is necessarily the optimal way of eval-\nuating text-level inferences, but we do claim that\nit is a natural ngram-level analog to the Narrative\nCloze task on events.\nIf a model infers text, we may also evaluate it on\nthe task of inferring events by automatically ex-\ntracting structured events from its output text (in\nthe same way as events are extracted from natural\ntext). This allows us to compare directly to previ-\nous event-based models on the task they are opti-\nmized for, namely, predicting structured events.\n3.2 Models\nStatistical script systems take a sequence of events\nfrom a document and infer additional events that\nare statistically probable. Exactly what constitutes\nan event varies: it may be a (verb, dependency)\npair inferred as relating to a particular discourse\nentity (Chambers and Jurafsky, 2008; Rudinger et\nal., 2015), a simplex verb (Chambers and Juraf-\nsky, 2009; Orr et al., 2014), or a verb with multi-\nple arguments (Pichotta and Mooney, 2014). In\nthe present work, we adopt a representation of\nevents as verbs with multiple arguments (Balasub-\nramanian et al., 2013; Pichotta and Mooney, 2014;\nModi and Titov, 2014). Formally, we deﬁne an\nevent to be a variadic tuple (v,s,o,p ∗), where v\nis a verb, sis a noun standing in subject relation\nto v, o is a noun standing as a direct object to v,\nand p∗ denotes an arbitrary number of (pobj, prep)\npairs, with prep a preposition and pobj a noun re-\nlated to the verb vvia the preposition prep.1 Any\nargument except vmay be null, indicating no noun\nﬁlls that slot. For example, the text\nNapoleon sent the letter to Josephine\nwould be represented by the event ( sent,\nNapoleon, letter, (Josephine, to) ). We rep-\nresent arguments by their grammatical head\nwords.\nWe evaluate on a number of different neural\nmodels which differ in their input and output. All\nmodels are LSTM-based encoder-decoder models.\nThese models encode a sentence (either its events\nor text) into a learned hidden vector state and then,\nsubsequently, decode that vector into its successor\nsentence (either its events or its text).\nOur general system architecture is as follows.\nAt each timestep t, the input token is repre-\nsented as a learned 100-dimensional embedding\nvector (learned jointly with the other parameters\nof the model), such that predictively similar words\nshould get similar embeddings. This embedding is\nfed as input to the LSTM unit (that is, it will be the\nvector xt in Section 2.2, the input to the LSTM).\nThe output of the LSTM unit (called zt in Section\n2.2) is then fed to a softmax layer via a learned\nlinear transformation.\nDuring the encoding phase the network is not\ntrained to produce any output. During the decod-\ning phase the output is a one-hot representation\nof the subsequent timestep’s input token (that is,\nwith a V-word vocabulary, the output will be a\nV-dimensional vector with one 1 and V −1 ze-\nros). In this way, the network is trained to con-\nsume an entire input sequence and, as a second\nstep, iteratively output the subsequent timestep’s\n1This is essentially the event representation of Pichotta\nand Mooney (2016), but whereas they limited events to hav-\ning a single prepositional phrase, we allow an arbitrary num-\nber, and we do not lemmatize words.\n282\nHello\n∅\n</S>\n∅\n<S>\n∅\nLSTM\n<GEN>\n<S>\n<S>\nGoodbye\nGoodbye\n</S>\n</S>\n∅\nInput\nHidden (zt)\nOutput (yt)\nEncoding\n Decoding\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nEmbedding (xt)\nFigure 2: Encoder-Decoder setup predicting the text “Goodbye” from “Hello”\ninput, which allows the prediction of full output\nsequences. This setup is pictured diagrammati-\ncally in Figure 2, which gives an example of in-\nput and output sequence for a token-level encoder-\ndecoder model, encoding the sentence “Hello .”\nand decoding the successor sentence “Goodbye\n.” Note that we add beginning-of-sequence and\nend-of-sequence pseudo-tokens to sentences. This\nformulation allows a system to be trained which\ncan encode a sentence and then infer a successor\nsentence by iteratively outputting next-input pre-\ndictions until the </S> end-of-sentence pseudo-\ntoken is predicted. We use different LSTMs for\nencoding and decoding, as the dynamics of the two\nstages need not be identical.\nWe notate the different systems as follows. Let\ns1 be the input sentence and s2 its successor sen-\ntence. Let t1 denote the sequence of raw tokens in\ns1, and t2 the tokens of s2. Further, let e1 and e2\nbe the sequence of structured events occurring in\ns1 and s2, respectively (described in more detail in\nSection 4.1), and let e2[0] denote the ﬁrst event of\ne2. The different systems we compare are named\nsystematically as follows:\n•The system t1 /shortrightarrowt2 is trained to encode a\nsentence’s tokens and decode its successor’s\ntokens.\n•The system e1 /shortrightarrowe2 is trained to encode a\nsentence’s events and decode its successor’s\nevents.\n•The system e1 /shortrightarrowe2 /shortrightarrowt2 is trained to en-\ncode a sentence’s events, decode its succes-\nsor’s events, and then encode the latter and\nsubsequently decode the successor’s text.\nWe will not explicitly enumerate all systems, but\nother systems are deﬁned analogously, with the\nschema X /shortrightarrowY describing a system which is\ntrained to encode X and subsequently decode Y,\nand X /shortrightarrowY /shortrightarrowZ indicating a system which is\ntrained to encode X, decode Y, and subsequently\nencode Y and decode Z. Note that in a system\nX /shortrightarrowY /shortrightarrowZ, only X is provided as input.\nWe also present results for systems of the form\nX a/shortrightarrowY, which signiﬁes that the system is trained\nto decode Y from Xwith the addition of an atten-\ntion mechanism. We use the attention mechanism\nof Vinyals et al. (2015). In short, these models\nhave additional parameters which can learn soft\nalignments between positions of encoded inputs\nand positions in decoded outputs. Attention mech-\nanisms have recently been shown to be quite em-\npirically valuable in many complex sequence pre-\ndiction tasks. For more details on the model, see\nVinyals et al. (2015).\nFigure 3 gives a diagrammatic representation\nof the different system setups. Text systems in-\nfer successor text and, optionally, parse that text\nand extract events from it; event sequences infer\nsuccessor events and, optionally, expand inferred\nevents into text.\nNote that the system t1 /shortrightarrowt2, in which both\nthe encoding and decoding steps operate on raw\ntext, is essentially a one-directional version of the\nskip-thought system of Kiros et al. (2015). 2 Fur-\nther, the system e1 /shortrightarrowe2 /shortrightarrowt2, which is trained to\ntake a sentence’s event sequence as input, predict\nits successor’s events, and then predict its succes-\nsor’s words, is comparable to the event inference\nsystem of Pichotta and Mooney (2016). They use\nan LSTM sequence model of events in sequence\n2The system of Kiros et al. (2015), in addition to being\ntrained to predict the next sentence, also contains a backward-\ndirectional RNN trained to predict a sentence’s predecessor;\nwe condition only on previous text. Kiros et al. (2015) also\nuse Gated Recurrent Units instead of LSTM.\n283\nt1 encode/decode\n“The dog chased the cat.” “The cat ran away.” ran_away(cat)\nt2 e2parse\ne1 encode/decode e2 t2encode/decode\nchased(dog, cat) ran_away(cat) “The cat ran away.”\nText representation\nEvent representation\nFigure 3: Different system setups for modeling the two-sentence sequence “The dog chased the cat.”\nfollowed by “The cat ran away.” The gray components inside dotted boxes are only present in some\nsystems.\nfor event inference, and optionally transform in-\nferred events to text using another LSTM; we, on\nthe other hand, use an encoder/decoder setup to\ninfer text directly.\n4 Evaluation\n4.1 Experimental Details\nWe train a number of LSTM encoder-decoder net-\nworks which vary in their input and output. Mod-\nels are trained on English Language Wikipedia,\nwith 1% of the documents held out as a validation\nset. Our test set consists of 10,000 unseen sen-\ntences (from articles in neither the training nor val-\nidation set). We train models with batch stochas-\ntic gradient descent with momentum, minimizing\nthe cross-entropy error of output predictions. All\nmodels are implemented in TensorFlow (Abadi et\nal., 2015). We use a vocabulary of the 50,000\nmost frequent tokens, replacing all other tokens\nwith an out-of-vocabulary pseudo-token. Learned\nword embeddings are 100-dimensional, and the la-\ntent LSTM vector is 500-dimensional. To extract\nevents from text, we use the Stanford Dependency\nParser (De Marneffe et al., 2006; Socher et al.,\n2013). We use the Moses toolkit (Koehn et al.,\n2007) to calculate BLEU .3\nWe evaluate the task of predicting held-out text\nwith three metrics. The ﬁrst metric isBLEU , which\nis standard BLEU (the geometric mean of modiﬁed\n1-, 2-, 3-, and 4-gram precision against a gold stan-\ndard, multiplied by a brevity penalty which pe-\nnalizes short candidates). The second metric we\npresent, BLEU -BP, is BLEU without the brevity\n3Via the script multi-bleu.pl.\npenalty: in the task of predicting successor sen-\ntences, depending on predictions’ end use, on-\ntopic brevity is not necessarily undesirable. Eval-\nuations are over top system inferences (that is, de-\ncoding is done by taking the argmax). Finally, we\nalso present values for unigram precision ( 1G P),\none of the components of BLEU .\nWe also evaluate on the task of predicting held-\nout verb-argument events, either directly or via in-\nferred text. We use two evaluation metrics for this\ntask. First, the Accuracy metric measures the per-\ncentage of a system’s most conﬁdent guesses that\nare totally correct. That is, for each held-out event,\na system makes its single most conﬁdent guess for\nthat event, and we calculate the total percentage of\nsuch guesses which are totally correct. Some au-\nthors (e.g. Jans et al. (2012), Pichotta and Mooney\n(2016)) present results on the “Recall at k” met-\nric, judging gold-standard recall against a list of\ntop kevent inferences; this metric is equivalent to\n“Recall at 1.” This is quite a stringent metric, as an\ninference is only counted correct if the verb and all\narguments are correct. To relax this requirement,\nwe also present results on what we call thePartial\nCredit metric, which is the percentage of held-out\nevent components identical to the respective com-\nponents in a system’s top inference.4\n4.2 Experimental Evaluation\nTable 1 gives the results of evaluating predicted\nsuccessor sentence text against the gold standard\nusing BLEU . The baseline system t1 /shortrightarrowt1 sim-\n4This metric was used in Pichotta and Mooney (2014),\nbut there it was called Accuracy. In the present work, we use\n“accuracy” only to mean Recall at 1.\n284\nSystem BLEU BLEU -BP 1G P\nt1 /shortrightarrowt1 1.88 1.88 22.6\ne1 /shortrightarrowe2 /shortrightarrowt2 0.34 0.66 19.9\ne1\na/shortrightarrowe2 /shortrightarrowt2 0.30 0.39 15.8\nt1 /shortrightarrowt2 5.20 7.84 30.9\nt1\na/shortrightarrowt2 4.68 8.09 32.2\nTable 1: Successor text predictions evaluated with\nBLEU .\nply reproduces the input sentence as its own suc-\ncessor.5 Below this are systems which make\npredictions from event information, with systems\nwhich make predictions from raw text under-\nneath. Transformations written X a/shortrightarrowY are, recall,\nencoder-decoder LSTMs with attention.\nNote, ﬁrst, that the text-level models outperform\nother models on BLEU . In particular, the two-step\nmodel e1 /shortrightarrowe2 /shortrightarrowt2 (and comparable model with\nattention) which ﬁrst predicts successor events and\nthen, as a separate step, expands these events into\ntext, performs quite poorly. This is perhaps due to\nthe fact that the translation from text to events is\nlossy, so reconstructing raw sentence tokens is not\nstraightforward.\nThe BLEU -BP scores, which are BLEU without\nthe brevity penalty, are noticeably higher in the\ntext-level models than the raw BLEU scores. This\nis in part because these models seem to produce\nshorter sentences, as illustrated below in section\n4.4.\nThe attention mechanism does not obviously\nbeneﬁt either text or event level prediction\nencoder-decoder models. This could be because\nthere is not an obvious alignment structure be-\ntween contiguous spans of raw text (or events) in\nnatural documents.\nThese results provide evidence that, if the Nar-\nrative Cloze task is deﬁned to evaluate prediction\nof held-out text from a document, then sentence-\nlevel RNN language models provide superior per-\nformance to RNN models operating at the event\nlevel. In other words, linguistic pre-processing\ndoes not obviously beneﬁt encoder-decoder mod-\nels trained to predict succeeding text.\nTable 2 gives results on the task of predicting\nthe next verb with its nominal arguments; that is,\nwhereas Table 1 gave results on a text analog to the\nNarrative Cloze evaluation (BLEU ), Table 2 gives\n5“t1 /shortrightarrowt1” is minor abuse of notation, as the system is not\nan encoder/decoder but a simple identity function.\nSystem Accuracy Partial Credit\nMost common 0.2 26.5\ne1 /shortrightarrowe2[0] 2.3 26.7\ne1\na/shortrightarrowe2[0] 2.2 25.6\nt1 /shortrightarrowt2 /shortrightarrowe2[0] 2.0 30.3\nt1\na/shortrightarrowt2 /shortrightarrowe2[0] 2.0 27.7\nTable 2: Next event prediction accuracy (numbers\nare percentages: maximum value is 100).\nresults on the verb-with-arguments prediction ver-\nsion. In the t1 /shortrightarrowt2 /shortrightarrowe2[0] system (and the\ncomparable system with attention), events are ex-\ntracted from automatically generated text by pars-\ning output text and applying the same event ex-\ntractor to this parse used to extract events from\nraw text.6 The row labeled Most commonin Ta-\nble 2 gives performance for the baseline system\nwhich always guesses the most common event in\nthe training set.\nThe LSTM models trained to directly predict\nevents are roughly comparable to systems which\noperate on raw text, performing slightly worse on\naccuracy and slightly better when taking partial\ncredit into account. As with the previous com-\nparisons with BLEU , the attention mechanism does\nnot provide an obvious improvement when decod-\ning inferences, perhaps, again, because the event\ninference problem lacks a clear alignment struc-\nture.\nThese systems infer their most probable guesses\nof e2[0], the ﬁrst event in the succeeding sentence.\nIn order for a system prediction to be counted as\ncorrect, it must have the correct strings for gram-\nmatical head words of all components of the cor-\nrect event. Note also that we judge only against a\nsystem’s single most conﬁdent prediction (as op-\nposed to some prior work (Jans et al., 2012; Pi-\nchotta and Mooney, 2014) which takes the top\nk predictions—the numbers presented here are\ntherefore noticeably lower). We do this mainly\nfor computational reasons: namely, a beam search\nover a full sentence’s text would be quite compu-\ntationally expensive.\n4.3 Adding Additional Context\nThe results given above are for systems which en-\ncode information about one sentence and decode\n6This is also a minor abuse of notation, as the second\ntransformation uses a statistical parser rather than an en-\ncoder/decoder.\n285\ninformation about its successor. This is within\nthe spirit of the skip-gram system of Kiros et al.\n(2015), but we may wish to condition on more\nof the document. To investigate this, we per-\nform an experiment varying the number of previ-\nous sentences input during the encoding step of\nt1 /shortrightarrowt2 text-level models without attention. We\ntrain three different models, which take either one,\nthree, or ﬁve sentences as input, respectively, and\nare trained to output the successor sentence.\nNum Prev Sents BLEU BLEU -BP 1G P\n1 5.80 8.59 29.4\n3 5.82 9.35 31.2\n5 6.83 6.83 21.4\nTable 3: Varying the amount of context in text-\nlevel models. “Num Prev Sents” is the number of\nprevious sentences supplied during encoding.\nTable 3 gives the results of running these mod-\nels on 10,000 sentences from the validation set. As\ncan be seen, in the training setup we investigate,\nmore additional context sentences have a mixed\neffect, depending on the metric. This is perhaps\ndue in part to the fact that we kept hyperparam-\neters ﬁxed between experiments, and a different\nhyperparameter regime would beneﬁt predictions\nfrom longer input sequences. More investigation\ncould prove fruitful.\n4.4 Qualitative Analysis\nFigure 4 gives some example automatic next-\nsentence text predictions, along with the input sen-\ntence and the gold-standard next sentence. Note\nthat gold-standard successor sentences frequently\nintroduce new details not obviously inferrable\nfrom previous text. Top system predictions, on\nthe other hand, are frequently fairly short. This\nis likely due part to the fact that the cross-entropy\nloss does not directly penalize short sentences and\npart to the fact that many details in gold-standard\nsuccessor text are inherently difﬁcult to predict.\n4.5 Discussion\nThe general low magnitude of the BLEU scores\npresented in Table 1, especially in comparison to\nthe scores typically reported in Machine Trans-\nlation results, indicates the difﬁculty of the task.\nIn open-domain text, a sentence is typically not\nstraightforwardly predictable from preceding text;\nif it were, it would likely not be stated.\nOn the task of verb-argument prediction in Ta-\nble 2, the difference between t1 /shortrightarrowt2 and e1 /shortrightarrow\ne2[0] is fairly marginal. This raises the general\nquestion of how much explicit syntactic analysis\nis required for the task of event inference, partic-\nularly in the encoder/decoder setup. These results\nprovide evidence that a sentence-level RNN lan-\nguage model which operates on raw tokens can\npredict what comes next in a document as well or\nnearly as well as an event-mediated script model.\n5 Future Work\nThere are a number of further extensions to this\nwork. First, in this work (and, more generally,\nNeural Machine Translation research), though\ngenerated text is evaluated using BLEU , systems\nare optimized for per-token cross-entropy error,\nwhich is a different objective (Luong et al. (2016)\ngive an example of a system which improves\ncross-entropy error but reduces BLEU score in the\nNeural Machine Translation context). Finding dif-\nferentiable objective functions that more directly\ntarget more complex evaluation metrics likeBLEU\nis an interesting future research direction.\nRelatedly, though we argue that BLEU is a\nnatural token-sequence-level analog to the verb-\nargument formulation of the Narrative Cloze task,\nit is not obviously the best metric for evaluat-\ning inferences of text, and comparing these auto-\nmated metrics with human judgments is an im-\nportant direction of future work. Pichotta and\nMooney (2016) present results on crowdsourced\nhuman evaluation of script inferences that could\nbe repeated for our RNN models.\nThough we focus here on forward-direction\nmodels predicting successor sentences, bidirec-\ntional encoder-decoder models, which predict sen-\ntences from both previous and subsequent text, are\nanother interesting future research direction.\n6 Related Work\nThe use of scripts in AI dates back to the 1970s\n(Minsky, 1974; Schank and Abelson, 1977); in\nthis conception, scripts were composed of com-\nplex events with no probabilistic semantics, which\nwere difﬁcult to learn automatically. In recent\nyears, a growing body of research has investigated\nlearning probabilistic co-occurrence models with\nsimpler events. Chambers and Jurafsky (2008)\npropose a model of co-occurrence of (verb, de-\npendency) pairs, which can be used to infer such\n286\nInput: As of October 1 , 2008 , ⟨OOV⟩ changed its company name to Panasonic Corporation.\nGold: ⟨OOV⟩ products that were branded “National” in Japan are currently marketed under the “Pana-\nsonic” brand.\nPredicted: The company’s name is now ⟨OOV⟩.\nInput: White died two days after Curly Bill shot him.\nGold: Before dying, White testiﬁed that he thought the pistol had accidentally discharged and that he\ndid not believe that Curly Bill shot him on purpose.\nPredicted: He was buried at ⟨OOV⟩ Cemetery.\nInput: The foundation stone was laid in 1867.\nGold: The members of the predominantly Irish working class parish managed to save £700 towards\nconstruction, a large sum at the time.\nPredicted: The ⟨OOV⟩ was founded in the early 20th century.\nInput: Soldiers arrive to tell him that ⟨OOV⟩ has been seen in camp and they call for his capture and\ndeath.\nGold: ⟨OOV⟩ agrees .\nPredicted: ⟨OOV⟩ is killed by the ⟨OOV⟩.\nFigure 4: Sample next-sentence text predictions. ⟨OOV⟩is the out-of-vocabulary pseudo-token, which\nfrequently replaces proper names.\npairs from documents; Jans et al. (2012) give a\nsuperior model in the same general framework.\nChambers and Jurafsky (2009) give a method of\ngeneralizing from single sequences of pair events\nto collections of such sequences. Rudinger et al.\n(2015) apply a discriminative language model to\nthe (verb, dependency) sequence modeling task,\nraising the question of to what extent event in-\nference can be performed with standard language\nmodels applied to event sequences. Pichotta and\nMooney (2014) describe a method of learning a\nco-occurrence based model of verbs with multiple\ncoreference-based entity arguments.\nThere is a body of related work focused on\nlearning models of co-occurring events to au-\ntomatically induce templates of complex events\ncomprising multiple verbs and arguments, aimed\nultimately at maximizing coherency of templates\n(Chambers, 2013; Cheung et al., 2013; Balasub-\nramanian et al., 2013). Ferraro and Van Durme\n(2016) give a model integrating various levels of\nevent information of increasing abstraction, evalu-\nating both on coherence of induced templates and\nlog-likelihood of predictions of held-out events.\nMcIntyre and Lapata (2010) describe a system that\nlearns a model of co-occurring events and uses this\nmodel to automatically generate stories via a Ge-\nnetic Algorithm.\nThere have been a number of recent published\nneural models for various event- and discourse-\nrelated tasks. Pichotta and Mooney (2016) show\nthat an LSTM event sequence model outper-\nforms previous co-occurrence methods for pre-\ndicting verbs with arguments. Granroth-Wilding\nand Clark (2016) describe a feedforward neu-\nral network which composes verbs and argu-\nments into low-dimensional vectors, evaluating on\na multiple-choice version of the Narrative Cloze\ntask. Modi and Titov (2014) describe a feedfor-\nward network which is trained to predict event or-\nderings. Kiros et al. (2015) give a method of em-\nbedding sentences in low-dimensional space such\nthat embeddings are predictive of neighboring sen-\ntences. Li et al. (2014) and Ji and Eisenstein\n(2015), use RNNs for discourse parsing; Liu et\nal. (2016) use a Convolutional Neural Network for\nimplicit discourse relation classiﬁcation.\n7 Conclusion\nWe have given what we believe to be the ﬁrst\nsystematic evaluation of sentence-level RNN lan-\nguage models on the task of predicting held-out\ndocument text. We have found that models oper-\nating on raw text perform roughly comparably to\nidentical models operating on predicate-argument\nevent structures when predicting the latter, and that\ntext models provide superior predictions of raw\ntext. This provides evidence that, for the task of\nheld-out event prediction, encoder/decoder mod-\nels mediated by automatically extracted events\nmay not be learning appreciably more structure\nthan systems trained on raw tokens alone.\nAcknowledgments\nThanks to Stephen Roller, Amelia Harrison, and\nthe UT NLP group for their help and feedback.\nThanks also to the anonymous reviewers for their\nvery helpful suggestions. This research was sup-\nported in part by the DARPA DEFT program un-\nder AFRL grant FA8750-13-2-0026.\n287\nReferences\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Andrew Harp,\nGeoffrey Irving, Michael Isard, Yangqing Jia, Rafal\nJozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\nLevenberg, Dan Man´e, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-\nwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-\nvan, Fernanda Vi ´egas, Oriol Vinyals, Pete Warden,\nMartin Wattenberg, Martin Wicke, Yuan Yu, and Xi-\naoqiang Zheng. 2015. TensorFlow: Large-scale\nmachine learning on heterogeneous systems. Soft-\nware available from tensorﬂow.org.\nDzmitry Bahdanau, KyungHyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by\njointly learning to align and translate. In Proceed-\nings of the 2015 International Conference on Learn-\ning Representations (ICLR 2015).\nNiranjan Balasubramanian, Stephen Soderland,\nMausam, and Oren Etzioni. 2013. Generating\ncoherent event schemas at scale. In Proceedings\nof the 2013 Conference on Empirical Methods in\nNatural Language Processing (EMNLP-2013).\nNathanael Chambers and Daniel Jurafsky. 2008. Un-\nsupervised learning of narrative event chains. In\nProceedings of the 46th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL-08) ,\npages 789–797.\nNathanael Chambers and Dan Jurafsky. 2009. Unsu-\npervised learning of narrative schemas and their par-\nticipants. In Proceedings of the 47th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL-09), pages 602–610.\nNathanael Chambers. 2013. Event schema induc-\ntion with a probabilistic entity-driven model. In\nProceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing (EMNLP-\n2013).\nJackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-\nderwende. 2013. Probabilistic frame induction. In\nProceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-13).\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. In NIPS Deep Learning Workshop.\nMarie-Catherine De Marneffe, Bill MacCartney, and\nChristopher D. Manning. 2006. Generating typed\ndependency parses from phrase structure parses. In\nProceedings of the 5th International Conference on\nLanguage Resources & Evaluation (LREC-2006) ,\nvolume 6, pages 449–454.\nJeffrey L. Elman. 1990. Finding structure in time.\nCognitive Science, 14:179–211.\nFrancis Ferraro and Benjamin Van Durme. 2016. A\nuniﬁed Bayesian model of scripts, frames and lan-\nguage. In Proceedings of the 30th AAAI Conference\non Artiﬁcial Intelligence (AAAI-16).\nMark Granroth-Wilding and Stephen Clark. 2016.\nWhat happens next? Event prediction using a com-\npositional neural network model. In Proceedings of\nthe 30th AAAI Conference on Artiﬁcial Intelligence\n(AAAI-16).\nKarl Moritz Hermann, Tom ´aˇs Ko ˇcisk`y, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. In Proceedings of\nthe 29th Annual Conference on Neural Information\nProcessing Systems (NIPS-15).\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nBram Jans, Steven Bethard, Ivan Vuli ´c, and\nMarie Francine Moens. 2012. Skip n-grams\nand ranking functions for predicting script events.\nIn Proceedings of the 13th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics (EACL-12), pages 336–344.\nYangfeng Ji and Jacob Eisenstein. 2015. One vector\nis not enough: Entity-augmented distributional se-\nmantics for discourse relations. Transactions of the\nAssociation for Computational Linguistics (TACL).\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Antonio Torralba, Raquel Urta-\nsun, and Sanja Fidler. 2015. Skip-thought vectors.\nIn Proceedings of the 29th Annual Conference on\nNeural Information Processing Systems (NIPS-15).\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL-07)\nCompanion Volume: Proceedings of the Demo and\nPoster Sessions, pages 177–180, Prague, Czech Re-\npublic.\nJiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recur-\nsive deep models for discourse parsing. In Proceed-\nings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n2061–2069, October.\n288\nYang Liu, Sujian Li, Xiaodong Zhang, and Zhifang\nSui. 2016. Implicit discourse relation classiﬁcation\nvia multi-task neural networks. In Proceedings of\nthe 30th AAAI Conference on Artiﬁcial Intelligence\n(AAAI-16).\nMinh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2016. Multi-task se-\nquence to sequence learning. In Proceedings of the\n4th International Conference on Learning Represen-\ntations (ICLR-16).\nNeil McIntyre and Mirella Lapata. 2010. Plot induc-\ntion and evolutionary search for story generation. In\nProceedings of the 48th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL-10) ,\npages 1562–1572.\nTomas Mikolov, Anoop Deoras, Stefan Kombrink,\nLukas Burget, and Jan Cernock `y. 2011. Empir-\nical evaluation and combination of advanced lan-\nguage modeling techniques. In Proceedings of the\n12th Annual Conference of the International Speech\nCommunication Association 2011 (INTERSPEECH\n2011), pages 605–608.\nMarvin Minsky. 1974. A framework for representing\nknowledge. Technical report, MIT-AI Laboratory.\nAshutosh Modi and Ivan Titov. 2014. Inducing neu-\nral models of script knowledge. In Proceedings of\nthe Eighteenth Conference on Computational Nat-\nural Language Learning (CoNLL-2014), Baltimore,\nMD, USA.\nJ Walker Orr, Prasad Tadepalli, Janardhan Rao Doppa,\nXiaoli Fern, and Thomas G Dietterich. 2014.\nLearning scripts as Hidden Markov Models. In Pro-\nceedings of the 28th AAAI Conference on Artiﬁcial\nIntelligence (AAAI-14).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic\nevaluation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics (ACL-02), pages 311–318.\nKarl Pichotta and Raymond J. Mooney. 2014. Statis-\ntical script learning with multi-argument events. In\nProceedings of the 14th Conference of the European\nChapter of the Association for Computational Lin-\nguistics (EACL 2014), pages 220–229.\nKarl Pichotta and Raymond J. Mooney. 2016. Learn-\ning statistical scripts with LSTM recurrent neural\nnetworks. In Proceedings of the 30th AAAI Con-\nference on Artiﬁcial Intelligence (AAAI-16).\nRachel Rudinger, Pushpendre Rastogi, Francis Ferraro,\nand Benjamin Van Durme. 2015. Script induction\nas language modeling. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP-15).\nRoger C. Schank and Robert P. Abelson. 1977.Scripts,\nPlans, Goals and Understanding: An Inquiry into\nHuman Knowledge Structures . Lawrence Erlbaum\nand Associates.\nIulian V . Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hier-\narchical neural network models. In Proceedings of\nthe 30th AAAI Conference on Artiﬁcial Intelligence\n(AAAI-16).\nRichard Socher, John Bauer, Christopher D. Manning,\nand Andrew Y . Ng. 2013. Parsing with compo-\nsitional vector grammars. In Proceedings of the\n51st Annual Meeting of the Association for Compu-\ntational Linguistics (ACL-13).\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Proceedings of the 28th Annual Con-\nference on Neural Information Processing Systems\n(NIPS-14), pages 3104–3112.\nOriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,\nIlya Sutskever, and Geoffrey Hinton. 2015. Gram-\nmar as a foreign language. In Proceedings of the\n29th Annual Conference on Neural Information Pro-\ncessing Systems (NIPS-15), pages 2755–2763.\nA Supplemental Material\nOur Wikipedia dump from which the training, de-\nvelopment, and test sets are constructed is from\nJan 2, 2014. We parse text using version 3.3.1 of\nthe Stanford CoreNLP system. We use a vocab\nconsisting of the 50,000 most common tokens, re-\nplacing all others with an Out-of-vocabulary pseu-\ndotoken. We train using batch stochastic gradi-\nent descent with momentum with a batch size of\n10 sequences, using an initial learning rate of 0.1,\ndamping the learning rate by 0.99 any time the\nprevious hundred updates’ average test error is\ngreater than any of the average losses in the previ-\nous ten groups of hundred updates. Our momen-\ntum parameter is 0.95. Our embedding vectors are\n100-dimensional, and our LSTM hidden state is\n500-dimensional. We train all models for 300k\nbatch updates (with the exception of the models\ncompared in §4.3, all of which we train for 150k\nbatch updates, as training is appreciably slower\nwith longer input sequences). Training takes ap-\nproximately 36 hours on an NVIDIA Titan Black\nGPU.\n289"
}