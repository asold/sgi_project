{
  "title": "LTRC_IIITH’s 2023 Submission for Prompting Large Language Models as Explainable Metrics Task",
  "url": "https://openalex.org/W4392669845",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2809329313",
      "name": "Pavan Baswani",
      "affiliations": [
        "Indian Institute of Technology Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A2117467440",
      "name": "Ananya Mukherjee",
      "affiliations": [
        "Indian Institute of Technology Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A2067306352",
      "name": "Manish Shrivastava",
      "affiliations": [
        "Indian Institute of Technology Hyderabad"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3211986039",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4389519446",
    "https://openalex.org/W4322760121"
  ],
  "abstract": "In this report, we share our contribution to the Eval4NLP Shared Task titled \"Prompting Large Language Models as Explainable Metrics.\" We build our prompts with a primary focus on effective prompting strategies, score-aggregation, and explainability for LLM-based metrics. We participated in the track for smaller models by submitting the scores along with their explanations. According to the Kendall correlation scores on the leaderboard, our MT evaluation submission ranks second-best, while our summarization evaluation submission ranks fourth, with only a 0.06 difference from the leading submission.",
  "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 156–163\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nLTRC_IIITH’s 2023 Submission for Prompting Large Language Models as\nExplainable Metrics Task\nPavan Baswani∗, Ananya Mukherjee∗, Manish Shrivastava\nLanguage Technologies Research Center, KCIS, IIIT Hyderabad, India.\n{pavan.baswani, ananya.mukherjee}@research.iiit.ac.in\nm.shrivastava@iiit.ac.in\nAbstract\nIn this report, we share our contribution to\nthe Eval4NLP Shared Task titled \"Prompt-\ning Large Language Models as Explainable\nMetrics.\" We build our prompts with a pri-\nmary focus on effectiveprompting strategies,\nscore-aggregation, andexplainability for LLM-\nbased metrics. We participated in the track\nfor smaller models by submitting the scores\nalong with their explanations. According to\nthe Kendall correlation scores on the leader-\nboard, our MT evaluation submission ranks\nsecond-best, while our summarization eval-\nuation submission ranks fourth, with only a\n0.06 difference from the leading submission.\nOurcodeisavailableat https://github.com/\npavanbaswani/Eval4NLP_SharedTask\n1 Introduction\nWith groundbreaking advancements in unsuper-\nvised learning and scalable architectures, the possi-\nbilitiesandassociatedrisks,ofautomaticallygener-\nating audio, images, videos, and text have become\nincredibly daunting. Conducting human evalua-\ntions of such content is not only costly but often\nlogistically challenging. Consequently, there is a\npressing need for automatic metrics that can reli-\nably assess the quality of generation systems and\ntheir outputs. Presently, the state-of-the-art metrics\nfor evaluating natural language generation (NLG)\nsystemsstillfallshortofreplicatingtheproficiency\nof human experts. These metrics primarily rely\non neural language models and typically yield a\nsingle quality score at the sentence level. This sin-\ngular score makes it arduous to explain their inter-\nnal decision-making processes and their resulting\nassessments (Leiter et al., 2023a).\nTheintroductionofAPIsforlargelanguagemod-\nels (LLMs), such as ChatGPT, and the recent open-\nsource availability of LLMs like LLaMA have ig-\n∗* Authors contributed equally\nnited a surge in NLP research, including the devel-\nopment of LLM-based metrics (Chiang and Lee,\n2023). Noteworthy examples include GEMBA\n(Kocmi and Federmann, 2023a), which delves into\nusingpromptswithChatGPT(OpenAI,2023a)and\nGPT4 (OpenAI, 2023b) directly as metrics, and\nInstructscore (Xu et al., 2023), which takes a dif-\nferent approach by fine-tuning a LLaMA model\nto provide a detailed error diagnosis of machine-\ntranslated content.\nIt is important to note that current research lacks\nsystematic evaluation of potential prompts and\nprompting techniques for metric usage. This in-\ncludes approaches that involve instructing a model\nor having the model explain a task on its own. Ad-\nditionally, there is a scarcity of assessments regard-\ning the performance of recent open-source LLMs,\ndespite their critical role in enhancing the repro-\nducibility of metric research compared to closed-\nsource alternatives.\nThis year’s Eval4NLP shared task (Leiter et al.,\n2023b) addresses these gaps, providing open-\nsource, pre-trained LLMs (Table 1) for assessing\nmachine translations and summaries. The focus is\non prompting techniques without LLM fine-tuning,\naiming to improve alignment with human evalu-\nations and enhance metric interpretability while\nidentifyingpromisingmodelsforfuturefine-tuning.\nThe shared task aims to achieve the following\nobjectives:\n• DevelopmentofpromptingstrategiesforLLM-\nbased metrics.\n• Establishment of a score aggregation method\nfor LLM-based metrics.\n• Enhancement of explainability in the context\nof LLM-based metrics.\nOur submission aligns with these objectives. We\nattainthesegoalsbyutilizingtheorca_mini_v3_7b\n156\nModel LanguageParamsSeq LengthSize (GB)Guanaco-65B-GPTQmultilingual65B 2048 33.5Platypus2-70B-Instruct-GPTQenglish 70B 4096 35.3WizardLM-13B-V1.1-GPTQenglish 13B 2048 7.45Nous-Hermes-13b english 13B 2048 26OpenOrca-Platypus2-13Benglish 13B 4096 26.03orca_mini_v3_7b english 7B 4096 13.48\nTable 1: List of LLMs provided in the Shared Task\n(Mathur,2023)modelandcraftingpromptsthrough\nacombinationoffine-grainedandchain-of-thought\npromptingstrategies. Additionally,wehaveadapted\n4-bit quantization to optimize model loading. We\nsubmit reference-free a) segment-level quality\nscores for all the language pairs (en-de, en-zh, en-\nes) listed under the MT evaluation task and b)\nsummary-level quality scores for all the documents\nprovided.\n2 Background\n2.1 LLM-Based Evaluation\nLarge Language Model (LLM)-based evaluation\ninvolves employing sophisticated language models\n(such as GPT-3 or similar) to evaluate the accuracy\nand quality of machine-generated text. An example\nof this is the work by Liu et al., 2023, who intro-\nduced G-Eval, a summarization evaluation model\nbuilt on GPT-4. Notably, G-Eval surpassed all pre-\nvious baseline models in summarization evaluation\nperformance according to their research. In the re-\ncent WMT22 metrics shared task (Freitag et al.,\n2022), the best-performing MT evaluation metric\nis METRICX XXL, a massive multi-task metric\nfine-tuned on LLM model checkpoints. However,\nKocmi and Federmann, 2023b shows that GEMBA,\na GPT-based metric that works both with a refer-\nence translation and without has outperformed all\nthe metrics that participated in the WMT22 shared\ntask.\nIt’s important to note that LLM-based evalua-\ntions usually generate a single score but lack the\ncapacity to provide detailed reasoning or explana-\ntions behind that score.\n2.2 Explainability\nExplainability has gained significant importance in\nAI research in recent years, offering potential bene-\nfits for AI system users, designers, and developers\n(Leiter et al., 2023a). Explainability is particularly\ndesirable for evaluation metrics. Sai et al., 2022\nexplainable Natural Language Generation (NLG)\nmetrics should prioritize offering comprehensive\ninformation beyond a single score. Eval4NLP 2021\n(Fomicheva et al., 2021) was the first shared task to\nemphasize explainability in MT evaluation.\nExplainable evaluations are assessment methods\nthatnotonlyprovideanumericalscoreforthequal-\nityofmachine-generatedtextbutalsoofferdetailed\ninsights or explanations regarding why a particular\nscore was assigned. These metrics aim to make\nthe evaluation process more transparent and in-\nterpretable by highlighting specific strengths and\nweaknesses in the generated text, such as fluency,\naccuracy,coherence,relevance,orsemanticfidelity.\nTheyarevaluableforbothimprovingNLGsystems\nand enabling users to better understand the quality\nof text.\n2.3 Prompt Engineering\nPrompt engineering is a dual-purpose AI engineer-\ning technique: it fine-tunes large language models\nwith specific prompts and guides the process of re-\nfining inputs for generative AI services to create\ntextorimages. Inthefollowing, we’lldiscusssome\nprompt-engineering techniques.\n1. Zero-Shot Prompting: Zero-shot prompting is\nan AI technique where models respond effec-\ntivelytopromptsthey’veneverseenbeforedur-\ning training. It leverages general knowledge\nto generate context-aware responses, often by\nproviding auxiliary information or examples.\nThis approach enhances the adaptability of AI\nmodels in tasks like language understanding\nand generation. It’s particularly valuable in\ndiverse, real-world applications.\n2. Few-Shot Prompting: Few-shot prompting is\nan AI approach where models are trained to\nperform tasks or generate responses with very\nlimited examples or data, typically fewer than\nfive instances. It relies on techniques like\nmeta-learning and transfer learning to enable\nmodels to generalize effectively from minimal\ntraining data. This method is essential for ap-\nplications requiring rapid adaptation to new\ntasks or domains.\n3. Chain of Thought (CoT): Chain of thought\nprompting is a cognitive technique involving\nstructured, sequential prompts or questions\ndesigned to guide systematic thinking and ex-\nploration of a topic. Large Language Models\n(LLMs) have shown enhanced capabilities of\nsolving novel tasks by reasoning step-by-step\n(Kim et al., 2023).\n157\n4. Fine-Grained Analysis: Fine-grained prompt-\ning is a method that involves detailed exam-\nination and analysis of data or information\nat a granular level. It is employed to gain a\ndeeper and more comprehensive understand-\ning by breaking them down into smaller, dis-\ntinct components for in-depth exploration and\nassessment. Fine-grained prompting is often\nused in research, data analysis, and various in-\ndustries to extract valuable insights and make\ninformed decisions.\n5. Translational Probability: Translational proba-\nbility prompting involves assessing the likeli-\nhood that a given translation accurately repre-\nsents the intended meaning of the source text.\nIt’sakeyfactorinevaluatingthequalityandfi-\ndelity of machine-generated translations. This\ntechnique helps measure how well an MT sys-\ntem produces translations that align with the\nexpected or reference translations, contribut-\ning to the assessment of translation accuracy\nand effectiveness.\n6. Majority Vote: Majority vote prompting is a\ndecision-making approach that relies on ag-\ngregating the opinions or votes of multiple\nindividuals or systems to make a final de-\ncision. This technique is used to enhance\ndecision-making by leveraging collective wis-\ndomandimprovingtheaccuracyorrobustness\nof choices.\n7. Self-Refinement: Self-refinement is a pro-\ncess of continuous improvement or self-\ndevelopment. Self-refinement prompting in-\nvolves providing prompts or questions that\nprompt reflection and self-assessment. These\nprompts encourage models to identify areas\nfor improvement and take action to enhance\ntheir performance.\nEach of these concepts plays a crucial role in vari-\nous domains, from machine learning and artificial\nintelligence to cognitive psychology and decision-\nmaking processes. Understanding and effectively\napplyingtheseconceptscanleadtomorerobustand\ninformed solutions in a wide range of applications.\n3 System Description\nWeoptedfororca_mini_v3_7bamongtheprovided\nLLMs due to its smaller size, which accommo-\ndated our resource constraints. We encountered\nchallenges when attempting to load other LLMs.\nWe curated prompts using a blend of fine-grained\nandchain-of-thoughtpromptingstrategies. Further-\nmore, using bitsandbytes1 we employed 4-bit quan-\ntization to enhance model loading efficiency and\nconsideredMAXTOKENSas512duringinference\n(refer Appendix 7 for computation details).\nOursubmissionincludes: a)Summary-levelqual-\nityscoresforallthedocumentsprovidedinthetask.\nb) Segment-level quality scores for language pairs\n(en-de, en-zh, en-es) in the MT evaluation task,\nwithout relying on references.\nThe summary-level scores and segment-level\nscores lies in the range of 0-100, where 0 is the\nleast score that can be awarded to a bad transla-\ntion/summary and 100 is the highest score that can\nbe assigned to a perfect translation/summary.\n3.1 Dataset\nTable2illustratestheprovidedtestsamplestatistics.\nThe reported token counts were computed using\nbert tokenizer2.\n# Entriesmin tokensmax tokensaverage tokens\nsummarizationsource (en)825 144 818 279.413target (en) 9 402 51.697\nen_de source (en)1425 18 137 37.935target (de) 17 156 41.297\nen_es source (en)1834 15 137 37.472target (es) 19 149 41.683\nen_zh source (en)1297 18 137 37.856target (zh) 21 212 51.436\nTable 2: Test Data Statistics\n3.2 Our Prompting Strategies\nWe outline our prompting strategies for this shared\ntask as follows.\n3.2.1 Approach-1 (Zero-shot W/o explanation)\n\"Zero-shot prompting without explanation\" means\nprompting the LLM to generate a response without\nproviding any additional information or context to\nclarifyorsupporttheprompt. Itreliessolelyonthe\ninitial instruction without further elaboration.\n3.2.2 Approach-2 (Zero-shot w/ explanation)\n\"Zero-shot prompting with explanation\" involves\nproviding a prompt or instruction to a system and\nsupplementing it with additional information or\ncontext to clarify or support the prompt (refer Ta-\nble 3 & 4). This approach aims to enhance the\n1https://huggingface.co/blog/\n4bit-transformers-bitsandbytes#advanced-usage\n2https://huggingface.co/\nbert-base-multilingual-cased\n158\nsystem’s understanding of the task or request by\noffering more details or background information\nalongside the initial instruction.\n3.2.3 Approach-3 (CoT + Fine-grained w/\nexplanation)\nWe aim to incorporate a strategic approach to facil-\nitate a deeper understanding, ultimately enhancing\nthe LLM’s ability to provide improved responses.\nOur approach involves a combination of chain of\nthought (CoT) prompting and fine-grained analysis,\nspecifically focusing on the aspects of Relevance,\nConsistency, Coherence, and Fluency for Summa-\nrization; and emphasizing on Adequacy, Faithful-\nness, and Fluency for MT\n• Fine-grained Analysis for Summarization:\nFirstly, the LLM is instructed to provide indi-\nvidual scores for Relevance, Consistency, Co-\nherence,andFluency. Theseindividualscores\narethenusedtopromptthemodeltoprovidea\nfinal overall summary score, ensuring a com-\nprehensive assessment of the summarization\nquality (refer Table 5). This approach enables\na more detailed and nuanced evaluation of the\nsummary’s performance in each aspect.\n• Fine-grained Analysis for MT:Initially, the\nLLM generates separate scores for Adequacy,\nFaithfulness, and Fluency. Subsequently, us-\ningthesescores,themodelispromptedtopro-\nduceafinaltranslation qualityscore, ensuring\nacomprehensiveevaluationofthetranslation’s\nperformanceineachdimension(referTable6).\nThis approach enhances our ability to assess\ntranslation quality thoroughly.\n4 Results & Analysis\nTable 7 depicts the summary-level Kendall correla-\ntion scores for the summarization evaluation task.\nWe can infer that our submission (LTRC) ranks\n4th with a very minute difference of 0.06 when\ncompared to the top submission. We initially used\nzero-shotpromptingwhichresultedinacorrelation\nof 0.41 in the leaderboard. After employing CoT\n+ Fine-grained prompting, the Kendall correlation\nimproved to 0.44. Hence, it is evident that strategic\npromptinghasshownapositiveimprovementinthe\nsystem’s performance.\nTable 8, 9, and 10 depict segment-level Kendall\ncorrelations for MT on en-de, en-zh, and en-es lan-\nguage pairs respectively. We can notice that our\nsubmissions have consistently ranked 2nd (in small\nmodels track) across the language pairs.\nFor the en-de language pair, zero-shot prompt-\ning resulted in a correlation of 0.11 which drasti-\ncally improved to 0.19 with CoT + Fine-grained\nprompting. Conversely, for en-zh, when CoT +\nFine-grained prompting was applied, the correla-\ntion score dropped to 0.09. Hence for en-zh and\nen-es,wehavemadeoursubmissionwithzero-shot\nprompting.\nAn interesting point to observe is that our sub-\nmissions have surpassed most of the submissions\nmade in the large model track except NLLG for\nen-de and en-es, and MysteryTest for en-es.\n4.1 Error Analysis\nWe conducted manual analysis on a few English-\nGermanMTsamples. Duringthisanalysis,weiden-\ntified a minor scoring issue emanating from lan-\nguage compatibility3. To illustrate this, we’ve pro-\nvided a few examples in Table 11. It’s notable that\nthe zero-shot prompting strategy yielded a notably\nhigh score, even though it overlooked translation\naccuracy(inthefirstcase)andgeneratedinaccurate\nexplanations(inbothexamples). Ontheotherhand,\nCoT + fine-grained prompting has penalized the\nfirst example by awarding a score of 70 but in the\nexplanation, it failed to identify the missing info\nand rather provided an incorrect assessment of text\nfluency. This observation underscores the need for\na more nuanced evaluation approach that consid-\ners not only the final scores but also the accuracy\nand reliability of the explanations provided by the\nmodel.\n5 Challenges\n• Resource Constraints:The process of load-\ning and utilizing large language models de-\nmands substantial computational resources.\nUnfortunately, due to limited available mem-\nory, we encountered difficulties loading alter-\nnative models. Despite successfully loading\nthe large models, we encountered issues when\nattempting to perform inference.\n• Language Compatibility:Using an English-\ntrained (orca_mini_v3_7b) model to evaluate\nGerman, Spanish, and Chinese translations\nmay have performance implications.\n3orca_mini_v3_7b was originally trained on English text\n159\n### InstructionThe task is to provide the overall score for the given summary with reference to the given article on a continuous scale from 0 to 10along with explanation in JSON format with \"score\" and \"explanation\" keys as follows: {\"score\": <float-value>, \"explanation\": <explanation-text>}.Where a score of 0 means the summary is \"irrelevant, factually incorrect and not readable\" and score of 10 means \"relevant, factually correct, good readability\".You must justify the score that you provided with clear and concise reason within 2 sentences interms of justifying the relevance, readability, factuality metrics.The article text and summary text is given in triple backticks “‘ with ### Article: and ### Summary: as prefix respectively.Note: The generated response must be in json format without any missed braces or incomplete text. Also, it should not provide any additional information other than JSON output.\n### Article: “‘{}“‘### Summary: “‘{}“‘### Response:\nTable 3: Zero-shot prompting for evaluating Summary\n### Instruction:The task is to score a translated text from {English} to {German} with respect to the source sentence on a continous scale from 0 to 100,along with explaination in JSON format with \"score\" and \"explanation\" keys as follows: {\"score\": <float-value>, \"explanation\": <explanation-text>}.Where a score of zero means \"no meaning preserved and poor translation quality\" and score of one hundred means \"excellant translation quality with perfect meaning and grammar\".You must justify the score that you provided with clear and concise reason within 2 sentences interms of justifying the adequacy, fluency, faithfulness metrics.The source sentence and target sentence is given in triple backticks with ### source sentence: and ### target sentence: as prefix respectively.Note: The generated response must be in json format without any missed braces or incomplete text. Also, it should not provide any additional information other than JSON output.\n### source sentence: “‘{}“‘### target sentence: “‘{}“‘### Response:\nTable 4: Zero-shot prompting for evaluating MT\n### Instruction\nYou will be given one summary written for a news article.\nYour task is to assign the single score for the summary on continuous scale from 0 to 10 along with explanation.\nPlease make sure you read and understand these instructions carefully. Please keep this document open while reviewing,\nand refer to it as needed. You must justify the score that you provided with clear and concise reason within 2 sentences in\nterms of justifying the relevance, fluency, coherence and consistency metrics.\nThe article text and summary text is given in triple backticks “‘ with \"Source Text:\" and \"Summary:\" as prefix respectively.\nEvaluation Criteria:\n1) Relevance (1-5) - selection of important content from the source. The summary should include only important information\nfrom the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information.\nHere, 1 is the lowest and 5 is the highest.\n2) Consistency (1-5) - the factual alignment between the summary and the summarized source. A factually consistent summary\ncontains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained\nhallucinated facts. Here, 1 is the lowest and 5 is the highest\n3) Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and\ncoherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related\ninformation, but should build from sentence to a coherent body of information about a topic.\". Here, 1 is the lowest and 5 is the highest.\n4) Fluency (1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n- 1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n- 2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n- 3: Good. The summary has few or no errors and is easy to read and follow.\nEvaluation Steps:\n1. Read the summary and the source document carefully.\n2. Compare the summary to the source document and identify the main points of the article.\n3. Assign scores for Relevance, Consistency, Coherence and Fluency based on the Evaluation Criteria.\n4. By utilizing the generated scores of Relevance, Readability, Coherence and Fluency, aggregate these scores to assign the single score\nfor the summary on continuous scale from 0 to 10 along with explanation in JSON format with \"score\" and \"explanation\" keys as follows:\n{\"score\": <float-value>, \"explanation\": <explanation-text>}.\n### Source Text: “‘{}“‘\n### Summary: “‘{}“‘\n### Response:\nTable 5: CoT + fine-grained prompting for evaluating summaries\n160\n### Instruction\nYou will be given one translated sentence in {Spanish} for a source sentence in {English}.\nYour task is to assign the single score for the translation on continuous scale from 0 to 100 along with explanation.\nPlease make sure you read and understand these instructions carefully. Please keep this document open while reviewing,\nand refer to it as needed. For explanation, you must justify the score that you provided with clear and concise reason within\n2 sentences interms of justifying the adequacy, fluency and faithfulness metrics.\nThe source text and translation text is given in triple backticks “‘ with \"Source Text:\" and \"Translation:\" as prefix respectively.\nEvaluation Criteria:\n1) Adequacy (1-5) - the correspondence of the target text to the source text, including the expressive means in translation.\nAnnotators were instructed to penalize translation which contained misinformation, redundancies and excess information.\nHere, 1 is the lowest and 5 is the highest.\n2) Faithfulness (1-5) - translation faithfulness to the meaning depends on how the translator interprets the speaker’s intention\nand does not imply that one should never or always translate literally. Here, 1 is the lowest and 5 is the highest.\n3) Fluency (1-3): the quality of the translation in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n- 1: Poor. The translation has many errors that make it hard to understand or sound unnatural.\n- 2: Fair. The translation has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n- 3: Good. The translation has few or no errors and is easy to read and follow.\nEvaluation Steps:\n1. Read the translation and the source document carefully.\n2. Compare the translation to the source text.\n3. Assign scores for Adequacy, Faithfulness and Fluency based on the Evaluation Criteria.\n4. By utilizing the generated scores of Adequacy, Faithfulness and Fluency, aggregate these scores to assign the single score for the\ntranslation on continuous scale from 0 to 100 along with explanation in JSON format with \"score\" and \"explanation\" keys as follows:\n{\"score\": <float-value>, \"explanation\": <explanation-text>}.\n### Source Text: “‘{}“‘\n### Translation: “‘{}“‘\n### Response:\nTable 6: CoT + fine-grained prompting for evaluating MT\n161\nTrack Team Name Summ\nSmall\nDSBA 0.5\niML 0.49\nIUST_NLP_Lab 0.48\nLTRC 0.44\nCompetitionEntrants 0.44\nBeginners 0.38\nManCity 0.25\nLarge NLLG 0.35\nTable 7: Summary-level Kendall Correlation for Sum-\nmarization Task\nTrack Team Name en-de\nSmall\nHIT-MI&T Lab 0.49\nLTRC 0.19\nuOttawa 0.12\nTaiwanSenior 0.04\nLarge\nNLLG 0.24\nMysteryTest 0.17\nEval4NLP 0\nTable 8: Segment-level Kendall Correlation for MT on\nEnglish-German pairs.\nTrack Team Name en-zh\nSmall HIT-MI&T Lab 0.32\nLTRC 0.13\nLarge\nNLLG 0.13\nMysteryTest 0.1\nEval4NLP 0.01\nTable 9: Segment-level Kendall Correlation for MT on\nEnglish-Chinese pairs.\nTrack Team Name en-es\nSmall HIT-MI&T Lab 0.42\nLTRC 0.11\nLarge\nNLLG 0.18\nMysteryTest 0.12\nEval4NLP -0.02\nTable 10: Segment-level Kendall Correlation for MT on\nEnglish-Spanish pairs.\n6 Conclusions\nIn this paper, we present our contribution to\nthe Eval4NLP shared task, which focuses on the\n\"Prompting Large Language Models as Explain-\nable Metrics Task.\" Our submission is specifically\ntailored to the small model track. Our evaluation\ninvolved the use of the \"orca_mini_v3_7b\" model\ntoassessmachinetranslation(MT)andsummariza-\ntion test data. To generate scores accompanied by\nexplanations, we employed both zero-shot and fine-\ngrained+CoTpromptingstrategies. Inourfindings,\nwe provide Kendall correlation scores in compari-\nson to other submissions. We also conducted error\nanalysis by discussing the model’s performance on\na few samples. Overall, our submission ranked 2nd\nin the machine translation task and 4th in the sum-\nmarizationtask. Webelievethatfurtherexploration\nof strategic prompting methods holds the potential\nto enhance model performance in terms of both\nscoring and explainability.\nReferences\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? InProceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15607–15631, Toronto,\nCanada. Association for Computational Linguistics.\nMarina Fomicheva, Piyawat Lertvittayakumjorn, Wei\nZhao, Steffen Eger, and Yang Gao. 2021. The\nEval4NLP shared task on explainable quality esti-\nmation: Overview and results. InProceedings of the\n2nd Workshop on Evaluation and Comparison of NLP\nSystems, pages 165–178, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nMarkusFreitag,RicardoRei,NitikaMathur,Chi-kiuLo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André F. T. Martins.\n2022. Results of WMT22 metrics shared task: Stop\nusing BLEU – neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference\non Machine Translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nSeungone Kim, Se Joo, Doyoung Kim, Joel Jang,\nSeonghyeon Ye, Jamin Shin, and Minjoon Seo. 2023.\nThe cot collection: Improving zero-shot and few-shot\nlearningoflanguagemodelsviachain-of-thoughtfine-\ntuning.\nTomKocmiandChristianFedermann.2023a. Largelan-\nguage models are state-of-the-art evaluators of trans-\nlation quality. In Proceedings of the 24th Annual\nConference of the European Association for Machine\nTranslation, pages 193–203, Tampere, Finland. Euro-\npean Association for Machine Translation.\nTomKocmiandChristianFedermann.2023b. Largelan-\nguage models are state-of-the-art evaluators of trans-\nlation quality. In Proceedings of the 24th Annual\nConference of the European Association for Machine\nTranslation, pages 193–203, Tampere, Finland. Euro-\npean Association for Machine Translation.\n162\nZero-Shot PromptingCoT + Fine-grained PromptingHuman EvaluationEnglish (en) German (de)Score Explanation Score Explanation Score Explanation\nSamson Lawal (born 25 April2004) is a Nigerian professionalfootballer who plays as amidfielder for Livingston.\nSamson Lawal (* 25. April2004 in Livingston) ist einnigerianischer Fußballspieler.90\nThe translation is adequate, with the maininformation about the player and his birthdatepreserved. However, the sentence structureis not as fluent as it could be, and the use ofthe German word \"ist\" instead of \"ist in\"could be improved.\n70\nThe translation is adequate as itconveys the main information fromthe source text, including thebirthdate, nationality, and positionof the footballer. However, the sentencestructure is not as fluent as it could be,and there are some minor grammatical errors.\n60 The translation misses some informationlike \\textit{Samson Lawal plays asa midfielder for Livingston} and insteadmentions Livingston as his birthplace.\nAndrii Ivanovych Dubchak(born 19 October 1976,Kalynivka, Vinnytsia Oblast)is a Ukrainian photographer,videographer, photojournalist,and war correspondent.\nAndrii Ivanovych Dubchak (*19. Oktober 1976 in Kalynivka,Vinnytsia Oblast) ist einukrainischer Fotograf,Videograf, Fotojournalistund Kriegskorrespondent.\n90\nThe translation is very fluent andmaintains the meaning of the originalsentence. However, there are some minorgrammatical errors, such as the use of’ist’ instead of ’ist ein’ for the first occurrenceof ’Andrii Ivanovych Dubchak’ and the missingcomma after ’Kalynivka, Vinnytsia Oblast).\n80\nThe translation is adequate, as it conveysthe main information about AndriiIvanovych Dubchak’s profession and birthplace.The fluency is good, with no major grammaticalor spelling errors. However, the faithfulness couldbe improved, as the sentence structure isnot entirely faithful to the original source text.\n95 hethetheThe translation is accurate and preservessource meaning. The only minor issue isthat letter ’U’ should be capitalized in\\textit{ukrainischer}.\nTable 11: Analysis on en-de MT pairs.\nChristoph Leiter, Piyawat Lertvittayakumjorn,\nM. Fomicheva, Wei Zhao, Yang Gao, and Steffen\nEger. 2023a. Towards explainable evaluation metrics\nfor machine translation.ArXiv, abs/2306.13041.\nChristoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao,\nRotem Dror, and Steffen Eger. 2023b. The eval4nlp\n2023sharedtaskonpromptinglargelanguagemodels\nas explainable metrics. InProceedings of the 4th\nWorkshop on Evaluation and Comparison for NLP\nsystems.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochenXu,andChenguangZhu.2023. G-eval: Nlg\nevaluation using gpt-4 with better human alignment.\nPankaj Mathur. 2023. orca_mini_v3_7b: An ex-\nplain tuned llama2-7b model.https://https://\nhuggingface.co/psmathur/orca_mini_v3_7b.\nOpenAI. 2023a. Chatgpt.\nOpenAI. 2023b. Gpt-4 technical report.\nAnanya B. Sai, Akash Kumar Mohankumar, and\nMitesh M. Khapra. 2022. A survey of evaluation\nmetrics used for nlg systems.ACM Comput. Surv.,\n55(2).\nWenda Xu, Danqing Wang, Liangming Pan, Zhenqiao\nSong, Markus Freitag, William Yang Wang, and Lei\nLi. 2023. Instructscore: Towards explainable text\ngeneration evaluation with automatic feedback.\n7 Appendices\nWe used the following computation for all infer-\nences.\n1. CPU:\n• Name: Intel(R) Xeon(R) CPU E5-2640\nv4 @ 2.40GHz\n• Total: 40\n• # Cores: 10\n• cache size: 25600 KB\n2. GPU:\n• Name: NVIDIA GeForce RTX 2080 Ti\n• Total: 4\n• Memory/GPU: 11GB\n163",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8995068073272705
    },
    {
      "name": "Computer science",
      "score": 0.7767620086669922
    },
    {
      "name": "Task (project management)",
      "score": 0.7176418900489807
    },
    {
      "name": "Correlation",
      "score": 0.5511073470115662
    },
    {
      "name": "Focus (optics)",
      "score": 0.49463436007499695
    },
    {
      "name": "Language model",
      "score": 0.4763510525226593
    },
    {
      "name": "Natural language processing",
      "score": 0.47321170568466187
    },
    {
      "name": "Track (disk drive)",
      "score": 0.4166014790534973
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3981253206729889
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ]
}