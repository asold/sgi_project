{
  "title": "Audiovisual Transformer Architectures for Large-Scale Classification and Synchronization of Weakly Labeled Audio Events",
  "url": "https://openalex.org/W2981822321",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5040527966",
      "name": "Wim Boes",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A5087514947",
      "name": "Hugo Van hamme",
      "affiliations": [
        "KU Leuven"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2408239454",
    "https://openalex.org/W2963723765",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2751116311",
    "https://openalex.org/W2995348821",
    "https://openalex.org/W2524365899",
    "https://openalex.org/W2402144811",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2962924597",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2903099953",
    "https://openalex.org/W2775794021",
    "https://openalex.org/W2798453914",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W2775505379",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2953384591"
  ],
  "abstract": "We tackle the task of environmental event classification by drawing\\ninspiration from the transformer neural network architecture used in machine\\ntranslation. We modify this attention-based feedforward structure in such a way\\nthat allows the resulting model to use audio as well as video to compute sound\\nevent predictions. We perform extensive experiments with these adapted\\ntransformers on an audiovisual data set, obtained by appending relevant visual\\ninformation to an existing large-scale weakly labeled audio collection. The\\nemployed multi-label data contains clip-level annotation indicating the\\npresence or absence of 17 classes of environmental sounds, and does not include\\ntemporal information. We show that the proposed modified transformers strongly\\nimprove upon previously introduced models and in fact achieve state-of-the-art\\nresults. We also make a compelling case for devoting more attention to research\\nin multimodal audiovisual classification by proving the usefulness of visual\\ninformation for the task at hand,namely audio event recognition. In addition,\\nwe visualize internal attention patterns of the audiovisual transformers and in\\ndoing so demonstrate their potential for performing multimodal synchronization.\\n",
  "full_text": "Audiovisual Transformer Architectures for\nLarge-Scale Classification and Synchronization of\nWeakly Labeled Audio Events\nWim Boes\nHugo Van hamme\nwim.boes@esat.kuleuven.be\nhugo.vanhamme@esat.kuleuven.be\nESAT, KU Leuven\nLeuven, Belgium\nABSTRACT\nWe tackle the task of environmental event classification by drawing\ninspiration from the transformer neural network architecture used\nin machine translation. We modify this attention-based feedforward\nstructure in such a way that allows the resulting model to use audio\nas well as video to compute sound event predictions.\nWe perform extensive experiments with these adapted trans-\nformers on an audiovisual data set, obtained by appending relevant\nvisual information to an existing large-scale weakly labeled au-\ndio collection. The employed multi-label data contains clip-level\nannotation indicating the presence or absence of 17 classes of envi-\nronmental sounds, and does not include temporal information.\nWe show that the proposed modified transformers strongly im-\nprove upon previously introduced models and in fact achieve state-\nof-the-art results. We also make a compelling case for devoting\nmore attention to research in multimodal audiovisual classification\nby proving the usefulness of visual information for the task at hand,\nnamely audio event recognition.\nIn addition, we visualize internal attention patterns of the audio-\nvisual transformers and in doing so demonstrate their potential for\nperforming multimodal synchronization.\nCCS CONCEPTS\n• Computing methodologies → Neural networks.\nKEYWORDS\naudio event classification, audiovisual classification, audiovisual\nsynchronization, transformer neural network architecture\nACM Reference Format:\nWim Boes and Hugo Van hamme. 2019. Audiovisual Transformer Architec-\ntures for Large-Scale Classification and Synchronization of Weakly Labeled\nAudio Events. In Proceedings of the 27th ACM International Conference on\nMultimedia (MM ’19), October 21–25, 2019, Nice, France. ACM, New York,\nNY, USA, 9 pages. https://doi.org/10.1145/3343031.3350873\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM ’19, October 21–25, 2019, Nice, France\n© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-6889-6/19/10. . . $15.00\nhttps://doi.org/10.1145/3343031.3350873\n1 INTRODUCTION\nHuman beings heavily depend on both auditory and visual cues to\ndetect environmental events and to infer how to act in response.\nIn order to create situation-aware machines, it is therefore vital\nto study and design models which are capable of utilizing both\nmodalities to accurately classify such events.\nImage classification has been a red hot topic ever since the ad-\nvent of the ImageNet challenge and data set. Much more recently,\nresearch into audio event classification has also taken off because\nof among other the introduction of the Audio Set data set and\nchallenges such as DCASE 2016, DCASE 2017 and DCASE 2018.\nHowever, in contrast to the unimodal classification tasks, multi-\nmodal classification tasks are much less prevalent in the literature,\neven though studying those endeavours could naturally lead to\nimportant practical consequences [4, 6, 11, 15, 19].\nIn this work, we draw inspiration from the machine translation\ncommunity to tackle classification of environmental events. As is\nthe case with many machine learning tasks, the domain of machine\ntranslation is currently dominated by the deep learning paradigm.\nIn particular, recently the transformer neural network has gained\na lot of popularity. This type of model attains state-of-the-art per-\nformance while also achieving comparatively quick training times\nand allowing for interpretation relatively easily [18].\nThe data used in the field of machine translation is similar to\naudiovisual data in the sense that two modalities are involved,\nnamely a source language and a target language. The comparison\ncan be taken even further as audio and vision could be regarded\nas two abstract languages capable of describing environmental\nevents. For example, a train passing by could be expressed in the\nabstract audio language by the sound of a train horn, while it could\nsimultaneously be represented in the abstract vision language by\na picture of a train. This loose analogy indicates that it could be\nuseful to apply machine translation techniques to audiovisual data.\nThe approach taken in this work is therefore the following: we\nmodify the transformer neural network architecture used in ma-\nchine translation to be viable for the task of audiovisual classifica-\ntion of environmental events. We then evaluate and analyze the\nresults obtained by these models.\nIn Section 2, we discuss the original transformer architecture\nand how it can be adapted for the task of audiovisual classification\nof environmental events. In Section 3, we discuss the setup for the\nconducted experiments. Next, in Section 4 we discuss and interpret\nthe obtained results. Finally, in Section 5 we draw a conclusion.\narXiv:1912.02615v1  [eess.AS]  2 Dec 2019\n2 AUDIOVISUAL TRANSFORMER\nNEURAL NETWORKS\nIn this section, we discuss some key concepts of the original trans-\nformer used for machine translation and explain how this model\ncan be adapted to become suitable for audiovisual classification.\n2.1 Machine translation transformer\nThe original transformer is a neural network based entirely on feed-\nforward layers, residual connections and attention. It is depicted in\nFigure 1. This encoder-decoder model converts a sequence of input\ntokens in a source language into a sentence in a target language\nby iteratively calculating conditional probabilities of each output\nword and combining them through beam search. In what follows,\nwe will further describe some of the key components of this model.\nHowever, we will not discuss elements which are not used in this\nwork such as the masking and shifting of output tokens and im-\nplementation details like the used regularization methods. For full\ndetails on the machine translation transformer, we refer the reader\nto the original work on this model [7, 18].\nInput words Output words\nPositional\nencodings\nEmbedding Embedding\nN×\nN×\nMulti-head\nattention\nMulti-head\nattention\nAddition and\nnormalization\nAddition and\nnormalization\nFeedforward\nReLU subnet\nMulti-head\nattention\nAddition and\nnormalization\nAddition and\nnormalization\nFeedforward\nReLU subnet\nAddition and\nnormalization\nSoftmax\nlayer\nOutput word probabilities\nEncoder\nDecoder\nFigure 1: Transformer model for machine translation [18]\n2.1.1 Multi-head attention. The transformer in Figure 1 uses multi-\nhead scaled dot product attention blocks. Scaled dot product atten-\ntion involves computing linear combinations of a set of values. The\nso-called attention weights of these sums are determined by com-\nparing a number of queries to a collection of keys corresponding\nto the values: they are obtained by calculating scaled dot products\nand applying the softmax function to normalize w.r.t. the keys and\nvalues. Mathematically, this can be expressed as follows [18]:\nA(Q, K, V )= softmax\n \nQKT\np\ndk\n!\nV (1)\nIn Equation (1), Q ∈Rnq ×dk , K ∈Rnk ×dk and V ∈Rnk ×dv de-\nnote the matrices containing respectively the queries, the keys and\nthe values. The number of queries and the number of keys, which is\nequal to the number of corresponding values, are symbolized bynq\nand nk respectively. The dimension of the keys, which is the same\nas the dimension of the queries, and the dimension of the values\nare indicated by dk and dv respectively. The matrix A ∈Rnq ×dv\ncontains the resulting weighted sums [18].\nThe multi-head attention blocks add flexibility on top of this bare\nattention mechanism in two ways. Firstly, linear transformations\nare introduced before and after the computation of the scaled dot\nproducts. Secondly, instead of one linear combination, multiple at-\ntention heads, i.e., weighted sums, are calculated and concatenated\nper query, explaining the name of the component [18].\nThe multi-head attention block is zoomed in on in Figure 2.\nV K Q\nLinear\nmap\nLinear\nmap\nLinear\nmap\nMultiple\nheads\nMultiple\nheads\nScaled dot product attention\nConcatenation\nLinear map\nA\nFigure 2: Multi-head scaled dot product attention block [18]\nIn the transformer architecture for machine translation, the\nqueries, keys and values consist of abstract word token represen-\ntations. It has been demonstrated that applying the multi-head at-\ntention mechanism to these word embeddings in both the encoder\nand decoder allows the model to capture a variety of interesting\nintra- as well as interlingual word relationships [18].\n2.1.2 Positional encodings. Because of the absence of recurrent and\nconvolutional connections, transformers have no real perception\nof order, which is obviously an undesirable property for neural\nmachine translation models. This issue is alleviated by injecting\nsequential information via the introduction of positional encodings.\nThis comes down to adding abstract vector representations of word\npositions to both the input and output word embeddings. In the\nmachine translation transformer, sinusoidal positional encodings\nare employed: for this type of encodings, fixed position differences\ncan be represented by simple linear transformations [5, 18].\n2.2 Adaptations to original architecture\nDue to the similarities between machine translation and audiovisual\nclassification, fitting the original transformer to the task at hand is\nrelatively easy. Below, we discuss the relevant changes.\nThe resulting audiovisual transformer architecture incorporating\nall of these adaptations is depicted in Figure 3.\n2.2.1 Data adaptation. Linguistic data is sequential in nature, de-\nfined at discrete word positions. In this work, we ensure that the\nused audiovisual data complies to this format by converting the au-\ndio into sequences of frames and sampling images from the videos.\nMore detailed information is provided in Section 3.\nIn machine translation, the data from the source language is\nserved as inputs, while the data from the target language consti-\ntutes the desired outputs. For the task at hand, all data from the\nconsidered modalities, i.e., audio and vision, can be used as inputs.\n2.2.2 Introduction of extra linear maps. For the machine translation\ntransformer, the size of the word embeddings constrains the amount\nof neurons used by the linear transformations in the multi-head at-\ntention blocks. This is a consequence of the residual connections in\nthe model. To relax this restriction, the audiovisual transformer con-\ntains additional linear maps projecting the employed auditory and\nvisual embeddings to vectors of some predefined dimensionality.\n2.2.3 Change of output layer. In contrast to machine translation,\nfor classification of environmental events the prediction categories\nare not mutually exclusive. Indeed, while word positions can only be\noccupied by a single token, multiple environmental events can occur\nat the same time. The softmax layer at the end of the transformer\nis therefore changed into a sigmoid layer as it is more appropriate.\n2.2.4 Addition of aggregation block. The goal of the machine trans-\nlation transformer is to compute the probability of a sentence. This\nrequires iteration as conditional probabilities have to be calculated\nfor each word in order to eventually multiply them. However, for the\ntask at hand and the used data (see Section 3), we only need a single\nprobability vector per audiovisual clip. Thus, we add an aggregation\nblock at the end of the model which combines frame-level scores by\nperforming a mean or max pooling operation. Consequently, unlike\nin the original transformer, there is no more need for iteration in\nthe adapted variant: inputs need to be passed through this model\njust once to obtain sound event predictions.\n2.2.5 Modification of attention function. Originally, the softmax\nfunction was used to calculate attention values as is evident from\nEquation (1). We also test some other straightforward attention\nfunctions, namely the sigmoid function σ and the normalized sig-\nmoid function σ . The latter function is characterized as follows:\nσ (x)= σ (x)\nnÍ\ni=1\nσ (xi )+ ϵ\n(2)\nIn Equation (2), x ∈Rn is a vector and ϵ is a very small positive\nreal number used to avoid overflow.\n2.2.6 Optional deletion of positional encodings. Lastly, we turn\nthe addition of positional encodings at the inputs into an optional\noperation and perform experiments with and without these abstract\nrepresentations as it is unclear how important ordering information\nis for the task and data at hand.\nInput from\nfirst modality\nInput from\nsecond modality\nPositional\nencodings\nEmbedding Embedding\nLinear map Linear map\nN×\nN×\nMulti-head\nattention\nMulti-head\nattention\nAddition and\nnormalization\nAddition and\nnormalization\nFeedforward\nReLU subnet\nMulti-head\nattention\nAddition and\nnormalization\nAddition and\nnormalization\nFeedforward\nReLU subnet\nAddition and\nnormalization\nSigmoid\nlayer\nAggregation\nEvent probabilities\nEncoder\nDecoder\nOptionalOptional\nFigure 3: Audiovisual transformer model\n3 EXPERIMENTAL SETUP\nIn this section, we expand upon the experimental setup. We provide\ndetails on the audiovisual data set used to train the multimodal\ntransformers and discuss implementation details such as the used\ndata preprocessing and regularization methods.\n3.1 Data set\nThe large-scale multi-label data set for task 4 of DCASE 2017 con-\nsists of 51172 training, 488 validation and 1103 test audio samples\noriginated from YouTube. Most of these audio files are 10 seconds\nlong, some are shorter. They are weakly labeled: only clip-level\nannotations are provided, indicating the presence or absence of\n17 audio event classes, which all represent environmental sounds.\nTemporal knowledge is not included, meaning that there is no\ninformation about the time boundaries of the sounds [12].\nTaking an approach similar to a procedure used in prior work,\nYouTube downloads allow us to readily append visual data to this\naudio data set. Unfortunately, due to availability issues, videos could\nnot be obtained for all of the audio files in the original data set.\nUltimately, we ended up creating a slightly reduced collection of\naudio and video files consisting of 48883 training, 465 validation\nand 1050 test samples: about 5% of samples of the original data was\ndiscarded in the process [14].\n3.2 Implementation details\nThe audiovisual transformers are built using the TensorFlow toolkit.\nIn this subsection, some important implementation details are elab-\norated upon [1].\n3.2.1 Data preprocessing. To obtain auditory frame embeddings,\nfirst logarithmic mel spectrograms are extracted from the audio\nsamples. Next, a sliding window is applied to these spectrograms\nto divide the signal into frames. The length of this window and\nits hop length are set to 960 ms. The spectral frames are passed\nthrough vggish, a model for video tag classification using audio,\npretrained using a preliminary version of the YouTube-8M data set.\nThe 128-dimensional outputs of the last feedforward layer of this\ndeep convolutional network are used as frame embeddings [2, 8].\nVisual frame embeddings are obtained in the following way:\nfirstly, still images are sampled from the videos. The sampling times\nare chosen to coincide with the centers of the possible positions of\nthe sliding window used in the audio preprocessing described above.\nThe resulting video frames are then fed into VGG16, a deep convo-\nlutional model for image classification, pretrained on the ImageNet\ndata set. The 4096-dimensional outputs of the last feedforward layer\nof this neural network are used as visual frame embeddings [4, 16].\nMost of the preprocessing steps described above are included in\nthe TensorFlow toolkit and are therefore easy to implement [1].\n3.2.2 Data sampling. The audiovisual data used for training is\nstrongly unbalanced: the ratio of number of samples containing a\nsound of the most common event class to the number of examples\ncomprising the least likely event is approximately equal to 130.\nTo alleviate this issue, we use a sampling scheme proposed in\nprior work: for each training batch, class labels are non-uniformly\nsampled and appropriate data examples are fetched. This happens\nin a way that limits the maximum class imbalance ratio to 5 [20].\nWe train the models for a maximum of 30 epochs. In each training\nepoch, 300 batches consisting of 40 data samples are presented to\nthe networks, resulting in a total of 12000 examples per epoch.\n3.2.3 Number of encoder and decoder blocks. The number of en-\ncoder and decoder blocks is referred to with the letter N in Figure 3.\nWe set this hyperparameter equal to three.\n3.2.4 Number of attention heads. We use three attention heads in\nall multi-head attention blocks in both the encoder and decoder of\nthe audiovisual transformers.\n3.2.5 Feedforward ReLU subnetworks. The feedforward ReLU sub-\nnetworks referenced in Figure 3 match the ones used in the original\ntransformer shown in Figure 1: each subnet consists of one ReLU\nlayer followed by a linear layer without activation function [18].\n3.2.6 Number of neurons. All feedforward layers with and with-\nout activation functions, including all linear transformations used,\nconsist of 128 neurons.\n3.2.7 Regularization methods. Identically to the original trans-\nformer architecture, computation of the output of each multi-head\nattention block of the audiovisual transformer is followed by ad-\ndition with its corresponding residual connection and layer nor-\nmalization, a popular regularization technique which performs\nsample-wise scaling. We also apply dropout with a rate of 0.1 to\nthe outputs of the following parts of the model [3, 7, 17, 18]:\n•Linear layers at the input of the encoder and decoder\n•Attention function in the multi-head attention blocks\n•Final linear map in the multi-head attention blocks\n•Both layers of the feedforward ReLU subnetworks\n3.2.8 Loss function and optimization algorithm. We use the well-\nknown binary cross entropy loss function in conjunction with the\nAdam optimization algorithm (using standard settings as defined\nin the TensorFlow toolkit) to train the transformers [1, 10].\nEarly stopping is applied if at any point during training the\ncross entropy calculated on the validation data is higher than all\nvalidation losses obtained after the last 7 epochs.\n3.2.9 Epoch-level ensembling. Instead of only using the model ob-\ntained after the final training epoch to obtain audio event class\nprobabilities, we employ the epoch-level ensembling method pro-\nposed in the original work on transformers: after each of the last 7\ntraining epochs the model is saved. Eventually, these saved states\nare used to obtain 7 sets of probabilities, which are then averaged\nclass-wise to obtain a more robust output [18].\n3.2.10 Prediction thresholding. The final goal of the audiovisual\ntransformer models is to make binary decisions: they should predict\nwhether or not certain audio event classes are present in a given\nclip. The continuous probabilities at the output of the transformer\nsystems thus should be converted into binary values via thresh-\nolding. Preliminary experiments have shown that using a global\nthreshold which maximizes the performance of the models on the\nvalidation data is favorable to utilizing class-dependent optimal\nthresholds, hence the former approach is taken.\n4 EXPERIMENTAL RESULTS\nIn this section, we discuss the results we obtained by training the\naudiovisual transformer neural networks. We split this discussion\ninto two parts: the first is the quantitative analysis, where the\nperformance of the models is numerically evaluated and compared\nto other approaches, the second part is the qualitative analysis\nwhich provides interesting, interpretable insights into the workings\nof the audiovisual transformers.\nTable 1: F1 scores of audiovisual transformer models\nmodality of\nfirst input\nmodality of\nsecond input\nmodalities\nutilizing\npositional\nencodings\nF1 score\nmean pooling as aggregation method max pooling as aggregation method\nsoftmax\nattention\nsigmoid\nattention\nnormalized\nsigmoid\nattention\nsoftmax\nattention\nsigmoid\nattention\nnormalized\nsigmoid\nattention\naudio/video 66.1% 66.0% 66.3% 66.2% 66.3% 66.4%\nvideo 69.5% 69.8% 69.4% 69.3% 69.5% 69.4%audio video\nnone 69.2% 69.4% 69.3% 69.1% 69.3% 68.9%\naudio/video 67.7% 67.5% 67.8% 67.3% 67.1% 67.6%\nvideo 69.7% 70.1% 69.7% 69.7% 69.8% 69.7%video audio\nnone 69.6% 69.8% 69.5% 69.7% 69.7% 69.7%\nvideo 59.1% 59.3% 59.3% 59.0% 59.1% 59.0%video video none 58.8% 59.1% 59.0% 58.8% 58.8% 58.8%\naudio 57.0% (57.0%) 56.9% (57.0%) 56.8% (57.1%) 56.2% (56.4%) 56.4% (56.7%) 56.4% (56.7%)audio audio none 58.6% (59.3%) 59.1% (59.5%) 58.8% (59.3%) 58.5% (59.0%) 58.9% (59.4%) 58.7% (59.0%)\n4.1 Quantitative analysis\nWe quantitatively evaluate the audiovisual transformer models by\nmeasuring micro-averaged F1 scores on the test samples of the\nemployed data. This metric is chosen as it is very often utilized in\nthe field of audio event recognition, e.g., in task 4 of the DCASE\n2017 challenge, where the used data originates from [12, 13].\nWe report F1 scores which are averaged over 25 training runs\nto ensure the results are reliable. This is practically feasible as a\nconsequence of the use of fixed pretrained embeddings and due\nto the fact that transformers are very quick to train, in contrast to\nneural models containing recurrent connections.\nTable 1 contains the measured F1 scores on the audiovisual data\nset created in this work which is described in Section 3. The referred\nsettings are described in detail in Section 2. This table contains\nresults for multimodal transformers employing both audio and\nvideo as well as transformers utilizing only a single modality. For\nthe models using only audio, performance on the slightly larger\n(complete) data set of task 4 of the DCASE 2017 challenge also\nmentioned in Section 3 is reported between parentheses [12].\nIn the rest of this section, we analyze these numerical results\nand compare to approaches presented in prior works.\n4.1.1 Evaluation of positional encodings. The unimodal video trans-\nformers benefit from the utilization of positional encodings, albeit\nonly slightly. However, employing these abstract position represen-\ntations is strongly detrimental to the models utilizing only audio.\nThe micro-averaged F1 scores obtained by multimodal transform-\ners employing both audio and video show a similar trend: adding\npositional encodings to the used visual frame embeddings provides\nminor improvements, adding these abstract position representa-\ntions to both the auditory and visual embeddings causes a severe\ndeterioration in performance.\n4.1.2 Evaluation of aggegration methods and attention functions.\nGenerally, audiovisual transformers employing mean pooling as\naggregation method seem to perform slightly better than those us-\ning max pooling. The disparity in F1 scores is the most pronounced\nfor the unimodal models utilizing only audio.\nRemarkably, transformers utilizing different types of attention\nfunctions perform very similarly. In particular, we note that there\nis no meaningful difference in F1 scores between models employ-\ning normalized (softmax, normalized sigmoid) and unnormalized\nattention functions (sigmoid).\n4.1.3 Comparison to prior work in audio classification. We list rele-\nvant prior audio event classification results measured on the test\nsamples of the data set of task 4 of the DCASE 2017 challenge in\nTable 2. These micro-averaged F1 scores can most directly be com-\npared to the outcomes reported between parentheses in Table 1, as\ntheir computation is based on the same data [12].\nTable 2: F1 scores of prior audio classification models\nmodel F1 score\nfusion of gated convolutional recurrent networks [21] 55.6%\ncapsule-based gated convolutional network [9] 58.6%\nThe transformers outdo the models in Table 2. We speculate\nthat this can be attributed to the power of the multi-head attention\nmechanism and the fact that they employ rich, externally pretrained\nembeddings instead of training audio features from scratch.\n4.1.4 Comparison to prior work in audiovisual classification. The\nmultimodal transformers employing both audio and video can be\nfairly compared to the two-stream audiovisual neural network in-\ntroduced in prior work: this model performs environmental audio\nevent classification based on audiovisual data acquired in the same\nmanner as the procedure used in this work. The micro-averaged F1\nscore achieved by this model is reported in Table 3.\nTable 3: F1 score of prior audiovisual classification model\nmodel F1 score\ntwo-stream audiovisual neural network [14] 64.2%\nFigure 4: Visualization for transformer using video/audio as first/second input modalities (sound produced by train)\nThe multimodal audiovisual transformers strongly improve upon\nthe two-stream audiovisual neural network. As both of these types\nof models use nearly the same data and employ similar externally\npretrained embeddings, this truly illustrates the power of the trans-\nformer architecture for the task at hand.\nAs can be inferred from Table 1, the best performing multi-\nmodal transformer type achieves a micro-averaged F1 score of\n70.1%. To the best of our knowledge, this is the best classification\nresult achieved for the considered data.\n4.1.5 Comparison of unimodal and multimodal transformers. A\ncomparison between the transformers using both audio and video\nand the transformers utilizing only audio unveils the usefulness of\nvisual information for environmental audio event classification: on\naverage, there is a difference in F1 scores of about 10%.\nInitially, it is reasonable to expect unimodal transformers em-\nploying visual information to perform worse than those utilizing\naudio, since the annotation of the data at hand only indicates the\npresence or absence of audio event classes as outlined in Section 3.\nAdditionally, the used audiovisual data set was created by simply\nextending labeled audio files originated from YouTube with their\ncorresponding videos, as also discussed in Section 3, and therefore\nthere is no guarantee that the appended visual material contains\nvaluable information. By conducting a manual inspection of the\ndata set, we were even able to confirm that not all of the down-\nloaded videos are informative for the considered classification task.\nDespite these remarks, we still find that the unimodal visual models\nslightly outperform the unimodal models based on audio.\nThis peculiar observation can be explained by the fact that in\nmany cases, images or videos are more useful than auditory signals.\nFor example, multiple classes in the used data set pertain to similar\nvehicle sounds such as police car, fire truck and ambulance sirens:\neven for humans, differentiating these events solely based on audio\ncan be quite difficult and visual information may be of great help.\nAnother plausible reason for this strange outcome is a disparity in\nrichness of the used pretrained auditory and visual embeddings,\nwhich are described in Section 3.\n4.2 Qualitative analysis\nWe qualitatively analyze the transformers utilizing both audio and\nvideo by visualizing cross-modal attention weights in a manner\nsimilar to a procedure followed in prior work, to investigate the\nability of these models to perform multimodal synchronization [18].\n4.2.1 Models using video/audio as first/second inputs respectively.\nFigures 4, 5 and 6 contain visualizations for transformers without\npositional encodings which utilize video as first and audio as sec-\nond input, mean pooling for aggregation and employ the softmax\nattention function. A full description of these configurations can\nbe found in Section 2. Each figure contains a number of auditory\n(spectral) and visual frames for a test sample of the audiovisual\ndata set described in Section 3, as well as a heat map of the cross-\nmodal attention weights obtained for the circled audio query frame\nfrom the cross-modal multi-head attention component in the last\ndecoder block of the corresponding model. Each row in each heat\nmap pertains to a different attention head.\nIn all of the cases visualized in Figures 4, 5 and 6, the models are\nable to detect visual frames relevant to the environmental sounds\npresent in the circled audio query frames.\nFigure 5: Visualization for transformer using video/audio as first/second input modalities (sound produced by motorcycle)\nFigure 6: Visualization for transformer using video/audio as first/second input modalities (police car and fire truck sirens)\nFigure 7: Visualization for transformer using audio/video as first/second input modalities (sound produced by skateboard)\nFor models using video as first input and audio as second input,\nwe were able to discover similarly interesting attention patterns\nfor many other validation and test samples. Strangely, this was\nonly possible for audiovisual transformers employing the softmax\nattention function: the architectures utilizing the sigmoid and nor-\nmalized sigmoid attention functions did not allow for meaningful\nvisualizations, even though they achieve comparable performance\nas reported in Section 4.1.2.\n4.2.2 Models using audio/video as first/second inputs respectively.\nIn Figure 7, we present an example for an audiovisual transformer\nwith visual positional encodings which utilizes audio and video\nas first and second input modalities respectively, mean pooling as\naggregation method and uses softmax attention. The figure contains\n10 (spectral) auditory and visual frames for a validation sample of\nthe data set described in Section 3, as well as a heat map of the\nattention weights obtained for the circled video query frame from\nthe cross-modal multi-head attention component in the last decoder\nblock of the associated model. Each row in the heat map pertains\nto a different attention head.\nThe circled video query frame of the sample visualized in Figure 7\nis a picture of someone skateboarding. Through its attention heads,\nthe corresponding model is able to detect the relevant auditory\nframes containing skateboarding sounds, which are circled.\nFor this type of transformer, discovering synchronization pat-\nterns in the attention weights proved to be quite difficult. This can\nlikely be explained by the fact that sounds are naturally much less\nlocalized compared to visual events. Indeed, also for humans, point-\ning to specific audio frames as containing valuable information is\nmuch harder than pointing to specific video frames, i.e., images.\n5 CONCLUSION\nWe tackled the task of environmental event classification by draw-\ning inspiration from machine translation: we changed the popular\ntransformer architecture into a model which can use auditory as\nwell as visual data to compute multi-label audio event predictions.\nWe experimented with these transformers on an audiovisual\ndata set, obtained by extending an existing weakly labeled auditory\ndata set with visual information. The used multi-label data con-\ntains clip-level annotation indicating the presence of 17 classes of\nenvironmental sounds, without disclosure of temporal information.\nWe showed that the proposed models strongly improve upon\nprior approaches in the fields of audio as well as audiovisual classi-\nfication and achieve state-of-the-art results.\nWe also proved the usefulness of visual information for the task\nat hand, i.e., audio event recognition. This shows that it certainly\ncan be worth researching multimodal audiovisual models.\nIn addition, we visualized internal attention patterns of the au-\ndiovisual transformers. This way, we were able to demonstrate that\nthey show potential for multimodal synchronization.\nIn the future, we will use these transformers to tackle some other\naudiovisual data collections and learn more about their properties.\nIn particular, we are keen to analyze the impact of scale: we will\nexperiment with the small-scale data set of task 4 of DCASE 2018,\nand Audio Set, which is humongous in size. In addition, we will look\ninto how the models can be used for unsupervised tasks [6, 15].\nACKNOWLEDGMENTS\nThis work is supported by a PhD Fellowship of Research Foundation\nFlanders (FWO-Vlaanderen).\nREFERENCES\n[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\nDean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.\n2016. TensorFlow: A System for Large-Scale Machine Learning. In 12th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 16) . 265–283.\n[2] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici,\nBalakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. 2016. Youtube-8M:\nA Large-Scale Video Classification Benchmark. arXiv preprint arXiv:1609.08675\n(2016).\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer Normaliza-\ntion. arXiv preprint arXiv:1607.06450 (2016).\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Ji Li, Kai Li, and Li Fei-Fei. 2009. ImageNet:\nA Large-Scale Hierarchical Image Database. In CVPR09.\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\n2017. Convolutional Sequence to Sequence Learning. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70 . 1243–1252.\n[6] Jort F. Gemmeke, Daniel P.W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,\nR. Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio Set: An\nontology and human-labeled dataset for audio events. In 2017 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) . 776–780.\n[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nLearning for Image Recognition. InProceedings of the IEEE conference on computer\nvision and pattern recognition . 770–778.\n[8] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F. Gemmeke, Aren\nJansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan\nSeybold, et al. 2017. CNN Architectures for Large-Scale Audio Classification.\nIn 2017 IEEE International Conference on Acoustics, Speech and Signal processing\n(ICASSP). 131–135.\n[9] Turab Iqbal, Yong Xu, Qiuqiang Kong, and Wenwu Wang. 2018. Capsule Routing\nfor Sound Event Detection. In 2018 26th European Signal Processing Conference\n(EUSIPCO). 2255–2259.\n[10] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[11] Annamaria Mesaros, Toni Heittola, Emmanouil Benetos, Peter Foster, Mathieu\nLagrange, Tuomas Virtanen, and Mark D. Plumbley. 2018. Detection and Classifi-\ncation of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge.\nIEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) 26, 2\n(2018), 379–393.\n[12] Annamaria Mesaros, Toni Heittola, Aleksandr Diment, Benjamin Elizalde, Ankit\nShah, Emmanuel Vincent, Bhiksha Raj, and Tuomas Virtanen. 2017. DCASE 2017\nchallenge setup: Tasks, datasets and baseline system. In DCASE 2017-Workshop\non Detection and Classification of Acoustic Scenes and Events .\n[13] Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen. 2016. Metrics for\nPolyphonic Sound Event Detection. Applied Sciences 6, 6 (2016), 162.\n[14] Sanjeel Parekh, Slim Essid, Alexey Ozerov, Ngoc QK Duong, Patrick Pérez, and\nGaël Richard. 2018. Weakly Supervised Representation Learning for Unsynchro-\nnized Audio-Visual Events. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition Workshops . 2518–2519.\n[15] Mark D. Plumbley, Christian Kroos, Juan P. Bello, GaÃńl Richard, Daniel P.W.\nEllis, and Annamaria Mesaros. 2018.Proceedings of the Detection and Classification\nof Acoustic Scenes and Events 2018 Workshop (DCASE2018) .\n[16] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-\nworks for Large-Scale Image Recognition. InInternational Conference on Learning\nRepresentations.\n[17] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Processing Systems . 5998–6008.\n[19] Tuomas Virtanen, Annamaria Mesaros, Toni Heittola, Aleksandr Diment, Em-\nmanuel Vincent, Emmanouil Benetos, and Benjamin Martinez Elizalde. 2017.\nProceedings of the Detection and Classification of Acoustic Scenes and Events 2017\nWorkshop (DCASE2017).\n[20] Yong Xu, Qiuqiang Kong, Wenwu Wang, and Mark D Plumbley. 2017. Surrey-\nCVSSP system for DCASE2017 challenge task4. arXiv preprint arXiv:1709.00551\n(2017).\n[21] Yong Xu, Qiuqiang Kong, Wenwu Wang, and Mark D Plumbley. 2018. Large-scale\nweakly supervised audio classification using gated convolutional neural network.\nIn 2018 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). 121–125.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7234218716621399
    },
    {
      "name": "Synchronization (alternating current)",
      "score": 0.5527339577674866
    },
    {
      "name": "Transformer",
      "score": 0.49767592549324036
    },
    {
      "name": "Speech recognition",
      "score": 0.4799673557281494
    },
    {
      "name": "Scale (ratio)",
      "score": 0.436600923538208
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32896971702575684
    },
    {
      "name": "Computer network",
      "score": 0.12466207146644592
    },
    {
      "name": "Engineering",
      "score": 0.11682963371276855
    },
    {
      "name": "Electrical engineering",
      "score": 0.07399305701255798
    },
    {
      "name": "Voltage",
      "score": 0.05250483751296997
    },
    {
      "name": "Physics",
      "score": 0.051416367292404175
    },
    {
      "name": "Channel (broadcasting)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99464096",
      "name": "KU Leuven",
      "country": "BE"
    }
  ]
}