{
  "title": "Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval",
  "url": "https://openalex.org/W4385895960",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098185351",
      "name": "Bin Yi",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2333383402",
      "name": "Li Haoxuan",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2681017036",
      "name": "Xu Yahui",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2127853080",
      "name": "Xu Xing",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2093908790",
      "name": "Yang Yang",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2294180459",
      "name": "Shen, Heng-Tao",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3206723373",
    "https://openalex.org/W2766237723",
    "https://openalex.org/W2020842694",
    "https://openalex.org/W6600688380",
    "https://openalex.org/W3175888430",
    "https://openalex.org/W2982260486",
    "https://openalex.org/W2998215884",
    "https://openalex.org/W3026458074",
    "https://openalex.org/W3100308117",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2070753207",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W1957706851",
    "https://openalex.org/W2982078236",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2190008860",
    "https://openalex.org/W3206675006",
    "https://openalex.org/W2138118304",
    "https://openalex.org/W3155230099",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3003735286",
    "https://openalex.org/W4288086191",
    "https://openalex.org/W2765440071",
    "https://openalex.org/W6601676141",
    "https://openalex.org/W3010277541",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W4386071847",
    "https://openalex.org/W2997525715",
    "https://openalex.org/W3035588244",
    "https://openalex.org/W3005971801",
    "https://openalex.org/W2887712318",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W4312761738",
    "https://openalex.org/W3035552787",
    "https://openalex.org/W2883311563",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W4310671573",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2972312591",
    "https://openalex.org/W2981586349",
    "https://openalex.org/W4300954432",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2115752676",
    "https://openalex.org/W4225681728",
    "https://openalex.org/W4385934203",
    "https://openalex.org/W2106277773",
    "https://openalex.org/W3029678209",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2123024445",
    "https://openalex.org/W3035454331",
    "https://openalex.org/W639708223",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4313855701",
    "https://openalex.org/W1527575280",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2778940641",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4317796337",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4319663732",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2988823324",
    "https://openalex.org/W4287547182",
    "https://openalex.org/W3092820619",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288102874",
    "https://openalex.org/W2990428574",
    "https://openalex.org/W2006031689",
    "https://openalex.org/W2774267535",
    "https://openalex.org/W3035605030",
    "https://openalex.org/W2994818707",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Most existing cross-modal retrieval methods employ two-stream encoders with different architectures for images and texts, \\textit{e.g.}, CNN for images and RNN/Transformer for texts. Such discrepancy in architectures may induce different semantic distribution spaces and limit the interactions between images and texts, and further result in inferior alignment between images and texts. To fill this research gap, inspired by recent advances of Transformers in vision tasks, we propose to unify the encoder architectures with Transformers for both modalities. Specifically, we design a cross-modal retrieval framework purely based on two-stream Transformers, dubbed \\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image Transformer, a text Transformer, and a hierarchical alignment module. With such identical architectures, the encoders could produce representations with more similar characteristics for images and texts, and make the interactions and alignments between them much easier. Besides, to leverage the rich semantics, we devise a hierarchical alignment scheme to explore multi-level correspondences of different layers between images and texts. To evaluate the effectiveness of the proposed HAT, we conduct extensive experiments on two benchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that HAT outperforms SOTA baselines by a large margin. Specifically, on two key tasks, \\textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves 7.6\\% and 16.7\\% relative score improvement of Recall@1 on MSCOCO, and 4.4\\% and 11.6\\% on Flickr30k respectively. The code is available at \\url{https://github.com/LuminosityX/HAT}.",
  "full_text": "Unifying Two-Stream Encoders with Transformers for\nCross-Modal Retrieval\nYi Binâˆ—\nNational University of Singapore\nSingapore\nyi.bin@hotmail.com\nHaoxuan Liâˆ—\nUniversity of Electronic Science and\nTechnology of China\nChengdu, China\nlhx980610@gmail.com\nYahui Xu\nUniversity of Electronic Science and\nTechnology of China\nChengdu, China\nyahui2727@163.com\nXing Xu\nUniversity of Electronic Science and\nTechnology of China\nChengdu, China\nxing.xu@uestc.edu.cn\nYang Yangâ€ \nUniversity of Electronic Science and\nTechnology of China\nChengdu, China\nyang.yang@uestc.edu.cn\nHeng Tao Shen\nUniversity of Electronic Science and\nTechnology of China\nChengdu, China\nshenhengtao@hotmail.com\nABSTRACT\nMost existing cross-modal retrieval methods employ two-stream\nencoders with different architectures for images and texts,e.g., CNN\nfor images and RNN/Transformer for texts. Such discrepancy in\narchitectures may induce different semantic distribution spaces and\nlimit the interactions between images and texts, and further result\nin inferior alignment between images and texts. To fill this research\ngap, inspired by recent advances of Transformers in vision tasks,\nwe propose to unify the encoder architectures with Transformers\nfor both modalities. Specifically, we design a cross-modal retrieval\nframework purely based on two-stream Transformers, dubbed Hi-\nerarchical Alignment Transformers (HAT) , which consists of\nan image Transformer, a text Transformer, and a hierarchical align-\nment module. With such identical architectures, the encoders could\nproduce representations with more similar characteristics for im-\nages and texts, and make the interactions and alignments between\nthem much easier. Besides, to leverage the rich semantics, we devise\na hierarchical alignment scheme to explore multi-level correspon-\ndences of different layers between images and texts. To evaluate\nthe effectiveness of the proposed HAT, we conduct extensive ex-\nperiments on two benchmark datasets, MSCOCO and Flickr30K.\nExperimental results demonstrate that HAT outperforms SOTA\nbaselines by a large margin. Specifically, on two key tasks, i.e.,\nimage-to-text and text-to-image retrieval, HAT achieves 7.6% and\n16.7% relative score improvement of Recall@1 on MSCOCO, and\n4.4% and 11.6% on Flickr30k respectively. The code is available at\nhttps://github.com/LuminosityX/HAT.\nâˆ—Both authors contributed equally to this research.\nâ€ Yang Yang is the corresponding author (Email: yang.yang@uestc.edu.cn).\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0108-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3581783.3612427\nCCS CONCEPTS\nâ€¢ Networks â†’Network design principles ; â€¢ Information sys-\ntems â†’Retrieval models and ranking; â€¢ Computing methodolo-\ngies â†’Information extraction.\nKEYWORDS\ncross-modal retrieval, discrepancy in architectures, hierarchical\nalignment transformers.\nACM Reference Format:\nYi Bin, Haoxuan Li, Yahui Xu, Xing Xu, Yang Yang, and Heng Tao Shen.\n2023. Unifying Two-Stream Encoders with Transformers for Cross-Modal\nRetrieval. In Proceedings of the 31st ACM International Conference on Multi-\nmedia (MM â€™23), October 29-November 3, 2023, Ottawa, ON, Canada.ACM,\nNew York, NY, USA, 10 pages. https://doi.org/10.1145/3581783.3612427\n1 INTRODUCTION\nHumans perceive and interact with the physical world via vari-\nant ways, e.g., vision, sound and tactile sense. To make the ma-\nchine simulate the perceiving process, simultaneously analyzing\ndata from multiple modalities is a fundamental and important abil-\nity. Visual information and text data are the two most prevalent\nmodalities in our daily life, and the research of vision and lan-\nguage also has been attracting broad attention in the past couples\nof years [4, 18, 25, 31, 68]. Benefiting from the great successes of\ndeep learning in computer vision (CV) and natural language pro-\ncessing (NLP), many tasks and problems associating vision and\nlanguage for multi-modal analysis also have achieved tremendous\nprogress, such as cross-modal retrieval [18, 31, 33, 51, 55], visual\nquestion answering (VQA) [1, 43], visual captioning [ 3, 25], and\nother tasks [11]. In this paper, we focus on the study of cross-modal\nretrieval, which is a fundamental multi-modal understanding task\nand benefits plenty of multimedia applications. However, it is still\nvery challenging for accurate retrieval, due to the requirements\nof exploring precise cross-modal alignment and comprehensive\ninter/intra-modal relations and interactions.\nTo align images and texts, early works apply canonical correla-\ntion analysis (CCA) to establish inter-modal associations between\ndifferent modalities with subspace learning [20, 29, 44, 49], or em-\nploy topic models to capture the relationship in the multi-modal\njoint distribution space [5, 24, 45]. With the development of deep\narXiv:2308.04343v1  [cs.CV]  8 Aug 2023\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Bin et al.\nlearning techniques, deep neural networks, e.g., CNN and RNN, are\napplied to extract visual and textural representations, followed with\nprojection functions to learn the mapping from the uni-modal to\ncross-modal spaces [18, 28, 61]. However, such holistic uni-modal\nrepresentations capture only the salient instances in the images\nor texts while ignore the non-salient ones or subtle relationships\namong instances. It therefore fails to explore the comprehensive\nfine-grained semantic associations underlying images and texts. To\ntackle this problem, many works step further to devise fine-grained\nalignment frameworks [25, 42], which first associate image regions\nand text words on the fragment-level, and aggregate the matched\nfragment pairs to obtain the final image-text pair. Besides, based on\nthe fragments, rich intra-modal interactions also could be explored\nand used to improve the retrieval accuracy.\nThe above approaches extract image and text features in two\nindependent streams, which cannot incorporate any cross-modal\ninteractions, with the heterogeneous gap between modalities re-\nmained. Actually, semantics in different modalities are compli-\ncated and diverse, which means that borrowing information from\nother modality may lead to better semantic representations. To\nthis end, some works model cross-modal interactions to help with\nthe uni-modal representation learning, for example, with the idea\nof fragment-level alignment [ 7, 31, 47, 62], which can be imple-\nmented by recurrent cross-modal messages passing, attentional\naggregation, and etc. From another perspective, motivated by the\npowerful representations of pre-trained Transformer in NLP, e.g.,\nBERT [26], ALBERT [30] and BART [32]. Many researchers dedi-\ncate to designing unified single-stream multi-modal representation\npre-training frameworks [35, 36, 67], and have achieved inspiring\nprogress. However, comprehensive interactions across modalities\nwithin single-stream are computationally expensive and suffer from\nhigh latency comparing with two-stream methods during inference,\nbecause they need to extract the representations of the given image-\ntext pair via the whole model.\nIn this paper, we focus on the two-stream framework, which is\ncomputational-friendly and more applicable for real world scenar-\nios. Compared with previous works [35, 36] which apply different\nvisual or textural backbones, e.g., CNN and RNN/Transformer, we\ndesign an entire Transformer-based model, namely Hierarchical\nAlignment Transformers (HAT), to eliminate the architecture dis-\ncrepancy between encoders in two-stream image-text matching\nframework. Specifically, motivated by the recent successes of vision\nTransformers, e.g., ViT [16], MAE [21], and Swin Transformer [39],\nwe employ Transformers for both image and text representation\nlearning steams. The unified architectures for different modalities\nmake the learned representations more compatible for semantic\nmapping and similarity measuring. Besides, previous works [40, 71]\npoint out that the shallow layers in CNN learn the texture features\nand the higher layers learn more semantic aspects. Transformers\nalso exhibit similar property, e.g., shallow layers in BERT tend to\nlearn the static representation of entities while the higher ones\ncapture more semantics in context [15, 52]. Towards this end, we\nintroduce a hierarchical alignment strategy to capture the rich cor-\nrespondences between images and texts with different semantic\nlevels. Most existing cross-modal retrieval methods only utilize\nsingle-level alignment to associate images and texts, while our pro-\nposed HAT integrates multi-level associations for final retrieval\nresults. In summary, the main contributions of this paper are:\nâ€¢We propose to unify the two-stream encoders of images and\ntexts with Transformers in cross-modal retrieval, which makes\nthe learned image and text representations more compatible for\naligning images and texts.\nâ€¢To comprehensively explore cross-modal interactions, we pro-\npose a hierarchical alignment strategy associating images and\ntexts with multi-level semantic clues.\nâ€¢Extensive experiments have been conducted on two commonly-\nused datasets, i.e., MSCOCO and Flickr30K. The experimental\nresults demonstrate that our proposed HAT outperforms all the\nSOTAs by a large margin.\n2 RELATED WORKS\n2.1 Cross-Modal Retrieval\nGlobal matching aims to explore the correlation by projecting\nthe entire image and text into a common semantic space. The early\nmethod proposed by Frome et al.[18], utilizing a CNN and Skip-\nGram [41] for visual and language feature extraction, respectively.\nKiros et al.[28] made the first attempt to encode text using a GRU\nand designed a triplet ranking loss to optimize the model. To make\nmore usage of informative pairs, some researchers have turned to\ndesign a more novel objective function. Faghri et al.[17]integrated\nthe hard negative mining technology into the triplet loss function.\nRecently, Zhenget al.[74] proposed a discriminative feature embed-\nding method with instance loss. Although these methods are very\neffective, they ignore the local cues between regions and words.\nLocal matching steps further to explore fine-grained corre-\nspondences between images and texts by extracting local features\n(e.g., image regions and text words). Karpathy et al.[25] first pro-\nposed to extract features for each image region with R-CNN [19],\nand then used the most similar region-word pairs for image-text\nmatching. With the great success of attention mechanism [2], Niu\net al.[42] extracted phrase-level features of text by using the LSTM\nto further align image regions with text words. Following the at-\ntention mechanism, a stacked cross attention model was proposed\nby Lee et al.[31], which finds all latent alignments between them\nby selectively aggregating regions and words. Later, Wu et al.[64]\nintroduced the self-attention mechanism and Qu et al. [47] de-\nsigned four types of interaction cells for cross-modal interaction,\nand introduced a dynamic routing mechanism to dynamically se-\nlect interaction paths based on inputs. More recently, Zhang et\nal. [72] proposed Negative-Aware Attention Framework based on\nthe SCAN method, which focuses on mining the negative effects\nof mismatched fragments and ultimately achieves more accurate\nsimilarity calculation. Chen et al. claimed that the strategy for\nfeature aggregation is crucial, and thus proposed the VSE âˆ[9]\nmethod, which learns the most suitable and best pooling strategy\nvia a generalized pooling operator (GPO)and sets a new SOTA.\n2.2 Transformers\nLanguage Transformers is first proposed by Vaswani et al. [54]\nfor machine translation, which is solely based on attention mecha-\nnisms, i.e., self-attention and multi-head attention. In the past few\nUnifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nYoung girl kicking \naround soccer balls \non a large soccer field.\nPos-emb\nCross-\nattention\nSwin Transformer \nBlock\nSwin Transformer \nBlock\nSwin Transformer \nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nHierarchical Alignment Module\nCross-\nattention\nCross-\nattention\nSimilarity \nMatrix\nWord\nEmbedding\nPatch\nPartition\nLinear\nEmbedding\nPatch\nMerging\nPatch\nMerging\nFigure 1: The overall flowchart of the proposed HAT. HAT consists of two-stream of Transformers, BERT and Swin Transformer\nin specific, for image and text encoding, and a hierarchical alignment module equipped with cross-attention mechanism. The\nhierarchical alignment module aligns the image and text representations from different semantic levels.\nyears, it has been widely used in natural language processing, e.g.,\nword representation, sentence representation and question answer-\ning, and achieved the state of the arts on a wide range of tasks.\nMotivated by the great success of the pre-training and fine-tuning\nscheme in computer vision, Howard et al.[23] proposed to design\nand pre-train a universal language model, based on Transformer, for\ntext classification, which significantly outperforms the state of the\narts and encourages the community to explore the potentials of pre-\ntraining for other NLP tasks. BERT [26] is one of the most famous\nand popular works, which employs several Transformer blocks for\nsentence encoding and designs a masked language model to learn\nthe comprehensive and contextual word representation. The simple\nyet powerful BERT could be pre-trained by self-supervision with\nvery large amount of data, yielding gorgeous results on a various of\nNLP tasks. This makes the community step further to design power-\nful architectures with Transformer to understand the language by\nmining large scale data. Brown et al.[6] devised an extremely large\nGenerative Pre-trained Transformer 3,175 billion parameters, as\nknown as GPT-3, pre-trained on 45TB plain text data, which could\nachieve strong performance on various downstream tasks without\nany fine-tuning. Besides, XLNET [69], ALBERT [30], and T5 [48] are\nsome representative works of large scale pre-training Transform-\ners. These powerful Transformer-based models, with the strong\nrepresentation capacity, have achieved significant breakthroughs\nin every respect of NLP.\nVision Transformers is by the great progress and success of\nTransformer architectures in the field of NLP, and have been ap-\nplied to replace CNNs on computer vision tasks. Chen et al.[10]\nproposed a pioneering generative pre-training Transformer frame-\nwork that predicts pixels instead language tokens to learn visual\nrepresentation with self-supervision. Their pixel Transformer treats\nthe images as pixel sequences and auto-regressively predicts the\npixels without 2D structure, and still achieves competitive results\non several classification tasks. ViT [ 16] directly implemented a\npure Transformer to the sequences of image patches and performs\nwell on image classification with sufficient training data. Another\npopular work is Swin-Transformer [ 39], constructing hierarchi-\ncal feature maps via a novel shifted windowing scheme and re-\ntaining only linear computational complexity to image size. With\nsuch hierarchical feature maps, the Swin Transformer could be\nconveniently equipped with advanced techniques and applied to\nvariants downstream applications, e.g., object detection, semantic\nand segmentation. More recently, He et al.[21] devised a simple\nframework of masked autoencoders (MAE) with an asymmetric\nencoder-decoder architecture to operate only on the visible subset\nof patches and reconstruct the original image from the latent repre-\nsentation and masked tokens. The MAE also reveals that masking\na high proportion of the input image could yield nontrivial results,\nas well as accelerating the training procedure. In addition to above\nworks, there also exist many vision Transformers for other vision\ntasks, including video understanding [56, 75], instance/semantic\nsegmentation [58, 65], and multi-modal understanding [35, 36].\n3 THE PROPOSED APPROACH\nMost existing cross-modal retrieval approaches [ 31, 47, 59] em-\nploy different architectures for representation learning, e.g., CNNs\nas image encoder and RNNs/Transformers as text encoder, and\nmeasure the similarity between images and texts only utilizing\nsingle level correspondences. The discrepancy between the encoder\nstructures of images and texts leads to different feature property\nand may result in inferior cross-modal performance. In this works,\nwe propose a novel framework, termed as Hierarchical Alignment\nTransformers (HAT), to unify the two-stream encoder architectures\nwith Transformers [54], and hierarchically align images and texts\nwith multi-level semantic correspondences. As the visual pipeline\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Bin et al.\nillustrated in Figure 1, the proposed HAT consists of Text Trans-\nformer, Image Transformer and Hierarchical Alignment Modules.\n3.1 Text Transformer\nGiven a text sentenceğ‘† = {ğ‘¤ğ‘¡|ğ‘¤1,ğ‘¤2,...,ğ‘¤ ğ‘‡}consisting ofğ‘‡ words,\nthe basic idea is to employ a recurrent neural network, e.g., an\nLSTM or GRU, to learn the sentence representation. To capture\nfine-grained correspondences between images and texts, the output\nstate of each step also be utilized as the local word representa-\ntion [31]. To enable such fine-grained association, we employ a\nTransformer structure, BERT in specific, to extract the contextual\nrepresentations of words, which could easily explore the bidirec-\ntional context interactions between all the word pairs. Specifically,\ngiven a sentence ğ‘†, BERT first projects each word ğ‘¤ğ‘¡ into a con-\ntiguous embedding space ğ‘†ğ‘’ = {ğ‘’ğ‘¡|ğ‘’1,ğ‘’2,...,ğ‘’ ğ‘‡}. Then, each word\nembedding ğ‘’ğ‘¡ is integrated with a positional embedding and a seg-\nment embedding via addition to feed into the Transformer blocks,\nand learn the contextual representation of each word with attention.\nWe finally obtain the word-level representations for the sentence\nğ‘†, denoted as ğ‘†ğ‘™ = {ğ‘¤ğ‘™\nğ‘¡|ğ‘¤ğ‘™\n1,ğ‘¤ğ‘™\n2,...,ğ‘¤ ğ‘™\nğ‘‡}, where ğ‘™ denotes the ğ‘™-th\nTransformer block of BERT. In this work, we use BERT-base con-\nsisting of 12 Transformer blocks. As pointed out in [15, 52], different\nlayers in BERT capture different levels of semantics, we therefore\nadopt the outputs of multiple Transformer blocks for multi-level\nsemantic representations of sentences and compute the hierarchical\nassociations with image representations.\n3.2 Image Transformer\nDifferent from adopting CNN-based models to learn image fea-\ntures, e.g., faster-RCNN [50] or ResNet [22], inspired by the recent\nadvances of vision Transformers [21, 39], we employ Transformer-\nbased model to encapsulate the images for dense representations\nand unify the backbones of different modalities. Specifically, we im-\nplement the recent successful Swin Transformer [39] as our vision\nbackbone, which significantly outperforms CNN and shows great\npotential in many vision tasks. Swin Transformer, equipped with a\nshifted windowing scheme, is able to construct hierarchical feature\nmaps and has linear computational complexity to image size.\nGiven an imageğ¼ âˆˆR3Ã—ğ»Ã—ğ‘Š, we first split the image into several\nnon-overlapping patches via the patch partition module, as most\nexisting vision Transformers. Each patch is with the size of 4 Ã—4\npixels and treated as a â€œtokenâ€ to feed into the Transformer blocks.\nInspired by previous work [14], we take the output feature maps of\nmultiple stages as hierarchical representations of images to express\nmore rich semantics, and denote the output feature map of each\nstage as ğ¼ğ‘–, where ğ‘– indicates the ğ‘–-th stage. In specific, we discard\nthe output of â€œStage 1â€ due to its large number of tokens that results\nin high dimension and expensive computation costs. The outputs of\nsubsequent stages, i.e., Stage 2-4, are also termed as low-, middle-,\nand high-level semantic representations of images, then the overall\nimage hierarchical representation ğ¼â„ could be denoted as:\nğ¼â„ = ğ‘†ğ‘¤ğ‘–ğ‘›ğ‘‡(ğ¼)= {ğ¼ğ‘ 2,ğ¼ğ‘ 3,ğ¼ğ‘ 4}. (1)\nTo be consistent with BERT layer representation, we convert stages\nin ğ¼ğ‘ 2, ğ¼ğ‘ 3, and ğ¼ğ‘ 4 to corresponding layers, i.e., 4, 10, 12-th layer in\nSwin Transformer.\n3.3 Hierarchical Alignment Module\nAfter obtained the representations of images and texts, the impor-\ntant thing is to align them with semantic correspondences. Most\nexisting works measure the similarities between images and texts\nwith the last layers of two-stream encoders [ 31]. As previous re-\nsearch pointed out, different layers of BERT capture different-level\nsemantics [15, 52], e.g., shallow layers tend to convey more static\nand local semantics and the deep layers express more contextual\ninformation. This observation is also consistent with the visual\nrepresentation captured by CNNs [40, 71]. Therefore, to enable our\nproposed model align images and texts leveraging rich semantics,\nwe design a hierarchical alignment scheme that simultaneously\ncomputes the similarity between given image-text pair with the\nrepresentations of multiple levels. As the cross-attention modules\nillustrated in the middle of Figure 1, our hierarchical alignment in-\ntegrates three-level semantics, i.e., low-, middle-, and high-level, to\nlearn the associations between images and texts. Swin Transformer\nconsists of four stages of modules, divided by the patch partition\noperations. We simply adopt these stages as our multi-level seman-\ntics split, excepting â€œStage 1â€ due to its large number of tokens\nthat would be computationally expensive. For hierarchical repre-\nsentations of text Transformer, because BERT has the same number\nof layers, i.e., 12 layers, as Swin Transformer, we symmetrically\nleverage the outputs of the same layers of BERT for multi-level\nsemantic representations. In other words, the the outputs of 4-, 10-,\nand 12-th layers of both Swin Transformer and BERT are extracted\nfor our overall representations of images and texts, respectively.\nWe compute the cosine similarities for each level and combine the\nhierarchical similarities by addition.\nFor the cross-modal alignment of each level, previous works [31,\n47, 66] have verified that fine-grained alignment with local semantic\naspects, e.g., visual regions and words, could significantly improve\nthe cross-modal retrieval performance. Towards this end, we com-\npute the similarity between a pair of given image and text by align-\ning the representations of â€œregion tokensâ€ and words, outputted\nby Swin Transformer and BERT. Specifically, inspired by the effec-\ntiveness of the stacked cross attention mechanism, we implement\nimage-text and text-image stacked cross attention respectively. Sup-\nposing the output of ğ‘™-th layer (ğ‘™ could be 4, 10, 12 in this work)\nof Swin Transformer ğ¼ğ‘™ = {ğ‘£ğ‘™\nğ‘˜|ğ‘£ğ‘™\n1,ğ‘£ğ‘™\n2,...,ğ‘£ ğ‘™\nğ¾}with ğ¾ â€œregion tokensâ€,\nand the output of ğ‘™-th layer of BERT ğ‘†ğ‘™ = {ğ‘¤ğ‘™\nğ‘¡|ğ‘¤ğ‘™\n1,ğ‘¤ğ‘™\n2,...,ğ‘¤ ğ‘™\nğ‘‡}with\nğ‘‡ words, the image-text stacked cross attention first computes the\nsimilarities between all region-word pairs:\nğ‘ ğ‘™\nğ‘˜ğ‘¡ =\nğ‘£ğ‘™\nğ‘˜\nğ‘‡ğ‘¤ğ‘™\nğ‘¡\n||ğ‘£ğ‘™\nğ‘˜||||ğ‘¤ğ‘™\nğ‘¡||\n, (2)\nwhere ğ‘ ğ‘™\nğ‘˜ğ‘¡ denotes the similarity betweenğ‘˜-th region and ğ‘¡-th word\nin ğ‘™-th layer of both Swin Transformer and BERT. Based on the\nregion-word similarity, we aggregate the weighted words represen-\ntations for each image region as:\nğ‘ğ‘™\nğ‘˜ =\nğ‘‡âˆ‘ï¸\nğ‘¡=1\nğ›¼ğ‘™\nğ‘˜ğ‘¡ğ‘¤ğ‘™\nğ‘¡, (3)\nğ›¼ğ‘™\nğ‘˜ğ‘¡ =\nğ‘’ğ‘¥ğ‘(ğœ†âˆ—Â¯ğ‘ ğ‘™\nğ‘˜ğ‘¡)\nÃğ‘‡\nğ‘¡=1 ğ‘’ğ‘¥ğ‘(ğœ†âˆ—Â¯ğ‘ ğ‘™\nğ‘˜ğ‘¡)\n, (4)\nUnifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nwhere ğœ†denotes the temperature, and Â¯ğ‘ ğ‘™\nğ‘˜ğ‘¡ is normalized similarity:\nÂ¯ğ‘ ğ‘™\nğ‘˜ğ‘¡ =\n[ğ‘ ğ‘™\nğ‘˜ğ‘¡]+\nâˆšï¸ƒÃğ¾\nğ‘˜=1 [ğ‘ ğ‘™\nğ‘˜ğ‘¡]2+\n, (5)\nwhere [ğ‘¥]+ = ğ‘šğ‘ğ‘¥(ğ‘¥,0)denotes the hinge function to keep the\nsimilarity with positive value. Finally, we measure the cosine simi-\nlarity between each region-word pair (ğ‘£ğ‘™\nğ‘˜,ğ‘ğ‘™\nğ‘˜)based on image-text\nstacked cross attention, and aggregate all the pairs for the over-\nall similarity between given image and text pair. The text-image\nstacked cross attention follows the similar formulations.\nWith such hierarchical alignment, the proposed model is able\nto capture multi-level semantic correspondences for image-text\nmatching, and successfully achieves significant improvement in the\nevaluation of all the metrics.\n4 EXPERIMENTS\nTo verify the effectiveness of our proposed hierarchical alignment\nTransformers, we conduct extensive experiments on two commonly\nused datasets, namely Flickr30k and MSCOCO. We mainly examine\nthe cross-modal retrieval under two dual scenarios: Image-to-Text\nretrieval (I2T) and Text-to-Image retrieval (T2I).\n4.1 Datasets and Evaluation Metrics\nFlickr30K [70] consists of 31,783 images collected from Flickr,\neach of which is annotated with 5 description sentences. Follow-\ning previous works [25, 31, 37], we split this dataset into 29,783,\n1,000, 1,000 iamges for training, validation, and testing respectively.\nMSCOCO [13] contains 123,287 images, where each image is also\ndescribed by 5 different sentences. For fair comparison, we follow\nprevious work [31, 46] using Karpathy split [25]. We conduct two\nevaluation settings on MSCOCO, which are 1) MSCOCO(5K):\nretrieving images within full 5K images or sentences within its\ncorresponding corpus; 2) MSCOCO(1K): 5-folds evaluation with\neach fold consisting of 1K images, and the final result is reported\nby averaging folds.\nEvaluation Metrics We follow previous works [31, 37] to evalu-\nate the performance with the Recall@K metric, short in R@K (K=1,\n5, 10). It measures the percentage of ground-truth hits in the top-K\nranking list. The higher R@K indicates the better performance.\n4.2 Implement Details\nWe implement our HAT with PyTorch and optimize it by the Adam\noptimizer [27]. For the learning rate, we use a small learning rate\nstarting with 1ğ‘’-5 and decay the learning rate by dividing 10 for\nevery 10 epochs. We employ the base Swin Transformer to extract\nthe image features and BERT-base to extract text features. Dur-\ning the training , we first freeze the pre-trained models, i.e., Swin\nTransformers and BERT, for 10 epochs. This strategy avoids that\nthe random initialization of hierarchical alignment module mis-\nleads the pre-trained weights to be optimized to inferior areas, and\nmakes the hierarchical alignment module more compatible with the\npre-trained parameters. Then the whole HAT framework is further\nfine-tune to more optimal scenario. The dimensions of the visual\ntext features extracted by the Swin Transformer and the BERT are\n1024 and 768, respectively. Moreover, we implement the triplet loss,\na variant for cross-modal retrieval in specific, as:\nL=[ğ‘šâˆ’ğ‘ ğ‘–ğ‘š(ğ¼,ğ‘†)+ğ‘ ğ‘–ğ‘š(ğ¼, Â¯ğ‘†)]+\n+[ğ‘šâˆ’ğ‘ ğ‘–ğ‘š(ğ¼,ğ‘†)+ğ‘ ğ‘–ğ‘š(Â¯ğ¼,ğ‘†)]+, (6)\nwhere ğ‘šis a margin set as 0.2, and [Â·]+= ğ‘šğ‘ğ‘¥(0,Â·). Â¯ğ‘† and Â¯ğ¼ denote\nthe negative sample to push away. ğ‘ ğ‘–ğ‘š(Â·)measures the similarity\nbetween images and texts, and cosine similarity is used here:\nğ‘ ğ‘–ğ‘š(ğ¼,ğ‘†)=\nâˆ‘ï¸\nğ‘™\nâˆ‘ï¸\nğ‘˜\nğ‘£ğ‘™\nğ‘˜\nğ‘‡ğ‘ğ‘™\nğ‘˜\n||ğ‘£ğ‘™\nğ‘˜||||ğ‘ğ‘™\nğ‘˜||\n. (7)\n4.3 Comparison Baselines\nTo evaluate the effectiveness of proposed hierarchical alignment\nTransformers, we compare the cross-modal retrieval performance\nwith several state of the art methods. As mentioned above, existing\nmethods could be grouped into several paradigms based on the\ninvolved interaction type.\nIntra-modal interaction: Methods that focus on intra-modal in-\nteractions, such as SAEM [ 64], VSRN [ 34], CAMERA [ 46], and\nVSEâˆ[9]. SAEM [64] models images and text separately through a\nself-attention mechanism. VSRN [34] performs semantic reasoning\nby constructing region graphs in order to employ graph convolu-\ntional networks. CAMERA [46] then further designs a gated self-\nattention mechanism for context multi-view modeling. VSEâˆ[9]\nproposes GPO to generate the best pooling strategy to improve the\nsemantic representation learning.\nInter-modal interaction: Methods that interact through differ-\nent cross-modal operations, including SCAN [ 31], CAMP [ 60],\nBFAN [37], IMRAM [8], ADAPT [62], and NAAF [72]. SCAN [31]\nand BFAN [37] selectively focus on text-related regions and image-\nrelated words via cross-modal attention. CAMP [60] combines im-\nage and text for modal interaction via cross-modal gated fusion\noperations. IMRAM [8] tries to mine deeper levels of modal interac-\ntions by iterative way. NAAF [72] refines similarity calculation by\nconsidering the negative impact caused by mismatched fragments.\nHybrid interaction: Methods that simultaneously perform inter-\nand intra-modal interactions are CAAN [73], DP-RNN [12], SGM [57],\nMMCA [63], GSMN [38], and DIME [47]. Hybrid interaction leads\nto better semantic alignment and retrieval performance.\nSeveral methods implement cross attention mechanism with\ndifferent directions, e.g., image-text (i-t) and text-image (t-i) cross\nattention, and ensemble models of two directions, including SCAN,\nBFAN, CAMP, and ADAPT, as well as our HAT. We present both\nsingle and ensemble models for comprehensive comparison. As\nlisted in Table 2, models with â€œ*â€ denote the ensemble ones. For\nsingle models, due to the space limitation, we only list the best one\nof i-t and t-i directions, reported in each paper, and add â€œsingleâ€\nin the bracket to indicate this. For our HAT, we illustrate single\nmodels with cross-modal guidance of both i-t and t-i directions,\nnamely HAT(i-t) and HAT (t-i), as well as the full ensemble HATâˆ—.\n4.4 Performance Comparison\nThe overall comparisons with state of the arts for MSCOCO and\nFlickr30K are exhibited in Table 1 and Table 2, respectively. From\nthe result comparison, we have several observations as follows:\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Bin et al.\nTable 1: Performance comparison between our proposed HAT and recent state of the arts on MSCOCO. MSCOCO(5K) and\nMSCOCO(1K) denote the evaluation settings of the full 5K and average of 5-folds 1K test images. For models with cross-attention,\nwe exhibit the best single model and the ensemble model reported in the corresponding paper, while report all the single\nmodels, including i-t and t-i model of HAT for comprehensive comparison. We use â€œsingleâ€ in bracket and superscript â€œ*â€ to\nindicate the best single and ensemble models. The best results are in bold, and the best results of baselines are underlined.\nMethod\nMSCOCO(1K) MSCOCO(5K)\nImage-to-Text Text-to-Image Image-to-Text Text-to-Image\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\nSCAN(single) [31] 70.9 94.5 97.8 56.4 87.0 93.9 46.4 77.4 87.2 34.4 63.7 75.7\nSCANâˆ—[31] 72.7 94.8 98.4 58.8 88.4 94.8 50.4 82.2 90.0 38.6 69.3 80.4\nCAMP [60] 72.3 94.8 98.3 58.5 87.9 95.0 50.1 82.1 89.7 39.0 68.9 80.2\nBFAN(single) [37] 73.7 94.9 - 58.3 88.4 - - - - - - -\nBFANâˆ—[37] 74.9 95.2 - 59.4 88.4 - - - - - - -\nSAEM [64] 71.2 94.1 97.7 57.8 88.6 94.9 - - - - - -\nCAAN [73] 75.5 95.4 98.5 61.3 89.7 95.2 52.5 83.3 90.9 41.2 70.3 82.9\nDP-RNN [12] 75.3 95.8 98.6 62.5 89.7 95.1 - - - - - -\nVSRN [34] 76.2 94.8 98.2 62.8 89.7 95.1 53.0 81.1 89.4 40.5 70.6 81.1\nSGM [57] 73.4 93.8 97.8 57.5 87.3 94.3 50.0 79.3 87.9 35.3 64.9 76.5\nIMRAM [8] 76.7 95.6 98.5 61.7 89.1 95.0 53.7 83.2 91.0 39.6 69.1 79.8\nMMCA [63] 74.8 95.6 97.7 61.6 89.8 95.2 54.0 82.5 90.7 38.7 69.7 80.8\nGSMN(single) [38] 76.1 95.6 98.3 60.4 88.7 95.0 - - - - - -\nGSMNâˆ—[38] 78.4 96.4 98.6 63.3 90.1 95.7 - - - - - -\nADAPT(single) [62] 75.3 95.1 98.4 63.3 90.0 95.5 - - - - - -\nADAPTâˆ—[62] 76.5 95.6 98.9 62.2 90.5 96.0 - - - - - -\nCAMERA(single) [46] 75.9 95.5 98.6 62.3 90.1 95.2 53.1 81.3 89.8 39.0 70.5 81.5\nCAMERAâˆ—[46] 77.5 96.3 98.8 63.4 90.9 95.8 55.1 82.9 91.2 40.5 71.7 82.5\nDIME(single) [47] 77.9 95.9 98.3 63.0 90.5 96.2 56.1 83.2 91.1 40.2 70.7 81.4\nDIMEâˆ—[47] 78.8 96.3 98.7 64.8 91.5 96.5 59.3 85.4 91.9 43.1 73.0 83.1\nVSEâˆ[9] 79.7 96.4 98.9 64.8 91.4 96.3 58.3 85.3 92.3 42.4 72.7 83.2\nNAAF [72] 80.5 96.5 98.8 64.1 90.7 96.5 58.9 85.2 92.0 42.5 70.9 81.4\nHAT(i-t) 81.3 97.0 99.2 68.9 92.7 97.1 61.6 87.3 93.4 47.9 76.4 85.8\nHAT(t-i) 81.8 96.7 99.1 69.8 93.0 97.1 61.9 87.6 93.3 48.8 77.3 86.1\nHATâˆ— 82.6 97.4 99.3 70.8 93.3 97.4 63.8 88.5 94.1 50.3 78.2 86.9\nâ€¢Our proposed HATâˆ—outperforms all the baselines with significant\nimprovements in all metrics, for both MSCOCO and Flickr30K\nbenchmarks, and all the single HAT models, namely HAT(i-t)\nand HAT(t-i) for different guidance directions, show the similar\nresults. Comparing with the state of the art, our best results (in\nbold) gain 7.6% and 16.7% relative score improvements for text\nretrieval and image retrieval, in Recall@1 on MSCOCO (5K). For\nFlickr30K, our proposed approach improves the corresponding\nperformance of text retrieval and image retrieval by 4.4% and\n11.6% (Recall@1) relatively. We also note that our HAT boosts\nthe performance of text-to-image retrieval with much larger\ngap (16.7% v.s. 7.6%, and 11.6% v.s. 4.4%) than image-to-text re-\ntrieval. This phenomenon mainly comes from that the overall\nperformance of text-to-image retrieval is inferior comparing with\nimage-to-text retrieval, resulting in a smaller value, which has\nmore scope to improve. Overall, these observations show the\nsuperiority of our proposed HAT for cross-modal retrieval.\nâ€¢As described in Section 4.3, we can divide the baselines into three\ngroups based on the type of modality interaction. Obviously,\nbenefiting from the rich interaction between images and texts,\nmost works adopt inter-modal interaction or hybrid interaction\n(also involving inter-modal) to learn the semantic associations\nbetween them, and usually achieve better retrieval performances.\nInter-modal interaction attempts to enrich the representations\nof images (or texts) with the counterpart guidance. With such\ncross-modal interactions, the models tend to learn the representa-\ntion involving heterogeneous information of both modalities and\nnarrow the semantic gap between representations of images and\ntexts, which is similar to the cross-modal pre-training [35, 36] and\nleads to better performance. However, CAMERA only exploring\nthe intra-modal interaction still outperforms most approaches\nand results in the second competitive baseline. The competitive\nperformance of CAMERA mainly attributes to the design of adap-\ntive gating module to capture contextual information and the\nleverage of multi-view summarization, which indicates that com-\nprehensive and elaborate intra-modal interactions exploration\nare also essential and crucial for boosting the performance of the\ncross-modal retrieval.\nâ€¢It is well known that powerful feature representations usually\nlead to impressive performance in many tasks [21, 22, 26], but\nhave not been well discussed in previous cross-modal retrieval\nUnifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nTable 2: Comparison results on the Flick30K. Symbols are\nthe same as the ones in Table 1.\nMethod Image-to-Text Text-to-Image\nR@1 R@5 R@10 R@1 R@5 R@10\nSCAN(single) [31] 67.9 89.0 94.4 43.9 74.2 82.8\nSCANâˆ—[31] 67.4 90.3 95.8 48.6 77.7 85.2\nCAMP [60] 68.1 89.7 95.2 51.5 77.1 85.2\nBFAN(single) [37] 65.5 89.4 - 47.9 77.6 -\nBFANâˆ—[37] 68.1 91.4 - 50.8 78.4 -\nSAEM [64] 69.1 91.0 95.1 52.4 81.1 88.1\nCAAN [73] 70.1 91.6 97.2 52.8 79.0 87.9\nDP-RNN [12] 70.2 91.6 95.8 55.5 81.3 88.2\nVSRN [34] 71.3 90.6 96.0 54.7 81.8 88.2\nSGM [57] 71.8 91.7 95.5 53.5 79.6 86.5\nIMRAM [8] 74.1 93.0 96.6 53.9 79.4 87.2\nMMCA [63] 74.2 92.8 96.4 54.8 81.4 87.8\nGSMN(single) [38] 72.6 93.5 96.8 53.7 80.0 87.0\nGSMNâˆ—[38] 76.4 94.3 97.3 57.4 82.3 89.0\nADAPT(single) [62] 73.6 93.7 96.7 57.0 83.6 90.3\nADAPTâˆ—[62] 76.6 95.4 97.6 60.7 86.6 92.0\nCAMERA(single) [46] 76.5 95.1 97.2 58.9 84.7 90.2\nCAMERAâˆ—[46] 78.0 95.1 97.9 60.3 85.9 91.7\nDIME(single) [47] 77.4 95.0 97.4 60.1 85.5 91.8\nDIMEâˆ—[47] 81.0 95.9 98.4 63.6 88.1 93.0\nVSEâˆ[9] 81.7 95.4 97.6 61.4 85.9 91.5\nNAAF [72] 81.9 96.1 98.3 61.0 85.3 90.6\nHAT(i-t) 83.9 96.7 98.3 68.8 90.5 94.6\nHAT(t-i) 84.3 97.3 98.9 70.2 90.8 94.5\nHATâˆ— 85.5 97.3 99.0 71.0 91.2 95.1\nworks. We note that almost all the baselines leverage Faster-\nRCNN or Bottom-Up as image feature extractor, and (Bi-)RNN or\nBERT as text representation encoder. As a powerful pre-trained\nlanguage model, BERT and its successors demonstrate superior-\nity in almost all the NLP tasks, including language understanding,\ngeneration, and infilling. Similar phenomenons are observed in\nthis work, in other words, the methods adopting BERT as text\nencoder, e.g., DIME, NAAF, RVSE++, and CAMERA, achieve more\nprecise retrieval results. In the visual part, different from all the\nbaselines, our HAT implements the recent powerful model, Swin\nTransformer, to extract visual representations and result in sig-\nnificant improvement. But we want to claim that the remarkable\nimprovement of HAT not only comes from the powerful pre-\ntrained model, but also benefits from the unified structures of\nimage and text encoders, i.e., Transformers, which will be com-\nprehensively investigated and discussed in Section 4.5.\n4.5 Investigation of Structural Discrepancy\nbetween Image and Text Encoders\nAs claimed above, using different structures of encoders for images\nand texts may degrade the representation learning of each modality\nand also affect the semantic alignment across modalities. In this\nsection, we will empirically investigate whether the discrepancy\nof encoders in structure has impact on the cross-modal retrieval\n29.1\n50.9 54.1\n30.1\n54.1\n66.3\n20\n40\n60\n80\n100\n43.0 \n63.1 70.0 \n43.2 \n70.0 \n79.8 \n20\n40\n60\n80\n100\nRes+GRU Res+Trans Trans+GRU Trans+Trans Trans+BERT\nI2T\nT2I\nA\nC\nAB BC C\nTraditional \nencoders\nUnifying enc structures & \nImproving vision encoder\nUnifying enc structure & \nImproving text encoder\nA B\n0.2\n6.9 9.8\n1.0\n3.2\n12.2\nFigure 2: Performance (Recall@1) of models with different\nencoder combos and pairwise comparisons on Flickr30K.\nTable 3: Performance comparison between HAT and its non-\nhierarchical variant on Flickr30K with the i-t cross-attention.\nImage-to-Text Text-to-ImageModel R@1 R@5 R@10 R@1 R@5 R@10\nw/o hierarchical 79.8 95.9 97.5 66.3 88.6 93.3\nHAT(i-t) 83.9 96.7 98.3 68.8 90.5 94.6\ntask and how it affects the final performance. Towards this end,\nwe test different encoder combos, combining image and text en-\ncoders with different architectures to build the cross-modal retrieval\nmodel. Specifically, we include two image encoders, ResNet and\nSwin Transformer, representing two types of the most typical image\nencoder structures, CNN and Transformer. For text encoder, we\nconsider two representative structures, RNN and Transformer, and\ninclude three models in total: GRU, vanilla Transformer, and BERT.\nFigure 2 illustrates the performance improvements (Recall@1)\nof the models combining different image and text encoders for\ntwo cross-modal retrieval tasks on Flickr30K. From the comparison\nwithin pair A we can first observe two models,i.e., ResNet+GRU and\nResNet+Transformer achieve similar performance for both retrieval\ntasks, showing the worst performances, and the Transformer gains\nlittle improvement over GRU being text encoders when cooperated\nwith ResNet. However, we observe great performance improvement\n(6.9 and 3.2 absolute improvements for I2T and T2I retrieval tasks)\nin pair B, where the structures of two encoders are further changed\nto be the same, i.e., replacing the GRU by a vanilla Transformer\nfor text encoder. This suggests the effectiveness of unifying the\nstructures of image and text encoders. Moreover, when both image\nand text encoders employ the unified Transformer structure (pairC),\nfurther advancing the text encoder with pre-trained and powerful\nBERT, can bring additional performance boost. This observation\nalso supports the assertion of more powerful feature representation\nleads to better performance in previous parts.\n4.6 Analyses of Hierarchical Alignment\nTo investigate the impact of our hierarchical alignment strategy,\nwe conduct ablation for quantitative analysis and visualize the at-\ntention map of each layer for intuitive analysis. We first analyze\nthe quantitative results, as illustrated in Table 3, the hierarchi-\ncal alignment brings evident improvement for both image-to-text\nand text-to-image retrieval tasks. This means that our hierarchical\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Bin et al.\n(b) A man walks his dog near the ocean.\n(a) A man walks his dog near the ocean.\nStage 2 Stage 3 Stage 4\nStage 2 Stage 3 Stage 4\n(d) A young woman running by an orange ribbon.\n(c) A  half naked man is sleeping on his chair outdoors.\nStage 2 Stage 3 Stage 4\nStage 2 Stage 3 Stage 4\nFigure 3: Illustrations of cross-attention map for text-to-image retrieval. We exhibit three-level attention maps for stage 2-4 of\nSwin Transformer to investigate the image-text associations of different levels.\nalignment leveraging complementary multi-level features is able\nto enrich semantic associations between images and texts, and is\nbeneficial to the overall retrieval goal. To straightforwardly under-\nstand the learned feature at different levels, we further visualize\nthe attention map in images for specific words of each level, as\nillustrated in Figure 3. The shallow layer (Stage 2) tends to distrib-\nute the attention to many isolated dots or pixels around the target.\nWhile the attention map of middle layer (Stage 3) exhibits primary\ncontext-aware ability, and the deep layer (Stage 4) is able to capture\nthe salient context with respect to the target word. The integra-\ntion of multi-level could leverage the high-semantic context and\nlow-semantic pixels to associate cross-modal information. From the\nabove observations, we can find that both the quantitative and qual-\nitative results have verified that our hierarchical alignment strategy\ncan capture more semantic context and achieve better performance.\n4.7 Visualization of Image and Text\nRepresentations\nIn this part, we study the impacts on learned representation distri-\nbution with different encoder architectures. We utilize t-SNE [53]\nto visualize the distributions of image and text representations with\nFlickr30K test set with different architecture combos for the en-\ncoders. As illustrated in Figure 4, we can observe that Swin+BERT\n(a) and Swin+Trans (b) employing Transformer architecture for\nboth image and text encoder learn similar distributions for images\nand texts. While Swin+GRU (c) and ResNet+GRU (d) exhibit very\ndifferent distribution patterns for image and text representations.\nThese observations further support that the architecture discrep-\nancy between encoders leads to different feature distribution for\nimages and texts. Besides, the unified structure of two-stream en-\ncoders, Transformer architecture in specific, enables the encoders\nlearn more compatible representation distributions for images and\ntexts, and makes them align better in the common semantic space,\nresulting in better retrieval performance.\n(a) Swin+BERT (b) Swin+Trans\n(c) Swin+GRU (d) ResNet+GRU\nFigure 4: Distributions of learned image and text representa-\ntions with different encoder combos. Both (a) and (b) employ\nTransformer architecture as image and text encoders, which\nshow similar distributions for image and text representa-\ntions. (c) and (d) show very different distribution patterns\nfor image and text representations, due to the discrepancy\nbetween image and text encoder architectures.\n5 CONCLUSIONS\nIn this work, we studied the problem of architecture discrepancy\nbetween two-stream encoders in cross-modal retrieval. To address\nthis issue, we proposed a novel framework, namely hierarchical\nalignment Transformers (HAT), which employed two-stream Trans-\nformers, e.g., Swin Transformer and BERT, to unify the architectures\nof image and text encoder, and enriched the cross-modal associa-\ntions by hierarchical alignment. The extensive experiments on two\ncommonly-used datasets, Flickr30K and MSCOCO, demonstrated\nUnifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nthe superiority of our proposed HAT. Besides, further analyses\nverified the effectiveness of unifying encoder architectures of im-\nages and texts with Transformer, as well as the valuable ability of\nhierarchical alignment strategy.\nACKNOWLEDGMENTS\nThis research/project is supported by the National Research Founda-\ntion, Singapore under its Industry Alignment Fund â€“ Pre-positioning\n(IAF-PP) Funding Initiative. Any opinions, findings and conclusions\nor recommendations expressed in this material are those of the\nauthor(s) and do not reflect the views of National Research Founda-\ntion, Singapore. This work was also partially supported by the Na-\ntional Natural Science Foundation of China under grant 62102070,\n62220106008, and U20B2063, and partially supported by Sichuan\nScience and Technology Program under grant 2023NSFSC1392.\nREFERENCES\n[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In\nICCV. 2425â€“2433.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine\ntranslation by jointly learning to align and translate. In ICLR.\n[3] Yi Bin, Xindi Shang, Bo Peng, Yujuan Ding, and Tat-Seng Chua. 2021. Multi-\nPerspective Video Captioning. In ACM Multimedia. 5110â€“5118.\n[4] Yi Bin, Yang Yang, Jie Zhou, Zi Huang, and Heng Tao Shen. 2017. Adaptively\nattending to visual attributes and linguistic knowledge for captioning. In ACM\nMultimedia. 1345â€“1353.\n[5] David M Blei and Michael I Jordan. 2003. Modeling annotated data. In SIGIR.\n127â€“134.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. In NeurIPS.\n[7] Chen Chen, Dan Wang, Bin Song, and Hao Tan. 2023. Inter-Intra Modal Represen-\ntation Augmentation with DCT-Transformer Adversarial Network for Image-Text\nMatching. TMM (2023).\n[8] Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, and Jungong Han. 2020.\nIMRAM: Iterative matching with recurrent attention memory for cross-modal\nimage-text retrieval. In CVPR. 12655â€“12663.\n[9] Jiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, and Changhu Wang. 2021.\nLearning the best pooling strategy for visual semantic embedding. In CVPR.\n15789â€“15798.\n[10] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan,\nand Ilya Sutskever. 2020. Generative pretraining from pixels. In ICML. PMLR,\n1691â€“1703.\n[11] Shizhe Chen, Bei Liu, Jianlong Fu, Ruihua Song, Qin Jin, Pingping Lin, Xiaoyu\nQi, Chunting Wang, and Jin Zhou. 2019. Neural storyboard artist: Visualizing\nstories with coherent image sequences. In ACM Multimedia. 2236â€“2244.\n[12] Tianlang Chen and Jiebo Luo. 2020. Expressing objects just like words: Recurrent\nvisual embedding for image-text matching. In AAAI, Vol. 34. 10583â€“10590.\n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Pi-\notr DollÃ¡r, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection\nand evaluation server. arXiv preprint arXiv:1504.00325(2015).\n[14] Yanbei Chen, Shaogang Gong, and Loris Bazzani. 2020. Image search with text\nfeedback by visiolinguistic attention learning. In CVPR. 3001â€“3011.\n[15] Wietse de Vries, Andreas van Cranenburgh, and Malvina Nissim. 2020. Whatâ€™s\nso special about BERTâ€™s layers? A closer look at the NLP pipeline in monolingual\nand multilingual models. In Findings of EMNLP. 4339â€“4350.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale. In ICLR.\n[17] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2018. Vse++:\nImproving visual-semantic embeddings with hard negatives. In BMVC.\n[18] Andrea Frome, Greg Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean,\nMarcâ€™Aurelio Ranzato, and Tomas Mikolov. 2013. Devise: A deep visual-semantic\nembedding model. In NeurIPS. 2121â€“2129.\n[19] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich\nfeature hierarchies for accurate object detection and semantic segmentation. In\nCVPR. 580â€“587.\n[20] Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik. 2014. A multi-view\nembedding space for modeling internet images, tags, and their semantics. IJCV\n106, 2 (2014), 210â€“233.\n[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick.\n2022. Masked autoencoders are scalable vision learners. In CVPR. 16000â€“16009.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In CVPR. 770â€“778.\n[23] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-\ntuning for Text Classification. In ACL. 328â€“339.\n[24] Yangqing Jia, Mathieu Salzmann, and Trevor Darrell. 2011. Learning cross-\nmodality similarity for multinomial data. In ICCV. IEEE, 2407â€“2414.\n[25] Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for\ngenerating image descriptions. In CVPR. 3128â€“3137.\n[26] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT. 4171â€“4186.\n[27] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\nmization. In ICLR.\n[28] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. 2014. Unifying visual-\nsemantic embeddings with multimodal neural language models. arXiv preprint\narXiv:1411.2539 (2014).\n[29] Benjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf. 2015. Associating neural\nword embeddings with deep image representations using fisher vectors. In CVPR.\n4437â€“4446.\n[30] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In ICLR.\n[31] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018.\nStacked cross attention for image-text matching. In ECCV. 201â€“216.\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In ACL. 7871â€“7880.\n[33] Haoxuan Li, Yi Bin, Junrong Liao, Yang Yang, and Heng Tao Shen. 2023. Your\nNegative May not Be True Negative: Boosting Image-Text Matching with False\nNegative Elimination. In ACM Multimedia.\n[34] Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. 2019. Visual semantic\nreasoning for image-text matching. In ICCV. 4654â€“4662.\n[35] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu,\nand Haifeng Wang. 2021. UNIMO: Towards Unified-Modal Understanding and\nGeneration via Cross-Modal Contrastive Learning. In ACL-IJCNLP. 2592â€“2607.\n[36] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan\nWang, Houdong Hu, Li Dong, Furu Wei, et al . 2020. Oscar: Object-semantics\naligned pre-training for vision-language tasks. In ECCV. Springer, 121â€“137.\n[37] Chunxiao Liu, Zhendong Mao, An-An Liu, Tianzhu Zhang, Bin Wang, and Yong-\ndong Zhang. 2019. Focus your attention: A bidirectional focal attention network\nfor image-text matching. In ACM Multimedia. 3â€“11.\n[38] Chunxiao Liu, Zhendong Mao, Tianzhu Zhang, Hongtao Xie, Bin Wang, and\nYongdong Zhang. 2020. Graph structured network for image-text matching. In\nCVPR. 10921â€“10930.\n[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using\nshifted windows. In ICCV. 10012â€“10022.\n[40] Aravindh Mahendran and Andrea Vedaldi. 2016. Visualizing deep convolutional\nneural networks using natural pre-images. IJCV 120, 3 (2016), 233â€“255.\n[41] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nestimation of word representations in vector space. In ICLR. 1â€“12.\n[42] Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. 2017. Hierarchical\nmultimodal lstm for dense visual-semantic embedding. In ICCV. 1881â€“1889.\n[43] Liang Peng, Shuangji Yang, Yi Bin, and Guoqing Wang. 2021. Progressive graph\nattention network for video question answering. InACM Multimedia. 2871â€“2879.\n[44] Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Nikhil Rasiwasia, Gert RG\nLanckriet, Roger Levy, and Nuno Vasconcelos. 2013. On the role of correlation\nand abstraction in cross-modal multimedia retrieval. IEEE TPAMI36, 3 (2013),\n521â€“535.\n[45] Duangmanee Putthividhy, Hagai T Attias, and Srikantan S Nagarajan. 2010. Topic\nregression multi-modal latent dirichlet allocation for image annotation. In CVPR.\nIEEE, 3408â€“3415.\n[46] Leigang Qu, Meng Liu, Da Cao, Liqiang Nie, and Qi Tian. 2020. Context-aware\nmulti-view summarization network for image-text matching. InACM Multimedia.\n1047â€“1055.\n[47] Leigang Qu, Meng Liu, Jianlong Wu, Zan Gao, and Liqiang Nie. 2021. Dynamic\nmodality interaction modeling for image-text retrieval. In SIGIR. 1104â€“1113.\n[48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits\nof Transfer Learning with a Unified Text-to-Text Transformer.JMLR 21 (2020),\n1â€“67.\n[49] Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert RG\nLanckriet, Roger Levy, and Nuno Vasconcelos. 2010. A new approach to cross-\nmodal multimedia retrieval. In ACM Multimedia. 251â€“260.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Bin et al.\n[50] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal networks. In NeurIPS.\n[51] Heng Tao Shen, Luchen Liu, Yang Yang, Xing Xu, Zi Huang, Fumin Shen, and\nRichang Hong. 2020. Exploiting subspace relation in semantic labels for cross-\nmodal hashing. IEEE TKDE33, 10 (2020), 3351â€“3365.\n[52] Betty Van Aken, Benjamin Winter, Alexander LÃ¶ser, and Felix A Gers. 2019. How\ndoes bert answer questions? a layer-wise analysis of transformer representations.\nIn CIKM. 1823â€“1832.\n[53] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJMLR 9, 11 (2008).\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS. 5998â€“6008.\n[55] Bokun Wang, Yang Yang, Xing Xu, Alan Hanjalic, and Heng Tao Shen. 2017.\nAdversarial cross-modal retrieval. In ACM Multimedia. 154â€“162.\n[56] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen\nLiu, Yu-Gang Jiang, Luowei Zhou, and Lu Yuan. 2021. Bevt: Bert pretraining of\nvideo transformers. arXiv preprint arXiv:2112.01529(2021).\n[57] Sijin Wang, Ruiping Wang, Ziwei Yao, Shiguang Shan, and Xilin Chen. 2020.\nCross-modal scene graph matching for relationship-aware image-text retrieval.\nIn WACV. 1508â€“1517.\n[58] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng,\nHao Shen, and Huaxia Xia. 2021. End-to-end video instance segmentation with\ntransformers. In CVPR. 8741â€“8750.\n[59] Zheng Wang, Zhenwei Gao, Kangshuai Guo, Yang Yang, Xiaoming Wang, and\nHeng Tao Shen. 2023. Multilateral Semantic Relations Modeling for Image Text\nRetrieval. In CVPR. 2830â€“2839.\n[60] Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang,\nand Jing Shao. 2019. Camp: Cross-modal adaptive message passing for text-image\nretrieval. In ICCV. 5764â€“5773.\n[61] Zheng Wang, Xing Xu, Guoqing Wang, Yang Yang, and Heng Tao Shen. 2023.\nQuaternion relation embedding for scene graph generation. TMM (2023).\n[62] JÃ´natas Wehrmann, Camila Kolling, and Rodrigo C Barros. 2020. Adaptive cross-\nmodal embeddings for image-text alignment. In AAAI, Vol. 34. 12313â€“12320.\n[63] Xi Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, and Feng Wu. 2020. Multi-\nmodality cross attention network for image and sentence matching. In CVPR.\n10941â€“10950.\n[64] Yiling Wu, Shuhui Wang, Guoli Song, and Qingming Huang. 2019. Learning\nfragment self-attention embeddings for image-text matching. InACM Multimedia.\n2088â€“2096.\n[65] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and\nPing Luo. 2021. SegFormer: Simple and efficient design for semantic segmentation\nwith transformers. In NeurIPS.\n[66] Xing Xu, Tan Wang, Yang Yang, Lin Zuo, Fumin Shen, and Heng Tao Shen. 2020.\nCross-modal attention with semantic consistence for imageâ€“text matching. IEEE\nTNNLS 31, 12 (2020), 5412â€“5425.\n[67] Yahui Xu, Yi Bin, Jiwei Wei, Yang Yang, Guoqing Wang, and Heng Tao Shen. 2023.\nMulti-Modal Transformer with Global-Local Alignment for Composed Query\nImage Retrieval. TMM (2023).\n[68] Yang Yang, Jie Zhou, Jiangbo Ai, Yi Bin, Alan Hanjalic, Heng Tao Shen, and Yanli\nJi. 2018. Video captioning by adversarial LSTM. TIP 27, 11 (2018), 5600â€“5611.\n[69] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,\nand Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language\nunderstanding. In NeurIPS.\n[70] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image\ndescriptions to visual denotations: New similarity metrics for semantic inference\nover event descriptions. TACL 2 (2014), 67â€“78.\n[71] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolu-\ntional networks. In ECCV. Springer, 818â€“833.\n[72] Kun Zhang, Zhendong Mao, Quan Wang, and Yongdong Zhang. 2022. Negative-\naware attention framework for image-text matching. In CVPR. 15661â€“15670.\n[73] Qi Zhang, Zhen Lei, Zhaoxiang Zhang, and Stan Z Li. 2020. Context-aware\nattention network for image-text retrieval. In CVPR. 3536â€“3545.\n[74] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu, and Yi-\nDong Shen. 2020. Dual-path convolutional image-text embeddings with instance\nloss. ACM TOMM16, 2 (2020), 1â€“23.\n[75] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong.\n2018. End-to-end dense video captioning with masked transformer. In CVPR.\n8739â€“8748.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7987934350967407
    },
    {
      "name": "Transformer",
      "score": 0.7608356475830078
    },
    {
      "name": "Encoder",
      "score": 0.6691019535064697
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5075644254684448
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49064794182777405
    },
    {
      "name": "Modal",
      "score": 0.4895373284816742
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.379438579082489
    },
    {
      "name": "Information retrieval",
      "score": 0.3456486463546753
    },
    {
      "name": "Natural language processing",
      "score": 0.3400493264198303
    },
    {
      "name": "Computer vision",
      "score": 0.3273223638534546
    },
    {
      "name": "Voltage",
      "score": 0.09363201260566711
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}