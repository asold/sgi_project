{
  "title": "Memory-Efficient Differentiable Transformer Architecture Search",
  "url": "https://openalex.org/W3176602224",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3099880284",
      "name": "Yuekai Zhao",
      "affiliations": [
        "Peking University",
        "Center for Interdisciplinary Studies"
      ]
    },
    {
      "id": "https://openalex.org/A1974723233",
      "name": "Li Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166559730",
      "name": "Yelong Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102713756",
      "name": "Zhihua Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108390110",
      "name": "Weiâ€Zhu Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963139417",
    "https://openalex.org/W2905672847",
    "https://openalex.org/W2963137684",
    "https://openalex.org/W2970900903",
    "https://openalex.org/W2970463839",
    "https://openalex.org/W2949264490",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2748513770",
    "https://openalex.org/W2737740651",
    "https://openalex.org/W2970832665",
    "https://openalex.org/W2962847160",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W3034309359",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W3096533519",
    "https://openalex.org/W2963540976",
    "https://openalex.org/W3098967720",
    "https://openalex.org/W2964343746",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2965225113",
    "https://openalex.org/W3092806700",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4298393859",
    "https://openalex.org/W4295185264",
    "https://openalex.org/W3024880312",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2898631838",
    "https://openalex.org/W2945278040",
    "https://openalex.org/W4289761690",
    "https://openalex.org/W2962949867",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W4287639392",
    "https://openalex.org/W2963684275",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2994749257",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2963778169",
    "https://openalex.org/W2981748264",
    "https://openalex.org/W3093155142",
    "https://openalex.org/W2803311163",
    "https://openalex.org/W2891927334",
    "https://openalex.org/W2994985593",
    "https://openalex.org/W2951886768",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2998227662",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2996409713",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W2796265726",
    "https://openalex.org/W2593744649",
    "https://openalex.org/W3034634365",
    "https://openalex.org/W4394650345",
    "https://openalex.org/W2594529350",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2995999070",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W2953181046",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2963473542",
    "https://openalex.org/W2963359731"
  ],
  "abstract": "Differentiable architecture search (DARTS) is successfully applied in many vision tasks.However, directly using DARTS for Transformers is memory-intensive, which renders the search process infeasible.To this end, we propose a multi-split reversible network and combine it with DARTS.Specifically, we devise a backpropagation-with-reconstruction algorithm so that we only need to store the last layer's outputs.By relieving the memory burden for DARTS, it allows us to search with larger hidden size and more candidate operations.We evaluate the searched architecture on three sequence-to-sequence datasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14 English-Czech.Experimental results show that our network consistently outperforms standard Transformers across the tasks.Moreover, our method compares favorably with big-size Evolved Transformers, reducing search computation by an order of magnitude.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4254â€“4264\nAugust 1â€“6, 2021. Â©2021 Association for Computational Linguistics\n4254\nMemory-Efï¬cient Differentiable Transformer Architecture Search\nYuekai Zhaoâ€ , Li Dong â€¡, Yelong Shen â€¡, Zhihua Zhang Â§, Furu Wei â€¡, Weizhu Chen â€¡\nâ€ Academy for Advanced Interdisciplinary Studies, Peking University\nÂ§School of Mathematical Sciences, Peking University\nâ€¡Microsoft Corporation\n{yuekaizhao@,zhzhang@math.}pku.edu.cn\n{lidong1,yeshe,fuwei,wzchen}@microsoft.com\nAbstract\nDifferentiable architecture search (DARTS)\nis successfully applied in many vision tasks.\nHowever, directly using DARTS for Trans-\nformers is memory-intensive, which renders\nthe search process infeasible. To this end, we\npropose a multi-split reversible network and\ncombine it with DARTS. Speciï¬cally, we de-\nvise a backpropagation-with-reconstruction al-\ngorithm so that we only need to store the last\nlayerâ€™s outputs. By relieving the memory bur-\nden for DARTS, it allows us to search with\nlarger hidden size and more candidate opera-\ntions. We evaluate the searched architecture\non three sequence-to-sequence datasets, i.e.,\nWMTâ€™14 English-German, WMTâ€™14 English-\nFrench, and WMTâ€™14 English-Czech. Exper-\nimental results show that our network con-\nsistently outperforms standard Transformers\nacross the tasks. Moreover, our method com-\npares favorably with big-size Evolved Trans-\nformers, reducing search computation by an\norder of magnitude.\n1 Introduction\nCurrent neural architecture search (NAS) studies\nhave produced models that surpass the performance\nof those designed by humans (Real et al., 2019; Lu\net al., 2020). For sequence tasks, efforts are made\nin reinforcement learning-based (Pham et al., 2018)\nand evolution-based (So et al., 2019; Wang et al.,\n2020) methods, which suffer from the huge compu-\ntational cost. Instead, gradient-based methods (Liu\net al., 2018; Jiang et al., 2019; Yang et al., 2020) are\nless demanding in computing resources and easy\nto implement, attracting many attentions recently.\nThe idea of gradient-based NAS is to train a\nsuper network covering all candidate operations.\nDifferent sub-graphs of the super network form\nthe search space. To ï¬nd a well-performing sub-\ngraph, Liu et al. (2018) (DARTS) introduced search\nparameters jointly optimized with the network\n300 400 500 600 700 800 900\nHidden size\n6000\n8000\n10000\n12000\n14000\n16000Memory consumption in MB DARTS in Transformers\nOut of memory\nOurs\nFigure 1: Memory comparison of using our reversible\nnetworks and Transformers as the backbone model\nof DARTS. Experiments are run on a single step of\nforward-backward pass on a batch of 3584 tokens\nwith a NVIDIA P100 GPU. Limited by GPU memory,\nDARTS in Transformers has to search in small sizes\nwhile evaluating in large sizes, which will cause perfor-\nmance gaps (Chen et al., 2019).\nweights. Operations corresponding to the largest\nsearch parameters are kept for each intermediate\nnode after searching. A limitation of DARTS is\nits memory inefï¬ciency because it needs to store\nthe intermediate outputs from all its candidate op-\nerations. This is much more pronounced when we\napply Transformers (Vaswani et al., 2017) as the\nbackbone of DARTS (the operation set is detailed\nin Section 2.5). As shown in Figure 1, memory\nconsumption grows extremely fast as we increase\nthe hidden size d, quickly running out of memory\nas d> 400. As a result, we can only use a limited\noperation set or a small hidden size, which may\nlead to worse model performance.\nTo address the unfavorable memory consump-\ntion issue in DARTS, we propose a variant of re-\nversible networks. Each input of a reversible net-\nwork layer can be reconstructed from its outputs.\nThus, it is unnecessary to store intermediate outputs\nexcept for the last layer because we can reconstruct\n4255\nthem during backpropagation (BP). Inspired by the\nidea of RevNets (Gomez et al., 2017), we devise a\nmulti-split reversible network. Each split contains\na mixed operation search node to enable DARTS.\nAlso, only a small modiï¬cation of BP is needed to\nenable gradient calculation with input reconstruc-\ntion. We show the memory consumption of our\nmethod in Figure 1, which on average halves the\namount of memory required in the vanilla DARTS.\nWe can search larger, deeper networks with a richer\ncandidate operation set under the same memory\nconstraint.\nOur method is generic to handle various network\nstructures. In this work, we focus on the sequence-\nto-sequence task. We ï¬rst perform the architec-\nture search using the WMTâ€™14 English-German\ntranslation task. The resulting architecture is then\nre-trained on three datasets: WMTâ€™14 English-\nGerman, WMTâ€™14 English-French, and WMTâ€™14\nEnglish-Czech. We achieve consistent improve-\nment over standard Transformers in all tasks. At a\nmedium model size, we can have the same trans-\nlation quality as the original â€œbigâ€ Transformer\nwith 69% fewer parameters. At a big model size,\nwe exceed the performance of the Evolved Trans-\nformer (So et al., 2019), with the computational\ncost lowered by an order of magnitude. We will\nmake our code and models publicly available.\n2 Methodology\nWe give a detailed description of our method. In\nSection 2.1, we introduce DARTS and its memory\ninefï¬ciency when applying in Transformers. In\nSection 2.2, we propose a multi-split reversible net-\nwork, which works as the backbone of our memory-\nefï¬cient architecture search approach. Section 2.3\nshows a backpropagation-with-reconstruction al-\ngorithm. In Section 2.4, we manage to combine\nDARTS with our reversible networks. Finally, in\nSection 2.5, we summarize the proposed algorithms\nwith more details.\n2.1 Differentiable Architecture Search in\nTransformers\nFollowing (Liu et al., 2018), we explain the idea of\ndifferentiable architecture search (DARTS) within\na one-layer block. Let O be the candidate operation\nset (e.g., Self Attention, FFN, Zero). Each opera-\ntion o âˆˆO represents some function that can be\napplied to the layer inputs or hidden states (denoted\nX). The key of DARTS is to use a mixed operation\nsearch node f(X) to relax the categorical choice of\na speciï¬c operation to a softmax over all candidate\noperations:\nf(X) =\nâˆ‘\noâˆˆO\nexp(Î±o)âˆ‘\noâ€² âˆˆO exp(Î±oâ€² )o(X), (1)\nwhere the Î±are trainable parameters of size |O|\nthat determines the mixing weights. During search-\ning, a one-layer block contains several search\nnodes. The task is to ï¬nd a suitable set of Î±for\neach search node. At the end of the search, the\nresulting operation in each node is determined by:\nf = arg max\noâˆˆO\nÎ±o. (2)\nWe optimize the Î±together with network weights\nÎ¸by gradient descent. A good architecture means\nperforming well on the searching validation set,\nsuch that we optimize Î±with validation loss Lval\nand Î¸with training loss Ltrain:\nmin\nÎ±\nLval(Î¸âˆ—(Î±),Î±),\ns.t. Î¸âˆ—(Î±) = arg min\nÎ¸\nLtrain(Î¸,Î±).\nIn practice, we update Î± by âˆ‡Î±Lval and Î¸ by\nâˆ‡Î¸Ltrain in each step.\nIt is easy to directly apply DARTS in Trans-\nformers by replacing some or all operations in a\nTransformer block with mixed operation search\nnodes. For example, we can change the transformer\ndecoder block from Self Attn â†’Cross Attn â†’\nFFN to Search Node 1 â†’ Cross Attn â†’\nSearch Node 2. Note that a search node outputs\na weighted sum of different operations. To enable\ngradient calculation in the backward pass, we need\nto store every operationâ€™s output, which results in a\nsteep rise in memory consumption during search-\ning. Figure 1 shows the memory consumption of\nusing 2 search nodes in both Transformer encoder\nand decoder. DARTS run out of memory easily,\neven at a small hidden size.\n2.2 Multi-split Reversible Networks\nTo relieve the memory burden of DARTS in Trans-\nformers, we use reversible networks. A reversible\nnetwork layerâ€™s input can be reconstructed from its\noutput. Suppose a network is comprised of several\nreversible layers. We do not need to store inter-\nmediate outputs except the last layer, because we\ncan reconstruct them from top to bottom during\nbackpropagation (BP). Denote by X and f(X) the\n4256\nð‘‹1\nð‘‹3\nð‘‹2\nActivations\nð‘‹2\nð‘Œ1\nð‘‹3\nð‘‹3\nð‘Œ2\nð‘Œ1\nð‘Œ1\nð‘Œ3\nð‘Œ2\nActivationsActivations\n+ ++\nð‘‹1\nð‘‹3\nð‘‹2\nPooling\n+\nOperation \nSearch\nð‘‹2\nð‘Œ1\nð‘‹3\nPooling\n+\nOperation \nSearch\nð‘‹3\nð‘Œ2\nð‘Œ1\nPooling\n+\nOperation \nSearch\nð‘Œ1\nð‘Œ3\nð‘Œ2\n(a)\n(b)\nFigure 2: This ï¬gure is a demonstration of DARTSformer. (a) shows an n-split (n= 3) reversible network, which\nserves the backbone of our method. (b) shows the design of activations to enable differentiable architecture search.\nEach Xk and Yk are in RlÃ—d. The k-th pooling takes the concatenation of Xi>k and Yi<k as the input, and outputs\na tensor in RlÃ—d. The operation search gives a weighted average of the outputs of each candidate operation.\nlayer input and the layer output, respectively. X\nis ï¬rst split along the embedding/channel dimen-\nsion into nequal parts {X1,Â·Â·Â· ,Xn}. A RevNets\n(Gomez et al., 2017) alike operation is applied to\neach Xk, which yields Yk. f(X) is a concatenation\nof {Y1,Â·Â·Â· ,Yn}along the split dimension:\nY1 = X1 + G1(Xi>1,Î¸1),\n...\nYk = Xk + Gk(Xi>k,Yi<k,Î¸k),\n...\nYn = Xn + Gn(Yi<n,Î¸n).\n(3)\nGk is a mixed operation node during the archi-\ntecture search process. After searching, Gk is a\ndeterministic operation given by arg maxoâˆˆO Î±o.\nDetailed discussions can be found in Section 2.4.\nThe reversibility of Eq. (3) needs rigorous vali-\ndation, such that the input X can be easily recon-\nstructed from f(X):\nXn = Yn âˆ’Gn(Yi<n,Î¸n),\n...\nXk = Yk âˆ’Gk(Xi>k,Yi<k,Î¸k),\n...\nX1 = Y1 âˆ’G1(Xi>1,Î¸1).\n(4)\nPart (a) of Figure 2 illustrates a 3-split reversible\nnetwork, which we frequently employ throughout\nour experiments for simplicity.\n2.3 Backpropagation with Reconstruction\nConsider the problem of backpropagating (BP)\nthrough a reversible layer. Based on the layer\noutput f(X) = Concat(Y1,Â·Â·Â· ,Yn) and its to-\ntal derivative df(X) = Concat(dY1,Â·Â·Â· ,dYn),\nwe need to calculate the layer input X =\nConcat(X1,Â·Â·Â· ,Xn), its total derivative dX =\nConcat(dX1,Â·Â·Â· ,dXn), and the derivatives of the\nnetwork weights dÎ¸1,Â·Â·Â· ,dÎ¸n.\nWe show the BP-with-reconstruction through a\nsingle layer in Algorithm 1. [Â·] represents Concat(Â·)\nfor simplicity reasons. In Line 9 of Algorithm 1,\ndÎ¸k is calculated as a side effect. Line 10 shows\nthe reconstruction process, where each split Xk\nis recovered in the order of nto 1. In Algorithm\n1, gradk works as a gradient accumulator, which\nkeeps track of all derivatives associated with Xk.\nA repetitive application of Algorithm 1 enables us\nto backpropagate through a sequence of reversible\nlayers. Only the top layerâ€™s outputs require storage,\nwhich makes it much more memory-efï¬cient.\nRoughly speaking, for a network with N con-\nnections, the forward and backward passes require\napproximately N and 2N add-multiply operations,\nrespectively. Since we need to reconstruct X from\nf(X), the re-calculation requires another N add-\nmultiply operations, making it 33% slower. Fortu-\nnately, we can only need Algorithm 1 for architec-\nture search and will re-train the resulting network\nwith ordinary BP. The search process turns out to\nconverge fast. The computational overhead does\nnot become a severe problem.\n1Automatic differentiation routines, e.g. tf.gradient,\ntorch.autograd.backward\n4257\nAlgorithm 1 BP-with-reconstruction Algorithm\nfor Multi-Split Reversible Networks\nInput:\nLayer output: f(X) = [Y1,Â·Â·Â· ,Yn];\nTotal derivatives: df(X) = [dY1,Â·Â·Â· ,dYn];\nOperations: G1,Â·Â·Â· ,Gn;\nOutput:\nLayer input: X = [X1,Â·Â·Â· ,Xn];\nDerivatives of X: dX = [dX1,Â·Â·Â· ,dXn];\n1: X = {}; dX = {}; Y = {Y1,Â·Â·Â· ,Yn}\n2: for k in nto 1 do\n3: C= Yk; Y = Y \\{Yk}\n4: if k== nthen\n5: gradk = dYk\n6: else\n7: gradk = dYk + C.grad\n8: end if\n9: gk = Gk(X,Y,Î¸ k); gk.backward1(gradk)\n10: Xk = Câˆ’gk; X = Xâˆª{Xk}\n11: end for\n12: dX1 = grad1,dX = {dX1}\n13: for k in 2 to ndo\n14: dXk = Xk.grad + gradk\n15: dX = dXâˆª{dXk}\n16: end for\n2.4 DARTS with Multi-split Reversible\nNetworks\nPerforming DARTS based onn-split reversible net-\nworks only requires specifying each Gk in Eq. (3).\nSuppose that each Xk âˆˆRlÃ—dn (lis the sequence\nlength and dis the hidden size, dn = d\nn), and that\neach Yk has the same size as Xk. The input of Gk\ncontains nâˆ’1 tensors in RlÃ—dn. To enable element-\nwise addition with Xk, the output of Gk must also\nbe in RlÃ—dn.\nGk is factorized into two parts. The ï¬rst part is a\npooling operation, which takes an lÃ—dnÃ—(nâˆ’1)\ntensor as input, and outputs an lÃ—dn Ã—1 tensor.\nThe second part is a mixed operation search node.\nGk is calculated as follows:\nHk = Pooling(Xi>k,Yi<k),\nGk =\nâˆ‘\noâˆˆO\nexp(Î±k\no)âˆ‘\noâ€² âˆˆO exp(Î±k\noâ€² )o(Hk), (5)\nwhere Î±k is randomly initialized. Figure 3 shows\nthe design of Gk. By substituting each Gk in Eq.\n(3) with Eq. (5), we are able to use Algorithm 1\nto perform memory-efï¬cient DARTS. We call this\nð‘‹ð‘˜+1\nâ€¦\nð‘‹ð‘›\nð‘Œ1â€¦\nð‘Œð‘˜âˆ’1\nPooling\nð‘‚ð‘ƒ1\nð‘‚ð‘ƒð‘€ â€¦\nð›½ð‘˜ = ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥(ð›¼ð‘˜)\nðºð‘˜\nOperation \nSearch\n=\nWeighted average \nof ð‘‚ð‘ƒ1,â€¦,ð‘‚ð‘ƒð‘€ by \nð›½ð‘˜\nFigure 3: Pooling and operation search in each split.\nmethod DARTSformer, which is illustrated by Part\n(b) of Figure 2 in a 3-split case.\nThe overall search space size is critical to the\nperformance of DARTSformer. In our experiments,\nwe focus on sequence-to-sequence tasks where the\nencoder and the decoder are searched simultane-\nously. Suppose that we have an m-split encoder\nand an n-split decoder. We search sconsecutive\nlayers. For example, s= 2means that we search\nwithin a 2-layer encoder block. Each layer in\nthe block is an m-split reversible layer. The en-\ncoder contains several identical 2-layer blocks, the\nsame to the decoder. The search space is of size\n|O|s(m+n). If |O|is large, it can easily introduce a\nlarge search space even with small m,n and s.\n2.5 Instantiation\nWe describe the instantiation of DARTSformer in\nthis section.\nOperation Set The candidate operation set O is\ndeï¬ned as follows:\nâ€¢Standard Conv wÃ—1 :for wâˆˆ{3,5,7,11}.\nâ€¢Dynamic Conv wÃ—1 :for wâˆˆ{3,7,11,15}.\nâ€¢Self Attention.\nâ€¢Cross Attention: Only available to decoder.\nâ€¢Gated Linear Unit (GLU).\nâ€¢FFN.\nâ€¢Zero: Return a zero tensor of the input size.\nâ€¢Identity: Return the input.\nThe Dynamic Conv is from Wu et al. (2019). The\nSelf Attention, Cross Attention and FFN are from\nVaswani et al. (2017). We use 8 attention heads.\nThe GLU is from Dauphin et al. (2017).\nResidual connections (He et al., 2016) and layer\nnormalization (Ba et al., 2016) are crucial for con-\nvergence in training Transformers (Vaswani et al.,\n2017). To make our network fully reversible, these\ntwo tricks can not be used directly. Instead, we put\n4258\nAlgorithm 2 The framework of DARTSformer\nInput:\nOperation set: O, Search parameters: Î±;\nNetwork weights: Î¸;\nOutput:\nBest candidate network: Nfinal;\n1: Setup a multi-split reversible network with op-\neration search nodes Nsuper(O,Î±,Î¸ ).\n2: while Î±not converge do\n3: Update Î¸by Algorithm 1 with Ltrain.\n4: Update Î±by Algorithm 1 with Lval.\n5: end while\n6: Get Nfinal(O,Î±,Î¸ ) with Eq. (2).\nthe residual connections and layer normalization\nwithin each operation Ëœo(X) = LayerNorm(X +\no(X)), except for Zero and Identity.\nEncoder and Decoder We use an n-split en-\ncoder and an (n+1)-split decoder for DARTS-\nformer. Each Gk in the encoder takes the format\nof Eq. (5). Instead for the decoder, Gk<n+1 still\nfollows Eq. (5), but the operation for the last split\nGn+1 is ï¬xed as Cross Attention. Our experiments\nshow that this constraint on the decoder yields ar-\nchitectures with better performances.\nSearch and Re-train We summarize the entire\nframework of DARTSformer in Algorithm 2. Note\nthat the search process is the most memory inten-\nsive part, such that we use BP-with-reconstruction\nas shown in Line 2-5 of Algorithm 2.\n3 Experiment Setup\n3.1 Datasets\nWe use three standard datasets to perform our ex-\nperiments as So et al. (2019): (1) WMTâ€™18 English-\nGerman (En-De) without ParaCrawl, which con-\nsists of 4.5 million training sentence pairs. (2)\nWMTâ€™14 French-English (En-Fr), which consists\nof 36 million training sentence pairs. (3) WMTâ€™18\nEnglish-Czech (En-Cs), again without ParaCrawl,\nwhich consists of 15.8 million training sentence\npairs. Tokenization is done by Moses2. We employ\nBPE (Sennrich et al., 2016) to generate a shared\nvocabulary for each language pair. The BPE merge\noperation numbers are 32K (WMTâ€™18 En-De), 40K\n(WMTâ€™14 En-Fr), 32K (WMTâ€™18 En-Cs). We dis-\ncard sentences longer than 250 tokens. For the re-\ntraining validation set, we randomly choose 3300\n2https://github.com/moses-smt/mosesdecoder\nsentence pairs from the training set. The evalua-\ntion metric is BLEU (Papineni et al., 2002). We\nuse beam search for test sets with a beam size of\n5, and we tune the length penalty parameter from\n0.5 to 1.0. Suppose the input length is m, and the\nmaximum output length is 1.2m+ 10.\n3.2 Search Conï¬guration\nThe architecture searches are all run on WMTâ€™14\nEn-De. DARTS is a bilevel optimization process,\nwhich updates network weights Î¸ on one dataset\nand search parameters Î±on another dataset. We\nsplit the 4.5 million sentence pairs into 2.5/2.0 mil-\nlion for Î¸ and Î±. Both Ltrain and Lval are cross\nentropy loss with a label smoothing factor of 0.1.\nThe split number nis 2 for the encoder and 3 for\nthe decoder. We set sto 1 or 2, which means the\nsuper network contains several identical 1-layer\nor 2-layer blocks. The candidate operations are\ndetailed in Section 2.5, where |O|= 13/14 for en-\ncoder and decoder, respectively. Along the analysis\nin Section 2.4, the largest size of the search space is\naround 1 billion. We use a factorized word embed-\nding matrix to save memory. |V|is the vocabulary\nsize, and dis the hidden size. The original word\nembedding matrix E âˆˆR|V|Ã—d is factorized into a\nmultiplication of two matrices of size |V|Ã—eand\neÃ—d, where eâ‰ªd. We let edenote the embedding\nsize. We set e= 256,d = 960. During searching,\nwe set the dropout probability to 0.1. Two Adam\noptimizers (Kingma and Ba, 2015) are used for\nupdating Î¸ and Î±, with Î²1 = 0.9 and Î²2 = 0.98.\nFor Î¸, we use the same learning rate scheduling\nstrategy as done in Vaswani et al. (2017) with a\nwarmup step of 10000. The maximum learning\nrate is set to 5 Ã—10âˆ’4. For Î±, we ï¬x the learning\nrate to 3 Ã—10âˆ’4 with a weight decay of 1 Ã—10âˆ’3,\nwhich is the same as Liu et al. (2018) does.\nDARTSformer requires us to specify a pooling\noperation as stated in Eq. (5). We experiment\nwith both max pooling and average pooling. All\nsearches run on the same 8 NVIDIA V100 hard-\nware. We use a batch size of 5000 tokens per GPU\nand save a checkpoint every 10,000 updates (5000\nfor Î¸and 5000 for Î±). Our search process ï¬nalizes\nafter 60,000 updates.\n3.3 Training Details\nAll the networks derived from the saved check-\npoints are re-trained on WMTâ€™14 En-De to select\nthe best performing one. We then train the se-\nlected network on all datasets in Section 3.1 to\n4259\nModel Pooling Search s\nLayers\nModel\nSize BLEU\nTransformer - - 61.1M 27.7\nET - - 64.1M 28.2\nSampling max 2 60.1M 18.7\nSampling avg 2 61.6M 16.8\nDARTSformer max 1 64.5M 27.9\nDARTSformer max 2 65.2M 28.4\nDARTSformer avg 1 66.0M 28.3\nDARTSformer avg 2 63.4M 28.3\nTable 1: BLEU scores of various search setups on\nWMTâ€™14 En-De test set. ET is the Evolved Trans-\nformer (So et al., 2019). We use a 2-split encoder and\na 3-split decoder.\nModel Pooling Splits BLEU\nDARTSformer max 2,3 28.4\nDARTSformer max 3,4 28.0\nDARTSformer max 4,5 27.4\nDARTSformer avg 2,3 28.3\nDARTSformer avg 3,4 27.9\nDARTSformer avg 4,5 27.1\nTable 2: BLEU scores of DARTSformer with different\nsplit numbers on WMTâ€™14 En-De test set. We use ann-\nsplit encoder and an n+ 1-split decoder. We searching\nthrough 2 consecutive layers.\nverify its generalization ability. We follow the set-\ntings of So et al. (2019) with both a base model\nand a big model. For the base model, we still use\ne= 256,d = 960without re-scaling. For the big\nmodel, we set e = 512,d = 1824. Unless other-\nwise stated, all the training run on 8 Tesla V100\nGPU cards with the batch size of 5000 tokens per\ncard.\n4 Results\n4.1 Comparison Between Search Setups\nWe search through a different number of consecu-\ntive layers with different pooling operations. For re-\ntraining, we use the same learning rate scheduling\nstrategy as in searching. We also keep the dropout\nrate unchanged. Results are summarized in Table\n1. DARTSformers yields better results than stan-\ndard Transformers in all experimental setups. The\nmaximum performance gain is 0.7 BLEU with max\npooling when searching through 2 consecutive lay-\nModel Price Steps Hardware\nET $150k 4.2 Ã—108 200 TPUs\nDARTSformer $1.25k 4.8 Ã—105 8 V100\nTable 3: Comparison for search cost between Evolved\nTransformer (ET; So et al. 2019) and DARTSformer.\nThe price for ET is from Strubell et al. (2019).\nis max pooling\nDecoder  Input  Splits\nð‘¿ðŸ ð‘¿ðŸ ð‘¿ðŸ‘\n+\nð‘¿ðŸ ð‘¿ðŸ‘ ð’€ðŸ\nFFN Self Attn\n+\nð‘¿ðŸ‘ ð’€ðŸ ð’€ðŸ\nCross Attn\n+\nð’€ðŸ ð’€ðŸ ð’€ðŸ‘\n+\nð’€ðŸ ð’€ðŸ‘ ð’ðŸ\nSelf Attn\n+\nð’€ðŸ‘ ð’ðŸ ð’ðŸ\nCross Attn\n+\nCross Attn\nð’ðŸ ð’ðŸ ð’ðŸ‘\nConcat\nLinear\nSoftmax\nSelf AttnCross Attn\nCross Attn FFN\nNÃ—\nð‘¨ðŸ ð‘¨ðŸ\nFFN Self Attn\n+\nð‘¨ðŸ ð‘©ðŸ\nFFN Self Attn\n+\nð‘©ðŸ ð‘©ðŸ\nFFN Self Attn\n+\nð‘©ðŸ ð‘ªðŸ\nFFN Self Attn\n+\nð‘ªðŸ ð‘ªðŸ\nConcat\nEncoder  Input  Splits\nMÃ—\nFigure 4: Architecture searched by DARTSformer.\ners. Also, DARTSformer achieves slightly better\nresults than the Evolved Transformer in three out\nof four runs.\nWe compare the search cost between the Evolved\nTransformers and DARTSformer from various as-\npects. DARTSformer takes about 40 hours to run\non an AWS p3dn.24xlarge node3. The price for a\nsingle run of search is about $1.25k. As reported\nby Strubell et al. (2019), the search process of\nEvolved Transformer takes up to $150k, which\nis extremely expensive. As for hardware, the evolu-\ntionary search employs 200 TPU V .2 chips to run,\nwhile our method only uses 8 NVIDIA V100 cards.\nThe reason for the evolutionary search algorithmâ€™s\nhuge cost is that it requires training multiple can-\ndidate networks from scratch. We compare the\nnumber of parameter update steps in Table 3. The\nevolutionary search needs approximately 874 times\nmore update steps than our method.\nA simple sampling-based NAS method (Guo\net al., 2020) can also reduce memory consumption.\n3https://aws.amazon.com/ec2/instance-types/p3/\n4260\nFor each batch of training data, we setGk in Eq. (5)\nas a uniformly sampled operation from the candi-\ndate set O. The search parameters Î±are discarded,\nand the resulting network is produced from an evo-\nlutionary search by evaluating on the re-training\nvalidation set. This method performs poorly in ma-\nchine translation, as shown in Table 1. We ï¬nd that\nsampling-based methods favor large-kernel convo-\nlutions and that the resulting architectures tend to\ngenerate repetitive sentences.\nWe also experiment with increased split numbers.\nAs shown in Table 2, an increased split number\nhurts the translation performance. The best results\nare all achieved by the smallest split. Also, the\nsearch process is harder to converge as the search\nspace becomes too large. The re-training and in-\nference speed will slow down when increasing the\nsplit number because more recurrence are intro-\nduced in the calculation as shown in Eq. (3).\nIn the following sections, we try the best search\nresult (DARTSformer + search 2 layers + 2 split\n+ max pooling) in various sequence-to-sequence\ntasks to see its generalization ability. We show this\nsearched architecture in Figure 4.\n4.2 Performance of DARTSformer on Other\nDatasets\nFirst, we train DARTSformer with a base model\nsize on three translation tasks in Section 3.1. We\nwould like to see whether DARTSformer only per-\nforms well on the task used for architecture search\nor generalizes to related tasks. Second, we scale up\nthe model size and the batch size to see whether the\nperformance gain of DARTSformer still exists. We\ncompare DARTSformer with standard Transform-\ners and Evolved Transformers with similar model\nsizes. Following Vaswani et al. (2017), the parame-\nter size is around 62.5M/214.7M for the base model\nand big model, respectively. To match the settings\nof So et al. (2019) when training big models, we\nincrease the dropout rate to 0.3 and the learning\nrate to 1 Ã—10âˆ’3. We also accumulate gradients for\ntwo batches.\nResults are shown in Table 4. At the base model\nsize, DARTSformer steadily outperforms standard\nTransformers. We achieved the same translation\nquality (28.4 BLEU, reported by Vaswani et al.\n(2017)) as the original big Transformer in WMTâ€™14\nEn-De, with about 69% fewer parameters. Also,\nthe maximum BLEU gain is 0.9 in WMTâ€™14 En-Cs,\nwhich is not the dataset we conduct our architecture\nModels En-De En-Fr En-Cs\nTransformer 27.7 40.0 27.0\nET (So et al., 2019) 28.2 40.6 27.6\nDARTSformer 28.4 40.1 27.9\n(a) Comparison for Base Model Size\nModels En-De En-Fr En-Cs\nTransformer 29.1 41.2 28.1\nET (So et al., 2019) 29.3 41.3 28.2\nDARTSformer 29.8 41.3 28.5\n(b) Comparison for Big Model Size\nTable 4: BLEU scores on WMTâ€™14 translation tasks.\nET is the Evolved Transformer. We use the best search\nresult from different DARTSformer search setups.\nsearch on. As for Evolved Transformers, we sur-\npass their performance in two out of three datasets,\nand our search algorithm is more computationally\nefï¬cient. At the big model size, DARTSformer\nexceeds both standard Transformers and Evolved\nTransformers, which indicates the good generaliza-\ntion ability of DARTSformer.\n4.3 Performance of DARTSformer vs.\nParameter Size\nIn Section 4.2, DARTSformer consistently im-\nproves the performance with a model size com-\nparable to the base and big Transformers. We are\nwondering whether the performance increase exists\nwith smaller model sizes. We experiment with a\nspectrum of model sizes for standard Transformers\nand DARTSformer on WMTâ€™14 En-De. Specif-\nically, we use four embedding sizes for standard\nTransformers, [small:128, medium:256, base:512,\nbig:1024], where its hidden size is identical to the\nembedding size. We also adjust the model size\nof DARTSformer accordingly. For base and big\nmodels, we use the results from Section 4.2. For\nsmall and medium models, we set the learning rate\nto 5 Ã—10âˆ’4, the dropout probability to 0.1, and\nupdate the model parameters for 200,000 steps on\nthe same 8 NVIDIA V100 hardware.\nFigure 5 shows the results for both architec-\ntures. DARTSformer performs better than standard\nTransformers at all sizes. The BLEU increase is\n[1.3/0.9/0.7/0.7] for [small/medium/base/big] mod-\nels. An interesting fact is that the performance gap\nbetween two models tends to be smaller as we in-\ncrease the model size, which is also observed in So\n4261\n0 25 50 75 100 125 150 175 200\nModel size (million)\n22\n24\n26\n28\n30WMT'14 En-De BLEU\nBLEU with different model size\nTransformer\nDARTSformer\nFigure 5: BLEU comparison between DARTSformer\nand standard Transformers with different model sizes.\net al. (2019). Based on this observation, DARTS-\nformer is more pronounced for environments with\nresource limitations, such as mobile phones. A pos-\nsible reason for the decreased performance gap at\nlarger model sizes is that the effect of overï¬tting\nbecomes more important. We expect that some data\naugmentation skills (Sennrich et al., 2015; Edunov\net al., 2018; Qu et al., 2020) might be of help.\n4.4 The Impact of Search Hidden Size\nThe main motivation for our presented method is\nthat we want to search in a large hidden size to\nreduce the performance gap between searching and\nre-training. However, whether this gap exists needs\nrigorous validation. Otherwise, it would sufï¬ce to\ninstead use a small hidden size din architecture\nsearch, and then increase dafter search for training\nthe actual model. We experiment with 4 search\nhidden sizes, namely, e = 128,d = 120 (tiny),\ne = 128,d = 240 (small), e = 256,d = 480\n(medium), e = 256,d = 960(DARTSformer). e\nis the word embedding size and d is the hidden\nsize as described in Section 3.2. After obtaining\nthe searched model, we set the model size to e=\n256,d = 960, and re-train it on WMTâ€™14 En-De.\nThe results are summarized in Table 5, which\nclearly shows that the translation quality is improv-\ning as the search hidden size gets larger. Also, note\nthat when searching with tiny, small and medium\nsettings, the ï¬nal BLEU scores fall behind that of\nstandard transformers. We argue that if one wants\nto evaluate the searched model in large model sizes,\nit is important to search with large hidden sizes.\nFurther more, we directly apply DARTS with stan-\ndard transformer as the backbone model. We set\ne = 320,d = 320. A larger search hidden size\noften causes memory failure due to the storage\nSearch Settings e d BLEU\nTiny 128 120 24.2\nSmall 128 240 26.3\nMedium 256 480 27.5\nDARTSformer 256 960 28.4\nDARTS + Transformer 320 320 27.7\nTransformer - - 27.7\nTable 5: BLEU scores of DARTS with different search\nhidden sizes on WMTâ€™14 En-De test set. All searched\narchitectures are re-trained with a parameter size simi-\nlar to DARTSformer.\nof many intermediate hidden states. As shown in\nTable 5, We can see that searching with a small hid-\nden size yields no performance gain on the standard\ntransformer.\n5 Related Work\nArchitecture Search The ï¬eld of neural archi-\ntecture search (NAS) has seen advances in recent\nyears. In the early stage, researchers focus on the\nreinforcement learning-based approaches (Baker\net al., 2016; Zoph and Le, 2016; Cai et al., 2018a;\nZhong et al., 2018) and evolution-based approaches\n(Liu et al., 2017; Real et al., 2017; Miikkulainen\net al., 2019; So et al., 2019; Wang et al., 2020).\nThese methods can produce architectures that out-\nperform human-designed ones (Zoph et al., 2018;\nReal et al., 2019). However, the computational\ncost is almost unbearable since it needs to fully\ntrain and evaluate every candidate network found\nin the search process. Weight sharing (Brock et al.,\n2017; Pham et al., 2018) is a practical solution\nwhere a super network is trained, and its sub-graphs\nform the search space. Liu et al. (2018) proposed\nDARTS to use search parameters together with a\nsuper network, which allows searching with gra-\ndient descent. Gradient-based methods (Cai et al.,\n2018b; Xie et al., 2018; Chen et al., 2019; Xu et al.,\n2019; Yao et al., 2020) attracts researchersâ€™ atten-\ntion since it is computationally efï¬cient and easy\nto implement. We base our method on DARTS and\ntake one step further to reduce the memory con-\nsumption of training the super network. Another\nrecent trend is the one-stage NAS (Cai et al., 2019;\nMei et al., 2019; Hu et al., 2020; Yang et al., 2020).\nMany NAS algorithms are in two stages. In the\nï¬rst stage, one searches for a good candidate net-\nwork. In the second stage, the resulting network\n4262\nis re-initialized and re-trained. One-stage NAS\ntries to search and optimize the network weights\nsimultaneously. After searching, one can have a\nready-to-run network. We use a simple one-stage\nNAS algorithm (Guo et al., 2020) as a baseline in\nSection 4.1.\nReversible networks The idea of reversible net-\nworks is ï¬rst introduced by RevNets (Gomez et al.,\n2017). Later on, Jacobsen et al. (2018); Chang\net al. (2018); Behrmann et al. (2019) invented dif-\nferent reversible architectures based on the ResNet\n(He et al., 2016). MacKay et al. (2018) extended\nRevNets to the recurrent network, which is partic-\nularly memory-efï¬cient. Bai et al. (2019, 2020)\nconducted experiments with reversible Transform-\ners by ï¬xed point iteration. Kitaev et al. (2020)\ncombined local sensitive hashing attention with\nreversible transformers to save memory in train-\ning with long sequences. An important application\nof reversible networks is the ï¬‚ow-based models\n(Kingma and Dhariwal, 2018; Huang et al., 2018;\nTran et al., 2019). For sequence tasks, Ma et al.\n(2019) achieved success in non-autoregressive ma-\nchine translation.\n6 Conclusion\nWe have proposed a memory-efï¬cient differen-\ntiable architecture search (DARTS) method on\nsequence-to-sequence tasks. In particular, we have\nï¬rst devised a multi-split reversible network whose\nintermediate layer outputs can be reconstructed\nfrom top to bottom by the last layerâ€™s output. We\nhave then combined this reversible network with\nDARTS and developed a backpropagation-with-\nreconstruction algorithm to signiï¬cantly relieve the\nmemory burden during the gradient-based archi-\ntecture search process. We have validated the best\nsearched architecture on three translation tasks.\nOur method consistently outperforms standard\nTransformers. We can achieve the same BLEU\nscore as the original big Transformer does with\n69% fewer parameters. At a large model size, we\nsurpass Evolved Transformers with a search cost\nlower by an order of magnitude. Our method is\ngeneric to handle other architectures, and we plan\nto explore more tasks in the future.\nAcknowledgments\nYuekai Zhao and Zhihua Zhang have been sup-\nported by the Beijing Natural Science Foundation\n(Z190001), National Key Research and Develop-\nment Project of China (No. 2018AAA0101004),\nand Beijing Academy of Artiï¬cial Intelligence\n(BAAI).\nReferences\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016. Layer normalization. CoRR,\nabs/1607.06450.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. 2019.\nDeep equilibrium models. In Advances in Neural\nInformation Processing Systems, pages 690â€“701.\nShaojie Bai, Vladlen Koltun, and J Zico Kolter. 2020.\nMultiscale deep equilibrium models. arXiv preprint\narXiv:2006.08656.\nBowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh\nRaskar. 2016. Designing neural network architec-\ntures using reinforcement learning. arXiv preprint\narXiv:1611.02167.\nJens Behrmann, Will Grathwohl, Ricky T. Q. Chen,\nDavid Duvenaud, and Joern-Henrik Jacobsen. 2019.\nInvertible residual networks. In Proceedings of the\n36th International Conference on Machine Learning,\nvolume 97 of Proceedings of Machine Learning Re-\nsearch, pages 573â€“582. PMLR.\nAndrew Brock, Theodore Lim, James M Ritchie, and\nNick Weston. 2017. Smash: one-shot model ar-\nchitecture search through hypernetworks. arXiv\npreprint arXiv:1708.05344.\nHan Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang,\nand Song Han. 2019. Once-for-all: Train one\nnetwork and specialize it for efï¬cient deployment.\narXiv preprint arXiv:1908.09791.\nHan Cai, Jiacheng Yang, Weinan Zhang, Song Han,\nand Yong Yu. 2018a. Path-level network transfor-\nmation for efï¬cient architecture search. In Interna-\ntional Conference on Machine Learning, pages 678â€“\n687. PMLR.\nHan Cai, Ligeng Zhu, and Song Han. 2018b.\nProxylessnas: Direct neural architecture search\non target task and hardware. arXiv preprint\narXiv:1812.00332.\nB. Chang, L. Meng, E. Haber, Lars Ruthotto, David\nBegert, and E. Holtham. 2018. Reversible architec-\ntures for arbitrarily deep residual neural networks.\nIn AAAI.\nXin Chen, Lingxi Xie, Jun Wu, and Qi Tian. 2019. Pro-\ngressive differentiable architecture search: Bridging\nthe depth gap between search and evaluation. InPro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 1294â€“1303.\n4263\nYann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In International conference on\nmachine learning, pages 933â€“941. PMLR.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. arXiv preprint arXiv:1808.09381.\nAidan N Gomez, Mengye Ren, Raquel Urtasun, and\nRoger B Grosse. 2017. The reversible residual net-\nwork: Backpropagation without storing activations.\nIn Advances in neural information processing sys-\ntems, pages 2214â€“2224.\nZichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,\nZechun Liu, Yichen Wei, and Jian Sun. 2020. Sin-\ngle path one-shot neural architecture search with uni-\nform sampling. In European Conference on Com-\nputer Vision, pages 544â€“560. Springer.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770â€“\n778.\nShoukang Hu, Sirui Xie, Hehui Zheng, Chunxiao Liu,\nJianping Shi, Xunying Liu, and Dahua Lin. 2020.\nDsnas: Direct neural architecture search without pa-\nrameter retraining. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 12084â€“12092.\nChin-Wei Huang, David Krueger, Alexandre Lacoste,\nand Aaron Courville. 2018. Neural autoregressive\nï¬‚ows. volume 80 of Proceedings of Machine Learn-\ning Research, pages 2078â€“2087, StockholmsmÂ¨assan,\nStockholm Sweden. PMLR.\nJÂ¨orn-Henrik Jacobsen, Arnold W.M. Smeulders, and\nEdouard Oyallon. 2018. i-revnet: Deep invertible\nnetworks. In International Conference on Learning\nRepresentations.\nYufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang,\nand Jingbo Zhu. 2019. Improved differentiable ar-\nchitecture search for language modeling and named\nentity recognition. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 3576â€“3581.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nDurk P Kingma and Prafulla Dhariwal. 2018. Glow:\nGenerative ï¬‚ow with invertible 1x1 convolutions. In\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 31, pages\n10215â€“10224. Curran Associates, Inc.\nNikita Kitaev, Åukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efï¬cient transformer. arXiv\npreprint arXiv:2001.04451.\nHanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisan-\ntha Fernando, and Koray Kavukcuoglu. 2017. Hi-\nerarchical representations for efï¬cient architecture\nsearch. arXiv preprint arXiv:1711.00436.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2018. Darts: Differentiable architecture search.\narXiv preprint arXiv:1806.09055.\nZhichao Lu, Gautam Sreekumar, Erik Goodman, Wolf-\ngang Banzhaf, Kalyanmoy Deb, and Vishnu Naresh\nBoddeti. 2020. Neural architecture transfer. arXiv\npreprint arXiv:2005.05859.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard Hovy. 2019. FlowSeq: Non-\nautoregressive conditional sequence generation with\ngenerative ï¬‚ow. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 4282â€“4292, Hong Kong, China. As-\nsociation for Computational Linguistics.\nMatthew MacKay, Paul Vicol, Jimmy Ba, and Roger\nGrosse. 2018. Reversible recurrent neural net-\nworks. In Neural Information Processing Systems\n(NeurIPS).\nJieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin,\nLinjie Yang, Alan Yuille, and Jianchao Yang. 2019.\nAtomnas: Fine-grained end-to-end neural architec-\nture search. arXiv preprint arXiv:1912.09640.\nRisto Miikkulainen, Jason Liang, Elliot Meyerson,\nAditya Rawal, Daniel Fink, Olivier Francon, Bala\nRaju, Hormoz Shahrzad, Arshak Navruzyan, Nigel\nDuffy, et al. 2019. Evolving deep neural networks.\nIn Artiï¬cial intelligence in the age of neural net-\nworks and brain computing , pages 293â€“312. Else-\nvier.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311â€“318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nHieu Pham, Melody Guan, Barret Zoph, Quoc Le, and\nJeff Dean. 2018. Efï¬cient neural architecture search\nvia parameters sharing. In International Conference\non Machine Learning, pages 4095â€“4104. PMLR.\nYanru Qu, Dinghan Shen, Yelong Shen, Sandra\nSajeev, Jiawei Han, and Weizhu Chen. 2020.\nCoda: Contrast-enhanced and diversity-promoting\ndata augmentation for natural language understand-\ning.\n4264\nEsteban Real, Alok Aggarwal, Yanping Huang, and\nQuoc V Le. 2019. Regularized evolution for image\nclassiï¬er architecture search. In Proceedings of the\naaai conference on artiï¬cial intelligence, volume 33,\npages 4780â€“4789.\nEsteban Real, Sherry Moore, Andrew Selle, Saurabh\nSaxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le,\nand Alexey Kurakin. 2017. Large-scale evolution\nof image classiï¬ers. In International Conference on\nMachine Learning, pages 2902â€“2911. PMLR.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Improving neural machine translation\nmodels with monolingual data. arXiv preprint\narXiv:1511.06709.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715â€“\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nDavid R So, Chen Liang, and Quoc V Le.\n2019. The evolved transformer. arXiv preprint\narXiv:1901.11117.\nEmma Strubell, Ananya Ganesh, and Andrew Mc-\nCallum. 2019. Energy and policy considera-\ntions for deep learning in nlp. arXiv preprint\narXiv:1906.02243.\nDustin Tran, Keyon Vafa, Kumar Agrawal, Laurent\nDinh, and Ben Poole. 2019. Discrete ï¬‚ows: In-\nvertible generative models of discrete data. In Ad-\nvances in Neural Information Processing Systems ,\npages 14719â€“14728.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998â€“6008.\nHanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai,\nLigeng Zhu, Chuang Gan, and Song Han. 2020.\nHAT: Hardware-aware transformers for efï¬cient nat-\nural language processing. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7675â€“7688, Online. As-\nsociation for Computational Linguistics.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430.\nSirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.\n2018. Snas: stochastic neural architecture search.\narXiv preprint arXiv:1812.09926.\nYuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen,\nGuo-Jun Qi, Qi Tian, and Hongkai Xiong. 2019.\nPc-darts: Partial channel connections for memory-\nefï¬cient architecture search. arXiv preprint\narXiv:1907.05737.\nYibo Yang, Hongyang Li, Shan You, Fei Wang, Chen\nQian, and Zhouchen Lin. 2020. Ista-nas: Efï¬cient\nand consistent neural architecture search by sparse\ncoding. arXiv preprint arXiv:2010.06176.\nQuanming Yao, Ju Xu, Wei-Wei Tu, and Zhanxing Zhu.\n2020. Efï¬cient neural architecture search via prox-\nimal iterations. In Proceedings of the AAAI Con-\nference on Artiï¬cial Intelligence , volume 34, pages\n6664â€“6671.\nZhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and\nCheng-Lin Liu. 2018. Practical block-wise neural\nnetwork architecture generation. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 2423â€“2432.\nBarret Zoph and Quoc V Le. 2016. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and\nQuoc V Le. 2018. Learning transferable architec-\ntures for scalable image recognition. In Proceedings\nof the IEEE conference on computer vision and pat-\ntern recognition, pages 8697â€“8710.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6695300340652466
    },
    {
      "name": "Architecture",
      "score": 0.5864488482475281
    },
    {
      "name": "Differentiable function",
      "score": 0.5703617334365845
    },
    {
      "name": "Transformer",
      "score": 0.5030063986778259
    },
    {
      "name": "Parallel computing",
      "score": 0.3832692801952362
    },
    {
      "name": "Computer architecture",
      "score": 0.37010979652404785
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33205166459083557
    },
    {
      "name": "Mathematics",
      "score": 0.15377363562583923
    },
    {
      "name": "Electrical engineering",
      "score": 0.09994551539421082
    },
    {
      "name": "Engineering",
      "score": 0.09767407178878784
    },
    {
      "name": "Voltage",
      "score": 0.06504461169242859
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}