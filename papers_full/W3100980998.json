{
  "title": "Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning",
  "url": "https://openalex.org/W3100980998",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2098095826",
      "name": "Bingbing LI",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3001296363",
      "name": "Zhenglun Kong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134571643",
      "name": "Tian-yun Zhang",
      "affiliations": [
        "Syracuse University"
      ]
    },
    {
      "id": "https://openalex.org/A2097746400",
      "name": "Ji Li",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2102697724",
      "name": "Zhengang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102382644",
      "name": "Hang Liu",
      "affiliations": [
        "Stevens Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2277556808",
      "name": "Caiwen Ding",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3007759824",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4292564261",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2951396542",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963000224",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2657126969",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2787752464",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W2107861471",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2964203871",
    "https://openalex.org/W3001800955",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2285660444",
    "https://openalex.org/W2043701535",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2767785892"
  ],
  "abstract": "Pretrained large-scale language models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. However, the limited weight storage and computational speed on hardware platforms have impeded the popularity of pretrained models, especially in the era of edge computing. In this work, we propose an efficient transformer-based large-scale language representation using hardware-friendly block structure pruning. We incorporate the reweighted group Lasso into block-structured pruning for optimization. Besides the significantly reduced weight storage and computation, the proposed approach achieves high compression rates. Experimental results on different models (BERT, RoBERTa, and DistilBERT) on the General Language Understanding Evaluation (GLUE) benchmark tasks show that we achieve up to 5.0x with zero or minor accuracy degradation on certain task(s). Our proposed method is also orthogonal to existing compact pretrained language models such as DistilBERT using knowledge distillation, since a further 1.79x average compression rate can be achieved on top of DistilBERT with zero or minor accuracy degradation. It is suitable to deploy the final compressed model on resource-constrained edge devices.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3187–3199\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3187\nEfﬁcient Transformer-based Large Scale Language Representations using\nHardware-friendly Block Structured Pruning\nBingbing Li ∗1, Zhenglun Kong ∗2, Tianyun Zhang 3, Ji Li 4, Zhengang Li 2, Hang Liu 5, Caiwen Ding 1\n1University of Connecticut, 2Northeastern University, 3Syracuse University,\n4Microsoft Corporation, 5Stevens Institute of Technology\n{bingbing.li, caiwen.ding}@uconn.edu, {kong.zhe, li.zhen}@northeastern.edu, tzhan120@syr.edu,\nchangzhouliji@gmail.com, hliu77@stevens.edu\nAbstract\nPre-trained large-scale language models have\nincreasingly demonstrated high accuracy on\nmany natural language processing (NLP) tasks.\nHowever, the limited weight storage and com-\nputational speed on hardware platforms have\nimpeded the popularity of pre-trained mod-\nels, especially in the era of edge comput-\ning. In this work, we propose an efﬁcient\ntransformer-based large-scale language repre-\nsentation using hardware-friendly block struc-\nture pruning. We incorporate the reweighted\ngroup Lasso into block-structured pruning for\noptimization. Besides the signiﬁcantly re-\nduced weight storage and computation, the\nproposed approach achieves high compression\nrates. Experimental results on different mod-\nels (BERT, RoBERTa, and DistilBERT) on\nthe General Language Understanding Evalua-\ntion (GLUE) benchmark tasks show that we\nachieve up to 5.0 ×with zero or minor ac-\ncuracy degradation on certain task(s). Our\nproposed method is also orthogonal to exist-\ning compact pre-trained language models such\nas DistilBERT using knowledge distillation,\nsince a further 1.79 × average compression\nrate can be achieved on top of DistilBERT with\nzero or minor accuracy degradation. It is suit-\nable to deploy the ﬁnal compressed model on\nresource-constrained edge devices. We share\nthe related codes and models at: https://bi\nt.ly/3cvs2N2\n1 Introduction\nTransformer-based language model pre-training\nhas proven to be highly effective in learning univer-\nsal language representations from large-scale unla-\nbeled data and being ﬁne-tuned to adapt to down-\nstream tasks (Peters et al., 2018; Sun et al., 2019).\nRepresentative works such as BERT (Devlin et al.,\n2018), XLNet (Yang et al., 2019), RoBERTa (Liu\n∗These authors contributed equally\net al., 2019b), MT-DNN (Liu et al., 2019a), AL-\nBERT (Lan et al., 2019), GPT-2 (Radford et al.),\nand UniLMv2 (Bao et al., 2020) have substantially\nadvanced the state-of-the-art across a variety of\ndownstream tasks, such as text classiﬁcation, natu-\nral language inference, and question answering.\nDespite its success in performance improve-\nment in natural language understanding and gen-\neration, the computational cost and data storage\nof Transformer-based pre-trained language model\nare two widely recognized concerns due to Trans-\nformer’s deep architecture and rich parameters.\nThese models typically contain several hundred\nmillion parameters. The recent released research\nmodels even reach multi-billion parameters, such\nas MegatronLM (8.3 billion parameters) (Shoeybi\net al., 2019), Turing-NLG (17 billion parame-\nters) (Microsoft, 2020) and GPT-3 (175 billion pa-\nrameters) (Brown et al., 2020), which require more\nadvanced computing facility. Hence, it is imper-\native to reduce the computational cost and model\nstorage of pre-trained Transformer-based language\nmodels in order to popularize their applications in\ncomputer systems, especially in edge devices with\nlimited resources.\nSeveral works have been developed in the con-\ntext of model compression, such as knowledge dis-\ntillation (Hinton et al., 2015; Sanh et al., 2019;\nJiao et al., 2019; Sun et al., 2019), weight prun-\ning (Han et al., 2015), parameter sharing (Lan\net al., 2019) and weight quantization (Polino et al.,\n2018). For computer vision, the information com-\npressed/reduced in image features can be partially\nretrieved from neighboring pixels since they share\nsimilar and uniform characteristics spatially. How-\never, for NLP, the syntax and semantics informa-\ntion of Transformer in language/text domain are\nmore sensitive than that of computer vision. A high\ncompression rate for large-scale language models\nis difﬁcult to achieve on downstream NLP tasks.\n3188\nAs a result, there are few works in exploring and\noptimizing hardware-friendly model compression\ntechniques for state-of-the-art Transformer-based\npre-trained language models, to reduce the weight\nstorage and computation on computer system while\nmaintaining prediction accuracy.\nIn this work, we propose an efﬁcient\nTransformer-based large-scale language rep-\nresentations using block structured pruning. The\ncontributions of this work are as follows.\n•To the best of our knowledge, we are the ﬁrst\nto investigate hardware-friendly weight pruning\non pre-trained large-scale language models. Be-\nsides the signiﬁcantly reduced weight storage\nand computation, the adopted block structure\npruning has high ﬂexibility in achieving a high\ncompression rate. The two advantages are crit-\nical for efﬁcient Transformer in NLP since the\nnon-uniformed syntax and semantics informa-\ntion in language/text domain makes weight prun-\ning more difﬁcult than computer vision.\n•We incorporate the reweighted group Lasso for\noptimization into block structured pruning-based\non pre-trained large-scale language models in-\ncluding BERT, RoBERTa, and DistilBERT. We\nrelax the hard constraints in weight pruning by\nadding regularization terms in the objective func-\ntion and use reweighted penalty parameters for\ndifferent blocks. The dynamical regularization\ntechnique achieves higher compression rate with\nzero or minor accuracy degradation.\n•Our proposed method is orthogonal to existing\ncompact pre-trained language models such as\nDistilBERT using knowledge distillation. We\ncan further reduce the model size using our\nmethod with zero or minor accuracy.\nWe evaluate the proposed approach on several\nGLUE benchmark tasks (Wang et al., 2018). Ex-\nperimental results show that we achieve high com-\npression rates with zero or minor accuracy degra-\ndation. With signiﬁcant gain in weight storage re-\nduction (up to 5×) and computation efﬁciency, our\napproach can maintain comparable accuracy score\nto original large models including DistilBERT. The\nhardware-friendly transformer-based acceleration\nmethod is suitable to be deployed on resource-\nconstrained edge devices.\n2 Related Work\nTo address the memory limitation and high com-\nputational requirement of commonly seen deep\nlearning platforms such as graphics processing unit\n(GPU), tensor processing unit (TPU) and ﬁeld-\nprogrammable gate array (FPGA) on large-scale\npre-trained language models, various of compact\nNLP models or model compression techniques\nhave been investigated. ALBERT (Lan et al., 2019)\nutilizes parameter sharing technique across en-\ncoders to reduce weight parameters and uses the\nsame layer structures as BERT. It achieves com-\nparable results on different benchmarks to BERT.\nDespite the weight storage reduction, the computa-\ntional overhead remains unchanged since ALBERT\nand BERT have the same network structure.\nKnowledge distillationis another type of model\ncompression technique, which distills the knowl-\nedge from a large teacher model or an ensemble\nof models to a light-weighted student model (Hin-\nton et al., 2015). The student model is trained\nto intimate the class probabilities produced by\nthe large teacher model. For instance, Distil-\nBERT (Sanh et al., 2019) applies knowledge dis-\ntillation to BERT, and achieves 1.67×model size\nreduction and 1.63 ×inference speedup, while re-\ntaining 97% accuracy on the dev sets on the GLUE\nbenchmark, compared to BERT. Patient knowledge\ndistillation (Sun et al., 2019) is used to learn from\nmultiple intermediate layers of the teacher model\nfor incremental knowledge extraction.\nEfﬁcient deep learning methods can reduce the\nmodel size and accelerate the computation. It is\nwell known that, in practice, the weight represen-\ntation in deep learning models is redundant. Af-\nter removing several redundant weights with ap-\npropriate model compression algorithms, the deep\nlearning model can have minor accuracy degrada-\ntion. Prior work focused on heuristic and iterative\nnon-structured magnitude weight pruning(a.k.a,\nirregular pruning) (Han et al., 2015). It causes over-\nhead in both weight storage and computation in\ncomputer systems. On weight storage, it results\nin irregular, sparse weight matrices(as arbitrary\nweights can be pruned), and relies on indices to be\nstored in a compressed format such as Coordinate\n(COO) format. The introduced indices cause extra\nmemory footprint, i.e., at least one index per non-\nzero value, further degrading the compression rate.\nOn computation, it is difﬁcult to be accelerated\non current GPU architectures as reported in (Han\n3189\nFigure 1: Block structured pruning for weight matrix.\net al., 2016; Wen et al., 2016; Yu et al., 2017). On\nthe other hand, structured pruningconsiders reg-\nularity in weight pruning focusing on generating\nregular but smaller and dense matrix with no index.\nHowever, it suffers notable accuracy loss due to the\npoor solution quality, and therefore not suitable for\npruning sensitive syntax and semantics information\nin Transformer.\n3 Block Structured Pruning\n3.1 Problem Formulation\nWe adopt a more ﬁne-grained block structured prun-\ning algorithm, where pruning is executed by exclud-\ning entire blocks of weights within weight matrices\nsuch as rows or columns, therefore signiﬁcantly re-\nducing the number of indices when storing on mem-\nory. On computation, it is compatible with parallel\ncomputing platforms such as GPUs or Field Pro-\ngrammable Gate Arrays (FPGAs) in implementing\nmatrix multiplications. We formulate the weight\npruning problem using reweighted group Lasso, to\norchestrate the block structured pruning. Thus, the\nTransformer-based large-scale models can be more\nefﬁcient on computer systems while satisfying the\naccuracy requirement. As shown in Figure 1, we di-\nvide the weight matrix into small blocks and apply\nrow pruningand column pruning on each block.\nFor each row/column block, we compute the l2\nnorm. We prune the weights within the block ac-\ncording to our pre-set threshold or percentile. The\npseudocode is shown in Algorithm 1.\nConsider an N-layer Transformer, we denote\nthe weights and biases of the n-th layer as Wn and\nbn. The loss function is f\n(\n{Wn}N\nn=1,{bn}N\nn=1\n)\n,\nwhich will be minimized during training. For the\nblock structured pruning problem, our target objec-\nAlgorithm 1 Block structured pruning\nInput: weight matrix W, matrix width n, matrix height m,\nrow division k(or column division k′), threshold tb\nOutput: pruned weight matrix Wp\nSet Wp = W\nDivide Wp into kmatrices: W1,W2,...,Wk\nSet l2 norms= zeros(k,m)\nfor i= 1to k do\nfor j = 1to m do\nl2 norms(i,j) equals the l2 norm of the jth row of\nWi\nif l2 norms(i,j) ≤tb then\nWi(j,:) = 0\nend if\nend for\nend for\nWp = concatenate(W1,W2,...,Wk)\ntive is to reduce the number of columns and rows\nin the blocks of weight matrix while maintaining\nthe prediction accuracy.\nminimize f\n(\n{Wn}N\nn=1,{bn}N\nn=1\n)\nsubject to # of non-zero block rows in Wn is less than rn\n# of non-zero block columns in Wn is less than cn\n(1)\nwhere rn and cn are the desired non-zero block\nrows and columns, respectively. Due to regularity\nin pruning, only the non-zero rows/columns at the\nblock level need to be indexed, as opposed to each\nnon-zero element in irregular pruning. The stor-\nage overhead is minor compared to non-structured\nirregular pruning (Han et al., 2016). Because struc-\ntured pruning is applied independently within each\nblock, the scheme has higher ﬂexibility, thereby\nhigher accuracy, compared to the straightforward\napplication on the whole weight matrix (Wen et al.,\n2016).\n3190\n3.2 Reweighted Group Lasso Optimization\nIn problem (1), we use hard constraints to formu-\nlate the block row/column pruning problem. How-\never, it is more difﬁcult to satisfy the hard con-\nstraints on NLP than on computer vision. There are\ntwo reasons: i) Information compressed in image\nfeatures can be partially retrieved from neighboring\npixels since spatially they share similar and uni-\nform characteristics, whereas syntax and semantics\ninformation in deep Transformer in language/text\ndomain are not uniformly characterized; ii) Intu-\nitively, the high-level semantic, syntax, and lan-\nguage understanding capability might be broken\nwhen we prune zero or near-zero weights in the la-\ntent space. Therefore, a high compression rate for\nlarge-scale language models is difﬁcult to achieve\non downstream NLP tasks.\nTo address this issue, we relax the hard con-\nstraints by adding regularization terms in the objec-\ntive function. Prior work SSL (Wen et al., 2016)\nuses group Lasso as the relaxation of the hard con-\nstraints. Inspired by (Candes et al., 2008), we use\nreweighted penalty parameters for different blocks\nto achieve a high compression rate under same\naccuracy requirement than using a ﬁxed penalty\nparameter to all the blocks in group Lasso method.\nWhen we use group Lasso for block row pruning,\nthe regularization term is\nN∑\nn=1\npn∑\ni=1\nqn∑\nα=1\n√\nαhn∑\nj=(α−1)hn+1\n(Wn)2\nij\nwhere hn is the block row size in the n-th layer, pn\nis the number of rows in Wn, qn is the number of\nblocks in a row of Wn. And the block row pruning\nproblem is\nmin\n{Wn},{bn}\nf\n(\n{Wn}N\nn=1,{bn}N\nn=1\n)\n+ λ\nN∑\nn=1\npn∑\ni=1\nqn∑\nα=1\nγi,α\n√\nαhn∑\nj=(α−1)hn+1\n(Wn)2\nij,\n(2)\nwhere λ is the penalty parameter. γi,α is the\npenalty weight corresponding to the α-th block\nin the i-th row, and it is updated by γi,α =\n1/(\n√∑αhn\nj=(α−1)hn+1(Wn)2\nij + ϵ), where ϵ is a\nsmall value preventing division by zero. Similarly,\nwhen we prune columns in a block, the problem\nAlgorithm 2 Reweighted group Lasso on Trans-\nformer pruning\nInput: pre-trained model, model weight matrix W, matrix\nwidth n, matrix height m\nSet milestones = m1, m2, ..., ms\nSet T1 as the number of iterations of reweighted training\nmethod\nSet T2 as the number of iterations of retraining method\nCalculate γ\nfor s= 1to T1 do\nif sin milestones then\nUpdate γ\nend if\nCalculate l1loss and prediction loss f(W,b)\nmixedloss = l1loss + f(W,b)\nUpdate model weight W to minimize mixedloss using\nAdam\nend for\nPrune the weight matrix W using block structured pruning\nMask = zeros(m,n)\nfor i= 1to m do\nfor j = 1to n do\nif Wi,j == 0then\nSet Maski,j = 0\nelse\nSet Maski,j = 1\nend if\nend for\nend for\nfor s= 1to T2 do\nCalculate the prediction loss f(W,b)\nUpdate model weight W to minimize f(W,b) using\nAdam\nW = W ∗Mask\nend for\nbecomes\nmin\n{Wn},{bn}\nf\n(\n{Wn}N\nn=1,{bn}N\nn=1\n)\n+ λ\nN∑\nn=1\nrn∑\nj=1\nsn∑\nβ=1\nγj,β\n√\nβdn∑\ni=(β−1)dn+1\n(Wn)2\nij,\n(3)\nwhere dn is the block column size in the n-th layer,\nrn is the number of columns in Wn. sn is the\nnumber of blocks in a column of Wn. γj,β is the\npenalty weight corresponding to the β-th block\nin the j-th column, and it is updated by γj,β =\n1/(\n√∑βdn\ni=(β−1)dn+1(Wn)2\nij + ϵ).\nWe start with a pre-trained model and initialize\nthe collection of penalty weights (γi,α or γj,β) us-\ning the parameters in the pre-trained model. We\nremove the rows or blocks in a block if their group\nl2 norm is smaller than a threshold after reweighted\ntraining. We reﬁne the Transformer models us-\ning the non-zero weights. λ is used for adjust-\ning regularization strength. When λis too small,\nthe reweighted training is close to the original\ntraining. When λis too large, it gives too much\npenalty on the weights and accuracy cannot be\n3191\nmaintained. Speciﬁcally, we start reweighted train-\ning with λ= 0to reproduce the original results and\nincrease λto derive sparsity of the weight matrices.\nWe stop increasing λwhen the reweighted train-\ning accuracy drops slightly and the accuracy will\nbe improved after retraining. Overall, using the\nsame training trails, our method can achieve higher\npruning rate than prior works using structured prun-\ning (Wen et al., 2016), as shown in Algorithm 2.\n4 Evaluation\n4.1 Datasets\nWe conduct experiments on GLUE benchmark\n(Wang et al., 2018), a comprehensive collection of\nnine natural language understanding tasks covering\nthree NLP task categories with different degrees of\ndifﬁculty and dataset scales: single-sentence tasks,\nparaphrase similarity matching tasks, and infer-\nence tasks. All datasets are public available. More\nspeciﬁcally, for single-sentence task, we consider\nthe Corpus of Linguistic Acceptability (CoLA)\n(Warstadt et al., 2018), which contains 10,657 sen-\ntences of English acceptability judgments from\nbooks and articles on linguistic theory, and the Stan-\nford Sentiment Treebank (SST-2) (Socher et al.,\n2013), which is comprised of 215,154 phrases in\nthe parse trees of 11,855 sentences from movie\nreviews with annotated emotions.\nFor paraphrase similarity matching tasks, we\nconsider the Microsoft Research Paraphrase Cor-\npus (MRPC) (Dolan and Brockett, 2005), which\ncontains 5,800 sentence-pairs corpora from online\nnews sources and are manually annotated where\nthe sentences in the sentence-pairs are semantically\nequivalent; the Semantic Textual Similarity Bench-\nmark (STS-B) (Cer et al., 2017), a collection of\n8,628 sentence pairs extracted from the news title,\nvideo title, image title, and natural language infer-\nence data; and the Quora Question Pairs (QQP) 1,\na collection of 400,000 lines of potential question\nduplicate pairs from the Quora website.\nFor inference tasks, we consider the Multi-\nGenre Natural Language Inference Corpus (MNLI)\n(Williams et al., 2018), a set of 433k premise\nhypothesis pairs to predict whether the premise\nstatement contains assumptions for the hypothe-\nsis statement; Question-answering NLI (QNLI)\n(Wang et al., 2018), a set of over 100,000+\nquestion-answer pairs from SQuAD (Rajpurkar\n1https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\net al., 2016); The Recognizing Textual Entailment\ndatasets (RTE) (Wang et al., 2018), which come\nfrom the PASCAL recognizing Textual Entailment\nChallenge; and Winograd NLI (WNLI) (Levesque\net al., 2012), a reading comprehension task that\ncomes from the Winograd Schema Challenge.\nIn all GLUE benchmarks, we report the metrics\nfollowing the conventions in (Wang et al., 2018),\ni.e., accuracy scores are reported for SST-2, QNLI,\nRTE, and WNLI; Matthews Correlation Coefﬁcient\n(MCC) is reported for CoLA; F1 scores are re-\nported for QQP and MRPC; Spearman correlations\nare reported for STS-B.\n4.2 Experimental Setup\nBaseline Models. Our baseline models are\nunpruned BERT BASE (Devlin et al., 2018),\nRoBERTaBASE (Liu et al., 2019b), and Distil-\nBERT (Sanh et al., 2019). As shown in Table 1,\nfor each transformer model, we list the reported\naccuracy/metrics from the original papers in the\nﬁrst row. We report our reproduced results using\nthe same network architectures in the second row.\nEvaluation Metrics. To evaluate our pro-\nposed framework on NLP model compression\nproblems, we apply our method on different\ntransformer-based models including BERT BASE,\nRoBERTaBASE, and DistilBERT. Reweighted l1\ntraining is carried out to addl1 regularization, block\nstructured pruning to obtain a sparse model, and\nretraining to improve the ﬁnal accuracy.\nWe access the GPU-AI (Bridges GPU Arti-\nﬁcial Intelligence) nodes on the Extreme Sci-\nence and Engineering Discovery Environment\n(XSEDE) (Towns et al., 2014). We use two node\ntypes: V olta 16 - nine HPE Apollo 6500 servers,\neach with 8 NVIDIA Tesla V100 GPUs with 16\nGB of GPU memory each, connected by NVLink\n2.0; V olta 32 - NVIDIA DGX-2 enterprise research\nAI system tightly coupling 16 NVIDIA Tesla V100\n(V olta) GPUs with 32 GB of GPU memory each,\nconnected by NVLink and NVSwitch. We also\nuse an 8×NVIDIA Quadro RTX 6000 GPU server\nwith 24 GB of GPU memory each for training.\nWe conduct the experiments using HuggingFace\nTransformer toolkit for the state-of-the-art NLP\n(Wolf et al., 2019) and the DeepLearningExamples\nrepository from NVIDIA (NVIDIA, 2020). Our\nexperiments are performed on Python 3.6.10, GCC\n7.3.0, PyTorch 1.4.0, and CUDA 10.1.\nWe show the prediction accuracy with respect to\n3192\nTable 1: Comparison of test accuracy using different transformer models among the nine GLUE benchmark tasks.\nModels MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE WNLI\nBERTBASE(Devlin et al., 2018)84.6 91.2 90.5 93.5 52.1 85.8 88.9 66.4 -\nBERTBASE(ours) 83.9 91.4 91.1 92.7 53.4 85.8 89.8 66.4 56.3\nBERTBASEprune (ours) 82.9 90.7 88.2 89.3 52.6 84.6 88.3 63.9 56.3\nCompression rate 1.428× 1.428× 1.428× 1.428× 1.428× 1.428× 1.428× 1.428× 2.0×\nRoBERTaBASE(Liu et al., 2019b)87.6 91.9 92.8 94.8 63.6 91.2 90.2 78.7 -\nRoBERTaBASE(ours) 87.8 91.6 93.0 94.7 60.1 90.2 91.1 77.3 56.3\nRoBERTa prune (ours) 86.3 87.0 90.0 89.2 55.3 88.8 90.2 74.0 56.3\nCompression rate 1.428× 1.428× 1.428× 1.428× 1.246× 1.428× 1.428× 1.428× 2.0×\nDistilBERT (Sanh et al., 2019)82.2 88.5 89.2 91.3 51.3 86.9 87.5 59.9 56.3\nDistilBERT (ours) 81.9 90.2 89.5 90.9 50.7 86.5 89.8 59.2 56.3\nDistilBERT prune (ours) 78.5 87.4 85.3 85.3 53.4 83.7 89.1 59.2 56.3\nCompression rate 2.0× 1.667× 1.667× 2.0× 1.197× 1.667× 1.207× 2.0× 2.0×\ndifferent compression rates and we evaluate our\nmethod on the GLUE benchmark (Wang et al.,\n2018) in Table 1. For BERT, we use the ofﬁ-\ncial BERTBASE, uncased model as our pre-trained\nmodel. There are 12 layers (L=12; hidden size H\n= 768; self-attention heads A= 12), with total num-\nber of parameters 110 Million. We use the same\nﬁne-tuning hyperparameters as the paper (Devlin\net al., 2018). For RoBERTa (Liu et al., 2019b),\nwe use the ofﬁcial RoBERTa BASE model as our\npre-trained model. It has the same structure as the\nBERTBASE model, with 12 layers (L=12; hidden\nsize H= 768; self-attention heads A= 12), and a\ntotal number of 125 Million parameters. For Distil-\nBERT (Sanh et al., 2019), a distilled model from\nthe BERTBASE, uncased checkpoint, is used as the\npre-trained model. The parameters are L= 6; H =\n768; A= 12; total parameters = 66 M. The block\nsize used for pruning has different types, e.g., 3×3,\n3×12, and 12×3.\n4.3 Implementation Details\nWe ﬁrst ﬁne-tune the pre-trained models for classi-\nﬁcation. BERT, RoBERTa, and DistilBERT share\nthe same steps. We add a single linear layer on\ntop of each original model. We train the model for\nthe nine downstream GLUE tasks with their corre-\nsponding datasets. As we feed the data, the entire\npre-trained model and the additional untrained clas-\nsiﬁcation layer is trained on our speciﬁc task. The\noriginal layers already have great English words\nrepresentation, and we only need to train the top\nlayer, with a bit of tweaking in the lower levels to\naccommodate our task.\nFor ﬁne-tuning, we run 4 epochs with initial\nlearning rate of 2e−5, batch size of 32 and warm\nup proportion of 0.1. For block structured prun-\ning, we adjust the reweighted penalty parameter,\ncompression rate and training steps for each task.\nWe use the same parameters as ﬁne-tuning (epochs,\nlearning rate, batch size), then we adjust some pa-\nrameters for each task, depending on the predic-\ntion performance. For BERTBASE, we set penalty\nfactor 1e−3 for WNLI and MRPC; penalty factor\n1e−4 for CoLA, QQP, MNLI, SST-2, and RTE;\npenalty factor 1e−5 for QNLI. The learning rate\nis 3e−5 and batch size is 32 on nine tasks. For\nRoBERTaBASE, we set penalty factor 1e−3 for\nWNLI; penalty factor 1e−4 for MRPC, QQP, SST-\n2, and RTE; penalty factor 1e−5 for QNLI, CoLA,\nand MNLI. The learning rate and batch size are\nthe same as BERTBASE. For DistilBERT model,\nthe hyperparamters for reweighted training and re-\ntraining are learning rate = 3e−5 and batch size =\n128 on nine datasets. We adjust other parameters,\nincluding penalty factors, number of blocks, and\ncompression ratios to achieve the satisﬁed perfor-\nmance on each task.\nWe consider three objectives: weight distribu-\ntion, loss, and accuracy. Weight distribution shows\nthe distribution of weights in each layer after train-\ning and retraining. We visualize the weight param-\neters in Figure 2. With different pruning hyper-\nparameters including penalty factors, learning rate,\nblock numbers, and compression rate, the weights\nare distributed differently. We look at two losses:\nreweighted loss and mixed loss (the object func-\ntion in Equation (3)). For all our tasks, BERTBASE,\nRoBERTaBASE, and DistilBERT are converged in\nless than 4 epochs. During training, we evaluate\nthe performance between each given steps.\n4.4 Experimental Results\nWe compare the performance (accuracy score) of\nour pruned models with the baselines. The results\nare shown in Table 1. For BERT BASE, we set a\n3193\nFigure 2: Parameters distribution of DistilBERT model on CoLA dataset: (a) before pruning, (b) after pruning.\nFigure 3: Mixed loss of reweighted training on MRPC\ndataset with DistilBERT model.\ncompression rate of 1.428×(i.e., 30% sparsity) or\nabove. The average accuracy degradation is within\n2% on all tasks. On WNLI task, there is no ac-\ncuracy loss. On MNLI, QQP, CoLA, STS-B, and\nMRPC tasks, the accuracy loss is within 1.5%. On\nSST-2, QNLI, and RTE tasks, the accuracy loss is\nalso small (within 2.9%), compared to two baseline\nmodels. For RoBERTa, the average accuracy degra-\ndation is within 3% on all tasks. There is no ac-\ncuracy loss on WNLI. The accuracy loss is within\n1% on MRPC, within 2% on MNLI and STS-B\ntasks, within 4% on QNLI and RTE tasks, around\n5% on QQP, SST-2 and CoLA tasks. For Distil-\nBERT, the average accuracy degradation is within\n5% on all tasks. The accuracy losses are within\n1% on MRPC task, 3% on MNLI, QQP, QNLI, and\nSTS-B tasks, and 5% on SST-2 task. On CoLA and\nWNLI datasets, the pruned models perform even\nbetter than the unpruned models and increase the\nFigure 4: F1 score of reweighted training and retraining\nwith DistilBERT model on MRPC dataset.\nTable 2: Pruning results of BERT BASE with different\ncompression rates.\nCompression rateQQP MNLI WNLI QNLI SST-2\n1× 91.4 83.9 56.3 91.1 92.7\n1.428× 90.7 82.9 56.3 88.2 89.3\n2.0× 90.0 81.2 56.3 85.5 87.0\n5.0× 86.9 76.6 56.3 79.5 82.3\nﬁnal accuracy by 3% (1.197 ×compression rate)\nand 4% (2.0×compression rate), respectively. Fig-\nure 3 and Figure 4 show the reweighted training\nand retraining results on MRPC dataset, respec-\ntively. We choose 256 as the number of blocks. For\nreweighted training, the mixed loss drops during\ntraining within every 116 steps (4 epochs) and in-\ncreases signiﬁcantly since we update the penalty\nmatrix γ. For retraining, the pruned model achieves\nhigher F1 score than the unpruned one.\nWe evaluate the accuracy changes when com-\npression rates are different on BERTBASE and re-\nport the accuracy scores for different tasks. Results\nindicate that the sensitivities of tasks vary signiﬁ-\ncantly under different levels of compression rates.\nAs shown in Table 2, different tasks show different\naccuracy degradation when using the same com-\npression rate. As we increase the compression rate,\nthe accuracy degradation increased. For speciﬁc\ntask (e.g., WNLI), we can achieve up to 5×com-\npression rate from baseline model with zero accu-\nracy loss. Results on tasks such as WNLI and QQP\nshow minor accuracy degradation while results on\nSST-2, MNLI, QNLI, show higher accuracy degra-\ndation when compression rate is 5.0×. The differ-\nent accuracy results are related to different dataset\nsizes, degrees of difﬁcult, and evaluation metrics.\nWe compare our BSP method with irregular\nsparse format and the block sparse format (Narang\net al., 2017; Gray et al., 2017) (pruning all weights\non selected blocks). Table 3 shows that under same\naccuracy, our method achieves a slightly lower\npruning ratio compared to irregular sparse format.\nThis is because irregular pruning has a larger ﬂex-\n3194\nTable 3: Comparison of test accuracy between our BSP\nmethod and irregular sparse format on GLUE bench-\nmarks.\nNetwork ModelMNLI QQP QNLI SST2 SSTB RTE WNLIBERTBASEprune82.9 90.7 88.2 89.3 84.6 63.9 56.3Prune ratio 1.428×1.428×1.428×1.428×1.428×1.428×2.0×BERTBASEirregular83.7 86.5 87.8 90.8 86.7 63.5 56.3Prune ratio 2.0× 2.0× 1.667×2.0× 2.5× 2.373×2.0×DistilBERT prune78.5 87.4 85.3 85.3 83.7 59.2 56.3Prune ratio 2.0× 1.667×1.667×2.0× 1.667×2.0× 2.0×DistilBERT irregular80.3 88.7 87.2 86.7 84.7 59.9 56.3Prune ratio 2.381×2.174×2.326×2.222×2.222×2.083×2.0×\nTable 4: Comparison of test accuracy between our BSP\nmethod and block sparse method (Narang et al., 2017)\non GLUE benchmarks.\nNetwork ModelMNLI QQP QNLI SST2 SSTB RTE WNLIDistilBERT 81.9 90.2 89.5 90.9 86.5 59.2 56.3DistilBERT-prune78.5 87.4 85.3 85.3 83.7 59.2 56.3Block Sparse78.3 87.2 85.2 83.9 82.2 58.8 49.3Accuracy Loss0.2 0.2 0.1 1.4 1.5 0.4 13\nibility in pruning. However, irregular pruning is\nless effective when applying to hardware. Irreg-\nular sparse format introduces signiﬁcant memory\nstorage overhead when using Coordinate Format\n(COO) storage format, therefore is not hardware-\nfriendly. Our method, block structured format\n(pruning a portion of rows/columns on each block)\nstrikes a better balance between accuracy and mem-\nory storage than irregular sparse format or block\nsparse format (Narang et al., 2017; Gray et al.,\n2017). For irregular sparse format, when storing\nor transmitting an irregular sparse matrix using the\nCOO format, we store the subsequent nonzeros and\nrelated coordinates in memory. Three vectors are\nneeded: row, col, data, where data[i] is value at\n(row[i], col[i]) position. More speciﬁcally, given\n50% sparsity for a 9 ×9 matrix with block size of\n3×3, the storage of COO format is 1.5 ×9 ×9 =\n122; the storage of block structured sparsity is\n9 ×0.5 ×3 ×(3 + 1)(i.e., #blocks ×sparsity ×\nblock size ×(values + positionind)=54. Table 4\nlists the accuracy of our method and block sparse\nformat using DistilBERT. Our method achieves\n3.04% higher accuracy in average compared with\nblock sparse format.\nAs the proposed pruning is hardware-friendly,\nthe pruned weights can be efﬁciently stored in\nhardware memory with minor overhead (compared\nto other pruning methods like irregular pruning).\nWe use a compiler-assisted acceleration framework\nother than sparse linear algebra libraries, which al-\nlows the model to speed up with a sparsity of 30%.\nWe also apply matrix reorder and compact model\nstorage to achieve speed up on edge devices (Ma\net al., 2020). Hence, it is suitable to deploy the ﬁnal\ncompressed model on resource-constrained edge\ndevices such as embedded systems and mobile de-\nvices.\n5 Ablation Studies\nIn this section, we perform ablation experiments\nover several parameters when pruning BERT and\nDistilBERT to better understand their relative im-\nportance and the procedure. We change the se-\nlection of following parameters: the numbers of\nblocks for reweighted training and block structured\npruning, retraining epochs, and penalty factors. We\nalso evaluate the knowledge distillation friendly.\n5.1 Number of Blocks\nAfter selecting penalty factor 3e−4 and compres-\nsion rate 2.0×for each layer (except embedding\nlayers), we choose different numbers of blocks to\ntest. As shown in Table 5, the ﬁnal accuracy is\nsigniﬁcantly improved for both BERTBASE and Di-\ntilBERT when we increase the number of blocks. It\nveriﬁes that with more number of blocks (smaller\nblock size), our weight pruning algorithm has\nhigher ﬂexibility in exploring model sparsity.\nTable 5: Number of blocks for reweighted training and\nretraining on CoLA dataset.\nNumber of blocks 8 128 256 768\nBERTBASEretraining MCC14.5 48.0 52.6 51.5\nDistilBERT retraining MCC32.2 43.8 47.2 53.4\n5.2 Number of Retraining Epochs\nBy default, all GLUE tests are carried out by run-\nning four epochs for pre-training. For reweighted\ntraining and retraining, more epochs usually lead\nto better ﬁnal accuracy. In this test, we try different\nreweighted training and retraining epochs. During\nreweighted training, the mixed loss will drop signif-\nicantly within every 4 epochs, while the evaluation\nloss keeps relatively stable. We summarize the re-\nsults in Table 6. The ﬁnal accuracy of retraining is\nimproved when we increase the training epochs.\nTable 6: Retraining epochs on STS-B dataset.\nNumber of epochs 4 8 16\nBERTBASEretraining Spearman84.2 84.4 84.6\nDistilBERT retraining Spearman74.6 79.1 80.9\n5.3 Penalty Factors\nThe reweighted training procedure is utilized to pe-\nnalize the l2 norm of the blocks and thus to reduce\nthe magnitude of the weights. Therefore, larger\npenalty factors help to achieve better retraining\n3195\naccuracy since more smaller weight values of the\nweight matrices are pruned. However, if the penalty\nfactors are too large, the reweighted training algo-\nrithm is not able to compress the model well, which\nleads to signiﬁcant accuracy degradation. The re-\nsults are summarized in Table 7. The retraining\naccuracy is improved when we increase the penalty\nfactor from 3e−5 to 1e−4 and declines from 3e−4\nto 1e−3.\nTable 7: Penalty selections on MNLI dataset.\nPenalty factor for each layer3e−5 1e−4 3e−4 1e−3\nBERTBASEretraining accuracy80.7 82.5 82.9 78.9\nDistilBERT retraining accuracy65.8 68.8 73.6 70.0\n5.4 Variance of results on multiple runs\nDuring our training, the random seeds are set to 42\nas default. We further conduct experiments choos-\ning different seeds and list the results in Table 8.\nWe observe our reported accuracy is aligned with\nthe results with different seeds.\nTable 8: Variance of results on multiple runs.\nSeed SST-2 CoLA STS-B MRPC\n42(default) 85.3 53.4 83.7 89.1\n1 83.14 53.75 83.19 89.3\n1000 82.8 54.08 83.32 89.3\n5000 82.91 54.22 83.03 89.0\n5.5 Knowledge Distillation Friendly\nTo evaluate the effectiveness of our pruning method\non distilled models, we focus on the BERT and Dis-\ntilBERT results in Table 1, where DistilBERT is a\nhighly distilled version of BERT. The average com-\npression rate of BERT and DistilBERT are 1.49×\nand 1.79×, respectively. Please note that model\nsize of BERT is 1.67×of DistilBERT, and there-\nfore is 2.99×of the ﬁnal compressed DistilBERT\nmodel size. This show that the proposed block\nstructured pruning is orthogonal to knowledge dis-\ntillation. With this knowledge distillation friendly\nproperty, we can ﬁrst apply the standard knowledge\ndistillation step to reduce the original large model\nand then apply the proposed pruning method to\nfurther reduce the size of the student model.\n6 Conclusion\nIn this work, we propose an hardware-friendly\nblock structured pruning pruning framework for\ntransformer-based large-scale language representa-\ntion. We incorporate the reweighted group Lasso\ninto for optimization and relax the hard constraints\nin block structured pruning. We signiﬁcantly re-\nduce weight storage and computational require-\nment. Experimental results on different models\n(BERT, RoBERTa, and DistilBERT) on the GLUE\nbenchmark tasks show that we achieve signiﬁcant\ncompression rates with zero or minor accuracy\ndegradation on certain benchmarks. Our proposed\nmethod is orthogonal to existing compact pre-\ntrained language models such as DistilBERT using\nknowledge distillation. It is suitable to deploy the\nﬁnal compressed model on resource-constrained\nedge devices.\nAcknowledgement\nThis work used the Extreme Science and Engineer-\ning Discovery Environment (XSEDE), which is\nsupported by National Science Foundation grant\nnumber ACI-1548562. In particular, it used the\nBridges-GPU AI system at the Pittsburgh Super-\ncomputing Center (PSC) through allocations TG-\nCCR200004.\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang,\nNan Yang, Xiaodong Liu, Yu Wang, Songhao\nPiao, Jianfeng Gao, Ming Zhou, et al. 2020.\nUnilmv2: Pseudo-masked language models for uni-\nﬁed language model pre-training. arXiv preprint\narXiv:2002.12804.\nTom B. Brown, Benjamin Pickman Mann, Nick Ryder,\nMelanie Subbiah, Jean Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, G. Kr ¨uger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric J Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nEmmanuel J Candes, Michael B Wakin, and Stephen P\nBoyd. 2008. Enhancing sparsity by reweighted l1\nminimization. Journal of Fourier analysis and ap-\nplications, 14(5-6):877–905.\nDaniel Cer, Mona Diab, Eneko Agirre, I ˜nigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\n3196\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third International Workshop on Paraphrasing\n(IWP2005). Asia Federation of Natural Language\nProcessing.\nScott Gray, A. Radford, and Diederik P. Kingma. 2017.\nGpu kernels for block-sparse weights.\nSong Han, Xingyu Liu, Huizi Mao, Jing Pu, Arda-\nvan Pedram, Mark A Horowitz, and William J Dally.\n2016. EIE: efﬁcient inference engine on compressed\ndeep neural network. In Proceedings of the 43rd\nInternational Symposium on Computer Architecture,\npages 243–254. IEEE Press.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefﬁcient neural network. In Advances in neural in-\nformation processing systems, pages 1135–1143.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. stat,\n1050:9.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4487–4496.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXiaolong Ma, Zhengang Li, Yifan Gong, Tianyun\nZhang, Wei Niu, Zheng Zhan, Pu Zhao, Jian Tang,\nXue Lin, Bin Ren, and Yanzhi Wang. 2020. Blk-\nrew: A uniﬁed block-based dnn pruning framework\nusing reweighted regularization method.\nMicrosoft. 2020. Turing-nlg: A 17-billion-parameter\nlanguage model by microsoft, 2020.\nSharan Narang, Eric Undersander, and Gregory Di-\namos. 2017. Block-sparse recurrent neural net-\nworks.\nNVIDIA. 2020. Deep learning examples for tensor\ncores. https://github.com/NVIDIA/Deep\nLearningExamples/tree/#commit-hash/PyT\norch/LanguageModeling/BERT.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nAntonio Polino, Razvan Pascanu, and Dan Alistarh.\n2018. Model compression via distillation and quan-\ntization. arXiv preprint arXiv:1802.05668.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100, 000+ ques-\ntions for machine comprehension of text. CoRR,\nabs/1606.05250.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4314–4323.\nJ. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither,\nA. Grimshaw, V . Hazlewood, S. Lathrop, D. Lifka,\nG. D. Peterson, R. Roskies, J. R. Scott, and\nN. Wilkins-Diehr. 2014. Xsede: Accelerating scien-\ntiﬁc discovery. Computing in Science & Engineer-\ning, 16(5):62–74.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\n3197\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judg-\nments.\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen,\nand Hai Li. 2016. Learning structured sparsity in\ndeep neural networks. In Advances in Neural Infor-\nmation Processing Systems, pages 2074–2082.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nThomas Wolf, L Debut, V Sanh, J Chaumond, C De-\nlangue, A Moi, P Cistac, T Rault, R Louf, M Fun-\ntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nJiecao Yu, Andrew Lukefahr, David Palframan, Ganesh\nDasika, Reetuparna Das, and Scott Mahlke. 2017.\nScalpel: Customizing dnn pruning to the underlying\nhardware parallelism. In Proceedings of the 44th An-\nnual International Symposium on Computer Archi-\ntecture, pages 548–560. ACM.\n7 Appendix\n7.1 Single-layer Sensitivity\nBefore retraining, block structured pruning is car-\nried out for the reweighted trained models by choos-\ning compression ratio for each layer. However, the\nsensitivity of different layers are different, which\nmay leads to signiﬁcant accuracy loss if the com-\npression ratios are not proper. To test the sensitiv-\nity, we prune 50% of each layer while keeping the\nother layers unpruned and obtain the ﬁnal accuracy\nafter retraining. According to tests, embedding\nlayers are sensitive on all datasets except WNLI.\nOn MRPC and RTE datasets, we choose 8 as the\nnumber of blocks and 3e−4 as the penalty factor.\nIn Figure 5, the ﬁrst two weight matrices are re-\nlated to embedding layers, while the third to the 38-\nth weights are related to transformer layers (each\ntransformer layer includes 6 weights). The last two\nlayers is related to classiﬁer layers. The results\nFigure 5: Layer sensitivity with DistilBERT model.\nshow that the embedding layers and linear weights\nin transformer layers are sensitive on CoLA and\nMRPC datasets. Therefore, we set the compression\nratios of corresponding weights zero to ensure the\nﬁnal accuracy.\n7.2 Number of Blocks\nFigure 6 and Figure 7 represent reweighted train-\ning and retraining accuracy of different block sizes,\nrespectively. During reweighted training, the ac-\ncuracy decreases when we increase the number of\nblocks, since the corresponding l1 loss increases\nsigniﬁcantly, which leads to mixedloss to increase\nas shown in Figure 8. The ﬁnal accuracy is im-\nproved when increasing the number of blocks since\nthe algorithm is capable to operate on smaller units\nof the weight matrices.\n7.3 Number of Retraining Epochs\nFor reweighted training, Figure 9 and Figure 10\nshow the results of mixed and evaluation loss, re-\nspectively, in which we update theγmatrix every\nfour epochs. For each selection of training epochs,\nwe use linear learning rate decay and thus the re-\nsults do not coincide with each other. The ﬁnal ac-\ncuracy of retraining is improved when we increase\nthe training epochs as shown in Figure 11.\nFigure 6: Reweighted training accuracy of different\nweight matrix block division on CoLA dataset with Dis-\ntilBERT model.\n3198\nFigure 7: Retraining accuracy of different weight ma-\ntrix block division on CoLA dataset with DistilBERT\nmodel.\nFigure 8: Mixed loss during reweighted training of dif-\nferent weight matrix block divisions on CoLA dataset\nwith DistilBERT model.\nFigure 9: Mixed loss of reweighted training with differ-\nent epochs on STS-B dataset with DistilBERT model.\nFigure 10: Evaluation loss of reweighted training with\ndifferent epochs on STS-B dataset with DistilBERT\nmodel.\nFigure 11: Retraining spearman correlation with dif-\nferent retraining epochs on STS-B dataset with Distil-\nBERT model.\nFigure 12: Retraining accuracy using different penalty\nfactors on MNLI dataset with DistilBERT model.\n7.4 Penalty Factors\nIn Figure 12, the retraining accuracy is improved\nwhen we increase the penalty factor from 3e−5 to\n1e−4 and declines from 3e−4 to 1e−3.\n7.5 Retrain Accuracy\nFigure 13 ∼Figure 21 show the accuracy with\nRoBERTaBASE model on nine GLUE benchmark\ntasks during retraining steps.\nFigure 13: Retraining accuracy on MNLI dataset with\nRoBERTa model.\n3199\nFigure 14: Retraining F1 on QQP dataset with\nRoBERTa model.\nFigure 15: Retraining accuracy on QNLI dataset with\nRoBERTa model.\nFigure 16: Retraining accuracy on SST-2 dataset with\nRoBERTa model.\nFigure 17: Retraining mcc on CoLA dataset with\nRoBERTa model.\nFigure 18: Spearman correlation on STS-B dataset\nwith RoBERTa model.\nFigure 19: Retraining F1 on MRPC dataset with\nRoBERTa model.\nFigure 20: Retraining accuracy on RTE dataset with\nRoBERTa model.\nFigure 21: Retraining accuracy on WNLI dataset with\nRoBERTa model.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8012768030166626
    },
    {
      "name": "Language model",
      "score": 0.7489310503005981
    },
    {
      "name": "Transformer",
      "score": 0.5531913042068481
    },
    {
      "name": "Pruning",
      "score": 0.5388069152832031
    },
    {
      "name": "Edge device",
      "score": 0.529265284538269
    },
    {
      "name": "Computation",
      "score": 0.5018081665039062
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4932395815849304
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.490354061126709
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47582271695137024
    },
    {
      "name": "Enhanced Data Rates for GSM Evolution",
      "score": 0.4434199035167694
    },
    {
      "name": "Natural language processing",
      "score": 0.35916855931282043
    },
    {
      "name": "Parallel computing",
      "score": 0.3394456207752228
    },
    {
      "name": "Computer engineering",
      "score": 0.3332550525665283
    },
    {
      "name": "Machine learning",
      "score": 0.3217161297798157
    },
    {
      "name": "Algorithm",
      "score": 0.23882552981376648
    },
    {
      "name": "Voltage",
      "score": 0.08981990814208984
    },
    {
      "name": "Mathematics",
      "score": 0.0840887725353241
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Cloud computing",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70983195",
      "name": "Syracuse University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210105678",
      "name": "Microsoft (Finland)",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I108468826",
      "name": "Stevens Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 36
}