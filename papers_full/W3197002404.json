{
  "title": "Mixed Attention Transformer for Leveraging Word-Level Knowledge to Neural Cross-Lingual Information Retrieval",
  "url": "https://openalex.org/W3197002404",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5090589471",
      "name": "Zhiqi Huang",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A5010532614",
      "name": "Hamed Bonab",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A5089659476",
      "name": "Sheikh Muhammad Sarwar",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A5103185916",
      "name": "Razieh Rahimi",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A5034070218",
      "name": "James Allan",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2976672264",
    "https://openalex.org/W3035540729",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2610935556",
    "https://openalex.org/W2536015822",
    "https://openalex.org/W3115729981",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2899469960",
    "https://openalex.org/W2963617771",
    "https://openalex.org/W2954573505",
    "https://openalex.org/W2940927814",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2917705549",
    "https://openalex.org/W2918134408",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3035473397",
    "https://openalex.org/W2952455395",
    "https://openalex.org/W2803620078",
    "https://openalex.org/W2250473075",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2068297964",
    "https://openalex.org/W3134665270",
    "https://openalex.org/W3032608552",
    "https://openalex.org/W3154079701",
    "https://openalex.org/W3035313607",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2988809116",
    "https://openalex.org/W3034779619",
    "https://openalex.org/W2595551253",
    "https://openalex.org/W2800252929",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3130347092",
    "https://openalex.org/W3032541519",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3028616679",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W3022979090",
    "https://openalex.org/W2977959150",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W1482214997",
    "https://openalex.org/W3102286003",
    "https://openalex.org/W2991612931",
    "https://openalex.org/W2938224028",
    "https://openalex.org/W2154200620",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W3166441238",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2252046065",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3098366475",
    "https://openalex.org/W2968289784",
    "https://openalex.org/W4300427681"
  ],
  "abstract": "Pretrained contextualized representations offer great success for many downstream tasks, including document ranking. The multilingual versions of such pretrained representations provide a possibility of jointly learning many languages with the same model. Although it is expected to gain big with such joint training, in the case of cross lingual information retrieval (CLIR), the models under a multilingual setting are not achieving the same level of performance as those under a monolingual setting. We hypothesize that the performance drop is due to the translation gap between query and documents. In the monolingual retrieval task, because of the same lexical inputs, it is easier for model to identify the query terms that occurred in documents. However, in the multilingual pretrained models that the words in different languages are projected into the same hyperspace, the model tends to translate query terms into related terms, i.e., terms that appear in a similar context, in addition to or sometimes rather than synonyms in the target language. This property is creating difficulties for the model to connect terms that cooccur in both query and document. To address this issue, we propose a novel Mixed Attention Transformer (MAT) that incorporates external word level knowledge, such as a dictionary or translation table. We design a sandwich like architecture to embed MAT into the recent transformer based deep neural models. By encoding the translation knowledge into an attention matrix, the model with MAT is able to focus on the mutually translated words in the input sequence. Experimental results demonstrate the effectiveness of the external knowledge and the significant improvement of MAT embedded neural reranking model on CLIR task.",
  "full_text": "Mixed Attention Transformer for Leveraging Word-Level\nKnowledge to Neural Cross-Lingual Information Retrieval\nZhiqi Huang, Hamed Bonab, Sheikh Muhammad Sarwar, Razieh Rahimi, and James Allan\nCenter for Intelligent Information Retrieval\nUniversity of Massachusetts Amherst\n{zhiqihuang,bonab,smsarwar,rahimi,allan}@cs.umass.edu\nABSTRACT\nPre-trained contextualized representations offer great success for\nmany downstream tasks, including document ranking. The mul-\ntilingual versions of such pre-trained representations provide a\npossibility of jointly learning many languages with the same model.\nAlthough it is expected to gain big with such joint training, in\nthe case of cross-lingual information retrieval (CLIR), the models\nunder a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize\nthat the performance drop is due to the translation gap between\nquery and documents. In the monolingual retrieval task, because of\nthe same lexical inputs, it is easier for model to identify the query\nterms that occurred in documents. However, in the multilingual\npre-trained models that the words in different languages are pro-\njected into the same hyperspace, the model tends to ‚Äútranslate‚Äù\nquery terms into related terms ‚Äì i.e., terms that appear in a similar\ncontext ‚Äì in addition to or sometimes rather than synonyms in\nthe target language. This property is creating difficulties for the\nmodel to connect terms that co-occur in both query and document.\nTo address this issue, we propose a novel Mixed Attention Trans-\nformer (MAT) that incorporates external word-level knowledge,\nsuch as a dictionary or translation table. We design a sandwich-\nlike architecture to embed MAT into the recent transformer-based\ndeep neural models. By encoding the translation knowledge into\nan attention matrix, the model with MAT is able to focus on the\nmutually translated words in the input sequence. Experimental re-\nsults demonstrate the effectiveness of the external knowledge and\nthe significant improvement of MAT-embedded neural reranking\nmodel on CLIR task.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíInformation retrieval; Multilingual\nand cross-lingual retrieval ; Retrieval models and ranking .\nKEYWORDS\nCross-lingual information retrieval; Attention mechanism; Neural\nnetwork\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nCIKM ‚Äô21, November 1‚Äì5, 2021, Virtual Event, QLD, Australia\n¬© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8446-9/21/11. . . $15.00\nhttps://doi.org/10.1145/3459637.3482452\nACM Reference Format:\nZhiqi Huang, Hamed Bonab, Sheikh Muhammad Sarwar, Razieh Rahimi, and\nJames Allan. 2021. Mixed Attention Transformer for Leveraging Word-Level\nKnowledge to Neural Cross-Lingual Information Retrieval. In Proceedings\nof the 30th ACM International Conference on Information and Knowledge\nManagement (CIKM ‚Äô21), November 1‚Äì5, 2021, Virtual Event, QLD, Australia.\nACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3459637.3482452\n1 INTRODUCTION\nWe study the problem of Cross-Lingual Information Retrieval (CLIR)\nin which the desired information is written in a language different\nthan that of the user‚Äôs query. From the modeling perspective, in\nthe CLIR setting some form of language translation is needed to\nmap the vocabulary of the query language to that of the documents‚Äô\nlanguage in addition to the ranking component. This translation\ngap can be bridged with simple dictionaries, translation tables,\nmachine translation, or more recently cross-language distributional\nrepresentations [2, 37, 45].\nEmbedding the translation component in the fine-tuning stage\nalong with the ranking makes the training of deep neural mod-\nels for the CLIR more challenging, particularly when dealing with\nresource-lean languages [1, 23]. Pre-trained language models such\nas BERT [12] have shown promising performance gains for mono-\nlingual information retrieval [15, 34, 46, 49]. This success is mainly\ndue to the unsupervised pre-training of context-aware transformer\narchitectures with an enormous number of parameters over large\ncorpora. To achieve success in the learning-to-rank task such mod-\nels are often fine-tuned with a relatively large collection of relevance\njudgments such as the MS MARCO passage ranking dataset [25].\nHowever, it is not feasible to obtain data in the scale of MS MARCO\nacross different languages. Thus, some studies leverage different\ntraining data (e.g., weak-supervised data, cross-lingual Wikipedia-\nbased data [38, 39]) and techniques (e.g., domain adaptation, few-\nshot learning) in order to adapt the model for the target task and\nlanguage, reporting improvements.\nThe multilingual versions of pre-trained Transformer-based lan-\nguage models, such as mBERT [ 12] and XLM-R [ 7], provide the\npossibility of jointly learning representations for multiple languages\nwith the same model. Fine-tuning these pre-trained multilingual\nlanguage models for ranking, similar to the monolingual setting,\nenables cross-language information retrieval. In the multilingual\npre-trained models that words in different languages are projected\ninto the same hyperspace, the model tends to map query terms into\ntarget language‚Äôs related terms ‚Äì i.e., terms that appear in a similar\ncontext ‚Äì in addition to or sometimes rather than synonyms [2, 34].\nWe hypothesize that this phenomena creates difficulties for the\narXiv:2109.02789v2  [cs.IR]  14 Sep 2021\nmodel to connect terms that match between the query and docu-\nment. It has been shown that the translation gap plays a significant\nrole in the suboptimal success of neural CLIR models and addressing\nthat can significantly boost the retrieval performance [2]. Therefore,\nthe multilingual language models for the CLIR task have not yet\nachieved the performance gain observed with the use of pre-trained\nlanguage models for monolingual information retrieval [ 22, 48].\nThis can happen in the CLIR task because the vocabulary size is\nalmost doubled, the possibility of exact match between query and\ndocument is limited, and training data (e.g., bilingual query log or\nclick data) is scarce. Most of the existing CLIR systems are thus\ndeployed along with a query translation component to reduce the\nproblem into monolingual retrieval. However, it is important to\nnote that having a translation component as a black-box limits the\nretrieval component due to translation errors.\nWe inject word-level translation knowledge into a model at the\ntime of fine-tuning it with relevance data. More specifically, we\nleverage the external knowledge in the form of a translation table,\nwhich is a look-up table that provides translation probabilities for\na pair of words in two different languages. We use the translation\ntable to create an attention matrix and use it in parallel with the\nTransformer‚Äôs multi-head attention ‚Äì both in our training and infer-\nence phase ‚Äì to improve the model‚Äôs cross-lingual understanding.\nWe refer to our extended component as Mixed Attention Trans-\nformer (MAT) and create MART, a sandwich-like architecture to\nembed MAT into the multilingual BERT (mBERT) model. By encod-\ning the translation knowledge into an attention matrix, we enable\nthe overall architecture to focus on the mutually translated words\nin the input sequence. Our experiments explore the effectiveness of\na variety of external knowledge sources and show the significant\ngain that we get from MART on CLIR task. MAT is a generalized\narchitecture capable to capture any form of lexical mapping and it\ncan be integrated with any transformer-based architecture.\nWe performed extensive experiments on ten different language\npairs for CLIR training and evaluation, three different resources to\nobtain the translation knowledge, and different qualities of transla-\ntions based on available translation resources for language pairs.\nOur experimental results demonstrate the varied effectiveness of dif-\nferent external knowledge sources and the significant improvement\nof MAT-embedded neural re-ranking model over strong baselines\non the CLIR task. In terms of mean average precision (MAP), our\nproposed model outperforms the neural baseline by 8% on high-\nresource languages and 12% on low-resource languages.\nThe rest of this paper is organized as follows. In Section 2 we\nprovide a review of related works. Section 3 presents our MAT\narchitecture for injecting external translation knowledge directly\ninto model. Section 4 and 5 provides our experimental design and\nresults with discussions and further analyses. We conclude our\nstudy in Section 6.\n2 RELATED WORK\nWe first provide a summary of existing CLIR models trained from\nboth word-embedding based representations as well as represen-\ntations from unsupervised language models based on the trans-\nformer architecture. We discuss the importance of the knowledge\nfrom sentence-level parallel data and how they enhance the perfor-\nmance of neural retrieval models. Finally, we also elaborate on the\ntransformer-based architectures that incorporate external knowl-\nedge for a variety of tasks and compare them to MAT.\n2.1 Neural Cross-lingual Representation Spaces\nCLIR tackles two sub-tasks: query translation and query-document\nmatching, and neural models are applicable to both the tasks. One\napproach is to translate the query to the language of the corpus by\nusing a Statistical Machine Translation (SMT) or Neural Machine\nTranslation (NMT) model and then apply a mono-lingual matching\nmodel to determine the relevance. While the translate-then-retrieve\napproach is a popular one, neural bilingual word representations\ncreates the opportunity to skip the translation step. As a result,\nquery-document matching can performed in a shared vector space\nfor two languages, where similar words in two different languages\nare mapped close to each other. The assumption is Cross-linual\nWord Embeddings (CLWE) are capable to bridge the translation\ngap between two languages.\nOne of the earliest works in this direction is from Vuliƒá and\nMoens [42], and they proposed a model to learn bilingual word\nembeddings using a document-aligned comparable data. Once all\nthe words in both languages are represented in a shared space, they\ncomputed query and document representations using the composi-\ntional distributional semantics model and calculated their matching\nscore based on cosine similarity metric. Litschko et al. [22] used the\nsame matching technique but created the shared space using only\nmonolingual data in two languages. Bonab et al. [2] assessed the\neffectiveness of several bilingual word embeddings under cosine\nsimilarity-based scoring framework for retrieval and found that\nall the existing word embeddings lack the capacity to translate a\nsource language word into the target language word ‚Äì they refer to\nthis phenomenon as the translation gap . The authors showed that a\nbilingual word embedding brings similar pair of words in two lan-\nguages close together, but often keeps the words that are translation\nof each other far than expected. This is because cross-lingual word\nembeddings are learned from the contextual information around a\nword but not from the translation of that word. The authors pro-\nposed a smart shuffling approach to include translation knowledge\ninto word embeddings and created a state-of-the-art cross-lingual\nword embedding for retrieval. While it is clear that translation\nknowledge brings significance gain in retrieval, there is no study\non how to incorporate this knowledge in the modern transformer\nbased query-document matching frameworks.\nUnsupervised multilingual language models based on the trans-\nformer architecture (also referred to as multilingual transformers)\nbrought a major advancement over the cross-lingual word embed-\ndings. There are two major realization of such models: mBERT\n(Multilingual BERT) [12] and XLM-R (XLM RoBERTa) [8]. These\nmodels offer a shared representation space for a large number of\nlanguages and the representation of a token is contextualized based\non the other tokens in a sequence. Thus these approaches capture\nhigher-level semantics compared to CLWE and once fine-tuned,\nthey have been shown to be effective across a wide variety of tasks,\nincluding CLIR [23, 36, 47]. However, we assume that thetranslation\ngap still exists in the multilingual transformers and it is important\nto inject translation knowledge into such architectures.\n2.2 Neural Matching Models for CLIR\nWhether we use cross-lingual word embeddings or multilingual\ntransformers for representing query and documents, we need to\nprovide relevance knowledge to these models for effective matching.\nThus, we need to further train these language representation spaces\nusing with relevance judgments from human [2, 21, 38, 52].\nSasaki et al. [38] constructed a large-scale weakly supervised\nCLIR collection by using the first sentence of a Wikipedia page as\nthe query and all the linked foreign language articles as documents.\nThey proposed a shallow learing-to-rank method and did not use a\nshared language representation space. Thus, their approach does\nnot explicitly close the language gap between the query and docu-\nment. Zhao et al. [52] leverages the sentence-aligned parallel data to\ncreate weakly supervised relevance judgments. They use a sentence\nfrom a language as a document and randomly select a word from\nthe translation of that sentence as query. Even though they close\nthe language gap using parallel data, they do not use relevance\njudgments explicitly. We use both parallel data and relevance judg-\nments and improve the architecture of a multilingual transformer\nto adapt these sources.\nRather than considering relevance and translation in isolation,\nLi and Cheng [21] took an adversarial learning approach to jointly\nlearn language alignment through translation knowledge and cross-\nlingual matching using relevance judgments. They created a weakly-\nsupervised collection of parallel data by translating AOL queries\nusing Google Translate. They use a Long Short Term Memory\n(LSTM) network to learn matching in contrast to the multilingual\ntransformer proposed in this work. Moreover, they use weak par-\nallel data to close the language gap, whereas we use word-level\nalignment learned from the parallel data or obtained from a dic-\ntionary in the fine-tuning stage. Bonab et al . [2] achieved state-\nof-the-art performance when they used their translation-oriented\nbilingual representations with DRMM matching model [ 13] and\ntrained the architecture using relevance data. They showed that\ndictionary-oriented word embeddings can improve the performance\nof a DRMM model when fine-tuned with relevance data. We pro-\npose a novel multilingual transformer architecture, MAT, which\nlearns jointly from relevance judgments and translation knowledge\nin the form of a dictionary or a translation table.\n2.3 Knowledge Injection into Transformers\nThere has been a number of efforts to inject structured world knowl-\nedge into unsupervised pretraining and contextualized representa-\ntions [14, 19, 20, 32, 44, 51]. Most of these works focus on integrating\nknowledge-graphs information such as type of an entity or relat-\nedness between a pair of entities. Lauscher et al. [19] incorporated\nlexical semantics into BERT by injecting word pairs that are syn-\nonyms or hold hyponym-hypernym relations in WordNet. Levine\net al. [20] injected word-supersense knowledge by predicting the\nsupersense of a masked word in the input and the ground truth\nis obtained from Wikipedia. All these works augment an extra\nknowledge-driven loss with the standard language modeling loss\nin the language model pre-training stage. We augment translation\nknowledge in the form of attention in the fine-tuning stage. Our\napproach is flexible as we can adapt new knowledge as more data\nfor fine-tuning becomes available.\nA recent work from Xia et al. [43] used attention-based approach\nto integrate lexical knowledge for the semantic textual matching\ntask. They created an attention matrix from WordNet and com-\nputed the Hadamard product of the attention matrix with BERT‚Äôs\nattention matrix. They investigated this approach for computing\nsentence similarity in a monolingual setting.\n3 METHODOLOGY\nOur goal is to incorporate additional knowledge from statistical\nmachine translation models or human-constructed dictionaries into\na transformer architecture to enable it to connect query and doc-\nument tokens ‚Äì not only based on relevance, but also based on\ntranslations. In this section, we first define the translation attention\nmatrix given an input query and a candidate document. Then we\nintroduce the translation attention head and the Mixed Attention\nTransformer (MAT) layer. Finally, we design a sandwich-like archi-\ntecture to embed MAT into the existing transformer-based neural\nranking model.\n3.1 Translation Attention Matrix\nWe define translation reference as a large structural dataset con-\ntaining knowledge to translate words from one language to another\ne.g, a human-constructed dictionary, or a translation table trained\non parallel corpora. In the CLIR task, the translation knowledge is\ndependent on the query and document. Therefore, we first design\nan algorithm that distill the translation knowledge based on tokens\nin the query and document.\nSuppose there exists a word-level translation reference ùëá. Given\nword ùë§ùë† in the source language and ùë§ùë° in the target language,\nùëá(ùë§ùë°,ùë§ùë† )returns the probability of ùë§ùë† being translated to ùë§ùë° :\nùëá(ùë§ùë°,ùë§ùë† )= ùëÉ(ùë§ùë° |ùë§ùë†,ùëá).\nWe assume the query is in the source language with length ofùëöùëû\nwords and the document is in the target language with length ofùëöùëë\nwords. Therefore, the concatenation of query and document [ùëû,ùëë]\nhas length ùëö = ùëöùëû +ùëöùëë . Then we construct a ùëö√óùëötranslation\nattention matrix ùëÄùë°ùëü based on [ùëû,ùëë]and ùëá(¬∑,¬∑)by symmetrically\nassigning translation probabilities between query tokens and doc-\nument tokens. We provide detailed instructions for constructing\nùëÄùë°ùëü in Algorithm 1.\nNote that the ùëòth row of ùëÄùë°ùëü represents the attention weights of\nùëòth token in the input assigned across all the input tokens. In Algo-\nrithm 1, lines 2-4 guarantee each token, including out-of-vocabulary\nword, is assigned a weight to itself and the self weight is the upper\nbound of all of its translation probabilities. Ifùëûùëñ and ùëëùëó are mutually\ntranslated words, they get their translation probabilities to each\nother from lines 5-9. Finally, the row normalization ensures that\nthe attention weights for each input token sum up to 1.\nTo encode rare words with limited vocabulary size, Byte Pair\nEncoding (BPE) is often used by pre-trained language models,\nwhich splits words into sub-word units. There is evidence that\nself-attention treats split words differently than non-split ones [10].\nTherefore, we use tokens before BPE to query the translation refer-\nence and then assign the same attention weight to all parts of the\nAlgorithm 1: Generate translation attention matrix\nInput: [ùëû,ùëë]and ùëá(¬∑,¬∑)\nOutput: ùëÄùë°ùëü\n1 Initialize ùëÄùë°ùëü as a ùëö√óùëözero matrix.\n2 for each token ùë§ùëò in the input sequence do\n3 ùëÄùë°ùëü\nùëòùëò = 1\n4 end\n5 for each query token ùë§ùëñ do\n6 for each document token ùë§ùëó do\n7 ùëÄùë°ùëü\nùëñ ùëó = ùëÄùë°ùëü\nùëóùëñ = ùëá(ùë§ùëó,ùë§ùëñ )\n8 end\n9 end\n10 ùëÄùë°ùëü ‚ÜêRowNorm(ùëÄùë°ùëü )\nreturn: ùëÄùë°ùëü\nFigure 1: A toy example for generating ùëÄùë°ùëü .\nsame word. The dimension ùëöof ùëÄùë°ùëü is the same as the length of\nsequence of [ùëû,ùëë]tokenized by a pre-trained language model. A\nsimplified example for generating ùëÄùë°ùëü with query ‚Äúcat‚Äù and docu-\nment ‚Äúkatze‚Äù (German translation of cat) is shown in Figure 1.\n3.2 Mixed Attention Transformer\nIn order to inject ùëÄùë°ùëü into a transformer-based model, we propose\na novel transformer network, named Mixed Attention Transformer\n(MAT) by combining the multi-head attention with translation-\nbased attention.\nThe multi-head attention [ 41] is the core of the transformer\narchitecture which consists ofùëõdifferent attention heads. Given the\nvector representations as the hidden states h, each head computes\nthe dot-product attention:\nAttentionùëñ (h)= softmax\n\u0010ùëäùëû\nùëñ h ¬∑ùëäùëò\nùëñ h\n‚àöÔ∏Å\nùëë/ùëõ\n\u0011\nùëäùúà\nùëñ h\nwhere h is a ùëë dimensional hidden vector for an input sequence. In\nBERT, the ùëäùëû\nùëñ , ùëäùëò\nùëñ and ùëäùúà\nùëñ are matrices with size ùëë/ùëõ√óùëë. Thus,\neach head projects to a different subspace of size ùëë/ùëõ, learning\ndifferent information.\nThen the outputs of the multi-head attention, MH(¬∑), are con-\ncatenated ùëõheads together and linearly transformed:\nMH(h)= ùëäùëú [Attention1,..., Attentionùëõ]\nIn parallel to multi-head attention, we introduce the translation\nattention head denoted asTH(¬∑). Inspired by the scaled dot-product\nattention, we replace the attention weights learned from matrices\nùëäùëû\nùëñ and ùëäùëò\nùëñ by the fixed attention weights in ùëÄùë°ùëü . Then, the multi-\nhead attention becomes a single fixed attention head as follows\nTH(h)= ùëäùëú\nTH\n\u0000ùëÄùë°ùëü (ùëäùúà\nTHh)\u0001,\nwhere both ùëäùëú\nTH and ùëäùúà\nTH are trainable matrices in TH(¬∑)with\ndimension ùëë√óùëë. By matrix multiplying with ùëÄùë°ùëü , the translation\nattention head is capable to reduce the distance between mutually\ntranslated tokens in the token representation hyperspace. We prove\nthe effect of ùëÄùë°ùëü on hidden states in a simplified scenario.\nLemma 1. Let convex combinations of vectors A and B beùõºùê¥+ùõΩùêµ\nand ùõΩùê¥+ùõºùêµ where ùõº+ùõΩ = 1. Then, the cosine similarity between\nùõºùê¥ +ùõΩùêµ and ùõΩùê¥+ùõºùêµ is greater or equal to the cosine similarity\nbetween ùê¥and ùêµ.\nProof.\nùëÜùëñùëö(ùõºùê¥+ùõΩùêµ,ùõΩùê¥ +ùõºùêµ)= (ùõºùê¥+ùõΩùêµ)¬∑( ùõΩùê¥+ùõºùêµ)\n‚à•ùõºùê¥+ùõΩùêµ‚à•‚à•ùõΩùê¥+ùõºùêµ‚à•\n‚â• (ùõº2 +ùõΩ2)ùê¥¬∑ùêµ+ùõºùõΩ(‚à•ùê¥‚à•2 +‚à•ùêµ‚à•2)\n(ùõº2 +ùõΩ2)‚à•ùê¥‚à•‚à•ùêµ‚à•+ùõºùõΩ(‚à•ùê¥‚à•2 +‚à•ùêµ‚à•2)\n‚â• ùê¥¬∑ùêµ\n‚à•ùê¥‚à•‚à•ùêµ‚à•.\nTherefore, ùëÜùëñùëö(ùõºùê¥+ùõΩùêµ,ùõΩùê¥ +ùõºùêµ)‚â• ùëÜùëñùëö(ùê¥,ùêµ).\nSuppose query word ùë§ùëñ and document word ùë§ùëó are the transla-\ntions of each other with probabilityùëù > 0, and words other thanùë§ùëó\nin documents all have zero translation probability with ùë§ùëñ . Then,\nthe only two non-zero weights in the ùëñth row of ùëÄùë°ùëü are self atten-\ntion (ùëÄùë°ùëü\nùëñùëñ ) and attention on ùë§ùëó (ùëÄùë°ùëü\nùëñ ùëó):\nùëÄùë°ùëü\nùëñùëñ = 1\n(1 +ùëù); ùëÄùë°ùëü\nùëñ ùëó = ùëù\n(1 +ùëù)\nSimilarly for ùë§ùëó , the non-zero weights in the ùëóth row are ùëÄùë°ùëü\nùëó ùëó =\n1/(1 +ùëù)and ùëÄùë°ùëü\nùëóùëñ = ùëù/(1 +ùëù). If we ignore the trainable matrices\nin TH(¬∑)and directly multiply ùëÄùë°ùëü with hidden states h, the trans-\nlation attention output of ùë§ùëñ and ùë§ùëó are a convex combination of\neach other‚Äôs hidden representations:\nTH(hùë§ùëñ )= 1\n1 +ùëùhùë§ùëñ + ùëù\n1 +ùëùhùë§ùëó\nTH(hùë§ùëó )= 1\n1 +ùëùhùë§ùëó + ùëù\n1 +ùëùhùë§ùëñ\nAccording to Lemma 1, because ùëù > 0,\nùëÜùëñùëö\u0000TH(hùë§ùëñ ),TH(hùë§ùëó )\u0001 > ùëÜùëñùëö(hùë§ùëñ ,hùë§ùëó )\nThus, when ùëù is large, the words in query and document are likely\nto be translation to each other. The attention matrix ùëÄùë°ùëü ‚Äúpays\nattention‚Äù to all these pair of words and TH(¬∑)tends to ‚Äúpull‚Äù their\nhidden representations closer in the hyperspace.\nThe complete attention mechanism in MAT is a combination of\nthe attention outputs from bothMH(¬∑)and TH(¬∑). We first employ a\nresidual connection around each type of attention output, followed\nby layer normalization, denoted as LN(¬∑), resulting two sub-layer\noutputs. Then we sum two sub-layer outputs:\nSublayerMH (h)= LN(h +MH(h))\nSublayerTH (h)= LN(h +TH(h))\nh‚Ä≤= SublayerMH (h)+SublayerTH (h)\nAnd apply the summed result to the position-wise feed-forward\nnetworks (FFN),\nFFN(ùë•)= max(0,ùë•ùëä1 +ùëè1)ùëä2 +ùëè2\nFigure 2: (left) Multi-Head Attention. (right) Translation At-\ntention Head. (middle) Mixed Attention Transformer Layer.\nThe final output of MAT is another residual connection around the\noutput of FFN:\nMAT(h)= LN(h‚Ä≤+FFN(h‚Ä≤))\nThe complete MAT architecture is depicted in Figure 2 (middle). The\nleft and right of Figure 2 are two types of attention component in\nMAT. The benefits of this network architecture are that the MAT can\nattend to both contextual information from multi-head attention\nand translation knowledge from translation attention head during\ntraining. Because we keep the multi-head attention mechanism\nand share the FFN sublayer, MAT contains a vanilla transformer\nnetwork. This design allows MAT to be easily embedded into recent\ntransformer-based pre-trained models and fully leverage the pre-\ntrained weights.\n3.3 Embed MAT into Pre-trained Model\nThe transformer-based models usually have the following architec-\nture: First, the embedding layer encodes the input tokens, segments,\nand positions into hidden representations. The representation of\neach input token is then updated by a stack of encoder layers based\non the attention mechanism. Finally, a specialized add-on network\nmaps the hidden representations to an output based on the task.\nQiao et al. [34] analyzed different ranking models based on BERT\nand found that the Last-Int approach which applies BERT on the\nconcatenated [ùëû,ùëë]sequence and uses the last layer‚Äôs represen-\ntation of the [CLS] token as the matching feature gives the best\nperformance. In this section, we use the same BERT (Last-Int) as\na re-ranker to discuss how to embed MAT into a transformer-based\npre-trained language model.\nMART (MAT+BERT), the new model architecture we propose is\nto keep the embedding layer and add-on network while replacing\nsome of the transformer layers in the middle by MAT.\nDuring fine-tuning, the BERT layers close to the output (higher\nlayers) are more sensitive than the lower layers [53]. Also, another\nstudy on BERT [40] has shown that most local syntactic phenomena\nare encoded in lower layers while higher layers capture more com-\nplex semantics. Consider the fine-tuning efficiency and semantic\nquality of the token representations, the layer replacement should\nstart from the higher layers of BERT. Moreover, in the Last-Int\nranking approach, the output score is only based on the [CLS] token\nin the last BERT layer. Therefore, we keep the last BERT ( Base)\nlayer as the output layer and start to embed MAT from the 11 th\nFigure 3: Use MAT layers in BERT ranking model\nlayer. Figure 3 shows an examples of the sandwich-like architecture\nbased on a BERT-based ranking model where MAT layers are em-\nbedded into 10th and 11th layers of BERT. Using the same hidden\ndimension as BERT, each MAT layer only introduces about 1.18M\nnew parameters comparing to the BERT layer. At initialization,\nMAT is able to use pre-trained weights of its corresponding BERT\nlayer. This compatibility increases the fine-tuning efficiency and\nreduces the training data requirement.\n4 EXPERIMENTAL SETUP\n4.1 Dataset\nCLIR dataset. We create our training and evaluation data from the\nCross-Language Evaluation Forum (CLEF) 2000-2008 campaign for\nbilingual ad-hoc retrieval tracks [3‚Äì6, 27‚Äì31]. We use the text fields\nof the documents to construct our retrieval corpus and discard other\nmeta data. We concatenate the title and description fields of a topic\nand consider it as our query. We consider all the topics and relevance\njudgments from all the tracks to show the consistent effectiveness\nof MAT across several cross-language retrieval settings on both\nhigh- and low-resource languages.\nTranslation Resources. Our goal is to leverage translation re-\nsources as external knowledge into the query-document match-\ning process and we compare the effectiveness of three types of\nresources: sentence-level parallel data, dictionary, and bi-lingual\nword embeddings. We use sentence-level parallel data with GIZA++\ntoolkit [26] to construct a translation table, which we use to gener-\nate ùëÄùë°ùëü . Translation tables for European languages are based on the\nEuroparl v7 sentence-aligned corpora [18]. For our limited-resource\n(both in terms of parallel data and relevance judgments) setting\nbased on Somali and Swahili languages, we use the translation\ntables provided by Zhang et al. [50].\nAs the dictionary-based translation resource we use Panlex, a\ndictionary [16] whose data acquisition strategy emphasizes high-\nquality lexical mapping and broad language coverage. Finally, we\nalso explore the a multilingual word embedding as a translation re-\nsource following Bonab et al. [2]. Given a pair of words we use their\nrepresentations from a multilingual word embedding model and\ncompute cosine similarity to model relatedness of the pair of words.\nIn our experiments, we use MUSE, an unsupervised multilingual\nword embedding from [9] as translation resource.\nText Pre-processing. In order to have consistent pieces of text\nacross different resources, we normalize characters by mapping\nTable 1: Summary of CLIR setting. First four rows indicate\nthe backward and the last row indicates the forward setting.\nCLIR Setting Collection Source Collection Size Query Size\nEng-Fre Le Monde, Sda French 129,689 185\nEng-Ita La Stampa, Sda Italian 144,040 176\nEng-Deu Der Spiegel, Frankfurter Rundschau 153,496 184\nEng-Spa EFE News 94-95 452,027 156\nXxx-Eng Los Angeles Times 94 113,005 246\ndiacritic characters to the corresponding unmarked characters and\nthen lower-casing text. For initial step of retrieval and translation\ntable extraction from parallel corpora, we remove non-alphabetic,\nnon-printable, and punctuation characters. We use NLTK library\nto tokenize and remove stop-words, but do not stem the tokens.\n4.2 CLIR Settings\nForward: Non-English Query and English Documents. In this\nsetting, we use non-English queries against an English document\ncollection. To evaluate cross-lingual matching performance, we use\nhuman translation of a fixed query set to obtain queries in different\nlanguages. While we have translations of queries in different lan-\nguages, we keep the content and language of the retrieval corpus\nfixed. We have both high-resource and low-resource CLIR settings\nin our experiments. In a high-resource setting, for example, French-\nEnglish, we have higher amount of sentence-level parallel data and\nrelevance judgments compared to a low-resource setting.\nThere are four high-resource language pairs in our experiments:\nFrench (Fre-Eng), Italian (Ita-Eng), German (Deu-Eng), and Span-\nish (Spa-Eng). Queries are selected from CLEF C001 ‚Äì C350 topic\nset for each language. We take the intersection of the topic ID and\nremove topics without any relevant document, resulting in 246\noverlapped queries across four languages. For cross-language infor-\nmation retrieval involving low-resource languages, we experiment\non Somali (Som-Eng) and Swahili (Swa-Eng). Bonab et al. [1] pro-\nvided Somali and Swahili translations of 151 English queries from\nthe CLEF C001 ‚Äì C200 topic set and we use those queries in our\nsetting. The collection of English documents is the Los Angeles\nTimes corpus comprised of 113k news articles.\nBackward: English Query and Non-English Documents In\nthis setting, we use English queries against document collections\nin four languages: French (Eng-Fre), Italian (Eng-Ita), German (Eng-\nDeu) and Spanish (Eng-Spa). For each language, we create a retrieval\ncorpus from a combination of sources which we report in Table 1.As\nthe retrieval corpus varies for each language, relevance judgments\nare not available for all the English topics from CLEF C001 ‚Äì C350\ntopic set. Thus, for each CLIR setting we have a different number\nof queries in the backward setting compared to the forward set-\nting. Table 1 provides information about query sets and document\ncollections in both the settings.\n4.3 Implementation Details\nPre-trained passage re-ranker Nogueira and Cho[25] fine-tuned\nthe Base, Uncased multilingual BERT (mBERT) on MS MARCO\ndocument retrieval dataset to create a passage ranking model. We\nrefer to this pre-trained model as m2BERT and further fine-tune\nit with cross-lingual relevance judgments. To prepare the input\nsequence for m 2BERT we concatenate a query and a document\nseparated by a special [SEP] token from mBERT‚Äôs vocabulary. We\nprefix the concatenated sequence with the special [CLS] token from\nmBERT‚Äôs vocabulary. We obtain the last layer representation of\nthis sequence from m 2BERT, but only use the representation of\nthe [CLS] token, and pass it through a linear combination layer to\nobtain the probability of the document being relevant to the query.\nAt test time, given a query, m2BERT computes the probability for\neach document independently and obtains a document ranking\nafter sorting with these probability scores. Because the mBERT\ninput sequence is limited to 512 tokens, longer documents are split\nevenly and [CLS] representations from all document segments are\naveraged to obtain a representation for fine-tuning. MacAvaney\net al. [24] used the same approach for monolingual retrieval.\nEvaluation. For evaluating retrieval effectiveness, we follow\nprior work on CLEF dataset [2, 23] and report mean average pre-\ncision (MAP) of the top 100 ranked documents and precision of\nthe top 10 retrieved documents (P@10). We determine statistical\nsignificance using the two-tailed paired t-test with p-value less\nthan 0.05 (i.e., 95% confidence level).\nModel training. We train all neural re-ranking models using\npairwise cross-entropy loss [11]. We use all the positive document\nfrom the query relevance judgments and randomly sample negative\ndocuments to form training pairs. We truncate document contents\nto the first 800 tokens and create two passages to represent a docu-\nment if the sum of the query length and document length is over\nthe 512 tokens, which is the limit of mBERT. We pass a two query-\ndocument pairs in each forward pass but use gradient accumulation\nto make our effective batch size to 16. We train all the models for 100\nepochs with an early stopping strategy with patience value of 20.\nAll models are trained using Adam‚Äôs optimization algorithm [17]\nwith a learning rate of 2e-5.\nGiven the limited number of queries in each language, we use\n5-fold cross-validation for robust evaluation. For each fold, the\ntraining, validation, and test data are 60%, 20%, and 20% of the query\nset, respectively. The reported evaluation metrics are averaged\nacross 5 folds. We also fix the random seed is set to guarantee\nthat all models receive the same training data. For the validation\nqueries, we re-rank the top 100 documents and use MAP to select\nthe best-performing model.\n4.4 Compared Methods.\nWe compare the proposed model with the methods in the following\n‚Ä¢SMT: We first use the GIZA++ toolkit [26] to build translation\ntables from parallel corpora. We select top-10 translations from\nthe translation table for each query term and apply Galago 1‚Äôs\nweighted #combine operator to form a translated query. Then\nwe use the Galago‚Äôs implementation of Okapi BM25 [35] with\ndefault parameters. This setting is taken from Bonab et al . [2],\nand we call this method statistical machine translation (SMT).\nIt serves as one of our baselines. Moreover, the training data\nfor neural re-ranking models are sampled based on the top 500\nretrieved documents by the SMT model.\n‚Ä¢m2BERT: To create the m 2BERT baseline we begin with the\npre-trained checkpoint provided by [ 25]. This checkpoint is a\n1https://www.lemurproject.org/galago.php/\nTable 2: Model performance on forward and backward settings for high-resource languages. The highest value for each column\nis marked with bold text. Statistically significant improvements are marked by ‚Ä†(over SMT) and ‚Ä°(over BERT).\nForward\nSetting\nModel Fre-Eng Ita-Eng Deu-Eng Spa-Eng\nMAP P@10 MAP P@10 MAP P@10 MAP P@10\nHuman Translation 0.4569 0 .3940 0 .4569 0 .3940 0 .4569 0 .3940 0 .4569 0 .3940\nSMT 0.3618 0 .3492 0 .3561 0 .3431 0 .3588 0 .3354 0 .3624 0 .3317\nm2BERT 0.3802‚Ä† 0.3799‚Ä† 0.3652 0 .3545 0 .3582 0 .3335 0 .3819‚Ä† 0.3693‚Ä†\nMART-PLB 0.3859‚Ä† 0.3666‚Ä† 0.3701 0 .3689‚Ä† 0.3593 0 .3501‚Ä† 0.3824‚Ä† 0.3676‚Ä†\nMART 0.4126‚Ä†‚Ä° 0.3935‚Ä†‚Ä° 0.3944‚Ä†‚Ä° 0.3732‚Ä†‚Ä° 0.3862‚Ä†‚Ä° 0.3770‚Ä†‚Ä° 0.3953‚Ä†‚Ä° 0.3830‚Ä†‚Ä°\nBackward\nSetting\nModel Eng-Fre Eng-Ita Eng-Deu Eng-Spa\nMAP P@10 MAP P@10 MAP P@10 MAP P@10\nHuman Translation 0.2955 0 .3054 0 .2629 0 .2892 0 .2970 0 .3060 0 .2518 0 .2436\nSMT 0.2258 0 .2319 0 .1883 0 .1852 0 .2614 0 .2424 0 .1985 0 .2088\nm2BERT 0.2841‚Ä† 0.2875‚Ä† 0.2635‚Ä† 0.2605‚Ä† 0.3241‚Ä† 0.3246‚Ä† 0.2355‚Ä† 0.2285‚Ä†\nMART-PLB 0.2807‚Ä† 0.2823‚Ä† 0.2713‚Ä† 0.2771‚Ä† 0.3262‚Ä† 0.3230‚Ä† 0.2389‚Ä† 0.2351‚Ä†\nMART 0.3002‚Ä†‚Ä° 0.3108‚Ä†‚Ä° 0.2823‚Ä†‚Ä° 0.2846‚Ä†‚Ä° 0.3433‚Ä†‚Ä° 0.3414‚Ä†‚Ä° 0.2558‚Ä†‚Ä° 0.2439‚Ä†‚Ä°\nresult of fine-tuning the multilingual BERT (mBERT) architecture\nwith MSMARCO passage ranking dataset. We further fine-tune it\nwith training data from a specific CLIR setting. We use the same\nfine-tuning approach described in section 4.3 for this baseline\nand our proposed model to ensure fair comparison.\n‚Ä¢MART-PLB: This is a variant of MART. In order to evaluate\nthe effect of external knowledge on MAT, we replaceùëÄùë°ùëü by an\nidentity matrix so that each token is only paying attention to\nitself. Therefore, instead of injecting translation knowledge into\nthe model, we design a ‚Äúplacebo‚Äù attention matrix for MAT. Using\nMART-PLB as a controlled experiment, we are able to evaluate\nthe effect of external knowledge.\nIn order to provide an empirical upper-bound on retrieval per-\nformance, we use human translation of the queries and apply BM25\nas the retrieval technique. The human translations of the queries\nare obtained from the CLEF dataset as they have a common topic\nID for the same queries across different languages.\n5 EXPERIMENTAL RESULTS\n5.1 Performance on High-resource Languages\nTable 2 lists evaluation results on both Forward (top) and Backward\n(bottom) settings for language pairs with high translation resources.\nAs a neural re-ranker, m2BERT significantly improves upon SMT\non all language pairs in backward setting and two language pairs\non the forward setting while performs on par with SMT for Deu-\nEng and Ita-Eng languages. While fine-tuned on English document\nretrieval dataset, m2BERT can transfer to cross-lingual task with\nsmall amount of fine-tuning data. This agrees with the previous\nfinding by Pires et al . [33] that mBERT is capable to generalize\nacross languages.\nWe observed substantial improvements on the retrieval perfor-\nmance when translation knowledge is incorporated into MART. For\nall language setting combination in Table 2, MART performs signifi-\ncantly better than the BERT architecture (m2BERT) in terms of both\nMAP and P@10. MART improves m2BERT by 8% on the forward\nand 7% on the backward settings in terms of MAP. This compre-\nhensive comparisons with vanilla BERT based ranker demonstrate\nthe effectiveness of the MAT-embedded model.\nReplacing ùëÄùë°ùëü by the identity matrix in MART-PLB, the trans-\nlation attention head degenerates to two additional feed-forward\nlayers. MART-PLB behaves insignificantly comparing to the vanilla\nBERT architecture on all languages. Such results indicate that the\nperformance gain in MART relies on injecting the external knowl-\nedge, not from adding new parameters. When ùëÄùë°ùëü becomes non-\ninformative, the translation attention head is ineffectual.\nComparing MART with Human Translation, we can see that\nin forward setting, correct translation with basic retrieval model\nstill lead the neural CLIR model. However, in backward setting,\nMART achieves relatively the same as (Eng-Fre, Eng-Spa) or better\nthan (Eng-Ita, Eng-Deu) Human Translation. We hypothesize that\nin the backward setting, translation tables provide higher quality\ntranslations which enable better semantic matching between query\nand document tokens.\n5.2 Performance on Low-resource Languages\nThe evaluation results for two language pairs with limited trans-\nlation resources on the forward setting are shown in Table 3. We\nmake several observations. First, m2BERT mostly under-performs\nSMT for both Somali and Swahili languages. Note that Somali is not\nincluded in the pre-training of mBERT. Even if Swahili is included,\nthere is only a small number of Swahili sentences in the pre-training\ndata. The low performance of m2BERT on low-resource language\npairs demonstrates that absence or inadequate pre-training data\non a particular language leads to poor performance on target tasks\ninvolving those languages.\nOn the other hand, the MART model achieves the highest MAP\nperformance for both Somali and Swahili languages. The consistent\nand significant improvements in terms of MAP over compared\nmethods make MART the best model in our experiments. Due to the\nlack of pre-training data, the translation gap is more critical in low-\nresource language pairs. The performance of MART for Somali and\nSwahili languages proves that leveraging the external translation\n(a) Deu-Eng\n (b) Swa-Eng\nFigure 4: The comparison of MART to m 2BERT on layer-wise token representations.\nTable 3: Model performance for low-resource languages\non Forward setting. The highest value for each column\nis marked with bold text. Statistically significant improve-\nments are marked by ‚Ä†(over SMT) and ‚Ä°(over m2BERT).\nModel Som-Eng Swa-Eng\nMAP P@10 MAP P@10\nHuman Translation 0.4563 0 .3940 0 .4563 0 .3940\nSMT 0.1948 0 .1865 0 .2184 0.2152\nm2BERT 0.1986 0 .1772 0 .2055 0 .2089\nMART-PLB 0.2049 0 .1972‚Ä†‚Ä° 0.2130 0 .2106\nMART 0.2207‚Ä†‚Ä° 0.2135‚Ä†‚Ä° 0.2348‚Ä†‚Ä° 0.2151\nknowledge can help to bridge the translation gap. Moreover, the\nexperiments with the placebo setting, similar to those for the high-\nresource languages, have shown no significance in performance\ncompared to m2BERT. These results strengthen the conclusion that\nthe translation attention matrix is the key component of MAT.\nHuman Translation leads neural ranking models by a large mar-\ngin in CLIR tasks involving low-resource languages. This is ex-\npected because, with less sentence-level parallel data, the CLIR\nmodels often suffer from low quality of translations.\n5.3 Representation Analysis\nTo study the influence of MAT on the translation gap in neural\nCLIR, we compare the token representation from each layer be-\ntween m2BERT and MART. Specifically, both models are fine-tuned\non Deu-Eng and Swa-Eng training data. Figure 4 shows the dis-\ntances between contextualized token representations in two model\narchitectures where x-axis represents layers from low to high and\ny-axis is the cosine similarity. We focus on two types of word pairs\n(one from query and another from document) in an input sequence:\n(i) Mutually translated words, where all pairs of words that are\ntranslations to each other according to the external translation\nknowledge are selected; and (ii) Random non-translated words,\nwhere we randomly sample 10 pairs of words which are not transla-\ntions of each other. We compute the average cosine similarity of the\ntoken representations at each layer for all selected word pairs in the\ntest data of Deu-Eng (high-resource) and Swa-Eng (low-resource).\nFrom the diagrams in Figure 4, we can see that in general, the\nsimilarity of token representations increases as the layer gets higher.\nTable 4: MART performance for different external knowl-\nedge. The highest value for each column is marked with bold\ntext. ‚Äú‚Äì‚Äù if language is not supported.\nExternal\nKnowledge\nForward Backward\nDeu-Eng Swa-Eng Eng-Deu\nMAP P@10 MAP P@10 MAP P@10\nParallel Corpus 0.3862 0 .3770 0 .2348 0 .2151 0.3433 0 .3414\nPanlex 0.3713 0 .3612 0 .2265 0 .2073 0.3326 0 .3360\nMUSE 0.3693 0 .3580 ‚Äì ‚Äì 0.3335 0 .3348\nAlso, the mutually translated words always have smaller cosine\ndistances than non-translated words. The closer lines between two\ntypes of word pairs in Swa-Eng prove that the translation gap is\nmore critical in resource-lean languages. We can also see that in\n10th and 11th layers, the similarity of two types of words in m2BERT\ndrops for both language pairs. According to the previous analy-\nsis [33], one hypothesis for such drop is that before fine-tuning\non MS MARCO dataset, mBERT was pre-trained on surrounding\ncontexts for language modeling, it needs more contextual infor-\nmation to correctly predict the missing words. Therefore, mBERT\nfavors text sequence pairs that are closer in their semantic meanings.\nSuch models trained on surrounding context are not as effective for\nad-hoc document ranking with respect to keyword queries [34].\nMART shows the same behavior as m 2BERT up to the MAT\nlayers. The representations of mutually translated words in MAT\nlayers become similar to each other in terms of cosine distance.\nThis matches the design purpose of MAT. Meanwhile, because MAT\nkeeps the native multi-head attention from BERT layer, the simi-\nlarity of non-translations still drops in MAT layers. The increased\nsimilarity on mutually translated words and decreased similarity\non non-translated words demonstrate that model is bridging trans-\nlation gap with the help of external knowledge.\n5.4 Effect of Translation Resources\nFrom the previous results, we have seen that the translation at-\ntention matrix is critical to the success of MAT. As a knowledge\ninjection model, it is palpable that the quality of the knowledge\naffects the model performance. In this experiment, we study the\neffect of different sources of external knowledge on the MART.\nBesides the translation table built from parallel data, we use two\n(a) Deu-Eng\n (b) Swa-Eng\n (c) Eng-Deu\nFigure 5: The performance comparison of different MART model architectures.\ndifferent translation knowledge for ùëÄùë°ùëü generation: Panlex dictio-\nnary [16] and multilingual word embedding (MUSE [9]). To obtain\ntranslation probability for a single word in Panlex, we uniformly\ndistribute weights to all possible translations. And in MUSE, we\nuse the 5 nearest neighbors of a word in the target languages as\nits potential translations and assign translation probability based\non their normalized cosine similarity. In order to cover different\nlanguages and retrieval settings, we select Deu-Eng (high-resource)\nand Swa-Eng (low-resource) from forward setting and Eng-Deu\nfrom backward setting for this experiment.\nTable 4 shows the results of all compared translation knowledge.\nWe observe a performance drop on both alternative knowledge\nresources. For Panlex, although the translations are more precise\nthan those in a translation table, they do not provide a broad cover-\nage of words. Multilingual word embeddings are learnt from the\ncontexts of words, not their translations. Therefore, given a word,\nthe embeddings of semantically similar words are often closer than\nthose of its translations to the embedding of a word [2]. Thus, using\nmultilingual word embeddings, the problem of the translation gap\nwill not be completely resolved.\n5.5 Ablation Study on Model Architecture\nIn this section, we empirically study the effects of different numbers\nand positions of MAT layers in a MART model. We further train\nand evaluate the MART with various combinations of MAT layers.\nIt is worth mentioning that given the number of layers in BERT\narchitecture, there exist exponential number of possible combina-\ntions. We only explore several representative models. Leaving the\nlast layer as the output layer, we still focus on the higher trans-\nformer layers of BERT architecture. For models with a single MAT\nlayer, we investigate MART with MAT embedded at 9th, 10th, or\n11th layer. For double MAT layers, we use the previous results from\nMAT at 10th and 11th layers. We also consider an architecture with\nthree MAT layers where 9th, 10th and 11th layers in BERT are all\nreplaced by the MAT layer.\nFigure 5 shows the performance of different MART model ar-\nchitectures on Deu-Eng, Swa-Eng and Eng-Deu. We can see that\nall model variants have the similar pattern across three selected\nCLIR tasks. Because higher BERT layers are more sensitive to fine-\ntuning [53] and their hidden representations capture complex se-\nmantic information [40], the retrieval performance for the single\nMAT layer increases from MAT at the 9th layer to MAT at the 11th\nlayer. The double MAT layer can further boost performance from\nthe single-layer approach. We also can see that models get less\nimproved when 9th in replaced by MAT. We hypothesize that the\ntoken representations after the 8th layer (the input of the 9th layer)\ndo not contain enough semantic information [40] so it is too early\nto apply the translation attentions.\n6 CONCLUSION\nIn this paper, we propose a novel Mixed Attention Transformer\n(MAT) network to leverage external translation knowledge for\ncross-lingual information retrieval tasks.\nFirst, we build attention matrix for mutually translated words be-\ntween query and document based on the translation resource. Then\nusing the attention matrix, we design a new translation attention\nhead and show that it is able to reduce the cosine distance between\nhidden representations of mutually translated words. Finally, the\ncomplete architecture of MAT is a combination of multi-head at-\ntention and translation attention head with shared feed-forward\nnetworks. As a layer component, we further design a sandwich-\nlike architecture to embed MAT into the Transformer model. Our\ncomprehensive experimental results demonstrate the effectiveness\nof external knowledge and the significant improvement of MAT-\nembedded neural model on CLIR task.\nFor future work, we are particularly interested in fine-tuning\nMART on a large CLIR dataset with a mix of cross-language set-\ntings to learn a language-agnostic neural ranking model. We also\nplan to apply MAT to other retrieval tasks, e.g., event retrieval, by\nincorporating information other than translation knowledge.\nACKNOWLEDGMENTS\nThis work was supported in part by the Center for Intelligent In-\nformation Retrieval, in part under USC (University of Southern\nCalifornia) subcontract no. 124338456 under IARPA prime contract\nno. 2019-19051600007., and in part by the Office of the Director\nof National Intelligence (ODNI), Intelligence Advanced Research\nProjects Activity (IARPA) via AFRL contact #FA8650-17-C-9116\nunder subcontract #94671240 from the University of Southern Cali-\nfornia. The views and conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily representing\nthe official policies or endorsements, either expressed or implied, of\nthe ODNI, IARPA, or the U.S. Government. The U.S. Government is\nauthorized to reproduce and distribute reprints for Governmental\npurposes notwithstanding any copyright annotation thereon.\nREFERENCES\n[1] Hamed Bonab, James Allan, and Ramesh Sitaraman. 2019. Simulating CLIR\nTranslation Resource Scarcity Using High-Resource Languages. In Proceedings of\nthe 2019 ACM SIGIR International Conference on Theory of Information Retrieval\n(Santa Clara, CA, USA) (ICTIR ‚Äô19) . Association for Computing Machinery, New\nYork, NY, USA, 129‚Äì136. https://doi.org/10.1145/3341981.3344236\n[2] Hamed Bonab, Sheikh Muhammad Sarwar, and James Allan. 2020. Training\nEffective Neural CLIR by Bridging the Translation Gap. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 9‚Äì18.\n[3] Martin Braschler. 2001. CLEF 2000 ‚Äî Overview of Results. In Cross-Language In-\nformation Retrieval and Evaluation , Carol Peters (Ed.). Springer Berlin Heidelberg,\nBerlin, Heidelberg, 89‚Äì101.\n[4] Martin Braschler. 2002. CLEF 2001 ‚Äî Overview of Results. In Evaluation of Cross-\nLanguage Information Retrieval Systems , Carol Peters, Martin Braschler, Julio\nGonzalo, and Michael Kluck (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg,\n9‚Äì26.\n[5] Martin Braschler. 2002. CLEF 2002‚ÄîOverview of results. In Workshop of the\nCross-Language Evaluation Forum for European Languages . Springer, 9‚Äì27.\n[6] Martin Braschler. 2003. CLEF 2003‚ÄìOverview of results. In Workshop of the\nCross-Language Evaluation Forum for European Languages . Springer, 44‚Äì63.\n[7] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzm√°n, √âdouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learn-\ning at Scale. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics . 8440‚Äì8451.\n[8] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning\nat Scale. In Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics . Association for Computational Linguistics, Online, 8440‚Äì8451.\nhttps://doi.org/10.18653/v1/2020.acl-main.747\n[9] Alexis Conneau, Guillaume Lample, Marc‚ÄôAurelio Ranzato, Ludovic Denoyer,\nand Herv√© J√©gou. 2017. Word translation without parallel data. arXiv preprint\narXiv:1710.04087 (2017).\n[10] Gon√ßalo M. Correia, Vlad Niculae, and Andr√© F. T. Martins. 2019. Adaptively\nSparse Transformers. In Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) . Association for Computational\nLinguistics, Hong Kong, China, 2174‚Äì2184. https://doi.org/10.18653/v1/D19-1223\n[11] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce\nCroft. 2017. Neural ranking models with weak supervision. In Proceedings of\nthe 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 65‚Äì74.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n4171‚Äì4186. https://doi.org/10.18653/v1/N19-1423\n[13] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance\nMatching Model for Ad-hoc Retrieval. Proceedings of the 25th ACM International\non Conference on Information and Knowledge Management (Oct 2016). https:\n//doi.org/10.1145/2983323.2983769\n[14] Bin He, Di Zhou, JingHui Xiao, X. Jiang, Qun Liu, Nicholas Jing Yuan, and T. Xu.\n2020. Integrating Graph Contextualized Knowledge into Pre-trained Language\nModels. ArXiv abs/1912.00147 (2020).\n[15] Zhuolin Jiang, Amro El-Jaroudi, William Hartmann, Damianos Karakos, and\nLingjun Zhao. 2020. Cross-lingual Information Retrieval with BERT. In Pro-\nceedings of the workshop on Cross-Language Search and Summarization of Text\nand Speech (CLSSTS2020) . European Language Resources Association, Marseille,\nFrance, 26‚Äì31. https://www.aclweb.org/anthology/2020.clssts-1.5\n[16] David Kamholz, Jonathan Pool, and Susan M Colowick. 2014. PanLex: Building a\nResource for Panlingual Lexical Translation.. In LREC. 3145‚Äì3150.\n[17] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\nmization. (2015).\n[18] Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine transla-\ntion. In MT summit , Vol. 5. Citeseer, 79‚Äì86.\n[19] Anne Lauscher, Ivan Vuli‚Äôc, E. Ponti, A. Korhonen, and Goran Glavavs. 2020. Spe-\ncializing Unsupervised Pretraining Models for Word-Level Semantic Similarity.\nIn COLING.\n[20] Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, S. Shalev-\nShwartz, A. Shashua, and Y. Shoham. 2020. SenseBERT: Driving Some Sense into\nBERT. ArXiv abs/1908.05646 (2020).\n[21] Bo Li and Ping Cheng. 2018. Learning neural representation for clir with adver-\nsarial framework. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing . 1861‚Äì1870.\n[22] Robert Litschko, Goran Glava≈°, Simone Paolo Ponzetto, and Ivan Vuliƒá. 2018.\nUnsupervised cross-lingual information retrieval using monolingual data only.\nIn The 41st International ACM SIGIR Conference on Research & Development in\nInformation Retrieval . 1253‚Äì1256.\n[23] Robert Litschko, Goran Glava≈°, Ivan Vulic, and Laura Dietz. 2019. Evaluating\nResource-Lean Cross-Lingual Embedding Models in Unsupervised Retrieval. In\nProceedings of the 42nd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (Paris, France) (SIGIR‚Äô19). Association for\nComputing Machinery, New York, NY, USA, 1109‚Äì1112. https://doi.org/10.1145/\n3331184.3331324\n[24] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR:\nContextualized embeddings for document ranking. In Proceedings of the 42nd\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 1101‚Äì1104.\n[25] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\narXiv preprint arXiv:1901.04085 (2019).\n[26] Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various\nstatistical alignment models. Computational linguistics 29, 1 (2003), 19‚Äì51.\n[27] Carol Peters. 2005. What Happened in CLEF 2004?. In Multilingual Information\nAccess for Text, Speech and Images , Carol Peters, Paul Clough, Julio Gonzalo,\nGareth J. F. Jones, Michael Kluck, and Bernardo Magnini (Eds.). Springer Berlin\nHeidelberg, Berlin, Heidelberg, 1‚Äì9.\n[28] Carol Peters. 2006. What Happened in CLEF 2005. In Accessing Multilingual\nInformation Repositories , Carol Peters, Fredric C. Gey, Julio Gonzalo, Henning\nM√ºller, Gareth J. F. Jones, Michael Kluck, Bernardo Magnini, and Maarten de Rijke\n(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 1‚Äì10.\n[29] Carol Peters. 2007. What Happened in CLEF 2006. In Evaluation of Multilingual\nand Multi-modal Information Retrieval , Carol Peters, Paul Clough, Fredric C.\nGey, Jussi Karlgren, Bernardo Magnini, Douglas W. Oard, Maarten de Rijke, and\nMaximilian Stempfhuber (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg,\n1‚Äì10.\n[30] Carol Peters. 2008. What Happened in CLEF 2007. In Advances in Multilingual\nand Multimodal Information Retrieval , Carol Peters, Valentin Jijkoun, Thomas\nMandl, Henning M√ºller, Douglas W. Oard, Anselmo Pe√±as, Vivien Petras, and\nDiana Santos (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 1‚Äì12.\n[31] Carol Peters. 2009. What Happened in CLEF 2008. In Evaluating Systems for\nMultilingual and Multimodal Information Access , Carol Peters, Thomas Deselaers,\nNicola Ferro, Julio Gonzalo, Gareth J. F. Jones, Mikko Kurimo, Thomas Mandl,\nAnselmo Pe√±as, and Vivien Petras (Eds.). Springer Berlin Heidelberg, Berlin,\nHeidelberg, 1‚Äì14.\n[32] Matthew E. Peters, Mark Neumann, IV RobertLLogan, Roy Schwartz, V. Joshi,\nSameer Singh, and Noah A. Smith. 2019. Knowledge Enhanced Contextual Word\nRepresentations. In EMNLP/IJCNLP.\n[33] Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How Multilingual is Multi-\nlingual BERT?. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence,\nItaly, 4996‚Äì5001. https://doi.org/10.18653/v1/P19-1493\n[34] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding\nthe Behaviors of BERT in Ranking. arXiv:1904.07531 [cs.IR]\n[35] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\nMike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995),\n109.\n[36] Shadi Saleh and Pavel Pecina. 2020. Document Translation vs. Query Trans-\nlation for Cross-Lingual Information Retrieval in the Medical Domain. In Pro-\nceedings of the 58th Annual Meeting of the Association for Computational Lin-\nguistics. Association for Computational Linguistics, Online, 6849‚Äì6860. https:\n//doi.org/10.18653/v1/2020.acl-main.613\n[37] Sheikh Muhammad Sarwar, Hamed Bonab, and James Allan. 2019. A Multi-Task\nArchitecture on Relevance-based Neural Query Translation. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Florence, Italy, 6339‚Äì6344. https://doi.org/10.\n18653/v1/P19-1639\n[38] Shota Sasaki, Shuo Sun, Shigehiko Schamoni, Kevin Duh, and Kentaro Inui. 2018.\nCross-lingual learning-to-rank with shared representations. In Proceedings of the\n2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 2 (Short Papers) . 458‚Äì463.\n[39] Shigehiko Schamoni, Felix Hieber, Artem Sokolov, and Stefan Riezler. 2014. Learn-\ning translational and knowledge-based similarities from relevance rankings for\ncross-language retrieval. In Proceedings of the 52nd Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 2: Short Papers) . 488‚Äì494.\n[40] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical\nNLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence,\nItaly, 4593‚Äì4601. https://doi.org/10.18653/v1/P19-1452\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762 (2017).\n[42] Ivan Vuliƒá and Marie-Francine Moens. 2015. Monolingual and cross-lingual in-\nformation retrieval models based on (bilingual) word embeddings. In Proceedings\nof the 38th international ACM SIGIR conference on research and development in\ninformation retrieval. 363‚Äì372.\n[43] Tingyu Xia, Yue Wang, Yuan Tian, and Yi Chang. 2021. Using Prior Knowledge\nto Guide BERT‚Äôs Attention in Semantic Textual Matching Tasks. (2021).\n[44] Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2020.\nPretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language\nModel. ArXiv abs/1912.09637 (2020).\n[45] Mahsa Yarmohammadi, Xutai Ma, Sorami Hisamoto, Muhammad Rahman, Yim-\ning Wang, Hainan Xu, Daniel Povey, Philipp Koehn, and Kevin Duh. 2019. Ro-\nbust Document Representations for Cross-Lingual Information Retrieval in Low-\nResource Settings. In Proceedings of Machine Translation Summit XVII Volume 1:\nResearch Track. European Association for Machine Translation, Dublin, Ireland,\n12‚Äì20. https://www.aclweb.org/anthology/W19-6602\n[46] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers\nfor Text Ranking: BERT and Beyond. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies: Tutorials . Association for Computational Linguistics,\nOnline, 1‚Äì4. https://www.aclweb.org/anthology/2021.naacl-tutorials.1\n[47] Puxuan Yu and James Allan. 2020. A Study of Neural Matching Models for Cross-\nLingual IR. Association for Computing Machinery, New York, NY, USA, 1637‚Äì1640.\nhttps://doi.org/10.1145/3397271.3401322\n[48] Puxuan Yu, Hongliang Fei, and Ping Li. 2021. Cross-Lingual Language Model\nPretraining for Retrieval. In Proceedings of the Web Conference 2021 (Ljubljana,\nSlovenia) (WWW ‚Äô21) . Association for Computing Machinery, New York, NY,\nUSA, 1029‚Äì1039. https://doi.org/10.1145/3442381.3449830\n[49] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An\nAnalysis of BERT in Document Ranking. In Proceedings of the 43rd International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(Virtual Event, China) (SIGIR ‚Äô20) . Association for Computing Machinery, New\nYork, NY, USA, 1941‚Äì1944. https://doi.org/10.1145/3397271.3401325\n[50] Le Zhang, Damianos Karakos, William Hartmann, Manaj Srivastava, Lee Tarlin,\nDavid Akodes, Sanjay Krishna Gouda, Numra Bathool, Lingjun Zhao, Zhuolin\nJiang, Richard Schwartz, and John Makhoul. 2020. The 2019 BBN Cross-lingual\nInformation Retrieval System. In Proceedings of the workshop on Cross-Language\nSearch and Summarization of Text and Speech (CLSSTS2020) . European Lan-\nguage Resources Association, Marseille, France, 44‚Äì51. https://www.aclweb.org/\nanthology/2020.clssts-1.8\n[51] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.\n2019. ERNIE: Enhanced Language Representation with Informative Entities.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Florence, Italy, 1441‚Äì1451.\nhttps://doi.org/10.18653/v1/P19-1139\n[52] Lingjun Zhao, Rabih Zbib, Zhuolin Jiang, Damianos Karakos, and Zhongqiang\nHuang. 2019. Weakly supervised attentional model for low resource ad-hoc\ncross-lingual information retrieval. In Proceedings of the 2nd Workshop on Deep\nLearning Approaches for Low-Resource NLP (DeepLo 2019) . 259‚Äì264.\n[53] Yiyun Zhao and Steven Bethard. 2020. How does BERT‚Äôs attention change when\nyou fine-tune? An analysis methodology and a case study in negation scope.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 4729‚Äì4747. https:\n//doi.org/10.18653/v1/2020.acl-main.429",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7687257528305054
    },
    {
      "name": "Transformer",
      "score": 0.6424863338470459
    },
    {
      "name": "Natural language processing",
      "score": 0.5384867191314697
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49301502108573914
    },
    {
      "name": "Artificial neural network",
      "score": 0.4615744352340698
    },
    {
      "name": "Word (group theory)",
      "score": 0.4510575532913208
    },
    {
      "name": "Information retrieval",
      "score": 0.3873485326766968
    },
    {
      "name": "Mathematics",
      "score": 0.10190358757972717
    },
    {
      "name": "Engineering",
      "score": 0.07600510120391846
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    }
  ],
  "cited_by": 26
}