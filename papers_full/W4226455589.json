{
  "title": "How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis",
  "url": "https://openalex.org/W4226455589",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097620437",
      "name": "Shaobo Li",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107872138",
      "name": "Xiaoguang Li",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2151842933",
      "name": "Lifeng Shang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A1995934682",
      "name": "Zhenhua Dong",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2097149160",
      "name": "Chengjie Sun",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107390885",
      "name": "Bingquan Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097306026",
      "name": "Zhenzhou Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (Sweden)",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2562979205",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W4312516176",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3209298086",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W3144194608",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4308067211",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W2949479686",
    "https://openalex.org/W2132917208",
    "https://openalex.org/W1593045043",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3207964656"
  ],
  "abstract": "Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs' ability to fill in the missing factual words in cloze-style prompts such as \"Dante was born in [MASK].\" However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1720 - 1732\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nHow Pre-trained Language Models Capture Factual Knowledge? A\nCausal-Inspired Analysis\nShaobo Li1∗, Xiaoguang Li2∗†, Lifeng Shang2, Zhenhua Dong2,\nChengjie Sun1†, Bingquan Liu1, Zhenzhou Ji1, Xin Jiang2 and Qun Liu2\n1Harbin Institute of Technology\n2Huawei Noah’s Ark Lab\nshli@insun.hit.edu.cn, {sunchengjie, liubq, jizhenzhou}@hit.edu.cn\n{lixiaoguang11, shang.lifeng, dongzhenhua, Jiang.Xin, qun.liu}@huawei.com\nAbstract\nRecently, there has been a trend to investigate\nthe factual knowledge captured by Pre-trained\nLanguage Models (PLMs). Many works show\nthe PLMs’ ability to ﬁll in the missing factual\nwords in cloze-style prompts such as “Dante\nwas born in [ MASK ].” However, it is still a\nmystery how PLMs generate the results cor-\nrectly: relying on effective clues or short-\ncut patterns? We try to answer this ques-\ntion by a causal-inspired analysis that quan-\ntitatively measures and evaluates the word-\nlevel patterns that PLMs depend on to gener-\nate the missing words. We check the words\nthat have three typical associations with the\nmissing words: knowledge-dependent, posi-\ntionally close , and highly co-occurred . Our\nanalysis shows: (1) PLMs generate the miss-\ning factual words more by the positionally\nclose and highly co-occurred words than the\nknowledge-dependent words; (2) the depen-\ndence on the knowledge-dependent words is\nmore effective than the positionally close and\nhighly co-occurred words. Accordingly, we\nconclude that the PLMs capture the factual\nknowledge ineffectively because of depending\non the inadequate associations.\n1 Introduction\nd Do Pre-trained Language Models (PLMs) capture\nfactual knowledge? LAMA benchmark (Petroni\net al., 2019) answers this question by quantita-\ntively measuring the factual knowledge captured\nin PLMs: query PLMs with cloze-style prompts\nsuch as “Dante was born in [MASK ]?” Filling in\nthe mask with the correct word “Florence” is con-\nsidered a successful capture of the corresponding\nfactual knowledge. The percentage of correct ﬁll-\nings over all the prompts can be used to estimate\nthe amount of factual knowledge captured. PLMs\n∗Authors contribute equally.\n†Corresponding authors: sunchengjie@hit.edu.cn, lixi-\naoguang11@huawei.com\nKnowledge-Dependent:\nColumbus born between 25 August and 31 October\n1451, died 20 May 1506 was an Italian explorer.\nPositionally Close:\nColumbus born between 25 August and 31 October\n1451, died 20 May 1506 was an Italian explorer.\nHighly Co-occurred:\nColumbus born between 25 August and 31 October\n1451, died 20 May 1506 was an Italian explorer.\nFigure 1: The associations we investigated. The un-\nderlined words are the missing words that need to be\ngenerated. The bold words, which hold speciﬁc asso-\nciations with the missing words, are considered as the\nword-level patterns that PLMs may use to generate the\nmissing words.\nshow a surprisingly strong ability to capture fac-\ntual knowledge in such probings (Jiang et al., 2020;\nShin et al., 2020; Zhong et al., 2021), which elicits\nfurther research on a more in-depth question (Cao\net al., 2021; Elazar et al., 2021a): How do PLMs\ncapture the factual knowledge? In this paper, we\ntry to answer this question with a two-fold analysis:\nResearch Question 1 Which association do\nPLMs depend on to capture factual knowledge?\nResearch Question 2 Is the association on which\nPLMs depend effective in capturing factual knowl-\nedge?\nWe use association to refer to the explicit associa-\ntion between the missing words and the remaining\nwords in the context. We deﬁne three typical asso-\nciations between words. Figure 1 illustrates these\nassociations in a mask-ﬁlling sample.\nDeﬁnition 1 Knowledge-Dependent (KD) : Ac-\ncording to a Knowledge Base (KB), the missing\nwords can be deterministically predicted when pro-\nviding the remaining words.\nDeﬁnition 2 Positionally Close (PC): The remain-\ning words are positionally close to the missing\nwords.\n1720\nColumbus born between 25 Aug-\nust and 31 October 1451, died \n[MASK]s was an Italian explorer.\n20 May 1506\nOriginal input \nPLM\n[MASK] born between 25 August \nand 31 October 1451, [MASK] \n[MASK]s was an Italian explorer.\n20 April 1506\nPLM\nIntervened input\nQuantify the differences in predictions\n(a) Dependence measure.\nPLM\nColumbus died in [MASK]s\n Columbus pass away in [MASK]s\nColumbus's life ended in [MASK]s\nDifferent prompts for the same fact\n20 May 1506 \n20 May 1506 \n21 April 1506 \nEvaluate the probing performance\nDependence\nPerformance\nCorrelation (b) Effectiveness measure.\nFigure 2: The overview of the proposed analysis framework. The dependence measure quantiﬁes how much the\nPLMs depend on each association to capture factual knowledge when per-training. The effectiveness measure\nevaluates whether the dependence on an association is good for the factual knowledge performance in probing.\nDeﬁnition 3 Highly Co-occurred (HC): The re-\nmaining words have a higher co-occurrence fre-\nquency with the missing words.\nQuestion 1 investigates how much PLMs depend\non a speciﬁc group of remaining words to predict\nthe missing words in pre-training samples. We\nselect the remaining words to be investigated ac-\ncording to their association with the missing words.\nWe propose a causal-inspired method to quantify\nthe word-level dependence in each sample. The\naverage dependence on the remaining words that\nhold the same association with the missing words\nover all the samples indicates how PLMs rely on\nthis association to predict the missing words. We\nrefer to this average dependence as dependence\non the association. The above analysis is named\ndependence measure.\nIn Question 2, we reveal the effectiveness of\ndependence by the correlation between the quanti-\nﬁed dependence on associations and the factual\nknowledge capturing performance. The perfor-\nmance is probed with additionally crafted cloze-\nstyle prompts(Elazar et al., 2021a). The more the\ndependence on an association positive correlates\nwith the probing performance, the more effective\nthis association is. We refer to the second analysis\nas effectiveness measure. According the experi-\nment results, we have the following observations:\nObservation 1 The PLMs depend more on the po-\nsitional close and highly co-occurred associations\nthan the knowledge-dependent association to cap-\nture factual knowledge.\nObservation 2 Depending on the knowledge-\ndependent association is more effective for factual\nknowledge capture than positional close and highly\nco-occurred associations.\nBy connecting the two observations, we can an-\nswer the question of “how PLMs capture factual\nknowledge” as: The PLMs are capturing factual\nknowledge ineffectively since the PLMs depend\nmore on the PC and HC association than the\nKD association.\nThe contribution of this paper can be summa-\nrized as follows: (1) We quantify the word-level\ndependence for mask ﬁlling with a causal-inspired\nmethod, revealing the word-level patterns that\nPLMs use to predict the missing words quantita-\ntively. (2) We compare the effectiveness of the\ndependence on different associations, which pro-\nvides direct insights for improving PLMs for fac-\ntual knowledge capture. (3) This paper introduces\ncausal theories into PLMs by formulating the effect\nmeasurement process in mask language modeling.\nIt paves the path to measure the causal effects be-\ntween entities or events described in natural lan-\nguage.\n2 Method\n2.1 Overview\nWe take a quick overview of our two-fold analy-\nsis with a running example in Figure 2. Figure 2a\nillustrates how to measure the dependence on the\nremaining words “Columbus” and “died” when pre-\ndicting the missing words “20 May 1506.” We let\nthe PLM generate the missing words based on the\noriginal input ﬁrst, then mask the remaining words\nin the input and let PLMs generate again. The\ndifference between these two predictions is quan-\ntiﬁed and used to measure the dependence. The\nremaining and missing words hold the knowledge-\ndependent association in this sample. We repeat\nthis measure on all the samples whose remaining\nand missing words have the KD association. Then\nthe dependence on the KD association can be esti-\nmated by the average of the quantiﬁed difference.\nFigure 2b measures the effectiveness of the de-\npendence on each association by calculating the\ncorrelation coefﬁcient between the dependence and\n1721\nAssociation Knowledge-Dependent Positionally Close Highly Co-occurred\nInput\nWt born between 25 August\nand 31 October 1451, Wt Wo\nwas an Italian explorer.\nColumbus born between 25\nAugust and 31 October 1451,\nWt Wo Wt an Italian explorer.\nWt born between 25 August\nand 31 October 1451, died Wo\nwas an Italian Wt.\nSCM\ncW\ntW\noW\ndo(Wt={MASK, MASK})\ndo(Wt={Columbus, died})\ndo(Wt={MASK, MASK})\ndo(Wt={died, was})\ncW\ntW\noW\ncW\ntW\noW\ndo(Wt={MASK, MASK})\ndo(Wt={Columbus, explorer})\nTable 1: To analyze the dependence of associations, we do interventions on treatment words to reveal their causal\neffects on the outcome words.\nthe probing performance. Following (Petroni et al.,\n2019; Elazar et al., 2021a), the probing perfor-\nmance is indicated by the prediction accuracy and\nconsistency when querying on the same fact with\ndifferent prompts. Since the dependence on asso-\nciations are quantiﬁed in the dependence measure,\nwe can calculate the correlation coefﬁcient between\nthe dependence and performance over all the sam-\nples. Their correlation measures whether the de-\npendence on an association is harmful or beneﬁcial\nto the performance, showing the effectiveness of\nthe dependence on an association quantitatively.\nSection Outline We organize the rest of this sec-\ntion as follows. In Section 2.2.1, we formalize how\nwe quantify the dependence with the causal effect\nestimation. Section 2.2.2 gives detail about how\nwe build the probing samples for different associa-\ntions. Section 2.3.1 introduces the metrics we used\nto indicate the performance of factual knowledge\ncapture. Section 2.3.2 describes the details about\nthe effectiveness measure of associations.\n2.2 Quantify the Dependence on Associations\n2.2.1 Causal Effect Estimation for PLMs\nTo study the causal effect of the different input\nwords, we build a Structured Causal Model (SCM)\nfor the missing words generation process and ap-\nply interventions on some input words to estimate\ntheir effect quantitatively. We consider the miss-\ning words as outcome words and the remaining\nwords that hold a certain association (e.g. position-\nally close) with the outcome words as treatment\nwords. Then, we can formally represent the word\ngeneration process with SCM, as the following\nstructural equations:\nwc = f(I),wt = PLM(wc)\nwo = PLM(wc,wt). (1)\nSeparate the words in a sentence into three\ngroups: treatment words Wt, outcome words Wo,\nand context words Wc (speciﬁed by wt, wo, and\nwc respectively). Equation 1 formulates the fol-\nlowing data generation process: (1) Sample a sen-\ntence from the natural text space I and get the con-\ntext words wc using function f. (2) Generate the\ntreatment words wt by the PLM based on wc only.\n(3) Generate outcome words wo based on both the\nwc and wt.\nTo obtain the quantitative causal effect of treat-\nment words Wt on outcome words Wo, we apply\nthe do-calculus do() (Pearl, 2009) on treatment\nwords Wt to introduce interventions for estimating\nthe causal effect. do() denotes the operation of\nforcibly setting the value ofWt. Then the causal ef-\nfect of Wt on Wo can be estimated by the Average\nTreatment Effect (ATE) (Rubin, 1974):\nE [P(Wo|do(Wt = ˆwt))]\n−E [P(Wo|do(Wt = wm))] . (2)\nAccordingly, we deﬁne ATE for PLMs as:\nτ =\n∑\nI\nPLM (do(Wt = ˆwt),wc) P(s)\n−\n∑\nI\nPLM (do(Wt = wm),wc) P(s),\n(3)\nwhere ˆwt is the ground truth of the treatment words\nWt (the original value of Wt without intervention).\nwm is the intervention value (several [MASK ]s) for\nWt, and we use it to simulate removing the ground-\ntruth value ˆwt from the input. P(s) denotes the\nprobability of selecting the sample sthat consists\nof wt, wo, and wc from I. PLM(·,·) denotes the\noutput of PLMs with certain input. Table 1 illus-\ntrates the interventions on the SCM for different\nassociations.\n1722\nThe raw output of PLMs is a probability distribu-\ntion over ﬁxed vocabulary. We transform the output\ninto reciprocal rank to quantify the differences:\nPLMk(wt,wc) =\n{ 1\nrank ˆwo\n, if rankˆwo ≤k\n0, otherwise\n.\n(4)\nˆwo is the ground-truth outcome words. rankˆwo is\nthe rank position of ˆwo according to the generation\nprobability of ˆwo output by PLM(wc,wt). We\nset k to 100 and use PLM100 to replace PLM in\nEquation 3 to calculate ATE. The ATE reﬂects the\neffect of Wt on the prediction of Wo, it can be\nregarded as a quantitative estimation of how much\nPLMs depend on Wt when generating Wo.\n2.2.2 Mark words by Associations\nWikipedia is a rich source of knowledge (Thom\net al., 2007; Hassanzadeh, 2021), and most of\nthe PLMs nowadays have been pre-trained on\nWikipedia (Devlin et al., 2019; Liu et al., 2019;\nLan et al., 2019), so we take Wikipedia sentences\nas pre-training samples to construct the probing\nsamples for dependence measure. We probe the\nmask-ﬁlling on these sentences to analyze what\nPLMs based on when capturing factual knowledge\nin pre-training.\nThe outcome we want to observe is the predic-\ntions of factual words in the sentences. In order to\nlocate the factual words, we align each sentence\nwith a triplet (subject, predicate, object) in the KB.\nThe words that correspond to the object serve as\noutcome words Wo for observation, and the remain-\ning words that hold an explicit association withWo\nare marked as treatment words Wt for intervention.\nFor different associations, the Wt is identiﬁed as:\n1. Knowledge-Dependent: all the remaining\nwords correspond to the predicate and object\nin the same triplet with the Wt.\n2. Positionally Close: the remaining words clos-\nest to Wo.\n3. Highly Co-occurred : the remaining words\nthat have higher Pointwise Mutual Informa-\ntion (PMI) (Church and Hanks, 1990) with\nWo. The PMI is calculated over all the\nWikipedia sentences using the following equa-\ntion:\nPMI(wi; ˆwo) =P(wi|ˆwo)\nP(wi) , (5)\nwhere ˆwo is a group of words (a span) and wi\nis a single word.\n4. We further deﬁne a Random (R) association,\nwhere the Wt are randomly selected remain-\ning words. It provides some empirical support\nfor how much the modiﬁcations in the context\naffect the mask-ﬁlling output.\nAccordingly, one sentence yields four probing\nsamples for the four associations, respectively. The\nfour probing samples share the same Wo but use\ndifferent words as Wt to show the dependence on\ndifferent associations when predicting the sameWo.\nWe preserve that the number of words in Wt is the\nsame among different associations. For example, if\nthere are two words used as Wt by the KD associa-\ntion, we select the top two closest words with Wo\nas Wt for PC, and the words have the top two PMI\nwith Wo for HC. We can obtain a set of probing\nsamples for each association. The sample sets for\ndifferent associations source from the same set of\nsentences and have the same size.\n2.3 Measure the Effectiveness of Associations\nThis section investigates which association can lead\nto better performance on factual knowledge cap-\nture. We ﬁrst deﬁne the metrics to evaluate the\nperformance, then we measure the effectiveness of\nan association by relating the dependence on this\nassociation with probing performance.\n2.3.1 Metrics for Factual Knowledge Probing\nSection 2.2 uses the original Wikipedia sentence\nas pre-training samples to quantify the dependence\nPLMs used to capture the corresponding fact in\npre-training. The performance of capturing the\ncorresponding fact is probed by having PLMs ﬁll\nmasks on crafted quires. We construct these queries\nby instantiating templates on triplets (Petroni et al.,\n2019). Ti(s) denotes the i-th query for the fact\ncorresponds to s. The accuracy mrr of capturing\nthis fact is obtained by averaging over all the pre-\ndictions obtained with different queries:\nmrr (s) = 1\nn\nn∑\ni\nPLMk(Ti(s)), (6)\nPLMk(Ti(s)) denotes the reciprocal rank of the\nground truth in the PLM’s output for queryTi(s),\nit is deﬁned in Equation 4.\nThe consistency of the capture is indicated by\nthe percentage of the pairs of queries that have the\n1723\nsame result (Elazar et al., 2021a):\ncon (s) =\n∑\ni̸=j 1PLM(Ti(s))=PLM(Tj(s))\nn(n−1) . (7)\nThere are n different queries on every fact, and\nwe can get\n(n\n2\n)\n= n(n−1) pairs of predictions in\ntotal. PLM(Ti(s)) denotes the top-1 output for the\nquery Ti(s). The value of 1PLM(Ti(s))=PLM(Tj(s))\nis an indicator function that takes the value 1 if\nthe PLMs returns identical at top-1 for Ti(s) and\nTj(s) and 0 otherwise. The PLMs are better on\nthe consistency metric if they keep the predictions\nconsistent when queries only vary on the surface\nforms. E.g., the two queries “Dante was born in\n[MASK ]” and “The birthday of Dante is [MASK ]”\nshould return the same results.\nFinally, we evaluate the factual knowledge cap-\nture performance by jointly examining the accuracy\nand consistency (Elazar et al., 2021a):\ntest(s) = mrr (s) ·con (s) (8)\ntest(s) measures the probing performance on\ntemplate-based queries. We also deﬁne a metric to\nmeasure how well the PLMs memorize the miss-\ning words in pre-training samples (Wikipedia sen-\ntences):\ntrain(s) = PLMk(s). (9)\n2.3.2 Correlate Performance with\nDependence\nWe have quantiﬁed the dependence on each asso-\nciation and deﬁned the metrics for probing perfor-\nmance in the above sections. We then calculate\nthe Pearson correlation coefﬁcient (Kirch, 2008)\nbetween dependence and probing performance to\nreveal the effectiveness of different associations.\nAn association is considered more effective if the\nprobing performance positively correlates with its\ndependence more.\nBecause only part of the facts has available tem-\nplates, the samples in the dependence measure\nwithout templates are ignored in the calculation.\nThe factual knowledge captured by different PLMs\nmay vary signiﬁcantly due to the differences in\nmodel scale, pre-training data, or other settings.\nTo make the correlation coefﬁcient comparable be-\ntween different PLMs, we calculate the correlation\nonly on the factual knowledge gathered correctly\nby the PLM.I.e., only the pre-training samples with\ntrain(s) = PLMk(s) = 1are involved.\nSample in Dependence Measure\n# Wikipedia sentences 4,779,753\n# Different triplets 3,795,229\n# Different predicates 565\n# Sentences with synthetic templates 1,119,875\nQueries in Effectiveness Measure\n# Template-based queries 7,645,635\n# Different triplets 654,112\n# Different predicates 38\n# Different templates 328\nTable 2: Statistics of the probing data.\n3 Experiments and Discussions\n3.1 Probing Data and PLMs\nWe use the TREX dataset (Elsahar et al., 2018),\nwhich aligns KB triplets with Wikipedia sentences,\nto construct the samples for the dependence mea-\nsure following the deﬁnition in Section 2.2.2. We\nemploy the templates from (Elazar et al., 2021a)\nto construct the queries to probe the factual knowl-\nedge for the effectiveness measure. Table 2 shows\nthe statistics for the data in the dependence mea-\nsure and the effectiveness measure. The PLMs\nwe analyzed include BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), SpanBERT (Joshi\net al., 2020), and ALBERT (Lan et al., 2019).\n3.2 Dependence on Associations\nThe dependence on an association is the aver-\nage ATE over the probing samples whose treat-\nment words hold that association with the outcome\nwords. Table 3 shows the quantiﬁed dependence\nof different associations. The accuracy in Table 3\nrepresents the accuracy of recovering the masked\nfactual words in pre-training samples, revealing\nhow well does PLMs memorize the pre-training\nsamples. It is calculated by Equation 9 with k= 1.\nWe ﬁnd a general trend over all the picked\nPLMs: the Positionally Close (PC) association\ntakes the dominant effect on the prediction results,\nthe Highly Co-occurred (HC) association comes\nsecond, and the least for the Knowledge-Dependent\n(KD) association. The trend does not change much\nas increasing the model scale (large vs. base), us-\ning additional training data (RoBERTavs. BERT),\nor improving the masking strategy (SpanBERT vs.\nBERT). Consequently, the accuracy drops the most\nwhen perturbing the positionally close words but\nleast on knowledge-dependent words1.\n1Table 6 in the Appendix shows the accuracy decrease\n1724\nModel Accuracy Dependences on Associations (k = 100)\nKD PC HC R\nBERT-base-cased 0.3623 0.1585 0.4085 0.1779 0.1081\nBERT-large-cased 0.3692 0.1603 0.4113 0.1791 0.0996\nBERT-large-cased-wwm 0.5030 0.1384 0.4477 0.2305 0.1072\nSpanBERT-large 0.5223 0.1351 0.3679 0.2383 0.1157\nRoBERTa-base 0.3511 0.1352 0.3926 0.2093 0.1053\nRoBERTa-large 0.4276 0.1360 0.3962 0.2162 0.0985\nBERT-large-uncased-wwm 0.5035 0.1410 0.4350 0.2290 0.1089\nALBERT-xxlarge-v2 0.4758 0.2852 0.4338 0.3801 0.2704\nTable 3: The quantiﬁed dependence on associations. Accuracy denotes the performance of ﬁlling in the masks in\npre-training samples. The PLMs use cased and uncased vocabularies are separated.\n0.2 0.4 0.6 0.8\nProbing Performance\n-0.5\n0.0\n0.5\n1.0Std. Dependence\nKD\nHC\nPC\n(a) BERT-large-cased-wwm\n0.2 0.4 0.6 0.8\nProbing Performance\n-0.5\n0.0\n0.5\n1.0Std. Dependence\nKD\nHC\nPC (b) RoBERTa-large\n0.2 0.4 0.6 0.8\nProbing Performance\n-0.5\n0.0\n0.5\n1.0Std. Dependence\nKD\nHC\nPC (c) SpanBERT-large\n0.2 0.4 0.6 0.8\nProbing Performance\n-0.5\n0.0\n0.5\n1.0Std. Dependence\nKD\nHC\nPC (d) ALBERT-xxlarge-v2\nFigure 3: The correlations between the dependence on associations and the probing performance on factual knowl-\nedge capture.\nThe results provide quantitative evidence for\nQuestion 1 of “ Which association do PLMs de-\npend on to capture factual knowledge? :” PLMs\nprefer the associations founded with position-\nally close or the highly co-occurred words to the\nknowledge-based clues. It is different from how a\nconventional KB works, e.g., an object can be re-\ntrieved by the corresponding subject and predicate.\n3.3 Correlations between Dependence and\nPerformance\nWe show the correlation between association’s de-\npendence and the probing performance in Figure 3.\nEach point in the ﬁgure represents a piece of fac-\ntual knowledge s. We refer to it as a fact for con-\nvenience. The horizontal axis indicates test(s) for\nthe fact, showing the probing performance of the\nfact with effectiveness measure. The vertical axis\nshows the dependence of associations when cap-\nturing this fact, which is quantiﬁed by the causal\neffect estimation deﬁned in Section 2.2.1. The\nstraight lines are the regression lines and different\nassociations are shown in different line styles2.\nwhen perturbing the different associations.\n2We standardize the quantiﬁed value of dependence (de-\nnoted as Std. Dependence) and plot a bucket of facts as a\nsingle point to show the trends clearly. The correlations with-\nAs we can see from the results, the dependence\non the KD association positively correlates with\nthe probing performance. The dependence on the\nHC association has a slightly positive correlation\nor almost has no correlations sometimes (such as\nALBERT in Figure 3d). The PC association holds\na negative correlation with the performance.\nThese results can give an empirical answer to “Is\nthe association on which PLMs depend effective in\ncapturing factual knowledge?:” the more PLMs\ndepend on the Knowledge-Dependent (KD) as-\nsociation, the better PLMs can capture the cor-\nresponding factual knowledge. Meanwhile, rely-\ning much on the positionally close association is\nharmful to the probing performance.\nThe dependence measure results reveal that\nthe PLMs depend most on the positionally close\nbut least on the knowledge-dependent association.\nHowever, in effectiveness measure, we ﬁnd that\nthe positionally close association is the most in-\neffective for factual knowledge capture while the\nknowledge-dependent association is the most effec-\ntive. By connecting the two results, we can con-\nclude the answer to the question in the title: The\nPLMs do not capture factual knowledge ideally,\nout standardization for more PLMs are in Table 8\n1725\nPre-training samples for the dependence measure (Wikipedia Sentence) Dep.\nKD: Kimwenza is a community in the Democratic Republic of the Congo in the Mont Ngafula commune in the south of the capital, Kinshasa . 0.8564\nCase 1\nPC: Kimwenza is a community in the Democratic Republic of the Congo in the Mont Ngafula commune in the south of the capital, Kinshasa . 0.0000\nHC: Kimwenza is a community in the Democratic Republic of the Congo in the Mont Ngafula commune in the south of the capital, Kinshasa . 0.0000\nTemplate-based Queries for the effectiveness measure MRR\nThe capital of Congo is Kinshasa . 1.0\nKinshasa is the capital of Congo . 1.0\nCongo’s capital isKinshasa . 1.0\nPre-training samples for the dependence measure (Wikipedia Sentence) ATE\nKD: Drayton is a hamlet in England, in the county of Northamptonshire, . . ., hundred of Fawsley, ¾ of a mile on the low-lying north western side of\nthe town of Daventry .\n0.0000\nCase 2\nPC: Drayton is a hamlet in England, . . ., in the parish and union of Daventry, hundred of Fawsley, ¾ of a mile on the low-lying north western side of\nthe town of Daventry .\n0.9496\nHC: Drayton is a hamlet in England, . . ., in the parish and union of Daventry, hundred of Fawsley, ¾ of a mile on the low-lying north western side of\nthe town of Daventry .\n0.8452\nTemplate-based Queries for the effectiveness measure MRR\nDrayton is located in Daventry . 0.0\nDrayton is in Daventry . 0.0\nDrayton can be found in Daventry . 0.0\nTable 4: Two cases from SpanBERT-large. The quantiﬁed dependence on associations (denoted by Dep.) and the\nperformance of factual knowledge capture (denoted by MRR).\nsince they depend more on the ineffective asso-\nciations than the effective one.\n3.4 Case Study\nTo illustrate the analysis result intuitively, we\nshow two cases with SpanBERT-large in Table 4.\nThe MRR shows the probing performance on the\ntemplate-based query (calculated by Equation 6).\nIn Case 1, the knowledge-dependent association\ngains the biggest effect, and the predictions are ro-\nbust in all the template-based probing. However,\nthe positionally close association takes the main\neffect in Case 2, while the PLM fails to recall the\nword “England” with the template-based queries.\n3.5 Discussions\nGenerality of the Proposed Probing Method\nGenerally, the dependence measure offers a way to\nmeasure how much the word-level patterns cause\nthe prediction of missing words in Mask Language\nModel (MLM). Because words are readable, di-\nrectly visible, and can be manipulated from the\ninput side directly, the word-level patterns can pro-\nvide more intuitive interpretations than numeric\nrepresentation vectors (Elazar et al., 2021b) or neu-\nrons (Vig et al.). We use the proposed method to\nestimate the causal effect of three typical associa-\ntions in this paper, while this method can be easily\nadapted to quantify the dependence on any word-\nlevel patterns.\nReconsidering “PLM as KB” If we want to use\na PLM like a KB, whether the PLM has the same\ninner workﬂow as KBs deserves to be considered.\nThe prevalent KBs index knowledge as subject-\npredicate-object triplets and can infer with triplets\n(Speer et al., 2017; Bollacker et al., 2008; Vran-\ndeˇci´c and Krötzsch, 2014). However, we ﬁnd out\nthat the knowledge-dependent association, which\nrepresents the process of inferring a missing object\nbased on the given subject and predicate, has the\nlowest dependence in the PLMs. It provides evi-\ndence that the PLMs work quite differently with\nKBs and can not serve stably as KBs for now.\nOverﬁting and Generalization Figure 4 shows\nthe correlations between the dependence on associ-\nations and the mask-ﬁlling accuracy on pre-training\nsamples (referred to as memorizing accuracy). The\nmemorizing accuracy increases most as the depen-\ndence of the PC association increases, demonstrat-\ning that the more PLMs depend on the positionally\nclose words, the better PLMs can recover the pre-\ntraining samples. However, there is an opposite\ntrend in probing performance as shown in Figure 3.\nThe additionally crafted queries used to evaluate\nthe probing performance are mostly unseen in per-\ntraining. If we consider these queries as the test\nset and the pre-training samples as the train set, we\ncan conclude that the dependence on the PC associ-\nations makes the PLMs tend to overﬁt the training\ndata and degrade the generalization on the test set.\nFactual Knowledge Capture in Pre-trainingWe\nwant to focus on the pre-training samples that help\nPLMs to capture factual knowledge. So we recon-\nstruct the pre-training samples that predict some\nmissing factual words (object) based on the factual\nclues (subject, predicate). We conduct the depen-\ndence measure on these samples to investigate how\nthe factual knowledge is captured in pre-training.\nThe mask-ﬁlling accuracy on these pre-training\nsamples denotes how well PLMs memorize them\n1726\n0.2 0.4 0.6 0.8\nMemorizing Accuracy\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5Std. Dependence\nKD\nHC\nPC\n(a) BERT-large-cased-wwm\n0.2 0.4 0.6 0.8\nMemorizing Accuracy\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5Std. Dependence\nKD\nHC\nPC (b) RoBERTa-large\n0.2 0.4 0.6 0.8\nMemorizing Accuracy\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5Std. Dependence\nKD\nHC\nPC (c) SpanBERT-large\n0.2 0.4 0.6 0.8\nMemorizing Accuracy\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.5Std. Dependence\nKD\nHC\nPC (d) ALBERT-xxlarge-v2\nFigure 4: The correlations between the dependence on associations and the mask-ﬁlling accuracy on the pre-\ntraining samples (Wikipedia sentences).\nin pre-training. We name it as “train” in Equation 9\nand “Memorizing Accuracy” in Figure 4.\nOverlap between Associations The clues for dif-\nferent associations overlap sometimes, e.g., some\nremaining words may hold the KD and PC asso-\nciations with the missing words at the same time.\nThe overlaps do not impair the estimations because\nwe use a set of samples to estimate the effect of\neach association. The samples that hold the same\nassociation stay in the same set, and the average\ncausal effect in all these samples is the quantiﬁed\ndependence of this association. The sample sets are\nquite different for different associations. Table 5\nshows the corresponding statistics of the overlaps.\n4 Related Works\nProbing Factual Knowledge in PLMs Factual\nKnowledge Probing in PLMs has attracted much\nattention recently. LAMA (Petroni et al., 2019)\npropose a benchmark that probes the factual knowl-\nedge in the PLMs with cloze-style prompts and\nshows PLMs’ ability to capture factual knowledge.\nThis ability can be further explored by tuning the\nprompts for probing Jiang et al. (2020); Shin et al.\n(2020); Zhong et al. (2021).\nMotivated by the probing results, some recent\nworks analyze the captured factual knowledge from\nmore perspectives. Cao et al. (2021) analyze the\ndistribution of predictions and the answer leakage\nin probing. Poerner et al. (2020) propose that the\nPLMs could predict based on some correlation be-\ntween surface forms rather than infer according to\nfacts. Elazar et al. (2021a) reveal that the PLMs’\noutputs are inconsistent as querying the same fact\nwith different prompts.\nThis paper proposes a more ﬁne-grained inspec-\ntion of word-level patterns in the input. In addition\nto constructing more challenging probing data as in-\nput or analyzing the outputs more detailedly, we try\nto reveal the inner mechanism of PLMs by conduct-\ning intervention on the input and then observing\nthe change in the output.\nCausal-inspired Interpretations in NLP A\ncausal-inspired approach to explanation is to gen-\nerate counterfactual examples and then compare\nthe predictions (Feder et al., 2021a). Feder et al.\n(2021b) propose a framework for producing expla-\nnation for NLP models using counterfactual rep-\nresentation. Vig et al. analyze the effect of neu-\nrons (or attention heads) on the gender bias using\ncausal mediation analysis. In this paper, we revisit\nthe word-level post-hot interpretation (Sun et al.,\n2021; Li et al.) from a causal-effect perspective:\nintervene on some speciﬁed words in the input and\nmeasure the difference in the output to estimate\nthe causal effect of these words. Furthermore, we\nevaluate the effectiveness of different causes by\ncalculating correlations between their effects and\nperformance. As far as we know, our work is the\nﬁrst study to probe and evaluate word-level patterns\nin the factual knowledge capture task.\n5 Conclusion\nIn this paper, we try to answer the question of\nHow Pre-trained Language Models Capture Fac-\ntual Knowledge by measuring and evaluating differ-\nent associations that PLMs use to capture factual\nknowledge. We present three word-level associ-\nations, knowledge-dependent, positionally close,\nand highly co-occurred in the analysis. The analy-\nsis results show that the PLMs rely more on the in-\neffective positionally close and highly co-occurred\nassociations when capturing factual knowledge,\nand somewhat ignore the effective knowledge-\ndependent clues. These ﬁndings indicate that\nwe should pay more attention to the knowledge-\ndependent association to let PLMs capture factual\nknowledge better.\n1727\nReferences\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the 2008 ACM\nSIGMOD international conference on Management\nof data, pages 1247–1250.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 1860–1874,\nOnline. Association for Computational Linguistics.\nKenneth Ward Church and Patrick Hanks. 1990. Word\nassociation norms, mutual information, and lexicog-\nraphy. Computational Linguistics, 16(1):22–29.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021a. Measuring and im-\nproving consistency in pretrained language models.\nTransactions of the Association for Computational\nLinguistics, 9:1012–1031.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2021b. Amnesic probing: Behavioral ex-\nplanation with amnesic counterfactuals. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:160–175.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018).\nAmir Feder, Katherine A Keith, Emaad Manzoor, Reid\nPryzant, Dhanya Sridhar, Zach Wood-Doughty, Ja-\ncob Eisenstein, Justin Grimmer, Roi Reichart, Mar-\ngaret E Roberts, et al. 2021a. Causal inference\nin natural language processing: Estimation, pre-\ndiction, interpretation and beyond. arXiv preprint\narXiv:2109.00725.\nAmir Feder, Nadav Oved, Uri Shalit, and Roi Re-\nichart. 2021b. Causalm: Causal model explanation\nthrough counterfactual language models. Computa-\ntional Linguistics, 47(2):333–386.\nOktie Hassanzadeh. 2021. Predicting the future with\nwikidata and wikipedia. In Proceedings of the\nISWC.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating knowl-\nedge in multilingual pretrained language models. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3250–3258, Online.\nAssociation for Computational Linguistics.\nWilhelm Kirch, editor. 2008. Pearson’s Correlation\nCoefﬁcient, pages 1090–1091. Springer Netherlands,\nDordrecht.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nJiwei Li, Will Monroe, and Dan Jurafsky. Understand-\ning neural networks through representation erasure.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nJudea Pearl. 2009. Causality. Cambridge university\npress.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-BERT: Efﬁcient-yet-effective entity em-\nbeddings for BERT. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 803–818, Online. Association for Computa-\ntional Linguistics.\nDonald B Rubin. 1974. Estimating causal effects of\ntreatments in randomized and nonrandomized stud-\nies. Journal of educational Psychology, 66(5):688.\n1728\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-ﬁrst AAAI conference on\nartiﬁcial intelligence.\nXiaofei Sun, Diyi Yang, Xiaoya Li, Tianwei Zhang,\nYuxian Meng, Qiu Han, Guoyin Wang, Eduard\nHovy, and Jiwei Li. 2021. Interpreting deep learn-\ning models in natural language processing: A review.\narXiv preprint arXiv:2110.10470.\nJames A Thom, Jovan Pehcevski, and Anne-Marie Ver-\ncoustre. 2007. Use of wikipedia categories in entity\nranking. arXiv preprint arXiv:0711.2917.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. Investigating gender bias in language\nmodels using causal mediation analysis.\nDenny Vrande ˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78–85.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5017–5033, Online. Association for\nComputational Linguistics.\n1729\nAppendix\nA Probing Sample Construction for the\nDependence Measure\nWe detail how we construct the probing samples\nfor the dependence measure as follows. We take\na subject-predicate-object triplet in Wikidata3 as a\npiece of factual knowledge (Petroni et al., 2019;\nCao et al., 2021; Kassner et al., 2021; Elazar\net al., 2021a). A subject-predicate-object triplet\nis aligned with a Wikipedia sentence by matching\nthe subject, predicate, and object with their corre-\nsponding spans, respectively. A subject-predicate-\nobject triplet is aligned with a Wikipedia sentence\nby matching the subject, predicate, and object with\ntheir corresponding spans, respectively. The words\nthat correspond to the object are the factual words\nthat are masked and need to be predicted, and we\ninvestigate how the different remaining words con-\ntribute to the prediction.\nThe remaining words that have three typical as-\nsociations with the masked words are considered\nin the analysis. The rules to identify the remaining\nwords that have the Knowledge-Dependent (KD)\nassociation are:\n1. The Wo and the Wt describe the same subject-\npredicate-object triplet in KB.\n2. The Wt are the natural language description\nof the subject and predicate, the Wo are that\nfor the object.\n3. If the subject and predicate corresponding to\nWt are given correctly ( i.e., Wt = ˆwt), the\nground-truth value of the object is unique in\nthe KB.\nThe ﬁrst rule makes the Wt and Wo grounded in\nthe same piece of factual knowledge. The second\nrule makes predicting the outcome words similar\nto inferring the object using a KB when giving\nthe subject and predicate. The third rule means\nif the treatment words are given correctly, there\nshould be one and only one ground-truth value for\nthe object. The third rule is similar to the N-1\nrelationship (Cao et al., 2021) in KB and lets the\nground-truth treatment words can be regarded as a\nsufﬁcient condition to predict the unique outcome\nwords deterministically.\n3https://www.wikidata.org/\nWe use the T-REx4 dataset to provide the initial\nalignment between KB triplets and the Wikipedia\nsentences. We use the aliases in KB as keys for\nfuzzy string match (Levenshtein distance is less\nthan 2, stemming before matching, etc.) to align\nmore subjects, predicates, and objects with spans\nin the sentence. The sentences that have no aligned\ntriplet are ﬁltered out.\nSometimes, the outcome words in a single sen-\ntence relate to multiple triplets that satisfy the\nrules for KB. E.g., there are two groups of remain-\ning words that can infer the outcome words de-\nterministically based on KB. We select them all\nas the Wt when probing the KB association and\nkeep the number of the masked words be the same\nin interventions for the other associations. DKD,\nDPC, and DHC denote the sample sets for the\nKnowledge-Dependent (KD), Positionally Close\n(PC), and Highly Co-occurred (HC) associations,\nrespectively.\nFor the Highly Co-occurred association (HC),\nthe remaining words that are top- k in PMI with\nthe ground-truth outcome words ˆwo are selected\nas Wt. The k is the number of words with the\nKD associations for the same sentence. The PMI\nbetween words is calculated by all the Wikipedia\nsentences. If the ˆwo consists of multiple words,\noccurring with all the words in ˆwo altogether are\ntaken as co-occurring. The order of the words in\nˆwo are ignored for efﬁciency. Table 5 shows more\ndetails about the probing samples.\nB More Probing Results\nThe Pearson correlation coefﬁcients between the\ndependence on associations (raw value without\nstandardization) and the performance are shown\nin Table 8. The three metrics, accuracy (deﬁned in\nEquation 6), consistency (deﬁned in Equation 7),\nand the overall performance metric (deﬁned in\nEquation 8), are reported respectively. The cor-\nrelation coefﬁcients between the dependence and\nthe performance are consistent with the slopes of\nthe regression lines in Figure 3. Table 6 shows the\naccuracy decreasing results after masking the treat-\nments words when generating the missing words\nin Wikipedia sentences.\n4https://hadyelsahar.github.io/t-rex/\n1730\nProbing Samples in Dependence Measure\n# Average treatment words 4.0023\n# Average outcome words 1.8588\n# Average words 23.1031\nWord-level Overlap Between Associations\nDKD ∩ DHC 44.25% (8,455,641)\nDKD ∩ DPC 20.83% (3,981,305)\nDPC ∩ DHC 18.75% (3,582,576)\nDKD ∩ DPC ∩ DHC 8.3349% (1,592,560)\nSample-level Overlap Between Associations\nDKD ∩ DHC 3.12% (149,479)\nDKD ∩ DPC 0.06% (2,995)\nDPC ∩ DHC 0.12% (5,777)\nDKD ∩ DPC ∩ DHC 0.0001% (547)\nTemplate-based Queries in Effectiveness Measure\n# Average treatment words 1.9484\n# Average outcome words 1.5844\n# Average word per sample 6.9617\nTable 5: Statistic of the probing data in the dependence measure.\nModel Input Context (Accuracy, %)\nComplete w/o KD w/o PC w/o HC w/o R\nBERT-base-cased 36.23 22.05 (-14.17) 2.67 (-33.56) 19.68 (-16.54) 26.71 ( -9.52)\nBERT-large-cased 36.92 22.11 (-14.81) 2.70 (-34.22) 19.04 (-17.88) 27.11 ( -9.81)\nBERT-large-cased-wwm 50.30 35.22 (-15.08) 11.50 (-38.80) 26.09 (-24.21) 39.04 (-11.25)\nSpanBERT-large 52.23 36.87 (-15.35) 16.66 (-35.57) 26.78 (-25.44) 39.87 (-12.35)\nRoBERTa-base 35.11 23.07 (-12.03) 4.81 (-30.30) 16.38 (-18.72) 25.70 ( -9.41)\nRoBERTa-large 42.76 28.26 (-14.50) 8.98 (-33.78) 21.21 (-21.54) 32.67 (-10.09)\nBERT-base-uncased 36.90 23.53 (-13.37) 3.33 (-33.58) 20.78 (-16.12) 28.53 ( -8.37)\nBERT-large-uncased 38.62 24.58 (-14.04) 2.88 (-35.74) 21.61 (-17.00) 29.83 ( -8.79)\nBERT-large-uncased-wwm 50.35 35.03 (-15.31) 11.92 (-38.43) 26.16 (-24.18) 39.03 (-11.32)\nALBERT-xxlarge-v2 47.58 23.02 (-24.56) 11.58 (-36.00) 15.25 (-32.34) 24.42 (-23.16)\nALBERT-xlarge-v2 40.87 16.13 (-24.74) 8.15 (-32.72) 9.57 (-31.30) 15.11 (-25.76)\nALBERT-large-v2 39.02 8.03 (-30.99) 3.82 (-35.20) 4.52 (-34.50) 9.00 (-30.02)\nALBERT-base-v2 32.76 3.63 (-29.13) 1.74 (-31.03) 1.76 (-31.00) 3.63 (-29.13)\nALBERT-xxlarge-v1 47.50 23.68 (-23.83) 12.20 (-35.30) 15.79 (-31.71) 25.27 (-22.24)\nALBERT-xlarge-v1 48.19 21.15 (-27.04) 11.81 (-36.38) 12.82 (-35.38) 22.57 (-25.62)\nALBERT-large-v1 43.64 14.14 (-29.50) 7.52 (-36.12) 7.93 (-35.71) 14.80 (-28.84)\nALBERT-base-v1 40.95 27.39 (-13.56) 13.13 (-27.83) 19.30 (-21.65) 30.05 (-10.90)\nTable 6: The accuracy of the predictions when different treatment words are missing.\n1731\nModel ATE of Association (k = 100/k = 1)\nKD PC HC R\nBERT-base-cased 0.1585/0.1416 0.4085/0.3354 0.1779/0.1654 0.1081/0.0950\nBERT-large-cased 0.1603/0.1480 0.4113/0.3420 0.1791/0.1788 0.0996/0.0979\nBERT-large-cased-wwm 0.1384/0.1506 0.4477/0.3879 0.2305/0.2421 0.1072/0.1124\nSpanBERT-large 0.1351/0.1533 0.3679/0.3555 0.2383/0.2544 0.1157/0.1233\nRoBERTa-large 0.1360/0.1438 0.3962/0.3366 0.2162/0.2154 0.0985/0.0997\nRoBERTa-base 0.1352/0.1192 0.3926/0.3018 0.2093/0.1872 0.1053/0.0929\nBERT-base-uncased 0.1439/0.1337 0.4112/0.3357 0.1643/0.1612 0.0901/0.0837\nBERT-large-uncased 0.1522/0.1403 0.4401/0.3573 0.1713/0.1700 0.0946/0.0879\nBERT-large-uncased-wwm 0.1410/0.1531 0.4350/0.3842 0.2290/0.2418 0.1089/0.1131\nALBERT-base-v2 0.3987/0.2911 0.4269/0.3100 0.4269/0.3100 0.4021/0.2911\nALBERT-large-v2 0.4075/0.3098 0.4716/0.3519 0.4566/0.3450 0.3958/0.3001\nALBERT-xlarge-v2 0.3279/0.2474 0.4336/0.3272 0.4170/0.3130 0.3468/0.2576\nALBERT-xxlarge-v2 0.2852/0.2457 0.4338/0.3601 0.3801/0.3234 0.2704/0.2317\nALBERT-base-v1 0.1429/0.1355 0.3235/0.2782 0.2287/0.2165 0.1167/0.1089\nALBERT-large-v1 0.3638/0.2951 0.4569/0.3612 0.4488/0.3571 0.3621/0.2884\nALBERT-xlarge-v1 0.3223/0.2705 0.4425/0.3639 0.4266/0.3538 0.3116/0.2563\nALBERT-xxlarge-v1 0.2761/0.2384 0.4230/0.3531 0.3708/0.3171 0.2590/0.2224\nTable 7: The ATE of associations in more PLMs.\nModel Associations (joint/accuracy/consistency)\nKD PC HC R\nBERT-base-cased 0.1523/0.2222/0.0768 -0.2156/-0.1839/-0.1597 0.0011/ 0.0180/-0.0137 -0.0823/-0.0774/-0.0621\nBERT-large-cased 0.1398/0.1879/0.0788 -0.1638/-0.1120/-0.1295 -0.0017/ 0.0095/-0.0141 -0.0810/-0.0741/-0.0638\nBERT-large-cased-wwm 0.2492/0.2795/0.1627 -0.1904/-0.1783/-0.1290 0.0800/ 0.0851/ 0.0422 -0.0487/-0.0455/-0.0417\nSpanBERT-large 0.2463/0.2784/0.1382 -0.1068/-0.0781/-0.1187 0.1017/ 0.1175/ 0.0254 -0.0384/-0.0307/-0.0391\nRoBERTa-base 0.2432/0.3062/0.1223 -0.0414/-0.0440/-0.0366 0.0966/ 0.1252/ 0.0297 -0.0201/-0.0054/-0.0423\nRoBERTa-large 0.2212/0.2666/0.1141 -0.1131/-0.1455/-0.0642 0.0749/ 0.0911/ 0.0156 -0.0311/-0.0441/-0.0236\nBERT-base-uncased 0.1635/0.2233/0.0954 -0.1454/-0.1269/-0.1182 0.0130/ 0.0410/-0.0114 -0.0659/-0.0630/-0.0596\nBERT-large-uncased 0.1507/0.2022/0.0749 -0.2056/-0.1900/-0.1084 0.0247/ 0.0355/ 0.0085 -0.0671/-0.0667/-0.0454\nBERT-large-uncased-wwm 0.2526/0.2776/0.1593 -0.1772/-0.1589/-0.1346 0.0866/ 0.0886/ 0.0344 -0.0462/-0.0419/-0.0401\nALBERT-base-v2 0.0453/0.0530/0.0347 -0.1054/-0.1333/-0.0417 0.0071/-0.0005/ 0.0371 -0.0886/-0.1186/-0.0117\nALBERT-large-v2 0.0809/0.0988/0.0370 -0.1130/-0.1457/-0.0826 0.0201/ 0.0158/ 0.0093 -0.0822/-0.1061/-0.0707\nALBERT-xlarge-v2 0.1515/0.2161/0.1064 -0.1184/-0.1152/-0.0759 0.0278/ 0.0524/ 0.0154 -0.0997/-0.0978/-0.0627\nALBERT-xxlarge-v2 0.1685/0.1954/0.1347 -0.1549/-0.1552/-0.1039 0.0445/ 0.0492/ 0.0496 -0.0783/-0.0759/-0.0559\nALBERT-base-v1 0.3034/0.3563/0.1764 -0.0650/-0.0504/-0.1010 0.1564/ 0.1724/ 0.0497 0.0111/ 0.0175/-0.0202\nALBERT-large-v1 0.1466/0.1741/0.1145 -0.0384/-0.0639/-0.0037 0.0598/ 0.0530/ 0.0606 -0.0186/-0.0408/ 0.0026\nALBERT-xlarge-v1 0.1593/0.1816/0.1486 -0.0514/-0.0629/-0.0006 0.0498/ 0.0338/ 0.1172 -0.0081/-0.0288/ 0.0431\nALBERT-xxlarge-v1 0.1926/0.2207/0.1462 -0.1314/-0.1346/-0.0929 0.0629/ 0.0647/ 0.0526 -0.0563/-0.0535/-0.0475\nTable 8: The Pearson correlation coefﬁcients between the ATEs and the factual knowledge capture metrics.\n1732",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.676047682762146
    },
    {
      "name": "Natural language processing",
      "score": 0.6173722743988037
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5529944896697998
    },
    {
      "name": "Word (group theory)",
      "score": 0.5215125679969788
    },
    {
      "name": "Style (visual arts)",
      "score": 0.4470040798187256
    },
    {
      "name": "Cognitive psychology",
      "score": 0.33275705575942993
    },
    {
      "name": "Linguistics",
      "score": 0.324685275554657
    },
    {
      "name": "Psychology",
      "score": 0.29615265130996704
    },
    {
      "name": "History",
      "score": 0.11735403537750244
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    }
  ]
}