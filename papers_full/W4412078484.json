{
    "title": "GlaucoRAG: A Retrieval-Augmented Large Language Model for Expert-Level Glaucoma Assessment",
    "url": "https://openalex.org/W4412078484",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A4321226832",
            "name": "Mohammad Aminan",
            "affiliations": [
                "University of Tennessee Health Science Center"
            ]
        },
        {
            "id": null,
            "name": "S. Solomon Darnell",
            "affiliations": [
                "University of Tennessee Health Science Center"
            ]
        },
        {
            "id": "https://openalex.org/A3158705612",
            "name": "Mohammad Delsoz",
            "affiliations": [
                "University of Tennessee Health Science Center"
            ]
        },
        {
            "id": "https://openalex.org/A2123678023",
            "name": "Amin Nabavi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A333560056",
            "name": "Claire E. Wright",
            "affiliations": [
                "University of Tennessee Health Science Center"
            ]
        },
        {
            "id": "https://openalex.org/A2766504612",
            "name": "Brian Jerkins",
            "affiliations": [
                "University of Tennessee Health Science Center"
            ]
        },
        {
            "id": "https://openalex.org/A2300720113",
            "name": "Siamak Yousefi",
            "affiliations": [
                "University of Tennessee Health Science Center"
            ]
        },
        {
            "id": "https://openalex.org/A4321226832",
            "name": "Mohammad Aminan",
            "affiliations": []
        },
        {
            "id": null,
            "name": "S. Solomon Darnell",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3158705612",
            "name": "Mohammad Delsoz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A333560056",
            "name": "Claire E. Wright",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2766504612",
            "name": "Brian Jerkins",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2300720113",
            "name": "Siamak Yousefi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4391251019",
        "https://openalex.org/W4391831282",
        "https://openalex.org/W4284962318",
        "https://openalex.org/W3011490500",
        "https://openalex.org/W3022840841",
        "https://openalex.org/W2557738935",
        "https://openalex.org/W2772246530",
        "https://openalex.org/W2529153069",
        "https://openalex.org/W4386697749",
        "https://openalex.org/W4386726071",
        "https://openalex.org/W4401814064",
        "https://openalex.org/W4403467309",
        "https://openalex.org/W4399242673",
        "https://openalex.org/W4368340908",
        "https://openalex.org/W4400775727",
        "https://openalex.org/W4403527710",
        "https://openalex.org/W4391878439"
    ],
    "abstract": "Abstract Purpose Purpose: Accurate glaucoma assessment is challenging because of the complexity and chronic nature of the disease; therefore, there is a critical need for models that provide evidence-based, accurate assessment. The purpose of this study was to evaluate the capabilities of a glaucoma specialized Retrieval-Augmented Generation (RAG) framework (GlaucoRAG) that leverages a large language model (LLM) for diagnosing glaucoma and answering to glaucoma specific questions. Design Evaluation of diagnostic capabilities and knowledge of emerging technologies in glaucoma assessment. Participants Detailed case reports from 11 patients and 250 multiple choice questions from the Basic and Clinical Science Course (BCSC) Self-Assessment were used to test the LLM based GlaucoRAG. No human participants were involved. Methods We developed GlaucoRAG, a RAG framework leveraging GPT-4.5-PREVIEW integrated with the R2R platform for automated question answering in glaucoma. We created a glaucoma knowledge base comprising more than 1,800 peer-reviewed glaucoma articles, 15 guidelines and three glaucoma textbooks. The diagnostic performance was tested on case reports and multiple-choice questions. Model outputs were compared with the independent answers of three glaucoma specialists, DeepSeek-R1, and GPT-4.5-PREVIEW (without RAG). Quantitative performance was further assessed with the RAG Assessment (RAGAS) framework, reporting faithfulness, context precision, context recall, and answer relevancy. Main Outcome Measures The primary outcome measure was GlaucoRAG’s diagnostic accuracy on patient case reports and percentage of correct responses to the BCSC Self-Assessment glaucoma items, compared with the performance of glaucoma specialists and two benchmark LLMs. Secondary outcomes included RAGAS sub scores. Results GlaucoRAG achieved an accuracy of 81.8% on glaucoma case reports, compared with 72.7% for GPT-4.5-PREVIEW and 63.7% for DeepSeek-R1. On glaucoma BCSC Self-Assessment questions, GlaucoRAG achieved 91.2% accuracy (228 / 250), whereas GPT-4.5-PREVIEW and DeepSeek-R1 attained 84.4% (211 / 250) and 76.0% (190 / 250), respectively. The RAGAS evaluation returned an answer relevancy of 91%, with 80% context recall, 70% faithfulness, and 59% context precision. Conclusions The glaucoma-specialized LLM, GlaucoRAG, showed encouraging performance in glaucoma assessment and may complement glaucoma research and clinical practice as well as question answering with glaucoma patients.",
    "full_text": "Journal, Vol. XXI, No. 1, 1-5, 2013\nAdditional note\nGlaucoRAG: A Retrieval-Augmented Large Language\nModel for Expert-Level Glaucoma Assessment\nMohammad Aminan1, S. Solomon Darnell2, Mohammad Delsoz1, Amin Nabavi1, Claire Wright1, Brian\nJerkins1, Siamak Yousefi1*\nAbstract\nPurpose: Purpose: Accurate glaucoma assessment is challenging because of the complexity and chronic nature of the\ndisease; therefore, there is a critical need for models that provide evidence-based, accurate assessment. The purpose of\nthis study was to evaluate the capabilities of a glaucoma specialized Retrieval-Augmented Generation (RAG) framework\n(GlaucoRAG) that leverages a large language model (LLM) for diagnosing glaucoma and answering to glaucoma\nspecific questions.\nDesign: Evaluation of diagnostic capabilities and knowledge of emerging technologies in glaucoma assessment.\nParticipants: Detailed case reports from 11 patients and 250 multiple choice questions from the Basic and Clinical\nScience Course (BCSC) Self-Assessment were used to test the LLM based GlaucoRAG. No human participants were\ninvolved.\nMethods: We developed GlaucoRAG, a RAG framework leveraging GPT-4.5-PREVIEW integrated with the R2R\nplatform for automated question answering in glaucoma. We created a glaucoma knowledge base comprising more\nthan 1,800 peer-reviewed glaucoma articles, 15 guidelines and three glaucoma textbooks. The diagnostic performance\nwas tested on case reports and multiple-choice questions. Model outputs were compared with the independent answers\nof three glaucoma specialists, DeepSeek-R1, and GPT-4.5-PREVIEW (without RAG). Quantitative performance was\nfurther assessed with the RAG Assessment (RAGAS) framework, reporting faithfulness, context precision, context\nrecall, and answer relevancy.\nMain Outcome Measures: The primary outcome measure was GlaucoRAG’s diagnostic accuracy on patient case\nreports and percentage of correct responses to the BCSC Self-Assessment glaucoma items, compared with the\nperformance of glaucoma specialists and two benchmark LLMs. Secondary outcomes included RAGAS sub scores.\nResults: GlaucoRAG achieved an accuracy of 81.8% on glaucoma case reports, compared with 72.7% for GPT-4.5-\nPREVIEW and 63.7% for DeepSeek-R1. On glaucoma BCSC Self-Assessment questions, GlaucoRAG achieved 91.2%\naccuracy (228 / 250), whereas GPT-4.5-PREVIEW and DeepSeek-R1 attained 84.4% (211 / 250) and 76.0% (190\n/ 250), respectively. The RAGAS evaluation returned an answer relevancy of 91%, with 80% context recall, 70%\nfaithfulness, and 59% context precision.\nConclusions: The glaucoma-specialized LLM, GlaucoRAG, showed encouraging performance in glaucoma assessment\nand may complement glaucoma research and clinical practice as well as question answering with glaucoma patients.\nKeywords\nGlaucoma, Large Language Mdoel (LLM), Retrieval-Augmented Generation (RAG), Glaucoma Specialized RAG\n(GlaucoRAG), Question Answering (QA)\n1Department of Ophthalmology, University of Tennessee Health Sciences Center Memphis, Tennessee, United States\n2Department of Genetics, Genomics and Informatics, University of Tennessee Health Sciences Center Memphis, Tennessee, United States\n*Corresponding author: siamak.yousefi@uthsc.edu\nContents\nIntroduction 2\n1 Methods 2\n1.1 Knowledge Base Sources and Pre-Processing . . . . 2\n1.2 GlaucoRAG Architecture . . . . . . . . . . . . . . . . . . 2\nOffline knowledge ingestion pipeline• Online inference pipeline\n1.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Result 3\n2.1 Case Report Tasks . . . . . . . . . . . . . . . . . . . . . . . 3\n2.2 BCSC multiple-choice question task . . . . . . . . . . . 3\n2.3 RAGAS Quality Metrics . . . . . . . . . . . . . . . . . . . 3\n3 Discussion 4\nAcknowledgments 5\nReferences 5\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 7, 2025. ; https://doi.org/10.1101/2025.07.03.25330805doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nGlaucoRAG: A Retrieval-Augmented Large Language Model for Expert-Level Glaucoma Assessment — 2/6\nIntroduction\nArtificialintelligence(AI)modelshavedemonstratedhigh\naccuracy in detecting several ophthalmic diseases, in-\ncluding diabetic retinopathy and glaucoma, from vari-\nousimagingmodalitiesoverthepastfewyears[ 1,2,3,\n4, 5, 6, 7, 8, 9]. More recently, large language mod-\nels (LLMs) have shown promise in answering clinical\nqueries[10, 11, 12, 13, 14]. While LLMs like GPT-4.5-\nPREVIEWholdpromiseforautomatedquestionanswer-\ning, there is a risk of producing hallucinated answers\nwithoutgrounding[15,16].\nRetrieval-Augmented Generation (RAG), which com-\nbines external, domain-specific knowledge, has been\nidentified as a valuable approach with the capacity to\nimprovethefactualaccuracyofLLMoutputs[ 16]. RAG\ncombines dense document embeddings with an LLM\nto generate responses that are validated through the\nembedded knowledge and typically provide evidence\nand grounding that minimizes hallucinations[\n1]. Previ-\nous RAG systems have performed well in radiology and\npathologyquestions[2]. However,RAG-basedmodels\nin ophthalmology remain relatively understudied.16 A\nhead-to-headcomparisonofChatGPTversusophthal-\nmologists on exam questions revealed that a general\npurpose LLM still performs lower on knowledge rele-\nvant to ophthalmologists, highlighting the necessity of\ndomain-specializedRAG systems[6].\nInthisstudy,wedevelopedGlaucoRAG,anaugmented-\nretrievalLLM-basedsystemtoassessglaucomausinga\nrigorous glaucoma-specific knowledge base to facilitate\nmoreevidence-basedquestionansweringinglaucoma.\n1. Methods\n1.1 Knowledge Base Sources and Pre-Processing\nTheimplementationofGlaucoRAGinvolvesdatacura-\ntion, ancillary open-source tools, and a popular state-of-\nthe-artLLM.TheknowledgebaseforGlaucoRAGwas\ncompiled from three sources: three authoritative glau-\ncomatextbooks,AmericanAcademyofOphthalmology\nBasic and Clinical Science Course BCSC Section 10,\nShields’TextbookofGlaucoma(7thed.),andGlaucoma\n(2nd ed.); 1,859 peer-reviewed articles published be-\ntween 2000 and March 2020. Articles were retrieved\nfrom Ophthalmology, Progress in Retinal and Eye Re-\nsearch, Ophthalmology Glaucoma, American Journal of\nOphthalmology,Surveyof Ophthalmology,and Journal\nofGlaucomausingthequeryterm“glaucoma”inthetitle\norabstract;and15guidelinedocumentsonassessment\nandmanagementofglaucomapublishedbyAmerican\nAcademyofOphthalmology, EuropeanGlaucomaSoci-\nety,and Royal Collegeof Ophthalmologists.\nAlimitedsetof250multiple-choicequestions,without\nanswers or explanatory content, were input into LLMs\nsolely to evaluate LLM models. This use aligns with\nFigure 1. Architectureof the glaucomaspecialized\nretrievalaugmented generation(GlaucoRAG)\nframework.\nthe AAO’s permitted usage guidelines and is consistent\nwith the principles of Fair Use under U.S. copyright law,\ngiven the non-commercial, transformative nature of the\nwork. No AAO answers, explanations, or proprietary\neducationalmaterialwerereproducedordistributedin\nthestudy.\n1.2 GlaucoRAG Architecture\nGlaucoRAG’s inference components are R2R[17], an\nopensourceagenticRAGsystemandGPT-4.5-PREVIEW[ 18],\nastate-of-the-artLLM.Figure1showsthearchitectureof\ntheGlaucoRAG.Thesystemiscomposedoftwocoupled\nsubsystems,anofflineknowledge-ingestionpipelineand\nanonlineinferencepipeline. Bothsystempipelinesare\norchestratedbyDocker-Composewhichexposesasin-\ngle REST/WS endpoint via the open-source R2R frame-\nwork(v3.6.5). Sourcecodeforeverycomponentisavail-\nablein the publicrepository (commit paper-v1).\n1.2.1 Offline knowledge ingestion pipeline\nDuringingestion,r2r_ingest.pyiterativelytraversedthe\ncorpus,eliminatingduplicatetitles,andsegmentedeach\ndocument using an adaptive, progressive-fallback strat-\negy. Initial segmentation employed 300-token chunks\nwith 15-token overlap; if this precise mode failed, chunk\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 7, 2025. ; https://doi.org/10.1101/2025.07.03.25330805doi: medRxiv preprint \nGlaucoRAG: A Retrieval-Augmented Large Language Model for Expert-Level Glaucoma Assessment — 3/6\nsize was reduced to 150 tokens with 10-token over-\nlap, and, if necessary, a final fast mode ingested≈\n1000-token chunks without overlap. Every resulting\nchunk was embedded with the default R2R dense en-\ncoder (OpenAI text-embedding-3-large) and stored in a\nPostgres-backedHNSWvectorindex usingcosinesim-\nilarity. ProvenancewasmaintainedbyrecordingSHA-\n256checksumsforeveryprocessedfile,whiler2r_log-\nging.pygeneratedcolor-coded,timestampedlogsthat\nwerewritten to /app/logs/ingest.log.\n1.2.2 Online inference pipeline\nAtquerytime,theretrievercomponentisinvokedthrough\neither r2r_chat_v4.py or r2r_query.py. R2R conducts\nan approximate nearest-neighbor search that initially re-\nturns20chunks;anoptionalcross-encoderre-ranking\nmodule (r2r_chat_v3_crossencoder.py) rescales similar-\nity scores without introducing external text. The eight\nhighest-rankedpassages( ≈ 3200 tokenstotal)arecon-\ncatenatedwiththeuserpromptandforwardedtoGPT-\n4.5-preview with deterministic generation settings (tem-\nperature 0; max_tokens = 1024). GlaucoRAG’s post-\nprocessing layer resolves numeric citations into human-\nreadable article or textbook titles with page numbers,\nintegrates them into the JSON response, and renders\ntheminlineinthechatinterface. Robustnesssafeguards\nincludeanautomaticretrythatreducescontexttofour\npassages if the language model returns an empty string\normalformedJSON;persistentfailurethentriggersan\nerrormessage to the client.\n1.3 Evaluation\nTwoevaluationtaskswereused: (1)Casereporttask,a\npilot set of 11 de-identified glaucoma case reports were\ndrawn from EyeRounds, the open teaching repository\nof the University of Iowa’s Department of Ophthalmol-\nogy and Visual Sciences[\n19]. Details of selection and\nredaction have been reported previously. Each case\nwaspresentedtoGlaucoRAG,twobaselineLLMs(GPT-\n4.5-PREVIEWandDeepSeek-R1),andthreefellowship-\ntrained glaucoma specialists, who independently sub-\nmitted a single best diagnosis. Reference diagnoses\nweretakenfromtheEyeRoundskey[ 19]. BCSCmultiple-\nchoicequestiontask. Twohundredfiftyglaucoma-focused\nitemswereextractedfromtheBCSCSelf-Assessment\nbank, each consisting of a stem and four or five answer\noptions. ThequestionswerefedtothesamethreeLLMs;\nresponseswere scored automatically against the official\nanswer key. Each item was sent programmatically to\nGPT-4.5-PREVIEW(OpenAIChatCompletions,temper-\nature0,max_tokens256)andDeepSeek-R1(DeepSeek\nAPIv1,temperature0,max_tokens256)asasingle-turn\nprompt containing only the question stem and answer\noptions; the raw JSON responses were parsed automat-\nically to extract the top-ranked answer choice. For both\ntasks, accuracy, expressed as the percentage of cases\nor questions answered correctly (top-1 match for the\ncasereports).\nBeyond conventional accuracy measurements, we\nutilized the RAG assessment (RAGAS) framework for a\nmulti-facetedanalysis. Fourmetricswereassessedby\nRAGAS: (1) Faithfulness, computedby extracting each\nclaim in the answer and checking it against the cited\npassages;(2)ContextPrecision,theshareofretrieved\npassagesthatcontainground-truthevidence;(3)Context\nRecall, the fraction of all ground-truth evidence found in\nthe retrieved set; and (4) Answer Relevancy, a cross-\nencoder similarity score between the user’s question\nandthe generated reply[20,21].\nAll four metrics are reportedon a 0 to 1, with higher\nvaluesindicating better performance.\nAccuracy was expressed as a binomial proportion\n(correct / total). Two-sided 95% confidence intervals\n(CI)were. Differencesbetweenindependentproportions\nwere assessed with a two-tailed two-proportion z-test.\nStatisticalsignificancewasdefinedas p < 0.05. Allanal-\nyseswere performed inPython3.1.\n2. Result\n2.1 Case Report Tasks\nOn the case report task, GlaucoRAG correctly identified\nthe reference diagnosis in 9 of 11, accuracy of81.8%\n(95% CI, 52.3–94.9%). This exceeded the scores of\nstandalone GPT-4.5-PREVIEW (8/11, 72.7%, 95% CI\n43.4–90.3%) and DeepSeek-R1 (7 / 11, 63.6%, 95% CI\n35.4–84.8%). Themeanaccuracyofthethreefellowship-\ntrainedglaucomaspecialistswas ≈ 7/11 cases(66.7%).\n(Table1)\n2.2 BCSC multiple-choice question task\nOutof250glaucoma-focuseditemsfromtheBCSCSelf-\nAssessmentbank,GlaucoRAGanswered228correctly\n(91.2%, 95% CI 87.0–94.1%), versus 211 (84.4%, 95%\nCI79.4–88.4%)forGPT-4.5-PREVIEWand190(76.0%,\n95%CI70.3–80.9%)forDeepSeek-R1. Atwo-sample\nz-test confirmed that the 6.8-point margin over GPT-4.5-\nPREVIEW was statistically significant (p = 0.02), and\nthe 15.2-point margin over DeepSeek-R1 was highly\nsignificant( p < 0.001). (Table 1)\n2.3 RAGAS Quality Metrics\nTable2showsdetailsoftheRAGASqualitymetrics. RA-\nGAS analysis on the 250 BCSC items yielded a faithful-\nnessscoreof0.70,contextprecision0.59,contextrecall\n0.80, and answer relevancy 0.91, indicating that most\nanswerswerewellsupportedbyretrievedevidencewhile\nmaintaininghighrelevancedespitearecall-orientedre-\ntrievalstrategy.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 7, 2025. ; https://doi.org/10.1101/2025.07.03.25330805doi: medRxiv preprint \nGlaucoRAG: A Retrieval-Augmented Large Language Model for Expert-Level Glaucoma Assessment — 4/6\nTask Method Correct / Total Accuracy (95% CI)\nCase reports DeepSeekR1 (baseline) 7/ 11 63.6%(35.4–84.8%)\nGPT-4.5-PREVIEW 8/ 11 72.7%(43.4–90.3%)\nGlaucomaAttending 1 8/ 11 72.7%(43.4–90.3%)\nGlaucomaAttending 2 7/ 11 63.6%(35.4–84.8%)\nGlaucomaAttending 3 7/ 11 63.6%(35.4–84.8%)\nGPT-4.5-PREVIEW+ RAG (GlaucoRAG) 9/ 11 81.8%(52.3–94.9%)\nBCSC Self- Assessment DeepSeekR1 (baseline) 190/ 250 76.00%(70.3–80.9%)\nGPT-4.5-PREVIEW 211/ 250 84.40%(79.4–88.4%)\nGPT-4.5-PREVIEW+ RAG (GlaucoRAG) 228/ 250 91.20%(87.0–94.1%)\nTable 1. Accuracy of glaucoma experts, LLM Models, and GlaucoRAG at assessment of case reports, and\nBCSC self-assessment. LLM:Large languagemodel; GlaucoRAG: Glaucoma specializedRetrieval-Augmented\nGeneration;BCSC: Basic andClinical Science Course;CI: Confidence interval.\nMetrics Values Interpretation\nFaithfulness 0.70 Strongfactual groundingwith minimal hallucinations\nContext Precision 0.59 Moderateretrieval specificity withoptimization potential\nContext Recall 0.80 Comprehensiveknowledge coverage\nAnswer Relevancy 0.91 Excellentquestion-answer semanticalignment\nTable 2. RAGAS evaluation results for GlaucoRAG framework. RAGAS:Retrieval-Augmented Generation\nAssessment;GlaucoRAG: Glaucoma SpecializedRetrieval-Augmented Generation.\n3. Discussion\nInthisstudy,weintroducedandevaluatedaRAGsys-\ntem,GlaucoRAG,designedtointegrateexpertcurated\ntextual knowledge for high accuracy question answer-\ning in glaucoma. Our results demonstrate a statistically\nsignificantimprovementovernon-RAGbaselines. The\nperformance boost observed with GlaucoRAG, 81.8%\non case-based queries and 91.2% on BCSC multiple\nchoicequestions,exceedsthedocumentedperformance\nofnon-RAGophthalmicmodels. Consistentgainsacross\nsmallscaleexpertcuratedquestions(18.2percentage\npoint improvement) and large-scale standardized as-\nsessments (6.8 percentage point improvement) validate\ntherobustness of our approach.\nTheadventofLLMlikeGPT-4.5-PREVIEWhasopened\nnew horizons in ophthalmology, particularly in domains\nthatrequirenuanced,evidence-basedinterpretation,such\nasglaucomamanagement. LLMsholdgreatpromisefor\ndecisionsupport,yettheirutilityinmedicineislimitedby\nwell-known pitfalls, most notably the tendency to “hallu-\ncinate”clinicallyplausiblebutunverifiedstatements,a\nproblemrootedingenerictrainingcorpora,weakdomain\nevaluation standards, and uneven data quality. Routing\ngenerationthrougharetrievallayerlargelyneutralizes\nthisriskbyobligingthemodeltoanchoreverystatement\nincitableevidence,anapproachthatmirrorsthewayclin-\nicians defend their conclusions. In addition, widespread\nadoption of LLMs in medicine still limited by privacy\nconcerns. GlaucoRAGaddressesthesedemandsand\ndemonstratingthatalocallyhosted,cost-effectivesys-\ntem can deliver expert level accuracy while maintaining\ntraceable, evidence-grounded responses. Similar to our\nmodel,Luoet. al. reportedaretrievalaugmentedoph-\nthalmic LLM aligning with expert consensus 84% of the\ntimeversus46%withoutretrieval[ 16],andGPT4alone\nachieved48% accuracy oncomplex cases[22].\nComprehensiveRAGASevaluationrevealednuanced\nsystemperformancecharacteristicsbeyondtraditional\naccuracy metrics. The model achieved a faithfulness\nscoreof70%indicatesthatsevenoutoftengenerated\nresponsesmatchedretrievedsourcecontentfactually,\nproviding significant protection against hallucinations, a\nkeyrequirementforclinicalapplications. Suchascore\nplaces GlaucoRAG firmly within the clinical RAG space,\nwherefaithfulnessvalues 65% aredeemedacceptable.\nNotably,therelativelylowcontextprecision(59%)along-\nside high context recall (80%) shows that our retrieval\npolicyprioritizesbroadknowledgecoverageoverstrin-\ngent precision filtering. This conservative approach min-\nimizesinformationlossbutincreasescontextualnoise.\nNevertheless,an80%recallconfirmsthatourknowledge\nbase supplies the information needed to answer four of\nfive clinical queries. In terms of relevancy, the score\nwas 91% that closely supports the overall accuracy of\n91.2% demonstrating robust question comprehension\nandappropriate responsegeneration.\nDespitethesepromisingcapabilities,severallimita-\ntions merit discussion. First, our current implementation\ndoesnotfuseophthalmicimagingwithtextualdata,an\nessential step for truly robust AI support in glaucoma,\nwhereaccurateimageinterpretationispivotal. Asoph-\nthalmiccareisinherentlymulti-modal,futureworkshould\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 7, 2025. ; https://doi.org/10.1101/2025.07.03.25330805doi: medRxiv preprint \nGlaucoRAG: A Retrieval-Augmented Large Language Model for Expert-Level Glaucoma Assessment — 5/6\ndevelopmulti-modalRAGframeworkscapableofretriev-\ning and reasoning over both textual and imaging data.\nHowever, current LLMs have limited native image un-\nderstandingcapability. Second,althoughRAGmitigates\nthestatic knowledge cutoff of LLMsby injecting external\ndocuments,performanceremainssensitivetoretrieval\nquality;criticalevidencemaybemissedorrankedlow,\nreducing answer fidelity[15,16]. Dynamic, continuously\nupdated knowledgebases will be essential for near real-\ntimeclinical relevance.\nIn conclusion, we have demonstrated that a RAG\napproachsignificantlyenhancesautomatedglaucoma\nquestionansweringacrossexpertcuratedandstandard-\nizedevaluations. TheGPT4.5\n+ R2Rarchitectureoffers\napromising,scalablemodelforevidencegroundedmed-\nicalquestion and answer.\nAcknowledgments\nThisworkwassupportedbyNIHGrantR01EY033005\n(SY). Thefundershad no role in study design, data col-\nlectionandanalysis, decisiontopublish, orpreparation\nofthe manuscript.\nReferences\n[1] Yeganeh Madadi, Mohammad Delsoz, Albert S.\nKhouri, Michael Boland, Andrzej Grzybowski, and\nSiamakYousefi.Applicationsofartificialintelligence-\nenabledrobotsandchatbotsinophthalmology: re-\ncentadvancesandfuturetrends. Current Opinion\nin Ophthalmology , 35 N2 - Purpose of review Re-\ncent advances in artificial intelligence (AI), robotics,\nand chatbots have brought these technologies to\ntheforefrontofmedicine,particularlyophthalmology.\nThese technologies have been applied in diagnosis,\nprognosis, surgical operations, and patient-specific\ncare in ophthalmology. It is thus both timely and\npertinent to assess the existing landscape, recent\nadvances, and trajectory of trends of AI, AI-enabled\nrobots, and chatbots in ophthalmology. Recent find-\nings Some recent developments have integrated AI\nenabled robotics with diagnosis, and surgical pro-\ncedures in ophthalmology. More recently, large lan-\nguage models (LLMs) like ChatGPT have shown\npromise in augmenting research capabilities and\ndiagnosing ophthalmic diseases. These develop-\nments may portend a new era of doctor-patient-\nmachine collaboration. Summary Ophthalmology is\nundergoingarevolutionarychangeinresearch,clin-\nical practice, and surgical interventions. Ophthalmic\nAI-enabledroboticsandchatbottechnologiesbased\non LLMs are converging to create a new era of digi-\ntalophthalmology.Collectively,thesedevelopments\nportendafutureinwhichconventionalophthalmic\nknowledgewillbeseamlesslyintegratedwithAIto\nimprove the patient experience and enhance thera-\npeuticoutcomes.(3), 2024.\n[2] Yeganeh Madadi, Hashem Abu-Serhan, and Sia-\nmakYousefi. DomainAdaptation-Baseddeeplearn-\ningmodelforforecastinganddiagnosisofglaucoma\ndisease. Biomedical Signal Processing and Control ,\n92:106061,June 2024.\n[3] Siamak Yousefi, Louis R. Pasquale, Michael V.\nBoland,andChrisA.Johnson. Machine-Identified\nPatternsofVisualFieldLossandanAssociationwith\nRapidProgressionintheOcularHypertensionTreat-\nment Study.Ophthalmology, 129(12):1402–1411,\nDecember2022.\n[4] Siamak Yousefi, Tobias Elze, Louis R. Pasquale,\nOsamah Saeedi, Mengyu Wang, Lucy Q. Shen,\nSarah R. Wellik, Carlos G. De Moraes, Jonathan S.\nMyers, and Michael V. Boland. Monitoring Glau-\ncomatous Functional Loss Using an Artificial In-\ntelligence–Enabled Dashboard. Ophthalmology,\n127(9):1170–1178,September2020.\n[5] Anshul Thakur, Michael Goldbaum, and Siamak\nYousefi. Predicting Glaucoma before Onset Us-\ning Deep Learning. Ophthalmology Glaucoma ,\n3(4):262–268,July 2020.\n[6] Varun Gulshan, Lily Peng, Marc Coram, Martin C.\nStumpe, Derek Wu, Arunachalam Narayanaswamy,\nSubhashini Venugopalan, Kasumi Widner, Tom\nMadams, Jorge Cuadros, Ramasamy Kim, Rajiv\nRaman, Philip C. Nelson, Jessica L. Mega, and\nDale R. Webster. Development and Validation of a\nDeepLearningAlgorithmforDetectionofDiabetic\nRetinopathy in Retinal Fundus Photographs.JAMA,\n316(22):2402–2410,December2016.\n[7] DanielShuWeiTing,CarolYim-LuiCheung,Gilbert\nLim, Gavin Siew Wei Tan, Nguyen D. Quang, Al-\nfred Gan, Haslina Hamzah, Renata Garcia-Franco,\nIan Yew San Yeo, Shu Yen Lee, Edmund Yick Mun\nWong,CharumathiSabanayagam,ManiBaskaran,\nFarah Ibrahim, Ngiap Chuan Tan, Eric A. Finkel-\nstein, Ecosse L. Lamoureux, Ian Y. Wong, Neil M.\nBressler, Sobha Sivaprasad, Rohit Varma, Jost B.\nJonas,MingGuangHe,Ching-YuCheng,Gemmy\nChui Ming Cheung, Tin Aung, Wynne Hsu, Mong Li\nLee, and Tien Yin Wong. Development and Val-\nidation of a Deep Learning System for Diabetic\nRetinopathy and Related Eye Diseases Using Reti-\nnalImages From MultiethnicPopulations With Dia-\nbetes. JAMA, 318(22):2211–2223, December 2017.\n[8] Michael David Abràmoff, Yiyue Lou, Ali Erginay,\nWarrenClarida,RyanAmelon,JamesC.Folk,and\nMeindert Niemeijer. Improved Automated Detec-\ntion of Diabetic Retinopathy on a Publicly Avail-\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 7, 2025. ; https://doi.org/10.1101/2025.07.03.25330805doi: medRxiv preprint \nGlaucoRAG: A Retrieval-Augmented Large Language Model for Expert-Level Glaucoma Assessment — 6/6\nable Dataset Through Integration of Deep Learn-\ning. Investigative Ophthalmology & Visual Science ,\n57(13):5200–5206,October 2016.\n[9] Yukun Zhou, Mark A. Chia, Siegfried K. Wag-\nner, Murat S. Ayhan, Dominic J. Williamson, Rob-\nbert R. Struyven, Timing Liu, Moucheng Xu, Ma-\nteo G. Lozano, Peter Woodward-Court, Yuka Ki-\nhara, Andre Altmann, Aaron Y. Lee, Eric J. Topol,\nAlastair K. Denniston, Daniel C. Alexander, and\nPearse A. Keane. A foundation model for gener-\nalizablediseasedetectionfromretinalimages. Na-\nture, 622(7981):156–163, October 2023. Publisher:\nNaturePublishing Group.\n[10] Mohammad Delsoz, Hina Raja, Yeganeh Madadi,\nAnthonyA.Tang,BarbaraM.Wirostko,MalikY.Ka-\nhook, and Siamak Yousefi. The Use of ChatGPT\nto Assist in Diagnosing Glaucoma Based on Clin-\nical Case Reports.Ophthalmology and Therapy ,\n12(6):3121–3132,December2023.\n[11] Hina Raja, Xiaoqin Huang, Mohammad Delsoz,\nYeganeh Madadi, Asma Poursoroush, Asim Mu-\nnawar, Malik Y. Kahook, and Siamak Yousefi. Di-\nagnosing Glaucoma Based on the Ocular Hyperten-\nsionTreatmentStudyDatasetUsingChatGenera-\ntive Pre-Trained Transformer as a Large Language\nModel. Ophthalmology Science,5(1):100599,Jan-\nuary2025.\n[12] Yeganeh Madadi, Mohammad Delsoz, Priscilla A.\nLao,JosephW.Fong,T.J.Hollingsworth,MalikY.\nKahook,andSiamakYousefi. ChatGPTAssisting\nDiagnosisofNeuro-OphthalmologyDiseasesBased\non Case Reports.Journal of Neuro-Ophthalmology ,\npage 10.1097/WNO.0000000000002274, April\n2022.\n[13] Xiaoqin Huang, Hina Raja, Yeganeh Madadi, Mo-\nhammadDelsoz,AsmaPoursoroush,MalikY.Ka-\nhook,andSiamakYousefi. PredictingGlaucomaBe-\nfore Onset Using a Large Language Model Chatbot.\nAmerican Journal of Ophthalmology , 266:289–299,\nOctober2024.\n[14] FaresAntaki,SamirTouma,DanielMilad,Jonathan\nEl-Khoury,andRenaudDuval.EvaluatingthePerfor-\nmance of ChatGPT in Ophthalmology: An Analysis\nofItsSuccessesandShortcomings. Ophthalmology\nScience,3(4):100324, December 2023.\n[15] WanZhangandJingZhang.HallucinationMitigation\nforRetrieval-AugmentedLargeLanguageModels:\nAReview. Mathematics,13(5):856,January2025.\nNumber: 5Publisher: MultidisciplinaryDigitalPub-\nlishingInstitute.\n[16] Ming-JieLuo,JianyuPang,ShaoweiBi,YunxiLai,\nJiamanZhao, Yuanrui Shang, Tingxin Cui, Yahan\nYang, Zhenzhe Lin, Lanqin Zhao, Xiaohang Wu,\nDuoru Lin, Jingjing Chen, and Haotian Lin. Devel-\nopment and Evaluation of a Retrieval-Augmented\nLarge Language Model Framework for Ophthal-\nmology. JAMA Ophthalmology , 142(9):798–805,\nSeptember2024.\n[17] NolanTremelling. R2R (Version3.6.5), June 2025.\n[18] ChatGPT-4.5-PREVIEW.\n[19] NayashaMadhanB,HeatherA.StiffM,ZeZhang\nM, Wallace L.M. Alward M, Geoffrey D. Lively M, W.\nL. Alward M, John H. Fingert MP, Brittni A. Scruggs\nMP, Tyler S. Quist M, Nasreen A. Syed M, Rixen J,\nYoungH. Kwon MP. 11Cases. EyeRounds.org.\n[20] ShahulEs,JithinJames,LuisEspinosa-Anke,and\nSteven Schockaert. Ragas: Automated Evalua-\ntion of Retrieval Augmented Generation, April 2025.\narXiv:2309.15217[cs].\n[21] S. Solomon Darnell, Rupert W. Overall, An-\ndrea Guarracino, Vicenza Colonna, Flavia Villani,\nErik Garrison, Arun Isaac, Priscilla Muli, Freder-\nick Muriuki Muriithi, Alexander Kabui, Munyoki Ki-\nlyungi, Felix Lisso, Adrian Kibet, Brian Muhia, Harm\nNijveen, Siamak Yousefi, David Ashbrook, Pengzhi\nHuang, G. Edward Suh, Muhammad Umar, Christo-\npher Batten, Hao Chen, Śaunak Sen, Robert W.\nWilliams, and Pjotr Prins. Creating a biomedical\nknowledgebase byaddressingGPTinaccuratere-\nsponses and benchmarking context, October 2024.\nPages: 2024.10.16.618663Section: New Results.\n[22] Daniel Milad, Fares Antaki, Jason Milad, Andrew\nFarah, Thomas Khairy, David Mikhail, Charles-\nÉdouardGiguère,SamirTouma,AllisonBernstein,\nAndrei-Alexandru Szigiato, Taylor Nayman, Guil-\nlaume A. Mullie, and Renaud Duval. Assessing\nthe medical reasoning skills of GPT-4 in complex\nophthalmology cases.The British Journal of Oph-\nthalmology,108(10):1398–1405, September2024.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 7, 2025. ; https://doi.org/10.1101/2025.07.03.25330805doi: medRxiv preprint \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 7, 2025. ; https://doi.org/10.1101/2025.07.03.25330805doi: medRxiv preprint "
}