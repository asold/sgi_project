{
  "title": "The Advance of GPTs and Language Model in Cyber Security",
  "url": "https://openalex.org/W4384526329",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5011007474",
      "name": "Mingze Gao",
      "affiliations": [
        "Tiangong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W3161207330",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4319079731",
    "https://openalex.org/W3097800629",
    "https://openalex.org/W3009449870"
  ],
  "abstract": "Nature language processing (NLP), one of the most remarkable machine learning techniques currently available, is gaining traction with the public and has achieved great success in many applications. Many companies have developed language models, such as BERT, BART models from Google, and GPT (generative pre-trained transformer) series models from OpenAI. GPT is an unsupervised learning model that generates responses and uses unsupervised pre-training and supervised fine-tuning. GPT-2 is a multitask unsupervised learner that completes tasks using an unsupervised pre-trained model, including a zero-shot setting. GPT-3 extends the few-shot learning approach introduced in GPT, which does not require any gradient updates or fine-tuning for specific tasks. InstructGPT focuses on the alignment that could fit human intention by fine-tuning with human feedback. The outputs of InstructGPT significantly improved in truthfulness and were slightly less toxic than GPT-3, but bias and simple mistakes still existed. This paper aims to provide a detailed overview of the technical advancements utilized in GPT, GPT2, GPT3, and InstructGPT, explore the techniques in different models, and focus on the applications in the cybersecurity aspect. This paper compares the upgrade of GPT models and summarizes the SecureBERT modelâ€™s effects on cyber security.",
  "full_text": "Highlights in Science, Engineering and Technology AIDML 2023 \nVolume 57 (2023)  \n \n195 \nThe Advance of GPTs and Language Model in Cyber Security \nMingze Gao* \nSchool of software, TianGong University, Tianjin, China \n*Corresponding author: 2011670201@tiangong.edu.cn \nAbstract. Nature language processing (NLP), one of the most remarkable machine learning \ntechniques currently available, is gaining traction with the public and has achieved great success in \nmany applications. Many companies have developed language models, such as BERT, BART \nmodels from Google, and GPT (generative p re-trained transformer) series models from OpenAI. \nGPT is an unsupervised learning model that generates responses and uses unsupervised pre -\ntraining and supervised fine-tuning. GPT-2 is a multitask unsupervised learner that completes tasks \nusing an unsupervised pre-trained model, including a zero-shot setting. GPT-3 extends the few-shot \nlearning approach introduced in GPT, which does not require any gradient updates or fine-tuning for \nspecific tasks. InstructGPT focuses on the alignment that could fit human  intention by fine -tuning \nwith human feedback. The outputs of InstructGPT significantly improved in truthfulness and were \nslightly less toxic than GPT-3, but bias and simple mistakes still existed. This paper aims to provide \na detailed overview of the technical advancements utilized in GPT, GPT2, GPT3, and InstructGPT, \nexplore the techniques in different models, and focus on the applications in the cybersecurity aspect. \nThis paper compares the upgrade of GPT models and summarizes the SecureBERT modelâ€™s effects \non cyber security. \nKeywords: Deep learning, Language model, Dialog applications, Cyber security. \n1. Introduction \nDeep learning rapidly developed in 2006. The issue of vanishing gradients during deep network \ntraining was addressed by Hinton, who proposed to initialize the weight of unsupervised pre-training \nwith supervised fine-tuning [1]. In 2012, Hinton's research group developed AlexNet, a CNN -based \narchitecture that achieved first place in the ImageNet Large -Scale Visual Recognition Challenge \n(ILSVRC). \nThe ReLU activation function was first employed in AlexNet, resulting in a significant increase in \nconvergence rate and effectively addressing the vanishing gradient problem [2]. AlexNet used only \nsupervised training and abandoned the \"pre-training + fine-tuning\" approach due to ReLU's ability to \nprevent gradient vanishing, which has since become the mainstream approach in deep learning. \nAdditionally, AlexNet utilized GPU for computing acceleration. \nDeep learning applications have become ubiquitous daily, with examples such as law enforcement \nusing them to analyze transactional data to detect potential criminal activity or fraud. The financial \nservices industry employs predictive analytics powered by deep learning to automate stock trading. \nCustomer service processes in various organizations also incorporate this technology, with chat robots \nlike Apple's Siri, Amazon's Alexa, and OpenAI's Chat GPT being widely utilized [3]. \nLanguage models for dialog applications are becoming more popular these days. As the most eye-\ncatching result of deep learning, the dialogue model has great potential. Developing dialogue systems, \nwhich involve communication between humans and machines, is a challenging yet promising area. \nNeural models have better performances th an traditional machine learning approaches. Google \nScholar search revealed that from 2006 to 2011, there were only 21,100 papers on dialogue systems \nand deep neural networks. However, the number of papers on this topic has increased steadily yearly. \nNatural Language Understanding (NLU) is a complex field involving various diverse tasks, such \nas language modeling, sentiment analysis, and question answering. While an abundance of unlabeled \ntext corpora is available for NLU tasks, obtaining labeled data for specific tasks can be challenging. \nIn the past, a popular approach in Computer Vision was to pre -train generative models on large \nunlabeled image datasets, followed by discriminative fine-tuning for specific tasks. However, because \nHighlights in Science, Engineering and Technology AIDML 2023 \nVolume 57 (2023)  \n \n196 \nof the lack of large-scale labeled datasets such as ImageNet, this method has yet to be widely used in \nNLP. \nRecently, a breakthrough in NLP research has led to the developing of the Generative Pre-trained \nTransformer (GPT) model. GPT is a generative language model that utilizes deep learning techniques \nto learn contextual relationships between words and generate high-quality text. GPT was pre-trained \non a large, unlabeled text corpus, followed by discriminative fine-tuning for specific NLU tasks [4]. \nGPT has quickly become a popular algorithm in the NLP field due to its ability to perform a wide \nrange of tasks with high accuracy, including language modeling, text generation, and language \ntranslation. Its success can be attributed to its ability to understand the context and meaning of words \nin a sentence, allowing it to generate high-quality, grammatically correct, and semantically coherent \ntext. \nThe main content of this article is a comprehensive review of the technical update of Open AI \ncompany's GPT series models and the security-related topics of the language dialogue model. \n2. Preliminaries \nGPT is based on the Transformer [4]. Transformer is a simple sequence transduction model \nnetwork architecture based on attention mechanisms that connect the encoder and decoder [5]. The \nTransformer is a novel transduction model that utilizes only self -attention mechanisms to generate \ninput and output representations without relying on traditional sequence -aligned RNNs or \nconvolutional methods. \nThe encoder-decoder structure has more competitive edges. The encoder maps an input sequence \n(x_1,â€¦, x_n) to a sequence of z=(z_1,â€¦, z_n), z_t is the vector representation of x_t which could \nunderstand by the machine. Then transport the z sequence to the decoder to generate an output y \nsequence (y_1,â€¦, y_m). Every step of the model is auto-regressive. \nTransformer consists of N=6 identical layers, each comprising two sub -layers: a multi -head \nattention mechanism and a fully connected feedforward network. The primary purpose of this \nstructure is to extract features from the input sequence and transform them into a lower-dimensional \nrepresentation to better capture important information in the sequence and reduce the impact of \nredundant information. To ensure optimal performance, residual connections and layer normalization \nwere incorporated around each sub-layer. The outputs of all sub-layers and embedding layers are of \ndimension d_{model}=512. The final output of the encoder can be represented as LayerNorm(x + \nSublayer(x)), where x is the input and Sublayer(x ) represents the function implemented by the \nrespective sub-layer. Therefore, the conclusion is that the encoder in a neural network consists of 6 \nlayers, each having two sub -layers with residual connection and layer normalization to improve \nperformance. The output of the encoder can be represented as LayerNorm(x + Sublayer(x)), where x \nis the input and Sublayer(x) represents the function implemented by the respective sub-layer [5]. \nBatchNorm is used to normalize the dimensions of the batch, that is, to ope rate on the same \ncharacteristics of different samples. LayerNorm is to normalize hidden dimensions, that is, to operate \non different features of a single sample. So LayerNorm is not limited by the number of samples. \nBatchNorm is to count all the samples in  each dimension and calculates the mean and variance; \nLayerNorm is just taking all the dimensions on each sample and calculating the mean and variance. \nFor the NLP field, because samples are often different sentences with different lengths, so it is more \nsuitable to use LayerNorm. \nThe decoder has a similar architecture to the encoder, including three sub -layers: a Multi -Head \nAttention mechanism, a fully connected feedforward network, and a third sub-layer called the masked \nmulti-head attention mechanism [5] . During training, the decoder is trained to output the result for \nthe current moment without access to the inputs beyond that moment. This approach ensures \nconsistency in the model's training and prediction behaviors, as the decoder cannot access information \nfrom future time steps that would not be available during actual prediction. Therefore, the conclusion \nis that the decoder in a neural network has three sub-layers, including the masked multi-head attention \nHighlights in Science, Engineering and Technology AIDML 2023 \nVolume 57 (2023)  \n \n197 \nmechanism, and generates output for each mome nt during training without access to future inputs, \nensuring consistency in the model's training and prediction behaviors. \nThe attention function generates a set of output vectors based on a query and a set of key -value \npairs. A compatibility function between the query and the corresponding key determines the weights \nassigned to each value. The input to the attention mechanism consists of queries and keys of \ndimension d_k and values of dimension d_v. The similarity between the query and the value is \ncomputed using a dot product and divided by \\sqrt{d_k}. The softmax function is then applied to \nobtain the weights assigned to each value, which are used to compute the weighted sum of the values \nto generate the output vectors [5]. Therefore, the attention functi on generates a set of output vectors \nbased on a query and a set of key-value pairs, where the weights assigned to each value are determined \nby a compatibility function, and the similarity is computed using a dot product before applying the \nsoftmax function to obtain the weights. \n3. Development of Chat Generative Pre-trained Transformer \n3.1. GPT \nThe OpenAI research team demonstrated the effectiveness of a semi -supervised approach that \ncombines unsupervised pre -training with supervised fine -tuning for NLP tasks. They  achieved \nnotable performance improvements using generative pre -training of a language model on various \nunlabeled text corpora followed by discriminative fine -tuning on certain tasks. This approach has \nshown that general task -agnostic models can outperform  discriminatively trained models that use \ntask-specific architectures. However, leveraging more word-level information from unlabeled text is \nstill challenging. The OpenAI team employed the Transformer architecture, which has a more \nstructured memory for h andling long -term dependencies in text, resulting in robust transfer \nperformance across diverse tasks. Performance is greatly improved by using an iterative approach to \ntransfer the process for task -specific input adaptation. In conclusion, the semi -supervised approach \ncombining unsupervised pre -training with supervised fine -tuning, along with the Transformer \narchitecture and task-specific input adaptations, has shown to be an effective method for NLP tasks. \nThe unsupervised pre-training on the model, GPT was given an unsupervised corpus of tokens U \n= {ğ‘¢1, . . . , ğ‘¢ğ‘›}, the sequence of the text wonâ€™t change. To maximize the likelihood, GPT employs \na standard language modeling objective [5]: \nğ¿1(ğ‘¢) = âˆ‘ ğ‘™ğ‘œğ‘”ğ‘ƒâ¡(ğ‘¢ğ‘–|ğ‘¢ğ‘–âˆ’ğ‘˜,â€¦,ğ‘¢ğ‘–âˆ’1;ğœƒ)â¡ğ‘–          (1) \nThe language model uses a multi -layer Transformer decoder to predict the probability of the i -th \nword appearing, with a context window size of k. The probability is modeled with parameters Î˜, \nwhich are trained through stochastic gradient descent [5]. \nTo generate a di stribution of output tokens, this model employs a multi -headed self -attention \nmechanism over the input context tokens, followed by position-wise feedforward layers [6]. \nâ„0 = ğ‘ˆğ‘Šğ‘’ +ğ‘Šğ‘ \nâ„ğ‘™ = ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘“ğ‘œğ‘Ÿğ‘šğ‘’ğ‘Ÿ_ğ‘ğ‘™ğ‘œğ‘ğ‘˜(â„ğ‘™âˆ’1)âˆ€â…ˆğœ–[1,ğ‘›]                 (2) \nğ‘ƒ(ğ‘¢) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(â„ğ‘›ğ‘Šğ‘’\nğ‘‡) \nFor unsupervised fine -tuning, the model utilized a labeled dataset C containing input sequences \nğ‘¥1, ..., ğ‘¥ğ‘š and their corresponding label y. The model predicted the probability of y appearing in \nthe data given from 1 to m [5]: \nğ‘ƒ(ğ‘¦|ğ‘¥1,â€¦,ğ‘¥ğ‘š) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(â„ğ‘™\nğ‘šğ‘Šğ‘¦)                      (3) \nThen maximize the true probability: \nğ¿2(ğ¶) = âˆ‘ ğ‘™ğ‘œğ‘”ğ‘ƒ(ğ‘¦|ğ‘¥1,â€¦,ğ‘¥ğ‘š)(ğ‘¥,ğ‘¦)                       (4) \nHighlights in Science, Engineering and Technology AIDML 2023 \nVolume 57 (2023)  \n \n198 \nAdding language modeling as a secondary task to the fine-tuning process can improve learning by \nenhancing the generalization ability of the supervised model and accelerating convergence. L1 is \nresponsible for predicting the next word of a given word sequence, while L2 generates the complete \nsequence and predicts its label. \nğ¿3(ğ¶) = ğ¿2(ğ¶)+ğœ†âˆ—ğ¿1(ğ¶)                        (5) \nThen is to consider how to make the specific tasks into the form that people accept, like a sequence \nand corresponding labels. \n3.2. GPT-2 \nGPT-2, a 1.5B parameter Transformer, was trained on a large and diverse dataset of  web pages \ncalled WebText. This unsupervised multitask learner demonstrated the ability to learn various \nlanguage tasks without explicit supervision. GPT -2 also introduced a zero -shot setting and \noutperformed its predecessor, GPT, in supervised tasks by le veraging the pre-training learned from \nunsupervised data. [6]. \nThe prevailing method for developing machine learning (ML) systems involves gathering training \nexamples that exhibit the correct behavior for a given task. The major disadvantage of this approa ch \nis the need for generalization, and it could not use by other models directly. Multitask learning in \nGPT-2 could train a model on multiple datasets simultaneously. The model could be used in many \ntasks by adding another loss function. That means the GPT -2â€™s generalization is significantly \nimproved. \nMultitask learning did not widely use in the NLP field. The best -performing systems at that time \non language tasks utilized a combination of pre -training and supervised fine -tuning. GPT -2 \ndemonstrated that lan guage models could perform downstream tasks without parameter or \narchitecture modification in a zero-shot setting. \nGPT pre-trained the language model on natural text, but for the downstream tasks, they constructed \nthe input and added a start, end, and deli miter. These symbols should have been mentioned in the \nmodel before. Because of fine-tuning, the model would eventually recognize these symbols. However, \nfor GPT-2, in the downstream, the model could not be adjusted for zero-shot, so we were not allowed \nto input symbols the model had not learned, but the input form was more like a natural language. The \ntraining dataset used a large amount of data from Reddit, which received at least three karma. \n3.3. GPT-3 \nGPT-3 attempted to evaluate the efficacy of GPT -2 by exploring the few-shot learning approach \nproposed in GPT, which involves providing a limited number of examples to the language model to \nsave resources. With an impressive 175 billion parameters, this model can be utilized without fine -\ntuning task-specific objectives or gradient updates. GPT-3 generated news article samples that were \nindistinguishable from those written by humans, leaving observers unable to discern whether the \narticle was created by a human or artificial intelligence [7].  \nUsing pre-trained language representations in NLP systems have limitation. The first problem is \nthat the large dataset that needs to be labeled is complicated, as mentioned in GPT-2. The second one \nis that when the example was not in data distribution, the generalization may  be worse than the \nprevious models. It is easy to overfit a large model on small data. The third issue is that humans only \nrequire a few datasets to accomplish tasks. \nGPT-3 is evacuated under three methods: 1) â€œfew -shot learning,â€ which provides every task  \ntypically 10 -100 examples to fit the context window. 2) â€œOne -shot learningâ€ offered only one \ndemonstration. 3) â€œzero-shotâ€ learning [8]. Figure 2 shows the difference between the three conditions: \nHighlights in Science, Engineering and Technology AIDML 2023 \nVolume 57 (2023)  \n \n199 \n \nFig. 2 Aggregate performance for all 42 accuracy-denominated benchmarks [7]. \n3.4. InstructGPT \nBoth InstructGPT and ChatGPT adopt the GPT-3 network structure to guide the learning to build \ntraining samples to train the reward model reflecting the effect of the predicted content and to use the \nscore of this reward mod el to guide the training of the reinforcement learning model. InstructGPT \nfocuses on the alignment that could fit human intention by fine -tuning with human feedback. They \ncollected many problems on OpenAI API and used the label tools to create prompts. Then, the dataset \nof rankings of model outputs was collected, and reinforcement learning was used to fine -tune the \nsupervised model. The result of the 1.3B parameter model is better than the 175B GPT -3 model in \noutput truthfulness and reductions in toxic output [8]. \nThe large language model could input tasks by the approach â€œprompt.â€ Nevertheless, these models \nmay exhibit unintended behaviors, including generating false information, producing discriminatory \nor harmful language, or failing to comply with user d irectives. There are differences between \npredicting the next token from the internet web pages and following usersâ€™ instructions helpfully and \nsafely. This is because objective functions used to train the models are often misaligned. InstructGPT \nwas trained to output honest, helpful, and harmless answers . \n \nFig. 3 A diagram illustrating the three steps training method [9]. \nAccording to Figure 3, the prompt method involves three steps. Firstly, a set of questions or \nprompts are selected and answered manuall y by labeling them, and this dataset is used to train the \nSFT model. Secondly, the trained SFT model is used to generate outputs, which are then scored \n\nHighlights in Science, Engineering and Technology AIDML 2023 \nVolume 57 (2023)  \n \n200 \nmanually, and this dataset is used to train the RM model. Thirdly, the RM model's output improves \nthe SFT model. \nAs a result of this process, the outputs generated by InstructGPT were significantly more truthful \nand slightly less toxic (with about 25% fewer toxic outputs) than those generated by GPT-3. However, \nbias and simple mistakes still exist. \nTo ensure a diverse range of prompts, labelers wrote three kinds: arbitrary tasks, instructions with \nmultiple query/response pairs, and prompts corresponding with use cases stated in waitlist \napplications. These prompts were placed on a playground interfac e as a dataset, and different users' \nprompts were collected and classified by user ID to reduce repetitive problems. This process aimed \nto improve the model's performance. \nPrompts generated three datasets: the SFT dataset, which was used to train the SFT models and \ncontained labeler demonstrations; the RM dataset, which contained labeler rankings of model outputs \nused to train the RM models; and the PPO dataset, which was no t human-labeled and was used as \ninputs for RLHF fine-tuning. About 40 contractors were hired as labelers after taking a test to ensure \nthe quality of data. During the labeling process, labelers were asked to prioritize the helpfulness of \nthe user, and in t he final evaluation stage, labelers were asked to prioritize truthfulness and \nharmlessness. \n4. Cyber Security Application \nAfter review about upgrading the GPT series models' technique, this part focuses on the language \nmodel's application in the cyber security industry, trying to find out how the language model affects \nnetwork security. This paper finds the model SecureBERT as a typical research objective. \nSecureBERT is a language model based on the transformer architecture built upon RoBERTa. Its \nmain focus is processing cybersecurity-related text language. The model adapts BERT, a commonly \nused language model in natural language processing. \nThe SecureBERT model is trained on many text corpus related to network security, including \nsecurity reports, vulnerability descriptions, and security blogs. This allows it to understand \ncybersecurity's specific language and terminology and identify patterns and relationships between \ndifferent security threats [9]. \nSecureBERT offers the benefit of automating numerous cyberse curity tasks, including but not \nlimited to identifying threats, assessing vulnerabilities, and responding to incidents. By understanding \nthe language and conte xt of cybersecurity, SecureBERT can help to identify potential threats and \nvulnerabilities more quickly and accurately than traditional approaches. \nWhen developing SecureBERT, new markers are built using already-trained English markers, and \nthe training weights are then changed to enhance the learning process. SecureBERT can automate \nvarious cybersecurity tasks with this method, including threat detection, vulnerability analysis, and \nincident response. The named entity recognition (NER) task and the Masked Language Model (MLM) \nchallenge were used to test SecureBERT's performance. \n5. Discussion \nAlthough GPT has some effects on undebugged tasks, its generalization ability is far lower than \nfine-tuned supervised tasks, so GPT is a fairly good language understanding tool rather than a \nconversational AI. \nGPT-2 does not introduce significant structural innovatio ns or changes to the original network \narchitecture. Instead, it utilized a larger number of network parameters and larger datasets. The model \ncontains up to 48 layers and 1.5 billion parameters, and it is trained using unsupervised pre -training \nfor supervised tasks. GPT-2 has shown exceptional abilities in various natural language generation \ntasks, such as summarization, chatbots, story-writing, and even generating fake news, phishing emails, \nHighlights in Science, Engineering and Technology AIDML 2023 \nVolume 57 (2023)  \n \n201 \nor role -playing scripts. Its performance has been shown to outper form other models in multiple \nspecific language modeling tasks, demonstrating its universal power. [6]. \nAs an unsupervised or self -supervised model, GPT -3 can carry out various NLP tasks, such as \nautomatic question answering, problem-focused search, comprehension of texts, pragmatic Inference, \nand automated translation. It has demonstrated impressive performance on a variety of tasks, \nincluding achieving cutting -edge outcomes in French -English and German -English automated \ntranslation tasks, automatically pr oducing articles that are nearly impossible to distinguish from \nhuman or machine (only 52% accuracy and random guess), and more surprisingly, achieving nearly \n100% accuracy in double-digit addition and subtraction tasks. It can even generate code automatically \nfollowing the task description [7]. There is potential for universal AI in the many consequences of an \nunsupervised model. \nA strong and dependable framework is crucial for processing cybersecurity text due to the high \nstakes involved in this field. Language models must have a thorough comprehension of the meanings \nand subtleties of words and sentences in order to handle cybersecurity text efficiently. As a result, the \nmodel can accurately identify potential threats in real-time and take appropriate action. This includes \nthe capacity to analyze the semantics at both the word and phrase levels. It is also essential to use a \nsufficiently generic model to cater to various cybersecurity tasks, such as malware analysis, intrusion \ndetection, phishing detection, and code analysis. By utilizing a robust framework and generic model, \ncybersecurity professionals can accurately and efficiently process and analyze cybersecurity text [9].  \nDeep learning and language models have been successfully applied to various natu ral language \nprocessing tasks, including dialogue applications such as chatbots and virtual assistants. However, \nthere are several pitfalls that researchers and practitioners should be aware of when developing and \ndeploying these systems. \nLarge amounts of high-quality training data are required for successful dialogue applications. Deep \nlearning models require large datasets to learn from, but collecting and annotating such data can be \ncostly and time -consuming. Furthermore, deep learning models can still n eed help generalizing to \nnew situations, even with large amounts of data. They may be prone to overfitting, where they \nmemorize the training data rather than learn to generalize [10]. \nAnother challenge is the potential for bias and ethical issues in dialog ue applications. Language \nmodels are trained on large datasets that may contain biases and stereotypes, and these biases can be \namplified in dialogue applications. For example, a chatbot trained on data that contains sexist or racist \nlanguage may inadvertently reproduce these biases in its responses. It is important for researchers and \npractitioners to consider the sources and quality of training data carefully and to monitor and mitigate \npotential biases in their models [11]. \nIn addition, deep learning mod els can be computationally expensive and require significant \ncomputing resources. This can make it challenging to deploy these models in resource -constrained \nenvironments, such as mobile devices or low-power edge devices. Researchers and practitioners must \nconsider the trade -offs between model accuracy and efficiency when designing and deploying \ndialogue applications. \nFinally, deep learning models can take time to interpret and explain. This can be a problem in \ndialogue applications where users may want to understand how the system arrived at a particular \nresponse. Researchers and practitioners need to develop methods for interpreting and explaining the \ndecisions made by these models in order to build trust and transparency with users. \n6. Conclusion \nThis paper comprehensively reviews the technological upgrade from GPT to InstructGPT and \nintroduces a language model application in cyber security called SecureBERT. GPT is designed to \nobtain a general text model using pre -training techniques through Transformer as t he basic model. \nGPT builds pre -training tasks from left to right and then gets a general pre -training model. GPT -2 \nuses models with more parameters and training data, and researchers propose that all supervised \nHighlights in Science, Engineering and Technology AIDML 2023 \nVolume 57 (2023)  \n \n202 \nlearning is a subset of the unsupervised language model. This idea is also the obvious predecessor of \nprompt learning. GPT-3 has eight different models with parameters ranging from 125 million to 175 \nbillion. It is an autoregressive model using a decoder-only architecture, using the next word to predict \nthe target. InstructGPT focuses on alignments with human feedback and gets the score to direct the \nreinforcement learning model from the reward model. The evaluation of SecureBERT demonstrated \npromising results in grasping cybersecurity language, which means it could handle most cyber attacks. \nReferences \n[1] Hinton, G. E. Reducing the Dimensionality of Data with Neural Networks. Science, vol. 313, no. 5786, \n28 July 2006, pp. 504â€“507. \n[2] Krizhevsky, Alex, et al. ImageNet Classification with Deep Convolutional N eural Networks. \nCommunications of the ACM, vol. 60, no. 6, 24 May 2012, pp. 84â€“90. \n[3] Ni, Jinjie, et al. Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey. \nArtificial Intelligence Review, 20 Aug. 2022. \n[4] Openai, Alec, et al. Improving Language Understanding by Generative Pre-Training. 2018. \n[5] Vaswani, Ashish, et al. Attention Is All You Need. Neural Information Processing Systems, Curran \nAssociates, Inc., 2017. \n[6] Radford, Alec, et al. Language Models Are Unsupervised Multitask Learners. 2019. \n[7] Brown, Tom, et al. Language Models Are Few-Shot Learners. Advances in Neural Information Processing \nSystems, vol. 33, 2020, pp. 1877â€“1901. \n[8] Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. Advances \nin Neural Information Processing Systems, vol. 35, 6 Dec. 2022, pp. 27730â€“27744. \n[9] Aghaei, Ehsan, et al. SecureBERT: A Domain-Specific Language Model for Cybersecurity. Lecture Notes \nof the Institute for Computer Sciences, Social Informatics and Telecommunication s Engineering, 2023, \npp. 39â€“56. \n[10] Gritta, Milan, et al. Conversation Graph: Data Augmentation, Training and Evaluation for Non -\nDeterministic Dialogue Management. ArXiv:2010.15411 [Cs], 4 Nov. 2020. \n[11] Kafai, Yasmin, et al. From Theory Bias to Theory Dialogue. A CM Inroads, vol. 11, no. 1, 13 Feb. 2020, \npp. 44â€“53. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7401378750801086
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6387274265289307
    },
    {
      "name": "Transformer",
      "score": 0.607831597328186
    },
    {
      "name": "Machine learning",
      "score": 0.5656371116638184
    },
    {
      "name": "Upgrade",
      "score": 0.5524706244468689
    },
    {
      "name": "Language model",
      "score": 0.5296820402145386
    },
    {
      "name": "Generative grammar",
      "score": 0.4391128420829773
    },
    {
      "name": "Generative model",
      "score": 0.43819957971572876
    },
    {
      "name": "Engineering",
      "score": 0.13373371958732605
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I198091727",
      "name": "Tiangong University",
      "country": "CN"
    }
  ],
  "cited_by": 4
}