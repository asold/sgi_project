{
    "title": "Scalable Transformers for Neural Machine Translation",
    "url": "https://openalex.org/W3128371468",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2109793426",
            "name": "Gao Peng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746964385",
            "name": "Geng, Shijie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2052563777",
            "name": "Qiao Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1832420164",
            "name": "Wang Xiaogang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2345450207",
            "name": "Dai, Jifeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1969345873",
            "name": "Li Hongsheng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2962746461",
        "https://openalex.org/W2126807068",
        "https://openalex.org/W2963766446",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3002557610",
        "https://openalex.org/W2903707108",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2953333557",
        "https://openalex.org/W2562731582",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2964268168",
        "https://openalex.org/W3124540476",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2767989436",
        "https://openalex.org/W2981698279",
        "https://openalex.org/W2912521296",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W3088313020",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2767421475",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2963736842"
    ],
    "abstract": "Transformer has been widely adopted in Neural Machine Translation (NMT) because of its large capacity and parallel training of sequence generation. However, the deployment of Transformer is challenging because different scenarios require models of different complexities and scales. Naively training multiple Transformers is redundant in terms of both computation and memory. In this paper, we propose a novel Scalable Transformers, which naturally contains sub-Transformers of different scales and have shared parameters. Each sub-Transformer can be easily obtained by cropping the parameters of the largest Transformer. A three-stage training scheme is proposed to tackle the difficulty of training the Scalable Transformers, which introduces additional supervisions from word-level and sequence-level self-distillation. Extensive experiments were conducted on WMT EN-De and En-Fr to validate our proposed Scalable Transformers.",
    "full_text": "Scalable Transformers for Neural Machine Translation\nPeng Gao†, Shijie Geng‡, Yu Qiao⋄, Xiaogang Wang†, Jifeng Dai§, Hongsheng Li†\n†Multimedia Laboratory, The Chinese University of Hong Kong ‡Rutgers University\n⋄Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences §SenseTime Research\n1155102382@link.cuhk.edu.hk sg1309@rutgers.edu\nyu.qiao@siat.ac.cn daijifeng@sensetime.com\n{xgwang, hsli}@ee.cuhk.edu.hk\nAbstract\nTransformer has been widely adopted in Neu-\nral Machine Translation (NMT) because of\nits large capacity and parallel training of se-\nquence generation. However, the deployment\nof Transformer is challenging because differ-\nent scenarios require models of different com-\nplexities and scales. Naively training multi-\nple Transformers is redundant in terms of both\ncomputation and memory. In this paper, we\npropose a novel Scalable Transformers, which\nnaturally contains sub-Transformers of differ-\nent scales and have shared parameters. Each\nsub-Transformer can be easily obtained by\ncropping the parameters of the largest Trans-\nformer. A three-stage training scheme is\nproposed to tackle the difﬁculty of training\nthe Scalable Transformers, which introduces\nadditional supervisions from word-level and\nsequence-level self-distillation. Extensive ex-\nperiments were conducted on WMT EN-De\nand En-Fr to validate our proposed Scalable\nTransformers.\n1 Introduction\nTransformers (Vaswani et al., 2017) have demon-\nstrated its superior performance on Machine Trans-\nlation (NMT) (Wu et al., 2016; Vaswani et al.,\n2017), Language Understanding (Devlin et al.,\n2018; Brown et al., 2020), Image Recognition (Tou-\nvron et al., 2020), and Visual-linguistic Reason-\ning (Gao et al., 2019a; Lu et al., 2019; Gao et al.,\n2019b). However, the scale (number of parame-\nters and FLOPs) of the Transformers cannot be\naltered once trained. This is contradictory to the\ndifferent scenarios of NMT which need models of\ndifferent scales. For instance, NMT systems on a\nsmartphone should have lower computational cost\nwhile those on clusters aim to achieve higher accu-\nracy. A naive approach would be to separately train\nmodels of different scales. The Transformer of de-\nsired scale is then deployed to the target scenario.\nTransformer\nN Layers\nD_min =\n256\nD_min =\n256\nD_max = 1024\nD_max = 1024\nChannel Depth\nD =\nRandom(min, max)\nD =\nRandom(min, max)\nHas\nThis\nIdea\nWorked?\nWeight \nLinear\nProjection\ninout\nLinear  Projection\nin\nout\nWeight \nLinear\nProjection\ninout\nWeight \nDistillation\nLoss\nDistillation\nLoss\nLogit\nCropCrop\nC_min C_random C_max\nFigure 1: Illustration of our proposed Scalable Trans-\nformer. After trained by the three-stage self-distillation\ntraining scheme, the Scalable Transformer contains a\nseries of weight-sharing sub-Transformers for machine\ntranslation. The sub-Transformers with lower FLOPs\ncan be easily obtained by cropping out from the widest\nTransformer without the need of re-training, thus sav-\ning redundant training and memory storage.\nHowever, such a strategy requires training and stor-\ning multiple Transformers. A natural question is:\ncan we build a single but Scalable Transformers\nthat can be ﬂexibly scaled up or down without re-\ntraining to run at different FLOPs, without sac-\nriﬁcing the translation accuracy at corresponding\nFLOPs?\nIn this paper, we propose a Scalable Transform-\ners (ST) that can adjust feature dimensions of all\nencoder and decoder layers within a large range\nof widths (from 256 to 1024) without re-training.\nThe largest model in our Scalable Transformers has\n1024 feature dimensions at all layers, and 6 encoder\nand 6 decoder layers. After properly training, its\nneural layers can be cropped to form sub-models.\nFor example, activating the ﬁrst 512 dimension\nin each layer, we would obtain a sub-Transformer\nwith 1/4 of its full parameters but can still perform\narXiv:2106.02242v2  [cs.CL]  18 Jun 2021\ntranslation accurately with limited performance\ndrop. The sub-models of different scales share\nparameters with the largest model.\nHowever, properly training the parameter-\nsharing sub-Transformers of different scales is non-\ntrivial. Jointly training them results in worse per-\nformance than independently-trained counterparts,\ndue to the interference between the parameter-\nsharing models. To solve the issue, we propose to\nincorporate online word-level and ofﬂine sequence-\nlevel self-distillation (Kim and Rush, 2016) to ef-\nfectively supervise the training of the Scalable\nTransformers. During training, we randomly sam-\nple Transformers of different scales. The word-\nlevel predictions generated by the largest Trans-\nformer would serve as the additional supervisions\nfor self-training smaller-scale sub-models with\nshared parameters. The training inference be-\ntween sub-models can be mitigated by this strat-\negy from two aspects. On the one hand, since the\nsmall sub-models share all their parameters with\nthe largest Transformer, the predictions generated\nfrom the largest Transformer are easier to ﬁt than\nthe hard ground-truth supervisions. On the other\nhand, if the smaller sub-models are better trained,\nthey would in turn enhance the performance of\nthe largest model as their parameters are all in-\ncluded in the largest model. We then generate of-\nﬂine sequence-level predictions from the largest\nTransformer as supervisions for further ﬁnetuning\nthe smaller-scale sub-models. After three stage\ntraining, all Transformers reach their optimal per-\nformances. Our Scalable Transformers have been\ntested on WMT’14 En-De and En-Fr benchmarks.\nAll sub-models achieve better or comparable per-\nformances than their independently trained coun-\nterparts but with fewer overall parameters.\nOur contributions can be summarized as three-\nfold:\n• We present a novel Scalable Transformers which\ncan dynamically scale across a wide spectrum of\nparameters and FLOPs with shared parameters.\n• We propose a novel three-stage training strat-\negy for the Scalable Transformers, which include\nonline word-level and ofﬂine sequence-level self-\ndistillation for mitigating training interference\nbetween the sub-models.\n• We perform extensive experiments on WMT’14\nEn-De and En-Fr datasets, which show\nfavourable performances of our Scalable Trans-\nformers at different scales.\n2 Related Work\n2.1 NMT Architectures\nStatistical Machine Translation (SMT) (Forcada\net al., 2011) dominated NMT in the early years.\nSeq2Seq (Sutskever et al., 2014) then surpassed\nSMT and has become the mainstream for NMT.\nThe main recent research turned to design architec-\ntures for seq2seq. NMT architectures evolved from\nLSTM (Hochreiter and Schmidhuber, 1997), CNN\n(Gehring et al., 2017), DepthwiseCNN (Kaiser\net al., 2017) to Transformer (Vaswani et al., 2017).\nThe Evolved Transformer (So et al., 2019) con-\nducted architecture search using evolution algo-\nrithms and achieved good performance on NMT.\nUniversal Transformer (Dehghani et al., 2018) pro-\nposed to share all layers of the encoder and decoder.\nScaling Neural Machine Translation (SNMT) (Ott\net al., 2018) performs extensive ablation study on\ntraining the Transformer. Previous NMT research\nfocuses on architectures and training tricks while\nour Scalable Transformers is proposed to build an\narchitecture that can be ﬂexibly adjusted to meet a\nlarge spectrum of resource constraints with a single\nmodel.\n2.2 Distillation and Self-distillation\nKnowledge Distillation (KD) (Hinton et al., 2015)\nwas ﬁrst proposed to transfer knowledge between\nteacher and student networks. DistillBERT (Sanh\net al., 2019) adopts KD to train a small BERT\nmodel given a large BERT model. Sequence-level\ndistillation (Kim and Rush, 2016) generalize KD\nto sequence generation. KD generally transfers\nknowledge between teacher and independent stu-\ndents different capacities. There are also self-\ndistillation methods (Yang et al., 2019b; Xie et al.,\n2019; Yang et al., 2019a; Geng et al., 2021) that use\nits previous versions to provide additional supervi-\nsions for improving itself. In contrast, we generate\nboth online soft and ofﬂine hard supervisions from\nthe largest model to train its parameter-sharing sub-\nmodels.\n2.3 Adaptive Neural Network\nTradition neural networks have ﬁxed architectures\nand computation complexity in training and test-\ning. Recently, neural networks with dynamic com-\nputation scales have been proposed in NLP and\ncomputer vision. Huang et al. (2016) proposed\na stochastic depth training strategy for convolu-\ntion neural networks, where residual layers are\nrandomly dropped. Huang et al. (2017); Graves\n(2016); Figurnov et al. (2017); Graves (2016) pro-\nposed adaptive computational mechanism for RNN,\nCNN and Transformer. For adaptive computation,\nthe calculation would stop if the conﬁdence score\nat certain layer is higher than a threshold. Adaptive\ncomputation can therefore early terminate the cal-\nculation for saving cost on easy-to-predict samples.\nGao et al. (2020) iteratively cascade weight-shared\ntransformer for deeper neural machine translation.\nSlimmable Neural Network (Yu et al., 2018; Yu\nand Huang, 2019) was proposed to train width\nadjustable Convolution Neural Network (CNN).\nAlthough both slimmable NN and our Scalable\nTransformers adjust layer width to achieve differ-\nent model capacities. There are two key differ-\nences. (1) Slimmable NN can only support equal\nlayer widths for all layers while our Scalable Trans-\nformers allows ﬂexibly setting different widths for\ndifferent layers. (2) We handle training interference\nvia a novel self-distillation training strategy while\nslimmable NN uses a small set of separate BN pa-\nrameters, which are not feasible for our Scalable\nTransformers with much freedom.\n2.4 Parameter Sharing\nParameter sharing between a network and its sub-\nnetworks (Pham et al., 2018) has been investi-\ngated for Neural Architecture Search (NAS). Dif-\nferent from Pham et al. (2018), we aim to train all\nsub-network jointly with the largest network and\nachieve good performance with self-distillation.\n3 Scalable Transformers\nIn this section, we ﬁrst brieﬂy revisit the Trans-\nformer for machine translation. We then explain\nhow to modify conventional Transformer to imple-\nment our Scalable Transformers (ST), which can\nﬂexibly choose its feature dimensions to match dif-\nferent target scales once trained (see Figure 2 for il-\nlustration). Since the training of the Scalable Trans-\nformers is non-trivial, we propose a three-stage\ntraining strategy, which consists of independent\ntraining (stage 1), online word-level self-distillation\n(stage 2), ofﬂine sequence-level self-distillation\n(stage 3).\n3.1 A Revisit of Transformer for Machine\nTranslation\nEmbedding. Let S ∈RN×L denote a source sen-\ntence of length L, where each word is represented\nby an one-hot vector of vocabulary size N. The\ninput sentence Sare encoded by word embedding\nWe as E = WeS, where We ∈RN×C.\nTransformer layer.Each Transformer layer con-\ntains one multi-head attention sub-layer followed\nby FFN sub-layer. The input features (word em-\nbedding Efor the 1st layer) are linearly projected\ninto key, query, value,K,Q,V ∈RL×D and prop-\nagated between positions as\nK = WkE+ bk,\nQ= WqE+ bq, (1)\nV = WvE+ bv,\nAttention(Q,K,V ) = softmax\n(\nQKT /\n√\nd\n)\nV,\nwhere Wk,Wq,Wv ∈ RC×D and bk,bq,bv ∈\nRD. The multi-head attention further splits the\nQ,K,V features along the channel dimension to\nform groups of features. Another linear projec-\ntion converts the attention output feature back to\nC-dimensional followed by a two-layer FFN sub-\nlayer,\nFFN = ReLU(XW1 + b1)W2 + b2, (2)\nwhere X is the input feature of the FNN, W1 ∈\nRC×4D,b1 ∈R4D,W2 ∈R4D×C,b2 ∈RC are\nthe transformation parameters. Each Transformer\nlayer contains the above two sub-layers. A residual\nconnection is also added around each of the above\nsub-layers followed by layer normalization.\n3.2 Scalable Transformers\nArchitecture and word predictions.The Trans-\nformer adopts an encoder-decoder architecture,\neach of which consists of 6 Transformer layers.\nGiven the last output feature O ∈ RN×C, it is\nmapped back to the word embedding space to pre-\ndict the output words as softmax(OWT\ne ), where\nWe is the transposed word embedding matrix.\nThe goal of our Scalable Transformers is that,\nonce trained, the FLOPs and number of parameters\nof the Transformer can be ﬂexibly adjusted accord-\ning to different use scenarios and without network\nre-training. To achieve this goal, we make thewidth\nof our encoder and decoder layers to be ﬂexibly\nmodiﬁed within a pre-deﬁned range. Our Scal-\nable Transformers shares the parameters with sub-\nmodels of different widths. Speciﬁcally, for each\nlayer, the wider Transformer contains all the param-\neters and computations of the sub-Transformers. If\nthe widest Transformer is properly trained, we can\nW_k\nWeight \nWeight \nW_q\nWeight \nW_v\nW_2\nWeight \nCin\nIn\nWeight \n+\nDot Product AttentionKey\nQuery\nValue Add & Norm FeedForward\nAdd \n&\n Norm\nW_k\nWeight \nWeight \nW_q\nWeight \nW_v\nWeight \n+\nDot Product AttentionKey\nQuery\nValue FeedForward\nAdd \n&\n Norm\nW_1\nWeight \nW_1\nWeight \nW_2\nWeight \nW_k\nWeight \nWeight \nW_q\nWeight \nW_v\nWeight \n+\nDot Product AttentionKey\nQuery\nValue Add & Norm FeedForward\nAdd \n&\n Norm\nW_1\nWeight \nW_2\nWeight \nHas\nThis\nIdea\nWork\n?\nHas\nThis\nIdea\nWork\n?\nChannelDepth\nHas\nThis\nIdea\nWork\n?\nChannelDepth\nChannelDepth\nAdd & Norm\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\ninout\nInputInputInput\nCrop Crop\nCrop CropCrop\nCrop\nLogit\nDistillation\nLoss\n[S] das funktioniert hat\nLogit\n[S] das funktioniert hat\nLogit\nDistillation\nLoss\n[S] das funktioniert hat\nCrop\nCrop\nFigure 2: The proposed Scalable Transformers (ST) can ﬂexibly change different layers’ feature width to meet dif-\nferent computational constraints. (Middle) The widest Transformer of a Scalable Transformers. (Top) A narrower\nsub-Transformer with ﬁxed input-output feature dimension and varying attention feature dimensions for different\nlayers. The weights of sub-Transformers are directly cropped from the widest Transformer without re-training.\n(Bottom) A narrower sub-Transformer with the same input-output and attention feature dimension for all layers.\nobtain smaller scale sub-Transformers by simply\ntruncating Transformer layers’ width and cropping\nthe parameter matrices. Sub-Transformers cropped\nfrom the widest Transformer can still conduct ac-\ncurate translation with limited performance drops.\nThe widest Transformer layer has C = Mmax and\nDi = Mmax as the input-output and attention fea-\nture dimension. The parameter matrices’ sizes can\nbe determined accordingly. For instance, we denote\nthe attention matrix parameters at layer i of the\nwidest Transformer as Wi\nk,max,Wi\nq,max,Wi\nv,max,\nwhich are all of size Mmax ×Mmax.\nNarrower Sub-Transformer layers. Once the\nwidest Transformer’s parameters are deﬁned, a nar-\nrower sub-Transformer’s layerican be obtained by\ncropping a subset of parameters from the widest\nlayer ideﬁned above. A narrower layer iwould\nadopt a smaller input-output dimensionC <Mmax\nas well as a smaller attention feature dimension\nDi < Mmax. Without loss of generality, here we\nonly discuss how to obtain the attention-key param-\neters of the narrower layer i, Wi\nk ∈RC×Di\nand\nb∈RD. The same operations can be generalized\nto obtain other parameters (Wi\nq, Wi\nv,Wi\no ,Wi\n1, Wi\n2,\netc.) of the narrower Transformer’s layer i. The\nattention-key parameters can be obtained as\nWi\nk = Wi\nk,max[1:C,1:Di], bi\nk = bi\nk,max[1:Di],\nwhere Wi\nk,max[1:C,1:Di] is the C-row and Di-\ncolumn top-left sub-matrix of the widest matrix\nWi\nk, and bi\nk[1:Di] denotes the ﬁrst Di dimensions\nof the widest bias vector bi\nk. It is obvious that the\nwidest Transformer layers contains all the param-\neters and computations of the narrower ones, and\nthus avoid introducing redundant parameters. The\nnarrower sub-Transformers can then be obtained\nby stacking such sub-Transformer layers with dif-\nferent Di dimensions. Note that although the input-\noutput dimension C can be ﬂexibly adjusted, we\nmake the entire narrower sub-Transformer share\nthe same C dimensions across all layers because\nof the requirement of residual connections, while\nDi can be different for different layers.\nInput and output projections. Our Scalable\nTransformers only modiﬁes the width of the\nTransformer layers. The dimension of the word\nembeddings remains to be Mmax for the sub-\nTransformers. To maintain the word embedding\ndimensions, for the widest Transformer, we use\none additional linear projection to convert the word\nembeddings Eto\nE′= W′\ne,maxE+ b′\ne,max, (3)\nwhere W′\ne, max ∈RMmax×Mmax ,b′\ne,max ∈RMmax .\nSimilarly, another linear projection is introduced to\ntransform the FFN output features O∈RN×Mmax\nto\nO′= W′\no,maxO+ b′\no,max, (4)\nwhich are then used for ﬁnal word predictions. For\nnarrower sub-Transformers with smaller intermedi-\nate feature width C <Mmax, we crop the param-\neters from the widest input and output projections\nfor sub-Transformers so that the word embedding\ncan be shared across different scales,\nW′\ne = W′\ne, max[1:Mmax,1:C], b ′\ne = b′\ne,max[1:C],\nW′\no = W′\ne, max[1:C,1:Mmax], b ′\no = b′\no,max.\n(5)\nScalable Transformer variants. In our experi-\nments, we mainly experiment with two types of\nScalable Transformers. In the type-1 Scalable\nTransformers, each sub-Transformer’s all layers\nadopt the same width from Mfor the input-output\nand attention feature dimensions, i.e., C = D1 =\n··· = D12. Therefore, there would be a total num-\nber of |M|sub-Transformers of different widths af-\nter the Scalable Transformers is trained. In thetype-\n2 Scalable Transformers, we ﬁx the input-output\ndimension C = Mmax but allows freely choos-\ning the attention dimension Di from any width in\nM= {M1,··· ,Mmax}at each layer. Therefore,\na total number of|M|12 different sub-Transformers\nexist after the ST is trained.\n3.3 Training Scalable Transformers with\nSelf-distillation\nTraining the Scalable Transformers is quite chal-\nlenging as it requires all sub-Transformers with\nshared parameters to have superior performance.\nSimply training all the sub-Transformers indepen-\ndently cannot result in satisfactory performances\nbecause of the gradient interference between sub-\nmodels. To tackle this issue, we propose a novel\nthree-stage training strategy. The key idea of train-\ning is to distill the knowledge from the widest\nTransformer and use them as the supervisions\nfor narrower sub-Transformers. Compared with\nground-truth annotations, the predictions by the\nwidest Transformers are easier to ﬁt for the sub-\nTransformers as all of them share parameters to\ncertain degrees. On the other hand, if the sub-\nTransformers are properly trained, their parame-\nters could also boost the performance of wider\nTransformers that fully contain them. We propose\na three-stage training strategy, where stage-1 fo-\ncuses on training the widest Transformer and its\nsub-Transformers independently, and stage-2 and\nstage-3 utilize word-level and sequence-level self-\ndistillation.\nStage 1: Joint sub-Transformer pre-training.\nThe stage-1 pre-training conducts jointly training\non all the sub-Transformers. At each iterationj, we\nrandomly sample a few sub-structures and always\ninclude the widest Transformer Tmax for parame-\nter updating. For each sub-Transformer, we ran-\ndomly sample the input-output dimension C and\nthe attention width Di from {M1,··· ,Mmax}for\ntheir Transformer layers following the layer width\nconstraints of type-1 or type-2 models, i.e., we ran-\ndomly sample from |M|and |M|12 sub-structures\nrespectively for independent pre-training. All the\nsampled architectures are trained with the cross-\nentropy loss Lce on ground truth words,\nL1 = Lce(Tmax(S); G) +\n∑\nT∈T(j)\nLce(T(S); G),\n(6)\nwhere Gis the ground-truth sentence, Tmax(S) and\nT(S) denote the predicted words by the widest\nTransformer Tmax and the sub-Transformer T\ngiven the input sequence S, and T(i) denotes the\nset of sampled sub-Transformers at iteration j.\nAlthough joint sub-model pre-training actually\nresults in worse performance than training only\nthe widest Transformer, it pretrains all the sub-\nstructures to have gradients of similar scales and\nalso makes compromises on the shared parame-\nters. Our stage-1 pre-training is almost the same\nas slimmable NN (Yu et al., 2018; Yu and Huang,\n2019) because Transformer uses Layer Normliaza-\ntion to avoid Batch Normalization. If removing the\nseparate BNs in slimmable NN, it is equivalent to\nour stage-1, which jointly trains the largest model\nand its sub-models.\nStage 2: Annealed word-level self-distillation.\nIn stage-1 pre-training, the widest model is always\nupdated in each iteration to ensure that it is more\nsufﬁciently trained than sub-Transformers. How-\never, the interference between the models with\nFeat. Dim. 256 320 384 448 512 576 640 704 768 832 896 960 1024\nDropout 0 0 0 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.3 0.3 0.3\nWith Input-output Projections\nIndep. 23.32 25.54 26.14 26.74\nIndep.+sep. teach 25.49 25.81 26.02 26.75\nScalable+sep. teach 25.53 25.49 25.71 25.78 25.99 26.02 26.05 26.01 26.19 26.21 26.19 26.33 26.59\nStage 1 23.76 23.85 24.27 24.57 24.86 25.25 25.56 25.94 26.23 26.21 26.50 26.58 26.61\nStage 1+2 24.76 24.93 24.93 25.35 25.61 25.58 25.84 26.00 26.34 26.37 26.45 26.53 26.65\nStage 2+3 24.91 24.59 25.11 25.12 25.32 25.36 25.41 25.53 25.57 26.01 25.99 26.19 26.21\nStage 1+3 25.41 25.29 25.85 25.89 26.01 25.99 26.11 26.23 26.31 26.45 26.44 26.41 26.59\nStage 1+2+3 25.59 25.59 26.06 26.05 26.15 26.11 26.06 26.23 26.30 26.49 26.61 26.65 26.71\nParams (M) 45 51 59 68 79 91 104 119 135 152 171 191 209\nFLOPs (G) 5.2 6.6 8.1 9.7 11.3 12.9 14.6 16.4 18.2 20.1 22.0 23.98 26.02\nWithout Input-output Projections\nIndep. 23.35 25.56 26.12 26.73\nI/O Cropping 25.11 25.07 25.18 25.42 25.71 26.02 25.98 26.07 26.32 26.22 26.31 26.48 26.46\nParams (M) 19 28 37 48 61 75 90 106 124 144 163 186 209\nFLOPs (G) 5.2 6.6 8.7 9.7 11.3 12.9 14.6 16.4 18.2 20.1 22.0 23.98 26.02\nTable 1: Ablation study on En-De validation (newtest2013) set with beam search 4. FLOPs are calculated with\nthe assumption that source and target length are 20.\nshared parameters prevents them from effective\nlearning. To mitigate the difﬁculty of training the\nsub-Transforms to ﬁt the “hard” ground truth (one-\nhot vectors), we propose to distill the knowledge\nfrom the widest Transformer for training its sub-\nTransformers. Speciﬁcally, for each input sequence\nS, the soft predictions of the widest Transformer\nTmax(S) is used as additional training supervisions\nfor its sub-Transformers T with the cross-entropy\nloss,\nL2 =Lce(Tmax(S); G) +λ(j)\n2\n∑\nT∈T(j)\nLce(T(S); G)\n+ (1−λ(j)\n2 )\n∑\nT∈T(j)\nLce(T(S); Tmax(S)),\n(7)\nwhere the ﬁrst two terms are the same as the\nstage-1 loss, Lce(T(S) ;Tmax(S)) denotes using\nthe widest model’s predictions as learning targets\nfor its sub-Transformers T, and λ(j)\n2 is a relative\nweight at iteration j balancing the soft supervi-\nsions.\nIn early iterations, our pre-trained Scalable\nTransformers from stage-1 still struggles because\nthe predicted word probabilities Tmax(S) might be\nnoisy. Therefore, we make λ(j)\n2 closer to 1 when\nj is small to more rely on the hard ground-truth\nwords. λ(j) then gradually decreases to involve the\nsoft predictions as additional supervisions as\nλ(j)\n2 =\n{\n1 −0.5 j\ntj\n, if j <tj,\n0.5, if j ≥tj.\n(8)\ntj is an iteration threshold, after which λ(j)\n2 is ﬁxed\nto be 0.5. In this way, the sub-Transformers can\ngradually ﬁt the soft predictions. Because the sub-\nTransformers’ are also parts of the widest Trans-\nformer. The widest Transformer can also be im-\nproved, which in turn further provides more accu-\nrate supervisions for training sub-Transformers.\nStage 3: Sequence-level self-distillation. After\nstage-2 training, both the ground-truth labels as\nwell as the word-level soft predictions might still\nbe noisy to hinder the further improvements of the\nmodels. For stage-3 training, we conduct ofﬂine\nbeam search with the widest Transformer to gen-\nerate reﬁned sequence-level predictions for all the\ntraining sequences. For our widest Transformer, it\nis trained with only the sequence-level reﬁned pre-\ndictions; for its sub-Transformers, they are trained\nwith both the ofﬂine beam-searched predictions\nand online word-level predictions with the below\nloss,\nL3 = Lce(Tmax(S); Tbeam\nmax (S))\n+ λ3\n∑\nT∈T(j)\nLce(T(S); Tbeam\nmax (S))\n+ (1−λ3)\n∑\nT∈T(j)\nLce(T(S); Tmax(S)), (9)\nwhere the 1st term is the loss for training the widest\nmodel, the 2nd and 3rd terms are for training the\nsub-Transformers with both ofﬂine sequence-level\npredictions Tbeam\nmax (S) and online word-level predic-\ntions Tmax(S), and λ3 is for weighting the two loss\nterms and is empirically ﬁxed to 0.1.\n4 Experiments\n4.1 Dataset and Experiment Setup\nDatasets. We test the ST on WMT 2014\nEnglish-German (En-De) and English-French (En-\nFr) datasets. We adopt the En-De pre-processed\nversion by Vaswani et al. (2017) for fair compari-\nson with previous approaches. Ablation study and\nhyperparameter tuning are conducted on WMT En-\nDe validation (newstest2013) set and tested on the\ntest (newstest2014). In En-Fr, we follow the same\nhyper-parameters of En-De. All ablation studies\nand experiments are conducted with type-1 Scal-\nable Transformers. BPE (Sennrich et al., 2015)\nwith shared dictionary between source and target\nwas adopted for tokenization of both datasets. 32k\nand 40k joint dictionaries are created for En-De and\nEn-Fr tasks. We measure the translation with case\nsensitive BLEU. All experiments use beam search\n4 and length penalty 0.6. Following Vaswani et al.\n(2017), we apply compound splitting.\nImplementation. Our Scalable Transformers con-\ntains 6 encoder layers and 6 decoder layers. There\nare 13 different widths in total, i.e. M =\n{256,320,··· ,1024}. The attention head dimen-\nsion is ﬁxed to 64. We set different dropout rates\nwithin ST according to the features’ dimensions.\nFor En-De, we gradually increase the dropout rate\n0 →0.3 as the feature dimension 256 →1024.\nFor En-Fr, since it has larger-scale training data,\nwe set dropout rates to 0 for widths in [256,576]\nand to 0.1 for widths in [640,1024]. Our optimiza-\ntion follows Ott et al. (2018) and details are can be\nfound in supplementary materials. Before stage-3\ntraining, for the input sequence, we conduct predic-\ntion by the widest Transformer with beam search 4\nand length penalty 0.6 to generate distillation tar-\ngets. The predicted sequences are reﬁned to remove\nthe those whose source/target length ratio>20 or\nlength>250 words. Our proposed and other com-\npared methods’ performances are reported based\non the ensemble of the last 10 training epochs, fol-\nlowing the experimental setting of Ott et al. (2018).\n4.2 Ablation Study on WMT En-De\nIndependent vs. Scalable Transformers. We\nﬁrst independently train separate Transform-\ners, which have uniﬁed feature widths of\n{256,512,768,1024}for all layers but have ﬁxed\n1024-d word embedding and the additional input-\noutput projections for fairly comparing with our\ntype-1 Scalable Transformers’ sub-Transformers.\nOur Scalable Transformers shows much better per-\nformance than the independently trained counter-\nparts (see “Stage 1+2+3” vs “Indep.” in Table 1).\nStage-1 vs. stage-2 vs. stage-3 training.We then\nshow the necessity of the proposed three-stage\ntraining scheme. Simple joint training all sub-\nTransformers in stage-1 (equivalent to slimmable\nNN (Yu et al., 2018) as we discussed in Sec. 3.3) re-\nsults in even worse performance than independent\ntraining (“Stage 1” vs. “Indep.” in Table 1), which\nillustrate the training interference between different\nsub-Transformers. Stage-2 training shows signif-\nicant improvements over stage-1 results. Stage-3\ntraining further improves the performance with of-\nﬂine sequence-level self-distillation (“Stage 1+2”\nand “Stage 1+2+3” in Table 1). We also test dis-\ncarding stage-1 training (“Stage 2+3”) and discard-\ning stage-2 training (“Stage 1+3”) from our three-\nstage training scheme. Both strategies lead to in-\nferior performances compared with our proposed\nthree-stage training scheme.\nIndependent models + separate widest teacher.\nThe above experiments show that parameter-\nsharing sub-models and the full model can mu-\ntually boost their performance via the proposed\ntraining scheme. To show the improvement is not\nfrom simple knowledge distillation, we train an\nindependent widest Transformer to generate the\nreﬁned sequence-level predictions. After the train-\ning converges, we use the widest transformer as\nthe teacher outputting both online soft and ofﬂine\nbeam-searched hard targets to guide the training of\n12 independent models with varying widths 256-\n1024. Results show that our Scalable Transformer\ncan achieve better performance than independent\ndistillation (“Indep.+sep. teach” vs. “indep.”) due\nto the parameter sharing between models of dif-\nferent width, especially on narrower models. (“In-\ndep.+sep. teach” vs. “Stage 1+2+3” in Table 1).\nScalable Transformer + separate widest teacher.\nAn alternative approach to the above study is to\nkeep the separate widest Transformer as the teacher\nto teach a proposed Scalable Transformer. The per-\nformances by this strategy (“scalable+sep. teach”\nvs. “Stage 1+2+3”) are only slightly lower than\nour proposed self-distilled scalable model. The\nreason might be that, although the scalable models\nshare parameters and are jointly trained. The sep-\narate widest teacher’s outputs might be less com-\npatible with those of the narrower models than the\nscalable model’s widest Transformer. In addition,\nFeat. Dim. 256 320 384 448 512 576 640 704 768 832 896 960 1024\nDropout 0 0 0 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.3 0.3 0.3\nIndep. 23.3 27.5 27.7 29.21\nStage 1 24.0 24.3 24.6 25.0 25.7 26.3 27.0 27.4 27.8 28.3 28.6 28.75 28.74\nStage 1+2 25.1 25.5 25.8 26.3 26.6 27.1 27.6 27.8 28.2 28.4 28.8 28.89 28.99\nStage 2+3 26.7 26.6 27.0 27.1 27.6 27.4 27.9 28.1 28.4 28.7 28.6 28.76 29.18\nStage 1+3 26.9 26.8 27.1 27.3 27.5 27.9 28.2 28.3 28.6 28.7 28.7 28.94 29.01\nStage 1+2+3 27.1 27.3 27.5 27.9 28.3 28.4 28.6 28.7 29.0 28.9 29.2 29.34 29.27\nTable 2: Performance of type-1 Scalable Transformers on En-De test (newtest2014) set.\nModel EN-De En-Fr\nTransformer (Vaswani et al., 2017) 28.4 40.5\nWeighted Transformer (Ahmed et al., 2017) 28.9 41.0\nRelative Transformer (Shaw et al., 2018) 29.2 41.5\nScaling NMT (Ott et al., 2018) 29.3 43.2\nEvolved Transformer (So et al., 2019) 29.8 -\n(NAS on 200 TPUs)\nBaseline 29.2 43.1\nType-1 Stage 1 (widest) 28.7 42.8\nType-1 Stage 1+2+3 (widest) 29.3 43.1\nType-2 Stage 1+2+3 (widest) 29.5 43.1\nType-2 Stage 1+2+3 (sub-model) 29.7 43.3\nTable 3: Comparison between Scalable Transformers\nwith state-of-the-art methods on the test sets.\nthis training strategy requires more training costs.\nFor each forward process, only 46G FLOPs are re-\nquired for our proposed training scheme, while 72G\nFLOPs are needed for this separate widest teacher\nstrategy, as the separate teacher model needs to be\nseparately forwarded to obtain the word-level soft\ntargets. Training the separate widest model also\nrequires additional resources.\nAdvantages of type-2 ST.Our type-2 ST can more\nﬂexibly adjust its FLOPs because it can freely\nchoose different feature widths Di for different\nlayers. Surprisingly, after the type-2 model is\ntrained, we found many of its sub-Transformers\ncan even surpass the widest Transformer. Given\nthe widest Transformers of our trained type-2\nmodel from the last 10 epochs, for each layer\nwidth Di, we randomly choose the feature dimen-\nsion from {896,960,1024}and conduct evalua-\ntion with 1,000 random such sub-Transformers on\nthe En-De validation set. We choose the best sub-\nTransformer on the validation (newtest2013) set\nand evaluate on the test (newtest2014) set. Such an\noptimal sub-model achieves 26.9 and 29.7 on the\nvalidation and test sets (see Table 7 in supplemental\nmaterials), which are even higher than the widest\nTransformer. We also calculate the mean and stan-\ndard deviation of the BLEU scores of the top-10\nsub-Transformers, which achieve 26.82 ±0.036\nand 29.60 ±0.061 on validation and test sets. All\ntop-10 sub-Transformers’ performances are higher\nthan those of the widest Transformer (26.7 for vali-\ndation and 29.3 for test), demonstrating the effec-\ntiveness of our type-2 Scalable Transformers. We\nwill continue to investigate how to improve the ef-\nﬁciency on searching the optimal sub-model. To\nverify that the performance improvement of type-2\nTransformer is not from the 1,000 searched sub-\nmodels, we store 1,000 checkpoints in the last 3\nepochs when training an independent widest trans-\nformer. We average the best 10 models out of 1,000\ncheckpoints, which achieve 29.4 on En-De test set\nand is very similar to the baseline’s 29.3. How-\never, such a result is still worse than our searched\noptimal Type-2 sub-model’s29.7.\nTraining time and memory of Scalable Trans-\nformer. Our proposed Scalable Transformer has\n209M parameters and is trained for 212 hours on 8\nV100 GPUs. If one separately trains 12 Transform-\ners with varying width, it would use 1290M param-\neters and 712 hours for training, both of which are\nmuch larger than those of our proposed method.\nOther factors.We provide more ablation studies\nin the appendix.\n4.3 Final Results on WMT EN-De and En-Fr\nWe test the proposed Scalable Transformers on\nWMT’14 EN-De test (newtest2014) set and En-Fr\ntest set. Performances of type-1 ST on En-De test\n(newtest2014) are listed in Table 2, which show\nsimilar tendency as those on En-De validation set.\nWe further compare the widest model from\nour type-1 Scalable Transformers and the ran-\ndomly searched optimal sub-model from our type-\n2 Scalable Transformers with the original Trans-\nformer (Vaswani et al., 2017), weighted Trans-\nformer (Ahmed et al., 2017), relative position\nTransformer (Shaw et al., 2018), scaling neural\nmachine translation (Ott et al., 2018) and evolved\nTransformer (So et al., 2019) in Table 3. The base-\nline Transformer is based on Ott et al. (2018). Our\nreproduction achieves 29.2 and 43.1 on En-De and\nEn-Fr, comparable with the original results. The\nsimple joint training (denoted as “Type-1 Stage-1\n(widest)”) results in slightly worse performance\nthan the independently trained baseline.\nThe widest Transformer from type-1 ST achieves\n29.3 and 43.1 on En-De and En-Fr, which are\ncomparable with the baseline Transformer, but\nwith 13 sub-models of different FLOPs and pa-\nrameter sizes. The best randomly searched sub-\nTransformer of our type-2 ST (denoted as “Type-2\nStage 1+2+3 (sub-model)”) on En-De can even\nreach 29.7, which has fewer parameters but outper-\nforms state-of-the-art standard Transformers with\n1024-dimensional features. Evolved Transformer\nperformed architecture search with 200 TPUs to\nﬁnd the optimal translation architecture. It achieved\n29.8 BLEU with 218M parameters. In contrast, our\ntype-2 Scalable Transformers is much easier to im-\nplement with existing libraries, only used 3 days\non 8 V100 GPUs for training, and achieved compa-\nrable 29.7 BLEU with 221M parameters.\n5 Conclusion\nIn this paper, we propose a novel Scalable Trans-\nformers to achieve efﬁcient and robust deploy-\nment of Transformers of different scales. We\nchoose to make the Scalable Transformers change\nits scale by modifying its layer width. Through\ncarefully designed word-level and sequence-level\nself-distillation, the proposed Scalable Transform-\ners can be trained with an incremental increase of\ntraining time. After training, sub-Transformers of\ndifferent scales can be easily obtained by cropping\nfrom the widest Transformer to achieve ﬂexible\ndeployment.\nReferences\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted transformer net-\nwork for machine translation. arXiv preprint\narXiv:1711.02132.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2018. Univer-\nsal transformers. arXiv preprint arXiv:1807.03819.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMichael Figurnov, Maxwell D Collins, Yukun Zhu,\nLi Zhang, Jonathan Huang, Dmitry Vetrov, and Rus-\nlan Salakhutdinov. 2017. Spatially adaptive compu-\ntation time for residual networks. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 1039–1048.\nMikel L Forcada, Mireia Ginestí-Rosell, Jacob Nord-\nfalk, Jim O’Regan, Sergio Ortiz-Rojas, Juan An-\ntonio Pérez-Ortiz, Felipe Sánchez-Martínez, Gema\nRamírez-Sánchez, and Francis M Tyers. 2011. Aper-\ntium: a free/open-source platform for rule-based ma-\nchine translation. Machine translation, 25(2):127–\n144.\nPeng Gao, Chiori Hori, Shijie Geng, Takaaki Hori,\nand Jonathan Le Roux. 2020. Multi-pass trans-\nformer for machine translation. arXiv preprint\narXiv:2009.11382.\nPeng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu,\nSteven CH Hoi, Xiaogang Wang, and Hongsheng\nLi. 2019a. Dynamic fusion with intra-and inter-\nmodality attention ﬂow for visual question answer-\ning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n6639–6648.\nPeng Gao, Haoxuan You, Zhanpeng Zhang, Xiaogang\nWang, and Hongsheng Li. 2019b. Multi-modality la-\ntent interaction network for visual question answer-\ning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 5825–5835.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1243–1252. JMLR. org.\nShijie Geng, Peng Gao, Zuohui Fu, and Yongfeng\nZhang. 2021. Romebert: Robust training of multi-\nexit bert. arXiv preprint arXiv:2101.09755.\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nGao Huang, Danlu Chen, Tianhong Li, Felix Wu,\nLaurens van der Maaten, and Kilian Q Wein-\nberger. 2017. Multi-scale dense networks for re-\nsource efﬁcient image classiﬁcation. arXiv preprint\narXiv:1703.09844.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and\nKilian Q Weinberger. 2016. Deep networks with\nstochastic depth. In European conference on com-\nputer vision, pages 646–661. Springer.\nLukasz Kaiser, Aidan N Gomez, and Francois Chol-\nlet. 2017. Depthwise separable convolutions\nfor neural machine translation. arXiv preprint\narXiv:1706.03059.\nYoon Kim and Alexander M Rush. 2016. Sequence-\nlevel knowledge distillation. arXiv preprint\narXiv:1606.07947.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. arXiv preprint arXiv:1908.02265.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. arXiv preprint arXiv:1806.00187.\nHieu Pham, Melody Y Guan, Barret Zoph, Quoc V\nLe, and Jeff Dean. 2018. Efﬁcient neural architec-\nture search via parameter sharing. arXiv preprint\narXiv:1802.03268.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155.\nDavid R So, Chen Liang, and Quoc V Le.\n2019. The evolved transformer. arXiv preprint\narXiv:1901.11117.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Hervé Jé-\ngou. 2020. Training data-efﬁcient image transform-\ners & distillation through attention. arXiv preprint\narXiv:2012.12877.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nQizhe Xie, Eduard Hovy, Minh-Thang Luong, and\nQuoc V Le. 2019. Self-training with noisy student\nimproves imagenet classiﬁcation. arXiv preprint\narXiv:1911.04252.\nChenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan L\nYuille. 2019a. Training deep neural networks in gen-\nerations: A more tolerant teacher educates better stu-\ndents. In Proceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence, volume 33, pages 5628–5635.\nChenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille.\n2019b. Snapshot distillation: Teacher-student opti-\nmization in one generation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 2859–2868.\nJiahui Yu and Thomas S Huang. 2019. Universally\nslimmable networks and improved training tech-\nniques. In Proceedings of the IEEE International\nConference on Computer Vision, pages 1803–1811.\nJiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and\nThomas Huang. 2018. Slimmable neural networks.\narXiv preprint arXiv:1812.08928.\nA Optimization Details\nADAM (Kingma and Ba, 2014) optimizer is\nadopted with β1 = 0.9, β2 = 0.98,ϵ = 10−8.\nOther optimization details can be found in Table 4.\nThe learning rate of each stage linearly increases\nfor the ﬁrst 4,000 iterations from 5 ×10−4 to the\nmaximum learning rate of each stage (denoted as\n“max lr” in Table 4). After reaching the maximum\nlearning rate, the learning rate decays according to\nlr =lrmax ×\n√\n4000√#iteration (10)\nwhere lrmax stands for the maximum, 4000 is the\nnumber of iterations for warming up, #iteration\nstands for the current iteration number. The learn-\ning rate scheme follows the original Transformer\n(Vaswani et al., 2017). The total training epoch\nfor each stage is denoted as “Epoches” in Table 4.\nLabel smoothing of 0.1 is conducted following Ott\net al. (2018). Weight decay is not used for all\nexperiments. We train the proposed Scalable Trans-\nformer with 8 V100 GPUs, each of which holds\n3584 and 5120 tokens for En-De and En-Fr, re-\nspectively. We adopt gradient accumulation of 16\nTraining stage Epoches max lr λ2/λ3\nEN-De stage-1 60 0.007 n/a\nEn-De stage-2 60 0.007 0.5\nEn-De stage-3 30 0.004 0.9\nEn-Fr stage-1 30 0.006 n/a\nEn-Fr stage-2 30 0.006 0.5\nEN-Fr stage-3 15 0.004 0.9\nTable 4: The hyperparameters for optimization on En-\nDe and En-Fr datasets.\nto further increase the training batch size. λ2 or\nλ3 weight the contributions of the cross-entropy\nloss and self-distillation loss and their settings are\nrecorded in Table 4 (denoted as “λ2/λ3”).\nB Additional Ablation Studies and\nExperiments\nDropout scheme. Dropout has major impact on\nthe ﬁnal performance for training Transformers.\nOur Scalable Transformer gradually increase the\ndropout rates from 0.1 to 0.3 as the feature dimen-\nsions increase from 256 to 1024. We test two al-\nternative dropout strategies. The ﬁrst one is to\nset dropout rates as 0.3 for all feature dimensions\nin {256,··· ,1024}. Another strategy is to set\ndropout rate to 0 for all feature dimensions except\nfor dimension 1024, which has a dropout rate of\n0.3. The results on En-De validation set is shown\nin Table 6, which show that our proposed dropout\nstrategy leads to better performance than those sim-\npliﬁed ones on all widths of the type-1 Scalable\nTransformer.\nEffects of input-output linear projections.Our\nScalable Transformers introduces two additional\nlinear projections W′\no and W′\ne to ensure that the\nsame word embedding can be used across sub-\nTransformers. We show that the gain is not\nfrom the extra linear projections. We indepen-\ndently train another 4 Transformer to have the\nsame word embedding and feature dimensions of\n{256,512,768,1024}respectively. Those 4 Trans-\nformers do not need the additional linear projec-\ntions. The two “Indep.” in Table 1 show that\nthey have almost the same performance as inde-\npendent Transformers with projections. Another\npossible way to replace the input-output projections\nis to crop the top corner of the embedding matrix\nto obtain narrower word embedding for the sub-\nTransformers. This strategy (“I/O Cropping” in\nTable 1) show inferior accuracy compared with the\nproposed additional input-output linear projections.\nEncoder-decoder Layers Layer Width EN-De Test Parameter (M)\n6-6 512 27.5 61\n3-3 768 26.3 73\nTable 5: Performance of (top) deep but thin Trans-\nformer versus (bottom) shallow but wide Transformer\nwith similar parameter sizes on En-De test set. The\nformer network has smaller performance drop.\nBeam search. We also test the impact of using\nbeam search during inference. The results are\nlisted in Tables 7 and 8. Beam search generally\nimproves the translation accuracy, especially on\nstage-1 or stage-2 results. After stage-3 training,\nthe improvements of “Stage 1+2+3” with beam\nsearch over “Stage 1+2+3” without beam search\nbecome marginal on the two datasets ( +0.17 for\nEn-De test set and +0.04 for En-Fr test set), which\nindicate that our three-stage training strategy can\neliminate the need for beam search at inference to\ncertain extent.\nWidth adaptive or depth adaptive?In our pro-\nposed Scalable Transformer, we decide to tune\nlayer width of the Transformer to achieve ad-\njustable FLOPs and parameter sizes. To show\ntuning layer width is better than tuning network\ndepth, we implement two independent Transform-\ners with approximately the same number of param-\neters. One contains 6 encoder and 6 decoder layers\nwith feature dimension 512. The other one con-\ntains 3 encoder and 3 decoder layers with feature\ndimension of 768. Their translation accuracy on\nEn-De test (newtest2014) set is reported in Table 5,\nwhich show that decreasing the depth of the Trans-\nformer signiﬁcantly impacts the translation accu-\nracy 29.2 →26.3, while decreasing the layer width\nhas smaller performance drop 29.2 →27.4. There-\nfore, it is better to tune the width of the Transformer\nlayers than the overall depth of the Transformer to\nbetter maintain the translation accuracy.\nTop-10 randomly searched type-2 sub-\nTransformers. We conduct random search\non type-2 Scalable Transformer’s sub-models to\nﬁnd the optimal performance as stated in Sec.\n4.2 of the main paper. Table 9 shows the top-10\nsub-models’ layer widths and their performances\non En-De validation and test sets. The results\nclearly show that many sub-Transformers of our\ntype-2 ST can even surpass the widest Transformer,\nwhich worth further studying how to effectively\nconduct sub-model search in the future.\nFeat. Dim. 256 320 384 448 512 576 640 704 768 832 896 960 1024\nProposed Dropout Stratety\nDropout 0 0 0 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.3 0.3 0.3\nStage 1+2+3 25.59 25.59 26.06 26.05 26.15 26.11 26.06 26.23 26.30 26.49 26.61 26.65 26.71\nUniform Maximal Dropout\nDropout 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3\nStage 1+2+3 23.87 23.97 24.28 24.32 24.15 24.32 24.69 25.74 26.23 25.96 26.45 26.32 26.47\nWidest Dropout Only\nDropout 0 0 0 0 0 0 0 0 0 0 0 0 0.3\nStage 1+2+3 24.89 24.91 24.95 25.02 25.17 25.28 25.34 25.56 25.88 26.10 26.22 26.01 26.35\nTable 6: Tuning dropout rates on type-1 Scalable Transformer when training on En-De validation (newtest2013)\nset with beam search 4.\nFeat. Dim. 256 320 384 448 512 576 640 704 768 832 896 960 1024\nDropout 0 0 0 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.3 0.3 0.3\nBeam Search 4\nStage 1 24.0 24.3 24.6 25.0 25.7 26.3 27.0 27.4 27.8 28.3 28.6 28.75 28.74\nStage 1+2 25.1 25.5 25.8 26.3 26.6 27.1 27.6 27.8 28.2 28.4 28.8 28.89 28.99\nStage 1+2+3 27.1 27.3 27.5 27.9 28.3 28.4 28.6 28.7 29.0 28.9 29.2 29.34 29.27\nBeam Search 1\nStage 1 23.3 23.4 23.7 24.3 24.8 25.5 26.1 26.9 27.2 27.2 27.3 27.58 27.90\nStage 1+2 23.7 24.2 24.6 25.1 25.5 26.0 26.7 27.0 27.2 27.4 27.6 28.02 28.19\nStage 1+2+3 26.7 26.8 27.1 27.4 27.8 28.0 28.1 28.3 28.6 28.7 28.8 28.89 29.10\nParams (M) 46 52 60 69 79 91 104 119 135 152 171 191 209\nFLOPs (G) 5.2 6.6 8.1 9.7 11.3 12.9 14.6 16.4 18.2 20.1 22.0 23.98 26.02\nTable 7: Performance of type-1 Scalable Transformer on En-De test (newtest2014) set with beam search 4 and 1.\nFeat. Dim. 256 320 384 448 512 576 640 704 768 832 896 960 1024\nDropout 0 0 0 0 0 0 0.1 0.1 0.1 0.1 0.1 0.1 0.1\nBeam Search 4\nIndep. 43.11\nStage 1 39.1 38.9 39.2 39.6 39.9 40.5 40.9 41.4 41.8 42.0 42.3 42.41 42.84\nStage 1+2 39.4 39.5 40.0 40.2 40.6 41.0 41.3 41.9 42.2 42.4 42.6 42.65 42.95\nStage 1+2+3 40.9 41.0 41.3 41.5 41.7 41.7 42.0 42.2 42.4 42.5 42.9 43.03 43.07\nBeam Search 1\nStage 1 38.0 38.0 38.5 38.9 39.3 39.7 40.5 40.8 41.3 41.5 41.7 41.92 42.11\nStage 1+2 38.6 38.6 39.1 39.4 39.8 40.2 40.7 41.1 41.4 41.7 41.9 42.10 42.40\nStage 1+2+3 40.4 40.6 40.8 41.0 41.4 41.5 41.9 42.1 42.3 42.4 42.7 42.84 43.03\nParams (M) 57 63 71 80 91 103 116 130 146 164 182 202 224\nFLOPs (G) 5.2 6.6 8.1 9.7 11.3 12.9 14.6 16.4 18.2 20.1 22.0 23.98 26.02\nTable 8: Performance of type-1 Scalable Transformer on En-Fr test set with beam search 4 and 1.\nModel newtest 2013 newtest 2014 FLOPs(G)\nWidest:[1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024,1024] 26.73 29.44 26.02\n[1024,1024,1024,1024,896,1024,1024,960,1024,1024,1024,896] 26.87 29.66 25.30\n[1024,1024,1024,1024,960,896,1024,896,960,1024,960,960] 26.85 29.64 24.86\n[1024,1024,1024,1024,960,896,1024,896,960,1024,1024,896] 26.85 29.62 24.86\n[1024,1024,1024,1024,960,1024,1024,896,1024,1024,960,896] 26.83 29.62 25.06\n[1024,1024,1024,1024,960,896,1024,896,960,1024,960,896] 26.83 29.60 24.69\n[1024,1024,1024,1024,1024,1024,1024,896,1024,1024,1024,960] 26.82 29.58 25.15\n[1024,1024,1024,1024,1024,1024,1024,960,1024,1024,960,960] 26.80 29.54 25.50\n[1024,1024,1024,1024,960,1024,1024,1024,1024,1024,1024,896] 26.77 29.69 25.57\n[1024,1024,1024,1024,1024,1024,1024,1024,1024,960,1024,896] 26.77 29.47 25.50\n[1024,1024,1024,1024,1024,1024,1024,896,1024,1024,1024,960] 26.76 29.64 25.50\nTable 9: BLEU scores of the top-10 randomly searched sub-Transformers from our type-2 Scalable Transformer\non En-De validation (newtest2013) and test (newtest2014) sets."
}