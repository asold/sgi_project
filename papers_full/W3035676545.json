{
  "title": "Learning Spoken Language Representations with Neural Lattice Language Modeling",
  "url": "https://openalex.org/W3035676545",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5051082988",
      "name": "Chao-Wei Huang",
      "affiliations": [
        null,
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5076610826",
      "name": "Yun-Nung Chen",
      "affiliations": [
        null,
        "National Taiwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952079278",
    "https://openalex.org/W2891539192",
    "https://openalex.org/W2097550833",
    "https://openalex.org/W2024632416",
    "https://openalex.org/W1971034924",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3108541407",
    "https://openalex.org/W2089654579",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W2951635603",
    "https://openalex.org/W2189256702",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1989996186",
    "https://openalex.org/W2963831883",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2950037171",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2900987791",
    "https://openalex.org/W2511962886",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2803392141",
    "https://openalex.org/W2007261869",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W4297683418",
    "https://openalex.org/W55328212",
    "https://openalex.org/W2024490156",
    "https://openalex.org/W2047237057",
    "https://openalex.org/W3006901707",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W2128970689",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2137871902",
    "https://openalex.org/W1503312748"
  ],
  "abstract": "Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language. Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs. The code is available at https://github.com/MiuLab/Lattice-ELMo.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3764–3769\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n3764\nLearning Spoken Language Representations with\nNeural Lattice Language Modeling\nChao-Wei Huang Yun-Nung (Vivian) Chen\nDepartment of Computer Science and Information Engineering\nNational Taiwan University, Taipei, Taiwan\nr07922069@ntu.edu.tw y.v.chen@ieee.org\nAbstract\nPre-trained language models have achieved\nhuge improvement on many NLP tasks. How-\never, these methods are usually designed for\nwritten text, so they do not consider the prop-\nerties of spoken language. Therefore, this\npaper aims at generalizing the idea of lan-\nguage model pre-training to lattices generated\nby recognition systems. We propose a frame-\nwork that trains neural lattice language models\nto provide contextualized representations for\nspoken language understanding tasks. The pro-\nposed two-stage pre-training approach reduces\nthe demands of speech data and has better ef-\nﬁciency. Experiments on intent detection and\ndialogue act recognition datasets demonstrate\nthat our proposed method consistently outper-\nforms strong baselines when evaluated on spo-\nken inputs.1\n1 Introduction\nThe task of spoken language understanding (SLU)\naims at extracting useful information from spoken\nutterances. Typically, SLU can be decomposed\nwith a two-stage method: 1) an accurate automatic\nspeech recognition (ASR) system transcribes the\ninput speech into texts, and then 2) language under-\nstanding techniques are applied to the transcribed\ntexts. These two modules can be developed sepa-\nrately, so most prior work developed the backend\nlanguage understanding systems based on manual\ntranscripts (Yao et al., 2014; Guo et al., 2014; Mes-\nnil et al., 2014; Goo et al., 2018).\nDespite the simplicity of the two-stage method,\nprior work showed that a tighter integration be-\ntween two components can lead to better perfor-\nmance. Researchers have extended the ASR 1-best\nresults to n-best lists or word confusion networks\nin order to preserve the ambiguity of the transcripts.\n1The scource code is available at: https://github.\ncom/MiuLab/Lattice-ELMo.\nAirfare\n0.3\nfair\n1.0\nto, 1.0 LA\n1.0\nFigure 1: Illustration of a lattice.\n(Tur et al., 2002; Hakkani-T¨ur et al., 2006; Hender-\nson et al., 2012; T¨ur et al., 2013; Masumura et al.,\n2018). Another line of research focused on using\nlattices produced by ASR systems. Lattices are\ndirected acyclic graphs (DAGs) that represent mul-\ntiple recognition hypotheses. An example of ASR\nlattice is shown in Figure 1. Ladhak et al. (2016) in-\ntroduced LatticeRNN, a variant of recurrent neural\nnetworks (RNNs) that generalize RNNs to lattice-\nstructured inputs in order to improve SLU. Zhang\nand Yang (2018) proposed a similar idea for Chi-\nnese name entity recognition. Sperber et al. (2019);\nXiao et al. (2019); Zhang et al. (2019) proposed ex-\ntensions to enable the transformer model (Vaswani\net al., 2017) to consume lattice inputs for machine\ntranslation. Huang and Chen (2019) proposed to\nadapt the transformer model originally pre-trained\non written texts to consume lattices in order to\nimprove SLU performance. Buckman and Neu-\nbig (2018) also found that utilizing lattices that\nrepresent multiple granularities of sentences can\nimprove language modeling.\nWith recent introduction of large pre-trained lan-\nguage models (LMs) such as ELMo (Peters et al.,\n2018), GPT (Radford, 2018) and BERT (Devlin\net al., 2019), we have observed huge improvements\non natural language understanding tasks. These\nmodels are pre-trained on large amount of written\ntexts so that they provide the downstream tasks\nwith high-quality representations. However, ap-\nplying these models to the spoken scenarios poses\n3765\nseveral discrepancies between the pre-training task\nand the target task, such as the domain mismatch\nbetween written texts and spoken utterances with\nASR errors. It has been shown that ﬁne-tuning\nthe pre-trained language models on the data from\nthe target tasks can mitigate the domain mismatch\nproblem (Howard and Ruder, 2018; Chronopoulou\net al., 2019). Siddhant et al. (2018) focused on\npre-training a language model speciﬁcally for spo-\nken content with huge amount of automatic tran-\nscripts, which requires a large collection of in-\ndomain speech.\nIn this paper, we propose a novel spoken lan-\nguage representation learning framework, which\nfocuses on learning contextualized representations\nof lattices based on our proposed lattice language\nmodeling objective. The proposed framework con-\nsists of two stages of LM pre-training to reduce\nthe demands for lattice data. We conduct experi-\nments on benchmark datasets for spoken language\nunderstanding, including intent classiﬁcation and\ndialogue act recognition. The proposed method\nconsistently achieves superior performance, with\nrelative error reduction ranging from 3% to 42%\ncompare to pre-trained sequential LM.\n2 Neural Lattice Language Model\nThe two-stage framework that learns contextual-\nized representations for spoken language is pro-\nposed and detailed below.\n2.1 Problem Formulation\nIn the SLU task, the model input is an utter-\nance X containing a sequence of words X =\n[x1,x2,··· ,x|X|], and the goal is to map X to\nits corresponding class y. The inputs can also\nbe stored in a lattice form, where we use edge-\nlabeled lattices in this work. A lattice L =\n{N,E}is deﬁned by a set of |N|nodes N =\n{n1,n2,··· ,n|N|}and a set of |E|transitions\nE = {e1,e2,··· ,e|E|}. A weighted transition\nis deﬁned as e = {prev[e],next[e],w[e],P(e)},\nwhere prev[e] and next[e] denote the previous\nnode and next node respectively,w[e] denotes the\nassociated word, and P(e) denotes the transition\nprobability. We use in[n] and out[n] to denote the\nsets of incoming and outgoing transitions of a node\nn. L<n = {N<n,E<n}denotes the sub-lattice\nwhich consists of all paths between the starting\nnode and a node n.\n2.2 LatticeRNN\nThe LatticeRNN (Ladhak et al., 2016) model gen-\neralizes sequential RNN to lattice-structured inputs.\nIt traverses the nodes and transitions of a lattice\nin a topological order. For each transition e, Lat-\nticeRNN takes w[e] as input and the representation\nof its previous node h[prev[e]] as the previous hid-\nden state, and then produces a new hidden state of\ne, h[e]. The representation of a node h[n] is ob-\ntained by pooling the hidden states of the incoming\ntransitions. In this work, we employ the Weight-\nedPool variant proposed by Ladhak et al. (2016),\nwhich computes the node representation as\nh[n] =\n∑\ne∈in[n]\nP(e) ·h[e].\nNote that we can represent any sequential text as\na linear-chain lattice, so LatticeRNN can be seen\nas a strict generalization of RNNs to DAG-like\nstructures. This property enables us to initialize\nthe weights in a LatticeRNN with the weights of a\nRNN as long as they use the same recurrent cell.\n2.3 Lattice Language Modeling\nLanguage models usually estimate p(X) by factor-\nizing it into\np(X) =\n|X|∏\nt=0\np(xt |X<t),\nwhere X<t = [x1,··· ,xt−1] denotes the previous\ncontext. Training a LM is essentially asking the\nmodel to predict a distribution of the next word\ngiven the previous words. We extend the sequen-\ntial LM analogously to lattice language modeling,\nwhere the model is expected to predict the next\ntransitions of a node n given L<n. The ground\ntruth distribution is therefore deﬁned as:\np(w|L<n)\n=\n{\nP(e), if ∃e∈out[n] s.t. w[e] =w\n0, otherwise.\nLatticeRNN is adopted as the backbone of our\nlattice language model. Since the node representa-\ntion h[n] encodes all information of L<n, we pass\nh[n] to a linear decoder to obtain the distribution\nof next transitions:\npθ(w|h[n]) =softmax(WTh[n]),\n3766\nLatticeLSTM\nLSTM LSTM LSTM\nWhat a day\nLinear\na day <EOS>\nthe, 1.0\n0.8\n0.2\nLinear\n0.9 1.0 1.0\n0.1\n1.0 1.0\nthe, 1.0\nLatticeLSTM\nMax pooling\nclassification\nTraining Target Task ClassifierStage 1: Pre-Training on \nSequential Texts\nStage 2: Pre-Training on Lattices\nLatticeLSTM\nFigure 2: Illustration of the proposed framework. The weights of the pre-trained LatticeLSTM LM are ﬁxed when\ntraining the target task classiﬁer (shown in white blocks), while the weights of the newly added LatticeLSTM\nclassiﬁer are trained from scratch (shown in colored block).\nwhere θdenotes the parameters of the LatticeRNN\nand W denotes the trainable parameters of the de-\ncoder. We train our lattice language model by mini-\nmizing the KL divergence between the ground truth\ndistribution p(w|L<n) and the predicted distribu-\ntion pθ(w|h[n]).\nNote that the objective for training sequential\nLM is a special case of the lattice language model-\ning objective deﬁned above, where the inputs are\nlinear-chain lattices. Hence, a sequential LM can\nbe viewed as a lattice LM trained on linear-chain\nlattices only. This property inspires us to pre-train\nour lattice LM in a 2-stage fashion described below.\n2.4 Two-Stage Pre-Training\nInspired by ULMFiT (Howard and Ruder, 2018),\nwe propose a two-stage pre-training method to train\nour lattice language model. The proposed method\nis illustrated in Figure 2.\n•Stage 1: Pre-train on sequential texts\nIn the ﬁrst stage, we follow the recent trend\nof pre-trained LMs by pre-training a bidi-\nrectional LSTM (Hochreiter and Schmidhu-\nber, 1997) LM on general domain text cor-\npus. Here the cell architecture is the same as\nELMo (Peters et al., 2018).\n•Stage 2: Pre-train on lattices\nIn this stage, we use a bidirectional LatticeL-\nSTM with the same cell architecture as the\nLSTM pre-trained in the previous stage. Note\nthat in the backward direction we use reversed\nlattices as input. We initialize the weights\nof the LatticeLSTM with the weights of the\npre-trained LSTM. The LatticeLSTM is fur-\nther pre-trained on lattices from the training\nset of the target task with the lattice language\nmodeling objective described above.\nWe consider this two-stage method more ap-\nproachable and efﬁcient than directly pre-training\na lattice LM on large amount of lattices because\n1) general domain written data is much easier to\ncollect than lattices which require spoken data, and\n2) LatticeRNNs are considered less efﬁcient than\nRNNs due to the difﬁculty of parallelization in\ncomputing.\n2.5 Target Task Classiﬁer Training\nAfter pre-training, our model is capable of provid-\ning representations for lattices. Following (Peters\net al., 2018), the pre-trained lattice LM is used to\nproduce contextualized node embeddings for down-\nstream classiﬁcation tasks, as illustrated in the right\npart of Figure 2. We use the same strategy as Peters\net al. (2018) to linearly combine the hidden states\nfrom different layers into a representation for each\nnode. The classiﬁer is a newly added 2-layer Lat-\nticeLSTM, which takes the node representations\nas input, followed by max-pooling over nodes, a\nlinear layer and ﬁnally a softmax layer. We use\nthe cross entropy loss to train the classiﬁer on each\ntarget classiﬁcation tasks. Note that the parameters\nof the pre-trained lattice LM are ﬁxed during this\nstage.\n3767\nATIS SNIPS SWDA MRDA\nManual (a) biLSTM - 97.00 71.19 79.99\n(b) (a) + ELMo - 96.80 72.18 81.48\nLattice oracle (c) biLSTM 92.97 94.02 63.92 70.49\n(d) (c) + ELMo 96.21 95.14 65.14 73.34\nASR 1-Best\n(e) biLSTM 91.60 91.89 60.54 67.35\n(f) (e) + ELMo 94.99 91.98 61.65 68.52\n(g) BERT-base 95.97 93.29 61.23 67.90\nLattices\n(h) biLatticeLSTM 91.69 93.43 61.29 69.95\n(i) Proposed 95.84 95.37 62.88 72.04\n(j) (i) w/o Stage 1 94.65 95.19 61.81 71.71\n(k) (i) w/o Stage 2 95.35 94.58 62.41 71.66\n(l) (i) evaluated on 1-best 95.05 92.40 61.12 68.04\nTable 2: Results of our experiments in terms of accuracy (%). Some audio ﬁles in ATIS are missing, so the testing\nsets of manual transcripts and ASR transcripts are different. Hence, we do not report the results for ATIS using\nmanual transcripts. The best results obtained by using ASR output for each dataset are marked in bold.\nATIS SNIPS SWDA MRDA\nTrain 4,478 13,084 103,326 73,588\nValid 500 700 8,989 15,037\nTest 869 700 15,927 14,800\n#Classes 22 7 43 5\nWER(%) 15.55 45.61 28.41 32.04\nOracle WER 9.19 18.79 17.15 21.53\nTable 1: Data statistics.\n3 Experiments\nIn order to evaluate the quality of the pre-trained\nlattice LM, we conduct the experiments for two\ncommon tasks in spoken language understanding.\n3.1 Tasks and Datasets\nIntent detection and dialogue act recognition are\ntwo common tasks about spoken language under-\nstanding. The benchmark datasets used for intent\ndetection are ATIS (Airline Travel Information Sys-\ntems) (Hemphill et al., 1990; Dahl et al., 1994; Tur\net al., 2010) and SNIPS (Coucke et al., 2018). We\nuse the NXT-format of the Switchboard (Stolcke\net al., 2000) Dialogue Act Corpus (SWDA) (Cal-\nhoun et al., 2010) and the ICSI Meeting Recorder\nDialogue Act Corpus (MRDA) (Shriberg et al.,\n2004) for benchmarking dialogue act recognition.\nThe SNIPS corpus only contains written text, so\nwe synthesize a spoken version of the dataset us-\ning a commercial text-to-speech service. We use\nan ASR system trained on WSJ (Paul and Baker,\n1992) with Kaldi (Povey et al., 2011) to transcribe\nATIS, and an ASR system released by Kaldi to\ntranscribe other datasets. The statistics of datasets\nare summarized in Table 1. All tasks are evaluated\nwith overall classiﬁcation accuracy.\n3.2 Model and Training Details\nIn order to conduct fair comparison with ELMo (Pe-\nters et al., 2018), we directly adopt their pre-trained\nmodel as our pre-trained sequential LM. The hid-\nden size of the LatticeLSTM classiﬁer is set to 300.\nWe use adam as the optimizer with learning rate\n0.0001 for LM pre-training and 0.001 for training\nthe classiﬁer. The checkpoint with the best valida-\ntion accuracy is used for evaluation.\n3.3 Results\nThe results in terms of the classiﬁcation accuracy\nare shown in Table 2. All reported numbers are\naveraged over at least three training runs. Rows\n(a) and (b) can be considered as the performance\nupperbound, where we use manual transcripts to\ntrain and evaluate the models. We also use BERT-\nbase (Devlin et al., 2019) as a strong baseline,\nwhich takes ASR 1-best as the input (row (g)).\nCompare with the results on manual transcripts, us-\ning ASR results largely degrades the performance\ndue to recognition errors, as shown in rows (e)-(g).\nIn addition, adding pre-trained ELMo embeddings\nbrings consistent improvement over the biLSTM\nbaseline, except for SNIPS when using manual\ntranscripts (row (b)). The baseline models trained\non ASR 1-best are also evaluated on lattice oracle\npaths. We report the results as the performance\nupperbound for the baseline models (rows (c)-(d)).\nIn the lattice setting, the baseline bidirectional\nLatticeLSTM (Ladhak et al., 2016) (row (h)) con-\n3768\nsistently outperforms the biLSTM with 1-best in-\nput (row (e)), demonstrating the importance of tak-\ning lattices into account. Our proposed method\nachieves the best results on all datasets except for\nATIS (row(i)), with relative error reduction rang-\ning from 3.2% to 42% compare to biLSTM+ELMo\n(row(f)). The proposed method also achieves per-\nformance comparable to BERT-base on ATIS. We\nperform ablation study for the proposed two-stage\npre-training method and report the results in rows\n(j) and (k). It is clear that skipping either stage\ndegrades the performance on all datasets, demon-\nstrating that both stages are crucial in the proposed\nframework. We also evaluate the proposed model\non 1-best results (row (l)). The results show that\nit is still beneﬁcial to use lattice as input after ﬁne-\ntuning.\n4 Conclusion\nIn this paper, we propose a spoken language repre-\nsentation learning framework that learns contextu-\nalized representation of lattices. We introduce the\nlattice language modeling objective and a two-stage\npre-training method that efﬁciently trains a neural\nlattice language model to provide the downstream\ntasks with contextualized lattice representations.\nThe experiments show that our proposed frame-\nwork is capable of providing high-quality represen-\ntations of lattices, yielding consistent improvement\non SLU tasks.\nAcknowledgement\nWe thank reviewers for their insightful comments.\nThis work was ﬁnancially supported from the\nYoung Scholar Fellowship Program by Ministry\nof Science and Technology (MOST) in Taiwan,\nunder Grant 109-2636-E-002-026.\nReferences\nJacob Buckman and Graham Neubig. 2018. Neural lat-\ntice language models. Transactions of the Associa-\ntion for Computational Linguistics, 6:529–541.\nSasha Calhoun, Jean Carletta, Jason M Brenier, Neil\nMayo, Dan Jurafsky, Mark Steedman, and David\nBeaver. 2010. The nxt-format switchboard corpus:\na rich resource for investigating the syntax, seman-\ntics, pragmatics and prosody of dialogue. Language\nresources and evaluation, 44(4):387–419.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pre-\ntrained language models. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2089–2095. ACL.\nAlice Coucke, Alaa Saade, Adrien Ball, Th ´eodore\nBluche, Alexandre Caulier, David Leroy, Cl ´ement\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips voice plat-\nform: an embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190.\nDeborah A Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the atis task:\nThe atis-3 corpus. In Proceedings of the workshop\non Human Language Technology, pages 43–48.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186. ACL.\nChih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li\nHuo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-\nNung Chen. 2018. Slot-gated modeling for joint\nslot ﬁlling and intent prediction. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Pa-\npers), pages 753–757. ACL.\nDaniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey\nZweig. 2014. Joint semantic utterance classiﬁcation\nand slot ﬁlling with recursive neural networks. In\n2014 IEEE Spoken Language Technology Workshop,\npages 554–559.\nDilek Hakkani-T ¨ur, Fr ´ed´eric B ´echet, Giuseppe Ric-\ncardi, and Gokhan Tur. 2006. Beyond ASR 1-\nbest: Using word confusion networks in spoken\nlanguage understanding. Computer Speech & Lan-\nguage, 20(4):495–514.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nMatthew Henderson, Milica Ga ˇsi´c, Blaise Thomson,\nPirros Tsiakoulis, Kai Yu, and Steve Young. 2012.\nDiscriminative spoken language understanding us-\ning word confusion networks. In 2012 IEEE Spoken\nLanguage Technology Workshop, pages 176–181.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation.\n3769\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339. ACL.\nChao-Wei Huang and Yun-Nung Chen. 2019. Adapt-\ning pretrained transformer to lattices for spoken lan-\nguage understanding. In Proceedings of 2019 IEEE\nAutomatic Speech Recognition and Understanding\nWorkshop, pages 845–852.\nFaisal Ladhak, Ankur Gandhe, Markus Dreyer, Lam-\nbert Mathias, Ariya Rastrow, and Bj¨orn Hoffmeister.\n2016. LatticeRNN: Recurrent neural networks over\nlattices. In Proceedings of INTERSPEECH , pages\n695–699.\nRyo Masumura, Yusuke Ijima, Taichi Asami, Hirokazu\nMasataki, and Ryuichiro Higashinaka. 2018. Neural\nconfnet classiﬁcation: Fully neural network based\nspoken utterance classiﬁcation using word confu-\nsion networks. In Proceedings of 2018 IEEE Inter-\nnational Conference on Acoustics, Speech and Sig-\nnal Processing, pages 6039–6043.\nGr´egoire Mesnil, Yann Dauphin, Kaisheng Yao,\nYoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-\naodong He, Larry Heck, Gokhan Tur, Dong Yu, and\nGeoffrey Zweig. 2014. Using recurrent neural net-\nworks for slot ﬁlling in spoken language understand-\ning. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 23(3):530–539.\nDouglas B. Paul and Janet M. Baker. 1992. The de-\nsign for the wall street journal-based csr corpus. In\nProceedings of the Workshop on Speech and Natural\nLanguage, HLT ’91.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237. ACL.\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas\nBurget, Ondrej Glembek, Nagendra Goel, Mirko\nHannemann, Petr Motlicek, Yanmin Qian, Petr\nSchwarz, et al. 2011. The Kaldi speech recognition\ntoolkit. Technical report.\nAlec Radford. 2018. Improving language understand-\ning by generative pre-training.\nElizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy\nAng, and Hannah Carvey. 2004. The ICSI meeting\nrecorder dialog act (MRDA) corpus. In Proceedings\nof the 5th SIGdial Workshop on Discourse and Di-\nalogue at HLT-NAACL 2004 , pages 97–100, Cam-\nbridge, Massachusetts, USA. ACL.\nAditya Siddhant, Anuj Goyal, and Angeliki Metallinou.\n2018. Unsupervised transfer learning for spoken\nlanguage understanding in intelligent agents. arXiv\npreprint arXiv:1811.05370.\nMatthias Sperber, Graham Neubig, Ngoc-Quan Pham,\nand Alex Waibel. 2019. Self-attentional models for\nlattice inputs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1185–1197. ACL.\nAndreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-\nbeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul\nTaylor, Rachel Martin, Carol Van Ess-Dykema, and\nMarie Meteer. 2000. Dialogue act modeling for au-\ntomatic tagging and recognition of conversational\nspeech. Computational Linguistics, 26(3):339–374.\nG¨okhan T¨ur, Anoop Deoras, and Dilek Z. Hakkani-T¨ur.\n2013. Semantic parsing using word confusion net-\nworks with conditional random ﬁelds. In Proceed-\nings of INTERSPEECH.\nGokhan Tur, Dilek Hakkani-T¨ur, and Larry Heck. 2010.\nWhat is left to be understood in ATIS? In Proceed-\nings of 2010 IEEE Spoken Language Technology\nWorkshop (SLT), pages 19–24.\nGokhan Tur, Jerry Wright, Allen Gorin, Giuseppe Ric-\ncardi, and Dilek Hakkani-T¨ur. 2002. Improving spo-\nken language understanding using word confusion\nnetworks. In Seventh International Conference on\nSpoken Language Processing.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, pages 6000–6010. Curran Associates Inc.\nFengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang,\nand Kehai Chen. 2019. Lattice-based transformer\nencoder for neural machine translation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3090–\n3097. ACL.\nKaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge-\noffrey Zweig, and Yangyang Shi. 2014. Spoken lan-\nguage understanding using long short-term memory\nneural networks. In 2014 IEEE Spoken Language\nTechnology Workshop, pages 189–194.\nPei Zhang, Niyu Ge, Boxing Chen, and Kai Fan. 2019.\nLattice transformer for speech translation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6475–\n6484. ACL.\nYue Zhang and Jie Yang. 2018. Chinese NER using\nlattice LSTM. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1554–1564.\nACL.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.852149248123169
    },
    {
      "name": "Language model",
      "score": 0.6898159980773926
    },
    {
      "name": "Natural language processing",
      "score": 0.6621430516242981
    },
    {
      "name": "Spoken language",
      "score": 0.6369950771331787
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5779446363449097
    },
    {
      "name": "Speech recognition",
      "score": 0.46802812814712524
    },
    {
      "name": "Code (set theory)",
      "score": 0.4313175082206726
    },
    {
      "name": "Programming language",
      "score": 0.08934682607650757
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ],
  "cited_by": 9
}