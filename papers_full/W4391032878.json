{
    "title": "DB-GPT: Large Language Model Meets Database",
    "url": "https://openalex.org/W4391032878",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2662626983",
            "name": "Xuanhe Zhou",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2137981111",
            "name": "Zhaoyan Sun",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2171250815",
            "name": "Guoliang Li",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2662626983",
            "name": "Xuanhe Zhou",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2137981111",
            "name": "Zhaoyan Sun",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2171250815",
            "name": "Guoliang Li",
            "affiliations": [
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3095319910",
        "https://openalex.org/W3108032709",
        "https://openalex.org/W3025775630",
        "https://openalex.org/W3173850788",
        "https://openalex.org/W3174969457",
        "https://openalex.org/W6675775337",
        "https://openalex.org/W4285129823",
        "https://openalex.org/W4385571789",
        "https://openalex.org/W6727875242",
        "https://openalex.org/W4389520103",
        "https://openalex.org/W4385573504",
        "https://openalex.org/W6600577311",
        "https://openalex.org/W6604662147",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W4385571445",
        "https://openalex.org/W4385571260",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W4221145545",
        "https://openalex.org/W2991530444",
        "https://openalex.org/W3180608480",
        "https://openalex.org/W3207553988",
        "https://openalex.org/W3097225903",
        "https://openalex.org/W3152893301",
        "https://openalex.org/W3037027022",
        "https://openalex.org/W6826116265",
        "https://openalex.org/W2962771342",
        "https://openalex.org/W6600388300",
        "https://openalex.org/W6600474023",
        "https://openalex.org/W3034368386",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2154137718",
        "https://openalex.org/W3187134297",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W4281972940",
        "https://openalex.org/W4312537169",
        "https://openalex.org/W3124277639",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W3138154797",
        "https://openalex.org/W3088409176"
    ],
    "abstract": "Abstract Large language models (LLMs) have shown superior performance in various areas. And LLMs have the potential to revolutionize data management by serving as the \"brain\" of next-generation database systems. However, there are several challenges that utilize LLMs to optimize databases. First, it is challenging to provide appropriate prompts (e.g., instructions and demonstration examples) to enable LLMs to understand the database optimization problems. Second, LLMs only capture the logical database characters (e.g., SQL semantics) but are not aware of physical characters (e.g., data distributions), and it requires to fine-tune LLMs to capture both physical and logical information. Third, LLMs are not well trained for databases with strict constraints (e.g., query plan equivalence) and privacy-preserving requirements, and it is challenging to train database-specific LLMs while ensuring database privacy. To overcome these challenges, this vision paper proposes a LLM-based database framework (), including automatic prompt generation, DB-specific model fine-tuning, and DB-specific model design and pre-training. Preliminary experiments show that achieves relatively good performance in database tasks like query rewrite and index tuning. The source code and datasets are available at github.com/TsinghuaDatabaseGroup/DB-GPT.",
    "full_text": "Vol:.(1234567890)\nData Science and Engineering (2024) 9:102–111\nhttps://doi.org/10.1007/s41019-023-00235-6\nRESEARCH PAPER\nDB‑GPT: Large Language Model Meets Database\nXuanhe Zhou1 · Zhaoyan Sun1 · Guoliang Li1 \nReceived: 6 June 2023 / Revised: 27 June 2023 / Accepted: 26 July 2023 / Published online: 19 January 2024 \n© The Author(s) 2024\nAbstract\nLarge language models (LLMs) have shown superior performance in various areas. And LLMs have the potential to revolu-\ntionize data management by serving as the \"brain\" of next-generation database systems. However, there are several challenges \nthat utilize LLMs to optimize databases. First, it is challenging to provide appropriate prompts (e.g., instructions and demon-\nstration examples) to enable LLMs to understand the database optimization problems. Second, LLMs only capture the logical \ndatabase characters (e.g., SQL semantics) but are not aware of physical characters (e.g., data distributions), and it requires to \nfine-tune LLMs to capture both physical and logical information. Third, LLMs are not well trained for databases with strict \nconstraints (e.g., query plan equivalence) and privacy-preserving requirements, and it is challenging to train database-specific \nLLMs while ensuring database privacy. To overcome these challenges, this vision paper proposes a LLM-based database \nframework (DB-GPT), including automatic prompt generation, DB-specific model fine-tuning, and DB-specific model design \nand pre-training. Preliminary experiments show that DB-GPT achieves relatively good performance in database tasks like \nquery rewrite and index tuning. The source code and datasets are available at github.com/TsinghuaDatabaseGroup/DB-GPT.\nKeywords Large language model · Database\n1 Introduction\nLarge language models (LLMs) are pre-trained with a super \nlarge model capacity (e.g., over 170 billion network param-\neters in GPT-3 [1 ]) and a large data corpus (e.g., over 8 \nmillion website pages as training data), which is good at \nunderstanding human knowledge and instructions. Recently, \nLLMs have demonstrated superiority in various tasks like \ntext generation [2 ], machine translation [3 ], and program \nsynthesis [4]. Thus, a natural question is whether LLMs can \nbe used to accomplish database tasks.\nTask 1: Query Rewrite  In Fig.  1, the query rewrite task is \ndescribed in three parts. (i) Instruction includes the overall pro-\ncedure and target of the task. In this case, we aim to write an \nequivalent query that can be executed on Postgres and achieves \nlower latency than origin query. Note it is critical to ensure the \nLLM captures the key points in the instruction, as these points \nmay be overlooked or misunderstood. (ii) Examples are simpli-\nfied demonstrations of query rewrite. These examples teach the \nLLM how to use rewrite rules, which cannot be well covered \nin the instruction. (iii) Input provides the necessary informa-\ntion to accomplish the task (e.g., the SQL query). We input the \nthree parts in the form of ([Instruction], [Examples], \n[Input]) into LLMs, which asks LLMs to rewrite the input \nquery (e.g., pulling up the nested subquery as table joins) and \nappend the rewritten query after the input.\nTask 2: Index Tuning Similarly, the index tuning task \ninvolves (i) the main procedure (e.g., creating a sequence \nof indexes) and task target (e.g., reducing the latency within \nlimited space) and (ii) examples like inputting a two-table \njoin query and outputting two indexes that use the columns \nin the queries and (iii) the input including some new queries, \nfor which we need to create suitable indexes. In this case, the \nLLM recommends some indexes so as to actually optimize \nthe bottleneck operators in the input queries (e.g., orderby) \nand avoid redundant indexes.\n * Guoliang Li \n liguoliang@tsinghua.edu.cn\n Xuanhe Zhou \n zhouxuan19@mails.tsinghua.edu.cn\n Zhaoyan Sun \n szy22@mails.tsinghua.edu.cn\n1 Department of Computer Science, Tsinghua University, \nBeijing, China\n103DB-GPT: Large Language Model Meets Database  \nCompared with existing AI4DB works [5 –7], LLMs for \ndatabase (LLM4DB) have three advantages. (1) Higher \ntransfer capability: Unlike existing instance-optimal works \nthat can optimize an instance but cannot be extended to \nother instances, LLM4DB demonstrates exceptional trans-\nfer capability. By leveraging just a few fine-tuning samples, \nLLM4DB can achieve comparable performance on novel \ndatabase tasks, making it adaptable to schema, workload, or \neven data and hardware changes; (2) User-friendly interface: \nLLM4DB offers an intuitive user experience by allowing \nusers to provide some prompts as hints to guide the model’s \ninference. Instead, AI4DB typically requires substantial \namounts of training data (supervised models) or multiple \niterations (reinforcement learning) to capture and incorpo-\nrate user feedback. (3) Prior Knowledge Learning: LLM4DB \nis capable of extracting insights from existing database com-\nponents, including documents or even code. By integrating \nthe strengths of these components, LLM4DB can enhance its \nperformance while mitigating their individual weaknesses.\nHowever, there remain some challenges to achieve com-\nparable or even better performance.\nC1 How to Generate Input Prompts for Database Tasks?  \nFirst, the quality of the instructions provided to the LLM can \naffect its performance on a specific task. For instance, the \nwriting style or complexity of the instructions may not be \nwell suited for the model’s comprehension, resulting in poor \nperformance. Therefore, it is crucial to automatically select \nsuitable task instructions (e.g., “rewrite the SQL query to \nreduce complexity and improve performance...”) from a large \npool of candidate instructions. Second, for the same task, it \nis important to provide some relevant examples for a given \ninput (e.g., the rewrites of queries that are similar to the input \nquery). These examples can provide insights on how to apply \nprior knowledge to handle complicated cases (e.g., rewriting \nqueries that require to apply multiple rules) [8].\nC2 How to Fine-Tune the LLMs for Database Tasks? First, \ndata characters (e.g., data distributions, indexes) may sig-\nnificantly affect the optimization decisions of LLMs (e.g., \nbuilding indexes for columns with a large number of distinct \nvalues). However, it is challenging for LLMs to capture the \nrelations between data distribution and target tasks, e.g., \ndescribing the critical data characters in natural language or \nmodel-friendly embeddings. Second, since some database \ntasks only offer limited high-quality labeled samples (e.g., \nreal queries with optimal rewritten strategy) for fine-tuning, \nwe should explore how to better utilize the training samples.\nC3 How to Design a Database-Specific LLM? First, differ-\nent from NLP tasks, database tasks involve strict constraints \n(e.g., providing equivalent query plans for query rewrite) \nand structural information (e.g., the plan tree of a query), \nwhich are hard to support or learn by only using existing \nLLMs. Second, there are numerous public texts in NLP \ntasks, which can be taken as the training samples for LLMs. \nHowever, in databases, the data and queries are of high pri-\nvacy, and it is vital to ensure the privacy while utilizing them \nto train the LLMs.\nTo tackle these challenges, we propose a database opti-\nmization framework by using LLMs (DB-GPT). We have \nthree main contributions. ❶ We recommend several prompt \ngeneration methods that offer the valuable text information \n(e.g., task instructions [ 9–12], demonstration examples [ 2, \n13–25]) to accomplish database tasks with high perfor -\nmance. The proposed methods include ( i) automatically \nselecting the suitable task instruction, (ii) efficiently select-\ning demonstration examples with an RL model, and (iii) \ntrading off between prompt length and LLM performance \nto reduce the model inference cost. ❷  We provide several \nmethods to facilitate model fine-tuning using a small num-\nber of labeled samples for specific database tasks [21, 23, \n26–29], including (i) non-text data embedding, (ii) annota-\ntions for low-quality data samples, (iii) contrastive learning \nfor additional sample generation, and (iv) delta tuning that \nreduces the number of tunable network parameters while \nachieving similar performance. ❸  We propose to design \nand train a database-specific LLM  [30, 31] with (i) validity \nchecking for the output of LLMs, (ii) structural information \nlearning from numerous database logs (composed of various \nworkloads, optimization actions, and even result data) and \n(iii) federated learning that preserves data privacy during \ntraining the LLM.\nFig. 1  Large language model for database\n104 X. Zhou et al.\n2  Opportunities of Using LLMs for DB\n2.1  Overview\nAs shown in Fig. 2, this section presents three strategies that lev-\nerage LLMs to optimize database tasks, including input prompt \ngeneration (Sect.  2.2), database-specific LLM fine-tuning  \n(Sect. 2.3), and database-specific LLM design and pre-training \n(Sect. 2.4). Input prompt generation aims to generate addi-\ntional text information to guide LLMs in understanding the task \nrequirements, which can directly use existing LLMs to optimize \ndatabase tasks. Input prompt generation will not re-train LLMs, \nmaking it most efficient to use. However, it requires the LLMs \nto own the relevant database knowledge ahead of time, while \nonly a few advanced LLMs like GPT−3.5 satisfy this require-\nment. LLM fine-tuning updates network parameters (a small \npart in delta tuning) so as to memorize task-specific knowledge, \nwhich can accept non-text features with additional embedding \nlayers and achieve better performance than input prompt genera-\ntion. DB-specific LLM design and pre-training require a large \nnumber of database-specific training samples to learn network \nparameters, which can serve as the foundation model for data-\nbases. That is, by providing essential system characters (e.g., \nequivalence verification) and training mechanisms (e.g., feder-\nated learning) for database tasks, it can enhance the effectiveness \nof both input prompt generation and LLM fine-tuning.\n2.2  Input Prompt Generation\nMotivation With the input x of a database task, we can add \nadditional text information to the input x , called the input \nprompt x/uni2032.var , which helps LLM to better understand the task \nrequirements. However, different inputs may correspond to \ndifferent optimal prompts (e.g., queries of different struc-\ntures may require different rewrite examples) [2, 16, 22, 24, \n25, 32, 33], and it is hard and tedious for users to give good \nprompts. We need to build a prompt generator to automati-\ncally derive the prompt for input x.\nMethodology There are mainly two critical parts in prompts: \n(i) instruction to guide LLM, and (ii) demonstration exam-\nples for LLM to simulate. The generated prompts should \nbe validated by the feedback of LLM, based on which we \ncan optimize the generator for good LLM performance. Fur-\nthermore, when we use prompts to interact with LLM, we \nshould reduce the interaction latency (e.g., reducing interac-\ntion rounds) and cost (e.g., the input token number of each \ninteraction round) for real applications (Table 1).\nChallenge 1 How to Automatically Generate Input \nPrompt? It is challenging to automatically generate appro-\npriate instructions and demonstration examples to guide \nLLMs to optimize different database tasks with a limited \nnumber of prompt tokens (or interaction rounds with LLMs).\nVision 1  We can concatenate instruction and demonstration \nexamples as additional text information in the prompt, which \nis organized as “[Instruction] [Examples] Input: [x] Output:.” \nNote we place task instruction before demonstration examples \nfor two reasons: (i) It follows the natural progression of teaching \nthe model to finish a task, i.e., introducing the problem before \nshowing how to solve it; (ii) Instruction provides the contextual \ninformation for the examples, making the model easier to build \nconnection between the examples and task purpose (e.g., given \nthe query rewrite instruction, the model can focus on the struc-\ntural changes in the examples). Next we, respectively, explain \nhow to automatically generate the instruction and demonstration \nexamples (Fig. 3).\n• Instruction\n The quality of task instructions can impact the performance \nof LLM on different tasks. Thus, we present a method to \nFig. 2  Three strategies of using LLM for database tasks\n105DB-GPT: Large Language Model Meets Database  \nautomatically generate instructions with a limited number \nof samples [10, 11]. First, we utilize LLM to suggest instruc-\ntion candidates based on a small set of input–output pairs \n(e.g., five pairs for an instruction). Second, we rank these \ngenerated instructions based on a customized scoring func-\ntion (e.g., the average performance on test workloads), and \nreserve the best instructions (e.g., top-10) as candidates. \nThird, we utilize search-based methods (e.g., Monte Carlo \nSearch) to improve the candidates with LLM (e.g., outputting \ninstruction variants with similar semantics as a candidate). \nFinally, we select the best instruction to serve as the input \nfor the task.\n• Demonstration examples\n are selected from a candidate set {si} . Unlike instruction \ngeneration, example selection depends on the input x . If an \nexample is more similar to the input, it provides more rel-\nevant information to the LLM. Specifically, we learn an input \nencoder EX(⋅) and an example encoder ES(⋅) , and calculate the \nsimilarity between EX (x) and E S (si) for all candidate examples \nusing L2 distance [2, 22, 24]. For instance, if x and si are both \nSQLs, they may share similar operators such as COUNT(⋅) or \nJOIN. We select the top k examples and place them before x \nin ascending order of similarity (i.e., more similar examples \nare closer to the input). Since adjacent tokens in an exam -\nple si and x have similar position embeddings, this helps the \nLLM focus on the input–output mappings of the most similar \nexamples [2]. Note that the candidate set of examples is typi-\ncally collected from real-world applications, such as the 36 \nexamples that cover typical rules for query rewrite. If there \nis no such candidate set for a new task, we can obtain a few \nhand-crafted examples from an expert, and use the LLM to \nderive more examples from them, (e.g., using prompts like \n“generate new queries with different structures but using the \nsame rewrite rules”).\nChallenge 2: How to Efficiently Interact with LLM Using \nPrompts? Many database tasks require low latency. How -\never, there are three factors that can increase the interac-\ntion latency and cost with LLM. (i ) It can be time-consum-\ning to actively generate prompt for input (e.g., retrieving \nsuitable examples from candidate ones). ( ii) Long prompts \noften include more useful information for LLM, but can \ntake longer processing time for LLM. (iii) Some complex \ntasks can be better solved by calling LLM for several \nrounds and interactively adjusting the prompt. Thus, it is \nimportant to efficiently generate prompts and reduce the \nlatency and cost of LLM interactions.\nVision 2  To address the issue of costly prompt genera-\ntion, one possible solution is to train a reinforcement learn-\ning (RL) model, such as Q-learning, on a set of candidate \nexamples. This model can be employed to identify the most \nsuitable example for selection, thus eliminating the necessity \nto search through the entire collection of candidates [20]. \nBy calling the RL model a fixed number of times, which is \nequal to the number of examples required in the prompt, we \ncan generate the prompt more efficiently.\nSecondly, it is important to strike a balance between \nprompt length and LLM performance. When selecting \ninstructions and demonstration samples, we should not only \nconsider their performance on the validation set but also \nprioritize shorter ones.\nFinally, incorporating information from previous rounds \ninto the prompt can improve the effectiveness of prompt-\nbased interactions with LLM. For instance, when adjusting \ndatabase knobs based on the outputs of the LLM, we can \nrecord the outputs (e.g., tuned knobs) and their actual per -\nformances (e.g., workload throughput under tuned knobs) \nfrom previous rounds, and incorporate them into the prompt. \nThis can help the LLM make more accurate inferences dur -\ning subsequent rounds of interaction [34].\nTable 1  LLM strategy comparison\nInput prompt Fine-tuning Pre-training\nTunable param-\neters\nThousands Tens of billion Hundred billion\nData samples Dozens Thousands Millions\nInput features Text data Text/non-text \ndata\nText/non-text \ndata\nFig. 3  Automatic prompt generator\n106 X. Zhou et al.\n2.3  LLM Fine‑Tuning\nMotivation Apart from text prompts, some database tasks \n(e.g., physical query plan generation) require non-textual \ninformation that is not readily expressible in natural lan -\nguage. Moreover, fine-tuning can enhance the task-specific \nperformance of LLM. However, the effectiveness of the fine-\ntuned model is significantly influenced by the size and qual-\nity of the labeled data samples.\nMethodology First, we can train non-text embeddings \nduring fine-tuning and combine them with natural language \nembeddings. Second, we should make good utilization of \nexisting data for model fine-tuning, and continuously collect \nnew fine-tuning data from application feedbacks of LLM. \nThird, we explore new methods to enhance LLM fine-tuning.\nChallenge 3 How to Embed Non-text Input Features?  \nSome database features cannot be directly embedded as text \ndue to its verbosity (e.g., data distributions), which easily \nexceeds the input length limit of LLM (e.g., ~3000 words \nfor GPT−3.5 [35]). To address this issue, we need to explore \nhow to embed non-text features and incorporate them with \ntext input features during fine-tuning.\nVision 3  First, we provide two examples of non-text \nembeddings. (i) Data distribution is an important factor that \naffects various aspects of database tasks (e.g., operator costs, \nquery results). We can use a model ED to embed the distribu-\ntion of table column data, and the embedding vector for col-\numn t.c is denoted as ED (t.c.data) . For instance, we can first \nuse quantiles such as (min, p01,..., p99, max) to approximate \nthe distribution of the column data and then embed them \nwith models like Transformer [30, 36]. Note that text embed-\ndings represent the semantic and syntactic characteristics \nof text words. Thus, ED should be trained to convert these \ndata characteristics into the same embedding space as text  \nduring fine-tuning [37]. (ii) Query correlations reflect the \nexecution state of workloads in the same database and form \na graph model, which cannot be learned well by sequential \nmodels. Therefore, we can use a model ER to embed the cor-\nrelations between concurrent queries. The embedding vector \nfor query q is denoted as E R (q, q.correlated_queries) , where \nq.correlated_queries  are the queries accessing the same table \ncolumns as q . For instance, we can create a graph where \neach query is a node and the query correlations can be repre-\nsented by the edge type and weight. With the graph, we can \nutilize models like graph neural network to embed the graph \nstructural information into a vector ER(⋅)  [38, 39]. Note that \nthe reason why non-text features are not described in natural \nlanguage is that it may cause extremely long text input (e.g., \na complex query may involve hundreds of columns and even \nmore concurrent queries) and the relevant information may \nscatter sparsely among the long text, making it difficult for \nLLM to capture useful information.\nSecond, we can set the non-text embeddings to have the \nsame dimensions as text embeddings, and train the non-text \nembedding models together with LLM in fine-tuning. During \nusage, we separately embed text features and non-text fea -\ntures, and then concatenate the embedding vectors to obtain \nan input sequence of LLM, which can be further processed.\nChallenge 4 How to Provide Sufficient High-Quality Data \nfor Fine-Tuning? Some database tasks may lack sufficient \nhigh-quality data (e.g., tens of thousands of samples) to fine-\ntune LLM. This may occur for two reasons. First, obtaining \ntask-specific data is costly. For example, it takes weeks to \ncollect fine-tuning data for knob tuning, which includes the \ntarget workload, knob settings, and the actual performance. \nSecond, even with abundant data, there may be lack of high-\nquality annotations (e.g., reasoning processes for slow SQL \nquery diagnosis) to facilitate the learning of task-specific \nknowledge by LLM. Therefore, it is essential to explore \nmethods to make good use of existing data, and continu-\nously collect data from LLM usage feedback.\nVision 4  We propose two potential solutions. First, we \ncan use contrastive learning to generate additional fine-tun-\ning samples from the dataset [ 27]. For instance, in knob \ntuning, we can obtain k  knob settings along with their cor -\nresponding performance metrics. By using LLM to compare \nthe performance of each pair of knob configurations, we can \ngenerate \n/parenleft.s3k\n2\n/parenright.s3\n samples (which is significantly larger than k) \nfor fine-tuning LLM.\nSecond, for low-quality data samples, we can leverage \nLLM to generate annotations, such as the reasoning process \nof data sample (chain of thought [23, 26, 40]), which can \nhelp to improve the quality of the data. For example, we can \nuse LLMs to diagnose the root causes of slow database per-\nformance (e.g., the latency of a workload is over 50% higher \nthan the normal latency), which can be used as annotations \nof monitoring metric data. Specifically, we first input the \nmonitoring data (e.g., system views, query logs) and a set \nof potential annotations to LLM. Next, the LLM selects the \nannotation with the highest probability of causing the slow \nperformance as the final annotation.\nThird, we can monitor the performance of LLM and \nrecord scenarios where the LLM performs poorly by log-\nging input features and its corresponding outputs. We add \nsuch data samples into fine-tuning data. We not only increase \nthe size of our fine-tuning data, but also capture weaknesses \nin the LLM.\nChallenge 5 How to efficiently fine-tune LLM? The large \nscale of LLM makes fine-tuning both time-consuming and \ncostly. Therefore, we need to explore efficient fine-tuning \nmethods.\n107DB-GPT: Large Language Model Meets Database  \nVision 5  Traditional fine-tuning allows to tune all the \nnetwork parameters. Instead, tuning a small part of the mode \nparameters (delta tuning [28]) can achieve similar perfor -\nmance for the task and is much more efficient. Here, we \nintroduce two delta-tuning methods. First, we can add extra \ntunable parameters to LLM. For example, we can inject \nadapter modules (e.g., small MLP) to each Transformer \nmodel [41]. We only tune these adapter modules during fine-\ntuning, and keep the original parameters of LLM frozen. \nNote that this method can also reduce the storage cost of \nthe fine-tuned LLM. If we fine-tune LLM for many tasks, \nwe can store the parameters of adapter modules and only \nstore one copy of the original LLM. Second, the fine-tuning \nprocess optimizes parameters from /u1D6E9 to /u1D6E9/uni2032.var , and we denote \nthe difference of parameters as /u1D6E5/u1D6E9= /u1D6E9� − /u1D6E9 . Then, we can \ndecompose /u1D6E5/u1D6E9 into low-dimensional representations, e.g., \nW = BA , where W denotes weight matrix of /u1D6E5/u1D6E9 and B , A \ndenote the weight parameters we actually tune [42]. We \nassume W ∈ Rd×k , B ∈ Rd×r , and A ∈ Rr×k ( r ≪ d ), where \nd, k denote the size of the weight matrix, and r  denotes the \ndecomposed rank. B and A have much fewer parameters than \nW and are more efficient to tune.\n2.4  DB‑Specific LLM Design and Pre‑training\nMotivation Unlike natural language tasks, database tasks \nhave unique characteristics such as strict output constraints \nand a large amount of unique data features (e.g., data distri-\nbutions, metadata, data statistics, and query logs). However, \nexisting LLMs only use text data on surface web but do not \ncapture the data in hidden database, and thus face difficulties \nin handling complicated database tasks.\nMethodology First, we need a new model design to \nensure the validity of the LLM output. Second, there are \na large-scale diverse data in database, e.g., SQLs and their \nphysical plans, slow SQLs, database metrics, database statis-\ntics, etc. We should collect and organize them into training \nsamples for LLMs and teach LLMs how databases work. \nThird, data in database tasks is often business-sensitive and \nmission critical. We should avoid data leakage during both \nLLM pre-training and LLM inference.\nChallenge 6 How to Ensure the Validity of LLM Output? \nAlthough LLM cannot guarantee 100% accuracy on the \ntask results, certain database tasks require strict constraints \n(e.g., the output of query rewrite must be a semantically \nequivalent query and the query must be executable on the \ndatabase). It is essential to ensure that all invalid outputs are \ndetected or avoided.\nVision 6  We propose to adopt a hybrid method to ensure \nthe validity of model output. First, we should design special \ntraining set so that LLM can maximize the possibility of \ngenerating valid outputs (e.g., queries that satisfy the SQL \nsyntax). Second, for relatively simple cases, we adopt a non-\nlearned checking layer to validate the output (e.g., using an \nSMT solver for simple SPJ queries [43]). For more complex \ncases, we adopt a learned checking layer (e.g., a binary clas-\nsifier) to validate the output. Similar to a learned Bloom \nfilter [44, 45], if the output is judged as \"invalid\", the learned \nchecking layer feeds back the illegal result to the LLM, and \nthe LLM regenerates the output. Otherwise, the output needs \nto be double-checked on simplified cases (e.g., comparing \nthe execution results of the original and rewritten queries on \nsampled data) [28].\nChallenge 7 How to Train LLM with Database Data? A \nsignificant amount of data is required to train a high-quality \nLLM  [1, 46–49]. For a database-specific LLM, we expect it \nto learn general knowledge that applies to all database tasks. \nIt can learn basic knowledge from database documents or \nblogs by human experts, but such natural language texts are \nof a limited size. Thus, it is extremely important to learn a \ndatabase-specific LLM.\nVision 7  Training data in databases has different char -\nacteristics compared to natural language text corpus. First, \nthe database training samples may have different formats, \ne.g., well-structured SQLs and query plans, semi-structured \nlogs, and unstructured documents. Thus, we need to well \nrepresent different data samples and concatenate them for \ntraining LLM effectively.\nFor example, we can record query execution and obtain a \nsequence like “[Table Data][Query]→[Logical Plan]→[Phys-\nical Plan]→[Result][Execution Time].” By learning the rel-\nevance within such sequences, LLM can automatically learn \nhow to conduct query optimization. Second, database data \ncontains plenty of structural information (e.g., tree structure \nof query plan). Special designs are required to enable the \nDB-specific LLM to better utilize the structure information. \nFor example, we can combine the design of graph neural net-\nworks to support complex graph topological structures [50].\nMoreover, since some database data is very long and \nexceeds the length limit of Transformer-based LLMs, we \ncan use long-range attention to learn correlations among dis-\ntant words [51]. Specifically, we can store the encoded vec-\ntors (keys and values) of historical tokens in a large external \nmemory. When training LLM for a particular token xi , we \ncan search for the k nearest vectors in the external memory \nand use the attention mechanism to encode xi with them. \nThis enables LLM to learn basic knowledge from long data-\nbase training samples.\nChallenge 8 How to pre-train and apply LLM while ensur-\ning data privacy? User data in a database must be protected \nfrom leakage due to privacy concerns. This prevents user \ndata from being integrated into the database provider for \n108 X. Zhou et al.\ncentralized LLM pre-training. Moreover, some users can-\nnot send their sensitive data as input to DB-GPT through \ninference APIs.\nVision 8  To address this challenge, we propose two solu-\ntions. First, we can use privacy-preserving federated learning \nto train LLM  [30, 31]. In this approach, a server (e.g., data-\nbase provider) collaborates with clients (e.g., users) to train \nLLM for several rounds. In each round, the client receives \nsome server information (e.g., server network parameters) \nand updates their local network parameters. They then train \ntheir local model with their local data and send some local \ninformation (e.g., their local gradients) to the server. The \nserver updates its network parameters by aggregating the \nlocal information from clients, and starts the next round by \nsending updated server information. This approach ensures \ndata privacy since users always manipulate their own data.\nSecondly, due to business reasons, we are unable to \ndirectly provide the network parameters in DB-GPT to \nusers. Instead, we can distill the knowledge of DB-GPT  \ninto a smaller model that performs similarly to LLMs for \ndesired tasks and provide this model to the user [26, 52]. \nSpecifically, we require the user to provide a dataset of their \ndatabase’s application scenarios (e.g., workloads, data dis-\ntributions). Using this dataset, we annotate with DB-GPT  \noutput (see details in Sect. 2.2) and train a simplified model \nwith this new dataset. It is important to note that user appli-\ncations are relatively deterministic, which makes a smaller \nmodel sufficient despite its lower generalizability.\n3  Practices\nWe have conducted preliminary practices to show the effec-\ntiveness of automatically generated prompts, i.e., LLMs \nequipped with these prompts can achieve comparable perfor-\nmance as the state-of-the-art methods. To explore the upper \nlimits of these LLMs, we test with two most advanced mod-\nels (checked until June, 2023), including “text-davinci-003’ \nthat owns 175 billion parameters and supports up to 4,097 \ntokens per request, “gpt-4” that supports up to 8,192 \ntokens per request and generally can follow more complex \ninstructions.\n3.1  Implementation\nThere are three main steps in our initial implementation of \nDB-GPT. First, we generate the instruction from a small \nnumber of collected samples (splitting into training and \nevaluation sets), i.e., deriving several instructions using the \nLLM on training set and choosing the best instruction by \nevaluating on evaluation set (Sect.  2.2). Second, based on \nthe task requirements, we collect other input features such \nas demonstration examples (e.g., query rewriting pairs) and \ndata statistics (e.g., distinct value ratios of the columns). \nFinally, we concatenate the instruction, collected features, \nand the input into a prompt sequence, and rely on the LLM to \noutput desired results based on the prompt sequence.\n3.2  DB-GPT for Query Rewrite\nDatasets We generate instructions on the training set and \nverify the performance on the evaluation set. (i) The training \nset contains 36 query rewrite pairs {(x, y)} , where y is rewrit-\nten from x with a unique rewrite rule. Thus, the training set \ncovers some frequently used rewrite rules in real usage. (ii) \nThe evaluation set contains 12 queries sampled from two \nreal-word datasets: the dataset Shopmall contains 78 tables \nand 298,208 sampled tuples; and the dataset Goods contains \n41 tables and 683,130 sampled tuples .1\nEmpirical Analysis As shown in Fig.  4a, DB-GPT  \n(davinci-003) and DB-GPT (gpt−3.5-turbo) separately out-\nperform the PostgreSQL rewriter by 9.8% and 22.4%. First, \nDB-GPT (davinci-003) effectively rewrites 6 out of the 12 \nqueries, which results in latency reductions ranging from \n0.6% to 82.5%. With the proper instruction, i.e., “Rewrite \nthe input SQL query to produce an equivalent query that \ncan be executed on a PostgreSQL database with decreased \nlatency,” DB-GPT (davinci-003) is inspired to apply more \ncomplex rewrite rules than the built-in rules in PostgreSQL, \nsuch as converting subqueries with aggregations into joins. \nSecond, DB-GPT (gpt−3.5-turbo) outperforms DB-GPT  \n(davinci-003) by optimizing some complex queries (e.g., \nqueries that involve both join reordering and join predicate \nFig. 4  Performance compari-\nson. a Query rewrite on 12 real \nqueries of the Shopmall and \nGoods datasets. b Index tuning \non 10 workloads of the imd-\nbload dataset. The index space \nis limited within 500 M\n1 https:// github. com/ Tsing huaDa tabas eGroup/ DB- GPT/ tree/ main/ \nprompt_ templ ate_ scrip ts\n109DB-GPT: Large Language Model Meets Database  \npushdown), where the davinci-003 model fails to address. \nBecause gpt-3.5-turbo is trained on larger human language \ndatasets, enabling it to undertake complex reasoning tasks \n(e.g., multi-step rewrites). Moreover, to further optimize the \nrewrite performance, we incorporate a demonstration exam-\nple in DB-GPT models’ prompt, which outlines the step-by-\nstep process for the complex rewrites that DB-GPT without \ndemonstration example fails to achieve (shown in Fig. 1).\nWe find the models used in DB-GPT learn to replace \nthe subquery “sku_id > (...detail.store_id = info.id)” with \nINNER JOIN operator, and the subquery no longer depends \non the external query. Note the demonstration example is \nnot the same as the input query (e.g., no “>” in the example, \nand the names are abbreviated), which guides DB-GPT to \nrewrite queries involving similar structures. Besides, DB-\nGPT can also replace the subquery “exists(...)” with INNER \nJOIN operator, which is not included in the example. That \nmeans that DB-GPT can utilize the knowledge from the \nexample without forgetting the rewrite rules learned from \npre-training.\nLimitations First, we should design a candidate example \nset for practical usage, which is large enough and covers all \ncommon rewrite skills (e.g., typical rules or even rule com-\nbinations). Then for an input query, similar examples can \nbe selected from it, which can guide LLM. Second, query \nrewrite is a multi-step reasoning task, and we may inter -\nact with LLM for multiple rounds to obtain a better rewrit-\nten query [34]. For instance, we can input the temporarily \nrewritten query to LLM, and ask it to further rewrite it. This \nprocess can be repeated until the output query is optimal \n(e.g., evaluated by LLM) or reaching time limit.\nDB-GPT for Index Tuning\nDatasets. We first generate 60 instructions from the 180 \npairs (workload, optimal indexes) on the TPC-H and TPC-\nDS datasets, from which we take the prompt that achieves \nthe best performance on the 20 workloads of the IMDB  \ndataset. Apart from instruction (LLM(TaskeDesc)), we test \nthe input prompt with table schema (LLM(Schema)) and the \ninput prompt with data statistics like distinct value ratios and \ntuple numbers (LLM(DataStats)).\nEmpirical Analysis The experimental results are shown in \nFig. 4b. We have two observations. First, the three prompt-\ning methods for DB-GPT all achieve performance improve-\nment over PostgreSQL, which does not recommend indexes \nfor specific workloads.\nBecause DB-GPT finds operators that can be enhanced by \nindexes and builds indexes on the columns of these operators \n(e.g., create index on person_info(person_id, info_type_id) for \nthe predicate “person_info.person_id > 4158523 \nAND person_info.info_type_id <> 22”). Second, \nfor the three different prompts, LLM(DataStats) achieves low-\nest estimated costs, while LLM(Schema) works best in actual \nlatency. Because LLM(Schema) selects many more single-col-\numn indexes than LLM(DataStats), which may achieve higher \nestimated cost but are more flexibly used in actual execution. \nThus, we need to judiciously select the data information (e.g., \ncolumn types and data distributions) for prompts. Moreover, it is \nnoteworthy that the performance of gpt-4 is even inferior to that \nof the text-davinci-003 model. This observation suggests that \nthe relation between desired indexes and the physical constraints \n(e.g., storage budget, data characters) cannot be readily acquired \nby even extremely powerful language models, and causes mis-\nunderstanding. For example, almost 78% indexes selected by \nthe gpt-4 model are removed due to storage limit, because their \nspace consumption cannot be well estimated by the language \nmodel without careful fine-tuning.\nLimitations First, DB-GPT should be aware of the stor -\nage space for each index, which guides to achieve trade-off  \nbetween performance and space consumption. Currently, we \ngreedily limit the adopted indexes by DB-GPT (i.e., remov-\ning the remaining indexes after the index space is full). \nSecond, different from query rewrite, index tuning involves \nmany queries in a workload, which may exceed the input \nlength limitation in the LLMs. Thus, it is vital to embed \nthe workload queries into acceptable length or design new \nLLMs, especially for high-concurrency transactions. Third, \nindex tuning also requires iteration mechanism that allows \nDB-GPT to update the index set for multiple times before \nfully optimizing the performance.\nIn summary, although LLMs demonstrate potential in \nhandling database tasks, there are still many opportuni-\nties for further research. Firstly, there is a need for a well-\ndesigned LLM4DB architecture that supports common data-\nbase functionalities. For instance, a comprehensive LLM \ncan be designed to forecast and schedule workloads across \ndifferent components, with each component or function is \ndriven by a fine-tuned LLM. Additionally, it is crucial to \nimprove the efficiency of model inference. This requires the \nmodel to generate reasonable predictions or decisions with-\nout excessively consuming system resources. Some potential \napproaches include distilling the model into smaller models \nand supporting concurrent model inferences.\n4  Related Work\nLarge Language Models\nDifferent from other ML models, large models are designed \nto scale with increasing amounts of training data and com-\nputation resources [53–56]. There are mainly two types of \nlarge model designs for natural language [57]. (i ) Unidi-\nrectional language models (e.g., GPT-3 [1 ]) use a single \ndirection of text (allowing for efficient parallelization on \nincreased computation power), i.e., assigning a possibility \n110 X. Zhou et al.\nto a token sequence x =( x1 , ...,xn) . For instance, if we break \ndown the sequence from left to right, the model calculates \nP(x)=P(x 1 )× ...× P(xn/uni007C.varx1 , ...,xn−1 ) . Thus, such models \nare good at predicting tokens following the input text (e.g., \ntext generation tasks). (ii) Masked language models (e.g., \nBERT [58], ERNIE [59]) mask some tokens in the text input, \nand aim to predict them based on the contextual information. \nFor instance, LLM can calculate P(x i/uni007C.varx1 , ...,xi−1, xi+1, ...,xn) , \nwhere xi is the masked token. Such models are more suitable \nto fill in the middle of the text (e.g., text classification tasks).\nLarge Language Models for Databases Some initial \nattempts have been made to use large language models for \ndatabase [60, 61]. DB-BERT [60] relies on the BERT model \nto summarize the database tuning manuals into rules. These \nrules can be used to enhance the configuration exploration \nprocedure, since the RL model only selects rules to tune the \ndatabase rather explores the whole space. On the other hand, \nCodexDB [ 61] aims to utilize the GPT-3 model to trans -\nlate SQL queries into other languages (e.g., Python), which \nachieves an accuracy of around 60%. These works show \npromise in promoting the use of large models for databases.\n5  Conclusion\nIn this paper, we explored the use of large language models \n(LLMs) for accomplishing database tasks. We proposed a \nsystem DB-GPT that effectively uses LLMs for optimiz-\ning database tasks, including prompt generation, fine-tuning \nLLM, and database-specific LLM design. In the initial prac-\ntice, we verified the relatively good performance of DB-\nGPT using prompt generation for database tasks like query \nrewrite and index tuning. We believe the use of LLMs will \ncontinue to benefit the field of database systems, including \ntext2SQL, SQL2Plan, database diagnosis, and data tuning.\nAcknowledgements This paper was supported by National Key R\\&D \nProgram of China (2023YFB4503600), NSF of China (61925205, \n62232009, 62102215), Zhongguancun Lab, Huawei, TAL education, \nand Beijing National Research Center for Information Science and \nTechnology (BNRist).\nAuthors' contributions GL makes contributions on the ideas. XZ con-\ntribution is fine-tuning LLM. ZS contribution is DB-specific LLM.\nData avilability https://github.com/TsinghuaDatabaseGroup/DB-GPT\nDeclarations \nCompeting interest Not applicable.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Brown Tom B et al (2020) Language models are few-shot learners. \nAdv Neural Inf Proc Syst 2020:1877–1901\n 2. Liu J, Shen D, Zhang Y, Dolan B, Carin L, Chen W (2022) \nWhat Makes Good In-Context Examples for GPT-3? DeeLIO \n2022(3):100–114\n 3. Floridi L, Chiriatti M (2020) GPT-3: its nature, scope, limits, and \nconsequences. Minds Mach 30(4):681–694\n 4. Svyatkovskiy A, Deng S K, Fu S, Sundaresan N (2020) Intel-\nlicode compose: code generation using transformer. In: FSE, pp \n1433–1443\n 5. Zhou X, Chai C, Li G, Sun J (2022) Database meets artificial intel-\nligence: a survey. IEEE Trans Knowl Data Eng 34(3):1096–1116\n 6. Li G, Zhou X, Cao L (2021) AI meets database: AI4DB and \nDB4AI. In: SIGMOD ’21: International conference on man-\nagement of data, virtual event, China, Jun 20-25. ACM, pp \n2859–2866\n 7. Zhang X, Wu H, Chang Z, Jin S, Tan J, Li F, Zhang T, Cui B \n(2021) restune: resource oriented tuning boosted by meta-learning \nfor cloud databases. In: SIGMOD ’21: International conference \non management of data, virtual event, China, Jun 20-25. ACM, \npp 2102–2114\n 8. Dong Q, Li L, Dai D, Zheng C, Wu Z, Chang B, Sun X, Xu J, Li \nL, Sui Z (2022) A survey for in-context learning. arXiv preprint \narXiv: 2301. 00234\n 9. Sorensen Taylor et al (2022) An information-theoretic approach \nto prompt engineering without ground truth labels. In: Proceed-\nings of the 60th annual meeting of the association for computa-\ntional linguistics, vol 1. Association for computational linguistics, \nStroudsburg, pp 819–862\n 10. Zhou Y, Muresanu A I, Han Z, Paster K, Pitis S, Chan H, Ba \nJ (2022) Large language models are human-level prompt engi-\nneers. In: The Eleventh International Conference on Learning \nRepresentations\n 11. Honovich O, Shaham S R B, Omer L (2022) instruction induction: \nfrom few examples to natural language task descriptions, pp 1–17\n 12. Razeghi Y, Logan R L, Wallace E, Singh S (2020) AUTO-\nPROMPT : eliciting knowledge from language models with auto-\nmatically generated prompts, pp 4222–4235\n 13. Yao L, Bartolo M, Moore A, Riedel S, Stenetorp P (2022) Fan-\ntastically ordered prompts and where to find them: overcoming \nfew-shot prompt order sensitivity. 1:8086–8098\n 14. Zhao T Z, Wallace E, Feng S, Klein D, Singh S (2021) Cali-\nbrate before use: improving few-shot performance of language \nmodels. In International Conference on Machine Learning, pp \n12697–12706\n 15. Fu Y, Peng H, Sabharwal A, Clark P, Khot T (2022) Complexity-\nbased prompting for multi-step reasoning, pp 1–14\n 16. Kim H J, Cho H, Kim J, Kim T, Yoo K M, Lee S-G (2022) Self-\ngenerated in-context learning: leveraging auto-regressive language \nmodels as a demonstration generator\n 17. Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi E, Le \nQ, Zhou D (2022) Chain-of-thought prompting elicits reasoning in \nlarge language models. Adv Neural Inf Proc Syst 35:24824–24837\n111DB-GPT: Large Language Model Meets Database  \n 18. Press O, Zhang M, Min S, Schmidt L, Smith N A, Lewis M (2022) \nMeasuring and narrowing the compositionality gap in language mod-\nels, pp 1–25\n 19. Zhou D, Schärli N, Hou L, Wei J, Scales N, Wang X, Schuurmans \nD, Cui C, Bousquet O, Le Q, Chi E (2022) Least-to-most prompting \nenables complex reasoning in large language models\n 20. Zhang Y, Feng S, Tan C (2022) Active example selection for in-con-\ntext learning\n 21. Wang X, Zhu W, Wang William Y (2023) Large language models are \nimplicitly topic models: explaining and finding good demonstrations \nfor in-context learning. arXiv preprint arXiv: 2301. 11916\n 22. Wu Z, Wang Y, Ye J, Kong L Self-adaptive in-context learning. arXiv \npreprint arXiv: 2212. 10375\n 23. Shao Z, Gong Y, Shen Y, Huang M, Duan N, Chen W (2023) Syn-\nthetic prompting: generating chain-of-thought demonstrations for large \nlanguage models\n 24. Rubin O, Herzig J, Berant J (2022) Learning to retrieve prompts for \nin-context learning. In: NAACL 2022 - 2022 Conference of the north \nAmerican chapter of the association for computational linguistics: \nhuman language technologies, proceedings of the conference, pp \n2655–2671\n 25. Levy I, Bogin B, Berant J (2022) Diverse demonstrations improve \nin-context compositional generalization\n 26. Magister L C, Mallinson J , Adamek J , Malmi E, Severyn A (2022) \nTeaching small language models to reason. arXiv preprint arXiv: 2212. \n08410\n 27. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C L, Mishkin P, \nZhang C, Agarwal S, Slama K, Ray A, Schulman J, Hilton J, Kelton \nF, Miller L, Simens M, Askell A, Welinder P, Christiano P, Leike \nJ, Lowe R (2022) Training language models to follow instructions \nwith human feedback. Advances in Neural Information Processing \nSystems, 35, 27730–27744\n 28. Ding N, Qin Y, Yang G, Wei F et al (2022) Delta tuning: a compre-\nhensive study of parameter efficient methods for pre-trained language \nmodels. CoRR, arXiv: abs/ 2203. 06904\n 29. Liu X, Zheng Y, Du Z, Ding M, Qian Y, Yang Z, Tang J (2021) GPT \nunderstands, too. AI Open\n 30. Yang Z, Liang E, Kamsetty A, Wu C, Duan Y, Chen X, Abbeel P, \nHellerstein JM, Krishnan S, Stoica I, Berkeley UC (2019) Deep unsu-\npervised cardinality estimation, vol  13\n 31. Yin X, Zhu Y, Hu J (2021) A comprehensive survey of privacy-pre-\nserving federated learning: a taxonomy, review, and future directions. \nACM Comput Surv 54(6):1\n 32. Liu R, Wei J, Gu S S, Wu T-Y, Vosoughi S, Cui C, Zhou D, Dai A M \n(2022) Mind’s eye: grounded language model reasoning through simu-\nlation, pp 1–18\n 33. Liu J, Liu A, Ximing L, Welleck S, West P, Le Bras R, Choi Y, \nHajishirzi H (2022) Generated knowledge prompting for common-\nsense reasoning. In Proceedings of the 60th Annual Meeting of the \nAssociation for Computational Linguistics (volume 1: Long Papers), \npp 3154–3169\n 34. Creswell A, Shanahan M (2022) Faithful reasoning using large lan-\nguage models. Number, pp 1–48\n 35. https:// platf orm. openai. com/ docs/ models/ gpt-3-5\n 36. Yang Z, Kamsetty A, Luan S, Liang E, Duan Y, Chen X, Stoica I \n(2020) Neurocard: one cardinality estimator for all tables. In: Proceed-\nings of the VLDB endowment, vol 14, pp 61–73\n 37. Bin W, Angela W, Fenxiao C, Wang Yuncheng C (2019) Methods and \nexperimental results. C. Jay Kuo, Evaluating word embedding models\n 38. Zhou J, Cui G, Shengding H, Zhang Z, Yang C, Liu Z, Wang L, Li \nC, Sun M (2020) Graph neural networks: a review of methods and \napplications. AI open 1:57–81\n 39. Zhou X, Sun J, Li G, Feng J (2020) Query performance prediction \nfor concurrent queries using graph embedding. Proc VLDB Endow \n13(9):1416–1428\n 40. Wiegreffe S, Hessel J, Swayamdipta S, Riedl M, Choi Y (2022) \nReframing human-AI collaboration for generating free-text explana-\ntions. NAACL 2022:632–658\n 41. Houlsby N, Giurgiu A, Jastrzçbski S, Morrone B, Laroussilhe Q de, \nGesmundo A, Attariyan M, Gelly S (2019) Parameter-efficient transfer \nlearning for NLP. In: 36th International conference on machine learn-\ning, ICML 2019, 2019 Jun, pp 4944–4953\n 42. Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Shean W L, W, and \nWeizhu C (2021) LoRA: low-rank adaptation of large language mod-\nels. 10:1–26\n 43. De Moura L, Bjørner N (2008) Z3: An efficient smt solver. ETAPS. \nSpringer, Cham, pp 337–340\n 44. Kraska T, Beutel A, Chi E H, Dean J, Polyzotis N (2018) The case \nfor learned index structures. In: Proceedings of the 2018 international \nconference on management of data, pp 489–504\n 45. Mitzenmacher M (2018) A model for learned bloom filters and related \nstructures. arXiv preprint arXiv: 1802. 00884,\n 46. Sun Y, Wang S, Feng S, Ding S, Pang C, Shang J, Liu J, Chen X, \nZhao Y, Yuxiang L, Liu W, Zhihua W, Gong W, Liang J, Shang Z, \nSun P, Liu W, Ouyang X, Dianhai Y, Tian H, Hua W, Wang H (2021) \nErnie 3.0: large-scale knowledge enhanced pre-training for language \nunderstanding and generation\n 47. Zhang S, Roller S , Goyal N, Artetxe M, Chen M, Chen S, Dewan \nC, Diab M, Li X, Lin XV, Mihaylov T, Ott M, Shleifer S, Shuster K, \nSimig D, Koura PS, Sridhar A, Wang T, Zettlemoyer L (2022) OPT: \nopen pre-trained transformer language models. arXiv preprint arXiv: \n2205. 01068\n 48. Zeng A, Liu X, Zhengxiao D, Wang Z, Lai H, Ding M, Yang Z, Yifan \nX, Zheng W, Xia X, Tam WL, Ma Z, Xue Y, Zhai J, Chen W, Zhang \nP, Dong Y, Tang J (2022) GLM-130B: an open bilingual pre-trained \nmodel. 06\n 49. Chowdhery A et al (2022) Palm: scaling language modeling with \npathways\n 50. Han J, Rong Y, Xu T, Huang W (2022) Geometrically equivariant \ngraph neural networks: a survey. arXiv preprint arXiv: 2202. 07230\n 51. Wu Y, Rabe M N, Hutchins D, Szegedy C (2022) Memorizing trans-\nformers, pp 1–19\n 52. Gou J, Baosheng Yu, Maybank SJ, Tao D (2021) Knowledge distilla-\ntion: a survey. Int J Comput Vision 129(6):1789–1819\n 53. Qiu XP, Sun TX, YiGe X, Shao YF, Dai N, Huang XJ (2020) Pre-\ntrained models for natural language processing: A survey. Sci Chin \nTechnol Sci 63(10):1872–1897\n 54. De Mulder W, Bethard S, Moens M-F (2015) A survey on the appli-\ncation of recurrent neural networks to statistical language modeling. \nComput Speech Language 30(1):61–98\n 55. Li J, Tang T, Zhao W X, Wen J-R (2021) Pretrained language models \nfor text generation: A survey. arXiv preprint arXiv: 2105. 10311,\n 56. Jing K, Xu J (2019) A survey on neural network language models. \narXiv preprint arXiv: 1906. 03591\n 57. Pengfei L, Weizhe Y, Jinlan F, Zhengbao J, Hiroaki H, Graham N \n(2023) Pre-train, prompt, and predict: a systematic survey of prompt-\ning methods in natural language processing 55:1–35\n 58. Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina \n(2019) Bert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of NAACL-HLT, pp 4171–4186\n 59. Zhang Z, Han X, Liu Z, Jiang X, Sun M, Liu Q (2020) ErniE: \nEnhanced language representation with informative entities. ACL, \npp 1441–1451\n 60. Trummer I (2022) DB-BERT: a database tuning tool that \"reads the \nmanual”. In: SIGMOD, pp 190–203\n 61. Trummer I (2022) Codexdb: synthesizing code for query process-\ning from natural language instructions using GPT-3 codex. Proc \nVLDB Endow 15(11):2921–2928"
}