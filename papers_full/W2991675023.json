{
    "title": "Transformer Based Memory Network for Sentiment Analysis of Web Comments",
    "url": "https://openalex.org/W2991675023",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2013978431",
            "name": "Ming Jiang",
            "affiliations": [
                "Hangzhou Dianzi University",
                "Institute of Software"
            ]
        },
        {
            "id": "https://openalex.org/A2411788195",
            "name": "Junlei Wu",
            "affiliations": [
                "Hangzhou Dianzi University",
                "Institute of Software"
            ]
        },
        {
            "id": "https://openalex.org/A2102056083",
            "name": "Xiangrong Shi",
            "affiliations": [
                "Zhejiang University of Finance and Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2005452123",
            "name": "Min Zhang",
            "affiliations": [
                "Institute of Software",
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A2013978431",
            "name": "Ming Jiang",
            "affiliations": [
                "Institute of Software",
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A2411788195",
            "name": "Junlei Wu",
            "affiliations": [
                "Hangzhou Dianzi University",
                "Institute of Software"
            ]
        },
        {
            "id": "https://openalex.org/A2102056083",
            "name": "Xiangrong Shi",
            "affiliations": [
                "Zhejiang University of Finance and Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2005452123",
            "name": "Min Zhang",
            "affiliations": [
                "Hangzhou Dianzi University",
                "Institute of Software"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2964164368",
        "https://openalex.org/W2804552794",
        "https://openalex.org/W6748540291",
        "https://openalex.org/W6748560189",
        "https://openalex.org/W2757541972",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6640212811",
        "https://openalex.org/W6747848841",
        "https://openalex.org/W2895547478",
        "https://openalex.org/W4211186029",
        "https://openalex.org/W2160660844",
        "https://openalex.org/W1964613733",
        "https://openalex.org/W587146513",
        "https://openalex.org/W2250275424",
        "https://openalex.org/W2161672051",
        "https://openalex.org/W2898227364",
        "https://openalex.org/W2306941105",
        "https://openalex.org/W2912194035",
        "https://openalex.org/W6762323134",
        "https://openalex.org/W2293771150",
        "https://openalex.org/W2940199079",
        "https://openalex.org/W2166706824",
        "https://openalex.org/W6604622236",
        "https://openalex.org/W2897510551",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2944698605",
        "https://openalex.org/W2252057809",
        "https://openalex.org/W6727807531",
        "https://openalex.org/W2251792193",
        "https://openalex.org/W2963168371",
        "https://openalex.org/W2562607067",
        "https://openalex.org/W6752770541",
        "https://openalex.org/W2767210791",
        "https://openalex.org/W2875308690",
        "https://openalex.org/W113211615",
        "https://openalex.org/W2787654308",
        "https://openalex.org/W2788610610",
        "https://openalex.org/W2529550020",
        "https://openalex.org/W2788810909",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2946627595"
    ],
    "abstract": "The boom in wireless networking technology has led to an exponential increase in the number of web comments. Therefore, sentiment analysis of web comments is vital, and aspect-based sentiment analysis(ABSA) is very useful for the sentiment feature extraction of web comments. Currently, context-dependent sentiment feature typically derives from recurrent neural networks (RNN), and an average target vector usually replaces the target vector. However, web comments have become increasingly complex, and RNN may lose some essential sentiment information. At the same time, the average target vector may be the wrong target feature. We propose a new Transformer based memory network (TF-MN) to correct the shortcomings of the previous method. In TF-MN, the task becomes the question-answering process, which optimizes context, question, and the memory module. We use a global self-attention mechanism and a local attention mechanism (memory network) to construct emotionally inclined web comment semantics. Since self-attention can only obtain global semantic links, words such as nouns, prepositions, and adverbs still affect the emotional extraction of comments. To shield the influence of unrelated vocabulary on classification, we propose to use improved memory networks to optimize the extraction of web comments semantics. We conduct experiments on two datasets, and experimental results show that our model exceeds the state-of-the-art model.",
    "full_text": "SPECIAL SECTION ON INNOVATION AND APPLICATION OF INTELLIGENT PROCESSING\nOF DATA, INFORMATION AND KNOWLEDGE AS RESOURCES IN EDGE COMPUTING\nReceived November 12, 2019, accepted November 28, 2019, date of publication December 2, 2019,\ndate of current version December 23, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2957192\nTransformer Based Memory Network for\nSentiment Analysis of Web Comments\nMING JIANG\n 1, (Member, IEEE), JUNLEI WU\n1, XIANGRONG SHI\n 2, AND MIN ZHANG\n1\n1Institute of Software and Intelligent Technology, Hangzhou Dianzi University, Hangzhou 310018, China\n2School of Information Management and Engineering, Zhejiang University of Finance and Economics, Hangzhou 310018, China\nCorresponding author: Xiangrong Shi (sxr@zufe.edu.cn)\nThis work was supported in part by the Zhejiang Provincial Technical Plan Project under Grant 2018C03039 and Grant 2018C03052, and in\npart by the Teaching Reform Research Project through the Zhejiang Higher Education in the 13th Five-Year Plan under Grant jg20180201.\nABSTRACT The boom in wireless networking technology has led to an exponential increase in the\nnumber of web comments. Therefore, sentiment analysis of web comments is vital, and aspect-based\nsentiment analysis(ABSA) is very useful for the sentiment feature extraction of web comments. Currently,\ncontext-dependent sentiment feature typically derives from recurrent neural networks (RNN), and an average\ntarget vector usually replaces the target vector. However, web comments have become increasingly complex,\nand RNN may lose some essential sentiment information. At the same time, the average target vector may\nbe the wrong target feature. We propose a new Transformer based memory network (TF-MN) to correct\nthe shortcomings of the previous method. In TF-MN, the task becomes the question-answering process,\nwhich optimizes context, question, and the memory module. We use a global self-attention mechanism and\na local attention mechanism (memory network) to construct emotionally inclined web comment semantics.\nSince self-attention can only obtain global semantic links, words such as nouns, prepositions, and adverbs\nstill affect the emotional extraction of comments. To shield the inﬂuence of unrelated vocabulary on\nclassiﬁcation, we propose to use improved memory networks to optimize the extraction of web comments\nsemantics. We conduct experiments on two datasets, and experimental results show that our model exceeds\nthe state-of-the-art model.\nINDEX TERMS ABSA, transformer, memory network, web comments.\nI. INTRODUCTION\nIn the future, there are billions of devices connected to\nthe Internet, and data surround us. [1], [2] So faster and\nmore reliable data processing becomes critical. In recent\nyears, the integration and centralized nature of cloud comput-\ning have proven to be cost-effective and ﬂexible. However,\nthe rise of the Internet of Things and mobile computing has\nput much pressure on network bandwidth. Ultimately, not all\nsmart devices need to run with cloud computing.\nIn some cases, the round-trip transmission of data is excess.\nFor example, web comments contain a large amount of text\nand images, and transferring this data requires many network\nresources. Using edge computing technology, we can store\nthis data in a nearby server, then extract vital information\nfrom the data and pass it back to the server. The emotional\nintelligence contained in the web comments is a critical\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Muhammad Afzal\n.\nfeature, and it is necessary to extract it [3]. Therefore,\nan emerging ﬁeld of sentiment and sentiment analysis\nappeared which uses human-computer interaction, informa-\ntion retrieval, and multi-modal signal processing to extract\npeople’s emotions from growing online social data [4]. In this\npaper, we mainly conduct sentiment analysis on the web\ncomments of Weibo.\nWeibo is a signiﬁcant social platform that covers enter-\ntainment, social, shopping, and food. It not only satisﬁes\npeople’s social needs but also becomes one of the essential\nreal-time information sources and centres of spreading public\nopinion. In the context of many Weibo users, each person’s\nviews or claims are universal and adaptable due to differ-\nences in academic qualiﬁcations and cognition. By perform-\ning sentiment analysis on the content published by Weibo\nusers, it can restore the real emotions of users as much as\npossible, help people to get hot topics in time, help control\nthe direction of public opinion, and help to analyze product\nreviews. This sentiment analysis technology not only assists\n179942 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/VOLUME 7, 2019\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nusers in optimizing their purchasing decisions, but also helps\nthem to self-improve, improve market competitiveness, and\naccurately discover and exploit the hidden business and social\nvalues in Weibo.\nWeibo sentiment analysis refers to judging the emotional\ntendency by analyzing and mining personal information in\nWeibo. At present, there are many studies on the emotional\nanalysis of Weibo. According to the granularity, there are\nﬁne-grained sentiment analysis and coarse-grained sentiment\nanalysis. The coarse-grained sentiment analysis is mainly\nbased on the paragraph level and the sentence level, and only\nthe emotional words are considered in the analysis process.\nThe emotions of the evaluation object and its attributes are not\nconsidered. Fine-grained sentiment analysis generally refers\nto lexical-level sentiment analysis.\nIn this paper, we use aspect level sentiment analy-\nsis(ABSA), a ﬁne-grained sentiment analysis technology,\nto process Weibo texts. A piece of Weibo text may contain\nmany aspects, each of which expresses a variety of emotions.\nFor example, the text, ‘‘Nice service but the food was too\nbad.’’ contains two aspects that express different emotions.\nWith the help of aspect level sentiment analysis, we can\nconduct a ﬁne-grained analysis of the opinions and emotions\nin Weibo. According to the above example, we analyze the\nemotions expressed in this text from two aspects. The sen-\ntiment polarity is positive when the target is ‘‘service’’, but\nit turns negative if ‘‘food’’ is the target. This hierarchical\nanalysis method is handy for digging deep into the emotions\nexpressed by a piece of text.\nIn terms of sentiment classiﬁcation, whether it is\ncoarse-grained or ﬁne-grained sentiment analysis, the meth-\nods used can be divided into three sentiment analysis\nmethods, which are supervised, unsupervised, and semi-\nsupervised.\nSupervised machine learning method performs supervised\ntraining and testing through classiﬁers by selecting emo-\ntional classiﬁcation features such as emotional words. The\nmilestone is that Pang et al. applied three representative clas-\nsiﬁers (support vector machine SVM, naive Bayes NB, max-\nimum entropy ME) to classify the text emotionally [5]. Some\nscholars compare different classiﬁcation algorithms. Yanxia\nYang used the Bayesian algorithm and SVM classiﬁcation\nalgorithm to analyze the sentiment of Weibo and compared\nthe advantages and disadvantages of the two algorithms in\nclassiﬁcation performance, showed that the Bayesian algo-\nrithm works better [6]. Some scholars have improved the clas-\nsiﬁcation algorithm to make the classiﬁcation better. Chen\nBingfeng et al. improved the Linear-chain CRF model. They\nproposed a two-layer CRF model, which can better satisfy the\ncar entity’s recognition of emotional entities and emotional\ntendency classiﬁcation needs [7].\nThe semi-supervised analysis method uses a small number\nof the labelled dataset and expands the size of the labelled\ndataset by testing some unlabeled data. Repeat the previ-\nous steps to predict the data step by step. Xiaoguang Zhu\ncombined the existing annotation set with the active learn-\ning method in semi-supervised learning to mark the emo-\ntional polarity and category of Weibo text, to reduce the\ncost of labelling, and apply the labelled dataset to supervised\nlearning [8].\nUnsupervised sentiment analysis methods are based pri-\nmarily on existing sentiment lexicons or the existing emo-\ntional dictionary, which is expanded to perform sentiment\nanalysis on the text. Currently, it is representative and widely\nused in dictionary resources. The English language mainly\nincludes WordNet and General Inquirer. Emotional dictio-\nnaries commonly used in Chinese are ‘‘HowNet’’, NTUSD,\nC-LIWC, DUTIR.\nSince supervised learning relies on sufﬁcient annotated\ncorpus, Weibo, such a large amount of Internet text, leads\nto the manual inability to label large-scale corpora, and\nits scope and scale are limited. Besides, Weibo contains\ntoo much uncertainty; the unsupervised method of con-\nstructing an emotional dictionary cannot cover all emotions.\nUnlike traditional machine learning methods, we combine\nsemi-supervised learning with the neural network learning\nmethod. A small set of annotation training sets is provided\nto predict the sentiment classiﬁcation of unlabeled datasets.\nLearning text features are vital for neural network learning.\nIn neural networks models, sequence transduction models are\none of the primary methods for learning text features. More-\nover, the mainstream sequence transduction models include\ncomplex recurrent neural networks (RNN) and convolutional\nneural networks (CNN), all of which contain an encoder\nand a decoder. The model proposed by [9] to connect the\nencoder and decoder through the self-attention units is cur-\nrently the best sequence transduction model, which takes\ninto account the computational efﬁciency and conversion\neffect. In the sentiment analysis task, long short-term mem-\nory (LSTM) [10] and gated recurrent (GRU) [11] neural\nnetwork models have learned the best text features. How-\never, Weibo comments have become increasingly complex,\nand LSTM or GRU is quite challenging to extract the text\nfeature of Weibo comments. To overcome this shortcom-\ning that LSTM and GRU cannot handle long text, refer-\nence [12] proposes that the text can be divided into four\nparts to ensure that no critical information is lost, which\ndirectly affects the extraction of context-dependent text\nfeatures.\nOn the other hand, a target usually consists of one or more\nwords, it cannot express complete semantics, and it is impos-\nsible to obtain text features using a sequence transduction\nmodel. Multiple experiments have shown that the average\ntarget vector is the best way to obtain the target feature [13].\nHowever, this method has signiﬁcant drawbacks. For exam-\nple, in the text ‘‘Nice macarons in France are not good at\nall.’’, the target is ‘‘Nice macarons’’, which consists of two\nwords. Since the word embedding does not have sufﬁcient\nvocabulary, it only contains ‘‘Nice’’, and ‘‘macarons’’ is equal\nto a set of microscopic random features. Thus, the average tar-\nget feature of ‘‘Nice macarons’’ is erroneously approximately\nequal to the feature of ‘‘Nice’’.\nVOLUME 7, 2019 179943\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nFIGURE 1. The flow chart of TF-MN.\nBased on the above analysis, we propose a memory net-\nwork model based on the Transformer. The Transformer is\na new sequence transduction model without recursive and\nconvolutional structures. Its basic unit is a self-attention\nmechanism that can interactively calculate each part of the\nsequence (regardless of the size of the sequence) for better\ncontext-dependent text features. Then, we use the memory\nnetwork to capture the sentiment information of the target in\nthe Weibo comment. Our model has four modules: a context\nmodule that encodes Weibo comments, a sentiment question\nmodule that converts the target and encodes sentiment ques-\ntions, a memory module that stores sentiment information,\nand an answer module that determines sentiment polarity.\nThe memory network comes from the question-answer\ntask. However, there are no exact questions in the\naspect-based sentiment analysis task. Previous methods typ-\nically constructed a question vector for a memory network\nusing zero-ﬁll or offset vectors, and we believe that each\ntarget in the text can become an emotional question. We pro-\npose a Transformer-based memory network model (TF-MN)\nwhose sentiment question module treats each target in the\ntext as an implicit question about ‘‘What is the emotional\ntendency of the target in the text?’’ Figure 1 is a ﬂow chart of\nthe TF-MN model.\nThe following three points are the contributions of our\nmodel:\n• In the sentiment analysis task, we use a Transformer to\nextract long text features. Our model effectively solves\nthe problem of not being able to extract long text features\nthrough LSTM and GRU accurately.\n• The sentiment question module does not average the\ntarget word vector but constructs emotional questions\nrelated to the target so that the target can accurately\nexpress the complete semantics.\n• Our model achieves the best accuracy on both datasets,\nand the experimental results show that using the Trans-\nformer and adding sentiment questions can improve the\nperformance of the model.\nII. RELATED WORK\nABSA is a sub-task in sentiment analysis, focusing on the\nextraction of ﬁne-grained sentiment information, and its\nimplementation has two categories [14].\nThe ﬁrst is a traditional method of using a lexicon or rule\nthat extracts features for training sentiment polarity classiﬁers\n(such as SVM). However, this method is labour-intensive,\nand its performance is highly dependent on the quality of\nthe manually labelled features. Reference [15] uses weights\nto calculate emotional word scores. Reference [16] proposes\na vocabulary-based approach that includes both explicit and\nimplicit opinions and improves the performance of the aspect\nratio relationship through multiple cores [17].\nMoreover, the second is the machine learning method.\nReference [18] uses a recently developed scalable statistical\nrelationship model called the hinge loss Markov random ﬁeld\nto model the relationship between aspects and emotions,\nthus developing a weakly supervised joint model for the\nemotional aspects of online courses. Then, it uses a hinge-\nless Markov random ﬁeld to solve the ABSA problem in\nMOOC. Reference [19] provides an emotionally consistent\ntopic model (SATM) that combines two types of external\nknowledge: an overall score distribution at the product level\nand an emotional dictionary at the word level. An emotional\nmatching model was proposed to predict the aspect ratio.\nReference [20] combines a vocabulary-based approach with\na feature-based support vector machine (SVM) and detects\nemotions for aspect words for the ﬁrst time in the SemEval\n14 competition.\nReference [21] constructs a binary phrase dependency tree\nof the target to construct an aspect word feature. It pro-\nvides a new way to identify the emotional polarity of the\nentity. It is an extension of RNN that takes into account the\ndependencies and composition of sentences. Reference [22]\ncombines target information in LSTM, evaluates it on the\nTwitter dataset, and proposes two methods, TD-LSTM and\nTC-LSTM. Reference [23] combines the attention mecha-\nnism with LSTM. When entering different targets, the model\ncan focus on different parts of the sentence.\nReference [24] describes a sincere memory network\napproach to solving ABSA tasks. It uses a hierarchical model\nin which sentences and targets are interrelated to make a\nﬁnal classiﬁcation by using attention units [25]. In the above\nstudy, Reference [26] proposes two new ways to improve\nattention. First of all, a target representation method is pro-\nposed to capture the semantics of the target better. Second,\nit introduces an attention model that incorporates syntactic\ninformation into the attention mechanism. It experimented\nwith the attention-based LSTM model on SemEval’s 2014,\n2015, and 2016 datasets. Although it uses syntactic informa-\ntion to implement a local attention mechanism, its approach\nis different from the local attention mechanism. It uses local\nattention mechanisms in larger windows to select words to\nmaintain semantic integrity. However, the local attention used\nin it tends to be smaller window sizes in order to capture\nrelatively pure sentiment information about the target [27].\nAlso, it uses distance weights in local attention to highlight\nwords that are close to the target, and the method uses a\ngating mechanism to assign weights to words in sentences\ndynamically [28]. More importantly, this method combines\n179944 VOLUME 7, 2019\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nFIGURE 2. The architecture of the sequence transduction model.\nthe local and global attention mechanisms to obtain the ﬁnal\nsentence vector, which can make up for the shortcomings of\nlocal attention [29].\nReference [12] divides each sentence into three parts and\nextracts context-related features using a two-way GRU. Ref-\nerence [30] uses a hierarchical attention mechanism that\ncombines the attention of the target level with the attention\nof the sentence level and inherits common sense knowledge\ninto the RNN. This model has a more signiﬁcant improve-\nment than the previous model, but due to the effect of the\nforgetting gate of RNN itself, this model still cannot handle\nlong text. Reference [31] proposes an LSTM model based\non segmentation attention(SA-LSTM-P), which can effec-\ntively capture the structural dependence between target and\nemotional expression through the linear-chain conditional\nrandom ﬁeld (CRF) layer. This model simulates the pro-\ncess by which humans infer emotional information while\nreading.\nDifferent from the above model, we mainly carry out emo-\ntional classiﬁcation from three aspects: optimizing feature\nextraction, transforming the form of target vector, eliminating\nthe inﬂuence of irrelevant emotional words.\nFirst of all, when we use LSTM to process text fea-\ntures, we ﬁnd that when LSTM processes long text (more\nthan 50 words), the gradient disappears. For this problem,\nwe received inspiration from the self-attention mechanism.\nBased on the Transformer, the best-performing self-attention\nmodel, we use it to solve the forgotten information of serial-\nized models such as LSTM.\nSecondly, we ﬁnd that the average target vector may be the\nwrong target feature. To solve this problem, we try to convert\nthe target into an emotional question. The experiments show\nthat the emotional question is more able to express the char-\nacteristics of the target.\nThirdly, we observed that there are some emotional words\nin the dataset that are not related to expressing emotions.\nWe believe that these unrelated emotional words mislead the\nresults of the experiment. Therefore, we use the memory net-\nwork to eliminate the inﬂuence of unrelated emotional words\non the classiﬁcation results by extracting emotional features\nmultiple times. The effect of classiﬁcation has improved,\nindicating that the memory network can eliminate the inﬂu-\nence of emotionally unrelated words.\nWe ran the other models mentioned above on our dataset.\nThe experimental results show that our model is better than\nthe best model in our data, indicating the superiority of our\nmodel.\nIII. THE COMPARISON OF SEQUENCE TRANSDUCTION\nAND SELF-ATTENTION MODEL\nThe ﬁrst thing to know is that the sequence transduction\nmodel architecture is like this Fig. 2 before the attention\nmechanism emerges. First, the sequence model input the text\ninto the encoder(the blue part of the Fig. 2), process it accord-\ning to the order of the text and get a feature representation\nof the input text. Then the decoder outputs the feature as a\nfeature representation of a ﬁxed shape. The blue part of Fig 2\nis the output of the sequence transduction model. However,\nthis way of transmitting information leads to a massive loss\nof information, exceptionally long sentences, and language\npairs with different word order, such as Fig. 2.\nWhen translating ‘‘nice’’ into the feature vector of e1,\nit must go through a long-distance, which may be mixed with\nnoise or lose some useful information, so it is challenging\nto extract speciﬁc text features. Even after the addition of a\npartial attention mechanism to assist in the extraction of text\nfeatures, extraction errors can occur. The reason is the insuf-\nﬁcient capture of the relationship. In the feature extraction\ntask of texts, we need to discover three kinds of relationships:\nthe relationship inside the source sentence, the relationship\ninside the target sentence, the relationship between the source\nsentence and the target sentence.\nEven if the above sequence transduction model uses the\nattention mechanism to capture the relationship between the\nsource and target sentences, but still uses RNN to capture\nthe relationship between the internal and the target sentences.\nThe model captures the relationship from side to side (better\nwith bidirectional sequence), but this is not straightforward,\nespecially for some far distances.\nIn addition to the lack of learning long-distance relation-\nships, seq2seq has one drawback: training slowly because it\ntakes a word to look from left to right one by one, which\nmakes RNN not take advantage of the parallel computing\npower of the GPU like CNN.\nTo solve the shortcomings of the sequence transduction\nmodel, we need to use a novel architecture, the self-attention\nmechanism, whose model structure is as shown in Fig. 3.\nFig. 3 is a simple self-attention mechanism framework.\nWe can see that each word in the sentence is correlated\n(for the sake of drawing, we omit the connection of some\nVOLUME 7, 2019 179945\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nFIGURE 3. The architecture of the self-attention model.\nFIGURE 4. The architecture of the transformer-self-attention model.\nwords). Therefore, the self-attention mechanism can achieve\nlong-distance dependence and solve the problem of gradient\ndisappearance. Furthermore, this calculation method has no\nsequence requirements; it can be operated in parallel, saving\nmuch time.\nFig. 4 is a Transformer self-attention model architecture\ndiagram, which uses mainly self-attention mechanism, except\nthat it is more complicated than Fig. 3. It mainly performs\nthe deformation and softmax operations on the results of the\nassociation calculation of each word, and ﬁnally combines\nthe results of each round of the self-attention mechanism to\nobtain a more comprehensive text feature representation.\nThe calculation process of Fig. 4’s self-attention mecha-\nnism can be expressed as the following formula:\nAttention(ei,ei+1) =softmax(\neieT\n1+1\n√dk\n) (1)\nheadi =Attention(ei,ei+1) (2)\nMultiHead(ei,ei+1) =Concat(headi)W O (3)\nwhere ei represents a word, dk represents the dimension of\nthe word vector, and W O is a randomly generated parameter\nmatrix.\nWe think the most signiﬁcant improvement of the trans-\nformer relative to the traditional sequence transduction is:\n• Propose to use the attention mechanism to directly learn\nthe internal relationship of the source language and the\ninternal relationship of the target language, instead of\nlearning with RNN as before;\n• A multi-head attention mechanism is proposed for\nthe assumption that there are many different relation-\nships, which is somewhat similar to the concept of\nmulti-channel in CNN;\n• The position of the word is encoded by the sin and cos\nfunctions of different frequencies.\nIV. THE PROPOSED MODEL\nIn this section, we mainly introduce the TF-MN model. The\nfollowing are assumptions of the ABSA task: First, given a\npiece of text C = {wC\n1 ,wC\n2 ,··· ,wC\nn−1,wC\nn }consisting of\nn words, marked as context. Second, we proposed a target\nT ={w T\n1 ,wT\n2 ,··· ,wT\ni−1,wT\ni }, consisting of multiple adja-\ncent words in the context, predicting the emotional polarity\nof the speciﬁed text.\nFigure 5 shows the TF-MN architecture, which uses a\npre-trained word embedding model in the context module\nto transform the context into a continuous low-dimensional\nsequence. Besides, the sentiment question module turns the\ntarget into a sentiment question, like ‘‘What is the emotional\ntendency of the target in the text?’’. Then, the memory module\nsaves the context sequence information after the Transformer\nprocesses the context and the sentiment question, and it\neliminates the inﬂuence of the unrelated words by multiple\nextractions. Finally, the answer module uses the softmax\nfunction to output the ﬁnal sentiment polarity. The following\nsections describe our model in detail.\nA. CONTEXT MODULE\nThe context module includes the following layers: the context\nencoder, the location encoder, and the fusion layer. The con-\ntext and the location encoder layer encode each context and\nlocation information into a vector separately, while the fusion\nlayer exchange information between these encoded vectors\nusing a Transformer.\n1) CONTEXT ENCODER LAYER\nSpeciﬁed a context C =\n{\nwC\n1 ,wC\n2 ,··· ,wC\nn−1,wC\nn\n}\n, every\nword in C is converted into a k-dimensional vector eC\ni ∈Rk\nwith a pre-trained word embedding matrix E ∈Rk∗|V |, such\nas Tencent AI Lab Embedding [32]:\neC\ni =E(wC\ni ) (4)\nwhere |V |and k are the size of vocabulary and word vector,\nrespectively.\n2) LOCATION ENCODER LAYER\nWe establish a context location word embedding matrix\nL =∈Rk∗n, which maps word location into a k-dimensional\n179946 VOLUME 7, 2019\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nFIGURE 5. The architecture of the TF-MN model.\nvector lC\ni ∈Rk :\nlC\ni =L(wC\ni ) (5)\nwhere n is the dimension of row vector in L. To provide\nrich location information for context, the row vector of L is\na k-dimensional vector consisting of k location information.\nMatrix L is a set of parameters to be trained, in which every\nrow vector is a sequence of location information with random\nnormal U (−0.02,0.02).\n3) FUSION LAYER\nThe fusion layer processes the context vector EC and context\nlocation vector LC , which contain exchanged information\namong vectors. We generate context representation HC ∈\nRk∗nc:\nHc =Transformer(EC ,LC ) (6)\nwhere nc denotes the max size of context (If the length is not\nenough, ﬁll it with 0).\nB. QUESTION MODULE\nThe question module further also contains these layers: the\nquestion encoder layer, the location encoder layer, and the\nfusion layer. The question encoder layer converts target into\nsentiment question ﬁrstly and then encodes question into a\nvector. The location encoder layer encodes location informa-\ntion into a vector. The fusion layer fuses these vectors into\nmore speciﬁc features through the Transformer.\n1) QUESTION ENCODER LAYER\nGiven a question T =\n{\nwT\n1 ,wT\n2 ,··· ,wT\ni\n}\n, T consists of\none or more words, which can be included in C or doesn’t\nappear in C. If T is embedded in sentiment question, you\nget the question as ‘‘What is the emotional tendency of the\ntarget in the text?’’. Every word in question is converted into\na k-dimensional vector eQ\ni ∈ Rk with a pre-trained word\nembedding matrix E:\neQ\ni =E(wQ\ni ) (7)\n2) LOCATION ENCODER LAYER\nWe also establish a question-location word embedding matrix\nL, which maps word location into a k-dimensional vector\nlQ\ni ∈Rk :\nlQ\ni =L\n(\nwQ\ni\n)\n(8)\nVOLUME 7, 2019 179947\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nwhere the location information matrix of Q is the same as the\ncontext module.\n3) FUSION LAYER\nThe fusion layer generates the sentiment question represen-\ntation HQ ∈Rk∗nq:\nHQ =Transformer\n(\nEQ,LQ\n)\n(9)\nwhere nq denotes the max size of the question.\nC. MEMORY MODULE\nThe memory module has three components: the attention\ngate, feature conversion, and the memory update gate, which\nis used to combine information from the context with target\nand purify of the target vector from the given context.\nThe output F from the context module, the question q∗from\nquestion module, and the acquired knowledge stored in the\nmemory vector mt−1 from the previous step.\nThe three inputs are transformed by:\nu =\n[\nF ∗q∗;\n⏐⏐F −q∗⏐\n⏐;F ∗mt−1;|F −mt−1|]\n(10)\nwhere ‘‘;’’ is concatenation. ‘‘∗,−,||’’ are element-wise prod-\nuct, subtraction and absolute value respectively. F is a matrix\nof size (1,HC ), while q∗and mt−1 are vectors of size\n(\n1,HQ\n)\nand (1,Hm), where Hm is the output size of the memory\nupdate gate. To allow element-wise operation, HC , HQ, and\nHm have the same shape. In equation (10), the ﬁrst two terms\nmeasure the similarity and difference between facts and the\nquestion. The last two terms have the same functionality for\ncontext and the last memory state.\nLet the i-th element in αto be the attention weight for wC\ni .\nαis obtained by transforming u using a two-layer perceptron:\nα=softmax (tanh (u ·Wm1)·Wm2) (11)\nwhere Wm1 and Wm2 are parameters of the perceptron, and we\nomit bias terms.\nThe feature conversion takes F and αas input and then get\nthe updated F:\nF =F ·α (12)\nThe memory update gate outputs the updated memory\nmt using question q∗, previous memory state mt−1 and the\nupdated F:\nmt =relu\n([\nq∗;mt−1;F\n]\n·Wu\n)\n(13)\nwhere Wu is the parameter of the linear layer.\nThe memory module could be iterated several times with a\nnew α generated for each time, allows the model to attend\nto different parts of the facts in different iterations, which\nenables the model to perform complicated reasoning across\nsentences. The memory module produces mt as the output at\nthe last iteration.\nFIGURE 6. The architecture of the memory network module.\nD. ELIMINATE THE EFFECT OF IRRELEVANT WORDS\nAfter introducing the composition of the above memory\nmodule, this section carefully introduces the workﬂow of\nthe memory module. This module is a signiﬁcant part of\nour model; it is mainly used to eliminate the inﬂuence of\nunrelated emotional words in the text. Fig. 6\n1) ATTENTION GATE\nFormula 10 describes the calculation steps of the atten-\ntion gate. First of all, the memory network has no mem-\nory. So we memory zero state m0 equals H question.\nMoreover, perform multiplication and subtraction between\ncontext and question or between context and memory respec-\ntively, mark this result as D. Finally get attention gate by a\nlinear transformation of D and memory bias. Expressed as\nfollows:\nm0 =Hquestion (14)\nD1 =[Hcontext ∗Hquestion;|H context −Hquestion|]\n(15)\nD2 =[Hcontext ∗m0;|H context −m0|] (16)\nD =[D1;D2] (17)\nAttentionGate =softmax(tanh(D ·m1\n0) ·m2\n0) (18)\nwhere m1\n0 indicates the bias on the x-axis and m2\n0 indicates the\nbias on the y-axis.\n2) UPDATE INFORMATION\nIn this section, we update two parts of information: context\nand memory. Firstly we use the linear transformation of\ncontext and attention gate to update the context. Secondly,\nwe update memory by transformed context.\nEach time the context and memory information is updated,\nthe weight of the unrelated emotional words becomes smaller\nand smaller. By setting the number of iterations consistently,\nwe set the impact of inappropriate emotional words to a\nminimum.\nThis update process can be described using the Formula 12\nand Formula 13.\n179948 VOLUME 7, 2019\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nE. ANSWER MODULE\nIn answer module, we regard the memory module outputs\nas the ﬁnal representation and put it into a softmax layer\nfor aspect-based sentiment analysis task. To minimize the\ncross-entropy error of sentiment classiﬁcation, we train the\nmodel in a supervised method in which loss function is\ndescribed as follows:\nloss =−\n∑\n(c,q)∈T\n∑\nlb∈LB\nPg\nlb (c,q)·log (Plb (c,q)) (19)\nwhere T is all training items, LB is the set of sentiment polar-\nities, (c,q)is a context-question pair. Our system outputs the\nprobability of class lb by computing the item (c,q). Pg\nlb (c,q)\nmeans zero or one, expressing whether the item is favourable\nor not. In the module, We calculate the gradients of the overall\nparameters by using back-propagating and update them in a\nstochastic gradient descent manner.\nV. EXPERIMENT\nA. DATASET\nWe evaluated our approach on two datasets: the Weibo\nand Semeval datasets. Although China has hosted many\nWeibo comment analysis competitions and produced\nmany high-quality datasets, such as NLPCC2013 1 and\nNLPCC2014,2 unfortunately, most datasets only analyze\nsentimental tendencies at the sentence level. For ﬁne-grained\nsentiment analysis, we construct datasets through web\ncrawlers. We can specify that the target can be an entity or an\nabstract entity in Weibo. We divide the emotional polarity of\nthe goal into negative, neutral, and positive. If the target dif-\nfers in three emotional polarities, we ignore the target entity.\nThe main thing we crawled is the data in the restaurant sector.\nThis data contains four aspects: transportation, service, price,\nand the environment.\nThen, we randomly selected 18,480 Weibo comments (a\ntotal of 22,821 Weibo comments) as the training set and\nthe remaining 4,431 comments as the test set. To prevent\nover-ﬁtting of the experiment, we set the entries for each\nemotional polarity of the data to be the same. Table 1\ndetails the Weibo dataset, and each of the Weibo comments\nreached more than 200 characters in length. To demon-\nstrate the versatility of our approach, we also experi-\nmented with the open-source the Semeval dataset. We use\nSemeval-2014-task4.3 The dataset contains two areas,\nnamely a restaurant and laptop. We set the ‘‘aspect-term’’\nand ‘‘categories’’ that appear in the data as targets, and then\nperform ABSA experiments based on the targets. Table 2 is\nthe details of our dataset.\nTo correct the shortcoming of the average target vector,\nwe construct an emotional question, such as ‘‘What is the\nemotional tendency of the target in the text?’’.\n1http://tcci.ccf.org.cn/conference/2013/\n2http://tcci.ccf.org.cn/conference/2014/\n3http://alt.qcri.org/semeval2014/task4/\nTABLE 1. Statistics of the Weibo dataset.\nB. EVALUATION\nWe use loose and rigorous indicators to evaluate our methods\nand benchmark models. Our multi-label classiﬁers are mea-\nsured using: F1 and Accuracy. F1 is a weighted harmonic\naveraging of Precision and Recall. It is a commonly used\nevaluation standard in the ﬁeld of IR (Information Retrieval)\nand is often used to evaluate the quality of a classiﬁcation\nmodel. Then these two indicators can be expressed using the\nfollowing formula:\n• Accuracy(Indicate the ratio of the sample that matches\nthe label to the total sample.): TP+TN\nTP+TN+FP+FN\n• Precise(Indicate the correct prediction of the proportion\nof positive samples in the actual sample as positive\nsamples.): TP\nTP+FP\n• Recall(Indicate the correct prediction of the proportion\nof positive samples to positive samples): TP\nTP+FN\n• F1: 2\n1\nPrecise + 1\nRecall\n=2 ·Precise·Recall\nPrecise+Recall\nwhere TP means correctly predict positive samples as pos-\nitive; FN means erroneously predict positive samples as\nnegative; FP means erroneously predict negative samples as\npositive; TN means correctly predict negative samples as\nnegative.\nC. PARAMETER SETTING\nWe use Tencent AI Lab Embedding 4 [32] to assign 200-\ndimensional vectors to the context and target-to-words in\nthe Weibo dataset. The Semeval dataset uses GloVe 5 as the\nword embedding matrix for the dataset, where each word is\nassigned a 300-dimensional vector. For all words in the two\ndatasets that are not included in the word embedding matrix,\nwe randomly assign a set of vectors by following the normal\ndistribution of U(-0.01, 0.01). To prevent data overﬁtting,\nwe set the loss rate to 0.1. The optimizer for our model is\nAdam, which has a batch-size and learning rate of 8 and\n6.25e-5, respectively. We use Jieba 6 to segment the Chinese\nphrases and then generate a word vector matrix. Note: Even\nif we set up the experimental seed, the experimental results\n4https://ai.tencent.com/ailab/nlp/embedding.html\n5https://nlp.stanford.edu/projects/glove/\n6https://github.com/fxsjy/jieba\nVOLUME 7, 2019 179949\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nTABLE 2. Statistics of the SEMEVAL dataset.\nchange with each randomly assigned word vector. To solve\nthis problem, we averaged the results of 10 experiments.\nD. EXPERIMENT SETTING\nTo test the performance of the model, we performed the fol-\nlowing set of experiments: TDLSTM [22], T +Att +LSTM,\nSQ +Att +LSTM, T +Att +TF, SQ +Att +TF, SQ +\nMM +TF, IAN [33], RAM [34] and SA-LSTM-P [31](T\nrepresents the target; SQ represents the sentiment question;\nAtt represents the attention mechanism; TF represents the\nTransformer.)\nFirst, we check if the emotional question affects the\nexperiment. We have performed some experiments based on\nTDLSTM, namely T +Att +LSTM and SQ +Att +LSTM.\nSecond, we replaced the LSTM with the Transformer mod-\nule, in which we used the target and the annotation as input.\nThen, compare the experimental results of the Transformer\nand LSTM to determine if the Transformer is losing critical\nemotional information.\nThen, we use the Transformer module to add the mem-\nory network module to the model. It is judged by previous\ncomparative experiments, whether the added storage network\nmodule can reduce the inﬂuence of unnecessary emotional\nwords.\nFinally, we have added IAN, RAM and SA-LSTM-P for\ncomparison and compared it with other paper models to\nhighlight the experimental results of our model.\nE. MODEL COMPARISON\nTable 3 and Table 4 show the performance of the Weibo\ndataset and the Semeval dataset, respectively. Compared with\nthe average target vector method, we can ﬁnd that the model\nusing the emotional question has a particular improvement\non both datasets. However, the improvement of the Weibo\ndataset is relatively small. We speculate that the words that\nmake up the target in the Weibo dataset are more common\nvocabulary and do not result in random assignment to smaller\nword vectors. The relatively vast improvement in the Semeval\ndataset shows that there are some uncommon words on the\nSemeval dataset, which are changed into the form of emo-\ntional questions to improve the classiﬁcation.\nBesides, we compare the models using the Transformer\nand LSTM, and we can see that the Transformer effect is\nbetter than the LSTM model on the Weibo dataset. However,\nTABLE 3. Performance on the Weibo dataset.\nwhat surprised us was that on the Semeval dataset, there\nwas almost no improvement in using the Transformer model.\nWe looked at Semeval’s dataset in detail. Most of the length\nof the comments is around 20 words, and a few are more\nthan 50 words. Therefore, this creates the illusion that the\nperformance of the LSTM model is similar to the Transformer\nmodel.\nTo our surprise, our model has dramatically improved its\nperformance after joining the memory network. Compared to\nthe previous model, the average is increased by 3%. However,\nthe improvement effect on the Weibo dataset is minimal.\nWe think this is because the size of the Weibo dataset is rela-\ntively large, and the memory network needs many iterations.\nTo better achieve the effect of classiﬁcation, we set different\niteration times for the memory network of each dataset.\nFrom Table 3 and Table 4, we can also see that the TF-MN\nmodel can improve the effect of sentiment classiﬁcation. The\nexperimental results of the TF-MN model are superior to\nthose of the IAN, RAM and SA-LSTM-P neural network\nmodels.\nF. VISUALIZATION OF ATTENTION\nWe visualize the weight of the Transformer model output of a\ncomment in Figure 7. The comment is in the restaurant ﬁeld,\nabout the emotional scoring of service attitudes, which review\nincludes both services and food. Both of these aspects have\nreceived signiﬁcant attention. However, when we conduct\nthe ‘‘service’’ emotional polarity judgment, we only need\ninformation about ‘‘service’’.\n179950 VOLUME 7, 2019\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nTABLE 4. Performance on the Semeval2014-task4 dataset.\nFIGURE 7. The weight of a comment by the Transformer.\nFIGURE 8. The weight of a comment by TF-MN.\nTABLE 5. The result of Memory network hop count comparison.\nTo do this, the model needs a more focused allocation of\nattention weights. Figure 8 shows the distribution of attention\nweights after we have used the memory network. When\nscoring ‘‘service’’, the attention of the comments concen-\ntrates on the ‘‘service’’ aspect and the surrounding adjectives.\nTherefore, the memory network can reduce the inﬂuence\nof irrelevant emotional words and entities and improve the\ncorrectness of sentiment classiﬁcation.\nG. MEMORY NETWORK OPTIMIZATION\nWe conduct two experiments to optimize the number of mem-\nory updates in order to improve the efﬁciency of the sentiment\ninformation extraction of the memory module. In the Weibo\ndataset, our model obtains the best classiﬁcation effect when\nthe number of iterations of the model is 5. Nevertheless, in the\nSemeval-2014-task4 dataset, the best number of memory\nupdates is 3. Table 5 shows the results of these experiments.\nWe conclude that excessive memory update operations cause\nthe local attention mechanism to repeatedly operate on the\nsame block of text, which reduces the performance of the\nmodel.\nVI. CONCLUSION\nIn this paper, we use the memory network model to migrate\nABSA tasks to Q&A tasks, whose key is to turn the target\ninto a sentiment question. At the same time, we also believe\nthat other effective network models can replace the memory\nnetwork. We think that the attention gate in the memory\nmodule of TF-MN can add syntax information. This task\ncan also inspire other tasks that have context but no appar-\nent questions. The TF-MN model uses the memory network\nmodel to model the Weibo sentiment analysis of the Q&A\ntask. We conduct several experiments on the Weibo dataset\nand the Semeval-2014-task4 dataset. The results show that\nour model is better than the state-of-art model. In future work,\nVOLUME 7, 2019 179951\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nwe will also study how to apply the capsule network to the\nABSA task.\nREFERENCES\n[1] Y . Yin, W. Zhang, Y . Xu, H. Zhang, Z. Mai, and L. Yu, ‘‘Qos prediction for\nmobile edge service recommendation with auto-encoder,’’ IEEE Access,\nvol. 7, pp. 62312–62324, 2019, doi: 10.1109/ACCESS.2019.2914737.\n[2] Y . Yin, L. Chen, Y . Xu, and J. Wan, ‘‘Location-aware service recommenda-\ntion with enhanced probabilistic matrix factorization,’’ IEEE Access, vol. 6,\npp. 62815–62825, 2018, doi: 10.1109/ACCESS.2018.2877137.\n[3] L. Qi, Y . Chen, Y . Yuan, S. Fu, X. Zhang, and X. Xu, ‘‘A qos-aware virtual\nmachine scheduling method for energy conservation in cloud-based cyber-\nphysical systems,’’ in World Wide Web. 2019, pp. 1–23.\n[4] E. Cambria, ‘‘Affective computing and sentiment analysis,’’\nIEEE Intell. Syst., vol. 31, no. 2, pp. 102–107, Mar./Apr. 2016,\ndoi: 10.1109/MIS.2016.31.\n[5] B. Pang, L. Lee, and S. Vaithyanathan, ‘‘Thumbs up?: Sentiment classiﬁca-\ntion using machine learning techniques,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2002, pp. 79–86.\n[6] Y . Yang and F. Zhou, ‘‘Microblog sentiment analysis algorithm research\nand implementation based on classiﬁcation,’’ in Proc. 14th Int. Symp.\nDistrib. Comput. Appl. Bus. Eng. Sci. (DCABES), Aug. 2015, pp. 288–291.\n[7] C. Bingfeng, H. Zhifeng, and C. Ruichu, ‘‘A ﬁne-grained sentiment anal-\nysis algorithm for automotive reviews,’’ J. Guangdong Univ. Technol.,\nvol. 34, no. 3, pp. 8–14, 2017.\n[8] S. Zhu, B. Xu, D. Zheng, and T. Zhao, ‘‘Chinese microblog sentiment\nanalysis based on semi-supervised learning,’’ in Proc. 6th Chin. Semantic\nWeb Symp. 1st Chin. Web Sci. Conf. CSWS, Shenzhen, China, Nov. 2012,\npp. 325–331, doi: 10.1007/978-1-4614-6880-6_28.\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., Long Beach, CA, USA, 2017, pp. 5998–6008.\n[Online]. Available: http://papers.nips.cc/paper/7181-attention-is-all-you-\nneed\n[10] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term mem-\nory,’’ Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997,\ndoi: 10.1162/neco.1997.9.8.1735.\n[11] J. Chung, C. Gulcehre, K. Cho, Y . Bengio, ‘‘Empirical evaluation of gated\nrecurrent neural networks on sequence modeling,’’ CoRR, 2014. [Online].\nAvailable: http://arxiv.org/abs/1412.3555\n[12] S. Zheng and R. Xia, ‘‘Left-center-right separated neural network for\naspect-based sentiment analysis with rotatory attention,’’ CoRR, 2018.\n[Online]. Available: http://arxiv.org/abs/1802.00892\n[13] H. H. Do, P. W. C. Prasad, A. Maag, and A. Alsadoon, ‘‘Deep learning\nfor aspect-based sentiment analysis: A comparative review,’’ Expert Syst.\nAppl., vol. 118, pp. 272–299, 2019, doi: 10.1016/j.eswa.2018.10.003.\n[14] B. Liu, ‘‘Sentiment analysis and opinion mining,’’ Synth. Lectures Hum.\nLang. Technol., vol. 5, no. 1, pp. 1–167, 2012.\n[15] M. Hu and B. Liu, ‘‘Mining and summarizing customer reviews,’’ in Proc.\n10th SIGKDD Int. Conf. Knowl. Discovery Data Mining, Washington, DC,\nUSA, Aug. 2004, pp. 168–177, doi: 10.1145/1014052.1014073.\n[16] X. Ding, B. Liu, and P. S. Yu, ‘‘A holistic lexicon-based approach\nto opinion mining,’’ in Proc. Int. Conf. Web Search Web Data Min-\ning WSDM , 2008, Palo Alto, CA, USA, Feb. 2008, pp. 231–240,\ndoi: 10.1145/1341531.1341561.\n[17] T. H. Nguyen and K. Shirai, ‘‘Aspect-based sentiment analysis using\ntree Kernel based relation extraction,’’ in Proc. Int. Conf. Intell. Text\nProcess. Comput. Linguistics, Cairo, Egypt, Apr. 2015, pp. 114–125,\ndoi: 10.1007/978-3-319-18117-2_9.\n[18] A. Ramesh, S. H. Kumar, J. R. Foulds, and L. Getoor, ‘‘Weakly supervised\nmodels of aspect-sentiment for online course discussion forums,’’ in Proc.\n53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural\nLang. Process., Beijing, China, Jul. 2015, pp. 74–83. [Online]. Available:\nhttp://aclweb.org/anthology/P/P15/P15-1008.pdf\n[19] H. Wang and M. Ester, ‘‘A sentiment-aligned topic model for prod-\nuct aspect rating prediction,’’ in Proc. Conf. Empirical Methods Natu-\nral Lang. Process. (EMNLP), 2014, pp. 1192–1202. [Online]. Available:\nhttp://aclweb.org/anthology/D/D14/D14-1126.pdf\n[20] S. Kiritchenko, X. Zhu, C. Cherry, and S. Mohammad, ‘‘Nrc-canada-\n2014: Detecting aspects and sentiment in customer reviews,’’ in Proc.\n8th Int. Workshop Semantic Eval. (SemEval), Dublin, Ireland, Aug. 2014,\npp. 437–442. [Online]. Available: http://aclweb.org/anthology/S/S14/S14-\n2076.pdf\n[21] T. H. Nguyen and K. Shirai, ‘‘PhraseRNN: Phrase recursive neural network\nfor aspect-based sentiment analysis,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process.2015, Lisbon, Portugal, Sep. 2015, pp. 2509–2514.\n[Online]. Available: http://aclweb.org/anthology/D/D15/D15-1298.pdf\n[22] D. Tang, B. Qin, X. Feng, and T. Liu, ‘‘Effective lstms for target-\ndependent sentiment classiﬁcation,’’ in Proc. COLING, Osaka,\nJapan, 2016, pp. 3298–3307. [Online]. Available: http://aclweb.\norg/anthology/C/C16/C16-1311.pdf\n[23] Y . Wang, M. Huang, X. Zhu, and L. Zhao, ‘‘Attention-based\nLSTM for aspect-level sentiment classiﬁcation,’’ in Proc. Conf.\nEmpirical Methods Natural Lang. Process. (EMNLP), Austin, Tx,\nUSA, Nov. 2016, pp. 606–615. [Online]. Available: http://aclweb.\norg/anthology/D/D16/D16-1058.pdf\n[24] D. Tang, B. Qin, and T. Liu, ‘‘Aspect level sentiment classiﬁcation with\ndeep memory network,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), Austin, Tx, USA, Nov. 2016, pp. 214–224. [Online].\nAvailable: http://aclweb.org/anthology/D/D16/D16-1021.pdf\n[25] J. Cheng, S. Zhao, J. Zhang, I. King, X. Zhang, and H. Wang, ‘‘Aspect-level\nsentiment classiﬁcation with HEAT (hierarchical attention) network,’’ in\nProc. Conf. Inf. Knowl. Manage. CIKM, Singapore, Nov. 2017, pp. 97–106,\ndoi: 10.1145/3132847.3133037.\n[26] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, ‘‘Effective attention\nmodeling for aspect-level sentiment classiﬁcation,’’ in Proc. 27th Int.\nConf. Comput. Linguistics (COLING), Santa Fe, NM, USA, Aug. 2018,\npp. 1121–1131. [Online]. Available: https://aclanthology.info/papers/C18-\n1096/c18-1096\n[27] S. Pang, H. Chen, H. Liu, J. Yao, and M. Wang, ‘‘A deadlock resolution\nstrategy based on spiking neural p systems,’’ J. Ambient Intell. Humanized\nComput., pp. 1–12, 2019.\n[28] T. Song, S. Pang, S. Hao, A. Rodríguez-Patón, and P. Zheng, ‘‘A par-\nallel image skeletonizing method using spiking neural P systems with\nweights,’’ Neural Process. Lett., vol. 50, no. 2, pp. 1485–1502, 2019,\ndoi: 10.1007/s11063-018-9947-9.\n[29] X. Cui, J. Li, J. Li, J. Liu, T. Huang, and H. Chen, ‘‘Research on autocorre-\nlation and cross-correlation analyses in vehicular nodes positioning,’’ Int.\nJ. Distrib. Sensor Netw., vol. 15, no. 4, 2019, Art. no. 1550147719843864.\n[30] Y . Ma, H. Peng, and E. Cambria, ‘‘Targeted aspect-based senti-\nment analysis via embedding commonsense knowledge into an atten-\ntive LSTM,’’ in Proc. 32nd Conf. Artif. Intell. (AAAI-18) , New\nOrleans, LA, USA, Feb. 2018, pp. 5876–5883. [Online]. Available:\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16541\n[31] B. Wang and W. Lu, ‘‘Learning latent opinions for aspect-level senti-\nment classiﬁcation,’’ in Proc. 32nd Conf. Artif. Intell. AAAI-18), New\nOrleans, LA, USA, Feb. 2018, pp. 5537–5544. [Online]. Available:\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17327\n[32] Y . Song, S. Shi, J. Li, and H. Zhang, ‘‘Directional skip-gram: Explicitly\ndistinguishing left and right context for word embeddings,’’ in Proc. Conf.\nNorth Amer. Chapter Assoc. Comput. Linguistics Hum. Lang. Technol.,\nNew Orleans, LA, USA, Jun. 2018, pp. 175–180. [Online]. Available:\nhttps://www.aclweb.org/anthology/N18-2028\n[33] D. Ma, S. Li, X. Zhang, and H. Wang, ‘‘Interactive attention networks for\naspect-level sentiment classiﬁcation,’’ in Proc. 26th Int. Joint Conf. Artif.\nIntell. (IJCAI), Melbourne, VIC, Australia, Aug. 2017, pp. 4068–4074,\ndoi: 10.24963/ijcai.2017/568.\n[34] P. Chen, Z. Sun, L. Bing, and W. Yang, ‘‘Recurrent attention net-\nwork on memory for aspect sentiment analysis,’’ in Proc. Conf. Empir-\nical Methods Natural Lang. Process. (EMNLP), Copenhagen, Den-\nmark, Sep. 2017, pp. 452–461. [Online]. Available: https://www.aclweb.\norg/anthology/D17-1047/\nMING JIANG received the Ph.D. degree in com-\nputer science from Zhejiang University, in 2004.\nHe is currently a Professor with the College of\nComputer Science, Hangzhou Dianzi University.\nHis research areas include data mining, image pro-\ncessing, and artiﬁcial intelligence.\n179952 VOLUME 7, 2019\nM. Jianget al.: Transformer-Based Memory Network for Sentiment Analysis of Web Comments\nJUNLEI WU received the B.S. degree from the\nSchool of Computer Science and Technology,\nSouthwest Minzu University, China, in 2017. He is\ncurrently pursuing the M.S. degree with the School\nof Computer Science and Technology, Hangzhou\nDianzi University, China. His research interests\ninclude natural language processing (NLP) and\nsentiment analysis.\nXIANGRONG SHIreceived the Ph.D. degree from\nthe Department of Control Science and Engineer-\ning, Zhejiang University, in 2014. He currently is\nan Associate Professor with the School of Informa-\ntion Management and Engineering, Zhejiang Uni-\nversity of Finance and Economics. His research\ninterests include data mining, machine learning,\nand decision support systems.\nMIN ZHANG received the Ph.D. degree in com-\nputer science from Zhejiang University, in 2012.\nHe is currently works with the College of Com-\nputer Science, Hangzhou Dianzi University. His\nresearch areas include image processing, machine\nvision, and machine learning.\nVOLUME 7, 2019 179953"
}