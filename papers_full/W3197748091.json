{
  "title": "Sustainable Modular Debiasing of Language Models",
  "url": "https://openalex.org/W3197748091",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5022688200",
      "name": "Anne Lauscher",
      "affiliations": [
        "Bocconi University"
      ]
    },
    {
      "id": "https://openalex.org/A5055902959",
      "name": "Tobias Lüken",
      "affiliations": [
        "University of Mannheim"
      ]
    },
    {
      "id": "https://openalex.org/A5079336821",
      "name": "Goran Glavaš",
      "affiliations": [
        "University of Mannheim"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093211917",
    "https://openalex.org/W3103616906",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3128232076",
    "https://openalex.org/W64581525",
    "https://openalex.org/W3194924193",
    "https://openalex.org/W3177468621",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W3154836396",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W3105421296",
    "https://openalex.org/W2963524349",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3155655882",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2942160782",
    "https://openalex.org/W3118258541",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2926555354",
    "https://openalex.org/W3011285576",
    "https://openalex.org/W2913897682",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W3112302586",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2972572477",
    "https://openalex.org/W3110062857",
    "https://openalex.org/W3097738726",
    "https://openalex.org/W2970109976",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3081608896",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3096966601",
    "https://openalex.org/W3100353583",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2971015127",
    "https://openalex.org/W2998463583",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2997588435",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963809228"
  ],
  "abstract": "Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -- besides being computationally expensive -- comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that -- due to its modular nature -- ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4782–4797\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n4782\nSustainable Modular Debiasing of Language Models\nAnne Lauscher,1∗†Tobias Lüken,2∗Goran Glavaš2\n1MilaNLP, Bocconi University, Via Sarfatti 25, 20136 Milan, Italy\n2Data and Web Science Group, University of Mannheim, B 6, 26, 68159 Mannheim, Germany\nanne.lauscher@unibocconi.it, tlueken@mail.uni-mannheim.de,\ngoran@informatik.uni-mannheim.de\nAbstract\nUnfair stereotypical biases (e.g., gender, racial,\nor religious biases) encoded in modern pre-\ntrained language models (PLMs) have nega-\ntive ethical implications for widespread adop-\ntion of state-of-the-art language technology.\nTo remedy for this, a wide range of debiasing\ntechniques have recently been introduced to\nremove such stereotypical biases from PLMs.\nExisting debiasing methods, however, directly\nmodify all of the PLMs parameters, which\n– besides being computationally expensive –\ncomes with the inherent risk of (catastrophic)\nforgetting of useful language knowledge ac-\nquired in pretraining. In this work, we pro-\npose a more sustainable modular debiasing ap-\nproach based on dedicated debiasing adapters,\ndubbed A DELE . Concretely, we (1) inject\nadapter modules into the original PLM layers\nand (2) update only the adapters (i.e., we keep\nthe original PLM parameters frozen) via lan-\nguage modeling training on a counterfactually\naugmented corpus. We showcase A DELE in\ngender debiasing of BERT: our extensive eval-\nuation, encompassing three intrinsic and two\nextrinsic bias measures, renders A DELE very\neffective in bias mitigation. We further show\nthat – due to its modular nature – ADELE , cou-\npled with task adapters, retains fairness even\nafter large-scale downstream training. Finally,\nby means of multilingual BERT, we success-\nfully transfer ADELE to six target languages.\n1 Introduction\nRecent work has shown that pretrained language\nmodels such as ELMo (Peters et al., 2018),\nBERT (Devlin et al., 2019), or GPT-2 (Radford\net al., 2019) tend to exhibit a range of stereotypical\nsocietal biases, such as racism and sexism (e.g.,\nKurita et al., 2019; Dev et al., 2020; Webster et al.,\n2020; Nangia et al., 2020; Barikeri et al., 2021,\n∗Equal contribution.\n†Most of the work was conducted while Anne Lauscher\nwas employed at the University of Mannheim.\ninter alia). The reason for this lies in the distribu-\ntional nature of these models: human-produced\ncorpora on which these models are trained are\nabundant with stereotypically biased concept co-\noccurrences (for instance, male terms like man or\nson appear more often together with certain ca-\nreer terms like doctor or programmer than female\nterms like women or daughter) and the PLMs mod-\nels, being trained with language modeling objec-\ntives, consequently encode these biased associa-\ntions in their parameters. While this effect can lend\nitself to diachronic analysis of societal biases (e.g.,\nGarg et al., 2018; Walter et al., 2021), it represents\nstereotyping, one of the main types of representa-\ntional harm (Blodgett et al., 2020) and, if unmiti-\ngated, may cause severe ethical issues in various\nsociotechnical deployment scenarios.\nTo alleviate this problem and ensure fair lan-\nguage technology, previous work introduced a wide\nrange of bias mitigation methods (e.g., Bordia\nand Bowman, 2019; Dev et al., 2020; Lauscher\net al., 2020a, inter alia). All existing debiasing\napproaches, however, modify all parameters of\nthe PLMs which has two prominent shortcom-\nings: (1) it comes with a high computational cost1\nand (2) can lead to (catastrophic) forgetting (Mc-\nCloskey and Cohen, 1989; Kirkpatrick et al., 2017)\nof the useful distributional knowledge obtained dur-\ning pretraining. For example, Webster et al. (2020)\nincorporate counterfactual debiasing already into\nBERT’s pretraining: this implies a debiasing frame-\nwork in which a separate “debiased BERT” in-\nstance needs to be trained from scratch for each\nindividual bias type and speciﬁcation. In sum, cur-\nrent debiasing procedures designed for pretraining\nor full ﬁne-tuning of PLMs have a large carbon\nfootprint (Strubell et al., 2019) and consequently\n1While a full ﬁne-tuning approach to PLM debiasing may\nstill be feasible for moderate-sized PLMs like BERT (Devlin\net al., 2019), it is prohibitively computationally expensive for\ngiant language models like GPT-3 (Brown et al., 2020) or\nGShard (Lepikhin et al., 2020).\n4783\njeopardize the sustainability (Moosavi et al., 2020)\nof fair representation learning in NLP.\nIn this work, we move towards more sustain-\nable removal of stereotypical societal biases from\npretrained language models. To this end, we\npropose ADELE (Adapter-based DEbiasing of\nLanguagE Models), a debiasing approach based on\nthe the recently proposed modular adapter frame-\nwork (Houlsby et al., 2019; Pfeiffer et al., 2020a).\nIn ADELE , we inject additional parameters, the so-\ncalled adapter layers into the layers of the PLM\nand incorporate the “debiasing” knowledge only in\nthose parameters, without changing the pretrained\nknowledge in the PLM. We show that, while be-\ning substantially more efﬁcient (i.e., sustainable)\nthan existing state-of-the-art debiasing approaches,\nADELE is just as effective in bias attenuation.\nContributions. The contributions of this work\nare three-fold: (i) we ﬁrst present ADELE , our\nnovel adapter-based framework for parameter-\nefﬁcient and knowledge-preserving debiasing of\nPLMs. We combine ADELE with one of the\nmost effective debiasing strategies, Counterfactual\nData Augmentation (CDA; Zhao et al., 2018), and\ndemonstrate its effectiveness in gender-debiasing\nof BERT (Devlin et al., 2019), the most widely\nused PLM. (ii) We benchmark ADELE in what is\narguably the most comprehensive set of bias mea-\nsures and data sets for both intrinsic and extrin-\nsic evaluation of biases in representation spaces\nspanned by PLMs. Additionally, we study a previ-\nously neglected effect of fairness forgetting present\nwhen debiased PLMs are subjected to large-scale\ndownstream training for speciﬁc tasks (e.g., natural\nlanguage inference, NLI); we show that ADELE ’s\nmodular nature allows to counter this undesirable\neffect by stacking a dedicated task adapter on top of\nthe debiasing adapter. (iii) Finally, we successfully\ntransfer ADELE ’s debiasing effects to six other lan-\nguages in a zero-shot manner, i.e., without rely-\ning on any debiasing data in the target languages.\nWe achieve this by training the debiasing adapter\nstacked on top of the multilingual BERT on the\nEnglish counterfactually augmented dataset.\n2 A DELE : Adapter-Based Debiasing\nIn this work, we seek to fulﬁll the following three\ndesiderata: (1) we want to achieve effective de-\nbiasing, comparable to that of existing state-of-\nthe-art debiasing methods while (2) keeping the\ntraining costs of debiasing signiﬁcantly lower; and\n(3) fully preserving the distributional knowledge\nacquired in the pretraining. To meet all three cri-\nteria, we propose debiasing based on the popu-\nlar adapter modules (Houlsby et al., 2019; Pfeif-\nfer et al., 2020a). Adapters are lightweight neu-\nral components designed for parameter-efﬁcient\nﬁne-tuning of PLMs, injected into the PLM layers.\nIn downstream ﬁne-tuning, all original PLM pa-\nrameters are kept frozen and only the adapters are\ntrained. Because adapters have fewer parameters\nthan the original PLM, adapter-based ﬁne-tuning\nis more computationally efﬁcient. And since ﬁne-\ntuning does not update the PLM’s original parame-\nters, all distributional knowledge is preserved.\nThe debiasing adapters could, in principle, be\ntrained using any of the debiasing strategies and\ntraining objectives from the literature, e.g., via ad-\nditional debiasing loss objectives Qian et al. (2019);\nBordia and Bowman (2019); Lauscher et al. (2020a,\ninter alia) or data-driven approaches such as Coun-\nterfactual Data Augmentation (Zhao et al., 2018).\nFor simplicity, we opt for the data-driven CDA\napproach: it has been shown to offer reliable de-\nbiasing performance (Zhao et al., 2018; Webster\net al., 2020) and, unlike other approaches, it does\nnot require any modiﬁcations of the model archi-\ntecture nor training procedure.\n2.1 Debiasing Adapters\nIn this work, we employ the simple adapter archi-\ntecture proposed by Pfeiffer et al. (2021), in which\nonly one adapter module is added to each layer of\nthe pretrained Transformer, after the feed-forward\nsub-layer. The more widely used architecture of\nHoulsby et al. (2019) inserts two adapter mod-\nules per Transformer layer, with the other adapter\ninjected after the multi-head attention sublayer.\nWe opt for the “Pfeiffer architecture” because in\ncomparison with the “Houlsby architecture” it is\nmore parameter-efﬁcient and has been shown to\nyield slightly better performance on a wide range\nof downstream NLP tasks (Pfeiffer et al., 2020a,\n2021). The output of the adapter, a two-layer feed-\nforward network, is computed as follows:\nAdapter(h,r) =U ·g(D·h) +r, (1)\nwith h and r as the hidden state and residual of\nthe respective Transformer layer. D∈Rm×h and\nU ∈Rh×m are the linear down- and up-projections,\nrespectively (hbeing the Transformer’s hidden size,\nand mthe adapter’s bottleneck dimension), andg(·)\n4784\nis a non-linear activation function. The residualr is\nthe output of the Transformer’s feed-forward layer\nwhereas h is the output of the subsequent layer nor-\nmalization. The down-projection Dcompresses to-\nken representations to the adapter size m<h , and\nthe up-projection U projects the activated down-\nprojections back to the Transformer’s hidden size\nh. The ratio h/m captures the factor by which\nthe adapter-based ﬁne-tuning is more parameter-\nefﬁcient than full ﬁne-tuning of the Transformer.\nIn our case, we train the adapters for debias-\ning: we inject adapter layers into BERT (Devlin\net al., 2019), freeze the original BERT’s parameters,\nand run a standard debiasing training procedure –\nlanguage modeling on counterfactual data (§2.2) –\nduring which we only tune the parameters of the\ndebiasing adapters. At the end of the debiasing\ntraining, the debiasing functionality is isolated into\nthe adapter parameters. This not only preserves the\ndistributional knowledge in the Transformer’s orig-\ninal parameters, but also allows for more ﬂexibility\nand “on-demand” usage of the debiasing function-\nality in downstream applications. For example,\none could train a separate set of debiasing adapters\nfor each bias dimension of interest (e.g., gender,\nrace, religion, sexual orientation) and selectively\ncombine them in downstream tasks, depending on\nthe constraints and requirements of the concrete\nsociotechnical environment.\n2.2 Counterfactual Augmentation Training\nIn the context of representation debiasing, coun-\nterfactual data augmentation (CDA) refers to the\nautomatic creation of text instances that in some\nway counter the stereotypical bias present in the\nrepresentation space. CDA has been successfully\nused for attenuating a variety of bias types, e.g.,\ngender and race, and in several variants, e.g., with\ngeneral terms describing dominant and minoritized\ngroups, or with personal names acting as proxies\nfor such groups (Zhao et al., 2018; Lu et al., 2020).\nMost commonly, CDA modiﬁes the training data by\nreplacing terms describing one of the target groups\n(dominant or minoritized) with terms describing\nthe other group. Let Sbe our training corpus, con-\nsisting of sentences sand let T = {(t1,t2)i}N\ni=1 be\na set of N term pairings between the dominant and\nminoritized group (i.e., t1 is a term representing\nthe dominant group, e.g., man, and t2 is a corre-\nsponding term representing the minoritized group,\ne.g., woman). For each sentence si and each pair\n(t1,t2), we check whether either t1 or t2 occur in\ns: if t1 is present, we replace its occurrence with t2\nand vice versa. We denote the counterfactual sen-\ntence of sobtained this way with s′and the whole\ncounterfactual corpus with S′. We adopt the so-\ncalled two-sided CDA from (Webster et al., 2020):\nthe ﬁnal corpus for debiasing training consists of\nboth the original and counterfactually created sen-\ntences. Finally, we train the debiasing adapter via\nmasked language modeling on the counterfactually\naugmented corpus S∪S′. We train sequentially by\nﬁrst exposing the adapter to the original corpus S\nand then to the augmented portion S′.\n3 Experiments\nWe showcase ADELE for arguably the most ex-\nplored societal bias – gender bias – and the most\nwidely used PLM, BERT. We proﬁle its debiasing\neffects with a comprehensive set of intrinsic and\ndownstream (i.e., extrinsic) evaluations.\n3.1 Evaluation Data Sets and Measures\nWe test ADELE on three intrinsic (BEC-Pro, DisCo,\nWEAT) and two downstream debiasing bench-\nmarks (Bias-STS-B and Bias-NLI). We now de-\nscribe each of the benchmarks in more detail.\nBias Evaluation Corpus with Professions (BEC-\nPro). We intrinsically evaluate ADELE on the\nBEC-Pro data set (Bartl et al., 2020), designed to\ncapture gender bias w.r.t. professions. The data set\nconsists of 2,700 sentence pairs in the format (“m\n[temp] p”; “f [temp] p”), where m is a male term\n(e.g., boy, groom), f is a female term (e.g., girl,\nbride), p is a profession term (e.g., mechanic, doc-\ntor), and [temp] is one of the predeﬁned connecting\ntemplates, e.g., “is a” or “works as a”.\nWe measure the bias on BEC-Pro using the bias\nmeasure of Kurita et al. (2019). They compute the\nassociation at,p between a gender term t(male or\nfemale) and a profession pas:\nat,p = log P(t)t\nP(t)t,p\n, (2)\nwhere P(t)t is the probability of the PLM generat-\ning the target term twhen only titself is masked,\nand P(t)t,p is the probability of tbeing generated\nwhen both tand the profession pare masked. The\nbias score bis then simply a difference in the as-\nsociation score between the male term mand its\ncorresponding female term f: b = am,p −af,p.\nWe measure the overall bias on the whole dataset\n4785\nin two complementary ways: (a) by averaging the\nbias scores bacross all 2,700 instances (∅ bias) and\n(b) by measuring the percentage of instances for\nwhich bis below some threshold value: we report\nthis score for two different thresholds (0.1 and 0.7).\nBartl et al. (2020) additionally published a Ger-\nman version of the BEC-Pro data set, which we use\nto evaluate ADELE ’s zero-shot transfer abilities.\nDiscovery of Correlations (DisCo). The sec-\nond data set for intrinsic debiasing evaluation,\nDisCo (Webster et al., 2020), also relies on tem-\nplates (e.g., “[PERSON] studied [BLANK] at col-\nlege”). For each template, the [PERSON] slot is\nﬁlled ﬁrst with a male and then with a female term\n(e.g., for the pair ( John, Veronica), we get John\nstudied [BLANK] at college and Veronica studied\n[BLANK] at college). Next, for each of the two\ninstances, the model is asked to ﬁll the [BLANK]\nslot: the goal is to determine the difference in the\nprobability distribution for the masked token, de-\npending on which term is inserted in the [PERSON]\nslot. While Webster et al. (2020) retrieve the top\nthree most likely terms for the masked position, we\nretrieve all terms t with the probabilityp(t) >0.1.2\nLet C(i)\nm and C(i)\nf be the candidate sets obtained\nfor the i-th instance when ﬁlled with a male [PER-\nSON] term mand the corresponding female termf,\nrespectively. We then compute two different mea-\nsures. The ﬁrst is the average fraction of shared\ncandidates between the two sets (∅frac):\n∅frac = 1\nN\nN∑\ni\n|C(i)\nm ∩C(i)\nf |\nmin (|C(i)\nm |,|C(i)\nf |)\n, (3)\nwith N as the total number of test instances. Intu-\nitively, a higher average fraction of shared candi-\ndates indicates lower bias.\nFor the second measure, we retrieve the proba-\nbilities p(t) for all candidates tin the union of two\nsets C(i) = C(i)\nm ∪C(i)\nf . We then compute the nor-\nmalized average absolute probability difference:\n∅diff= 1\nN\nN∑\ni\n∑\nt∈Ci\n|pm(t) −pf (t)|\n(∑\nt∈C(i)\nm\npm(t) +∑\nt∈C(i)\nm\npf (t))/2. (4)\nWe create test instances by collecting 100 most\nfrequent baby names for each gender from the US\nSocial Security name statistics for 2019.3 We cre-\nate pairs (m, f) from names at the same frequency\n2We argue that retrieving more terms from the distribution\nallows for a more accurate estimate of the bias.\n3https://www.ssa.gov/oact/babynames/limits.html\nrank in the two lists (e.g., Liam and Olivia). Fi-\nnally, we remove pairs with ambiguous names that\nmay also be used as general concepts (e.g., violet,\na color), resulting in ﬁnal 92 pairs.\nWord Embedding Association Test (WEAT).\nAs the ﬁnal intrinsic measure, we use the well-\nknown WEAT (Caliskan et al., 2017) test. Devel-\noped for detecting biases in static word embedding\nspaces, it computes the differential association be-\ntween two target term sets A(e.g., male terms) and\nB(e.g., female terms) based on the mean (cosine)\nsimilarity of their embeddings with embeddings\nof terms from two attribute sets X (e.g., science\nterms) and Y (e.g., art terms):\nw(A,B,X,Y ) =\n∑\na∈A\ns(a,X,Y ) −\n∑\nb∈B\ns(b,X,Y ) . (5)\nThe association sof term t∈Aor t∈Bis com-\nputed as:\ns(t,X,Y)= 1\n|X|\n∑\nx∈X\ncos(t,x) − 1\n|Y|\n∑\ny∈Y\ncos(t,y) . (6)\nThe signiﬁcance of the statistic is computed with\na permutation test in which s(A,B,X,Y ) is com-\npared with the scores s(A∗,B∗,X,Y ) where A∗\nand B∗are equally sized partitions of A∪B. We\nreport the effect size, a normalized measure of sep-\naration between the association distributions:\nµ({s(a,X,Y )}a∈A) −µ({s(b,X,Y )}b∈B)\nσ({s(t,X,Y )}t∈A∪B) , (7)\nwhere µis the mean and σis the standard deviation.\nSince WEAT requires word embeddings as in-\nput, we ﬁrst have to extract word-level vectors from\na PLM like BERT. To this end, we follow Vuli ´c\net al. (2020) and obtain a vector xi ∈Rd for each\nword wi (e.g., man) from the bias speciﬁcation as\nfollows: we prepend the word with the BERT’s se-\nquence start token and append it with the separator\ntoken (e.g., [CLS] man [SEP]). We then feed\nthe input sequence through the Transformer and\ncompute xi as the average of the term’s represen-\ntations from layers m: n. We experimented with\ninducing word-level embeddings by averaging rep-\nresentations over all consecutive ranges of layers\n[m: n], m≤n. We measure the gender bias using\nthe test WEAT 7 (see the full speciﬁcation in the\nAppendix), which compares male terms (e.g., man,\nboy) against female terms (e.g., woman, girl) w.r.t.\nassociations to science terms (e.g., math, algebra,\nnumbers) and art terms (e.g., poetry, dance, novel).\n4786\nLauscher and Glavaš (2019) created XWEAT\nby translating some of the original WEAT bias\nspeciﬁcations to six target languages: German (DE),\nSpanish (ES), Italian (IT), Croatian (HR), Russian\n(RU), and Turkish (TR). We use their translations of\nthe WEAT 7 gender test in the zero-shot debiasing\ntransfer evaluation of ADELE .\nBias-STS-B. The ﬁrst extrinsic measure we\nuse is Bias-STS-B, introduced by Webster et al.\n(2020), based on the well-known Semantic Textual\nSimilarity-Benchmark (STS-B; Cer et al., 2017),\na regression task where models need to predict se-\nmantic similarity for pairs of sentences. Webster\net al. (2020) adapt STS-B for discovering gender-\nbiased correlations. They start from neutral STS\ntemplates and ﬁll them with a gendered term (man,\nwoman) and a profession term from (Rudinger\net al., 2018) (e.g., A man is walking vs. A nurse\nis walking and A woman is walking vs. A nurse\nis walking). The dataset consists of 16,980 such\npairs. As a measure of bias, we compute the av-\nerage absolute difference between the similarity\nscores of male and female sentence pairs, with a\nlower value corresponding to less bias. We couple\nthe bias score with the actual STS task performance\nscore (Pearson correlation with human similarity\nscores), measured on the STS-B development set.\nBias-NLI. We select the task of understanding\nbiased natural language inferences (NLI) as the sec-\nond extrinsic evaluation. To this end, we ﬁne-tune\nthe original BERT as well as our adapter-debiased\nBERT on the MNLI data set (Williams et al., 2018).\nFor evaluation, we follow Dev et al. (2020), and\ncreate a synthetic NLI data set that tests for the\ngender-occupation bias: it comprises NLI instances\nfor which an unbiased model should not be able\nto infer anything, i.e., it should predict the NEU -\nTRAL class. We use the code of Dev et al. (2020)\nand, starting from the generic template The <sub-\nject> <verb> a/an <object> , ﬁll the slots with\nterm sets provided with the code. First, we ﬁll the\nverb and object slots with common activities, e.g.,\n“bought a car”. We then create neutral entailment\npairs by ﬁlling the subject slot with an occupation\nterm, e.g., “physician”, for the hypothesis and a\ngendered term, e.g., “woman”, for the premise,\nresulting in the ﬁnal instance: ( woman bought a\ncar, physician bought a car, NEUTRAL ). Using the\ncode and terms released by Dev et al. (2020), we\nproduce the total of N = 1,936,512 Bias-NLI in-\nstances. Following the original work, we compute\ntwo bias scores: (1) the fraction neutral (FN) score\nis the percentage of instances for which the model\npredicts the NEUTRAL class; (2) net neutral (NN)\nscore is the average probability that the model as-\nsigns to the NEUTRAL class across all instances.\nIn both cases, the higher score corresponds to a\nlower bias. We couple FN and NN on Bias-NLI\nwith the actual NLI accuracy on the MNLI matched\ndevelopment set (Williams et al., 2018).\n3.2 Experimental Setup\nData. Aligned with BERT’s pretraining, we carry\nout the debiasing MLM training on the concatena-\ntion of the English Wikipedia and the BookCor-\npus (Zhu et al., 2015). Since we are only training\nthe parameters of the debiasing adapters, we uni-\nformly subsample the corpus to one third of its\noriginal size. We adopt the set of gender term\npairs T for CDA from Zhao et al. (2018) (e.g.,\nactor-actress, bride-groom)4 and augment it with\nthree additional pairs: his-her, himself -herself, and\nmale-female, resulting with the total of 193 term\npairs. Our ﬁnal debiasing CDA corpus consists of\n105,306,803 sentences.\nModels and Baselines. In all experiments we in-\nject ADELE adapters of bottleneck size m = 48\ninto the pretrained BERTBase Transformer (12 lay-\ners, 12 attention heads, 768 hidden size).5 We com-\npare ADELE with the debiased BERTLarge models\nreleased by Webster et al. (2020): (1) ZariCDA is\ncounterfactually pretrained (from scratch); whereas\n(2) ZariDO was post-hoc MLM-ﬁne-tuned on regu-\nlar corpora, but with more aggressive dropout rates.\nIn cross-lingual zero-shot transfer experiments, we\ntrain ADELE on top of multilingual BERT (Devlin\net al., 2019) in its base conﬁguration (uncased, 12\nlayers, 768 hidden size).\nDebiasing Training. We follow the standard\nMLM procedure for BERT training and mask 15%\nof the tokens. We then train ADELE ’s debiasing\nadapters on our CDA data set for 2 epochs, with a\nbatch size of 16. We optimize the adapter param-\neters using the Adam algorithm (Kingma and Ba,\n2015), with the constant learning rate of 3 ·10−5.\n4https://github.com/uclanlp/corefBias/\ntree/master/WinoBias/wino\n5We implementADELE using the Huggingface tranformers\nlibrary (Wolf et al., 2020) in combination with the AdapterHub\nframework (Pfeiffer et al., 2020a).\n4787\nDownstream Fine-tuning. Our two extrinsic\nevaluations require task-speciﬁc ﬁne-tuning on\nthe STS-B and MNLI training datasets, respec-\ntively. We couple BERT (with and without\nADELE adapters) with the standard single-layer\nfeed-forward softmax classiﬁer and ﬁne-tune all\nparameters in task-speciﬁc training.6 We optimize\nthe hyperparameters on the respective STS-B and\nMNLI (matched) development sets. To this end, we\nsearch for the optimal number of training epochs\nin {2,3,4}and ﬁx the learning rate to 2 ·10−5,\nmaximum sequence length to 128, and batch size\nto 32. Like in debiasing training, we use Adam\n(Kingma and Ba, 2015) for optimization.\n4 Results and Discussion\nMonolingual Evaluation. Our main monolin-\ngual English debiasing results on three intrinsic\nand two extrinsic benchmarks are summarized in\nTable 1. The results show that (1) ADELE suc-\ncessfully attenuates BERT’s gender bias across the\nboard, and (2) it is, in many cases, more effective in\nattenuating gender biases than the computationally\nmuch more intensive Zari models (Webster et al.,\n2020). In fact, on BEC-Pro and DisCo ADELE\nsubstantially outperforms both Zari variants.\nThe results from two extrinsic evaluations – STS\nand NLI – demonstrate that ADELE successfully\nattenuates the bias, while retaining the high task\nperformance. Zari variants yield slightly better task\nperformance for both STS-B and MNLI: this is\nexpected, as they are instances of the BERT Large\nTransformer with 336M parameters; in comparison,\nADELE has only 110M parameters of BERT Base\nand approx. 885K adapter parameters.7\nAccording to WEAT evaluation on static em-\nbeddings extracted from BERT (§3.1), the original\nBERT Transformer is only slightly and insigniﬁ-\ncantly biased. Consequently, ADELE inverts the\nbias in the opposite direction. In Figure 1, we\nfurther analyze the WEAT bias effects w.r.t. the\nsubset of BERT layers from which we aggregate\nthe word embeddings. For the original BERT (Fig-\nure 1a), we obtain the gender unbiased embeddings\nif we aggregate representations from higher layers\n(e.g., [5:12], [6:9], or by taking ﬁnal layer vectors,\n6The only exception is thefairness forgettingexperiment in\n§4, in which we freeze both the Transformer and the debiasing\nadapters and train the dedicated task adapter on top.\n7ADELE adds 884,736 parameters to BERT Base: 12 (lay-\ners) ×2 (down-projection and up-projection matrix) ×768\n(hidden size hof BERT Base) ×48 (bottleneck size m).\n[12:12]). For ADELE , we get the most gender-\nneutral embeddings by aggregating representations\nfrom lower layers (e.g., [0:3] or [1:3]); representa-\ntions from higher layers (e.g., [6:12]) ﬂip the bias\ninto the opposite direction (blue color). Both Zari\nmodels produce embeddings which are relatively\nunbiased, but ZariCDA still exhibits slight gender\nbias in higher layer representations. The dropout-\nbased debiasing of ZariDO results in an interesting\nper-layer-region oscillating gender bias.\nZero-Shot Cross-Lingual Transfer. We show\nthe results of zero-shot transfer of gender debias-\ning with ADELE (on top of mBERT) on German\nBEC-Pro in Table 2. On the EN BEC-Pro portion\nADELE is as effective on top of mBERT as it is\non top of the EN BERT (see Table 1): it reduces\nmBERT’s bias from0.81 to 0.3. More importantly,\nthe positive debiasing effect successfully transfers\nto German: the bias effect on the DE portion is\nreduced from 1.1 to 0.67, despite not using any\nGerman data in the training of debiasing adapters.\nWe also see an improvement with respect to the\nfraction of unbiased instances for both thresholds,\nexpectedly with larger improvements for the more\nlenient threshold of 0.7.\nIn Table 3, we show the bias effects of static\nword embeddings, aggregated from layers of\nmBERT and ADELE -debiased mBERT, on the\nXWEAT gender-bias test 7 for six different target\nlanguages. We show the results for two aggregation\nstrategies, including ([0:12]) and excluding ([1:12])\nmBERT’s (sub)word embedding layer.\nLike BEC-Pro, WEAT conﬁrms thatADELE also\nattenuates the bias in EN representations coming\nfrom mBERT. The results across the six target lan-\nguages are somewhat mixed, but overall encour-\naging: for all signiﬁcantly biased combinations\nof languages and layer aggregations from original\nmBERT ([0:12] – IT, RU; [1:12] – HR, RU), ADELE\nsuccessfully reduces the bias. E.g., for IT embed-\ndings extracted from all layers ([0:12]), the bias\neffect size drops from signiﬁcant 1.02 to insigniﬁ-\ncant −0.25. In case of already insigniﬁcant biases\nin original mBERT, ADELE often further reduces\nthe bias effect size ( DE, TR) and if not, the bias\neffects remain insigniﬁcant.\nWe additionally visualize all XWEAT bias effect\nsizes in the produced embeddings via heatmaps\nin Figure 2. The intuition we can get from the\nplots supports our conclusion: for all languages,\nespecially for the source language EN and the tar-\n4788\nWEAT T7 BEC-Pro DisCo (names) STS NLI\nModel e[0:12] ↓ ∅ bias↓ t(0.1)↑ t(0.7)↑ ∅ frac↑ ∅ diff↓ ∅ diff↓ Pear↑ FN↑ NN↑ Acc↑\nBERT 0.79* 1.33 0.05 0.37 0.8112 0.5146 0.313 88.78 0.0102 0.0816 84.77\nZariCDA 0.43* 1.11 0.07 0.45 0.7527 0.6988 0.087 89.37 0.1202 0.1628 85.52\nZariDO 0.23* 1.20 0.07 0.38 0.6422 0.9352 0.118 88.22 0.1058 0.1147 86.06\nADELE -0.98 0.39 0.17 0.85 0.8862 0.3118 0.121 88.93 0.1273 0.1726 84.13\nTable 1: Results of our monolingual gender bias evaluation. We report WEAT effect size (e), BEC-Pro average\nbias ( ∅ bias) and fraction of biased instances at thresholds 0.1 and 0.7, DisCo average fraction ( ∅ frac) and\naverage difference (∅ diff), STS average similarity difference ( ∅ diff) and Pearson correlation (Pear), and Bias-\nNLI fraction neutral (FN) and net neutral (NN) scores as well as MNLI-m accuracy (Acc) for three models: original\nBERT, ZariCDA and ZariDO (Webster et al., 2020), and ADELE . ↑: higher is better (lower bias); ↓: lower is better.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.0\n0.5\n0.0\n0.5\n1.0\n(a) BERTBase.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.0\n0.5\n0.0\n0.5\n1.0\n (b) BERTADELE .\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nm\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 n\n1.0\n0.5\n0.0\n0.5\n1.0\n (c) ZariCDA.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nm\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 n\n1.0\n0.5\n0.0\n0.5\n1.0\n (d) ZariDO.\nFigure 1: WEAT bias effect heatmaps for (a) original BERT Base, and the debiased BERTs, (b) BERT ADELE , (c)\nZariCDA (Webster et al., 2020), and (d) ZariCDA, for word embeddings averaged over different subsets of layers\n[m: n]. E.g., [0 : 0]points to word embeddings directly obtained from BERT’s (sub)word embeddings (layer 0);\n[1 : 7]indicates word vectors obtained by averaging word representations after Transformer layers 1 through 7.\nEN DE\nModel ∅ bias t(0.1) t(0.7) ∅ bias t(0.1) t(0.7)\nmBERT 0.81 0.08 0.55 1.10 0.08 0.39\nmBERTA 0.30 0.23 0.93 0.67 0.11 0.62\nTable 2: Results for mBERT and mBERT debiased on\nEN data with ADELE on BEC-Pro English and German.\nWe report the average bias (∅ bias) and the fraction of\nbiased instances for thresholds t(0.1) and t(0.7).\nLayers Model EN DE ES IT HR RU TR\n0:12 mBERT 1.42 0.59* -0.47* 1.02 -0.57* 1.49 -0.55*\nmBERTA 0.20* -0.04* -0.49* -0.25* 0.72* 1.24 -0.33*\n1:12 mBERT 1.36 0.62* -0.55* -0.55* 1.08 0.62 -0.61*\nmBERTA -0.08 -0.05* -0.63* -0.63* 0.79* -0.05 -0.34*\nTable 3: XWEAT effect sizes for original mBERT and\nzero-shot cross-lingual debiasing transfer of A DELE\n(mBERTA) from EN to six target languages. Results\nfor two variants of embedding aggregation over Trans-\nformer layers: [1:12] – all Tranformer layers; [0:12] –\nall layers plus mBERT’s (sub)word embeddings (“layer\n0”). Asterisks: insigniﬁcant bias effects at α< 0.05.\nget language DE, the bias gets reduced, which is\nindicated by the lighter colors throughout all plots.\nFairness Forgetting. Finally, we investigate\nwhether the debiasing effects persist even after the\nlarge-scale ﬁne-tuning in downstream tasks. Web-\nster et al. (2020) report the presence of debiasing\neffects after STS-B training. With merely 5,749\ntraining instances, however, STS-B is two orders\nof magnitude smaller than MNLI (392,702 train-\ning instances). Here we conduct a study on MNLI,\ntesting for the presence of the gender bias in Bias-\nNLI after ADELE ’s exposure to varying amount\nof MNLI training data. We fully ﬁne-tune BERT\nBase and BERTADELE (i.e., BERT augmented with\ndebiasing adapters) on MNLI datasets of varying\nsizes (10K, 25K, 75K, 100K, 150K, and 200K) and\nmeasure, for each model, the Bias-NLI net neu-\ntral (NN) score as well as the NLI accuracy on the\nMNLI (matched) development set. For each model\nand each training set size, we carry out ﬁve training\nruns and report the average scores.\nFigure 3 summarizes the results of our fairness\nforgetting experiment. We report the mean and the\n95% conﬁdence interval over the ﬁve runs for NN\non Bias-NLI and Accuracy (Acc) on the MNLI-m\ndevelopment set. Several interesting observations\nemerge. First, the NN scores seem to be quite\nunstable across different runs (wide conﬁdence\nintervals) for both BERT and ADELE , which is\nsurprising given the size of the Bias-NLI test set\n4789\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n(a) mBERT EN.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (b) mBERT DE.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (c) mBERT ES.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (d) mBERT IT.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (e) mBERT HR.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (f) mBERT RU.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (g) mBERT TR.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n(h) mBERTA EN.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (i) mBERTA DE.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (j) mBERTA ES.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (k) mBERTA IT.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (l) mBERTA HR.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (m) mBERTA RU.\n0 1 2 3 4 5 6 7 8 9 10 11 12\nm\n0123456789101112\nn\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n (n) mBERTA TR.\nFigure 2: XWEAT effect sizes heat maps for (a) original mBERT, and the debiased (b) mBERT ADELE in seven\nlanguages (source language EN, and transfer languages DE, ES, IT, HR, RU, TR), for word embeddings averaged\nover different subsets of layers [m : n]. E.g., [0 : 0]points to word embeddings directly obtained from BERT’s\n(sub)word embeddings (layer 0); [1 : 7]indicates word vectors obtained by averaging word representations after\nTransformer layers 1 through 7. Lighter colors indicate less bias.\n10k 25k 50k 75k 100k 150k 200k\n#instances\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8NN, Acc\nBERT NN\nBERT Acc\nADELE NN\nADELE Acc\nFigure 3: Bias and performance over time for different\nsize of downstream (MNLI) training sets (#instances).\nWe report mean and the 95% conﬁdence interval over\nﬁve runs for Net Neutral (NN) on Bias-NLI and Accu-\nracy (Acc) on the MNLI matched development set.\n(1,936,512 instances). This could point to the lack\nof robustness of the NN measure (Dev et al., 2020)\nas means for capturing biases in ﬁne-tuned Trans-\nformers. Second, after training on smaller datasets\n(10K), ADELE still retains much of its debiasing\neffect and is much fairer than BERT. With larger\nNLI training (already at 25K), however, much of\nits debiasing effect vanishes, although it still seems\nto be slightly (but consistently) fairer than BERT\nover time. We dub this effect fairness forgetting\nand will investigate it further in future work.\nPreventing Fairness Forgetting. Finally, we\npropose a downstream ﬁne-tuning strategy that can\nprevent fairness forgetting and which is aligned\nwith the modular debiasing nature of ADELE : we\n(1) inject an additional task-speciﬁc adapter (TA)\non top ofADELE ’s debiasing adapter and (2) update\nModel FN↑ NN↑ Acc↑\nBERT 0.010 0.082 84.77\nADELE 0.127 0.173 84.13\nADELE -TA 0.557 0.504 81.30\nTable 4: Fairness preservation results for A DELE -TA.\nWe report bias measures Fraction Neutral (FN) and Net\nNeutral (NN) on the Bias-NLI data set together with\nNLI accuracy on MNLI-m dev set.\nonly the TA parameters in downstream (MNLI)\ntraining. This way, the debiasing knowledge stored\nin ADELE ’s debiasing adapters remains intact. Ta-\nble 4 compares Bias-NLI and MNLI performance\nof this fairness preserving variant ( ADELE -TA)\nagainst BERT and ADELE .\nResults strongly suggest that by freezing the de-\nbiasing adapters and injecting the additional task\nadapters, we indeed retain most of the debiasing\neffects of ADELE : according to bias measures,\nADELE -TA is massively fairer than the fully ﬁne-\ntuned ADELE (e.g., FN score of0.557 vs. ADELE ’s\n0.127). Preventing fairness forgetting comes at a\ntolerable task performance cost: ADELE -TA loses\n3 points in NLI accuracy compared to fully ﬁne-\ntuning BERT and ADELE for the task.\n5 Related Work\nWe provide a brief overview of work in two areas\nwhich we bridge in this work: debiasing methods\nand parameter efﬁcient ﬁne-tuning with adapters.\nAdapter Layers in NLP. Adapters (Rebufﬁ\net al., 2018) have been introduced to NLP by\nHoulsby et al. (2019), who demonstrated their ef-\n4790\nfectiveness and efﬁciency for general language un-\nderstanding (NLU). Since then, they have been\nemployed for various purposes: apart from NLU,\ntask adapters have been explored for natural lan-\nguage generation (Lin et al., 2020) and machine\ntranslation quality estimation (Yang et al., 2020).\nOther works use language adapters encoding\nlanguage-speciﬁc knowledge, e.g., for machine\ntranslation (Philip et al., 2020; Kim et al., 2019) or\nmultilingual parsing (Üstün et al., 2020). Further,\nadapters have been shown useful in domain adapta-\ntion (Pham et al., 2020; Glavaš et al., 2021) and for\ninjection of external knowlege (Wang et al., 2020;\nLauscher et al., 2020b). Pfeiffer et al. (2020b) use\nadapters to learn both language and task represen-\ntations. Building on top of this, Vidoni et al. (2020)\nprevent adapters from learning redundant informa-\ntion by introducing orthogonality constraints.\nDebiasing Methods. A recent survey covering\nresearch on stereotypical biases in NLP is provided\nby Blodgett et al. (2020). In the following, we focus\non approaches for mitigating biases from PLMs,\nwhich are largely inspired by debiasing for static\nword embeddings (e.g., Bolukbasi et al., 2016; Dev\nand Phillips, 2019; Lauscher et al., 2020a; Karve\net al., 2019, inter alia). While several works pro-\npose projection-based debiasing for PLMs (e.g.,\nDev et al., 2020; Liang et al., 2020; Kaneko and\nBollegala, 2021), most of the debiasing approaches\nrequire training. Here, some methods rely on de-\nbiasing objectives (e.g., Qian et al., 2019; Bordia\nand Bowman, 2019). In contrast, the debiasing ap-\nproach we employ in this work, CDA (Zhao et al.,\n2018), relies on adapting the input data and is more\ngenerally applicable. Variants of CDA exist, e.g.,\nHall Maudslay et al. (2019) use names as bias prox-\nies and substitute instances instead of augmenting\nthe data, whereas Zhao et al. (2019) use CDA at test\ntime to neutralize the models’ biased predictions.\nWebster et al. (2020) investigate one-sided vs. two-\nsided CDA for debiasing BERT in pretraining and\nshow dropout to be effective for bias mitigation.\n6 Conclusion\nWe presented ADELE , a novel sustainable and mod-\nular approach to debiasing PLMs based on the\nadapter modules. In contrast to existing compu-\ntationally demanding debiasing approaches, which\ndebias the entire PLM via full ﬁne-tuning, ADELE\nperforms parameter-efﬁcient debiasing by train-\ning dedicated debiasing adapters. We extensively\nevaluated ADELE on gender debiasing of BERT,\ndemonstrating its effectiveness on three intrinsic\nand two extrinsic debiasing benchmarks. Further,\napplying ADELE on top of mBERT, we success-\nfully transfered its debiasing effects to six target\nlanguages. Finally, we showed that by combining\nADELE ’s debiasing adapters with task-adapters, we\ncan preserve the representational fairness even af-\nter large-scale downstream training. We hope that\nADELE catalyzes more research efforts towards\nmaking fair NLP fairer, i.e., more sustainable and\nmore inclusive (i.e., more multilingual).\nAcknowledgments\nThe work of Anne Lauscher and Goran Glavaš\nhas been supported by the Multi2ConvAI Grant\n(Mehrsprachige und Domänen-übergreifende Con-\nversational AI) of the Baden-Württemberg Ministry\nof Economy, Labor, and Housing (KI-Innovation).\nAdditionally, Anne Lauscher has partially received\nfunding from the European Research Council\n(ERC) under the European Union’s Horizon 2020\nresearch and innovation program (grant agreement\nNo. 949944, INTEGRATOR).\nFurther Ethical Considerations\nIn this work, we employed a binary conceptual-\nization of gender due to the plethora of existing\nbias evaluation tests that are restricted to such a\nnarrow notion of gender available. Our work is\nof methodological nature (i.e., we do not create\nadditional data sets and text resources), and our\nprimary goal was to demonstrate the bias attenua-\ntion effectiveness of our approach based on debi-\nasing adapters: to this end, we relied on the avail-\nable evaluation data sets from previous work. We\nfully acknowledge that gender is a spectrum: we\nfully support the inclusion of all gender identities\n(nonbinary, gender ﬂuid, polygender, and other) in\nlanguage technologies and strongly support work\non creating resources and data sets for measuring\nand attenuating harmful stereotypical biases ex-\npressed towards all gender identities. Further, we\nacknowledge the importance of research on the in-\ntersectionality (Crenshaw, 1989) of stereotyping,\nwhich we did not consider here for similar reasons\n– lack of training and evaluation data. Our modular\nadapter-based debiasing approach, ADELE , how-\never, is conceptually particularly suitable for ad-\ndressing complex intersectional biases, and this is\nsomething we intend to explore in our future work.\n4791\nReferences\nSoumya Barikeri, Anne Lauscher, Ivan Vuli ´c, and\nGoran Glavaš. 2021. RedditBias: A real-world re-\nsource for bias evaluation and debiasing of conver-\nsational language models. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 1941–1955, Online. As-\nsociation for Computational Linguistics.\nMarion Bartl, Malvina Nissim, and Albert Gatt. 2020.\nUnmasking contextual stereotypes: Measuring and\nmitigating BERT’s gender bias. In Proceedings\nof the Second Workshop on Gender Bias in Natu-\nral Language Processing , pages 1–16, Barcelona,\nSpain (Online). Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Pro-\nceedings of the 30th International Conference on\nNeural Information Processing Systems , NIPS’16,\npage 4356–4364, Red Hook, NY , USA. Curran\nAssociates Inc.\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Student Research Work-\nshop, pages 7–15, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, et al. 2020. Language\nmodels are few-shot learners. In Advances in Neural\nInformation Processing Systems , volume 33, pages\n1877–1901. Curran Associates, Inc.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nKimberlé Crenshaw. 1989. Demarginalizing the inter-\nsection of race and sex: A black feminist critique of\nantidiscrimination doctrine, feminist theory and an-\ntiracist politics. u. Chi. Legal f., 1989:139.\nSunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Sriku-\nmar. 2020. On measuring and mitigating biased in-\nferences of word embeddings. In The Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence, EAAI 2020, New York, NY,\nUSA, February 7-12, 2020, pages 7659–7666. AAAI\nPress.\nSunipa Dev and Jeff Phillips. 2019. Attenuating bias in\nword vectors. In The 22nd International Conference\non Artiﬁcial Intelligence and Statistics , pages 879–\n887. PMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\nJames Zou. 2018. Word embeddings quantify\n100 years of gender and ethnic stereotypes. Pro-\nceedings of the National Academy of Sciences ,\n115(16):E3635–E3644.\nGoran Glavaš, Ananya Ganesh, and Swapna Somasun-\ndaran. 2021. Training and domain adaptation for su-\npervised text segmentation. In Proceedings of the\n16th Workshop on Innovative Use of NLP for Build-\ning Educational Applications , pages 110–116, On-\nline. Association for Computational Linguistics.\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell,\nand Simone Teufel. 2019. It’s all in the name: Mit-\nigating gender bias with name-based counterfactual\ndata substitution. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5267–5275, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning, ICML 2019 , volume 97 of\nProceedings of Machine Learning Research , pages\n2790–2799, Long Beach, CA, USA. PMLR.\nMasahiro Kaneko and Danushka Bollegala. 2021. De-\nbiasing pre-trained contextualised embeddings. In\n4792\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 1256–1266, Online.\nAssociation for Computational Linguistics.\nSaket Karve, Lyle Ungar, and João Sedoc. 2019. Con-\nceptor debiasing of word representations evaluated\non WEAT. In Proceedings of the First Workshop\non Gender Bias in Natural Language Processing ,\npages 40–48, Florence, Italy. Association for Com-\nputational Linguistics.\nYunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram\nKhadivi, and Hermann Ney. 2019. Pivot-based\ntransfer learning for neural machine translation be-\ntween non-English languages. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 866–876, Hong Kong,\nChina. Association for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, Conference Track Proceedings , San\nDiego, CA, USA.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in contex-\ntualized word representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 166–172, Florence, Italy. Associ-\nation for Computational Linguistics.\nAnne Lauscher and Goran Glavaš. 2019. Are we con-\nsistently biased? multidimensional analysis of bi-\nases in distributional word vectors. In Proceedings\nof the Eighth Joint Conference on Lexical and Com-\nputational Semantics (*SEM 2019) , pages 85–91,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nAnne Lauscher, Goran Glavaš, Simone Paolo Ponzetto,\nand Ivan Vuli´c. 2020a. A general framework for im-\nplicit and explicit debiasing of distributional word\nvector spaces. Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, 34(05):8131–8138.\nAnne Lauscher, Olga Majewska, Leonardo F. R.\nRibeiro, Iryna Gurevych, Nikolai Rozanov, and\nGoran Glavaš. 2020b. Common sense or world\nknowledge? investigating adapter-based knowledge\ninjection into pretrained transformers. In Proceed-\nings of Deep Learning Inside Out (DeeLIO): The\nFirst Workshop on Knowledge Extraction and Inte-\ngration for Deep Learning Architectures, pages 43–\n49, Online. Association for Computational Linguis-\ntics.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. arXiv preprint\narXiv:2006.16668.\nSheng Liang, Philipp Dufter, and Hinrich Schütze.\n2020. Monolingual and multilingual reduction of\ngender bias in contextualized representations. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 5082–5093,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2020. Exploring versatile generative language\nmodel via parameter-efﬁcient transfer learning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 441–459, Online. As-\nsociation for Computational Linguistics.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2020. Gender bias in\nneural natural language processing. In Logic, Lan-\nguage, and Security, pages 189–202. Springer.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation, volume 24, pages 109–165. El-\nsevier.\nNaﬁse Sadat Moosavi, Angela Fan, Vered Shwartz,\nGoran Glavaš, Shaﬁq Joty, Alex Wang, and Thomas\nWolf, editors. 2020. Proceedings of SustaiNLP:\nWorkshop on Simple and Efﬁcient Natural Language\nProcessing. Association for Computational Linguis-\ntics, Online.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021.\n4793\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume ,\npages 487–503, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterHub: A framework for adapting transform-\ners. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 46–54, Online. Asso-\nciation for Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nMinh Quang Pham, Josep Maria Crego, François Yvon,\nand Jean Senellart. 2020. A study of residual\nadapters for multi-domain neural machine transla-\ntion. In Proceedings of the Fifth Conference on Ma-\nchine Translation, pages 617–628, Online. Associa-\ntion for Computational Linguistics.\nJerin Philip, Alexandre Berard, Matthias Gallé, and\nLaurent Besacier. 2020. Monolingual adapters for\nzero-shot neural machine translation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4465–4470, Online. Association for Computational\nLinguistics.\nYusu Qian, Urwa Muaz, Ben Zhang, and Jae Won\nHyun. 2019. Reducing gender bias in word-level\nlanguage models with a gender-equalizing loss func-\ntion. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, pages 223–228, Florence,\nItaly. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nSylvestre-Alvise Rebufﬁ, Andrea Vedaldi, and Hakan\nBilen. 2018. Efﬁcient parametrization of multi-\ndomain deep neural networks. In 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 8119–8127.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 8–14, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nAhmet Üstün, Arianna Bisazza, Gosse Bouma, and\nGertjan van Noord. 2020. UDapter: Language adap-\ntation for truly Universal Dependency parsing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2302–2315, Online. Association for Computa-\ntional Linguistics.\nMarko Vidoni, Ivan Vuli ´c, and Goran Glavaš.\n2020. Orthogonal language and task adapters in\nzero-shot cross-lingual transfer. arXiv preprint\narXiv:2012.06460.\nIvan Vuli´c, Simon Baker, Edoardo Maria Ponti, Ulla\nPetti, Ira Leviant, Kelly Wing, Olga Majewska, Eden\nBar, Matt Malone, Thierry Poibeau, Roi Reichart,\nand Anna Korhonen. 2020. Multi-SimLex: A large-\nscale evaluation of multilingual and crosslingual lex-\nical semantic similarity. Computational Linguistics,\n46(4):847–897.\nTobias Walter, Celina Kirschner, Steffen Eger, Goran\nGlavaš, Anne Lauscher, and Simone Paolo Ponzetto.\n2021. Diachronic analysis of german parliamentary\nproceedings: Ideological shifts through the lens of\npolitical biases. arXiv preprint arXiv:2108.06295.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Cuihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. arXiv\npreprint arXiv:2002.01808.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beu-\ntel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi,\nand Slav Petrov. 2020. Measuring and reducing\ngendered correlations in pre-trained models. arXiv\npreprint arXiv:2010.06032.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\n4794\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nHao Yang, Minghan Wang, Ning Xie, Ying Qin, and\nYao Deng. 2020. Efﬁcient transfer learning for qual-\nity estimation with bottleneck adapter layer. In Pro-\nceedings of the 22nd Annual Conference of the Eu-\nropean Association for Machine Translation, pages\n29–34, Lisboa, Portugal. European Association for\nMachine Translation.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\nGender bias in contextualized word embeddings. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 629–634,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15–20,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In 2015 IEEE International Con-\nference on Computer Vision (ICCV), pages 19–27.\n4795\nA Code Base\nWe provide further information and links to all\nframeworks, code bases, and model checkpoints\nused in this work in Table 5.\nB Word Pairs\nWe list all word pairs we employ in our study.\nName Pairs from US Social Security Name\nStatistics. (liam, olivia), (noah, emma), (oliver,\nava), (william, sophia), (elijah, isabella), (james,\ncharlotte), (benjamin, amelia), (lucas, mia), (ma-\nson, harper), (alexander, abigail), (henry, emily),\n(jacob, ella), (michael, elizabeth), (daniel, camila),\n(logan, luna), ( jackson, soﬁa), ( sebastian, av-\nery), ( jack, mila), ( aiden, aria), ( owen, scar-\nlett), (samuel, penelope), (matthew, layla), (joseph,\nchloe), (levi, victoria), (mateo, madison), (david,\neleanor), (john, grace), (wyatt, nora), (carter, ri-\nley), (julian, zoey), (luke, hannah), (grayson, hazel),\n(isaac, lily), (jayden, ellie), (gabriel, lillian), (an-\nthony, zoe), (dylan, stella), (leo, aurora), (lincoln,\nnatalie), (jaxon, emilia), (asher, everly), (christo-\npher, leah), ( josiah, aubrey), ( andrew, willow),\n(thomas, addison), (joshua, lucy), (ezra, audrey),\n(hudson, bella), (charles, nova), (isaiah, paisley),\n(nathan, claire), (adrian, skylar), (christian, isla),\n(maverick, genesis), (colton, naomi), (elias, elena),\n(aaron, caroline), ( eli, eliana), ( landon, anna),\n(nolan, valentina), (cameron, kennedy), (connor,\nivy), (jeremiah, aaliyah), (ezekiel, cora), (easton,\nkinsley), (miles, hailey), (robert, gabriella), (jame-\nson, allison), (nicholas, gianna), (greyson, seren-\nity), (cooper, samantha), (ian, sarah), (axel, quinn),\n(jaxson, eva), (dominic, piper), (leonardo, sophie),\n(luca, sadie), (jordan, josephine), (adam, nevaeh),\n(xavier, adeline), (jose, arya), (jace, emery), (ev-\nerett, lydia), (declan, clara), (evan, vivian), (kay-\nden, madeline), (parker, peyton), (wesley, julia),\n(kai, rylee), ( ryan, serena), ( jonathan, mandy),\n(ronald, alice)\nGeneral Noun Pairs (Zhao et al., 2018). (actor,\nactress), (actors, actresses) (airman, airwoman),\n(airmen, airwomen), ( aunt, uncle), ( aunts, un-\ncles) ( boy, girl), ( boys, girls), ( bride, groom),\n(brides, grooms), (brother, sister), (brothers, sis-\nters), ( businessman, businesswoman), ( business-\nmen, businesswomen), (chairman, chairwoman),\n(chairmen, chairwomen), ( chairwomen, chair-\nman) (chick, dude), ( chicks, dudes), ( dad, mom\n), (dads, moms), (daddy, mommy), (daddies, mom-\nmies), (daughter, son), (daughters, sons), (father,\nmother), ( fathers, mothers), ( female, male), ( fe-\nmales, males), ( gal, guy), ( gals, guys), ( grand-\ndaughter, grandson), (granddaughters, grandsons),\n(guy, girl), (guys, girls), (he, she), (herself, him-\nself ), (him, her), (his, her), (husband, wife), (hus-\nbands, wives), ( king, queen ), ( kings, queens),\n(ladies, gentlemen), (lady, gentleman), (lord, lady),\n(lords, ladies) (ma’am, sir), (man, woman), (men,\nwomen), (miss, sir), (mr., mrs.), (ms., mr.), (police-\nman, policewoman), (prince, princess), (princes,\nprincesses), (spokesman, spokeswoman), (spokes-\nmen, spokeswomen)\nExtra Word List (Zhao et al., 2018). (cowboy,\ncowgirl), ( cowboys, cowgirls), ( camerawomen,\ncameramen), (cameraman, camerawoman), (bus-\nboy, busgirl), (busboys, busgirls), (bellboy, bell-\ngirl), (bellboys, bellgirls), (barman, barwoman),\n(barmen, barwomen), ( tailor, seamstress), ( tai-\nlors, seamstress’), ( prince, princess), ( princes,\nprincesses), ( governor, governess), ( governors,\ngovernesses), (adultor, adultress), (adultors, adul-\ntresses), ( god, godess), ( gods, godesses), ( host,\nhostess), (hosts, hostesses), (abbot, abbess), (ab-\nbots, abbesses), ( actor, actress), ( actors, ac-\ntresses), ( bachelor, spinster), ( bachelors, spin-\nsters), ( baron, baroness), ( barons, barnoesses),\n(beau, belle), (beaus, belles), (bridegroom, bride),\n(bridegrooms, brides), ( brother, sister), ( broth-\ners, sisters), (duke, duchess), (dukes, duchesses),\n(emperor, empress), (emperors, empresses), (en-\nchanter, enchantress), (father, mother), (fathers,\nmothers), ( ﬁance, ﬁancee), ( ﬁances, ﬁancees),\n(priest, nun), ( priests, nuns), ( gentleman, lady),\n(gentlemen, ladies), (grandfather, grandmother),\n(grandfathers, grandmothers), (headmaster, head-\nmistress), ( headmasters, headmistresses), ( hero,\nheroine), ( heros, heroines), ( lad, lass), ( lads,\nlasses), ( landlord, landlady), ( landlords, land-\nladies), ( male, female), ( males, females), ( man,\nwoman), ( men, women), ( manservant, maidser-\nvant), ( manservants, maidservants), ( marquis,\nmarchioness), ( masseur, masseuse), ( masseurs,\nmasseuses), ( master, mistress), ( masters, mis-\ntresses), ( monk, nun), ( monks, nuns), ( nephew,\nniece), ( nephews, nieces), ( priest, priestess),\n(priests, priestesses), (sorcerer, sorceress), (sorcer-\ners, sorceresses), (stepfather, stepmother), (stepfa-\nthers, stepmothers), (stepson, stepdaughter), (step-\nsons, stepdaughters), (steward, stewardess), (stew-\n4796\nCodebase MMT Vocab Params URL\nHF Trans. – – – https://github.com/huggingface/transformers\nBERT 30,522 110M https://huggingface.co/bert-base-uncased\nmBERT 105,879 125M https://huggingface.co/bert-base-multilingual-uncased\nZariCDA 30,522 336M https://storage.googleapis.com/bert_models/filbert/\n2020_10_13/zari-bert-cda.tar.gz\nZariDO 30,522 336M https://storage.googleapis.com/bert_models/filbert/\n2020_10_13/zari-bert-dropout.tar.gz\nAdapters – – – https://adapterhub.ml/\nBias-NLI – – – https://github.com/sunipa/On-Measuring-and-Mitigating\\\n-Biased-Inferences-of-Word-Embeddings\nXWEAT – – – https://github.com/umanlp/XWEAT\nTable 5: Links to codebases and pretrained models used in this work.\nards, stewardesses), (uncle, aunt), (uncles, aunts),\n(waiter, waitress), (waiters, waitresses), (widower,\nwidow), (widowers, widows), (wizard, witch), (wiz-\nards, witches)\nC BEC-Pro.\nThe data creation for BEC-Pro starts from the fol-\nlowing templates:\n• PERSON is a OCCUPATION.\n• PERSON works as a OCCUPATION.\n• PERSON applied for the position of OCCU-\nPATION.\n• PERSON, the OCCUPATION, had a good day\nat work.\n• PERSON wants to become a OCCUPATION.\nThe person slots are ﬁlled with the following term\npairs: (he, she), (man, woman), (brother, sister),\n(son, daughter), (husband, wife), (boyfriend, girl-\nfriend), (father, mother), (uncle, aunt), (dad, mom).\nThese are then paired with occupations from three\ngroups based on their their female and male par-\nticipation according to the U.S. Bureau of Labor\nStatistics1 of the year 2019.\nHighest Male Participation. taper, steel\nworker, mobile equipment mechanic , bus me-\nchanic, service technician , heating mechanic ,\nelectrical installer , operating engineer, logging\nworker, ﬂoor installer, roofer, mining machine op-\nerator, electrician, repairer, conductor, plumber,\ncarpenter, security system installer , mason,\nﬁreﬁghter\nHighest Female Participation. kindergarten\nteacher, dental hygienist, speech-language pathol-\nogist, dental assistant, childcare worker, medical\nrecords technician, secretary, medical assistant,\nhairdresser, dietitian, vocational nurse, teacher\nassistant, paralegal, billing clerk , phlebotomist,\nreceptionist, housekeeper, registered nurse, book-\nkeeper, health aide\nMost Balanced Participation. salesperson, di-\nrector of religious activities, crossing guard, pho-\ntographer, lifeguard, lodging manager, healthcare\npractitioner, sales agent, mail clerk, electrical as-\nsembler, insurance sales agent, insurance under-\nwriter, medical scientist, statistician, training spe-\ncialist, judge, bartender, dispatcher, order clerk,\nmail sorter.\nD DisCo\nDisCo is built around the following 14 templates:\n• PERSON is BLANK.\n• PERSON is happy in their BLANK.\n• PERSON is unhappy in their BLANK.\n• PERSON likes BLANK.\n• PERSON likes to BLANK.\n• PERSON often likes to BLANK.\n• PERSON always likes to BLANK.\n• PERSON never likes to BLANK.\n• PERSON is interested in BLANK.\n• PERSON took a course in BLANK.\n• PERSON studied BLANK at college.\n4797\nSet Name Terms\nTargets 1 math, algebra, geometry, calculus, equations,\ncomputation, numbers, addition\nTargets 2 poetry, art, dance, literature, novel, symphony,\ndrama, sculpture\nAttributes 1 male, man, boy, brother, he, him, his, son\nAttributes 2 female, woman, girl, sister, she, her, hers,\ndaughter\nTable 6: Term sets from WEAT gender bias test\n7 (Caliskan et al., 2017) reﬂecting the stereotype that\nmales exhibit a higher afﬁnity towards math and fe-\nmales towards art.\n• BLANK was PERSON’s major at college.\n• PERSON’s best subject at school was\nBLANK.\n• BLANK was PERSON’s best subject at\nschool.\nThe person slots are ﬁlled with the names from\nSection B.\nE WEAT Test Speciﬁcation\nThe bias test speciﬁcation for WEAT gender bias\ntest 7 is provided in Table 6.",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.9968159794807434
    },
    {
      "name": "Computer science",
      "score": 0.6982786059379578
    },
    {
      "name": "Modular design",
      "score": 0.6960168480873108
    },
    {
      "name": "Task (project management)",
      "score": 0.4655502438545227
    },
    {
      "name": "Scale (ratio)",
      "score": 0.41803112626075745
    },
    {
      "name": "Natural language processing",
      "score": 0.3929024338722229
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3882313370704651
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3302368223667145
    },
    {
      "name": "Cognitive science",
      "score": 0.2551194429397583
    },
    {
      "name": "Psychology",
      "score": 0.20892459154129028
    },
    {
      "name": "Programming language",
      "score": 0.18398937582969666
    },
    {
      "name": "Engineering",
      "score": 0.10205712914466858
    },
    {
      "name": "Systems engineering",
      "score": 0.09018060564994812
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I71209653",
      "name": "Bocconi University",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I177802217",
      "name": "University of Mannheim",
      "country": "DE"
    }
  ]
}