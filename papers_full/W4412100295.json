{
  "title": "Application of artificial intelligence large language models in drug target discovery",
  "url": "https://openalex.org/W4412100295",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2115338485",
      "name": "Xinyu Liu",
      "affiliations": [
        "Shandong Provincial Hospital",
        "Shandong First Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2108184173",
      "name": "Jia Zhang",
      "affiliations": [
        "Shandong First Medical University",
        "Shandong Provincial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2108984889",
      "name": "Xiaoran Wang",
      "affiliations": [
        "Shandong Provincial Hospital",
        "Shandong First Medical University"
      ]
    },
    {
      "id": null,
      "name": "Maoda Teng",
      "affiliations": [
        "Linyi People's Hospital",
        "Linzhou Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1995242104",
      "name": "Guoying Wang",
      "affiliations": [
        "Dongyang People's Hospital",
        "Wenzhou Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2055201436",
      "name": "Xiaoming Zhou",
      "affiliations": [
        "Wenzhou Medical University",
        "Dongyang People's Hospital",
        "Shandong Provincial Hospital",
        "Shandong First Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2115338485",
      "name": "Xinyu Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108184173",
      "name": "Jia Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108984889",
      "name": "Xiaoran Wang",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Maoda Teng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1995242104",
      "name": "Guoying Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2055201436",
      "name": "Xiaoming Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4396721167",
    "https://openalex.org/W4213095938",
    "https://openalex.org/W3203588026",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W4312129273",
    "https://openalex.org/W4394763992",
    "https://openalex.org/W3166730506",
    "https://openalex.org/W4392168151",
    "https://openalex.org/W2900674118",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4319335178",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W4391610180",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4399387478",
    "https://openalex.org/W3038922487",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4378174056",
    "https://openalex.org/W4404665097",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4280625391",
    "https://openalex.org/W4206092375",
    "https://openalex.org/W4221054384",
    "https://openalex.org/W3214112664",
    "https://openalex.org/W4322761411",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W6860347561",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4321452549",
    "https://openalex.org/W4291014905",
    "https://openalex.org/W3081871609",
    "https://openalex.org/W4388685549",
    "https://openalex.org/W4404349982",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W4390510841",
    "https://openalex.org/W4385572540",
    "https://openalex.org/W4384831976",
    "https://openalex.org/W4313880353",
    "https://openalex.org/W4366998330",
    "https://openalex.org/W4403118713",
    "https://openalex.org/W4366823237",
    "https://openalex.org/W4404595545",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W4210371902",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4210421570",
    "https://openalex.org/W4280504434",
    "https://openalex.org/W4400667915",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W4280594315",
    "https://openalex.org/W4403174218",
    "https://openalex.org/W3204213738",
    "https://openalex.org/W4385230279",
    "https://openalex.org/W3195980265",
    "https://openalex.org/W4403593408",
    "https://openalex.org/W4388584897",
    "https://openalex.org/W3210331473",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W4390783938"
  ],
  "abstract": "Drug target discovery is a fundamental aspect of contemporary drug research and development. However, the use of conventional biochemical screening, omics analysis, and related approaches is constrained by substantial technical complexity and significant resource requirements. With the advancement of artificial intelligence-based large language models, notable progress has been achieved in drug target identification. During target mining, large language models with natural language comprehension capabilities can efficiently integrate literature data resources and systematically analyze disease-associated biological pathways and potential targets. Notably, models specifically designed for biomolecular “language” have demonstrated advantages across multiple aspects. The genomics-focused large language model has significantly enhanced the accuracy of pathogenic gene variant identification and gene expression prediction. In transcriptomics, large language models enable comprehensive reconstruction of gene regulatory networks. In proteomics, advancements have been made in protein structure analysis, function prediction, and interaction inference. Additionally, the single-cell multi-omics large language model facilitates data integration across different omics technologies. These technological advancements provide multi-dimensional biological evidence supporting drug target discovery and contribute to a more efficient screening process for candidate targets. The development of these models is generally based on deep neural networks of Transformer architecture, and powerful representation capabilities are obtained through large-scale unsupervised pre-training (such as mask language modeling, autoregressive prediction) combined with task-specific supervised fine-tuning. This review systematically examines recent advancements in the application of large language models in drug target discovery, emphasizing existing technical challenges and potential future research directions.",
  "full_text": "Application of artiﬁcial\nintelligence large language\nmodels in drug target discovery\nXinyu Liu1, Jia Zhang1, Xiaoran Wang1, Maoda Teng2,\nGuoying Wang3* and Xiaoming Zhou4,5*\n1Shandong Provincial Hospital Afﬁliated to Shandong First Medical University, Jinan, China,2Linyi Central\nHospital, Dezhou, China,3The Second People’s Hospital of Dongying, Dongying, China,4Department of\nResearch, Shandong Provincial Hospital Afﬁliated to Shandong First Medical University, Jinan Shandong,\nChina, 5Department of Pharmacy, Dongying People’s Hospital, Dongying, Shandong, China\nDrug target discovery is a fundamental aspect of contemporary drug research\nand development. However, the use of conventional biochemical screening,\nomics analysis, and related approaches is constrained by substantial technical\ncomplexity and signiﬁcant resource requirements. With the advancement of\nartiﬁcial intelligence-based large language models, notable progress has been\nachieved in drug target identiﬁcation. During target mining, large language\nmodels with natural language comprehension capabilities can ef ﬁciently\nintegrate literature data resources and systematically analyze disease-\nassociated biological pathways and potential targets. Notably, models\nspeciﬁcally designed for biomolecular “language” have demonstrated\nadvantages across multiple aspects. The genomics-focused large language\nmodel has signiﬁcantly enhanced the accuracy of pathogenic gene variant\nidentiﬁcation and gene expression prediction. In transcriptomics, large\nlanguage models enable comprehensive reconstruction of gene regulatory\nnetworks. In proteomics, advancements have been made in protein structure\nanalysis, function prediction, and interaction inference. Additionally, the single-\ncell multi-omics large language model facilitates data integration across different\nomics technologies. These technological advancements provide multi-\ndimensional biological evidence supporting drug target discovery and\ncontribute to a more efﬁcient screening process for candidate targets. The\ndevelopment of these models is generally based on deep neural networks of\nTransformer architecture, and powerful representation capabilities are obtained\nthrough large-scale unsupervised pre-training (such as mask language modeling,\nautoregressive prediction) combined with task-speciﬁc supervised ﬁne-tuning.\nThis review systematically examines recent advancements in the application of\nlarge language models in drug target discovery, emphasizing existing technical\nchallenges and potential future research directions.\nKEYWORDS\nlarge language model, drug target discovery, bioinformatics, multi-omics integration,\nprotein structure prediction\nOPEN ACCESS\nEDITED BY\nJohn M. Seubert,\nUniversity of Alberta, Canada\nREVIEWED BY\nEva Torres Sangiao,\nComplejo Hospitalario Universitario de\nSantiago, Spain\nVineet Mahajan,\nUniversity of Texas Medical Branch at\nGalveston, United States\nKrzysztof Brzezinski,\nPolish Academy of Sciences, Poland\n*CORRESPONDENCE\nGuoying Wang,\nicuwangguoying@163.com\nXiaoming Zhou,\nsdslyy@yeah.net\nRECEIVED 21 March 2025\nACCEPTED 27 June 2025\nPUBLISHED 08 July 2025\nCITATION\nLiu X, Zhang J, Wang X, Teng M, Wang G and\nZhou X (2025) Application of artiﬁcial\nintelligence large language models in drug\ntarget discovery.\nFront. Pharmacol. 16:1597351.\ndoi: 10.3389/fphar.2025.1597351\nCOPYRIGHT\n© 2025 Liu, Zhang, Wang, Teng, Wang and\nZhou. This is an open-access article distributed\nunder the terms of theCreative Commons\nAttribution License (CC BY). The use,\ndistribution or reproduction in other forums is\npermitted, provided the original author(s) and\nthe copyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nFrontiers inPharmacology frontiersin.org01\nTYPE Review\nPUBLISHED 08 July 2025\nDOI 10.3389/fphar.2025.1597351\n1 Introduction\nDrug development is characterized by an extended timeline,\nsubstantial costs, and considerable risk. From initial research toﬁnal\nproduct approval, the process typically spans nearly a decade and\nrequires an investment exceeding two billion US dollars (Hinkson et al.,\n2020). This progression encompasses target identiﬁcation, candidate\ncompound screening and optimization, preclinical evaluation, clinical\ntrials, and commercial application. Each stage demands extensive\nresources, exhibits a low suc cess rate, and presents signi ﬁcant\nindustry challenges. As a fund amental stage in research and\ndevelopment, drug target identiﬁcation plays a decisive role in\nproject success. The primary objective of this process is to identify\nbiological molecules or cellular pathways that serve as key regulators in\ndisease pathogenesis and determine therapeutic intervention points,\nwhich include biological macromolecules such as gene loci and\nmembrane receptors. The identiﬁcation of innovative drug targets\nforms the basis of the modern drug research and development\nsystem, enhancing treatment precision and minimizing adverse effects.\nDrug target discovery faces signi ﬁcant challenges due to\ntechnical complexities, high resource demands, and intricate\ndisease mechanisms. As of 2022, the number of empirically\nvalidated drug targets worldwide remained below 500 ( Zhou\net al., 2022 ), highlighting the urgent need to enhance target\ndiscovery efﬁciency, with technological innovation being a critical\nfactor. Currently, mainstream technical strategies include\nexperimental-based approaches, multi-omics integrated analysis,\nand computer-aided prediction methods ( Pun et al., 2023 ).\nExperimental-based techniques, such as small molecule af ﬁnity\nprobe labeling, play a crucial role in target validation. Multi-\nomics strategies integrate diverse omics data to identify potential\ntargets; however, these methods require high-quality samples and\nsubstantial resources. For example, CRISPR-based functional\ngenome screens can systematically identify essential genes, but\ntheir large-scale application is limited by experimental\nthroughput and cost. Computer-aided prediction methods offer\npotential for identifying novel molecular targets based on the\nchemical properties of compounds, yet their applicability is\nrestricted by the reliance on three-dimensional protein structural\ninformation. Between 2013 and 2022, the median cost and duration\nof new drug development increased steadily, with median costs\nreaching about 2.4 billion US dollars— about a 20% rise compared to\na decade earlier— and development timelines extending by one to\n2 years. Advancing target discovery and achieving technological\nbreakthroughs are essential for improving research and\ndevelopment efﬁciency.\nArtiﬁcial Intelligence (AI), recognized as a transformative\ntechnology of the 21st century, has achieved signi ﬁcant\nadvancements in computer vision and natural language\nprocessing while also reshaping the entire innovation process in\ndrug research and development (Gangwal et al., 2024). Insilico\nMedicine, an innovative company leveraging AI to accelerate\ndrug discovery, has developed an “end-to-end” AI platform\n(PandaOmics + Chemistry42) that has demonstrated high\nefﬁciency in drug-target identiﬁcation and preclinical candidate\nscreening. For instance, in the case of idiopathic pulmonary\nﬁbrosis, AI platforms facilitated new target discovery, enabled the\nlaunch of theﬁrst AI-generated drug, and advanced it to phase II\nclinical trials within 18 months. Similarly, for HCC treatment,\nPandaOmics identi ﬁed CDK20 as a novel target, and in\ncombination with AlphaFold-predicted structures,\nChemistry42 generated a novel inhibitor, ISM042-2–048 (IC50 =\n33.4 nmol/L), validating the AI platform’s\n“end-to-end” capabilities\n(Ren et al., 2023 ). These practical applications highlight AI ’s\nadvantages in enhancing target screening accuracy, reducing drug\ndevelopment timelines, and optimizing research and development\nefﬁciency, providing an innovative technical pathway for addressing\ncomplex diseases. The emergence of ChatGPT has driven\nwidespread adoption of arti ﬁcial intelligence large language\nmodels (LLMs), characterized by an extremely high number of\nparameters. These models employ deep learning to perform\nlanguage rule modeling, syntactic and semantic parsing, and text\ngeneration in natural language processing by analyzing extensive\ntext datasets. Their underlying technology is based on the\nTransformer architecture, introduced by Vaswani’s team in 2017\n(Vaswani et al., 2017), with the self-attention mechanism as a core\nfeature, dynamically assessing text relevance and capturing long-\nrange dependencies, revolutionizing natural language processing\nand sequence transformation. The integration of large language\nmodels into drug discovery represents a paradigm shift in research\nand development (Zheng et al., 2024). In healthcare, Google’s Med-\nPaLM model set the bar. Med-PaLM was theﬁrst medical LLM to\npass the United States Medical Licensing Examination (USMLE),\ndemonstrating its authority in medical question answering tasks. Its\niterative version Med-PaLM 2 is based on the more powerful PaLM\n2 basic model, and introduces “ensemble optimization ” and\n“retrieval chain” strategies to signiﬁcantly improve the inference\nability. It achieves an accuracy of 86.5% on clinical topic datasets\nsuch as MedQA, which is close to or better than the existing optimal\nlevel, and performs better in adversarial problem processing and\nfactual accuracy. The multilingual support and generation\ncapabilities of Med-PaLM 2, such as report generation, have been\napplied in several scenarios such as clinical decision support, health\neducation, and drug discovery. In drug target discovery, these\nmodels facilitate literature mining and patent data analysis to\nexplore disease-related biological pathways and core targets.\nSpecialized models trained on biomolecular “language” can\nanalyze and predict multi-omics data, such as genomics, to\nenhance candidate target identiﬁcation. Protein language models,\nincluding ESMFold (Lin et al., 2023), overcome traditional structural\nsimilarity analysis limitations by employing 3D structure prediction\ntechnologies. This review systematically examines the innovative\napplications of large language models in drug target discovery,\naddressing current technical bottlenecks and development\nchallenges (Figure 1).\nThis ﬁgure summarizes the application of artiﬁcial intelligence\nbig language models in drug target discovery and their\nrepresentative models, including natural language models and\nlanguage models applied to genomics, transcriptomics,\nproteomics and multi-omics.\nAbbreviations: DOAJ, Directory of open access journals; LD, Linear\ndichroism; MDPI, Multidisciplinary Digital Publishing Institute; TLA, Three\nletter acronym.\nFrontiers inPharmacology frontiersin.org02\nLiu et al. 10.3389/fphar.2025.1597351\n2 Analysis of the application of the\nnatural language model in drug target\ntext information mining\nPre-trained language models based on the Transformer\narchitecture, such as GPT and BERT, have gained signi ﬁcant\nattention in recent years. These models have enabled large-scale\nadvancements in natural language processing due to their strong\nsemantic parsing and text generation capabilities. In biomedical\nresearch, they provide innovative solutions for analyzing disease\npathogenesis and identifying therapeutic targets through techniques\nsuch as literature mining and professional term recognition. The\ngeneralization capability of language models pre-trained on extensive\ntext corpora allows for the effective identiﬁcation of cross-domain\nlinguistic features, while the acquired common language rules\nsigniﬁcantly improve the efﬁciency of downstream tasks (Zheng\net al., 2024). Biology-speciﬁc language models, designed and trained\nspeciﬁcally for the biomedical domain, possess an enhanced ability to\ninterpret the semantics of specialized terminology and accurately\nanalyze complex sentence structures and domain-speciﬁcc o n c e p t s\nwithin biomedical literature. In drug target design, both general-\npurpose and domain-speciﬁc language models offer unique technical\nadvantages and play an indispensable role in advancing research and\ndevelopment.\nFIGURE 1\nApplication of AI big language models in drug target discovery.\nFrontiers inPharmacology frontiersin.org03\nLiu et al. 10.3389/fphar.2025.1597351\n2.1 General purpose natural language model\nGeneral natural language models are trained on extensive text\ndatasets, including scienti ﬁc papers, textbooks, and general\nliterature. The large-scale training data enable these models to\ncomprehend diverse human languages while also developing a\ndeep understanding of scienti ﬁc background knowledge. In\nscientiﬁc applications such as drug target discovery, general\nlanguage models, including GPT-4 ( Microsoft Research\nAI4Science Microsoft Quantum, 2023), DeepSeek, BERT (Devlin\net al., 2019), and Claude, can analyze vast amounts of literature,\nintegrate extracted data into knowledge maps, and reveal internal\nrelationships between genes and diseases, enhancing target\ninterpretability. These models contribute to exploring disease\nmechanisms (Savage, 2023). Their ability to effectively process\nboth complex scientiﬁc language and general knowledge provides\nsigniﬁcant advantages, particularly in terms of broad knowledge\ncoverage and the capability to establish connections across\ndifferent topics.\n2.2 Dedicated natural language models\nGeneral natural language models have contributed\nsigniﬁcantly to biomedical text mining. However, these models\ndirectly transfer the word distribution of natural language from\ngeneral corpora to biomedical corpora, limiting their ability to\nprocess text requiring specialized biomedical knowledge. With\nadvancements in biomedicine, domain-speci ﬁc natural language\nmodels have been developed. Medical corpora such as PubMed\nand PubMed Central (PMC) literature are commonly used in the\npre-training stage of biomedical-speciﬁc language models (Zhou\net al., 2023 ). BERT-derived models, including BioBERT ( Lee\net al., 2020 )a n dP u b M e d B E R T(Gu et al., 2021 ), as well as\nGPT-derived models, such as BioGPT ( Luo et al., 2022 )a n d\nChatPandaGPT, have enhanced accuracy and ef ﬁciency in\nbiomedical natural language processing tasks. BioGPT,\ndeveloped by Microsoft Re search, is based on the GPT\narchitecture and has been optimized through large-scale\nliterature training in the biomedical ﬁeld. It demonstrates\noutstanding performance in understanding professional terms\nand complex conceptual relationships in various multi-task\nscenarios, such as relation extraction, question answering, and\ntext classiﬁcation, signi ﬁcantly outperforming its predecessors.\nIts open-source nature also lowers the barriers for research and\napplication. BioBERT, ﬁne-tuned using data from the Human\nProtein Atlas, performs multiple functions such as biomedical\nnamed entity recognition, relationship extraction, and question\nanswering. Additionally, it ext racts information from scienti ﬁc\nliterature to identify novel drug targets ( Lee et al., 2020 ).\nChatPandaGPT, integrated int o the PandaOmics platform by\nInsilico Medicine, facilitates the review of complex data and\nenables the identi ﬁcation of potential therapeutic targets and\nbiomarkers through natural language interactions. Furthermore,\nGalactica automatically extrac ts molecular interactions and\npathway information from scienti ﬁc literature, improving the\nunderstanding of complex biological processes and aiding in drug\ntarget discovery (Park et al., 2023). DeepSeek also demonstrates\npotential in biomedical text mining by learning from extensive\nbiomedical literature, enabling precise identi ﬁcation of\nspecialized terminology and complex concepts, thereby\nproviding valuable support for drug target identi ﬁcation. It is\nworth noting that building high-performance biomedical-\nspeciﬁc models typically relies on effective domain adaptation\nﬁne-tuning. This is not merely about continuing the pre-training\nof a general model on biomedical corpora, but rather involves\nﬁne-tuning for speci ﬁc downstream tasks such as relation\nextraction and target entity recognition. Common strategies\ninclude parameter-ef ﬁcient ﬁne-tuning techniques like LoRA\n(Low-Rank Adaptation), as well as hierarchical learning rate\nadjustments, to ef ﬁciently inject domain-speci ﬁc knowledge\nwhile retaining general language knowledge and optimizing\ntask performance. This targeted ﬁne-tuning is a key technical\napproach to overcoming domain shift and improving precision\nand recall in drug target text mining tasks.\nWith the capability to parse natural semantics and interpret\ncomplex scientiﬁc concepts, natural language models serve as a\ncrucial technological tool for enhancing the efﬁciency of drug target\ndiscovery. General-purpose language models offer signi ﬁcant\nadaptability in multi-task scenarios; however, when applied to\nspecialized domains, optimization through domain-adaptiveﬁne-\ntuning is often required to improve the accuracy of professional term\ninterpretation and contextual understanding. The primary strength\nof domain-speciﬁc language models lies in their ability to deeply\nintegrate subject-speci ﬁc knowledge. However, their high\nspecialization limits interdisciplinary applications due to technical\nconstraints. Future advancements are expected to focus on\ndeveloping hybrid architectures that balance domain-speci ﬁc\naccuracy with cross-domain generalization, providing more\neffective solutions for multidisciplinary research, including\nbiomedicine. It is important to note that current models are\npredominantly trained on historical literature databases, which\nmay result in the algorithm inheriting biases inherent in human\ncognition. Moreover, excessive dependence on existing literature\ndata could restrict the model ’s capacity for breakthrough\ninnovations in novel drug target discovery. Establishing a multi-\ndimensional collaborative framework that integrates natural\nlanguage models, computational biology models, and\nexperimental validation systems may represent a critical technical\npathway for identifying innovative and efﬁcient drug targets (Pun\net al., 2023; Sarumi and Heider, 2024).\n3 Analysis of the application of\ngenomics large language model in the\ndiscovery of drug target gene code\nWith the increasing demand for biological data mining in drug\ntarget discovery and new drug development, research has expanded\nthe application of natural language processing technology to\nbiological data, which is larger in scale, more complex, and\nhighly specialized. This has led to the emergence of genomics-\nfocused large language models. Genomics primarily investigates\nan organism’s complete DNA, emphasizing the detailed analysis\nof genome structure, function, evolution, mapping, and editing.\nAdvancements in next-generation genomic technologies have\nFrontiers inPharmacology frontiersin.org04\nLiu et al. 10.3389/fphar.2025.1597351\nenabled researchers to generate vast amounts of genomic data (Li R.\net al., 2022). The integration of large language models with genomic\nanalysis is now creating new research pathways and application\nscenarios. Trained on extensive genomic datasets, genome-focused\nlarge language models provide deeper insights into gene function,\nregulation, and interactions while possessing the capability to\npredict pathogenic variants and gene expression. These\ncapabilities establish a theoretical foundation for drug target\ndiscovery and offer strong support for the development of new\ntherapeutic agents.\n3.1 Gene function prediction\nThe genomics-focused large language model integrates DNA\nsequence data to identify functional elements, genetic variations,\nand structural features, provid ing a theoretical foundation for\ndrug target discovery. Notable examples include DNABERT (Ji\net al., 2021), which transforms DNA sequences into language\nsymbols and utilizes k-mers to capture intricate patterns,\nenabling high-precision predic tions of disease-associated\nmutations and DNA-protei n binding sites. LOGO ( Yang M.\net al., 2022 ), a lightweight human genome language model,\nwas developed using pre-trained parameters as initial weights.\nAfter task-speci ﬁc ﬁne-tuning, it has been effectively applied to\npromoter region identi ﬁcation, enhancer-promoter interaction\nprediction, chromatin feature inference, and pathogenic variant\nprioritization. Evo (Nguyen et al., 2024), a multimodal genome\ninfrastructure developed by Arc Institute, facilitates the analysis\nof natural genomic variations andp r e d i c t st h ee f f e c t so fs m a l l\nDNA modi ﬁcations on organism adaptability. This model\nrepresents a signi ﬁcant advancement in understanding and\ndesigning cross-modal biologi cal systems, establishing a\ntechnical foundation for precise target screening.\n3.2 Regulation of gene expression\nLarge language models signiﬁcantly enhance the identiﬁcation\nefﬁciency of core factors involved in gene expression regulation and\nenable high-precision prediction of gene interaction networks,\nproviding deeper mechanistic insights into gene regulatory\nnetwork analysis (Joachimiak et al., 2024). The Enformer model,\ndeveloped by DeepMind, constructs a quantitative prediction\nframework for enhancer-regulated gene expression by integrating\nlong-range interaction data spanning up to 200,000 base pairs in the\ngenome, exceeding traditional methods by more than ﬁvefold\n(Avsec et al., 2021). Notably, the progression of various diseases,\nincluding tumors, is often associated with epigenetic abnormalities,\nsuch asﬂuctuations in DNA methylation levels and disruptions in\nhistone modi ﬁcations, which can be targeted through\npharmacological interventions. The optimized GeneBERT model,\nbuilt on the BERT framework, specializes in genome function\nprediction, effectively inferring the impact of histone\nmodiﬁcation variations on gene expression and analyzing gene\nregulatory mechanisms. Additionally, models such as BERT6mA\n(Tsukiyama et al., 2022), iDNA-ABT (Yu et al., 2021), and MuLan-\nMethyl (Zeng et al., 2023) facilitate the analysis of DNA sequence\nmethylation characteristics and their potential inﬂuence on gene\nregulatory networks. These advancements have expanded the\nunderstanding of how epigenetic modi ﬁcations regulate gene\nexpression ( Sarumi and Heider, 2024 ), introducing a novel\nresearch paradigm for innovative drug target development.\n4 Analysis of the application of\ntranscriptomics large language model\nin the construction of drug-target\nrelated regulatory network\nWith advancements in genomic data analysis, research has\nincreasingly shifted toward exploring dynamic gene regulatory\nsystems, emphasizing the systematic study of the spatiotemporal\ncharacteristics of gene expression networks and regulatory\nmechanisms. Transcriptomics, which systematically examines\nall transcript products within an organism, provides essential\ndata for studying biological pro cesses by analyzing changes in\ngene expression levels and their functional regulation under\nvarious physiological and pathological conditions. This ﬁeld\nplays a crucial role in disease mechanism analysis and\nprecision medicine, offering a scienti ﬁcf o u n d a t i o nf o r\noptimizing clinical di agnostic and therapeutic strategies and\ndeveloping personalized medical approaches. Transcriptomics\nanalysis based on large language models facilitates key\nresearch tasks, including disease-speci ﬁc gene expression\nproﬁling, gene regulatory network reconstruction, and\npathogenic mechanism interpretation. Additionally, it\nestablishes a multi-dimensional data support framework for\ndrug target discovery, enhancing the efﬁciency and accuracy of\nidentifying potential therapeutic targets.\n4.1 RNA structure prediction\nStructural changes in RNA are often closely associated with its\nfunction. Predicting RNA secondary and tertiary structures allows for a\ndeeper understanding of its speciﬁc roles in biological processes and\nfacilitates the identiﬁcation of novel therapeutic targets. RNABERT\n(Akiyama and Sakakibara, 2022), a pre-trained model based on the\nBERT architecture, is speciﬁcally designed for secondary structure\nprediction and RNA clustering. This model ef ﬁciently aligns\nunknown sequences with existing RNA families, providing a\nvaluable tool for annotating newlydiscovered transcripts. RhoFold +\nintegrates the large-scale pre-trained RNA language model RNA-FM to\nextract sequence features and employs a deep learning module for end-\nto-end RNA three-dimensional st ructure prediction, addressing\nchallenges related to data scarcity (Shen et al., 2024). RNA structure\nprediction not only enhances the understanding of RNA function and\nbinding sites but also serves as a critical structural foundation for drug\ntarget discovery and the development of RNA-targeted therapeutics.\n4.2 Gene expression analysis\nIn May 2023, the Theodoris research team introduced Geneformer,\nthe ﬁrst general large language model in theﬁeld of transcriptomics\nFrontiers inPharmacology frontiersin.org05\nLiu et al. 10.3389/fphar.2025.1597351\n(Theodoris et al., 2023). This model was pre-trained using over\n30 million single-cell transcriptome datasets and enables three core\nfunctions: predicting gene network dynamics, mapping gene networks,\nand accelerating the identiﬁcation of therapeutic targets for diseases,\neven under sparse data conditions (Theodoris et al., 2023). The model\nwas applied to research on hypertrophic cardiomyopathy and dilated\ncardiomyopathy, successfully screening more than 400 associated genes\nfor each condition. In the case of hypertrophic cardiomyopathy,\nGeneformer accurately identiﬁed speciﬁc therapeutic targets and\npatented drug targets in cardiomyocytes. Additionally, for dilated\ncardiomyopathy, the inhibition of candidate genes predicted by the\nmodel demonstrated an improvement in cardiomyocyte function\nwithin disease models. These empirical ﬁndings highlight the\ntechnical signiﬁcance of Geneformer in the discovery of therapeutic\ntargets for human diseases.\nGeneCompass (Yang et al., 2024), developed by the research\nteam at the Chinese Academy of Sciences, is a cross-species\nfoundational model capable of deciphering gene regulatory codes.\nIt demonstrates signi ﬁcant potential in identifying key factors\ninvolved in cell fate regulation and screening candidate drug\ntargets. The Lomics ( Wong et al., 2024 ) model enhances the\nanalysis of complex gene interaction networks by improving the\naccuracy of pathway analysis and gene set enrichment in\ntranscriptome data, enabling the integrated analysis of multi-\nomics data. Notably, single-cell large model technologies, such as\nscBERT (Yang F. et al., 2022) and scFoundation (Hao et al., 2024),\nalso exhibit promising applications in single-cell transcriptome data\nanalysis and related research scenarios. In 2024, it was reported that\nscFoundation showed “the highest accuracy ” in the cell type\nannotation task, especially in identifying rare cell types such as\nCD4+ T helper 2 and CD34+ cells.\n4.3 Post-transcriptional regulatory studies\nPost-transcriptional regulation plays a crucial role in controlling\ngene expression through mechanisms such as RNA splicing, editing,\nstability regulation, transport, and translation, which are essential\nfor maintaining gene expression homeostasis. The SpliceBERT\nmodel ( Chen et al., 2024 ) enhances splice site prediction\naccuracy, enabling in-depth analysis of splice variant regulatory\nmechanisms in biological processes and their impact on gene\nexpression. Long non-coding RNA (lncRNA), a key transcription\nproduct, signiﬁcantly inﬂuences malignant tumor progression and\ndisease development. Recent studies have identiﬁed small open\nreading frames (sORFs) within lncRNAs capable of encoding\nfunctional peptides. The LncCat tool ( Feng et al., 2023 )i s\ndesigned to identify lncRNA molecules containing sORFs,\nproviding technical support for discovering novel regulatory\nelements. RNA modiﬁcations are widely involved in life activity\nregulation and disease evolution. The BERT-m7G system (Zhang\net al., 2021) accurately identiﬁes m7G modiﬁcation sites in RNA\nsequences, offering a foundation for understanding the regulatory\neffects of this modiﬁcation on gene function. By elucidating the\ndynamic regulatory network of gene expression, post-\ntranscriptional regulation research introduces innovative\ndirections for drug target identiﬁcation and signiﬁcantly advances\nthe development of novel therapeutics and personalized medicine.\n5 Analysis of the application of\nproteomics big language model in\naccelerating the prediction of drug\ntarget structure and function\nWhile investigating gene regulatory networks, drug target discovery\ncan also be approached by analyzing protein-level characteristics.\nProteins play a fundamental rolein life processes, serving as key\nexecutors of most cellular biological functions. Many diseases are\ndirectly associated with the dysfunction of speciﬁc proteins. Through\nan in-depth examination of protein structure, function, and\ninteractions, disease-related targets can be precisely identi ﬁed,\nfacilitating the development of highly speciﬁc and effective drugs. By\nlearning and analyzing protein sequence, structure, and omics data,\nlarge language models have demonstrated signiﬁcant potential in\naccelerating data analysis, enhancing drug target screening and\ndesign, and improving structure prediction. These advancements not\nonly increase research efﬁciency but also contribute to reducing overall\nresearch costs.\n5.1 Protein structure prediction\nThe three-dimensional conformation of proteins plays a crucial\nrole in drug target identiﬁcation, as the speciﬁc binding of drug\nmolecules to target proteins typically relies on precise structural\ncompatibility. As of 2025, the UniProt database contains over\n250 million protein sequences, while the PDB database holds\nonly about 240,000 3D structures covering about 70,000 proteins,\nrepresenting less than 0.1% of known proteins. Traditional\nexperimental techniques for protein structure analysis are time-\nintensive and costly, creating a signiﬁcant gap between output\nefﬁciency and the demands of drug research and development\n(Dana et al., 2019). In recent years, deep learning and artiﬁcial\nintelligence technologies have transformed protein structure\nprediction. A major breakthrough was achieved in 2020 with the\ndevelopment of AlphaFold2 by DeepMind. Utilizing a homologous\nsequence alignment strategy, this model reached near-experimental\naccuracy. Following the prediction of over 200 million protein\nstructures, about 35% met high-conﬁdence criteria, and 80% of\nthe structural data exhibited multi-dimensional characteristics. By\nintegrating an attention mechanism with a self-distillation training\nstrategy, the model signiﬁcantly enhances predictive performance,\nadvancing theﬁeld of protein structure analysis and its applications\nin drug discovery. RoseTTAFold, introduced alongside AlphaFold2,\nemploys a three-track attention architecture that enables neural\nnetworks to process three-dimensional information simultaneously,\nestablishing a technical benchmark comparable to AlphaFold2. In\n2022, Andrew G. Jamieson’s research group utilized the AlphaFold-\npredicted structure of the GPR84 protein to conduct a structure-\nactivity relationship study on GPR84 antagonists. This study\nidentiﬁed compounds 7 and 8 as exhibiting favorable activity and\nselectivity, validating the utility of AI-predicted structures in target\noptimization (Marsango et al., 2022 ; Mahin et al., 2022 ). On\n31 October 2023, AlphaFold3, jointly developed by DeepMind\nand Isomorphic Labs, introduced a diffusion module to replace\nthe original structure module, reducing reliance on homologous\nsequences. This model achieves high-accuracy predictions of protein\nFrontiers inPharmacology frontiersin.org06\nLiu et al. 10.3389/fphar.2025.1597351\ninteractions with various biomolecules, improving overall accuracy\nby more than 50% compared to its predecessor and achieving a\ntwofold breakthrough in key performance metrics. Recent studies\nintegrating AlphaFold3 with Mendelian randomization methods\nsuccessfully identi ﬁed seven proteins with structural\nabnormalities resulting from missense mutations, providing new\ninsights into the mechanisms of Alzheimer’s disease and facilitating\nthe screening of potential therapeutic targets.\nThe ﬁeld of protein structure prediction is undergoing a\nparadigm shift, with protein language models based on direct\nsequence prediction offering advantages in both computational\nefﬁciency and accuracy. RGN2, introduced in 2022, employs an\nend-to-end microcyclic geometric network architecture, where the\nAminoBERT protein language model extracts potential structural\nfeatures from unaligned sequences. This approach has demonstrated\nsuperior performance over traditional homology alignment\nmethods from both practical and theoretical perspectives.\nSimultaneously, Meta released the ESMFold model, built on the\nTransformer framework with 15 billion parameters, capable of\npredicting amino acid sequence structures with atomic-level\naccuracy. This model achieves a computational speed ten times\nfaster than AlphaFold2 while maintaining comparable prediction\naccuracy, enabling large-scale real-time resolution of metagenomic\nprotein structures. The average GDT_TS score of ESMFold on\nCASP15 was 61.62, close to the 73.06 of AlphaFold2, indicating\ngood performance but still a gap. ESMFold, on the other hand,\npredicted shorter amino acid sequences an order of magnitude faster\nthan AlphaFold2, even 60 times faster in some cases. Its subsequent\nversion, ESM2, further set the bar for protein language models.\nESM2 is also based on the Transformer architecture and focuses on\nefﬁcient generation of semantic representations of protein\nsequences. It performs well in protein structure prediction,\nfunction annotation (e.g., GO classi ﬁcation, BP function\nprediction), and drug target identi ﬁcation (e.g., TCR-pMHC\ninteraction prediction). Compared with models such as ProtT5,\nESM2 is particularly outstanding in terms of speed and performance\nbalance. Its simple “no external encoder” design simpliﬁes the\nprotein processing process and provides a powerful new tool for\nprotein engineering and target discovery. These advancements\nhighlight the capability of language models to identify\nevolutionary patterns and structural features from extensive\nsequence data, establishing a robust foundation for reverse\nmolecular docking and binding site similarity analysis in\nstructural biology.\n5.2 Protein sequence generation\nThe advancement of big data and arti ﬁcial intelligence\ntechnologies has introduced an innovative approach to target\ndiscovery— a new paradigm centered on protein sequence\ngeneration. The ProGen2 model (Nijkamp et al., 2023) generates\nnovel protein sequences with predeﬁned structural and functional\nproperties by analyzing complex sequence patterns and their\ninterrelations. ProtGPT2 (Ferruz et al., 2022), developed using\nthe GPT-2 autoregressive architecture, enhances protein\nengineering design and function prediction capabilities. The\ngenerated sequences exhibit compositional tendencies that align\nwith the distribution characteristics of natural amino acids. These\nAI-generated proteins not only adhere to the principles of biological\nevolution but can also be tailored to exhibit speciﬁc functional\nproperties, providing a valuable tool for identifying potential\ntargets in previously unexplored biological domains. Molecular\ndocking simulation technology allows researchers to virtually\nscreen these AI-generated sequences against existing drug\nlibraries, enabling the selection of candidate proteins with high\nbinding af ﬁnity, followed by targeted experimental\nvalidation studies.\n5.3 Protein function prediction\nProteins serve as core functional modules in regulating cellular\nmetabolism, signal transduction networks, and structural support\nsystems. Therefore, systematic analysis of protein biological\nfunctions holds signi ﬁcant scienti ﬁc value for drug target\ndiscovery and disease mechanism research (Liu et al., 2024). The\nProteinBERT model ( Brandes et al., 2022 )e fﬁciently captures\ncomplex sequence features and biological characteristics by\nprocessing large-scale sequence data, demonstrating strong\nversatility across various protein research tasks. The ProtST\nframework (Xu et al., 2023), a multimodal learning system for\nprotein sequences and biomedical texts, integrates sequence data\nwith textual information to enhance feature representation and\nenable in-depth protein function analysis. This model facilitates\nthe identiﬁcation of functional proteins from extensive databases,\neven in cases lacking functional annotation. ESM-1b applies a self-\nsupervised learning strategy to process vast numbers of unlabeled\nsequences, effectively extracting evolutionarily conserved features\nand residue interactions. The QuoteTarget method (Chen et al.,\n2023) innovatively combines ESM-1b with a graph convolutional\nneural network to achieve ef ﬁcient protein coding using only\nsequence information. This approach achieved 95% classiﬁcation\naccuracy on a non-redundant drug target validation dataset. When\napplied to the entire human proteome, it successfully identiﬁed\n1,213 previously unexplored potential therapeutic targets.\nIntrinsic disordered regions (IDRs) represent a distinct class\nof domains within protein se quences, characterized by\nconformational dynamics and t he absence of a stable three-\ndimensional structure under ph ysiological conditions. The\nstructural ﬂexibility of IDRs allows them to interact with\nvarious ligand molecules, making them highly valuable sites\nfor drug action. Accurate identi ﬁcation of IDRs and\nelucidation of their function al mechanisms are crucial for\nenhancing drug design ef ﬁciency ( Pang and Liu, 2024 ). The\nDisoFLAG model ( Pang and Liu, 2024 ), a specialized\nprediction tool for disordered regions, employs a sequence-\ndriven strategy to precisely locate IDRs and analyze their\nf u n c t i o n a lp r o p e r t i e s .A sa ne m e r g i n gt a r g e ti nd r u g\ndevelopment, IDRs may provide a novel pathway for\ntherapeutic discovery. In drug-target interaction studies,\naccurate assessment of binding af ﬁnity is a critical step in the\nresearch and development process. Traditional experimental\nmethods face limitations in scalability, leading to the\ndevelopment of various computational prediction models,\nincluding sequence-driven approa ches, graph neural networks,\nFrontiers inPharmacology frontiersin.org07\nLiu et al. 10.3389/fphar.2025.1597351\nand multimodal fusion techniques. While these methods have\nshown progress, further impro vements in prediction accuracy\nand mechanistic interpreta bility remain necessary.\n5.4 Antigen-receptor interaction and\nrecognition of neoantigens\nIn the study of cancer, immune system diseases, and infectious\ndiseases, a comprehensive understanding of the interaction\nmechanisms between antigens and receptors may contribute to\nthe advancement of drug target development. These fundamental\nresearch breakthroughs also provide new opportunities for the\nimplementation of personalized medical strategies. In China ’s\ncurrent medical innovation framework, malignant tumor research\nholds a dominant position in clinical trials and drug development (Li\net al., 2022b). Neoantigens, which serve as speciﬁc tumor markers,\narise from gene mutations or abnormal alterations in genetic\nmaterial within cancer cells. These antigens are entirely absent in\nhealthy tissues. Due to their unique tumor-speci ﬁc properties,\nneoantigens can effectively activate the immune response, making\nthem a focal point in immunotherapy research (Li et al., 2023). For\nexample, a team from Peking University Cancer Institute\nsuccessfully used a neoantigen prediction model combined with\nTCR sequencing to screen highly immunogenic neoantigens for\nspeciﬁc solid tumor patients and used them for individualized TCR-\nT cell therapy. The initiation of an immune response relies on the\nbinding of antigen peptides to T cell receptors (TCR), a process\nmediated by major histocompatibility complex (MHC) molecules.\nAs the central recognition element of T lymphocytes in identifying\npathogens and malignant cells, TCR plays a crucial role in the\nimmune defense mechanism. For the quantitative assessment of the\nbinding afﬁnity between MHC-I and MHC-II molecules and\npeptides, prediction models such as MHCRoBERTa ( Wang F.\net al., 2022 ) and BERTMHC ( Cheng et al., 2021 ) have been\ndeveloped, signi ﬁcantly enhancing the ef ﬁciency of predicting\ninteractions between key immune molecules. Based on the BERT\nframework, the TCR-BERT model (Wu et al., 2024) enables the\nanalysis and prediction of TCR-antigen interactions using deep\nlearning technology. This approach not only expands the\nanalytical scope of antigen recognition but also allows for more\nprecise and adaptable characterization of binding properties.\nNotably, the complementarity-determining region 3 (CDR3) of\nTCR molecules serves as a functional domain directly involved in\nantigen contact, with its sequence variation being a critical\ndeterminant of TCR receptor diversity. The TCR-BERT model\neffectively extracted generalized feature representations of TCR\nsequences by training on extensive unlabeled CDR3 sequence\ndata, and this pre-training strategy signi ﬁcantly improved the\nperformance of subsequent antigen-speci ﬁc recognition\nprediction models (Liu et al., 2024).\n5.5 Large language models of antibodies\nIn the advancement of immunology and biomedicine, large\nlanguage model-based technologies are increasingly\ndemonstrating their signi ﬁcance. These models, inspired by\nprotein language models, enable the prediction of key parameters\nsuch as antibody structural features, functional activity, interaction\ndynamics, and binding afﬁnity. By deciphering the“language rules”\nof antibodies, AntiBERTa can trace the developmental origins of\nB-cell-derived antibodies, evaluate immunogenicity strength, and\npredict potential binding sites and other complex tasks (Leem et al.,\n2022). As a deep learning-based epitope prediction tool,\nParaAntiProt efﬁciently extracts predictive feature embeddings by\nintegrating pre-trained protein and antibody language models. This\napproach requires only amino acid sequence data, eliminating\ndependence on antigen-related information ( Kalemati et al.,\n2024). Due to the unique gene rearrangement mechanism and\nthe diversity of complementarity-determining regions, antibody\nthree-dimensional structure prediction presents distinct\nchallenges in protein structure research. The pre-trained IgFold\nmodel (Ruffolo et al., 2023), trained on a dataset of 558 million\nnatural antibody sequences, achieves atomic coordinate prediction\nwith accuracy comparable to AlphaFold while offering signiﬁcant\nadvantages in computational efﬁciency. As an advanced iteration,\nAlphaFold3 has introduced breakthroughs in antibody structure\nprediction, particularly in the structural modeling of the heavy chain\ncomplementarity-determining region 3 (CDR H3), a critical domain\nthat deﬁnes antigen binding speciﬁcity and afﬁnity. This model has\nmarkedly reduced the root mean square deviation of predictions\nfrom 2.74 Å to 1.34 Å (Abramson et al., 2024). The application of\nlarge language models has greatly enhanced the efﬁciency and\noptimization of antibody design, providing essential technical\nsupport for novel drug target development. Additionally, these\nadvancements establish a crucial foundation for precision\nmedicine, vaccine development, and improvements in antibody-\nbased therapeutics.\n5.6 LLM-driven automated drug\nmolecule design\nAfter successfully identifying and validating protein targets and\ntheir key features (such as binding sites and functional domains),\none of the core challenges in drug development lies in efﬁciently\ndesigning candidate drug molecules that can effectively act on these\ntargets. Large language models (LLMs) are being utilized to build\nend-to-end automated molecular design systems, signi ﬁcantly\naccelerating this process. Take the DrugAgent system as an\nexample. As a multi-agent framework based on LLMs, it aims to\nautomate key steps in the drug discovery process, including data\nacquisition, model training, result evaluation, andﬁnal molecular\ndesign optimization, achieving an end-to-end design from target\ninformation to candidate molecules. The core of this system is\ndriven by two agent components: the LLM Director is\nresponsible for integrating professional knowledge from ﬁelds\nsuch as drug chemistry, biology, and pharmacology to guide the\ndirection of the entire molecule generation and optimization\nprocess; the LLM Planner is responsible for optimizing the search\nstrategy in the molecular space and dynamically adjusting the\ngeneration direction based on experimental feedback or\nprediction results. In empirical studies, the model guided by the\nDrugAgent system achieved signiﬁcant performance improvements\nin predicting the intestinal absorption characteristics of drug\nFrontiers inPharmacology frontiersin.org08\nLiu et al. 10.3389/fphar.2025.1597351\nmolecules (based on the PAMPA dataset), with an F1 score reaching\n0.92, signiﬁcantly outperforming traditional methods, effectively\ndemonstrating the effectiveness of LLMs in integrating\nknowledge, guiding model selection, and optimizing the\nprediction process. The technical highlight of this system lies in\nits dynamic idea space management capability, which can generate\nstructurally diverse molecules in the early exploration stage to\nexpand the coverage of the chemical space; and it builds an\niterative optimization closed loop, continuously improving the\nquality of generated molecules based on experimental data or\ncomputational prediction feedback (such as absorption, activity,\ntoxicity, etc.). Ultimately, through the automatic coordination of\ndata, models, and decisions by the agents, it signiﬁcantly reduces\nmanual intervention and lowers the reliance on the highly expert-\nexperience-dependent and manual operation-intensive steps in\ntraditional drug design, signiﬁcantly enhancing overall efﬁciency.\nThe DrugAgent system is a typical representative of the LLM-\nenabled “rapid design” paradigm for drug molecules. Its core lies\nin constructing a highly automated “design-predict-verify (or\nsimulate)-redesign” closed loop. The LLM Planner can adjust the\nsearch strategy and generation direction in real time based on\nfeedback (whether from experiments or high-precision\ncomputational simulations, such as molecular docking and\nADMET prediction models), for example, focusing on speci ﬁc\nchemical spaces, avoiding known toxic groups, and optimizing\nspeciﬁc properties. This dynamic, data-driven iterative process\ngreatly accelerates the cycle of lead compound discovery and\noptimization, signi ﬁcantly differentiating from the time-\nconsuming and labor-intensive trial-and-error exploration in\ntraditional methods. By combining expert knowledge (encoded in\nthe LLM Director) with automated iterative optimization, such\nsystems can explore the vast chemical space at a speed far\nexceeding traditional methods, achieving rapid discovery and\nevolution of candidate molecules.\n6 Analysis of the application of single-\ncell multi-omics large language model\nin the integration of multi-dimensional\ndata for drug target discovery\nAcross multiple research domains, including genomics,\ntranscriptomics, and proteomics, large-scale language models\nfacilitate targeted analysis and provide critical insights for drug\ntarget screening. Notably, single-cell multi-omics language models\nsigniﬁcantly broaden the scope of target identiﬁcation by integrating\nmulti-dimensional data, enabling the discovery of potential\ntherapeutic targets that may not be captured using traditional\nmethods. As systems medicine continues to drive improvements\nin drug research and development ef ﬁciency, multi-omics\ntechnology has emerged as a key enabler (Wang M. et al., 2022).\nBy collaboratively analyzing biological data across genomics,\ntranscriptomics, proteomics, and metabolomics, researchers can\nperform cross-comparisons and in-depth assessments of multi-\nsource omics data. This approach allows for precise identiﬁcation\nof disease-associated signaling pathways and core regulatory\nelements, ultimately leading to the selection of candidate targets\nwith therapeutic potential. This systematic research framework not\nonly enhances the understanding of disease mechanisms but also\nprovides a scientiﬁc foundation for drug molecular design and\noptimization of therapeutic efﬁcacy, achieving the dual objectives\nof improving treatment outcomes while minimizing adverse effects.\nThe application of large language models in multi-omics data\nintegration is demonstrating signiﬁcant breakthroughs. The scGPT\nmodel (Cui et al., 2024), leveraging single-cell multi-omics data,\nenables in-depth analysis of gene interactions at single-cell\nresolution through the collaborative examination of genetic\nregulatory networks, enhancing the biological interpretability of\nthe model. The scMVP system (Li et al., 2022c) is an innovative\nframework speciﬁcally designed for the integrated analysis of single-\ncell transcriptome sequencing (RNA-seq) and epigenome\nsequencing (ATAC-seq) data, facilitating simultaneous\nexamination of gene expression patterns and chromatin\naccessibility within individual cells. For single-cell multimodal\ndata, including RNA sequencing, ATAC sequencing, and\nantibody marker sequencing, the DeepMAPS model (Ma et al.,\n2023) successfully establishes the mapping relationship between cell\nsubtypes and gene functional modules by constructing a gene-cell\ntwo-dimensional network, enabling the collaborative learning of\nboth local and global features. By incorporating high-precision\nmulti-dimensional data, single-cell multi-omics research offers\ninnovative solutions to address challenges such as data variability,\nsample dispersion, and cell subset diversity ( Liu et al.,\n2024)( Figure 2).\n7 Summary and outlook\nDrug development, a time-intensive and costly process,\nprioritizes drug target discovery, which involves identifying\nbiomolecules or regulatory pathways that play a critical role in\ndisease mechanisms. However, due to the high complexity and\ntechnical challenges in this ﬁeld, the number of validated\neffective drug targets remains limited. In recent years,\nadvancements in experimental technology, multi-omics analysis\nplatforms, and computational methods have signi ﬁcantly\ncontributed to the reﬁnement of target identiﬁcation strategies.\nDespite these advancements, traditional experimental approaches\nand multi-omics studies often require substantial resources, and the\nreliability of results is highly dependent on the standardization of\nbiological samples. Artiﬁcial intelligence technology, particularly\nmodels based on the Transformer architecture, is reshaping the\ndrug research and development landscape. These models achieve\nhuman language parsing and logical text generation through deep\nlearning from large-scale text datasets. In drug target design, natural\nlanguage processing enables systematic literature mining and the\nconstruction of patent maps, enhancing the efﬁciency and accuracy\nof target discovery. Specialized models such as BioBERT\nsigniﬁcantly enhance the accuracy and efﬁciency of biomedical\ntext processing by precisely analyzing scientiﬁc concepts. Notably,\nlarge language models have demonstrated breakthrough potential in\ngenome analysis, transcriptional regulation research, protein system\nanalysis, and single-cell multi-omics integration. In genomics,\ndedicated models have deepened the understanding of gene\nfunction, regulatory mechanisms, and interactions while\nsigniﬁcantly improving the prediction of pathogenic mutations\nFrontiers inPharmacology frontiersin.org09\nLiu et al. 10.3389/fphar.2025.1597351\nand inference of expression patterns. In transcriptomics, tools such\nas Geneformer simulate dynamic changes in gene networks,\nproviding a multi-dimensional biological foundation for drug\ntarget discovery. Protein language models play a crucial role in\nstructure-activity relationship prediction, target screening, and\noptimization, continuously improving research ef ﬁciency and\nreducing development costs. Single-cell multi-omics integration\nmodels effectively analyze disease-related signaling pathways and\ncore regulatory elements by integrating multi-dimensional data.\nThese technological advancements not only enhance the\nunderstanding of fundamental biological processes but also\naccelerate target identiﬁcation and drug development, paving the\nway for personalized medicine.\nAlthough arti ﬁcial intelligence has achieved signi ﬁcant\nadvancements in target discover y, several challenges remain in\nits practical application. A m ajor limitation is the need to\nimprove the interpretability of algorithmic predictions, which\nis essential for gainin g acceptance in scienti ﬁcr e s e a r c ha n d\nmedical ﬁelds. Current models primarily focus on sequence\nfeatures and functional analysis of gene expression regulatory\nnetworks. However, limited consideration of critical parameters\nsuch as target speciﬁcity, tissue distribution characteristics, and\ntoxicological properties may re duce the translational value of\nprediction results. Strengthen i n gt h e s ea s p e c t si sc r u c i a lf o r\nenhancing the reliability and applicability of AI-driven target\ndiscovery in drug development. Data bias presents a signiﬁcant\nFIGURE 2\nPerformance comparison and applicable scenarios of different large language models in drug target discovery.\nFrontiers inPharmacology frontiersin.org10\nLiu et al. 10.3389/fphar.2025.1597351\nchallenge in model training, as models developed using biased\ndatasets may reinforce in herent cognitive biases ( Sarumi and\nHeider, 2024). Addressing this issue requires the construction of\ndiverse and representative train ing datasets to enhance model\ngeneralization. Additionally, i n the extensive data collection and\nmodel development processes, ethical review, privacy protection,\nand compliance framework establishment remain critical areas\nfor improvement (Pun et al., 2023). Going forward, establishing a\nuniﬁed, comprehensive and challenging Benchmarking system is\nessential to promote the development of LLM in theﬁeld of drug\ntarget discovery. Currently, the data sets, evaluation metrics, and\ncomparison baselines used by d ifferent research teams vary\ngreatly, making it dif ﬁcult to objectively compare model\nperformance. There is an urgent need to build standardized\nbenchmark datasets and evaluation protocols covering the\nwhole process of target discovery, from literature knowledge\nmining, omics data analysis, t arget property prediction to\nmolecule generation and optimization. These benchmarks\nshould incorporate publicly ava ilable gold standard datasets\n(e.g., known validated target s, protein-ligand complex\nstructures, activity/toxicity da ta), negative samples (nontarget/\ninactive molecules), and tasks that mimic real-world complexity\n(e.g., predicting novel targets, processing noisy data, generalizing\nacross tissues/diseases). At the same time, the evaluation index\nshould go beyond the simple accuracy or AUC, and should\ninclude the comprehensive consi deration of the biological\nrationality, interpretability, computational ef ﬁciency and\nﬁnally the impact on the success rate of experimental\nveriﬁcation. Strong benchmarking will facilitate iterative\nmodel optimization, fair comp arison of different technology\nroutes, and ultimately drive the establishment of best practices\nin the ﬁeld. With continuous technological advancements, large\nlanguage models are expected to further expand their\napplications in drug research and development, accelerating\nprogress through innovative analytical paradigms. These\nadvancements will enhance the ef ﬁciency, innovation, and\ncost-effectiveness of drug target discovery and new drug\ndevelopment, ultimately driving a transformative shift in the\npharmaceutical industry.\nAuthor contributions\nXL: Formal Analysis, Methodology, Writing– original draft,\nData curation, Conceptualization. JZ: Writing– original draft. XW:\nWriting – original draft. MT: Writing – original draft. GW:\nWriting – review & editing. XZ: Writing– review & editing.\nFunding\nThe author(s) declare thatﬁnancial support was received for the\nresearch and/or publication of this article. This work was supported\nby the Shandong Province Health Science and Technology\nInnovation Team Construction Project and Project of the Key\nResearch and Development Program of Shandong Province\n(Major Science and Technology Innovation Project)\n2023CXGC010509.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors and\ndo not necessarily represent those of their afﬁliated organizations, or\nthose of the publisher, the editors and the reviewers. Any product that\nmay be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAbramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., et al. (2024).\nAccurate structure prediction of biomolecular interactions with AlphaFold 3.Nature\n630, 493–500. doi:10.1038/s41586-024-07487-w\nAkiyama, M., and Sakakibara, Y. (2022). Informative RNA base embedding for RNA\nstructural alignment and clustering by deep representation learning. NAR Genom.\nBioinform 4, lqac012. doi:10.1093/nargab/lqac012\nAvsec, Ž., Agarwal, V., Visentin, D., Ledsam, J. R., Grabska-Barwinska, A., Taylor, K.\nR., et al. (2021). Effective gene expression prediction from sequence by integrating long-\nrange interactions. Nat. Methods 18, 1196–1203. doi:10.1038/s41592-021-01252-x\nBrandes, N., Ofer, D., Peleg, Y., Rappoport, N., and Linial, M. (2022). ProteinBERT: a\nuniversal deep-learning model of protein sequence and function.Bioinformatics 38,\n2102–2110. doi:10.1093/bioinformatics/btac020\nChen, J., Gu, Z., Xu, Y., Deng, M., Lai, L., and Pei, J. (2023). QuoteTarget: a sequence-\nbased transformer protein language model to identify potentially druggable protein\ntargets. Protein Sci. 32, e4555. doi:10.1002/pro.4555\nChen, K., Zhou, Y., Ding, M., Wang, Y., Ren, Z., and Yang, Y. (2024). Self-supervised\nlearning on millions of primary RNA sequences from 72 vertebrates improves\nsequence-based RNA splicing prediction.Brief. Bioinform. 25, bbae163. doi:10.1093/\nbib/bbae163\nCheng, J., Bendjama, K., Rittner, K., and Malone, B. (2021). BERTMHC: improved\nMHC–peptide class II interaction prediction with transformer and multiple instance\nlearning. Bioinformatics 37, 4172–4179. doi:10.1093/bioinformatics/btab422\nCui, H., Wang, C., Maan, H., Pang, K., Luo, F., Duan, N., et al. (2024). scGPT: toward\nbuilding a foundation model for single-cell multi-omics using generative AI.Nat.\nMethods 21, 1470–1480. doi:10.1038/s41592-024-02201-0\nDana, J. M., Gutmanas, A., Tyagi, N., Qi, G., O’Donovan, C., Martin, M., et al. (2019).\nSIFTS: updated structure integration with function, taxonomy and sequences resource\nallows 40-fold increase in coverage of structure-based annotations for proteins.Nucleic\nAcids Res. 47, D482–D489. doi:10.1093/nar/gky1114\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. (2019).“Bert: pre-training of deep\nbidirectional transformers for language understanding, ” in Proceedings of the\n2019 conference of the north American chapter of the Association for computational\nlinguistics: human Language technologies (Minneapolis, Minnesota: Association for\nComputational Linguistics).\nFeng, H., Wang, S., Wang, Y., Ni, X., Yang, Z., Hu, X., et al. (2023). LncCat: an ORF\nattention model to identify LncRNA based on ensemble learning strategy and fused\nsequence information.Comput. Struct. Biotechnol. J.21, 1433–1447. doi:10.1016/j.csbj.\n2023.02.012\nFrontiers inPharmacology frontiersin.org11\nLiu et al. 10.3389/fphar.2025.1597351\nF e r r u z ,N . ,S c h m i d t ,S . ,a n dH ö c k e r ,B .( 2 0 2 2). ProtGPT2 is a deep unsupervised language\nmodel for protein design.Nat. Commun.13, 4348. doi:10.1038/s41467-022-32007-7\nGangwal, A., Ansari, A., Ahmad, I., Azad, A. K., Kumarasamy, V., Subramaniyan, V.,\net al. (2024). Generative artiﬁcial intelligence in drug discovery: basic framework, recent\nadvances, challenges, and opportunities.Front. Pharmacol. 15, 1331062. doi:10.3389/\nfphar.2024.1331062\nGu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., et al. (2021). Domain-\nspeciﬁc language model pretraining for biomedical natural language processing.ACM\nTrans. Comput. Healthc.3, 1–23. doi:10.1145/3458754\nHao, M., Gong, J., Zeng, X., Liu, C., Guo, Y., Cheng, X., et al. (2024). Large-scale\nfoundation model on single-cell transcriptomics.Nat. Methods21, 1481–1491. doi:10.\n1038/s41592-024-02305-7\nHinkson, I. V., Madej, B., and Stahlberg, E. A. (2020). Accelerating therapeutics for\nopportunities in medicine: A paradigm shift in drug discovery.Front. Pharmacol. 11,\n770. doi:10.3389/fphar.2020.00770\nJ i ,Y . ,Z h o u ,Z . ,L i u ,H . ,a n dD a v u l u r i ,R .V .( 2 0 2 1 ) .D N A B E R T :P r e - t r a i n e d\nbidirectional encoder representations from transformers model for DNA-\nlanguage in genome. Bioinformatics 37, 2112 –2120. doi:10.1093/\nbioinformatics/btab083\nJoachimiak, M. P., Cauﬁe l d ,J .H . ,H a r r i s ,N .L . ,K i m ,H . ,a n dM u n g a l l ,C .J .( 2 0 2 4 ) .G e n es e t\nsummarization using large language models.arXiv, 2305. doi:10.48550/arxiv.2305.13338\nKalemati, M., Noroozi, A., Shahbakhsh, A., and Koohi, S. (2024). ParaAntiProt\nprovides paratope prediction using antibody and protein language models.Sci. Rep.14,\n29141. doi:10.1038/s41598-024-80940-y\nLee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., et al. (2020). BioBERT: A pre-\ntrained biomedical language representation model for biomedical text mining.\nBioinformatics 36, 1234–1240. doi:10.1093/bioinformatics/btz682\nLeem, J., Mitchell, L. S., Farmery, J. H. R., Barton, J., and Galson, J. D. (2022).\nDeciphering the language of antibodies using self-supervised learning. Patterns 3,\n100513. doi:10.1016/j.patter.2022.100513\nLi, G., Fu, S., Wang, S., Zhu, C., Duan, B., Tang, C., et al. (2022c). A deep generative\nmodel for multi-view proﬁling of single-cell RNA-seq and ATAC-seq data.Genome\nBiol. 23, 20. doi:10.1186/s13059-021-02595-6\nLi, G., Liu, Y., Hu, H., Yuan, S., Zhou, L., and Chen, X. (2022b). Evolution of\ninnovative drug R&D in China. Nat. Rev. Drug Discov. 21, 553–554. doi:10.1038/\nd41573-022-00058-6\nLi, R., Li, L., Xu, Y., and Yang, J. (2022a). Machine learning meets omics: applications\nand perspectives. Brief. Bioinform. 23, bbab460. doi:10.1093/bib/bbab460\nLi, T., Li, Y., Zhu, X., He, Y., Wu, Y., Ying, T., et al. (2023). Artiﬁcial intelligence in\ncancer immunotherapy: applications in neoantigen recognition, antibody design and\nimmunotherapy response prediction.Seminars Cancer Biol.91, 50–69. doi:10.1016/j.\nsemcancer.2023.02.007\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., et al. (2023). Evolutionary-scale\nprediction of atomic-level protein structure with a language model. Science 379,\n1123–1130. doi:10.1126/science.ade2574\nLiu, J., Yang, M., Yu, Y., Xu, H., Li, K., and Zhou, X. (2024). Large language models in\nbioinformatics: applications and perspectives.arXiv\n, 2401.\nLuo, R., Sun, L., Xia, Y., Qin, T., Zhang, S., Poon, H., et al. (2022). BioGPT: generative\npre-trained transformer for biomedical text generation and mining.Brief. Bioinform.23,\nbbac409. doi:10.1093/bib/bbac409\nMa, A., Wang, X., Li, J., Wang, C., Xiao, T., Liu, Y., et al. (2023). Single-cell biological\nnetwork inference using a heterogeneous graph transformer.Nat. Commun. 14, 964.\ndoi:10.1038/s41467-023-36559-0\nMahindra, A., Jenkins, L., Marsango, S., Huggett, M., Huggett, M., Robinson, L., et al.\n(2022). Investigating the structure–activity relationship of 1, 2, 4-triazine G-protein-\ncoupled receptor 84 (GPR84) antagonists.J. Med. Chem.65, 11270–11290. doi:10.1021/\nacs.jmedchem.2c00804\nMarsango, S., Barki, N., Jenkins, L., Tobin, A. B., and Milligan, G. (2022). Therapeutic\nvalidation of an orphan G protein-coupled receptor: the case of GPR84. Br.\nJ. Pharmacol. 179, 3529–3541. doi:10.1111/bph.15248\nMicrosoft Research AI4ScienceMicrosoft Quantum (2023). The impact of large\nlanguage models on scientiﬁc discovery: a preliminary study using gpt-4. arXiv.\ndoi:10.48550/arXiv.2311.07361\nNguyen, E., Poli, M., Durrant, M. G., Kang, B., Katrekar, D., Li, D. B., et al. (2024).\nSequence modeling and design from molecular to genome scale with Evo.Science 386,\neado9336. doi:10.1126/science.ado9336\nNijkamp, E., Ruffolo, J. A., Weinstein, E. N., Naik, N., and Madani, A. (2023).\nProgen2: exploring the boundaries of protein language models.Cell Syst.14, 968–978.e3.\ndoi:10.1016/j.cels.2023.10.002\nPang, Y., and Liu, B. (2024). DisoFLAG: accurate prediction of protein intrinsic\ndisorder and its functions using graph-based interaction protein language model.BMC\nBiol. 22, 3. doi:10.1186/s12915-023-01803-y\nPark, G., Yoon, B. J., Luo, X., Lpez-Marrero, V., Johnstone, P., Yoo, S., et al. (2023).\n“Automated extraction of molecular interactions and pathway knowledge using large\nlanguage model, galactica: opportunities and challenges,” in The 22nd workshop on\nbiomedical natural Language processing and BioNLP shared tasks(Toronto, Canada:\nAssociation for Computational Linguistics).\nPun, F. W., Ozerov, I. V., and Zhavoronkov, A. (2023). AI-powered therapeutic target\ndiscovery. Trends Pharmacol. Sci.44, 561–572. doi:10.1016/j.tips.2023.06.010\nRen, F., Ding, X., Zheng, M., Korzinkin, M., Cai, X., Zhu, W., et al. (2023). AlphaFold\naccelerates artiﬁcial intelligence powered drug discovery: efﬁcient discovery of a novel\nCDK20 small molecule inhibitor.Chem. Sci. 14, 1443–1452. doi:10.1039/d2sc05709c\nRuffolo, J. A., Chu, L.-S., Mahajan, S. P., and Gray, J. J. (2023). Fast, accurate antibody\nstructure prediction from deep learning on massive set of natural antibodies.Nat.\nCommun. 14, 2389. doi:10.1038/s41467-023-38063-x\nSarumi, O. A., and Heider, D. (2024). Large language models and their applications in\nbioinformatics.Comput. Struct. Biotechnol. J.23, 3498–3505. doi:10.1016/j.csbj.2024.09.031\nSavage, N. (2023). Drug discovery companies are customizing ChatGPT: here’s how.\nNat. Biotechnol. 41, 585–586. doi:10.1038/s41587-023-01788-7\nShen, T., Hu, Z., Sun, S., Liu, D., Wong, F., Wang, J., et al. (2024). Accurate RNA 3D\nstructure prediction using a language model-based deep learning approach. Nat.\nMethods 21, 2287–2298. doi:10.1038/s41592-024-02487-0\nTheodoris, C. V., Xiao, L., Chopra, A., Chafﬁn, M. D., Al Sayed, Z. R., Hill, M. C., et al.\n(2023). Transfer learning enables predictions in network biology.Nature 618, 616–624.\ndoi:10.1038/s41586-023-06139-9\nTsukiyama, S., Hasan, M. M., Deng, H.-W., and Kurata, H. (2022). BERT6mA:\nprediction of DNA N6-methyladenine site using deep learning-based approaches.Brief.\nBioinform. 23, bbac053. doi:10.1093/bib/bbac053\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in 31st annual conference on neural\ninformation processing systems, neural information processing systems (Long\nBeach, CA, USA).\nW a n g ,F . ,W a n g ,H . ,W a n g ,L . ,L u ,H . ,Q i u ,S . ,Z a n g ,T . ,e ta l .( 2 0 2 2 a ) .M H C R o B E R T a :p a n -\nspeciﬁcp e p t i d e–MHC class I binding prediction through transfer learning with label-agnostic\nprotein sequences.Brief. Bioinform.23, bbab595. doi:10.1093/bib/bbab595\nW a n g ,M . ,Z h a n g ,Z . ,L i u ,J . ,S o n g ,M . ,Z h a n g ,T . ,C h e n ,Y . ,e ta l .( 2 0 2 2 b ) .G eﬁtinib and\nfostamatinib target EGFR and SYK to attenuate silicosis: a multi-omics study with drug\nexploration.Signal Transduct. Target Ther.7, 157. doi:10.1038/s41392-022-00959-3\nWong, C. K., Choo, A., Cheng, E. C. C., San, W. C., Cheng, K. C. K., Lau, Y. M., et al.\n(2024). Lomics: generation of pathways and gene sets using large language models for\ntranscriptomic analysis. arXiv. doi:10.48550/arXiv.2407.09089\nW u ,K .E . ,Y o s t ,K . ,D a n i e l ,B . ,B e l k ,J . ,X i a ,Y . ,E g a w a ,T . ,e ta l .( 2 0 2 4 ) .“TCR-\nBERT: learning the grammar of T-cell receptors for ﬂexible antigen-binding\nanalyses, ” in Proceedings of the 18th machine learning in computational biology\nmeeting (PMLR).\nXu, M., Yuan, X., Miret, S., and Tang, J. P. (2023).“Multi-modality learning of protein\nsequences and biomedical texts,” in Proceedings of the 40th international conference on\nmachine learning (PMLR).\nYang, F., Wang, W., Wang, F., Fang, Y., Tang, D., Huang, J., et al. (2022b). scBERT as\na large-scale pretrained deep language model for cell type annotation of single-cell\nRNA-seq data. Nat. Mach. Intell.4, 852–866. doi:10.1038/s42256-022-00534-z\nYang, M., Huang, L., Huang, H., Tang, H., Zhang, N., Yang, H., et al. (2022a).\nIntegrating convolution and self-attention improves language model of human genome\nfor interpreting non-coding regions at base-resolution. Nucleic Acids Res. 50, e81.\ndoi:10.1093/nar/gkac326\nYang, X., Liu, G., Feng, G., Bu, D., Wang, P., Jiang, J., et al. (2024). GeneCompass:\ndeciphering universal gene regulatory mechanisms with a knowledge-informed cross-\nspecies foundation model.Cell Res. 34, 830–845. doi:10.1038/s41422-024-01034-y\nYu, Y., He, W., Jin, J., Xiao, G., Cui, L., Zeng, R., et al. (2021). iDNA-ABT: advanced\ndeep learning model for detecting DNA methylation with adaptive features and\ntransductive information maximization. Bioinformatics 37, 4603–4610. doi:10.1093/\nbioinformatics/btab677\nZeng, W., Gautam, A., and Huson, D. H. (2023). MuLan-Methyl — Multiple\ntransformer-based language models for accurate DNA methylation prediction.\nGigascience 12, giad054. doi:10.1093/gigascience/giad054\nZhang, L., Qin, X., Liu, M., Liu, G., and Ren, Y. (2021). BERT-m7G: a transformer\narchitecture based on BERT and stacking ensemble to identify RNA N7-\nMethylguanosine sites from sequence information. Comput. Math. Methods Med.\n2021, 7764764. doi:10.1155/2021/7764764\nZheng, Y., Koh, H. Y., Yang, M., Li, L., May, L. T., Webb, G. I., et al. (2024). Large\nlanguage models in drug discovery and development: from disease mechanisms to\nclinical trials. arXiv. doi:10.48550/arXiv.2409.04481\nZhou, H., Liu, F., Gu, B., Zou, X., Huang, J., Wu, J., et al. (2023). A survey of large\nlanguage models in medicine: progress, application, and challenge.arXiv. doi:10.48550/\narXiv.2311.05112\nZ h o u ,Y . ,Z h a n g ,Y . ,L i a n ,X . ,L i ,F . ,W a n g ,C . ,Z h u ,F . ,e ta l .( 2 0 2 2 ) .T h e r a p e u t i c\ntarget database update 2022: facilitating drug discovery with enriched comparative\ndata of targeted agents. Nucleic Acids Res. 50, D1398 –D1407. doi:10.1093/\nnar/gkab953\nFrontiers inPharmacology frontiersin.org12\nLiu et al. 10.3389/fphar.2025.1597351",
  "topic": "Drug discovery",
  "concepts": [
    {
      "name": "Drug discovery",
      "score": 0.8030003309249878
    },
    {
      "name": "Computer science",
      "score": 0.5171846151351929
    },
    {
      "name": "Drug",
      "score": 0.4657277464866638
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4087143540382385
    },
    {
      "name": "Computational biology",
      "score": 0.3760013282299042
    },
    {
      "name": "Natural language processing",
      "score": 0.370846688747406
    },
    {
      "name": "Pharmacology",
      "score": 0.26826024055480957
    },
    {
      "name": "Medicine",
      "score": 0.2645680010318756
    },
    {
      "name": "Bioinformatics",
      "score": 0.24910318851470947
    },
    {
      "name": "Biology",
      "score": 0.19568410515785217
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210163399",
      "name": "Shandong First Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116517",
      "name": "Shandong Provincial Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210097945",
      "name": "Linyi People's Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210156545",
      "name": "Dongyang People's Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I27781120",
      "name": "Wenzhou Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210145099",
      "name": "Xian Yang Central Hospital",
      "country": "CN"
    }
  ],
  "cited_by": 5
}